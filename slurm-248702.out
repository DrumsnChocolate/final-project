/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/20 01:15:07 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 01:15:07 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 01:15:07 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/20 01:15:07 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 01:15:07 visual_prompt]: Training with config:
[11/20 01:15:07 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.005_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 01:15:07 visual_prompt]: Loading training data...
[11/20 01:15:07 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 01:15:07 visual_prompt]: Loading validation data...
[11/20 01:15:07 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 01:15:07 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/20 01:15:09 visual_prompt]: Enable all parameters update during training
[11/20 01:15:09 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/20 01:15:09 visual_prompt]: tuned percent:100.000
[11/20 01:15:10 visual_prompt]: Device used for model: 0
[11/20 01:15:10 visual_prompt]: Setting up Evaluator...
[11/20 01:15:10 visual_prompt]: Setting up Trainer...
[11/20 01:15:10 visual_prompt]: 	Setting up the optimizer...
[11/20 01:15:10 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 01:16:56 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9294 s / batch. (data: 1.04e-02). ETA=14:15:05, max mem: 23.5 GB 
[11/20 01:18:35 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9206 s / batch. (data: 2.56e-04). ETA=14:05:22, max mem: 23.5 GB 
[11/20 01:20:12 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9331 s / batch. (data: 5.35e-03). ETA=14:15:21, max mem: 23.5 GB 
[11/20 01:21:45 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9525 s / batch. (data: 2.45e-02). ETA=14:31:34, max mem: 23.5 GB 
[11/20 01:23:22 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9233 s / batch. (data: 5.37e-03). ETA=14:03:15, max mem: 23.5 GB 
[11/20 01:24:11 visual_prompt]: Epoch 1 / 100: avg data time: 5.12e-02, avg batch time: 0.9789, average train loss: 7.6130
[11/20 01:25:08 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3035, average loss: 6.9126
[11/20 01:25:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/20 01:25:08 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/20 01:26:52 visual_prompt]: 	Training 100/553. train loss: 0.9451,	0.9158 s / batch. (data: 2.75e-04). ETA=13:54:06, max mem: 23.5 GB 
[11/20 01:28:26 visual_prompt]: 	Training 200/553. train loss: 0.6654,	1.2250 s / batch. (data: 2.97e-01). ETA=18:33:42, max mem: 23.5 GB 
[11/20 01:30:04 visual_prompt]: 	Training 300/553. train loss: 0.9097,	0.9210 s / batch. (data: 1.04e-02). ETA=13:55:45, max mem: 23.5 GB 
[11/20 01:31:40 visual_prompt]: 	Training 400/553. train loss: 0.6860,	0.9371 s / batch. (data: 5.81e-03). ETA=14:08:47, max mem: 23.5 GB 
[11/20 01:33:16 visual_prompt]: 	Training 500/553. train loss: 1.2700,	0.9369 s / batch. (data: 1.29e-02). ETA=14:07:04, max mem: 23.5 GB 
[11/20 01:34:05 visual_prompt]: Epoch 2 / 100: avg data time: 5.09e-02, avg batch time: 0.9712, average train loss: 1.2961
[11/20 01:35:01 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3019, average loss: 0.8355
[11/20 01:35:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.98	
[11/20 01:35:01 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/20 01:36:46 visual_prompt]: 	Training 100/553. train loss: 0.6008,	0.9056 s / batch. (data: 2.53e-04). ETA=13:36:28, max mem: 23.5 GB 
[11/20 01:38:23 visual_prompt]: 	Training 200/553. train loss: 3.0857,	0.9238 s / batch. (data: 2.48e-04). ETA=13:51:21, max mem: 23.5 GB 
[11/20 01:39:58 visual_prompt]: 	Training 300/553. train loss: 0.6149,	0.9440 s / batch. (data: 7.10e-04). ETA=14:07:55, max mem: 23.5 GB 
[11/20 01:41:33 visual_prompt]: 	Training 400/553. train loss: 0.3387,	0.9281 s / batch. (data: 1.34e-02). ETA=13:52:06, max mem: 23.5 GB 
[11/20 01:43:08 visual_prompt]: 	Training 500/553. train loss: 1.9179,	0.9315 s / batch. (data: 2.74e-04). ETA=13:53:34, max mem: 23.5 GB 
[11/20 01:43:58 visual_prompt]: Epoch 3 / 100: avg data time: 4.39e-02, avg batch time: 0.9692, average train loss: 0.8792
[11/20 01:44:54 visual_prompt]: Inference (val):avg data time: 4.43e-04, avg batch time: 0.3025, average loss: 1.0940
[11/20 01:44:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.47	
[11/20 01:44:54 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/20 01:46:39 visual_prompt]: 	Training 100/553. train loss: 1.3328,	0.9203 s / batch. (data: 9.12e-03). ETA=13:41:11, max mem: 23.5 GB 
[11/20 01:48:14 visual_prompt]: 	Training 200/553. train loss: 0.9479,	1.5000 s / batch. (data: 5.83e-01). ETA=22:16:01, max mem: 23.5 GB 
[11/20 01:49:49 visual_prompt]: 	Training 300/553. train loss: 1.2966,	0.9099 s / batch. (data: 2.82e-04). ETA=13:28:55, max mem: 23.5 GB 
[11/20 01:51:25 visual_prompt]: 	Training 400/553. train loss: 0.5658,	0.9016 s / batch. (data: 7.98e-03). ETA=13:20:02, max mem: 23.5 GB 
[11/20 01:53:01 visual_prompt]: 	Training 500/553. train loss: 0.6658,	0.9290 s / batch. (data: 1.04e-02). ETA=13:42:49, max mem: 23.5 GB 
[11/20 01:53:52 visual_prompt]: Epoch 4 / 100: avg data time: 5.14e-02, avg batch time: 0.9723, average train loss: 0.8759
[11/20 01:54:49 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3024, average loss: 0.6920
[11/20 01:54:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.57	
[11/20 01:54:49 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/20 01:56:33 visual_prompt]: 	Training 100/553. train loss: 0.5621,	0.9360 s / batch. (data: 3.54e-04). ETA=13:46:38, max mem: 23.5 GB 
[11/20 01:58:09 visual_prompt]: 	Training 200/553. train loss: 0.6167,	0.9130 s / batch. (data: 1.04e-02). ETA=13:24:47, max mem: 23.5 GB 
[11/20 01:59:44 visual_prompt]: 	Training 300/553. train loss: 1.4363,	0.9268 s / batch. (data: 2.53e-04). ETA=13:35:25, max mem: 23.5 GB 
[11/20 02:01:21 visual_prompt]: 	Training 400/553. train loss: 0.7091,	4.0841 s / batch. (data: 3.15e+00). ETA=2 days, 11:46:23, max mem: 23.5 GB 
[11/20 02:02:57 visual_prompt]: 	Training 500/553. train loss: 0.5974,	0.9239 s / batch. (data: 5.80e-03). ETA=13:29:44, max mem: 23.5 GB 
[11/20 02:03:46 visual_prompt]: Epoch 5 / 100: avg data time: 5.05e-02, avg batch time: 0.9711, average train loss: 0.8442
[11/20 02:04:43 visual_prompt]: Inference (val):avg data time: 2.51e-04, avg batch time: 0.3044, average loss: 0.7190
[11/20 02:04:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.82	
[11/20 02:04:43 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/20 02:06:29 visual_prompt]: 	Training 100/553. train loss: 0.7303,	0.9265 s / batch. (data: 2.73e-04). ETA=13:29:40, max mem: 23.5 GB 
[11/20 02:08:03 visual_prompt]: 	Training 200/553. train loss: 1.0015,	0.9351 s / batch. (data: 1.82e-02). ETA=13:35:37, max mem: 23.5 GB 
[11/20 02:09:38 visual_prompt]: 	Training 300/553. train loss: 0.8532,	0.9496 s / batch. (data: 5.45e-03). ETA=13:46:42, max mem: 23.5 GB 
[11/20 02:11:14 visual_prompt]: 	Training 400/553. train loss: 0.7338,	1.5160 s / batch. (data: 5.63e-01). ETA=21:57:16, max mem: 23.5 GB 
[11/20 02:12:52 visual_prompt]: 	Training 500/553. train loss: 0.9896,	0.9405 s / batch. (data: 6.92e-04). ETA=13:35:36, max mem: 23.5 GB 
[11/20 02:13:40 visual_prompt]: Epoch 6 / 100: avg data time: 5.03e-02, avg batch time: 0.9714, average train loss: 0.8615
[11/20 02:14:37 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3049, average loss: 0.6962
[11/20 02:14:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 56.36	
[11/20 02:14:37 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/20 02:16:26 visual_prompt]: 	Training 100/553. train loss: 0.7986,	0.9117 s / batch. (data: 2.40e-04). ETA=13:08:20, max mem: 23.5 GB 
[11/20 02:18:00 visual_prompt]: 	Training 200/553. train loss: 0.6371,	0.9280 s / batch. (data: 6.98e-04). ETA=13:20:51, max mem: 23.5 GB 
[11/20 02:19:33 visual_prompt]: 	Training 300/553. train loss: 0.7842,	0.9400 s / batch. (data: 7.02e-04). ETA=13:29:41, max mem: 23.5 GB 
[11/20 02:21:06 visual_prompt]: 	Training 400/553. train loss: 0.7257,	0.9199 s / batch. (data: 6.05e-04). ETA=13:10:52, max mem: 23.5 GB 
[11/20 02:22:42 visual_prompt]: 	Training 500/553. train loss: 0.6699,	0.9431 s / batch. (data: 2.73e-04). ETA=13:29:11, max mem: 23.5 GB 
[11/20 02:23:31 visual_prompt]: Epoch 7 / 100: avg data time: 4.38e-02, avg batch time: 0.9658, average train loss: 0.8165
[11/20 02:24:28 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3020, average loss: 0.6922
[11/20 02:24:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 55.27	
[11/20 02:24:28 visual_prompt]: Best epoch 7: best metric: -0.692
[11/20 02:24:28 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/20 02:26:13 visual_prompt]: 	Training 100/553. train loss: 0.6253,	0.9080 s / batch. (data: 2.72e-04). ETA=12:56:46, max mem: 23.5 GB 
[11/20 02:27:50 visual_prompt]: 	Training 200/553. train loss: 0.8450,	0.9413 s / batch. (data: 5.38e-03). ETA=13:23:42, max mem: 23.5 GB 
[11/20 02:29:26 visual_prompt]: 	Training 300/553. train loss: 0.7321,	0.9405 s / batch. (data: 5.83e-03). ETA=13:21:25, max mem: 23.5 GB 
[11/20 02:31:02 visual_prompt]: 	Training 400/553. train loss: 0.6887,	0.9329 s / batch. (data: 5.35e-03). ETA=13:13:27, max mem: 23.5 GB 
[11/20 02:32:39 visual_prompt]: 	Training 500/553. train loss: 0.5907,	0.9120 s / batch. (data: 3.05e-04). ETA=12:54:08, max mem: 23.5 GB 
[11/20 02:33:28 visual_prompt]: Epoch 8 / 100: avg data time: 5.34e-02, avg batch time: 0.9751, average train loss: 0.7905
[11/20 02:34:25 visual_prompt]: Inference (val):avg data time: 1.39e-04, avg batch time: 0.3039, average loss: 0.7026
[11/20 02:34:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.29	
[11/20 02:34:25 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/20 02:36:13 visual_prompt]: 	Training 100/553. train loss: 0.7546,	0.9070 s / batch. (data: 5.37e-03). ETA=12:47:32, max mem: 23.5 GB 
[11/20 02:37:48 visual_prompt]: 	Training 200/553. train loss: 1.3179,	0.9231 s / batch. (data: 7.96e-03). ETA=12:59:36, max mem: 23.5 GB 
[11/20 02:39:22 visual_prompt]: 	Training 300/553. train loss: 0.7842,	0.9333 s / batch. (data: 1.04e-02). ETA=13:06:43, max mem: 23.5 GB 
[11/20 02:40:57 visual_prompt]: 	Training 400/553. train loss: 0.6804,	0.9134 s / batch. (data: 5.84e-03). ETA=12:48:23, max mem: 23.5 GB 
[11/20 02:42:29 visual_prompt]: 	Training 500/553. train loss: 0.7594,	0.9241 s / batch. (data: 2.73e-04). ETA=12:55:50, max mem: 23.5 GB 
[11/20 02:43:20 visual_prompt]: Epoch 9 / 100: avg data time: 4.70e-02, avg batch time: 0.9675, average train loss: 0.7733
[11/20 02:44:17 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3038, average loss: 0.7007
[11/20 02:44:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.15	
[11/20 02:44:17 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/20 02:46:02 visual_prompt]: 	Training 100/553. train loss: 0.9384,	0.9238 s / batch. (data: 5.34e-03). ETA=12:53:13, max mem: 23.5 GB 
[11/20 02:47:39 visual_prompt]: 	Training 200/553. train loss: 0.6375,	0.9139 s / batch. (data: 5.40e-03). ETA=12:43:28, max mem: 23.5 GB 
[11/20 02:49:13 visual_prompt]: 	Training 300/553. train loss: 0.7728,	0.9273 s / batch. (data: 2.55e-04). ETA=12:53:07, max mem: 23.5 GB 
[11/20 02:50:47 visual_prompt]: 	Training 400/553. train loss: 0.6486,	0.9017 s / batch. (data: 2.79e-04). ETA=12:30:16, max mem: 23.5 GB 
[11/20 02:52:23 visual_prompt]: 	Training 500/553. train loss: 0.7088,	0.9021 s / batch. (data: 3.03e-04). ETA=12:29:03, max mem: 23.5 GB 
[11/20 02:53:14 visual_prompt]: Epoch 10 / 100: avg data time: 4.94e-02, avg batch time: 0.9711, average train loss: 0.7236
[11/20 02:54:11 visual_prompt]: Inference (val):avg data time: 1.32e-04, avg batch time: 0.3029, average loss: 0.7068
[11/20 02:54:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.30	
[11/20 02:54:11 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/20 02:55:59 visual_prompt]: 	Training 100/553. train loss: 0.7761,	0.9091 s / batch. (data: 2.55e-04). ETA=12:32:37, max mem: 23.5 GB 
[11/20 02:57:33 visual_prompt]: 	Training 200/553. train loss: 0.7492,	0.9368 s / batch. (data: 1.55e-02). ETA=12:53:55, max mem: 23.5 GB 
[11/20 02:59:06 visual_prompt]: 	Training 300/553. train loss: 0.5739,	0.9047 s / batch. (data: 1.04e-02). ETA=12:25:54, max mem: 23.5 GB 
[11/20 03:00:42 visual_prompt]: 	Training 400/553. train loss: 0.7338,	0.9040 s / batch. (data: 2.70e-04). ETA=12:23:50, max mem: 23.5 GB 
[11/20 03:02:18 visual_prompt]: 	Training 500/553. train loss: 0.8237,	0.9407 s / batch. (data: 1.59e-02). ETA=12:52:29, max mem: 23.5 GB 
[11/20 03:03:07 visual_prompt]: Epoch 11 / 100: avg data time: 4.85e-02, avg batch time: 0.9689, average train loss: 0.7320
[11/20 03:04:04 visual_prompt]: Inference (val):avg data time: 4.68e-05, avg batch time: 0.3083, average loss: 0.7056
[11/20 03:04:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.01	
[11/20 03:04:04 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/20 03:05:46 visual_prompt]: 	Training 100/553. train loss: 0.5620,	0.9173 s / batch. (data: 5.35e-03). ETA=12:30:57, max mem: 23.5 GB 
[11/20 03:07:25 visual_prompt]: 	Training 200/553. train loss: 0.7338,	0.9320 s / batch. (data: 6.85e-04). ETA=12:41:23, max mem: 23.5 GB 
[11/20 03:09:00 visual_prompt]: 	Training 300/553. train loss: 0.6988,	0.9238 s / batch. (data: 2.50e-04). ETA=12:33:07, max mem: 23.5 GB 
[11/20 03:10:36 visual_prompt]: 	Training 400/553. train loss: 0.8334,	0.9295 s / batch. (data: 2.62e-04). ETA=12:36:15, max mem: 23.5 GB 
[11/20 03:12:11 visual_prompt]: 	Training 500/553. train loss: 0.7286,	0.9404 s / batch. (data: 5.37e-03). ETA=12:43:33, max mem: 23.5 GB 
[11/20 03:13:01 visual_prompt]: Epoch 12 / 100: avg data time: 5.04e-02, avg batch time: 0.9713, average train loss: 0.7163
[11/20 03:13:58 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3049, average loss: 0.7262
[11/20 03:13:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.33	
[11/20 03:13:58 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/20 03:15:44 visual_prompt]: 	Training 100/553. train loss: 0.6924,	0.9154 s / batch. (data: 7.98e-03). ETA=12:20:54, max mem: 23.5 GB 
[11/20 03:17:18 visual_prompt]: 	Training 200/553. train loss: 0.6131,	0.9307 s / batch. (data: 1.04e-02). ETA=12:31:46, max mem: 23.5 GB 
[11/20 03:18:54 visual_prompt]: 	Training 300/553. train loss: 0.5803,	0.9480 s / batch. (data: 9.21e-04). ETA=12:44:08, max mem: 23.5 GB 
[11/20 03:20:32 visual_prompt]: 	Training 400/553. train loss: 0.6992,	0.9063 s / batch. (data: 2.67e-04). ETA=12:09:01, max mem: 23.5 GB 
[11/20 03:22:07 visual_prompt]: 	Training 500/553. train loss: 0.6227,	0.9401 s / batch. (data: 2.56e-04). ETA=12:34:37, max mem: 23.5 GB 
[11/20 03:22:56 visual_prompt]: Epoch 13 / 100: avg data time: 5.01e-02, avg batch time: 0.9714, average train loss: 0.7006
[11/20 03:23:53 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3031, average loss: 0.6886
[11/20 03:23:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.20	
[11/20 03:23:53 visual_prompt]: Best epoch 13: best metric: -0.689
[11/20 03:23:53 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/20 03:25:40 visual_prompt]: 	Training 100/553. train loss: 0.6946,	0.9277 s / batch. (data: 5.71e-03). ETA=12:22:21, max mem: 23.5 GB 
[11/20 03:27:18 visual_prompt]: 	Training 200/553. train loss: 0.6298,	0.9160 s / batch. (data: 2.61e-04). ETA=12:11:26, max mem: 23.5 GB 
[11/20 03:28:53 visual_prompt]: 	Training 300/553. train loss: 0.7712,	1.3671 s / batch. (data: 4.50e-01). ETA=18:09:20, max mem: 23.5 GB 
[11/20 03:30:26 visual_prompt]: 	Training 400/553. train loss: 0.6977,	0.9303 s / batch. (data: 5.39e-03). ETA=12:19:44, max mem: 23.5 GB 
[11/20 03:32:03 visual_prompt]: 	Training 500/553. train loss: 0.7028,	0.9213 s / batch. (data: 8.60e-03). ETA=12:11:05, max mem: 23.5 GB 
[11/20 03:32:51 visual_prompt]: Epoch 14 / 100: avg data time: 5.23e-02, avg batch time: 0.9743, average train loss: 0.6938
[11/20 03:33:48 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3048, average loss: 0.6910
[11/20 03:33:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.62	
[11/20 03:33:48 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/20 03:35:35 visual_prompt]: 	Training 100/553. train loss: 0.5986,	0.9280 s / batch. (data: 5.38e-03). ETA=12:14:00, max mem: 23.5 GB 
[11/20 03:37:11 visual_prompt]: 	Training 200/553. train loss: 0.6993,	0.9250 s / batch. (data: 9.11e-03). ETA=12:10:04, max mem: 23.5 GB 
[11/20 03:38:46 visual_prompt]: 	Training 300/553. train loss: 0.7558,	0.9127 s / batch. (data: 2.82e-04). ETA=11:58:53, max mem: 23.5 GB 
[11/20 03:40:22 visual_prompt]: 	Training 400/553. train loss: 0.7604,	0.9440 s / batch. (data: 2.65e-04). ETA=12:21:56, max mem: 23.5 GB 
[11/20 03:41:56 visual_prompt]: 	Training 500/553. train loss: 0.8445,	0.9254 s / batch. (data: 2.52e-04). ETA=12:05:48, max mem: 23.5 GB 
[11/20 03:42:45 visual_prompt]: Epoch 15 / 100: avg data time: 4.94e-02, avg batch time: 0.9707, average train loss: 0.6932
[11/20 03:43:42 visual_prompt]: Inference (val):avg data time: 6.07e-04, avg batch time: 0.3012, average loss: 0.6928
[11/20 03:43:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.83	
[11/20 03:43:42 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/20 03:45:26 visual_prompt]: 	Training 100/553. train loss: 0.6996,	0.9261 s / batch. (data: 5.37e-03). ETA=12:04:00, max mem: 23.5 GB 
[11/20 03:46:58 visual_prompt]: 	Training 200/553. train loss: 0.7254,	0.9349 s / batch. (data: 1.55e-02). ETA=12:09:17, max mem: 23.5 GB 
[11/20 03:48:38 visual_prompt]: 	Training 300/553. train loss: 0.6921,	0.9240 s / batch. (data: 2.56e-04). ETA=11:59:15, max mem: 23.5 GB 
[11/20 03:50:14 visual_prompt]: 	Training 400/553. train loss: 0.7421,	0.9274 s / batch. (data: 1.55e-02). ETA=12:00:20, max mem: 23.5 GB 
[11/20 03:51:48 visual_prompt]: 	Training 500/553. train loss: 0.7267,	0.9209 s / batch. (data: 2.57e-04). ETA=11:53:46, max mem: 23.5 GB 
[11/20 03:52:38 visual_prompt]: Epoch 16 / 100: avg data time: 4.62e-02, avg batch time: 0.9683, average train loss: 0.6919
[11/20 03:53:34 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3022, average loss: 0.6912
[11/20 03:53:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.35	
[11/20 03:53:34 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/20 03:55:16 visual_prompt]: 	Training 100/553. train loss: 0.7059,	0.9036 s / batch. (data: 2.61e-04). ETA=11:38:02, max mem: 23.5 GB 
[11/20 03:56:55 visual_prompt]: 	Training 200/553. train loss: 0.7249,	0.9286 s / batch. (data: 8.66e-03). ETA=11:55:48, max mem: 23.5 GB 
[11/20 03:58:28 visual_prompt]: 	Training 300/553. train loss: 0.6983,	0.9199 s / batch. (data: 5.38e-03). ETA=11:47:33, max mem: 23.5 GB 
[11/20 04:00:06 visual_prompt]: 	Training 400/553. train loss: 0.7075,	0.9148 s / batch. (data: 5.39e-03). ETA=11:42:07, max mem: 23.5 GB 
[11/20 04:01:41 visual_prompt]: 	Training 500/553. train loss: 0.7844,	0.9178 s / batch. (data: 5.35e-03). ETA=11:42:54, max mem: 23.5 GB 
[11/20 04:02:32 visual_prompt]: Epoch 17 / 100: avg data time: 5.05e-02, avg batch time: 0.9712, average train loss: 0.6926
[11/20 04:03:28 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3033, average loss: 0.6885
[11/20 04:03:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.97	
[11/20 04:03:28 visual_prompt]: Best epoch 17: best metric: -0.688
[11/20 04:03:28 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/20 04:05:17 visual_prompt]: 	Training 100/553. train loss: 0.6989,	0.9288 s / batch. (data: 2.50e-04). ETA=11:48:56, max mem: 23.5 GB 
[11/20 04:06:53 visual_prompt]: 	Training 200/553. train loss: 0.6942,	0.9062 s / batch. (data: 6.20e-03). ETA=11:30:10, max mem: 23.5 GB 
[11/20 04:08:25 visual_prompt]: 	Training 300/553. train loss: 0.7108,	0.9440 s / batch. (data: 3.01e-04). ETA=11:57:25, max mem: 23.5 GB 
[11/20 04:09:59 visual_prompt]: 	Training 400/553. train loss: 0.6442,	0.9184 s / batch. (data: 2.56e-04). ETA=11:36:28, max mem: 23.5 GB 
[11/20 04:11:37 visual_prompt]: 	Training 500/553. train loss: 0.7756,	0.9299 s / batch. (data: 6.79e-04). ETA=11:43:34, max mem: 23.5 GB 
[11/20 04:12:26 visual_prompt]: Epoch 18 / 100: avg data time: 5.21e-02, avg batch time: 0.9720, average train loss: 0.6945
[11/20 04:13:23 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3042, average loss: 0.6902
[11/20 04:13:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.19	
[11/20 04:13:23 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/20 04:15:08 visual_prompt]: 	Training 100/553. train loss: 0.6468,	0.9056 s / batch. (data: 2.67e-04). ETA=11:22:55, max mem: 23.5 GB 
[11/20 04:16:46 visual_prompt]: 	Training 200/553. train loss: 0.7035,	3.4387 s / batch. (data: 2.52e+00). ETA=1 day, 19:07:21, max mem: 23.5 GB 
[11/20 04:18:19 visual_prompt]: 	Training 300/553. train loss: 0.7034,	0.9368 s / batch. (data: 6.69e-04). ETA=11:43:18, max mem: 23.5 GB 
[11/20 04:19:55 visual_prompt]: 	Training 400/553. train loss: 0.6678,	0.9010 s / batch. (data: 2.79e-04). ETA=11:14:57, max mem: 23.5 GB 
[11/20 04:21:30 visual_prompt]: 	Training 500/553. train loss: 0.6054,	0.9419 s / batch. (data: 5.94e-03). ETA=11:44:02, max mem: 23.5 GB 
[11/20 04:22:19 visual_prompt]: Epoch 19 / 100: avg data time: 4.91e-02, avg batch time: 0.9701, average train loss: 0.6912
[11/20 04:23:16 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3026, average loss: 0.6964
[11/20 04:23:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.61	
[11/20 04:23:16 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/20 04:25:04 visual_prompt]: 	Training 100/553. train loss: 0.6909,	0.9210 s / batch. (data: 2.43e-02). ETA=11:26:03, max mem: 23.5 GB 
[11/20 04:26:37 visual_prompt]: 	Training 200/553. train loss: 0.6978,	0.9063 s / batch. (data: 3.99e-03). ETA=11:13:35, max mem: 23.5 GB 
[11/20 04:28:12 visual_prompt]: 	Training 300/553. train loss: 0.7044,	0.9341 s / batch. (data: 2.50e-04). ETA=11:32:42, max mem: 23.5 GB 
[11/20 04:29:48 visual_prompt]: 	Training 400/553. train loss: 0.8645,	0.9244 s / batch. (data: 5.80e-03). ETA=11:23:58, max mem: 23.5 GB 
[11/20 04:31:22 visual_prompt]: 	Training 500/553. train loss: 0.6979,	0.9159 s / batch. (data: 2.88e-04). ETA=11:16:09, max mem: 23.5 GB 
[11/20 04:32:15 visual_prompt]: Epoch 20 / 100: avg data time: 5.30e-02, avg batch time: 0.9739, average train loss: 0.6910
[11/20 04:33:12 visual_prompt]: Inference (val):avg data time: 8.55e-05, avg batch time: 0.3037, average loss: 0.6970
[11/20 04:33:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.10	
[11/20 04:33:12 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/20 04:34:59 visual_prompt]: 	Training 100/553. train loss: 0.7023,	0.9200 s / batch. (data: 7.99e-03). ETA=11:16:48, max mem: 23.5 GB 
[11/20 04:36:36 visual_prompt]: 	Training 200/553. train loss: 0.6551,	0.9232 s / batch. (data: 3.33e-04). ETA=11:17:37, max mem: 23.5 GB 
[11/20 04:38:11 visual_prompt]: 	Training 300/553. train loss: 0.6934,	0.9106 s / batch. (data: 2.73e-04). ETA=11:06:50, max mem: 23.5 GB 
[11/20 04:39:46 visual_prompt]: 	Training 400/553. train loss: 0.7746,	0.9239 s / batch. (data: 1.76e-02). ETA=11:15:01, max mem: 23.5 GB 
[11/20 04:41:20 visual_prompt]: 	Training 500/553. train loss: 0.7411,	0.9280 s / batch. (data: 2.92e-04). ETA=11:16:30, max mem: 23.5 GB 
[11/20 04:42:09 visual_prompt]: Epoch 21 / 100: avg data time: 4.93e-02, avg batch time: 0.9709, average train loss: 0.6901
[11/20 04:43:05 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3009, average loss: 0.6882
[11/20 04:43:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.50	
[11/20 04:43:05 visual_prompt]: Best epoch 21: best metric: -0.688
[11/20 04:43:05 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/20 04:44:50 visual_prompt]: 	Training 100/553. train loss: 0.6990,	0.9223 s / batch. (data: 2.49e-04). ETA=11:10:00, max mem: 23.5 GB 
[11/20 04:46:28 visual_prompt]: 	Training 200/553. train loss: 0.6988,	0.9538 s / batch. (data: 5.90e-03). ETA=11:31:18, max mem: 23.5 GB 
[11/20 04:48:04 visual_prompt]: 	Training 300/553. train loss: 0.7490,	0.9400 s / batch. (data: 7.46e-04). ETA=11:19:45, max mem: 23.5 GB 
[11/20 04:49:37 visual_prompt]: 	Training 400/553. train loss: 0.8482,	0.9370 s / batch. (data: 2.69e-04). ETA=11:15:58, max mem: 23.5 GB 
[11/20 04:51:13 visual_prompt]: 	Training 500/553. train loss: 0.6941,	0.9440 s / batch. (data: 3.97e-03). ETA=11:19:27, max mem: 23.5 GB 
[11/20 04:52:02 visual_prompt]: Epoch 22 / 100: avg data time: 4.82e-02, avg batch time: 0.9703, average train loss: 0.6954
[11/20 04:52:59 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3023, average loss: 0.6906
[11/20 04:52:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.29	
[11/20 04:52:59 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/20 04:54:45 visual_prompt]: 	Training 100/553. train loss: 0.6998,	0.9440 s / batch. (data: 7.41e-04). ETA=11:17:04, max mem: 23.5 GB 
[11/20 04:56:21 visual_prompt]: 	Training 200/553. train loss: 0.6446,	0.9480 s / batch. (data: 7.17e-04). ETA=11:18:20, max mem: 23.5 GB 
[11/20 04:57:53 visual_prompt]: 	Training 300/553. train loss: 0.6116,	0.9131 s / batch. (data: 2.61e-04). ETA=10:51:49, max mem: 23.5 GB 
[11/20 04:59:30 visual_prompt]: 	Training 400/553. train loss: 0.6999,	0.9302 s / batch. (data: 3.03e-02). ETA=11:02:31, max mem: 23.5 GB 
[11/20 05:01:04 visual_prompt]: 	Training 500/553. train loss: 0.7151,	0.9184 s / batch. (data: 2.62e-04). ETA=10:52:36, max mem: 23.5 GB 
[11/20 05:01:55 visual_prompt]: Epoch 23 / 100: avg data time: 4.71e-02, avg batch time: 0.9691, average train loss: 0.6964
[11/20 05:02:52 visual_prompt]: Inference (val):avg data time: 2.76e-04, avg batch time: 0.3041, average loss: 0.6948
[11/20 05:02:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.79	
[11/20 05:02:52 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/20 05:04:35 visual_prompt]: 	Training 100/553. train loss: 0.7006,	0.9293 s / batch. (data: 2.54e-04). ETA=10:57:58, max mem: 23.5 GB 
[11/20 05:06:11 visual_prompt]: 	Training 200/553. train loss: 0.7022,	0.9240 s / batch. (data: 5.37e-03). ETA=10:52:41, max mem: 23.5 GB 
[11/20 05:07:49 visual_prompt]: 	Training 300/553. train loss: 0.7021,	0.9160 s / batch. (data: 2.53e-04). ETA=10:45:29, max mem: 23.5 GB 
[11/20 05:09:23 visual_prompt]: 	Training 400/553. train loss: 0.7003,	0.8968 s / batch. (data: 2.52e-04). ETA=10:30:26, max mem: 23.5 GB 
[11/20 05:10:59 visual_prompt]: 	Training 500/553. train loss: 0.6963,	0.9434 s / batch. (data: 1.04e-02). ETA=11:01:37, max mem: 23.5 GB 
[11/20 05:11:48 visual_prompt]: Epoch 24 / 100: avg data time: 4.86e-02, avg batch time: 0.9695, average train loss: 0.6906
[11/20 05:12:45 visual_prompt]: Inference (val):avg data time: 2.03e-04, avg batch time: 0.3020, average loss: 0.6890
[11/20 05:12:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.43	
[11/20 05:12:45 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/20 05:14:26 visual_prompt]: 	Training 100/553. train loss: 0.6424,	0.9509 s / batch. (data: 6.62e-03). ETA=11:04:27, max mem: 23.5 GB 
[11/20 05:16:05 visual_prompt]: 	Training 200/553. train loss: 0.7333,	0.9413 s / batch. (data: 2.07e-04). ETA=10:56:10, max mem: 23.5 GB 
[11/20 05:17:39 visual_prompt]: 	Training 300/553. train loss: 0.6956,	0.8964 s / batch. (data: 2.68e-04). ETA=10:23:24, max mem: 23.5 GB 
[11/20 05:19:16 visual_prompt]: 	Training 400/553. train loss: 0.7148,	0.9160 s / batch. (data: 2.61e-04). ETA=10:35:32, max mem: 23.5 GB 
[11/20 05:20:53 visual_prompt]: 	Training 500/553. train loss: 0.6271,	0.9316 s / batch. (data: 2.56e-04). ETA=10:44:48, max mem: 23.5 GB 
[11/20 05:21:43 visual_prompt]: Epoch 25 / 100: avg data time: 5.44e-02, avg batch time: 0.9738, average train loss: 0.6922
[11/20 05:22:40 visual_prompt]: Inference (val):avg data time: 6.88e-04, avg batch time: 0.3039, average loss: 0.6895
[11/20 05:22:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.43	
[11/20 05:22:40 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/20 05:24:25 visual_prompt]: 	Training 100/553. train loss: 0.6485,	0.9360 s / batch. (data: 6.79e-04). ETA=10:45:27, max mem: 23.5 GB 
[11/20 05:26:02 visual_prompt]: 	Training 200/553. train loss: 0.5986,	0.9336 s / batch. (data: 7.99e-03). ETA=10:42:15, max mem: 23.5 GB 
[11/20 05:27:40 visual_prompt]: 	Training 300/553. train loss: 0.7382,	0.9075 s / batch. (data: 3.36e-04). ETA=10:22:44, max mem: 23.5 GB 
[11/20 05:29:13 visual_prompt]: 	Training 400/553. train loss: 0.6961,	0.9225 s / batch. (data: 2.80e-04). ETA=10:31:30, max mem: 23.5 GB 
[11/20 05:30:49 visual_prompt]: 	Training 500/553. train loss: 0.6451,	0.9255 s / batch. (data: 5.61e-03). ETA=10:32:02, max mem: 23.5 GB 
[11/20 05:31:39 visual_prompt]: Epoch 26 / 100: avg data time: 5.17e-02, avg batch time: 0.9738, average train loss: 0.6912
[11/20 05:32:35 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3014, average loss: 0.7449
[11/20 05:32:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.76	
[11/20 05:32:35 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/20 05:34:19 visual_prompt]: 	Training 100/553. train loss: 0.7255,	0.9174 s / batch. (data: 2.48e-04). ETA=10:24:11, max mem: 23.5 GB 
[11/20 05:35:55 visual_prompt]: 	Training 200/553. train loss: 0.6986,	0.9139 s / batch. (data: 8.90e-03). ETA=10:20:15, max mem: 23.5 GB 
[11/20 05:37:28 visual_prompt]: 	Training 300/553. train loss: 0.7466,	0.9640 s / batch. (data: 7.24e-04). ETA=10:52:39, max mem: 23.5 GB 
[11/20 05:39:02 visual_prompt]: 	Training 400/553. train loss: 0.7417,	0.9560 s / batch. (data: 5.86e-03). ETA=10:45:38, max mem: 23.5 GB 
[11/20 05:40:42 visual_prompt]: 	Training 500/553. train loss: 0.7399,	0.9184 s / batch. (data: 3.95e-03). ETA=10:18:41, max mem: 23.5 GB 
[11/20 05:41:31 visual_prompt]: Epoch 27 / 100: avg data time: 4.63e-02, avg batch time: 0.9686, average train loss: 0.6948
[11/20 05:42:28 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.3091, average loss: 0.6892
[11/20 05:42:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.00	
[11/20 05:42:28 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/20 05:44:11 visual_prompt]: 	Training 100/553. train loss: 0.6926,	0.9456 s / batch. (data: 5.38e-03). ETA=10:34:38, max mem: 23.5 GB 
[11/20 05:45:48 visual_prompt]: 	Training 200/553. train loss: 0.6493,	0.9320 s / batch. (data: 2.52e-04). ETA=10:23:58, max mem: 23.5 GB 
[11/20 05:47:24 visual_prompt]: 	Training 300/553. train loss: 0.6832,	0.9149 s / batch. (data: 2.66e-04). ETA=10:11:01, max mem: 23.5 GB 
[11/20 05:49:00 visual_prompt]: 	Training 400/553. train loss: 0.6942,	1.6080 s / batch. (data: 6.77e-01). ETA=17:51:10, max mem: 23.5 GB 
[11/20 05:50:34 visual_prompt]: 	Training 500/553. train loss: 0.7605,	0.9267 s / batch. (data: 5.79e-03). ETA=10:15:46, max mem: 23.5 GB 
[11/20 05:51:24 visual_prompt]: Epoch 28 / 100: avg data time: 4.78e-02, avg batch time: 0.9689, average train loss: 0.6967
[11/20 05:52:21 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3017, average loss: 0.6898
[11/20 05:52:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.16	
[11/20 05:52:21 visual_prompt]: Stopping early.
[11/20 05:52:21 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 05:52:21 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 05:52:21 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/20 05:52:21 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 05:52:21 visual_prompt]: Training with config:
[11/20 05:52:21 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.005_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 05:52:21 visual_prompt]: Loading training data...
[11/20 05:52:21 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 05:52:21 visual_prompt]: Loading validation data...
[11/20 05:52:21 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 05:52:21 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/20 05:52:23 visual_prompt]: Enable all parameters update during training
[11/20 05:52:23 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/20 05:52:23 visual_prompt]: tuned percent:100.000
[11/20 05:52:23 visual_prompt]: Device used for model: 0
[11/20 05:52:23 visual_prompt]: Setting up Evaluator...
[11/20 05:52:23 visual_prompt]: Setting up Trainer...
[11/20 05:52:23 visual_prompt]: 	Setting up the optimizer...
[11/20 05:52:23 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 05:54:03 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9117 s / batch. (data: 5.39e-03). ETA=13:58:44, max mem: 24.8 GB 
[11/20 05:55:36 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9319 s / batch. (data: 3.96e-03). ETA=14:15:50, max mem: 24.8 GB 
[11/20 05:57:09 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9280 s / batch. (data: 1.05e-02). ETA=14:10:37, max mem: 24.8 GB 
[11/20 05:58:42 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9275 s / batch. (data: 7.25e-04). ETA=14:08:40, max mem: 24.8 GB 
[11/20 06:00:15 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9438 s / batch. (data: 1.55e-02). ETA=14:22:01, max mem: 24.8 GB 
[11/20 06:01:03 visual_prompt]: Epoch 1 / 100: avg data time: 2.00e-02, avg batch time: 0.9405, average train loss: 7.6130
[11/20 06:01:57 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3035, average loss: 6.9126
[11/20 06:01:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/20 06:01:57 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/20 06:03:38 visual_prompt]: 	Training 100/553. train loss: 0.9501,	0.9390 s / batch. (data: 2.43e-02). ETA=14:15:13, max mem: 24.8 GB 
[11/20 06:05:10 visual_prompt]: 	Training 200/553. train loss: 0.6631,	0.9144 s / batch. (data: 5.53e-03). ETA=13:51:15, max mem: 24.8 GB 
[11/20 06:06:43 visual_prompt]: 	Training 300/553. train loss: 0.8936,	0.9225 s / batch. (data: 2.50e-04). ETA=13:57:08, max mem: 24.8 GB 
[11/20 06:08:16 visual_prompt]: 	Training 400/553. train loss: 0.6786,	0.9352 s / batch. (data: 6.97e-04). ETA=14:07:07, max mem: 24.8 GB 
[11/20 06:09:48 visual_prompt]: 	Training 500/553. train loss: 1.2749,	0.9266 s / batch. (data: 1.55e-02). ETA=13:57:45, max mem: 24.8 GB 
[11/20 06:10:37 visual_prompt]: Epoch 2 / 100: avg data time: 2.04e-02, avg batch time: 0.9406, average train loss: 1.2958
[11/20 06:11:31 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3034, average loss: 0.8355
[11/20 06:11:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.96	
[11/20 06:11:31 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/20 06:13:13 visual_prompt]: 	Training 100/553. train loss: 0.6013,	0.9273 s / batch. (data: 7.20e-04). ETA=13:55:59, max mem: 24.8 GB 
[11/20 06:14:45 visual_prompt]: 	Training 200/553. train loss: 3.0954,	0.9204 s / batch. (data: 2.40e-04). ETA=13:48:18, max mem: 24.8 GB 
[11/20 06:16:18 visual_prompt]: 	Training 300/553. train loss: 0.6327,	0.9200 s / batch. (data: 8.19e-04). ETA=13:46:21, max mem: 24.8 GB 
[11/20 06:17:51 visual_prompt]: 	Training 400/553. train loss: 0.3445,	0.9268 s / batch. (data: 1.04e-02). ETA=13:50:56, max mem: 24.8 GB 
[11/20 06:19:23 visual_prompt]: 	Training 500/553. train loss: 1.9292,	0.9314 s / batch. (data: 1.09e-02). ETA=13:53:30, max mem: 24.8 GB 
[11/20 06:20:12 visual_prompt]: Epoch 3 / 100: avg data time: 2.05e-02, avg batch time: 0.9402, average train loss: 0.8792
[11/20 06:21:06 visual_prompt]: Inference (val):avg data time: 1.60e-04, avg batch time: 0.3050, average loss: 1.1006
[11/20 06:21:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.56	
[11/20 06:21:06 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/20 06:22:51 visual_prompt]: 	Training 100/553. train loss: 1.4282,	0.9240 s / batch. (data: 2.97e-04). ETA=13:44:31, max mem: 24.8 GB 
[11/20 06:24:23 visual_prompt]: 	Training 200/553. train loss: 0.9198,	0.9360 s / batch. (data: 2.56e-04). ETA=13:53:41, max mem: 24.8 GB 
[11/20 06:25:56 visual_prompt]: 	Training 300/553. train loss: 1.1460,	0.9530 s / batch. (data: 1.09e-02). ETA=14:07:12, max mem: 24.8 GB 
[11/20 06:27:28 visual_prompt]: 	Training 400/553. train loss: 0.5819,	0.9520 s / batch. (data: 7.34e-04). ETA=14:04:46, max mem: 24.8 GB 
[11/20 06:29:01 visual_prompt]: 	Training 500/553. train loss: 0.6792,	0.9320 s / batch. (data: 2.85e-04). ETA=13:45:29, max mem: 24.8 GB 
[11/20 06:29:50 visual_prompt]: Epoch 4 / 100: avg data time: 2.63e-02, avg batch time: 0.9464, average train loss: 0.8864
[11/20 06:30:44 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3047, average loss: 0.6996
[11/20 06:30:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.79	
[11/20 06:30:44 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/20 06:32:25 visual_prompt]: 	Training 100/553. train loss: 0.5374,	0.9414 s / batch. (data: 2.41e-04). ETA=13:51:24, max mem: 24.8 GB 
[11/20 06:33:58 visual_prompt]: 	Training 200/553. train loss: 0.6227,	0.9000 s / batch. (data: 2.79e-04). ETA=13:13:18, max mem: 24.8 GB 
[11/20 06:35:32 visual_prompt]: 	Training 300/553. train loss: 1.4364,	0.9049 s / batch. (data: 5.38e-03). ETA=13:16:10, max mem: 24.8 GB 
[11/20 06:37:04 visual_prompt]: 	Training 400/553. train loss: 0.7145,	0.9274 s / batch. (data: 2.93e-04). ETA=13:34:23, max mem: 24.8 GB 
[11/20 06:38:38 visual_prompt]: 	Training 500/553. train loss: 0.6051,	0.9494 s / batch. (data: 1.59e-02). ETA=13:52:07, max mem: 24.8 GB 
[11/20 06:39:26 visual_prompt]: Epoch 5 / 100: avg data time: 2.10e-02, avg batch time: 0.9435, average train loss: 0.8372
[11/20 06:40:21 visual_prompt]: Inference (val):avg data time: 2.92e-04, avg batch time: 0.3021, average loss: 0.7255
[11/20 06:40:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.38	
[11/20 06:40:21 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/20 06:42:03 visual_prompt]: 	Training 100/553. train loss: 0.7741,	0.9146 s / batch. (data: 1.30e-02). ETA=13:19:19, max mem: 24.8 GB 
[11/20 06:43:36 visual_prompt]: 	Training 200/553. train loss: 0.9685,	0.9186 s / batch. (data: 3.13e-03). ETA=13:21:14, max mem: 24.8 GB 
[11/20 06:45:08 visual_prompt]: 	Training 300/553. train loss: 0.8369,	0.9231 s / batch. (data: 1.04e-02). ETA=13:23:39, max mem: 24.8 GB 
[11/20 06:46:41 visual_prompt]: 	Training 400/553. train loss: 0.7314,	0.8956 s / batch. (data: 2.77e-04). ETA=12:58:13, max mem: 24.8 GB 
[11/20 06:48:13 visual_prompt]: 	Training 500/553. train loss: 1.0409,	0.8951 s / batch. (data: 2.58e-04). ETA=12:56:16, max mem: 24.8 GB 
[11/20 06:49:02 visual_prompt]: Epoch 6 / 100: avg data time: 2.36e-02, avg batch time: 0.9419, average train loss: 0.8644
[11/20 06:49:56 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3011, average loss: 0.6925
[11/20 06:49:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 56.37	
[11/20 06:49:56 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/20 06:51:42 visual_prompt]: 	Training 100/553. train loss: 0.8330,	0.9415 s / batch. (data: 2.56e-02). ETA=13:34:06, max mem: 24.8 GB 
[11/20 06:53:15 visual_prompt]: 	Training 200/553. train loss: 0.6509,	0.9192 s / batch. (data: 4.02e-03). ETA=13:13:20, max mem: 24.8 GB 
[11/20 06:54:47 visual_prompt]: 	Training 300/553. train loss: 0.7825,	0.9245 s / batch. (data: 1.69e-02). ETA=13:16:18, max mem: 24.8 GB 
[11/20 06:56:20 visual_prompt]: 	Training 400/553. train loss: 0.6607,	0.9344 s / batch. (data: 5.77e-03). ETA=13:23:17, max mem: 24.8 GB 
[11/20 06:57:53 visual_prompt]: 	Training 500/553. train loss: 0.6634,	0.9126 s / batch. (data: 3.45e-04). ETA=13:03:02, max mem: 24.8 GB 
[11/20 06:58:41 visual_prompt]: Epoch 7 / 100: avg data time: 2.96e-02, avg batch time: 0.9488, average train loss: 0.8182
[11/20 06:59:36 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3028, average loss: 0.6854
[11/20 06:59:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.44	
[11/20 06:59:36 visual_prompt]: Best epoch 7: best metric: -0.685
[11/20 06:59:36 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/20 07:01:19 visual_prompt]: 	Training 100/553. train loss: 0.6111,	0.9298 s / batch. (data: 2.52e-04). ETA=13:15:23, max mem: 24.8 GB 
[11/20 07:02:51 visual_prompt]: 	Training 200/553. train loss: 0.8345,	0.9240 s / batch. (data: 2.34e-04). ETA=13:08:55, max mem: 24.8 GB 
[11/20 07:04:23 visual_prompt]: 	Training 300/553. train loss: 0.7347,	0.9155 s / batch. (data: 6.80e-04). ETA=13:00:07, max mem: 24.8 GB 
[11/20 07:05:56 visual_prompt]: 	Training 400/553. train loss: 0.6878,	0.9183 s / batch. (data: 4.08e-03). ETA=13:00:59, max mem: 24.8 GB 
[11/20 07:07:28 visual_prompt]: 	Training 500/553. train loss: 0.6011,	0.9538 s / batch. (data: 5.81e-03). ETA=13:29:34, max mem: 24.8 GB 
[11/20 07:08:17 visual_prompt]: Epoch 8 / 100: avg data time: 2.32e-02, avg batch time: 0.9412, average train loss: 0.7880
[11/20 07:09:11 visual_prompt]: Inference (val):avg data time: 1.34e-04, avg batch time: 0.3032, average loss: 0.7062
[11/20 07:09:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 55.62	
[11/20 07:09:11 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/20 07:10:56 visual_prompt]: 	Training 100/553. train loss: 0.7587,	0.9040 s / batch. (data: 3.97e-03). ETA=12:45:00, max mem: 24.8 GB 
[11/20 07:12:29 visual_prompt]: 	Training 200/553. train loss: 1.3027,	0.9297 s / batch. (data: 5.81e-03). ETA=13:05:11, max mem: 24.8 GB 
[11/20 07:14:01 visual_prompt]: 	Training 300/553. train loss: 0.7107,	0.9076 s / batch. (data: 2.73e-04). ETA=12:45:03, max mem: 24.8 GB 
[11/20 07:15:33 visual_prompt]: 	Training 400/553. train loss: 0.6634,	0.9149 s / batch. (data: 7.99e-03). ETA=12:49:41, max mem: 24.8 GB 
[11/20 07:17:05 visual_prompt]: 	Training 500/553. train loss: 0.7630,	0.9321 s / batch. (data: 3.98e-04). ETA=13:02:36, max mem: 24.8 GB 
[11/20 07:17:54 visual_prompt]: Epoch 9 / 100: avg data time: 2.70e-02, avg batch time: 0.9450, average train loss: 0.7719
[11/20 07:18:49 visual_prompt]: Inference (val):avg data time: 1.21e-04, avg batch time: 0.3008, average loss: 0.6997
[11/20 07:18:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.08	
[11/20 07:18:49 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/20 07:20:30 visual_prompt]: 	Training 100/553. train loss: 0.9605,	0.9225 s / batch. (data: 1.04e-02). ETA=12:52:08, max mem: 24.8 GB 
[11/20 07:22:02 visual_prompt]: 	Training 200/553. train loss: 0.6906,	0.9529 s / batch. (data: 1.68e-02). ETA=13:16:00, max mem: 24.8 GB 
[11/20 07:23:36 visual_prompt]: 	Training 300/553. train loss: 0.8016,	1.6298 s / batch. (data: 5.00e-04). ETA=22:38:48, max mem: 24.8 GB 
[11/20 07:25:08 visual_prompt]: 	Training 400/553. train loss: 0.6477,	0.9181 s / batch. (data: 5.83e-03). ETA=12:43:56, max mem: 24.8 GB 
[11/20 07:26:40 visual_prompt]: 	Training 500/553. train loss: 0.7158,	0.8931 s / batch. (data: 2.53e-04). ETA=12:21:38, max mem: 24.8 GB 
[11/20 07:27:29 visual_prompt]: Epoch 10 / 100: avg data time: 2.04e-02, avg batch time: 0.9405, average train loss: 0.7245
[11/20 07:28:24 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3039, average loss: 0.7072
[11/20 07:28:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.57	
[11/20 07:28:24 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/20 07:30:08 visual_prompt]: 	Training 100/553. train loss: 0.7754,	0.9143 s / batch. (data: 6.25e-03). ETA=12:36:53, max mem: 24.8 GB 
[11/20 07:31:41 visual_prompt]: 	Training 200/553. train loss: 0.7753,	0.9144 s / batch. (data: 2.47e-04). ETA=12:35:27, max mem: 24.8 GB 
[11/20 07:33:13 visual_prompt]: 	Training 300/553. train loss: 0.5670,	0.9260 s / batch. (data: 2.68e-04). ETA=12:43:31, max mem: 24.8 GB 
[11/20 07:34:46 visual_prompt]: 	Training 400/553. train loss: 0.6489,	0.9281 s / batch. (data: 8.42e-04). ETA=12:43:40, max mem: 24.8 GB 
[11/20 07:36:18 visual_prompt]: 	Training 500/553. train loss: 0.7773,	0.8996 s / batch. (data: 2.74e-04). ETA=12:18:44, max mem: 24.8 GB 
[11/20 07:37:07 visual_prompt]: Epoch 11 / 100: avg data time: 2.61e-02, avg batch time: 0.9458, average train loss: 0.7154
[11/20 07:38:02 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3023, average loss: 0.7244
[11/20 07:38:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.56	
[11/20 07:38:02 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/20 07:39:40 visual_prompt]: 	Training 100/553. train loss: 0.5611,	0.9498 s / batch. (data: 1.39e-02). ETA=12:57:33, max mem: 24.8 GB 
[11/20 07:41:14 visual_prompt]: 	Training 200/553. train loss: 0.6651,	0.9182 s / batch. (data: 5.37e-03). ETA=12:30:08, max mem: 24.8 GB 
[11/20 07:42:47 visual_prompt]: 	Training 300/553. train loss: 0.7179,	0.8936 s / batch. (data: 2.75e-04). ETA=12:08:33, max mem: 24.8 GB 
[11/20 07:44:20 visual_prompt]: 	Training 400/553. train loss: 0.8449,	0.9404 s / batch. (data: 2.06e-02). ETA=12:45:06, max mem: 24.8 GB 
[11/20 07:45:52 visual_prompt]: 	Training 500/553. train loss: 0.7377,	0.9249 s / batch. (data: 2.48e-04). ETA=12:31:00, max mem: 24.8 GB 
[11/20 07:46:41 visual_prompt]: Epoch 12 / 100: avg data time: 1.97e-02, avg batch time: 0.9389, average train loss: 0.7063
[11/20 07:47:36 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3033, average loss: 0.7603
[11/20 07:47:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.69	
[11/20 07:47:36 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/20 07:49:18 visual_prompt]: 	Training 100/553. train loss: 0.6436,	0.9106 s / batch. (data: 5.85e-03). ETA=12:17:02, max mem: 24.8 GB 
[11/20 07:50:50 visual_prompt]: 	Training 200/553. train loss: 0.7165,	0.9281 s / batch. (data: 2.54e-04). ETA=12:29:37, max mem: 24.8 GB 
[11/20 07:52:23 visual_prompt]: 	Training 300/553. train loss: 0.6877,	0.9132 s / batch. (data: 5.36e-03). ETA=12:16:05, max mem: 24.8 GB 
[11/20 07:53:55 visual_prompt]: 	Training 400/553. train loss: 0.6715,	0.9252 s / batch. (data: 5.36e-03). ETA=12:24:14, max mem: 24.8 GB 
[11/20 07:55:28 visual_prompt]: 	Training 500/553. train loss: 0.6533,	0.9198 s / batch. (data: 1.05e-02). ETA=12:18:21, max mem: 24.8 GB 
[11/20 07:56:16 visual_prompt]: Epoch 13 / 100: avg data time: 2.20e-02, avg batch time: 0.9407, average train loss: 0.6963
[11/20 07:57:11 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3026, average loss: 0.6923
[11/20 07:57:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 52.81	
[11/20 07:57:11 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/20 07:58:54 visual_prompt]: 	Training 100/553. train loss: 0.6847,	0.9231 s / batch. (data: 5.50e-03). ETA=12:18:37, max mem: 24.8 GB 
[11/20 08:00:28 visual_prompt]: 	Training 200/553. train loss: 0.5378,	0.9151 s / batch. (data: 3.57e-04). ETA=12:10:42, max mem: 24.8 GB 
[11/20 08:02:00 visual_prompt]: 	Training 300/553. train loss: 0.7381,	0.9139 s / batch. (data: 5.38e-03). ETA=12:08:16, max mem: 24.8 GB 
[11/20 08:03:33 visual_prompt]: 	Training 400/553. train loss: 0.6894,	0.9298 s / batch. (data: 6.84e-04). ETA=12:19:22, max mem: 24.8 GB 
[11/20 08:05:05 visual_prompt]: 	Training 500/553. train loss: 0.6891,	0.9146 s / batch. (data: 2.59e-04). ETA=12:05:45, max mem: 24.8 GB 
[11/20 08:05:54 visual_prompt]: Epoch 14 / 100: avg data time: 2.69e-02, avg batch time: 0.9452, average train loss: 0.6893
[11/20 08:06:49 visual_prompt]: Inference (val):avg data time: 8.38e-05, avg batch time: 0.3037, average loss: 0.6866
[11/20 08:06:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 54.35	
[11/20 08:06:49 visual_prompt]: Stopping early.
[11/20 08:06:49 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 08:06:49 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 08:06:49 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/20 08:06:49 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 08:06:49 visual_prompt]: Training with config:
[11/20 08:06:49 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.005_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 08:06:49 visual_prompt]: Loading training data...
[11/20 08:06:49 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 08:06:49 visual_prompt]: Loading validation data...
[11/20 08:06:49 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 08:06:49 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/20 08:06:50 visual_prompt]: Enable all parameters update during training
[11/20 08:06:50 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/20 08:06:50 visual_prompt]: tuned percent:100.000
[11/20 08:06:50 visual_prompt]: Device used for model: 0
[11/20 08:06:50 visual_prompt]: Setting up Evaluator...
[11/20 08:06:50 visual_prompt]: Setting up Trainer...
[11/20 08:06:50 visual_prompt]: 	Setting up the optimizer...
[11/20 08:06:50 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 08:08:31 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9452 s / batch. (data: 1.31e-02). ETA=14:29:32, max mem: 26.1 GB 
[11/20 08:10:04 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9080 s / batch. (data: 7.99e-03). ETA=13:53:49, max mem: 26.1 GB 
[11/20 08:11:37 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9132 s / batch. (data: 1.55e-02). ETA=13:57:08, max mem: 26.1 GB 
[11/20 08:13:10 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9186 s / batch. (data: 5.83e-03). ETA=14:00:32, max mem: 26.1 GB 
[11/20 08:14:43 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9035 s / batch. (data: 5.37e-03). ETA=13:45:10, max mem: 26.1 GB 
[11/20 08:15:32 visual_prompt]: Epoch 1 / 100: avg data time: 1.98e-02, avg batch time: 0.9425, average train loss: 7.6130
[11/20 08:16:27 visual_prompt]: Inference (val):avg data time: 4.63e-05, avg batch time: 0.3109, average loss: 6.9126
[11/20 08:16:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/20 08:16:27 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/20 08:18:08 visual_prompt]: 	Training 100/553. train loss: 0.9499,	0.9041 s / batch. (data: 2.80e-04). ETA=13:43:24, max mem: 26.1 GB 
[11/20 08:19:41 visual_prompt]: 	Training 200/553. train loss: 0.6627,	0.9295 s / batch. (data: 5.39e-03). ETA=14:05:01, max mem: 26.1 GB 
[11/20 08:21:14 visual_prompt]: 	Training 300/553. train loss: 0.8926,	0.9373 s / batch. (data: 8.32e-03). ETA=14:10:30, max mem: 26.1 GB 
[11/20 08:22:48 visual_prompt]: 	Training 400/553. train loss: 0.6830,	0.9617 s / batch. (data: 1.09e-02). ETA=14:31:07, max mem: 26.1 GB 
[11/20 08:24:20 visual_prompt]: 	Training 500/553. train loss: 1.2696,	0.9012 s / batch. (data: 5.73e-03). ETA=13:34:49, max mem: 26.1 GB 
[11/20 08:25:09 visual_prompt]: Epoch 2 / 100: avg data time: 2.05e-02, avg batch time: 0.9428, average train loss: 1.2961
[11/20 08:26:03 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.3059, average loss: 0.8351
[11/20 08:26:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.97	
[11/20 08:26:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/20 08:27:45 visual_prompt]: 	Training 100/553. train loss: 0.5964,	0.9360 s / batch. (data: 2.43e-04). ETA=14:03:52, max mem: 26.1 GB 
[11/20 08:29:18 visual_prompt]: 	Training 200/553. train loss: 3.0681,	0.9178 s / batch. (data: 5.36e-03). ETA=13:45:57, max mem: 26.1 GB 
[11/20 08:30:50 visual_prompt]: 	Training 300/553. train loss: 0.6392,	0.9124 s / batch. (data: 2.54e-04). ETA=13:39:31, max mem: 26.1 GB 
[11/20 08:32:23 visual_prompt]: 	Training 400/553. train loss: 0.3437,	0.9202 s / batch. (data: 2.93e-04). ETA=13:45:02, max mem: 26.1 GB 
[11/20 08:33:55 visual_prompt]: 	Training 500/553. train loss: 1.9334,	0.9251 s / batch. (data: 8.85e-03). ETA=13:47:50, max mem: 26.1 GB 
[11/20 08:34:44 visual_prompt]: Epoch 3 / 100: avg data time: 2.20e-02, avg batch time: 0.9407, average train loss: 0.8800
[11/20 08:35:39 visual_prompt]: Inference (val):avg data time: 1.23e-04, avg batch time: 0.3050, average loss: 1.1027
[11/20 08:35:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.51	
[11/20 08:35:39 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/20 08:37:22 visual_prompt]: 	Training 100/553. train loss: 1.4018,	0.9211 s / batch. (data: 7.42e-04). ETA=13:41:55, max mem: 26.1 GB 
[11/20 08:38:55 visual_prompt]: 	Training 200/553. train loss: 0.9711,	0.9606 s / batch. (data: 1.08e-02). ETA=14:15:35, max mem: 26.1 GB 
[11/20 08:40:27 visual_prompt]: 	Training 300/553. train loss: 1.0446,	0.9105 s / batch. (data: 5.36e-03). ETA=13:29:26, max mem: 26.1 GB 
[11/20 08:42:00 visual_prompt]: 	Training 400/553. train loss: 0.5654,	0.9170 s / batch. (data: 2.79e-04). ETA=13:33:40, max mem: 26.1 GB 
[11/20 08:43:33 visual_prompt]: 	Training 500/553. train loss: 0.7252,	0.9280 s / batch. (data: 2.71e-04). ETA=13:41:56, max mem: 26.1 GB 
[11/20 08:44:22 visual_prompt]: Epoch 4 / 100: avg data time: 2.62e-02, avg batch time: 0.9459, average train loss: 0.8889
[11/20 08:45:16 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3046, average loss: 0.7072
[11/20 08:45:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.09	
[11/20 08:45:16 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/20 08:46:57 visual_prompt]: 	Training 100/553. train loss: 0.5246,	0.9415 s / batch. (data: 1.35e-02). ETA=13:51:29, max mem: 26.1 GB 
[11/20 08:48:30 visual_prompt]: 	Training 200/553. train loss: 0.6183,	0.9029 s / batch. (data: 2.75e-04). ETA=13:15:51, max mem: 26.1 GB 
[11/20 08:50:03 visual_prompt]: 	Training 300/553. train loss: 1.4526,	0.9360 s / batch. (data: 2.58e-04). ETA=13:43:30, max mem: 26.1 GB 
[11/20 08:51:35 visual_prompt]: 	Training 400/553. train loss: 0.6817,	0.9381 s / batch. (data: 2.18e-02). ETA=13:43:48, max mem: 26.1 GB 
[11/20 08:53:08 visual_prompt]: 	Training 500/553. train loss: 0.6258,	0.9184 s / batch. (data: 5.36e-03). ETA=13:24:54, max mem: 26.1 GB 
[11/20 08:53:57 visual_prompt]: Epoch 5 / 100: avg data time: 2.11e-02, avg batch time: 0.9402, average train loss: 0.8387
[11/20 08:54:51 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3034, average loss: 0.7150
[11/20 08:54:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.11	
[11/20 08:54:51 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/20 08:56:34 visual_prompt]: 	Training 100/553. train loss: 0.7126,	0.8998 s / batch. (data: 2.91e-04). ETA=13:06:18, max mem: 26.1 GB 
[11/20 08:58:07 visual_prompt]: 	Training 200/553. train loss: 1.0536,	0.8957 s / batch. (data: 2.25e-04). ETA=13:01:15, max mem: 26.1 GB 
[11/20 08:59:39 visual_prompt]: 	Training 300/553. train loss: 0.9572,	0.9256 s / batch. (data: 5.37e-03). ETA=13:25:46, max mem: 26.1 GB 
[11/20 09:01:12 visual_prompt]: 	Training 400/553. train loss: 0.7501,	0.9440 s / batch. (data: 2.98e-04). ETA=13:40:15, max mem: 26.1 GB 
[11/20 09:02:45 visual_prompt]: 	Training 500/553. train loss: 1.0046,	0.9287 s / batch. (data: 3.51e-02). ETA=13:25:26, max mem: 26.1 GB 
[11/20 09:03:34 visual_prompt]: Epoch 6 / 100: avg data time: 2.40e-02, avg batch time: 0.9445, average train loss: 0.8601
[11/20 09:04:29 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3020, average loss: 0.6997
[11/20 09:04:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 52.79	
[11/20 09:04:29 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/20 09:06:14 visual_prompt]: 	Training 100/553. train loss: 0.8322,	0.9240 s / batch. (data: 2.52e-04). ETA=13:18:58, max mem: 26.1 GB 
[11/20 09:07:47 visual_prompt]: 	Training 200/553. train loss: 0.6356,	0.9290 s / batch. (data: 5.81e-03). ETA=13:21:46, max mem: 26.1 GB 
[11/20 09:09:20 visual_prompt]: 	Training 300/553. train loss: 0.7915,	0.9246 s / batch. (data: 7.20e-04). ETA=13:16:24, max mem: 26.1 GB 
[11/20 09:10:52 visual_prompt]: 	Training 400/553. train loss: 0.6341,	0.9344 s / batch. (data: 5.38e-03). ETA=13:23:16, max mem: 26.1 GB 
[11/20 09:12:25 visual_prompt]: 	Training 500/553. train loss: 0.6592,	0.9131 s / batch. (data: 7.16e-04). ETA=13:03:26, max mem: 26.1 GB 
[11/20 09:13:14 visual_prompt]: Epoch 7 / 100: avg data time: 3.03e-02, avg batch time: 0.9491, average train loss: 0.8135
[11/20 09:14:08 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3016, average loss: 0.7008
[11/20 09:14:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 53.83	
[11/20 09:14:08 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/20 09:15:52 visual_prompt]: 	Training 100/553. train loss: 0.5925,	0.9134 s / batch. (data: 2.76e-04). ETA=13:01:22, max mem: 26.1 GB 
[11/20 09:17:25 visual_prompt]: 	Training 200/553. train loss: 0.8538,	0.9159 s / batch. (data: 2.05e-02). ETA=13:01:58, max mem: 26.1 GB 
[11/20 09:18:57 visual_prompt]: 	Training 300/553. train loss: 0.7122,	0.9400 s / batch. (data: 7.95e-04). ETA=13:20:58, max mem: 26.1 GB 
[11/20 09:20:30 visual_prompt]: 	Training 400/553. train loss: 0.6868,	0.9041 s / batch. (data: 1.04e-02). ETA=12:48:53, max mem: 26.1 GB 
[11/20 09:22:03 visual_prompt]: 	Training 500/553. train loss: 0.5937,	0.9014 s / batch. (data: 2.86e-04). ETA=12:45:05, max mem: 26.1 GB 
[11/20 09:22:52 visual_prompt]: Epoch 8 / 100: avg data time: 2.51e-02, avg batch time: 0.9465, average train loss: 0.7906
[11/20 09:23:47 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3004, average loss: 0.7115
[11/20 09:23:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.00	
[11/20 09:23:47 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/20 09:25:32 visual_prompt]: 	Training 100/553. train loss: 0.7511,	0.9338 s / batch. (data: 5.35e-03). ETA=13:10:14, max mem: 26.1 GB 
[11/20 09:27:05 visual_prompt]: 	Training 200/553. train loss: 1.2584,	0.9366 s / batch. (data: 2.18e-02). ETA=13:11:05, max mem: 26.1 GB 
[11/20 09:28:37 visual_prompt]: 	Training 300/553. train loss: 0.6291,	0.9289 s / batch. (data: 7.02e-03). ETA=13:03:00, max mem: 26.1 GB 
[11/20 09:30:09 visual_prompt]: 	Training 400/553. train loss: 0.6827,	0.9360 s / batch. (data: 7.10e-04). ETA=13:07:25, max mem: 26.1 GB 
[11/20 09:31:41 visual_prompt]: 	Training 500/553. train loss: 0.7737,	0.9010 s / batch. (data: 2.55e-04). ETA=12:36:26, max mem: 26.1 GB 
[11/20 09:32:30 visual_prompt]: Epoch 9 / 100: avg data time: 2.77e-02, avg batch time: 0.9452, average train loss: 0.7714
[11/20 09:33:24 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3049, average loss: 0.7048
[11/20 09:33:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.07	
[11/20 09:33:24 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/20 09:35:06 visual_prompt]: 	Training 100/553. train loss: 0.9325,	0.9567 s / batch. (data: 1.21e-02). ETA=13:20:45, max mem: 26.1 GB 
[11/20 09:36:39 visual_prompt]: 	Training 200/553. train loss: 0.6761,	0.9055 s / batch. (data: 4.04e-04). ETA=12:36:26, max mem: 26.1 GB 
[11/20 09:38:11 visual_prompt]: 	Training 300/553. train loss: 0.7781,	0.9182 s / batch. (data: 5.79e-03). ETA=12:45:31, max mem: 26.1 GB 
[11/20 09:39:43 visual_prompt]: 	Training 400/553. train loss: 0.6422,	0.9200 s / batch. (data: 2.71e-04). ETA=12:45:29, max mem: 26.1 GB 
[11/20 09:41:15 visual_prompt]: 	Training 500/553. train loss: 0.7135,	0.9282 s / batch. (data: 1.80e-02). ETA=12:50:43, max mem: 26.1 GB 
[11/20 09:42:04 visual_prompt]: Epoch 10 / 100: avg data time: 2.13e-02, avg batch time: 0.9400, average train loss: 0.7255
[11/20 09:42:58 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.3025, average loss: 0.7066
[11/20 09:42:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 54.30	
[11/20 09:42:58 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/20 09:44:40 visual_prompt]: 	Training 100/553. train loss: 0.7875,	0.9018 s / batch. (data: 2.45e-04). ETA=12:26:32, max mem: 26.1 GB 
[11/20 09:46:13 visual_prompt]: 	Training 200/553. train loss: 0.7611,	0.9207 s / batch. (data: 2.77e-04). ETA=12:40:38, max mem: 26.1 GB 
[11/20 09:47:46 visual_prompt]: 	Training 300/553. train loss: 0.5699,	0.9080 s / batch. (data: 2.75e-04). ETA=12:28:40, max mem: 26.1 GB 
[11/20 09:49:18 visual_prompt]: 	Training 400/553. train loss: 0.7389,	0.9072 s / batch. (data: 2.52e-04). ETA=12:26:30, max mem: 26.1 GB 
[11/20 09:50:50 visual_prompt]: 	Training 500/553. train loss: 0.7724,	0.9231 s / batch. (data: 1.09e-02). ETA=12:38:02, max mem: 26.1 GB 
[11/20 09:51:39 visual_prompt]: Epoch 11 / 100: avg data time: 2.15e-02, avg batch time: 0.9418, average train loss: 0.7160
[11/20 09:52:34 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3033, average loss: 0.7312
[11/20 09:52:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.90	
[11/20 09:52:34 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/20 09:54:12 visual_prompt]: 	Training 100/553. train loss: 0.5709,	0.9364 s / batch. (data: 5.37e-03). ETA=12:46:34, max mem: 26.1 GB 
[11/20 09:55:46 visual_prompt]: 	Training 200/553. train loss: 0.6747,	0.9120 s / batch. (data: 2.69e-04). ETA=12:25:03, max mem: 26.1 GB 
[11/20 09:57:19 visual_prompt]: 	Training 300/553. train loss: 0.7204,	0.9025 s / batch. (data: 2.88e-04). ETA=12:15:48, max mem: 26.1 GB 
[11/20 09:58:52 visual_prompt]: 	Training 400/553. train loss: 0.8390,	0.9410 s / batch. (data: 2.43e-04). ETA=12:45:36, max mem: 26.1 GB 
[11/20 10:00:24 visual_prompt]: 	Training 500/553. train loss: 0.7346,	0.9270 s / batch. (data: 1.59e-02). ETA=12:32:42, max mem: 26.1 GB 
[11/20 10:01:13 visual_prompt]: Epoch 12 / 100: avg data time: 2.03e-02, avg batch time: 0.9392, average train loss: 0.7085
[11/20 10:02:09 visual_prompt]: Inference (val):avg data time: 4.32e-05, avg batch time: 0.3158, average loss: 0.8601
[11/20 10:02:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.42	
[11/20 10:02:09 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/20 10:03:51 visual_prompt]: 	Training 100/553. train loss: 0.7062,	0.9161 s / batch. (data: 5.84e-03). ETA=12:21:31, max mem: 26.1 GB 
[11/20 10:05:23 visual_prompt]: 	Training 200/553. train loss: 0.6118,	0.9320 s / batch. (data: 5.37e-03). ETA=12:32:49, max mem: 26.1 GB 
[11/20 10:06:56 visual_prompt]: 	Training 300/553. train loss: 0.5785,	0.9294 s / batch. (data: 8.01e-03). ETA=12:29:07, max mem: 26.1 GB 
[11/20 10:08:29 visual_prompt]: 	Training 400/553. train loss: 0.6912,	0.8940 s / batch. (data: 3.34e-04). ETA=11:59:08, max mem: 26.1 GB 
[11/20 10:10:02 visual_prompt]: 	Training 500/553. train loss: 0.6253,	0.9286 s / batch. (data: 5.37e-03). ETA=12:25:26, max mem: 26.1 GB 
[11/20 10:10:50 visual_prompt]: Epoch 13 / 100: avg data time: 2.19e-02, avg batch time: 0.9427, average train loss: 0.7023
[11/20 10:11:45 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3020, average loss: 0.6862
[11/20 10:11:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 51.85	
[11/20 10:11:45 visual_prompt]: Best epoch 13: best metric: -0.686
[11/20 10:11:45 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/20 10:13:28 visual_prompt]: 	Training 100/553. train loss: 0.6926,	0.9267 s / batch. (data: 2.50e-04). ETA=12:21:30, max mem: 26.1 GB 
[11/20 10:15:02 visual_prompt]: 	Training 200/553. train loss: 0.7445,	0.9152 s / batch. (data: 6.94e-04). ETA=12:10:50, max mem: 26.1 GB 
[11/20 10:16:35 visual_prompt]: 	Training 300/553. train loss: 0.7187,	0.9320 s / batch. (data: 2.47e-04). ETA=12:22:38, max mem: 26.1 GB 
[11/20 10:18:07 visual_prompt]: 	Training 400/553. train loss: 0.6941,	0.9162 s / batch. (data: 5.37e-03). ETA=12:08:30, max mem: 26.1 GB 
[11/20 10:19:40 visual_prompt]: 	Training 500/553. train loss: 0.7081,	0.9377 s / batch. (data: 7.23e-04). ETA=12:24:03, max mem: 26.1 GB 
[11/20 10:20:29 visual_prompt]: Epoch 14 / 100: avg data time: 2.85e-02, avg batch time: 0.9466, average train loss: 0.6939
[11/20 10:21:23 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.3033, average loss: 0.6872
[11/20 10:21:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 55.53	
[11/20 10:21:23 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/20 10:23:08 visual_prompt]: 	Training 100/553. train loss: 0.6009,	0.9140 s / batch. (data: 1.04e-02). ETA=12:02:55, max mem: 26.1 GB 
[11/20 10:24:41 visual_prompt]: 	Training 200/553. train loss: 0.6712,	0.9140 s / batch. (data: 2.65e-04). ETA=12:01:27, max mem: 26.1 GB 
[11/20 10:26:14 visual_prompt]: 	Training 300/553. train loss: 0.8176,	0.9137 s / batch. (data: 7.25e-03). ETA=11:59:40, max mem: 26.1 GB 
[11/20 10:27:46 visual_prompt]: 	Training 400/553. train loss: 0.7377,	0.9329 s / batch. (data: 7.32e-04). ETA=12:13:12, max mem: 26.1 GB 
[11/20 10:29:18 visual_prompt]: 	Training 500/553. train loss: 0.8488,	0.9294 s / batch. (data: 5.37e-03). ETA=12:08:57, max mem: 26.1 GB 
[11/20 10:30:07 visual_prompt]: Epoch 15 / 100: avg data time: 2.60e-02, avg batch time: 0.9463, average train loss: 0.6910
[11/20 10:31:02 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3048, average loss: 0.6910
[11/20 10:31:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.36	
[11/20 10:31:02 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/20 10:32:43 visual_prompt]: 	Training 100/553. train loss: 0.6972,	0.9380 s / batch. (data: 2.60e-02). ETA=12:13:16, max mem: 26.1 GB 
[11/20 10:34:16 visual_prompt]: 	Training 200/553. train loss: 0.7541,	0.9240 s / batch. (data: 8.49e-04). ETA=12:00:47, max mem: 26.1 GB 
[11/20 10:35:48 visual_prompt]: 	Training 300/553. train loss: 0.6799,	0.9225 s / batch. (data: 7.33e-04). ETA=11:58:05, max mem: 26.1 GB 
[11/20 10:37:21 visual_prompt]: 	Training 400/553. train loss: 0.7620,	0.9285 s / batch. (data: 1.05e-02). ETA=12:01:11, max mem: 26.1 GB 
[11/20 10:38:53 visual_prompt]: 	Training 500/553. train loss: 0.7387,	0.9465 s / batch. (data: 7.20e-04). ETA=12:13:39, max mem: 26.1 GB 
[11/20 10:39:42 visual_prompt]: Epoch 16 / 100: avg data time: 2.18e-02, avg batch time: 0.9408, average train loss: 0.6885
[11/20 10:40:37 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3022, average loss: 0.6896
[11/20 10:40:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.12	
[11/20 10:40:37 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/20 10:42:17 visual_prompt]: 	Training 100/553. train loss: 0.7138,	0.9178 s / batch. (data: 5.37e-03). ETA=11:49:03, max mem: 26.1 GB 
[11/20 10:43:51 visual_prompt]: 	Training 200/553. train loss: 0.7029,	0.9264 s / batch. (data: 3.00e-04). ETA=11:54:09, max mem: 26.1 GB 
[11/20 10:45:23 visual_prompt]: 	Training 300/553. train loss: 0.7297,	0.9082 s / batch. (data: 7.32e-04). ETA=11:38:35, max mem: 26.1 GB 
[11/20 10:46:55 visual_prompt]: 	Training 400/553. train loss: 0.6639,	0.9314 s / batch. (data: 2.40e-04). ETA=11:54:53, max mem: 26.1 GB 
[11/20 10:48:28 visual_prompt]: 	Training 500/553. train loss: 0.7858,	0.9135 s / batch. (data: 4.31e-04). ETA=11:39:39, max mem: 26.1 GB 
[11/20 10:49:18 visual_prompt]: Epoch 17 / 100: avg data time: 2.07e-02, avg batch time: 0.9416, average train loss: 0.6898
[11/20 10:50:13 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3028, average loss: 0.6891
[11/20 10:50:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.80	
[11/20 10:50:13 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/20 10:51:56 visual_prompt]: 	Training 100/553. train loss: 0.6959,	0.9238 s / batch. (data: 1.09e-02). ETA=11:45:10, max mem: 26.1 GB 
[11/20 10:53:29 visual_prompt]: 	Training 200/553. train loss: 0.6943,	0.9185 s / batch. (data: 2.85e-04). ETA=11:39:36, max mem: 26.1 GB 
[11/20 10:55:02 visual_prompt]: 	Training 300/553. train loss: 0.6970,	0.9323 s / batch. (data: 2.94e-04). ETA=11:48:30, max mem: 26.1 GB 
[11/20 10:56:34 visual_prompt]: 	Training 400/553. train loss: 0.6422,	0.9333 s / batch. (data: 3.31e-02). ETA=11:47:45, max mem: 26.1 GB 
[11/20 10:58:06 visual_prompt]: 	Training 500/553. train loss: 0.7390,	0.9112 s / batch. (data: 1.04e-02). ETA=11:29:26, max mem: 26.1 GB 
[11/20 10:58:55 visual_prompt]: Epoch 18 / 100: avg data time: 2.60e-02, avg batch time: 0.9443, average train loss: 0.6937
[11/20 10:59:50 visual_prompt]: Inference (val):avg data time: 7.42e-05, avg batch time: 0.3013, average loss: 0.6908
[11/20 10:59:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.04	
[11/20 10:59:50 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/20 11:01:32 visual_prompt]: 	Training 100/553. train loss: 0.6474,	0.9330 s / batch. (data: 8.43e-04). ETA=11:43:32, max mem: 26.1 GB 
[11/20 11:03:04 visual_prompt]: 	Training 200/553. train loss: 0.7212,	0.9164 s / batch. (data: 1.04e-02). ETA=11:29:31, max mem: 26.1 GB 
[11/20 11:04:37 visual_prompt]: 	Training 300/553. train loss: 0.7059,	0.9433 s / batch. (data: 5.86e-03). ETA=11:48:12, max mem: 26.1 GB 
[11/20 11:06:10 visual_prompt]: 	Training 400/553. train loss: 0.6619,	0.9200 s / batch. (data: 5.37e-03). ETA=11:29:09, max mem: 26.1 GB 
[11/20 11:07:43 visual_prompt]: 	Training 500/553. train loss: 0.6103,	0.9322 s / batch. (data: 1.09e-02). ETA=11:36:44, max mem: 26.1 GB 
[11/20 11:08:32 visual_prompt]: Epoch 19 / 100: avg data time: 2.30e-02, avg batch time: 0.9436, average train loss: 0.6906
[11/20 11:09:26 visual_prompt]: Inference (val):avg data time: 8.47e-05, avg batch time: 0.3042, average loss: 0.7199
[11/20 11:09:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.58	
[11/20 11:09:26 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/20 11:11:11 visual_prompt]: 	Training 100/553. train loss: 0.6932,	0.9004 s / batch. (data: 5.47e-03). ETA=11:10:40, max mem: 26.1 GB 
[11/20 11:12:44 visual_prompt]: 	Training 200/553. train loss: 0.6989,	0.8985 s / batch. (data: 2.73e-04). ETA=11:07:45, max mem: 26.1 GB 
[11/20 11:14:16 visual_prompt]: 	Training 300/553. train loss: 0.7025,	0.9394 s / batch. (data: 7.54e-03). ETA=11:36:37, max mem: 26.1 GB 
[11/20 11:15:48 visual_prompt]: 	Training 400/553. train loss: 0.8511,	0.9160 s / batch. (data: 7.40e-04). ETA=11:17:42, max mem: 26.1 GB 
[11/20 11:17:21 visual_prompt]: 	Training 500/553. train loss: 0.6970,	0.9094 s / batch. (data: 1.60e-03). ETA=11:11:21, max mem: 26.1 GB 
[11/20 11:18:10 visual_prompt]: Epoch 20 / 100: avg data time: 2.68e-02, avg batch time: 0.9464, average train loss: 0.6915
[11/20 11:19:05 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3023, average loss: 0.6946
[11/20 11:19:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.04	
[11/20 11:19:05 visual_prompt]: Stopping early.
[11/20 11:19:05 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 11:19:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 11:19:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/20 11:19:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 11:19:05 visual_prompt]: Training with config:
[11/20 11:19:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.005_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 11:19:05 visual_prompt]: Loading training data...
[11/20 11:19:05 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 11:19:05 visual_prompt]: Loading validation data...
[11/20 11:19:05 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 11:19:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/20 11:19:06 visual_prompt]: Enable all parameters update during training
[11/20 11:19:06 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/20 11:19:06 visual_prompt]: tuned percent:100.000
[11/20 11:19:06 visual_prompt]: Device used for model: 0
[11/20 11:19:06 visual_prompt]: Setting up Evaluator...
[11/20 11:19:06 visual_prompt]: Setting up Trainer...
[11/20 11:19:06 visual_prompt]: 	Setting up the optimizer...
[11/20 11:19:06 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 11:20:47 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9150 s / batch. (data: 6.99e-03). ETA=14:01:49, max mem: 27.1 GB 
[11/20 11:22:21 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9360 s / batch. (data: 2.36e-04). ETA=14:19:33, max mem: 27.1 GB 
[11/20 11:23:54 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9160 s / batch. (data: 1.51e-03). ETA=13:59:40, max mem: 27.1 GB 
[11/20 11:25:26 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9240 s / batch. (data: 7.41e-04). ETA=14:05:28, max mem: 27.1 GB 
[11/20 11:26:59 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9520 s / batch. (data: 1.19e-02). ETA=14:29:28, max mem: 27.1 GB 
[11/20 11:27:47 visual_prompt]: Epoch 1 / 100: avg data time: 2.32e-02, avg batch time: 0.9418, average train loss: 7.6130
[11/20 11:28:43 visual_prompt]: Inference (val):avg data time: 4.08e-04, avg batch time: 0.3042, average loss: 6.9126
[11/20 11:28:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/20 11:28:43 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/20 11:30:25 visual_prompt]: 	Training 100/553. train loss: 1.3963,	0.9119 s / batch. (data: 7.55e-04). ETA=13:50:35, max mem: 27.1 GB 
[11/20 11:31:58 visual_prompt]: 	Training 200/553. train loss: 2.1056,	0.9017 s / batch. (data: 7.01e-04). ETA=13:39:42, max mem: 27.1 GB 
[11/20 11:33:30 visual_prompt]: 	Training 300/553. train loss: 1.7242,	0.9174 s / batch. (data: 2.90e-04). ETA=13:52:28, max mem: 27.1 GB 
[11/20 11:35:02 visual_prompt]: 	Training 400/553. train loss: 1.5477,	0.9120 s / batch. (data: 3.02e-04). ETA=13:46:04, max mem: 27.1 GB 
[11/20 11:36:35 visual_prompt]: 	Training 500/553. train loss: 0.6127,	0.9485 s / batch. (data: 1.55e-02). ETA=14:17:33, max mem: 27.1 GB 
[11/20 11:37:24 visual_prompt]: Epoch 2 / 100: avg data time: 2.33e-02, avg batch time: 0.9418, average train loss: 2.0394
[11/20 11:38:18 visual_prompt]: Inference (val):avg data time: 8.59e-05, avg batch time: 0.3037, average loss: 1.3743
[11/20 11:38:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.65	
[11/20 11:38:18 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/20 11:40:00 visual_prompt]: 	Training 100/553. train loss: 0.8032,	0.9606 s / batch. (data: 2.10e-02). ETA=14:26:00, max mem: 27.1 GB 
[11/20 11:41:33 visual_prompt]: 	Training 200/553. train loss: 4.7643,	0.9161 s / batch. (data: 3.95e-03). ETA=13:44:23, max mem: 27.1 GB 
[11/20 11:43:05 visual_prompt]: 	Training 300/553. train loss: 1.0289,	0.9360 s / batch. (data: 7.49e-04). ETA=14:00:45, max mem: 27.1 GB 
[11/20 11:44:38 visual_prompt]: 	Training 400/553. train loss: 0.6309,	0.9320 s / batch. (data: 7.97e-03). ETA=13:55:35, max mem: 27.1 GB 
[11/20 11:46:10 visual_prompt]: 	Training 500/553. train loss: 0.8896,	0.9155 s / batch. (data: 2.97e-04). ETA=13:39:15, max mem: 27.1 GB 
[11/20 11:46:59 visual_prompt]: Epoch 3 / 100: avg data time: 2.28e-02, avg batch time: 0.9406, average train loss: 1.4848
[11/20 11:47:53 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3041, average loss: 4.2904
[11/20 11:47:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.95	
[11/20 11:47:53 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/20 11:49:37 visual_prompt]: 	Training 100/553. train loss: 1.2382,	0.8934 s / batch. (data: 5.39e-03). ETA=13:17:11, max mem: 27.1 GB 
[11/20 11:51:09 visual_prompt]: 	Training 200/553. train loss: 0.8508,	0.9240 s / batch. (data: 7.22e-04). ETA=13:42:57, max mem: 27.1 GB 
[11/20 11:52:41 visual_prompt]: 	Training 300/553. train loss: 2.1958,	0.9345 s / batch. (data: 2.88e-04). ETA=13:50:47, max mem: 27.1 GB 
[11/20 11:54:13 visual_prompt]: 	Training 400/553. train loss: 0.5063,	0.9080 s / batch. (data: 4.00e-03). ETA=13:25:42, max mem: 27.1 GB 
[11/20 11:55:45 visual_prompt]: 	Training 500/553. train loss: 0.5892,	0.9320 s / batch. (data: 1.60e-02). ETA=13:45:29, max mem: 27.1 GB 
[11/20 11:56:34 visual_prompt]: Epoch 4 / 100: avg data time: 2.59e-02, avg batch time: 0.9407, average train loss: 2.0988
[11/20 11:57:27 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.3041, average loss: 0.7096
[11/20 11:57:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 56.96	
[11/20 11:57:27 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/20 11:59:07 visual_prompt]: 	Training 100/553. train loss: 2.5433,	0.9200 s / batch. (data: 5.35e-03). ETA=13:32:29, max mem: 27.1 GB 
[11/20 12:00:39 visual_prompt]: 	Training 200/553. train loss: 0.6965,	0.9076 s / batch. (data: 3.14e-04). ETA=13:19:58, max mem: 27.1 GB 
[11/20 12:02:12 visual_prompt]: 	Training 300/553. train loss: 2.4589,	0.9000 s / batch. (data: 2.67e-04). ETA=13:11:49, max mem: 27.1 GB 
[11/20 12:03:44 visual_prompt]: 	Training 400/553. train loss: 0.9253,	0.9320 s / batch. (data: 7.95e-03). ETA=13:38:24, max mem: 27.1 GB 
[11/20 12:05:16 visual_prompt]: 	Training 500/553. train loss: 1.2319,	0.9437 s / batch. (data: 7.42e-04). ETA=13:47:08, max mem: 27.1 GB 
[11/20 12:06:05 visual_prompt]: Epoch 5 / 100: avg data time: 1.99e-02, avg batch time: 0.9353, average train loss: 1.9162
[11/20 12:06:59 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.3035, average loss: 2.2505
[11/20 12:06:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.98	
[11/20 12:06:59 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/20 12:08:40 visual_prompt]: 	Training 100/553. train loss: 0.6717,	0.9147 s / batch. (data: 1.59e-02). ETA=13:19:24, max mem: 27.1 GB 
[11/20 12:10:12 visual_prompt]: 	Training 200/553. train loss: 2.9368,	0.9335 s / batch. (data: 1.21e-03). ETA=13:34:13, max mem: 27.1 GB 
[11/20 12:11:44 visual_prompt]: 	Training 300/553. train loss: 0.5175,	0.9091 s / batch. (data: 1.04e-02). ETA=13:11:26, max mem: 27.1 GB 
[11/20 12:13:16 visual_prompt]: 	Training 400/553. train loss: 1.3113,	0.9309 s / batch. (data: 5.50e-03). ETA=13:28:50, max mem: 27.1 GB 
[11/20 12:14:48 visual_prompt]: 	Training 500/553. train loss: 2.4888,	0.9185 s / batch. (data: 7.36e-04). ETA=13:16:35, max mem: 27.1 GB 
[11/20 12:15:36 visual_prompt]: Epoch 6 / 100: avg data time: 2.08e-02, avg batch time: 0.9354, average train loss: 2.1332
[11/20 12:16:30 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.3032, average loss: 1.0585
[11/20 12:16:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.34	
[11/20 12:16:30 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/20 12:18:13 visual_prompt]: 	Training 100/553. train loss: 1.2244,	0.9161 s / batch. (data: 2.74e-04). ETA=13:12:08, max mem: 27.1 GB 
[11/20 12:19:45 visual_prompt]: 	Training 200/553. train loss: 1.2928,	0.9152 s / batch. (data: 7.37e-04). ETA=13:09:51, max mem: 27.1 GB 
[11/20 12:21:17 visual_prompt]: 	Training 300/553. train loss: 0.7993,	0.8920 s / batch. (data: 2.29e-04). ETA=12:48:21, max mem: 27.1 GB 
[11/20 12:22:49 visual_prompt]: 	Training 400/553. train loss: 1.6056,	0.9185 s / batch. (data: 7.00e-04). ETA=13:09:37, max mem: 27.1 GB 
[11/20 12:24:21 visual_prompt]: 	Training 500/553. train loss: 0.3530,	0.9512 s / batch. (data: 5.84e-03). ETA=13:36:09, max mem: 27.1 GB 
[11/20 12:25:10 visual_prompt]: Epoch 7 / 100: avg data time: 2.50e-02, avg batch time: 0.9402, average train loss: 1.9014
[11/20 12:26:03 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3044, average loss: 1.0548
[11/20 12:26:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.63	
[11/20 12:26:03 visual_prompt]: Best epoch 7: best metric: -1.055
[11/20 12:26:03 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/20 12:27:45 visual_prompt]: 	Training 100/553. train loss: 0.8545,	0.9160 s / batch. (data: 7.81e-04). ETA=13:03:38, max mem: 27.1 GB 
[11/20 12:29:17 visual_prompt]: 	Training 200/553. train loss: 3.1202,	0.9160 s / batch. (data: 6.86e-04). ETA=13:02:07, max mem: 27.1 GB 
[11/20 12:30:50 visual_prompt]: 	Training 300/553. train loss: 2.5421,	0.9200 s / batch. (data: 2.68e-04). ETA=13:03:58, max mem: 27.1 GB 
[11/20 12:32:21 visual_prompt]: 	Training 400/553. train loss: 0.5800,	0.9349 s / batch. (data: 5.31e-03). ETA=13:15:04, max mem: 27.1 GB 
[11/20 12:33:53 visual_prompt]: 	Training 500/553. train loss: 1.0029,	0.8978 s / batch. (data: 7.11e-04). ETA=12:42:03, max mem: 27.1 GB 
[11/20 12:34:44 visual_prompt]: Epoch 8 / 100: avg data time: 2.56e-02, avg batch time: 0.9406, average train loss: 1.5482
[11/20 12:35:38 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.3021, average loss: 0.7027
[11/20 12:35:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 56.06	
[11/20 12:35:38 visual_prompt]: Best epoch 8: best metric: -0.703
[11/20 12:35:38 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/20 12:37:21 visual_prompt]: 	Training 100/553. train loss: 0.5298,	0.9244 s / batch. (data: 7.39e-04). ETA=13:02:16, max mem: 27.1 GB 
[11/20 12:38:53 visual_prompt]: 	Training 200/553. train loss: 0.5712,	0.9400 s / batch. (data: 1.20e-02). ETA=13:13:55, max mem: 27.1 GB 
[11/20 12:40:25 visual_prompt]: 	Training 300/553. train loss: 3.3145,	0.9035 s / batch. (data: 6.74e-04). ETA=12:41:33, max mem: 27.1 GB 
[11/20 12:41:57 visual_prompt]: 	Training 400/553. train loss: 0.5816,	0.9120 s / batch. (data: 2.62e-04). ETA=12:47:15, max mem: 27.1 GB 
[11/20 12:43:29 visual_prompt]: 	Training 500/553. train loss: 1.4883,	0.9280 s / batch. (data: 7.52e-04). ETA=12:59:08, max mem: 27.1 GB 
[11/20 12:44:18 visual_prompt]: Epoch 9 / 100: avg data time: 2.42e-02, avg batch time: 0.9395, average train loss: 1.3946
[11/20 12:45:11 visual_prompt]: Inference (val):avg data time: 1.25e-04, avg batch time: 0.3036, average loss: 1.4787
[11/20 12:45:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.88	
[11/20 12:45:11 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/20 12:46:52 visual_prompt]: 	Training 100/553. train loss: 0.8986,	0.9079 s / batch. (data: 2.31e-04). ETA=12:39:56, max mem: 27.1 GB 
[11/20 12:48:24 visual_prompt]: 	Training 200/553. train loss: 0.9839,	0.9520 s / batch. (data: 5.87e-03). ETA=13:15:17, max mem: 27.1 GB 
[11/20 12:49:56 visual_prompt]: 	Training 300/553. train loss: 5.4133,	0.9354 s / batch. (data: 7.82e-04). ETA=12:59:51, max mem: 27.1 GB 
[11/20 12:51:28 visual_prompt]: 	Training 400/553. train loss: 0.6195,	0.9120 s / batch. (data: 8.03e-03). ETA=12:38:49, max mem: 27.1 GB 
[11/20 12:53:01 visual_prompt]: 	Training 500/553. train loss: 1.1776,	0.9040 s / batch. (data: 2.79e-04). ETA=12:30:40, max mem: 27.1 GB 
[11/20 12:53:49 visual_prompt]: Epoch 10 / 100: avg data time: 1.86e-02, avg batch time: 0.9358, average train loss: 1.3512
[11/20 12:54:43 visual_prompt]: Inference (val):avg data time: 1.24e-04, avg batch time: 0.3022, average loss: 1.0282
[11/20 12:54:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.35	
[11/20 12:54:43 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/20 12:56:25 visual_prompt]: 	Training 100/553. train loss: 1.4625,	0.9200 s / batch. (data: 1.05e-02). ETA=12:41:36, max mem: 27.1 GB 
[11/20 12:57:57 visual_prompt]: 	Training 200/553. train loss: 0.9745,	0.8962 s / batch. (data: 5.40e-03). ETA=12:20:24, max mem: 27.1 GB 
[11/20 12:59:29 visual_prompt]: 	Training 300/553. train loss: 2.3240,	0.9173 s / batch. (data: 6.98e-04). ETA=12:36:16, max mem: 27.1 GB 
[11/20 13:01:01 visual_prompt]: 	Training 400/553. train loss: 0.3646,	0.9110 s / batch. (data: 2.41e-04). ETA=12:29:34, max mem: 27.1 GB 
[11/20 13:02:33 visual_prompt]: 	Training 500/553. train loss: 0.5210,	0.9520 s / batch. (data: 7.25e-04). ETA=13:01:43, max mem: 27.1 GB 
[11/20 13:03:22 visual_prompt]: Epoch 11 / 100: avg data time: 2.20e-02, avg batch time: 0.9378, average train loss: 1.0978
[11/20 13:04:15 visual_prompt]: Inference (val):avg data time: 8.22e-05, avg batch time: 0.3042, average loss: 0.9069
[11/20 13:04:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.46	
[11/20 13:04:15 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/20 13:05:53 visual_prompt]: 	Training 100/553. train loss: 0.8481,	0.9158 s / batch. (data: 1.09e-02). ETA=12:29:41, max mem: 27.1 GB 
[11/20 13:07:25 visual_prompt]: 	Training 200/553. train loss: 0.5997,	0.9453 s / batch. (data: 1.55e-02). ETA=12:52:16, max mem: 27.1 GB 
[11/20 13:08:57 visual_prompt]: 	Training 300/553. train loss: 0.9778,	0.9298 s / batch. (data: 7.55e-04). ETA=12:38:03, max mem: 27.1 GB 
[11/20 13:10:29 visual_prompt]: 	Training 400/553. train loss: 0.9656,	0.9192 s / batch. (data: 5.41e-03). ETA=12:27:54, max mem: 27.1 GB 
[11/20 13:12:01 visual_prompt]: 	Training 500/553. train loss: 0.8099,	0.9224 s / batch. (data: 5.45e-03). ETA=12:28:58, max mem: 27.1 GB 
[11/20 13:12:50 visual_prompt]: Epoch 12 / 100: avg data time: 1.48e-02, avg batch time: 0.9304, average train loss: 1.0555
[11/20 13:13:43 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.3063, average loss: 0.7125
[11/20 13:13:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 56.00	
[11/20 13:13:43 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/20 13:15:24 visual_prompt]: 	Training 100/553. train loss: 0.9033,	0.9240 s / batch. (data: 3.02e-04). ETA=12:27:53, max mem: 27.1 GB 
[11/20 13:16:56 visual_prompt]: 	Training 200/553. train loss: 1.8660,	0.9548 s / batch. (data: 2.65e-02). ETA=12:51:13, max mem: 27.1 GB 
[11/20 13:18:28 visual_prompt]: 	Training 300/553. train loss: 3.2839,	0.9099 s / batch. (data: 5.41e-03). ETA=12:13:26, max mem: 27.1 GB 
[11/20 13:20:01 visual_prompt]: 	Training 400/553. train loss: 1.1157,	0.9440 s / batch. (data: 7.76e-04). ETA=12:39:21, max mem: 27.1 GB 
[11/20 13:21:33 visual_prompt]: 	Training 500/553. train loss: 0.8360,	0.9256 s / batch. (data: 7.03e-04). ETA=12:23:00, max mem: 27.1 GB 
[11/20 13:22:21 visual_prompt]: Epoch 13 / 100: avg data time: 2.01e-02, avg batch time: 0.9358, average train loss: 0.8886
[11/20 13:23:15 visual_prompt]: Inference (val):avg data time: 1.43e-04, avg batch time: 0.3021, average loss: 0.7053
[11/20 13:23:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.40	
[11/20 13:23:15 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/20 13:24:56 visual_prompt]: 	Training 100/553. train loss: 1.1322,	0.9261 s / batch. (data: 3.19e-04). ETA=12:21:04, max mem: 27.1 GB 
[11/20 13:26:29 visual_prompt]: 	Training 200/553. train loss: 0.6630,	0.9372 s / batch. (data: 7.63e-04). ETA=12:28:19, max mem: 27.1 GB 
[11/20 13:28:02 visual_prompt]: 	Training 300/553. train loss: 0.8366,	0.9720 s / batch. (data: 5.42e-03). ETA=12:54:32, max mem: 27.1 GB 
[11/20 13:29:34 visual_prompt]: 	Training 400/553. train loss: 1.1180,	0.9440 s / batch. (data: 5.41e-03). ETA=12:30:38, max mem: 27.1 GB 
[11/20 13:31:06 visual_prompt]: 	Training 500/553. train loss: 0.6892,	0.9240 s / batch. (data: 7.14e-04). ETA=12:13:12, max mem: 27.1 GB 
[11/20 13:31:54 visual_prompt]: Epoch 14 / 100: avg data time: 2.41e-02, avg batch time: 0.9396, average train loss: 0.8682
[11/20 13:32:50 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.3045, average loss: 0.8463
[11/20 13:32:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.71	
[11/20 13:32:50 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/20 13:34:35 visual_prompt]: 	Training 100/553. train loss: 0.6017,	0.9203 s / batch. (data: 7.96e-03). ETA=12:07:53, max mem: 27.1 GB 
[11/20 13:36:09 visual_prompt]: 	Training 200/553. train loss: 0.7326,	0.9400 s / batch. (data: 2.73e-04). ETA=12:21:55, max mem: 27.1 GB 
[11/20 13:37:46 visual_prompt]: 	Training 300/553. train loss: 0.5697,	0.9188 s / batch. (data: 2.82e-04). ETA=12:03:40, max mem: 27.1 GB 
[11/20 13:39:23 visual_prompt]: 	Training 400/553. train loss: 0.5153,	0.9520 s / batch. (data: 2.69e-04). ETA=12:28:14, max mem: 27.1 GB 
[11/20 13:40:56 visual_prompt]: 	Training 500/553. train loss: 0.5454,	0.9427 s / batch. (data: 1.55e-02). ETA=12:19:21, max mem: 27.1 GB 
[11/20 13:41:46 visual_prompt]: Epoch 15 / 100: avg data time: 5.34e-02, avg batch time: 0.9696, average train loss: 0.8801
[11/20 13:42:45 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3022, average loss: 0.9888
[11/20 13:42:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.04	
[11/20 13:42:45 visual_prompt]: Stopping early.
[11/20 13:42:45 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 13:42:45 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 13:42:45 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/20 13:42:45 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 13:42:45 visual_prompt]: Training with config:
[11/20 13:42:45 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.001_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 13:42:45 visual_prompt]: Loading training data...
[11/20 13:42:45 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 13:42:45 visual_prompt]: Loading validation data...
[11/20 13:42:45 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 13:42:45 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/20 13:42:50 visual_prompt]: Enable all parameters update during training
[11/20 13:42:50 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/20 13:42:50 visual_prompt]: tuned percent:100.000
[11/20 13:42:50 visual_prompt]: Device used for model: 0
[11/20 13:42:50 visual_prompt]: Setting up Evaluator...
[11/20 13:42:50 visual_prompt]: Setting up Trainer...
[11/20 13:42:50 visual_prompt]: 	Setting up the optimizer...
[11/20 13:42:50 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 13:44:35 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9175 s / batch. (data: 2.57e-04). ETA=14:04:04, max mem: 27.1 GB 
[11/20 13:46:10 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9400 s / batch. (data: 2.68e-04). ETA=14:23:13, max mem: 27.1 GB 
[11/20 13:47:45 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9216 s / batch. (data: 2.94e-04). ETA=14:04:47, max mem: 27.1 GB 
[11/20 13:49:19 visual_prompt]: 	Training 400/553. train loss: 7.5668,	1.4480 s / batch. (data: 4.96e-01). ETA=22:04:54, max mem: 27.1 GB 
[11/20 13:50:56 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9155 s / batch. (data: 2.73e-04). ETA=13:56:10, max mem: 27.1 GB 
[11/20 13:51:45 visual_prompt]: Epoch 1 / 100: avg data time: 4.45e-02, avg batch time: 0.9669, average train loss: 7.6130
[11/20 13:52:44 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3028, average loss: 6.9126
[11/20 13:52:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/20 13:52:44 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/20 13:54:25 visual_prompt]: 	Training 100/553. train loss: 1.3817,	0.9284 s / batch. (data: 7.01e-04). ETA=14:05:35, max mem: 27.1 GB 
[11/20 13:55:57 visual_prompt]: 	Training 200/553. train loss: 0.9801,	0.9381 s / batch. (data: 1.07e-02). ETA=14:12:50, max mem: 27.1 GB 
[11/20 13:57:31 visual_prompt]: 	Training 300/553. train loss: 0.7379,	0.9205 s / batch. (data: 2.37e-04). ETA=13:55:17, max mem: 27.1 GB 
[11/20 13:59:06 visual_prompt]: 	Training 400/553. train loss: 1.0492,	0.9027 s / batch. (data: 7.99e-03). ETA=13:37:39, max mem: 27.1 GB 
[11/20 14:00:38 visual_prompt]: 	Training 500/553. train loss: 1.1154,	0.8991 s / batch. (data: 2.58e-04). ETA=13:32:54, max mem: 27.1 GB 
[11/20 14:01:27 visual_prompt]: Epoch 2 / 100: avg data time: 2.56e-02, avg batch time: 0.9455, average train loss: 1.0187
[11/20 14:02:23 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3020, average loss: 0.8943
[11/20 14:02:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.13	
[11/20 14:02:23 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/20 14:04:09 visual_prompt]: 	Training 100/553. train loss: 0.6932,	0.9356 s / batch. (data: 1.60e-02). ETA=14:03:30, max mem: 27.1 GB 
[11/20 14:05:46 visual_prompt]: 	Training 200/553. train loss: 3.2207,	0.9111 s / batch. (data: 5.37e-03). ETA=13:39:54, max mem: 27.1 GB 
[11/20 14:07:22 visual_prompt]: 	Training 300/553. train loss: 0.6822,	0.9308 s / batch. (data: 2.33e-04). ETA=13:56:02, max mem: 27.1 GB 
[11/20 14:08:56 visual_prompt]: 	Training 400/553. train loss: 0.9141,	0.9280 s / batch. (data: 2.88e-04). ETA=13:52:00, max mem: 27.1 GB 
[11/20 14:10:29 visual_prompt]: 	Training 500/553. train loss: 1.2243,	0.9402 s / batch. (data: 3.12e-04). ETA=14:01:23, max mem: 27.1 GB 
[11/20 14:11:19 visual_prompt]: Epoch 3 / 100: avg data time: 4.63e-02, avg batch time: 0.9690, average train loss: 0.8714
[11/20 14:12:18 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3029, average loss: 0.7655
[11/20 14:12:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.16	
[11/20 14:12:18 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/20 14:14:05 visual_prompt]: 	Training 100/553. train loss: 0.8694,	0.9143 s / batch. (data: 2.54e-04). ETA=13:35:54, max mem: 27.1 GB 
[11/20 14:15:39 visual_prompt]: 	Training 200/553. train loss: 0.9992,	0.9509 s / batch. (data: 5.34e-03). ETA=14:06:54, max mem: 27.1 GB 
[11/20 14:17:13 visual_prompt]: 	Training 300/553. train loss: 1.4141,	0.9312 s / batch. (data: 5.38e-03). ETA=13:47:49, max mem: 27.1 GB 
[11/20 14:18:48 visual_prompt]: 	Training 400/553. train loss: 0.5163,	0.9141 s / batch. (data: 1.34e-02). ETA=13:31:06, max mem: 27.1 GB 
[11/20 14:20:24 visual_prompt]: 	Training 500/553. train loss: 0.9663,	0.9318 s / batch. (data: 7.01e-03). ETA=13:45:18, max mem: 27.1 GB 
[11/20 14:21:15 visual_prompt]: Epoch 4 / 100: avg data time: 4.74e-02, avg batch time: 0.9702, average train loss: 0.8383
[11/20 14:22:13 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3046, average loss: 0.7395
[11/20 14:22:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 60.59	
[11/20 14:22:13 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/20 14:23:58 visual_prompt]: 	Training 100/553. train loss: 0.6154,	0.9520 s / batch. (data: 7.98e-03). ETA=14:00:46, max mem: 27.1 GB 
[11/20 14:25:35 visual_prompt]: 	Training 200/553. train loss: 0.5125,	0.9159 s / batch. (data: 4.45e-04). ETA=13:27:22, max mem: 27.1 GB 
[11/20 14:27:09 visual_prompt]: 	Training 300/553. train loss: 1.0695,	0.9420 s / batch. (data: 2.61e-04). ETA=13:48:43, max mem: 27.1 GB 
[11/20 14:28:42 visual_prompt]: 	Training 400/553. train loss: 1.3980,	0.9455 s / batch. (data: 1.34e-02). ETA=13:50:14, max mem: 27.1 GB 
[11/20 14:30:19 visual_prompt]: 	Training 500/553. train loss: 0.7004,	0.9520 s / batch. (data: 8.02e-03). ETA=13:54:25, max mem: 27.1 GB 
[11/20 14:31:08 visual_prompt]: Epoch 5 / 100: avg data time: 4.46e-02, avg batch time: 0.9670, average train loss: 0.8071
[11/20 14:32:06 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3059, average loss: 0.7127
[11/20 14:32:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 61.17	
[11/20 14:32:06 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/20 14:33:52 visual_prompt]: 	Training 100/553. train loss: 0.7612,	0.9192 s / batch. (data: 2.92e-04). ETA=13:23:15, max mem: 27.1 GB 
[11/20 14:35:26 visual_prompt]: 	Training 200/553. train loss: 0.8402,	0.9374 s / batch. (data: 1.04e-02). ETA=13:37:36, max mem: 27.1 GB 
[11/20 14:37:01 visual_prompt]: 	Training 300/553. train loss: 1.0670,	0.9763 s / batch. (data: 2.57e-02). ETA=14:09:58, max mem: 27.1 GB 
[11/20 14:38:38 visual_prompt]: 	Training 400/553. train loss: 0.6823,	2.0288 s / batch. (data: 1.12e+00). ETA=1 day, 5:22:48, max mem: 27.1 GB 
[11/20 14:40:16 visual_prompt]: 	Training 500/553. train loss: 0.8314,	0.9480 s / batch. (data: 2.11e-02). ETA=13:42:09, max mem: 27.1 GB 
[11/20 14:41:04 visual_prompt]: Epoch 6 / 100: avg data time: 5.20e-02, avg batch time: 0.9738, average train loss: 0.7958
[11/20 14:42:02 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.3056, average loss: 0.6685
[11/20 14:42:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 61.28	
[11/20 14:42:02 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/20 14:43:52 visual_prompt]: 	Training 100/553. train loss: 0.7129,	0.9224 s / batch. (data: 2.61e-04). ETA=13:17:38, max mem: 27.1 GB 
[11/20 14:45:26 visual_prompt]: 	Training 200/553. train loss: 0.5655,	0.9150 s / batch. (data: 3.31e-03). ETA=13:09:38, max mem: 27.1 GB 
[11/20 14:47:00 visual_prompt]: 	Training 300/553. train loss: 0.6893,	0.9266 s / batch. (data: 2.61e-04). ETA=13:18:07, max mem: 27.1 GB 
[11/20 14:48:33 visual_prompt]: 	Training 400/553. train loss: 0.7508,	0.9241 s / batch. (data: 3.22e-04). ETA=13:14:26, max mem: 27.1 GB 
[11/20 14:50:10 visual_prompt]: 	Training 500/553. train loss: 0.6026,	0.9219 s / batch. (data: 5.40e-03). ETA=13:11:03, max mem: 27.1 GB 
[11/20 14:51:00 visual_prompt]: Epoch 7 / 100: avg data time: 4.94e-02, avg batch time: 0.9715, average train loss: 0.7769
[11/20 14:51:56 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3067, average loss: 0.6720
[11/20 14:51:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 59.51	
[11/20 14:51:56 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/20 14:53:38 visual_prompt]: 	Training 100/553. train loss: 0.5933,	0.9104 s / batch. (data: 8.28e-03). ETA=12:58:52, max mem: 27.1 GB 
[11/20 14:55:10 visual_prompt]: 	Training 200/553. train loss: 0.6819,	0.9149 s / batch. (data: 5.37e-03). ETA=13:01:07, max mem: 27.1 GB 
[11/20 14:56:43 visual_prompt]: 	Training 300/553. train loss: 0.7424,	0.8996 s / batch. (data: 2.48e-04). ETA=12:46:33, max mem: 27.1 GB 
[11/20 14:58:16 visual_prompt]: 	Training 400/553. train loss: 0.5796,	0.9214 s / batch. (data: 6.71e-04). ETA=13:03:40, max mem: 27.1 GB 
[11/20 14:59:48 visual_prompt]: 	Training 500/553. train loss: 0.5737,	0.9162 s / batch. (data: 1.96e-04). ETA=12:57:40, max mem: 27.1 GB 
[11/20 15:00:37 visual_prompt]: Epoch 8 / 100: avg data time: 2.32e-02, avg batch time: 0.9416, average train loss: 0.7714
[11/20 15:01:30 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.3030, average loss: 0.6831
[11/20 15:01:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 61.35	
[11/20 15:01:30 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/20 15:03:13 visual_prompt]: 	Training 100/553. train loss: 0.8664,	0.9002 s / batch. (data: 2.61e-04). ETA=12:41:47, max mem: 27.1 GB 
[11/20 15:04:46 visual_prompt]: 	Training 200/553. train loss: 1.2752,	0.9113 s / batch. (data: 4.35e-03). ETA=12:49:41, max mem: 27.1 GB 
[11/20 15:06:18 visual_prompt]: 	Training 300/553. train loss: 0.7098,	0.9536 s / batch. (data: 7.41e-04). ETA=13:23:51, max mem: 27.1 GB 
[11/20 15:07:50 visual_prompt]: 	Training 400/553. train loss: 0.6561,	0.9220 s / batch. (data: 1.04e-02). ETA=12:55:40, max mem: 27.1 GB 
[11/20 15:09:23 visual_prompt]: 	Training 500/553. train loss: 0.6645,	0.9234 s / batch. (data: 5.61e-03). ETA=12:55:15, max mem: 27.1 GB 
[11/20 15:10:11 visual_prompt]: Epoch 9 / 100: avg data time: 2.39e-02, avg batch time: 0.9417, average train loss: 0.7780
[11/20 15:11:05 visual_prompt]: Inference (val):avg data time: 6.13e-04, avg batch time: 0.3058, average loss: 0.6942
[11/20 15:11:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 63.10	
[11/20 15:11:05 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/20 15:12:45 visual_prompt]: 	Training 100/553. train loss: 0.7986,	0.9250 s / batch. (data: 5.83e-03). ETA=12:54:15, max mem: 27.1 GB 
[11/20 15:14:18 visual_prompt]: 	Training 200/553. train loss: 0.7284,	0.9427 s / batch. (data: 7.06e-04). ETA=13:07:29, max mem: 27.1 GB 
[11/20 15:15:50 visual_prompt]: 	Training 300/553. train loss: 0.7109,	0.9161 s / batch. (data: 9.15e-03). ETA=12:43:45, max mem: 27.1 GB 
[11/20 15:17:23 visual_prompt]: 	Training 400/553. train loss: 0.6472,	0.9100 s / batch. (data: 1.04e-02). ETA=12:37:10, max mem: 27.1 GB 
[11/20 15:18:55 visual_prompt]: 	Training 500/553. train loss: 0.6786,	0.9011 s / batch. (data: 2.54e-04). ETA=12:28:16, max mem: 27.1 GB 
[11/20 15:19:44 visual_prompt]: Epoch 10 / 100: avg data time: 1.89e-02, avg batch time: 0.9369, average train loss: 0.7591
[11/20 15:20:37 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3037, average loss: 0.7828
[11/20 15:20:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.37	
[11/20 15:20:37 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/20 15:22:19 visual_prompt]: 	Training 100/553. train loss: 0.9770,	0.9221 s / batch. (data: 2.48e-04). ETA=12:43:21, max mem: 27.1 GB 
[11/20 15:23:52 visual_prompt]: 	Training 200/553. train loss: 0.8182,	0.9190 s / batch. (data: 6.81e-04). ETA=12:39:13, max mem: 27.1 GB 
[11/20 15:25:24 visual_prompt]: 	Training 300/553. train loss: 0.5113,	0.9305 s / batch. (data: 6.54e-04). ETA=12:47:11, max mem: 27.1 GB 
[11/20 15:26:57 visual_prompt]: 	Training 400/553. train loss: 2.4501,	0.9246 s / batch. (data: 2.42e-04). ETA=12:40:46, max mem: 27.1 GB 
[11/20 15:28:29 visual_prompt]: 	Training 500/553. train loss: 0.6878,	0.9466 s / batch. (data: 2.20e-02). ETA=12:57:20, max mem: 27.1 GB 
[11/20 15:29:18 visual_prompt]: Epoch 11 / 100: avg data time: 2.26e-02, avg batch time: 0.9409, average train loss: 0.8338
[11/20 15:30:12 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.3079, average loss: 0.6913
[11/20 15:30:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.70	
[11/20 15:30:12 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/20 15:31:50 visual_prompt]: 	Training 100/553. train loss: 0.5589,	0.9389 s / batch. (data: 1.08e-02). ETA=12:48:38, max mem: 27.1 GB 
[11/20 15:33:23 visual_prompt]: 	Training 200/553. train loss: 0.8072,	0.9229 s / batch. (data: 2.37e-04). ETA=12:33:58, max mem: 27.1 GB 
[11/20 15:34:55 visual_prompt]: 	Training 300/553. train loss: 0.8593,	0.9196 s / batch. (data: 2.50e-02). ETA=12:29:43, max mem: 27.1 GB 
[11/20 15:36:27 visual_prompt]: 	Training 400/553. train loss: 1.0791,	0.9099 s / batch. (data: 6.65e-04). ETA=12:20:16, max mem: 27.1 GB 
[11/20 15:37:59 visual_prompt]: 	Training 500/553. train loss: 0.6610,	0.9286 s / batch. (data: 7.01e-04). ETA=12:33:59, max mem: 27.1 GB 
[11/20 15:38:48 visual_prompt]: Epoch 12 / 100: avg data time: 1.59e-02, avg batch time: 0.9333, average train loss: 0.7877
[11/20 15:39:42 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.3055, average loss: 1.0338
[11/20 15:39:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.69	
[11/20 15:39:42 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/20 15:41:23 visual_prompt]: 	Training 100/553. train loss: 0.6832,	0.9217 s / batch. (data: 5.39e-03). ETA=12:26:03, max mem: 27.1 GB 
[11/20 15:42:56 visual_prompt]: 	Training 200/553. train loss: 0.8406,	0.9409 s / batch. (data: 2.19e-02). ETA=12:40:00, max mem: 27.1 GB 
[11/20 15:44:28 visual_prompt]: 	Training 300/553. train loss: 0.6229,	0.9408 s / batch. (data: 1.21e-03). ETA=12:38:20, max mem: 27.1 GB 
[11/20 15:46:01 visual_prompt]: 	Training 400/553. train loss: 0.6370,	0.9188 s / batch. (data: 5.39e-03). ETA=12:19:05, max mem: 27.1 GB 
[11/20 15:47:33 visual_prompt]: 	Training 500/553. train loss: 0.7877,	0.9029 s / batch. (data: 5.34e-03). ETA=12:04:45, max mem: 27.1 GB 
[11/20 15:48:21 visual_prompt]: Epoch 13 / 100: avg data time: 2.16e-02, avg batch time: 0.9388, average train loss: 0.7480
[11/20 15:49:15 visual_prompt]: Inference (val):avg data time: 2.69e-04, avg batch time: 0.3038, average loss: 0.7055
[11/20 15:49:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.03	
[11/20 15:49:15 visual_prompt]: Stopping early.
[11/20 15:49:15 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 15:49:15 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 15:49:15 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/20 15:49:15 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 15:49:15 visual_prompt]: Training with config:
[11/20 15:49:15 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.001_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 15:49:15 visual_prompt]: Loading training data...
[11/20 15:49:15 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 15:49:15 visual_prompt]: Loading validation data...
[11/20 15:49:15 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 15:49:15 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/20 15:49:21 visual_prompt]: Enable all parameters update during training
[11/20 15:49:21 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/20 15:49:21 visual_prompt]: tuned percent:100.000
[11/20 15:49:21 visual_prompt]: Device used for model: 0
[11/20 15:49:21 visual_prompt]: Setting up Evaluator...
[11/20 15:49:21 visual_prompt]: Setting up Trainer...
[11/20 15:49:21 visual_prompt]: 	Setting up the optimizer...
[11/20 15:49:21 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 15:51:01 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9426 s / batch. (data: 7.99e-03). ETA=14:27:10, max mem: 27.1 GB 
[11/20 15:52:34 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9157 s / batch. (data: 1.04e-02). ETA=14:00:53, max mem: 27.1 GB 
[11/20 15:54:06 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9119 s / batch. (data: 2.69e-04). ETA=13:55:52, max mem: 27.1 GB 
[11/20 15:55:38 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9292 s / batch. (data: 6.84e-04). ETA=14:10:11, max mem: 27.1 GB 
[11/20 15:57:11 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9082 s / batch. (data: 1.08e-02). ETA=13:49:30, max mem: 27.1 GB 
[11/20 15:57:59 visual_prompt]: Epoch 1 / 100: avg data time: 1.84e-02, avg batch time: 0.9376, average train loss: 7.6130
[11/20 15:58:55 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3036, average loss: 6.9126
[11/20 15:58:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/20 15:58:55 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/20 16:00:39 visual_prompt]: 	Training 100/553. train loss: 1.3837,	0.8951 s / batch. (data: 6.78e-04). ETA=13:35:14, max mem: 27.1 GB 
[11/20 16:02:11 visual_prompt]: 	Training 200/553. train loss: 0.9804,	0.9678 s / batch. (data: 2.97e-04). ETA=14:39:48, max mem: 27.1 GB 
[11/20 16:03:44 visual_prompt]: 	Training 300/553. train loss: 0.7288,	0.9160 s / batch. (data: 2.50e-04). ETA=13:51:14, max mem: 27.1 GB 
[11/20 16:05:17 visual_prompt]: 	Training 400/553. train loss: 1.0503,	0.9063 s / batch. (data: 2.54e-04). ETA=13:40:56, max mem: 27.1 GB 
[11/20 16:06:49 visual_prompt]: 	Training 500/553. train loss: 1.1298,	0.8958 s / batch. (data: 2.92e-04). ETA=13:29:55, max mem: 27.1 GB 
[11/20 16:07:38 visual_prompt]: Epoch 2 / 100: avg data time: 2.44e-02, avg batch time: 0.9445, average train loss: 1.0188
[11/20 16:08:36 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3016, average loss: 0.8895
[11/20 16:08:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.84	
[11/20 16:08:36 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/20 16:10:22 visual_prompt]: 	Training 100/553. train loss: 0.6927,	0.9284 s / batch. (data: 2.42e-04). ETA=13:57:02, max mem: 27.1 GB 
[11/20 16:11:58 visual_prompt]: 	Training 200/553. train loss: 3.2334,	0.9280 s / batch. (data: 2.73e-04). ETA=13:55:03, max mem: 27.1 GB 
[11/20 16:13:33 visual_prompt]: 	Training 300/553. train loss: 0.6932,	0.9440 s / batch. (data: 6.89e-04). ETA=14:07:54, max mem: 27.1 GB 
[11/20 16:15:08 visual_prompt]: 	Training 400/553. train loss: 0.9126,	0.9332 s / batch. (data: 5.35e-03). ETA=13:56:41, max mem: 27.1 GB 
[11/20 16:16:42 visual_prompt]: 	Training 500/553. train loss: 1.2216,	0.9198 s / batch. (data: 7.70e-04). ETA=13:43:06, max mem: 27.1 GB 
[11/20 16:17:31 visual_prompt]: Epoch 3 / 100: avg data time: 4.55e-02, avg batch time: 0.9682, average train loss: 0.8728
[11/20 16:18:29 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3046, average loss: 0.7673
[11/20 16:18:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.00	
[11/20 16:18:29 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/20 16:20:15 visual_prompt]: 	Training 100/553. train loss: 0.8723,	0.9274 s / batch. (data: 1.04e-02). ETA=13:47:32, max mem: 27.1 GB 
[11/20 16:21:49 visual_prompt]: 	Training 200/553. train loss: 1.0018,	1.3757 s / batch. (data: 4.60e-01). ETA=20:25:21, max mem: 27.1 GB 
[11/20 16:23:24 visual_prompt]: 	Training 300/553. train loss: 1.4278,	0.9267 s / batch. (data: 5.43e-03). ETA=13:43:49, max mem: 27.1 GB 
[11/20 16:25:00 visual_prompt]: 	Training 400/553. train loss: 0.4547,	0.9577 s / batch. (data: 5.35e-03). ETA=14:09:50, max mem: 27.1 GB 
[11/20 16:26:37 visual_prompt]: 	Training 500/553. train loss: 0.9870,	0.8954 s / batch. (data: 3.31e-04). ETA=13:13:03, max mem: 27.1 GB 
[11/20 16:27:28 visual_prompt]: Epoch 4 / 100: avg data time: 5.14e-02, avg batch time: 0.9752, average train loss: 0.8366
[11/20 16:28:27 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3035, average loss: 0.7395
[11/20 16:28:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 61.01	
[11/20 16:28:27 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/20 16:30:12 visual_prompt]: 	Training 100/553. train loss: 0.6013,	0.9107 s / batch. (data: 2.87e-04). ETA=13:24:18, max mem: 27.1 GB 
[11/20 16:31:49 visual_prompt]: 	Training 200/553. train loss: 0.4982,	0.9404 s / batch. (data: 3.13e-04). ETA=13:48:58, max mem: 27.1 GB 
[11/20 16:33:25 visual_prompt]: 	Training 300/553. train loss: 1.0814,	0.9206 s / batch. (data: 1.04e-02). ETA=13:29:54, max mem: 27.1 GB 
[11/20 16:35:03 visual_prompt]: 	Training 400/553. train loss: 1.3843,	4.7080 s / batch. (data: 3.81e+00). ETA=2 days, 20:54:17, max mem: 27.1 GB 
[11/20 16:36:41 visual_prompt]: 	Training 500/553. train loss: 0.7102,	0.9080 s / batch. (data: 3.14e-04). ETA=13:15:47, max mem: 27.1 GB 
[11/20 16:37:30 visual_prompt]: Epoch 5 / 100: avg data time: 5.83e-02, avg batch time: 0.9816, average train loss: 0.8066
[11/20 16:38:28 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3032, average loss: 0.7533
[11/20 16:38:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.65	
[11/20 16:38:28 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/20 16:40:16 visual_prompt]: 	Training 100/553. train loss: 0.7678,	0.9303 s / batch. (data: 5.47e-03). ETA=13:32:59, max mem: 27.1 GB 
[11/20 16:41:51 visual_prompt]: 	Training 200/553. train loss: 0.8150,	0.9281 s / batch. (data: 3.22e-04). ETA=13:29:33, max mem: 27.1 GB 
[11/20 16:43:27 visual_prompt]: 	Training 300/553. train loss: 1.0378,	0.9120 s / batch. (data: 2.92e-04). ETA=13:13:58, max mem: 27.1 GB 
[11/20 16:45:03 visual_prompt]: 	Training 400/553. train loss: 0.7057,	2.0800 s / batch. (data: 1.17e+00). ETA=1 day, 6:07:20, max mem: 27.1 GB 
[11/20 16:46:41 visual_prompt]: 	Training 500/553. train loss: 0.8167,	0.9396 s / batch. (data: 2.68e-04). ETA=13:34:53, max mem: 27.1 GB 
[11/20 16:47:30 visual_prompt]: Epoch 6 / 100: avg data time: 5.48e-02, avg batch time: 0.9798, average train loss: 0.7921
[11/20 16:48:29 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3034, average loss: 0.6719
[11/20 16:48:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 61.86	
[11/20 16:48:29 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/20 16:50:21 visual_prompt]: 	Training 100/553. train loss: 0.7287,	0.9167 s / batch. (data: 5.33e-03). ETA=13:12:41, max mem: 27.1 GB 
[11/20 16:51:54 visual_prompt]: 	Training 200/553. train loss: 0.5864,	0.9722 s / batch. (data: 3.23e-02). ETA=13:59:02, max mem: 27.1 GB 
[11/20 16:53:29 visual_prompt]: 	Training 300/553. train loss: 0.7161,	0.9521 s / batch. (data: 2.43e-02). ETA=13:40:05, max mem: 27.1 GB 
[11/20 16:55:02 visual_prompt]: 	Training 400/553. train loss: 0.7113,	0.9024 s / batch. (data: 2.77e-04). ETA=12:55:49, max mem: 27.1 GB 
[11/20 16:56:37 visual_prompt]: 	Training 500/553. train loss: 0.5651,	0.9343 s / batch. (data: 8.01e-03). ETA=13:21:39, max mem: 27.1 GB 
[11/20 16:57:26 visual_prompt]: Epoch 7 / 100: avg data time: 4.81e-02, avg batch time: 0.9716, average train loss: 0.7802
[11/20 16:58:24 visual_prompt]: Inference (val):avg data time: 2.04e-04, avg batch time: 0.3015, average loss: 0.6721
[11/20 16:58:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 60.25	
[11/20 16:58:24 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/20 17:00:10 visual_prompt]: 	Training 100/553. train loss: 0.5715,	0.9259 s / batch. (data: 7.38e-04). ETA=13:12:06, max mem: 27.1 GB 
[11/20 17:01:47 visual_prompt]: 	Training 200/553. train loss: 0.6736,	0.9161 s / batch. (data: 4.02e-03). ETA=13:02:08, max mem: 27.1 GB 
[11/20 17:03:20 visual_prompt]: 	Training 300/553. train loss: 0.8898,	0.9561 s / batch. (data: 2.47e-02). ETA=13:34:44, max mem: 27.1 GB 
[11/20 17:04:57 visual_prompt]: 	Training 400/553. train loss: 0.5530,	0.9371 s / batch. (data: 5.09e-03). ETA=13:17:01, max mem: 27.1 GB 
[11/20 17:06:32 visual_prompt]: 	Training 500/553. train loss: 0.6079,	0.9275 s / batch. (data: 2.48e-04). ETA=13:07:15, max mem: 27.1 GB 
[11/20 17:07:21 visual_prompt]: Epoch 8 / 100: avg data time: 4.85e-02, avg batch time: 0.9709, average train loss: 0.7725
[11/20 17:08:19 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3013, average loss: 0.6720
[11/20 17:08:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 62.62	
[11/20 17:08:19 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/20 17:10:07 visual_prompt]: 	Training 100/553. train loss: 0.7599,	0.9119 s / batch. (data: 2.94e-04). ETA=12:51:42, max mem: 27.1 GB 
[11/20 17:11:41 visual_prompt]: 	Training 200/553. train loss: 1.0751,	0.9320 s / batch. (data: 7.93e-03). ETA=13:07:09, max mem: 27.1 GB 
[11/20 17:13:14 visual_prompt]: 	Training 300/553. train loss: 0.5559,	0.9494 s / batch. (data: 2.54e-04). ETA=13:20:18, max mem: 27.1 GB 
[11/20 17:14:49 visual_prompt]: 	Training 400/553. train loss: 0.7013,	0.9444 s / batch. (data: 2.62e-02). ETA=13:14:31, max mem: 27.1 GB 
[11/20 17:16:22 visual_prompt]: 	Training 500/553. train loss: 0.6188,	0.9498 s / batch. (data: 5.28e-03). ETA=13:17:25, max mem: 27.1 GB 
[11/20 17:17:14 visual_prompt]: Epoch 9 / 100: avg data time: 4.57e-02, avg batch time: 0.9682, average train loss: 0.7758
[11/20 17:18:12 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3030, average loss: 0.7012
[11/20 17:18:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.56	
[11/20 17:18:12 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/20 17:20:02 visual_prompt]: 	Training 100/553. train loss: 0.8811,	0.9040 s / batch. (data: 2.43e-04). ETA=12:36:41, max mem: 27.1 GB 
[11/20 17:21:42 visual_prompt]: 	Training 200/553. train loss: 0.7856,	0.8935 s / batch. (data: 2.53e-04). ETA=12:26:26, max mem: 27.1 GB 
[11/20 17:23:19 visual_prompt]: 	Training 300/553. train loss: 0.7197,	0.9222 s / batch. (data: 1.04e-02). ETA=12:48:50, max mem: 27.1 GB 
[11/20 17:24:57 visual_prompt]: 	Training 400/553. train loss: 0.6183,	0.9312 s / batch. (data: 3.98e-03). ETA=12:54:48, max mem: 27.1 GB 
[11/20 17:26:33 visual_prompt]: 	Training 500/553. train loss: 0.6983,	1.8735 s / batch. (data: 9.44e-01). ETA=1 day, 1:55:45, max mem: 27.1 GB 
[11/20 17:27:26 visual_prompt]: Epoch 10 / 100: avg data time: 8.16e-02, avg batch time: 1.0012, average train loss: 0.7470
[11/20 17:28:27 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3022, average loss: 0.7684
[11/20 17:28:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.10	
[11/20 17:28:27 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/20 17:30:16 visual_prompt]: 	Training 100/553. train loss: 1.0683,	0.9451 s / batch. (data: 2.62e-04). ETA=13:02:23, max mem: 27.1 GB 
[11/20 17:31:51 visual_prompt]: 	Training 200/553. train loss: 0.8867,	0.9318 s / batch. (data: 2.17e-04). ETA=12:49:47, max mem: 27.1 GB 
[11/20 17:33:24 visual_prompt]: 	Training 300/553. train loss: 0.4673,	0.9230 s / batch. (data: 7.84e-04). ETA=12:40:59, max mem: 27.1 GB 
[11/20 17:34:58 visual_prompt]: 	Training 400/553. train loss: 0.8501,	0.9200 s / batch. (data: 2.47e-04). ETA=12:37:00, max mem: 27.1 GB 
[11/20 17:36:34 visual_prompt]: 	Training 500/553. train loss: 0.5124,	0.9356 s / batch. (data: 7.98e-03). ETA=12:48:17, max mem: 27.1 GB 
[11/20 17:37:23 visual_prompt]: Epoch 11 / 100: avg data time: 4.63e-02, avg batch time: 0.9694, average train loss: 0.7321
[11/20 17:38:21 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3033, average loss: 0.6736
[11/20 17:38:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 60.89	
[11/20 17:38:21 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/20 17:40:05 visual_prompt]: 	Training 100/553. train loss: 0.5410,	0.9461 s / batch. (data: 2.36e-04). ETA=12:54:30, max mem: 27.1 GB 
[11/20 17:41:44 visual_prompt]: 	Training 200/553. train loss: 0.8052,	0.9100 s / batch. (data: 2.75e-04). ETA=12:23:26, max mem: 27.1 GB 
[11/20 17:43:19 visual_prompt]: 	Training 300/553. train loss: 0.8312,	0.9365 s / batch. (data: 2.78e-04). ETA=12:43:30, max mem: 27.1 GB 
[11/20 17:44:53 visual_prompt]: 	Training 400/553. train loss: 0.9557,	0.9485 s / batch. (data: 1.04e-02). ETA=12:51:42, max mem: 27.1 GB 
[11/20 17:46:29 visual_prompt]: 	Training 500/553. train loss: 0.6507,	0.9414 s / batch. (data: 1.49e-02). ETA=12:44:23, max mem: 27.1 GB 
[11/20 17:47:20 visual_prompt]: Epoch 12 / 100: avg data time: 5.03e-02, avg batch time: 0.9727, average train loss: 0.7691
[11/20 17:48:18 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3057, average loss: 1.0212
[11/20 17:48:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.96	
[11/20 17:48:18 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/20 17:50:04 visual_prompt]: 	Training 100/553. train loss: 0.5943,	0.9209 s / batch. (data: 6.85e-04). ETA=12:25:21, max mem: 27.1 GB 
[11/20 17:51:38 visual_prompt]: 	Training 200/553. train loss: 0.8823,	0.9596 s / batch. (data: 7.22e-03). ETA=12:55:03, max mem: 27.1 GB 
[11/20 17:53:14 visual_prompt]: 	Training 300/553. train loss: 0.8681,	0.9400 s / batch. (data: 2.32e-04). ETA=12:37:41, max mem: 27.1 GB 
[11/20 17:54:52 visual_prompt]: 	Training 400/553. train loss: 0.6926,	0.9127 s / batch. (data: 2.96e-04). ETA=12:14:11, max mem: 27.1 GB 
[11/20 17:56:25 visual_prompt]: 	Training 500/553. train loss: 0.7495,	0.9431 s / batch. (data: 6.79e-04). ETA=12:37:03, max mem: 27.1 GB 
[11/20 17:57:14 visual_prompt]: Epoch 13 / 100: avg data time: 4.59e-02, avg batch time: 0.9691, average train loss: 0.7370
[11/20 17:58:12 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3008, average loss: 0.7247
[11/20 17:58:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.73	
[11/20 17:58:12 visual_prompt]: Stopping early.
[11/20 17:58:12 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 17:58:12 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 17:58:12 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/20 17:58:12 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 17:58:12 visual_prompt]: Training with config:
[11/20 17:58:12 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.001_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 17:58:12 visual_prompt]: Loading training data...
[11/20 17:58:12 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 17:58:13 visual_prompt]: Loading validation data...
[11/20 17:58:13 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 17:58:13 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/20 17:58:19 visual_prompt]: Enable all parameters update during training
[11/20 17:58:19 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/20 17:58:19 visual_prompt]: tuned percent:100.000
[11/20 17:58:19 visual_prompt]: Device used for model: 0
[11/20 17:58:19 visual_prompt]: Setting up Evaluator...
[11/20 17:58:19 visual_prompt]: Setting up Trainer...
[11/20 17:58:19 visual_prompt]: 	Setting up the optimizer...
[11/20 17:58:19 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 18:00:03 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9322 s / batch. (data: 3.97e-03). ETA=14:17:37, max mem: 27.1 GB 
[11/20 18:01:41 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9229 s / batch. (data: 3.27e-04). ETA=14:07:30, max mem: 27.1 GB 
[11/20 18:03:16 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9000 s / batch. (data: 2.56e-04). ETA=13:45:01, max mem: 27.1 GB 
[11/20 18:04:49 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9239 s / batch. (data: 5.39e-03). ETA=14:05:20, max mem: 27.1 GB 
[11/20 18:06:25 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9201 s / batch. (data: 1.37e-03). ETA=14:00:20, max mem: 27.1 GB 
[11/20 18:07:14 visual_prompt]: Epoch 1 / 100: avg data time: 4.11e-02, avg batch time: 0.9666, average train loss: 7.6130
[11/20 18:08:11 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3024, average loss: 6.9126
[11/20 18:08:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/20 18:08:11 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/20 18:09:57 visual_prompt]: 	Training 100/553. train loss: 1.3859,	0.9249 s / batch. (data: 7.42e-04). ETA=14:02:21, max mem: 27.1 GB 
[11/20 18:11:31 visual_prompt]: 	Training 200/553. train loss: 0.9839,	0.8940 s / batch. (data: 2.99e-04). ETA=13:32:47, max mem: 27.1 GB 
[11/20 18:13:07 visual_prompt]: 	Training 300/553. train loss: 0.7386,	0.9202 s / batch. (data: 5.43e-03). ETA=13:55:04, max mem: 27.1 GB 
[11/20 18:14:42 visual_prompt]: 	Training 400/553. train loss: 1.0451,	0.9481 s / batch. (data: 7.72e-04). ETA=14:18:44, max mem: 27.1 GB 
[11/20 18:16:17 visual_prompt]: 	Training 500/553. train loss: 1.0887,	0.9349 s / batch. (data: 5.39e-03). ETA=14:05:16, max mem: 27.1 GB 
[11/20 18:17:06 visual_prompt]: Epoch 2 / 100: avg data time: 4.32e-02, avg batch time: 0.9673, average train loss: 1.0183
[11/20 18:18:03 visual_prompt]: Inference (val):avg data time: 3.91e-04, avg batch time: 0.3036, average loss: 0.8942
[11/20 18:18:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.98	
[11/20 18:18:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/20 18:19:48 visual_prompt]: 	Training 100/553. train loss: 0.6852,	0.9192 s / batch. (data: 2.88e-04). ETA=13:48:42, max mem: 27.1 GB 
[11/20 18:21:24 visual_prompt]: 	Training 200/553. train loss: 3.2131,	0.9151 s / batch. (data: 3.39e-03). ETA=13:43:28, max mem: 27.1 GB 
[11/20 18:22:58 visual_prompt]: 	Training 300/553. train loss: 0.6773,	0.9071 s / batch. (data: 2.26e-04). ETA=13:34:45, max mem: 27.1 GB 
[11/20 18:24:32 visual_prompt]: 	Training 400/553. train loss: 0.9133,	0.9218 s / batch. (data: 2.54e-04). ETA=13:46:28, max mem: 27.1 GB 
[11/20 18:26:05 visual_prompt]: 	Training 500/553. train loss: 1.2094,	0.9565 s / batch. (data: 1.55e-02). ETA=14:15:56, max mem: 27.1 GB 
[11/20 18:26:56 visual_prompt]: Epoch 3 / 100: avg data time: 3.99e-02, avg batch time: 0.9634, average train loss: 0.8719
[11/20 18:27:53 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3033, average loss: 0.7659
[11/20 18:27:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.01	
[11/20 18:27:53 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/20 18:29:39 visual_prompt]: 	Training 100/553. train loss: 0.8648,	0.9226 s / batch. (data: 5.36e-03). ETA=13:43:15, max mem: 27.1 GB 
[11/20 18:31:12 visual_prompt]: 	Training 200/553. train loss: 1.0123,	0.9523 s / batch. (data: 3.99e-03). ETA=14:08:09, max mem: 27.1 GB 
[11/20 18:32:47 visual_prompt]: 	Training 300/553. train loss: 1.4177,	0.9394 s / batch. (data: 2.79e-04). ETA=13:55:08, max mem: 27.1 GB 
[11/20 18:34:20 visual_prompt]: 	Training 400/553. train loss: 0.4549,	0.9290 s / batch. (data: 1.05e-02). ETA=13:44:23, max mem: 27.1 GB 
[11/20 18:35:54 visual_prompt]: 	Training 500/553. train loss: 0.9918,	0.9416 s / batch. (data: 1.36e-02). ETA=13:53:55, max mem: 27.1 GB 
[11/20 18:36:45 visual_prompt]: Epoch 4 / 100: avg data time: 3.87e-02, avg batch time: 0.9621, average train loss: 0.8382
[11/20 18:37:43 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3035, average loss: 0.7356
[11/20 18:37:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 61.23	
[11/20 18:37:43 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/20 18:39:27 visual_prompt]: 	Training 100/553. train loss: 0.6089,	0.8994 s / batch. (data: 2.45e-04). ETA=13:14:16, max mem: 27.1 GB 
[11/20 18:41:03 visual_prompt]: 	Training 200/553. train loss: 0.5099,	0.9078 s / batch. (data: 7.95e-03). ETA=13:20:11, max mem: 27.1 GB 
[11/20 18:42:35 visual_prompt]: 	Training 300/553. train loss: 1.0931,	0.9044 s / batch. (data: 3.08e-04). ETA=13:15:43, max mem: 27.1 GB 
[11/20 18:44:10 visual_prompt]: 	Training 400/553. train loss: 1.4016,	2.4964 s / batch. (data: 1.57e+00). ETA=1 day, 12:32:11, max mem: 27.1 GB 
[11/20 18:45:47 visual_prompt]: 	Training 500/553. train loss: 0.7066,	0.9517 s / batch. (data: 3.20e-02). ETA=13:54:08, max mem: 27.1 GB 
[11/20 18:46:36 visual_prompt]: Epoch 5 / 100: avg data time: 4.13e-02, avg batch time: 0.9642, average train loss: 0.8067
[11/20 18:47:32 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3026, average loss: 0.7369
[11/20 18:47:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.06	
[11/20 18:47:32 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/20 18:49:17 visual_prompt]: 	Training 100/553. train loss: 0.7412,	0.9291 s / batch. (data: 2.88e-04). ETA=13:31:57, max mem: 27.1 GB 
[11/20 18:50:50 visual_prompt]: 	Training 200/553. train loss: 0.8591,	0.9272 s / batch. (data: 3.01e-04). ETA=13:28:45, max mem: 27.1 GB 
[11/20 18:52:24 visual_prompt]: 	Training 300/553. train loss: 1.0649,	0.9080 s / batch. (data: 2.85e-04). ETA=13:10:30, max mem: 27.1 GB 
[11/20 18:53:57 visual_prompt]: 	Training 400/553. train loss: 0.7085,	0.9289 s / batch. (data: 5.39e-03). ETA=13:27:08, max mem: 27.1 GB 
[11/20 18:55:33 visual_prompt]: 	Training 500/553. train loss: 0.8043,	0.9147 s / batch. (data: 7.12e-03). ETA=13:13:17, max mem: 27.1 GB 
[11/20 18:56:22 visual_prompt]: Epoch 6 / 100: avg data time: 3.53e-02, avg batch time: 0.9571, average train loss: 0.7977
[11/20 18:57:19 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3059, average loss: 0.6780
[11/20 18:57:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 60.11	
[11/20 18:57:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/20 18:59:07 visual_prompt]: 	Training 100/553. train loss: 0.7123,	0.9196 s / batch. (data: 1.53e-02). ETA=13:15:08, max mem: 27.1 GB 
[11/20 19:00:41 visual_prompt]: 	Training 200/553. train loss: 0.5436,	0.9110 s / batch. (data: 1.04e-02). ETA=13:06:12, max mem: 27.1 GB 
[11/20 19:02:13 visual_prompt]: 	Training 300/553. train loss: 0.6903,	0.9156 s / batch. (data: 5.36e-03). ETA=13:08:38, max mem: 27.1 GB 
[11/20 19:03:46 visual_prompt]: 	Training 400/553. train loss: 0.7125,	0.9446 s / batch. (data: 5.39e-03). ETA=13:32:04, max mem: 27.1 GB 
[11/20 19:05:19 visual_prompt]: 	Training 500/553. train loss: 0.5923,	0.9332 s / batch. (data: 2.23e-04). ETA=13:20:41, max mem: 27.1 GB 
[11/20 19:06:09 visual_prompt]: Epoch 7 / 100: avg data time: 3.58e-02, avg batch time: 0.9579, average train loss: 0.7797
[11/20 19:07:05 visual_prompt]: Inference (val):avg data time: 3.15e-04, avg batch time: 0.3038, average loss: 0.6687
[11/20 19:07:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 60.93	
[11/20 19:07:05 visual_prompt]: Best epoch 7: best metric: -0.669
[11/20 19:07:05 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/20 19:08:50 visual_prompt]: 	Training 100/553. train loss: 0.5855,	0.9454 s / batch. (data: 5.44e-03). ETA=13:28:47, max mem: 27.1 GB 
[11/20 19:10:24 visual_prompt]: 	Training 200/553. train loss: 0.6596,	0.9144 s / batch. (data: 5.35e-03). ETA=13:00:46, max mem: 27.1 GB 
[11/20 19:11:57 visual_prompt]: 	Training 300/553. train loss: 0.7834,	0.9199 s / batch. (data: 2.05e-04). ETA=13:03:54, max mem: 27.1 GB 
[11/20 19:13:33 visual_prompt]: 	Training 400/553. train loss: 0.5824,	0.9362 s / batch. (data: 2.57e-04). ETA=13:16:15, max mem: 27.1 GB 
[11/20 19:15:09 visual_prompt]: 	Training 500/553. train loss: 0.5665,	0.9242 s / batch. (data: 6.92e-04). ETA=13:04:27, max mem: 27.1 GB 
[11/20 19:15:58 visual_prompt]: Epoch 8 / 100: avg data time: 4.05e-02, avg batch time: 0.9625, average train loss: 0.7719
[11/20 19:16:54 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3021, average loss: 0.6770
[11/20 19:16:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 61.49	
[11/20 19:16:54 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/20 19:18:42 visual_prompt]: 	Training 100/553. train loss: 0.8622,	0.9239 s / batch. (data: 2.59e-04). ETA=13:01:52, max mem: 27.1 GB 
[11/20 19:20:19 visual_prompt]: 	Training 200/553. train loss: 1.0854,	0.9080 s / batch. (data: 2.96e-04). ETA=12:46:52, max mem: 27.1 GB 
[11/20 19:21:52 visual_prompt]: 	Training 300/553. train loss: 0.5622,	0.9186 s / batch. (data: 1.04e-02). ETA=12:54:20, max mem: 27.1 GB 
[11/20 19:23:25 visual_prompt]: 	Training 400/553. train loss: 0.8128,	0.9435 s / batch. (data: 7.13e-04). ETA=13:13:43, max mem: 27.1 GB 
[11/20 19:24:57 visual_prompt]: 	Training 500/553. train loss: 0.6588,	0.9373 s / batch. (data: 7.47e-03). ETA=13:06:58, max mem: 27.1 GB 
[11/20 19:25:47 visual_prompt]: Epoch 9 / 100: avg data time: 3.85e-02, avg batch time: 0.9618, average train loss: 0.7807
[11/20 19:26:44 visual_prompt]: Inference (val):avg data time: 9.40e-05, avg batch time: 0.3064, average loss: 0.6970
[11/20 19:26:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 63.45	
[11/20 19:26:44 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/20 19:28:30 visual_prompt]: 	Training 100/553. train loss: 0.7961,	0.9383 s / batch. (data: 6.92e-03). ETA=13:05:24, max mem: 27.1 GB 
[11/20 19:30:06 visual_prompt]: 	Training 200/553. train loss: 0.7843,	0.9163 s / batch. (data: 4.32e-04). ETA=12:45:28, max mem: 27.1 GB 
[11/20 19:31:38 visual_prompt]: 	Training 300/553. train loss: 0.7017,	0.9576 s / batch. (data: 6.24e-04). ETA=13:18:22, max mem: 27.1 GB 
[11/20 19:33:12 visual_prompt]: 	Training 400/553. train loss: 0.5646,	0.9215 s / batch. (data: 2.85e-04). ETA=12:46:46, max mem: 27.1 GB 
[11/20 19:34:47 visual_prompt]: 	Training 500/553. train loss: 0.6134,	0.9320 s / batch. (data: 2.78e-04). ETA=12:53:55, max mem: 27.1 GB 
[11/20 19:35:37 visual_prompt]: Epoch 10 / 100: avg data time: 4.10e-02, avg batch time: 0.9635, average train loss: 0.7496
[11/20 19:36:35 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3039, average loss: 0.7639
[11/20 19:36:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.17	
[11/20 19:36:35 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/20 19:38:22 visual_prompt]: 	Training 100/553. train loss: 1.0253,	0.9383 s / batch. (data: 1.51e-02). ETA=12:56:47, max mem: 27.1 GB 
[11/20 19:39:56 visual_prompt]: 	Training 200/553. train loss: 0.8199,	0.9402 s / batch. (data: 2.84e-04). ETA=12:56:47, max mem: 27.1 GB 
[11/20 19:41:29 visual_prompt]: 	Training 300/553. train loss: 0.5043,	0.9086 s / batch. (data: 2.55e-04). ETA=12:29:06, max mem: 27.1 GB 
[11/20 19:43:03 visual_prompt]: 	Training 400/553. train loss: 1.1834,	0.9057 s / batch. (data: 1.04e-02). ETA=12:25:16, max mem: 27.1 GB 
[11/20 19:44:39 visual_prompt]: 	Training 500/553. train loss: 0.5467,	0.9301 s / batch. (data: 5.38e-03). ETA=12:43:47, max mem: 27.1 GB 
[11/20 19:45:30 visual_prompt]: Epoch 11 / 100: avg data time: 4.39e-02, avg batch time: 0.9672, average train loss: 0.7425
[11/20 19:46:28 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.3029, average loss: 0.6833
[11/20 19:46:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 57.46	
[11/20 19:46:28 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/20 19:48:10 visual_prompt]: 	Training 100/553. train loss: 0.5720,	0.9446 s / batch. (data: 1.65e-02). ETA=12:53:15, max mem: 27.1 GB 
[11/20 19:49:49 visual_prompt]: 	Training 200/553. train loss: 0.8595,	0.9201 s / batch. (data: 2.57e-04). ETA=12:31:41, max mem: 27.1 GB 
[11/20 19:51:23 visual_prompt]: 	Training 300/553. train loss: 0.8333,	0.9320 s / batch. (data: 2.69e-04). ETA=12:39:49, max mem: 27.1 GB 
[11/20 19:53:00 visual_prompt]: 	Training 400/553. train loss: 0.9910,	0.9452 s / batch. (data: 5.81e-03). ETA=12:49:02, max mem: 27.1 GB 
[11/20 19:54:36 visual_prompt]: 	Training 500/553. train loss: 0.6493,	0.9080 s / batch. (data: 2.64e-04). ETA=12:17:14, max mem: 27.1 GB 
[11/20 19:55:26 visual_prompt]: Epoch 12 / 100: avg data time: 5.05e-02, avg batch time: 0.9722, average train loss: 0.7590
[11/20 19:56:22 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3037, average loss: 1.0115
[11/20 19:56:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.16	
[11/20 19:56:22 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/20 19:58:10 visual_prompt]: 	Training 100/553. train loss: 0.6389,	0.9386 s / batch. (data: 1.05e-02). ETA=12:39:42, max mem: 27.1 GB 
[11/20 19:59:43 visual_prompt]: 	Training 200/553. train loss: 0.8198,	0.9498 s / batch. (data: 1.40e-02). ETA=12:47:12, max mem: 27.1 GB 
[11/20 20:01:19 visual_prompt]: 	Training 300/553. train loss: 0.6138,	0.9503 s / batch. (data: 5.47e-03). ETA=12:45:59, max mem: 27.1 GB 
[11/20 20:02:55 visual_prompt]: 	Training 400/553. train loss: 0.6288,	0.9100 s / batch. (data: 2.37e-04). ETA=12:11:59, max mem: 27.1 GB 
[11/20 20:04:31 visual_prompt]: 	Training 500/553. train loss: 0.7232,	0.9450 s / batch. (data: 3.07e-04). ETA=12:38:36, max mem: 27.1 GB 
[11/20 20:05:20 visual_prompt]: Epoch 13 / 100: avg data time: 4.90e-02, avg batch time: 0.9716, average train loss: 0.7339
[11/20 20:06:17 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3032, average loss: 0.7490
[11/20 20:06:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.01	
[11/20 20:06:17 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/20 20:08:04 visual_prompt]: 	Training 100/553. train loss: 0.6854,	0.9306 s / batch. (data: 2.19e-04). ETA=12:24:38, max mem: 27.1 GB 
[11/20 20:09:43 visual_prompt]: 	Training 200/553. train loss: 0.2787,	0.9356 s / batch. (data: 7.56e-04). ETA=12:27:06, max mem: 27.1 GB 
[11/20 20:11:18 visual_prompt]: 	Training 300/553. train loss: 0.7086,	0.9356 s / batch. (data: 5.41e-03). ETA=12:25:33, max mem: 27.1 GB 
[11/20 20:12:51 visual_prompt]: 	Training 400/553. train loss: 0.9262,	0.9199 s / batch. (data: 2.77e-04). ETA=12:11:28, max mem: 27.1 GB 
[11/20 20:14:26 visual_prompt]: 	Training 500/553. train loss: 0.6709,	0.9458 s / batch. (data: 9.81e-03). ETA=12:30:31, max mem: 27.1 GB 
[11/20 20:15:16 visual_prompt]: Epoch 14 / 100: avg data time: 5.11e-02, avg batch time: 0.9741, average train loss: 0.7404
[11/20 20:16:14 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3053, average loss: 0.6922
[11/20 20:16:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 59.61	
[11/20 20:16:14 visual_prompt]: Stopping early.
[11/20 20:16:14 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 20:16:14 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 20:16:14 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/20 20:16:14 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 20:16:14 visual_prompt]: Training with config:
[11/20 20:16:14 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.001_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 20:16:14 visual_prompt]: Loading training data...
[11/20 20:16:14 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 20:16:14 visual_prompt]: Loading validation data...
[11/20 20:16:14 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 20:16:14 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/20 20:16:16 visual_prompt]: Enable all parameters update during training
[11/20 20:16:16 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/20 20:16:16 visual_prompt]: tuned percent:100.000
[11/20 20:16:16 visual_prompt]: Device used for model: 0
[11/20 20:16:16 visual_prompt]: Setting up Evaluator...
[11/20 20:16:16 visual_prompt]: Setting up Trainer...
[11/20 20:16:16 visual_prompt]: 	Setting up the optimizer...
[11/20 20:16:16 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 20:17:59 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9165 s / batch. (data: 2.93e-04). ETA=14:03:09, max mem: 27.1 GB 
[11/20 20:19:37 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9608 s / batch. (data: 2.57e-02). ETA=14:42:20, max mem: 27.1 GB 
[11/20 20:21:13 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9160 s / batch. (data: 3.07e-04). ETA=13:59:40, max mem: 27.1 GB 
[11/20 20:22:46 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9200 s / batch. (data: 5.57e-03). ETA=14:01:48, max mem: 27.1 GB 
[11/20 20:24:21 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9200 s / batch. (data: 5.43e-03). ETA=14:00:15, max mem: 27.1 GB 
[11/20 20:25:10 visual_prompt]: Epoch 1 / 100: avg data time: 4.52e-02, avg batch time: 0.9646, average train loss: 7.6130
[11/20 20:26:07 visual_prompt]: Inference (val):avg data time: 8.84e-05, avg batch time: 0.3024, average loss: 6.9126
[11/20 20:26:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/20 20:26:07 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/20 20:27:54 visual_prompt]: 	Training 100/553. train loss: 1.5928,	0.9182 s / batch. (data: 7.12e-04). ETA=13:56:18, max mem: 27.1 GB 
[11/20 20:29:27 visual_prompt]: 	Training 200/553. train loss: 0.8232,	1.4788 s / batch. (data: 5.91e-01). ETA=22:24:23, max mem: 27.1 GB 
[11/20 20:31:02 visual_prompt]: 	Training 300/553. train loss: 0.9183,	0.9160 s / batch. (data: 3.99e-03). ETA=13:51:13, max mem: 27.1 GB 
[11/20 20:32:40 visual_prompt]: 	Training 400/553. train loss: 1.1900,	0.9480 s / batch. (data: 7.96e-04). ETA=14:18:40, max mem: 27.1 GB 
[11/20 20:34:16 visual_prompt]: 	Training 500/553. train loss: 0.7783,	0.9280 s / batch. (data: 4.03e-04). ETA=13:59:02, max mem: 27.1 GB 
[11/20 20:35:05 visual_prompt]: Epoch 2 / 100: avg data time: 5.39e-02, avg batch time: 0.9721, average train loss: 1.1140
[11/20 20:36:02 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3036, average loss: 0.9010
[11/20 20:36:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.88	
[11/20 20:36:02 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/20 20:37:46 visual_prompt]: 	Training 100/553. train loss: 0.7303,	0.9160 s / batch. (data: 3.29e-04). ETA=13:45:49, max mem: 27.1 GB 
[11/20 20:39:23 visual_prompt]: 	Training 200/553. train loss: 3.2522,	0.9200 s / batch. (data: 3.94e-03). ETA=13:47:54, max mem: 27.1 GB 
[11/20 20:40:58 visual_prompt]: 	Training 300/553. train loss: 0.8818,	0.9270 s / batch. (data: 5.44e-03). ETA=13:52:40, max mem: 27.1 GB 
[11/20 20:42:33 visual_prompt]: 	Training 400/553. train loss: 0.7402,	0.9520 s / batch. (data: 7.28e-04). ETA=14:13:34, max mem: 27.1 GB 
[11/20 20:44:06 visual_prompt]: 	Training 500/553. train loss: 1.3311,	0.8993 s / batch. (data: 2.99e-04). ETA=13:24:47, max mem: 27.1 GB 
[11/20 20:44:56 visual_prompt]: Epoch 3 / 100: avg data time: 4.76e-02, avg batch time: 0.9660, average train loss: 0.9413
[11/20 20:45:54 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3023, average loss: 1.0368
[11/20 20:45:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.06	
[11/20 20:45:54 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/20 20:47:48 visual_prompt]: 	Training 100/553. train loss: 1.5008,	0.9120 s / batch. (data: 2.83e-04). ETA=13:33:49, max mem: 27.1 GB 
[11/20 20:49:37 visual_prompt]: 	Training 200/553. train loss: 0.8746,	3.3028 s / batch. (data: 2.40e+00). ETA=2 days, 1:01:46, max mem: 27.1 GB 
[11/20 20:51:17 visual_prompt]: 	Training 300/553. train loss: 0.7101,	0.9121 s / batch. (data: 1.32e-02). ETA=13:30:52, max mem: 27.1 GB 
[11/20 20:52:56 visual_prompt]: 	Training 400/553. train loss: 0.9865,	0.9057 s / batch. (data: 3.13e-04). ETA=13:23:38, max mem: 27.1 GB 
[11/20 20:54:32 visual_prompt]: 	Training 500/553. train loss: 0.7810,	0.8969 s / batch. (data: 5.40e-03). ETA=13:14:21, max mem: 27.1 GB 
[11/20 20:55:23 visual_prompt]: Epoch 4 / 100: avg data time: 1.16e-01, avg batch time: 1.0282, average train loss: 0.9274
[11/20 20:56:22 visual_prompt]: Inference (val):avg data time: 2.45e-04, avg batch time: 0.3024, average loss: 0.6654
[11/20 20:56:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 61.77	
[11/20 20:56:22 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/20 20:58:07 visual_prompt]: 	Training 100/553. train loss: 0.6732,	0.9480 s / batch. (data: 5.40e-03). ETA=13:57:14, max mem: 27.1 GB 
[11/20 20:59:43 visual_prompt]: 	Training 200/553. train loss: 0.4223,	0.9200 s / batch. (data: 3.08e-04). ETA=13:30:57, max mem: 27.1 GB 
[11/20 21:01:16 visual_prompt]: 	Training 300/553. train loss: 2.5886,	0.9318 s / batch. (data: 1.19e-02). ETA=13:39:49, max mem: 27.1 GB 
[11/20 21:02:51 visual_prompt]: 	Training 400/553. train loss: 0.9191,	2.8200 s / batch. (data: 1.91e+00). ETA=1 day, 17:16:20, max mem: 27.1 GB 
[11/20 21:04:29 visual_prompt]: 	Training 500/553. train loss: 0.6148,	0.9400 s / batch. (data: 7.97e-04). ETA=13:43:52, max mem: 27.1 GB 
[11/20 21:05:18 visual_prompt]: Epoch 5 / 100: avg data time: 5.11e-02, avg batch time: 0.9699, average train loss: 0.9903
[11/20 21:06:15 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3056, average loss: 1.2279
[11/20 21:06:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.84	
[11/20 21:06:15 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/20 21:08:01 visual_prompt]: 	Training 100/553. train loss: 0.5761,	0.9120 s / batch. (data: 5.46e-03). ETA=13:17:00, max mem: 27.1 GB 
[11/20 21:09:35 visual_prompt]: 	Training 200/553. train loss: 1.6799,	0.9080 s / batch. (data: 5.43e-03). ETA=13:12:01, max mem: 27.1 GB 
[11/20 21:11:10 visual_prompt]: 	Training 300/553. train loss: 0.4776,	0.9280 s / batch. (data: 2.99e-04). ETA=13:27:53, max mem: 27.1 GB 
[11/20 21:12:46 visual_prompt]: 	Training 400/553. train loss: 0.9125,	2.3300 s / batch. (data: 1.42e+00). ETA=1 day, 9:44:32, max mem: 27.1 GB 
[11/20 21:14:23 visual_prompt]: 	Training 500/553. train loss: 1.1524,	0.9320 s / batch. (data: 7.12e-04). ETA=13:28:16, max mem: 27.1 GB 
[11/20 21:15:12 visual_prompt]: Epoch 6 / 100: avg data time: 5.19e-02, avg batch time: 0.9706, average train loss: 1.0239
[11/20 21:16:10 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3024, average loss: 1.2501
[11/20 21:16:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.64	
[11/20 21:16:10 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/20 21:17:59 visual_prompt]: 	Training 100/553. train loss: 0.4078,	0.9079 s / batch. (data: 2.90e-04). ETA=13:05:05, max mem: 27.1 GB 
[11/20 21:19:33 visual_prompt]: 	Training 200/553. train loss: 0.0438,	0.9122 s / batch. (data: 3.21e-04). ETA=13:07:17, max mem: 27.1 GB 
[11/20 21:21:06 visual_prompt]: 	Training 300/553. train loss: 0.5592,	0.9153 s / batch. (data: 1.05e-02). ETA=13:08:23, max mem: 27.1 GB 
[11/20 21:22:39 visual_prompt]: 	Training 400/553. train loss: 0.8967,	0.9440 s / batch. (data: 3.27e-04). ETA=13:31:32, max mem: 27.1 GB 
[11/20 21:24:13 visual_prompt]: 	Training 500/553. train loss: 0.4814,	0.9240 s / batch. (data: 2.90e-04). ETA=13:12:50, max mem: 27.1 GB 
[11/20 21:25:03 visual_prompt]: Epoch 7 / 100: avg data time: 4.53e-02, avg batch time: 0.9643, average train loss: 1.0254
[11/20 21:26:00 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3047, average loss: 0.7111
[11/20 21:26:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 64.31	
[11/20 21:26:00 visual_prompt]: Best epoch 7: best metric: -0.711
[11/20 21:26:00 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/20 21:27:46 visual_prompt]: 	Training 100/553. train loss: 0.4157,	0.9069 s / batch. (data: 7.10e-03). ETA=12:55:50, max mem: 27.1 GB 
[11/20 21:29:22 visual_prompt]: 	Training 200/553. train loss: 0.7027,	0.9209 s / batch. (data: 1.84e-02). ETA=13:06:18, max mem: 27.1 GB 
[11/20 21:30:58 visual_prompt]: 	Training 300/553. train loss: 1.2349,	0.9080 s / batch. (data: 7.84e-04). ETA=12:53:44, max mem: 27.1 GB 
[11/20 21:32:33 visual_prompt]: 	Training 400/553. train loss: 0.5066,	0.9360 s / batch. (data: 6.65e-04). ETA=13:16:03, max mem: 27.1 GB 
[11/20 21:34:08 visual_prompt]: 	Training 500/553. train loss: 0.8883,	0.9280 s / batch. (data: 3.06e-04). ETA=13:07:41, max mem: 27.1 GB 
[11/20 21:34:57 visual_prompt]: Epoch 8 / 100: avg data time: 5.22e-02, avg batch time: 0.9703, average train loss: 1.0333
[11/20 21:35:54 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3041, average loss: 0.8904
[11/20 21:35:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 66.96	
[11/20 21:35:54 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/20 21:37:43 visual_prompt]: 	Training 100/553. train loss: 1.3792,	0.9040 s / batch. (data: 2.85e-04). ETA=12:45:01, max mem: 27.1 GB 
[11/20 21:39:18 visual_prompt]: 	Training 200/553. train loss: 0.8641,	0.9400 s / batch. (data: 1.05e-02). ETA=13:13:54, max mem: 27.1 GB 
[11/20 21:40:52 visual_prompt]: 	Training 300/553. train loss: 1.9486,	0.9280 s / batch. (data: 7.96e-03). ETA=13:02:14, max mem: 27.1 GB 
[11/20 21:42:26 visual_prompt]: 	Training 400/553. train loss: 0.4780,	0.9480 s / batch. (data: 7.34e-04). ETA=13:17:31, max mem: 27.1 GB 
[11/20 21:43:59 visual_prompt]: 	Training 500/553. train loss: 1.7729,	0.9160 s / batch. (data: 2.80e-04). ETA=12:49:05, max mem: 27.1 GB 
[11/20 21:44:48 visual_prompt]: Epoch 9 / 100: avg data time: 4.62e-02, avg batch time: 0.9652, average train loss: 1.1020
[11/20 21:45:46 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3055, average loss: 1.1360
[11/20 21:45:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 66.25	
[11/20 21:45:46 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/20 21:47:31 visual_prompt]: 	Training 100/553. train loss: 1.2174,	0.9480 s / batch. (data: 2.70e-04). ETA=13:13:32, max mem: 27.1 GB 
[11/20 21:49:07 visual_prompt]: 	Training 200/553. train loss: 0.9880,	0.9142 s / batch. (data: 2.57e-02). ETA=12:43:43, max mem: 27.1 GB 
[11/20 21:50:40 visual_prompt]: 	Training 300/553. train loss: 3.0603,	0.9176 s / batch. (data: 2.96e-04). ETA=12:45:02, max mem: 27.1 GB 
[11/20 21:52:14 visual_prompt]: 	Training 400/553. train loss: 2.2048,	0.9280 s / batch. (data: 7.40e-04). ETA=12:52:08, max mem: 27.1 GB 
[11/20 21:53:48 visual_prompt]: 	Training 500/553. train loss: 0.1673,	0.9120 s / batch. (data: 3.17e-04). ETA=12:37:17, max mem: 27.1 GB 
[11/20 21:54:39 visual_prompt]: Epoch 10 / 100: avg data time: 4.60e-02, avg batch time: 0.9637, average train loss: 0.9862
[11/20 21:55:36 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3072, average loss: 0.8295
[11/20 21:55:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 69.56	
[11/20 21:55:36 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/20 21:57:24 visual_prompt]: 	Training 100/553. train loss: 2.0505,	0.9079 s / batch. (data: 2.91e-04). ETA=12:31:37, max mem: 27.1 GB 
[11/20 21:58:58 visual_prompt]: 	Training 200/553. train loss: 0.8687,	0.9040 s / batch. (data: 3.99e-03). ETA=12:26:50, max mem: 27.1 GB 
[11/20 22:00:31 visual_prompt]: 	Training 300/553. train loss: 2.0034,	0.9217 s / batch. (data: 2.86e-04). ETA=12:39:55, max mem: 27.1 GB 
[11/20 22:02:07 visual_prompt]: 	Training 400/553. train loss: 0.2429,	0.9436 s / batch. (data: 2.94e-04). ETA=12:56:25, max mem: 27.1 GB 
[11/20 22:03:40 visual_prompt]: 	Training 500/553. train loss: 0.5406,	0.9520 s / batch. (data: 1.06e-02). ETA=13:01:43, max mem: 27.1 GB 
[11/20 22:04:29 visual_prompt]: Epoch 11 / 100: avg data time: 4.30e-02, avg batch time: 0.9630, average train loss: 0.9980
[11/20 22:05:27 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3022, average loss: 0.7742
[11/20 22:05:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 67.01	
[11/20 22:05:27 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/20 22:07:08 visual_prompt]: 	Training 100/553. train loss: 1.0892,	0.9425 s / batch. (data: 2.15e-03). ETA=12:51:34, max mem: 27.1 GB 
[11/20 22:08:48 visual_prompt]: 	Training 200/553. train loss: 0.5981,	0.9360 s / batch. (data: 8.24e-04). ETA=12:44:37, max mem: 27.1 GB 
[11/20 22:10:21 visual_prompt]: 	Training 300/553. train loss: 1.0464,	0.9080 s / batch. (data: 5.46e-03). ETA=12:20:15, max mem: 27.1 GB 
[11/20 22:11:56 visual_prompt]: 	Training 400/553. train loss: 0.8715,	0.9350 s / batch. (data: 2.11e-02). ETA=12:40:41, max mem: 27.1 GB 
[11/20 22:13:30 visual_prompt]: 	Training 500/553. train loss: 0.5655,	0.9438 s / batch. (data: 5.94e-03). ETA=12:46:20, max mem: 27.1 GB 
[11/20 22:14:20 visual_prompt]: Epoch 12 / 100: avg data time: 4.61e-02, avg batch time: 0.9645, average train loss: 1.0132
[11/20 22:15:18 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3107, average loss: 0.6369
[11/20 22:15:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 68.88	
[11/20 22:15:18 visual_prompt]: Best epoch 12: best metric: -0.637
[11/20 22:15:18 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/20 22:17:03 visual_prompt]: 	Training 100/553. train loss: 1.0259,	0.9170 s / batch. (data: 7.15e-04). ETA=12:22:15, max mem: 27.1 GB 
[11/20 22:18:36 visual_prompt]: 	Training 200/553. train loss: 1.8123,	0.9240 s / batch. (data: 5.39e-03). ETA=12:26:21, max mem: 27.1 GB 
[11/20 22:20:12 visual_prompt]: 	Training 300/553. train loss: 1.9821,	0.8988 s / batch. (data: 3.22e-04). ETA=12:04:27, max mem: 27.1 GB 
[11/20 22:21:51 visual_prompt]: 	Training 400/553. train loss: 1.2729,	0.9200 s / batch. (data: 3.11e-04). ETA=12:20:01, max mem: 27.1 GB 
[11/20 22:23:24 visual_prompt]: 	Training 500/553. train loss: 0.9376,	0.9815 s / batch. (data: 2.63e-02). ETA=13:07:54, max mem: 27.1 GB 
[11/20 22:24:12 visual_prompt]: Epoch 13 / 100: avg data time: 4.72e-02, avg batch time: 0.9668, average train loss: 0.8504
[11/20 22:25:10 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3039, average loss: 0.7052
[11/20 22:25:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 67.87	
[11/20 22:25:10 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/20 22:26:59 visual_prompt]: 	Training 100/553. train loss: 1.3439,	0.9413 s / batch. (data: 2.92e-02). ETA=12:33:10, max mem: 27.1 GB 
[11/20 22:28:36 visual_prompt]: 	Training 200/553. train loss: 1.6888,	0.9156 s / batch. (data: 9.39e-03). ETA=12:11:09, max mem: 27.1 GB 
[11/20 22:30:10 visual_prompt]: 	Training 300/553. train loss: 1.2187,	0.8939 s / batch. (data: 3.42e-04). ETA=11:52:18, max mem: 27.1 GB 
[11/20 22:31:43 visual_prompt]: 	Training 400/553. train loss: 2.3758,	0.9167 s / batch. (data: 5.47e-03). ETA=12:08:57, max mem: 27.1 GB 
[11/20 22:33:17 visual_prompt]: 	Training 500/553. train loss: 0.7955,	0.9360 s / batch. (data: 7.97e-03). ETA=12:22:44, max mem: 27.1 GB 
[11/20 22:34:06 visual_prompt]: Epoch 14 / 100: avg data time: 4.86e-02, avg batch time: 0.9692, average train loss: 0.9193
[11/20 22:35:04 visual_prompt]: Inference (val):avg data time: 8.56e-05, avg batch time: 0.3025, average loss: 0.7081
[11/20 22:35:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 69.43	
[11/20 22:35:04 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/20 22:36:51 visual_prompt]: 	Training 100/553. train loss: 0.3624,	0.9400 s / batch. (data: 3.95e-03). ETA=12:23:29, max mem: 27.1 GB 
[11/20 22:38:27 visual_prompt]: 	Training 200/553. train loss: 0.5793,	0.9642 s / batch. (data: 1.56e-02). ETA=12:41:03, max mem: 27.1 GB 
[11/20 22:40:02 visual_prompt]: 	Training 300/553. train loss: 0.5172,	0.9320 s / batch. (data: 2.88e-04). ETA=12:14:04, max mem: 27.1 GB 
[11/20 22:41:38 visual_prompt]: 	Training 400/553. train loss: 0.6786,	0.9268 s / batch. (data: 3.00e-02). ETA=12:08:24, max mem: 27.1 GB 
[11/20 22:43:12 visual_prompt]: 	Training 500/553. train loss: 0.8836,	0.9200 s / batch. (data: 2.87e-04). ETA=12:01:32, max mem: 27.1 GB 
[11/20 22:44:01 visual_prompt]: Epoch 15 / 100: avg data time: 5.15e-02, avg batch time: 0.9704, average train loss: 0.8978
[11/20 22:44:58 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3043, average loss: 0.6678
[11/20 22:44:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 65.88	
[11/20 22:44:58 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/20 22:46:42 visual_prompt]: 	Training 100/553. train loss: 0.6637,	0.9240 s / batch. (data: 5.45e-03). ETA=12:02:20, max mem: 27.1 GB 
[11/20 22:48:16 visual_prompt]: 	Training 200/553. train loss: 0.3571,	1.2763 s / batch. (data: 3.52e-01). ETA=16:35:36, max mem: 27.1 GB 
[11/20 22:49:55 visual_prompt]: 	Training 300/553. train loss: 0.7969,	0.9280 s / batch. (data: 1.60e-02). ETA=12:02:22, max mem: 27.1 GB 
[11/20 22:51:31 visual_prompt]: 	Training 400/553. train loss: 0.3532,	0.9262 s / batch. (data: 2.22e-02). ETA=11:59:26, max mem: 27.1 GB 
[11/20 22:53:05 visual_prompt]: 	Training 500/553. train loss: 0.4219,	0.9600 s / batch. (data: 7.57e-04). ETA=12:24:04, max mem: 27.1 GB 
[11/20 22:53:54 visual_prompt]: Epoch 16 / 100: avg data time: 4.94e-02, avg batch time: 0.9674, average train loss: 0.8100
[11/20 22:54:51 visual_prompt]: Inference (val):avg data time: 1.33e-04, avg batch time: 0.3043, average loss: 0.8583
[11/20 22:54:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 68.64	
[11/20 22:54:51 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/20 22:56:32 visual_prompt]: 	Training 100/553. train loss: 0.5097,	0.9200 s / batch. (data: 7.95e-03). ETA=11:50:43, max mem: 27.1 GB 
[11/20 22:58:11 visual_prompt]: 	Training 200/553. train loss: 1.0968,	0.9080 s / batch. (data: 2.86e-04). ETA=11:39:57, max mem: 27.1 GB 
[11/20 22:59:44 visual_prompt]: 	Training 300/553. train loss: 0.4818,	0.9316 s / batch. (data: 2.94e-04). ETA=11:56:34, max mem: 27.1 GB 
[11/20 23:01:21 visual_prompt]: 	Training 400/553. train loss: 1.1706,	0.9367 s / batch. (data: 2.20e-02). ETA=11:58:59, max mem: 27.1 GB 
[11/20 23:02:55 visual_prompt]: 	Training 500/553. train loss: 0.6527,	0.9240 s / batch. (data: 3.13e-04). ETA=11:47:39, max mem: 27.1 GB 
[11/20 23:03:45 visual_prompt]: Epoch 17 / 100: avg data time: 4.58e-02, avg batch time: 0.9658, average train loss: 0.9323
[11/20 23:04:42 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3023, average loss: 1.0422
[11/20 23:04:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 69.05	
[11/20 23:04:42 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/20 23:06:30 visual_prompt]: 	Training 100/553. train loss: 0.6527,	0.9560 s / batch. (data: 7.24e-04). ETA=12:09:44, max mem: 27.1 GB 
[11/20 23:08:05 visual_prompt]: 	Training 200/553. train loss: 0.9669,	0.8935 s / batch. (data: 4.65e-04). ETA=11:20:31, max mem: 27.1 GB 
[11/20 23:09:39 visual_prompt]: 	Training 300/553. train loss: 0.8056,	0.9691 s / batch. (data: 3.28e-04). ETA=12:16:29, max mem: 27.1 GB 
[11/20 23:11:13 visual_prompt]: 	Training 400/553. train loss: 0.1544,	0.9414 s / batch. (data: 2.14e-02). ETA=11:53:54, max mem: 27.1 GB 
[11/20 23:12:50 visual_prompt]: 	Training 500/553. train loss: 0.4929,	0.9440 s / batch. (data: 7.29e-04). ETA=11:54:16, max mem: 27.1 GB 
[11/20 23:13:38 visual_prompt]: Epoch 18 / 100: avg data time: 5.07e-02, avg batch time: 0.9690, average train loss: 0.8530
[11/20 23:14:36 visual_prompt]: Inference (val):avg data time: 2.51e-04, avg batch time: 0.3042, average loss: 0.6877
[11/20 23:14:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 67.34	
[11/20 23:14:36 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/20 23:16:21 visual_prompt]: 	Training 100/553. train loss: 0.2888,	0.9280 s / batch. (data: 8.03e-04). ETA=11:39:48, max mem: 27.1 GB 
[11/20 23:17:58 visual_prompt]: 	Training 200/553. train loss: 0.8582,	2.1936 s / batch. (data: 1.27e+00). ETA=1 day, 3:30:33, max mem: 27.1 GB 
[11/20 23:19:30 visual_prompt]: 	Training 300/553. train loss: 0.4738,	0.9520 s / batch. (data: 5.41e-03). ETA=11:54:43, max mem: 27.1 GB 
[11/20 23:21:06 visual_prompt]: 	Training 400/553. train loss: 0.5215,	0.9216 s / batch. (data: 7.94e-03). ETA=11:30:23, max mem: 27.1 GB 
[11/20 23:22:41 visual_prompt]: 	Training 500/553. train loss: 0.9860,	0.9200 s / batch. (data: 2.30e-04). ETA=11:27:40, max mem: 27.1 GB 
[11/20 23:23:30 visual_prompt]: Epoch 19 / 100: avg data time: 4.52e-02, avg batch time: 0.9654, average train loss: 0.7673
[11/20 23:24:27 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3055, average loss: 0.6410
[11/20 23:24:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 70.44	
[11/20 23:24:27 visual_prompt]: Stopping early.
[11/20 23:24:27 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 23:24:27 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 23:24:27 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/20 23:24:27 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 23:24:27 visual_prompt]: Training with config:
[11/20 23:24:27 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0005_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 23:24:27 visual_prompt]: Loading training data...
[11/20 23:24:27 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 23:24:27 visual_prompt]: Loading validation data...
[11/20 23:24:27 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 23:24:27 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/20 23:24:29 visual_prompt]: Enable all parameters update during training
[11/20 23:24:29 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/20 23:24:29 visual_prompt]: tuned percent:100.000
[11/20 23:24:29 visual_prompt]: Device used for model: 0
[11/20 23:24:29 visual_prompt]: Setting up Evaluator...
[11/20 23:24:29 visual_prompt]: Setting up Trainer...
[11/20 23:24:29 visual_prompt]: 	Setting up the optimizer...
[11/20 23:24:29 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 23:26:12 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9329 s / batch. (data: 1.68e-02). ETA=14:18:13, max mem: 28.4 GB 
[11/20 23:27:53 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9223 s / batch. (data: 7.48e-03). ETA=14:07:00, max mem: 28.4 GB 
[11/20 23:29:28 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9255 s / batch. (data: 2.57e-04). ETA=14:08:19, max mem: 28.4 GB 
[11/20 23:31:01 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9150 s / batch. (data: 2.64e-04). ETA=13:57:16, max mem: 28.4 GB 
[11/20 23:32:34 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9509 s / batch. (data: 6.85e-03). ETA=14:28:27, max mem: 28.4 GB 
[11/20 23:33:23 visual_prompt]: Epoch 1 / 100: avg data time: 3.89e-02, avg batch time: 0.9656, average train loss: 7.6130
[11/20 23:34:20 visual_prompt]: Inference (val):avg data time: 2.33e-04, avg batch time: 0.3061, average loss: 6.9126
[11/20 23:34:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/20 23:34:20 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/20 23:36:05 visual_prompt]: 	Training 100/553. train loss: 0.8563,	0.9066 s / batch. (data: 6.56e-04). ETA=13:45:43, max mem: 28.4 GB 
[11/20 23:37:38 visual_prompt]: 	Training 200/553. train loss: 1.5939,	0.9331 s / batch. (data: 5.06e-03). ETA=14:08:17, max mem: 28.4 GB 
[11/20 23:39:13 visual_prompt]: 	Training 300/553. train loss: 0.8433,	0.9311 s / batch. (data: 1.55e-02). ETA=14:04:57, max mem: 28.4 GB 
[11/20 23:40:49 visual_prompt]: 	Training 400/553. train loss: 0.6131,	0.9386 s / batch. (data: 7.03e-04). ETA=14:10:11, max mem: 28.4 GB 
[11/20 23:42:23 visual_prompt]: 	Training 500/553. train loss: 0.9009,	0.9270 s / batch. (data: 2.98e-04). ETA=13:58:06, max mem: 28.4 GB 
[11/20 23:43:12 visual_prompt]: Epoch 2 / 100: avg data time: 3.68e-02, avg batch time: 0.9614, average train loss: 0.9655
[11/20 23:44:09 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3034, average loss: 1.1157
[11/20 23:44:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.59	
[11/20 23:44:09 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/20 23:45:53 visual_prompt]: 	Training 100/553. train loss: 0.6887,	0.9440 s / batch. (data: 2.47e-04). ETA=14:11:05, max mem: 28.4 GB 
[11/20 23:47:28 visual_prompt]: 	Training 200/553. train loss: 2.5432,	0.9240 s / batch. (data: 3.97e-03). ETA=13:51:31, max mem: 28.4 GB 
[11/20 23:49:02 visual_prompt]: 	Training 300/553. train loss: 0.9082,	0.9242 s / batch. (data: 7.54e-04). ETA=13:50:07, max mem: 28.4 GB 
[11/20 23:50:35 visual_prompt]: 	Training 400/553. train loss: 0.7115,	0.9323 s / batch. (data: 2.50e-04). ETA=13:55:49, max mem: 28.4 GB 
[11/20 23:52:08 visual_prompt]: 	Training 500/553. train loss: 1.0530,	0.9283 s / batch. (data: 2.68e-04). ETA=13:50:44, max mem: 28.4 GB 
[11/20 23:52:57 visual_prompt]: Epoch 3 / 100: avg data time: 2.96e-02, avg batch time: 0.9539, average train loss: 0.8294
[11/20 23:53:54 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3050, average loss: 0.6984
[11/20 23:53:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 61.39	
[11/20 23:53:54 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/20 23:55:39 visual_prompt]: 	Training 100/553. train loss: 0.6826,	0.9171 s / batch. (data: 2.89e-04). ETA=13:38:23, max mem: 28.4 GB 
[11/20 23:57:12 visual_prompt]: 	Training 200/553. train loss: 0.9806,	0.9642 s / batch. (data: 2.81e-02). ETA=14:18:46, max mem: 28.4 GB 
[11/20 23:58:47 visual_prompt]: 	Training 300/553. train loss: 0.9623,	0.9158 s / batch. (data: 2.92e-04). ETA=13:34:11, max mem: 28.4 GB 
[11/21 00:00:20 visual_prompt]: 	Training 400/553. train loss: 0.4989,	0.9403 s / batch. (data: 5.35e-03). ETA=13:54:20, max mem: 28.4 GB 
[11/21 00:01:54 visual_prompt]: 	Training 500/553. train loss: 0.9613,	0.9458 s / batch. (data: 5.46e-03). ETA=13:57:38, max mem: 28.4 GB 
[11/21 00:02:44 visual_prompt]: Epoch 4 / 100: avg data time: 3.15e-02, avg batch time: 0.9580, average train loss: 0.8114
[11/21 00:03:40 visual_prompt]: Inference (val):avg data time: 2.83e-04, avg batch time: 0.3043, average loss: 0.6979
[11/21 00:03:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 62.47	
[11/21 00:03:40 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/21 00:05:23 visual_prompt]: 	Training 100/553. train loss: 0.6687,	0.9372 s / batch. (data: 5.37e-03). ETA=13:47:39, max mem: 28.4 GB 
[11/21 00:06:58 visual_prompt]: 	Training 200/553. train loss: 0.6417,	0.9078 s / batch. (data: 2.95e-04). ETA=13:20:10, max mem: 28.4 GB 
[11/21 00:08:31 visual_prompt]: 	Training 300/553. train loss: 0.7792,	0.9320 s / batch. (data: 2.67e-04). ETA=13:39:58, max mem: 28.4 GB 
[11/21 00:10:04 visual_prompt]: 	Training 400/553. train loss: 1.2178,	0.9261 s / batch. (data: 2.67e-04). ETA=13:33:12, max mem: 28.4 GB 
[11/21 00:11:40 visual_prompt]: 	Training 500/553. train loss: 0.7483,	0.9285 s / batch. (data: 1.48e-02). ETA=13:33:48, max mem: 28.4 GB 
[11/21 00:12:29 visual_prompt]: Epoch 5 / 100: avg data time: 3.03e-02, avg batch time: 0.9549, average train loss: 0.7717
[11/21 00:13:25 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3030, average loss: 0.8552
[11/21 00:13:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.97	
[11/21 00:13:25 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/21 00:15:10 visual_prompt]: 	Training 100/553. train loss: 0.7380,	0.9295 s / batch. (data: 5.38e-03). ETA=13:32:19, max mem: 28.4 GB 
[11/21 00:16:43 visual_prompt]: 	Training 200/553. train loss: 0.7103,	0.9480 s / batch. (data: 2.52e-04). ETA=13:46:53, max mem: 28.4 GB 
[11/21 00:18:18 visual_prompt]: 	Training 300/553. train loss: 1.1287,	0.9272 s / batch. (data: 5.38e-03). ETA=13:27:14, max mem: 28.4 GB 
[11/21 00:19:51 visual_prompt]: 	Training 400/553. train loss: 0.8618,	0.9440 s / batch. (data: 7.98e-03). ETA=13:40:14, max mem: 28.4 GB 
[11/21 00:21:28 visual_prompt]: 	Training 500/553. train loss: 1.0967,	0.9449 s / batch. (data: 1.54e-02). ETA=13:39:26, max mem: 28.4 GB 
[11/21 00:22:17 visual_prompt]: Epoch 6 / 100: avg data time: 3.75e-02, avg batch time: 0.9606, average train loss: 0.7425
[11/21 00:23:13 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3058, average loss: 0.6687
[11/21 00:23:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 62.88	
[11/21 00:23:13 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/21 00:25:02 visual_prompt]: 	Training 100/553. train loss: 0.6148,	0.9280 s / batch. (data: 3.05e-04). ETA=13:22:26, max mem: 28.4 GB 
[11/21 00:26:36 visual_prompt]: 	Training 200/553. train loss: 0.5016,	0.9337 s / batch. (data: 6.88e-04). ETA=13:25:47, max mem: 28.4 GB 
[11/21 00:28:09 visual_prompt]: 	Training 300/553. train loss: 0.6904,	0.9462 s / batch. (data: 7.29e-04). ETA=13:35:02, max mem: 28.4 GB 
[11/21 00:29:42 visual_prompt]: 	Training 400/553. train loss: 0.8839,	0.9052 s / batch. (data: 5.40e-03). ETA=12:58:13, max mem: 28.4 GB 
[11/21 00:31:16 visual_prompt]: 	Training 500/553. train loss: 0.5571,	0.9345 s / batch. (data: 1.04e-02). ETA=13:21:52, max mem: 28.4 GB 
[11/21 00:32:05 visual_prompt]: Epoch 7 / 100: avg data time: 3.62e-02, avg batch time: 0.9613, average train loss: 0.7479
[11/21 00:33:02 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3042, average loss: 0.6754
[11/21 00:33:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.28	
[11/21 00:33:02 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/21 00:34:47 visual_prompt]: 	Training 100/553. train loss: 0.5971,	0.9241 s / batch. (data: 7.99e-03). ETA=13:10:32, max mem: 28.4 GB 
[11/21 00:36:23 visual_prompt]: 	Training 200/553. train loss: 0.6487,	0.9082 s / batch. (data: 1.05e-02). ETA=12:55:24, max mem: 28.4 GB 
[11/21 00:37:56 visual_prompt]: 	Training 300/553. train loss: 0.8611,	0.9689 s / batch. (data: 1.69e-02). ETA=13:45:36, max mem: 28.4 GB 
[11/21 00:39:31 visual_prompt]: 	Training 400/553. train loss: 0.6217,	0.9204 s / batch. (data: 3.03e-04). ETA=13:02:48, max mem: 28.4 GB 
[11/21 00:41:07 visual_prompt]: 	Training 500/553. train loss: 0.6322,	0.9314 s / batch. (data: 7.78e-04). ETA=13:10:35, max mem: 28.4 GB 
[11/21 00:41:56 visual_prompt]: Epoch 8 / 100: avg data time: 4.25e-02, avg batch time: 0.9655, average train loss: 0.7494
[11/21 00:42:53 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3051, average loss: 0.6757
[11/21 00:42:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 63.06	
[11/21 00:42:53 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/21 00:44:43 visual_prompt]: 	Training 100/553. train loss: 0.8413,	0.9236 s / batch. (data: 5.41e-03). ETA=13:01:38, max mem: 28.4 GB 
[11/21 00:46:18 visual_prompt]: 	Training 200/553. train loss: 1.2155,	0.9160 s / batch. (data: 3.98e-03). ETA=12:53:38, max mem: 28.4 GB 
[11/21 00:47:51 visual_prompt]: 	Training 300/553. train loss: 0.6143,	0.9364 s / batch. (data: 2.74e-04). ETA=13:09:21, max mem: 28.4 GB 
[11/21 00:49:24 visual_prompt]: 	Training 400/553. train loss: 0.7373,	0.9360 s / batch. (data: 6.83e-04). ETA=13:07:23, max mem: 28.4 GB 
[11/21 00:50:56 visual_prompt]: 	Training 500/553. train loss: 0.7948,	0.9222 s / batch. (data: 2.92e-04). ETA=12:54:18, max mem: 28.4 GB 
[11/21 00:51:46 visual_prompt]: Epoch 9 / 100: avg data time: 3.79e-02, avg batch time: 0.9626, average train loss: 0.7589
[11/21 00:52:42 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3055, average loss: 0.7304
[11/21 00:52:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 64.07	
[11/21 00:52:42 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/21 00:54:27 visual_prompt]: 	Training 100/553. train loss: 0.6751,	0.9317 s / batch. (data: 7.94e-03). ETA=12:59:50, max mem: 28.4 GB 
[11/21 00:56:04 visual_prompt]: 	Training 200/553. train loss: 0.8877,	0.9233 s / batch. (data: 2.63e-04). ETA=12:51:20, max mem: 28.4 GB 
[11/21 00:57:37 visual_prompt]: 	Training 300/553. train loss: 0.7356,	0.9700 s / batch. (data: 5.88e-03). ETA=13:28:40, max mem: 28.4 GB 
[11/21 00:59:10 visual_prompt]: 	Training 400/553. train loss: 0.6028,	0.9256 s / batch. (data: 2.80e-04). ETA=12:50:08, max mem: 28.4 GB 
[11/21 01:00:44 visual_prompt]: 	Training 500/553. train loss: 0.6422,	0.9110 s / batch. (data: 1.05e-02). ETA=12:36:26, max mem: 28.4 GB 
[11/21 01:01:35 visual_prompt]: Epoch 10 / 100: avg data time: 3.84e-02, avg batch time: 0.9637, average train loss: 0.7376
[11/21 01:02:32 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3035, average loss: 0.6937
[11/21 01:02:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.77	
[11/21 01:02:32 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/21 01:04:19 visual_prompt]: 	Training 100/553. train loss: 0.8813,	0.9111 s / batch. (data: 7.71e-03). ETA=12:34:14, max mem: 28.4 GB 
[11/21 01:05:54 visual_prompt]: 	Training 200/553. train loss: 0.7343,	0.9249 s / batch. (data: 2.30e-04). ETA=12:44:09, max mem: 28.4 GB 
[11/21 01:07:26 visual_prompt]: 	Training 300/553. train loss: 0.5339,	0.9484 s / batch. (data: 7.32e-04). ETA=13:01:59, max mem: 28.4 GB 
[11/21 01:09:00 visual_prompt]: 	Training 400/553. train loss: 0.9153,	0.9034 s / batch. (data: 2.84e-04). ETA=12:23:21, max mem: 28.4 GB 
[11/21 01:10:36 visual_prompt]: 	Training 500/553. train loss: 0.5443,	0.9223 s / batch. (data: 1.44e-02). ETA=12:37:21, max mem: 28.4 GB 
[11/21 01:11:25 visual_prompt]: Epoch 11 / 100: avg data time: 4.00e-02, avg batch time: 0.9637, average train loss: 0.7396
[11/21 01:12:22 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3047, average loss: 0.6747
[11/21 01:12:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 59.49	
[11/21 01:12:22 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/21 01:14:04 visual_prompt]: 	Training 100/553. train loss: 0.5661,	0.9063 s / batch. (data: 7.97e-03). ETA=12:21:52, max mem: 28.4 GB 
[11/21 01:15:43 visual_prompt]: 	Training 200/553. train loss: 0.8039,	0.9153 s / batch. (data: 5.48e-03). ETA=12:27:45, max mem: 28.4 GB 
[11/21 01:17:17 visual_prompt]: 	Training 300/553. train loss: 0.8192,	0.9074 s / batch. (data: 3.23e-04). ETA=12:19:49, max mem: 28.4 GB 
[11/21 01:18:51 visual_prompt]: 	Training 400/553. train loss: 1.0837,	0.9446 s / batch. (data: 5.90e-03). ETA=12:48:31, max mem: 28.4 GB 
[11/21 01:20:26 visual_prompt]: 	Training 500/553. train loss: 0.6743,	0.9393 s / batch. (data: 1.15e-03). ETA=12:42:38, max mem: 28.4 GB 
[11/21 01:21:17 visual_prompt]: Epoch 12 / 100: avg data time: 4.44e-02, avg batch time: 0.9664, average train loss: 0.7638
[11/21 01:22:14 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3043, average loss: 1.0203
[11/21 01:22:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.76	
[11/21 01:22:14 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/21 01:24:01 visual_prompt]: 	Training 100/553. train loss: 0.5964,	0.9137 s / batch. (data: 9.46e-03). ETA=12:19:32, max mem: 28.4 GB 
[11/21 01:25:33 visual_prompt]: 	Training 200/553. train loss: 0.8763,	0.9282 s / batch. (data: 4.26e-04). ETA=12:29:43, max mem: 28.4 GB 
[11/21 01:27:09 visual_prompt]: 	Training 300/553. train loss: 0.6980,	0.9240 s / batch. (data: 2.52e-04). ETA=12:24:47, max mem: 28.4 GB 
[11/21 01:28:46 visual_prompt]: 	Training 400/553. train loss: 0.6789,	0.9105 s / batch. (data: 7.94e-03). ETA=12:12:22, max mem: 28.4 GB 
[11/21 01:30:19 visual_prompt]: 	Training 500/553. train loss: 0.7394,	0.9280 s / batch. (data: 2.48e-04). ETA=12:24:55, max mem: 28.4 GB 
[11/21 01:31:08 visual_prompt]: Epoch 13 / 100: avg data time: 4.24e-02, avg batch time: 0.9646, average train loss: 0.7340
[11/21 01:32:05 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3028, average loss: 0.7203
[11/21 01:32:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 60.21	
[11/21 01:32:05 visual_prompt]: Stopping early.
[11/21 01:32:05 visual_prompt]: Rank of current process: 0. World size: 1
[11/21 01:32:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 01:32:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/21 01:32:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/21 01:32:05 visual_prompt]: Training with config:
[11/21 01:32:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0005_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/21 01:32:05 visual_prompt]: Loading training data...
[11/21 01:32:05 visual_prompt]: Constructing mammo-cbis dataset train...
[11/21 01:32:05 visual_prompt]: Loading validation data...
[11/21 01:32:05 visual_prompt]: Constructing mammo-cbis dataset val...
[11/21 01:32:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/21 01:32:07 visual_prompt]: Enable all parameters update during training
[11/21 01:32:07 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/21 01:32:07 visual_prompt]: tuned percent:100.000
[11/21 01:32:07 visual_prompt]: Device used for model: 0
[11/21 01:32:07 visual_prompt]: Setting up Evaluator...
[11/21 01:32:07 visual_prompt]: Setting up Trainer...
[11/21 01:32:07 visual_prompt]: 	Setting up the optimizer...
[11/21 01:32:07 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/21 01:33:50 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9283 s / batch. (data: 7.13e-03). ETA=14:14:02, max mem: 29.7 GB 
[11/21 01:35:27 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9343 s / batch. (data: 6.25e-03). ETA=14:18:02, max mem: 29.7 GB 
[11/21 01:37:02 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9361 s / batch. (data: 2.48e-04). ETA=14:18:07, max mem: 29.7 GB 
[11/21 01:38:35 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9320 s / batch. (data: 2.74e-04). ETA=14:12:47, max mem: 29.7 GB 
[11/21 01:40:09 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9351 s / batch. (data: 5.39e-03). ETA=14:14:01, max mem: 29.7 GB 
[11/21 01:40:59 visual_prompt]: Epoch 1 / 100: avg data time: 3.63e-02, avg batch time: 0.9604, average train loss: 7.6130
[11/21 01:41:55 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3042, average loss: 6.9126
[11/21 01:41:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/21 01:41:55 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/21 01:43:40 visual_prompt]: 	Training 100/553. train loss: 0.8510,	0.9333 s / batch. (data: 5.83e-03). ETA=14:10:00, max mem: 29.7 GB 
[11/21 01:45:13 visual_prompt]: 	Training 200/553. train loss: 1.5941,	0.9080 s / batch. (data: 2.83e-04). ETA=13:45:29, max mem: 29.7 GB 
[11/21 01:46:49 visual_prompt]: 	Training 300/553. train loss: 0.8433,	0.9033 s / batch. (data: 3.18e-04). ETA=13:39:44, max mem: 29.7 GB 
[11/21 01:48:26 visual_prompt]: 	Training 400/553. train loss: 0.6307,	0.9285 s / batch. (data: 2.52e-04). ETA=14:01:01, max mem: 29.7 GB 
[11/21 01:50:01 visual_prompt]: 	Training 500/553. train loss: 0.9170,	0.9186 s / batch. (data: 5.40e-03). ETA=13:50:32, max mem: 29.7 GB 
[11/21 01:50:50 visual_prompt]: Epoch 2 / 100: avg data time: 4.09e-02, avg batch time: 0.9662, average train loss: 0.9659
[11/21 01:51:47 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3035, average loss: 1.1181
[11/21 01:51:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.22	
[11/21 01:51:47 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/21 01:53:31 visual_prompt]: 	Training 100/553. train loss: 0.6658,	0.9219 s / batch. (data: 1.10e-02). ETA=13:51:08, max mem: 29.7 GB 
[11/21 01:55:07 visual_prompt]: 	Training 200/553. train loss: 2.5454,	0.9261 s / batch. (data: 5.35e-03). ETA=13:53:25, max mem: 29.7 GB 
[11/21 01:56:41 visual_prompt]: 	Training 300/553. train loss: 0.9503,	0.9240 s / batch. (data: 7.59e-04). ETA=13:49:56, max mem: 29.7 GB 
[11/21 01:58:15 visual_prompt]: 	Training 400/553. train loss: 0.7049,	0.9025 s / batch. (data: 3.05e-04). ETA=13:29:10, max mem: 29.7 GB 
[11/21 01:59:49 visual_prompt]: 	Training 500/553. train loss: 1.0208,	0.9606 s / batch. (data: 7.35e-04). ETA=14:19:36, max mem: 29.7 GB 
[11/21 02:00:38 visual_prompt]: Epoch 3 / 100: avg data time: 3.56e-02, avg batch time: 0.9603, average train loss: 0.8327
[11/21 02:01:35 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3034, average loss: 0.6887
[11/21 02:01:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 61.60	
[11/21 02:01:35 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/21 02:03:21 visual_prompt]: 	Training 100/553. train loss: 0.6701,	0.9190 s / batch. (data: 5.37e-03). ETA=13:40:05, max mem: 29.7 GB 
[11/21 02:04:54 visual_prompt]: 	Training 200/553. train loss: 0.9847,	0.9363 s / batch. (data: 3.50e-04). ETA=13:53:57, max mem: 29.7 GB 
[11/21 02:06:30 visual_prompt]: 	Training 300/553. train loss: 0.9503,	0.9178 s / batch. (data: 3.19e-04). ETA=13:35:55, max mem: 29.7 GB 
[11/21 02:08:04 visual_prompt]: 	Training 400/553. train loss: 0.5027,	0.9391 s / batch. (data: 5.37e-03). ETA=13:53:19, max mem: 29.7 GB 
[11/21 02:09:36 visual_prompt]: 	Training 500/553. train loss: 0.9748,	0.9198 s / batch. (data: 7.15e-04). ETA=13:34:38, max mem: 29.7 GB 
[11/21 02:10:25 visual_prompt]: Epoch 4 / 100: avg data time: 3.42e-02, avg batch time: 0.9585, average train loss: 0.8099
[11/21 02:11:19 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.3051, average loss: 0.6996
[11/21 02:11:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 62.13	
[11/21 02:11:19 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/21 02:12:58 visual_prompt]: 	Training 100/553. train loss: 0.6577,	0.9068 s / batch. (data: 5.35e-03). ETA=13:20:48, max mem: 29.7 GB 
[11/21 02:14:31 visual_prompt]: 	Training 200/553. train loss: 0.6375,	0.9221 s / batch. (data: 2.81e-04). ETA=13:32:49, max mem: 29.7 GB 
[11/21 02:16:04 visual_prompt]: 	Training 300/553. train loss: 0.7690,	0.9217 s / batch. (data: 5.80e-03). ETA=13:30:55, max mem: 29.7 GB 
[11/21 02:17:36 visual_prompt]: 	Training 400/553. train loss: 1.1924,	0.9372 s / batch. (data: 2.79e-04). ETA=13:43:01, max mem: 29.7 GB 
[11/21 02:19:09 visual_prompt]: 	Training 500/553. train loss: 0.7501,	0.9402 s / batch. (data: 7.20e-04). ETA=13:44:03, max mem: 29.7 GB 
[11/21 02:19:58 visual_prompt]: Epoch 5 / 100: avg data time: 1.71e-02, avg batch time: 0.9371, average train loss: 0.7720
[11/21 02:20:51 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3043, average loss: 0.8441
[11/21 02:20:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 63.87	
[11/21 02:20:51 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/21 02:22:33 visual_prompt]: 	Training 100/553. train loss: 0.7311,	0.9391 s / batch. (data: 7.03e-04). ETA=13:40:41, max mem: 29.7 GB 
[11/21 02:24:05 visual_prompt]: 	Training 200/553. train loss: 0.7062,	0.9322 s / batch. (data: 7.08e-04). ETA=13:33:04, max mem: 29.7 GB 
[11/21 02:25:38 visual_prompt]: 	Training 300/553. train loss: 1.1631,	0.9159 s / batch. (data: 2.78e-04). ETA=13:17:20, max mem: 29.7 GB 
[11/21 02:27:11 visual_prompt]: 	Training 400/553. train loss: 0.9205,	0.9218 s / batch. (data: 1.38e-02). ETA=13:21:00, max mem: 29.7 GB 
[11/21 02:28:43 visual_prompt]: 	Training 500/553. train loss: 0.9625,	0.8959 s / batch. (data: 2.63e-04). ETA=12:56:57, max mem: 29.7 GB 
[11/21 02:29:32 visual_prompt]: Epoch 6 / 100: avg data time: 2.08e-02, avg batch time: 0.9414, average train loss: 0.7439
[11/21 02:30:27 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3069, average loss: 0.6690
[11/21 02:30:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.36	
[11/21 02:30:27 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/21 02:32:11 visual_prompt]: 	Training 100/553. train loss: 0.6539,	0.9133 s / batch. (data: 5.92e-03). ETA=13:09:41, max mem: 29.7 GB 
[11/21 02:33:45 visual_prompt]: 	Training 200/553. train loss: 0.5023,	0.9387 s / batch. (data: 6.76e-04). ETA=13:30:07, max mem: 29.7 GB 
[11/21 02:35:17 visual_prompt]: 	Training 300/553. train loss: 0.6837,	0.9073 s / batch. (data: 7.44e-04). ETA=13:01:29, max mem: 29.7 GB 
[11/21 02:36:50 visual_prompt]: 	Training 400/553. train loss: 0.7548,	0.9017 s / batch. (data: 5.38e-03). ETA=12:55:12, max mem: 29.7 GB 
[11/21 02:38:23 visual_prompt]: 	Training 500/553. train loss: 0.6352,	0.9127 s / batch. (data: 2.45e-04). ETA=13:03:09, max mem: 29.7 GB 
[11/21 02:39:11 visual_prompt]: Epoch 7 / 100: avg data time: 2.90e-02, avg batch time: 0.9487, average train loss: 0.7423
[11/21 02:40:06 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3040, average loss: 0.6692
[11/21 02:40:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 64.23	
[11/21 02:40:06 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/21 02:41:49 visual_prompt]: 	Training 100/553. train loss: 0.6155,	0.9200 s / batch. (data: 2.59e-04). ETA=13:07:01, max mem: 29.7 GB 
[11/21 02:43:21 visual_prompt]: 	Training 200/553. train loss: 0.6317,	0.9313 s / batch. (data: 7.01e-04). ETA=13:15:07, max mem: 29.7 GB 
[11/21 02:44:54 visual_prompt]: 	Training 300/553. train loss: 0.8109,	0.9207 s / batch. (data: 5.79e-03). ETA=13:04:35, max mem: 29.7 GB 
[11/21 02:46:26 visual_prompt]: 	Training 400/553. train loss: 0.5582,	0.9359 s / batch. (data: 5.82e-03). ETA=13:15:56, max mem: 29.7 GB 
[11/21 02:47:59 visual_prompt]: 	Training 500/553. train loss: 0.6192,	0.9432 s / batch. (data: 2.21e-04). ETA=13:20:36, max mem: 29.7 GB 
[11/21 02:48:48 visual_prompt]: Epoch 8 / 100: avg data time: 2.40e-02, avg batch time: 0.9431, average train loss: 0.7478
[11/21 02:49:42 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3061, average loss: 0.6558
[11/21 02:49:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 66.03	
[11/21 02:49:42 visual_prompt]: Best epoch 8: best metric: -0.656
[11/21 02:49:42 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/21 02:51:27 visual_prompt]: 	Training 100/553. train loss: 0.9301,	0.9260 s / batch. (data: 2.98e-03). ETA=13:03:37, max mem: 29.7 GB 
[11/21 02:52:59 visual_prompt]: 	Training 200/553. train loss: 1.2789,	0.9281 s / batch. (data: 5.79e-03). ETA=13:03:50, max mem: 29.7 GB 
[11/21 02:54:31 visual_prompt]: 	Training 300/553. train loss: 0.5766,	0.9209 s / batch. (data: 5.36e-03). ETA=12:56:13, max mem: 29.7 GB 
[11/21 02:56:04 visual_prompt]: 	Training 400/553. train loss: 0.7481,	0.9236 s / batch. (data: 1.04e-02). ETA=12:56:58, max mem: 29.7 GB 
[11/21 02:57:36 visual_prompt]: 	Training 500/553. train loss: 0.6756,	0.9385 s / batch. (data: 7.33e-04). ETA=13:07:59, max mem: 29.7 GB 
[11/21 02:58:25 visual_prompt]: Epoch 9 / 100: avg data time: 2.61e-02, avg batch time: 0.9449, average train loss: 0.7543
[11/21 02:59:20 visual_prompt]: Inference (val):avg data time: 5.07e-05, avg batch time: 0.3122, average loss: 0.6850
[11/21 02:59:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 63.69	
[11/21 02:59:20 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/21 03:01:00 visual_prompt]: 	Training 100/553. train loss: 0.7108,	0.9138 s / batch. (data: 2.54e-04). ETA=12:44:52, max mem: 29.7 GB 
[11/21 03:02:33 visual_prompt]: 	Training 200/553. train loss: 0.6862,	0.9331 s / batch. (data: 2.51e-04). ETA=12:59:29, max mem: 29.7 GB 
[11/21 03:04:06 visual_prompt]: 	Training 300/553. train loss: 0.8202,	0.9228 s / batch. (data: 7.41e-04). ETA=12:49:20, max mem: 29.7 GB 
[11/21 03:05:38 visual_prompt]: 	Training 400/553. train loss: 0.7072,	0.9103 s / batch. (data: 7.00e-04). ETA=12:37:22, max mem: 29.7 GB 
[11/21 03:07:11 visual_prompt]: 	Training 500/553. train loss: 0.6887,	0.8991 s / batch. (data: 2.88e-04). ETA=12:26:36, max mem: 29.7 GB 
[11/21 03:07:59 visual_prompt]: Epoch 10 / 100: avg data time: 1.98e-02, avg batch time: 0.9395, average train loss: 0.7375
[11/21 03:08:54 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.3038, average loss: 0.7566
[11/21 03:08:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 62.07	
[11/21 03:08:54 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/21 03:10:37 visual_prompt]: 	Training 100/553. train loss: 1.2507,	0.9151 s / batch. (data: 1.15e-02). ETA=12:37:30, max mem: 29.7 GB 
[11/21 03:12:10 visual_prompt]: 	Training 200/553. train loss: 0.8465,	0.9096 s / batch. (data: 2.34e-04). ETA=12:31:27, max mem: 29.7 GB 
[11/21 03:13:42 visual_prompt]: 	Training 300/553. train loss: 0.4550,	0.9420 s / batch. (data: 1.09e-02). ETA=12:56:42, max mem: 29.7 GB 
[11/21 03:15:15 visual_prompt]: 	Training 400/553. train loss: 0.8194,	0.9489 s / batch. (data: 1.77e-02). ETA=13:00:45, max mem: 29.7 GB 
[11/21 03:16:47 visual_prompt]: 	Training 500/553. train loss: 0.5031,	0.9310 s / batch. (data: 3.10e-02). ETA=12:44:28, max mem: 29.7 GB 
[11/21 03:17:36 visual_prompt]: Epoch 11 / 100: avg data time: 2.52e-02, avg batch time: 0.9444, average train loss: 0.7262
[11/21 03:18:31 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3047, average loss: 0.6743
[11/21 03:18:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 59.53	
[11/21 03:18:31 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/21 03:20:09 visual_prompt]: 	Training 100/553. train loss: 0.4967,	0.9533 s / batch. (data: 5.76e-03). ETA=13:00:21, max mem: 29.7 GB 
[11/21 03:21:42 visual_prompt]: 	Training 200/553. train loss: 0.7513,	0.9234 s / batch. (data: 7.99e-03). ETA=12:34:23, max mem: 29.7 GB 
[11/21 03:23:16 visual_prompt]: 	Training 300/553. train loss: 0.7705,	0.9408 s / batch. (data: 7.07e-04). ETA=12:47:03, max mem: 29.7 GB 
[11/21 03:24:48 visual_prompt]: 	Training 400/553. train loss: 0.8901,	0.9302 s / batch. (data: 1.04e-02). ETA=12:36:50, max mem: 29.7 GB 
[11/21 03:26:20 visual_prompt]: 	Training 500/553. train loss: 0.6767,	0.9172 s / batch. (data: 8.01e-03). ETA=12:24:42, max mem: 29.7 GB 
[11/21 03:27:09 visual_prompt]: Epoch 12 / 100: avg data time: 1.76e-02, avg batch time: 0.9374, average train loss: 0.7510
[11/21 03:28:04 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3031, average loss: 1.0027
[11/21 03:28:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.75	
[11/21 03:28:04 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/21 03:29:46 visual_prompt]: 	Training 100/553. train loss: 0.5906,	0.9463 s / batch. (data: 7.22e-04). ETA=12:45:57, max mem: 29.7 GB 
[11/21 03:31:19 visual_prompt]: 	Training 200/553. train loss: 0.9190,	0.9186 s / batch. (data: 2.54e-04). ETA=12:21:56, max mem: 29.7 GB 
[11/21 03:32:52 visual_prompt]: 	Training 300/553. train loss: 0.6852,	0.9524 s / batch. (data: 5.88e-03). ETA=12:47:40, max mem: 29.7 GB 
[11/21 03:34:25 visual_prompt]: 	Training 400/553. train loss: 0.6976,	0.9631 s / batch. (data: 1.09e-02). ETA=12:54:42, max mem: 29.7 GB 
[11/21 03:35:58 visual_prompt]: 	Training 500/553. train loss: 0.7113,	0.9218 s / batch. (data: 7.67e-04). ETA=12:19:57, max mem: 29.7 GB 
[11/21 03:36:46 visual_prompt]: Epoch 13 / 100: avg data time: 2.20e-02, avg batch time: 0.9449, average train loss: 0.7335
[11/21 03:37:41 visual_prompt]: Inference (val):avg data time: 4.80e-05, avg batch time: 0.3112, average loss: 0.7197
[11/21 03:37:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.65	
[11/21 03:37:41 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/21 03:39:24 visual_prompt]: 	Training 100/553. train loss: 0.6589,	0.9143 s / batch. (data: 2.45e-04). ETA=12:11:35, max mem: 29.7 GB 
[11/21 03:40:57 visual_prompt]: 	Training 200/553. train loss: 0.6126,	0.9191 s / batch. (data: 5.83e-03). ETA=12:13:54, max mem: 29.7 GB 
[11/21 03:42:30 visual_prompt]: 	Training 300/553. train loss: 0.6713,	0.9256 s / batch. (data: 5.84e-03). ETA=12:17:35, max mem: 29.7 GB 
[11/21 03:44:03 visual_prompt]: 	Training 400/553. train loss: 1.4160,	0.9704 s / batch. (data: 6.77e-04). ETA=12:51:39, max mem: 29.7 GB 
[11/21 03:45:35 visual_prompt]: 	Training 500/553. train loss: 0.6440,	0.9213 s / batch. (data: 6.82e-04). ETA=12:11:05, max mem: 29.7 GB 
[11/21 03:46:24 visual_prompt]: Epoch 14 / 100: avg data time: 2.38e-02, avg batch time: 0.9447, average train loss: 0.7484
[11/21 03:47:18 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3027, average loss: 0.7174
[11/21 03:47:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 55.51	
[11/21 03:47:18 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/21 03:49:03 visual_prompt]: 	Training 100/553. train loss: 0.6726,	0.9300 s / batch. (data: 3.16e-04). ETA=12:15:36, max mem: 29.7 GB 
[11/21 03:50:35 visual_prompt]: 	Training 200/553. train loss: 0.6739,	0.9160 s / batch. (data: 7.38e-04). ETA=12:03:00, max mem: 29.7 GB 
[11/21 03:52:08 visual_prompt]: 	Training 300/553. train loss: 0.9486,	0.9044 s / batch. (data: 2.78e-04). ETA=11:52:21, max mem: 29.7 GB 
[11/21 03:53:41 visual_prompt]: 	Training 400/553. train loss: 0.6023,	0.9179 s / batch. (data: 8.10e-03). ETA=12:01:24, max mem: 29.7 GB 
[11/21 03:55:13 visual_prompt]: 	Training 500/553. train loss: 0.8647,	0.8954 s / batch. (data: 2.77e-04). ETA=11:42:17, max mem: 29.7 GB 
[11/21 03:56:02 visual_prompt]: Epoch 15 / 100: avg data time: 2.62e-02, avg batch time: 0.9466, average train loss: 0.7323
[11/21 03:56:56 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3020, average loss: 0.7365
[11/21 03:56:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.60	
[11/21 03:56:56 visual_prompt]: Stopping early.
[11/21 03:56:56 visual_prompt]: Rank of current process: 0. World size: 1
[11/21 03:56:56 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 03:56:56 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/21 03:56:56 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/21 03:56:56 visual_prompt]: Training with config:
[11/21 03:56:56 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0005_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/21 03:56:56 visual_prompt]: Loading training data...
[11/21 03:56:56 visual_prompt]: Constructing mammo-cbis dataset train...
[11/21 03:56:56 visual_prompt]: Loading validation data...
[11/21 03:56:56 visual_prompt]: Constructing mammo-cbis dataset val...
[11/21 03:56:56 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/21 03:56:58 visual_prompt]: Enable all parameters update during training
[11/21 03:56:58 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/21 03:56:58 visual_prompt]: tuned percent:100.000
[11/21 03:56:58 visual_prompt]: Device used for model: 0
[11/21 03:56:58 visual_prompt]: Setting up Evaluator...
[11/21 03:56:58 visual_prompt]: Setting up Trainer...
[11/21 03:56:58 visual_prompt]: 	Setting up the optimizer...
[11/21 03:56:58 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/21 03:58:38 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9214 s / batch. (data: 2.77e-04). ETA=14:07:43, max mem: 31.0 GB 
[11/21 04:00:11 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9160 s / batch. (data: 2.59e-04). ETA=14:01:10, max mem: 31.0 GB 
[11/21 04:01:44 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9109 s / batch. (data: 2.65e-04). ETA=13:55:00, max mem: 31.0 GB 
[11/21 04:03:16 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9306 s / batch. (data: 6.69e-04). ETA=14:11:28, max mem: 31.0 GB 
[11/21 04:04:49 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9261 s / batch. (data: 1.05e-02). ETA=14:05:49, max mem: 31.0 GB 
[11/21 04:05:38 visual_prompt]: Epoch 1 / 100: avg data time: 1.81e-02, avg batch time: 0.9398, average train loss: 7.6130
[11/21 04:06:33 visual_prompt]: Inference (val):avg data time: 1.05e-04, avg batch time: 0.3042, average loss: 6.9126
[11/21 04:06:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/21 04:06:33 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/21 04:08:13 visual_prompt]: 	Training 100/553. train loss: 0.8582,	0.9366 s / batch. (data: 5.34e-03). ETA=14:13:00, max mem: 31.0 GB 
[11/21 04:09:46 visual_prompt]: 	Training 200/553. train loss: 1.5948,	0.9385 s / batch. (data: 1.14e-02). ETA=14:13:11, max mem: 31.0 GB 
[11/21 04:11:19 visual_prompt]: 	Training 300/553. train loss: 0.8386,	0.9301 s / batch. (data: 5.37e-03). ETA=14:04:00, max mem: 31.0 GB 
[11/21 04:12:52 visual_prompt]: 	Training 400/553. train loss: 0.6139,	0.9378 s / batch. (data: 1.06e-03). ETA=14:09:28, max mem: 31.0 GB 
[11/21 04:14:24 visual_prompt]: 	Training 500/553. train loss: 0.9054,	0.9070 s / batch. (data: 3.06e-04). ETA=13:40:03, max mem: 31.0 GB 
[11/21 04:15:13 visual_prompt]: Epoch 2 / 100: avg data time: 1.99e-02, avg batch time: 0.9400, average train loss: 0.9655
[11/21 04:16:07 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3018, average loss: 1.1188
[11/21 04:16:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.39	
[11/21 04:16:07 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/21 04:17:48 visual_prompt]: 	Training 100/553. train loss: 0.6744,	0.9088 s / batch. (data: 7.21e-04). ETA=13:39:23, max mem: 31.0 GB 
[11/21 04:19:21 visual_prompt]: 	Training 200/553. train loss: 2.5273,	0.9414 s / batch. (data: 9.47e-03). ETA=14:07:10, max mem: 31.0 GB 
[11/21 04:20:54 visual_prompt]: 	Training 300/553. train loss: 0.8926,	0.9383 s / batch. (data: 7.05e-04). ETA=14:02:48, max mem: 31.0 GB 
[11/21 04:22:26 visual_prompt]: 	Training 400/553. train loss: 0.7175,	0.9242 s / batch. (data: 1.10e-02). ETA=13:48:38, max mem: 31.0 GB 
[11/21 04:23:59 visual_prompt]: 	Training 500/553. train loss: 1.0449,	0.9264 s / batch. (data: 7.31e-04). ETA=13:49:00, max mem: 31.0 GB 
[11/21 04:24:47 visual_prompt]: Epoch 3 / 100: avg data time: 2.11e-02, avg batch time: 0.9408, average train loss: 0.8306
[11/21 04:25:42 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3015, average loss: 0.6930
[11/21 04:25:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 61.78	
[11/21 04:25:42 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/21 04:27:26 visual_prompt]: 	Training 100/553. train loss: 0.6771,	0.9097 s / batch. (data: 8.39e-04). ETA=13:31:46, max mem: 31.0 GB 
[11/21 04:28:59 visual_prompt]: 	Training 200/553. train loss: 0.9825,	0.9123 s / batch. (data: 5.36e-03). ETA=13:32:35, max mem: 31.0 GB 
[11/21 04:30:31 visual_prompt]: 	Training 300/553. train loss: 0.9639,	0.9445 s / batch. (data: 6.81e-04). ETA=13:59:39, max mem: 31.0 GB 
[11/21 04:32:04 visual_prompt]: 	Training 400/553. train loss: 0.5019,	0.9353 s / batch. (data: 2.05e-02). ETA=13:49:57, max mem: 31.0 GB 
[11/21 04:33:37 visual_prompt]: 	Training 500/553. train loss: 0.9707,	0.9560 s / batch. (data: 6.87e-04). ETA=14:06:43, max mem: 31.0 GB 
[11/21 04:34:26 visual_prompt]: Epoch 4 / 100: avg data time: 2.63e-02, avg batch time: 0.9478, average train loss: 0.8109
[11/21 04:35:21 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.3049, average loss: 0.6996
[11/21 04:35:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 62.26	
[11/21 04:35:21 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/21 04:37:00 visual_prompt]: 	Training 100/553. train loss: 0.6675,	0.9215 s / batch. (data: 2.56e-04). ETA=13:33:46, max mem: 31.0 GB 
[11/21 04:38:33 visual_prompt]: 	Training 200/553. train loss: 0.6466,	0.9083 s / batch. (data: 5.36e-03). ETA=13:20:39, max mem: 31.0 GB 
[11/21 04:40:05 visual_prompt]: 	Training 300/553. train loss: 0.7667,	0.9344 s / batch. (data: 2.10e-02). ETA=13:42:04, max mem: 31.0 GB 
[11/21 04:41:38 visual_prompt]: 	Training 400/553. train loss: 1.2091,	0.9399 s / batch. (data: 1.04e-02). ETA=13:45:19, max mem: 31.0 GB 
[11/21 04:43:11 visual_prompt]: 	Training 500/553. train loss: 0.7421,	0.9250 s / batch. (data: 7.04e-04). ETA=13:30:46, max mem: 31.0 GB 
[11/21 04:44:00 visual_prompt]: Epoch 5 / 100: avg data time: 1.75e-02, avg batch time: 0.9381, average train loss: 0.7724
[11/21 04:44:54 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3055, average loss: 0.8436
[11/21 04:44:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 64.08	
[11/21 04:44:54 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/21 04:46:36 visual_prompt]: 	Training 100/553. train loss: 0.7277,	0.9234 s / batch. (data: 7.25e-04). ETA=13:27:00, max mem: 31.0 GB 
[11/21 04:48:08 visual_prompt]: 	Training 200/553. train loss: 0.7078,	0.9014 s / batch. (data: 5.42e-03). ETA=13:06:14, max mem: 31.0 GB 
[11/21 04:49:41 visual_prompt]: 	Training 300/553. train loss: 1.1804,	0.9312 s / batch. (data: 8.06e-03). ETA=13:30:40, max mem: 31.0 GB 
[11/21 04:51:13 visual_prompt]: 	Training 400/553. train loss: 0.9909,	0.9199 s / batch. (data: 2.72e-04). ETA=13:19:18, max mem: 31.0 GB 
[11/21 04:52:46 visual_prompt]: 	Training 500/553. train loss: 1.0193,	0.9334 s / batch. (data: 2.59e-02). ETA=13:29:27, max mem: 31.0 GB 
[11/21 04:53:35 visual_prompt]: Epoch 6 / 100: avg data time: 2.16e-02, avg batch time: 0.9414, average train loss: 0.7430
[11/21 04:54:29 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3055, average loss: 0.6717
[11/21 04:54:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 63.00	
[11/21 04:54:29 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/21 04:56:14 visual_prompt]: 	Training 100/553. train loss: 0.6724,	0.9183 s / batch. (data: 5.35e-03). ETA=13:14:03, max mem: 31.0 GB 
[11/21 04:57:47 visual_prompt]: 	Training 200/553. train loss: 0.6431,	0.9341 s / batch. (data: 5.79e-03). ETA=13:26:12, max mem: 31.0 GB 
[11/21 04:59:20 visual_prompt]: 	Training 300/553. train loss: 0.6541,	0.9071 s / batch. (data: 5.37e-03). ETA=13:01:21, max mem: 31.0 GB 
[11/21 05:00:52 visual_prompt]: 	Training 400/553. train loss: 0.7325,	0.9289 s / batch. (data: 5.81e-03). ETA=13:18:32, max mem: 31.0 GB 
[11/21 05:02:25 visual_prompt]: 	Training 500/553. train loss: 0.6104,	0.9175 s / batch. (data: 2.59e-04). ETA=13:07:13, max mem: 31.0 GB 
[11/21 05:03:13 visual_prompt]: Epoch 7 / 100: avg data time: 2.81e-02, avg batch time: 0.9470, average train loss: 0.7464
[11/21 05:04:07 visual_prompt]: Inference (val):avg data time: 1.76e-04, avg batch time: 0.3055, average loss: 0.6740
[11/21 05:04:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 63.51	
[11/21 05:04:07 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/21 05:05:49 visual_prompt]: 	Training 100/553. train loss: 0.6164,	0.9320 s / batch. (data: 2.54e-04). ETA=13:17:17, max mem: 31.0 GB 
[11/21 05:07:22 visual_prompt]: 	Training 200/553. train loss: 0.6270,	0.9497 s / batch. (data: 1.50e-02). ETA=13:30:50, max mem: 31.0 GB 
[11/21 05:08:54 visual_prompt]: 	Training 300/553. train loss: 0.7767,	0.9067 s / batch. (data: 2.80e-04). ETA=12:52:37, max mem: 31.0 GB 
[11/21 05:10:27 visual_prompt]: 	Training 400/553. train loss: 0.5829,	0.9514 s / batch. (data: 1.59e-02). ETA=13:29:08, max mem: 31.0 GB 
[11/21 05:11:59 visual_prompt]: 	Training 500/553. train loss: 0.6311,	0.9404 s / batch. (data: 6.68e-04). ETA=13:18:12, max mem: 31.0 GB 
[11/21 05:12:48 visual_prompt]: Epoch 8 / 100: avg data time: 2.27e-02, avg batch time: 0.9420, average train loss: 0.7506
[11/21 05:13:43 visual_prompt]: Inference (val):avg data time: 4.64e-04, avg batch time: 0.3049, average loss: 0.6645
[11/21 05:13:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 65.14	
[11/21 05:13:43 visual_prompt]: Best epoch 8: best metric: -0.664
[11/21 05:13:43 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/21 05:15:26 visual_prompt]: 	Training 100/553. train loss: 0.8623,	0.9520 s / batch. (data: 2.65e-04). ETA=13:25:37, max mem: 31.0 GB 
[11/21 05:16:59 visual_prompt]: 	Training 200/553. train loss: 1.2804,	0.9289 s / batch. (data: 6.76e-04). ETA=13:04:35, max mem: 31.0 GB 
[11/21 05:18:31 visual_prompt]: 	Training 300/553. train loss: 0.5291,	0.9373 s / batch. (data: 6.95e-04). ETA=13:10:04, max mem: 31.0 GB 
[11/21 05:20:03 visual_prompt]: 	Training 400/553. train loss: 0.6673,	0.9215 s / batch. (data: 5.85e-03). ETA=12:55:15, max mem: 31.0 GB 
[11/21 05:21:35 visual_prompt]: 	Training 500/553. train loss: 0.9356,	0.9306 s / batch. (data: 5.82e-03). ETA=13:01:20, max mem: 31.0 GB 
[11/21 05:22:24 visual_prompt]: Epoch 9 / 100: avg data time: 2.55e-02, avg batch time: 0.9426, average train loss: 0.7565
[11/21 05:23:18 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.3045, average loss: 0.7147
[11/21 05:23:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 63.45	
[11/21 05:23:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/21 05:24:59 visual_prompt]: 	Training 100/553. train loss: 0.9184,	0.8946 s / batch. (data: 2.39e-04). ETA=12:28:48, max mem: 31.0 GB 
[11/21 05:26:32 visual_prompt]: 	Training 200/553. train loss: 0.7967,	0.9240 s / batch. (data: 5.38e-03). ETA=12:51:53, max mem: 31.0 GB 
[11/21 05:28:04 visual_prompt]: 	Training 300/553. train loss: 0.8796,	0.9436 s / batch. (data: 7.11e-04). ETA=13:06:42, max mem: 31.0 GB 
[11/21 05:29:37 visual_prompt]: 	Training 400/553. train loss: 0.6208,	0.9230 s / batch. (data: 8.52e-03). ETA=12:48:00, max mem: 31.0 GB 
[11/21 05:31:09 visual_prompt]: 	Training 500/553. train loss: 0.6944,	0.9040 s / batch. (data: 2.37e-04). ETA=12:30:38, max mem: 31.0 GB 
[11/21 05:31:57 visual_prompt]: Epoch 10 / 100: avg data time: 2.00e-02, avg batch time: 0.9382, average train loss: 0.7628
[11/21 05:32:52 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3039, average loss: 0.7781
[11/21 05:32:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 59.12	
[11/21 05:32:52 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/21 05:34:35 visual_prompt]: 	Training 100/553. train loss: 0.9203,	0.9440 s / batch. (data: 2.53e-04). ETA=13:01:27, max mem: 31.0 GB 
[11/21 05:36:07 visual_prompt]: 	Training 200/553. train loss: 0.8080,	0.9108 s / batch. (data: 2.94e-04). ETA=12:32:28, max mem: 31.0 GB 
[11/21 05:37:40 visual_prompt]: 	Training 300/553. train loss: 0.4993,	0.9201 s / batch. (data: 5.35e-03). ETA=12:38:36, max mem: 31.0 GB 
[11/21 05:39:13 visual_prompt]: 	Training 400/553. train loss: 1.0450,	0.9261 s / batch. (data: 4.32e-03). ETA=12:42:00, max mem: 31.0 GB 
[11/21 05:40:45 visual_prompt]: 	Training 500/553. train loss: 0.4796,	0.9515 s / batch. (data: 7.27e-04). ETA=13:01:18, max mem: 31.0 GB 
[11/21 05:41:34 visual_prompt]: Epoch 11 / 100: avg data time: 2.44e-02, avg batch time: 0.9437, average train loss: 0.7334
[11/21 05:42:28 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3017, average loss: 0.6785
[11/21 05:42:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 59.20	
[11/21 05:42:28 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/21 05:44:07 visual_prompt]: 	Training 100/553. train loss: 0.5582,	0.9115 s / batch. (data: 2.11e-04). ETA=12:26:10, max mem: 31.0 GB 
[11/21 05:45:40 visual_prompt]: 	Training 200/553. train loss: 0.8869,	0.9353 s / batch. (data: 1.59e-02). ETA=12:44:06, max mem: 31.0 GB 
[11/21 05:47:12 visual_prompt]: 	Training 300/553. train loss: 0.7795,	0.9295 s / batch. (data: 1.04e-02). ETA=12:37:47, max mem: 31.0 GB 
[11/21 05:48:45 visual_prompt]: 	Training 400/553. train loss: 1.0089,	0.9568 s / batch. (data: 6.66e-04). ETA=12:58:28, max mem: 31.0 GB 
[11/21 05:50:17 visual_prompt]: 	Training 500/553. train loss: 0.6530,	0.9328 s / batch. (data: 1.09e-02). ETA=12:37:23, max mem: 31.0 GB 
[11/21 05:51:06 visual_prompt]: Epoch 12 / 100: avg data time: 1.59e-02, avg batch time: 0.9360, average train loss: 0.7583
[11/21 05:52:01 visual_prompt]: Inference (val):avg data time: 8.43e-05, avg batch time: 0.3020, average loss: 1.0233
[11/21 05:52:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.21	
[11/21 05:52:01 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/21 05:53:42 visual_prompt]: 	Training 100/553. train loss: 0.6064,	0.9601 s / batch. (data: 5.83e-03). ETA=12:57:03, max mem: 31.0 GB 
[11/21 05:55:14 visual_prompt]: 	Training 200/553. train loss: 1.0188,	0.9525 s / batch. (data: 1.65e-02). ETA=12:49:24, max mem: 31.0 GB 
[11/21 05:56:47 visual_prompt]: 	Training 300/553. train loss: 0.7827,	0.9332 s / batch. (data: 5.83e-03). ETA=12:32:14, max mem: 31.0 GB 
[11/21 05:58:19 visual_prompt]: 	Training 400/553. train loss: 0.6455,	0.9472 s / batch. (data: 6.90e-04). ETA=12:41:57, max mem: 31.0 GB 
[11/21 05:59:52 visual_prompt]: 	Training 500/553. train loss: 0.7621,	0.9391 s / batch. (data: 1.08e-02). ETA=12:33:51, max mem: 31.0 GB 
[11/21 06:00:40 visual_prompt]: Epoch 13 / 100: avg data time: 1.99e-02, avg batch time: 0.9394, average train loss: 0.7344
[11/21 06:01:35 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3036, average loss: 0.7322
[11/21 06:01:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.07	
[11/21 06:01:35 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/21 06:03:17 visual_prompt]: 	Training 100/553. train loss: 0.6347,	0.9076 s / batch. (data: 2.38e-04). ETA=12:06:13, max mem: 31.0 GB 
[11/21 06:04:50 visual_prompt]: 	Training 200/553. train loss: 0.2577,	0.9234 s / batch. (data: 1.09e-02). ETA=12:17:19, max mem: 31.0 GB 
[11/21 06:06:23 visual_prompt]: 	Training 300/553. train loss: 0.7281,	0.9320 s / batch. (data: 3.17e-04). ETA=12:22:38, max mem: 31.0 GB 
[11/21 06:07:55 visual_prompt]: 	Training 400/553. train loss: 0.8903,	0.9080 s / batch. (data: 7.24e-04). ETA=12:02:02, max mem: 31.0 GB 
[11/21 06:09:28 visual_prompt]: 	Training 500/553. train loss: 0.6655,	0.9125 s / batch. (data: 7.12e-04). ETA=12:04:06, max mem: 31.0 GB 
[11/21 06:10:17 visual_prompt]: Epoch 14 / 100: avg data time: 2.46e-02, avg batch time: 0.9445, average train loss: 0.7400
[11/21 06:11:12 visual_prompt]: Inference (val):avg data time: 1.13e-04, avg batch time: 0.3038, average loss: 0.7069
[11/21 06:11:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 57.16	
[11/21 06:11:12 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/21 06:12:56 visual_prompt]: 	Training 100/553. train loss: 0.6834,	0.9209 s / batch. (data: 8.86e-03). ETA=12:08:24, max mem: 31.0 GB 
[11/21 06:14:28 visual_prompt]: 	Training 200/553. train loss: 0.5857,	0.9400 s / batch. (data: 7.00e-04). ETA=12:21:56, max mem: 31.0 GB 
[11/21 06:16:01 visual_prompt]: 	Training 300/553. train loss: 0.8939,	0.9199 s / batch. (data: 2.56e-04). ETA=12:04:31, max mem: 31.0 GB 
[11/21 06:17:34 visual_prompt]: 	Training 400/553. train loss: 0.5610,	0.9310 s / batch. (data: 7.18e-04). ETA=12:11:44, max mem: 31.0 GB 
[11/21 06:19:06 visual_prompt]: 	Training 500/553. train loss: 0.9017,	0.9081 s / batch. (data: 1.06e-02). ETA=11:52:14, max mem: 31.0 GB 
[11/21 06:19:54 visual_prompt]: Epoch 15 / 100: avg data time: 2.57e-02, avg batch time: 0.9450, average train loss: 0.7264
[11/21 06:20:49 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3037, average loss: 0.7403
[11/21 06:20:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.32	
[11/21 06:20:49 visual_prompt]: Stopping early.
[11/21 06:20:49 visual_prompt]: Rank of current process: 0. World size: 1
[11/21 06:20:49 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 06:20:49 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/21 06:20:49 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/21 06:20:49 visual_prompt]: Training with config:
[11/21 06:20:49 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0005_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/21 06:20:49 visual_prompt]: Loading training data...
[11/21 06:20:49 visual_prompt]: Constructing mammo-cbis dataset train...
[11/21 06:20:49 visual_prompt]: Loading validation data...
[11/21 06:20:49 visual_prompt]: Constructing mammo-cbis dataset val...
[11/21 06:20:49 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/21 06:20:51 visual_prompt]: Enable all parameters update during training
[11/21 06:20:51 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/21 06:20:51 visual_prompt]: tuned percent:100.000
[11/21 06:20:51 visual_prompt]: Device used for model: 0
[11/21 06:20:51 visual_prompt]: Setting up Evaluator...
[11/21 06:20:51 visual_prompt]: Setting up Trainer...
[11/21 06:20:51 visual_prompt]: 	Setting up the optimizer...
[11/21 06:20:51 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/21 06:22:31 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9160 s / batch. (data: 4.00e-03). ETA=14:02:43, max mem: 32.0 GB 
[11/21 06:24:04 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9352 s / batch. (data: 7.94e-03). ETA=14:18:49, max mem: 32.0 GB 
[11/21 06:25:36 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.8960 s / batch. (data: 2.53e-03). ETA=13:41:19, max mem: 32.0 GB 
[11/21 06:27:09 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.8927 s / batch. (data: 2.28e-04). ETA=13:36:46, max mem: 32.0 GB 
[11/21 06:28:41 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9342 s / batch. (data: 6.19e-03). ETA=14:13:13, max mem: 32.0 GB 
[11/21 06:29:30 visual_prompt]: Epoch 1 / 100: avg data time: 2.01e-02, avg batch time: 0.9380, average train loss: 7.6130
[11/21 06:30:23 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.3062, average loss: 6.9126
[11/21 06:30:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/21 06:30:23 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/21 06:32:04 visual_prompt]: 	Training 100/553. train loss: 2.2232,	0.9200 s / batch. (data: 2.71e-04). ETA=13:57:54, max mem: 32.0 GB 
[11/21 06:33:36 visual_prompt]: 	Training 200/553. train loss: 1.4969,	0.9280 s / batch. (data: 7.53e-04). ETA=14:03:39, max mem: 32.0 GB 
[11/21 06:35:09 visual_prompt]: 	Training 300/553. train loss: 1.0337,	0.9040 s / batch. (data: 3.92e-03). ETA=13:40:21, max mem: 32.0 GB 
[11/21 06:36:41 visual_prompt]: 	Training 400/553. train loss: 0.6481,	0.9343 s / batch. (data: 7.07e-04). ETA=14:06:17, max mem: 32.0 GB 
[11/21 06:38:14 visual_prompt]: 	Training 500/553. train loss: 0.8394,	0.9151 s / batch. (data: 7.31e-04). ETA=13:47:22, max mem: 32.0 GB 
[11/21 06:39:03 visual_prompt]: Epoch 2 / 100: avg data time: 1.98e-02, avg batch time: 0.9390, average train loss: 0.9620
[11/21 06:39:57 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3035, average loss: 0.9572
[11/21 06:39:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.36	
[11/21 06:39:57 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/21 06:41:38 visual_prompt]: 	Training 100/553. train loss: 0.6894,	0.9039 s / batch. (data: 7.21e-04). ETA=13:34:57, max mem: 32.0 GB 
[11/21 06:43:10 visual_prompt]: 	Training 200/553. train loss: 2.7539,	0.9183 s / batch. (data: 2.75e-04). ETA=13:46:24, max mem: 32.0 GB 
[11/21 06:44:43 visual_prompt]: 	Training 300/553. train loss: 0.9280,	0.9280 s / batch. (data: 7.84e-04). ETA=13:53:33, max mem: 32.0 GB 
[11/21 06:46:15 visual_prompt]: 	Training 400/553. train loss: 0.8162,	0.9042 s / batch. (data: 2.61e-04). ETA=13:30:40, max mem: 32.0 GB 
[11/21 06:47:47 visual_prompt]: 	Training 500/553. train loss: 1.4699,	0.9511 s / batch. (data: 2.71e-02). ETA=14:11:09, max mem: 32.0 GB 
[11/21 06:48:36 visual_prompt]: Epoch 3 / 100: avg data time: 2.07e-02, avg batch time: 0.9373, average train loss: 0.8729
[11/21 06:49:30 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3041, average loss: 0.6950
[11/21 06:49:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 58.37	
[11/21 06:49:30 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/21 06:51:14 visual_prompt]: 	Training 100/553. train loss: 0.6585,	0.9148 s / batch. (data: 7.98e-03). ETA=13:36:21, max mem: 32.0 GB 
[11/21 06:52:46 visual_prompt]: 	Training 200/553. train loss: 1.4912,	0.9240 s / batch. (data: 4.01e-03). ETA=13:42:56, max mem: 32.0 GB 
[11/21 06:54:19 visual_prompt]: 	Training 300/553. train loss: 1.2777,	0.9281 s / batch. (data: 7.28e-04). ETA=13:45:07, max mem: 32.0 GB 
[11/21 06:55:51 visual_prompt]: 	Training 400/553. train loss: 0.6454,	0.9090 s / batch. (data: 7.89e-04). ETA=13:26:36, max mem: 32.0 GB 
[11/21 06:57:23 visual_prompt]: 	Training 500/553. train loss: 0.8242,	0.9040 s / batch. (data: 2.94e-04). ETA=13:20:41, max mem: 32.0 GB 
[11/21 06:58:12 visual_prompt]: Epoch 4 / 100: avg data time: 2.56e-02, avg batch time: 0.9434, average train loss: 0.8440
[11/21 06:59:06 visual_prompt]: Inference (val):avg data time: 3.63e-04, avg batch time: 0.3049, average loss: 0.7088
[11/21 06:59:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.42	
[11/21 06:59:06 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/21 07:00:45 visual_prompt]: 	Training 100/553. train loss: 0.4674,	0.9233 s / batch. (data: 2.06e-02). ETA=13:35:24, max mem: 32.0 GB 
[11/21 07:02:17 visual_prompt]: 	Training 200/553. train loss: 0.4137,	0.9173 s / batch. (data: 2.87e-04). ETA=13:28:34, max mem: 32.0 GB 
[11/21 07:03:49 visual_prompt]: 	Training 300/553. train loss: 1.7953,	0.9076 s / batch. (data: 3.18e-04). ETA=13:18:29, max mem: 32.0 GB 
[11/21 07:05:21 visual_prompt]: 	Training 400/553. train loss: 0.9072,	0.9107 s / batch. (data: 2.83e-04). ETA=13:19:42, max mem: 32.0 GB 
[11/21 07:06:54 visual_prompt]: 	Training 500/553. train loss: 0.7129,	0.9040 s / batch. (data: 2.99e-04). ETA=13:12:19, max mem: 32.0 GB 
[11/21 07:07:42 visual_prompt]: Epoch 5 / 100: avg data time: 1.70e-02, avg batch time: 0.9334, average train loss: 0.7992
[11/21 07:08:36 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.3018, average loss: 0.7120
[11/21 07:08:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 63.10	
[11/21 07:08:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/21 07:10:17 visual_prompt]: 	Training 100/553. train loss: 0.4939,	0.9159 s / batch. (data: 3.07e-04). ETA=13:20:27, max mem: 32.0 GB 
[11/21 07:11:49 visual_prompt]: 	Training 200/553. train loss: 1.3128,	0.9133 s / batch. (data: 3.18e-04). ETA=13:16:35, max mem: 32.0 GB 
[11/21 07:13:21 visual_prompt]: 	Training 300/553. train loss: 0.6119,	0.9400 s / batch. (data: 6.88e-04). ETA=13:38:21, max mem: 32.0 GB 
[11/21 07:14:53 visual_prompt]: 	Training 400/553. train loss: 0.9590,	0.9400 s / batch. (data: 2.94e-04). ETA=13:36:44, max mem: 32.0 GB 
[11/21 07:16:25 visual_prompt]: 	Training 500/553. train loss: 1.0593,	0.8960 s / batch. (data: 2.86e-04). ETA=12:57:04, max mem: 32.0 GB 
[11/21 07:17:14 visual_prompt]: Epoch 6 / 100: avg data time: 2.03e-02, avg batch time: 0.9363, average train loss: 0.8541
[11/21 07:18:08 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3061, average loss: 0.6584
[11/21 07:18:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.04	
[11/21 07:18:08 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/21 07:19:52 visual_prompt]: 	Training 100/553. train loss: 0.5032,	0.9200 s / batch. (data: 3.53e-04). ETA=13:15:32, max mem: 32.0 GB 
[11/21 07:21:24 visual_prompt]: 	Training 200/553. train loss: 0.4223,	0.9000 s / batch. (data: 1.04e-03). ETA=12:56:43, max mem: 32.0 GB 
[11/21 07:22:56 visual_prompt]: 	Training 300/553. train loss: 0.5789,	0.9120 s / batch. (data: 7.22e-04). ETA=13:05:32, max mem: 32.0 GB 
[11/21 07:24:28 visual_prompt]: 	Training 400/553. train loss: 0.9191,	0.8923 s / batch. (data: 2.49e-04). ETA=12:47:05, max mem: 32.0 GB 
[11/21 07:26:01 visual_prompt]: 	Training 500/553. train loss: 0.5336,	0.9197 s / batch. (data: 7.40e-04). ETA=13:09:06, max mem: 32.0 GB 
[11/21 07:26:49 visual_prompt]: Epoch 7 / 100: avg data time: 2.68e-02, avg batch time: 0.9423, average train loss: 0.8219
[11/21 07:27:43 visual_prompt]: Inference (val):avg data time: 8.34e-05, avg batch time: 0.3049, average loss: 0.6447
[11/21 07:27:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 66.64	
[11/21 07:27:43 visual_prompt]: Best epoch 7: best metric: -0.645
[11/21 07:27:43 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/21 07:29:25 visual_prompt]: 	Training 100/553. train loss: 0.4579,	0.9560 s / batch. (data: 8.04e-04). ETA=13:37:50, max mem: 32.0 GB 
[11/21 07:30:58 visual_prompt]: 	Training 200/553. train loss: 0.6689,	0.9080 s / batch. (data: 6.95e-04). ETA=12:55:15, max mem: 32.0 GB 
[11/21 07:32:30 visual_prompt]: 	Training 300/553. train loss: 1.0318,	0.9320 s / batch. (data: 2.70e-04). ETA=13:14:14, max mem: 32.0 GB 
[11/21 07:34:02 visual_prompt]: 	Training 400/553. train loss: 0.6207,	0.9160 s / batch. (data: 2.86e-04). ETA=12:59:02, max mem: 32.0 GB 
[11/21 07:35:35 visual_prompt]: 	Training 500/553. train loss: 0.7423,	0.9160 s / batch. (data: 2.57e-04). ETA=12:57:31, max mem: 32.0 GB 
[11/21 07:36:23 visual_prompt]: Epoch 8 / 100: avg data time: 2.25e-02, avg batch time: 0.9406, average train loss: 0.8245
[11/21 07:37:17 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3045, average loss: 0.6230
[11/21 07:37:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 68.18	
[11/21 07:37:17 visual_prompt]: Best epoch 8: best metric: -0.623
[11/21 07:37:17 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/21 07:39:01 visual_prompt]: 	Training 100/553. train loss: 1.2139,	0.9367 s / batch. (data: 7.77e-04). ETA=13:12:41, max mem: 32.0 GB 
[11/21 07:40:33 visual_prompt]: 	Training 200/553. train loss: 0.9011,	0.9081 s / batch. (data: 3.51e-04). ETA=12:46:56, max mem: 32.0 GB 
[11/21 07:42:05 visual_prompt]: 	Training 300/553. train loss: 0.6706,	0.9070 s / batch. (data: 5.79e-03). ETA=12:44:32, max mem: 32.0 GB 
[11/21 07:43:37 visual_prompt]: 	Training 400/553. train loss: 1.2430,	0.9137 s / batch. (data: 1.04e-02). ETA=12:48:38, max mem: 32.0 GB 
[11/21 07:45:09 visual_prompt]: 	Training 500/553. train loss: 1.2643,	0.9400 s / batch. (data: 7.97e-03). ETA=13:09:12, max mem: 32.0 GB 
[11/21 07:45:57 visual_prompt]: Epoch 9 / 100: avg data time: 2.34e-02, avg batch time: 0.9401, average train loss: 0.8515
[11/21 07:46:51 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3046, average loss: 0.8071
[11/21 07:46:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 65.91	
[11/21 07:46:51 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/21 07:48:31 visual_prompt]: 	Training 100/553. train loss: 1.1876,	0.9172 s / batch. (data: 2.80e-04). ETA=12:47:45, max mem: 32.0 GB 
[11/21 07:50:04 visual_prompt]: 	Training 200/553. train loss: 0.6452,	0.9480 s / batch. (data: 2.81e-04). ETA=13:11:55, max mem: 32.0 GB 
[11/21 07:51:36 visual_prompt]: 	Training 300/553. train loss: 1.5138,	0.9281 s / batch. (data: 6.85e-04). ETA=12:53:44, max mem: 32.0 GB 
[11/21 07:53:08 visual_prompt]: 	Training 400/553. train loss: 1.5325,	0.9341 s / batch. (data: 5.80e-03). ETA=12:57:12, max mem: 32.0 GB 
[11/21 07:54:40 visual_prompt]: 	Training 500/553. train loss: 0.5375,	0.9203 s / batch. (data: 5.87e-03). ETA=12:44:11, max mem: 32.0 GB 
[11/21 07:55:29 visual_prompt]: Epoch 10 / 100: avg data time: 1.97e-02, avg batch time: 0.9365, average train loss: 0.8086
[11/21 07:56:23 visual_prompt]: Inference (val):avg data time: 3.96e-04, avg batch time: 0.3049, average loss: 0.6398
[11/21 07:56:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 67.78	
[11/21 07:56:23 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/21 07:58:05 visual_prompt]: 	Training 100/553. train loss: 0.9903,	0.9132 s / batch. (data: 2.87e-04). ETA=12:35:56, max mem: 32.0 GB 
[11/21 07:59:37 visual_prompt]: 	Training 200/553. train loss: 1.0664,	0.9040 s / batch. (data: 2.68e-04). ETA=12:26:51, max mem: 32.0 GB 
[11/21 08:01:10 visual_prompt]: 	Training 300/553. train loss: 0.6624,	0.9148 s / batch. (data: 8.23e-04). ETA=12:34:14, max mem: 32.0 GB 
[11/21 08:02:42 visual_prompt]: 	Training 400/553. train loss: 0.4831,	0.9539 s / batch. (data: 1.60e-02). ETA=13:04:53, max mem: 32.0 GB 
[11/21 08:04:14 visual_prompt]: 	Training 500/553. train loss: 0.5480,	0.8978 s / batch. (data: 5.82e-03). ETA=12:17:14, max mem: 32.0 GB 
[11/21 08:05:03 visual_prompt]: Epoch 11 / 100: avg data time: 2.30e-02, avg batch time: 0.9398, average train loss: 0.7769
[11/21 08:05:57 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3022, average loss: 0.9503
[11/21 08:05:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 69.77	
[11/21 08:05:57 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/21 08:07:35 visual_prompt]: 	Training 100/553. train loss: 0.9022,	0.9080 s / batch. (data: 8.07e-04). ETA=12:23:17, max mem: 32.0 GB 
[11/21 08:09:07 visual_prompt]: 	Training 200/553. train loss: 0.9922,	0.9044 s / batch. (data: 6.96e-04). ETA=12:18:52, max mem: 32.0 GB 
[11/21 08:10:40 visual_prompt]: 	Training 300/553. train loss: 1.1944,	0.9145 s / batch. (data: 7.54e-04). ETA=12:25:35, max mem: 32.0 GB 
[11/21 08:12:12 visual_prompt]: 	Training 400/553. train loss: 0.2773,	0.9383 s / batch. (data: 5.85e-03). ETA=12:43:25, max mem: 32.0 GB 
[11/21 08:13:44 visual_prompt]: 	Training 500/553. train loss: 0.3577,	0.9172 s / batch. (data: 6.89e-04). ETA=12:24:44, max mem: 32.0 GB 
[11/21 08:14:33 visual_prompt]: Epoch 12 / 100: avg data time: 1.52e-02, avg batch time: 0.9331, average train loss: 0.8073
[11/21 08:15:27 visual_prompt]: Inference (val):avg data time: 6.69e-04, avg batch time: 0.3059, average loss: 0.7271
[11/21 08:15:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 67.92	
[11/21 08:15:27 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/21 08:17:08 visual_prompt]: 	Training 100/553. train loss: 0.4523,	0.9230 s / batch. (data: 7.75e-04). ETA=12:27:03, max mem: 32.0 GB 
[11/21 08:18:41 visual_prompt]: 	Training 200/553. train loss: 1.9962,	0.9200 s / batch. (data: 2.93e-04). ETA=12:23:06, max mem: 32.0 GB 
[11/21 08:20:13 visual_prompt]: 	Training 300/553. train loss: 2.0611,	0.9145 s / batch. (data: 5.41e-03). ETA=12:17:10, max mem: 32.0 GB 
[11/21 08:21:45 visual_prompt]: 	Training 400/553. train loss: 0.7818,	0.9068 s / batch. (data: 1.55e-02). ETA=12:09:27, max mem: 32.0 GB 
[11/21 08:23:17 visual_prompt]: 	Training 500/553. train loss: 0.3502,	0.9280 s / batch. (data: 7.06e-04). ETA=12:24:57, max mem: 32.0 GB 
[11/21 08:24:05 visual_prompt]: Epoch 13 / 100: avg data time: 2.19e-02, avg batch time: 0.9372, average train loss: 0.7275
[11/21 08:24:59 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.3031, average loss: 0.7681
[11/21 08:24:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 68.24	
[11/21 08:24:59 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/21 08:26:40 visual_prompt]: 	Training 100/553. train loss: 0.8503,	0.9273 s / batch. (data: 3.96e-03). ETA=12:22:01, max mem: 32.0 GB 
[11/21 08:28:13 visual_prompt]: 	Training 200/553. train loss: 0.1181,	0.9179 s / batch. (data: 7.40e-04). ETA=12:12:57, max mem: 32.0 GB 
[11/21 08:29:45 visual_prompt]: 	Training 300/553. train loss: 0.9450,	0.9164 s / batch. (data: 7.16e-04). ETA=12:10:16, max mem: 32.0 GB 
[11/21 08:31:17 visual_prompt]: 	Training 400/553. train loss: 1.3818,	0.9120 s / batch. (data: 7.94e-03). ETA=12:05:12, max mem: 32.0 GB 
[11/21 08:32:49 visual_prompt]: 	Training 500/553. train loss: 0.7469,	0.9080 s / batch. (data: 7.67e-04). ETA=12:00:30, max mem: 32.0 GB 
[11/21 08:33:38 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e-02, avg batch time: 0.9382, average train loss: 0.7649
[11/21 08:34:32 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.3061, average loss: 0.7524
[11/21 08:34:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 68.53	
[11/21 08:34:32 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/21 08:36:15 visual_prompt]: 	Training 100/553. train loss: 0.2443,	0.8882 s / batch. (data: 3.01e-04). ETA=11:42:30, max mem: 32.0 GB 
[11/21 08:37:47 visual_prompt]: 	Training 200/553. train loss: 0.6835,	0.9081 s / batch. (data: 3.11e-04). ETA=11:56:46, max mem: 32.0 GB 
[11/21 08:39:19 visual_prompt]: 	Training 300/553. train loss: 0.3756,	0.9361 s / batch. (data: 2.37e-04). ETA=12:17:18, max mem: 32.0 GB 
[11/21 08:40:52 visual_prompt]: 	Training 400/553. train loss: 0.3429,	0.8982 s / batch. (data: 7.94e-03). ETA=11:45:57, max mem: 32.0 GB 
[11/21 08:42:24 visual_prompt]: 	Training 500/553. train loss: 0.5061,	0.9080 s / batch. (data: 2.84e-04). ETA=11:52:08, max mem: 32.0 GB 
[11/21 08:43:12 visual_prompt]: Epoch 15 / 100: avg data time: 2.49e-02, avg batch time: 0.9402, average train loss: 0.7477
[11/21 08:44:06 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.3025, average loss: 1.0687
[11/21 08:44:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 67.13	
[11/21 08:44:06 visual_prompt]: Stopping early.
[11/21 08:44:06 visual_prompt]: Rank of current process: 0. World size: 1
[11/21 08:44:06 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 08:44:06 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/21 08:44:06 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/21 08:44:06 visual_prompt]: Training with config:
[11/21 08:44:06 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0001_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/21 08:44:06 visual_prompt]: Loading training data...
[11/21 08:44:06 visual_prompt]: Constructing mammo-cbis dataset train...
[11/21 08:44:06 visual_prompt]: Loading validation data...
[11/21 08:44:06 visual_prompt]: Constructing mammo-cbis dataset val...
[11/21 08:44:06 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/21 08:44:07 visual_prompt]: Enable all parameters update during training
[11/21 08:44:07 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/21 08:44:07 visual_prompt]: tuned percent:100.000
[11/21 08:44:07 visual_prompt]: Device used for model: 0
[11/21 08:44:07 visual_prompt]: Setting up Evaluator...
[11/21 08:44:07 visual_prompt]: Setting up Trainer...
[11/21 08:44:07 visual_prompt]: 	Setting up the optimizer...
[11/21 08:44:07 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/21 08:45:48 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9464 s / batch. (data: 7.09e-04). ETA=14:30:43, max mem: 33.3 GB 
[11/21 08:47:20 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9200 s / batch. (data: 2.83e-04). ETA=14:04:51, max mem: 33.3 GB 
[11/21 08:48:53 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9349 s / batch. (data: 5.79e-03). ETA=14:17:01, max mem: 33.3 GB 
[11/21 08:50:25 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9313 s / batch. (data: 7.01e-04). ETA=14:12:05, max mem: 33.3 GB 
[11/21 08:51:57 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9307 s / batch. (data: 7.18e-04). ETA=14:10:03, max mem: 33.3 GB 
[11/21 08:52:46 visual_prompt]: Epoch 1 / 100: avg data time: 1.83e-02, avg batch time: 0.9370, average train loss: 7.6130
[11/21 08:53:40 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3047, average loss: 6.9126
[11/21 08:53:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/21 08:53:40 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/21 08:55:21 visual_prompt]: 	Training 100/553. train loss: 1.6329,	0.8938 s / batch. (data: 2.59e-04). ETA=13:34:03, max mem: 33.3 GB 
[11/21 08:56:53 visual_prompt]: 	Training 200/553. train loss: 1.0616,	0.9240 s / batch. (data: 7.35e-04). ETA=14:00:03, max mem: 33.3 GB 
[11/21 08:58:26 visual_prompt]: 	Training 300/553. train loss: 1.0085,	0.8961 s / batch. (data: 4.07e-04). ETA=13:33:08, max mem: 33.3 GB 
[11/21 08:59:59 visual_prompt]: 	Training 400/553. train loss: 0.4560,	0.9122 s / batch. (data: 5.35e-03). ETA=13:46:14, max mem: 33.3 GB 
[11/21 09:01:31 visual_prompt]: 	Training 500/553. train loss: 1.1177,	0.9556 s / batch. (data: 1.05e-02). ETA=14:24:00, max mem: 33.3 GB 
[11/21 09:02:20 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e-02, avg batch time: 0.9405, average train loss: 0.9255
[11/21 09:03:14 visual_prompt]: Inference (val):avg data time: 8.42e-05, avg batch time: 0.3065, average loss: 0.8540
[11/21 09:03:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.57	
[11/21 09:03:14 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/21 09:04:54 visual_prompt]: 	Training 100/553. train loss: 0.7208,	0.9055 s / batch. (data: 7.23e-04). ETA=13:36:21, max mem: 33.3 GB 
[11/21 09:06:27 visual_prompt]: 	Training 200/553. train loss: 1.9834,	0.9051 s / batch. (data: 2.55e-04). ETA=13:34:31, max mem: 33.3 GB 
[11/21 09:07:59 visual_prompt]: 	Training 300/553. train loss: 1.1070,	0.9314 s / batch. (data: 1.09e-02). ETA=13:56:39, max mem: 33.3 GB 
[11/21 09:09:31 visual_prompt]: 	Training 400/553. train loss: 0.3047,	0.9234 s / batch. (data: 2.84e-04). ETA=13:47:50, max mem: 33.3 GB 
[11/21 09:11:03 visual_prompt]: 	Training 500/553. train loss: 0.8553,	0.9104 s / batch. (data: 5.36e-03). ETA=13:34:45, max mem: 33.3 GB 
[11/21 09:11:52 visual_prompt]: Epoch 3 / 100: avg data time: 1.92e-02, avg batch time: 0.9377, average train loss: 0.7983
[11/21 09:12:46 visual_prompt]: Inference (val):avg data time: 8.32e-05, avg batch time: 0.3067, average loss: 0.7085
[11/21 09:12:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 60.54	
[11/21 09:12:46 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/21 09:14:30 visual_prompt]: 	Training 100/553. train loss: 0.5737,	0.9076 s / batch. (data: 7.24e-04). ETA=13:29:54, max mem: 33.3 GB 
[11/21 09:16:02 visual_prompt]: 	Training 200/553. train loss: 0.9392,	0.9275 s / batch. (data: 6.81e-04). ETA=13:46:03, max mem: 33.3 GB 
[11/21 09:17:34 visual_prompt]: 	Training 300/553. train loss: 0.8439,	0.9200 s / batch. (data: 6.92e-04). ETA=13:37:53, max mem: 33.3 GB 
[11/21 09:19:07 visual_prompt]: 	Training 400/553. train loss: 0.5890,	0.9084 s / batch. (data: 5.39e-03). ETA=13:26:02, max mem: 33.3 GB 
[11/21 09:20:39 visual_prompt]: 	Training 500/553. train loss: 1.3352,	0.9125 s / batch. (data: 2.61e-04). ETA=13:28:08, max mem: 33.3 GB 
[11/21 09:21:28 visual_prompt]: Epoch 4 / 100: avg data time: 2.61e-02, avg batch time: 0.9437, average train loss: 0.7797
[11/21 09:22:22 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3035, average loss: 0.6791
[11/21 09:22:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 64.35	
[11/21 09:22:22 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/21 09:24:01 visual_prompt]: 	Training 100/553. train loss: 0.6266,	0.9160 s / batch. (data: 2.51e-04). ETA=13:28:57, max mem: 33.3 GB 
[11/21 09:25:34 visual_prompt]: 	Training 200/553. train loss: 0.4863,	0.9370 s / batch. (data: 1.04e-02). ETA=13:45:58, max mem: 33.3 GB 
[11/21 09:27:06 visual_prompt]: 	Training 300/553. train loss: 0.8930,	0.9103 s / batch. (data: 6.90e-04). ETA=13:20:50, max mem: 33.3 GB 
[11/21 09:28:39 visual_prompt]: 	Training 400/553. train loss: 0.9497,	0.9651 s / batch. (data: 1.60e-02). ETA=14:07:31, max mem: 33.3 GB 
[11/21 09:30:11 visual_prompt]: 	Training 500/553. train loss: 0.8432,	0.9176 s / batch. (data: 2.75e-04). ETA=13:24:14, max mem: 33.3 GB 
[11/21 09:31:00 visual_prompt]: Epoch 5 / 100: avg data time: 1.64e-02, avg batch time: 0.9361, average train loss: 0.7322
[11/21 09:31:54 visual_prompt]: Inference (val):avg data time: 8.29e-05, avg batch time: 0.3050, average loss: 0.6564
[11/21 09:31:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 70.99	
[11/21 09:31:54 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/21 09:33:35 visual_prompt]: 	Training 100/553. train loss: 0.5624,	0.9271 s / batch. (data: 3.99e-03). ETA=13:30:11, max mem: 33.3 GB 
[11/21 09:35:08 visual_prompt]: 	Training 200/553. train loss: 0.5885,	0.9400 s / batch. (data: 6.60e-04). ETA=13:39:54, max mem: 33.3 GB 
[11/21 09:36:40 visual_prompt]: 	Training 300/553. train loss: 0.6862,	0.9134 s / batch. (data: 2.60e-04). ETA=13:15:12, max mem: 33.3 GB 
[11/21 09:38:12 visual_prompt]: 	Training 400/553. train loss: 1.0185,	0.9399 s / batch. (data: 8.41e-03). ETA=13:36:40, max mem: 33.3 GB 
[11/21 09:39:45 visual_prompt]: 	Training 500/553. train loss: 1.3610,	0.9099 s / batch. (data: 6.90e-04). ETA=13:09:05, max mem: 33.3 GB 
[11/21 09:40:33 visual_prompt]: Epoch 6 / 100: avg data time: 2.04e-02, avg batch time: 0.9393, average train loss: 0.7091
[11/21 09:41:27 visual_prompt]: Inference (val):avg data time: 1.70e-04, avg batch time: 0.3063, average loss: 0.6384
[11/21 09:41:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 70.16	
[11/21 09:41:27 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/21 09:43:10 visual_prompt]: 	Training 100/553. train loss: 0.5357,	0.9166 s / batch. (data: 2.23e-04). ETA=13:12:37, max mem: 33.3 GB 
[11/21 09:44:43 visual_prompt]: 	Training 200/553. train loss: 0.4143,	0.9158 s / batch. (data: 5.32e-03). ETA=13:10:22, max mem: 33.3 GB 
[11/21 09:46:15 visual_prompt]: 	Training 300/553. train loss: 0.6156,	0.9252 s / batch. (data: 8.97e-03). ETA=13:16:55, max mem: 33.3 GB 
[11/21 09:47:48 visual_prompt]: 	Training 400/553. train loss: 0.7250,	0.9237 s / batch. (data: 5.39e-03). ETA=13:14:05, max mem: 33.3 GB 
[11/21 09:49:20 visual_prompt]: 	Training 500/553. train loss: 0.5822,	0.9210 s / batch. (data: 6.33e-04). ETA=13:10:13, max mem: 33.3 GB 
[11/21 09:50:09 visual_prompt]: Epoch 7 / 100: avg data time: 2.50e-02, avg batch time: 0.9434, average train loss: 0.7008
[11/21 09:51:03 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.3035, average loss: 0.6046
[11/21 09:51:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.07	rocauc: 74.34	
[11/21 09:51:03 visual_prompt]: Best epoch 7: best metric: -0.605
[11/21 09:51:03 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/21 09:52:45 visual_prompt]: 	Training 100/553. train loss: 0.5152,	0.8946 s / batch. (data: 2.38e-04). ETA=12:45:18, max mem: 33.3 GB 
[11/21 09:54:18 visual_prompt]: 	Training 200/553. train loss: 0.5735,	0.9209 s / batch. (data: 5.35e-03). ETA=13:06:18, max mem: 33.3 GB 
[11/21 09:55:50 visual_prompt]: 	Training 300/553. train loss: 0.5338,	0.9300 s / batch. (data: 1.04e-02). ETA=13:12:29, max mem: 33.3 GB 
[11/21 09:57:23 visual_prompt]: 	Training 400/553. train loss: 0.7151,	0.9595 s / batch. (data: 2.47e-02). ETA=13:36:04, max mem: 33.3 GB 
[11/21 09:58:55 visual_prompt]: 	Training 500/553. train loss: 0.4904,	0.9069 s / batch. (data: 2.31e-04). ETA=12:49:49, max mem: 33.3 GB 
[11/21 09:59:44 visual_prompt]: Epoch 8 / 100: avg data time: 2.36e-02, avg batch time: 0.9420, average train loss: 0.6789
[11/21 10:00:38 visual_prompt]: Inference (val):avg data time: 8.49e-05, avg batch time: 0.3052, average loss: 0.6017
[11/21 10:00:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 73.95	
[11/21 10:00:38 visual_prompt]: Best epoch 8: best metric: -0.602
[11/21 10:00:38 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/21 10:02:21 visual_prompt]: 	Training 100/553. train loss: 0.6843,	0.9493 s / batch. (data: 1.28e-02). ETA=13:23:20, max mem: 33.3 GB 
[11/21 10:03:54 visual_prompt]: 	Training 200/553. train loss: 0.8952,	0.9182 s / batch. (data: 5.34e-03). ETA=12:55:32, max mem: 33.3 GB 
[11/21 10:05:26 visual_prompt]: 	Training 300/553. train loss: 0.4980,	0.9074 s / batch. (data: 2.49e-04). ETA=12:44:55, max mem: 33.3 GB 
[11/21 10:06:58 visual_prompt]: 	Training 400/553. train loss: 0.4954,	0.9359 s / batch. (data: 5.83e-03). ETA=13:07:20, max mem: 33.3 GB 
[11/21 10:08:30 visual_prompt]: 	Training 500/553. train loss: 0.9133,	0.9240 s / batch. (data: 2.98e-04). ETA=12:55:48, max mem: 33.3 GB 
[11/21 10:09:19 visual_prompt]: Epoch 9 / 100: avg data time: 2.42e-02, avg batch time: 0.9422, average train loss: 0.6750
[11/21 10:10:13 visual_prompt]: Inference (val):avg data time: 2.64e-04, avg batch time: 0.3057, average loss: 0.6317
[11/21 10:10:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 73.82	
[11/21 10:10:13 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/21 10:11:53 visual_prompt]: 	Training 100/553. train loss: 0.7259,	0.9368 s / batch. (data: 5.83e-03). ETA=13:04:10, max mem: 33.3 GB 
[11/21 10:13:26 visual_prompt]: 	Training 200/553. train loss: 0.4541,	0.9036 s / batch. (data: 2.52e-04). ETA=12:34:49, max mem: 33.3 GB 
[11/21 10:14:58 visual_prompt]: 	Training 300/553. train loss: 1.0697,	0.9295 s / batch. (data: 1.09e-02). ETA=12:54:58, max mem: 33.3 GB 
[11/21 10:16:31 visual_prompt]: 	Training 400/553. train loss: 0.6524,	0.9287 s / batch. (data: 2.29e-04). ETA=12:52:45, max mem: 33.3 GB 
[11/21 10:18:03 visual_prompt]: 	Training 500/553. train loss: 0.6335,	0.9301 s / batch. (data: 6.99e-04). ETA=12:52:20, max mem: 33.3 GB 
[11/21 10:18:52 visual_prompt]: Epoch 10 / 100: avg data time: 1.90e-02, avg batch time: 0.9390, average train loss: 0.6344
[11/21 10:19:46 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3074, average loss: 0.6725
[11/21 10:19:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 68.72	
[11/21 10:19:46 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/21 10:21:29 visual_prompt]: 	Training 100/553. train loss: 0.8639,	0.9240 s / batch. (data: 2.42e-04). ETA=12:44:55, max mem: 33.3 GB 
[11/21 10:23:01 visual_prompt]: 	Training 200/553. train loss: 1.0549,	0.9244 s / batch. (data: 2.46e-04). ETA=12:43:43, max mem: 33.3 GB 
[11/21 10:24:33 visual_prompt]: 	Training 300/553. train loss: 0.4697,	0.9372 s / batch. (data: 9.39e-03). ETA=12:52:43, max mem: 33.3 GB 
[11/21 10:26:05 visual_prompt]: 	Training 400/553. train loss: 0.4307,	0.9395 s / batch. (data: 1.08e-02). ETA=12:53:05, max mem: 33.3 GB 
[11/21 10:27:38 visual_prompt]: 	Training 500/553. train loss: 0.6629,	0.9384 s / batch. (data: 7.15e-04). ETA=12:50:36, max mem: 33.3 GB 
[11/21 10:28:26 visual_prompt]: Epoch 11 / 100: avg data time: 2.19e-02, avg batch time: 0.9405, average train loss: 0.6254
[11/21 10:29:20 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3050, average loss: 0.6486
[11/21 10:29:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 68.96	
[11/21 10:29:20 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/21 10:30:59 visual_prompt]: 	Training 100/553. train loss: 0.5077,	0.9200 s / batch. (data: 6.90e-04). ETA=12:33:07, max mem: 33.3 GB 
[11/21 10:32:32 visual_prompt]: 	Training 200/553. train loss: 0.9063,	0.9383 s / batch. (data: 1.08e-02). ETA=12:46:31, max mem: 33.3 GB 
[11/21 10:34:04 visual_prompt]: 	Training 300/553. train loss: 0.6235,	0.9238 s / batch. (data: 6.82e-04). ETA=12:33:09, max mem: 33.3 GB 
[11/21 10:35:36 visual_prompt]: 	Training 400/553. train loss: 0.8210,	0.9326 s / batch. (data: 2.86e-04). ETA=12:38:45, max mem: 33.3 GB 
[11/21 10:37:09 visual_prompt]: 	Training 500/553. train loss: 0.6212,	0.9105 s / batch. (data: 6.59e-04). ETA=12:19:17, max mem: 33.3 GB 
[11/21 10:37:57 visual_prompt]: Epoch 12 / 100: avg data time: 1.58e-02, avg batch time: 0.9344, average train loss: 0.6649
[11/21 10:38:51 visual_prompt]: Inference (val):avg data time: 8.20e-05, avg batch time: 0.3052, average loss: 0.9845
[11/21 10:38:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.68	
[11/21 10:38:51 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/21 10:40:32 visual_prompt]: 	Training 100/553. train loss: 0.6197,	0.9290 s / batch. (data: 2.79e-04). ETA=12:31:55, max mem: 33.3 GB 
[11/21 10:42:04 visual_prompt]: 	Training 200/553. train loss: 0.8855,	0.9173 s / batch. (data: 2.88e-04). ETA=12:20:56, max mem: 33.3 GB 
[11/21 10:43:37 visual_prompt]: 	Training 300/553. train loss: 1.1187,	0.9035 s / batch. (data: 2.52e-04). ETA=12:08:18, max mem: 33.3 GB 
[11/21 10:45:10 visual_prompt]: 	Training 400/553. train loss: 0.6038,	0.9400 s / batch. (data: 7.15e-04). ETA=12:36:07, max mem: 33.3 GB 
[11/21 10:46:42 visual_prompt]: 	Training 500/553. train loss: 0.4962,	0.9228 s / batch. (data: 5.34e-03). ETA=12:20:43, max mem: 33.3 GB 
[11/21 10:47:31 visual_prompt]: Epoch 13 / 100: avg data time: 2.02e-02, avg batch time: 0.9398, average train loss: 0.6505
[11/21 10:48:24 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3048, average loss: 0.7324
[11/21 10:48:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 68.53	
[11/21 10:48:24 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/21 10:50:05 visual_prompt]: 	Training 100/553. train loss: 0.5740,	0.9160 s / batch. (data: 3.99e-03). ETA=12:12:58, max mem: 33.3 GB 
[11/21 10:51:38 visual_prompt]: 	Training 200/553. train loss: 0.1939,	0.9232 s / batch. (data: 7.72e-04). ETA=12:17:12, max mem: 33.3 GB 
[11/21 10:53:11 visual_prompt]: 	Training 300/553. train loss: 1.1003,	0.9182 s / batch. (data: 5.90e-03). ETA=12:11:41, max mem: 33.3 GB 
[11/21 10:54:43 visual_prompt]: 	Training 400/553. train loss: 0.8862,	0.9094 s / batch. (data: 1.04e-02). ETA=12:03:06, max mem: 33.3 GB 
[11/21 10:56:15 visual_prompt]: 	Training 500/553. train loss: 0.7651,	0.9127 s / batch. (data: 2.59e-04). ETA=12:04:15, max mem: 33.3 GB 
[11/21 10:57:04 visual_prompt]: Epoch 14 / 100: avg data time: 2.00e-02, avg batch time: 0.9388, average train loss: 0.6396
[11/21 10:57:58 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.3059, average loss: 0.6889
[11/21 10:57:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 67.24	
[11/21 10:57:58 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/21 10:59:41 visual_prompt]: 	Training 100/553. train loss: 0.2131,	0.9196 s / batch. (data: 2.69e-04). ETA=12:07:22, max mem: 33.3 GB 
[11/21 11:01:14 visual_prompt]: 	Training 200/553. train loss: 0.6106,	0.9455 s / batch. (data: 5.82e-03). ETA=12:26:15, max mem: 33.3 GB 
[11/21 11:02:46 visual_prompt]: 	Training 300/553. train loss: 0.8593,	0.9084 s / batch. (data: 5.46e-03). ETA=11:55:28, max mem: 33.3 GB 
[11/21 11:04:19 visual_prompt]: 	Training 400/553. train loss: 0.3094,	0.9261 s / batch. (data: 5.39e-03). ETA=12:07:53, max mem: 33.3 GB 
[11/21 11:05:51 visual_prompt]: 	Training 500/553. train loss: 0.8509,	0.9121 s / batch. (data: 7.02e-04). ETA=11:55:20, max mem: 33.3 GB 
[11/21 11:06:39 visual_prompt]: Epoch 15 / 100: avg data time: 2.47e-02, avg batch time: 0.9430, average train loss: 0.5880
[11/21 11:07:33 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3060, average loss: 0.7840
[11/21 11:07:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.05	
[11/21 11:07:33 visual_prompt]: Stopping early.
[11/21 11:07:33 visual_prompt]: Rank of current process: 0. World size: 1
[11/21 11:07:33 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 11:07:33 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/21 11:07:33 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/21 11:07:33 visual_prompt]: Training with config:
[11/21 11:07:33 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0001_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/21 11:07:33 visual_prompt]: Loading training data...
[11/21 11:07:33 visual_prompt]: Constructing mammo-cbis dataset train...
[11/21 11:07:33 visual_prompt]: Loading validation data...
[11/21 11:07:33 visual_prompt]: Constructing mammo-cbis dataset val...
[11/21 11:07:33 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/21 11:07:35 visual_prompt]: Enable all parameters update during training
[11/21 11:07:35 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/21 11:07:35 visual_prompt]: tuned percent:100.000
[11/21 11:07:35 visual_prompt]: Device used for model: 0
[11/21 11:07:35 visual_prompt]: Setting up Evaluator...
[11/21 11:07:35 visual_prompt]: Setting up Trainer...
[11/21 11:07:35 visual_prompt]: 	Setting up the optimizer...
[11/21 11:07:35 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/21 11:09:15 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9401 s / batch. (data: 2.65e-04). ETA=14:24:50, max mem: 34.6 GB 
[11/21 11:10:48 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9224 s / batch. (data: 2.90e-04). ETA=14:07:04, max mem: 34.6 GB 
[11/21 11:12:20 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.8984 s / batch. (data: 2.62e-04). ETA=13:43:32, max mem: 34.6 GB 
[11/21 11:13:52 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9118 s / batch. (data: 8.98e-03). ETA=13:54:15, max mem: 34.6 GB 
[11/21 11:15:25 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9253 s / batch. (data: 5.39e-03). ETA=14:05:05, max mem: 34.6 GB 
[11/21 11:16:13 visual_prompt]: Epoch 1 / 100: avg data time: 1.74e-02, avg batch time: 0.9366, average train loss: 7.6130
[11/21 11:17:07 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3044, average loss: 6.9126
[11/21 11:17:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/21 11:17:07 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/21 11:18:48 visual_prompt]: 	Training 100/553. train loss: 1.6343,	0.9326 s / batch. (data: 1.04e-02). ETA=14:09:21, max mem: 34.6 GB 
[11/21 11:20:20 visual_prompt]: 	Training 200/553. train loss: 1.0649,	0.9323 s / batch. (data: 7.62e-04). ETA=14:07:35, max mem: 34.6 GB 
[11/21 11:21:53 visual_prompt]: 	Training 300/553. train loss: 1.0053,	0.9355 s / batch. (data: 1.05e-02). ETA=14:08:54, max mem: 34.6 GB 
[11/21 11:23:26 visual_prompt]: 	Training 400/553. train loss: 0.4584,	0.9302 s / batch. (data: 1.09e-02). ETA=14:02:34, max mem: 34.6 GB 
[11/21 11:24:58 visual_prompt]: 	Training 500/553. train loss: 1.1114,	0.9380 s / batch. (data: 2.42e-02). ETA=14:08:05, max mem: 34.6 GB 
[11/21 11:25:46 visual_prompt]: Epoch 2 / 100: avg data time: 2.02e-02, avg batch time: 0.9382, average train loss: 0.9256
[11/21 11:26:40 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3030, average loss: 0.8489
[11/21 11:26:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 56.56	
[11/21 11:26:40 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/21 11:28:21 visual_prompt]: 	Training 100/553. train loss: 0.7137,	0.9414 s / batch. (data: 1.04e-02). ETA=14:08:46, max mem: 34.6 GB 
[11/21 11:29:54 visual_prompt]: 	Training 200/553. train loss: 1.9809,	0.8975 s / batch. (data: 2.68e-04). ETA=13:27:41, max mem: 34.6 GB 
[11/21 11:31:26 visual_prompt]: 	Training 300/553. train loss: 1.1345,	0.9035 s / batch. (data: 2.62e-04). ETA=13:31:33, max mem: 34.6 GB 
[11/21 11:32:58 visual_prompt]: 	Training 400/553. train loss: 0.3064,	0.9385 s / batch. (data: 1.09e-02). ETA=14:01:27, max mem: 34.6 GB 
[11/21 11:34:30 visual_prompt]: 	Training 500/553. train loss: 0.8596,	0.9383 s / batch. (data: 2.21e-02). ETA=13:59:42, max mem: 34.6 GB 
[11/21 11:35:19 visual_prompt]: Epoch 3 / 100: avg data time: 1.92e-02, avg batch time: 0.9378, average train loss: 0.7983
[11/21 11:36:13 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3028, average loss: 0.7112
[11/21 11:36:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.38	
[11/21 11:36:13 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/21 11:37:56 visual_prompt]: 	Training 100/553. train loss: 0.5839,	0.9180 s / batch. (data: 7.26e-04). ETA=13:39:12, max mem: 34.6 GB 
[11/21 11:39:29 visual_prompt]: 	Training 200/553. train loss: 0.9359,	0.9332 s / batch. (data: 2.98e-02). ETA=13:51:11, max mem: 34.6 GB 
[11/21 11:41:01 visual_prompt]: 	Training 300/553. train loss: 0.8145,	0.9112 s / batch. (data: 5.35e-03). ETA=13:30:01, max mem: 34.6 GB 
[11/21 11:42:33 visual_prompt]: 	Training 400/553. train loss: 0.6013,	0.9091 s / batch. (data: 2.76e-04). ETA=13:26:41, max mem: 34.6 GB 
[11/21 11:44:06 visual_prompt]: 	Training 500/553. train loss: 1.3576,	0.9429 s / batch. (data: 5.35e-03). ETA=13:55:08, max mem: 34.6 GB 
[11/21 11:44:55 visual_prompt]: Epoch 4 / 100: avg data time: 2.61e-02, avg batch time: 0.9434, average train loss: 0.7797
[11/21 11:45:48 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.3046, average loss: 0.6761
[11/21 11:45:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 63.80	
[11/21 11:45:48 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/21 11:47:27 visual_prompt]: 	Training 100/553. train loss: 0.6212,	0.9430 s / batch. (data: 5.35e-03). ETA=13:52:46, max mem: 34.6 GB 
[11/21 11:49:01 visual_prompt]: 	Training 200/553. train loss: 0.4877,	0.9422 s / batch. (data: 1.55e-02). ETA=13:50:31, max mem: 34.6 GB 
[11/21 11:50:35 visual_prompt]: 	Training 300/553. train loss: 0.8545,	0.9161 s / batch. (data: 6.64e-03). ETA=13:25:59, max mem: 34.6 GB 
[11/21 11:52:09 visual_prompt]: 	Training 400/553. train loss: 0.9262,	2.5080 s / batch. (data: 1.59e+00). ETA=1 day, 12:42:20, max mem: 34.6 GB 
[11/21 11:53:42 visual_prompt]: 	Training 500/553. train loss: 0.8352,	0.9297 s / batch. (data: 5.86e-03). ETA=13:34:49, max mem: 34.6 GB 
[11/21 11:54:31 visual_prompt]: Epoch 5 / 100: avg data time: 2.63e-02, avg batch time: 0.9438, average train loss: 0.7323
[11/21 11:55:24 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.3036, average loss: 0.6535
[11/21 11:55:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 71.03	
[11/21 11:55:24 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/21 11:57:06 visual_prompt]: 	Training 100/553. train loss: 0.5286,	0.9024 s / batch. (data: 5.20e-03). ETA=13:08:36, max mem: 34.6 GB 
[11/21 11:58:38 visual_prompt]: 	Training 200/553. train loss: 0.5549,	0.9411 s / batch. (data: 5.79e-03). ETA=13:40:54, max mem: 34.6 GB 
[11/21 12:00:11 visual_prompt]: 	Training 300/553. train loss: 0.6792,	0.9224 s / batch. (data: 2.52e-04). ETA=13:23:01, max mem: 34.6 GB 
[11/21 12:01:43 visual_prompt]: 	Training 400/553. train loss: 0.9575,	0.9429 s / batch. (data: 5.81e-03). ETA=13:39:18, max mem: 34.6 GB 
[11/21 12:03:15 visual_prompt]: 	Training 500/553. train loss: 1.4864,	0.9111 s / batch. (data: 2.47e-04). ETA=13:10:09, max mem: 34.6 GB 
[11/21 12:04:04 visual_prompt]: Epoch 6 / 100: avg data time: 2.04e-02, avg batch time: 0.9391, average train loss: 0.7068
[11/21 12:04:58 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.3045, average loss: 0.6362
[11/21 12:04:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 69.93	
[11/21 12:04:58 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/21 12:06:41 visual_prompt]: 	Training 100/553. train loss: 0.5099,	0.9443 s / batch. (data: 9.95e-03). ETA=13:36:30, max mem: 34.6 GB 
[11/21 12:08:14 visual_prompt]: 	Training 200/553. train loss: 0.4123,	0.9239 s / batch. (data: 7.76e-04). ETA=13:17:21, max mem: 34.6 GB 
[11/21 12:09:46 visual_prompt]: 	Training 300/553. train loss: 0.5864,	0.9061 s / batch. (data: 1.04e-02). ETA=13:00:30, max mem: 34.6 GB 
[11/21 12:11:18 visual_prompt]: 	Training 400/553. train loss: 0.7247,	0.9052 s / batch. (data: 2.41e-04). ETA=12:58:12, max mem: 34.6 GB 
[11/21 12:12:51 visual_prompt]: 	Training 500/553. train loss: 0.5516,	0.9181 s / batch. (data: 1.04e-02). ETA=13:07:47, max mem: 34.6 GB 
[11/21 12:13:39 visual_prompt]: Epoch 7 / 100: avg data time: 2.53e-02, avg batch time: 0.9431, average train loss: 0.6910
[11/21 12:14:33 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.3040, average loss: 0.6263
[11/21 12:14:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 72.47	
[11/21 12:14:33 visual_prompt]: Best epoch 7: best metric: -0.626
[11/21 12:14:33 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/21 12:16:15 visual_prompt]: 	Training 100/553. train loss: 0.4704,	0.9360 s / batch. (data: 7.52e-04). ETA=13:20:44, max mem: 34.6 GB 
[11/21 12:17:48 visual_prompt]: 	Training 200/553. train loss: 0.6142,	0.9256 s / batch. (data: 6.85e-04). ETA=13:10:16, max mem: 34.6 GB 
[11/21 12:19:20 visual_prompt]: 	Training 300/553. train loss: 0.6194,	0.9352 s / batch. (data: 5.80e-03). ETA=13:16:56, max mem: 34.6 GB 
[11/21 12:20:52 visual_prompt]: 	Training 400/553. train loss: 0.6492,	0.9318 s / batch. (data: 5.80e-03). ETA=13:12:31, max mem: 34.6 GB 
[11/21 12:22:24 visual_prompt]: 	Training 500/553. train loss: 0.3755,	0.9093 s / batch. (data: 2.90e-04). ETA=12:51:49, max mem: 34.6 GB 
[11/21 12:23:13 visual_prompt]: Epoch 8 / 100: avg data time: 2.21e-02, avg batch time: 0.9396, average train loss: 0.6776
[11/21 12:24:07 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3039, average loss: 0.6072
[11/21 12:24:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 72.70	
[11/21 12:24:07 visual_prompt]: Best epoch 8: best metric: -0.607
[11/21 12:24:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/21 12:25:51 visual_prompt]: 	Training 100/553. train loss: 0.7333,	0.9203 s / batch. (data: 6.99e-04). ETA=12:58:46, max mem: 34.6 GB 
[11/21 12:27:23 visual_prompt]: 	Training 200/553. train loss: 0.9784,	0.9245 s / batch. (data: 1.14e-02). ETA=13:00:49, max mem: 34.6 GB 
[11/21 12:28:55 visual_prompt]: 	Training 300/553. train loss: 0.5778,	0.9301 s / batch. (data: 5.76e-03). ETA=13:03:59, max mem: 34.6 GB 
[11/21 12:30:27 visual_prompt]: 	Training 400/553. train loss: 0.4190,	0.9015 s / batch. (data: 2.32e-04). ETA=12:38:22, max mem: 34.6 GB 
[11/21 12:31:59 visual_prompt]: 	Training 500/553. train loss: 0.8439,	0.9141 s / batch. (data: 2.59e-04). ETA=12:47:28, max mem: 34.6 GB 
[11/21 12:32:48 visual_prompt]: Epoch 9 / 100: avg data time: 2.43e-02, avg batch time: 0.9416, average train loss: 0.6856
[11/21 12:33:42 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.3039, average loss: 0.6278
[11/21 12:33:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 74.09	
[11/21 12:33:42 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/21 12:35:22 visual_prompt]: 	Training 100/553. train loss: 0.6882,	0.9173 s / batch. (data: 2.49e-04). ETA=12:47:51, max mem: 34.6 GB 
[11/21 12:36:54 visual_prompt]: 	Training 200/553. train loss: 0.5282,	0.9554 s / batch. (data: 3.10e-04). ETA=13:18:07, max mem: 34.6 GB 
[11/21 12:38:32 visual_prompt]: 	Training 300/553. train loss: 0.9637,	0.9162 s / batch. (data: 2.64e-04). ETA=12:43:49, max mem: 34.6 GB 
[11/21 12:40:08 visual_prompt]: 	Training 400/553. train loss: 0.6854,	0.9124 s / batch. (data: 2.78e-04). ETA=12:39:09, max mem: 34.6 GB 
[11/21 12:41:44 visual_prompt]: 	Training 500/553. train loss: 0.5607,	0.9296 s / batch. (data: 3.15e-04). ETA=12:51:53, max mem: 34.6 GB 
[11/21 12:42:36 visual_prompt]: Epoch 10 / 100: avg data time: 4.57e-02, avg batch time: 0.9667, average train loss: 0.6331
[11/21 12:43:33 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3036, average loss: 0.7649
[11/21 12:43:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 71.84	
[11/21 12:43:33 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/21 12:45:21 visual_prompt]: 	Training 100/553. train loss: 1.0361,	0.8982 s / batch. (data: 2.42e-04). ETA=12:23:32, max mem: 34.6 GB 
[11/21 12:46:55 visual_prompt]: 	Training 200/553. train loss: 0.8593,	0.9005 s / batch. (data: 5.39e-03). ETA=12:23:56, max mem: 34.6 GB 
[11/21 12:48:28 visual_prompt]: 	Training 300/553. train loss: 0.6283,	0.9156 s / batch. (data: 2.64e-04). ETA=12:34:56, max mem: 34.6 GB 
[11/21 12:50:02 visual_prompt]: 	Training 400/553. train loss: 0.4721,	0.9165 s / batch. (data: 2.49e-04). ETA=12:34:09, max mem: 34.6 GB 
[11/21 12:51:35 visual_prompt]: 	Training 500/553. train loss: 0.5450,	0.9135 s / batch. (data: 2.54e-04). ETA=12:30:10, max mem: 34.6 GB 
[11/21 12:52:24 visual_prompt]: Epoch 11 / 100: avg data time: 3.73e-02, avg batch time: 0.9599, average train loss: 0.6160
[11/21 12:53:21 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3033, average loss: 0.6569
[11/21 12:53:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.70	rocauc: 70.29	
[11/21 12:53:21 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/21 12:55:03 visual_prompt]: 	Training 100/553. train loss: 0.5008,	0.9635 s / batch. (data: 3.02e-04). ETA=13:08:41, max mem: 34.6 GB 
[11/21 12:56:42 visual_prompt]: 	Training 200/553. train loss: 0.6452,	0.9347 s / batch. (data: 7.90e-04). ETA=12:43:34, max mem: 34.6 GB 
[11/21 12:58:16 visual_prompt]: 	Training 300/553. train loss: 0.6416,	0.8955 s / batch. (data: 2.71e-04). ETA=12:10:03, max mem: 34.6 GB 
[11/21 12:59:49 visual_prompt]: 	Training 400/553. train loss: 0.2918,	0.9357 s / batch. (data: 1.07e-02). ETA=12:41:18, max mem: 34.6 GB 
[11/21 13:01:22 visual_prompt]: 	Training 500/553. train loss: 0.5033,	0.9187 s / batch. (data: 2.51e-04). ETA=12:25:54, max mem: 34.6 GB 
[11/21 13:02:12 visual_prompt]: Epoch 12 / 100: avg data time: 3.65e-02, avg batch time: 0.9598, average train loss: 0.6239
[11/21 13:03:10 visual_prompt]: Inference (val):avg data time: 4.27e-04, avg batch time: 0.3058, average loss: 0.8487
[11/21 13:03:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 68.47	
[11/21 13:03:10 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/21 13:04:57 visual_prompt]: 	Training 100/553. train loss: 0.5721,	0.8939 s / batch. (data: 2.98e-04). ETA=12:03:30, max mem: 34.6 GB 
[11/21 13:06:30 visual_prompt]: 	Training 200/553. train loss: 0.5947,	0.9403 s / batch. (data: 4.29e-03). ETA=12:39:32, max mem: 34.6 GB 
[11/21 13:08:05 visual_prompt]: 	Training 300/553. train loss: 0.7186,	0.9600 s / batch. (data: 3.97e-03). ETA=12:53:48, max mem: 34.6 GB 
[11/21 13:09:41 visual_prompt]: 	Training 400/553. train loss: 0.6405,	0.9219 s / batch. (data: 2.90e-04). ETA=12:21:32, max mem: 34.6 GB 
[11/21 13:11:14 visual_prompt]: 	Training 500/553. train loss: 0.4203,	0.9281 s / batch. (data: 2.45e-04). ETA=12:25:02, max mem: 34.6 GB 
[11/21 13:12:03 visual_prompt]: Epoch 13 / 100: avg data time: 4.05e-02, avg batch time: 0.9634, average train loss: 0.5907
[11/21 13:13:01 visual_prompt]: Inference (val):avg data time: 8.38e-05, avg batch time: 0.3021, average loss: 0.7102
[11/21 13:13:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 70.78	
[11/21 13:13:01 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/21 13:14:47 visual_prompt]: 	Training 100/553. train loss: 0.2668,	0.9330 s / batch. (data: 7.57e-03). ETA=12:26:32, max mem: 34.6 GB 
[11/21 13:16:24 visual_prompt]: 	Training 200/553. train loss: 0.2239,	0.9095 s / batch. (data: 8.54e-04). ETA=12:06:14, max mem: 34.6 GB 
[11/21 13:17:59 visual_prompt]: 	Training 300/553. train loss: 1.0301,	0.9274 s / batch. (data: 6.09e-03). ETA=12:18:59, max mem: 34.6 GB 
[11/21 13:19:31 visual_prompt]: 	Training 400/553. train loss: 0.7373,	0.9280 s / batch. (data: 2.83e-04). ETA=12:17:55, max mem: 34.6 GB 
[11/21 13:21:05 visual_prompt]: 	Training 500/553. train loss: 0.6908,	0.9081 s / batch. (data: 2.78e-04). ETA=12:00:37, max mem: 34.6 GB 
[11/21 13:21:54 visual_prompt]: Epoch 14 / 100: avg data time: 4.13e-02, avg batch time: 0.9636, average train loss: 0.5768
[11/21 13:22:51 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.3038, average loss: 0.9433
[11/21 13:22:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 69.99	
[11/21 13:22:51 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/21 13:24:38 visual_prompt]: 	Training 100/553. train loss: 0.1573,	0.9545 s / batch. (data: 6.48e-03). ETA=12:34:57, max mem: 34.6 GB 
[11/21 13:26:12 visual_prompt]: 	Training 200/553. train loss: 0.2612,	0.9562 s / batch. (data: 1.73e-02). ETA=12:34:45, max mem: 34.6 GB 
[11/21 13:27:46 visual_prompt]: 	Training 300/553. train loss: 0.9284,	0.9008 s / batch. (data: 2.44e-04). ETA=11:49:29, max mem: 34.6 GB 
[11/21 13:29:20 visual_prompt]: 	Training 400/553. train loss: 0.1887,	0.8979 s / batch. (data: 2.92e-04). ETA=11:45:45, max mem: 34.6 GB 
[11/21 13:30:53 visual_prompt]: 	Training 500/553. train loss: 1.1141,	0.9195 s / batch. (data: 7.55e-03). ETA=12:01:10, max mem: 34.6 GB 
[11/21 13:31:42 visual_prompt]: Epoch 15 / 100: avg data time: 3.70e-02, avg batch time: 0.9602, average train loss: 0.5423
[11/21 13:32:39 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3037, average loss: 0.8514
[11/21 13:32:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 66.23	
[11/21 13:32:39 visual_prompt]: Stopping early.
[11/21 13:32:39 visual_prompt]: Rank of current process: 0. World size: 1
[11/21 13:32:39 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 13:32:39 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/21 13:32:39 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/21 13:32:39 visual_prompt]: Training with config:
[11/21 13:32:39 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0001_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/21 13:32:39 visual_prompt]: Loading training data...
[11/21 13:32:39 visual_prompt]: Constructing mammo-cbis dataset train...
[11/21 13:32:39 visual_prompt]: Loading validation data...
[11/21 13:32:39 visual_prompt]: Constructing mammo-cbis dataset val...
[11/21 13:32:39 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/21 13:32:41 visual_prompt]: Enable all parameters update during training
[11/21 13:32:41 visual_prompt]: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/21 13:32:41 visual_prompt]: tuned percent:100.000
[11/21 13:32:41 visual_prompt]: Device used for model: 0
[11/21 13:32:41 visual_prompt]: Setting up Evaluator...
[11/21 13:32:41 visual_prompt]: Setting up Trainer...
[11/21 13:32:41 visual_prompt]: 	Setting up the optimizer...
[11/21 13:32:41 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/21 13:34:23 visual_prompt]: 	Training 100/553. train loss: 10.8600,	0.9444 s / batch. (data: 1.25e-02). ETA=14:28:51, max mem: 34.6 GB 
[11/21 13:36:02 visual_prompt]: 	Training 200/553. train loss: 9.4496,	0.9408 s / batch. (data: 2.82e-04). ETA=14:24:00, max mem: 34.6 GB 
[11/21 13:37:36 visual_prompt]: 	Training 300/553. train loss: 9.8169,	0.9159 s / batch. (data: 2.57e-04). ETA=13:59:32, max mem: 34.6 GB 
[11/21 13:39:09 visual_prompt]: 	Training 400/553. train loss: 7.5668,	0.9240 s / batch. (data: 2.78e-04). ETA=14:05:26, max mem: 34.6 GB 
[11/21 13:40:43 visual_prompt]: 	Training 500/553. train loss: 3.1665,	0.9128 s / batch. (data: 3.99e-03). ETA=13:53:42, max mem: 34.6 GB 
[11/21 13:41:32 visual_prompt]: Epoch 1 / 100: avg data time: 3.52e-02, avg batch time: 0.9599, average train loss: 7.6130
[11/21 13:42:29 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3030, average loss: 6.9126
[11/21 13:42:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/21 13:42:29 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/21 13:44:15 visual_prompt]: 	Training 100/553. train loss: 1.6292,	0.9240 s / batch. (data: 3.24e-04). ETA=14:01:31, max mem: 34.6 GB 
[11/21 13:45:48 visual_prompt]: 	Training 200/553. train loss: 1.0635,	0.9209 s / batch. (data: 2.65e-03). ETA=13:57:11, max mem: 34.6 GB 
[11/21 13:47:23 visual_prompt]: 	Training 300/553. train loss: 0.9988,	0.9280 s / batch. (data: 2.59e-04). ETA=14:02:07, max mem: 34.6 GB 
[11/21 13:49:00 visual_prompt]: 	Training 400/553. train loss: 0.4613,	0.9407 s / batch. (data: 5.38e-03). ETA=14:12:06, max mem: 34.6 GB 
[11/21 13:50:43 visual_prompt]: 	Training 500/553. train loss: 1.1142,	0.9439 s / batch. (data: 1.55e-02). ETA=14:13:21, max mem: 34.6 GB 
[11/21 13:51:32 visual_prompt]: Epoch 2 / 100: avg data time: 5.89e-02, avg batch time: 0.9806, average train loss: 0.9252
[11/21 13:52:30 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3048, average loss: 0.8575
[11/21 13:52:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.38	
[11/21 13:52:30 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/21 13:54:17 visual_prompt]: 	Training 100/553. train loss: 0.7423,	0.9161 s / batch. (data: 2.37e-04). ETA=13:45:57, max mem: 34.6 GB 
[11/21 13:55:55 visual_prompt]: 	Training 200/553. train loss: 1.9635,	0.9331 s / batch. (data: 2.51e-04). ETA=13:59:41, max mem: 34.6 GB 
[11/21 13:57:29 visual_prompt]: 	Training 300/553. train loss: 1.1252,	0.9480 s / batch. (data: 7.86e-04). ETA=14:11:29, max mem: 34.6 GB 
[11/21 13:59:02 visual_prompt]: 	Training 400/553. train loss: 0.3076,	0.9320 s / batch. (data: 7.04e-04). ETA=13:55:35, max mem: 34.6 GB 
[11/21 14:00:41 visual_prompt]: 	Training 500/553. train loss: 0.8465,	0.8945 s / batch. (data: 1.94e-04). ETA=13:20:26, max mem: 34.6 GB 
[11/21 14:01:38 visual_prompt]: Epoch 3 / 100: avg data time: 6.91e-02, avg batch time: 0.9901, average train loss: 0.7973
[11/21 14:02:37 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3041, average loss: 0.7022
[11/21 14:02:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 60.45	
[11/21 14:02:37 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/21 14:04:22 visual_prompt]: 	Training 100/553. train loss: 0.5805,	0.9130 s / batch. (data: 3.10e-04). ETA=13:34:43, max mem: 34.6 GB 
[11/21 14:05:55 visual_prompt]: 	Training 200/553. train loss: 0.9560,	0.9117 s / batch. (data: 3.03e-04). ETA=13:32:04, max mem: 34.6 GB 
[11/21 14:07:31 visual_prompt]: 	Training 300/553. train loss: 0.8010,	0.9355 s / batch. (data: 8.00e-03). ETA=13:51:38, max mem: 34.6 GB 
[11/21 14:09:05 visual_prompt]: 	Training 400/553. train loss: 0.5910,	0.9520 s / batch. (data: 8.00e-03). ETA=14:04:45, max mem: 34.6 GB 
[11/21 14:10:40 visual_prompt]: 	Training 500/553. train loss: 1.3434,	0.9320 s / batch. (data: 2.63e-04). ETA=13:45:27, max mem: 34.6 GB 
[11/21 14:11:31 visual_prompt]: Epoch 4 / 100: avg data time: 4.33e-02, avg batch time: 0.9654, average train loss: 0.7801
[11/21 14:12:28 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3054, average loss: 0.6785
[11/21 14:12:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 64.19	
[11/21 14:12:28 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/21 14:14:11 visual_prompt]: 	Training 100/553. train loss: 0.6244,	0.9120 s / batch. (data: 2.71e-04). ETA=13:25:25, max mem: 34.6 GB 
[11/21 14:15:47 visual_prompt]: 	Training 200/553. train loss: 0.4835,	0.9071 s / batch. (data: 3.22e-04). ETA=13:19:33, max mem: 34.6 GB 
[11/21 14:17:20 visual_prompt]: 	Training 300/553. train loss: 0.8763,	0.9321 s / batch. (data: 7.96e-03). ETA=13:40:03, max mem: 34.6 GB 
[11/21 14:18:54 visual_prompt]: 	Training 400/553. train loss: 0.9416,	1.5451 s / batch. (data: 6.13e-01). ETA=22:36:49, max mem: 34.6 GB 
[11/21 14:20:39 visual_prompt]: 	Training 500/553. train loss: 0.8459,	0.9481 s / batch. (data: 2.99e-02). ETA=13:51:00, max mem: 34.6 GB 
[11/21 14:21:28 visual_prompt]: Epoch 5 / 100: avg data time: 5.40e-02, avg batch time: 0.9761, average train loss: 0.7319
[11/21 14:22:25 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3046, average loss: 0.6640
[11/21 14:22:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 70.24	
[11/21 14:22:25 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/21 14:24:11 visual_prompt]: 	Training 100/553. train loss: 0.5398,	0.9293 s / batch. (data: 5.44e-03). ETA=13:32:07, max mem: 34.6 GB 
[11/21 14:25:45 visual_prompt]: 	Training 200/553. train loss: 0.5671,	0.9168 s / batch. (data: 3.16e-04). ETA=13:19:41, max mem: 34.6 GB 
[11/21 14:27:22 visual_prompt]: 	Training 300/553. train loss: 0.7094,	0.9325 s / batch. (data: 1.05e-02). ETA=13:31:48, max mem: 34.6 GB 
[11/21 14:28:56 visual_prompt]: 	Training 400/553. train loss: 0.9450,	2.2859 s / batch. (data: 1.39e+00). ETA=1 day, 9:06:15, max mem: 34.6 GB 
[11/21 14:30:33 visual_prompt]: 	Training 500/553. train loss: 1.3895,	0.8943 s / batch. (data: 3.02e-04). ETA=12:55:35, max mem: 34.6 GB 
[11/21 14:31:22 visual_prompt]: Epoch 6 / 100: avg data time: 4.98e-02, avg batch time: 0.9712, average train loss: 0.7059
[11/21 14:32:19 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3036, average loss: 0.6699
[11/21 14:32:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 66.19	
[11/21 14:32:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/21 14:34:09 visual_prompt]: 	Training 100/553. train loss: 0.5488,	0.9325 s / batch. (data: 2.45e-04). ETA=13:26:20, max mem: 34.6 GB 
[11/21 14:35:43 visual_prompt]: 	Training 200/553. train loss: 0.5910,	0.9213 s / batch. (data: 3.74e-04). ETA=13:15:08, max mem: 34.6 GB 
[11/21 14:37:16 visual_prompt]: 	Training 300/553. train loss: 0.6046,	0.9235 s / batch. (data: 3.24e-04). ETA=13:15:28, max mem: 34.6 GB 
[11/21 14:38:49 visual_prompt]: 	Training 400/553. train loss: 0.6931,	0.9322 s / batch. (data: 3.07e-04). ETA=13:21:23, max mem: 34.6 GB 
[11/21 14:40:22 visual_prompt]: 	Training 500/553. train loss: 0.6200,	0.9278 s / batch. (data: 2.69e-04). ETA=13:16:02, max mem: 34.6 GB 
[11/21 14:41:11 visual_prompt]: Epoch 7 / 100: avg data time: 3.94e-02, avg batch time: 0.9617, average train loss: 0.6916
[11/21 14:42:08 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3054, average loss: 0.6210
[11/21 14:42:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 73.59	
[11/21 14:42:08 visual_prompt]: Best epoch 7: best metric: -0.621
[11/21 14:42:08 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/21 14:43:53 visual_prompt]: 	Training 100/553. train loss: 0.5034,	0.9055 s / batch. (data: 2.85e-04). ETA=12:54:36, max mem: 34.6 GB 
[11/21 14:45:28 visual_prompt]: 	Training 200/553. train loss: 0.6254,	0.9360 s / batch. (data: 2.68e-04). ETA=13:19:08, max mem: 34.6 GB 

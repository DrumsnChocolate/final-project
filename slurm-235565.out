[09/16 20:12:04 visual_prompt]: Rank of current process: 0. World size: 1
[09/16 20:12:05 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/16 20:12:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed42'], train_type='')
[09/16 20:12:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/16 20:12:05 visual_prompt]: Training with config:
[09/16 20:12:05 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-dmlab',
          'NO_TEST': False,
          'NUMBER_CLASSES': 6,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed42/vtab-dmlab/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/16 20:12:05 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-16 20:12:05.102285: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-16 20:12:05.274876: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-16 20:12:10.065009: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 20:12:10.065110: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 20:12:10.065123: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-16 20:12:22.038929: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 20:12:22.039059: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 20:12:22.039075: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/16 20:12:22 visual_prompt]: Constructing vtab-dmlab dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
2023-09-16 20:12:22.177814: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dmlab for split train[:800]+validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/16 20:12:24 visual_prompt]: Number of images: 1000
[09/16 20:12:24 visual_prompt]: Number of classes: 6 / 6
[09/16 20:12:24 visual_prompt]: Loading validation data...
[09/16 20:12:24 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/16 20:12:25 visual_prompt]: Number of images: 200
[09/16 20:12:25 visual_prompt]: Number of classes: 6 / 6
[09/16 20:12:25 visual_prompt]: Loading test data...
[09/16 20:12:25 visual_prompt]: Constructing vtab-dmlab dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dmlab for split test, from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/16 20:13:08 visual_prompt]: Number of images: 22735
[09/16 20:13:08 visual_prompt]: Number of classes: 6 / 6
[09/16 20:13:08 visual_prompt]: Constructing models...
[09/16 20:13:11 visual_prompt]: Total Parameters: 86724870	 Gradient Parameters: 926214
[09/16 20:13:11 visual_prompt]: tuned percent:1.068
[09/16 20:13:14 visual_prompt]: Device used for model: 0
[09/16 20:13:14 visual_prompt]: Setting up Evalutator...
[09/16 20:13:14 visual_prompt]: Setting up Trainer...
[09/16 20:13:14 visual_prompt]: 	Setting up the optimizer...
[09/16 20:13:14 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/16 20:13:25 visual_prompt]: Epoch 1 / 100: avg data time: 1.42e-01, avg batch time: 0.6189, average train loss: 2.5105
[09/16 20:13:29 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1420, average loss: 2.6066
[09/16 20:13:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 79.50	
[09/16 20:13:50 visual_prompt]: 	Test 100/356. loss: 2.299, 0.1906 s / batch. (data: 1.47e-04)max mem: 17.22445 GB 
[09/16 20:14:10 visual_prompt]: 	Test 200/356. loss: 2.327, 0.1821 s / batch. (data: 1.50e-04)max mem: 17.22445 GB 
[09/16 20:14:29 visual_prompt]: 	Test 300/356. loss: 2.293, 0.1824 s / batch. (data: 3.91e-05)max mem: 17.22445 GB 
[09/16 20:14:41 visual_prompt]: Inference (test):avg data time: 8.07e-03, avg batch time: 0.1929, average loss: 2.4274
[09/16 20:14:42 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 81.44	
[09/16 20:14:42 visual_prompt]: Best epoch 1: best metric: 0.165
[09/16 20:14:42 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/16 20:14:51 visual_prompt]: Epoch 2 / 100: avg data time: 1.25e-01, avg batch time: 0.5282, average train loss: 4.4751
[09/16 20:14:55 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1426, average loss: 2.2932
[09/16 20:14:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/16 20:15:17 visual_prompt]: 	Test 100/356. loss: 2.080, 0.1928 s / batch. (data: 1.03e-02)max mem: 17.22445 GB 
[09/16 20:15:36 visual_prompt]: 	Test 200/356. loss: 2.017, 0.1965 s / batch. (data: 1.35e-02)max mem: 17.22445 GB 
[09/16 20:15:56 visual_prompt]: 	Test 300/356. loss: 2.095, 0.1842 s / batch. (data: 1.71e-04)max mem: 17.22445 GB 
[09/16 20:16:08 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1941, average loss: 2.1395
[09/16 20:16:08 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 88.42	
[09/16 20:16:08 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/16 20:16:19 visual_prompt]: Epoch 3 / 100: avg data time: 1.29e-01, avg batch time: 0.5347, average train loss: 2.3541
[09/16 20:16:23 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1429, average loss: 2.2448
[09/16 20:16:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/16 20:16:44 visual_prompt]: 	Test 100/356. loss: 2.345, 0.2120 s / batch. (data: 2.92e-02)max mem: 17.22445 GB 
[09/16 20:17:03 visual_prompt]: 	Test 200/356. loss: 2.411, 0.1975 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/16 20:17:23 visual_prompt]: 	Test 300/356. loss: 2.344, 0.1959 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/16 20:17:35 visual_prompt]: Inference (test):avg data time: 7.93e-03, avg batch time: 0.1942, average loss: 2.3091
[09/16 20:17:36 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 77.82	
[09/16 20:17:36 visual_prompt]: Best epoch 3: best metric: 0.205
[09/16 20:17:36 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/16 20:17:45 visual_prompt]: Epoch 4 / 100: avg data time: 1.21e-01, avg batch time: 0.5258, average train loss: 2.1804
[09/16 20:17:49 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1427, average loss: 2.3209
[09/16 20:17:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/16 20:18:11 visual_prompt]: 	Test 100/356. loss: 2.446, 0.1930 s / batch. (data: 1.18e-04)max mem: 17.22445 GB 
[09/16 20:18:31 visual_prompt]: 	Test 200/356. loss: 2.352, 0.2009 s / batch. (data: 1.48e-02)max mem: 17.22445 GB 
[09/16 20:18:50 visual_prompt]: 	Test 300/356. loss: 2.386, 0.1994 s / batch. (data: 1.56e-02)max mem: 17.22445 GB 
[09/16 20:19:03 visual_prompt]: Inference (test):avg data time: 7.13e-03, avg batch time: 0.1948, average loss: 2.3524
[09/16 20:19:03 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 88.42	
[09/16 20:19:03 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/16 20:19:13 visual_prompt]: Epoch 5 / 100: avg data time: 1.31e-01, avg batch time: 0.5328, average train loss: 2.2737
[09/16 20:19:17 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1429, average loss: 3.2443
[09/16 20:19:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/16 20:19:38 visual_prompt]: 	Test 100/356. loss: 3.205, 0.2163 s / batch. (data: 3.36e-02)max mem: 17.22445 GB 
[09/16 20:19:58 visual_prompt]: 	Test 200/356. loss: 2.658, 0.2072 s / batch. (data: 2.44e-02)max mem: 17.22445 GB 
[09/16 20:20:17 visual_prompt]: 	Test 300/356. loss: 2.991, 0.1843 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/16 20:20:30 visual_prompt]: Inference (test):avg data time: 8.34e-03, avg batch time: 0.1945, average loss: 3.0526
[09/16 20:20:30 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 82.27	
[09/16 20:20:30 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/16 20:20:40 visual_prompt]: Epoch 6 / 100: avg data time: 1.36e-01, avg batch time: 0.5387, average train loss: 2.4689
[09/16 20:20:44 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1429, average loss: 2.2682
[09/16 20:20:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 82.50	
[09/16 20:21:05 visual_prompt]: 	Test 100/356. loss: 2.361, 0.1835 s / batch. (data: 1.28e-04)max mem: 17.22445 GB 
[09/16 20:21:25 visual_prompt]: 	Test 200/356. loss: 2.319, 0.1869 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/16 20:21:44 visual_prompt]: 	Test 300/356. loss: 2.140, 0.1839 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/16 20:21:56 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1931, average loss: 2.2991
[09/16 20:21:56 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.61	top5: 82.27	
[09/16 20:21:56 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/16 20:22:06 visual_prompt]: Epoch 7 / 100: avg data time: 1.28e-01, avg batch time: 0.5315, average train loss: 3.3816
[09/16 20:22:10 visual_prompt]: Inference (val):avg data time: 4.58e-05, avg batch time: 0.1432, average loss: 5.3089
[09/16 20:22:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/16 20:22:32 visual_prompt]: 	Test 100/356. loss: 5.356, 0.1922 s / batch. (data: 1.33e-04)max mem: 17.22445 GB 
[09/16 20:22:52 visual_prompt]: 	Test 200/356. loss: 5.546, 0.2024 s / batch. (data: 1.46e-02)max mem: 17.22445 GB 
[09/16 20:23:11 visual_prompt]: 	Test 300/356. loss: 5.639, 0.2106 s / batch. (data: 1.38e-02)max mem: 17.22445 GB 
[09/16 20:23:24 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1946, average loss: 5.3731
[09/16 20:23:24 visual_prompt]: Classification results with test_vtab-dmlab: top1: 14.60	top5: 88.42	
[09/16 20:23:24 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/16 20:23:34 visual_prompt]: Epoch 8 / 100: avg data time: 1.22e-01, avg batch time: 0.5306, average train loss: 8.0870
[09/16 20:23:38 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1439, average loss: 3.6877
[09/16 20:23:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/16 20:23:59 visual_prompt]: 	Test 100/356. loss: 4.063, 0.1848 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/16 20:24:19 visual_prompt]: 	Test 200/356. loss: 3.381, 0.1839 s / batch. (data: 1.25e-04)max mem: 17.22445 GB 
[09/16 20:24:38 visual_prompt]: 	Test 300/356. loss: 3.577, 0.1883 s / batch. (data: 9.54e-05)max mem: 17.22445 GB 
[09/16 20:24:50 visual_prompt]: Inference (test):avg data time: 6.58e-03, avg batch time: 0.1934, average loss: 3.7128
[09/16 20:24:50 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 82.27	
[09/16 20:24:50 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/16 20:25:01 visual_prompt]: Epoch 9 / 100: avg data time: 1.43e-01, avg batch time: 0.5458, average train loss: 4.4698
[09/16 20:25:05 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.1458, average loss: 5.0421
[09/16 20:25:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/16 20:25:27 visual_prompt]: 	Test 100/356. loss: 4.530, 0.1979 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/16 20:25:46 visual_prompt]: 	Test 200/356. loss: 5.118, 0.1837 s / batch. (data: 1.04e-04)max mem: 17.22445 GB 
[09/16 20:26:06 visual_prompt]: 	Test 300/356. loss: 4.843, 0.1842 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/16 20:26:18 visual_prompt]: Inference (test):avg data time: 8.16e-03, avg batch time: 0.1950, average loss: 4.9709
[09/16 20:26:18 visual_prompt]: Classification results with test_vtab-dmlab: top1: 11.58	top5: 81.44	
[09/16 20:26:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/16 20:26:28 visual_prompt]: Epoch 10 / 100: avg data time: 1.36e-01, avg batch time: 0.5397, average train loss: 5.8164
[09/16 20:26:32 visual_prompt]: Inference (val):avg data time: 4.89e-05, avg batch time: 0.1430, average loss: 5.3588
[09/16 20:26:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/16 20:26:53 visual_prompt]: 	Test 100/356. loss: 5.751, 0.1835 s / batch. (data: 1.36e-04)max mem: 17.22445 GB 
[09/16 20:27:13 visual_prompt]: 	Test 200/356. loss: 7.680, 0.2003 s / batch. (data: 1.71e-02)max mem: 17.22445 GB 
[09/16 20:27:33 visual_prompt]: 	Test 300/356. loss: 6.462, 0.1900 s / batch. (data: 7.17e-03)max mem: 17.22445 GB 
[09/16 20:27:45 visual_prompt]: Inference (test):avg data time: 6.56e-03, avg batch time: 0.1942, average loss: 6.1437
[09/16 20:27:45 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 77.81	
[09/16 20:27:45 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/16 20:27:55 visual_prompt]: Epoch 11 / 100: avg data time: 1.34e-01, avg batch time: 0.5361, average train loss: 5.7882
[09/16 20:27:59 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1428, average loss: 4.9979
[09/16 20:27:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/16 20:28:21 visual_prompt]: 	Test 100/356. loss: 5.328, 0.2012 s / batch. (data: 1.83e-02)max mem: 17.22445 GB 
[09/16 20:28:40 visual_prompt]: 	Test 200/356. loss: 3.795, 0.2079 s / batch. (data: 2.47e-02)max mem: 17.22445 GB 
[09/16 20:29:00 visual_prompt]: 	Test 300/356. loss: 5.110, 0.1838 s / batch. (data: 1.70e-04)max mem: 17.22445 GB 
[09/16 20:29:12 visual_prompt]: Inference (test):avg data time: 7.07e-03, avg batch time: 0.1945, average loss: 4.7407
[09/16 20:29:12 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 84.67	
[09/16 20:29:12 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/16 20:29:22 visual_prompt]: Epoch 12 / 100: avg data time: 1.35e-01, avg batch time: 0.5362, average train loss: 5.1699
[09/16 20:29:26 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1429, average loss: 3.9494
[09/16 20:29:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/16 20:29:48 visual_prompt]: 	Test 100/356. loss: 4.344, 0.1979 s / batch. (data: 1.54e-02)max mem: 17.22445 GB 
[09/16 20:30:07 visual_prompt]: 	Test 200/356. loss: 3.601, 0.1838 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/16 20:30:27 visual_prompt]: 	Test 300/356. loss: 4.125, 0.1860 s / batch. (data: 4.77e-05)max mem: 17.22445 GB 
[09/16 20:30:39 visual_prompt]: Inference (test):avg data time: 7.66e-03, avg batch time: 0.1934, average loss: 3.9396
[09/16 20:30:39 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 84.67	
[09/16 20:30:39 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/16 20:30:49 visual_prompt]: Epoch 13 / 100: avg data time: 1.22e-01, avg batch time: 0.5266, average train loss: 4.9310
[09/16 20:30:53 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1440, average loss: 4.8105
[09/16 20:30:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.00	
[09/16 20:31:14 visual_prompt]: 	Test 100/356. loss: 5.022, 0.1838 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/16 20:31:34 visual_prompt]: 	Test 200/356. loss: 5.800, 0.1982 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/16 20:31:53 visual_prompt]: 	Test 300/356. loss: 5.614, 0.1846 s / batch. (data: 1.33e-04)max mem: 17.22445 GB 
[09/16 20:32:06 visual_prompt]: Inference (test):avg data time: 7.14e-03, avg batch time: 0.1931, average loss: 5.1612
[09/16 20:32:06 visual_prompt]: Classification results with test_vtab-dmlab: top1: 14.60	top5: 77.86	
[09/16 20:32:06 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/16 20:32:16 visual_prompt]: Epoch 14 / 100: avg data time: 1.36e-01, avg batch time: 0.5497, average train loss: 6.7823
[09/16 20:32:20 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1431, average loss: 4.6455
[09/16 20:32:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/16 20:32:41 visual_prompt]: 	Test 100/356. loss: 5.521, 0.1902 s / batch. (data: 1.06e-04)max mem: 17.22445 GB 
[09/16 20:33:01 visual_prompt]: 	Test 200/356. loss: 6.817, 0.1970 s / batch. (data: 1.34e-02)max mem: 17.22445 GB 
[09/16 20:33:20 visual_prompt]: 	Test 300/356. loss: 6.185, 0.2016 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/16 20:33:33 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1937, average loss: 5.5575
[09/16 20:33:33 visual_prompt]: Classification results with test_vtab-dmlab: top1: 14.61	top5: 77.81	
[09/16 20:33:33 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/16 20:33:43 visual_prompt]: Epoch 15 / 100: avg data time: 1.32e-01, avg batch time: 0.5362, average train loss: 7.0373
[09/16 20:33:47 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.1429, average loss: 5.4978
[09/16 20:33:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/16 20:34:09 visual_prompt]: 	Test 100/356. loss: 5.412, 0.1858 s / batch. (data: 1.09e-04)max mem: 17.22445 GB 
[09/16 20:34:28 visual_prompt]: 	Test 200/356. loss: 5.596, 0.1992 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/16 20:34:48 visual_prompt]: 	Test 300/356. loss: 5.689, 0.2258 s / batch. (data: 4.30e-02)max mem: 17.22445 GB 
[09/16 20:35:00 visual_prompt]: Inference (test):avg data time: 7.89e-03, avg batch time: 0.1943, average loss: 5.4355
[09/16 20:35:00 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 88.42	
[09/16 20:35:00 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/16 20:35:10 visual_prompt]: Epoch 16 / 100: avg data time: 1.26e-01, avg batch time: 0.5293, average train loss: 7.8769
[09/16 20:35:14 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1429, average loss: 7.4448
[09/16 20:35:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/16 20:35:36 visual_prompt]: 	Test 100/356. loss: 6.649, 0.1954 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/16 20:35:55 visual_prompt]: 	Test 200/356. loss: 5.273, 0.1839 s / batch. (data: 1.31e-04)max mem: 17.22445 GB 
[09/16 20:36:15 visual_prompt]: 	Test 300/356. loss: 6.040, 0.1839 s / batch. (data: 1.37e-04)max mem: 17.22445 GB 
[09/16 20:36:27 visual_prompt]: Inference (test):avg data time: 6.57e-03, avg batch time: 0.1932, average loss: 6.4347
[09/16 20:36:27 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 88.42	
[09/16 20:36:27 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/16 20:36:37 visual_prompt]: Epoch 17 / 100: avg data time: 1.14e-01, avg batch time: 0.5206, average train loss: 5.9809
[09/16 20:36:41 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1428, average loss: 5.7559
[09/16 20:36:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/16 20:37:02 visual_prompt]: 	Test 100/356. loss: 5.562, 0.1838 s / batch. (data: 1.60e-04)max mem: 17.22445 GB 
[09/16 20:37:22 visual_prompt]: 	Test 200/356. loss: 4.678, 0.1834 s / batch. (data: 4.86e-05)max mem: 17.22445 GB 
[09/16 20:37:42 visual_prompt]: 	Test 300/356. loss: 5.297, 0.1845 s / batch. (data: 1.54e-04)max mem: 17.22445 GB 
[09/16 20:37:54 visual_prompt]: Inference (test):avg data time: 7.23e-03, avg batch time: 0.1943, average loss: 5.4185
[09/16 20:37:54 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 81.44	
[09/16 20:37:54 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/16 20:38:04 visual_prompt]: Epoch 18 / 100: avg data time: 1.27e-01, avg batch time: 0.5322, average train loss: 3.7649
[09/16 20:38:08 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1429, average loss: 2.2295
[09/16 20:38:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 93.00	
[09/16 20:38:30 visual_prompt]: 	Test 100/356. loss: 2.188, 0.2064 s / batch. (data: 2.34e-02)max mem: 17.22445 GB 
[09/16 20:38:49 visual_prompt]: 	Test 200/356. loss: 1.974, 0.2305 s / batch. (data: 4.67e-02)max mem: 17.22445 GB 
[09/16 20:39:09 visual_prompt]: 	Test 300/356. loss: 1.922, 0.2070 s / batch. (data: 2.35e-02)max mem: 17.22445 GB 
[09/16 20:39:21 visual_prompt]: Inference (test):avg data time: 8.20e-03, avg batch time: 0.1952, average loss: 2.1261
[09/16 20:39:22 visual_prompt]: Classification results with test_vtab-dmlab: top1: 26.68	top5: 89.69	
[09/16 20:39:22 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/16 20:39:32 visual_prompt]: Epoch 19 / 100: avg data time: 1.33e-01, avg batch time: 0.5389, average train loss: 2.3195
[09/16 20:39:36 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1428, average loss: 2.0877
[09/16 20:39:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 90.00	
[09/16 20:39:57 visual_prompt]: 	Test 100/356. loss: 2.068, 0.2036 s / batch. (data: 2.09e-02)max mem: 17.22445 GB 
[09/16 20:40:17 visual_prompt]: 	Test 200/356. loss: 1.774, 0.1832 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/16 20:40:36 visual_prompt]: 	Test 300/356. loss: 1.792, 0.1992 s / batch. (data: 1.60e-02)max mem: 17.22445 GB 
[09/16 20:40:49 visual_prompt]: Inference (test):avg data time: 8.09e-03, avg batch time: 0.1945, average loss: 2.0237
[09/16 20:40:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 27.82	top5: 90.49	
[09/16 20:40:49 visual_prompt]: Best epoch 19: best metric: 0.245
[09/16 20:40:49 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/16 20:40:59 visual_prompt]: Epoch 20 / 100: avg data time: 1.33e-01, avg batch time: 0.5392, average train loss: 2.0538
[09/16 20:41:03 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.1430, average loss: 1.6358
[09/16 20:41:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 97.50	
[09/16 20:41:25 visual_prompt]: 	Test 100/356. loss: 1.511, 0.1840 s / batch. (data: 1.29e-04)max mem: 17.22445 GB 
[09/16 20:41:44 visual_prompt]: 	Test 200/356. loss: 1.785, 0.1836 s / batch. (data: 1.55e-04)max mem: 17.22445 GB 
[09/16 20:42:04 visual_prompt]: 	Test 300/356. loss: 1.682, 0.2079 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/16 20:42:16 visual_prompt]: Inference (test):avg data time: 6.80e-03, avg batch time: 0.1936, average loss: 1.6459
[09/16 20:42:16 visual_prompt]: Classification results with test_vtab-dmlab: top1: 26.07	top5: 96.19	
[09/16 20:42:16 visual_prompt]: Best epoch 20: best metric: 0.290
[09/16 20:42:16 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/16 20:42:26 visual_prompt]: Epoch 21 / 100: avg data time: 1.43e-01, avg batch time: 0.5471, average train loss: 1.8201
[09/16 20:42:30 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1429, average loss: 2.6640
[09/16 20:42:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.50	top5: 84.00	
[09/16 20:42:52 visual_prompt]: 	Test 100/356. loss: 2.110, 0.1838 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/16 20:43:11 visual_prompt]: 	Test 200/356. loss: 2.378, 0.1832 s / batch. (data: 1.27e-04)max mem: 17.22445 GB 
[09/16 20:43:31 visual_prompt]: 	Test 300/356. loss: 2.235, 0.1857 s / batch. (data: 1.41e-04)max mem: 17.22445 GB 
[09/16 20:43:43 visual_prompt]: Inference (test):avg data time: 7.49e-03, avg batch time: 0.1937, average loss: 2.3630
[09/16 20:43:43 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.62	top5: 88.42	
[09/16 20:43:43 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/16 20:43:53 visual_prompt]: Epoch 22 / 100: avg data time: 1.43e-01, avg batch time: 0.5457, average train loss: 1.7667
[09/16 20:43:57 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1430, average loss: 1.5814
[09/16 20:43:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 99.00	
[09/16 20:44:19 visual_prompt]: 	Test 100/356. loss: 1.604, 0.1957 s / batch. (data: 1.32e-02)max mem: 17.22445 GB 
[09/16 20:44:39 visual_prompt]: 	Test 200/356. loss: 1.684, 0.1959 s / batch. (data: 1.35e-04)max mem: 17.22445 GB 
[09/16 20:44:58 visual_prompt]: 	Test 300/356. loss: 1.503, 0.1841 s / batch. (data: 1.30e-04)max mem: 17.22445 GB 
[09/16 20:45:10 visual_prompt]: Inference (test):avg data time: 7.34e-03, avg batch time: 0.1952, average loss: 1.6108
[09/16 20:45:10 visual_prompt]: Classification results with test_vtab-dmlab: top1: 28.52	top5: 96.23	
[09/16 20:45:10 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/16 20:45:21 visual_prompt]: Epoch 23 / 100: avg data time: 1.38e-01, avg batch time: 0.5418, average train loss: 1.6683
[09/16 20:45:24 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1432, average loss: 2.0086
[09/16 20:45:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.00	top5: 79.50	
[09/16 20:45:46 visual_prompt]: 	Test 100/356. loss: 1.940, 0.2041 s / batch. (data: 1.12e-02)max mem: 17.22445 GB 
[09/16 20:46:05 visual_prompt]: 	Test 200/356. loss: 2.317, 0.1922 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/16 20:46:25 visual_prompt]: 	Test 300/356. loss: 1.969, 0.1839 s / batch. (data: 1.53e-04)max mem: 17.22445 GB 
[09/16 20:46:37 visual_prompt]: Inference (test):avg data time: 7.85e-03, avg batch time: 0.1940, average loss: 2.0680
[09/16 20:46:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 29.22	top5: 81.48	
[09/16 20:46:37 visual_prompt]: Best epoch 23: best metric: 0.300
[09/16 20:46:37 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/16 20:46:47 visual_prompt]: Epoch 24 / 100: avg data time: 1.36e-01, avg batch time: 0.5389, average train loss: 1.7446
[09/16 20:46:51 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1438, average loss: 1.8977
[09/16 20:46:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 96.50	
[09/16 20:47:13 visual_prompt]: 	Test 100/356. loss: 1.846, 0.2081 s / batch. (data: 2.53e-02)max mem: 17.22445 GB 
[09/16 20:47:33 visual_prompt]: 	Test 200/356. loss: 2.136, 0.1983 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/16 20:47:52 visual_prompt]: 	Test 300/356. loss: 1.762, 0.1962 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/16 20:48:04 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1939, average loss: 1.9016
[09/16 20:48:04 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.00	top5: 95.68	
[09/16 20:48:04 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/16 20:48:14 visual_prompt]: Epoch 25 / 100: avg data time: 1.30e-01, avg batch time: 0.5315, average train loss: 1.5981
[09/16 20:48:18 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1429, average loss: 1.7426
[09/16 20:48:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 93.50	
[09/16 20:48:40 visual_prompt]: 	Test 100/356. loss: 1.456, 0.1892 s / batch. (data: 5.11e-03)max mem: 17.22445 GB 
[09/16 20:48:59 visual_prompt]: 	Test 200/356. loss: 1.548, 0.1960 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/16 20:49:19 visual_prompt]: 	Test 300/356. loss: 1.748, 0.1838 s / batch. (data: 1.28e-04)max mem: 17.22445 GB 
[09/16 20:49:31 visual_prompt]: Inference (test):avg data time: 7.11e-03, avg batch time: 0.1945, average loss: 1.6823
[09/16 20:49:32 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.22	top5: 95.06	
[09/16 20:49:32 visual_prompt]: Best epoch 25: best metric: 0.320
[09/16 20:49:32 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/16 20:49:41 visual_prompt]: Epoch 26 / 100: avg data time: 1.33e-01, avg batch time: 0.5363, average train loss: 1.8458
[09/16 20:49:45 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1432, average loss: 1.3800
[09/16 20:49:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 99.00	
[09/16 20:50:07 visual_prompt]: 	Test 100/356. loss: 1.349, 0.1831 s / batch. (data: 9.94e-05)max mem: 17.22445 GB 
[09/16 20:50:26 visual_prompt]: 	Test 200/356. loss: 1.630, 0.2040 s / batch. (data: 2.09e-02)max mem: 17.22445 GB 
[09/16 20:50:46 visual_prompt]: 	Test 300/356. loss: 1.436, 0.1836 s / batch. (data: 1.15e-04)max mem: 17.22445 GB 
[09/16 20:50:59 visual_prompt]: Inference (test):avg data time: 8.15e-03, avg batch time: 0.1945, average loss: 1.5039
[09/16 20:50:59 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.12	top5: 97.42	
[09/16 20:50:59 visual_prompt]: Best epoch 26: best metric: 0.425
[09/16 20:50:59 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/16 20:51:09 visual_prompt]: Epoch 27 / 100: avg data time: 1.19e-01, avg batch time: 0.5241, average train loss: 1.5564
[09/16 20:51:12 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1430, average loss: 1.6440
[09/16 20:51:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.50	
[09/16 20:51:34 visual_prompt]: 	Test 100/356. loss: 1.446, 0.1859 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/16 20:51:53 visual_prompt]: 	Test 200/356. loss: 1.540, 0.1990 s / batch. (data: 1.82e-04)max mem: 17.22445 GB 
[09/16 20:52:13 visual_prompt]: 	Test 300/356. loss: 1.507, 0.2304 s / batch. (data: 1.60e-02)max mem: 17.22445 GB 
[09/16 20:52:25 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1942, average loss: 1.6235
[09/16 20:52:26 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.20	top5: 98.14	
[09/16 20:52:26 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/16 20:52:36 visual_prompt]: Epoch 28 / 100: avg data time: 1.32e-01, avg batch time: 0.5373, average train loss: 1.6859
[09/16 20:52:39 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1433, average loss: 1.6337
[09/16 20:52:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 98.50	
[09/16 20:53:01 visual_prompt]: 	Test 100/356. loss: 1.908, 0.2008 s / batch. (data: 1.80e-02)max mem: 17.22445 GB 
[09/16 20:53:20 visual_prompt]: 	Test 200/356. loss: 2.140, 0.2264 s / batch. (data: 2.43e-02)max mem: 17.22445 GB 
[09/16 20:53:40 visual_prompt]: 	Test 300/356. loss: 1.891, 0.1959 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/16 20:53:52 visual_prompt]: Inference (test):avg data time: 7.73e-03, avg batch time: 0.1939, average loss: 1.8675
[09/16 20:53:52 visual_prompt]: Classification results with test_vtab-dmlab: top1: 28.49	top5: 97.62	
[09/16 20:53:52 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/16 20:54:02 visual_prompt]: Epoch 29 / 100: avg data time: 1.37e-01, avg batch time: 0.5392, average train loss: 1.6527
[09/16 20:54:06 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1436, average loss: 2.8009
[09/16 20:54:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 99.00	
[09/16 20:54:28 visual_prompt]: 	Test 100/356. loss: 3.295, 0.1978 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/16 20:54:47 visual_prompt]: 	Test 200/356. loss: 3.010, 0.1958 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/16 20:55:07 visual_prompt]: 	Test 300/356. loss: 2.818, 0.1860 s / batch. (data: 1.18e-04)max mem: 17.22445 GB 
[09/16 20:55:19 visual_prompt]: Inference (test):avg data time: 7.61e-03, avg batch time: 0.1939, average loss: 2.9959
[09/16 20:55:19 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.88	top5: 96.50	
[09/16 20:55:19 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/16 20:55:30 visual_prompt]: Epoch 30 / 100: avg data time: 1.30e-01, avg batch time: 0.5716, average train loss: 1.9038
[09/16 20:55:33 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1431, average loss: 1.5814
[09/16 20:55:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 99.00	
[09/16 20:55:55 visual_prompt]: 	Test 100/356. loss: 1.734, 0.1981 s / batch. (data: 1.52e-02)max mem: 17.22445 GB 
[09/16 20:56:14 visual_prompt]: 	Test 200/356. loss: 2.152, 0.2153 s / batch. (data: 3.25e-02)max mem: 17.22445 GB 
[09/16 20:56:34 visual_prompt]: 	Test 300/356. loss: 1.803, 0.1957 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/16 20:56:46 visual_prompt]: Inference (test):avg data time: 7.90e-03, avg batch time: 0.1934, average loss: 1.8581
[09/16 20:56:46 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.27	top5: 96.34	
[09/16 20:56:46 visual_prompt]: Best epoch 30: best metric: 0.455
[09/16 20:56:46 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/16 20:56:56 visual_prompt]: Epoch 31 / 100: avg data time: 1.27e-01, avg batch time: 0.5308, average train loss: 1.9078
[09/16 20:57:00 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1432, average loss: 1.5960
[09/16 20:57:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.00	
[09/16 20:57:22 visual_prompt]: 	Test 100/356. loss: 1.796, 0.2026 s / batch. (data: 1.97e-02)max mem: 17.22445 GB 
[09/16 20:57:41 visual_prompt]: 	Test 200/356. loss: 2.140, 0.1957 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/16 20:58:01 visual_prompt]: 	Test 300/356. loss: 1.663, 0.1969 s / batch. (data: 1.08e-02)max mem: 17.22445 GB 
[09/16 20:58:13 visual_prompt]: Inference (test):avg data time: 7.66e-03, avg batch time: 0.1944, average loss: 1.8032
[09/16 20:58:13 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.21	top5: 95.00	
[09/16 20:58:13 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/16 20:58:23 visual_prompt]: Epoch 32 / 100: avg data time: 1.36e-01, avg batch time: 0.5394, average train loss: 1.4719
[09/16 20:58:27 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1432, average loss: 1.2867
[09/16 20:58:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 99.00	
[09/16 20:58:49 visual_prompt]: 	Test 100/356. loss: 1.406, 0.1834 s / batch. (data: 1.53e-04)max mem: 17.22445 GB 
[09/16 20:59:08 visual_prompt]: 	Test 200/356. loss: 1.414, 0.1956 s / batch. (data: 1.24e-02)max mem: 17.22445 GB 
[09/16 20:59:28 visual_prompt]: 	Test 300/356. loss: 1.354, 0.1956 s / batch. (data: 1.24e-02)max mem: 17.22445 GB 
[09/16 20:59:40 visual_prompt]: Inference (test):avg data time: 7.60e-03, avg batch time: 0.1934, average loss: 1.4687
[09/16 20:59:40 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.07	top5: 97.87	
[09/16 20:59:40 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/16 20:59:50 visual_prompt]: Epoch 33 / 100: avg data time: 1.29e-01, avg batch time: 0.5313, average train loss: 1.3870
[09/16 20:59:54 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3177, average loss: 1.3783
[09/16 20:59:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 99.50	
[09/16 21:00:16 visual_prompt]: 	Test 100/356. loss: 1.431, 0.1833 s / batch. (data: 1.65e-04)max mem: 17.22445 GB 
[09/16 21:00:35 visual_prompt]: 	Test 200/356. loss: 1.886, 0.1830 s / batch. (data: 1.47e-04)max mem: 17.22445 GB 
[09/16 21:00:55 visual_prompt]: 	Test 300/356. loss: 1.566, 0.1940 s / batch. (data: 1.30e-04)max mem: 17.22445 GB 
[09/16 21:01:07 visual_prompt]: Inference (test):avg data time: 6.86e-03, avg batch time: 0.1930, average loss: 1.6087
[09/16 21:01:07 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.58	top5: 97.32	
[09/16 21:01:07 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/16 21:01:17 visual_prompt]: Epoch 34 / 100: avg data time: 1.29e-01, avg batch time: 0.5350, average train loss: 1.3347
[09/16 21:01:21 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1432, average loss: 1.3952
[09/16 21:01:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 99.00	
[09/16 21:01:43 visual_prompt]: 	Test 100/356. loss: 1.467, 0.1838 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/16 21:02:03 visual_prompt]: 	Test 200/356. loss: 1.823, 0.2377 s / batch. (data: 4.71e-02)max mem: 17.22445 GB 
[09/16 21:02:23 visual_prompt]: 	Test 300/356. loss: 1.574, 0.1838 s / batch. (data: 1.10e-04)max mem: 17.22445 GB 
[09/16 21:02:36 visual_prompt]: Inference (test):avg data time: 8.63e-03, avg batch time: 0.1982, average loss: 1.6236
[09/16 21:02:36 visual_prompt]: Classification results with test_vtab-dmlab: top1: 28.04	top5: 96.75	
[09/16 21:02:36 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/16 21:02:46 visual_prompt]: Epoch 35 / 100: avg data time: 1.29e-01, avg batch time: 0.5339, average train loss: 1.3097
[09/16 21:02:50 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1432, average loss: 1.4054
[09/16 21:02:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 98.00	
[09/16 21:03:11 visual_prompt]: 	Test 100/356. loss: 1.465, 0.2092 s / batch. (data: 2.62e-02)max mem: 17.22445 GB 
[09/16 21:03:31 visual_prompt]: 	Test 200/356. loss: 1.419, 0.1972 s / batch. (data: 1.45e-02)max mem: 17.22445 GB 
[09/16 21:03:50 visual_prompt]: 	Test 300/356. loss: 1.391, 0.1915 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/16 21:04:03 visual_prompt]: Inference (test):avg data time: 7.30e-03, avg batch time: 0.1949, average loss: 1.5591
[09/16 21:04:03 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.05	top5: 96.78	
[09/16 21:04:03 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/16 21:04:13 visual_prompt]: Epoch 36 / 100: avg data time: 1.31e-01, avg batch time: 0.5317, average train loss: 1.5435
[09/16 21:04:17 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1430, average loss: 1.4276
[09/16 21:04:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 98.00	
[09/16 21:04:39 visual_prompt]: 	Test 100/356. loss: 1.614, 0.1939 s / batch. (data: 1.12e-02)max mem: 17.22445 GB 
[09/16 21:04:58 visual_prompt]: 	Test 200/356. loss: 1.662, 0.2194 s / batch. (data: 1.38e-02)max mem: 17.22445 GB 
[09/16 21:05:18 visual_prompt]: 	Test 300/356. loss: 1.437, 0.1962 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/16 21:05:30 visual_prompt]: Inference (test):avg data time: 7.60e-03, avg batch time: 0.1950, average loss: 1.6430
[09/16 21:05:31 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.32	top5: 96.81	
[09/16 21:05:31 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/16 21:05:40 visual_prompt]: Epoch 37 / 100: avg data time: 1.32e-01, avg batch time: 0.5344, average train loss: 1.7114
[09/16 21:05:44 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.1429, average loss: 1.3143
[09/16 21:05:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 98.50	
[09/16 21:06:06 visual_prompt]: 	Test 100/356. loss: 1.539, 0.1956 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/16 21:06:26 visual_prompt]: 	Test 200/356. loss: 1.739, 0.2111 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/16 21:06:45 visual_prompt]: 	Test 300/356. loss: 1.437, 0.1838 s / batch. (data: 1.65e-04)max mem: 17.22445 GB 
[09/16 21:06:58 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1958, average loss: 1.5383
[09/16 21:06:58 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.72	top5: 97.84	
[09/16 21:06:58 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/16 21:07:08 visual_prompt]: Epoch 38 / 100: avg data time: 1.20e-01, avg batch time: 0.5266, average train loss: 1.4660
[09/16 21:07:12 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1432, average loss: 1.3384
[09/16 21:07:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 99.50	
[09/16 21:07:34 visual_prompt]: 	Test 100/356. loss: 1.547, 0.1865 s / batch. (data: 1.25e-04)max mem: 17.22445 GB 
[09/16 21:07:53 visual_prompt]: 	Test 200/356. loss: 1.447, 0.1992 s / batch. (data: 1.13e-04)max mem: 17.22445 GB 
[09/16 21:08:13 visual_prompt]: 	Test 300/356. loss: 1.479, 0.2125 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/16 21:08:25 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1954, average loss: 1.5592
[09/16 21:08:26 visual_prompt]: Classification results with test_vtab-dmlab: top1: 33.27	top5: 97.68	
[09/16 21:08:26 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/16 21:08:36 visual_prompt]: Epoch 39 / 100: avg data time: 1.35e-01, avg batch time: 0.5362, average train loss: 1.3357
[09/16 21:08:39 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1435, average loss: 1.6996
[09/16 21:08:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 93.50	
[09/16 21:09:01 visual_prompt]: 	Test 100/356. loss: 1.902, 0.1831 s / batch. (data: 1.29e-04)max mem: 17.22445 GB 
[09/16 21:09:21 visual_prompt]: 	Test 200/356. loss: 1.799, 0.2081 s / batch. (data: 2.53e-02)max mem: 17.22445 GB 
[09/16 21:09:40 visual_prompt]: 	Test 300/356. loss: 1.656, 0.1893 s / batch. (data: 1.35e-04)max mem: 17.22445 GB 
[09/16 21:09:52 visual_prompt]: Inference (test):avg data time: 8.44e-03, avg batch time: 0.1935, average loss: 1.9057
[09/16 21:09:52 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.35	top5: 93.93	
[09/16 21:09:52 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/16 21:10:02 visual_prompt]: Epoch 40 / 100: avg data time: 1.35e-01, avg batch time: 0.5374, average train loss: 1.4625
[09/16 21:10:06 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1433, average loss: 1.6209
[09/16 21:10:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 99.50	
[09/16 21:10:28 visual_prompt]: 	Test 100/356. loss: 2.061, 0.2032 s / batch. (data: 2.03e-02)max mem: 17.22445 GB 
[09/16 21:10:47 visual_prompt]: 	Test 200/356. loss: 1.862, 0.2030 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/16 21:11:06 visual_prompt]: 	Test 300/356. loss: 1.623, 0.1997 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/16 21:11:19 visual_prompt]: Inference (test):avg data time: 8.00e-03, avg batch time: 0.1937, average loss: 1.8644
[09/16 21:11:19 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.67	top5: 98.27	
[09/16 21:11:19 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/16 21:11:29 visual_prompt]: Epoch 41 / 100: avg data time: 1.34e-01, avg batch time: 0.5358, average train loss: 1.4210
[09/16 21:11:33 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.1432, average loss: 1.5565
[09/16 21:11:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.00	top5: 99.00	
[09/16 21:11:54 visual_prompt]: 	Test 100/356. loss: 1.732, 0.1973 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/16 21:12:14 visual_prompt]: 	Test 200/356. loss: 2.084, 0.1956 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/16 21:12:33 visual_prompt]: 	Test 300/356. loss: 1.620, 0.2159 s / batch. (data: 1.55e-02)max mem: 17.22445 GB 
[09/16 21:12:46 visual_prompt]: Inference (test):avg data time: 8.11e-03, avg batch time: 0.1935, average loss: 1.8211
[09/16 21:12:46 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.41	top5: 97.69	
[09/16 21:12:46 visual_prompt]: Best epoch 41: best metric: 0.470
[09/16 21:12:46 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/16 21:12:56 visual_prompt]: Epoch 42 / 100: avg data time: 1.37e-01, avg batch time: 0.5408, average train loss: 1.3833
[09/16 21:13:00 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.1465, average loss: 1.2226
[09/16 21:13:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 98.50	
[09/16 21:13:21 visual_prompt]: 	Test 100/356. loss: 1.394, 0.1829 s / batch. (data: 1.52e-04)max mem: 17.22445 GB 
[09/16 21:13:41 visual_prompt]: 	Test 200/356. loss: 1.823, 0.1844 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/16 21:14:01 visual_prompt]: 	Test 300/356. loss: 1.347, 0.1844 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/16 21:14:13 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1944, average loss: 1.4868
[09/16 21:14:13 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.97	top5: 97.96	
[09/16 21:14:13 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/16 21:14:23 visual_prompt]: Epoch 43 / 100: avg data time: 1.29e-01, avg batch time: 0.5312, average train loss: 1.2733
[09/16 21:14:27 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1431, average loss: 1.9165
[09/16 21:14:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 99.00	
[09/16 21:14:49 visual_prompt]: 	Test 100/356. loss: 2.002, 0.1959 s / batch. (data: 1.31e-02)max mem: 17.22445 GB 
[09/16 21:15:08 visual_prompt]: 	Test 200/356. loss: 1.837, 0.1844 s / batch. (data: 1.18e-04)max mem: 17.22445 GB 
[09/16 21:15:28 visual_prompt]: 	Test 300/356. loss: 1.894, 0.1835 s / batch. (data: 1.18e-04)max mem: 17.22445 GB 
[09/16 21:15:41 visual_prompt]: Inference (test):avg data time: 8.22e-03, avg batch time: 0.1965, average loss: 2.0683
[09/16 21:15:41 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.00	top5: 97.77	
[09/16 21:15:41 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/16 21:15:51 visual_prompt]: Epoch 44 / 100: avg data time: 1.27e-01, avg batch time: 0.5286, average train loss: 1.4867
[09/16 21:15:55 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1431, average loss: 1.4519
[09/16 21:15:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 99.00	
[09/16 21:16:17 visual_prompt]: 	Test 100/356. loss: 1.556, 0.1829 s / batch. (data: 1.50e-04)max mem: 17.22445 GB 
[09/16 21:16:37 visual_prompt]: 	Test 200/356. loss: 1.460, 0.1836 s / batch. (data: 1.37e-04)max mem: 17.22445 GB 
[09/16 21:16:56 visual_prompt]: 	Test 300/356. loss: 1.489, 0.1840 s / batch. (data: 1.52e-04)max mem: 17.22445 GB 
[09/16 21:17:09 visual_prompt]: Inference (test):avg data time: 8.19e-03, avg batch time: 0.1958, average loss: 1.6334
[09/16 21:17:09 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.57	top5: 97.91	
[09/16 21:17:09 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/16 21:17:19 visual_prompt]: Epoch 45 / 100: avg data time: 1.19e-01, avg batch time: 0.5228, average train loss: 1.2783
[09/16 21:17:23 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.1455, average loss: 1.1686
[09/16 21:17:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 48.00	top5: 99.50	
[09/16 21:17:44 visual_prompt]: 	Test 100/356. loss: 1.402, 0.1951 s / batch. (data: 1.22e-02)max mem: 17.22445 GB 
[09/16 21:18:04 visual_prompt]: 	Test 200/356. loss: 1.419, 0.1955 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/16 21:18:23 visual_prompt]: 	Test 300/356. loss: 1.284, 0.2001 s / batch. (data: 1.38e-04)max mem: 17.22445 GB 
[09/16 21:18:35 visual_prompt]: Inference (test):avg data time: 7.62e-03, avg batch time: 0.1934, average loss: 1.4630
[09/16 21:18:36 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.56	top5: 98.13	
[09/16 21:18:36 visual_prompt]: Best epoch 45: best metric: 0.480
[09/16 21:18:36 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/16 21:18:45 visual_prompt]: Epoch 46 / 100: avg data time: 1.23e-01, avg batch time: 0.5281, average train loss: 1.3410
[09/16 21:18:49 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1432, average loss: 1.1955
[09/16 21:18:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 48.50	top5: 99.00	
[09/16 21:19:11 visual_prompt]: 	Test 100/356. loss: 1.668, 0.1831 s / batch. (data: 1.42e-04)max mem: 17.22445 GB 
[09/16 21:19:30 visual_prompt]: 	Test 200/356. loss: 1.913, 0.1888 s / batch. (data: 1.45e-04)max mem: 17.22445 GB 
[09/16 21:19:50 visual_prompt]: 	Test 300/356. loss: 1.363, 0.1957 s / batch. (data: 1.24e-02)max mem: 17.22445 GB 
[09/16 21:20:02 visual_prompt]: Inference (test):avg data time: 6.80e-03, avg batch time: 0.1930, average loss: 1.5905
[09/16 21:20:02 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.68	top5: 96.83	
[09/16 21:20:02 visual_prompt]: Best epoch 46: best metric: 0.485
[09/16 21:20:02 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/16 21:20:12 visual_prompt]: Epoch 47 / 100: avg data time: 1.14e-01, avg batch time: 0.5199, average train loss: 1.2506
[09/16 21:20:16 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1431, average loss: 1.2513
[09/16 21:20:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 99.00	
[09/16 21:20:37 visual_prompt]: 	Test 100/356. loss: 1.572, 0.1835 s / batch. (data: 1.36e-04)max mem: 17.22445 GB 
[09/16 21:20:57 visual_prompt]: 	Test 200/356. loss: 1.546, 0.1967 s / batch. (data: 1.92e-04)max mem: 17.22445 GB 
[09/16 21:21:16 visual_prompt]: 	Test 300/356. loss: 1.368, 0.1971 s / batch. (data: 1.40e-02)max mem: 17.22445 GB 
[09/16 21:21:28 visual_prompt]: Inference (test):avg data time: 7.00e-03, avg batch time: 0.1931, average loss: 1.5397
[09/16 21:21:28 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.33	top5: 98.21	
[09/16 21:21:28 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/16 21:21:38 visual_prompt]: Epoch 48 / 100: avg data time: 1.25e-01, avg batch time: 0.5282, average train loss: 1.2036
[09/16 21:21:42 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1432, average loss: 1.3103
[09/16 21:21:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 99.50	
[09/16 21:22:05 visual_prompt]: 	Test 100/356. loss: 1.761, 0.1986 s / batch. (data: 9.61e-05)max mem: 17.22445 GB 
[09/16 21:22:24 visual_prompt]: 	Test 200/356. loss: 1.701, 0.2083 s / batch. (data: 2.51e-02)max mem: 17.22445 GB 
[09/16 21:22:44 visual_prompt]: 	Test 300/356. loss: 1.367, 0.1842 s / batch. (data: 1.77e-04)max mem: 17.22445 GB 
[09/16 21:22:56 visual_prompt]: Inference (test):avg data time: 8.14e-03, avg batch time: 0.1960, average loss: 1.6124
[09/16 21:22:56 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.78	top5: 97.92	
[09/16 21:22:56 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/16 21:23:06 visual_prompt]: Epoch 49 / 100: avg data time: 1.23e-01, avg batch time: 0.5274, average train loss: 1.2029
[09/16 21:23:09 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1438, average loss: 1.1539
[09/16 21:23:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 49.50	top5: 99.50	
[09/16 21:23:31 visual_prompt]: 	Test 100/356. loss: 1.557, 0.1834 s / batch. (data: 1.33e-04)max mem: 17.22445 GB 
[09/16 21:23:51 visual_prompt]: 	Test 200/356. loss: 1.738, 0.1840 s / batch. (data: 1.30e-04)max mem: 17.22445 GB 
[09/16 21:24:10 visual_prompt]: 	Test 300/356. loss: 1.438, 0.2086 s / batch. (data: 1.62e-02)max mem: 17.22445 GB 
[09/16 21:24:22 visual_prompt]: Inference (test):avg data time: 8.00e-03, avg batch time: 0.1938, average loss: 1.5767
[09/16 21:24:22 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.78	top5: 96.99	
[09/16 21:24:22 visual_prompt]: Best epoch 49: best metric: 0.495
[09/16 21:24:22 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/16 21:24:33 visual_prompt]: Epoch 50 / 100: avg data time: 1.39e-01, avg batch time: 0.5686, average train loss: 1.0794
[09/16 21:24:37 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1432, average loss: 1.3318
[09/16 21:24:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 49.00	top5: 99.50	
[09/16 21:24:58 visual_prompt]: 	Test 100/356. loss: 1.622, 0.1840 s / batch. (data: 1.01e-04)max mem: 17.22445 GB 
[09/16 21:25:18 visual_prompt]: 	Test 200/356. loss: 1.840, 0.1999 s / batch. (data: 1.68e-02)max mem: 17.22445 GB 
[09/16 21:25:38 visual_prompt]: 	Test 300/356. loss: 1.402, 0.1995 s / batch. (data: 1.61e-02)max mem: 17.22445 GB 
[09/16 21:25:50 visual_prompt]: Inference (test):avg data time: 7.83e-03, avg batch time: 0.1947, average loss: 1.6680
[09/16 21:25:50 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.48	top5: 97.04	
[09/16 21:25:50 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/16 21:26:00 visual_prompt]: Epoch 51 / 100: avg data time: 1.43e-01, avg batch time: 0.5455, average train loss: 1.1803
[09/16 21:26:04 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1432, average loss: 1.1758
[09/16 21:26:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 100.00	
[09/16 21:26:26 visual_prompt]: 	Test 100/356. loss: 1.605, 0.2013 s / batch. (data: 1.52e-02)max mem: 17.22445 GB 
[09/16 21:26:46 visual_prompt]: 	Test 200/356. loss: 1.903, 0.1853 s / batch. (data: 1.58e-04)max mem: 17.22445 GB 
[09/16 21:27:05 visual_prompt]: 	Test 300/356. loss: 1.479, 0.1839 s / batch. (data: 1.28e-04)max mem: 17.22445 GB 
[09/16 21:27:18 visual_prompt]: Inference (test):avg data time: 7.50e-03, avg batch time: 0.1947, average loss: 1.6066
[09/16 21:27:18 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.04	top5: 97.33	
[09/16 21:27:18 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/16 21:27:28 visual_prompt]: Epoch 52 / 100: avg data time: 1.10e-01, avg batch time: 0.5168, average train loss: 1.0974
[09/16 21:27:31 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1434, average loss: 1.1342
[09/16 21:27:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 100.00	
[09/16 21:27:53 visual_prompt]: 	Test 100/356. loss: 1.550, 0.1937 s / batch. (data: 1.12e-02)max mem: 17.22445 GB 
[09/16 21:28:12 visual_prompt]: 	Test 200/356. loss: 2.109, 0.1838 s / batch. (data: 1.19e-04)max mem: 17.22445 GB 
[09/16 21:28:32 visual_prompt]: 	Test 300/356. loss: 1.609, 0.1902 s / batch. (data: 1.10e-04)max mem: 17.22445 GB 
[09/16 21:28:45 visual_prompt]: Inference (test):avg data time: 7.54e-03, avg batch time: 0.1942, average loss: 1.7397
[09/16 21:28:45 visual_prompt]: Classification results with test_vtab-dmlab: top1: 30.61	top5: 97.46	
[09/16 21:28:45 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/16 21:28:54 visual_prompt]: Epoch 53 / 100: avg data time: 1.24e-01, avg batch time: 0.5272, average train loss: 1.1042
[09/16 21:28:59 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1432, average loss: 1.5282
[09/16 21:28:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 97.50	
[09/16 21:29:20 visual_prompt]: 	Test 100/356. loss: 1.893, 0.1963 s / batch. (data: 1.35e-02)max mem: 17.22445 GB 
[09/16 21:29:39 visual_prompt]: 	Test 200/356. loss: 2.010, 0.1977 s / batch. (data: 1.40e-02)max mem: 17.22445 GB 
[09/16 21:29:59 visual_prompt]: 	Test 300/356. loss: 1.737, 0.1855 s / batch. (data: 1.54e-04)max mem: 17.22445 GB 
[09/16 21:30:12 visual_prompt]: Inference (test):avg data time: 7.76e-03, avg batch time: 0.1942, average loss: 1.9111
[09/16 21:30:12 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.57	top5: 97.20	
[09/16 21:30:12 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/16 21:30:22 visual_prompt]: Epoch 54 / 100: avg data time: 1.42e-01, avg batch time: 0.5449, average train loss: 1.1534
[09/16 21:30:26 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1432, average loss: 0.8997
[09/16 21:30:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 50.00	top5: 99.50	
[09/16 21:30:47 visual_prompt]: 	Test 100/356. loss: 1.623, 0.2032 s / batch. (data: 1.10e-02)max mem: 17.22445 GB 
[09/16 21:31:07 visual_prompt]: 	Test 200/356. loss: 1.620, 0.1840 s / batch. (data: 1.41e-04)max mem: 17.22445 GB 
[09/16 21:31:26 visual_prompt]: 	Test 300/356. loss: 1.402, 0.1995 s / batch. (data: 1.59e-02)max mem: 17.22445 GB 
[09/16 21:31:38 visual_prompt]: Inference (test):avg data time: 7.32e-03, avg batch time: 0.1934, average loss: 1.5285
[09/16 21:31:39 visual_prompt]: Classification results with test_vtab-dmlab: top1: 33.70	top5: 97.74	
[09/16 21:31:39 visual_prompt]: Best epoch 54: best metric: 0.500
[09/16 21:31:39 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/16 21:31:49 visual_prompt]: Epoch 55 / 100: avg data time: 1.36e-01, avg batch time: 0.5381, average train loss: 1.0978
[09/16 21:31:53 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1430, average loss: 0.9830
[09/16 21:31:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 52.50	top5: 99.50	
[09/16 21:32:14 visual_prompt]: 	Test 100/356. loss: 1.815, 0.1977 s / batch. (data: 1.52e-02)max mem: 17.22445 GB 
[09/16 21:32:34 visual_prompt]: 	Test 200/356. loss: 1.934, 0.1974 s / batch. (data: 1.07e-04)max mem: 17.22445 GB 
[09/16 21:32:53 visual_prompt]: 	Test 300/356. loss: 1.452, 0.1839 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/16 21:33:06 visual_prompt]: Inference (test):avg data time: 7.98e-03, avg batch time: 0.1940, average loss: 1.7052
[09/16 21:33:06 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.00	top5: 96.16	
[09/16 21:33:06 visual_prompt]: Best epoch 55: best metric: 0.525
[09/16 21:33:06 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/16 21:33:16 visual_prompt]: Epoch 56 / 100: avg data time: 1.38e-01, avg batch time: 0.5423, average train loss: 1.0873
[09/16 21:33:20 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.1433, average loss: 1.1805
[09/16 21:33:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 48.00	top5: 99.00	
[09/16 21:33:42 visual_prompt]: 	Test 100/356. loss: 1.712, 0.1903 s / batch. (data: 1.01e-04)max mem: 17.22445 GB 
[09/16 21:34:01 visual_prompt]: 	Test 200/356. loss: 1.799, 0.2078 s / batch. (data: 2.49e-02)max mem: 17.22445 GB 
[09/16 21:34:21 visual_prompt]: 	Test 300/356. loss: 1.700, 0.1968 s / batch. (data: 1.65e-04)max mem: 17.22445 GB 
[09/16 21:34:33 visual_prompt]: Inference (test):avg data time: 8.58e-03, avg batch time: 0.1947, average loss: 1.7309
[09/16 21:34:33 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.34	top5: 96.65	
[09/16 21:34:33 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/16 21:34:43 visual_prompt]: Epoch 57 / 100: avg data time: 1.23e-01, avg batch time: 0.5292, average train loss: 1.0071
[09/16 21:34:47 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1432, average loss: 0.7934
[09/16 21:34:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 63.00	top5: 100.00	
[09/16 21:35:09 visual_prompt]: 	Test 100/356. loss: 1.521, 0.1901 s / batch. (data: 1.12e-04)max mem: 17.22445 GB 
[09/16 21:35:28 visual_prompt]: 	Test 200/356. loss: 1.522, 0.1980 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/16 21:35:48 visual_prompt]: 	Test 300/356. loss: 1.312, 0.1896 s / batch. (data: 5.22e-03)max mem: 17.22445 GB 
[09/16 21:36:00 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1935, average loss: 1.5109
[09/16 21:36:00 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.30	top5: 97.70	
[09/16 21:36:00 visual_prompt]: Best epoch 57: best metric: 0.630
[09/16 21:36:00 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/16 21:36:10 visual_prompt]: Epoch 58 / 100: avg data time: 1.33e-01, avg batch time: 0.5382, average train loss: 0.8590
[09/16 21:36:14 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1432, average loss: 0.7954
[09/16 21:36:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 63.00	top5: 100.00	
[09/16 21:36:36 visual_prompt]: 	Test 100/356. loss: 1.884, 0.1864 s / batch. (data: 1.32e-04)max mem: 17.22445 GB 
[09/16 21:36:56 visual_prompt]: 	Test 200/356. loss: 1.602, 0.1970 s / batch. (data: 1.41e-02)max mem: 17.22445 GB 
[09/16 21:37:15 visual_prompt]: 	Test 300/356. loss: 1.606, 0.1958 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/16 21:37:28 visual_prompt]: Inference (test):avg data time: 6.87e-03, avg batch time: 0.1952, average loss: 1.7474
[09/16 21:37:28 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.89	top5: 97.21	
[09/16 21:37:28 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/16 21:37:38 visual_prompt]: Epoch 59 / 100: avg data time: 1.39e-01, avg batch time: 0.5419, average train loss: 0.9759
[09/16 21:37:42 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1447, average loss: 0.8750
[09/16 21:37:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 61.00	top5: 100.00	
[09/16 21:38:04 visual_prompt]: 	Test 100/356. loss: 1.561, 0.1947 s / batch. (data: 1.68e-04)max mem: 17.22445 GB 
[09/16 21:38:23 visual_prompt]: 	Test 200/356. loss: 1.610, 0.2104 s / batch. (data: 2.62e-02)max mem: 17.22445 GB 
[09/16 21:38:43 visual_prompt]: 	Test 300/356. loss: 1.288, 0.1978 s / batch. (data: 1.27e-04)max mem: 17.22445 GB 
[09/16 21:38:55 visual_prompt]: Inference (test):avg data time: 7.06e-03, avg batch time: 0.1946, average loss: 1.5009
[09/16 21:38:55 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.26	top5: 97.02	
[09/16 21:38:55 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/16 21:39:05 visual_prompt]: Epoch 60 / 100: avg data time: 1.33e-01, avg batch time: 0.5366, average train loss: 0.9002
[09/16 21:39:09 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1435, average loss: 0.8244
[09/16 21:39:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 58.00	top5: 100.00	
[09/16 21:39:31 visual_prompt]: 	Test 100/356. loss: 1.763, 0.2208 s / batch. (data: 3.68e-02)max mem: 17.22445 GB 
[09/16 21:39:50 visual_prompt]: 	Test 200/356. loss: 1.876, 0.1909 s / batch. (data: 7.37e-03)max mem: 17.22445 GB 
[09/16 21:40:10 visual_prompt]: 	Test 300/356. loss: 1.479, 0.1842 s / batch. (data: 1.46e-04)max mem: 17.22445 GB 
[09/16 21:40:23 visual_prompt]: Inference (test):avg data time: 9.41e-03, avg batch time: 0.1949, average loss: 1.7126
[09/16 21:40:23 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.87	top5: 97.35	
[09/16 21:40:23 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/16 21:40:33 visual_prompt]: Epoch 61 / 100: avg data time: 1.39e-01, avg batch time: 0.5418, average train loss: 0.8957
[09/16 21:40:37 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1433, average loss: 0.9720
[09/16 21:40:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 57.00	top5: 100.00	
[09/16 21:40:58 visual_prompt]: 	Test 100/356. loss: 1.895, 0.1925 s / batch. (data: 1.31e-04)max mem: 17.22445 GB 
[09/16 21:41:18 visual_prompt]: 	Test 200/356. loss: 1.657, 0.1838 s / batch. (data: 1.28e-04)max mem: 17.22445 GB 
[09/16 21:41:37 visual_prompt]: 	Test 300/356. loss: 1.288, 0.1963 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/16 21:41:50 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1939, average loss: 1.6746
[09/16 21:41:50 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.82	top5: 97.54	
[09/16 21:41:50 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/16 21:42:00 visual_prompt]: Epoch 62 / 100: avg data time: 1.24e-01, avg batch time: 0.5303, average train loss: 0.9161
[09/16 21:42:04 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1434, average loss: 0.7979
[09/16 21:42:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 62.50	top5: 99.50	
[09/16 21:42:26 visual_prompt]: 	Test 100/356. loss: 1.917, 0.2299 s / batch. (data: 3.62e-02)max mem: 17.22445 GB 
[09/16 21:42:45 visual_prompt]: 	Test 200/356. loss: 1.781, 0.1967 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/16 21:43:05 visual_prompt]: 	Test 300/356. loss: 1.767, 0.1961 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/16 21:43:17 visual_prompt]: Inference (test):avg data time: 8.98e-03, avg batch time: 0.1952, average loss: 1.7664
[09/16 21:43:17 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.25	top5: 94.56	
[09/16 21:43:17 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/16 21:43:27 visual_prompt]: Epoch 63 / 100: avg data time: 1.41e-01, avg batch time: 0.5442, average train loss: 0.9779
[09/16 21:43:31 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1438, average loss: 0.8310
[09/16 21:43:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 60.00	top5: 100.00	
[09/16 21:43:53 visual_prompt]: 	Test 100/356. loss: 1.613, 0.1861 s / batch. (data: 1.19e-04)max mem: 17.22445 GB 
[09/16 21:44:13 visual_prompt]: 	Test 200/356. loss: 1.546, 0.2076 s / batch. (data: 2.46e-02)max mem: 17.22445 GB 
[09/16 21:44:32 visual_prompt]: 	Test 300/356. loss: 1.229, 0.1840 s / batch. (data: 1.36e-04)max mem: 17.22445 GB 
[09/16 21:44:45 visual_prompt]: Inference (test):avg data time: 8.47e-03, avg batch time: 0.1946, average loss: 1.5468
[09/16 21:44:45 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.84	top5: 95.23	
[09/16 21:44:45 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/16 21:44:55 visual_prompt]: Epoch 64 / 100: avg data time: 1.37e-01, avg batch time: 0.5423, average train loss: 0.8303
[09/16 21:44:59 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1434, average loss: 0.9072
[09/16 21:44:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 61.00	top5: 100.00	
[09/16 21:45:20 visual_prompt]: 	Test 100/356. loss: 2.247, 0.2110 s / batch. (data: 6.75e-05)max mem: 17.22445 GB 
[09/16 21:45:40 visual_prompt]: 	Test 200/356. loss: 1.989, 0.1980 s / batch. (data: 1.49e-02)max mem: 17.22445 GB 
[09/16 21:45:59 visual_prompt]: 	Test 300/356. loss: 1.768, 0.1844 s / batch. (data: 1.19e-04)max mem: 17.22445 GB 
[09/16 21:46:12 visual_prompt]: Inference (test):avg data time: 8.28e-03, avg batch time: 0.1941, average loss: 2.0758
[09/16 21:46:12 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.63	top5: 96.88	
[09/16 21:46:12 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/16 21:46:22 visual_prompt]: Epoch 65 / 100: avg data time: 1.25e-01, avg batch time: 0.5295, average train loss: 0.8323
[09/16 21:46:26 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1430, average loss: 0.8901
[09/16 21:46:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 55.00	top5: 100.00	
[09/16 21:46:47 visual_prompt]: 	Test 100/356. loss: 1.888, 0.2001 s / batch. (data: 1.77e-02)max mem: 17.22445 GB 
[09/16 21:47:07 visual_prompt]: 	Test 200/356. loss: 1.838, 0.2065 s / batch. (data: 2.37e-02)max mem: 17.22445 GB 
[09/16 21:47:26 visual_prompt]: 	Test 300/356. loss: 1.602, 0.1946 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/16 21:47:39 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1941, average loss: 1.8749
[09/16 21:47:39 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.56	top5: 96.14	
[09/16 21:47:39 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/16 21:47:49 visual_prompt]: Epoch 66 / 100: avg data time: 1.25e-01, avg batch time: 0.5326, average train loss: 0.7605
[09/16 21:47:52 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1444, average loss: 0.7242
[09/16 21:47:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 66.50	top5: 100.00	
[09/16 21:48:14 visual_prompt]: 	Test 100/356. loss: 2.227, 0.1969 s / batch. (data: 1.41e-02)max mem: 17.22445 GB 
[09/16 21:48:34 visual_prompt]: 	Test 200/356. loss: 2.073, 0.2073 s / batch. (data: 2.48e-02)max mem: 17.22445 GB 
[09/16 21:48:53 visual_prompt]: 	Test 300/356. loss: 1.908, 0.1845 s / batch. (data: 1.97e-04)max mem: 17.22445 GB 
[09/16 21:49:06 visual_prompt]: Inference (test):avg data time: 7.35e-03, avg batch time: 0.1946, average loss: 2.0242
[09/16 21:49:06 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.22	top5: 97.21	
[09/16 21:49:06 visual_prompt]: Best epoch 66: best metric: 0.665
[09/16 21:49:06 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/16 21:49:16 visual_prompt]: Epoch 67 / 100: avg data time: 1.31e-01, avg batch time: 0.5393, average train loss: 0.6867
[09/16 21:49:20 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1431, average loss: 0.6511
[09/16 21:49:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 67.00	top5: 99.50	
[09/16 21:49:42 visual_prompt]: 	Test 100/356. loss: 2.077, 0.2048 s / batch. (data: 2.24e-02)max mem: 17.22445 GB 
[09/16 21:50:02 visual_prompt]: 	Test 200/356. loss: 2.004, 0.2145 s / batch. (data: 1.34e-02)max mem: 17.22445 GB 
[09/16 21:50:21 visual_prompt]: 	Test 300/356. loss: 1.646, 0.1844 s / batch. (data: 1.52e-04)max mem: 17.22445 GB 
[09/16 21:50:34 visual_prompt]: Inference (test):avg data time: 6.98e-03, avg batch time: 0.1960, average loss: 1.8795
[09/16 21:50:34 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.83	top5: 96.67	
[09/16 21:50:34 visual_prompt]: Best epoch 67: best metric: 0.670
[09/16 21:50:34 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/16 21:50:44 visual_prompt]: Epoch 68 / 100: avg data time: 1.35e-01, avg batch time: 0.5433, average train loss: 0.6570
[09/16 21:50:48 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1432, average loss: 0.6320
[09/16 21:50:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 69.00	top5: 100.00	
[09/16 21:51:10 visual_prompt]: 	Test 100/356. loss: 2.180, 0.1982 s / batch. (data: 1.52e-02)max mem: 17.22445 GB 
[09/16 21:51:29 visual_prompt]: 	Test 200/356. loss: 2.030, 0.1970 s / batch. (data: 1.37e-02)max mem: 17.22445 GB 
[09/16 21:51:48 visual_prompt]: 	Test 300/356. loss: 1.670, 0.2041 s / batch. (data: 4.36e-05)max mem: 17.22445 GB 
[09/16 21:52:01 visual_prompt]: Inference (test):avg data time: 7.61e-03, avg batch time: 0.1931, average loss: 2.0732
[09/16 21:52:01 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.20	top5: 97.34	
[09/16 21:52:01 visual_prompt]: Best epoch 68: best metric: 0.690
[09/16 21:52:01 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/16 21:52:11 visual_prompt]: Epoch 69 / 100: avg data time: 1.25e-01, avg batch time: 0.5275, average train loss: 0.5928
[09/16 21:52:15 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1432, average loss: 0.7827
[09/16 21:52:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 66.00	top5: 100.00	
[09/16 21:52:36 visual_prompt]: 	Test 100/356. loss: 2.357, 0.1837 s / batch. (data: 1.38e-04)max mem: 17.22445 GB 
[09/16 21:52:56 visual_prompt]: 	Test 200/356. loss: 2.422, 0.1838 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/16 21:53:16 visual_prompt]: 	Test 300/356. loss: 1.884, 0.2112 s / batch. (data: 2.78e-02)max mem: 17.22445 GB 
[09/16 21:53:29 visual_prompt]: Inference (test):avg data time: 8.18e-03, avg batch time: 0.1955, average loss: 2.2583
[09/16 21:53:29 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.54	top5: 96.75	
[09/16 21:53:29 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/16 21:53:38 visual_prompt]: Epoch 70 / 100: avg data time: 1.17e-01, avg batch time: 0.5234, average train loss: 0.6119
[09/16 21:53:42 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.1432, average loss: 0.5450
[09/16 21:53:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 75.00	top5: 100.00	
[09/16 21:54:04 visual_prompt]: 	Test 100/356. loss: 2.307, 0.1997 s / batch. (data: 1.11e-02)max mem: 17.22445 GB 
[09/16 21:54:24 visual_prompt]: 	Test 200/356. loss: 2.267, 0.1848 s / batch. (data: 3.68e-04)max mem: 17.22445 GB 
[09/16 21:54:44 visual_prompt]: 	Test 300/356. loss: 1.999, 0.1994 s / batch. (data: 1.60e-02)max mem: 17.22445 GB 
[09/16 21:54:56 visual_prompt]: Inference (test):avg data time: 7.91e-03, avg batch time: 0.1959, average loss: 2.1697
[09/16 21:54:56 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.10	top5: 96.93	
[09/16 21:54:56 visual_prompt]: Best epoch 70: best metric: 0.750
[09/16 21:54:56 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/16 21:55:06 visual_prompt]: Epoch 71 / 100: avg data time: 1.20e-01, avg batch time: 0.5244, average train loss: 0.6718
[09/16 21:55:10 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1430, average loss: 0.7580
[09/16 21:55:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 61.00	top5: 100.00	
[09/16 21:55:31 visual_prompt]: 	Test 100/356. loss: 2.519, 0.1833 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/16 21:55:51 visual_prompt]: 	Test 200/356. loss: 2.155, 0.1968 s / batch. (data: 1.43e-02)max mem: 17.22445 GB 
[09/16 21:56:10 visual_prompt]: 	Test 300/356. loss: 2.180, 0.2094 s / batch. (data: 2.07e-02)max mem: 17.22445 GB 
[09/16 21:56:23 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1943, average loss: 2.1697
[09/16 21:56:23 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.73	top5: 97.12	
[09/16 21:56:23 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/16 21:56:33 visual_prompt]: Epoch 72 / 100: avg data time: 1.25e-01, avg batch time: 0.5323, average train loss: 0.6151
[09/16 21:56:37 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1429, average loss: 0.6331
[09/16 21:56:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 71.50	top5: 100.00	
[09/16 21:56:59 visual_prompt]: 	Test 100/356. loss: 2.412, 0.1958 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/16 21:57:18 visual_prompt]: 	Test 200/356. loss: 2.633, 0.1836 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/16 21:57:38 visual_prompt]: 	Test 300/356. loss: 2.195, 0.1840 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/16 21:57:50 visual_prompt]: Inference (test):avg data time: 7.22e-03, avg batch time: 0.1938, average loss: 2.2506
[09/16 21:57:50 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.34	top5: 95.25	
[09/16 21:57:50 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/16 21:58:00 visual_prompt]: Epoch 73 / 100: avg data time: 1.19e-01, avg batch time: 0.5235, average train loss: 0.5571
[09/16 21:58:04 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.1431, average loss: 0.5309
[09/16 21:58:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 77.50	top5: 100.00	
[09/16 21:58:25 visual_prompt]: 	Test 100/356. loss: 2.473, 0.1826 s / batch. (data: 9.42e-05)max mem: 17.22445 GB 
[09/16 21:58:45 visual_prompt]: 	Test 200/356. loss: 2.460, 0.1835 s / batch. (data: 1.27e-04)max mem: 17.22445 GB 
[09/16 21:59:04 visual_prompt]: 	Test 300/356. loss: 2.128, 0.1842 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/16 21:59:17 visual_prompt]: Inference (test):avg data time: 7.91e-03, avg batch time: 0.1938, average loss: 2.2274
[09/16 21:59:17 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.39	top5: 95.80	
[09/16 21:59:17 visual_prompt]: Best epoch 73: best metric: 0.775
[09/16 21:59:17 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/16 21:59:27 visual_prompt]: Epoch 74 / 100: avg data time: 1.31e-01, avg batch time: 0.5322, average train loss: 0.4774
[09/16 21:59:31 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1430, average loss: 0.4912
[09/16 21:59:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 79.50	top5: 100.00	
[09/16 21:59:52 visual_prompt]: 	Test 100/356. loss: 2.888, 0.1842 s / batch. (data: 1.45e-04)max mem: 17.22445 GB 
[09/16 22:00:11 visual_prompt]: 	Test 200/356. loss: 2.504, 0.1840 s / batch. (data: 1.21e-04)max mem: 17.22445 GB 
[09/16 22:00:31 visual_prompt]: 	Test 300/356. loss: 2.138, 0.1853 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/16 22:00:43 visual_prompt]: Inference (test):avg data time: 7.77e-03, avg batch time: 0.1935, average loss: 2.4470
[09/16 22:00:44 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.00	top5: 96.49	
[09/16 22:00:44 visual_prompt]: Best epoch 74: best metric: 0.795
[09/16 22:00:44 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/16 22:00:54 visual_prompt]: Epoch 75 / 100: avg data time: 1.35e-01, avg batch time: 0.5376, average train loss: 0.4590
[09/16 22:00:57 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1432, average loss: 0.5397
[09/16 22:00:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 75.00	top5: 100.00	
[09/16 22:01:19 visual_prompt]: 	Test 100/356. loss: 2.953, 0.2164 s / batch. (data: 3.16e-02)max mem: 17.22445 GB 
[09/16 22:01:39 visual_prompt]: 	Test 200/356. loss: 2.944, 0.1879 s / batch. (data: 4.10e-05)max mem: 17.22445 GB 
[09/16 22:01:58 visual_prompt]: 	Test 300/356. loss: 2.582, 0.1838 s / batch. (data: 1.21e-04)max mem: 17.22445 GB 
[09/16 22:02:11 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1941, average loss: 2.6623
[09/16 22:02:11 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.38	top5: 96.58	
[09/16 22:02:11 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/16 22:02:21 visual_prompt]: Epoch 76 / 100: avg data time: 1.28e-01, avg batch time: 0.5344, average train loss: 0.4631
[09/16 22:02:25 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1432, average loss: 0.3395
[09/16 22:02:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 82.50	top5: 100.00	
[09/16 22:02:46 visual_prompt]: 	Test 100/356. loss: 2.519, 0.1953 s / batch. (data: 1.32e-02)max mem: 17.22445 GB 
[09/16 22:03:05 visual_prompt]: 	Test 200/356. loss: 3.044, 0.1996 s / batch. (data: 1.43e-02)max mem: 17.22445 GB 
[09/16 22:03:25 visual_prompt]: 	Test 300/356. loss: 2.523, 0.1840 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/16 22:03:37 visual_prompt]: Inference (test):avg data time: 6.99e-03, avg batch time: 0.1930, average loss: 2.4595
[09/16 22:03:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.97	top5: 96.66	
[09/16 22:03:37 visual_prompt]: Best epoch 76: best metric: 0.825
[09/16 22:03:37 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/16 22:03:47 visual_prompt]: Epoch 77 / 100: avg data time: 1.20e-01, avg batch time: 0.5245, average train loss: 0.3768
[09/16 22:03:51 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1432, average loss: 0.4322
[09/16 22:03:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 78.50	top5: 100.00	
[09/16 22:04:13 visual_prompt]: 	Test 100/356. loss: 3.688, 0.1961 s / batch. (data: 1.35e-02)max mem: 17.22445 GB 
[09/16 22:04:32 visual_prompt]: 	Test 200/356. loss: 3.446, 0.2242 s / batch. (data: 3.84e-02)max mem: 17.22445 GB 
[09/16 22:04:52 visual_prompt]: 	Test 300/356. loss: 2.917, 0.1993 s / batch. (data: 1.62e-02)max mem: 17.22445 GB 
[09/16 22:05:05 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1956, average loss: 3.0377
[09/16 22:05:05 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.46	top5: 97.47	
[09/16 22:05:05 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/16 22:05:15 visual_prompt]: Epoch 78 / 100: avg data time: 1.40e-01, avg batch time: 0.5626, average train loss: 0.4308
[09/16 22:05:19 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1431, average loss: 0.3976
[09/16 22:05:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 77.00	top5: 100.00	
[09/16 22:05:41 visual_prompt]: 	Test 100/356. loss: 2.339, 0.1834 s / batch. (data: 1.28e-04)max mem: 17.22445 GB 
[09/16 22:06:00 visual_prompt]: 	Test 200/356. loss: 2.717, 0.1976 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/16 22:06:20 visual_prompt]: 	Test 300/356. loss: 2.243, 0.1958 s / batch. (data: 1.22e-02)max mem: 17.22445 GB 
[09/16 22:06:32 visual_prompt]: Inference (test):avg data time: 8.14e-03, avg batch time: 0.1937, average loss: 2.3807
[09/16 22:06:32 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.89	top5: 96.84	
[09/16 22:06:32 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/16 22:06:42 visual_prompt]: Epoch 79 / 100: avg data time: 1.31e-01, avg batch time: 0.5332, average train loss: 0.3372
[09/16 22:06:46 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1431, average loss: 0.3697
[09/16 22:06:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 82.00	top5: 100.00	
[09/16 22:07:08 visual_prompt]: 	Test 100/356. loss: 3.252, 0.1841 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/16 22:07:27 visual_prompt]: 	Test 200/356. loss: 3.064, 0.2102 s / batch. (data: 1.36e-02)max mem: 17.22445 GB 
[09/16 22:07:47 visual_prompt]: 	Test 300/356. loss: 2.856, 0.2043 s / batch. (data: 2.12e-02)max mem: 17.22445 GB 
[09/16 22:07:59 visual_prompt]: Inference (test):avg data time: 8.21e-03, avg batch time: 0.1937, average loss: 2.9336
[09/16 22:07:59 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.38	top5: 97.69	
[09/16 22:07:59 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/16 22:08:09 visual_prompt]: Epoch 80 / 100: avg data time: 1.18e-01, avg batch time: 0.5243, average train loss: 0.3212
[09/16 22:08:13 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1467, average loss: 0.2438
[09/16 22:08:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 88.00	top5: 100.00	
[09/16 22:08:34 visual_prompt]: 	Test 100/356. loss: 3.211, 0.1933 s / batch. (data: 1.29e-04)max mem: 17.22445 GB 
[09/16 22:08:53 visual_prompt]: 	Test 200/356. loss: 3.128, 0.1891 s / batch. (data: 1.35e-04)max mem: 17.22445 GB 
[09/16 22:09:13 visual_prompt]: 	Test 300/356. loss: 3.143, 0.1839 s / batch. (data: 1.35e-04)max mem: 17.22445 GB 
[09/16 22:09:25 visual_prompt]: Inference (test):avg data time: 6.88e-03, avg batch time: 0.1928, average loss: 3.0192
[09/16 22:09:25 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.39	top5: 97.32	
[09/16 22:09:25 visual_prompt]: Best epoch 80: best metric: 0.880
[09/16 22:09:25 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/16 22:09:36 visual_prompt]: Epoch 81 / 100: avg data time: 1.14e-01, avg batch time: 0.5798, average train loss: 0.2579
[09/16 22:09:40 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1430, average loss: 0.2863
[09/16 22:09:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 87.00	top5: 100.00	
[09/16 22:10:01 visual_prompt]: 	Test 100/356. loss: 3.505, 0.2121 s / batch. (data: 2.49e-02)max mem: 17.22445 GB 
[09/16 22:10:21 visual_prompt]: 	Test 200/356. loss: 3.635, 0.1925 s / batch. (data: 1.32e-04)max mem: 17.22445 GB 
[09/16 22:10:40 visual_prompt]: 	Test 300/356. loss: 3.419, 0.1960 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/16 22:10:53 visual_prompt]: Inference (test):avg data time: 6.88e-03, avg batch time: 0.1928, average loss: 3.3879
[09/16 22:10:53 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.72	top5: 97.27	
[09/16 22:10:53 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/16 22:11:03 visual_prompt]: Epoch 82 / 100: avg data time: 1.37e-01, avg batch time: 0.5430, average train loss: 0.3163
[09/16 22:11:07 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1445, average loss: 0.2711
[09/16 22:11:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 88.50	top5: 100.00	
[09/16 22:11:28 visual_prompt]: 	Test 100/356. loss: 3.730, 0.1963 s / batch. (data: 1.35e-02)max mem: 17.22445 GB 
[09/16 22:11:48 visual_prompt]: 	Test 200/356. loss: 3.300, 0.1870 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/16 22:12:07 visual_prompt]: 	Test 300/356. loss: 2.928, 0.2046 s / batch. (data: 5.22e-03)max mem: 17.22445 GB 
[09/16 22:12:20 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1938, average loss: 3.3167
[09/16 22:12:20 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.84	top5: 97.28	
[09/16 22:12:20 visual_prompt]: Best epoch 82: best metric: 0.885
[09/16 22:12:20 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/16 22:12:30 visual_prompt]: Epoch 83 / 100: avg data time: 1.18e-01, avg batch time: 0.5229, average train loss: 0.2606
[09/16 22:12:33 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.1432, average loss: 0.2378
[09/16 22:12:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 90.00	top5: 100.00	
[09/16 22:12:55 visual_prompt]: 	Test 100/356. loss: 3.423, 0.1832 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/16 22:13:15 visual_prompt]: 	Test 200/356. loss: 3.209, 0.1838 s / batch. (data: 1.58e-04)max mem: 17.22445 GB 
[09/16 22:13:35 visual_prompt]: 	Test 300/356. loss: 3.054, 0.1966 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/16 22:13:47 visual_prompt]: Inference (test):avg data time: 7.98e-03, avg batch time: 0.1964, average loss: 3.1466
[09/16 22:13:48 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.41	top5: 96.68	
[09/16 22:13:48 visual_prompt]: Best epoch 83: best metric: 0.900
[09/16 22:13:48 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/16 22:13:58 visual_prompt]: Epoch 84 / 100: avg data time: 1.32e-01, avg batch time: 0.5859, average train loss: 0.2323
[09/16 22:14:02 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1432, average loss: 0.2883
[09/16 22:14:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 87.50	top5: 100.00	
[09/16 22:14:24 visual_prompt]: 	Test 100/356. loss: 3.987, 0.1993 s / batch. (data: 1.19e-04)max mem: 17.22445 GB 
[09/16 22:14:43 visual_prompt]: 	Test 200/356. loss: 3.387, 0.1840 s / batch. (data: 1.60e-04)max mem: 17.22445 GB 
[09/16 22:15:03 visual_prompt]: 	Test 300/356. loss: 3.521, 0.1837 s / batch. (data: 1.16e-04)max mem: 17.22445 GB 
[09/16 22:15:15 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1938, average loss: 3.5114
[09/16 22:15:15 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.07	top5: 97.35	
[09/16 22:15:15 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/16 22:15:25 visual_prompt]: Epoch 85 / 100: avg data time: 1.19e-01, avg batch time: 0.5225, average train loss: 0.2155
[09/16 22:15:29 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1432, average loss: 0.2242
[09/16 22:15:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 88.00	top5: 100.00	
[09/16 22:15:50 visual_prompt]: 	Test 100/356. loss: 4.119, 0.1834 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/16 22:16:10 visual_prompt]: 	Test 200/356. loss: 4.175, 0.1971 s / batch. (data: 1.41e-04)max mem: 17.22445 GB 
[09/16 22:16:29 visual_prompt]: 	Test 300/356. loss: 4.181, 0.1837 s / batch. (data: 1.32e-04)max mem: 17.22445 GB 
[09/16 22:16:42 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1938, average loss: 3.8317
[09/16 22:16:42 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.70	top5: 96.64	
[09/16 22:16:42 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/16 22:16:52 visual_prompt]: Epoch 86 / 100: avg data time: 1.24e-01, avg batch time: 0.5257, average train loss: 0.2087
[09/16 22:16:56 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1432, average loss: 0.3673
[09/16 22:16:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 87.00	top5: 100.00	
[09/16 22:17:17 visual_prompt]: 	Test 100/356. loss: 4.597, 0.2097 s / batch. (data: 2.66e-02)max mem: 17.22445 GB 
[09/16 22:17:37 visual_prompt]: 	Test 200/356. loss: 4.119, 0.1979 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/16 22:17:56 visual_prompt]: 	Test 300/356. loss: 4.265, 0.2074 s / batch. (data: 2.43e-02)max mem: 17.22445 GB 
[09/16 22:18:09 visual_prompt]: Inference (test):avg data time: 8.65e-03, avg batch time: 0.1940, average loss: 4.1189
[09/16 22:18:09 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.83	top5: 97.76	
[09/16 22:18:09 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/16 22:18:19 visual_prompt]: Epoch 87 / 100: avg data time: 1.35e-01, avg batch time: 0.5381, average train loss: 0.1938
[09/16 22:18:23 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1433, average loss: 0.1595
[09/16 22:18:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 93.00	top5: 100.00	
[09/16 22:18:44 visual_prompt]: 	Test 100/356. loss: 4.347, 0.1984 s / batch. (data: 1.54e-02)max mem: 17.22445 GB 
[09/16 22:19:04 visual_prompt]: 	Test 200/356. loss: 3.846, 0.2009 s / batch. (data: 1.38e-02)max mem: 17.22445 GB 
[09/16 22:19:24 visual_prompt]: 	Test 300/356. loss: 4.110, 0.1995 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/16 22:19:36 visual_prompt]: Inference (test):avg data time: 8.30e-03, avg batch time: 0.1956, average loss: 3.9919
[09/16 22:19:36 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.30	top5: 97.44	
[09/16 22:19:36 visual_prompt]: Best epoch 87: best metric: 0.930
[09/16 22:19:36 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/16 22:19:46 visual_prompt]: Epoch 88 / 100: avg data time: 1.31e-01, avg batch time: 0.5335, average train loss: 0.1680
[09/16 22:19:50 visual_prompt]: Inference (val):avg data time: 4.97e-05, avg batch time: 0.1432, average loss: 0.1653
[09/16 22:19:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 92.50	top5: 100.00	
[09/16 22:20:12 visual_prompt]: 	Test 100/356. loss: 4.411, 0.1853 s / batch. (data: 1.15e-04)max mem: 17.22445 GB 
[09/16 22:20:31 visual_prompt]: 	Test 200/356. loss: 3.957, 0.1838 s / batch. (data: 1.28e-04)max mem: 17.22445 GB 
[09/16 22:20:51 visual_prompt]: 	Test 300/356. loss: 4.141, 0.1997 s / batch. (data: 1.64e-02)max mem: 17.22445 GB 
[09/16 22:21:04 visual_prompt]: Inference (test):avg data time: 8.69e-03, avg batch time: 0.1956, average loss: 4.0681
[09/16 22:21:04 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.66	top5: 97.70	
[09/16 22:21:04 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/16 22:21:14 visual_prompt]: Epoch 89 / 100: avg data time: 1.34e-01, avg batch time: 0.5352, average train loss: 0.1458
[09/16 22:21:18 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1433, average loss: 0.1531
[09/16 22:21:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 93.50	top5: 100.00	
[09/16 22:21:39 visual_prompt]: 	Test 100/356. loss: 4.525, 0.1832 s / batch. (data: 1.54e-04)max mem: 17.22445 GB 
[09/16 22:21:59 visual_prompt]: 	Test 200/356. loss: 4.416, 0.1835 s / batch. (data: 1.15e-04)max mem: 17.22445 GB 
[09/16 22:22:18 visual_prompt]: 	Test 300/356. loss: 4.925, 0.1957 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/16 22:22:31 visual_prompt]: Inference (test):avg data time: 7.90e-03, avg batch time: 0.1936, average loss: 4.3598
[09/16 22:22:31 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.64	top5: 97.24	
[09/16 22:22:31 visual_prompt]: Best epoch 89: best metric: 0.935
[09/16 22:22:31 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/16 22:22:41 visual_prompt]: Epoch 90 / 100: avg data time: 1.38e-01, avg batch time: 0.5408, average train loss: 0.1211
[09/16 22:22:45 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1432, average loss: 0.1482
[09/16 22:22:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 92.50	top5: 100.00	
[09/16 22:23:06 visual_prompt]: 	Test 100/356. loss: 5.343, 0.1835 s / batch. (data: 1.02e-04)max mem: 17.22445 GB 
[09/16 22:23:26 visual_prompt]: 	Test 200/356. loss: 4.513, 0.1962 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/16 22:23:45 visual_prompt]: 	Test 300/356. loss: 5.605, 0.1838 s / batch. (data: 9.11e-05)max mem: 17.22445 GB 
[09/16 22:23:58 visual_prompt]: Inference (test):avg data time: 8.14e-03, avg batch time: 0.1940, average loss: 5.0297
[09/16 22:23:58 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.49	top5: 97.05	
[09/16 22:23:58 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/16 22:24:08 visual_prompt]: Epoch 91 / 100: avg data time: 1.40e-01, avg batch time: 0.5417, average train loss: 0.1186
[09/16 22:24:12 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.1485, average loss: 0.1401
[09/16 22:24:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 93.50	top5: 100.00	
[09/16 22:24:34 visual_prompt]: 	Test 100/356. loss: 5.554, 0.1830 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/16 22:24:53 visual_prompt]: 	Test 200/356. loss: 4.909, 0.2224 s / batch. (data: 2.55e-02)max mem: 17.22445 GB 
[09/16 22:25:13 visual_prompt]: 	Test 300/356. loss: 5.757, 0.2185 s / batch. (data: 3.50e-02)max mem: 17.22445 GB 
[09/16 22:25:26 visual_prompt]: Inference (test):avg data time: 8.19e-03, avg batch time: 0.1962, average loss: 5.3811
[09/16 22:25:26 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.25	top5: 97.10	
[09/16 22:25:26 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/16 22:25:36 visual_prompt]: Epoch 92 / 100: avg data time: 1.24e-01, avg batch time: 0.5298, average train loss: 0.1165
[09/16 22:25:40 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1431, average loss: 0.1353
[09/16 22:25:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 95.00	top5: 100.00	
[09/16 22:26:02 visual_prompt]: 	Test 100/356. loss: 5.569, 0.1831 s / batch. (data: 1.60e-04)max mem: 17.22445 GB 
[09/16 22:26:21 visual_prompt]: 	Test 200/356. loss: 4.770, 0.1835 s / batch. (data: 1.46e-04)max mem: 17.22445 GB 
[09/16 22:26:42 visual_prompt]: 	Test 300/356. loss: 5.536, 0.1974 s / batch. (data: 1.55e-04)max mem: 17.22445 GB 
[09/16 22:26:54 visual_prompt]: Inference (test):avg data time: 7.69e-03, avg batch time: 0.1963, average loss: 5.1775
[09/16 22:26:54 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.33	top5: 97.40	
[09/16 22:26:54 visual_prompt]: Best epoch 92: best metric: 0.950
[09/16 22:26:54 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/16 22:27:04 visual_prompt]: Epoch 93 / 100: avg data time: 1.37e-01, avg batch time: 0.5384, average train loss: 0.1153
[09/16 22:27:08 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1429, average loss: 0.0967
[09/16 22:27:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 96.50	top5: 100.00	
[09/16 22:27:30 visual_prompt]: 	Test 100/356. loss: 5.716, 0.1837 s / batch. (data: 4.15e-05)max mem: 17.22445 GB 
[09/16 22:27:49 visual_prompt]: 	Test 200/356. loss: 4.888, 0.1834 s / batch. (data: 1.88e-04)max mem: 17.22445 GB 
[09/16 22:28:09 visual_prompt]: 	Test 300/356. loss: 5.570, 0.1961 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/16 22:28:21 visual_prompt]: Inference (test):avg data time: 7.44e-03, avg batch time: 0.1936, average loss: 5.2026
[09/16 22:28:21 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.56	top5: 97.42	
[09/16 22:28:21 visual_prompt]: Best epoch 93: best metric: 0.965
[09/16 22:28:21 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/16 22:28:31 visual_prompt]: Epoch 94 / 100: avg data time: 1.26e-01, avg batch time: 0.5283, average train loss: 0.0883
[09/16 22:28:35 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1431, average loss: 0.1018
[09/16 22:28:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 96.00	top5: 100.00	
[09/16 22:28:56 visual_prompt]: 	Test 100/356. loss: 5.927, 0.1971 s / batch. (data: 1.46e-02)max mem: 17.22445 GB 
[09/16 22:29:16 visual_prompt]: 	Test 200/356. loss: 5.022, 0.1958 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/16 22:29:36 visual_prompt]: 	Test 300/356. loss: 5.837, 0.1915 s / batch. (data: 1.15e-04)max mem: 17.22445 GB 
[09/16 22:29:48 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1952, average loss: 5.3521
[09/16 22:29:48 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.45	top5: 97.34	
[09/16 22:29:48 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/16 22:29:58 visual_prompt]: Epoch 95 / 100: avg data time: 1.18e-01, avg batch time: 0.5239, average train loss: 0.0991
[09/16 22:30:02 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1433, average loss: 0.0974
[09/16 22:30:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 97.50	top5: 100.00	
[09/16 22:30:24 visual_prompt]: 	Test 100/356. loss: 5.759, 0.1832 s / batch. (data: 9.97e-05)max mem: 17.22445 GB 
[09/16 22:30:43 visual_prompt]: 	Test 200/356. loss: 5.140, 0.2214 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/16 22:31:03 visual_prompt]: 	Test 300/356. loss: 5.692, 0.2002 s / batch. (data: 1.46e-02)max mem: 17.22445 GB 
[09/16 22:31:15 visual_prompt]: Inference (test):avg data time: 7.74e-03, avg batch time: 0.1939, average loss: 5.3523
[09/16 22:31:15 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.60	top5: 97.30	
[09/16 22:31:15 visual_prompt]: Best epoch 95: best metric: 0.975
[09/16 22:31:15 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/16 22:31:25 visual_prompt]: Epoch 96 / 100: avg data time: 1.31e-01, avg batch time: 0.5361, average train loss: 0.0730
[09/16 22:31:29 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.1430, average loss: 0.1051
[09/16 22:31:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 95.50	top5: 100.00	
[09/16 22:31:51 visual_prompt]: 	Test 100/356. loss: 5.818, 0.2059 s / batch. (data: 5.14e-03)max mem: 17.22445 GB 
[09/16 22:32:11 visual_prompt]: 	Test 200/356. loss: 5.282, 0.2095 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/16 22:32:31 visual_prompt]: 	Test 300/356. loss: 5.878, 0.1991 s / batch. (data: 1.58e-02)max mem: 17.22445 GB 
[09/16 22:32:43 visual_prompt]: Inference (test):avg data time: 7.47e-03, avg batch time: 0.1982, average loss: 5.5020
[09/16 22:32:44 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.60	top5: 97.33	
[09/16 22:32:44 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/16 22:32:53 visual_prompt]: Epoch 97 / 100: avg data time: 1.25e-01, avg batch time: 0.5273, average train loss: 0.0764
[09/16 22:32:57 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.1429, average loss: 0.0927
[09/16 22:32:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 96.50	top5: 100.00	
[09/16 22:33:19 visual_prompt]: 	Test 100/356. loss: 6.043, 0.1895 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/16 22:33:39 visual_prompt]: 	Test 200/356. loss: 5.405, 0.1974 s / batch. (data: 1.17e-04)max mem: 17.22445 GB 
[09/16 22:33:58 visual_prompt]: 	Test 300/356. loss: 6.069, 0.1994 s / batch. (data: 1.57e-02)max mem: 17.22445 GB 
[09/16 22:34:11 visual_prompt]: Inference (test):avg data time: 7.25e-03, avg batch time: 0.1947, average loss: 5.6169
[09/16 22:34:11 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.48	top5: 97.35	
[09/16 22:34:11 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/16 22:34:21 visual_prompt]: Epoch 98 / 100: avg data time: 1.32e-01, avg batch time: 0.5381, average train loss: 0.0735
[09/16 22:34:25 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1431, average loss: 0.0878
[09/16 22:34:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 96.50	top5: 100.00	
[09/16 22:34:46 visual_prompt]: 	Test 100/356. loss: 6.087, 0.1909 s / batch. (data: 1.56e-04)max mem: 17.22445 GB 
[09/16 22:35:06 visual_prompt]: 	Test 200/356. loss: 5.443, 0.1833 s / batch. (data: 1.61e-04)max mem: 17.22445 GB 
[09/16 22:35:25 visual_prompt]: 	Test 300/356. loss: 6.041, 0.2042 s / batch. (data: 1.23e-04)max mem: 17.22445 GB 
[09/16 22:35:38 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1937, average loss: 5.6489
[09/16 22:35:38 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.45	top5: 97.33	
[09/16 22:35:38 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/16 22:35:48 visual_prompt]: Epoch 99 / 100: avg data time: 1.23e-01, avg batch time: 0.5573, average train loss: 0.0633
[09/16 22:35:52 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1431, average loss: 0.0929
[09/16 22:35:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 96.50	top5: 100.00	
[09/16 22:36:14 visual_prompt]: 	Test 100/356. loss: 6.067, 0.1860 s / batch. (data: 1.45e-04)max mem: 17.22445 GB 
[09/16 22:36:34 visual_prompt]: 	Test 200/356. loss: 5.447, 0.1971 s / batch. (data: 1.26e-04)max mem: 17.22445 GB 
[09/16 22:36:53 visual_prompt]: 	Test 300/356. loss: 6.042, 0.1842 s / batch. (data: 1.54e-04)max mem: 17.22445 GB 
[09/16 22:37:06 visual_prompt]: Inference (test):avg data time: 6.93e-03, avg batch time: 0.1954, average loss: 5.6644
[09/16 22:37:06 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.52	top5: 97.34	
[09/16 22:37:06 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/16 22:37:17 visual_prompt]: Epoch 100 / 100: avg data time: 1.34e-01, avg batch time: 0.5765, average train loss: 0.0691
[09/16 22:37:20 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1432, average loss: 0.0929
[09/16 22:37:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 96.50	top5: 100.00	
[09/16 22:37:42 visual_prompt]: 	Test 100/356. loss: 6.074, 0.1861 s / batch. (data: 1.50e-04)max mem: 17.22445 GB 
[09/16 22:38:01 visual_prompt]: 	Test 200/356. loss: 5.451, 0.1971 s / batch. (data: 1.19e-04)max mem: 17.22445 GB 
[09/16 22:38:21 visual_prompt]: 	Test 300/356. loss: 6.048, 0.1959 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/16 22:38:33 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1936, average loss: 5.6703
[09/16 22:38:34 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.51	top5: 97.34	
[09/16 22:38:52 visual_prompt]: Rank of current process: 0. World size: 1
[09/16 22:38:52 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/16 22:38:52 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed44'], train_type='')
[09/16 22:38:52 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/16 22:38:52 visual_prompt]: Training with config:
[09/16 22:38:52 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-dmlab',
          'NO_TEST': False,
          'NUMBER_CLASSES': 6,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed44/vtab-dmlab/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/16 22:38:52 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-16 22:38:52.526042: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-16 22:38:52.688534: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-16 22:38:53.685214: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 22:38:53.685295: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 22:38:53.685304: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-16 22:38:56.056432: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 22:38:56.056547: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 22:38:56.056562: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/16 22:38:56 visual_prompt]: Constructing vtab-dmlab dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
2023-09-16 22:38:56.072084: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dmlab for split train[:800]+validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/16 22:38:58 visual_prompt]: Number of images: 1000
[09/16 22:38:58 visual_prompt]: Number of classes: 6 / 6
[09/16 22:38:58 visual_prompt]: Loading validation data...
[09/16 22:38:58 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/16 22:38:58 visual_prompt]: Number of images: 200
[09/16 22:38:58 visual_prompt]: Number of classes: 6 / 6
[09/16 22:38:58 visual_prompt]: Loading test data...
[09/16 22:38:58 visual_prompt]: Constructing vtab-dmlab dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dmlab for split test, from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/16 22:39:39 visual_prompt]: Number of images: 22735
[09/16 22:39:39 visual_prompt]: Number of classes: 6 / 6
[09/16 22:39:39 visual_prompt]: Constructing models...
[09/16 22:39:42 visual_prompt]: Total Parameters: 86724870	 Gradient Parameters: 926214
[09/16 22:39:42 visual_prompt]: tuned percent:1.068
[09/16 22:39:45 visual_prompt]: Device used for model: 0
[09/16 22:39:45 visual_prompt]: Setting up Evalutator...
[09/16 22:39:45 visual_prompt]: Setting up Trainer...
[09/16 22:39:45 visual_prompt]: 	Setting up the optimizer...
[09/16 22:39:45 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/16 22:39:56 visual_prompt]: Epoch 1 / 100: avg data time: 1.34e-01, avg batch time: 0.6312, average train loss: 2.1938
[09/16 22:40:01 visual_prompt]: Inference (val):avg data time: 4.18e-04, avg batch time: 0.2596, average loss: 2.1488
[09/16 22:40:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.00	
[09/16 22:40:23 visual_prompt]: 	Test 100/356. loss: 2.071, 0.1883 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/16 22:40:42 visual_prompt]: 	Test 200/356. loss: 2.188, 0.1828 s / batch. (data: 1.07e-04)max mem: 17.22445 GB 
[09/16 22:41:01 visual_prompt]: 	Test 300/356. loss: 2.092, 0.2083 s / batch. (data: 2.54e-02)max mem: 17.22445 GB 
[09/16 22:41:14 visual_prompt]: Inference (test):avg data time: 8.54e-03, avg batch time: 0.1931, average loss: 2.1222
[09/16 22:41:14 visual_prompt]: Classification results with test_vtab-dmlab: top1: 12.34	top5: 81.34	
[09/16 22:41:14 visual_prompt]: Best epoch 1: best metric: 0.160
[09/16 22:41:14 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/16 22:41:24 visual_prompt]: Epoch 2 / 100: avg data time: 1.49e-01, avg batch time: 0.5509, average train loss: 4.0219
[09/16 22:41:28 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.1430, average loss: 2.5827
[09/16 22:41:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/16 22:41:50 visual_prompt]: 	Test 100/356. loss: 2.184, 0.1837 s / batch. (data: 1.31e-04)max mem: 17.22445 GB 
[09/16 22:42:10 visual_prompt]: 	Test 200/356. loss: 2.092, 0.1968 s / batch. (data: 1.43e-02)max mem: 17.22445 GB 
[09/16 22:42:29 visual_prompt]: 	Test 300/356. loss: 2.345, 0.1959 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/16 22:42:42 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1949, average loss: 2.2946
[09/16 22:42:42 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 88.42	
[09/16 22:42:42 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/16 22:42:52 visual_prompt]: Epoch 3 / 100: avg data time: 1.42e-01, avg batch time: 0.5438, average train loss: 2.0870
[09/16 22:42:56 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1431, average loss: 1.9197
[09/16 22:42:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 84.50	
[09/16 22:43:18 visual_prompt]: 	Test 100/356. loss: 2.021, 0.1993 s / batch. (data: 1.21e-04)max mem: 17.22445 GB 
[09/16 22:43:38 visual_prompt]: 	Test 200/356. loss: 2.183, 0.1954 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/16 22:43:57 visual_prompt]: 	Test 300/356. loss: 2.052, 0.1956 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/16 22:44:10 visual_prompt]: Inference (test):avg data time: 8.20e-03, avg batch time: 0.1945, average loss: 2.0284
[09/16 22:44:10 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.14	top5: 77.81	
[09/16 22:44:10 visual_prompt]: Best epoch 3: best metric: 0.180
[09/16 22:44:10 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/16 22:44:20 visual_prompt]: Epoch 4 / 100: avg data time: 1.43e-01, avg batch time: 0.5469, average train loss: 2.0836
[09/16 22:44:24 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1431, average loss: 1.8609
[09/16 22:44:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/16 22:44:46 visual_prompt]: 	Test 100/356. loss: 1.857, 0.1943 s / batch. (data: 9.58e-05)max mem: 17.22445 GB 
[09/16 22:45:05 visual_prompt]: 	Test 200/356. loss: 1.931, 0.1834 s / batch. (data: 1.42e-04)max mem: 17.22445 GB 
[09/16 22:45:25 visual_prompt]: 	Test 300/356. loss: 1.917, 0.1892 s / batch. (data: 1.35e-04)max mem: 17.22445 GB 
[09/16 22:45:37 visual_prompt]: Inference (test):avg data time: 7.46e-03, avg batch time: 0.1934, average loss: 1.8768
[09/16 22:45:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 14.60	top5: 88.68	
[09/16 22:45:37 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/16 22:45:48 visual_prompt]: Epoch 5 / 100: avg data time: 1.42e-01, avg batch time: 0.5440, average train loss: 2.1338
[09/16 22:45:52 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1433, average loss: 2.5270
[09/16 22:45:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/16 22:46:14 visual_prompt]: 	Test 100/356. loss: 2.726, 0.1830 s / batch. (data: 1.09e-04)max mem: 17.22445 GB 
[09/16 22:46:33 visual_prompt]: 	Test 200/356. loss: 3.146, 0.1973 s / batch. (data: 1.37e-02)max mem: 17.22445 GB 
[09/16 22:46:53 visual_prompt]: 	Test 300/356. loss: 3.001, 0.1841 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/16 22:47:05 visual_prompt]: Inference (test):avg data time: 6.45e-03, avg batch time: 0.1927, average loss: 2.7735
[09/16 22:47:05 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 77.81	
[09/16 22:47:05 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/16 22:47:15 visual_prompt]: Epoch 6 / 100: avg data time: 1.48e-01, avg batch time: 0.5504, average train loss: 2.0589
[09/16 22:47:19 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1435, average loss: 2.1954
[09/16 22:47:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 83.00	
[09/16 22:47:41 visual_prompt]: 	Test 100/356. loss: 2.178, 0.2076 s / batch. (data: 2.52e-02)max mem: 17.22445 GB 
[09/16 22:48:00 visual_prompt]: 	Test 200/356. loss: 2.053, 0.1960 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/16 22:48:20 visual_prompt]: 	Test 300/356. loss: 1.978, 0.1994 s / batch. (data: 1.59e-02)max mem: 17.22445 GB 
[09/16 22:48:33 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1945, average loss: 2.1425
[09/16 22:48:33 visual_prompt]: Classification results with test_vtab-dmlab: top1: 26.30	top5: 82.85	
[09/16 22:48:33 visual_prompt]: Best epoch 6: best metric: 0.215
[09/16 22:48:33 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/16 22:48:43 visual_prompt]: Epoch 7 / 100: avg data time: 1.38e-01, avg batch time: 0.5433, average train loss: 3.5887
[09/16 22:48:47 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1434, average loss: 8.5943
[09/16 22:48:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.50	top5: 81.00	
[09/16 22:49:09 visual_prompt]: 	Test 100/356. loss: 7.241, 0.1981 s / batch. (data: 1.52e-02)max mem: 17.22445 GB 
[09/16 22:49:28 visual_prompt]: 	Test 200/356. loss: 6.987, 0.1967 s / batch. (data: 4.15e-05)max mem: 17.22445 GB 
[09/16 22:49:48 visual_prompt]: 	Test 300/356. loss: 8.239, 0.2026 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/16 22:50:00 visual_prompt]: Inference (test):avg data time: 7.42e-03, avg batch time: 0.1933, average loss: 7.7353
[09/16 22:50:00 visual_prompt]: Classification results with test_vtab-dmlab: top1: 16.00	top5: 82.14	
[09/16 22:50:00 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/16 22:50:11 visual_prompt]: Epoch 8 / 100: avg data time: 1.49e-01, avg batch time: 0.5510, average train loss: 9.5128
[09/16 22:50:15 visual_prompt]: Inference (val):avg data time: 4.43e-05, avg batch time: 0.1431, average loss: 5.1937
[09/16 22:50:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/16 22:50:36 visual_prompt]: 	Test 100/356. loss: 5.525, 0.1827 s / batch. (data: 1.62e-04)max mem: 17.22445 GB 
[09/16 22:50:56 visual_prompt]: 	Test 200/356. loss: 4.271, 0.1838 s / batch. (data: 1.38e-04)max mem: 17.22445 GB 
[09/16 22:51:16 visual_prompt]: 	Test 300/356. loss: 5.248, 0.2081 s / batch. (data: 2.50e-02)max mem: 17.22445 GB 
[09/16 22:51:28 visual_prompt]: Inference (test):avg data time: 8.30e-03, avg batch time: 0.1946, average loss: 5.0189
[09/16 22:51:28 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 84.67	
[09/16 22:51:28 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/16 22:51:39 visual_prompt]: Epoch 9 / 100: avg data time: 1.47e-01, avg batch time: 0.5489, average train loss: 8.2107
[09/16 22:51:43 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1431, average loss: 9.0740
[09/16 22:51:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/16 22:52:05 visual_prompt]: 	Test 100/356. loss: 8.923, 0.1832 s / batch. (data: 1.28e-04)max mem: 17.22445 GB 
[09/16 22:52:24 visual_prompt]: 	Test 200/356. loss: 10.095, 0.1955 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/16 22:52:43 visual_prompt]: 	Test 300/356. loss: 8.974, 0.2082 s / batch. (data: 1.46e-02)max mem: 17.22445 GB 
[09/16 22:52:56 visual_prompt]: Inference (test):avg data time: 8.04e-03, avg batch time: 0.1932, average loss: 9.2551
[09/16 22:52:56 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 88.42	
[09/16 22:52:56 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/16 22:53:06 visual_prompt]: Epoch 10 / 100: avg data time: 1.47e-01, avg batch time: 0.5508, average train loss: 10.8932
[09/16 22:53:11 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1432, average loss: 11.1015
[09/16 22:53:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/16 22:53:32 visual_prompt]: 	Test 100/356. loss: 11.081, 0.1833 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/16 22:53:52 visual_prompt]: 	Test 200/356. loss: 9.889, 0.2046 s / batch. (data: 1.58e-02)max mem: 17.22445 GB 
[09/16 22:54:11 visual_prompt]: 	Test 300/356. loss: 11.167, 0.1841 s / batch. (data: 1.42e-04)max mem: 17.22445 GB 
[09/16 22:54:24 visual_prompt]: Inference (test):avg data time: 7.71e-03, avg batch time: 0.1938, average loss: 10.6302
[09/16 22:54:24 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 88.42	
[09/16 22:54:24 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/16 22:54:34 visual_prompt]: Epoch 11 / 100: avg data time: 1.43e-01, avg batch time: 0.5453, average train loss: 9.4392
[09/16 22:54:39 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1432, average loss: 6.1653
[09/16 22:54:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/16 22:55:00 visual_prompt]: 	Test 100/356. loss: 6.581, 0.1957 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/16 22:55:20 visual_prompt]: 	Test 200/356. loss: 4.691, 0.1960 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/16 22:55:39 visual_prompt]: 	Test 300/356. loss: 6.228, 0.2022 s / batch. (data: 1.93e-02)max mem: 17.22445 GB 
[09/16 22:55:52 visual_prompt]: Inference (test):avg data time: 8.20e-03, avg batch time: 0.1939, average loss: 5.8611
[09/16 22:55:52 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 84.67	
[09/16 22:55:52 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/16 22:56:02 visual_prompt]: Epoch 12 / 100: avg data time: 1.50e-01, avg batch time: 0.5556, average train loss: 16.0108
[09/16 22:56:06 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1449, average loss: 23.9290
[09/16 22:56:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/16 22:56:28 visual_prompt]: 	Test 100/356. loss: 25.731, 0.1991 s / batch. (data: 1.62e-02)max mem: 17.22445 GB 
[09/16 22:56:47 visual_prompt]: 	Test 200/356. loss: 18.914, 0.1959 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/16 22:57:07 visual_prompt]: 	Test 300/356. loss: 20.225, 0.1958 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/16 22:57:20 visual_prompt]: Inference (test):avg data time: 7.15e-03, avg batch time: 0.1941, average loss: 22.8977
[09/16 22:57:20 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 85.40	
[09/16 22:57:20 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/16 22:57:30 visual_prompt]: Epoch 13 / 100: avg data time: 1.26e-01, avg batch time: 0.5311, average train loss: 15.3385
[09/16 22:57:34 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1432, average loss: 22.2198
[09/16 22:57:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/16 22:57:56 visual_prompt]: 	Test 100/356. loss: 23.482, 0.2199 s / batch. (data: 3.72e-02)max mem: 17.22445 GB 
[09/16 22:58:15 visual_prompt]: 	Test 200/356. loss: 20.618, 0.1956 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/16 22:58:34 visual_prompt]: 	Test 300/356. loss: 23.073, 0.1959 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/16 22:58:47 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1931, average loss: 22.1263
[09/16 22:58:47 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 84.67	
[09/16 22:58:47 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/16 22:58:57 visual_prompt]: Epoch 14 / 100: avg data time: 1.47e-01, avg batch time: 0.5486, average train loss: 15.5382
[09/16 22:59:02 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1430, average loss: 10.4838
[09/16 22:59:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/16 22:59:23 visual_prompt]: 	Test 100/356. loss: 10.186, 0.1987 s / batch. (data: 1.10e-02)max mem: 17.22445 GB 
[09/16 22:59:43 visual_prompt]: 	Test 200/356. loss: 9.203, 0.1939 s / batch. (data: 1.03e-02)max mem: 17.22445 GB 
[09/16 23:00:02 visual_prompt]: 	Test 300/356. loss: 10.351, 0.1961 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/16 23:00:15 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1933, average loss: 9.8813
[09/16 23:00:15 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 88.42	
[09/16 23:00:15 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/16 23:00:25 visual_prompt]: Epoch 15 / 100: avg data time: 1.31e-01, avg batch time: 0.5371, average train loss: 11.1709
[09/16 23:00:30 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1432, average loss: 11.6985
[09/16 23:00:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/16 23:00:51 visual_prompt]: 	Test 100/356. loss: 11.275, 0.2313 s / batch. (data: 1.31e-02)max mem: 17.22445 GB 
[09/16 23:01:11 visual_prompt]: 	Test 200/356. loss: 10.367, 0.1836 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/16 23:01:30 visual_prompt]: 	Test 300/356. loss: 9.594, 0.1995 s / batch. (data: 1.59e-02)max mem: 17.22445 GB 
[09/16 23:01:43 visual_prompt]: Inference (test):avg data time: 8.62e-03, avg batch time: 0.1936, average loss: 11.0529
[09/16 23:01:43 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 82.27	
[09/16 23:01:43 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/16 23:01:53 visual_prompt]: Epoch 16 / 100: avg data time: 1.45e-01, avg batch time: 0.5469, average train loss: 7.7068
[09/16 23:01:57 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1462, average loss: 4.3139
[09/16 23:01:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 83.50	
[09/16 23:02:19 visual_prompt]: 	Test 100/356. loss: 4.259, 0.1969 s / batch. (data: 1.38e-02)max mem: 17.22445 GB 
[09/16 23:02:39 visual_prompt]: 	Test 200/356. loss: 3.570, 0.1959 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/16 23:02:59 visual_prompt]: 	Test 300/356. loss: 3.902, 0.2000 s / batch. (data: 1.24e-02)max mem: 17.22445 GB 
[09/16 23:03:11 visual_prompt]: Inference (test):avg data time: 7.57e-03, avg batch time: 0.1947, average loss: 4.0388
[09/16 23:03:11 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 85.79	
[09/16 23:03:11 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/16 23:03:22 visual_prompt]: Epoch 17 / 100: avg data time: 1.51e-01, avg batch time: 0.5523, average train loss: 5.7207
[09/16 23:03:26 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1471, average loss: 3.8943
[09/16 23:03:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/16 23:03:47 visual_prompt]: 	Test 100/356. loss: 3.699, 0.1835 s / batch. (data: 1.12e-04)max mem: 17.22445 GB 
[09/16 23:04:07 visual_prompt]: 	Test 200/356. loss: 4.150, 0.1835 s / batch. (data: 1.23e-04)max mem: 17.22445 GB 
[09/16 23:04:26 visual_prompt]: 	Test 300/356. loss: 4.075, 0.1844 s / batch. (data: 1.64e-04)max mem: 17.22445 GB 
[09/16 23:04:39 visual_prompt]: Inference (test):avg data time: 7.30e-03, avg batch time: 0.1928, average loss: 3.9080
[09/16 23:04:39 visual_prompt]: Classification results with test_vtab-dmlab: top1: 14.60	top5: 88.42	
[09/16 23:04:39 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/16 23:04:49 visual_prompt]: Epoch 18 / 100: avg data time: 1.46e-01, avg batch time: 0.5488, average train loss: 3.4599
[09/16 23:04:53 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1431, average loss: 2.9169
[09/16 23:04:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/16 23:05:15 visual_prompt]: 	Test 100/356. loss: 3.270, 0.1990 s / batch. (data: 1.35e-02)max mem: 17.22445 GB 
[09/16 23:05:35 visual_prompt]: 	Test 200/356. loss: 2.935, 0.1835 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/16 23:05:54 visual_prompt]: 	Test 300/356. loss: 2.950, 0.1982 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/16 23:06:07 visual_prompt]: Inference (test):avg data time: 8.34e-03, avg batch time: 0.1939, average loss: 3.0158
[09/16 23:06:07 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 85.40	
[09/16 23:06:07 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/16 23:06:17 visual_prompt]: Epoch 19 / 100: avg data time: 1.47e-01, avg batch time: 0.5505, average train loss: 2.8451
[09/16 23:06:22 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1431, average loss: 2.2749
[09/16 23:06:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/16 23:06:43 visual_prompt]: 	Test 100/356. loss: 2.338, 0.1842 s / batch. (data: 1.52e-04)max mem: 17.22445 GB 
[09/16 23:07:03 visual_prompt]: 	Test 200/356. loss: 2.392, 0.1963 s / batch. (data: 1.32e-02)max mem: 17.22445 GB 
[09/16 23:07:22 visual_prompt]: 	Test 300/356. loss: 2.258, 0.1845 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/16 23:07:35 visual_prompt]: Inference (test):avg data time: 8.35e-03, avg batch time: 0.1935, average loss: 2.3380
[09/16 23:07:35 visual_prompt]: Classification results with test_vtab-dmlab: top1: 11.58	top5: 82.27	
[09/16 23:07:35 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/16 23:07:45 visual_prompt]: Epoch 20 / 100: avg data time: 1.33e-01, avg batch time: 0.5404, average train loss: 3.1356
[09/16 23:07:49 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1433, average loss: 2.7355
[09/16 23:07:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/16 23:08:11 visual_prompt]: 	Test 100/356. loss: 2.332, 0.2201 s / batch. (data: 3.73e-02)max mem: 17.22445 GB 
[09/16 23:08:30 visual_prompt]: 	Test 200/356. loss: 2.482, 0.1992 s / batch. (data: 1.63e-02)max mem: 17.22445 GB 
[09/16 23:08:49 visual_prompt]: 	Test 300/356. loss: 2.533, 0.1957 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/16 23:09:02 visual_prompt]: Inference (test):avg data time: 7.02e-03, avg batch time: 0.1929, average loss: 2.5111
[09/16 23:09:02 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 88.42	
[09/16 23:09:02 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/16 23:09:12 visual_prompt]: Epoch 21 / 100: avg data time: 1.38e-01, avg batch time: 0.5414, average train loss: 2.5354
[09/16 23:09:16 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1431, average loss: 3.2781
[09/16 23:09:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/16 23:09:38 visual_prompt]: 	Test 100/356. loss: 3.093, 0.2102 s / batch. (data: 2.73e-02)max mem: 17.22445 GB 
[09/16 23:09:57 visual_prompt]: 	Test 200/356. loss: 2.704, 0.1977 s / batch. (data: 1.49e-02)max mem: 17.22445 GB 
[09/16 23:10:17 visual_prompt]: 	Test 300/356. loss: 3.134, 0.1841 s / batch. (data: 1.18e-04)max mem: 17.22445 GB 
[09/16 23:10:29 visual_prompt]: Inference (test):avg data time: 7.44e-03, avg batch time: 0.1929, average loss: 3.0423
[09/16 23:10:30 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 88.42	
[09/16 23:10:30 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/16 23:10:40 visual_prompt]: Epoch 22 / 100: avg data time: 1.41e-01, avg batch time: 0.5455, average train loss: 2.4722
[09/16 23:10:44 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1431, average loss: 2.4475
[09/16 23:10:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/16 23:11:06 visual_prompt]: 	Test 100/356. loss: 2.314, 0.1950 s / batch. (data: 1.22e-02)max mem: 17.22445 GB 
[09/16 23:11:25 visual_prompt]: 	Test 200/356. loss: 2.390, 0.2051 s / batch. (data: 2.22e-02)max mem: 17.22445 GB 
[09/16 23:11:45 visual_prompt]: 	Test 300/356. loss: 2.510, 0.1833 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/16 23:11:57 visual_prompt]: Inference (test):avg data time: 8.08e-03, avg batch time: 0.1937, average loss: 2.3998
[09/16 23:11:57 visual_prompt]: Classification results with test_vtab-dmlab: top1: 14.60	top5: 81.44	
[09/16 23:11:57 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/16 23:12:08 visual_prompt]: Epoch 23 / 100: avg data time: 1.49e-01, avg batch time: 0.5518, average train loss: 2.5400
[09/16 23:12:12 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1431, average loss: 2.3505
[09/16 23:12:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/16 23:12:34 visual_prompt]: 	Test 100/356. loss: 2.395, 0.1846 s / batch. (data: 1.08e-04)max mem: 17.22445 GB 
[09/16 23:12:54 visual_prompt]: 	Test 200/356. loss: 1.982, 0.2068 s / batch. (data: 2.42e-02)max mem: 17.22445 GB 
[09/16 23:13:13 visual_prompt]: 	Test 300/356. loss: 2.131, 0.1960 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/16 23:13:26 visual_prompt]: Inference (test):avg data time: 8.17e-03, avg batch time: 0.1937, average loss: 2.2490
[09/16 23:13:26 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 85.40	
[09/16 23:13:26 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/16 23:13:36 visual_prompt]: Epoch 24 / 100: avg data time: 1.49e-01, avg batch time: 0.5528, average train loss: 2.3250
[09/16 23:13:40 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1430, average loss: 2.2180
[09/16 23:13:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/16 23:14:02 visual_prompt]: 	Test 100/356. loss: 2.201, 0.1978 s / batch. (data: 1.46e-02)max mem: 17.22445 GB 
[09/16 23:14:21 visual_prompt]: 	Test 200/356. loss: 1.913, 0.1857 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/16 23:14:41 visual_prompt]: 	Test 300/356. loss: 2.008, 0.1876 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/16 23:14:53 visual_prompt]: Inference (test):avg data time: 6.42e-03, avg batch time: 0.1927, average loss: 2.1197
[09/16 23:14:53 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 85.40	
[09/16 23:14:53 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/16 23:15:04 visual_prompt]: Epoch 25 / 100: avg data time: 1.46e-01, avg batch time: 0.5464, average train loss: 2.1387
[09/16 23:15:08 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1431, average loss: 1.9183
[09/16 23:15:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/16 23:15:29 visual_prompt]: 	Test 100/356. loss: 2.057, 0.2083 s / batch. (data: 2.53e-02)max mem: 17.22445 GB 
[09/16 23:15:49 visual_prompt]: 	Test 200/356. loss: 2.041, 0.1986 s / batch. (data: 1.56e-02)max mem: 17.22445 GB 
[09/16 23:16:08 visual_prompt]: 	Test 300/356. loss: 2.053, 0.1965 s / batch. (data: 1.22e-04)max mem: 17.22445 GB 
[09/16 23:16:21 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1936, average loss: 2.0011
[09/16 23:16:21 visual_prompt]: Classification results with test_vtab-dmlab: top1: 11.58	top5: 84.67	
[09/16 23:16:21 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/16 23:16:32 visual_prompt]: Epoch 26 / 100: avg data time: 1.48e-01, avg batch time: 0.5484, average train loss: 2.0210
[09/16 23:16:36 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1433, average loss: 2.0057
[09/16 23:16:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/16 23:16:58 visual_prompt]: 	Test 100/356. loss: 1.883, 0.2024 s / batch. (data: 1.99e-02)max mem: 17.22445 GB 
[09/16 23:17:17 visual_prompt]: 	Test 200/356. loss: 1.733, 0.1960 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/16 23:17:36 visual_prompt]: 	Test 300/356. loss: 1.883, 0.1841 s / batch. (data: 1.54e-04)max mem: 17.22445 GB 
[09/16 23:17:49 visual_prompt]: Inference (test):avg data time: 7.05e-03, avg batch time: 0.1938, average loss: 1.8820
[09/16 23:17:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 88.42	
[09/16 23:17:49 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/16 23:18:00 visual_prompt]: Epoch 27 / 100: avg data time: 1.44e-01, avg batch time: 0.5478, average train loss: 2.0227
[09/16 23:18:04 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1431, average loss: 2.1328
[09/16 23:18:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/16 23:18:26 visual_prompt]: 	Test 100/356. loss: 2.098, 0.1832 s / batch. (data: 1.09e-04)max mem: 17.22445 GB 
[09/16 23:18:45 visual_prompt]: 	Test 200/356. loss: 2.009, 0.1951 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/16 23:19:05 visual_prompt]: 	Test 300/356. loss: 2.026, 0.1834 s / batch. (data: 1.29e-04)max mem: 17.22445 GB 
[09/16 23:19:17 visual_prompt]: Inference (test):avg data time: 7.50e-03, avg batch time: 0.1937, average loss: 2.0661
[09/16 23:19:17 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 88.42	
[09/16 23:19:17 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/16 23:19:28 visual_prompt]: Epoch 28 / 100: avg data time: 1.44e-01, avg batch time: 0.5455, average train loss: 2.1448
[09/16 23:19:32 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1434, average loss: 1.8848
[09/16 23:19:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/16 23:19:54 visual_prompt]: 	Test 100/356. loss: 1.962, 0.1855 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/16 23:20:13 visual_prompt]: 	Test 200/356. loss: 2.083, 0.1844 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/16 23:20:33 visual_prompt]: 	Test 300/356. loss: 1.969, 0.1837 s / batch. (data: 1.25e-04)max mem: 17.22445 GB 
[09/16 23:20:46 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1957, average loss: 1.9678
[09/16 23:20:46 visual_prompt]: Classification results with test_vtab-dmlab: top1: 11.58	top5: 77.81	
[09/16 23:20:46 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/16 23:20:56 visual_prompt]: Epoch 29 / 100: avg data time: 1.35e-01, avg batch time: 0.5436, average train loss: 2.5820
[09/16 23:21:01 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1431, average loss: 2.2288
[09/16 23:21:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/16 23:21:23 visual_prompt]: 	Test 100/356. loss: 2.205, 0.2104 s / batch. (data: 2.78e-02)max mem: 17.22445 GB 
[09/16 23:21:42 visual_prompt]: 	Test 200/356. loss: 2.529, 0.1843 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/16 23:22:01 visual_prompt]: 	Test 300/356. loss: 2.310, 0.2100 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/16 23:22:14 visual_prompt]: Inference (test):avg data time: 8.30e-03, avg batch time: 0.1932, average loss: 2.3010
[09/16 23:22:14 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 77.81	
[09/16 23:22:14 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/16 23:22:24 visual_prompt]: Epoch 30 / 100: avg data time: 1.45e-01, avg batch time: 0.5486, average train loss: 2.2439
[09/16 23:22:28 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.1432, average loss: 2.4745
[09/16 23:22:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/16 23:22:50 visual_prompt]: 	Test 100/356. loss: 2.606, 0.2021 s / batch. (data: 3.81e-05)max mem: 17.22445 GB 
[09/16 23:23:09 visual_prompt]: 	Test 200/356. loss: 2.674, 0.1965 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/16 23:23:29 visual_prompt]: 	Test 300/356. loss: 2.700, 0.1839 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/16 23:23:42 visual_prompt]: Inference (test):avg data time: 8.17e-03, avg batch time: 0.1955, average loss: 2.5744
[09/16 23:23:42 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 77.81	
[09/16 23:23:42 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/16 23:23:52 visual_prompt]: Epoch 31 / 100: avg data time: 1.45e-01, avg batch time: 0.5468, average train loss: 2.1137
[09/16 23:23:57 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1471, average loss: 1.9067
[09/16 23:23:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 85.50	
[09/16 23:24:18 visual_prompt]: 	Test 100/356. loss: 1.922, 0.1835 s / batch. (data: 1.37e-04)max mem: 17.22445 GB 
[09/16 23:24:38 visual_prompt]: 	Test 200/356. loss: 1.803, 0.1837 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/16 23:24:57 visual_prompt]: 	Test 300/356. loss: 1.867, 0.2080 s / batch. (data: 2.47e-02)max mem: 17.22445 GB 
[09/16 23:25:10 visual_prompt]: Inference (test):avg data time: 7.85e-03, avg batch time: 0.1935, average loss: 1.8838
[09/16 23:25:10 visual_prompt]: Classification results with test_vtab-dmlab: top1: 23.44	top5: 84.67	
[09/16 23:25:10 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/16 23:25:20 visual_prompt]: Epoch 32 / 100: avg data time: 1.38e-01, avg batch time: 0.5416, average train loss: 1.9803
[09/16 23:25:24 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1440, average loss: 1.8259
[09/16 23:25:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/16 23:25:46 visual_prompt]: 	Test 100/356. loss: 1.841, 0.1983 s / batch. (data: 1.04e-04)max mem: 17.22445 GB 
[09/16 23:26:05 visual_prompt]: 	Test 200/356. loss: 1.744, 0.1834 s / batch. (data: 1.50e-04)max mem: 17.22445 GB 
[09/16 23:26:25 visual_prompt]: 	Test 300/356. loss: 1.785, 0.1955 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/16 23:26:37 visual_prompt]: Inference (test):avg data time: 7.26e-03, avg batch time: 0.1941, average loss: 1.7967
[09/16 23:26:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 85.40	
[09/16 23:26:37 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/16 23:26:48 visual_prompt]: Epoch 33 / 100: avg data time: 1.45e-01, avg batch time: 0.5476, average train loss: 1.8824
[09/16 23:26:52 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1435, average loss: 1.7678
[09/16 23:26:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 93.00	
[09/16 23:27:14 visual_prompt]: 	Test 100/356. loss: 1.769, 0.1984 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/16 23:27:33 visual_prompt]: 	Test 200/356. loss: 1.855, 0.1838 s / batch. (data: 1.32e-04)max mem: 17.22445 GB 
[09/16 23:27:53 visual_prompt]: 	Test 300/356. loss: 1.773, 0.1842 s / batch. (data: 1.06e-04)max mem: 17.22445 GB 
[09/16 23:28:05 visual_prompt]: Inference (test):avg data time: 8.28e-03, avg batch time: 0.1944, average loss: 1.7831
[09/16 23:28:05 visual_prompt]: Classification results with test_vtab-dmlab: top1: 16.20	top5: 90.85	
[09/16 23:28:05 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/16 23:28:16 visual_prompt]: Epoch 34 / 100: avg data time: 1.48e-01, avg batch time: 0.5511, average train loss: 1.9192
[09/16 23:28:20 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1432, average loss: 2.1375
[09/16 23:28:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 79.50	
[09/16 23:28:42 visual_prompt]: 	Test 100/356. loss: 1.869, 0.1826 s / batch. (data: 1.47e-04)max mem: 17.22445 GB 
[09/16 23:29:02 visual_prompt]: 	Test 200/356. loss: 1.982, 0.1892 s / batch. (data: 5.10e-03)max mem: 17.22445 GB 
[09/16 23:29:21 visual_prompt]: 	Test 300/356. loss: 1.985, 0.2015 s / batch. (data: 1.69e-04)max mem: 17.22445 GB 
[09/16 23:29:33 visual_prompt]: Inference (test):avg data time: 6.98e-03, avg batch time: 0.1944, average loss: 2.0129
[09/16 23:29:34 visual_prompt]: Classification results with test_vtab-dmlab: top1: 27.81	top5: 81.44	
[09/16 23:29:34 visual_prompt]: Best epoch 34: best metric: 0.230
[09/16 23:29:34 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/16 23:29:44 visual_prompt]: Epoch 35 / 100: avg data time: 1.40e-01, avg batch time: 0.5417, average train loss: 1.8539
[09/16 23:29:48 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1430, average loss: 1.7295
[09/16 23:29:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.00	top5: 87.50	
[09/16 23:30:10 visual_prompt]: 	Test 100/356. loss: 1.719, 0.2099 s / batch. (data: 2.13e-02)max mem: 17.22445 GB 
[09/16 23:30:29 visual_prompt]: 	Test 200/356. loss: 1.695, 0.1877 s / batch. (data: 1.68e-04)max mem: 17.22445 GB 
[09/16 23:30:49 visual_prompt]: 	Test 300/356. loss: 1.672, 0.1977 s / batch. (data: 1.41e-02)max mem: 17.22445 GB 
[09/16 23:31:01 visual_prompt]: Inference (test):avg data time: 7.76e-03, avg batch time: 0.1936, average loss: 1.7354
[09/16 23:31:01 visual_prompt]: Classification results with test_vtab-dmlab: top1: 26.71	top5: 89.53	
[09/16 23:31:01 visual_prompt]: Best epoch 35: best metric: 0.260
[09/16 23:31:01 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/16 23:31:12 visual_prompt]: Epoch 36 / 100: avg data time: 1.49e-01, avg batch time: 0.5535, average train loss: 2.1617
[09/16 23:31:16 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.1432, average loss: 2.5220
[09/16 23:31:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/16 23:31:38 visual_prompt]: 	Test 100/356. loss: 2.445, 0.1837 s / batch. (data: 1.31e-04)max mem: 17.22445 GB 
[09/16 23:31:57 visual_prompt]: 	Test 200/356. loss: 2.168, 0.1969 s / batch. (data: 1.35e-02)max mem: 17.22445 GB 
[09/16 23:32:17 visual_prompt]: 	Test 300/356. loss: 2.396, 0.1995 s / batch. (data: 1.62e-02)max mem: 17.22445 GB 
[09/16 23:32:29 visual_prompt]: Inference (test):avg data time: 8.34e-03, avg batch time: 0.1941, average loss: 2.3814
[09/16 23:32:30 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 88.42	
[09/16 23:32:30 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/16 23:32:40 visual_prompt]: Epoch 37 / 100: avg data time: 1.44e-01, avg batch time: 0.5462, average train loss: 1.8549
[09/16 23:32:44 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1432, average loss: 1.8022
[09/16 23:32:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 96.00	
[09/16 23:33:06 visual_prompt]: 	Test 100/356. loss: 1.805, 0.2126 s / batch. (data: 2.96e-02)max mem: 17.22445 GB 
[09/16 23:33:26 visual_prompt]: 	Test 200/356. loss: 1.776, 0.1959 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/16 23:33:45 visual_prompt]: 	Test 300/356. loss: 1.678, 0.1840 s / batch. (data: 1.06e-04)max mem: 17.22445 GB 
[09/16 23:33:58 visual_prompt]: Inference (test):avg data time: 9.00e-03, avg batch time: 0.1955, average loss: 1.7330
[09/16 23:33:58 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.16	top5: 95.70	
[09/16 23:33:58 visual_prompt]: Best epoch 37: best metric: 0.275
[09/16 23:33:58 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/16 23:34:08 visual_prompt]: Epoch 38 / 100: avg data time: 1.53e-01, avg batch time: 0.5568, average train loss: 1.7135
[09/16 23:34:12 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1441, average loss: 1.9159
[09/16 23:34:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 84.50	
[09/16 23:34:35 visual_prompt]: 	Test 100/356. loss: 2.094, 0.1887 s / batch. (data: 1.27e-04)max mem: 17.22445 GB 
[09/16 23:34:54 visual_prompt]: 	Test 200/356. loss: 2.524, 0.1917 s / batch. (data: 1.62e-04)max mem: 17.22445 GB 
[09/16 23:35:14 visual_prompt]: 	Test 300/356. loss: 2.181, 0.1878 s / batch. (data: 1.26e-04)max mem: 17.22445 GB 
[09/16 23:35:27 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1965, average loss: 2.1239
[09/16 23:35:27 visual_prompt]: Classification results with test_vtab-dmlab: top1: 30.28	top5: 78.40	
[09/16 23:35:27 visual_prompt]: Best epoch 38: best metric: 0.335
[09/16 23:35:27 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/16 23:35:37 visual_prompt]: Epoch 39 / 100: avg data time: 1.37e-01, avg batch time: 0.5412, average train loss: 1.7393
[09/16 23:35:41 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1428, average loss: 1.5712
[09/16 23:35:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 87.50	
[09/16 23:36:03 visual_prompt]: 	Test 100/356. loss: 1.568, 0.1839 s / batch. (data: 1.50e-04)max mem: 17.22445 GB 
[09/16 23:36:23 visual_prompt]: 	Test 200/356. loss: 1.469, 0.1938 s / batch. (data: 1.04e-02)max mem: 17.22445 GB 
[09/16 23:36:42 visual_prompt]: 	Test 300/356. loss: 1.461, 0.1842 s / batch. (data: 1.27e-04)max mem: 17.22445 GB 
[09/16 23:36:55 visual_prompt]: Inference (test):avg data time: 8.18e-03, avg batch time: 0.1937, average loss: 1.5485
[09/16 23:36:55 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.64	top5: 87.14	
[09/16 23:36:55 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/16 23:37:05 visual_prompt]: Epoch 40 / 100: avg data time: 1.45e-01, avg batch time: 0.5572, average train loss: 1.8809
[09/16 23:37:09 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1450, average loss: 1.7152
[09/16 23:37:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 91.50	
[09/16 23:37:31 visual_prompt]: 	Test 100/356. loss: 1.569, 0.1956 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/16 23:37:51 visual_prompt]: 	Test 200/356. loss: 1.734, 0.1967 s / batch. (data: 1.38e-02)max mem: 17.22445 GB 
[09/16 23:38:10 visual_prompt]: 	Test 300/356. loss: 1.632, 0.2042 s / batch. (data: 1.52e-02)max mem: 17.22445 GB 
[09/16 23:38:22 visual_prompt]: Inference (test):avg data time: 7.04e-03, avg batch time: 0.1930, average loss: 1.6763
[09/16 23:38:22 visual_prompt]: Classification results with test_vtab-dmlab: top1: 33.65	top5: 89.52	
[09/16 23:38:22 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/16 23:38:33 visual_prompt]: Epoch 41 / 100: avg data time: 1.45e-01, avg batch time: 0.5487, average train loss: 2.3475
[09/16 23:38:37 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1430, average loss: 2.6233
[09/16 23:38:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/16 23:38:59 visual_prompt]: 	Test 100/356. loss: 2.197, 0.1981 s / batch. (data: 1.52e-02)max mem: 17.22445 GB 
[09/16 23:39:19 visual_prompt]: 	Test 200/356. loss: 2.624, 0.1840 s / batch. (data: 1.59e-04)max mem: 17.22445 GB 
[09/16 23:39:38 visual_prompt]: 	Test 300/356. loss: 2.428, 0.1838 s / batch. (data: 1.12e-04)max mem: 17.22445 GB 
[09/16 23:39:51 visual_prompt]: Inference (test):avg data time: 7.32e-03, avg batch time: 0.1946, average loss: 2.5060
[09/16 23:39:51 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.19	top5: 81.44	
[09/16 23:39:51 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/16 23:40:01 visual_prompt]: Epoch 42 / 100: avg data time: 1.41e-01, avg batch time: 0.5435, average train loss: 1.7126
[09/16 23:40:05 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1433, average loss: 1.4650
[09/16 23:40:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 98.50	
[09/16 23:40:27 visual_prompt]: 	Test 100/356. loss: 1.431, 0.2021 s / batch. (data: 1.92e-02)max mem: 17.22445 GB 
[09/16 23:40:46 visual_prompt]: 	Test 200/356. loss: 1.425, 0.1981 s / batch. (data: 1.46e-02)max mem: 17.22445 GB 
[09/16 23:41:06 visual_prompt]: 	Test 300/356. loss: 1.395, 0.1840 s / batch. (data: 1.54e-04)max mem: 17.22445 GB 
[09/16 23:41:18 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1936, average loss: 1.4551
[09/16 23:41:18 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.12	top5: 97.60	
[09/16 23:41:18 visual_prompt]: Best epoch 42: best metric: 0.340
[09/16 23:41:18 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/16 23:41:29 visual_prompt]: Epoch 43 / 100: avg data time: 1.45e-01, avg batch time: 0.5490, average train loss: 1.5403
[09/16 23:41:33 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1429, average loss: 1.5359
[09/16 23:41:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 90.00	
[09/16 23:41:54 visual_prompt]: 	Test 100/356. loss: 1.584, 0.1974 s / batch. (data: 1.45e-02)max mem: 17.22445 GB 
[09/16 23:42:14 visual_prompt]: 	Test 200/356. loss: 1.570, 0.1843 s / batch. (data: 1.36e-04)max mem: 17.22445 GB 
[09/16 23:42:33 visual_prompt]: 	Test 300/356. loss: 1.444, 0.1959 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/16 23:42:46 visual_prompt]: Inference (test):avg data time: 7.67e-03, avg batch time: 0.1934, average loss: 1.5405
[09/16 23:42:46 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.66	top5: 91.32	
[09/16 23:42:46 visual_prompt]: Best epoch 43: best metric: 0.345
[09/16 23:42:46 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/16 23:42:56 visual_prompt]: Epoch 44 / 100: avg data time: 1.44e-01, avg batch time: 0.5474, average train loss: 1.6238
[09/16 23:43:01 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1429, average loss: 1.6630
[09/16 23:43:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 96.00	
[09/16 23:43:22 visual_prompt]: 	Test 100/356. loss: 1.707, 0.1834 s / batch. (data: 1.31e-04)max mem: 17.22445 GB 
[09/16 23:43:42 visual_prompt]: 	Test 200/356. loss: 1.794, 0.2374 s / batch. (data: 5.63e-05)max mem: 17.22445 GB 
[09/16 23:44:01 visual_prompt]: 	Test 300/356. loss: 1.574, 0.1843 s / batch. (data: 1.16e-04)max mem: 17.22445 GB 
[09/16 23:44:14 visual_prompt]: Inference (test):avg data time: 7.91e-03, avg batch time: 0.1935, average loss: 1.7519
[09/16 23:44:14 visual_prompt]: Classification results with test_vtab-dmlab: top1: 29.14	top5: 95.34	
[09/16 23:44:14 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/16 23:44:24 visual_prompt]: Epoch 45 / 100: avg data time: 1.37e-01, avg batch time: 0.5404, average train loss: 1.6058
[09/16 23:44:28 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1431, average loss: 1.7465
[09/16 23:44:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 96.00	
[09/16 23:44:50 visual_prompt]: 	Test 100/356. loss: 1.755, 0.1989 s / batch. (data: 1.55e-02)max mem: 17.22445 GB 
[09/16 23:45:10 visual_prompt]: 	Test 200/356. loss: 1.940, 0.2081 s / batch. (data: 2.45e-02)max mem: 17.22445 GB 
[09/16 23:45:29 visual_prompt]: 	Test 300/356. loss: 1.577, 0.1849 s / batch. (data: 1.32e-04)max mem: 17.22445 GB 
[09/16 23:45:42 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1943, average loss: 1.7521
[09/16 23:45:42 visual_prompt]: Classification results with test_vtab-dmlab: top1: 29.52	top5: 95.55	
[09/16 23:45:42 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/16 23:45:52 visual_prompt]: Epoch 46 / 100: avg data time: 1.39e-01, avg batch time: 0.5420, average train loss: 1.5162
[09/16 23:45:57 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1430, average loss: 1.4684
[09/16 23:45:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 97.00	
[09/16 23:46:18 visual_prompt]: 	Test 100/356. loss: 1.578, 0.1836 s / batch. (data: 1.61e-04)max mem: 17.22445 GB 
[09/16 23:46:38 visual_prompt]: 	Test 200/356. loss: 1.644, 0.1895 s / batch. (data: 1.18e-04)max mem: 17.22445 GB 
[09/16 23:46:57 visual_prompt]: 	Test 300/356. loss: 1.456, 0.1839 s / batch. (data: 1.25e-04)max mem: 17.22445 GB 
[09/16 23:47:10 visual_prompt]: Inference (test):avg data time: 7.90e-03, avg batch time: 0.1935, average loss: 1.5313
[09/16 23:47:10 visual_prompt]: Classification results with test_vtab-dmlab: top1: 33.77	top5: 96.32	
[09/16 23:47:10 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/16 23:47:20 visual_prompt]: Epoch 47 / 100: avg data time: 1.35e-01, avg batch time: 0.5466, average train loss: 1.4909
[09/16 23:47:25 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1431, average loss: 1.8124
[09/16 23:47:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.50	top5: 94.00	
[09/16 23:47:46 visual_prompt]: 	Test 100/356. loss: 1.724, 0.1838 s / batch. (data: 1.72e-04)max mem: 17.22445 GB 
[09/16 23:48:06 visual_prompt]: 	Test 200/356. loss: 1.567, 0.1978 s / batch. (data: 1.49e-02)max mem: 17.22445 GB 
[09/16 23:48:25 visual_prompt]: 	Test 300/356. loss: 1.805, 0.1887 s / batch. (data: 1.27e-04)max mem: 17.22445 GB 
[09/16 23:48:38 visual_prompt]: Inference (test):avg data time: 7.57e-03, avg batch time: 0.1938, average loss: 1.8118
[09/16 23:48:38 visual_prompt]: Classification results with test_vtab-dmlab: top1: 30.37	top5: 93.24	
[09/16 23:48:38 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/16 23:48:48 visual_prompt]: Epoch 48 / 100: avg data time: 1.42e-01, avg batch time: 0.5460, average train loss: 1.6105
[09/16 23:48:52 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1453, average loss: 1.4737
[09/16 23:48:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 98.50	
[09/16 23:49:14 visual_prompt]: 	Test 100/356. loss: 1.571, 0.2161 s / batch. (data: 3.34e-02)max mem: 17.22445 GB 
[09/16 23:49:33 visual_prompt]: 	Test 200/356. loss: 1.463, 0.1961 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/16 23:49:53 visual_prompt]: 	Test 300/356. loss: 1.414, 0.1960 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/16 23:50:05 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1937, average loss: 1.5400
[09/16 23:50:06 visual_prompt]: Classification results with test_vtab-dmlab: top1: 33.70	top5: 97.97	
[09/16 23:50:06 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/16 23:50:16 visual_prompt]: Epoch 49 / 100: avg data time: 1.39e-01, avg batch time: 0.5420, average train loss: 1.4106
[09/16 23:50:20 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1431, average loss: 1.2857
[09/16 23:50:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 99.00	
[09/16 23:50:42 visual_prompt]: 	Test 100/356. loss: 1.503, 0.1901 s / batch. (data: 1.28e-04)max mem: 17.22445 GB 
[09/16 23:51:02 visual_prompt]: 	Test 200/356. loss: 1.558, 0.2126 s / batch. (data: 2.95e-02)max mem: 17.22445 GB 
[09/16 23:51:21 visual_prompt]: 	Test 300/356. loss: 1.427, 0.1906 s / batch. (data: 1.88e-04)max mem: 17.22445 GB 
[09/16 23:51:34 visual_prompt]: Inference (test):avg data time: 7.61e-03, avg batch time: 0.1947, average loss: 1.4758
[09/16 23:51:34 visual_prompt]: Classification results with test_vtab-dmlab: top1: 32.83	top5: 96.43	
[09/16 23:51:34 visual_prompt]: Best epoch 49: best metric: 0.415
[09/16 23:51:34 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/16 23:51:44 visual_prompt]: Epoch 50 / 100: avg data time: 1.29e-01, avg batch time: 0.5310, average train loss: 1.4058
[09/16 23:51:48 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.1447, average loss: 1.4022
[09/16 23:51:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.50	
[09/16 23:52:10 visual_prompt]: 	Test 100/356. loss: 1.538, 0.2027 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/16 23:52:29 visual_prompt]: 	Test 200/356. loss: 1.781, 0.1836 s / batch. (data: 1.59e-04)max mem: 17.22445 GB 
[09/16 23:52:49 visual_prompt]: 	Test 300/356. loss: 1.392, 0.1843 s / batch. (data: 1.56e-04)max mem: 17.22445 GB 
[09/16 23:53:01 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1930, average loss: 1.5271
[09/16 23:53:01 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.34	top5: 95.93	
[09/16 23:53:01 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/16 23:53:11 visual_prompt]: Epoch 51 / 100: avg data time: 1.30e-01, avg batch time: 0.5366, average train loss: 1.3399
[09/16 23:53:15 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1433, average loss: 1.5271
[09/16 23:53:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 98.00	
[09/16 23:53:37 visual_prompt]: 	Test 100/356. loss: 1.411, 0.1957 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/16 23:53:56 visual_prompt]: 	Test 200/356. loss: 1.479, 0.1862 s / batch. (data: 1.12e-04)max mem: 17.22445 GB 
[09/16 23:54:17 visual_prompt]: 	Test 300/356. loss: 1.379, 0.1939 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/16 23:54:29 visual_prompt]: Inference (test):avg data time: 8.55e-03, avg batch time: 0.1951, average loss: 1.5258
[09/16 23:54:29 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.05	top5: 97.07	
[09/16 23:54:29 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/16 23:54:39 visual_prompt]: Epoch 52 / 100: avg data time: 1.36e-01, avg batch time: 0.5397, average train loss: 1.3548
[09/16 23:54:43 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1430, average loss: 1.3526
[09/16 23:54:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 99.00	
[09/16 23:55:05 visual_prompt]: 	Test 100/356. loss: 1.512, 0.1959 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/16 23:55:25 visual_prompt]: 	Test 200/356. loss: 1.823, 0.1957 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/16 23:55:44 visual_prompt]: 	Test 300/356. loss: 1.574, 0.1838 s / batch. (data: 1.17e-04)max mem: 17.22445 GB 
[09/16 23:55:57 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1934, average loss: 1.5548
[09/16 23:55:57 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.23	top5: 96.74	
[09/16 23:55:57 visual_prompt]: Best epoch 52: best metric: 0.435
[09/16 23:55:57 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/16 23:56:07 visual_prompt]: Epoch 53 / 100: avg data time: 1.43e-01, avg batch time: 0.5462, average train loss: 1.4120
[09/16 23:56:11 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.1472, average loss: 1.4485
[09/16 23:56:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 99.50	
[09/16 23:56:33 visual_prompt]: 	Test 100/356. loss: 1.535, 0.1981 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/16 23:56:53 visual_prompt]: 	Test 200/356. loss: 1.825, 0.2087 s / batch. (data: 2.58e-02)max mem: 17.22445 GB 
[09/16 23:57:12 visual_prompt]: 	Test 300/356. loss: 1.484, 0.1848 s / batch. (data: 9.92e-05)max mem: 17.22445 GB 
[09/16 23:57:25 visual_prompt]: Inference (test):avg data time: 8.33e-03, avg batch time: 0.1953, average loss: 1.6197
[09/16 23:57:25 visual_prompt]: Classification results with test_vtab-dmlab: top1: 32.04	top5: 96.45	
[09/16 23:57:25 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/16 23:57:35 visual_prompt]: Epoch 54 / 100: avg data time: 1.36e-01, avg batch time: 0.5385, average train loss: 1.4571
[09/16 23:57:40 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1433, average loss: 1.6406
[09/16 23:57:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 100.00	
[09/16 23:58:01 visual_prompt]: 	Test 100/356. loss: 1.943, 0.1952 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/16 23:58:20 visual_prompt]: 	Test 200/356. loss: 1.863, 0.1972 s / batch. (data: 1.40e-04)max mem: 17.22445 GB 
[09/16 23:58:40 visual_prompt]: 	Test 300/356. loss: 1.561, 0.1962 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/16 23:58:53 visual_prompt]: Inference (test):avg data time: 7.21e-03, avg batch time: 0.1929, average loss: 1.8143
[09/16 23:58:53 visual_prompt]: Classification results with test_vtab-dmlab: top1: 32.31	top5: 97.99	
[09/16 23:58:53 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/16 23:59:03 visual_prompt]: Epoch 55 / 100: avg data time: 1.46e-01, avg batch time: 0.5468, average train loss: 1.7148
[09/16 23:59:07 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1460, average loss: 1.7077
[09/16 23:59:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 99.00	
[09/16 23:59:29 visual_prompt]: 	Test 100/356. loss: 1.841, 0.2078 s / batch. (data: 2.52e-02)max mem: 17.22445 GB 
[09/16 23:59:48 visual_prompt]: 	Test 200/356. loss: 1.666, 0.1833 s / batch. (data: 1.22e-04)max mem: 17.22445 GB 
[09/17 00:00:07 visual_prompt]: 	Test 300/356. loss: 1.622, 0.1988 s / batch. (data: 1.47e-02)max mem: 17.22445 GB 
[09/17 00:00:20 visual_prompt]: Inference (test):avg data time: 7.60e-03, avg batch time: 0.1933, average loss: 1.7304
[09/17 00:00:20 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.80	top5: 97.56	
[09/17 00:00:20 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/17 00:00:30 visual_prompt]: Epoch 56 / 100: avg data time: 1.44e-01, avg batch time: 0.5475, average train loss: 1.4745
[09/17 00:00:35 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1433, average loss: 1.6807
[09/17 00:00:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 96.50	
[09/17 00:00:56 visual_prompt]: 	Test 100/356. loss: 1.544, 0.1834 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 00:01:16 visual_prompt]: 	Test 200/356. loss: 1.552, 0.1985 s / batch. (data: 1.55e-02)max mem: 17.22445 GB 
[09/17 00:01:35 visual_prompt]: 	Test 300/356. loss: 1.539, 0.1966 s / batch. (data: 9.44e-05)max mem: 17.22445 GB 
[09/17 00:01:48 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1928, average loss: 1.6682
[09/17 00:01:48 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.91	top5: 96.35	
[09/17 00:01:48 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/17 00:01:58 visual_prompt]: Epoch 57 / 100: avg data time: 1.46e-01, avg batch time: 0.5489, average train loss: 1.4372
[09/17 00:02:02 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1434, average loss: 1.2640
[09/17 00:02:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 99.00	
[09/17 00:02:24 visual_prompt]: 	Test 100/356. loss: 1.487, 0.1834 s / batch. (data: 9.89e-05)max mem: 17.22445 GB 
[09/17 00:02:43 visual_prompt]: 	Test 200/356. loss: 1.538, 0.1960 s / batch. (data: 3.09e-03)max mem: 17.22445 GB 
[09/17 00:03:03 visual_prompt]: 	Test 300/356. loss: 1.278, 0.1993 s / batch. (data: 1.61e-02)max mem: 17.22445 GB 
[09/17 00:03:15 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1933, average loss: 1.4716
[09/17 00:03:15 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.18	top5: 97.80	
[09/17 00:03:15 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/17 00:03:26 visual_prompt]: Epoch 58 / 100: avg data time: 1.43e-01, avg batch time: 0.5466, average train loss: 1.2815
[09/17 00:03:30 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1432, average loss: 1.3441
[09/17 00:03:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 99.00	
[09/17 00:03:52 visual_prompt]: 	Test 100/356. loss: 1.434, 0.2071 s / batch. (data: 2.47e-02)max mem: 17.22445 GB 
[09/17 00:04:11 visual_prompt]: 	Test 200/356. loss: 1.701, 0.1897 s / batch. (data: 1.22e-04)max mem: 17.22445 GB 
[09/17 00:04:30 visual_prompt]: 	Test 300/356. loss: 1.420, 0.1840 s / batch. (data: 1.31e-04)max mem: 17.22445 GB 
[09/17 00:04:43 visual_prompt]: Inference (test):avg data time: 7.76e-03, avg batch time: 0.1933, average loss: 1.5944
[09/17 00:04:43 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.20	top5: 97.10	
[09/17 00:04:43 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/17 00:04:53 visual_prompt]: Epoch 59 / 100: avg data time: 1.40e-01, avg batch time: 0.5436, average train loss: 1.2279
[09/17 00:04:58 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1433, average loss: 1.1282
[09/17 00:04:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 99.00	
[09/17 00:05:19 visual_prompt]: 	Test 100/356. loss: 1.203, 0.1830 s / batch. (data: 1.90e-04)max mem: 17.22445 GB 
[09/17 00:05:39 visual_prompt]: 	Test 200/356. loss: 1.525, 0.1836 s / batch. (data: 1.54e-04)max mem: 17.22445 GB 
[09/17 00:05:58 visual_prompt]: 	Test 300/356. loss: 1.358, 0.1847 s / batch. (data: 1.35e-04)max mem: 17.22445 GB 
[09/17 00:06:11 visual_prompt]: Inference (test):avg data time: 7.06e-03, avg batch time: 0.1929, average loss: 1.3812
[09/17 00:06:11 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.77	top5: 97.23	
[09/17 00:06:11 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/17 00:06:21 visual_prompt]: Epoch 60 / 100: avg data time: 1.39e-01, avg batch time: 0.5407, average train loss: 1.1640
[09/17 00:06:25 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1433, average loss: 1.1239
[09/17 00:06:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 99.50	
[09/17 00:06:47 visual_prompt]: 	Test 100/356. loss: 1.476, 0.2043 s / batch. (data: 1.47e-02)max mem: 17.22445 GB 
[09/17 00:07:07 visual_prompt]: 	Test 200/356. loss: 1.669, 0.1836 s / batch. (data: 1.05e-04)max mem: 17.22445 GB 
[09/17 00:07:26 visual_prompt]: 	Test 300/356. loss: 1.381, 0.1844 s / batch. (data: 1.42e-04)max mem: 17.22445 GB 
[09/17 00:07:39 visual_prompt]: Inference (test):avg data time: 7.30e-03, avg batch time: 0.1942, average loss: 1.5099
[09/17 00:07:39 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.51	top5: 97.05	
[09/17 00:07:39 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/17 00:07:49 visual_prompt]: Epoch 61 / 100: avg data time: 1.45e-01, avg batch time: 0.5476, average train loss: 1.1124
[09/17 00:07:53 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1430, average loss: 1.1541
[09/17 00:07:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 100.00	
[09/17 00:08:15 visual_prompt]: 	Test 100/356. loss: 1.312, 0.1835 s / batch. (data: 1.27e-04)max mem: 17.22445 GB 
[09/17 00:08:34 visual_prompt]: 	Test 200/356. loss: 1.297, 0.2099 s / batch. (data: 4.52e-03)max mem: 17.22445 GB 
[09/17 00:08:54 visual_prompt]: 	Test 300/356. loss: 1.337, 0.1962 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/17 00:09:07 visual_prompt]: Inference (test):avg data time: 8.13e-03, avg batch time: 0.1946, average loss: 1.4500
[09/17 00:09:07 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.81	top5: 98.06	
[09/17 00:09:07 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/17 00:09:17 visual_prompt]: Epoch 62 / 100: avg data time: 1.51e-01, avg batch time: 0.5527, average train loss: 1.0909
[09/17 00:09:21 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1433, average loss: 1.3118
[09/17 00:09:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 100.00	
[09/17 00:09:43 visual_prompt]: 	Test 100/356. loss: 1.560, 0.1837 s / batch. (data: 1.18e-04)max mem: 17.22445 GB 
[09/17 00:10:03 visual_prompt]: 	Test 200/356. loss: 1.404, 0.1832 s / batch. (data: 1.23e-04)max mem: 17.22445 GB 
[09/17 00:10:23 visual_prompt]: 	Test 300/356. loss: 1.248, 0.1984 s / batch. (data: 1.49e-02)max mem: 17.22445 GB 
[09/17 00:10:35 visual_prompt]: Inference (test):avg data time: 7.50e-03, avg batch time: 0.1950, average loss: 1.5691
[09/17 00:10:35 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.43	top5: 98.07	
[09/17 00:10:35 visual_prompt]: Best epoch 62: best metric: 0.450
[09/17 00:10:35 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/17 00:10:45 visual_prompt]: Epoch 63 / 100: avg data time: 1.32e-01, avg batch time: 0.5363, average train loss: 1.2283
[09/17 00:10:50 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1430, average loss: 1.1359
[09/17 00:10:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.50	top5: 98.50	
[09/17 00:11:11 visual_prompt]: 	Test 100/356. loss: 1.491, 0.1833 s / batch. (data: 1.19e-04)max mem: 17.22445 GB 
[09/17 00:11:31 visual_prompt]: 	Test 200/356. loss: 1.470, 0.1900 s / batch. (data: 1.54e-04)max mem: 17.22445 GB 
[09/17 00:11:50 visual_prompt]: 	Test 300/356. loss: 1.262, 0.1961 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 00:12:03 visual_prompt]: Inference (test):avg data time: 8.28e-03, avg batch time: 0.1938, average loss: 1.4809
[09/17 00:12:03 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.78	top5: 97.31	
[09/17 00:12:03 visual_prompt]: Best epoch 63: best metric: 0.475
[09/17 00:12:03 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/17 00:12:13 visual_prompt]: Epoch 64 / 100: avg data time: 1.44e-01, avg batch time: 0.5462, average train loss: 1.1465
[09/17 00:12:18 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1465, average loss: 1.1470
[09/17 00:12:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 99.00	
[09/17 00:12:39 visual_prompt]: 	Test 100/356. loss: 1.482, 0.1838 s / batch. (data: 1.72e-04)max mem: 17.22445 GB 
[09/17 00:12:59 visual_prompt]: 	Test 200/356. loss: 1.923, 0.1978 s / batch. (data: 1.49e-02)max mem: 17.22445 GB 
[09/17 00:13:19 visual_prompt]: 	Test 300/356. loss: 1.405, 0.1864 s / batch. (data: 1.23e-04)max mem: 17.22445 GB 
[09/17 00:13:31 visual_prompt]: Inference (test):avg data time: 7.80e-03, avg batch time: 0.1950, average loss: 1.5284
[09/17 00:13:31 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.90	top5: 96.78	
[09/17 00:13:31 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/17 00:13:42 visual_prompt]: Epoch 65 / 100: avg data time: 1.38e-01, avg batch time: 0.5401, average train loss: 1.1513
[09/17 00:13:46 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1432, average loss: 1.0386
[09/17 00:13:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.50	top5: 99.50	
[09/17 00:14:08 visual_prompt]: 	Test 100/356. loss: 1.471, 0.1838 s / batch. (data: 1.55e-04)max mem: 17.22445 GB 
[09/17 00:14:27 visual_prompt]: 	Test 200/356. loss: 1.623, 0.2177 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 00:14:46 visual_prompt]: 	Test 300/356. loss: 1.296, 0.1958 s / batch. (data: 1.24e-02)max mem: 17.22445 GB 
[09/17 00:14:59 visual_prompt]: Inference (test):avg data time: 7.54e-03, avg batch time: 0.1929, average loss: 1.4368
[09/17 00:14:59 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.78	top5: 97.77	
[09/17 00:14:59 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/17 00:15:09 visual_prompt]: Epoch 66 / 100: avg data time: 1.30e-01, avg batch time: 0.5394, average train loss: 1.0762
[09/17 00:15:13 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1432, average loss: 1.2161
[09/17 00:15:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 99.50	
[09/17 00:15:35 visual_prompt]: 	Test 100/356. loss: 1.616, 0.1828 s / batch. (data: 1.79e-04)max mem: 17.22445 GB 
[09/17 00:15:54 visual_prompt]: 	Test 200/356. loss: 1.899, 0.1838 s / batch. (data: 1.66e-04)max mem: 17.22445 GB 
[09/17 00:16:14 visual_prompt]: 	Test 300/356. loss: 1.493, 0.1860 s / batch. (data: 1.27e-04)max mem: 17.22445 GB 
[09/17 00:16:26 visual_prompt]: Inference (test):avg data time: 8.04e-03, avg batch time: 0.1931, average loss: 1.6340
[09/17 00:16:27 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.89	top5: 96.87	
[09/17 00:16:27 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/17 00:16:37 visual_prompt]: Epoch 67 / 100: avg data time: 1.47e-01, avg batch time: 0.5493, average train loss: 1.1216
[09/17 00:16:41 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1432, average loss: 1.1579
[09/17 00:16:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 48.00	top5: 100.00	
[09/17 00:17:03 visual_prompt]: 	Test 100/356. loss: 1.435, 0.2098 s / batch. (data: 1.61e-02)max mem: 17.22445 GB 
[09/17 00:17:22 visual_prompt]: 	Test 200/356. loss: 1.665, 0.1869 s / batch. (data: 1.19e-04)max mem: 17.22445 GB 
[09/17 00:17:42 visual_prompt]: 	Test 300/356. loss: 1.351, 0.1856 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/17 00:17:54 visual_prompt]: Inference (test):avg data time: 7.54e-03, avg batch time: 0.1940, average loss: 1.5618
[09/17 00:17:54 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.63	top5: 97.91	
[09/17 00:17:54 visual_prompt]: Best epoch 67: best metric: 0.480
[09/17 00:17:54 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/17 00:18:05 visual_prompt]: Epoch 68 / 100: avg data time: 1.45e-01, avg batch time: 0.5471, average train loss: 1.0502
[09/17 00:18:09 visual_prompt]: Inference (val):avg data time: 4.84e-05, avg batch time: 0.1433, average loss: 1.0270
[09/17 00:18:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 49.50	top5: 99.50	
[09/17 00:18:31 visual_prompt]: 	Test 100/356. loss: 1.493, 0.1950 s / batch. (data: 1.02e-02)max mem: 17.22445 GB 
[09/17 00:18:51 visual_prompt]: 	Test 200/356. loss: 1.975, 0.1962 s / batch. (data: 1.31e-02)max mem: 17.22445 GB 
[09/17 00:19:10 visual_prompt]: 	Test 300/356. loss: 1.483, 0.1991 s / batch. (data: 1.53e-02)max mem: 17.22445 GB 
[09/17 00:19:23 visual_prompt]: Inference (test):avg data time: 8.76e-03, avg batch time: 0.1954, average loss: 1.5697
[09/17 00:19:23 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.34	top5: 96.18	
[09/17 00:19:23 visual_prompt]: Best epoch 68: best metric: 0.495
[09/17 00:19:23 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/17 00:19:33 visual_prompt]: Epoch 69 / 100: avg data time: 1.25e-01, avg batch time: 0.5303, average train loss: 1.0216
[09/17 00:19:38 visual_prompt]: Inference (val):avg data time: 4.79e-05, avg batch time: 0.1466, average loss: 1.1063
[09/17 00:19:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 49.00	top5: 100.00	
[09/17 00:20:00 visual_prompt]: 	Test 100/356. loss: 1.598, 0.1848 s / batch. (data: 1.54e-04)max mem: 17.22445 GB 
[09/17 00:20:19 visual_prompt]: 	Test 200/356. loss: 1.729, 0.1957 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 00:20:40 visual_prompt]: 	Test 300/356. loss: 1.431, 0.1911 s / batch. (data: 9.63e-05)max mem: 17.22445 GB 
[09/17 00:20:52 visual_prompt]: Inference (test):avg data time: 8.21e-03, avg batch time: 0.1967, average loss: 1.5808
[09/17 00:20:52 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.60	top5: 97.44	
[09/17 00:20:52 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/17 00:21:03 visual_prompt]: Epoch 70 / 100: avg data time: 1.36e-01, avg batch time: 0.5408, average train loss: 1.0765
[09/17 00:21:07 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1431, average loss: 1.3947
[09/17 00:21:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 99.00	
[09/17 00:21:29 visual_prompt]: 	Test 100/356. loss: 1.872, 0.1833 s / batch. (data: 1.45e-04)max mem: 17.22445 GB 
[09/17 00:21:48 visual_prompt]: 	Test 200/356. loss: 1.653, 0.1871 s / batch. (data: 6.53e-05)max mem: 17.22445 GB 
[09/17 00:22:07 visual_prompt]: 	Test 300/356. loss: 1.647, 0.1954 s / batch. (data: 9.73e-05)max mem: 17.22445 GB 
[09/17 00:22:20 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1933, average loss: 1.8379
[09/17 00:22:20 visual_prompt]: Classification results with test_vtab-dmlab: top1: 33.77	top5: 95.40	
[09/17 00:22:20 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/17 00:22:30 visual_prompt]: Epoch 71 / 100: avg data time: 1.43e-01, avg batch time: 0.5448, average train loss: 1.1770
[09/17 00:22:35 visual_prompt]: Inference (val):avg data time: 4.52e-05, avg batch time: 0.1435, average loss: 1.0262
[09/17 00:22:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 49.00	top5: 99.50	
[09/17 00:22:56 visual_prompt]: 	Test 100/356. loss: 1.442, 0.1836 s / batch. (data: 1.53e-04)max mem: 17.22445 GB 
[09/17 00:23:16 visual_prompt]: 	Test 200/356. loss: 1.716, 0.2045 s / batch. (data: 1.54e-02)max mem: 17.22445 GB 
[09/17 00:23:35 visual_prompt]: 	Test 300/356. loss: 1.393, 0.2033 s / batch. (data: 2.00e-02)max mem: 17.22445 GB 
[09/17 00:23:48 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1942, average loss: 1.4957
[09/17 00:23:48 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.78	top5: 97.73	
[09/17 00:23:48 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/17 00:23:58 visual_prompt]: Epoch 72 / 100: avg data time: 1.43e-01, avg batch time: 0.5459, average train loss: 1.0144
[09/17 00:24:02 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1433, average loss: 1.0243
[09/17 00:24:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 48.00	top5: 100.00	
[09/17 00:24:24 visual_prompt]: 	Test 100/356. loss: 1.518, 0.1978 s / batch. (data: 1.48e-02)max mem: 17.22445 GB 
[09/17 00:24:43 visual_prompt]: 	Test 200/356. loss: 1.642, 0.2101 s / batch. (data: 2.72e-02)max mem: 17.22445 GB 
[09/17 00:25:03 visual_prompt]: 	Test 300/356. loss: 1.315, 0.1841 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/17 00:25:16 visual_prompt]: Inference (test):avg data time: 8.38e-03, avg batch time: 0.1937, average loss: 1.5915
[09/17 00:25:16 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.51	top5: 97.54	
[09/17 00:25:16 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/17 00:25:26 visual_prompt]: Epoch 73 / 100: avg data time: 1.46e-01, avg batch time: 0.5507, average train loss: 1.0361
[09/17 00:25:31 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1432, average loss: 1.0025
[09/17 00:25:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.50	top5: 99.00	
[09/17 00:25:52 visual_prompt]: 	Test 100/356. loss: 1.638, 0.1829 s / batch. (data: 1.33e-04)max mem: 17.22445 GB 
[09/17 00:26:12 visual_prompt]: 	Test 200/356. loss: 1.497, 0.1837 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/17 00:26:31 visual_prompt]: 	Test 300/356. loss: 1.139, 0.1970 s / batch. (data: 1.38e-02)max mem: 17.22445 GB 
[09/17 00:26:43 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1927, average loss: 1.4751
[09/17 00:26:43 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.29	top5: 97.39	
[09/17 00:26:43 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/17 00:26:54 visual_prompt]: Epoch 74 / 100: avg data time: 1.47e-01, avg batch time: 0.5478, average train loss: 1.0224
[09/17 00:26:58 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1433, average loss: 0.9466
[09/17 00:26:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.50	top5: 99.50	
[09/17 00:27:20 visual_prompt]: 	Test 100/356. loss: 1.441, 0.1964 s / batch. (data: 1.35e-02)max mem: 17.22445 GB 
[09/17 00:27:39 visual_prompt]: 	Test 200/356. loss: 1.808, 0.1971 s / batch. (data: 1.41e-02)max mem: 17.22445 GB 
[09/17 00:27:59 visual_prompt]: 	Test 300/356. loss: 1.239, 0.1998 s / batch. (data: 1.45e-04)max mem: 17.22445 GB 
[09/17 00:28:11 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1933, average loss: 1.5064
[09/17 00:28:11 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.39	top5: 97.00	
[09/17 00:28:11 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/17 00:28:22 visual_prompt]: Epoch 75 / 100: avg data time: 1.44e-01, avg batch time: 0.5475, average train loss: 0.9404
[09/17 00:28:26 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1430, average loss: 0.8957
[09/17 00:28:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 51.00	top5: 100.00	
[09/17 00:28:47 visual_prompt]: 	Test 100/356. loss: 1.413, 0.1833 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 00:29:07 visual_prompt]: 	Test 200/356. loss: 1.819, 0.1959 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 00:29:26 visual_prompt]: 	Test 300/356. loss: 1.240, 0.2128 s / batch. (data: 1.54e-02)max mem: 17.22445 GB 
[09/17 00:29:39 visual_prompt]: Inference (test):avg data time: 7.37e-03, avg batch time: 0.1930, average loss: 1.5346
[09/17 00:29:39 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.42	top5: 97.34	
[09/17 00:29:39 visual_prompt]: Best epoch 75: best metric: 0.510
[09/17 00:29:39 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/17 00:29:49 visual_prompt]: Epoch 76 / 100: avg data time: 1.36e-01, avg batch time: 0.5377, average train loss: 0.9029
[09/17 00:29:53 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1433, average loss: 0.9554
[09/17 00:29:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 52.50	top5: 100.00	
[09/17 00:30:15 visual_prompt]: 	Test 100/356. loss: 1.449, 0.1831 s / batch. (data: 4.65e-05)max mem: 17.22445 GB 
[09/17 00:30:34 visual_prompt]: 	Test 200/356. loss: 1.805, 0.1960 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/17 00:30:54 visual_prompt]: 	Test 300/356. loss: 1.403, 0.1986 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/17 00:31:06 visual_prompt]: Inference (test):avg data time: 8.15e-03, avg batch time: 0.1935, average loss: 1.6299
[09/17 00:31:06 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.59	top5: 97.19	
[09/17 00:31:06 visual_prompt]: Best epoch 76: best metric: 0.525
[09/17 00:31:06 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/17 00:31:17 visual_prompt]: Epoch 77 / 100: avg data time: 1.46e-01, avg batch time: 0.5475, average train loss: 0.8771
[09/17 00:31:21 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1432, average loss: 0.9085
[09/17 00:31:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 51.00	top5: 100.00	
[09/17 00:31:43 visual_prompt]: 	Test 100/356. loss: 1.583, 0.1837 s / batch. (data: 1.19e-04)max mem: 17.22445 GB 
[09/17 00:32:02 visual_prompt]: 	Test 200/356. loss: 1.657, 0.1839 s / batch. (data: 1.41e-04)max mem: 17.22445 GB 
[09/17 00:32:21 visual_prompt]: 	Test 300/356. loss: 1.336, 0.2233 s / batch. (data: 2.44e-02)max mem: 17.22445 GB 
[09/17 00:32:34 visual_prompt]: Inference (test):avg data time: 7.43e-03, avg batch time: 0.1927, average loss: 1.6324
[09/17 00:32:34 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.84	top5: 97.66	
[09/17 00:32:34 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/17 00:32:44 visual_prompt]: Epoch 78 / 100: avg data time: 1.46e-01, avg batch time: 0.5489, average train loss: 0.8579
[09/17 00:32:49 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1432, average loss: 0.8428
[09/17 00:32:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 51.50	top5: 100.00	
[09/17 00:33:11 visual_prompt]: 	Test 100/356. loss: 1.519, 0.1832 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/17 00:33:30 visual_prompt]: 	Test 200/356. loss: 1.703, 0.1835 s / batch. (data: 1.75e-04)max mem: 17.22445 GB 
[09/17 00:33:49 visual_prompt]: 	Test 300/356. loss: 1.333, 0.1839 s / batch. (data: 1.41e-04)max mem: 17.22445 GB 
[09/17 00:34:02 visual_prompt]: Inference (test):avg data time: 8.50e-03, avg batch time: 0.1933, average loss: 1.6075
[09/17 00:34:02 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.92	top5: 97.54	
[09/17 00:34:02 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/17 00:34:12 visual_prompt]: Epoch 79 / 100: avg data time: 1.39e-01, avg batch time: 0.5443, average train loss: 0.8107
[09/17 00:34:16 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.1461, average loss: 0.8349
[09/17 00:34:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 53.00	top5: 100.00	
[09/17 00:34:38 visual_prompt]: 	Test 100/356. loss: 1.714, 0.2171 s / batch. (data: 3.48e-02)max mem: 17.22445 GB 
[09/17 00:34:58 visual_prompt]: 	Test 200/356. loss: 1.967, 0.1979 s / batch. (data: 3.17e-05)max mem: 17.22445 GB 
[09/17 00:35:17 visual_prompt]: 	Test 300/356. loss: 1.556, 0.1920 s / batch. (data: 1.37e-04)max mem: 17.22445 GB 
[09/17 00:35:30 visual_prompt]: Inference (test):avg data time: 8.77e-03, avg batch time: 0.1944, average loss: 1.7723
[09/17 00:35:30 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.75	top5: 97.24	
[09/17 00:35:30 visual_prompt]: Best epoch 79: best metric: 0.530
[09/17 00:35:30 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/17 00:35:41 visual_prompt]: Epoch 80 / 100: avg data time: 1.46e-01, avg batch time: 0.5473, average train loss: 0.8374
[09/17 00:35:45 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1432, average loss: 0.8597
[09/17 00:35:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 51.50	top5: 100.00	
[09/17 00:36:07 visual_prompt]: 	Test 100/356. loss: 1.816, 0.2003 s / batch. (data: 1.75e-02)max mem: 17.22445 GB 
[09/17 00:36:26 visual_prompt]: 	Test 200/356. loss: 2.173, 0.2001 s / batch. (data: 1.24e-02)max mem: 17.22445 GB 
[09/17 00:36:46 visual_prompt]: 	Test 300/356. loss: 1.786, 0.1929 s / batch. (data: 1.83e-04)max mem: 17.22445 GB 
[09/17 00:36:58 visual_prompt]: Inference (test):avg data time: 8.58e-03, avg batch time: 0.1944, average loss: 1.8503
[09/17 00:36:59 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.96	top5: 96.97	
[09/17 00:36:59 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/17 00:37:09 visual_prompt]: Epoch 81 / 100: avg data time: 1.45e-01, avg batch time: 0.5465, average train loss: 0.8213
[09/17 00:37:13 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1432, average loss: 0.7571
[09/17 00:37:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 56.00	top5: 100.00	
[09/17 00:37:35 visual_prompt]: 	Test 100/356. loss: 1.595, 0.1836 s / batch. (data: 1.67e-04)max mem: 17.22445 GB 
[09/17 00:37:55 visual_prompt]: 	Test 200/356. loss: 2.217, 0.1845 s / batch. (data: 1.41e-04)max mem: 17.22445 GB 
[09/17 00:38:14 visual_prompt]: 	Test 300/356. loss: 1.635, 0.2019 s / batch. (data: 1.45e-04)max mem: 17.22445 GB 
[09/17 00:38:27 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1941, average loss: 1.7949
[09/17 00:38:27 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.09	top5: 97.28	
[09/17 00:38:27 visual_prompt]: Best epoch 81: best metric: 0.560
[09/17 00:38:27 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/17 00:38:37 visual_prompt]: Epoch 82 / 100: avg data time: 1.47e-01, avg batch time: 0.5506, average train loss: 0.7864
[09/17 00:38:41 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1431, average loss: 0.9129
[09/17 00:38:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 48.50	top5: 100.00	
[09/17 00:39:03 visual_prompt]: 	Test 100/356. loss: 1.661, 0.1836 s / batch. (data: 1.22e-04)max mem: 17.22445 GB 
[09/17 00:39:23 visual_prompt]: 	Test 200/356. loss: 1.922, 0.1955 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 00:39:42 visual_prompt]: 	Test 300/356. loss: 1.587, 0.1845 s / batch. (data: 1.40e-04)max mem: 17.22445 GB 
[09/17 00:39:55 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1937, average loss: 1.8831
[09/17 00:39:55 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.34	top5: 97.31	
[09/17 00:39:55 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/17 00:40:06 visual_prompt]: Epoch 83 / 100: avg data time: 1.54e-01, avg batch time: 0.5564, average train loss: 0.7772
[09/17 00:40:10 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1429, average loss: 0.7421
[09/17 00:40:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 55.50	top5: 100.00	
[09/17 00:40:32 visual_prompt]: 	Test 100/356. loss: 1.500, 0.1837 s / batch. (data: 1.37e-04)max mem: 17.22445 GB 
[09/17 00:40:51 visual_prompt]: 	Test 200/356. loss: 2.211, 0.1835 s / batch. (data: 5.51e-05)max mem: 17.22445 GB 
[09/17 00:41:11 visual_prompt]: 	Test 300/356. loss: 1.516, 0.1977 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 00:41:24 visual_prompt]: Inference (test):avg data time: 7.19e-03, avg batch time: 0.1935, average loss: 1.8391
[09/17 00:41:24 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.49	top5: 97.42	
[09/17 00:41:24 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/17 00:41:34 visual_prompt]: Epoch 84 / 100: avg data time: 1.52e-01, avg batch time: 0.5545, average train loss: 0.7498
[09/17 00:41:39 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1432, average loss: 0.7231
[09/17 00:41:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 59.00	top5: 100.00	
[09/17 00:42:01 visual_prompt]: 	Test 100/356. loss: 1.542, 0.1984 s / batch. (data: 1.56e-02)max mem: 17.22445 GB 
[09/17 00:42:20 visual_prompt]: 	Test 200/356. loss: 2.070, 0.1841 s / batch. (data: 1.04e-04)max mem: 17.22445 GB 
[09/17 00:42:40 visual_prompt]: 	Test 300/356. loss: 1.660, 0.1835 s / batch. (data: 1.09e-04)max mem: 17.22445 GB 
[09/17 00:42:52 visual_prompt]: Inference (test):avg data time: 8.18e-03, avg batch time: 0.1942, average loss: 1.8504
[09/17 00:42:52 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.93	top5: 97.05	
[09/17 00:42:52 visual_prompt]: Best epoch 84: best metric: 0.590
[09/17 00:42:52 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/17 00:43:03 visual_prompt]: Epoch 85 / 100: avg data time: 1.47e-01, avg batch time: 0.5497, average train loss: 0.7566
[09/17 00:43:07 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1430, average loss: 0.7396
[09/17 00:43:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 57.50	top5: 100.00	
[09/17 00:43:29 visual_prompt]: 	Test 100/356. loss: 1.593, 0.1991 s / batch. (data: 1.40e-02)max mem: 17.22445 GB 
[09/17 00:43:49 visual_prompt]: 	Test 200/356. loss: 2.398, 0.2008 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/17 00:44:09 visual_prompt]: 	Test 300/356. loss: 1.709, 0.1837 s / batch. (data: 1.31e-04)max mem: 17.22445 GB 
[09/17 00:44:21 visual_prompt]: Inference (test):avg data time: 8.20e-03, avg batch time: 0.1945, average loss: 1.9142
[09/17 00:44:21 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.49	top5: 96.94	
[09/17 00:44:21 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/17 00:44:32 visual_prompt]: Epoch 86 / 100: avg data time: 1.30e-01, avg batch time: 0.5349, average train loss: 0.7472
[09/17 00:44:36 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.1439, average loss: 0.7613
[09/17 00:44:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 52.50	top5: 100.00	
[09/17 00:44:58 visual_prompt]: 	Test 100/356. loss: 1.736, 0.2017 s / batch. (data: 1.40e-02)max mem: 17.22445 GB 
[09/17 00:45:17 visual_prompt]: 	Test 200/356. loss: 2.178, 0.2019 s / batch. (data: 1.67e-02)max mem: 17.22445 GB 
[09/17 00:45:37 visual_prompt]: 	Test 300/356. loss: 1.675, 0.2004 s / batch. (data: 1.45e-02)max mem: 17.22445 GB 
[09/17 00:45:50 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1939, average loss: 1.9787
[09/17 00:45:50 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.91	top5: 97.37	
[09/17 00:45:50 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/17 00:46:00 visual_prompt]: Epoch 87 / 100: avg data time: 1.44e-01, avg batch time: 0.5471, average train loss: 0.6912
[09/17 00:46:04 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1429, average loss: 0.7235
[09/17 00:46:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 55.00	top5: 100.00	
[09/17 00:46:26 visual_prompt]: 	Test 100/356. loss: 1.862, 0.1961 s / batch. (data: 1.31e-02)max mem: 17.22445 GB 
[09/17 00:46:46 visual_prompt]: 	Test 200/356. loss: 2.237, 0.1839 s / batch. (data: 1.40e-04)max mem: 17.22445 GB 
[09/17 00:47:05 visual_prompt]: 	Test 300/356. loss: 1.809, 0.1843 s / batch. (data: 1.16e-04)max mem: 17.22445 GB 
[09/17 00:47:18 visual_prompt]: Inference (test):avg data time: 7.83e-03, avg batch time: 0.1943, average loss: 2.0361
[09/17 00:47:18 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.83	top5: 97.09	
[09/17 00:47:18 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/17 00:47:28 visual_prompt]: Epoch 88 / 100: avg data time: 1.38e-01, avg batch time: 0.5429, average train loss: 0.7146
[09/17 00:47:33 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1430, average loss: 0.6631
[09/17 00:47:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 64.00	top5: 100.00	
[09/17 00:47:54 visual_prompt]: 	Test 100/356. loss: 1.618, 0.2080 s / batch. (data: 1.11e-02)max mem: 17.22445 GB 
[09/17 00:48:14 visual_prompt]: 	Test 200/356. loss: 2.522, 0.1935 s / batch. (data: 1.03e-02)max mem: 17.22445 GB 
[09/17 00:48:33 visual_prompt]: 	Test 300/356. loss: 1.974, 0.1983 s / batch. (data: 1.52e-02)max mem: 17.22445 GB 
[09/17 00:48:47 visual_prompt]: Inference (test):avg data time: 8.30e-03, avg batch time: 0.1958, average loss: 1.9346
[09/17 00:48:47 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.39	top5: 97.30	
[09/17 00:48:47 visual_prompt]: Best epoch 88: best metric: 0.640
[09/17 00:48:47 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/17 00:48:57 visual_prompt]: Epoch 89 / 100: avg data time: 1.48e-01, avg batch time: 0.5514, average train loss: 0.7322
[09/17 00:49:02 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1430, average loss: 0.6948
[09/17 00:49:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 59.00	top5: 100.00	
[09/17 00:49:24 visual_prompt]: 	Test 100/356. loss: 1.802, 0.2069 s / batch. (data: 2.44e-02)max mem: 17.22445 GB 
[09/17 00:49:43 visual_prompt]: 	Test 200/356. loss: 2.351, 0.1838 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/17 00:50:02 visual_prompt]: 	Test 300/356. loss: 1.875, 0.1965 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/17 00:50:15 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1938, average loss: 2.0300
[09/17 00:50:15 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.77	top5: 97.55	
[09/17 00:50:15 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/17 00:50:26 visual_prompt]: Epoch 90 / 100: avg data time: 1.52e-01, avg batch time: 0.5553, average train loss: 0.6964
[09/17 00:50:30 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.1431, average loss: 0.7381
[09/17 00:50:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 55.00	top5: 100.00	
[09/17 00:50:52 visual_prompt]: 	Test 100/356. loss: 1.807, 0.2078 s / batch. (data: 2.51e-02)max mem: 17.22445 GB 
[09/17 00:51:11 visual_prompt]: 	Test 200/356. loss: 2.293, 0.1840 s / batch. (data: 1.07e-04)max mem: 17.22445 GB 
[09/17 00:51:31 visual_prompt]: 	Test 300/356. loss: 1.819, 0.2866 s / batch. (data: 4.39e-04)max mem: 17.22445 GB 
[09/17 00:51:44 visual_prompt]: Inference (test):avg data time: 8.15e-03, avg batch time: 0.1945, average loss: 2.0539
[09/17 00:51:44 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.25	top5: 97.43	
[09/17 00:51:44 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/17 00:51:54 visual_prompt]: Epoch 91 / 100: avg data time: 1.54e-01, avg batch time: 0.5575, average train loss: 0.6724
[09/17 00:51:59 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1433, average loss: 0.6384
[09/17 00:51:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 63.00	top5: 100.00	
[09/17 00:52:21 visual_prompt]: 	Test 100/356. loss: 1.778, 0.1958 s / batch. (data: 1.31e-02)max mem: 17.22445 GB 
[09/17 00:52:40 visual_prompt]: 	Test 200/356. loss: 2.477, 0.1954 s / batch. (data: 9.66e-05)max mem: 17.22445 GB 
[09/17 00:52:59 visual_prompt]: 	Test 300/356. loss: 1.870, 0.1841 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/17 00:53:12 visual_prompt]: Inference (test):avg data time: 6.76e-03, avg batch time: 0.1937, average loss: 2.0358
[09/17 00:53:12 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.65	top5: 97.48	
[09/17 00:53:12 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/17 00:53:23 visual_prompt]: Epoch 92 / 100: avg data time: 1.49e-01, avg batch time: 0.5519, average train loss: 0.6423
[09/17 00:53:27 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1432, average loss: 0.6030
[09/17 00:53:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 66.50	top5: 100.00	
[09/17 00:53:49 visual_prompt]: 	Test 100/356. loss: 1.875, 0.2219 s / batch. (data: 6.48e-03)max mem: 17.22445 GB 
[09/17 00:54:09 visual_prompt]: 	Test 200/356. loss: 2.565, 0.1972 s / batch. (data: 1.42e-02)max mem: 17.22445 GB 
[09/17 00:54:28 visual_prompt]: 	Test 300/356. loss: 1.978, 0.2080 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/17 00:54:41 visual_prompt]: Inference (test):avg data time: 7.94e-03, avg batch time: 0.1944, average loss: 2.1413
[09/17 00:54:41 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.56	top5: 97.40	
[09/17 00:54:41 visual_prompt]: Best epoch 92: best metric: 0.665
[09/17 00:54:41 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/17 00:54:51 visual_prompt]: Epoch 93 / 100: avg data time: 1.40e-01, avg batch time: 0.5439, average train loss: 0.6352
[09/17 00:54:55 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1430, average loss: 0.6160
[09/17 00:54:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 67.00	top5: 100.00	
[09/17 00:55:17 visual_prompt]: 	Test 100/356. loss: 1.904, 0.1837 s / batch. (data: 1.23e-04)max mem: 17.22445 GB 
[09/17 00:55:37 visual_prompt]: 	Test 200/356. loss: 2.555, 0.1975 s / batch. (data: 1.44e-02)max mem: 17.22445 GB 
[09/17 00:55:56 visual_prompt]: 	Test 300/356. loss: 1.995, 0.1974 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 00:56:09 visual_prompt]: Inference (test):avg data time: 6.95e-03, avg batch time: 0.1933, average loss: 2.1979
[09/17 00:56:09 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.66	top5: 97.45	
[09/17 00:56:09 visual_prompt]: Best epoch 93: best metric: 0.670
[09/17 00:56:09 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/17 00:56:20 visual_prompt]: Epoch 94 / 100: avg data time: 1.54e-01, avg batch time: 0.5574, average train loss: 0.6115
[09/17 00:56:24 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1431, average loss: 0.5749
[09/17 00:56:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 70.00	top5: 100.00	
[09/17 00:56:45 visual_prompt]: 	Test 100/356. loss: 1.911, 0.1829 s / batch. (data: 4.24e-05)max mem: 17.22445 GB 
[09/17 00:57:05 visual_prompt]: 	Test 200/356. loss: 2.763, 0.1840 s / batch. (data: 1.68e-04)max mem: 17.22445 GB 
[09/17 00:57:24 visual_prompt]: 	Test 300/356. loss: 2.103, 0.2206 s / batch. (data: 3.73e-02)max mem: 17.22445 GB 
[09/17 00:57:37 visual_prompt]: Inference (test):avg data time: 7.34e-03, avg batch time: 0.1937, average loss: 2.2579
[09/17 00:57:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.06	top5: 97.25	
[09/17 00:57:37 visual_prompt]: Best epoch 94: best metric: 0.700
[09/17 00:57:37 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/17 00:57:47 visual_prompt]: Epoch 95 / 100: avg data time: 1.50e-01, avg batch time: 0.5522, average train loss: 0.6032
[09/17 00:57:52 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1429, average loss: 0.5705
[09/17 00:57:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 71.50	top5: 100.00	
[09/17 00:58:13 visual_prompt]: 	Test 100/356. loss: 1.953, 0.2068 s / batch. (data: 2.43e-02)max mem: 17.22445 GB 
[09/17 00:58:33 visual_prompt]: 	Test 200/356. loss: 2.703, 0.1998 s / batch. (data: 1.40e-02)max mem: 17.22445 GB 
[09/17 00:58:52 visual_prompt]: 	Test 300/356. loss: 2.103, 0.1965 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 00:59:05 visual_prompt]: Inference (test):avg data time: 7.77e-03, avg batch time: 0.1932, average loss: 2.2917
[09/17 00:59:05 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.54	top5: 97.24	
[09/17 00:59:05 visual_prompt]: Best epoch 95: best metric: 0.715
[09/17 00:59:05 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/17 00:59:15 visual_prompt]: Epoch 96 / 100: avg data time: 1.43e-01, avg batch time: 0.5485, average train loss: 0.5866
[09/17 00:59:20 visual_prompt]: Inference (val):avg data time: 4.79e-05, avg batch time: 0.1433, average loss: 0.5815
[09/17 00:59:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 70.50	top5: 100.00	
[09/17 00:59:42 visual_prompt]: 	Test 100/356. loss: 2.089, 0.1927 s / batch. (data: 1.27e-04)max mem: 17.22445 GB 
[09/17 01:00:01 visual_prompt]: 	Test 200/356. loss: 2.777, 0.1870 s / batch. (data: 1.45e-04)max mem: 17.22445 GB 
[09/17 01:00:20 visual_prompt]: 	Test 300/356. loss: 2.206, 0.2114 s / batch. (data: 2.82e-02)max mem: 17.22445 GB 
[09/17 01:00:33 visual_prompt]: Inference (test):avg data time: 6.87e-03, avg batch time: 0.1936, average loss: 2.3958
[09/17 01:00:33 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.89	top5: 97.18	
[09/17 01:00:33 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/17 01:00:43 visual_prompt]: Epoch 97 / 100: avg data time: 1.37e-01, avg batch time: 0.5412, average train loss: 0.5730
[09/17 01:00:48 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1444, average loss: 0.5491
[09/17 01:00:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 72.00	top5: 100.00	
[09/17 01:01:09 visual_prompt]: 	Test 100/356. loss: 2.086, 0.2060 s / batch. (data: 2.31e-02)max mem: 17.22445 GB 
[09/17 01:01:29 visual_prompt]: 	Test 200/356. loss: 2.920, 0.1963 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 01:01:48 visual_prompt]: 	Test 300/356. loss: 2.279, 0.2078 s / batch. (data: 1.56e-02)max mem: 17.22445 GB 
[09/17 01:02:01 visual_prompt]: Inference (test):avg data time: 7.18e-03, avg batch time: 0.1934, average loss: 2.4269
[09/17 01:02:01 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.50	top5: 97.11	
[09/17 01:02:01 visual_prompt]: Best epoch 97: best metric: 0.720
[09/17 01:02:01 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/17 01:02:11 visual_prompt]: Epoch 98 / 100: avg data time: 1.48e-01, avg batch time: 0.5501, average train loss: 0.5511
[09/17 01:02:16 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1431, average loss: 0.5400
[09/17 01:02:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 71.00	top5: 100.00	
[09/17 01:02:38 visual_prompt]: 	Test 100/356. loss: 2.136, 0.1836 s / batch. (data: 1.45e-04)max mem: 17.22445 GB 
[09/17 01:02:57 visual_prompt]: 	Test 200/356. loss: 2.993, 0.1942 s / batch. (data: 1.07e-02)max mem: 17.22445 GB 
[09/17 01:03:17 visual_prompt]: 	Test 300/356. loss: 2.341, 0.1962 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/17 01:03:29 visual_prompt]: Inference (test):avg data time: 8.72e-03, avg batch time: 0.1944, average loss: 2.4843
[09/17 01:03:30 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.44	top5: 97.10	
[09/17 01:03:30 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/17 01:03:40 visual_prompt]: Epoch 99 / 100: avg data time: 1.55e-01, avg batch time: 0.5562, average train loss: 0.5535
[09/17 01:03:45 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1430, average loss: 0.5373
[09/17 01:03:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 72.50	top5: 100.00	
[09/17 01:04:07 visual_prompt]: 	Test 100/356. loss: 2.164, 0.1937 s / batch. (data: 1.11e-02)max mem: 17.22445 GB 
[09/17 01:04:26 visual_prompt]: 	Test 200/356. loss: 3.001, 0.2040 s / batch. (data: 1.52e-02)max mem: 17.22445 GB 
[09/17 01:04:46 visual_prompt]: 	Test 300/356. loss: 2.354, 0.1843 s / batch. (data: 1.16e-04)max mem: 17.22445 GB 
[09/17 01:04:58 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1939, average loss: 2.5190
[09/17 01:04:58 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.63	top5: 97.10	
[09/17 01:04:58 visual_prompt]: Best epoch 99: best metric: 0.725
[09/17 01:04:58 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/17 01:05:09 visual_prompt]: Epoch 100 / 100: avg data time: 1.44e-01, avg batch time: 0.5473, average train loss: 0.5542
[09/17 01:05:13 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1432, average loss: 0.5401
[09/17 01:05:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 73.00	top5: 100.00	
[09/17 01:05:35 visual_prompt]: 	Test 100/356. loss: 2.172, 0.1972 s / batch. (data: 1.39e-02)max mem: 17.22445 GB 
[09/17 01:05:54 visual_prompt]: 	Test 200/356. loss: 3.014, 0.2129 s / batch. (data: 1.39e-02)max mem: 17.22445 GB 
[09/17 01:06:14 visual_prompt]: 	Test 300/356. loss: 2.367, 0.2067 s / batch. (data: 1.24e-02)max mem: 17.22445 GB 
[09/17 01:06:26 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1930, average loss: 2.5323
[09/17 01:06:26 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.69	top5: 97.11	
[09/17 01:06:26 visual_prompt]: Best epoch 100: best metric: 0.730
[09/17 01:06:44 visual_prompt]: Rank of current process: 0. World size: 1
[09/17 01:06:44 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/17 01:06:44 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed82'], train_type='')
[09/17 01:06:44 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/17 01:06:44 visual_prompt]: Training with config:
[09/17 01:06:44 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-dmlab',
          'NO_TEST': False,
          'NUMBER_CLASSES': 6,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed82/vtab-dmlab/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/17 01:06:44 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-17 01:06:44.606702: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-17 01:06:44.779356: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-17 01:06:45.692881: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 01:06:45.692966: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 01:06:45.692977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-17 01:06:47.753197: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 01:06:47.753311: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 01:06:47.753325: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/17 01:06:47 visual_prompt]: Constructing vtab-dmlab dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
2023-09-17 01:06:47.902151: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dmlab for split train[:800]+validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/17 01:06:50 visual_prompt]: Number of images: 1000
[09/17 01:06:50 visual_prompt]: Number of classes: 6 / 6
[09/17 01:06:50 visual_prompt]: Loading validation data...
[09/17 01:06:50 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/17 01:06:50 visual_prompt]: Number of images: 200
[09/17 01:06:50 visual_prompt]: Number of classes: 6 / 6
[09/17 01:06:50 visual_prompt]: Loading test data...
[09/17 01:06:50 visual_prompt]: Constructing vtab-dmlab dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dmlab for split test, from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/17 01:07:31 visual_prompt]: Number of images: 22735
[09/17 01:07:31 visual_prompt]: Number of classes: 6 / 6
[09/17 01:07:31 visual_prompt]: Constructing models...
[09/17 01:07:34 visual_prompt]: Total Parameters: 86724870	 Gradient Parameters: 926214
[09/17 01:07:34 visual_prompt]: tuned percent:1.068
[09/17 01:07:37 visual_prompt]: Device used for model: 0
[09/17 01:07:37 visual_prompt]: Setting up Evalutator...
[09/17 01:07:37 visual_prompt]: Setting up Trainer...
[09/17 01:07:37 visual_prompt]: 	Setting up the optimizer...
[09/17 01:07:37 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/17 01:07:48 visual_prompt]: Epoch 1 / 100: avg data time: 1.49e-01, avg batch time: 0.6247, average train loss: 2.3020
[09/17 01:07:52 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1426, average loss: 2.1768
[09/17 01:07:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/17 01:08:14 visual_prompt]: 	Test 100/356. loss: 2.395, 0.2002 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 01:08:33 visual_prompt]: 	Test 200/356. loss: 2.184, 0.1936 s / batch. (data: 1.40e-04)max mem: 17.22445 GB 
[09/17 01:08:53 visual_prompt]: 	Test 300/356. loss: 2.356, 0.1957 s / batch. (data: 1.32e-02)max mem: 17.22445 GB 
[09/17 01:09:06 visual_prompt]: Inference (test):avg data time: 6.40e-03, avg batch time: 0.1952, average loss: 2.2525
[09/17 01:09:06 visual_prompt]: Classification results with test_vtab-dmlab: top1: 11.58	top5: 84.79	
[09/17 01:09:06 visual_prompt]: Best epoch 1: best metric: 0.160
[09/17 01:09:06 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/17 01:09:16 visual_prompt]: Epoch 2 / 100: avg data time: 1.41e-01, avg batch time: 0.5438, average train loss: 5.2815
[09/17 01:09:20 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1428, average loss: 5.5906
[09/17 01:09:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 84.00	
[09/17 01:09:42 visual_prompt]: 	Test 100/356. loss: 3.929, 0.2318 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 01:10:01 visual_prompt]: 	Test 200/356. loss: 3.884, 0.2002 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 01:10:21 visual_prompt]: 	Test 300/356. loss: 4.663, 0.1969 s / batch. (data: 1.33e-02)max mem: 17.22445 GB 
[09/17 01:10:33 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1930, average loss: 4.4777
[09/17 01:10:33 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.32	top5: 88.42	
[09/17 01:10:33 visual_prompt]: Best epoch 2: best metric: 0.170
[09/17 01:10:33 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/17 01:10:43 visual_prompt]: Epoch 3 / 100: avg data time: 1.21e-01, avg batch time: 0.5280, average train loss: 5.4684
[09/17 01:10:47 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1432, average loss: 3.4753
[09/17 01:10:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/17 01:11:09 visual_prompt]: 	Test 100/356. loss: 3.895, 0.1912 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/17 01:11:29 visual_prompt]: 	Test 200/356. loss: 3.574, 0.1990 s / batch. (data: 1.57e-02)max mem: 17.22445 GB 
[09/17 01:11:48 visual_prompt]: 	Test 300/356. loss: 3.574, 0.1959 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/17 01:12:01 visual_prompt]: Inference (test):avg data time: 8.05e-03, avg batch time: 0.1939, average loss: 3.6413
[09/17 01:12:01 visual_prompt]: Classification results with test_vtab-dmlab: top1: 11.58	top5: 85.40	
[09/17 01:12:01 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/17 01:12:11 visual_prompt]: Epoch 4 / 100: avg data time: 1.38e-01, avg batch time: 0.5401, average train loss: 2.9602
[09/17 01:12:15 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1432, average loss: 2.0128
[09/17 01:12:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 83.50	
[09/17 01:12:37 visual_prompt]: 	Test 100/356. loss: 1.880, 0.1976 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/17 01:12:57 visual_prompt]: 	Test 200/356. loss: 1.866, 0.1867 s / batch. (data: 1.28e-04)max mem: 17.22445 GB 
[09/17 01:13:17 visual_prompt]: 	Test 300/356. loss: 1.858, 0.2078 s / batch. (data: 2.47e-02)max mem: 17.22445 GB 
[09/17 01:13:30 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1965, average loss: 1.9228
[09/17 01:13:30 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 86.52	
[09/17 01:13:30 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/17 01:13:40 visual_prompt]: Epoch 5 / 100: avg data time: 1.33e-01, avg batch time: 0.5395, average train loss: 2.4364
[09/17 01:13:44 visual_prompt]: Inference (val):avg data time: 4.74e-05, avg batch time: 0.1432, average loss: 2.3062
[09/17 01:13:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.50	
[09/17 01:14:06 visual_prompt]: 	Test 100/356. loss: 2.223, 0.1839 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/17 01:14:25 visual_prompt]: 	Test 200/356. loss: 1.909, 0.1835 s / batch. (data: 1.11e-04)max mem: 17.22445 GB 
[09/17 01:14:46 visual_prompt]: 	Test 300/356. loss: 2.199, 0.1836 s / batch. (data: 1.56e-04)max mem: 17.22445 GB 
[09/17 01:14:58 visual_prompt]: Inference (test):avg data time: 8.02e-03, avg batch time: 0.1970, average loss: 2.1579
[09/17 01:14:59 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 87.05	
[09/17 01:14:59 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/17 01:15:09 visual_prompt]: Epoch 6 / 100: avg data time: 1.47e-01, avg batch time: 0.5491, average train loss: 2.4024
[09/17 01:15:13 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1428, average loss: 2.4894
[09/17 01:15:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/17 01:15:35 visual_prompt]: 	Test 100/356. loss: 2.753, 0.1959 s / batch. (data: 1.31e-02)max mem: 17.22445 GB 
[09/17 01:15:54 visual_prompt]: 	Test 200/356. loss: 2.382, 0.1841 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/17 01:16:14 visual_prompt]: 	Test 300/356. loss: 2.612, 0.1877 s / batch. (data: 3.98e-03)max mem: 17.22445 GB 
[09/17 01:16:27 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1942, average loss: 2.5521
[09/17 01:16:27 visual_prompt]: Classification results with test_vtab-dmlab: top1: 11.58	top5: 84.67	
[09/17 01:16:27 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/17 01:16:37 visual_prompt]: Epoch 7 / 100: avg data time: 1.31e-01, avg batch time: 0.5401, average train loss: 5.1077
[09/17 01:16:41 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1433, average loss: 6.0197
[09/17 01:16:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/17 01:17:03 visual_prompt]: 	Test 100/356. loss: 6.792, 0.1919 s / batch. (data: 1.35e-04)max mem: 17.22445 GB 
[09/17 01:17:22 visual_prompt]: 	Test 200/356. loss: 5.150, 0.1836 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/17 01:17:42 visual_prompt]: 	Test 300/356. loss: 6.313, 0.1840 s / batch. (data: 1.26e-04)max mem: 17.22445 GB 
[09/17 01:17:54 visual_prompt]: Inference (test):avg data time: 6.97e-03, avg batch time: 0.1932, average loss: 6.0355
[09/17 01:17:54 visual_prompt]: Classification results with test_vtab-dmlab: top1: 11.58	top5: 84.67	
[09/17 01:17:54 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/17 01:18:04 visual_prompt]: Epoch 8 / 100: avg data time: 1.41e-01, avg batch time: 0.5541, average train loss: 6.9700
[09/17 01:18:08 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1431, average loss: 6.5941
[09/17 01:18:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/17 01:18:30 visual_prompt]: 	Test 100/356. loss: 6.815, 0.1869 s / batch. (data: 1.36e-04)max mem: 17.22445 GB 
[09/17 01:18:50 visual_prompt]: 	Test 200/356. loss: 6.544, 0.1842 s / batch. (data: 1.53e-04)max mem: 17.22445 GB 
[09/17 01:19:09 visual_prompt]: 	Test 300/356. loss: 6.062, 0.1839 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/17 01:19:22 visual_prompt]: Inference (test):avg data time: 6.79e-03, avg batch time: 0.1936, average loss: 6.5923
[09/17 01:19:22 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 82.27	
[09/17 01:19:22 visual_prompt]: Best epoch 8: best metric: 0.205
[09/17 01:19:22 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/17 01:19:32 visual_prompt]: Epoch 9 / 100: avg data time: 1.41e-01, avg batch time: 0.5440, average train loss: 9.7721
[09/17 01:19:36 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1434, average loss: 9.4878
[09/17 01:19:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/17 01:19:58 visual_prompt]: 	Test 100/356. loss: 9.461, 0.1981 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/17 01:20:18 visual_prompt]: 	Test 200/356. loss: 8.375, 0.2115 s / batch. (data: 1.88e-02)max mem: 17.22445 GB 
[09/17 01:20:38 visual_prompt]: 	Test 300/356. loss: 9.422, 0.2089 s / batch. (data: 2.58e-02)max mem: 17.22445 GB 
[09/17 01:20:50 visual_prompt]: Inference (test):avg data time: 8.50e-03, avg batch time: 0.1955, average loss: 9.1826
[09/17 01:20:50 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 84.67	
[09/17 01:20:50 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/17 01:21:00 visual_prompt]: Epoch 10 / 100: avg data time: 1.47e-01, avg batch time: 0.5491, average train loss: 7.4807
[09/17 01:21:04 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1436, average loss: 8.8989
[09/17 01:21:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/17 01:21:26 visual_prompt]: 	Test 100/356. loss: 7.899, 0.1958 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/17 01:21:46 visual_prompt]: 	Test 200/356. loss: 9.157, 0.1994 s / batch. (data: 1.60e-02)max mem: 17.22445 GB 
[09/17 01:22:05 visual_prompt]: 	Test 300/356. loss: 8.986, 0.1954 s / batch. (data: 1.21e-02)max mem: 17.22445 GB 
[09/17 01:22:18 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1944, average loss: 8.6314
[09/17 01:22:18 visual_prompt]: Classification results with test_vtab-dmlab: top1: 14.60	top5: 88.42	
[09/17 01:22:18 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/17 01:22:28 visual_prompt]: Epoch 11 / 100: avg data time: 1.54e-01, avg batch time: 0.5587, average train loss: 7.1546
[09/17 01:22:33 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.1433, average loss: 10.0212
[09/17 01:22:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/17 01:22:54 visual_prompt]: 	Test 100/356. loss: 10.493, 0.1961 s / batch. (data: 1.31e-02)max mem: 17.22445 GB 
[09/17 01:23:14 visual_prompt]: 	Test 200/356. loss: 10.781, 0.1952 s / batch. (data: 1.22e-02)max mem: 17.22445 GB 
[09/17 01:23:34 visual_prompt]: 	Test 300/356. loss: 10.298, 0.1994 s / batch. (data: 1.59e-02)max mem: 17.22445 GB 
[09/17 01:23:46 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1944, average loss: 10.4523
[09/17 01:23:46 visual_prompt]: Classification results with test_vtab-dmlab: top1: 11.58	top5: 82.27	
[09/17 01:23:46 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/17 01:23:56 visual_prompt]: Epoch 12 / 100: avg data time: 1.33e-01, avg batch time: 0.5462, average train loss: 7.3652
[09/17 01:24:00 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1434, average loss: 2.6802
[09/17 01:24:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/17 01:24:22 visual_prompt]: 	Test 100/356. loss: 2.685, 0.1897 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/17 01:24:42 visual_prompt]: 	Test 200/356. loss: 2.463, 0.2080 s / batch. (data: 1.12e-02)max mem: 17.22445 GB 
[09/17 01:25:01 visual_prompt]: 	Test 300/356. loss: 2.722, 0.1956 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/17 01:25:14 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1938, average loss: 2.6154
[09/17 01:25:14 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 88.22	
[09/17 01:25:14 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/17 01:25:24 visual_prompt]: Epoch 13 / 100: avg data time: 1.46e-01, avg batch time: 0.5480, average train loss: 5.2411
[09/17 01:25:28 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1433, average loss: 3.4294
[09/17 01:25:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/17 01:25:50 visual_prompt]: 	Test 100/356. loss: 3.522, 0.1983 s / batch. (data: 1.53e-02)max mem: 17.22445 GB 
[09/17 01:26:10 visual_prompt]: 	Test 200/356. loss: 3.511, 0.1839 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/17 01:26:30 visual_prompt]: 	Test 300/356. loss: 3.207, 0.1833 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/17 01:26:43 visual_prompt]: Inference (test):avg data time: 8.86e-03, avg batch time: 0.1981, average loss: 3.4593
[09/17 01:26:43 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 85.40	
[09/17 01:26:43 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/17 01:26:53 visual_prompt]: Epoch 14 / 100: avg data time: 1.40e-01, avg batch time: 0.5419, average train loss: 2.8940
[09/17 01:26:57 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1432, average loss: 2.6379
[09/17 01:26:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/17 01:27:20 visual_prompt]: 	Test 100/356. loss: 2.421, 0.1977 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/17 01:27:39 visual_prompt]: 	Test 200/356. loss: 2.520, 0.2055 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 01:27:58 visual_prompt]: 	Test 300/356. loss: 2.549, 0.1843 s / batch. (data: 1.26e-04)max mem: 17.22445 GB 
[09/17 01:28:11 visual_prompt]: Inference (test):avg data time: 7.81e-03, avg batch time: 0.1945, average loss: 2.5481
[09/17 01:28:11 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 81.44	
[09/17 01:28:11 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/17 01:28:21 visual_prompt]: Epoch 15 / 100: avg data time: 1.44e-01, avg batch time: 0.5453, average train loss: 3.2572
[09/17 01:28:25 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1432, average loss: 2.7964
[09/17 01:28:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 84.00	
[09/17 01:28:47 visual_prompt]: 	Test 100/356. loss: 2.333, 0.2080 s / batch. (data: 2.51e-02)max mem: 17.22445 GB 
[09/17 01:29:07 visual_prompt]: 	Test 200/356. loss: 2.349, 0.2055 s / batch. (data: 1.59e-02)max mem: 17.22445 GB 
[09/17 01:29:26 visual_prompt]: 	Test 300/356. loss: 2.605, 0.2130 s / batch. (data: 1.47e-02)max mem: 17.22445 GB 
[09/17 01:29:39 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1944, average loss: 2.4953
[09/17 01:29:39 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.20	top5: 88.42	
[09/17 01:29:39 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/17 01:29:49 visual_prompt]: Epoch 16 / 100: avg data time: 1.49e-01, avg batch time: 0.5528, average train loss: 3.0507
[09/17 01:29:53 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1430, average loss: 3.3066
[09/17 01:29:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/17 01:30:15 visual_prompt]: 	Test 100/356. loss: 3.342, 0.1962 s / batch. (data: 1.31e-02)max mem: 17.22445 GB 
[09/17 01:30:35 visual_prompt]: 	Test 200/356. loss: 2.737, 0.1845 s / batch. (data: 1.31e-04)max mem: 17.22445 GB 
[09/17 01:30:54 visual_prompt]: 	Test 300/356. loss: 2.735, 0.1976 s / batch. (data: 1.39e-02)max mem: 17.22445 GB 
[09/17 01:31:06 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1937, average loss: 3.1269
[09/17 01:31:07 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 85.40	
[09/17 01:31:07 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/17 01:31:17 visual_prompt]: Epoch 17 / 100: avg data time: 1.43e-01, avg batch time: 0.5462, average train loss: 2.5821
[09/17 01:31:21 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1434, average loss: 2.8465
[09/17 01:31:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 84.00	
[09/17 01:31:43 visual_prompt]: 	Test 100/356. loss: 2.924, 0.1837 s / batch. (data: 1.46e-04)max mem: 17.22445 GB 
[09/17 01:32:02 visual_prompt]: 	Test 200/356. loss: 2.865, 0.1895 s / batch. (data: 1.42e-04)max mem: 17.22445 GB 
[09/17 01:32:22 visual_prompt]: 	Test 300/356. loss: 2.607, 0.1840 s / batch. (data: 1.91e-04)max mem: 17.22445 GB 
[09/17 01:32:34 visual_prompt]: Inference (test):avg data time: 8.06e-03, avg batch time: 0.1939, average loss: 2.8620
[09/17 01:32:34 visual_prompt]: Classification results with test_vtab-dmlab: top1: 12.81	top5: 85.40	
[09/17 01:32:34 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/17 01:32:44 visual_prompt]: Epoch 18 / 100: avg data time: 1.47e-01, avg batch time: 0.5498, average train loss: 2.5250
[09/17 01:32:48 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1432, average loss: 2.4773
[09/17 01:32:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 84.50	
[09/17 01:33:10 visual_prompt]: 	Test 100/356. loss: 2.610, 0.1976 s / batch. (data: 1.45e-02)max mem: 17.22445 GB 
[09/17 01:33:30 visual_prompt]: 	Test 200/356. loss: 2.924, 0.1857 s / batch. (data: 1.57e-04)max mem: 17.22445 GB 
[09/17 01:33:50 visual_prompt]: 	Test 300/356. loss: 2.727, 0.1980 s / batch. (data: 1.46e-02)max mem: 17.22445 GB 
[09/17 01:34:02 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1948, average loss: 2.6602
[09/17 01:34:02 visual_prompt]: Classification results with test_vtab-dmlab: top1: 19.72	top5: 77.84	
[09/17 01:34:02 visual_prompt]: Best epoch 18: best metric: 0.250
[09/17 01:34:02 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/17 01:34:12 visual_prompt]: Epoch 19 / 100: avg data time: 1.41e-01, avg batch time: 0.5479, average train loss: 2.4780
[09/17 01:34:16 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1432, average loss: 2.2456
[09/17 01:34:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 92.00	
[09/17 01:34:38 visual_prompt]: 	Test 100/356. loss: 2.524, 0.1959 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 01:34:58 visual_prompt]: 	Test 200/356. loss: 2.646, 0.1998 s / batch. (data: 1.66e-02)max mem: 17.22445 GB 
[09/17 01:35:17 visual_prompt]: 	Test 300/356. loss: 2.611, 0.1957 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 01:35:30 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1940, average loss: 2.4397
[09/17 01:35:30 visual_prompt]: Classification results with test_vtab-dmlab: top1: 19.99	top5: 90.18	
[09/17 01:35:30 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/17 01:35:40 visual_prompt]: Epoch 20 / 100: avg data time: 1.36e-01, avg batch time: 0.5397, average train loss: 2.0053
[09/17 01:35:44 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1434, average loss: 2.1250
[09/17 01:35:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 89.50	
[09/17 01:36:06 visual_prompt]: 	Test 100/356. loss: 2.179, 0.1831 s / batch. (data: 1.08e-04)max mem: 17.22445 GB 
[09/17 01:36:25 visual_prompt]: 	Test 200/356. loss: 2.317, 0.1901 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/17 01:36:45 visual_prompt]: 	Test 300/356. loss: 1.950, 0.1890 s / batch. (data: 1.40e-04)max mem: 17.22445 GB 
[09/17 01:36:58 visual_prompt]: Inference (test):avg data time: 7.94e-03, avg batch time: 0.1940, average loss: 2.1452
[09/17 01:36:58 visual_prompt]: Classification results with test_vtab-dmlab: top1: 23.59	top5: 88.28	
[09/17 01:36:58 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/17 01:37:08 visual_prompt]: Epoch 21 / 100: avg data time: 1.47e-01, avg batch time: 0.5508, average train loss: 1.7898
[09/17 01:37:12 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1432, average loss: 1.6001
[09/17 01:37:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.50	top5: 98.00	
[09/17 01:37:34 visual_prompt]: 	Test 100/356. loss: 1.431, 0.1832 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/17 01:37:54 visual_prompt]: 	Test 200/356. loss: 1.743, 0.1833 s / batch. (data: 1.10e-04)max mem: 17.22445 GB 
[09/17 01:38:13 visual_prompt]: 	Test 300/356. loss: 1.540, 0.1972 s / batch. (data: 1.38e-02)max mem: 17.22445 GB 
[09/17 01:38:26 visual_prompt]: Inference (test):avg data time: 8.55e-03, avg batch time: 0.1949, average loss: 1.6075
[09/17 01:38:26 visual_prompt]: Classification results with test_vtab-dmlab: top1: 26.33	top5: 97.19	
[09/17 01:38:26 visual_prompt]: Best epoch 21: best metric: 0.285
[09/17 01:38:26 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/17 01:38:36 visual_prompt]: Epoch 22 / 100: avg data time: 1.42e-01, avg batch time: 0.5434, average train loss: 1.7377
[09/17 01:38:40 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1433, average loss: 1.7713
[09/17 01:38:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 99.00	
[09/17 01:39:03 visual_prompt]: 	Test 100/356. loss: 1.886, 0.2046 s / batch. (data: 1.59e-02)max mem: 17.22445 GB 
[09/17 01:39:22 visual_prompt]: 	Test 200/356. loss: 2.266, 0.2012 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/17 01:39:42 visual_prompt]: 	Test 300/356. loss: 1.971, 0.1991 s / batch. (data: 1.57e-02)max mem: 17.22445 GB 
[09/17 01:39:54 visual_prompt]: Inference (test):avg data time: 7.11e-03, avg batch time: 0.1953, average loss: 1.9569
[09/17 01:39:54 visual_prompt]: Classification results with test_vtab-dmlab: top1: 20.33	top5: 96.31	
[09/17 01:39:54 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/17 01:40:05 visual_prompt]: Epoch 23 / 100: avg data time: 1.44e-01, avg batch time: 0.5471, average train loss: 1.7017
[09/17 01:40:09 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1433, average loss: 1.6996
[09/17 01:40:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 99.00	
[09/17 01:40:30 visual_prompt]: 	Test 100/356. loss: 1.603, 0.1851 s / batch. (data: 1.63e-04)max mem: 17.22445 GB 
[09/17 01:40:50 visual_prompt]: 	Test 200/356. loss: 1.839, 0.1836 s / batch. (data: 1.25e-04)max mem: 17.22445 GB 
[09/17 01:41:09 visual_prompt]: 	Test 300/356. loss: 1.644, 0.1985 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/17 01:41:22 visual_prompt]: Inference (test):avg data time: 8.15e-03, avg batch time: 0.1936, average loss: 1.7416
[09/17 01:41:22 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.70	top5: 97.18	
[09/17 01:41:22 visual_prompt]: Best epoch 23: best metric: 0.320
[09/17 01:41:22 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/17 01:41:33 visual_prompt]: Epoch 24 / 100: avg data time: 1.37e-01, avg batch time: 0.5455, average train loss: 1.7288
[09/17 01:41:37 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1433, average loss: 1.4335
[09/17 01:41:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 98.50	
[09/17 01:41:59 visual_prompt]: 	Test 100/356. loss: 1.401, 0.1836 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/17 01:42:18 visual_prompt]: 	Test 200/356. loss: 1.347, 0.1960 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/17 01:42:38 visual_prompt]: 	Test 300/356. loss: 1.330, 0.1985 s / batch. (data: 1.52e-02)max mem: 17.22445 GB 
[09/17 01:42:50 visual_prompt]: Inference (test):avg data time: 8.22e-03, avg batch time: 0.1945, average loss: 1.4366
[09/17 01:42:50 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.94	top5: 97.91	
[09/17 01:42:50 visual_prompt]: Best epoch 24: best metric: 0.360
[09/17 01:42:50 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/17 01:43:01 visual_prompt]: Epoch 25 / 100: avg data time: 1.49e-01, avg batch time: 0.5810, average train loss: 1.6005
[09/17 01:43:05 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1433, average loss: 1.6633
[09/17 01:43:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 93.00	
[09/17 01:43:27 visual_prompt]: 	Test 100/356. loss: 1.793, 0.1878 s / batch. (data: 1.22e-04)max mem: 17.22445 GB 
[09/17 01:43:46 visual_prompt]: 	Test 200/356. loss: 2.324, 0.1999 s / batch. (data: 1.45e-04)max mem: 17.22445 GB 
[09/17 01:44:06 visual_prompt]: 	Test 300/356. loss: 1.788, 0.1882 s / batch. (data: 1.55e-04)max mem: 17.22445 GB 
[09/17 01:44:19 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1951, average loss: 1.8791
[09/17 01:44:19 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.07	top5: 92.98	
[09/17 01:44:19 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/17 01:44:29 visual_prompt]: Epoch 26 / 100: avg data time: 1.47e-01, avg batch time: 0.5500, average train loss: 1.6090
[09/17 01:44:33 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.1433, average loss: 1.9317
[09/17 01:44:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 99.50	
[09/17 01:44:55 visual_prompt]: 	Test 100/356. loss: 1.909, 0.1845 s / batch. (data: 1.18e-04)max mem: 17.22445 GB 
[09/17 01:45:14 visual_prompt]: 	Test 200/356. loss: 1.933, 0.1985 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/17 01:45:34 visual_prompt]: 	Test 300/356. loss: 1.681, 0.1959 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 01:45:46 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1937, average loss: 1.9206
[09/17 01:45:46 visual_prompt]: Classification results with test_vtab-dmlab: top1: 30.87	top5: 97.65	
[09/17 01:45:46 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/17 01:45:57 visual_prompt]: Epoch 27 / 100: avg data time: 1.46e-01, avg batch time: 0.5491, average train loss: 1.6299
[09/17 01:46:01 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1432, average loss: 2.0483
[09/17 01:46:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 99.00	
[09/17 01:46:23 visual_prompt]: 	Test 100/356. loss: 2.068, 0.1886 s / batch. (data: 1.32e-04)max mem: 17.22445 GB 
[09/17 01:46:42 visual_prompt]: 	Test 200/356. loss: 1.881, 0.1961 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 01:47:02 visual_prompt]: 	Test 300/356. loss: 1.926, 0.1847 s / batch. (data: 1.15e-04)max mem: 17.22445 GB 
[09/17 01:47:15 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1956, average loss: 2.0680
[09/17 01:47:15 visual_prompt]: Classification results with test_vtab-dmlab: top1: 29.00	top5: 97.39	
[09/17 01:47:15 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/17 01:47:25 visual_prompt]: Epoch 28 / 100: avg data time: 1.47e-01, avg batch time: 0.5506, average train loss: 1.7486
[09/17 01:47:29 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1432, average loss: 2.5204
[09/17 01:47:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 99.00	
[09/17 01:47:51 visual_prompt]: 	Test 100/356. loss: 2.622, 0.2082 s / batch. (data: 2.56e-02)max mem: 17.22445 GB 
[09/17 01:48:11 visual_prompt]: 	Test 200/356. loss: 2.192, 0.2004 s / batch. (data: 1.49e-02)max mem: 17.22445 GB 
[09/17 01:48:30 visual_prompt]: 	Test 300/356. loss: 2.295, 0.1974 s / batch. (data: 1.42e-02)max mem: 17.22445 GB 
[09/17 01:48:42 visual_prompt]: Inference (test):avg data time: 7.91e-03, avg batch time: 0.1940, average loss: 2.4498
[09/17 01:48:42 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.18	top5: 97.86	
[09/17 01:48:42 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/17 01:48:53 visual_prompt]: Epoch 29 / 100: avg data time: 1.31e-01, avg batch time: 0.5348, average train loss: 1.6241
[09/17 01:48:57 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1432, average loss: 2.0231
[09/17 01:48:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 90.50	
[09/17 01:49:19 visual_prompt]: 	Test 100/356. loss: 2.220, 0.1831 s / batch. (data: 1.31e-04)max mem: 17.22445 GB 
[09/17 01:49:38 visual_prompt]: 	Test 200/356. loss: 1.658, 0.2001 s / batch. (data: 1.70e-02)max mem: 17.22445 GB 
[09/17 01:49:57 visual_prompt]: 	Test 300/356. loss: 2.084, 0.1986 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/17 01:50:10 visual_prompt]: Inference (test):avg data time: 6.94e-03, avg batch time: 0.1935, average loss: 2.0226
[09/17 01:50:10 visual_prompt]: Classification results with test_vtab-dmlab: top1: 32.91	top5: 89.36	
[09/17 01:50:10 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/17 01:50:20 visual_prompt]: Epoch 30 / 100: avg data time: 1.36e-01, avg batch time: 0.5418, average train loss: 1.8693
[09/17 01:50:24 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1435, average loss: 2.2434
[09/17 01:50:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 91.50	
[09/17 01:50:46 visual_prompt]: 	Test 100/356. loss: 1.872, 0.1872 s / batch. (data: 1.63e-04)max mem: 17.22445 GB 
[09/17 01:51:06 visual_prompt]: 	Test 200/356. loss: 2.152, 0.1830 s / batch. (data: 1.61e-04)max mem: 17.22445 GB 
[09/17 01:51:25 visual_prompt]: 	Test 300/356. loss: 2.092, 0.1843 s / batch. (data: 1.74e-04)max mem: 17.22445 GB 
[09/17 01:51:38 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1940, average loss: 2.1560
[09/17 01:51:38 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.88	top5: 93.27	
[09/17 01:51:38 visual_prompt]: Best epoch 30: best metric: 0.380
[09/17 01:51:38 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/17 01:51:48 visual_prompt]: Epoch 31 / 100: avg data time: 1.42e-01, avg batch time: 0.5461, average train loss: 2.0483
[09/17 01:51:52 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1433, average loss: 2.0631
[09/17 01:51:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 98.00	
[09/17 01:52:14 visual_prompt]: 	Test 100/356. loss: 2.200, 0.1834 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/17 01:52:34 visual_prompt]: 	Test 200/356. loss: 2.178, 0.1988 s / batch. (data: 1.56e-02)max mem: 17.22445 GB 
[09/17 01:52:54 visual_prompt]: 	Test 300/356. loss: 1.934, 0.1836 s / batch. (data: 1.08e-04)max mem: 17.22445 GB 
[09/17 01:53:06 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1957, average loss: 2.1461
[09/17 01:53:06 visual_prompt]: Classification results with test_vtab-dmlab: top1: 33.91	top5: 96.00	
[09/17 01:53:06 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/17 01:53:17 visual_prompt]: Epoch 32 / 100: avg data time: 1.43e-01, avg batch time: 0.5439, average train loss: 2.1343
[09/17 01:53:21 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1432, average loss: 1.7055
[09/17 01:53:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 97.50	
[09/17 01:53:43 visual_prompt]: 	Test 100/356. loss: 1.664, 0.1942 s / batch. (data: 1.12e-02)max mem: 17.22445 GB 
[09/17 01:54:02 visual_prompt]: 	Test 200/356. loss: 1.969, 0.1965 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 01:54:22 visual_prompt]: 	Test 300/356. loss: 1.697, 0.1858 s / batch. (data: 1.14e-04)max mem: 17.22445 GB 
[09/17 01:54:34 visual_prompt]: Inference (test):avg data time: 8.05e-03, avg batch time: 0.1961, average loss: 1.8162
[09/17 01:54:35 visual_prompt]: Classification results with test_vtab-dmlab: top1: 29.91	top5: 95.08	
[09/17 01:54:35 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/17 01:54:45 visual_prompt]: Epoch 33 / 100: avg data time: 1.36e-01, avg batch time: 0.5380, average train loss: 1.6833
[09/17 01:54:49 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1433, average loss: 1.3734
[09/17 01:54:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 99.00	
[09/17 01:55:11 visual_prompt]: 	Test 100/356. loss: 1.450, 0.1908 s / batch. (data: 1.66e-03)max mem: 17.22445 GB 
[09/17 01:55:30 visual_prompt]: 	Test 200/356. loss: 1.433, 0.1999 s / batch. (data: 1.29e-04)max mem: 17.22445 GB 
[09/17 01:55:50 visual_prompt]: 	Test 300/356. loss: 1.381, 0.2082 s / batch. (data: 2.47e-02)max mem: 17.22445 GB 
[09/17 01:56:02 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1938, average loss: 1.4617
[09/17 01:56:02 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.41	top5: 98.23	
[09/17 01:56:02 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/17 01:56:12 visual_prompt]: Epoch 34 / 100: avg data time: 1.44e-01, avg batch time: 0.5478, average train loss: 1.4190
[09/17 01:56:16 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1431, average loss: 1.3785
[09/17 01:56:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 98.50	
[09/17 01:56:38 visual_prompt]: 	Test 100/356. loss: 1.370, 0.1981 s / batch. (data: 1.55e-02)max mem: 17.22445 GB 
[09/17 01:56:58 visual_prompt]: 	Test 200/356. loss: 1.546, 0.1967 s / batch. (data: 1.37e-02)max mem: 17.22445 GB 
[09/17 01:57:17 visual_prompt]: 	Test 300/356. loss: 1.324, 0.1852 s / batch. (data: 1.22e-04)max mem: 17.22445 GB 
[09/17 01:57:30 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1941, average loss: 1.4749
[09/17 01:57:30 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.23	top5: 97.48	
[09/17 01:57:30 visual_prompt]: Best epoch 34: best metric: 0.435
[09/17 01:57:30 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/17 01:57:40 visual_prompt]: Epoch 35 / 100: avg data time: 1.41e-01, avg batch time: 0.5432, average train loss: 1.4394
[09/17 01:57:44 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.1433, average loss: 1.5251
[09/17 01:57:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 97.00	
[09/17 01:58:06 visual_prompt]: 	Test 100/356. loss: 1.407, 0.1935 s / batch. (data: 1.07e-04)max mem: 17.22445 GB 
[09/17 01:58:26 visual_prompt]: 	Test 200/356. loss: 1.577, 0.1941 s / batch. (data: 9.75e-05)max mem: 17.22445 GB 
[09/17 01:58:45 visual_prompt]: 	Test 300/356. loss: 1.345, 0.1893 s / batch. (data: 1.36e-04)max mem: 17.22445 GB 
[09/17 01:58:58 visual_prompt]: Inference (test):avg data time: 7.60e-03, avg batch time: 0.1947, average loss: 1.5593
[09/17 01:58:58 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.54	top5: 97.44	
[09/17 01:58:58 visual_prompt]: Best epoch 35: best metric: 0.445
[09/17 01:58:58 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/17 01:59:08 visual_prompt]: Epoch 36 / 100: avg data time: 1.32e-01, avg batch time: 0.5369, average train loss: 1.4629
[09/17 01:59:12 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1431, average loss: 1.7046
[09/17 01:59:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/17 01:59:34 visual_prompt]: 	Test 100/356. loss: 1.699, 0.1984 s / batch. (data: 1.55e-02)max mem: 17.22445 GB 
[09/17 01:59:53 visual_prompt]: 	Test 200/356. loss: 1.900, 0.1842 s / batch. (data: 1.23e-04)max mem: 17.22445 GB 
[09/17 02:00:13 visual_prompt]: 	Test 300/356. loss: 1.675, 0.1883 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/17 02:00:26 visual_prompt]: Inference (test):avg data time: 8.94e-03, avg batch time: 0.1949, average loss: 1.8045
[09/17 02:00:26 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.94	top5: 96.97	
[09/17 02:00:26 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/17 02:00:36 visual_prompt]: Epoch 37 / 100: avg data time: 1.43e-01, avg batch time: 0.5478, average train loss: 1.4964
[09/17 02:00:40 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1463, average loss: 1.7113
[09/17 02:00:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 98.00	
[09/17 02:01:02 visual_prompt]: 	Test 100/356. loss: 1.688, 0.1883 s / batch. (data: 1.41e-04)max mem: 17.22445 GB 
[09/17 02:01:21 visual_prompt]: 	Test 200/356. loss: 2.107, 0.1936 s / batch. (data: 1.29e-04)max mem: 17.22445 GB 
[09/17 02:01:41 visual_prompt]: 	Test 300/356. loss: 1.698, 0.2141 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 02:01:53 visual_prompt]: Inference (test):avg data time: 7.07e-03, avg batch time: 0.1933, average loss: 1.8268
[09/17 02:01:53 visual_prompt]: Classification results with test_vtab-dmlab: top1: 26.72	top5: 97.26	
[09/17 02:01:53 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/17 02:02:03 visual_prompt]: Epoch 38 / 100: avg data time: 1.42e-01, avg batch time: 0.5440, average train loss: 1.4155
[09/17 02:02:08 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1432, average loss: 1.5794
[09/17 02:02:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 95.00	
[09/17 02:02:29 visual_prompt]: 	Test 100/356. loss: 1.391, 0.1840 s / batch. (data: 1.50e-04)max mem: 17.22445 GB 
[09/17 02:02:49 visual_prompt]: 	Test 200/356. loss: 1.603, 0.1961 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/17 02:03:08 visual_prompt]: 	Test 300/356. loss: 1.458, 0.1848 s / batch. (data: 1.42e-04)max mem: 17.22445 GB 
[09/17 02:03:21 visual_prompt]: Inference (test):avg data time: 7.89e-03, avg batch time: 0.1936, average loss: 1.5938
[09/17 02:03:21 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.67	top5: 95.56	
[09/17 02:03:21 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/17 02:03:31 visual_prompt]: Epoch 39 / 100: avg data time: 1.46e-01, avg batch time: 0.5492, average train loss: 1.4831
[09/17 02:03:35 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1431, average loss: 1.6364
[09/17 02:03:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 99.00	
[09/17 02:03:57 visual_prompt]: 	Test 100/356. loss: 1.520, 0.1839 s / batch. (data: 1.37e-04)max mem: 17.22445 GB 
[09/17 02:04:17 visual_prompt]: 	Test 200/356. loss: 1.824, 0.2011 s / batch. (data: 1.81e-02)max mem: 17.22445 GB 
[09/17 02:04:37 visual_prompt]: 	Test 300/356. loss: 1.708, 0.1871 s / batch. (data: 1.09e-04)max mem: 17.22445 GB 
[09/17 02:04:49 visual_prompt]: Inference (test):avg data time: 7.93e-03, avg batch time: 0.1954, average loss: 1.6930
[09/17 02:04:50 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.77	top5: 97.18	
[09/17 02:04:50 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/17 02:05:00 visual_prompt]: Epoch 40 / 100: avg data time: 1.49e-01, avg batch time: 0.5511, average train loss: 1.3830
[09/17 02:05:04 visual_prompt]: Inference (val):avg data time: 7.45e-05, avg batch time: 0.1446, average loss: 1.4453
[09/17 02:05:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/17 02:05:26 visual_prompt]: 	Test 100/356. loss: 1.660, 0.1905 s / batch. (data: 2.97e-04)max mem: 17.22445 GB 
[09/17 02:05:45 visual_prompt]: 	Test 200/356. loss: 1.891, 0.1840 s / batch. (data: 1.06e-04)max mem: 17.22445 GB 
[09/17 02:06:05 visual_prompt]: 	Test 300/356. loss: 1.550, 0.1856 s / batch. (data: 1.56e-04)max mem: 17.22445 GB 
[09/17 02:06:17 visual_prompt]: Inference (test):avg data time: 6.84e-03, avg batch time: 0.1934, average loss: 1.6277
[09/17 02:06:17 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.96	top5: 96.84	
[09/17 02:06:17 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/17 02:06:27 visual_prompt]: Epoch 41 / 100: avg data time: 1.49e-01, avg batch time: 0.5505, average train loss: 1.3613
[09/17 02:06:32 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1431, average loss: 1.4829
[09/17 02:06:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.00	
[09/17 02:06:53 visual_prompt]: 	Test 100/356. loss: 1.713, 0.2280 s / batch. (data: 4.10e-02)max mem: 17.22445 GB 
[09/17 02:07:13 visual_prompt]: 	Test 200/356. loss: 2.229, 0.1920 s / batch. (data: 8.89e-03)max mem: 17.22445 GB 
[09/17 02:07:32 visual_prompt]: 	Test 300/356. loss: 1.736, 0.2063 s / batch. (data: 1.34e-02)max mem: 17.22445 GB 
[09/17 02:07:45 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1931, average loss: 1.7864
[09/17 02:07:45 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.68	top5: 92.30	
[09/17 02:07:45 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/17 02:07:55 visual_prompt]: Epoch 42 / 100: avg data time: 1.37e-01, avg batch time: 0.5421, average train loss: 1.4091
[09/17 02:07:59 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1430, average loss: 1.6565
[09/17 02:07:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.50	
[09/17 02:08:21 visual_prompt]: 	Test 100/356. loss: 1.793, 0.2293 s / batch. (data: 2.46e-02)max mem: 17.22445 GB 
[09/17 02:08:41 visual_prompt]: 	Test 200/356. loss: 1.581, 0.1885 s / batch. (data: 5.26e-03)max mem: 17.22445 GB 
[09/17 02:09:00 visual_prompt]: 	Test 300/356. loss: 1.588, 0.1957 s / batch. (data: 1.37e-04)max mem: 17.22445 GB 
[09/17 02:09:14 visual_prompt]: Inference (test):avg data time: 6.73e-03, avg batch time: 0.1950, average loss: 1.7263
[09/17 02:09:14 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.90	top5: 96.12	
[09/17 02:09:14 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/17 02:09:24 visual_prompt]: Epoch 43 / 100: avg data time: 1.50e-01, avg batch time: 0.5554, average train loss: 1.3480
[09/17 02:09:28 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1431, average loss: 1.4973
[09/17 02:09:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 98.50	
[09/17 02:09:50 visual_prompt]: 	Test 100/356. loss: 1.851, 0.1835 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/17 02:10:10 visual_prompt]: 	Test 200/356. loss: 2.093, 0.1878 s / batch. (data: 1.63e-04)max mem: 17.22445 GB 
[09/17 02:10:29 visual_prompt]: 	Test 300/356. loss: 1.653, 0.1989 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/17 02:10:41 visual_prompt]: Inference (test):avg data time: 7.98e-03, avg batch time: 0.1935, average loss: 1.8536
[09/17 02:10:41 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.01	top5: 97.22	
[09/17 02:10:41 visual_prompt]: Best epoch 43: best metric: 0.460
[09/17 02:10:41 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/17 02:10:52 visual_prompt]: Epoch 44 / 100: avg data time: 1.40e-01, avg batch time: 0.5457, average train loss: 1.4676
[09/17 02:10:56 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1463, average loss: 1.5166
[09/17 02:10:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 99.50	
[09/17 02:11:18 visual_prompt]: 	Test 100/356. loss: 1.598, 0.1980 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/17 02:11:37 visual_prompt]: 	Test 200/356. loss: 1.869, 0.1838 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/17 02:11:57 visual_prompt]: 	Test 300/356. loss: 1.715, 0.1842 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/17 02:12:09 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1937, average loss: 1.6973
[09/17 02:12:09 visual_prompt]: Classification results with test_vtab-dmlab: top1: 29.61	top5: 97.87	
[09/17 02:12:09 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/17 02:12:20 visual_prompt]: Epoch 45 / 100: avg data time: 1.40e-01, avg batch time: 0.5421, average train loss: 1.4089
[09/17 02:12:24 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1433, average loss: 1.4945
[09/17 02:12:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 99.00	
[09/17 02:12:46 visual_prompt]: 	Test 100/356. loss: 1.733, 0.1980 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 02:13:06 visual_prompt]: 	Test 200/356. loss: 2.278, 0.1895 s / batch. (data: 1.12e-04)max mem: 17.22445 GB 
[09/17 02:13:25 visual_prompt]: 	Test 300/356. loss: 1.824, 0.1843 s / batch. (data: 1.18e-04)max mem: 17.22445 GB 
[09/17 02:13:38 visual_prompt]: Inference (test):avg data time: 7.49e-03, avg batch time: 0.1954, average loss: 1.8245
[09/17 02:13:38 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.24	top5: 96.70	
[09/17 02:13:38 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/17 02:13:48 visual_prompt]: Epoch 46 / 100: avg data time: 1.46e-01, avg batch time: 0.5469, average train loss: 1.3535
[09/17 02:13:52 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.1433, average loss: 1.2683
[09/17 02:13:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 99.50	
[09/17 02:14:14 visual_prompt]: 	Test 100/356. loss: 1.373, 0.2054 s / batch. (data: 1.49e-02)max mem: 17.22445 GB 
[09/17 02:14:34 visual_prompt]: 	Test 200/356. loss: 1.532, 0.1985 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/17 02:14:53 visual_prompt]: 	Test 300/356. loss: 1.399, 0.1844 s / batch. (data: 1.38e-04)max mem: 17.22445 GB 
[09/17 02:15:06 visual_prompt]: Inference (test):avg data time: 8.45e-03, avg batch time: 0.1953, average loss: 1.4622
[09/17 02:15:06 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.69	top5: 97.16	
[09/17 02:15:06 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/17 02:15:16 visual_prompt]: Epoch 47 / 100: avg data time: 1.40e-01, avg batch time: 0.5446, average train loss: 1.3221
[09/17 02:15:20 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1432, average loss: 1.3017
[09/17 02:15:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 99.00	
[09/17 02:15:42 visual_prompt]: 	Test 100/356. loss: 1.314, 0.1966 s / batch. (data: 1.33e-02)max mem: 17.22445 GB 
[09/17 02:16:01 visual_prompt]: 	Test 200/356. loss: 1.723, 0.1836 s / batch. (data: 1.32e-04)max mem: 17.22445 GB 
[09/17 02:16:21 visual_prompt]: 	Test 300/356. loss: 1.520, 0.1848 s / batch. (data: 1.11e-04)max mem: 17.22445 GB 
[09/17 02:16:33 visual_prompt]: Inference (test):avg data time: 6.97e-03, avg batch time: 0.1930, average loss: 1.5315
[09/17 02:16:33 visual_prompt]: Classification results with test_vtab-dmlab: top1: 33.85	top5: 97.58	
[09/17 02:16:33 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/17 02:16:43 visual_prompt]: Epoch 48 / 100: avg data time: 1.39e-01, avg batch time: 0.5427, average train loss: 1.2070
[09/17 02:16:47 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1444, average loss: 1.3924
[09/17 02:16:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 100.00	
[09/17 02:17:09 visual_prompt]: 	Test 100/356. loss: 1.536, 0.1846 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/17 02:17:29 visual_prompt]: 	Test 200/356. loss: 1.626, 0.1843 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/17 02:17:48 visual_prompt]: 	Test 300/356. loss: 1.491, 0.1957 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 02:18:01 visual_prompt]: Inference (test):avg data time: 6.77e-03, avg batch time: 0.1935, average loss: 1.6237
[09/17 02:18:01 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.39	top5: 98.15	
[09/17 02:18:01 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/17 02:18:12 visual_prompt]: Epoch 49 / 100: avg data time: 1.45e-01, avg batch time: 0.5459, average train loss: 1.3564
[09/17 02:18:16 visual_prompt]: Inference (val):avg data time: 4.76e-05, avg batch time: 0.1433, average loss: 1.4237
[09/17 02:18:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 48.00	top5: 97.50	
[09/17 02:18:38 visual_prompt]: 	Test 100/356. loss: 1.441, 0.1960 s / batch. (data: 1.32e-02)max mem: 17.22445 GB 
[09/17 02:18:57 visual_prompt]: 	Test 200/356. loss: 1.550, 0.1842 s / batch. (data: 9.87e-05)max mem: 17.22445 GB 
[09/17 02:19:18 visual_prompt]: 	Test 300/356. loss: 1.534, 0.1972 s / batch. (data: 6.91e-05)max mem: 17.22445 GB 
[09/17 02:19:31 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1984, average loss: 1.5587
[09/17 02:19:31 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.90	top5: 97.32	
[09/17 02:19:31 visual_prompt]: Best epoch 49: best metric: 0.480
[09/17 02:19:31 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/17 02:19:41 visual_prompt]: Epoch 50 / 100: avg data time: 1.34e-01, avg batch time: 0.5525, average train loss: 1.1733
[09/17 02:19:45 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1432, average loss: 1.6204
[09/17 02:19:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 48.00	top5: 99.00	
[09/17 02:20:07 visual_prompt]: 	Test 100/356. loss: 1.677, 0.1835 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 02:20:27 visual_prompt]: 	Test 200/356. loss: 1.895, 0.1855 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/17 02:20:46 visual_prompt]: 	Test 300/356. loss: 1.761, 0.1847 s / batch. (data: 1.62e-04)max mem: 17.22445 GB 
[09/17 02:20:59 visual_prompt]: Inference (test):avg data time: 7.75e-03, avg batch time: 0.1936, average loss: 1.8129
[09/17 02:20:59 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.68	top5: 97.13	
[09/17 02:20:59 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/17 02:21:09 visual_prompt]: Epoch 51 / 100: avg data time: 1.42e-01, avg batch time: 0.5474, average train loss: 1.1380
[09/17 02:21:13 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1434, average loss: 1.1589
[09/17 02:21:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 99.50	
[09/17 02:21:35 visual_prompt]: 	Test 100/356. loss: 1.598, 0.1975 s / batch. (data: 1.45e-02)max mem: 17.22445 GB 
[09/17 02:21:54 visual_prompt]: 	Test 200/356. loss: 1.445, 0.2001 s / batch. (data: 1.67e-02)max mem: 17.22445 GB 
[09/17 02:22:14 visual_prompt]: 	Test 300/356. loss: 1.481, 0.1843 s / batch. (data: 1.25e-04)max mem: 17.22445 GB 
[09/17 02:22:26 visual_prompt]: Inference (test):avg data time: 7.73e-03, avg batch time: 0.1936, average loss: 1.4803
[09/17 02:22:26 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.87	top5: 98.25	
[09/17 02:22:26 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/17 02:22:37 visual_prompt]: Epoch 52 / 100: avg data time: 1.44e-01, avg batch time: 0.5442, average train loss: 1.2401
[09/17 02:22:41 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1434, average loss: 1.2764
[09/17 02:22:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 99.00	
[09/17 02:23:03 visual_prompt]: 	Test 100/356. loss: 1.706, 0.1840 s / batch. (data: 2.17e-04)max mem: 17.22445 GB 
[09/17 02:23:22 visual_prompt]: 	Test 200/356. loss: 1.844, 0.1960 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 02:23:42 visual_prompt]: 	Test 300/356. loss: 1.642, 0.1993 s / batch. (data: 1.53e-02)max mem: 17.22445 GB 
[09/17 02:23:54 visual_prompt]: Inference (test):avg data time: 8.64e-03, avg batch time: 0.1948, average loss: 1.6461
[09/17 02:23:54 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.78	top5: 97.63	
[09/17 02:23:54 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/17 02:24:05 visual_prompt]: Epoch 53 / 100: avg data time: 1.40e-01, avg batch time: 0.5413, average train loss: 1.1500
[09/17 02:24:09 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1433, average loss: 1.1847
[09/17 02:24:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 99.50	
[09/17 02:24:30 visual_prompt]: 	Test 100/356. loss: 1.553, 0.1838 s / batch. (data: 1.60e-04)max mem: 17.22445 GB 
[09/17 02:24:50 visual_prompt]: 	Test 200/356. loss: 1.448, 0.1844 s / batch. (data: 1.65e-04)max mem: 17.22445 GB 
[09/17 02:25:09 visual_prompt]: 	Test 300/356. loss: 1.583, 0.1843 s / batch. (data: 1.17e-04)max mem: 17.22445 GB 
[09/17 02:25:22 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1937, average loss: 1.5782
[09/17 02:25:22 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.73	top5: 97.79	
[09/17 02:25:22 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/17 02:25:32 visual_prompt]: Epoch 54 / 100: avg data time: 1.35e-01, avg batch time: 0.5396, average train loss: 1.0693
[09/17 02:25:36 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1441, average loss: 0.9867
[09/17 02:25:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 58.00	top5: 99.50	
[09/17 02:25:58 visual_prompt]: 	Test 100/356. loss: 1.601, 0.2214 s / batch. (data: 1.54e-02)max mem: 17.22445 GB 
[09/17 02:26:17 visual_prompt]: 	Test 200/356. loss: 1.429, 0.1837 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/17 02:26:36 visual_prompt]: 	Test 300/356. loss: 1.469, 0.1839 s / batch. (data: 1.62e-04)max mem: 17.22445 GB 
[09/17 02:26:49 visual_prompt]: Inference (test):avg data time: 7.43e-03, avg batch time: 0.1931, average loss: 1.4660
[09/17 02:26:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.23	top5: 98.09	
[09/17 02:26:49 visual_prompt]: Best epoch 54: best metric: 0.580
[09/17 02:26:49 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/17 02:27:00 visual_prompt]: Epoch 55 / 100: avg data time: 1.44e-01, avg batch time: 0.5456, average train loss: 1.0759
[09/17 02:27:04 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1431, average loss: 1.3067
[09/17 02:27:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 98.50	
[09/17 02:27:26 visual_prompt]: 	Test 100/356. loss: 1.848, 0.2629 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/17 02:27:45 visual_prompt]: 	Test 200/356. loss: 1.878, 0.1834 s / batch. (data: 1.30e-04)max mem: 17.22445 GB 
[09/17 02:28:05 visual_prompt]: 	Test 300/356. loss: 1.677, 0.1886 s / batch. (data: 1.53e-04)max mem: 17.22445 GB 
[09/17 02:28:17 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1949, average loss: 1.7473
[09/17 02:28:17 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.55	top5: 97.36	
[09/17 02:28:17 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/17 02:28:28 visual_prompt]: Epoch 56 / 100: avg data time: 1.40e-01, avg batch time: 0.5417, average train loss: 1.0902
[09/17 02:28:32 visual_prompt]: Inference (val):avg data time: 5.25e-05, avg batch time: 0.1501, average loss: 1.1484
[09/17 02:28:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 99.50	
[09/17 02:28:54 visual_prompt]: 	Test 100/356. loss: 1.800, 0.2078 s / batch. (data: 2.52e-02)max mem: 17.22445 GB 
[09/17 02:29:13 visual_prompt]: 	Test 200/356. loss: 1.799, 0.1841 s / batch. (data: 1.03e-04)max mem: 17.22445 GB 
[09/17 02:29:32 visual_prompt]: 	Test 300/356. loss: 1.563, 0.1831 s / batch. (data: 1.19e-04)max mem: 17.22445 GB 
[09/17 02:29:45 visual_prompt]: Inference (test):avg data time: 6.97e-03, avg batch time: 0.1932, average loss: 1.6597
[09/17 02:29:45 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.44	top5: 97.77	
[09/17 02:29:45 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/17 02:29:55 visual_prompt]: Epoch 57 / 100: avg data time: 1.44e-01, avg batch time: 0.5458, average train loss: 0.9984
[09/17 02:29:59 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1441, average loss: 0.8805
[09/17 02:29:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 62.50	top5: 100.00	
[09/17 02:30:21 visual_prompt]: 	Test 100/356. loss: 1.483, 0.1833 s / batch. (data: 5.20e-05)max mem: 17.22445 GB 
[09/17 02:30:41 visual_prompt]: 	Test 200/356. loss: 1.778, 0.1834 s / batch. (data: 3.77e-05)max mem: 17.22445 GB 
[09/17 02:31:00 visual_prompt]: 	Test 300/356. loss: 1.361, 0.2071 s / batch. (data: 2.38e-02)max mem: 17.22445 GB 
[09/17 02:31:13 visual_prompt]: Inference (test):avg data time: 8.13e-03, avg batch time: 0.1938, average loss: 1.5235
[09/17 02:31:13 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.83	top5: 97.18	
[09/17 02:31:13 visual_prompt]: Best epoch 57: best metric: 0.625
[09/17 02:31:13 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/17 02:31:23 visual_prompt]: Epoch 58 / 100: avg data time: 1.40e-01, avg batch time: 0.5448, average train loss: 1.0152
[09/17 02:31:27 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1433, average loss: 1.0243
[09/17 02:31:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 57.50	top5: 99.50	
[09/17 02:31:49 visual_prompt]: 	Test 100/356. loss: 1.586, 0.1967 s / batch. (data: 1.42e-02)max mem: 17.22445 GB 
[09/17 02:32:08 visual_prompt]: 	Test 200/356. loss: 1.893, 0.1841 s / batch. (data: 1.79e-04)max mem: 17.22445 GB 
[09/17 02:32:28 visual_prompt]: 	Test 300/356. loss: 1.484, 0.1841 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/17 02:32:40 visual_prompt]: Inference (test):avg data time: 6.59e-03, avg batch time: 0.1931, average loss: 1.6766
[09/17 02:32:40 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.26	top5: 96.51	
[09/17 02:32:40 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/17 02:32:51 visual_prompt]: Epoch 59 / 100: avg data time: 1.45e-01, avg batch time: 0.5538, average train loss: 1.0604
[09/17 02:32:55 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1431, average loss: 1.0861
[09/17 02:32:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 52.00	top5: 99.50	
[09/17 02:33:17 visual_prompt]: 	Test 100/356. loss: 1.801, 0.2010 s / batch. (data: 1.32e-02)max mem: 17.22445 GB 
[09/17 02:33:36 visual_prompt]: 	Test 200/356. loss: 2.011, 0.2211 s / batch. (data: 2.04e-02)max mem: 17.22445 GB 
[09/17 02:33:56 visual_prompt]: 	Test 300/356. loss: 1.538, 0.1987 s / batch. (data: 1.57e-02)max mem: 17.22445 GB 
[09/17 02:34:09 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1947, average loss: 1.7525
[09/17 02:34:09 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.61	top5: 96.76	
[09/17 02:34:09 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/17 02:34:19 visual_prompt]: Epoch 60 / 100: avg data time: 1.38e-01, avg batch time: 0.5393, average train loss: 1.0037
[09/17 02:34:23 visual_prompt]: Inference (val):avg data time: 4.79e-05, avg batch time: 0.1484, average loss: 0.9789
[09/17 02:34:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 56.00	top5: 100.00	
[09/17 02:34:46 visual_prompt]: 	Test 100/356. loss: 1.468, 0.2121 s / batch. (data: 2.61e-02)max mem: 17.22445 GB 
[09/17 02:35:05 visual_prompt]: 	Test 200/356. loss: 1.442, 0.1941 s / batch. (data: 1.26e-04)max mem: 17.22445 GB 
[09/17 02:35:25 visual_prompt]: 	Test 300/356. loss: 1.371, 0.1982 s / batch. (data: 1.44e-02)max mem: 17.22445 GB 
[09/17 02:35:37 visual_prompt]: Inference (test):avg data time: 8.03e-03, avg batch time: 0.1958, average loss: 1.5259
[09/17 02:35:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.98	top5: 97.50	
[09/17 02:35:37 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/17 02:35:48 visual_prompt]: Epoch 61 / 100: avg data time: 1.51e-01, avg batch time: 0.5536, average train loss: 0.9403
[09/17 02:35:52 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1432, average loss: 0.9419
[09/17 02:35:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 52.50	top5: 100.00	
[09/17 02:36:14 visual_prompt]: 	Test 100/356. loss: 1.778, 0.1955 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 02:36:34 visual_prompt]: 	Test 200/356. loss: 1.637, 0.1841 s / batch. (data: 1.57e-04)max mem: 17.22445 GB 
[09/17 02:36:53 visual_prompt]: 	Test 300/356. loss: 1.540, 0.1981 s / batch. (data: 2.21e-04)max mem: 17.22445 GB 
[09/17 02:37:06 visual_prompt]: Inference (test):avg data time: 7.62e-03, avg batch time: 0.1955, average loss: 1.7125
[09/17 02:37:06 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.98	top5: 98.05	
[09/17 02:37:06 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/17 02:37:16 visual_prompt]: Epoch 62 / 100: avg data time: 1.46e-01, avg batch time: 0.5472, average train loss: 0.9365
[09/17 02:37:21 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1433, average loss: 0.9232
[09/17 02:37:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 52.00	top5: 100.00	
[09/17 02:37:43 visual_prompt]: 	Test 100/356. loss: 1.759, 0.2006 s / batch. (data: 1.74e-02)max mem: 17.22445 GB 
[09/17 02:38:02 visual_prompt]: 	Test 200/356. loss: 1.841, 0.1954 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 02:38:21 visual_prompt]: 	Test 300/356. loss: 1.445, 0.1848 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/17 02:38:34 visual_prompt]: Inference (test):avg data time: 8.28e-03, avg batch time: 0.1946, average loss: 1.7870
[09/17 02:38:34 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.76	top5: 97.85	
[09/17 02:38:34 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/17 02:38:44 visual_prompt]: Epoch 63 / 100: avg data time: 1.41e-01, avg batch time: 0.5437, average train loss: 0.8734
[09/17 02:38:48 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1431, average loss: 0.8081
[09/17 02:38:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 61.50	top5: 100.00	
[09/17 02:39:10 visual_prompt]: 	Test 100/356. loss: 1.443, 0.1986 s / batch. (data: 1.23e-04)max mem: 17.22445 GB 
[09/17 02:39:30 visual_prompt]: 	Test 200/356. loss: 1.605, 0.1960 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 02:39:50 visual_prompt]: 	Test 300/356. loss: 1.308, 0.1839 s / batch. (data: 1.38e-04)max mem: 17.22445 GB 
[09/17 02:40:02 visual_prompt]: Inference (test):avg data time: 8.71e-03, avg batch time: 0.1951, average loss: 1.6470
[09/17 02:40:02 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.31	top5: 96.45	
[09/17 02:40:02 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/17 02:40:13 visual_prompt]: Epoch 64 / 100: avg data time: 1.34e-01, avg batch time: 0.5382, average train loss: 0.8390
[09/17 02:40:17 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1432, average loss: 0.7737
[09/17 02:40:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 64.50	top5: 100.00	
[09/17 02:40:39 visual_prompt]: 	Test 100/356. loss: 1.445, 0.1961 s / batch. (data: 1.38e-02)max mem: 17.22445 GB 
[09/17 02:40:59 visual_prompt]: 	Test 200/356. loss: 2.079, 0.1991 s / batch. (data: 1.55e-02)max mem: 17.22445 GB 
[09/17 02:41:18 visual_prompt]: 	Test 300/356. loss: 1.402, 0.2118 s / batch. (data: 2.80e-02)max mem: 17.22445 GB 
[09/17 02:41:31 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1958, average loss: 1.6518
[09/17 02:41:31 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.97	top5: 97.27	
[09/17 02:41:31 visual_prompt]: Best epoch 64: best metric: 0.645
[09/17 02:41:31 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/17 02:41:41 visual_prompt]: Epoch 65 / 100: avg data time: 1.46e-01, avg batch time: 0.5493, average train loss: 0.7797
[09/17 02:41:45 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1433, average loss: 0.9411
[09/17 02:41:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 57.50	top5: 100.00	
[09/17 02:42:07 visual_prompt]: 	Test 100/356. loss: 1.826, 0.2002 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/17 02:42:26 visual_prompt]: 	Test 200/356. loss: 1.915, 0.1954 s / batch. (data: 1.22e-02)max mem: 17.22445 GB 
[09/17 02:42:46 visual_prompt]: 	Test 300/356. loss: 1.850, 0.1960 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/17 02:42:58 visual_prompt]: Inference (test):avg data time: 7.76e-03, avg batch time: 0.1938, average loss: 1.9403
[09/17 02:42:58 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.28	top5: 97.40	
[09/17 02:42:58 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/17 02:43:09 visual_prompt]: Epoch 66 / 100: avg data time: 1.35e-01, avg batch time: 0.5390, average train loss: 0.8161
[09/17 02:43:13 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1433, average loss: 0.8953
[09/17 02:43:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 60.00	top5: 100.00	
[09/17 02:43:34 visual_prompt]: 	Test 100/356. loss: 1.651, 0.2000 s / batch. (data: 1.33e-02)max mem: 17.22445 GB 
[09/17 02:43:54 visual_prompt]: 	Test 200/356. loss: 1.948, 0.1841 s / batch. (data: 1.33e-04)max mem: 17.22445 GB 
[09/17 02:44:13 visual_prompt]: 	Test 300/356. loss: 1.605, 0.1839 s / batch. (data: 1.40e-04)max mem: 17.22445 GB 
[09/17 02:44:26 visual_prompt]: Inference (test):avg data time: 8.29e-03, avg batch time: 0.1946, average loss: 1.7214
[09/17 02:44:26 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.87	top5: 97.91	
[09/17 02:44:26 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/17 02:44:36 visual_prompt]: Epoch 67 / 100: avg data time: 1.38e-01, avg batch time: 0.5421, average train loss: 0.8732
[09/17 02:44:41 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1432, average loss: 0.9037
[09/17 02:44:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 61.00	top5: 100.00	
[09/17 02:45:02 visual_prompt]: 	Test 100/356. loss: 1.423, 0.1985 s / batch. (data: 1.52e-04)max mem: 17.22445 GB 
[09/17 02:45:22 visual_prompt]: 	Test 200/356. loss: 1.999, 0.2175 s / batch. (data: 2.17e-02)max mem: 17.22445 GB 
[09/17 02:45:42 visual_prompt]: 	Test 300/356. loss: 1.629, 0.1841 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/17 02:45:54 visual_prompt]: Inference (test):avg data time: 8.60e-03, avg batch time: 0.1950, average loss: 1.7528
[09/17 02:45:54 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.51	top5: 97.64	
[09/17 02:45:54 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/17 02:46:04 visual_prompt]: Epoch 68 / 100: avg data time: 1.40e-01, avg batch time: 0.5403, average train loss: 0.7683
[09/17 02:46:08 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1432, average loss: 1.0023
[09/17 02:46:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 53.00	top5: 100.00	
[09/17 02:46:30 visual_prompt]: 	Test 100/356. loss: 2.069, 0.1836 s / batch. (data: 1.41e-04)max mem: 17.22445 GB 
[09/17 02:46:50 visual_prompt]: 	Test 200/356. loss: 1.978, 0.2097 s / batch. (data: 2.66e-02)max mem: 17.22445 GB 
[09/17 02:47:09 visual_prompt]: 	Test 300/356. loss: 1.604, 0.1971 s / batch. (data: 1.37e-04)max mem: 17.22445 GB 
[09/17 02:47:22 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1941, average loss: 2.0259
[09/17 02:47:22 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.42	top5: 97.70	
[09/17 02:47:22 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/17 02:47:32 visual_prompt]: Epoch 69 / 100: avg data time: 1.45e-01, avg batch time: 0.5474, average train loss: 0.7804
[09/17 02:47:36 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1431, average loss: 0.7977
[09/17 02:47:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 66.00	top5: 100.00	
[09/17 02:47:58 visual_prompt]: 	Test 100/356. loss: 1.861, 0.1832 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 02:48:17 visual_prompt]: 	Test 200/356. loss: 2.435, 0.1955 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 02:48:36 visual_prompt]: 	Test 300/356. loss: 1.839, 0.1987 s / batch. (data: 1.56e-02)max mem: 17.22445 GB 
[09/17 02:48:49 visual_prompt]: Inference (test):avg data time: 7.05e-03, avg batch time: 0.1925, average loss: 1.9893
[09/17 02:48:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.07	top5: 95.79	
[09/17 02:48:49 visual_prompt]: Best epoch 69: best metric: 0.660
[09/17 02:48:49 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/17 02:48:59 visual_prompt]: Epoch 70 / 100: avg data time: 1.34e-01, avg batch time: 0.5386, average train loss: 0.6481
[09/17 02:49:03 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1432, average loss: 0.5971
[09/17 02:49:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 70.50	top5: 100.00	
[09/17 02:49:25 visual_prompt]: 	Test 100/356. loss: 1.715, 0.1954 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/17 02:49:44 visual_prompt]: 	Test 200/356. loss: 2.099, 0.1840 s / batch. (data: 1.11e-04)max mem: 17.22445 GB 
[09/17 02:50:04 visual_prompt]: 	Test 300/356. loss: 1.624, 0.1976 s / batch. (data: 1.63e-04)max mem: 17.22445 GB 
[09/17 02:50:17 visual_prompt]: Inference (test):avg data time: 7.09e-03, avg batch time: 0.1945, average loss: 1.8511
[09/17 02:50:17 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.01	top5: 97.72	
[09/17 02:50:17 visual_prompt]: Best epoch 70: best metric: 0.705
[09/17 02:50:17 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/17 02:50:27 visual_prompt]: Epoch 71 / 100: avg data time: 1.44e-01, avg batch time: 0.5482, average train loss: 0.5457
[09/17 02:50:31 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1433, average loss: 0.7326
[09/17 02:50:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 65.00	top5: 100.00	
[09/17 02:50:53 visual_prompt]: 	Test 100/356. loss: 2.324, 0.1902 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/17 02:51:12 visual_prompt]: 	Test 200/356. loss: 2.043, 0.1960 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 02:51:32 visual_prompt]: 	Test 300/356. loss: 1.870, 0.1990 s / batch. (data: 1.57e-02)max mem: 17.22445 GB 
[09/17 02:51:44 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1939, average loss: 2.2588
[09/17 02:51:44 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.91	top5: 97.66	
[09/17 02:51:44 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/17 02:51:55 visual_prompt]: Epoch 72 / 100: avg data time: 1.46e-01, avg batch time: 0.5484, average train loss: 0.5129
[09/17 02:51:59 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1431, average loss: 0.5263
[09/17 02:51:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 73.00	top5: 100.00	
[09/17 02:52:21 visual_prompt]: 	Test 100/356. loss: 2.013, 0.1835 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/17 02:52:40 visual_prompt]: 	Test 200/356. loss: 2.655, 0.1931 s / batch. (data: 1.07e-04)max mem: 17.22445 GB 
[09/17 02:53:00 visual_prompt]: 	Test 300/356. loss: 1.726, 0.2050 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/17 02:53:12 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1946, average loss: 2.2578
[09/17 02:53:12 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.03	top5: 96.71	
[09/17 02:53:12 visual_prompt]: Best epoch 72: best metric: 0.730
[09/17 02:53:12 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/17 02:53:23 visual_prompt]: Epoch 73 / 100: avg data time: 1.46e-01, avg batch time: 0.5491, average train loss: 0.5277
[09/17 02:53:27 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1431, average loss: 0.6245
[09/17 02:53:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 71.50	top5: 100.00	
[09/17 02:53:49 visual_prompt]: 	Test 100/356. loss: 2.242, 0.1936 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/17 02:54:08 visual_prompt]: 	Test 200/356. loss: 2.148, 0.1970 s / batch. (data: 1.47e-04)max mem: 17.22445 GB 
[09/17 02:54:28 visual_prompt]: 	Test 300/356. loss: 1.867, 0.2124 s / batch. (data: 3.86e-05)max mem: 17.22445 GB 
[09/17 02:54:40 visual_prompt]: Inference (test):avg data time: 7.69e-03, avg batch time: 0.1936, average loss: 2.2003
[09/17 02:54:40 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.84	top5: 97.27	
[09/17 02:54:40 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/17 02:54:50 visual_prompt]: Epoch 74 / 100: avg data time: 1.39e-01, avg batch time: 0.5405, average train loss: 0.5138
[09/17 02:54:55 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1432, average loss: 0.5803
[09/17 02:54:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 72.50	top5: 100.00	
[09/17 02:55:16 visual_prompt]: 	Test 100/356. loss: 2.434, 0.2131 s / batch. (data: 1.52e-02)max mem: 17.22445 GB 
[09/17 02:55:36 visual_prompt]: 	Test 200/356. loss: 2.673, 0.1937 s / batch. (data: 1.46e-04)max mem: 17.22445 GB 
[09/17 02:55:55 visual_prompt]: 	Test 300/356. loss: 2.105, 0.1988 s / batch. (data: 1.55e-02)max mem: 17.22445 GB 
[09/17 02:56:08 visual_prompt]: Inference (test):avg data time: 7.89e-03, avg batch time: 0.1937, average loss: 2.4355
[09/17 02:56:08 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.60	top5: 96.46	
[09/17 02:56:08 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/17 02:56:18 visual_prompt]: Epoch 75 / 100: avg data time: 1.26e-01, avg batch time: 0.5318, average train loss: 0.5400
[09/17 02:56:22 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1430, average loss: 0.5627
[09/17 02:56:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 75.50	top5: 100.00	
[09/17 02:56:44 visual_prompt]: 	Test 100/356. loss: 2.429, 0.2075 s / batch. (data: 2.49e-02)max mem: 17.22445 GB 
[09/17 02:57:04 visual_prompt]: 	Test 200/356. loss: 2.869, 0.1879 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 02:57:24 visual_prompt]: 	Test 300/356. loss: 2.099, 0.2000 s / batch. (data: 3.77e-05)max mem: 17.22445 GB 
[09/17 02:57:36 visual_prompt]: Inference (test):avg data time: 6.74e-03, avg batch time: 0.1956, average loss: 2.4681
[09/17 02:57:36 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.55	top5: 95.87	
[09/17 02:57:36 visual_prompt]: Best epoch 75: best metric: 0.755
[09/17 02:57:36 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/17 02:57:47 visual_prompt]: Epoch 76 / 100: avg data time: 1.45e-01, avg batch time: 0.5491, average train loss: 0.5099
[09/17 02:57:51 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1449, average loss: 0.4373
[09/17 02:57:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 79.00	top5: 100.00	
[09/17 02:58:13 visual_prompt]: 	Test 100/356. loss: 2.228, 0.1869 s / batch. (data: 1.68e-04)max mem: 17.22445 GB 
[09/17 02:58:32 visual_prompt]: 	Test 200/356. loss: 2.037, 0.1838 s / batch. (data: 1.70e-04)max mem: 17.22445 GB 
[09/17 02:58:52 visual_prompt]: 	Test 300/356. loss: 1.924, 0.1843 s / batch. (data: 1.64e-04)max mem: 17.22445 GB 
[09/17 02:59:04 visual_prompt]: Inference (test):avg data time: 8.21e-03, avg batch time: 0.1952, average loss: 2.0815
[09/17 02:59:05 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.96	top5: 97.24	
[09/17 02:59:05 visual_prompt]: Best epoch 76: best metric: 0.790
[09/17 02:59:05 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/17 02:59:15 visual_prompt]: Epoch 77 / 100: avg data time: 1.44e-01, avg batch time: 0.5452, average train loss: 0.4428
[09/17 02:59:19 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.1432, average loss: 0.3989
[09/17 02:59:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 83.50	top5: 100.00	
[09/17 02:59:41 visual_prompt]: 	Test 100/356. loss: 2.350, 0.1986 s / batch. (data: 1.16e-04)max mem: 17.22445 GB 
[09/17 03:00:00 visual_prompt]: 	Test 200/356. loss: 2.364, 0.1841 s / batch. (data: 1.29e-04)max mem: 17.22445 GB 
[09/17 03:00:20 visual_prompt]: 	Test 300/356. loss: 2.248, 0.1983 s / batch. (data: 1.52e-02)max mem: 17.22445 GB 
[09/17 03:00:32 visual_prompt]: Inference (test):avg data time: 7.35e-03, avg batch time: 0.1931, average loss: 2.3639
[09/17 03:00:32 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.66	top5: 97.30	
[09/17 03:00:32 visual_prompt]: Best epoch 77: best metric: 0.835
[09/17 03:00:32 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/17 03:00:42 visual_prompt]: Epoch 78 / 100: avg data time: 1.44e-01, avg batch time: 0.5480, average train loss: 0.3890
[09/17 03:00:46 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1432, average loss: 0.3861
[09/17 03:00:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 80.00	top5: 100.00	
[09/17 03:01:08 visual_prompt]: 	Test 100/356. loss: 2.649, 0.1858 s / batch. (data: 1.18e-04)max mem: 17.22445 GB 
[09/17 03:01:28 visual_prompt]: 	Test 200/356. loss: 2.935, 0.1964 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 03:01:47 visual_prompt]: 	Test 300/356. loss: 2.531, 0.1965 s / batch. (data: 5.20e-03)max mem: 17.22445 GB 
[09/17 03:02:00 visual_prompt]: Inference (test):avg data time: 7.50e-03, avg batch time: 0.1935, average loss: 2.6023
[09/17 03:02:00 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.89	top5: 96.82	
[09/17 03:02:00 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/17 03:02:10 visual_prompt]: Epoch 79 / 100: avg data time: 1.42e-01, avg batch time: 0.5477, average train loss: 0.4111
[09/17 03:02:14 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1431, average loss: 0.4300
[09/17 03:02:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 79.00	top5: 100.00	
[09/17 03:02:36 visual_prompt]: 	Test 100/356. loss: 2.774, 0.1978 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/17 03:02:56 visual_prompt]: 	Test 200/356. loss: 2.945, 0.2073 s / batch. (data: 2.46e-02)max mem: 17.22445 GB 
[09/17 03:03:15 visual_prompt]: 	Test 300/356. loss: 2.166, 0.1842 s / batch. (data: 1.15e-04)max mem: 17.22445 GB 
[09/17 03:03:27 visual_prompt]: Inference (test):avg data time: 7.77e-03, avg batch time: 0.1947, average loss: 2.6011
[09/17 03:03:28 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.33	top5: 96.46	
[09/17 03:03:28 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/17 03:03:38 visual_prompt]: Epoch 80 / 100: avg data time: 1.39e-01, avg batch time: 0.5427, average train loss: 0.3526
[09/17 03:03:42 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1431, average loss: 0.3864
[09/17 03:03:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 81.50	top5: 100.00	
[09/17 03:04:04 visual_prompt]: 	Test 100/356. loss: 2.927, 0.2107 s / batch. (data: 1.78e-02)max mem: 17.22445 GB 
[09/17 03:04:23 visual_prompt]: 	Test 200/356. loss: 2.687, 0.1843 s / batch. (data: 1.64e-04)max mem: 17.22445 GB 
[09/17 03:04:43 visual_prompt]: 	Test 300/356. loss: 2.262, 0.1849 s / batch. (data: 1.40e-04)max mem: 17.22445 GB 
[09/17 03:04:55 visual_prompt]: Inference (test):avg data time: 7.98e-03, avg batch time: 0.1940, average loss: 2.7194
[09/17 03:04:56 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.89	top5: 97.21	
[09/17 03:04:56 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/17 03:05:06 visual_prompt]: Epoch 81 / 100: avg data time: 1.42e-01, avg batch time: 0.5441, average train loss: 0.2935
[09/17 03:05:10 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1432, average loss: 0.3840
[09/17 03:05:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 81.50	top5: 100.00	
[09/17 03:05:31 visual_prompt]: 	Test 100/356. loss: 3.070, 0.1836 s / batch. (data: 1.65e-04)max mem: 17.22445 GB 
[09/17 03:05:51 visual_prompt]: 	Test 200/356. loss: 3.289, 0.1992 s / batch. (data: 1.58e-02)max mem: 17.22445 GB 
[09/17 03:06:11 visual_prompt]: 	Test 300/356. loss: 2.330, 0.1846 s / batch. (data: 1.53e-04)max mem: 17.22445 GB 
[09/17 03:06:23 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1946, average loss: 3.0516
[09/17 03:06:23 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.70	top5: 97.01	
[09/17 03:06:23 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/17 03:06:34 visual_prompt]: Epoch 82 / 100: avg data time: 1.48e-01, avg batch time: 0.5511, average train loss: 0.3001
[09/17 03:06:38 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1453, average loss: 0.2582
[09/17 03:06:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 85.50	top5: 100.00	
[09/17 03:06:59 visual_prompt]: 	Test 100/356. loss: 3.429, 0.1996 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 03:07:19 visual_prompt]: 	Test 200/356. loss: 2.986, 0.1839 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/17 03:07:38 visual_prompt]: 	Test 300/356. loss: 2.435, 0.1959 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 03:07:51 visual_prompt]: Inference (test):avg data time: 8.15e-03, avg batch time: 0.1942, average loss: 3.1833
[09/17 03:07:51 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.25	top5: 96.89	
[09/17 03:07:51 visual_prompt]: Best epoch 82: best metric: 0.855
[09/17 03:07:51 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/17 03:08:01 visual_prompt]: Epoch 83 / 100: avg data time: 1.42e-01, avg batch time: 0.5445, average train loss: 0.2364
[09/17 03:08:05 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1432, average loss: 0.2236
[09/17 03:08:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 92.50	top5: 100.00	
[09/17 03:08:27 visual_prompt]: 	Test 100/356. loss: 3.763, 0.1958 s / batch. (data: 1.31e-02)max mem: 17.22445 GB 
[09/17 03:08:46 visual_prompt]: 	Test 200/356. loss: 3.195, 0.1959 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 03:09:06 visual_prompt]: 	Test 300/356. loss: 2.792, 0.1957 s / batch. (data: 1.11e-02)max mem: 17.22445 GB 
[09/17 03:09:18 visual_prompt]: Inference (test):avg data time: 8.20e-03, avg batch time: 0.1937, average loss: 3.3823
[09/17 03:09:18 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.11	top5: 97.00	
[09/17 03:09:18 visual_prompt]: Best epoch 83: best metric: 0.925
[09/17 03:09:18 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/17 03:09:29 visual_prompt]: Epoch 84 / 100: avg data time: 1.33e-01, avg batch time: 0.5715, average train loss: 0.2524
[09/17 03:09:34 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1433, average loss: 0.3474
[09/17 03:09:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 83.00	top5: 100.00	
[09/17 03:09:55 visual_prompt]: 	Test 100/356. loss: 3.740, 0.1973 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 03:10:15 visual_prompt]: 	Test 200/356. loss: 3.974, 0.1963 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 03:10:34 visual_prompt]: 	Test 300/356. loss: 2.853, 0.2117 s / batch. (data: 2.57e-02)max mem: 17.22445 GB 
[09/17 03:10:47 visual_prompt]: Inference (test):avg data time: 8.06e-03, avg batch time: 0.1943, average loss: 3.5820
[09/17 03:10:47 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.15	top5: 96.36	
[09/17 03:10:47 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/17 03:10:57 visual_prompt]: Epoch 85 / 100: avg data time: 1.34e-01, avg batch time: 0.5381, average train loss: 0.2269
[09/17 03:11:01 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1433, average loss: 0.2382
[09/17 03:11:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 90.50	top5: 100.00	
[09/17 03:11:23 visual_prompt]: 	Test 100/356. loss: 4.007, 0.1959 s / batch. (data: 1.32e-02)max mem: 17.22445 GB 
[09/17 03:11:43 visual_prompt]: 	Test 200/356. loss: 3.700, 0.2054 s / batch. (data: 1.38e-04)max mem: 17.22445 GB 
[09/17 03:12:03 visual_prompt]: 	Test 300/356. loss: 3.041, 0.1983 s / batch. (data: 1.54e-02)max mem: 17.22445 GB 
[09/17 03:12:16 visual_prompt]: Inference (test):avg data time: 8.22e-03, avg batch time: 0.1959, average loss: 3.7284
[09/17 03:12:16 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.63	top5: 95.13	
[09/17 03:12:16 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/17 03:12:26 visual_prompt]: Epoch 86 / 100: avg data time: 1.49e-01, avg batch time: 0.5508, average train loss: 0.1948
[09/17 03:12:30 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1432, average loss: 0.2344
[09/17 03:12:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 89.00	top5: 100.00	
[09/17 03:12:52 visual_prompt]: 	Test 100/356. loss: 4.194, 0.1840 s / batch. (data: 1.31e-04)max mem: 17.22445 GB 
[09/17 03:13:11 visual_prompt]: 	Test 200/356. loss: 3.576, 0.1991 s / batch. (data: 1.56e-02)max mem: 17.22445 GB 
[09/17 03:13:31 visual_prompt]: 	Test 300/356. loss: 2.938, 0.2108 s / batch. (data: 2.73e-02)max mem: 17.22445 GB 
[09/17 03:13:44 visual_prompt]: Inference (test):avg data time: 8.13e-03, avg batch time: 0.1947, average loss: 3.7933
[09/17 03:13:44 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.82	top5: 96.74	
[09/17 03:13:44 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/17 03:13:54 visual_prompt]: Epoch 87 / 100: avg data time: 1.49e-01, avg batch time: 0.5520, average train loss: 0.1713
[09/17 03:13:58 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1429, average loss: 0.2439
[09/17 03:13:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 86.50	top5: 100.00	
[09/17 03:14:20 visual_prompt]: 	Test 100/356. loss: 4.143, 0.1978 s / batch. (data: 4.02e-04)max mem: 17.22445 GB 
[09/17 03:14:40 visual_prompt]: 	Test 200/356. loss: 4.366, 0.1995 s / batch. (data: 1.60e-02)max mem: 17.22445 GB 
[09/17 03:14:59 visual_prompt]: 	Test 300/356. loss: 3.144, 0.1850 s / batch. (data: 1.56e-04)max mem: 17.22445 GB 
[09/17 03:15:12 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1944, average loss: 4.0233
[09/17 03:15:12 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.44	top5: 95.86	
[09/17 03:15:12 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/17 03:15:22 visual_prompt]: Epoch 88 / 100: avg data time: 1.33e-01, avg batch time: 0.5377, average train loss: 0.1815
[09/17 03:15:26 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.1454, average loss: 0.1357
[09/17 03:15:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 96.50	top5: 100.00	
[09/17 03:15:48 visual_prompt]: 	Test 100/356. loss: 4.050, 0.1997 s / batch. (data: 3.91e-05)max mem: 17.22445 GB 
[09/17 03:16:07 visual_prompt]: 	Test 200/356. loss: 4.233, 0.1840 s / batch. (data: 1.42e-04)max mem: 17.22445 GB 
[09/17 03:16:27 visual_prompt]: 	Test 300/356. loss: 3.219, 0.1844 s / batch. (data: 1.17e-04)max mem: 17.22445 GB 
[09/17 03:16:39 visual_prompt]: Inference (test):avg data time: 7.21e-03, avg batch time: 0.1927, average loss: 4.0237
[09/17 03:16:39 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.12	top5: 96.60	
[09/17 03:16:39 visual_prompt]: Best epoch 88: best metric: 0.965
[09/17 03:16:39 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/17 03:16:50 visual_prompt]: Epoch 89 / 100: avg data time: 1.46e-01, avg batch time: 0.5500, average train loss: 0.1346
[09/17 03:16:54 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1447, average loss: 0.1233
[09/17 03:16:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 95.50	top5: 100.00	
[09/17 03:17:15 visual_prompt]: 	Test 100/356. loss: 4.159, 0.1979 s / batch. (data: 1.52e-02)max mem: 17.22445 GB 
[09/17 03:17:35 visual_prompt]: 	Test 200/356. loss: 4.271, 0.1923 s / batch. (data: 8.57e-03)max mem: 17.22445 GB 
[09/17 03:17:54 visual_prompt]: 	Test 300/356. loss: 3.550, 0.1960 s / batch. (data: 1.24e-02)max mem: 17.22445 GB 
[09/17 03:18:07 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1930, average loss: 4.2494
[09/17 03:18:07 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.03	top5: 96.71	
[09/17 03:18:07 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/17 03:18:17 visual_prompt]: Epoch 90 / 100: avg data time: 1.38e-01, avg batch time: 0.5439, average train loss: 0.1214
[09/17 03:18:21 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1434, average loss: 0.1401
[09/17 03:18:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 94.00	top5: 100.00	
[09/17 03:18:43 visual_prompt]: 	Test 100/356. loss: 4.409, 0.1966 s / batch. (data: 1.64e-04)max mem: 17.22445 GB 
[09/17 03:19:02 visual_prompt]: 	Test 200/356. loss: 4.768, 0.1840 s / batch. (data: 1.25e-04)max mem: 17.22445 GB 
[09/17 03:19:22 visual_prompt]: 	Test 300/356. loss: 3.680, 0.1843 s / batch. (data: 1.36e-04)max mem: 17.22445 GB 
[09/17 03:19:34 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1941, average loss: 4.5430
[09/17 03:19:34 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.64	top5: 96.69	
[09/17 03:19:34 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/17 03:19:45 visual_prompt]: Epoch 91 / 100: avg data time: 1.35e-01, avg batch time: 0.5450, average train loss: 0.1013
[09/17 03:19:49 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1432, average loss: 0.1570
[09/17 03:19:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 90.50	top5: 100.00	
[09/17 03:20:11 visual_prompt]: 	Test 100/356. loss: 4.549, 0.1981 s / batch. (data: 1.48e-02)max mem: 17.22445 GB 
[09/17 03:20:30 visual_prompt]: 	Test 200/356. loss: 4.792, 0.1848 s / batch. (data: 1.01e-04)max mem: 17.22445 GB 
[09/17 03:20:50 visual_prompt]: 	Test 300/356. loss: 3.999, 0.1996 s / batch. (data: 1.50e-04)max mem: 17.22445 GB 
[09/17 03:21:02 visual_prompt]: Inference (test):avg data time: 7.94e-03, avg batch time: 0.1937, average loss: 4.7236
[09/17 03:21:02 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.70	top5: 96.91	
[09/17 03:21:02 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/17 03:21:13 visual_prompt]: Epoch 92 / 100: avg data time: 1.43e-01, avg batch time: 0.5472, average train loss: 0.0982
[09/17 03:21:17 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1431, average loss: 0.0915
[09/17 03:21:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 96.00	top5: 100.00	
[09/17 03:21:38 visual_prompt]: 	Test 100/356. loss: 4.572, 0.2021 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 03:21:58 visual_prompt]: 	Test 200/356. loss: 4.959, 0.1839 s / batch. (data: 1.37e-04)max mem: 17.22445 GB 
[09/17 03:22:17 visual_prompt]: 	Test 300/356. loss: 4.060, 0.1908 s / batch. (data: 7.08e-03)max mem: 17.22445 GB 
[09/17 03:22:30 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1940, average loss: 4.8322
[09/17 03:22:30 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.56	top5: 96.66	
[09/17 03:22:30 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/17 03:22:40 visual_prompt]: Epoch 93 / 100: avg data time: 1.44e-01, avg batch time: 0.5448, average train loss: 0.0848
[09/17 03:22:44 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1432, average loss: 0.1024
[09/17 03:22:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 96.00	top5: 100.00	
[09/17 03:23:06 visual_prompt]: 	Test 100/356. loss: 4.731, 0.1893 s / batch. (data: 1.42e-04)max mem: 17.22445 GB 
[09/17 03:23:26 visual_prompt]: 	Test 200/356. loss: 5.047, 0.1846 s / batch. (data: 1.37e-04)max mem: 17.22445 GB 
[09/17 03:23:45 visual_prompt]: 	Test 300/356. loss: 4.259, 0.1850 s / batch. (data: 1.58e-04)max mem: 17.22445 GB 
[09/17 03:23:58 visual_prompt]: Inference (test):avg data time: 7.67e-03, avg batch time: 0.1946, average loss: 4.9051
[09/17 03:23:58 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.54	top5: 96.67	
[09/17 03:23:58 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/17 03:24:08 visual_prompt]: Epoch 94 / 100: avg data time: 1.45e-01, avg batch time: 0.5554, average train loss: 0.0853
[09/17 03:24:12 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1429, average loss: 0.1100
[09/17 03:24:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 94.00	top5: 100.00	
[09/17 03:24:34 visual_prompt]: 	Test 100/356. loss: 4.773, 0.1953 s / batch. (data: 1.24e-02)max mem: 17.22445 GB 
[09/17 03:24:53 visual_prompt]: 	Test 200/356. loss: 5.147, 0.2376 s / batch. (data: 3.63e-02)max mem: 17.22445 GB 
[09/17 03:25:13 visual_prompt]: 	Test 300/356. loss: 4.174, 0.2157 s / batch. (data: 6.32e-03)max mem: 17.22445 GB 
[09/17 03:25:25 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1935, average loss: 4.9342
[09/17 03:25:25 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.75	top5: 96.70	
[09/17 03:25:25 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/17 03:25:35 visual_prompt]: Epoch 95 / 100: avg data time: 1.44e-01, avg batch time: 0.5452, average train loss: 0.0856
[09/17 03:25:40 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1431, average loss: 0.1182
[09/17 03:25:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 93.50	top5: 100.00	
[09/17 03:26:01 visual_prompt]: 	Test 100/356. loss: 4.783, 0.1860 s / batch. (data: 1.52e-04)max mem: 17.22445 GB 
[09/17 03:26:21 visual_prompt]: 	Test 200/356. loss: 5.111, 0.1838 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/17 03:26:41 visual_prompt]: 	Test 300/356. loss: 4.158, 0.1998 s / batch. (data: 1.38e-04)max mem: 17.22445 GB 
[09/17 03:26:53 visual_prompt]: Inference (test):avg data time: 7.57e-03, avg batch time: 0.1948, average loss: 4.9395
[09/17 03:26:53 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.83	top5: 96.75	
[09/17 03:26:53 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/17 03:27:04 visual_prompt]: Epoch 96 / 100: avg data time: 1.52e-01, avg batch time: 0.5563, average train loss: 0.0760
[09/17 03:27:08 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1433, average loss: 0.0894
[09/17 03:27:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 97.00	top5: 100.00	
[09/17 03:27:30 visual_prompt]: 	Test 100/356. loss: 4.818, 0.1832 s / batch. (data: 1.05e-04)max mem: 17.22445 GB 
[09/17 03:27:49 visual_prompt]: 	Test 200/356. loss: 5.176, 0.1840 s / batch. (data: 1.64e-04)max mem: 17.22445 GB 
[09/17 03:28:09 visual_prompt]: 	Test 300/356. loss: 4.267, 0.1836 s / batch. (data: 1.67e-04)max mem: 17.22445 GB 
[09/17 03:28:21 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1934, average loss: 4.9763
[09/17 03:28:21 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.54	top5: 96.80	
[09/17 03:28:21 visual_prompt]: Best epoch 96: best metric: 0.970
[09/17 03:28:21 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/17 03:28:31 visual_prompt]: Epoch 97 / 100: avg data time: 1.35e-01, avg batch time: 0.5388, average train loss: 0.0726
[09/17 03:28:35 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1432, average loss: 0.0882
[09/17 03:28:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 96.00	top5: 100.00	
[09/17 03:28:57 visual_prompt]: 	Test 100/356. loss: 4.945, 0.2148 s / batch. (data: 1.63e-04)max mem: 17.22445 GB 
[09/17 03:29:17 visual_prompt]: 	Test 200/356. loss: 5.329, 0.1916 s / batch. (data: 4.17e-05)max mem: 17.22445 GB 
[09/17 03:29:36 visual_prompt]: 	Test 300/356. loss: 4.315, 0.1936 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/17 03:29:49 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1940, average loss: 5.0864
[09/17 03:29:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.80	top5: 96.86	
[09/17 03:29:49 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/17 03:29:59 visual_prompt]: Epoch 98 / 100: avg data time: 1.43e-01, avg batch time: 0.5463, average train loss: 0.0643
[09/17 03:30:03 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1433, average loss: 0.0862
[09/17 03:30:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 95.50	top5: 100.00	
[09/17 03:30:25 visual_prompt]: 	Test 100/356. loss: 4.933, 0.1934 s / batch. (data: 1.37e-04)max mem: 17.22445 GB 
[09/17 03:30:45 visual_prompt]: 	Test 200/356. loss: 5.329, 0.1847 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 03:31:04 visual_prompt]: 	Test 300/356. loss: 4.311, 0.2043 s / batch. (data: 2.03e-02)max mem: 17.22445 GB 
[09/17 03:31:17 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1949, average loss: 5.0845
[09/17 03:31:17 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.80	top5: 96.86	
[09/17 03:31:17 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/17 03:31:27 visual_prompt]: Epoch 99 / 100: avg data time: 1.34e-01, avg batch time: 0.5384, average train loss: 0.0654
[09/17 03:31:31 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.1432, average loss: 0.0838
[09/17 03:31:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 96.00	top5: 100.00	
[09/17 03:31:53 visual_prompt]: 	Test 100/356. loss: 4.922, 0.1993 s / batch. (data: 1.65e-02)max mem: 17.22445 GB 
[09/17 03:32:13 visual_prompt]: 	Test 200/356. loss: 5.307, 0.1957 s / batch. (data: 1.86e-04)max mem: 17.22445 GB 
[09/17 03:32:32 visual_prompt]: 	Test 300/356. loss: 4.306, 0.2194 s / batch. (data: 3.58e-02)max mem: 17.22445 GB 
[09/17 03:32:44 visual_prompt]: Inference (test):avg data time: 7.28e-03, avg batch time: 0.1934, average loss: 5.0752
[09/17 03:32:45 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.71	top5: 96.85	
[09/17 03:32:45 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/17 03:32:55 visual_prompt]: Epoch 100 / 100: avg data time: 1.39e-01, avg batch time: 0.5402, average train loss: 0.0614
[09/17 03:32:59 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1433, average loss: 0.0832
[09/17 03:32:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 96.00	top5: 100.00	
[09/17 03:33:21 visual_prompt]: 	Test 100/356. loss: 4.917, 0.1846 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 03:33:40 visual_prompt]: 	Test 200/356. loss: 5.301, 0.1938 s / batch. (data: 1.30e-04)max mem: 17.22445 GB 
[09/17 03:34:00 visual_prompt]: 	Test 300/356. loss: 4.304, 0.1886 s / batch. (data: 1.12e-04)max mem: 17.22445 GB 
[09/17 03:34:12 visual_prompt]: Inference (test):avg data time: 7.35e-03, avg batch time: 0.1939, average loss: 5.0716
[09/17 03:34:12 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.68	top5: 96.86	
[09/17 03:34:27 visual_prompt]: Rank of current process: 0. World size: 1
[09/17 03:34:27 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/17 03:34:27 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed100'], train_type='')
[09/17 03:34:27 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/17 03:34:27 visual_prompt]: Training with config:
[09/17 03:34:27 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-dmlab',
          'NO_TEST': False,
          'NUMBER_CLASSES': 6,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed100/vtab-dmlab/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/17 03:34:27 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-17 03:34:27.481209: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-17 03:34:27.653336: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-17 03:34:28.564847: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 03:34:28.564933: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 03:34:28.564942: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-17 03:34:30.726171: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 03:34:30.726283: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 03:34:30.726297: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/17 03:34:30 visual_prompt]: Constructing vtab-dmlab dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
2023-09-17 03:34:30.756457: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dmlab for split train[:800]+validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/17 03:34:33 visual_prompt]: Number of images: 1000
[09/17 03:34:33 visual_prompt]: Number of classes: 6 / 6
[09/17 03:34:33 visual_prompt]: Loading validation data...
[09/17 03:34:33 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/17 03:34:33 visual_prompt]: Number of images: 200
[09/17 03:34:33 visual_prompt]: Number of classes: 6 / 6
[09/17 03:34:33 visual_prompt]: Loading test data...
[09/17 03:34:33 visual_prompt]: Constructing vtab-dmlab dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dmlab for split test, from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/17 03:35:14 visual_prompt]: Number of images: 22735
[09/17 03:35:14 visual_prompt]: Number of classes: 6 / 6
[09/17 03:35:14 visual_prompt]: Constructing models...
[09/17 03:35:17 visual_prompt]: Total Parameters: 86724870	 Gradient Parameters: 926214
[09/17 03:35:17 visual_prompt]: tuned percent:1.068
[09/17 03:35:20 visual_prompt]: Device used for model: 0
[09/17 03:35:20 visual_prompt]: Setting up Evalutator...
[09/17 03:35:20 visual_prompt]: Setting up Trainer...
[09/17 03:35:20 visual_prompt]: 	Setting up the optimizer...
[09/17 03:35:20 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/17 03:35:31 visual_prompt]: Epoch 1 / 100: avg data time: 1.51e-01, avg batch time: 0.6263, average train loss: 2.4251
[09/17 03:35:35 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.1422, average loss: 2.5898
[09/17 03:35:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/17 03:35:57 visual_prompt]: 	Test 100/356. loss: 2.280, 0.1823 s / batch. (data: 1.27e-04)max mem: 17.22445 GB 
[09/17 03:36:16 visual_prompt]: 	Test 200/356. loss: 2.279, 0.2103 s / batch. (data: 2.82e-02)max mem: 17.22445 GB 
[09/17 03:36:36 visual_prompt]: 	Test 300/356. loss: 2.263, 0.1941 s / batch. (data: 1.18e-02)max mem: 17.22445 GB 
[09/17 03:36:49 visual_prompt]: Inference (test):avg data time: 7.42e-03, avg batch time: 0.1928, average loss: 2.3923
[09/17 03:36:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 88.42	
[09/17 03:36:49 visual_prompt]: Best epoch 1: best metric: 0.145
[09/17 03:36:49 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/17 03:36:59 visual_prompt]: Epoch 2 / 100: avg data time: 1.48e-01, avg batch time: 0.5503, average train loss: 7.5165
[09/17 03:37:03 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.1430, average loss: 2.7284
[09/17 03:37:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/17 03:37:25 visual_prompt]: 	Test 100/356. loss: 3.017, 0.1939 s / batch. (data: 1.11e-02)max mem: 17.22445 GB 
[09/17 03:37:45 visual_prompt]: 	Test 200/356. loss: 2.523, 0.1836 s / batch. (data: 1.19e-04)max mem: 17.22445 GB 
[09/17 03:38:04 visual_prompt]: 	Test 300/356. loss: 2.951, 0.1840 s / batch. (data: 1.36e-04)max mem: 17.22445 GB 
[09/17 03:38:16 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1938, average loss: 2.7511
[09/17 03:38:17 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 84.67	
[09/17 03:38:17 visual_prompt]: Best epoch 2: best metric: 0.175
[09/17 03:38:17 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/17 03:38:27 visual_prompt]: Epoch 3 / 100: avg data time: 1.50e-01, avg batch time: 0.5522, average train loss: 3.2745
[09/17 03:38:31 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1433, average loss: 2.8469
[09/17 03:38:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/17 03:38:53 visual_prompt]: 	Test 100/356. loss: 2.496, 0.1863 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/17 03:39:13 visual_prompt]: 	Test 200/356. loss: 2.695, 0.1837 s / batch. (data: 1.33e-04)max mem: 17.22445 GB 
[09/17 03:39:32 visual_prompt]: 	Test 300/356. loss: 2.733, 0.1981 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/17 03:39:44 visual_prompt]: Inference (test):avg data time: 8.02e-03, avg batch time: 0.1933, average loss: 2.6954
[09/17 03:39:45 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 84.95	
[09/17 03:39:45 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/17 03:39:55 visual_prompt]: Epoch 4 / 100: avg data time: 1.37e-01, avg batch time: 0.5390, average train loss: 2.9262
[09/17 03:39:59 visual_prompt]: Inference (val):avg data time: 4.17e-04, avg batch time: 0.2525, average loss: 3.7111
[09/17 03:39:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 86.00	
[09/17 03:40:21 visual_prompt]: 	Test 100/356. loss: 4.021, 0.1831 s / batch. (data: 1.20e-04)max mem: 17.22445 GB 
[09/17 03:40:40 visual_prompt]: 	Test 200/356. loss: 2.939, 0.1877 s / batch. (data: 1.72e-04)max mem: 17.22445 GB 
[09/17 03:41:00 visual_prompt]: 	Test 300/356. loss: 3.427, 0.1841 s / batch. (data: 1.33e-04)max mem: 17.22445 GB 
[09/17 03:41:12 visual_prompt]: Inference (test):avg data time: 8.00e-03, avg batch time: 0.1935, average loss: 3.5537
[09/17 03:41:12 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 84.87	
[09/17 03:41:12 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/17 03:41:23 visual_prompt]: Epoch 5 / 100: avg data time: 1.30e-01, avg batch time: 0.5407, average train loss: 5.9800
[09/17 03:41:27 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1433, average loss: 4.3377
[09/17 03:41:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/17 03:41:49 visual_prompt]: 	Test 100/356. loss: 4.783, 0.1828 s / batch. (data: 1.72e-04)max mem: 17.22445 GB 
[09/17 03:42:08 visual_prompt]: 	Test 200/356. loss: 3.978, 0.1983 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/17 03:42:27 visual_prompt]: 	Test 300/356. loss: 4.307, 0.1963 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 03:42:40 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1937, average loss: 4.3327
[09/17 03:42:40 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 84.67	
[09/17 03:42:40 visual_prompt]: Best epoch 5: best metric: 0.205
[09/17 03:42:40 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/17 03:42:50 visual_prompt]: Epoch 6 / 100: avg data time: 1.47e-01, avg batch time: 0.5482, average train loss: 5.6158
[09/17 03:42:54 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1430, average loss: 6.6278
[09/17 03:42:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/17 03:43:16 visual_prompt]: 	Test 100/356. loss: 5.068, 0.1860 s / batch. (data: 1.17e-04)max mem: 17.22445 GB 
[09/17 03:43:36 visual_prompt]: 	Test 200/356. loss: 5.591, 0.1962 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/17 03:43:55 visual_prompt]: 	Test 300/356. loss: 6.008, 0.1990 s / batch. (data: 1.53e-02)max mem: 17.22445 GB 
[09/17 03:44:08 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1939, average loss: 5.7733
[09/17 03:44:08 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 88.42	
[09/17 03:44:08 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/17 03:44:18 visual_prompt]: Epoch 7 / 100: avg data time: 1.42e-01, avg batch time: 0.5473, average train loss: 7.4960
[09/17 03:44:22 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.1432, average loss: 5.7022
[09/17 03:44:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/17 03:44:44 visual_prompt]: 	Test 100/356. loss: 5.313, 0.1962 s / batch. (data: 1.34e-02)max mem: 17.22445 GB 
[09/17 03:45:04 visual_prompt]: 	Test 200/356. loss: 4.807, 0.2012 s / batch. (data: 1.30e-04)max mem: 17.22445 GB 
[09/17 03:45:23 visual_prompt]: 	Test 300/356. loss: 4.629, 0.2153 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/17 03:45:36 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1940, average loss: 5.2631
[09/17 03:45:36 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 85.40	
[09/17 03:45:36 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/17 03:45:46 visual_prompt]: Epoch 8 / 100: avg data time: 1.40e-01, avg batch time: 0.5583, average train loss: 9.8874
[09/17 03:45:50 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1431, average loss: 11.6442
[09/17 03:45:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/17 03:46:12 visual_prompt]: 	Test 100/356. loss: 11.010, 0.1834 s / batch. (data: 1.60e-04)max mem: 17.22445 GB 
[09/17 03:46:31 visual_prompt]: 	Test 200/356. loss: 9.589, 0.1916 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 03:46:51 visual_prompt]: 	Test 300/356. loss: 11.248, 0.1956 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 03:47:04 visual_prompt]: Inference (test):avg data time: 7.97e-03, avg batch time: 0.1940, average loss: 10.8182
[09/17 03:47:04 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 88.42	
[09/17 03:47:04 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/17 03:47:14 visual_prompt]: Epoch 9 / 100: avg data time: 1.44e-01, avg batch time: 0.5465, average train loss: 8.3240
[09/17 03:47:18 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.1431, average loss: 8.5354
[09/17 03:47:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/17 03:47:40 visual_prompt]: 	Test 100/356. loss: 8.097, 0.2080 s / batch. (data: 2.52e-02)max mem: 17.22445 GB 
[09/17 03:48:00 visual_prompt]: 	Test 200/356. loss: 8.568, 0.1840 s / batch. (data: 1.69e-04)max mem: 17.22445 GB 
[09/17 03:48:19 visual_prompt]: 	Test 300/356. loss: 7.910, 0.1841 s / batch. (data: 1.30e-04)max mem: 17.22445 GB 
[09/17 03:48:32 visual_prompt]: Inference (test):avg data time: 6.82e-03, avg batch time: 0.1934, average loss: 8.3497
[09/17 03:48:32 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 85.40	
[09/17 03:48:32 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/17 03:48:42 visual_prompt]: Epoch 10 / 100: avg data time: 1.41e-01, avg batch time: 0.5433, average train loss: 18.1281
[09/17 03:48:46 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1429, average loss: 19.2356
[09/17 03:48:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/17 03:49:08 visual_prompt]: 	Test 100/356. loss: 17.493, 0.1831 s / batch. (data: 1.22e-04)max mem: 17.22445 GB 
[09/17 03:49:28 visual_prompt]: 	Test 200/356. loss: 21.697, 0.1834 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/17 03:49:47 visual_prompt]: 	Test 300/356. loss: 20.483, 0.1907 s / batch. (data: 1.61e-04)max mem: 17.22445 GB 
[09/17 03:50:00 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1935, average loss: 19.3833
[09/17 03:50:00 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 88.42	
[09/17 03:50:00 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/17 03:50:10 visual_prompt]: Epoch 11 / 100: avg data time: 1.34e-01, avg batch time: 0.5390, average train loss: 16.6177
[09/17 03:50:14 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.1434, average loss: 19.6762
[09/17 03:50:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/17 03:50:36 visual_prompt]: 	Test 100/356. loss: 21.997, 0.2076 s / batch. (data: 2.50e-02)max mem: 17.22445 GB 
[09/17 03:50:55 visual_prompt]: 	Test 200/356. loss: 25.398, 0.2072 s / batch. (data: 1.47e-04)max mem: 17.22445 GB 
[09/17 03:51:15 visual_prompt]: 	Test 300/356. loss: 22.702, 0.1946 s / batch. (data: 1.12e-02)max mem: 17.22445 GB 
[09/17 03:51:27 visual_prompt]: Inference (test):avg data time: 7.18e-03, avg batch time: 0.1933, average loss: 22.1573
[09/17 03:51:27 visual_prompt]: Classification results with test_vtab-dmlab: top1: 11.58	top5: 77.81	
[09/17 03:51:27 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/17 03:51:38 visual_prompt]: Epoch 12 / 100: avg data time: 1.52e-01, avg batch time: 0.5561, average train loss: 13.0144
[09/17 03:51:42 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1432, average loss: 14.3990
[09/17 03:51:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/17 03:52:04 visual_prompt]: 	Test 100/356. loss: 13.854, 0.1837 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/17 03:52:23 visual_prompt]: 	Test 200/356. loss: 14.017, 0.2252 s / batch. (data: 4.09e-02)max mem: 17.22445 GB 
[09/17 03:52:43 visual_prompt]: 	Test 300/356. loss: 14.489, 0.1847 s / batch. (data: 1.19e-04)max mem: 17.22445 GB 
[09/17 03:52:56 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1952, average loss: 14.0770
[09/17 03:52:56 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 88.42	
[09/17 03:52:56 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/17 03:53:06 visual_prompt]: Epoch 13 / 100: avg data time: 1.47e-01, avg batch time: 0.5497, average train loss: 16.7496
[09/17 03:53:10 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.1434, average loss: 25.0569
[09/17 03:53:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/17 03:53:32 visual_prompt]: 	Test 100/356. loss: 22.995, 0.1833 s / batch. (data: 1.47e-04)max mem: 17.22445 GB 
[09/17 03:53:51 visual_prompt]: 	Test 200/356. loss: 20.621, 0.1984 s / batch. (data: 1.55e-02)max mem: 17.22445 GB 
[09/17 03:54:11 visual_prompt]: 	Test 300/356. loss: 20.258, 0.1980 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/17 03:54:23 visual_prompt]: Inference (test):avg data time: 7.14e-03, avg batch time: 0.1933, average loss: 23.0182
[09/17 03:54:23 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 85.40	
[09/17 03:54:23 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/17 03:54:34 visual_prompt]: Epoch 14 / 100: avg data time: 1.48e-01, avg batch time: 0.5493, average train loss: 15.9644
[09/17 03:54:38 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1451, average loss: 21.7645
[09/17 03:54:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/17 03:55:00 visual_prompt]: 	Test 100/356. loss: 21.987, 0.1916 s / batch. (data: 1.28e-04)max mem: 17.22445 GB 
[09/17 03:55:19 visual_prompt]: 	Test 200/356. loss: 22.460, 0.1844 s / batch. (data: 1.15e-04)max mem: 17.22445 GB 
[09/17 03:55:38 visual_prompt]: 	Test 300/356. loss: 20.490, 0.1889 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/17 03:55:51 visual_prompt]: Inference (test):avg data time: 6.97e-03, avg batch time: 0.1928, average loss: 21.8204
[09/17 03:55:51 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 82.27	
[09/17 03:55:51 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/17 03:56:02 visual_prompt]: Epoch 15 / 100: avg data time: 1.53e-01, avg batch time: 0.5553, average train loss: 21.3338
[09/17 03:56:06 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1431, average loss: 4.1857
[09/17 03:56:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/17 03:56:27 visual_prompt]: 	Test 100/356. loss: 4.706, 0.1974 s / batch. (data: 1.47e-02)max mem: 17.22445 GB 
[09/17 03:56:47 visual_prompt]: 	Test 200/356. loss: 4.424, 0.1858 s / batch. (data: 1.70e-04)max mem: 17.22445 GB 
[09/17 03:57:06 visual_prompt]: 	Test 300/356. loss: 4.583, 0.1840 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 03:57:19 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1935, average loss: 4.4010
[09/17 03:57:19 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 85.40	
[09/17 03:57:19 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/17 03:57:29 visual_prompt]: Epoch 16 / 100: avg data time: 1.50e-01, avg batch time: 0.5541, average train loss: 14.2380
[09/17 03:57:33 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1432, average loss: 9.7272
[09/17 03:57:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/17 03:57:55 visual_prompt]: 	Test 100/356. loss: 9.232, 0.1925 s / batch. (data: 9.83e-03)max mem: 17.22445 GB 
[09/17 03:58:15 visual_prompt]: 	Test 200/356. loss: 7.215, 0.1985 s / batch. (data: 1.52e-02)max mem: 17.22445 GB 
[09/17 03:58:34 visual_prompt]: 	Test 300/356. loss: 7.759, 0.2046 s / batch. (data: 1.58e-02)max mem: 17.22445 GB 
[09/17 03:58:47 visual_prompt]: Inference (test):avg data time: 7.15e-03, avg batch time: 0.1947, average loss: 8.7437
[09/17 03:58:47 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 85.40	
[09/17 03:58:47 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/17 03:58:58 visual_prompt]: Epoch 17 / 100: avg data time: 1.52e-01, avg batch time: 0.5772, average train loss: 11.1226
[09/17 03:59:02 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1431, average loss: 12.2156
[09/17 03:59:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/17 03:59:24 visual_prompt]: 	Test 100/356. loss: 10.697, 0.1957 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 03:59:44 visual_prompt]: 	Test 200/356. loss: 10.052, 0.1935 s / batch. (data: 1.09e-04)max mem: 17.22445 GB 
[09/17 04:00:04 visual_prompt]: 	Test 300/356. loss: 10.700, 0.1850 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/17 04:00:16 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1956, average loss: 11.1632
[09/17 04:00:16 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 81.44	
[09/17 04:00:16 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/17 04:00:26 visual_prompt]: Epoch 18 / 100: avg data time: 1.29e-01, avg batch time: 0.5356, average train loss: 10.1115
[09/17 04:00:30 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1432, average loss: 13.5035
[09/17 04:00:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/17 04:00:52 visual_prompt]: 	Test 100/356. loss: 13.850, 0.1837 s / batch. (data: 1.27e-04)max mem: 17.22445 GB 
[09/17 04:01:12 visual_prompt]: 	Test 200/356. loss: 12.710, 0.1839 s / batch. (data: 1.56e-04)max mem: 17.22445 GB 
[09/17 04:01:31 visual_prompt]: 	Test 300/356. loss: 12.329, 0.1841 s / batch. (data: 1.32e-04)max mem: 17.22445 GB 
[09/17 04:01:44 visual_prompt]: Inference (test):avg data time: 7.94e-03, avg batch time: 0.1948, average loss: 13.2489
[09/17 04:01:44 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 82.27	
[09/17 04:01:44 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/17 04:01:54 visual_prompt]: Epoch 19 / 100: avg data time: 1.40e-01, avg batch time: 0.5447, average train loss: 7.9204
[09/17 04:01:59 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.1432, average loss: 9.3268
[09/17 04:01:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/17 04:02:20 visual_prompt]: 	Test 100/356. loss: 9.325, 0.2179 s / batch. (data: 2.92e-02)max mem: 17.22445 GB 
[09/17 04:02:40 visual_prompt]: 	Test 200/356. loss: 7.230, 0.1968 s / batch. (data: 1.34e-02)max mem: 17.22445 GB 
[09/17 04:02:59 visual_prompt]: 	Test 300/356. loss: 7.854, 0.2126 s / batch. (data: 1.20e-02)max mem: 17.22445 GB 
[09/17 04:03:12 visual_prompt]: Inference (test):avg data time: 7.50e-03, avg batch time: 0.1940, average loss: 8.6695
[09/17 04:03:12 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 85.40	
[09/17 04:03:12 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/17 04:03:22 visual_prompt]: Epoch 20 / 100: avg data time: 1.46e-01, avg batch time: 0.5475, average train loss: 8.8231
[09/17 04:03:27 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.1433, average loss: 13.0049
[09/17 04:03:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/17 04:03:48 visual_prompt]: 	Test 100/356. loss: 15.417, 0.2037 s / batch. (data: 2.10e-02)max mem: 17.22445 GB 
[09/17 04:04:08 visual_prompt]: 	Test 200/356. loss: 13.109, 0.1839 s / batch. (data: 1.03e-04)max mem: 17.22445 GB 
[09/17 04:04:27 visual_prompt]: 	Test 300/356. loss: 14.256, 0.1984 s / batch. (data: 1.47e-02)max mem: 17.22445 GB 
[09/17 04:04:40 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1934, average loss: 13.8142
[09/17 04:04:40 visual_prompt]: Classification results with test_vtab-dmlab: top1: 11.58	top5: 84.67	
[09/17 04:04:40 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/17 04:04:51 visual_prompt]: Epoch 21 / 100: avg data time: 1.42e-01, avg batch time: 0.5451, average train loss: 13.2928
[09/17 04:04:55 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1431, average loss: 11.8169
[09/17 04:04:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/17 04:05:16 visual_prompt]: 	Test 100/356. loss: 9.617, 0.1834 s / batch. (data: 1.18e-04)max mem: 17.22445 GB 
[09/17 04:05:36 visual_prompt]: 	Test 200/356. loss: 8.901, 0.2082 s / batch. (data: 2.43e-02)max mem: 17.22445 GB 
[09/17 04:05:55 visual_prompt]: 	Test 300/356. loss: 9.613, 0.1844 s / batch. (data: 9.54e-05)max mem: 17.22445 GB 
[09/17 04:06:08 visual_prompt]: Inference (test):avg data time: 7.43e-03, avg batch time: 0.1936, average loss: 10.1662
[09/17 04:06:08 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 88.42	
[09/17 04:06:08 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/17 04:06:18 visual_prompt]: Epoch 22 / 100: avg data time: 1.43e-01, avg batch time: 0.5469, average train loss: 10.0391
[09/17 04:06:23 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1432, average loss: 5.8774
[09/17 04:06:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/17 04:06:45 visual_prompt]: 	Test 100/356. loss: 5.660, 0.1834 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/17 04:07:04 visual_prompt]: 	Test 200/356. loss: 4.694, 0.2229 s / batch. (data: 3.63e-02)max mem: 17.22445 GB 
[09/17 04:07:24 visual_prompt]: 	Test 300/356. loss: 6.046, 0.2291 s / batch. (data: 3.49e-02)max mem: 17.22445 GB 
[09/17 04:07:36 visual_prompt]: Inference (test):avg data time: 7.11e-03, avg batch time: 0.1949, average loss: 5.4684
[09/17 04:07:36 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 84.67	
[09/17 04:07:36 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/17 04:07:47 visual_prompt]: Epoch 23 / 100: avg data time: 1.44e-01, avg batch time: 0.5481, average train loss: 7.6622
[09/17 04:07:51 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1429, average loss: 9.1651
[09/17 04:07:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/17 04:08:13 visual_prompt]: 	Test 100/356. loss: 8.382, 0.1957 s / batch. (data: 1.32e-02)max mem: 17.22445 GB 
[09/17 04:08:32 visual_prompt]: 	Test 200/356. loss: 9.426, 0.1838 s / batch. (data: 1.14e-04)max mem: 17.22445 GB 
[09/17 04:08:52 visual_prompt]: 	Test 300/356. loss: 8.633, 0.1888 s / batch. (data: 1.62e-04)max mem: 17.22445 GB 
[09/17 04:09:04 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1937, average loss: 9.0641
[09/17 04:09:04 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 81.44	
[09/17 04:09:04 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/17 04:09:14 visual_prompt]: Epoch 24 / 100: avg data time: 1.30e-01, avg batch time: 0.5352, average train loss: 9.1664
[09/17 04:09:19 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1433, average loss: 7.7731
[09/17 04:09:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/17 04:09:41 visual_prompt]: 	Test 100/356. loss: 7.521, 0.2267 s / batch. (data: 7.27e-05)max mem: 17.22445 GB 
[09/17 04:10:00 visual_prompt]: 	Test 200/356. loss: 8.908, 0.1834 s / batch. (data: 1.26e-04)max mem: 17.22445 GB 
[09/17 04:10:20 visual_prompt]: 	Test 300/356. loss: 7.914, 0.1985 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/17 04:10:32 visual_prompt]: Inference (test):avg data time: 8.16e-03, avg batch time: 0.1940, average loss: 8.0273
[09/17 04:10:32 visual_prompt]: Classification results with test_vtab-dmlab: top1: 14.60	top5: 82.27	
[09/17 04:10:32 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/17 04:10:43 visual_prompt]: Epoch 25 / 100: avg data time: 1.42e-01, avg batch time: 0.5461, average train loss: 6.8262
[09/17 04:10:47 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1435, average loss: 6.1398
[09/17 04:10:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/17 04:11:09 visual_prompt]: 	Test 100/356. loss: 7.346, 0.2132 s / batch. (data: 1.61e-02)max mem: 17.22445 GB 
[09/17 04:11:29 visual_prompt]: 	Test 200/356. loss: 8.520, 0.1844 s / batch. (data: 1.19e-04)max mem: 17.22445 GB 
[09/17 04:11:48 visual_prompt]: 	Test 300/356. loss: 7.554, 0.1998 s / batch. (data: 1.59e-02)max mem: 17.22445 GB 
[09/17 04:12:01 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1950, average loss: 7.2358
[09/17 04:12:01 visual_prompt]: Classification results with test_vtab-dmlab: top1: 11.58	top5: 77.81	
[09/17 04:12:01 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/17 04:12:11 visual_prompt]: Epoch 26 / 100: avg data time: 1.37e-01, avg batch time: 0.5444, average train loss: 5.1994
[09/17 04:12:15 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1433, average loss: 5.4088
[09/17 04:12:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/17 04:12:37 visual_prompt]: 	Test 100/356. loss: 4.650, 0.1885 s / batch. (data: 1.33e-04)max mem: 17.22445 GB 
[09/17 04:12:57 visual_prompt]: 	Test 200/356. loss: 5.378, 0.1950 s / batch. (data: 1.17e-02)max mem: 17.22445 GB 
[09/17 04:13:16 visual_prompt]: 	Test 300/356. loss: 5.047, 0.1843 s / batch. (data: 1.30e-04)max mem: 17.22445 GB 
[09/17 04:13:29 visual_prompt]: Inference (test):avg data time: 7.27e-03, avg batch time: 0.1934, average loss: 5.1859
[09/17 04:13:29 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 81.44	
[09/17 04:13:29 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/17 04:13:39 visual_prompt]: Epoch 27 / 100: avg data time: 1.44e-01, avg batch time: 0.5471, average train loss: 3.3121
[09/17 04:13:43 visual_prompt]: Inference (val):avg data time: 4.99e-05, avg batch time: 0.1453, average loss: 2.5663
[09/17 04:13:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/17 04:14:05 visual_prompt]: 	Test 100/356. loss: 2.442, 0.1833 s / batch. (data: 1.05e-04)max mem: 17.22445 GB 
[09/17 04:14:25 visual_prompt]: 	Test 200/356. loss: 2.287, 0.2264 s / batch. (data: 4.33e-02)max mem: 17.22445 GB 
[09/17 04:14:44 visual_prompt]: 	Test 300/356. loss: 2.559, 0.1841 s / batch. (data: 9.39e-05)max mem: 17.22445 GB 
[09/17 04:14:57 visual_prompt]: Inference (test):avg data time: 7.89e-03, avg batch time: 0.1942, average loss: 2.4579
[09/17 04:14:57 visual_prompt]: Classification results with test_vtab-dmlab: top1: 14.60	top5: 84.67	
[09/17 04:14:57 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/17 04:15:07 visual_prompt]: Epoch 28 / 100: avg data time: 1.27e-01, avg batch time: 0.5327, average train loss: 3.5465
[09/17 04:15:11 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1432, average loss: 3.9831
[09/17 04:15:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/17 04:15:33 visual_prompt]: 	Test 100/356. loss: 3.556, 0.1835 s / batch. (data: 4.29e-05)max mem: 17.22445 GB 
[09/17 04:15:52 visual_prompt]: 	Test 200/356. loss: 4.145, 0.2088 s / batch. (data: 5.23e-03)max mem: 17.22445 GB 
[09/17 04:16:12 visual_prompt]: 	Test 300/356. loss: 4.016, 0.1848 s / batch. (data: 1.53e-04)max mem: 17.22445 GB 
[09/17 04:16:25 visual_prompt]: Inference (test):avg data time: 8.34e-03, avg batch time: 0.1944, average loss: 3.9325
[09/17 04:16:25 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 81.44	
[09/17 04:16:25 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/17 04:16:35 visual_prompt]: Epoch 29 / 100: avg data time: 1.44e-01, avg batch time: 0.5492, average train loss: 3.2994
[09/17 04:16:40 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.1433, average loss: 2.4022
[09/17 04:16:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/17 04:17:02 visual_prompt]: 	Test 100/356. loss: 2.627, 0.1921 s / batch. (data: 1.31e-04)max mem: 17.22445 GB 
[09/17 04:17:22 visual_prompt]: 	Test 200/356. loss: 2.585, 0.1954 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 04:17:41 visual_prompt]: 	Test 300/356. loss: 2.470, 0.1831 s / batch. (data: 4.67e-05)max mem: 17.22445 GB 
[09/17 04:17:54 visual_prompt]: Inference (test):avg data time: 7.13e-03, avg batch time: 0.1970, average loss: 2.5136
[09/17 04:17:54 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 82.27	
[09/17 04:17:54 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/17 04:18:04 visual_prompt]: Epoch 30 / 100: avg data time: 1.43e-01, avg batch time: 0.5471, average train loss: 3.4051
[09/17 04:18:09 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1430, average loss: 4.8585
[09/17 04:18:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/17 04:18:30 visual_prompt]: 	Test 100/356. loss: 4.541, 0.1830 s / batch. (data: 1.61e-04)max mem: 17.22445 GB 
[09/17 04:18:50 visual_prompt]: 	Test 200/356. loss: 4.455, 0.1960 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 04:19:09 visual_prompt]: 	Test 300/356. loss: 4.668, 0.1977 s / batch. (data: 1.41e-02)max mem: 17.22445 GB 
[09/17 04:19:22 visual_prompt]: Inference (test):avg data time: 7.75e-03, avg batch time: 0.1934, average loss: 4.5916
[09/17 04:19:22 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 88.42	
[09/17 04:19:22 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/17 04:19:32 visual_prompt]: Epoch 31 / 100: avg data time: 1.44e-01, avg batch time: 0.5465, average train loss: 4.1433
[09/17 04:19:37 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1430, average loss: 6.2100
[09/17 04:19:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/17 04:19:58 visual_prompt]: 	Test 100/356. loss: 6.560, 0.1954 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 04:20:18 visual_prompt]: 	Test 200/356. loss: 6.956, 0.1838 s / batch. (data: 1.63e-04)max mem: 17.22445 GB 
[09/17 04:20:37 visual_prompt]: 	Test 300/356. loss: 6.243, 0.1993 s / batch. (data: 1.53e-02)max mem: 17.22445 GB 
[09/17 04:20:50 visual_prompt]: Inference (test):avg data time: 7.26e-03, avg batch time: 0.1936, average loss: 6.4804
[09/17 04:20:50 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 85.40	
[09/17 04:20:50 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/17 04:21:00 visual_prompt]: Epoch 32 / 100: avg data time: 1.46e-01, avg batch time: 0.5513, average train loss: 4.8921
[09/17 04:21:05 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1434, average loss: 4.0268
[09/17 04:21:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/17 04:21:27 visual_prompt]: 	Test 100/356. loss: 3.944, 0.2073 s / batch. (data: 2.47e-02)max mem: 17.22445 GB 
[09/17 04:21:47 visual_prompt]: 	Test 200/356. loss: 3.639, 0.1960 s / batch. (data: 1.24e-02)max mem: 17.22445 GB 
[09/17 04:22:06 visual_prompt]: 	Test 300/356. loss: 3.783, 0.1845 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/17 04:22:19 visual_prompt]: Inference (test):avg data time: 7.42e-03, avg batch time: 0.1961, average loss: 3.8457
[09/17 04:22:19 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 88.42	
[09/17 04:22:19 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/17 04:22:29 visual_prompt]: Epoch 33 / 100: avg data time: 1.36e-01, avg batch time: 0.5411, average train loss: 4.0920
[09/17 04:22:33 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1431, average loss: 3.6842
[09/17 04:22:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/17 04:22:55 visual_prompt]: 	Test 100/356. loss: 3.791, 0.2103 s / batch. (data: 1.39e-02)max mem: 17.22445 GB 
[09/17 04:23:15 visual_prompt]: 	Test 200/356. loss: 4.079, 0.1959 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 04:23:34 visual_prompt]: 	Test 300/356. loss: 3.550, 0.1851 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/17 04:23:47 visual_prompt]: Inference (test):avg data time: 7.89e-03, avg batch time: 0.1935, average loss: 3.8175
[09/17 04:23:47 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 82.27	
[09/17 04:23:47 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/17 04:23:57 visual_prompt]: Epoch 34 / 100: avg data time: 1.51e-01, avg batch time: 0.5515, average train loss: 3.6423
[09/17 04:24:01 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1432, average loss: 3.4800
[09/17 04:24:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/17 04:24:23 visual_prompt]: 	Test 100/356. loss: 3.950, 0.1837 s / batch. (data: 1.58e-04)max mem: 17.22445 GB 
[09/17 04:24:43 visual_prompt]: 	Test 200/356. loss: 3.727, 0.1957 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/17 04:25:02 visual_prompt]: 	Test 300/356. loss: 3.914, 0.1986 s / batch. (data: 1.53e-02)max mem: 17.22445 GB 
[09/17 04:25:15 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1940, average loss: 3.7058
[09/17 04:25:15 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 84.67	
[09/17 04:25:15 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/17 04:25:25 visual_prompt]: Epoch 35 / 100: avg data time: 1.47e-01, avg batch time: 0.5509, average train loss: 3.6855
[09/17 04:25:30 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1432, average loss: 4.0236
[09/17 04:25:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/17 04:25:51 visual_prompt]: 	Test 100/356. loss: 3.479, 0.1958 s / batch. (data: 1.24e-02)max mem: 17.22445 GB 
[09/17 04:26:11 visual_prompt]: 	Test 200/356. loss: 3.701, 0.2115 s / batch. (data: 2.80e-02)max mem: 17.22445 GB 
[09/17 04:26:31 visual_prompt]: 	Test 300/356. loss: 3.569, 0.2039 s / batch. (data: 1.59e-02)max mem: 17.22445 GB 
[09/17 04:26:43 visual_prompt]: Inference (test):avg data time: 7.80e-03, avg batch time: 0.1943, average loss: 3.7850
[09/17 04:26:43 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.34	top5: 81.44	
[09/17 04:26:43 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/17 04:26:54 visual_prompt]: Epoch 36 / 100: avg data time: 1.33e-01, avg batch time: 0.5385, average train loss: 6.2011
[09/17 04:26:58 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1430, average loss: 6.5758
[09/17 04:26:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/17 04:27:19 visual_prompt]: 	Test 100/356. loss: 5.849, 0.1950 s / batch. (data: 1.13e-04)max mem: 17.22445 GB 
[09/17 04:27:39 visual_prompt]: 	Test 200/356. loss: 6.171, 0.1850 s / batch. (data: 1.58e-04)max mem: 17.22445 GB 
[09/17 04:27:58 visual_prompt]: 	Test 300/356. loss: 6.688, 0.1996 s / batch. (data: 1.61e-02)max mem: 17.22445 GB 
[09/17 04:28:11 visual_prompt]: Inference (test):avg data time: 8.04e-03, avg batch time: 0.1943, average loss: 6.3107
[09/17 04:28:11 visual_prompt]: Classification results with test_vtab-dmlab: top1: 14.60	top5: 81.44	
[09/17 04:28:11 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/17 04:28:22 visual_prompt]: Epoch 37 / 100: avg data time: 1.39e-01, avg batch time: 0.5471, average train loss: 7.8943
[09/17 04:28:26 visual_prompt]: Inference (val):avg data time: 5.95e-05, avg batch time: 0.2814, average loss: 12.9139
[09/17 04:28:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/17 04:28:49 visual_prompt]: 	Test 100/356. loss: 12.025, 0.1859 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/17 04:29:08 visual_prompt]: 	Test 200/356. loss: 14.406, 0.2069 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 04:29:28 visual_prompt]: 	Test 300/356. loss: 12.900, 0.1851 s / batch. (data: 1.46e-04)max mem: 17.22445 GB 
[09/17 04:29:40 visual_prompt]: Inference (test):avg data time: 6.15e-03, avg batch time: 0.1943, average loss: 13.1033
[09/17 04:29:40 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 77.81	
[09/17 04:29:40 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/17 04:29:51 visual_prompt]: Epoch 38 / 100: avg data time: 1.53e-01, avg batch time: 0.5547, average train loss: 8.2382
[09/17 04:29:55 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.1446, average loss: 6.0312
[09/17 04:29:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/17 04:30:17 visual_prompt]: 	Test 100/356. loss: 6.641, 0.1831 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/17 04:30:36 visual_prompt]: 	Test 200/356. loss: 6.259, 0.1925 s / batch. (data: 9.43e-03)max mem: 17.22445 GB 
[09/17 04:30:56 visual_prompt]: 	Test 300/356. loss: 6.913, 0.1965 s / batch. (data: 1.35e-02)max mem: 17.22445 GB 
[09/17 04:31:09 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1944, average loss: 6.2774
[09/17 04:31:09 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 84.67	
[09/17 04:31:09 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/17 04:31:19 visual_prompt]: Epoch 39 / 100: avg data time: 1.51e-01, avg batch time: 0.5540, average train loss: 11.1393
[09/17 04:31:24 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1431, average loss: 14.8887
[09/17 04:31:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/17 04:31:45 visual_prompt]: 	Test 100/356. loss: 15.045, 0.1836 s / batch. (data: 1.42e-04)max mem: 17.22445 GB 
[09/17 04:32:05 visual_prompt]: 	Test 200/356. loss: 13.621, 0.2064 s / batch. (data: 2.28e-02)max mem: 17.22445 GB 
[09/17 04:32:24 visual_prompt]: 	Test 300/356. loss: 12.673, 0.1946 s / batch. (data: 1.11e-02)max mem: 17.22445 GB 
[09/17 04:32:37 visual_prompt]: Inference (test):avg data time: 7.22e-03, avg batch time: 0.1933, average loss: 14.4365
[09/17 04:32:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 85.40	
[09/17 04:32:37 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/17 04:32:47 visual_prompt]: Epoch 40 / 100: avg data time: 1.47e-01, avg batch time: 0.5507, average train loss: 9.4831
[09/17 04:32:51 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1431, average loss: 6.9601
[09/17 04:32:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/17 04:33:13 visual_prompt]: 	Test 100/356. loss: 6.873, 0.1939 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/17 04:33:32 visual_prompt]: 	Test 200/356. loss: 7.437, 0.1895 s / batch. (data: 1.30e-04)max mem: 17.22445 GB 
[09/17 04:33:52 visual_prompt]: 	Test 300/356. loss: 7.047, 0.1841 s / batch. (data: 1.62e-04)max mem: 17.22445 GB 
[09/17 04:34:04 visual_prompt]: Inference (test):avg data time: 7.07e-03, avg batch time: 0.1927, average loss: 7.0056
[09/17 04:34:04 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 88.42	
[09/17 04:34:04 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/17 04:34:15 visual_prompt]: Epoch 41 / 100: avg data time: 1.33e-01, avg batch time: 0.5409, average train loss: 7.4547
[09/17 04:34:19 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1436, average loss: 6.3627
[09/17 04:34:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/17 04:34:41 visual_prompt]: 	Test 100/356. loss: 6.628, 0.1961 s / batch. (data: 1.32e-02)max mem: 17.22445 GB 
[09/17 04:35:01 visual_prompt]: 	Test 200/356. loss: 4.959, 0.2080 s / batch. (data: 2.45e-02)max mem: 17.22445 GB 
[09/17 04:35:21 visual_prompt]: 	Test 300/356. loss: 6.396, 0.2007 s / batch. (data: 1.45e-04)max mem: 17.22445 GB 
[09/17 04:35:33 visual_prompt]: Inference (test):avg data time: 7.60e-03, avg batch time: 0.1958, average loss: 5.9727
[09/17 04:35:33 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 84.67	
[09/17 04:35:33 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/17 04:35:44 visual_prompt]: Epoch 42 / 100: avg data time: 1.43e-01, avg batch time: 0.5484, average train loss: 5.1279
[09/17 04:35:48 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1431, average loss: 3.3931
[09/17 04:35:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/17 04:36:11 visual_prompt]: 	Test 100/356. loss: 3.531, 0.1837 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/17 04:36:30 visual_prompt]: 	Test 200/356. loss: 3.414, 0.1846 s / batch. (data: 1.05e-04)max mem: 17.22445 GB 
[09/17 04:36:50 visual_prompt]: 	Test 300/356. loss: 3.391, 0.1844 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 04:37:02 visual_prompt]: Inference (test):avg data time: 7.30e-03, avg batch time: 0.1959, average loss: 3.4621
[09/17 04:37:02 visual_prompt]: Classification results with test_vtab-dmlab: top1: 11.58	top5: 82.27	
[09/17 04:37:02 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/17 04:37:13 visual_prompt]: Epoch 43 / 100: avg data time: 1.30e-01, avg batch time: 0.5400, average train loss: 3.7716
[09/17 04:37:17 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1431, average loss: 3.2500
[09/17 04:37:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/17 04:37:39 visual_prompt]: 	Test 100/356. loss: 3.619, 0.1959 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 04:37:58 visual_prompt]: 	Test 200/356. loss: 3.029, 0.1955 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/17 04:38:18 visual_prompt]: 	Test 300/356. loss: 3.098, 0.1841 s / batch. (data: 1.32e-04)max mem: 17.22445 GB 
[09/17 04:38:30 visual_prompt]: Inference (test):avg data time: 7.49e-03, avg batch time: 0.1939, average loss: 3.2769
[09/17 04:38:30 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 85.40	
[09/17 04:38:30 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/17 04:38:41 visual_prompt]: Epoch 44 / 100: avg data time: 1.45e-01, avg batch time: 0.5473, average train loss: 3.4756
[09/17 04:38:45 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1432, average loss: 6.8686
[09/17 04:38:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/17 04:39:07 visual_prompt]: 	Test 100/356. loss: 6.430, 0.1947 s / batch. (data: 1.21e-04)max mem: 17.22445 GB 
[09/17 04:39:26 visual_prompt]: 	Test 200/356. loss: 5.686, 0.1836 s / batch. (data: 4.20e-05)max mem: 17.22445 GB 
[09/17 04:39:46 visual_prompt]: 	Test 300/356. loss: 5.957, 0.1955 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 04:39:59 visual_prompt]: Inference (test):avg data time: 7.75e-03, avg batch time: 0.1949, average loss: 6.2842
[09/17 04:39:59 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 88.42	
[09/17 04:39:59 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/17 04:40:09 visual_prompt]: Epoch 45 / 100: avg data time: 1.55e-01, avg batch time: 0.5577, average train loss: 4.3626
[09/17 04:40:14 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1432, average loss: 3.3535
[09/17 04:40:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/17 04:40:35 visual_prompt]: 	Test 100/356. loss: 3.165, 0.1835 s / batch. (data: 1.37e-04)max mem: 17.22445 GB 
[09/17 04:40:55 visual_prompt]: 	Test 200/356. loss: 3.978, 0.1918 s / batch. (data: 1.40e-04)max mem: 17.22445 GB 
[09/17 04:41:14 visual_prompt]: 	Test 300/356. loss: 3.561, 0.1840 s / batch. (data: 1.56e-04)max mem: 17.22445 GB 
[09/17 04:41:27 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1934, average loss: 3.5013
[09/17 04:41:27 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 81.58	
[09/17 04:41:27 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/17 04:41:37 visual_prompt]: Epoch 46 / 100: avg data time: 1.50e-01, avg batch time: 0.5540, average train loss: 2.5506
[09/17 04:41:41 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1429, average loss: 2.5576
[09/17 04:41:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/17 04:42:03 visual_prompt]: 	Test 100/356. loss: 2.670, 0.1833 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/17 04:42:23 visual_prompt]: 	Test 200/356. loss: 2.267, 0.1955 s / batch. (data: 1.22e-02)max mem: 17.22445 GB 
[09/17 04:42:42 visual_prompt]: 	Test 300/356. loss: 2.251, 0.1962 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 04:42:55 visual_prompt]: Inference (test):avg data time: 8.40e-03, avg batch time: 0.1940, average loss: 2.4882
[09/17 04:42:55 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 85.40	
[09/17 04:42:55 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/17 04:43:05 visual_prompt]: Epoch 47 / 100: avg data time: 1.43e-01, avg batch time: 0.5471, average train loss: 2.3641
[09/17 04:43:09 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.1432, average loss: 2.2945
[09/17 04:43:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/17 04:43:31 visual_prompt]: 	Test 100/356. loss: 2.200, 0.2021 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 04:43:51 visual_prompt]: 	Test 200/356. loss: 1.942, 0.1887 s / batch. (data: 1.16e-04)max mem: 17.22445 GB 
[09/17 04:44:10 visual_prompt]: 	Test 300/356. loss: 2.001, 0.1836 s / batch. (data: 1.09e-04)max mem: 17.22445 GB 
[09/17 04:44:23 visual_prompt]: Inference (test):avg data time: 7.21e-03, avg batch time: 0.1935, average loss: 2.1521
[09/17 04:44:23 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 85.40	
[09/17 04:44:23 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/17 04:44:33 visual_prompt]: Epoch 48 / 100: avg data time: 1.42e-01, avg batch time: 0.5438, average train loss: 2.2609
[09/17 04:44:37 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1431, average loss: 1.9976
[09/17 04:44:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.50	top5: 84.50	
[09/17 04:44:59 visual_prompt]: 	Test 100/356. loss: 2.064, 0.1837 s / batch. (data: 1.55e-04)max mem: 17.22445 GB 
[09/17 04:45:18 visual_prompt]: 	Test 200/356. loss: 2.362, 0.1955 s / batch. (data: 1.22e-02)max mem: 17.22445 GB 
[09/17 04:45:38 visual_prompt]: 	Test 300/356. loss: 2.184, 0.1840 s / batch. (data: 1.26e-04)max mem: 17.22445 GB 
[09/17 04:45:50 visual_prompt]: Inference (test):avg data time: 7.22e-03, avg batch time: 0.1931, average loss: 2.1245
[09/17 04:45:50 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.26	top5: 77.81	
[09/17 04:45:50 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/17 04:46:01 visual_prompt]: Epoch 49 / 100: avg data time: 1.48e-01, avg batch time: 0.5517, average train loss: 2.3524
[09/17 04:46:05 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1434, average loss: 2.1956
[09/17 04:46:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/17 04:46:27 visual_prompt]: 	Test 100/356. loss: 2.407, 0.1831 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/17 04:46:46 visual_prompt]: 	Test 200/356. loss: 2.063, 0.1997 s / batch. (data: 1.67e-02)max mem: 17.22445 GB 
[09/17 04:47:06 visual_prompt]: 	Test 300/356. loss: 2.162, 0.2240 s / batch. (data: 3.79e-02)max mem: 17.22445 GB 
[09/17 04:47:18 visual_prompt]: Inference (test):avg data time: 7.84e-03, avg batch time: 0.1944, average loss: 2.2168
[09/17 04:47:18 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.89	top5: 84.67	
[09/17 04:47:18 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/17 04:47:29 visual_prompt]: Epoch 50 / 100: avg data time: 1.54e-01, avg batch time: 0.5605, average train loss: 2.2563
[09/17 04:47:33 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1432, average loss: 1.9023
[09/17 04:47:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 89.50	
[09/17 04:47:55 visual_prompt]: 	Test 100/356. loss: 2.068, 0.1975 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/17 04:48:14 visual_prompt]: 	Test 200/356. loss: 2.025, 0.1842 s / batch. (data: 1.60e-04)max mem: 17.22445 GB 
[09/17 04:48:34 visual_prompt]: 	Test 300/356. loss: 1.993, 0.1944 s / batch. (data: 1.55e-04)max mem: 17.22445 GB 
[09/17 04:48:47 visual_prompt]: Inference (test):avg data time: 7.17e-03, avg batch time: 0.1933, average loss: 1.9874
[09/17 04:48:47 visual_prompt]: Classification results with test_vtab-dmlab: top1: 11.56	top5: 85.76	
[09/17 04:48:47 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/17 04:48:57 visual_prompt]: Epoch 51 / 100: avg data time: 1.46e-01, avg batch time: 0.5480, average train loss: 2.0382
[09/17 04:49:01 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1437, average loss: 1.8942
[09/17 04:49:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 84.00	
[09/17 04:49:23 visual_prompt]: 	Test 100/356. loss: 1.800, 0.1841 s / batch. (data: 1.40e-04)max mem: 17.22445 GB 
[09/17 04:49:43 visual_prompt]: 	Test 200/356. loss: 1.822, 0.1839 s / batch. (data: 1.69e-04)max mem: 17.22445 GB 
[09/17 04:50:02 visual_prompt]: 	Test 300/356. loss: 1.820, 0.1860 s / batch. (data: 1.11e-04)max mem: 17.22445 GB 
[09/17 04:50:15 visual_prompt]: Inference (test):avg data time: 7.67e-03, avg batch time: 0.1956, average loss: 1.8432
[09/17 04:50:15 visual_prompt]: Classification results with test_vtab-dmlab: top1: 21.46	top5: 88.51	
[09/17 04:50:15 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/17 04:50:26 visual_prompt]: Epoch 52 / 100: avg data time: 1.50e-01, avg batch time: 0.5529, average train loss: 1.8673
[09/17 04:50:30 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1431, average loss: 2.0234
[09/17 04:50:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/17 04:50:52 visual_prompt]: 	Test 100/356. loss: 1.987, 0.2131 s / batch. (data: 1.75e-02)max mem: 17.22445 GB 
[09/17 04:51:11 visual_prompt]: 	Test 200/356. loss: 2.049, 0.1833 s / batch. (data: 3.58e-05)max mem: 17.22445 GB 
[09/17 04:51:30 visual_prompt]: 	Test 300/356. loss: 1.940, 0.1995 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/17 04:51:43 visual_prompt]: Inference (test):avg data time: 7.30e-03, avg batch time: 0.1932, average loss: 2.0215
[09/17 04:51:43 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 82.27	
[09/17 04:51:43 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/17 04:51:53 visual_prompt]: Epoch 53 / 100: avg data time: 1.45e-01, avg batch time: 0.5481, average train loss: 1.9969
[09/17 04:51:57 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.1431, average loss: 2.0252
[09/17 04:51:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/17 04:52:19 visual_prompt]: 	Test 100/356. loss: 2.145, 0.1836 s / batch. (data: 1.28e-04)max mem: 17.22445 GB 
[09/17 04:52:39 visual_prompt]: 	Test 200/356. loss: 2.183, 0.1988 s / batch. (data: 1.55e-02)max mem: 17.22445 GB 
[09/17 04:52:58 visual_prompt]: 	Test 300/356. loss: 2.146, 0.1836 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/17 04:53:10 visual_prompt]: Inference (test):avg data time: 6.79e-03, avg batch time: 0.1937, average loss: 2.0971
[09/17 04:53:11 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 77.81	
[09/17 04:53:11 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/17 04:53:21 visual_prompt]: Epoch 54 / 100: avg data time: 1.49e-01, avg batch time: 0.5512, average train loss: 2.1790
[09/17 04:53:25 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1430, average loss: 1.9511
[09/17 04:53:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 82.50	
[09/17 04:53:47 visual_prompt]: 	Test 100/356. loss: 1.970, 0.2105 s / batch. (data: 2.79e-02)max mem: 17.22445 GB 
[09/17 04:54:06 visual_prompt]: 	Test 200/356. loss: 2.051, 0.1840 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/17 04:54:26 visual_prompt]: 	Test 300/356. loss: 1.947, 0.1850 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/17 04:54:38 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1944, average loss: 1.9782
[09/17 04:54:39 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.58	top5: 82.27	
[09/17 04:54:39 visual_prompt]: Best epoch 54: best metric: 0.220
[09/17 04:54:39 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/17 04:54:49 visual_prompt]: Epoch 55 / 100: avg data time: 1.49e-01, avg batch time: 0.5523, average train loss: 2.2928
[09/17 04:54:53 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1433, average loss: 2.4388
[09/17 04:54:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/17 04:55:15 visual_prompt]: 	Test 100/356. loss: 2.632, 0.2073 s / batch. (data: 2.45e-02)max mem: 17.22445 GB 
[09/17 04:55:35 visual_prompt]: 	Test 200/356. loss: 2.599, 0.1843 s / batch. (data: 5.77e-05)max mem: 17.22445 GB 
[09/17 04:55:54 visual_prompt]: 	Test 300/356. loss: 2.709, 0.1955 s / batch. (data: 1.22e-02)max mem: 17.22445 GB 
[09/17 04:56:07 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1945, average loss: 2.5381
[09/17 04:56:07 visual_prompt]: Classification results with test_vtab-dmlab: top1: 20.24	top5: 84.67	
[09/17 04:56:07 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/17 04:56:17 visual_prompt]: Epoch 56 / 100: avg data time: 1.46e-01, avg batch time: 0.5490, average train loss: 2.3021
[09/17 04:56:21 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.1430, average loss: 2.1000
[09/17 04:56:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/17 04:56:43 visual_prompt]: 	Test 100/356. loss: 2.233, 0.1846 s / batch. (data: 3.37e-04)max mem: 17.22445 GB 
[09/17 04:57:03 visual_prompt]: 	Test 200/356. loss: 2.222, 0.1940 s / batch. (data: 1.03e-02)max mem: 17.22445 GB 
[09/17 04:57:22 visual_prompt]: 	Test 300/356. loss: 2.070, 0.2077 s / batch. (data: 2.46e-02)max mem: 17.22445 GB 
[09/17 04:57:35 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1935, average loss: 2.1730
[09/17 04:57:35 visual_prompt]: Classification results with test_vtab-dmlab: top1: 11.58	top5: 82.27	
[09/17 04:57:35 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/17 04:57:45 visual_prompt]: Epoch 57 / 100: avg data time: 1.43e-01, avg batch time: 0.5468, average train loss: 2.0277
[09/17 04:57:49 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1431, average loss: 1.9295
[09/17 04:57:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 84.00	
[09/17 04:58:11 visual_prompt]: 	Test 100/356. loss: 1.940, 0.1978 s / batch. (data: 1.52e-02)max mem: 17.22445 GB 
[09/17 04:58:31 visual_prompt]: 	Test 200/356. loss: 1.883, 0.1842 s / batch. (data: 1.26e-04)max mem: 17.22445 GB 
[09/17 04:58:51 visual_prompt]: 	Test 300/356. loss: 1.810, 0.1845 s / batch. (data: 1.23e-04)max mem: 17.22445 GB 
[09/17 04:59:03 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1955, average loss: 1.9071
[09/17 04:59:03 visual_prompt]: Classification results with test_vtab-dmlab: top1: 20.60	top5: 85.40	
[09/17 04:59:03 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/17 04:59:14 visual_prompt]: Epoch 58 / 100: avg data time: 1.51e-01, avg batch time: 0.5521, average train loss: 1.9547
[09/17 04:59:18 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1431, average loss: 1.9500
[09/17 04:59:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 84.00	
[09/17 04:59:40 visual_prompt]: 	Test 100/356. loss: 1.914, 0.1834 s / batch. (data: 1.23e-04)max mem: 17.22445 GB 
[09/17 04:59:59 visual_prompt]: 	Test 200/356. loss: 1.812, 0.2022 s / batch. (data: 4.55e-05)max mem: 17.22445 GB 
[09/17 05:00:19 visual_prompt]: 	Test 300/356. loss: 1.780, 0.2083 s / batch. (data: 2.51e-02)max mem: 17.22445 GB 
[09/17 05:00:31 visual_prompt]: Inference (test):avg data time: 7.71e-03, avg batch time: 0.1943, average loss: 1.8999
[09/17 05:00:32 visual_prompt]: Classification results with test_vtab-dmlab: top1: 26.31	top5: 85.40	
[09/17 05:00:32 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/17 05:00:42 visual_prompt]: Epoch 59 / 100: avg data time: 1.52e-01, avg batch time: 0.5536, average train loss: 1.7618
[09/17 05:00:46 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1427, average loss: 1.7644
[09/17 05:00:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.50	top5: 91.50	
[09/17 05:01:08 visual_prompt]: 	Test 100/356. loss: 1.846, 0.1960 s / batch. (data: 1.42e-04)max mem: 17.22445 GB 
[09/17 05:01:28 visual_prompt]: 	Test 200/356. loss: 1.785, 0.2035 s / batch. (data: 1.48e-02)max mem: 17.22445 GB 
[09/17 05:01:47 visual_prompt]: 	Test 300/356. loss: 1.704, 0.1955 s / batch. (data: 1.22e-02)max mem: 17.22445 GB 
[09/17 05:02:00 visual_prompt]: Inference (test):avg data time: 7.91e-03, avg batch time: 0.1941, average loss: 1.7731
[09/17 05:02:00 visual_prompt]: Classification results with test_vtab-dmlab: top1: 24.93	top5: 90.34	
[09/17 05:02:00 visual_prompt]: Best epoch 59: best metric: 0.255
[09/17 05:02:00 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/17 05:02:11 visual_prompt]: Epoch 60 / 100: avg data time: 1.48e-01, avg batch time: 0.5491, average train loss: 2.0025
[09/17 05:02:15 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1432, average loss: 1.8118
[09/17 05:02:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 92.00	
[09/17 05:02:37 visual_prompt]: 	Test 100/356. loss: 1.854, 0.1832 s / batch. (data: 1.68e-04)max mem: 17.22445 GB 
[09/17 05:02:57 visual_prompt]: 	Test 200/356. loss: 1.747, 0.1833 s / batch. (data: 1.33e-04)max mem: 17.22445 GB 
[09/17 05:03:16 visual_prompt]: 	Test 300/356. loss: 1.772, 0.1838 s / batch. (data: 1.45e-04)max mem: 17.22445 GB 
[09/17 05:03:29 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1951, average loss: 1.8069
[09/17 05:03:29 visual_prompt]: Classification results with test_vtab-dmlab: top1: 25.97	top5: 89.92	
[09/17 05:03:29 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/17 05:03:39 visual_prompt]: Epoch 61 / 100: avg data time: 1.48e-01, avg batch time: 0.5508, average train loss: 1.9087
[09/17 05:03:43 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1431, average loss: 1.8870
[09/17 05:03:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 87.00	
[09/17 05:04:05 visual_prompt]: 	Test 100/356. loss: 1.715, 0.1837 s / batch. (data: 1.17e-04)max mem: 17.22445 GB 
[09/17 05:04:25 visual_prompt]: 	Test 200/356. loss: 1.802, 0.2076 s / batch. (data: 2.47e-02)max mem: 17.22445 GB 
[09/17 05:04:44 visual_prompt]: 	Test 300/356. loss: 1.752, 0.1906 s / batch. (data: 1.12e-04)max mem: 17.22445 GB 
[09/17 05:04:57 visual_prompt]: Inference (test):avg data time: 7.17e-03, avg batch time: 0.1952, average loss: 1.7959
[09/17 05:04:57 visual_prompt]: Classification results with test_vtab-dmlab: top1: 28.25	top5: 90.39	
[09/17 05:04:57 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/17 05:05:07 visual_prompt]: Epoch 62 / 100: avg data time: 1.25e-01, avg batch time: 0.5346, average train loss: 1.7838
[09/17 05:05:12 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1432, average loss: 1.7308
[09/17 05:05:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 97.50	
[09/17 05:05:34 visual_prompt]: 	Test 100/356. loss: 1.784, 0.1840 s / batch. (data: 1.38e-04)max mem: 17.22445 GB 
[09/17 05:05:53 visual_prompt]: 	Test 200/356. loss: 1.731, 0.1838 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/17 05:06:13 visual_prompt]: 	Test 300/356. loss: 1.649, 0.2040 s / batch. (data: 2.00e-02)max mem: 17.22445 GB 
[09/17 05:06:25 visual_prompt]: Inference (test):avg data time: 7.94e-03, avg batch time: 0.1944, average loss: 1.7244
[09/17 05:06:25 visual_prompt]: Classification results with test_vtab-dmlab: top1: 30.93	top5: 95.46	
[09/17 05:06:25 visual_prompt]: Best epoch 62: best metric: 0.265
[09/17 05:06:25 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/17 05:06:36 visual_prompt]: Epoch 63 / 100: avg data time: 1.45e-01, avg batch time: 0.5491, average train loss: 1.7446
[09/17 05:06:40 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.1431, average loss: 1.6806
[09/17 05:06:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.50	top5: 98.00	
[09/17 05:07:01 visual_prompt]: 	Test 100/356. loss: 1.764, 0.1836 s / batch. (data: 1.30e-04)max mem: 17.22445 GB 
[09/17 05:07:21 visual_prompt]: 	Test 200/356. loss: 1.747, 0.1836 s / batch. (data: 1.30e-04)max mem: 17.22445 GB 
[09/17 05:07:40 visual_prompt]: 	Test 300/356. loss: 1.722, 0.1836 s / batch. (data: 1.22e-04)max mem: 17.22445 GB 
[09/17 05:07:53 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1934, average loss: 1.7096
[09/17 05:07:53 visual_prompt]: Classification results with test_vtab-dmlab: top1: 27.90	top5: 96.44	
[09/17 05:07:53 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/17 05:08:03 visual_prompt]: Epoch 64 / 100: avg data time: 1.39e-01, avg batch time: 0.5424, average train loss: 1.6550
[09/17 05:08:07 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1429, average loss: 1.6119
[09/17 05:08:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 95.00	
[09/17 05:08:29 visual_prompt]: 	Test 100/356. loss: 1.522, 0.1866 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/17 05:08:49 visual_prompt]: 	Test 200/356. loss: 1.853, 0.2062 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/17 05:09:08 visual_prompt]: 	Test 300/356. loss: 1.685, 0.1889 s / batch. (data: 1.33e-04)max mem: 17.22445 GB 
[09/17 05:09:21 visual_prompt]: Inference (test):avg data time: 8.77e-03, avg batch time: 0.1950, average loss: 1.6954
[09/17 05:09:21 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.51	top5: 94.95	
[09/17 05:09:21 visual_prompt]: Best epoch 64: best metric: 0.325
[09/17 05:09:21 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/17 05:09:32 visual_prompt]: Epoch 65 / 100: avg data time: 1.45e-01, avg batch time: 0.5477, average train loss: 1.6223
[09/17 05:09:36 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1432, average loss: 1.5651
[09/17 05:09:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 97.50	
[09/17 05:09:58 visual_prompt]: 	Test 100/356. loss: 1.499, 0.1955 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 05:10:17 visual_prompt]: 	Test 200/356. loss: 1.573, 0.1959 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 05:10:36 visual_prompt]: 	Test 300/356. loss: 1.446, 0.1922 s / batch. (data: 1.50e-04)max mem: 17.22445 GB 
[09/17 05:10:49 visual_prompt]: Inference (test):avg data time: 7.98e-03, avg batch time: 0.1934, average loss: 1.5428
[09/17 05:10:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.86	top5: 96.65	
[09/17 05:10:49 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/17 05:11:00 visual_prompt]: Epoch 66 / 100: avg data time: 1.51e-01, avg batch time: 0.5543, average train loss: 1.5258
[09/17 05:11:04 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1430, average loss: 1.6354
[09/17 05:11:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 97.00	
[09/17 05:11:26 visual_prompt]: 	Test 100/356. loss: 1.696, 0.2098 s / batch. (data: 2.58e-02)max mem: 17.22445 GB 
[09/17 05:11:45 visual_prompt]: 	Test 200/356. loss: 1.458, 0.1964 s / batch. (data: 1.33e-02)max mem: 17.22445 GB 
[09/17 05:12:04 visual_prompt]: 	Test 300/356. loss: 1.484, 0.1837 s / batch. (data: 1.26e-04)max mem: 17.22445 GB 
[09/17 05:12:17 visual_prompt]: Inference (test):avg data time: 7.62e-03, avg batch time: 0.1935, average loss: 1.6129
[09/17 05:12:17 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.08	top5: 96.53	
[09/17 05:12:17 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/17 05:12:28 visual_prompt]: Epoch 67 / 100: avg data time: 1.39e-01, avg batch time: 0.5433, average train loss: 1.4777
[09/17 05:12:32 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1432, average loss: 1.4361
[09/17 05:12:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/17 05:12:54 visual_prompt]: 	Test 100/356. loss: 1.412, 0.2084 s / batch. (data: 2.53e-02)max mem: 17.22445 GB 
[09/17 05:13:13 visual_prompt]: 	Test 200/356. loss: 1.627, 0.2215 s / batch. (data: 2.36e-02)max mem: 17.22445 GB 
[09/17 05:13:32 visual_prompt]: 	Test 300/356. loss: 1.366, 0.1915 s / batch. (data: 1.23e-04)max mem: 17.22445 GB 
[09/17 05:13:45 visual_prompt]: Inference (test):avg data time: 7.03e-03, avg batch time: 0.1936, average loss: 1.4821
[09/17 05:13:45 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.34	top5: 97.73	
[09/17 05:13:45 visual_prompt]: Best epoch 67: best metric: 0.375
[09/17 05:13:45 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/17 05:13:55 visual_prompt]: Epoch 68 / 100: avg data time: 1.42e-01, avg batch time: 0.5437, average train loss: 1.3999
[09/17 05:13:59 visual_prompt]: Inference (val):avg data time: 4.86e-05, avg batch time: 0.1430, average loss: 1.4858
[09/17 05:13:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.00	
[09/17 05:14:21 visual_prompt]: 	Test 100/356. loss: 1.350, 0.1837 s / batch. (data: 1.18e-04)max mem: 17.22445 GB 
[09/17 05:14:41 visual_prompt]: 	Test 200/356. loss: 1.469, 0.1840 s / batch. (data: 1.42e-04)max mem: 17.22445 GB 
[09/17 05:15:00 visual_prompt]: 	Test 300/356. loss: 1.447, 0.1841 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/17 05:15:13 visual_prompt]: Inference (test):avg data time: 7.04e-03, avg batch time: 0.1945, average loss: 1.5056
[09/17 05:15:13 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.87	top5: 97.11	
[09/17 05:15:13 visual_prompt]: Best epoch 68: best metric: 0.380
[09/17 05:15:13 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/17 05:15:23 visual_prompt]: Epoch 69 / 100: avg data time: 1.36e-01, avg batch time: 0.5392, average train loss: 1.3933
[09/17 05:15:27 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1431, average loss: 1.2727
[09/17 05:15:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 99.00	
[09/17 05:15:49 visual_prompt]: 	Test 100/356. loss: 1.306, 0.1987 s / batch. (data: 1.64e-04)max mem: 17.22445 GB 
[09/17 05:16:09 visual_prompt]: 	Test 200/356. loss: 1.555, 0.1843 s / batch. (data: 1.23e-04)max mem: 17.22445 GB 
[09/17 05:16:29 visual_prompt]: 	Test 300/356. loss: 1.324, 0.1930 s / batch. (data: 3.79e-04)max mem: 17.22445 GB 
[09/17 05:16:41 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1943, average loss: 1.3700
[09/17 05:16:41 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.36	top5: 97.37	
[09/17 05:16:41 visual_prompt]: Best epoch 69: best metric: 0.460
[09/17 05:16:41 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/17 05:16:51 visual_prompt]: Epoch 70 / 100: avg data time: 1.29e-01, avg batch time: 0.5322, average train loss: 1.3419
[09/17 05:16:56 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1433, average loss: 1.4536
[09/17 05:16:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 99.50	
[09/17 05:17:17 visual_prompt]: 	Test 100/356. loss: 1.461, 0.1838 s / batch. (data: 1.18e-04)max mem: 17.22445 GB 
[09/17 05:17:37 visual_prompt]: 	Test 200/356. loss: 1.412, 0.1961 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/17 05:17:56 visual_prompt]: 	Test 300/356. loss: 1.431, 0.1849 s / batch. (data: 2.65e-04)max mem: 17.22445 GB 
[09/17 05:18:09 visual_prompt]: Inference (test):avg data time: 7.94e-03, avg batch time: 0.1942, average loss: 1.5458
[09/17 05:18:09 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.32	top5: 97.92	
[09/17 05:18:09 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/17 05:18:20 visual_prompt]: Epoch 71 / 100: avg data time: 1.57e-01, avg batch time: 0.5634, average train loss: 1.3301
[09/17 05:18:24 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1432, average loss: 1.5677
[09/17 05:18:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 95.50	
[09/17 05:18:46 visual_prompt]: 	Test 100/356. loss: 1.612, 0.1974 s / batch. (data: 1.44e-02)max mem: 17.22445 GB 
[09/17 05:19:05 visual_prompt]: 	Test 200/356. loss: 1.552, 0.2128 s / batch. (data: 8.29e-03)max mem: 17.22445 GB 
[09/17 05:19:25 visual_prompt]: 	Test 300/356. loss: 1.595, 0.1842 s / batch. (data: 1.41e-04)max mem: 17.22445 GB 
[09/17 05:19:37 visual_prompt]: Inference (test):avg data time: 7.28e-03, avg batch time: 0.1930, average loss: 1.6534
[09/17 05:19:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.36	top5: 94.68	
[09/17 05:19:37 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/17 05:19:47 visual_prompt]: Epoch 72 / 100: avg data time: 1.44e-01, avg batch time: 0.5473, average train loss: 1.3354
[09/17 05:19:52 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1429, average loss: 1.3964
[09/17 05:19:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/17 05:20:13 visual_prompt]: 	Test 100/356. loss: 1.320, 0.1842 s / batch. (data: 1.21e-04)max mem: 17.22445 GB 
[09/17 05:20:33 visual_prompt]: 	Test 200/356. loss: 1.374, 0.1858 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/17 05:20:53 visual_prompt]: 	Test 300/356. loss: 1.382, 0.1979 s / batch. (data: 1.46e-02)max mem: 17.22445 GB 
[09/17 05:21:05 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1940, average loss: 1.4570
[09/17 05:21:05 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.55	top5: 95.87	
[09/17 05:21:05 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/17 05:21:15 visual_prompt]: Epoch 73 / 100: avg data time: 1.44e-01, avg batch time: 0.5454, average train loss: 1.2336
[09/17 05:21:20 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1429, average loss: 1.2768
[09/17 05:21:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 99.50	
[09/17 05:21:41 visual_prompt]: 	Test 100/356. loss: 1.560, 0.1961 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/17 05:22:01 visual_prompt]: 	Test 200/356. loss: 1.713, 0.1847 s / batch. (data: 1.77e-04)max mem: 17.22445 GB 
[09/17 05:22:20 visual_prompt]: 	Test 300/356. loss: 1.398, 0.2139 s / batch. (data: 3.09e-02)max mem: 17.22445 GB 
[09/17 05:22:33 visual_prompt]: Inference (test):avg data time: 7.39e-03, avg batch time: 0.1939, average loss: 1.5223
[09/17 05:22:33 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.05	top5: 97.60	
[09/17 05:22:33 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/17 05:22:43 visual_prompt]: Epoch 74 / 100: avg data time: 1.50e-01, avg batch time: 0.5511, average train loss: 1.2150
[09/17 05:22:48 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1432, average loss: 1.2344
[09/17 05:22:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.50	
[09/17 05:23:09 visual_prompt]: 	Test 100/356. loss: 1.344, 0.1830 s / batch. (data: 1.53e-04)max mem: 17.22445 GB 
[09/17 05:23:29 visual_prompt]: 	Test 200/356. loss: 1.319, 0.1960 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 05:23:48 visual_prompt]: 	Test 300/356. loss: 1.315, 0.2114 s / batch. (data: 2.81e-02)max mem: 17.22445 GB 
[09/17 05:24:01 visual_prompt]: Inference (test):avg data time: 7.25e-03, avg batch time: 0.1940, average loss: 1.4217
[09/17 05:24:01 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.61	top5: 96.77	
[09/17 05:24:01 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/17 05:24:12 visual_prompt]: Epoch 75 / 100: avg data time: 1.48e-01, avg batch time: 0.5497, average train loss: 1.2205
[09/17 05:24:16 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1433, average loss: 1.1511
[09/17 05:24:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 99.50	
[09/17 05:24:39 visual_prompt]: 	Test 100/356. loss: 1.311, 0.2120 s / batch. (data: 2.53e-02)max mem: 17.22445 GB 
[09/17 05:24:58 visual_prompt]: 	Test 200/356. loss: 1.467, 0.1841 s / batch. (data: 1.17e-04)max mem: 17.22445 GB 
[09/17 05:25:18 visual_prompt]: 	Test 300/356. loss: 1.301, 0.1836 s / batch. (data: 1.29e-04)max mem: 17.22445 GB 
[09/17 05:25:30 visual_prompt]: Inference (test):avg data time: 8.15e-03, avg batch time: 0.1967, average loss: 1.3659
[09/17 05:25:30 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.92	top5: 97.76	
[09/17 05:25:30 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/17 05:25:41 visual_prompt]: Epoch 76 / 100: avg data time: 1.52e-01, avg batch time: 0.5559, average train loss: 1.1408
[09/17 05:25:45 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1433, average loss: 1.1652
[09/17 05:25:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 99.50	
[09/17 05:26:08 visual_prompt]: 	Test 100/356. loss: 1.459, 0.2115 s / batch. (data: 2.88e-02)max mem: 17.22445 GB 
[09/17 05:26:27 visual_prompt]: 	Test 200/356. loss: 1.596, 0.1984 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/17 05:26:47 visual_prompt]: 	Test 300/356. loss: 1.400, 0.1958 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/17 05:26:59 visual_prompt]: Inference (test):avg data time: 8.10e-03, avg batch time: 0.1966, average loss: 1.4823
[09/17 05:26:59 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.10	top5: 97.63	
[09/17 05:26:59 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/17 05:27:10 visual_prompt]: Epoch 77 / 100: avg data time: 1.50e-01, avg batch time: 0.5522, average train loss: 1.1781
[09/17 05:27:14 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1430, average loss: 1.1534
[09/17 05:27:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 99.50	
[09/17 05:27:36 visual_prompt]: 	Test 100/356. loss: 1.382, 0.1955 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 05:27:55 visual_prompt]: 	Test 200/356. loss: 1.516, 0.1992 s / batch. (data: 1.62e-02)max mem: 17.22445 GB 
[09/17 05:28:15 visual_prompt]: 	Test 300/356. loss: 1.377, 0.1960 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 05:28:27 visual_prompt]: Inference (test):avg data time: 7.22e-03, avg batch time: 0.1935, average loss: 1.4249
[09/17 05:28:27 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.63	top5: 97.92	
[09/17 05:28:27 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/17 05:28:38 visual_prompt]: Epoch 78 / 100: avg data time: 1.49e-01, avg batch time: 0.5518, average train loss: 1.1300
[09/17 05:28:42 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1432, average loss: 1.1396
[09/17 05:28:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.00	top5: 99.50	
[09/17 05:29:03 visual_prompt]: 	Test 100/356. loss: 1.280, 0.1894 s / batch. (data: 6.50e-03)max mem: 17.22445 GB 
[09/17 05:29:23 visual_prompt]: 	Test 200/356. loss: 1.556, 0.1876 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/17 05:29:43 visual_prompt]: 	Test 300/356. loss: 1.308, 0.1881 s / batch. (data: 1.33e-04)max mem: 17.22445 GB 
[09/17 05:29:55 visual_prompt]: Inference (test):avg data time: 7.91e-03, avg batch time: 0.1948, average loss: 1.4173
[09/17 05:29:56 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.81	top5: 97.41	
[09/17 05:29:56 visual_prompt]: Best epoch 78: best metric: 0.470
[09/17 05:29:56 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/17 05:30:06 visual_prompt]: Epoch 79 / 100: avg data time: 1.42e-01, avg batch time: 0.5432, average train loss: 1.1732
[09/17 05:30:10 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1431, average loss: 1.1658
[09/17 05:30:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 100.00	
[09/17 05:30:32 visual_prompt]: 	Test 100/356. loss: 1.335, 0.1916 s / batch. (data: 8.40e-03)max mem: 17.22445 GB 
[09/17 05:30:51 visual_prompt]: 	Test 200/356. loss: 1.515, 0.1840 s / batch. (data: 1.78e-04)max mem: 17.22445 GB 
[09/17 05:31:11 visual_prompt]: 	Test 300/356. loss: 1.392, 0.1891 s / batch. (data: 1.41e-04)max mem: 17.22445 GB 
[09/17 05:31:23 visual_prompt]: Inference (test):avg data time: 8.18e-03, avg batch time: 0.1935, average loss: 1.4309
[09/17 05:31:23 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.17	top5: 97.64	
[09/17 05:31:23 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/17 05:31:34 visual_prompt]: Epoch 80 / 100: avg data time: 1.46e-01, avg batch time: 0.5500, average train loss: 1.1038
[09/17 05:31:38 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1431, average loss: 1.0654
[09/17 05:31:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 49.00	top5: 99.50	
[09/17 05:31:59 visual_prompt]: 	Test 100/356. loss: 1.378, 0.1830 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/17 05:32:19 visual_prompt]: 	Test 200/356. loss: 1.507, 0.1972 s / batch. (data: 1.41e-02)max mem: 17.22445 GB 
[09/17 05:32:39 visual_prompt]: 	Test 300/356. loss: 1.362, 0.2111 s / batch. (data: 2.79e-02)max mem: 17.22445 GB 
[09/17 05:32:51 visual_prompt]: Inference (test):avg data time: 8.41e-03, avg batch time: 0.1942, average loss: 1.4306
[09/17 05:32:51 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.10	top5: 97.57	
[09/17 05:32:51 visual_prompt]: Best epoch 80: best metric: 0.490
[09/17 05:32:51 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/17 05:33:02 visual_prompt]: Epoch 81 / 100: avg data time: 1.51e-01, avg batch time: 0.5528, average train loss: 1.0898
[09/17 05:33:06 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1428, average loss: 1.0730
[09/17 05:33:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 99.50	
[09/17 05:33:28 visual_prompt]: 	Test 100/356. loss: 1.399, 0.1977 s / batch. (data: 1.22e-04)max mem: 17.22445 GB 
[09/17 05:33:47 visual_prompt]: 	Test 200/356. loss: 1.685, 0.1842 s / batch. (data: 1.33e-04)max mem: 17.22445 GB 
[09/17 05:34:07 visual_prompt]: 	Test 300/356. loss: 1.438, 0.1839 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/17 05:34:20 visual_prompt]: Inference (test):avg data time: 8.24e-03, avg batch time: 0.1954, average loss: 1.4649
[09/17 05:34:20 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.28	top5: 97.12	
[09/17 05:34:20 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/17 05:34:30 visual_prompt]: Epoch 82 / 100: avg data time: 1.37e-01, avg batch time: 0.5416, average train loss: 1.0531
[09/17 05:34:34 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1431, average loss: 0.9903
[09/17 05:34:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.00	top5: 100.00	
[09/17 05:34:56 visual_prompt]: 	Test 100/356. loss: 1.371, 0.1884 s / batch. (data: 5.44e-03)max mem: 17.22445 GB 
[09/17 05:35:16 visual_prompt]: 	Test 200/356. loss: 1.441, 0.1951 s / batch. (data: 1.20e-04)max mem: 17.22445 GB 
[09/17 05:35:35 visual_prompt]: 	Test 300/356. loss: 1.280, 0.1873 s / batch. (data: 1.15e-04)max mem: 17.22445 GB 
[09/17 05:35:48 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1935, average loss: 1.4300
[09/17 05:35:48 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.75	top5: 97.71	
[09/17 05:35:48 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/17 05:35:58 visual_prompt]: Epoch 83 / 100: avg data time: 1.39e-01, avg batch time: 0.5486, average train loss: 1.0040
[09/17 05:36:02 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1432, average loss: 0.9482
[09/17 05:36:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.50	top5: 99.50	
[09/17 05:36:24 visual_prompt]: 	Test 100/356. loss: 1.367, 0.2078 s / batch. (data: 2.51e-02)max mem: 17.22445 GB 
[09/17 05:36:44 visual_prompt]: 	Test 200/356. loss: 1.709, 0.1892 s / batch. (data: 1.47e-04)max mem: 17.22445 GB 
[09/17 05:37:03 visual_prompt]: 	Test 300/356. loss: 1.451, 0.1959 s / batch. (data: 1.36e-04)max mem: 17.22445 GB 
[09/17 05:37:16 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1940, average loss: 1.4852
[09/17 05:37:16 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.65	top5: 97.00	
[09/17 05:37:16 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/17 05:37:26 visual_prompt]: Epoch 84 / 100: avg data time: 1.49e-01, avg batch time: 0.5574, average train loss: 0.9661
[09/17 05:37:31 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1433, average loss: 0.9238
[09/17 05:37:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 52.50	top5: 100.00	
[09/17 05:37:53 visual_prompt]: 	Test 100/356. loss: 1.240, 0.1830 s / batch. (data: 1.54e-04)max mem: 17.22445 GB 
[09/17 05:38:12 visual_prompt]: 	Test 200/356. loss: 1.445, 0.1838 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 05:38:32 visual_prompt]: 	Test 300/356. loss: 1.418, 0.1840 s / batch. (data: 1.31e-04)max mem: 17.22445 GB 
[09/17 05:38:44 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1951, average loss: 1.4378
[09/17 05:38:45 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.48	top5: 97.68	
[09/17 05:38:45 visual_prompt]: Best epoch 84: best metric: 0.525
[09/17 05:38:45 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/17 05:38:55 visual_prompt]: Epoch 85 / 100: avg data time: 1.47e-01, avg batch time: 0.5489, average train loss: 0.9276
[09/17 05:38:59 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1432, average loss: 0.8805
[09/17 05:38:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 53.50	top5: 100.00	
[09/17 05:39:21 visual_prompt]: 	Test 100/356. loss: 1.384, 0.1835 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/17 05:39:40 visual_prompt]: 	Test 200/356. loss: 1.541, 0.1963 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/17 05:40:00 visual_prompt]: 	Test 300/356. loss: 1.382, 0.1972 s / batch. (data: 1.13e-04)max mem: 17.22445 GB 
[09/17 05:40:13 visual_prompt]: Inference (test):avg data time: 7.49e-03, avg batch time: 0.1949, average loss: 1.4770
[09/17 05:40:13 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.00	top5: 97.92	
[09/17 05:40:13 visual_prompt]: Best epoch 85: best metric: 0.535
[09/17 05:40:13 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/17 05:40:23 visual_prompt]: Epoch 86 / 100: avg data time: 1.41e-01, avg batch time: 0.5438, average train loss: 0.8749
[09/17 05:40:28 visual_prompt]: Inference (val):avg data time: 1.39e-04, avg batch time: 0.2917, average loss: 0.9767
[09/17 05:40:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 100.00	
[09/17 05:40:50 visual_prompt]: 	Test 100/356. loss: 1.508, 0.2066 s / batch. (data: 1.40e-02)max mem: 17.22445 GB 
[09/17 05:41:09 visual_prompt]: 	Test 200/356. loss: 1.573, 0.1835 s / batch. (data: 1.38e-04)max mem: 17.22445 GB 
[09/17 05:41:28 visual_prompt]: 	Test 300/356. loss: 1.639, 0.1937 s / batch. (data: 1.42e-04)max mem: 17.22445 GB 
[09/17 05:41:41 visual_prompt]: Inference (test):avg data time: 6.71e-03, avg batch time: 0.1929, average loss: 1.6693
[09/17 05:41:41 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.35	top5: 97.55	
[09/17 05:41:41 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/17 05:41:52 visual_prompt]: Epoch 87 / 100: avg data time: 1.51e-01, avg batch time: 0.5530, average train loss: 0.8645
[09/17 05:41:56 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1430, average loss: 0.8797
[09/17 05:41:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 52.00	top5: 100.00	
[09/17 05:42:18 visual_prompt]: 	Test 100/356. loss: 1.338, 0.1955 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 05:42:37 visual_prompt]: 	Test 200/356. loss: 1.612, 0.1976 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 05:42:57 visual_prompt]: 	Test 300/356. loss: 1.471, 0.1959 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 05:43:09 visual_prompt]: Inference (test):avg data time: 7.80e-03, avg batch time: 0.1933, average loss: 1.5226
[09/17 05:43:09 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.52	top5: 97.51	
[09/17 05:43:09 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/17 05:43:20 visual_prompt]: Epoch 88 / 100: avg data time: 1.63e-01, avg batch time: 0.5639, average train loss: 0.8676
[09/17 05:43:24 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.1441, average loss: 0.9108
[09/17 05:43:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 50.50	top5: 100.00	
[09/17 05:43:46 visual_prompt]: 	Test 100/356. loss: 1.518, 0.1841 s / batch. (data: 4.10e-05)max mem: 17.22445 GB 
[09/17 05:44:06 visual_prompt]: 	Test 200/356. loss: 1.541, 0.1842 s / batch. (data: 1.33e-04)max mem: 17.22445 GB 
[09/17 05:44:25 visual_prompt]: 	Test 300/356. loss: 1.502, 0.1998 s / batch. (data: 1.65e-02)max mem: 17.22445 GB 
[09/17 05:44:38 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1939, average loss: 1.5659
[09/17 05:44:38 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.61	top5: 97.89	
[09/17 05:44:38 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/17 05:44:48 visual_prompt]: Epoch 89 / 100: avg data time: 1.45e-01, avg batch time: 0.5482, average train loss: 0.8634
[09/17 05:44:53 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.1432, average loss: 0.7933
[09/17 05:44:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 63.00	top5: 100.00	
[09/17 05:45:14 visual_prompt]: 	Test 100/356. loss: 1.390, 0.2067 s / batch. (data: 2.36e-02)max mem: 17.22445 GB 
[09/17 05:45:34 visual_prompt]: 	Test 200/356. loss: 1.737, 0.1902 s / batch. (data: 1.54e-04)max mem: 17.22445 GB 
[09/17 05:45:54 visual_prompt]: 	Test 300/356. loss: 1.472, 0.1840 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/17 05:46:07 visual_prompt]: Inference (test):avg data time: 8.15e-03, avg batch time: 0.1965, average loss: 1.5455
[09/17 05:46:07 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.61	top5: 97.62	
[09/17 05:46:07 visual_prompt]: Best epoch 89: best metric: 0.630
[09/17 05:46:07 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/17 05:46:17 visual_prompt]: Epoch 90 / 100: avg data time: 1.39e-01, avg batch time: 0.5450, average train loss: 0.8157
[09/17 05:46:22 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.1431, average loss: 0.8527
[09/17 05:46:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 57.50	top5: 100.00	
[09/17 05:46:44 visual_prompt]: 	Test 100/356. loss: 1.435, 0.2240 s / batch. (data: 2.51e-02)max mem: 17.22445 GB 
[09/17 05:47:03 visual_prompt]: 	Test 200/356. loss: 1.517, 0.1984 s / batch. (data: 1.52e-02)max mem: 17.22445 GB 
[09/17 05:47:23 visual_prompt]: 	Test 300/356. loss: 1.566, 0.1960 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 05:47:36 visual_prompt]: Inference (test):avg data time: 7.74e-03, avg batch time: 0.1947, average loss: 1.6029
[09/17 05:47:36 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.05	top5: 97.62	
[09/17 05:47:36 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/17 05:47:46 visual_prompt]: Epoch 91 / 100: avg data time: 1.48e-01, avg batch time: 0.5522, average train loss: 0.7999
[09/17 05:47:51 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1430, average loss: 0.8037
[09/17 05:47:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 51.50	top5: 100.00	
[09/17 05:48:13 visual_prompt]: 	Test 100/356. loss: 1.531, 0.1956 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 05:48:32 visual_prompt]: 	Test 200/356. loss: 1.814, 0.1987 s / batch. (data: 1.55e-02)max mem: 17.22445 GB 
[09/17 05:48:52 visual_prompt]: 	Test 300/356. loss: 1.698, 0.1837 s / batch. (data: 1.42e-04)max mem: 17.22445 GB 
[09/17 05:49:05 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1955, average loss: 1.6886
[09/17 05:49:05 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.45	top5: 97.66	
[09/17 05:49:05 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/17 05:49:15 visual_prompt]: Epoch 92 / 100: avg data time: 1.50e-01, avg batch time: 0.5543, average train loss: 0.7632
[09/17 05:49:19 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1428, average loss: 0.7802
[09/17 05:49:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 56.00	top5: 100.00	
[09/17 05:49:41 visual_prompt]: 	Test 100/356. loss: 1.589, 0.1840 s / batch. (data: 1.18e-04)max mem: 17.22445 GB 
[09/17 05:50:00 visual_prompt]: 	Test 200/356. loss: 1.880, 0.1845 s / batch. (data: 1.32e-04)max mem: 17.22445 GB 
[09/17 05:50:20 visual_prompt]: 	Test 300/356. loss: 1.722, 0.1874 s / batch. (data: 1.33e-04)max mem: 17.22445 GB 
[09/17 05:50:33 visual_prompt]: Inference (test):avg data time: 8.29e-03, avg batch time: 0.1940, average loss: 1.7606
[09/17 05:50:33 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.01	top5: 97.66	
[09/17 05:50:33 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/17 05:50:43 visual_prompt]: Epoch 93 / 100: avg data time: 1.43e-01, avg batch time: 0.5474, average train loss: 0.7524
[09/17 05:50:47 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1433, average loss: 0.7184
[09/17 05:50:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 66.00	top5: 100.00	
[09/17 05:51:09 visual_prompt]: 	Test 100/356. loss: 1.484, 0.1877 s / batch. (data: 4.65e-03)max mem: 17.22445 GB 
[09/17 05:51:28 visual_prompt]: 	Test 200/356. loss: 1.777, 0.1942 s / batch. (data: 1.71e-04)max mem: 17.22445 GB 
[09/17 05:51:48 visual_prompt]: 	Test 300/356. loss: 1.781, 0.1982 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/17 05:52:01 visual_prompt]: Inference (test):avg data time: 8.05e-03, avg batch time: 0.1941, average loss: 1.7254
[09/17 05:52:01 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.37	top5: 97.66	
[09/17 05:52:01 visual_prompt]: Best epoch 93: best metric: 0.660
[09/17 05:52:01 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/17 05:52:11 visual_prompt]: Epoch 94 / 100: avg data time: 1.47e-01, avg batch time: 0.5492, average train loss: 0.7422
[09/17 05:52:15 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1432, average loss: 0.7506
[09/17 05:52:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 62.50	top5: 100.00	
[09/17 05:52:37 visual_prompt]: 	Test 100/356. loss: 1.499, 0.2172 s / batch. (data: 3.47e-02)max mem: 17.22445 GB 
[09/17 05:52:57 visual_prompt]: 	Test 200/356. loss: 1.589, 0.1979 s / batch. (data: 6.45e-03)max mem: 17.22445 GB 
[09/17 05:53:16 visual_prompt]: 	Test 300/356. loss: 1.692, 0.1840 s / batch. (data: 1.20e-04)max mem: 17.22445 GB 
[09/17 05:53:29 visual_prompt]: Inference (test):avg data time: 7.69e-03, avg batch time: 0.1941, average loss: 1.6992
[09/17 05:53:29 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.39	top5: 97.59	
[09/17 05:53:29 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/17 05:53:39 visual_prompt]: Epoch 95 / 100: avg data time: 1.43e-01, avg batch time: 0.5451, average train loss: 0.7152
[09/17 05:53:44 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.1430, average loss: 0.7284
[09/17 05:53:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 59.00	top5: 100.00	
[09/17 05:54:06 visual_prompt]: 	Test 100/356. loss: 1.585, 0.2261 s / batch. (data: 2.48e-02)max mem: 17.22445 GB 
[09/17 05:54:25 visual_prompt]: 	Test 200/356. loss: 1.735, 0.1896 s / batch. (data: 1.41e-04)max mem: 17.22445 GB 
[09/17 05:54:45 visual_prompt]: 	Test 300/356. loss: 1.723, 0.2002 s / batch. (data: 1.69e-02)max mem: 17.22445 GB 
[09/17 05:54:58 visual_prompt]: Inference (test):avg data time: 8.55e-03, avg batch time: 0.1954, average loss: 1.7659
[09/17 05:54:58 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.79	top5: 97.53	
[09/17 05:54:58 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/17 05:55:08 visual_prompt]: Epoch 96 / 100: avg data time: 1.41e-01, avg batch time: 0.5645, average train loss: 0.6787
[09/17 05:55:12 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1432, average loss: 0.6747
[09/17 05:55:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 69.50	top5: 100.00	
[09/17 05:55:34 visual_prompt]: 	Test 100/356. loss: 1.590, 0.2080 s / batch. (data: 2.54e-02)max mem: 17.22445 GB 
[09/17 05:55:54 visual_prompt]: 	Test 200/356. loss: 1.903, 0.1906 s / batch. (data: 7.53e-03)max mem: 17.22445 GB 
[09/17 05:56:14 visual_prompt]: 	Test 300/356. loss: 1.814, 0.1855 s / batch. (data: 1.47e-04)max mem: 17.22445 GB 
[09/17 05:56:26 visual_prompt]: Inference (test):avg data time: 8.37e-03, avg batch time: 0.1942, average loss: 1.8100
[09/17 05:56:26 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.38	top5: 97.42	
[09/17 05:56:26 visual_prompt]: Best epoch 96: best metric: 0.695
[09/17 05:56:26 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/17 05:56:37 visual_prompt]: Epoch 97 / 100: avg data time: 1.37e-01, avg batch time: 0.5421, average train loss: 0.6604
[09/17 05:56:41 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.1430, average loss: 0.6977
[09/17 05:56:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 63.50	top5: 100.00	
[09/17 05:57:03 visual_prompt]: 	Test 100/356. loss: 1.613, 0.2054 s / batch. (data: 1.49e-02)max mem: 17.22445 GB 
[09/17 05:57:22 visual_prompt]: 	Test 200/356. loss: 1.935, 0.2011 s / batch. (data: 1.71e-02)max mem: 17.22445 GB 
[09/17 05:57:42 visual_prompt]: 	Test 300/356. loss: 1.808, 0.1965 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 05:57:55 visual_prompt]: Inference (test):avg data time: 8.10e-03, avg batch time: 0.1954, average loss: 1.8457
[09/17 05:57:55 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.47	top5: 97.40	
[09/17 05:57:55 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/17 05:58:05 visual_prompt]: Epoch 98 / 100: avg data time: 1.52e-01, avg batch time: 0.5545, average train loss: 0.6595
[09/17 05:58:10 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1431, average loss: 0.6795
[09/17 05:58:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 66.00	top5: 100.00	
[09/17 05:58:31 visual_prompt]: 	Test 100/356. loss: 1.605, 0.1951 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/17 05:58:51 visual_prompt]: 	Test 200/356. loss: 1.927, 0.1961 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/17 05:59:10 visual_prompt]: 	Test 300/356. loss: 1.866, 0.1889 s / batch. (data: 1.40e-04)max mem: 17.22445 GB 
[09/17 05:59:23 visual_prompt]: Inference (test):avg data time: 8.21e-03, avg batch time: 0.1941, average loss: 1.8614
[09/17 05:59:23 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.12	top5: 97.48	
[09/17 05:59:23 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/17 05:59:33 visual_prompt]: Epoch 99 / 100: avg data time: 1.46e-01, avg batch time: 0.5494, average train loss: 0.6516
[09/17 05:59:38 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.1431, average loss: 0.7007
[09/17 05:59:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 61.50	top5: 100.00	
[09/17 05:59:59 visual_prompt]: 	Test 100/356. loss: 1.662, 0.1853 s / batch. (data: 1.35e-04)max mem: 17.22445 GB 
[09/17 06:00:19 visual_prompt]: 	Test 200/356. loss: 1.918, 0.1838 s / batch. (data: 9.80e-05)max mem: 17.22445 GB 
[09/17 06:00:38 visual_prompt]: 	Test 300/356. loss: 1.906, 0.2237 s / batch. (data: 4.07e-02)max mem: 17.22445 GB 
[09/17 06:00:51 visual_prompt]: Inference (test):avg data time: 6.65e-03, avg batch time: 0.1946, average loss: 1.9035
[09/17 06:00:51 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.13	top5: 97.50	
[09/17 06:00:51 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/17 06:01:02 visual_prompt]: Epoch 100 / 100: avg data time: 1.45e-01, avg batch time: 0.5456, average train loss: 0.6435
[09/17 06:01:06 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1433, average loss: 0.6968
[09/17 06:01:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 61.50	top5: 100.00	
[09/17 06:01:28 visual_prompt]: 	Test 100/356. loss: 1.662, 0.1905 s / batch. (data: 1.15e-04)max mem: 17.22445 GB 
[09/17 06:01:48 visual_prompt]: 	Test 200/356. loss: 1.923, 0.1839 s / batch. (data: 1.10e-04)max mem: 17.22445 GB 
[09/17 06:02:08 visual_prompt]: 	Test 300/356. loss: 1.892, 0.1841 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/17 06:02:20 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1972, average loss: 1.8947
[09/17 06:02:21 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.27	top5: 97.52	
[09/17 06:02:37 visual_prompt]: Rank of current process: 0. World size: 1
[09/17 06:02:37 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/17 06:02:37 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed800'], train_type='')
[09/17 06:02:37 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/17 06:02:37 visual_prompt]: Training with config:
[09/17 06:02:37 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-dmlab',
          'NO_TEST': False,
          'NUMBER_CLASSES': 6,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed800/vtab-dmlab/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/17 06:02:37 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-17 06:02:37.143367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-17 06:02:37.319408: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-17 06:02:38.333305: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 06:02:38.333409: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 06:02:38.333419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-17 06:02:40.585016: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 06:02:40.585137: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 06:02:40.585152: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/17 06:02:40 visual_prompt]: Constructing vtab-dmlab dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
2023-09-17 06:02:40.695713: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dmlab for split train[:800]+validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/17 06:02:43 visual_prompt]: Number of images: 1000
[09/17 06:02:43 visual_prompt]: Number of classes: 6 / 6
[09/17 06:02:43 visual_prompt]: Loading validation data...
[09/17 06:02:43 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/17 06:02:43 visual_prompt]: Number of images: 200
[09/17 06:02:43 visual_prompt]: Number of classes: 6 / 6
[09/17 06:02:43 visual_prompt]: Loading test data...
[09/17 06:02:43 visual_prompt]: Constructing vtab-dmlab dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset dmlab for split test, from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/17 06:03:24 visual_prompt]: Number of images: 22735
[09/17 06:03:24 visual_prompt]: Number of classes: 6 / 6
[09/17 06:03:24 visual_prompt]: Constructing models...
[09/17 06:03:27 visual_prompt]: Total Parameters: 86724870	 Gradient Parameters: 926214
[09/17 06:03:27 visual_prompt]: tuned percent:1.068
[09/17 06:03:29 visual_prompt]: Device used for model: 0
[09/17 06:03:29 visual_prompt]: Setting up Evalutator...
[09/17 06:03:29 visual_prompt]: Setting up Trainer...
[09/17 06:03:29 visual_prompt]: 	Setting up the optimizer...
[09/17 06:03:29 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/17 06:03:41 visual_prompt]: Epoch 1 / 100: avg data time: 1.38e-01, avg batch time: 0.6212, average train loss: 2.2438
[09/17 06:03:45 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1425, average loss: 2.1841
[09/17 06:03:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 88.50	
[09/17 06:04:06 visual_prompt]: 	Test 100/356. loss: 2.138, 0.1946 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 06:04:25 visual_prompt]: 	Test 200/356. loss: 2.373, 0.1830 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/17 06:04:45 visual_prompt]: 	Test 300/356. loss: 2.297, 0.1830 s / batch. (data: 1.18e-04)max mem: 17.22445 GB 
[09/17 06:04:57 visual_prompt]: Inference (test):avg data time: 7.74e-03, avg batch time: 0.1926, average loss: 2.2251
[09/17 06:04:57 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.18	top5: 86.41	
[09/17 06:04:57 visual_prompt]: Best epoch 1: best metric: 0.170
[09/17 06:04:57 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/17 06:05:07 visual_prompt]: Epoch 2 / 100: avg data time: 1.35e-01, avg batch time: 0.5366, average train loss: 4.7452
[09/17 06:05:11 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1430, average loss: 3.5600
[09/17 06:05:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/17 06:05:33 visual_prompt]: 	Test 100/356. loss: 3.527, 0.1954 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/17 06:05:53 visual_prompt]: 	Test 200/356. loss: 2.861, 0.1980 s / batch. (data: 1.54e-02)max mem: 17.22445 GB 
[09/17 06:06:12 visual_prompt]: 	Test 300/356. loss: 2.883, 0.1973 s / batch. (data: 1.55e-04)max mem: 17.22445 GB 
[09/17 06:06:25 visual_prompt]: Inference (test):avg data time: 7.32e-03, avg batch time: 0.1944, average loss: 3.3028
[09/17 06:06:25 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.34	top5: 85.40	
[09/17 06:06:25 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/17 06:06:35 visual_prompt]: Epoch 3 / 100: avg data time: 1.40e-01, avg batch time: 0.5482, average train loss: 2.5806
[09/17 06:06:39 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1431, average loss: 2.0318
[09/17 06:06:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/17 06:07:01 visual_prompt]: 	Test 100/356. loss: 1.948, 0.1833 s / batch. (data: 1.29e-04)max mem: 17.22445 GB 
[09/17 06:07:20 visual_prompt]: 	Test 200/356. loss: 1.795, 0.1839 s / batch. (data: 1.35e-04)max mem: 17.22445 GB 
[09/17 06:07:39 visual_prompt]: 	Test 300/356. loss: 1.852, 0.1836 s / batch. (data: 1.21e-04)max mem: 17.22445 GB 
[09/17 06:07:52 visual_prompt]: Inference (test):avg data time: 7.75e-03, avg batch time: 0.1933, average loss: 1.9337
[09/17 06:07:52 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 85.40	
[09/17 06:07:52 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/17 06:08:02 visual_prompt]: Epoch 4 / 100: avg data time: 1.44e-01, avg batch time: 0.5463, average train loss: 2.1386
[09/17 06:08:06 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1431, average loss: 2.5975
[09/17 06:08:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 85.50	
[09/17 06:08:28 visual_prompt]: 	Test 100/356. loss: 2.619, 0.2229 s / batch. (data: 1.53e-02)max mem: 17.22445 GB 
[09/17 06:08:47 visual_prompt]: 	Test 200/356. loss: 2.220, 0.1967 s / batch. (data: 1.31e-02)max mem: 17.22445 GB 
[09/17 06:09:07 visual_prompt]: 	Test 300/356. loss: 2.631, 0.1839 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/17 06:09:20 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1950, average loss: 2.4977
[09/17 06:09:20 visual_prompt]: Classification results with test_vtab-dmlab: top1: 23.31	top5: 84.67	
[09/17 06:09:20 visual_prompt]: Best epoch 4: best metric: 0.195
[09/17 06:09:20 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/17 06:09:30 visual_prompt]: Epoch 5 / 100: avg data time: 1.32e-01, avg batch time: 0.5594, average train loss: 2.2166
[09/17 06:09:34 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.1511, average loss: 2.6304
[09/17 06:09:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/17 06:09:56 visual_prompt]: 	Test 100/356. loss: 2.913, 0.2192 s / batch. (data: 3.65e-02)max mem: 17.22445 GB 
[09/17 06:10:15 visual_prompt]: 	Test 200/356. loss: 2.624, 0.2053 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 06:10:35 visual_prompt]: 	Test 300/356. loss: 2.601, 0.1857 s / batch. (data: 1.40e-04)max mem: 17.22445 GB 
[09/17 06:10:47 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1936, average loss: 2.7103
[09/17 06:10:47 visual_prompt]: Classification results with test_vtab-dmlab: top1: 11.58	top5: 85.40	
[09/17 06:10:47 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/17 06:10:57 visual_prompt]: Epoch 6 / 100: avg data time: 1.37e-01, avg batch time: 0.5412, average train loss: 2.8168
[09/17 06:11:01 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1433, average loss: 2.9916
[09/17 06:11:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 85.50	
[09/17 06:11:23 visual_prompt]: 	Test 100/356. loss: 3.093, 0.1837 s / batch. (data: 1.35e-04)max mem: 17.22445 GB 
[09/17 06:11:42 visual_prompt]: 	Test 200/356. loss: 2.500, 0.2001 s / batch. (data: 1.64e-04)max mem: 17.22445 GB 
[09/17 06:12:02 visual_prompt]: 	Test 300/356. loss: 3.185, 0.2075 s / batch. (data: 2.43e-02)max mem: 17.22445 GB 
[09/17 06:12:14 visual_prompt]: Inference (test):avg data time: 7.69e-03, avg batch time: 0.1950, average loss: 2.8976
[09/17 06:12:15 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.97	top5: 84.67	
[09/17 06:12:15 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/17 06:12:24 visual_prompt]: Epoch 7 / 100: avg data time: 1.23e-01, avg batch time: 0.5277, average train loss: 5.0479
[09/17 06:12:29 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1434, average loss: 4.0161
[09/17 06:12:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/17 06:12:50 visual_prompt]: 	Test 100/356. loss: 3.484, 0.1982 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/17 06:13:10 visual_prompt]: 	Test 200/356. loss: 4.104, 0.2010 s / batch. (data: 1.67e-04)max mem: 17.22445 GB 
[09/17 06:13:30 visual_prompt]: 	Test 300/356. loss: 3.880, 0.1958 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 06:13:42 visual_prompt]: Inference (test):avg data time: 6.82e-03, avg batch time: 0.1952, average loss: 3.8644
[09/17 06:13:42 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 88.42	
[09/17 06:13:42 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/17 06:13:53 visual_prompt]: Epoch 8 / 100: avg data time: 1.33e-01, avg batch time: 0.5675, average train loss: 6.7248
[09/17 06:13:57 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1432, average loss: 10.2579
[09/17 06:13:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/17 06:14:19 visual_prompt]: 	Test 100/356. loss: 10.713, 0.1960 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/17 06:14:38 visual_prompt]: 	Test 200/356. loss: 11.393, 0.1839 s / batch. (data: 1.12e-04)max mem: 17.22445 GB 
[09/17 06:14:58 visual_prompt]: 	Test 300/356. loss: 11.169, 0.1956 s / batch. (data: 1.22e-02)max mem: 17.22445 GB 
[09/17 06:15:10 visual_prompt]: Inference (test):avg data time: 8.04e-03, avg batch time: 0.1952, average loss: 10.8145
[09/17 06:15:10 visual_prompt]: Classification results with test_vtab-dmlab: top1: 14.60	top5: 82.27	
[09/17 06:15:10 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/17 06:15:21 visual_prompt]: Epoch 9 / 100: avg data time: 1.37e-01, avg batch time: 0.5445, average train loss: 6.3132
[09/17 06:15:25 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1438, average loss: 3.8694
[09/17 06:15:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 83.50	
[09/17 06:15:46 visual_prompt]: 	Test 100/356. loss: 3.216, 0.2076 s / batch. (data: 2.48e-02)max mem: 17.22445 GB 
[09/17 06:16:06 visual_prompt]: 	Test 200/356. loss: 3.217, 0.2048 s / batch. (data: 1.54e-02)max mem: 17.22445 GB 
[09/17 06:16:25 visual_prompt]: 	Test 300/356. loss: 3.652, 0.1958 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 06:16:37 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1938, average loss: 3.4920
[09/17 06:16:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 88.45	
[09/17 06:16:37 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/17 06:16:48 visual_prompt]: Epoch 10 / 100: avg data time: 1.38e-01, avg batch time: 0.5449, average train loss: 3.8883
[09/17 06:16:52 visual_prompt]: Inference (val):avg data time: 4.50e-05, avg batch time: 0.1432, average loss: 3.4330
[09/17 06:16:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/17 06:17:13 visual_prompt]: 	Test 100/356. loss: 3.508, 0.2085 s / batch. (data: 2.59e-02)max mem: 17.22445 GB 
[09/17 06:17:32 visual_prompt]: 	Test 200/356. loss: 3.309, 0.1984 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/17 06:17:52 visual_prompt]: 	Test 300/356. loss: 3.792, 0.1959 s / batch. (data: 1.24e-02)max mem: 17.22445 GB 
[09/17 06:18:04 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1932, average loss: 3.4341
[09/17 06:18:04 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.86	top5: 84.67	
[09/17 06:18:04 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/17 06:18:15 visual_prompt]: Epoch 11 / 100: avg data time: 1.30e-01, avg batch time: 0.5464, average train loss: 4.0961
[09/17 06:18:19 visual_prompt]: Inference (val):avg data time: 5.48e-04, avg batch time: 0.2643, average loss: 4.8346
[09/17 06:18:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 83.50	
[09/17 06:18:40 visual_prompt]: 	Test 100/356. loss: 5.467, 0.2080 s / batch. (data: 2.51e-02)max mem: 17.22445 GB 
[09/17 06:19:00 visual_prompt]: 	Test 200/356. loss: 5.560, 0.1954 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 06:19:20 visual_prompt]: 	Test 300/356. loss: 5.304, 0.1835 s / batch. (data: 4.03e-05)max mem: 17.22445 GB 
[09/17 06:19:32 visual_prompt]: Inference (test):avg data time: 7.23e-03, avg batch time: 0.1949, average loss: 5.2002
[09/17 06:19:32 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 84.14	
[09/17 06:19:32 visual_prompt]: Best epoch 11: best metric: 0.205
[09/17 06:19:32 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/17 06:19:42 visual_prompt]: Epoch 12 / 100: avg data time: 1.24e-01, avg batch time: 0.5288, average train loss: 5.4209
[09/17 06:19:46 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1432, average loss: 6.4520
[09/17 06:19:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 85.50	
[09/17 06:20:08 visual_prompt]: 	Test 100/356. loss: 7.369, 0.2104 s / batch. (data: 2.76e-02)max mem: 17.22445 GB 
[09/17 06:20:28 visual_prompt]: 	Test 200/356. loss: 5.267, 0.1917 s / batch. (data: 1.67e-04)max mem: 17.22445 GB 
[09/17 06:20:47 visual_prompt]: 	Test 300/356. loss: 6.067, 0.1877 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/17 06:21:00 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1943, average loss: 6.3749
[09/17 06:21:00 visual_prompt]: Classification results with test_vtab-dmlab: top1: 27.62	top5: 84.67	
[09/17 06:21:00 visual_prompt]: Best epoch 12: best metric: 0.250
[09/17 06:21:00 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/17 06:21:10 visual_prompt]: Epoch 13 / 100: avg data time: 1.38e-01, avg batch time: 0.5434, average train loss: 4.8130
[09/17 06:21:14 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1432, average loss: 7.9516
[09/17 06:21:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 89.50	
[09/17 06:21:36 visual_prompt]: 	Test 100/356. loss: 7.875, 0.1851 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/17 06:21:55 visual_prompt]: 	Test 200/356. loss: 8.795, 0.1998 s / batch. (data: 1.58e-02)max mem: 17.22445 GB 
[09/17 06:22:15 visual_prompt]: 	Test 300/356. loss: 8.825, 0.2019 s / batch. (data: 1.88e-02)max mem: 17.22445 GB 
[09/17 06:22:27 visual_prompt]: Inference (test):avg data time: 7.10e-03, avg batch time: 0.1930, average loss: 8.1846
[09/17 06:22:27 visual_prompt]: Classification results with test_vtab-dmlab: top1: 14.60	top5: 84.66	
[09/17 06:22:27 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/17 06:22:37 visual_prompt]: Epoch 14 / 100: avg data time: 1.19e-01, avg batch time: 0.5278, average train loss: 4.9483
[09/17 06:22:41 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1431, average loss: 5.0003
[09/17 06:22:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/17 06:23:03 visual_prompt]: 	Test 100/356. loss: 4.860, 0.1962 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/17 06:23:22 visual_prompt]: 	Test 200/356. loss: 5.047, 0.1921 s / batch. (data: 1.42e-04)max mem: 17.22445 GB 
[09/17 06:23:42 visual_prompt]: 	Test 300/356. loss: 5.198, 0.1841 s / batch. (data: 1.21e-04)max mem: 17.22445 GB 
[09/17 06:23:55 visual_prompt]: Inference (test):avg data time: 7.67e-03, avg batch time: 0.1953, average loss: 4.9695
[09/17 06:23:55 visual_prompt]: Classification results with test_vtab-dmlab: top1: 14.60	top5: 88.42	
[09/17 06:23:55 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/17 06:24:05 visual_prompt]: Epoch 15 / 100: avg data time: 1.40e-01, avg batch time: 0.5474, average train loss: 5.7579
[09/17 06:24:09 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1432, average loss: 4.3564
[09/17 06:24:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 84.00	
[09/17 06:24:31 visual_prompt]: 	Test 100/356. loss: 4.453, 0.2069 s / batch. (data: 2.46e-02)max mem: 17.22445 GB 
[09/17 06:24:50 visual_prompt]: 	Test 200/356. loss: 4.179, 0.1981 s / batch. (data: 1.52e-02)max mem: 17.22445 GB 
[09/17 06:25:10 visual_prompt]: 	Test 300/356. loss: 4.107, 0.2153 s / batch. (data: 1.47e-02)max mem: 17.22445 GB 
[09/17 06:25:22 visual_prompt]: Inference (test):avg data time: 8.51e-03, avg batch time: 0.1954, average loss: 4.3194
[09/17 06:25:23 visual_prompt]: Classification results with test_vtab-dmlab: top1: 25.26	top5: 85.40	
[09/17 06:25:23 visual_prompt]: Best epoch 15: best metric: 0.305
[09/17 06:25:23 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/17 06:25:33 visual_prompt]: Epoch 16 / 100: avg data time: 1.45e-01, avg batch time: 0.5478, average train loss: 4.1796
[09/17 06:25:37 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1432, average loss: 2.8148
[09/17 06:25:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 84.00	
[09/17 06:25:58 visual_prompt]: 	Test 100/356. loss: 2.741, 0.1834 s / batch. (data: 1.29e-04)max mem: 17.22445 GB 
[09/17 06:26:18 visual_prompt]: 	Test 200/356. loss: 2.461, 0.1896 s / batch. (data: 1.11e-04)max mem: 17.22445 GB 
[09/17 06:26:37 visual_prompt]: 	Test 300/356. loss: 2.330, 0.1908 s / batch. (data: 1.21e-04)max mem: 17.22445 GB 
[09/17 06:26:50 visual_prompt]: Inference (test):avg data time: 8.20e-03, avg batch time: 0.1938, average loss: 2.6990
[09/17 06:26:50 visual_prompt]: Classification results with test_vtab-dmlab: top1: 30.46	top5: 85.40	
[09/17 06:26:50 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/17 06:27:00 visual_prompt]: Epoch 17 / 100: avg data time: 1.36e-01, avg batch time: 0.5377, average train loss: 3.1768
[09/17 06:27:04 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1431, average loss: 3.5925
[09/17 06:27:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 94.00	
[09/17 06:27:26 visual_prompt]: 	Test 100/356. loss: 3.289, 0.2076 s / batch. (data: 2.48e-02)max mem: 17.22445 GB 
[09/17 06:27:45 visual_prompt]: 	Test 200/356. loss: 3.056, 0.1838 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 06:28:04 visual_prompt]: 	Test 300/356. loss: 2.890, 0.2121 s / batch. (data: 1.03e-04)max mem: 17.22445 GB 
[09/17 06:28:17 visual_prompt]: Inference (test):avg data time: 7.38e-03, avg batch time: 0.1936, average loss: 3.3082
[09/17 06:28:17 visual_prompt]: Classification results with test_vtab-dmlab: top1: 33.72	top5: 91.95	
[09/17 06:28:17 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/17 06:28:27 visual_prompt]: Epoch 18 / 100: avg data time: 1.32e-01, avg batch time: 0.5407, average train loss: 2.6072
[09/17 06:28:31 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1432, average loss: 2.0538
[09/17 06:28:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.00	
[09/17 06:28:53 visual_prompt]: 	Test 100/356. loss: 2.466, 0.1834 s / batch. (data: 1.67e-04)max mem: 17.22445 GB 
[09/17 06:29:13 visual_prompt]: 	Test 200/356. loss: 2.029, 0.1892 s / batch. (data: 1.12e-04)max mem: 17.22445 GB 
[09/17 06:29:32 visual_prompt]: 	Test 300/356. loss: 2.115, 0.1997 s / batch. (data: 1.11e-02)max mem: 17.22445 GB 
[09/17 06:29:44 visual_prompt]: Inference (test):avg data time: 7.60e-03, avg batch time: 0.1946, average loss: 2.1552
[09/17 06:29:44 visual_prompt]: Classification results with test_vtab-dmlab: top1: 33.59	top5: 95.80	
[09/17 06:29:44 visual_prompt]: Best epoch 18: best metric: 0.355
[09/17 06:29:44 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/17 06:29:55 visual_prompt]: Epoch 19 / 100: avg data time: 1.40e-01, avg batch time: 0.5433, average train loss: 2.3685
[09/17 06:29:58 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1433, average loss: 1.9017
[09/17 06:29:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 95.00	
[09/17 06:30:21 visual_prompt]: 	Test 100/356. loss: 1.688, 0.2028 s / batch. (data: 1.37e-02)max mem: 17.22445 GB 
[09/17 06:30:40 visual_prompt]: 	Test 200/356. loss: 1.870, 0.1953 s / batch. (data: 1.22e-02)max mem: 17.22445 GB 
[09/17 06:31:00 visual_prompt]: 	Test 300/356. loss: 1.766, 0.1837 s / batch. (data: 1.36e-04)max mem: 17.22445 GB 
[09/17 06:31:13 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1974, average loss: 1.7917
[09/17 06:31:13 visual_prompt]: Classification results with test_vtab-dmlab: top1: 27.31	top5: 96.48	
[09/17 06:31:13 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/17 06:31:23 visual_prompt]: Epoch 20 / 100: avg data time: 1.18e-01, avg batch time: 0.5263, average train loss: 2.2150
[09/17 06:31:27 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1432, average loss: 1.7903
[09/17 06:31:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 98.50	
[09/17 06:31:49 visual_prompt]: 	Test 100/356. loss: 1.777, 0.1831 s / batch. (data: 1.15e-04)max mem: 17.22445 GB 
[09/17 06:32:08 visual_prompt]: 	Test 200/356. loss: 1.962, 0.1991 s / batch. (data: 1.11e-02)max mem: 17.22445 GB 
[09/17 06:32:28 visual_prompt]: 	Test 300/356. loss: 1.865, 0.1847 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/17 06:32:40 visual_prompt]: Inference (test):avg data time: 7.62e-03, avg batch time: 0.1948, average loss: 1.9305
[09/17 06:32:41 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.63	top5: 97.65	
[09/17 06:32:41 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/17 06:32:51 visual_prompt]: Epoch 21 / 100: avg data time: 1.33e-01, avg batch time: 0.5390, average train loss: 1.8109
[09/17 06:32:55 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1430, average loss: 2.6043
[09/17 06:32:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 97.50	
[09/17 06:33:16 visual_prompt]: 	Test 100/356. loss: 2.506, 0.1835 s / batch. (data: 1.28e-04)max mem: 17.22445 GB 
[09/17 06:33:36 visual_prompt]: 	Test 200/356. loss: 3.014, 0.1973 s / batch. (data: 1.46e-04)max mem: 17.22445 GB 
[09/17 06:33:55 visual_prompt]: 	Test 300/356. loss: 2.445, 0.1989 s / batch. (data: 1.54e-02)max mem: 17.22445 GB 
[09/17 06:34:08 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1938, average loss: 2.6238
[09/17 06:34:08 visual_prompt]: Classification results with test_vtab-dmlab: top1: 19.42	top5: 97.66	
[09/17 06:34:08 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/17 06:34:18 visual_prompt]: Epoch 22 / 100: avg data time: 1.37e-01, avg batch time: 0.5401, average train loss: 1.9399
[09/17 06:34:22 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1444, average loss: 1.8250
[09/17 06:34:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/17 06:34:44 visual_prompt]: 	Test 100/356. loss: 2.144, 0.2063 s / batch. (data: 1.32e-02)max mem: 17.22445 GB 
[09/17 06:35:03 visual_prompt]: 	Test 200/356. loss: 2.568, 0.1967 s / batch. (data: 1.33e-02)max mem: 17.22445 GB 
[09/17 06:35:23 visual_prompt]: 	Test 300/356. loss: 2.108, 0.2039 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 06:35:35 visual_prompt]: Inference (test):avg data time: 7.00e-03, avg batch time: 0.1939, average loss: 2.1527
[09/17 06:35:35 visual_prompt]: Classification results with test_vtab-dmlab: top1: 29.68	top5: 95.40	
[09/17 06:35:35 visual_prompt]: Best epoch 22: best metric: 0.375
[09/17 06:35:35 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/17 06:35:45 visual_prompt]: Epoch 23 / 100: avg data time: 1.24e-01, avg batch time: 0.5314, average train loss: 1.7739
[09/17 06:35:49 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1428, average loss: 2.1172
[09/17 06:35:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 88.00	
[09/17 06:36:11 visual_prompt]: 	Test 100/356. loss: 2.402, 0.1833 s / batch. (data: 3.55e-05)max mem: 17.22445 GB 
[09/17 06:36:31 visual_prompt]: 	Test 200/356. loss: 2.371, 0.2113 s / batch. (data: 2.81e-02)max mem: 17.22445 GB 
[09/17 06:36:50 visual_prompt]: 	Test 300/356. loss: 2.187, 0.1843 s / batch. (data: 1.33e-04)max mem: 17.22445 GB 
[09/17 06:37:03 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1953, average loss: 2.3003
[09/17 06:37:03 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.11	top5: 90.21	
[09/17 06:37:03 visual_prompt]: Best epoch 23: best metric: 0.435
[09/17 06:37:03 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/17 06:37:13 visual_prompt]: Epoch 24 / 100: avg data time: 1.32e-01, avg batch time: 0.5367, average train loss: 1.6878
[09/17 06:37:17 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1432, average loss: 1.3131
[09/17 06:37:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 100.00	
[09/17 06:37:38 visual_prompt]: 	Test 100/356. loss: 1.454, 0.1830 s / batch. (data: 9.68e-05)max mem: 17.22445 GB 
[09/17 06:37:58 visual_prompt]: 	Test 200/356. loss: 1.647, 0.1902 s / batch. (data: 1.77e-04)max mem: 17.22445 GB 
[09/17 06:38:17 visual_prompt]: 	Test 300/356. loss: 1.389, 0.1939 s / batch. (data: 1.03e-02)max mem: 17.22445 GB 
[09/17 06:38:30 visual_prompt]: Inference (test):avg data time: 8.07e-03, avg batch time: 0.1936, average loss: 1.4702
[09/17 06:38:30 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.11	top5: 98.13	
[09/17 06:38:30 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/17 06:38:40 visual_prompt]: Epoch 25 / 100: avg data time: 1.34e-01, avg batch time: 0.5357, average train loss: 1.4513
[09/17 06:38:44 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1429, average loss: 1.6369
[09/17 06:38:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.00	
[09/17 06:39:06 visual_prompt]: 	Test 100/356. loss: 1.965, 0.2123 s / batch. (data: 2.56e-02)max mem: 17.22445 GB 
[09/17 06:39:25 visual_prompt]: 	Test 200/356. loss: 2.364, 0.1960 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 06:39:44 visual_prompt]: 	Test 300/356. loss: 1.974, 0.1918 s / batch. (data: 1.61e-04)max mem: 17.22445 GB 
[09/17 06:39:57 visual_prompt]: Inference (test):avg data time: 8.15e-03, avg batch time: 0.1939, average loss: 1.9412
[09/17 06:39:57 visual_prompt]: Classification results with test_vtab-dmlab: top1: 25.48	top5: 93.33	
[09/17 06:39:57 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/17 06:40:07 visual_prompt]: Epoch 26 / 100: avg data time: 1.38e-01, avg batch time: 0.5412, average train loss: 1.5632
[09/17 06:40:11 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1435, average loss: 1.4017
[09/17 06:40:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 99.50	
[09/17 06:40:33 visual_prompt]: 	Test 100/356. loss: 1.539, 0.1923 s / batch. (data: 9.24e-03)max mem: 17.22445 GB 
[09/17 06:40:52 visual_prompt]: 	Test 200/356. loss: 1.656, 0.1938 s / batch. (data: 1.33e-04)max mem: 17.22445 GB 
[09/17 06:41:12 visual_prompt]: 	Test 300/356. loss: 1.456, 0.1835 s / batch. (data: 1.02e-04)max mem: 17.22445 GB 
[09/17 06:41:27 visual_prompt]: Inference (test):avg data time: 8.18e-03, avg batch time: 0.2023, average loss: 1.6377
[09/17 06:41:27 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.20	top5: 98.19	
[09/17 06:41:27 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/17 06:41:38 visual_prompt]: Epoch 27 / 100: avg data time: 1.53e-01, avg batch time: 0.5810, average train loss: 1.4189
[09/17 06:41:42 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1435, average loss: 1.3642
[09/17 06:41:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 99.50	
[09/17 06:42:04 visual_prompt]: 	Test 100/356. loss: 1.617, 0.1844 s / batch. (data: 1.31e-04)max mem: 17.22445 GB 
[09/17 06:42:31 visual_prompt]: 	Test 200/356. loss: 1.659, 0.1827 s / batch. (data: 1.85e-04)max mem: 17.22445 GB 
[09/17 06:42:51 visual_prompt]: 	Test 300/356. loss: 1.489, 0.1837 s / batch. (data: 9.47e-05)max mem: 17.22445 GB 
[09/17 06:43:03 visual_prompt]: Inference (test):avg data time: 7.81e-03, avg batch time: 0.2158, average loss: 1.6309
[09/17 06:43:03 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.22	top5: 98.30	
[09/17 06:43:03 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/17 06:43:13 visual_prompt]: Epoch 28 / 100: avg data time: 1.33e-01, avg batch time: 0.5367, average train loss: 1.3620
[09/17 06:43:17 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1434, average loss: 1.8738
[09/17 06:43:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 99.00	
[09/17 06:43:39 visual_prompt]: 	Test 100/356. loss: 2.088, 0.1850 s / batch. (data: 1.60e-04)max mem: 17.22445 GB 
[09/17 06:43:58 visual_prompt]: 	Test 200/356. loss: 1.799, 0.2069 s / batch. (data: 2.44e-02)max mem: 17.22445 GB 
[09/17 06:44:18 visual_prompt]: 	Test 300/356. loss: 1.779, 0.1995 s / batch. (data: 1.61e-02)max mem: 17.22445 GB 
[09/17 06:44:30 visual_prompt]: Inference (test):avg data time: 7.23e-03, avg batch time: 0.1932, average loss: 2.0392
[09/17 06:44:30 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.08	top5: 97.91	
[09/17 06:44:30 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/17 06:44:40 visual_prompt]: Epoch 29 / 100: avg data time: 1.39e-01, avg batch time: 0.5415, average train loss: 1.7389
[09/17 06:44:44 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1433, average loss: 1.9017
[09/17 06:44:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 89.50	
[09/17 06:45:06 visual_prompt]: 	Test 100/356. loss: 1.919, 0.1836 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/17 06:45:25 visual_prompt]: 	Test 200/356. loss: 2.412, 0.1838 s / batch. (data: 1.12e-04)max mem: 17.22445 GB 
[09/17 06:45:44 visual_prompt]: 	Test 300/356. loss: 1.916, 0.2036 s / batch. (data: 2.03e-02)max mem: 17.22445 GB 
[09/17 06:45:57 visual_prompt]: Inference (test):avg data time: 7.30e-03, avg batch time: 0.1931, average loss: 2.0355
[09/17 06:45:57 visual_prompt]: Classification results with test_vtab-dmlab: top1: 32.74	top5: 89.43	
[09/17 06:45:57 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/17 06:46:07 visual_prompt]: Epoch 30 / 100: avg data time: 1.32e-01, avg batch time: 0.5340, average train loss: 1.4523
[09/17 06:46:11 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1432, average loss: 1.9819
[09/17 06:46:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 99.00	
[09/17 06:46:33 visual_prompt]: 	Test 100/356. loss: 1.828, 0.1851 s / batch. (data: 1.41e-04)max mem: 17.22445 GB 
[09/17 06:47:05 visual_prompt]: 	Test 200/356. loss: 2.073, 0.2446 s / batch. (data: 2.53e-04)max mem: 17.22445 GB 
[09/17 06:47:25 visual_prompt]: 	Test 300/356. loss: 1.789, 0.1832 s / batch. (data: 2.03e-04)max mem: 17.22445 GB 
[09/17 06:47:37 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.2304, average loss: 2.0388
[09/17 06:47:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.61	top5: 97.49	
[09/17 06:47:37 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/17 06:47:47 visual_prompt]: Epoch 31 / 100: avg data time: 1.37e-01, avg batch time: 0.5420, average train loss: 1.6282
[09/17 06:47:52 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.1432, average loss: 1.4400
[09/17 06:47:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 97.50	
[09/17 06:48:14 visual_prompt]: 	Test 100/356. loss: 1.667, 0.1959 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 06:48:51 visual_prompt]: 	Test 200/356. loss: 1.476, 0.1827 s / batch. (data: 1.93e-04)max mem: 17.22445 GB 
[09/17 06:49:10 visual_prompt]: 	Test 300/356. loss: 1.687, 0.1832 s / batch. (data: 1.40e-04)max mem: 17.22445 GB 
[09/17 06:49:22 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.2034, average loss: 1.6635
[09/17 06:49:22 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.45	top5: 96.83	
[09/17 06:49:22 visual_prompt]: Best epoch 31: best metric: 0.450
[09/17 06:49:22 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/17 06:49:32 visual_prompt]: Epoch 32 / 100: avg data time: 1.32e-01, avg batch time: 0.5325, average train loss: 1.6434
[09/17 06:49:36 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1433, average loss: 1.9310
[09/17 06:49:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.50	
[09/17 06:49:57 visual_prompt]: 	Test 100/356. loss: 1.905, 0.2137 s / batch. (data: 3.09e-02)max mem: 17.22445 GB 
[09/17 06:50:17 visual_prompt]: 	Test 200/356. loss: 1.686, 0.1833 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/17 06:50:36 visual_prompt]: 	Test 300/356. loss: 1.695, 0.1850 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/17 06:50:49 visual_prompt]: Inference (test):avg data time: 8.07e-03, avg batch time: 0.1935, average loss: 1.9718
[09/17 06:50:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.63	top5: 94.79	
[09/17 06:50:49 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/17 06:50:58 visual_prompt]: Epoch 33 / 100: avg data time: 1.29e-01, avg batch time: 0.5320, average train loss: 1.5091
[09/17 06:51:02 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1430, average loss: 1.3934
[09/17 06:51:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/17 06:51:46 visual_prompt]: 	Test 100/356. loss: 1.582, 20.0676 s / batch. (data: 9.06e-02)max mem: 17.22445 GB 
[09/17 06:52:05 visual_prompt]: 	Test 200/356. loss: 2.300, 0.1954 s / batch. (data: 1.53e-04)max mem: 17.22445 GB 
[09/17 06:52:25 visual_prompt]: 	Test 300/356. loss: 1.715, 0.1916 s / batch. (data: 8.80e-03)max mem: 17.22445 GB 
[09/17 06:52:37 visual_prompt]: Inference (test):avg data time: 8.23e-03, avg batch time: 0.2557, average loss: 1.7652
[09/17 06:52:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 30.91	top5: 94.65	
[09/17 06:52:37 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/17 06:52:47 visual_prompt]: Epoch 34 / 100: avg data time: 1.30e-01, avg batch time: 0.5329, average train loss: 1.5506
[09/17 06:52:51 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1432, average loss: 1.5411
[09/17 06:52:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 95.50	
[09/17 06:53:12 visual_prompt]: 	Test 100/356. loss: 1.449, 0.1837 s / batch. (data: 1.41e-04)max mem: 17.22445 GB 
[09/17 06:53:32 visual_prompt]: 	Test 200/356. loss: 1.612, 0.1958 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 06:53:55 visual_prompt]: 	Test 300/356. loss: 1.499, 0.1954 s / batch. (data: 1.24e-02)max mem: 17.22445 GB 
[09/17 06:54:08 visual_prompt]: Inference (test):avg data time: 8.02e-03, avg batch time: 0.2053, average loss: 1.5900
[09/17 06:54:08 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.30	top5: 95.39	
[09/17 06:54:08 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/17 06:54:18 visual_prompt]: Epoch 35 / 100: avg data time: 1.33e-01, avg batch time: 0.5347, average train loss: 1.3672
[09/17 06:54:21 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1430, average loss: 1.5529
[09/17 06:54:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 99.00	
[09/17 06:54:43 visual_prompt]: 	Test 100/356. loss: 1.720, 0.1958 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/17 06:55:02 visual_prompt]: 	Test 200/356. loss: 1.713, 0.1839 s / batch. (data: 1.23e-04)max mem: 17.22445 GB 
[09/17 06:55:22 visual_prompt]: 	Test 300/356. loss: 1.578, 0.1839 s / batch. (data: 1.16e-04)max mem: 17.22445 GB 
[09/17 06:55:34 visual_prompt]: Inference (test):avg data time: 8.30e-03, avg batch time: 0.1939, average loss: 1.7235
[09/17 06:55:34 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.11	top5: 96.86	
[09/17 06:55:34 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/17 06:55:44 visual_prompt]: Epoch 36 / 100: avg data time: 1.15e-01, avg batch time: 0.5203, average train loss: 1.6459
[09/17 06:55:48 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1431, average loss: 2.0023
[09/17 06:55:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/17 06:56:09 visual_prompt]: 	Test 100/356. loss: 2.268, 0.1897 s / batch. (data: 1.35e-04)max mem: 17.22445 GB 
[09/17 06:56:28 visual_prompt]: 	Test 200/356. loss: 1.919, 0.1841 s / batch. (data: 1.50e-04)max mem: 17.22445 GB 
[09/17 06:56:57 visual_prompt]: 	Test 300/356. loss: 1.859, 0.1971 s / batch. (data: 1.43e-02)max mem: 17.22445 GB 
[09/17 06:57:10 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.2200, average loss: 2.1957
[09/17 06:57:10 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.48	top5: 98.27	
[09/17 06:57:10 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/17 06:57:20 visual_prompt]: Epoch 37 / 100: avg data time: 1.26e-01, avg batch time: 0.5294, average train loss: 2.8494
[09/17 06:57:23 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1429, average loss: 5.3814
[09/17 06:57:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 84.00	
[09/17 06:57:45 visual_prompt]: 	Test 100/356. loss: 5.942, 0.1951 s / batch. (data: 1.22e-02)max mem: 17.22445 GB 
[09/17 06:58:04 visual_prompt]: 	Test 200/356. loss: 4.429, 0.1955 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 06:58:24 visual_prompt]: 	Test 300/356. loss: 4.267, 0.1972 s / batch. (data: 1.39e-02)max mem: 17.22445 GB 
[09/17 06:58:36 visual_prompt]: Inference (test):avg data time: 8.18e-03, avg batch time: 0.1934, average loss: 5.3127
[09/17 06:58:36 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.52	top5: 85.40	
[09/17 06:58:36 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/17 06:58:45 visual_prompt]: Epoch 38 / 100: avg data time: 1.20e-01, avg batch time: 0.5238, average train loss: 5.2983
[09/17 06:58:49 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1431, average loss: 5.1785
[09/17 06:58:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 90.00	
[09/17 06:59:11 visual_prompt]: 	Test 100/356. loss: 4.979, 0.2044 s / batch. (data: 1.49e-02)max mem: 17.22445 GB 
[09/17 06:59:34 visual_prompt]: 	Test 200/356. loss: 5.154, 0.1837 s / batch. (data: 1.47e-04)max mem: 17.22445 GB 
[09/17 06:59:54 visual_prompt]: 	Test 300/356. loss: 5.657, 0.1841 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/17 07:00:06 visual_prompt]: Inference (test):avg data time: 7.25e-03, avg batch time: 0.2058, average loss: 5.2272
[09/17 07:00:06 visual_prompt]: Classification results with test_vtab-dmlab: top1: 27.11	top5: 89.14	
[09/17 07:00:06 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/17 07:00:16 visual_prompt]: Epoch 39 / 100: avg data time: 1.26e-01, avg batch time: 0.5300, average train loss: 4.1719
[09/17 07:00:20 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1430, average loss: 3.4612
[09/17 07:00:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.00	top5: 88.50	
[09/17 07:00:41 visual_prompt]: 	Test 100/356. loss: 3.431, 0.1836 s / batch. (data: 1.57e-04)max mem: 17.22445 GB 
[09/17 07:01:01 visual_prompt]: 	Test 200/356. loss: 4.782, 0.2001 s / batch. (data: 1.66e-02)max mem: 17.22445 GB 
[09/17 07:01:20 visual_prompt]: 	Test 300/356. loss: 3.684, 0.1963 s / batch. (data: 1.24e-02)max mem: 17.22445 GB 
[09/17 07:01:33 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1949, average loss: 3.9057
[09/17 07:01:33 visual_prompt]: Classification results with test_vtab-dmlab: top1: 16.68	top5: 82.33	
[09/17 07:01:33 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/17 07:01:42 visual_prompt]: Epoch 40 / 100: avg data time: 1.23e-01, avg batch time: 0.5259, average train loss: 1.9271
[09/17 07:01:46 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1439, average loss: 1.4019
[09/17 07:01:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.50	top5: 98.50	
[09/17 07:02:08 visual_prompt]: 	Test 100/356. loss: 1.615, 0.1963 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 07:02:32 visual_prompt]: 	Test 200/356. loss: 1.855, 0.1836 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/17 07:02:52 visual_prompt]: 	Test 300/356. loss: 1.814, 0.1931 s / batch. (data: 1.21e-04)max mem: 17.22445 GB 
[09/17 07:03:04 visual_prompt]: Inference (test):avg data time: 7.77e-03, avg batch time: 0.2088, average loss: 1.7323
[09/17 07:03:04 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.27	top5: 96.63	
[09/17 07:03:04 visual_prompt]: Best epoch 40: best metric: 0.465
[09/17 07:03:04 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/17 07:03:14 visual_prompt]: Epoch 41 / 100: avg data time: 1.22e-01, avg batch time: 0.5279, average train loss: 1.6270
[09/17 07:03:18 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1433, average loss: 2.1395
[09/17 07:03:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 97.50	
[09/17 07:03:40 visual_prompt]: 	Test 100/356. loss: 2.411, 0.2026 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 07:03:59 visual_prompt]: 	Test 200/356. loss: 2.218, 0.2078 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/17 07:04:19 visual_prompt]: 	Test 300/356. loss: 2.024, 0.1841 s / batch. (data: 1.55e-04)max mem: 17.22445 GB 
[09/17 07:04:31 visual_prompt]: Inference (test):avg data time: 8.18e-03, avg batch time: 0.1946, average loss: 2.4080
[09/17 07:04:31 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.21	top5: 95.28	
[09/17 07:04:31 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/17 07:04:41 visual_prompt]: Epoch 42 / 100: avg data time: 1.30e-01, avg batch time: 0.5318, average train loss: 1.8852
[09/17 07:04:44 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1431, average loss: 1.4586
[09/17 07:04:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/17 07:05:06 visual_prompt]: 	Test 100/356. loss: 1.732, 0.1961 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/17 07:05:26 visual_prompt]: 	Test 200/356. loss: 1.858, 0.2121 s / batch. (data: 2.90e-02)max mem: 17.22445 GB 
[09/17 07:05:54 visual_prompt]: 	Test 300/356. loss: 1.788, 0.1863 s / batch. (data: 1.62e-04)max mem: 17.22445 GB 
[09/17 07:06:06 visual_prompt]: Inference (test):avg data time: 7.74e-03, avg batch time: 0.2192, average loss: 1.7795
[09/17 07:06:06 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.30	top5: 97.77	
[09/17 07:06:06 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/17 07:06:16 visual_prompt]: Epoch 43 / 100: avg data time: 1.13e-01, avg batch time: 0.5174, average train loss: 1.7163
[09/17 07:06:20 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1431, average loss: 1.7002
[09/17 07:06:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 95.50	
[09/17 07:06:41 visual_prompt]: 	Test 100/356. loss: 2.027, 0.2075 s / batch. (data: 2.50e-02)max mem: 17.22445 GB 
[09/17 07:07:00 visual_prompt]: 	Test 200/356. loss: 2.414, 0.1961 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/17 07:07:20 visual_prompt]: 	Test 300/356. loss: 1.709, 0.1838 s / batch. (data: 1.84e-04)max mem: 17.22445 GB 
[09/17 07:07:32 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1930, average loss: 2.0446
[09/17 07:07:32 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.03	top5: 94.60	
[09/17 07:07:32 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/17 07:07:42 visual_prompt]: Epoch 44 / 100: avg data time: 1.32e-01, avg batch time: 0.5343, average train loss: 1.7012
[09/17 07:07:45 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1434, average loss: 1.6000
[09/17 07:07:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 99.00	
[09/17 07:08:07 visual_prompt]: 	Test 100/356. loss: 1.997, 0.1953 s / batch. (data: 1.19e-02)max mem: 17.22445 GB 
[09/17 07:08:27 visual_prompt]: 	Test 200/356. loss: 2.371, 0.1840 s / batch. (data: 1.38e-04)max mem: 17.22445 GB 
[09/17 07:08:46 visual_prompt]: 	Test 300/356. loss: 1.921, 0.2109 s / batch. (data: 1.46e-02)max mem: 17.22445 GB 
[09/17 07:08:58 visual_prompt]: Inference (test):avg data time: 8.09e-03, avg batch time: 0.1943, average loss: 2.1170
[09/17 07:08:58 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.92	top5: 96.53	
[09/17 07:08:58 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/17 07:09:08 visual_prompt]: Epoch 45 / 100: avg data time: 1.43e-01, avg batch time: 0.5447, average train loss: 1.5179
[09/17 07:09:12 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1430, average loss: 1.1511
[09/17 07:09:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 99.50	
[09/17 07:09:33 visual_prompt]: 	Test 100/356. loss: 1.501, 0.1835 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/17 07:09:53 visual_prompt]: 	Test 200/356. loss: 1.394, 0.2018 s / batch. (data: 1.91e-02)max mem: 17.22445 GB 
[09/17 07:10:12 visual_prompt]: 	Test 300/356. loss: 1.458, 0.1840 s / batch. (data: 1.66e-04)max mem: 17.22445 GB 
[09/17 07:10:24 visual_prompt]: Inference (test):avg data time: 7.09e-03, avg batch time: 0.1926, average loss: 1.6027
[09/17 07:10:24 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.68	top5: 97.71	
[09/17 07:10:24 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/17 07:10:34 visual_prompt]: Epoch 46 / 100: avg data time: 1.35e-01, avg batch time: 0.5389, average train loss: 1.6003
[09/17 07:10:38 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1432, average loss: 1.4411
[09/17 07:10:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 99.50	
[09/17 07:10:59 visual_prompt]: 	Test 100/356. loss: 1.874, 0.1985 s / batch. (data: 1.56e-02)max mem: 17.22445 GB 
[09/17 07:11:19 visual_prompt]: 	Test 200/356. loss: 1.542, 0.1844 s / batch. (data: 1.05e-04)max mem: 17.22445 GB 
[09/17 07:11:39 visual_prompt]: 	Test 300/356. loss: 1.553, 0.1839 s / batch. (data: 1.53e-04)max mem: 17.22445 GB 
[09/17 07:11:51 visual_prompt]: Inference (test):avg data time: 8.44e-03, avg batch time: 0.1957, average loss: 1.7696
[09/17 07:11:51 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.49	top5: 97.99	
[09/17 07:11:51 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/17 07:12:02 visual_prompt]: Epoch 47 / 100: avg data time: 1.28e-01, avg batch time: 0.5727, average train loss: 1.3771
[09/17 07:12:06 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1431, average loss: 1.1979
[09/17 07:12:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 100.00	
[09/17 07:12:27 visual_prompt]: 	Test 100/356. loss: 1.341, 0.1840 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/17 07:12:47 visual_prompt]: 	Test 200/356. loss: 1.555, 0.1839 s / batch. (data: 1.60e-04)max mem: 17.22445 GB 
[09/17 07:13:06 visual_prompt]: 	Test 300/356. loss: 1.377, 0.1962 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 07:13:18 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1943, average loss: 1.6288
[09/17 07:13:19 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.76	top5: 97.98	
[09/17 07:13:19 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/17 07:13:28 visual_prompt]: Epoch 48 / 100: avg data time: 1.15e-01, avg batch time: 0.5201, average train loss: 1.5632
[09/17 07:13:32 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1429, average loss: 1.7361
[09/17 07:13:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 99.50	
[09/17 07:13:53 visual_prompt]: 	Test 100/356. loss: 1.914, 0.2076 s / batch. (data: 2.45e-02)max mem: 17.22445 GB 
[09/17 07:14:13 visual_prompt]: 	Test 200/356. loss: 2.509, 0.1967 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/17 07:14:33 visual_prompt]: 	Test 300/356. loss: 2.187, 0.2075 s / batch. (data: 2.43e-02)max mem: 17.22445 GB 
[09/17 07:14:45 visual_prompt]: Inference (test):avg data time: 8.88e-03, avg batch time: 0.1956, average loss: 2.2343
[09/17 07:14:45 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.47	top5: 97.53	
[09/17 07:14:45 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/17 07:14:55 visual_prompt]: Epoch 49 / 100: avg data time: 1.25e-01, avg batch time: 0.5289, average train loss: 1.3969
[09/17 07:14:59 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1430, average loss: 1.5264
[09/17 07:14:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 99.00	
[09/17 07:15:20 visual_prompt]: 	Test 100/356. loss: 2.114, 0.1838 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/17 07:15:40 visual_prompt]: 	Test 200/356. loss: 2.435, 0.2093 s / batch. (data: 1.14e-04)max mem: 17.22445 GB 
[09/17 07:15:59 visual_prompt]: 	Test 300/356. loss: 1.734, 0.1954 s / batch. (data: 1.22e-02)max mem: 17.22445 GB 
[09/17 07:16:11 visual_prompt]: Inference (test):avg data time: 7.37e-03, avg batch time: 0.1936, average loss: 2.0424
[09/17 07:16:11 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.22	top5: 95.65	
[09/17 07:16:11 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/17 07:16:21 visual_prompt]: Epoch 50 / 100: avg data time: 1.29e-01, avg batch time: 0.5323, average train loss: 1.1675
[09/17 07:16:25 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1431, average loss: 1.2020
[09/17 07:16:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 52.50	top5: 99.50	
[09/17 07:16:46 visual_prompt]: 	Test 100/356. loss: 1.733, 0.2028 s / batch. (data: 1.08e-04)max mem: 17.22445 GB 
[09/17 07:17:06 visual_prompt]: 	Test 200/356. loss: 2.155, 0.1837 s / batch. (data: 1.41e-04)max mem: 17.22445 GB 
[09/17 07:17:25 visual_prompt]: 	Test 300/356. loss: 1.709, 0.1847 s / batch. (data: 1.57e-04)max mem: 17.22445 GB 
[09/17 07:17:37 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1931, average loss: 1.8264
[09/17 07:17:38 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.44	top5: 97.33	
[09/17 07:17:38 visual_prompt]: Best epoch 50: best metric: 0.525
[09/17 07:17:38 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/17 07:17:47 visual_prompt]: Epoch 51 / 100: avg data time: 1.23e-01, avg batch time: 0.5267, average train loss: 1.2806
[09/17 07:17:51 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.1433, average loss: 1.1020
[09/17 07:17:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 54.50	top5: 99.50	
[09/17 07:18:12 visual_prompt]: 	Test 100/356. loss: 1.447, 0.1956 s / batch. (data: 1.24e-02)max mem: 17.22445 GB 
[09/17 07:18:32 visual_prompt]: 	Test 200/356. loss: 1.856, 0.1835 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/17 07:18:52 visual_prompt]: 	Test 300/356. loss: 1.479, 0.1841 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 07:19:04 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1943, average loss: 1.6397
[09/17 07:19:04 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.60	top5: 95.40	
[09/17 07:19:04 visual_prompt]: Best epoch 51: best metric: 0.545
[09/17 07:19:04 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/17 07:19:14 visual_prompt]: Epoch 52 / 100: avg data time: 1.32e-01, avg batch time: 0.5358, average train loss: 1.1553
[09/17 07:19:17 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1432, average loss: 1.3575
[09/17 07:19:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 99.50	
[09/17 07:19:39 visual_prompt]: 	Test 100/356. loss: 1.693, 0.1835 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/17 07:19:58 visual_prompt]: 	Test 200/356. loss: 1.992, 0.1967 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/17 07:20:18 visual_prompt]: 	Test 300/356. loss: 1.713, 0.1842 s / batch. (data: 1.52e-04)max mem: 17.22445 GB 
[09/17 07:20:30 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1937, average loss: 1.8815
[09/17 07:20:30 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.79	top5: 97.87	
[09/17 07:20:30 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/17 07:20:40 visual_prompt]: Epoch 53 / 100: avg data time: 1.29e-01, avg batch time: 0.5328, average train loss: 1.3332
[09/17 07:20:44 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1432, average loss: 1.1509
[09/17 07:20:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 54.50	top5: 99.50	
[09/17 07:21:05 visual_prompt]: 	Test 100/356. loss: 1.505, 0.1839 s / batch. (data: 1.35e-04)max mem: 17.22445 GB 
[09/17 07:21:25 visual_prompt]: 	Test 200/356. loss: 1.757, 0.1836 s / batch. (data: 1.27e-04)max mem: 17.22445 GB 
[09/17 07:21:44 visual_prompt]: 	Test 300/356. loss: 1.462, 0.2049 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/17 07:21:56 visual_prompt]: Inference (test):avg data time: 7.77e-03, avg batch time: 0.1941, average loss: 1.6321
[09/17 07:21:56 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.53	top5: 97.52	
[09/17 07:21:56 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/17 07:22:06 visual_prompt]: Epoch 54 / 100: avg data time: 1.21e-01, avg batch time: 0.5254, average train loss: 1.1267
[09/17 07:22:10 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1433, average loss: 0.8635
[09/17 07:22:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 58.00	top5: 100.00	
[09/17 07:22:31 visual_prompt]: 	Test 100/356. loss: 1.412, 0.1980 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/17 07:22:51 visual_prompt]: 	Test 200/356. loss: 1.725, 0.2087 s / batch. (data: 2.61e-02)max mem: 17.22445 GB 
[09/17 07:23:10 visual_prompt]: 	Test 300/356. loss: 1.402, 0.2039 s / batch. (data: 2.04e-02)max mem: 17.22445 GB 
[09/17 07:23:22 visual_prompt]: Inference (test):avg data time: 8.73e-03, avg batch time: 0.1938, average loss: 1.5150
[09/17 07:23:23 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.19	top5: 98.05	
[09/17 07:23:23 visual_prompt]: Best epoch 54: best metric: 0.580
[09/17 07:23:23 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/17 07:23:32 visual_prompt]: Epoch 55 / 100: avg data time: 1.28e-01, avg batch time: 0.5305, average train loss: 0.9039
[09/17 07:23:36 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1429, average loss: 0.9420
[09/17 07:23:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 54.00	top5: 100.00	
[09/17 07:23:58 visual_prompt]: 	Test 100/356. loss: 1.282, 0.1885 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/17 07:24:17 visual_prompt]: 	Test 200/356. loss: 1.645, 0.1958 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 07:24:36 visual_prompt]: 	Test 300/356. loss: 1.237, 0.2002 s / batch. (data: 1.36e-02)max mem: 17.22445 GB 
[09/17 07:24:49 visual_prompt]: Inference (test):avg data time: 8.62e-03, avg batch time: 0.1943, average loss: 1.4474
[09/17 07:24:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.50	top5: 97.22	
[09/17 07:24:49 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/17 07:24:59 visual_prompt]: Epoch 56 / 100: avg data time: 1.31e-01, avg batch time: 0.5350, average train loss: 0.8774
[09/17 07:25:03 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1429, average loss: 1.0890
[09/17 07:25:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 56.50	top5: 99.50	
[09/17 07:25:24 visual_prompt]: 	Test 100/356. loss: 2.002, 0.1960 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 07:25:43 visual_prompt]: 	Test 200/356. loss: 2.205, 0.1944 s / batch. (data: 1.10e-02)max mem: 17.22445 GB 
[09/17 07:26:03 visual_prompt]: 	Test 300/356. loss: 1.898, 0.1981 s / batch. (data: 1.47e-02)max mem: 17.22445 GB 
[09/17 07:26:15 visual_prompt]: Inference (test):avg data time: 6.93e-03, avg batch time: 0.1928, average loss: 2.0180
[09/17 07:26:15 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.05	top5: 97.48	
[09/17 07:26:15 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/17 07:26:25 visual_prompt]: Epoch 57 / 100: avg data time: 1.30e-01, avg batch time: 0.5320, average train loss: 1.0766
[09/17 07:26:29 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1429, average loss: 0.9231
[09/17 07:26:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 61.00	top5: 99.50	
[09/17 07:26:50 visual_prompt]: 	Test 100/356. loss: 1.469, 0.2024 s / batch. (data: 1.23e-04)max mem: 17.22445 GB 
[09/17 07:27:10 visual_prompt]: 	Test 200/356. loss: 1.604, 0.2034 s / batch. (data: 1.41e-04)max mem: 17.22445 GB 
[09/17 07:27:29 visual_prompt]: 	Test 300/356. loss: 1.351, 0.2126 s / batch. (data: 2.94e-02)max mem: 17.22445 GB 
[09/17 07:27:42 visual_prompt]: Inference (test):avg data time: 8.04e-03, avg batch time: 0.1943, average loss: 1.5573
[09/17 07:27:42 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.14	top5: 97.62	
[09/17 07:27:42 visual_prompt]: Best epoch 57: best metric: 0.610
[09/17 07:27:42 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/17 07:27:52 visual_prompt]: Epoch 58 / 100: avg data time: 1.28e-01, avg batch time: 0.5311, average train loss: 0.8834
[09/17 07:27:55 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1430, average loss: 1.0300
[09/17 07:27:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 54.00	top5: 99.50	
[09/17 07:28:17 visual_prompt]: 	Test 100/356. loss: 1.513, 0.1957 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 07:28:37 visual_prompt]: 	Test 200/356. loss: 1.601, 0.1999 s / batch. (data: 1.36e-04)max mem: 17.22445 GB 
[09/17 07:28:56 visual_prompt]: 	Test 300/356. loss: 1.489, 0.1985 s / batch. (data: 1.36e-02)max mem: 17.22445 GB 
[09/17 07:29:08 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1942, average loss: 1.7762
[09/17 07:29:08 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.29	top5: 98.24	
[09/17 07:29:08 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/17 07:29:18 visual_prompt]: Epoch 59 / 100: avg data time: 1.15e-01, avg batch time: 0.5221, average train loss: 1.0047
[09/17 07:29:22 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1431, average loss: 0.9197
[09/17 07:29:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 59.00	top5: 100.00	
[09/17 07:29:43 visual_prompt]: 	Test 100/356. loss: 1.543, 0.1838 s / batch. (data: 1.53e-04)max mem: 17.22445 GB 
[09/17 07:30:03 visual_prompt]: 	Test 200/356. loss: 1.823, 0.2189 s / batch. (data: 3.60e-02)max mem: 17.22445 GB 
[09/17 07:30:22 visual_prompt]: 	Test 300/356. loss: 1.667, 0.1832 s / batch. (data: 1.34e-04)max mem: 17.22445 GB 
[09/17 07:30:34 visual_prompt]: Inference (test):avg data time: 7.12e-03, avg batch time: 0.1933, average loss: 1.7423
[09/17 07:30:34 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.08	top5: 97.06	
[09/17 07:30:34 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/17 07:30:44 visual_prompt]: Epoch 60 / 100: avg data time: 1.12e-01, avg batch time: 0.5163, average train loss: 1.0876
[09/17 07:30:48 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.1430, average loss: 1.1165
[09/17 07:30:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 48.00	top5: 99.50	
[09/17 07:31:09 visual_prompt]: 	Test 100/356. loss: 2.107, 0.1957 s / batch. (data: 1.24e-02)max mem: 17.22445 GB 
[09/17 07:31:29 visual_prompt]: 	Test 200/356. loss: 1.818, 0.1851 s / batch. (data: 1.45e-03)max mem: 17.22445 GB 
[09/17 07:31:48 visual_prompt]: 	Test 300/356. loss: 1.513, 0.1846 s / batch. (data: 1.71e-04)max mem: 17.22445 GB 
[09/17 07:32:00 visual_prompt]: Inference (test):avg data time: 7.94e-03, avg batch time: 0.1936, average loss: 2.0447
[09/17 07:32:00 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.60	top5: 98.22	
[09/17 07:32:00 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/17 07:32:10 visual_prompt]: Epoch 61 / 100: avg data time: 1.25e-01, avg batch time: 0.5298, average train loss: 1.0102
[09/17 07:32:14 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1431, average loss: 1.0742
[09/17 07:32:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 50.50	top5: 97.00	
[09/17 07:32:36 visual_prompt]: 	Test 100/356. loss: 1.866, 0.1980 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/17 07:32:55 visual_prompt]: 	Test 200/356. loss: 2.119, 0.1833 s / batch. (data: 1.33e-04)max mem: 17.22445 GB 
[09/17 07:33:15 visual_prompt]: 	Test 300/356. loss: 1.717, 0.1962 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/17 07:33:27 visual_prompt]: Inference (test):avg data time: 7.83e-03, avg batch time: 0.1948, average loss: 2.0616
[09/17 07:33:27 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.01	top5: 95.84	
[09/17 07:33:27 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/17 07:33:37 visual_prompt]: Epoch 62 / 100: avg data time: 1.24e-01, avg batch time: 0.5326, average train loss: 0.9470
[09/17 07:33:40 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1429, average loss: 1.1542
[09/17 07:33:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 99.50	
[09/17 07:34:02 visual_prompt]: 	Test 100/356. loss: 1.990, 0.1960 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/17 07:34:22 visual_prompt]: 	Test 200/356. loss: 2.624, 0.1960 s / batch. (data: 1.21e-04)max mem: 17.22445 GB 
[09/17 07:34:41 visual_prompt]: 	Test 300/356. loss: 1.794, 0.1840 s / batch. (data: 1.38e-04)max mem: 17.22445 GB 
[09/17 07:34:53 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1936, average loss: 2.1080
[09/17 07:34:53 visual_prompt]: Classification results with test_vtab-dmlab: top1: 29.71	top5: 96.05	
[09/17 07:34:53 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/17 07:35:03 visual_prompt]: Epoch 63 / 100: avg data time: 1.18e-01, avg batch time: 0.5226, average train loss: 0.9357
[09/17 07:35:07 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1431, average loss: 0.7740
[09/17 07:35:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 63.50	top5: 100.00	
[09/17 07:35:28 visual_prompt]: 	Test 100/356. loss: 1.707, 0.1836 s / batch. (data: 1.56e-04)max mem: 17.22445 GB 
[09/17 07:35:48 visual_prompt]: 	Test 200/356. loss: 1.447, 0.1993 s / batch. (data: 1.58e-02)max mem: 17.22445 GB 
[09/17 07:36:07 visual_prompt]: 	Test 300/356. loss: 1.188, 0.1844 s / batch. (data: 1.59e-04)max mem: 17.22445 GB 
[09/17 07:36:19 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1934, average loss: 1.6839
[09/17 07:36:19 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.79	top5: 97.71	
[09/17 07:36:19 visual_prompt]: Best epoch 63: best metric: 0.635
[09/17 07:36:19 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/17 07:36:29 visual_prompt]: Epoch 64 / 100: avg data time: 1.21e-01, avg batch time: 0.5315, average train loss: 0.7325
[09/17 07:36:33 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1431, average loss: 0.7626
[09/17 07:36:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 63.00	top5: 100.00	
[09/17 07:36:54 visual_prompt]: 	Test 100/356. loss: 1.964, 0.1841 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/17 07:37:14 visual_prompt]: 	Test 200/356. loss: 2.104, 0.1946 s / batch. (data: 3.84e-05)max mem: 17.22445 GB 
[09/17 07:37:33 visual_prompt]: 	Test 300/356. loss: 1.813, 0.1839 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/17 07:37:46 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1955, average loss: 1.8893
[09/17 07:37:46 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.01	top5: 96.34	
[09/17 07:37:46 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/17 07:37:56 visual_prompt]: Epoch 65 / 100: avg data time: 1.15e-01, avg batch time: 0.5185, average train loss: 0.7215
[09/17 07:38:00 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1431, average loss: 0.7095
[09/17 07:38:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 64.50	top5: 100.00	
[09/17 07:38:21 visual_prompt]: 	Test 100/356. loss: 1.509, 0.1839 s / batch. (data: 1.27e-04)max mem: 17.22445 GB 
[09/17 07:38:41 visual_prompt]: 	Test 200/356. loss: 1.847, 0.2035 s / batch. (data: 1.26e-02)max mem: 17.22445 GB 
[09/17 07:39:00 visual_prompt]: 	Test 300/356. loss: 1.807, 0.1947 s / batch. (data: 4.53e-05)max mem: 17.22445 GB 
[09/17 07:39:12 visual_prompt]: Inference (test):avg data time: 7.66e-03, avg batch time: 0.1934, average loss: 1.9219
[09/17 07:39:12 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.71	top5: 97.70	
[09/17 07:39:13 visual_prompt]: Best epoch 65: best metric: 0.645
[09/17 07:39:13 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/17 07:39:22 visual_prompt]: Epoch 66 / 100: avg data time: 1.17e-01, avg batch time: 0.5222, average train loss: 0.6207
[09/17 07:39:26 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1432, average loss: 0.5912
[09/17 07:39:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 71.50	top5: 100.00	
[09/17 07:39:48 visual_prompt]: 	Test 100/356. loss: 1.981, 0.2059 s / batch. (data: 2.30e-02)max mem: 17.22445 GB 
[09/17 07:40:07 visual_prompt]: 	Test 200/356. loss: 2.534, 0.1838 s / batch. (data: 1.40e-04)max mem: 17.22445 GB 
[09/17 07:40:27 visual_prompt]: 	Test 300/356. loss: 1.896, 0.1847 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/17 07:40:39 visual_prompt]: Inference (test):avg data time: 8.30e-03, avg batch time: 0.1961, average loss: 2.0990
[09/17 07:40:39 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.44	top5: 96.84	
[09/17 07:40:39 visual_prompt]: Best epoch 66: best metric: 0.715
[09/17 07:40:39 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/17 07:40:49 visual_prompt]: Epoch 67 / 100: avg data time: 1.21e-01, avg batch time: 0.5248, average train loss: 0.5882
[09/17 07:40:52 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1431, average loss: 0.7228
[09/17 07:40:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 62.50	top5: 100.00	
[09/17 07:41:14 visual_prompt]: 	Test 100/356. loss: 1.877, 0.1985 s / batch. (data: 1.56e-04)max mem: 17.22445 GB 
[09/17 07:41:33 visual_prompt]: 	Test 200/356. loss: 2.119, 0.2026 s / batch. (data: 1.93e-02)max mem: 17.22445 GB 
[09/17 07:41:53 visual_prompt]: 	Test 300/356. loss: 1.908, 0.2398 s / batch. (data: 1.36e-02)max mem: 17.22445 GB 
[09/17 07:42:05 visual_prompt]: Inference (test):avg data time: 8.12e-03, avg batch time: 0.1937, average loss: 2.1666
[09/17 07:42:05 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.98	top5: 97.98	
[09/17 07:42:05 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/17 07:42:15 visual_prompt]: Epoch 68 / 100: avg data time: 1.16e-01, avg batch time: 0.5344, average train loss: 0.5738
[09/17 07:42:19 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1430, average loss: 0.5825
[09/17 07:42:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 73.50	top5: 100.00	
[09/17 07:42:40 visual_prompt]: 	Test 100/356. loss: 2.063, 0.1893 s / batch. (data: 1.47e-04)max mem: 17.22445 GB 
[09/17 07:43:00 visual_prompt]: 	Test 200/356. loss: 2.138, 0.2123 s / batch. (data: 2.87e-02)max mem: 17.22445 GB 
[09/17 07:43:19 visual_prompt]: 	Test 300/356. loss: 1.969, 0.1992 s / batch. (data: 1.58e-02)max mem: 17.22445 GB 
[09/17 07:43:32 visual_prompt]: Inference (test):avg data time: 8.20e-03, avg batch time: 0.1940, average loss: 2.1873
[09/17 07:43:32 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.48	top5: 98.17	
[09/17 07:43:32 visual_prompt]: Best epoch 68: best metric: 0.735
[09/17 07:43:32 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/17 07:43:42 visual_prompt]: Epoch 69 / 100: avg data time: 1.25e-01, avg batch time: 0.5278, average train loss: 0.5959
[09/17 07:43:45 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1431, average loss: 0.5207
[09/17 07:43:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 75.50	top5: 100.00	
[09/17 07:44:07 visual_prompt]: 	Test 100/356. loss: 1.934, 0.1957 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 07:44:26 visual_prompt]: 	Test 200/356. loss: 2.268, 0.1845 s / batch. (data: 1.04e-04)max mem: 17.22445 GB 
[09/17 07:44:46 visual_prompt]: 	Test 300/356. loss: 2.058, 0.2076 s / batch. (data: 2.45e-02)max mem: 17.22445 GB 
[09/17 07:44:58 visual_prompt]: Inference (test):avg data time: 7.85e-03, avg batch time: 0.1951, average loss: 2.2716
[09/17 07:44:58 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.31	top5: 97.97	
[09/17 07:44:58 visual_prompt]: Best epoch 69: best metric: 0.755
[09/17 07:44:58 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/17 07:45:08 visual_prompt]: Epoch 70 / 100: avg data time: 1.30e-01, avg batch time: 0.5320, average train loss: 0.5129
[09/17 07:45:12 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1431, average loss: 0.4738
[09/17 07:45:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 79.50	top5: 100.00	
[09/17 07:45:34 visual_prompt]: 	Test 100/356. loss: 2.017, 0.2001 s / batch. (data: 1.72e-04)max mem: 17.22445 GB 
[09/17 07:45:53 visual_prompt]: 	Test 200/356. loss: 2.285, 0.1957 s / batch. (data: 1.23e-02)max mem: 17.22445 GB 
[09/17 07:46:13 visual_prompt]: 	Test 300/356. loss: 2.016, 0.1976 s / batch. (data: 1.43e-02)max mem: 17.22445 GB 
[09/17 07:46:25 visual_prompt]: Inference (test):avg data time: 7.80e-03, avg batch time: 0.1939, average loss: 2.3261
[09/17 07:46:25 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.41	top5: 97.69	
[09/17 07:46:25 visual_prompt]: Best epoch 70: best metric: 0.795
[09/17 07:46:25 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/17 07:46:35 visual_prompt]: Epoch 71 / 100: avg data time: 1.18e-01, avg batch time: 0.5267, average train loss: 0.4781
[09/17 07:46:38 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1430, average loss: 0.6496
[09/17 07:46:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 72.50	top5: 100.00	
[09/17 07:47:00 visual_prompt]: 	Test 100/356. loss: 2.222, 0.1898 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 07:47:19 visual_prompt]: 	Test 200/356. loss: 2.883, 0.1842 s / batch. (data: 1.41e-04)max mem: 17.22445 GB 
[09/17 07:47:39 visual_prompt]: 	Test 300/356. loss: 2.762, 0.1839 s / batch. (data: 1.26e-04)max mem: 17.22445 GB 
[09/17 07:47:51 visual_prompt]: Inference (test):avg data time: 7.83e-03, avg batch time: 0.1947, average loss: 2.5446
[09/17 07:47:51 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.59	top5: 97.10	
[09/17 07:47:51 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/17 07:48:01 visual_prompt]: Epoch 72 / 100: avg data time: 1.18e-01, avg batch time: 0.5209, average train loss: 0.4932
[09/17 07:48:05 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1430, average loss: 0.6733
[09/17 07:48:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 70.50	top5: 100.00	
[09/17 07:48:26 visual_prompt]: 	Test 100/356. loss: 2.301, 0.1965 s / batch. (data: 1.31e-02)max mem: 17.22445 GB 
[09/17 07:48:46 visual_prompt]: 	Test 200/356. loss: 2.837, 0.1833 s / batch. (data: 1.59e-04)max mem: 17.22445 GB 
[09/17 07:49:05 visual_prompt]: 	Test 300/356. loss: 2.111, 0.1835 s / batch. (data: 1.50e-04)max mem: 17.22445 GB 
[09/17 07:49:17 visual_prompt]: Inference (test):avg data time: 7.94e-03, avg batch time: 0.1936, average loss: 2.5987
[09/17 07:49:17 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.36	top5: 97.24	
[09/17 07:49:17 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/17 07:49:27 visual_prompt]: Epoch 73 / 100: avg data time: 1.31e-01, avg batch time: 0.5328, average train loss: 0.5694
[09/17 07:49:31 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.1435, average loss: 0.9289
[09/17 07:49:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 66.00	top5: 100.00	
[09/17 07:49:53 visual_prompt]: 	Test 100/356. loss: 2.420, 0.1963 s / batch. (data: 1.53e-04)max mem: 17.22445 GB 
[09/17 07:50:13 visual_prompt]: 	Test 200/356. loss: 2.900, 0.1841 s / batch. (data: 1.99e-04)max mem: 17.22445 GB 
[09/17 07:50:32 visual_prompt]: 	Test 300/356. loss: 2.426, 0.1845 s / batch. (data: 1.35e-04)max mem: 17.22445 GB 
[09/17 07:50:45 visual_prompt]: Inference (test):avg data time: 8.10e-03, avg batch time: 0.1960, average loss: 2.6534
[09/17 07:50:45 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.03	top5: 97.69	
[09/17 07:50:45 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/17 07:50:55 visual_prompt]: Epoch 74 / 100: avg data time: 1.30e-01, avg batch time: 0.5315, average train loss: 0.6377
[09/17 07:50:58 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1432, average loss: 0.5533
[09/17 07:50:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 77.00	top5: 100.00	
[09/17 07:51:20 visual_prompt]: 	Test 100/356. loss: 2.073, 0.1831 s / batch. (data: 1.19e-04)max mem: 17.22445 GB 
[09/17 07:51:39 visual_prompt]: 	Test 200/356. loss: 2.414, 0.1931 s / batch. (data: 1.35e-04)max mem: 17.22445 GB 
[09/17 07:51:59 visual_prompt]: 	Test 300/356. loss: 1.826, 0.1862 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 07:52:11 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1936, average loss: 2.2398
[09/17 07:52:11 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.08	top5: 97.00	
[09/17 07:52:11 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/17 07:52:21 visual_prompt]: Epoch 75 / 100: avg data time: 1.14e-01, avg batch time: 0.5189, average train loss: 0.4222
[09/17 07:52:24 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1431, average loss: 0.3119
[09/17 07:52:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 88.50	top5: 100.00	
[09/17 07:52:46 visual_prompt]: 	Test 100/356. loss: 2.118, 0.1840 s / batch. (data: 1.22e-04)max mem: 17.22445 GB 
[09/17 07:53:06 visual_prompt]: 	Test 200/356. loss: 2.686, 0.2071 s / batch. (data: 1.22e-02)max mem: 17.22445 GB 
[09/17 07:53:25 visual_prompt]: 	Test 300/356. loss: 2.102, 0.1983 s / batch. (data: 1.48e-02)max mem: 17.22445 GB 
[09/17 07:53:38 visual_prompt]: Inference (test):avg data time: 7.32e-03, avg batch time: 0.1951, average loss: 2.4219
[09/17 07:53:38 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.66	top5: 97.71	
[09/17 07:53:38 visual_prompt]: Best epoch 75: best metric: 0.885
[09/17 07:53:38 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/17 07:53:48 visual_prompt]: Epoch 76 / 100: avg data time: 1.28e-01, avg batch time: 0.5309, average train loss: 0.3198
[09/17 07:53:51 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1431, average loss: 0.2745
[09/17 07:53:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 91.00	top5: 100.00	
[09/17 07:54:13 visual_prompt]: 	Test 100/356. loss: 2.476, 0.1853 s / batch. (data: 1.21e-04)max mem: 17.22445 GB 
[09/17 07:54:32 visual_prompt]: 	Test 200/356. loss: 3.220, 0.2048 s / batch. (data: 1.47e-02)max mem: 17.22445 GB 
[09/17 07:54:52 visual_prompt]: 	Test 300/356. loss: 2.377, 0.1846 s / batch. (data: 1.44e-04)max mem: 17.22445 GB 
[09/17 07:55:04 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1933, average loss: 2.8356
[09/17 07:55:04 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.57	top5: 97.70	
[09/17 07:55:04 visual_prompt]: Best epoch 76: best metric: 0.910
[09/17 07:55:04 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/17 07:55:14 visual_prompt]: Epoch 77 / 100: avg data time: 1.23e-01, avg batch time: 0.5262, average train loss: 0.2795
[09/17 07:55:17 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1431, average loss: 0.3009
[09/17 07:55:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 89.00	top5: 100.00	
[09/17 07:55:39 visual_prompt]: 	Test 100/356. loss: 2.712, 0.1970 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 07:55:58 visual_prompt]: 	Test 200/356. loss: 3.254, 0.1960 s / batch. (data: 1.29e-02)max mem: 17.22445 GB 
[09/17 07:56:18 visual_prompt]: 	Test 300/356. loss: 2.712, 0.1846 s / batch. (data: 1.58e-04)max mem: 17.22445 GB 
[09/17 07:56:30 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1935, average loss: 2.8583
[09/17 07:56:30 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.16	top5: 97.45	
[09/17 07:56:30 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/17 07:56:40 visual_prompt]: Epoch 78 / 100: avg data time: 1.17e-01, avg batch time: 0.5222, average train loss: 0.2794
[09/17 07:56:43 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1432, average loss: 0.2187
[09/17 07:56:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 88.50	top5: 100.00	
[09/17 07:57:05 visual_prompt]: 	Test 100/356. loss: 2.476, 0.1995 s / batch. (data: 1.60e-02)max mem: 17.22445 GB 
[09/17 07:57:24 visual_prompt]: 	Test 200/356. loss: 3.456, 0.1843 s / batch. (data: 1.38e-04)max mem: 17.22445 GB 
[09/17 07:57:44 visual_prompt]: 	Test 300/356. loss: 3.139, 0.1845 s / batch. (data: 1.29e-04)max mem: 17.22445 GB 
[09/17 07:57:56 visual_prompt]: Inference (test):avg data time: 7.32e-03, avg batch time: 0.1928, average loss: 2.9901
[09/17 07:57:56 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.34	top5: 96.91	
[09/17 07:57:56 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/17 07:58:05 visual_prompt]: Epoch 79 / 100: avg data time: 1.10e-01, avg batch time: 0.5154, average train loss: 0.2435
[09/17 07:58:09 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1433, average loss: 0.2432
[09/17 07:58:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 90.00	top5: 100.00	
[09/17 07:58:31 visual_prompt]: 	Test 100/356. loss: 2.784, 0.1961 s / batch. (data: 1.28e-02)max mem: 17.22445 GB 
[09/17 07:58:50 visual_prompt]: 	Test 200/356. loss: 2.820, 0.2027 s / batch. (data: 1.38e-04)max mem: 17.22445 GB 
[09/17 07:59:09 visual_prompt]: 	Test 300/356. loss: 2.656, 0.1842 s / batch. (data: 1.29e-04)max mem: 17.22445 GB 
[09/17 07:59:22 visual_prompt]: Inference (test):avg data time: 8.54e-03, avg batch time: 0.1937, average loss: 3.0495
[09/17 07:59:22 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.15	top5: 96.39	
[09/17 07:59:22 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/17 07:59:32 visual_prompt]: Epoch 80 / 100: avg data time: 1.22e-01, avg batch time: 0.5272, average train loss: 0.2777
[09/17 07:59:35 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1432, average loss: 0.2090
[09/17 07:59:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 91.00	top5: 100.00	
[09/17 07:59:57 visual_prompt]: 	Test 100/356. loss: 2.817, 0.1839 s / batch. (data: 1.38e-04)max mem: 17.22445 GB 
[09/17 08:00:17 visual_prompt]: 	Test 200/356. loss: 3.249, 0.1841 s / batch. (data: 1.39e-04)max mem: 17.22445 GB 
[09/17 08:00:36 visual_prompt]: 	Test 300/356. loss: 2.899, 0.1926 s / batch. (data: 1.38e-04)max mem: 17.22445 GB 
[09/17 08:00:49 visual_prompt]: Inference (test):avg data time: 8.12e-03, avg batch time: 0.1946, average loss: 3.0414
[09/17 08:00:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.37	top5: 97.53	
[09/17 08:00:49 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/17 08:00:58 visual_prompt]: Epoch 81 / 100: avg data time: 1.15e-01, avg batch time: 0.5188, average train loss: 0.1954
[09/17 08:01:02 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1433, average loss: 0.1027
[09/17 08:01:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 97.00	top5: 100.00	
[09/17 08:01:24 visual_prompt]: 	Test 100/356. loss: 2.682, 0.1835 s / batch. (data: 1.25e-04)max mem: 17.22445 GB 
[09/17 08:01:43 visual_prompt]: 	Test 200/356. loss: 3.600, 0.1894 s / batch. (data: 1.47e-04)max mem: 17.22445 GB 
[09/17 08:02:02 visual_prompt]: 	Test 300/356. loss: 2.879, 0.1842 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/17 08:02:14 visual_prompt]: Inference (test):avg data time: 7.01e-03, avg batch time: 0.1926, average loss: 3.1930
[09/17 08:02:15 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.70	top5: 95.91	
[09/17 08:02:15 visual_prompt]: Best epoch 81: best metric: 0.970
[09/17 08:02:15 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/17 08:02:24 visual_prompt]: Epoch 82 / 100: avg data time: 1.12e-01, avg batch time: 0.5164, average train loss: 0.2204
[09/17 08:02:28 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1430, average loss: 0.2204
[09/17 08:02:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 92.50	top5: 100.00	
[09/17 08:02:49 visual_prompt]: 	Test 100/356. loss: 2.867, 0.1844 s / batch. (data: 1.48e-04)max mem: 17.22445 GB 
[09/17 08:03:09 visual_prompt]: 	Test 200/356. loss: 3.506, 0.1862 s / batch. (data: 1.02e-04)max mem: 17.22445 GB 
[09/17 08:03:28 visual_prompt]: 	Test 300/356. loss: 2.766, 0.1960 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 08:03:41 visual_prompt]: Inference (test):avg data time: 8.21e-03, avg batch time: 0.1942, average loss: 3.0972
[09/17 08:03:41 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.08	top5: 97.09	
[09/17 08:03:41 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/17 08:03:50 visual_prompt]: Epoch 83 / 100: avg data time: 1.18e-01, avg batch time: 0.5219, average train loss: 0.1889
[09/17 08:03:54 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.1434, average loss: 0.1327
[09/17 08:03:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 94.50	top5: 100.00	
[09/17 08:04:15 visual_prompt]: 	Test 100/356. loss: 2.899, 0.1977 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/17 08:04:35 visual_prompt]: 	Test 200/356. loss: 3.534, 0.2015 s / batch. (data: 1.69e-02)max mem: 17.22445 GB 
[09/17 08:04:54 visual_prompt]: 	Test 300/356. loss: 2.798, 0.1839 s / batch. (data: 1.65e-04)max mem: 17.22445 GB 
[09/17 08:05:07 visual_prompt]: Inference (test):avg data time: 8.33e-03, avg batch time: 0.1940, average loss: 3.2015
[09/17 08:05:07 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.16	top5: 97.07	
[09/17 08:05:07 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/17 08:05:17 visual_prompt]: Epoch 84 / 100: avg data time: 1.31e-01, avg batch time: 0.5318, average train loss: 0.1201
[09/17 08:05:20 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1430, average loss: 0.0688
[09/17 08:05:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 98.50	top5: 100.00	
[09/17 08:05:42 visual_prompt]: 	Test 100/356. loss: 3.196, 0.1840 s / batch. (data: 1.07e-04)max mem: 17.22445 GB 
[09/17 08:06:01 visual_prompt]: 	Test 200/356. loss: 3.891, 0.1838 s / batch. (data: 1.49e-04)max mem: 17.22445 GB 
[09/17 08:06:20 visual_prompt]: 	Test 300/356. loss: 2.862, 0.1962 s / batch. (data: 1.27e-04)max mem: 17.22445 GB 
[09/17 08:06:33 visual_prompt]: Inference (test):avg data time: 7.81e-03, avg batch time: 0.1929, average loss: 3.5778
[09/17 08:06:33 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.74	top5: 96.63	
[09/17 08:06:33 visual_prompt]: Best epoch 84: best metric: 0.985
[09/17 08:06:33 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/17 08:06:42 visual_prompt]: Epoch 85 / 100: avg data time: 1.21e-01, avg batch time: 0.5240, average train loss: 0.1395
[09/17 08:06:46 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1432, average loss: 0.1094
[09/17 08:06:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 96.00	top5: 100.00	
[09/17 08:07:08 visual_prompt]: 	Test 100/356. loss: 3.421, 0.1957 s / batch. (data: 1.25e-02)max mem: 17.22445 GB 
[09/17 08:07:27 visual_prompt]: 	Test 200/356. loss: 4.148, 0.1845 s / batch. (data: 1.51e-04)max mem: 17.22445 GB 
[09/17 08:07:47 visual_prompt]: 	Test 300/356. loss: 3.319, 0.5628 s / batch. (data: 2.38e-02)max mem: 17.22445 GB 
[09/17 08:07:59 visual_prompt]: Inference (test):avg data time: 6.97e-03, avg batch time: 0.1953, average loss: 3.8076
[09/17 08:08:00 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.09	top5: 96.17	
[09/17 08:08:00 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/17 08:08:09 visual_prompt]: Epoch 86 / 100: avg data time: 1.16e-01, avg batch time: 0.5223, average train loss: 0.1149
[09/17 08:08:13 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1431, average loss: 0.0637
[09/17 08:08:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 98.00	top5: 100.00	
[09/17 08:08:34 visual_prompt]: 	Test 100/356. loss: 3.594, 0.1977 s / batch. (data: 1.61e-04)max mem: 17.22445 GB 
[09/17 08:08:54 visual_prompt]: 	Test 200/356. loss: 4.714, 0.1959 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 08:09:13 visual_prompt]: 	Test 300/356. loss: 3.408, 0.2115 s / batch. (data: 1.54e-02)max mem: 17.22445 GB 
[09/17 08:09:25 visual_prompt]: Inference (test):avg data time: 7.61e-03, avg batch time: 0.1933, average loss: 3.9518
[09/17 08:09:26 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.64	top5: 97.10	
[09/17 08:09:26 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/17 08:09:35 visual_prompt]: Epoch 87 / 100: avg data time: 1.32e-01, avg batch time: 0.5352, average train loss: 0.0756
[09/17 08:09:39 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1431, average loss: 0.0412
[09/17 08:09:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 99.00	top5: 100.00	
[09/17 08:10:01 visual_prompt]: 	Test 100/356. loss: 3.647, 0.1962 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/17 08:10:20 visual_prompt]: 	Test 200/356. loss: 4.494, 0.1841 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/17 08:10:39 visual_prompt]: 	Test 300/356. loss: 3.452, 0.1983 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/17 08:10:52 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1933, average loss: 4.0740
[09/17 08:10:52 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.42	top5: 95.71	
[09/17 08:10:52 visual_prompt]: Best epoch 87: best metric: 0.990
[09/17 08:10:52 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/17 08:11:02 visual_prompt]: Epoch 88 / 100: avg data time: 1.33e-01, avg batch time: 0.5400, average train loss: 0.0455
[09/17 08:11:05 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1431, average loss: 0.0304
[09/17 08:11:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 99.50	top5: 100.00	
[09/17 08:11:27 visual_prompt]: 	Test 100/356. loss: 3.796, 0.2057 s / batch. (data: 2.28e-02)max mem: 17.22445 GB 
[09/17 08:11:47 visual_prompt]: 	Test 200/356. loss: 4.613, 0.1973 s / batch. (data: 1.39e-02)max mem: 17.22445 GB 
[09/17 08:12:06 visual_prompt]: 	Test 300/356. loss: 3.645, 0.1838 s / batch. (data: 1.07e-04)max mem: 17.22445 GB 
[09/17 08:12:18 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1945, average loss: 4.1953
[09/17 08:12:18 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.33	top5: 95.37	
[09/17 08:12:18 visual_prompt]: Best epoch 88: best metric: 0.995
[09/17 08:12:18 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/17 08:12:28 visual_prompt]: Epoch 89 / 100: avg data time: 1.23e-01, avg batch time: 0.5266, average train loss: 0.0429
[09/17 08:12:32 visual_prompt]: Inference (val):avg data time: 4.89e-05, avg batch time: 0.1434, average loss: 0.0460
[09/17 08:12:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 99.00	top5: 100.00	
[09/17 08:12:53 visual_prompt]: 	Test 100/356. loss: 4.055, 0.2011 s / batch. (data: 1.60e-02)max mem: 17.22445 GB 
[09/17 08:13:13 visual_prompt]: 	Test 200/356. loss: 4.652, 0.1841 s / batch. (data: 1.10e-04)max mem: 17.22445 GB 
[09/17 08:13:32 visual_prompt]: 	Test 300/356. loss: 3.757, 0.1841 s / batch. (data: 1.19e-04)max mem: 17.22445 GB 
[09/17 08:13:45 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1946, average loss: 4.3285
[09/17 08:13:45 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.84	top5: 95.40	
[09/17 08:13:45 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/17 08:13:54 visual_prompt]: Epoch 90 / 100: avg data time: 1.22e-01, avg batch time: 0.5261, average train loss: 0.0448
[09/17 08:13:58 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1431, average loss: 0.0188
[09/17 08:13:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 100.00	top5: 100.00	
[09/17 08:14:19 visual_prompt]: 	Test 100/356. loss: 4.038, 0.1960 s / batch. (data: 1.31e-02)max mem: 17.22445 GB 
[09/17 08:14:39 visual_prompt]: 	Test 200/356. loss: 4.738, 0.1982 s / batch. (data: 1.22e-02)max mem: 17.22445 GB 
[09/17 08:14:58 visual_prompt]: 	Test 300/356. loss: 3.963, 0.1970 s / batch. (data: 1.37e-04)max mem: 17.22445 GB 
[09/17 08:15:10 visual_prompt]: Inference (test):avg data time: 7.43e-03, avg batch time: 0.1927, average loss: 4.3878
[09/17 08:15:10 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.83	top5: 95.36	
[09/17 08:15:10 visual_prompt]: Best epoch 90: best metric: 1.000
[09/17 08:15:11 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/17 08:15:20 visual_prompt]: Epoch 91 / 100: avg data time: 1.26e-01, avg batch time: 0.5316, average train loss: 0.0324
[09/17 08:15:24 visual_prompt]: Inference (val):avg data time: 4.83e-05, avg batch time: 0.1448, average loss: 0.0265
[09/17 08:15:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 99.50	top5: 100.00	
[09/17 08:15:45 visual_prompt]: 	Test 100/356. loss: 4.242, 0.1832 s / batch. (data: 1.45e-04)max mem: 17.22445 GB 
[09/17 08:16:05 visual_prompt]: 	Test 200/356. loss: 4.921, 0.1838 s / batch. (data: 1.24e-04)max mem: 17.22445 GB 
[09/17 08:16:25 visual_prompt]: 	Test 300/356. loss: 4.035, 0.1919 s / batch. (data: 8.87e-03)max mem: 17.22445 GB 
[09/17 08:16:37 visual_prompt]: Inference (test):avg data time: 8.03e-03, avg batch time: 0.1940, average loss: 4.4728
[09/17 08:16:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.76	top5: 95.10	
[09/17 08:16:37 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/17 08:16:47 visual_prompt]: Epoch 92 / 100: avg data time: 1.28e-01, avg batch time: 0.5316, average train loss: 0.0285
[09/17 08:16:50 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1430, average loss: 0.0115
[09/17 08:16:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 100.00	top5: 100.00	
[09/17 08:17:12 visual_prompt]: 	Test 100/356. loss: 4.479, 0.1959 s / batch. (data: 1.27e-02)max mem: 17.22445 GB 
[09/17 08:17:31 visual_prompt]: 	Test 200/356. loss: 4.996, 0.2067 s / batch. (data: 1.41e-02)max mem: 17.22445 GB 
[09/17 08:17:51 visual_prompt]: 	Test 300/356. loss: 4.091, 0.1983 s / batch. (data: 1.50e-02)max mem: 17.22445 GB 
[09/17 08:18:03 visual_prompt]: Inference (test):avg data time: 8.21e-03, avg batch time: 0.1940, average loss: 4.5849
[09/17 08:18:03 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.30	top5: 94.96	
[09/17 08:18:03 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/17 08:18:13 visual_prompt]: Epoch 93 / 100: avg data time: 1.29e-01, avg batch time: 0.5342, average train loss: 0.0163
[09/17 08:18:17 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.1429, average loss: 0.0086
[09/17 08:18:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 100.00	top5: 100.00	
[09/17 08:18:38 visual_prompt]: 	Test 100/356. loss: 4.430, 0.1984 s / batch. (data: 3.65e-05)max mem: 17.22445 GB 
[09/17 08:18:58 visual_prompt]: 	Test 200/356. loss: 5.088, 0.1891 s / batch. (data: 1.91e-04)max mem: 17.22445 GB 
[09/17 08:19:17 visual_prompt]: 	Test 300/356. loss: 4.132, 0.2683 s / batch. (data: 4.46e-04)max mem: 17.22445 GB 
[09/17 08:19:29 visual_prompt]: Inference (test):avg data time: 7.03e-03, avg batch time: 0.1936, average loss: 4.6300
[09/17 08:19:29 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.35	top5: 95.06	
[09/17 08:19:29 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/17 08:19:39 visual_prompt]: Epoch 94 / 100: avg data time: 1.24e-01, avg batch time: 0.5267, average train loss: 0.0212
[09/17 08:19:43 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1431, average loss: 0.0063
[09/17 08:19:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 100.00	top5: 100.00	
[09/17 08:20:04 visual_prompt]: 	Test 100/356. loss: 4.328, 0.1983 s / batch. (data: 1.51e-02)max mem: 17.22445 GB 
[09/17 08:20:24 visual_prompt]: 	Test 200/356. loss: 5.201, 0.1895 s / batch. (data: 1.60e-04)max mem: 17.22445 GB 
[09/17 08:20:43 visual_prompt]: 	Test 300/356. loss: 4.238, 0.1840 s / batch. (data: 1.65e-04)max mem: 17.22445 GB 
[09/17 08:20:55 visual_prompt]: Inference (test):avg data time: 7.27e-03, avg batch time: 0.1930, average loss: 4.5826
[09/17 08:20:56 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.75	top5: 95.12	
[09/17 08:20:56 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/17 08:21:05 visual_prompt]: Epoch 95 / 100: avg data time: 1.09e-01, avg batch time: 0.5157, average train loss: 0.0151
[09/17 08:21:09 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1431, average loss: 0.0068
[09/17 08:21:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 100.00	top5: 100.00	
[09/17 08:21:30 visual_prompt]: 	Test 100/356. loss: 4.373, 0.1844 s / batch. (data: 1.72e-04)max mem: 17.22445 GB 
[09/17 08:21:50 visual_prompt]: 	Test 200/356. loss: 5.246, 0.2031 s / batch. (data: 1.43e-02)max mem: 17.22445 GB 
[09/17 08:22:09 visual_prompt]: 	Test 300/356. loss: 4.294, 0.1844 s / batch. (data: 1.45e-04)max mem: 17.22445 GB 
[09/17 08:22:22 visual_prompt]: Inference (test):avg data time: 7.67e-03, avg batch time: 0.1945, average loss: 4.6205
[09/17 08:22:22 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.98	top5: 95.29	
[09/17 08:22:22 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/17 08:22:31 visual_prompt]: Epoch 96 / 100: avg data time: 1.24e-01, avg batch time: 0.5269, average train loss: 0.0117
[09/17 08:22:35 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1431, average loss: 0.0064
[09/17 08:22:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 100.00	top5: 100.00	
[09/17 08:22:56 visual_prompt]: 	Test 100/356. loss: 4.433, 0.1988 s / batch. (data: 1.56e-02)max mem: 17.22445 GB 
[09/17 08:23:17 visual_prompt]: 	Test 200/356. loss: 5.332, 0.1842 s / batch. (data: 1.32e-04)max mem: 17.22445 GB 
[09/17 08:23:36 visual_prompt]: 	Test 300/356. loss: 4.330, 0.1994 s / batch. (data: 1.61e-02)max mem: 17.22445 GB 
[09/17 08:23:48 visual_prompt]: Inference (test):avg data time: 7.13e-03, avg batch time: 0.1950, average loss: 4.6877
[09/17 08:23:48 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.83	top5: 95.32	
[09/17 08:23:48 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/17 08:23:58 visual_prompt]: Epoch 97 / 100: avg data time: 1.25e-01, avg batch time: 0.5295, average train loss: 0.0122
[09/17 08:24:02 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.1431, average loss: 0.0054
[09/17 08:24:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 100.00	top5: 100.00	
[09/17 08:24:23 visual_prompt]: 	Test 100/356. loss: 4.482, 0.1835 s / batch. (data: 1.20e-04)max mem: 17.22445 GB 
[09/17 08:24:43 visual_prompt]: 	Test 200/356. loss: 5.335, 0.1839 s / batch. (data: 1.28e-04)max mem: 17.22445 GB 
[09/17 08:25:03 visual_prompt]: 	Test 300/356. loss: 4.337, 0.1841 s / batch. (data: 1.18e-04)max mem: 17.22445 GB 
[09/17 08:25:15 visual_prompt]: Inference (test):avg data time: 7.80e-03, avg batch time: 0.1949, average loss: 4.7265
[09/17 08:25:15 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.84	top5: 95.09	
[09/17 08:25:15 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/17 08:25:25 visual_prompt]: Epoch 98 / 100: avg data time: 1.24e-01, avg batch time: 0.5276, average train loss: 0.0123
[09/17 08:25:29 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1430, average loss: 0.0058
[09/17 08:25:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 100.00	top5: 100.00	
[09/17 08:25:50 visual_prompt]: 	Test 100/356. loss: 4.508, 0.1840 s / batch. (data: 1.43e-04)max mem: 17.22445 GB 
[09/17 08:26:10 visual_prompt]: 	Test 200/356. loss: 5.360, 0.1937 s / batch. (data: 1.03e-02)max mem: 17.22445 GB 
[09/17 08:26:29 visual_prompt]: 	Test 300/356. loss: 4.362, 0.1843 s / batch. (data: 1.40e-04)max mem: 17.22445 GB 
[09/17 08:26:41 visual_prompt]: Inference (test):avg data time: 7.09e-03, avg batch time: 0.1939, average loss: 4.7485
[09/17 08:26:41 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.82	top5: 95.05	
[09/17 08:26:41 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/17 08:26:51 visual_prompt]: Epoch 99 / 100: avg data time: 1.26e-01, avg batch time: 0.5296, average train loss: 0.0106
[09/17 08:26:55 visual_prompt]: Inference (val):avg data time: 4.79e-05, avg batch time: 0.1432, average loss: 0.0061
[09/17 08:26:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 100.00	top5: 100.00	
[09/17 08:27:16 visual_prompt]: 	Test 100/356. loss: 4.517, 0.1838 s / batch. (data: 1.47e-04)max mem: 17.22445 GB 
[09/17 08:27:36 visual_prompt]: 	Test 200/356. loss: 5.362, 0.2305 s / batch. (data: 4.73e-02)max mem: 17.22445 GB 
[09/17 08:27:55 visual_prompt]: 	Test 300/356. loss: 4.367, 0.1837 s / batch. (data: 1.13e-04)max mem: 17.22445 GB 
[09/17 08:28:08 visual_prompt]: Inference (test):avg data time: 8.45e-03, avg batch time: 0.1941, average loss: 4.7538
[09/17 08:28:08 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.90	top5: 95.01	
[09/17 08:28:08 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/17 08:28:17 visual_prompt]: Epoch 100 / 100: avg data time: 1.26e-01, avg batch time: 0.5284, average train loss: 0.0119
[09/17 08:28:21 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1431, average loss: 0.0064
[09/17 08:28:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 100.00	top5: 100.00	
[09/17 08:28:42 visual_prompt]: 	Test 100/356. loss: 4.516, 0.1959 s / batch. (data: 1.30e-02)max mem: 17.22445 GB 
[09/17 08:29:02 visual_prompt]: 	Test 200/356. loss: 5.365, 0.2077 s / batch. (data: 2.45e-02)max mem: 17.22445 GB 
[09/17 08:29:21 visual_prompt]: 	Test 300/356. loss: 4.367, 0.1835 s / batch. (data: 1.80e-04)max mem: 17.22445 GB 
[09/17 08:29:34 visual_prompt]: Inference (test):avg data time: 7.05e-03, avg batch time: 0.1933, average loss: 4.7554
[09/17 08:29:34 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.89	top5: 95.01	

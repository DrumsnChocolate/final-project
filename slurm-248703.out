/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '32', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '500', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/20 01:15:05 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 01:15:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 01:15:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '32', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '500', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/20 01:15:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 01:15:05 visual_prompt]: Training with config:
[11/20 01:15:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size500/val/seed0/lr0.005_wd0.01/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 500, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 32, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 01:15:05 visual_prompt]: Loading training data...
[11/20 01:15:05 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 01:15:05 visual_prompt]: Loading validation data...
[11/20 01:15:05 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 01:15:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 962, 768])
load_pretrained: grid-size from 14 to 31
[11/20 01:15:07 visual_prompt]: Enable all parameters update during training
[11/20 01:15:07 visual_prompt]: Total Parameters: 86387714	 Gradient Parameters: 86387714
[11/20 01:15:07 visual_prompt]: tuned percent:100.000
[11/20 01:15:07 visual_prompt]: Device used for model: 0
[11/20 01:15:07 visual_prompt]: Setting up Evaluator...
[11/20 01:15:07 visual_prompt]: Setting up Trainer...
[11/20 01:15:07 visual_prompt]: 	Setting up the optimizer...
[11/20 01:15:07 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 01:22:55 visual_prompt]: Epoch 1 / 100: avg data time: 5.01e+00, avg batch time: 6.6810, average train loss: 7.2380
[11/20 01:23:50 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.5441, average loss: 6.4181
[11/20 01:23:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 52.79	
[11/20 01:23:50 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/20 01:31:21 visual_prompt]: Epoch 2 / 100: avg data time: 4.81e+00, avg batch time: 6.4475, average train loss: 2.2194
[11/20 01:32:14 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.5419, average loss: 0.7706
[11/20 01:32:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.72	
[11/20 01:32:14 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/20 01:39:51 visual_prompt]: Epoch 3 / 100: avg data time: 4.88e+00, avg batch time: 6.5202, average train loss: 1.2897
[11/20 01:40:44 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.5451, average loss: 0.7009
[11/20 01:40:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.60	
[11/20 01:40:44 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/20 01:48:19 visual_prompt]: Epoch 4 / 100: avg data time: 4.86e+00, avg batch time: 6.4972, average train loss: 0.8243
[11/20 01:49:12 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.5415, average loss: 0.7718
[11/20 01:49:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.54	
[11/20 01:49:12 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/20 01:56:43 visual_prompt]: Epoch 5 / 100: avg data time: 4.82e+00, avg batch time: 6.4513, average train loss: 0.8154
[11/20 01:57:36 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.5423, average loss: 0.7126
[11/20 01:57:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.49	
[11/20 01:57:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/20 02:05:12 visual_prompt]: Epoch 6 / 100: avg data time: 4.87e+00, avg batch time: 6.5088, average train loss: 0.7972
[11/20 02:06:05 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.5427, average loss: 0.9161
[11/20 02:06:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.61	
[11/20 02:06:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/20 02:13:36 visual_prompt]: Epoch 7 / 100: avg data time: 4.81e+00, avg batch time: 6.4420, average train loss: 0.8133
[11/20 02:14:29 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.5442, average loss: 0.8203
[11/20 02:14:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.41	
[11/20 02:14:29 visual_prompt]: Best epoch 7: best metric: -0.820
[11/20 02:14:29 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/20 02:21:59 visual_prompt]: Epoch 8 / 100: avg data time: 4.79e+00, avg batch time: 6.4248, average train loss: 0.7528
[11/20 02:22:52 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.5425, average loss: 1.1130
[11/20 02:22:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.02	
[11/20 02:22:52 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/20 02:30:23 visual_prompt]: Epoch 9 / 100: avg data time: 4.80e+00, avg batch time: 6.4375, average train loss: 0.8056
[11/20 02:31:16 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.5441, average loss: 0.9090
[11/20 02:31:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.73	
[11/20 02:31:16 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/20 02:38:46 visual_prompt]: Epoch 10 / 100: avg data time: 4.80e+00, avg batch time: 6.4323, average train loss: 0.9338
[11/20 02:39:40 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5415, average loss: 0.7372
[11/20 02:39:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.72	
[11/20 02:39:40 visual_prompt]: Best epoch 10: best metric: -0.737
[11/20 02:39:40 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/20 02:47:12 visual_prompt]: Epoch 11 / 100: avg data time: 4.83e+00, avg batch time: 6.4582, average train loss: 0.7282
[11/20 02:48:05 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.5444, average loss: 0.7280
[11/20 02:48:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.28	
[11/20 02:48:05 visual_prompt]: Best epoch 11: best metric: -0.728
[11/20 02:48:05 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/20 02:55:37 visual_prompt]: Epoch 12 / 100: avg data time: 4.81e+00, avg batch time: 6.4499, average train loss: 0.7372
[11/20 02:56:30 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.5449, average loss: 1.1409
[11/20 02:56:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.66	
[11/20 02:56:30 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/20 03:04:04 visual_prompt]: Epoch 13 / 100: avg data time: 4.85e+00, avg batch time: 6.4856, average train loss: 0.7875
[11/20 03:04:58 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.5430, average loss: 0.7149
[11/20 03:04:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.24	
[11/20 03:04:58 visual_prompt]: Best epoch 13: best metric: -0.715
[11/20 03:04:58 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/20 03:12:36 visual_prompt]: Epoch 14 / 100: avg data time: 4.91e+00, avg batch time: 6.5413, average train loss: 0.7222
[11/20 03:13:30 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.5437, average loss: 0.6866
[11/20 03:13:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 57.77	
[11/20 03:13:30 visual_prompt]: Best epoch 14: best metric: -0.687
[11/20 03:13:30 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/20 03:21:06 visual_prompt]: Epoch 15 / 100: avg data time: 4.88e+00, avg batch time: 6.5102, average train loss: 0.7388
[11/20 03:21:59 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.5430, average loss: 0.7300
[11/20 03:21:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 59.11	
[11/20 03:21:59 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/20 03:29:34 visual_prompt]: Epoch 16 / 100: avg data time: 4.85e+00, avg batch time: 6.4843, average train loss: 0.7620
[11/20 03:30:27 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.5441, average loss: 0.6825
[11/20 03:30:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 57.69	
[11/20 03:30:27 visual_prompt]: Best epoch 16: best metric: -0.682
[11/20 03:30:27 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/20 03:38:03 visual_prompt]: Epoch 17 / 100: avg data time: 4.87e+00, avg batch time: 6.5075, average train loss: 0.7626
[11/20 03:38:56 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.5456, average loss: 0.7966
[11/20 03:38:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.82	
[11/20 03:38:56 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/20 03:46:30 visual_prompt]: Epoch 18 / 100: avg data time: 4.84e+00, avg batch time: 6.4802, average train loss: 0.7330
[11/20 03:47:23 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.5426, average loss: 0.8289
[11/20 03:47:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.75	
[11/20 03:47:23 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/20 03:54:59 visual_prompt]: Epoch 19 / 100: avg data time: 4.87e+00, avg batch time: 6.5067, average train loss: 0.7413
[11/20 03:55:53 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.5438, average loss: 0.7132
[11/20 03:55:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.18	
[11/20 03:55:53 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/20 04:03:29 visual_prompt]: Epoch 20 / 100: avg data time: 4.87e+00, avg batch time: 6.5095, average train loss: 0.7654
[11/20 04:04:22 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.5412, average loss: 0.6959
[11/20 04:04:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 56.51	
[11/20 04:04:22 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/20 04:11:58 visual_prompt]: Epoch 21 / 100: avg data time: 4.87e+00, avg batch time: 6.5066, average train loss: 0.6998
[11/20 04:12:52 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.5406, average loss: 0.6842
[11/20 04:12:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 54.43	
[11/20 04:12:52 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/20 04:20:29 visual_prompt]: Epoch 22 / 100: avg data time: 4.89e+00, avg batch time: 6.5274, average train loss: 1.0809
[11/20 04:21:23 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.5405, average loss: 0.6903
[11/20 04:21:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 56.82	
[11/20 04:21:23 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/20 04:28:56 visual_prompt]: Epoch 23 / 100: avg data time: 4.83e+00, avg batch time: 6.4698, average train loss: 0.7217
[11/20 04:29:49 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5428, average loss: 1.0101
[11/20 04:29:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.54	
[11/20 04:29:49 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/20 04:37:28 visual_prompt]: Epoch 24 / 100: avg data time: 4.92e+00, avg batch time: 6.5484, average train loss: 0.7400
[11/20 04:38:21 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5415, average loss: 0.6848
[11/20 04:38:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 57.10	
[11/20 04:38:21 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/20 04:45:58 visual_prompt]: Epoch 25 / 100: avg data time: 4.89e+00, avg batch time: 6.5288, average train loss: 0.7381
[11/20 04:46:52 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.5414, average loss: 0.8803
[11/20 04:46:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.38	
[11/20 04:46:52 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/20 04:54:27 visual_prompt]: Epoch 26 / 100: avg data time: 4.87e+00, avg batch time: 6.4965, average train loss: 0.7286
[11/20 04:55:20 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.5427, average loss: 0.6825
[11/20 04:55:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 57.61	
[11/20 04:55:20 visual_prompt]: Best epoch 26: best metric: -0.682
[11/20 04:55:20 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/20 05:02:57 visual_prompt]: Epoch 27 / 100: avg data time: 4.89e+00, avg batch time: 6.5260, average train loss: 0.7828
[11/20 05:03:51 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.5413, average loss: 0.6897
[11/20 05:03:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 57.78	
[11/20 05:03:51 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/20 05:11:25 visual_prompt]: Epoch 28 / 100: avg data time: 4.85e+00, avg batch time: 6.4894, average train loss: 0.7002
[11/20 05:12:19 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5405, average loss: 0.6897
[11/20 05:12:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.85	
[11/20 05:12:19 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.004311063011977723
[11/20 05:19:59 visual_prompt]: Epoch 29 / 100: avg data time: 4.93e+00, avg batch time: 6.5647, average train loss: 0.7287
[11/20 05:20:52 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.5412, average loss: 0.8027
[11/20 05:20:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.72	
[11/20 05:20:52 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.004253091960681222
[11/20 05:28:34 visual_prompt]: Epoch 30 / 100: avg data time: 4.95e+00, avg batch time: 6.5842, average train loss: 0.7160
[11/20 05:29:27 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.5459, average loss: 0.8022
[11/20 05:29:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.85	
[11/20 05:29:27 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.004193203929064353
[11/20 05:37:06 visual_prompt]: Epoch 31 / 100: avg data time: 4.92e+00, avg batch time: 6.5512, average train loss: 0.7725
[11/20 05:38:00 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.5433, average loss: 0.8350
[11/20 05:38:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.76	
[11/20 05:38:00 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.004131464403810421
[11/20 05:45:38 visual_prompt]: Epoch 32 / 100: avg data time: 4.90e+00, avg batch time: 6.5360, average train loss: 0.7050
[11/20 05:46:31 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.5437, average loss: 0.6943
[11/20 05:46:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 57.80	
[11/20 05:46:31 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.004067940896183842
[11/20 05:54:09 visual_prompt]: Epoch 33 / 100: avg data time: 4.91e+00, avg batch time: 6.5466, average train loss: 0.7377
[11/20 05:55:03 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.5418, average loss: 0.8513
[11/20 05:55:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.79	
[11/20 05:55:03 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.004002702868207563
[11/20 06:02:44 visual_prompt]: Epoch 34 / 100: avg data time: 4.94e+00, avg batch time: 6.5714, average train loss: 0.7433
[11/20 06:03:37 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.5402, average loss: 0.6928
[11/20 06:03:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.91	
[11/20 06:03:37 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.0039358216567073594
[11/20 06:11:12 visual_prompt]: Epoch 35 / 100: avg data time: 4.86e+00, avg batch time: 6.4855, average train loss: 0.7311
[11/20 06:12:05 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.5433, average loss: 0.6930
[11/20 06:12:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.85	rocauc: 57.68	
[11/20 06:12:05 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.003867370395306068
[11/20 06:19:42 visual_prompt]: Epoch 36 / 100: avg data time: 4.89e+00, avg batch time: 6.5300, average train loss: 0.7020
[11/20 06:20:36 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.5397, average loss: 0.7789
[11/20 06:20:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.55	
[11/20 06:20:36 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.0037974239344530382
[11/20 06:28:14 visual_prompt]: Epoch 37 / 100: avg data time: 4.91e+00, avg batch time: 6.5421, average train loss: 0.7208
[11/20 06:29:08 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.5418, average loss: 0.6881
[11/20 06:29:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.21	
[11/20 06:29:08 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.0037260587595762708
[11/20 06:36:45 visual_prompt]: Epoch 38 / 100: avg data time: 4.90e+00, avg batch time: 6.5311, average train loss: 0.7162
[11/20 06:37:38 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.5404, average loss: 0.8725
[11/20 06:37:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.19	
[11/20 06:37:38 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.0036533529074467197
[11/20 06:45:15 visual_prompt]: Epoch 39 / 100: avg data time: 4.89e+00, avg batch time: 6.5241, average train loss: 0.7172
[11/20 06:46:09 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.5421, average loss: 0.7034
[11/20 06:46:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.77	
[11/20 06:46:09 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.003579385880846232
[11/20 06:53:49 visual_prompt]: Epoch 40 / 100: avg data time: 4.94e+00, avg batch time: 6.5744, average train loss: 0.6949
[11/20 06:54:43 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.5422, average loss: 0.6892
[11/20 06:54:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 58.43	
[11/20 06:54:43 visual_prompt]: Stopping early.
[11/20 06:54:43 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 06:54:43 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 06:54:43 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '32', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '500', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/20 06:54:43 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 06:54:43 visual_prompt]: Training with config:
[11/20 06:54:43 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size500/val/seed0/lr0.005_wd0.001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 500, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 32, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 06:54:43 visual_prompt]: Loading training data...
[11/20 06:54:43 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 06:54:43 visual_prompt]: Loading validation data...
[11/20 06:54:43 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 06:54:43 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 962, 768])
load_pretrained: grid-size from 14 to 31
[11/20 06:54:45 visual_prompt]: Enable all parameters update during training
[11/20 06:54:45 visual_prompt]: Total Parameters: 86387714	 Gradient Parameters: 86387714
[11/20 06:54:45 visual_prompt]: tuned percent:100.000
[11/20 06:54:45 visual_prompt]: Device used for model: 0
[11/20 06:54:45 visual_prompt]: Setting up Evaluator...
[11/20 06:54:45 visual_prompt]: Setting up Trainer...
[11/20 06:54:45 visual_prompt]: 	Setting up the optimizer...
[11/20 06:54:45 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 07:02:16 visual_prompt]: Epoch 1 / 100: avg data time: 4.80e+00, avg batch time: 6.4338, average train loss: 7.2380
[11/20 07:03:09 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.5453, average loss: 6.4181
[11/20 07:03:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 52.79	
[11/20 07:03:09 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/20 07:10:35 visual_prompt]: Epoch 2 / 100: avg data time: 4.73e+00, avg batch time: 6.3668, average train loss: 2.2192
[11/20 07:11:28 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.5426, average loss: 0.7752
[11/20 07:11:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.36	
[11/20 07:11:28 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/20 07:18:57 visual_prompt]: Epoch 3 / 100: avg data time: 4.78e+00, avg batch time: 6.4161, average train loss: 1.2827
[11/20 07:19:50 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.5443, average loss: 0.6874
[11/20 07:19:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 55.72	
[11/20 07:19:50 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/20 07:27:19 visual_prompt]: Epoch 4 / 100: avg data time: 4.77e+00, avg batch time: 6.4058, average train loss: 0.8255
[11/20 07:28:11 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.5438, average loss: 0.7770
[11/20 07:28:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.69	
[11/20 07:28:11 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/20 07:35:40 visual_prompt]: Epoch 5 / 100: avg data time: 4.78e+00, avg batch time: 6.4116, average train loss: 0.8075
[11/20 07:36:33 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5426, average loss: 0.6897
[11/20 07:36:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 56.31	
[11/20 07:36:33 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/20 07:44:03 visual_prompt]: Epoch 6 / 100: avg data time: 4.78e+00, avg batch time: 6.4195, average train loss: 0.7958
[11/20 07:44:56 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.5411, average loss: 0.9376
[11/20 07:44:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.08	
[11/20 07:44:56 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/20 07:52:24 visual_prompt]: Epoch 7 / 100: avg data time: 4.76e+00, avg batch time: 6.4020, average train loss: 0.8203
[11/20 07:53:17 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.5440, average loss: 0.8084
[11/20 07:53:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.81	
[11/20 07:53:17 visual_prompt]: Best epoch 7: best metric: -0.808
[11/20 07:53:17 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/20 08:00:46 visual_prompt]: Epoch 8 / 100: avg data time: 4.78e+00, avg batch time: 6.4175, average train loss: 0.7706
[11/20 08:01:39 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5422, average loss: 0.9924
[11/20 08:01:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.47	
[11/20 08:01:39 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/20 08:09:09 visual_prompt]: Epoch 9 / 100: avg data time: 4.79e+00, avg batch time: 6.4246, average train loss: 0.7915
[11/20 08:10:02 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.5418, average loss: 0.8176
[11/20 08:10:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.37	
[11/20 08:10:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/20 08:17:32 visual_prompt]: Epoch 10 / 100: avg data time: 4.78e+00, avg batch time: 6.4209, average train loss: 0.8857
[11/20 08:18:24 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.5449, average loss: 0.6856
[11/20 08:18:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 56.79	
[11/20 08:18:24 visual_prompt]: Best epoch 10: best metric: -0.686
[11/20 08:18:24 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/20 08:25:54 visual_prompt]: Epoch 11 / 100: avg data time: 4.79e+00, avg batch time: 6.4257, average train loss: 0.7243
[11/20 08:26:48 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.5430, average loss: 0.7529
[11/20 08:26:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.48	
[11/20 08:26:48 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/20 08:34:17 visual_prompt]: Epoch 12 / 100: avg data time: 4.77e+00, avg batch time: 6.4122, average train loss: 0.7480
[11/20 08:35:10 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.5460, average loss: 1.1404
[11/20 08:35:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.49	
[11/20 08:35:10 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/20 08:42:40 visual_prompt]: Epoch 13 / 100: avg data time: 4.79e+00, avg batch time: 6.4230, average train loss: 0.7771
[11/20 08:43:33 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5422, average loss: 0.6976
[11/20 08:43:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 56.11	
[11/20 08:43:33 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/20 08:51:03 visual_prompt]: Epoch 14 / 100: avg data time: 4.79e+00, avg batch time: 6.4254, average train loss: 0.7200
[11/20 08:51:56 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.5441, average loss: 0.6877
[11/20 08:51:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 57.71	
[11/20 08:51:56 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/20 08:59:23 visual_prompt]: Epoch 15 / 100: avg data time: 4.74e+00, avg batch time: 6.3834, average train loss: 0.7429
[11/20 09:00:16 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.5425, average loss: 0.7309
[11/20 09:00:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 57.48	
[11/20 09:00:16 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/20 09:07:44 visual_prompt]: Epoch 16 / 100: avg data time: 4.76e+00, avg batch time: 6.3965, average train loss: 0.7794
[11/20 09:08:37 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.5440, average loss: 0.7134
[11/20 09:08:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.67	
[11/20 09:08:37 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/20 09:16:07 visual_prompt]: Epoch 17 / 100: avg data time: 4.79e+00, avg batch time: 6.4298, average train loss: 0.7646
[11/20 09:17:00 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.5421, average loss: 0.8459
[11/20 09:17:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.16	
[11/20 09:17:00 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/20 09:24:27 visual_prompt]: Epoch 18 / 100: avg data time: 4.75e+00, avg batch time: 6.3815, average train loss: 0.7518
[11/20 09:25:20 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.5427, average loss: 0.8831
[11/20 09:25:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.97	
[11/20 09:25:20 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/20 09:32:54 visual_prompt]: Epoch 19 / 100: avg data time: 4.84e+00, avg batch time: 6.4751, average train loss: 0.7500
[11/20 09:33:47 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.5440, average loss: 0.7796
[11/20 09:33:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.64	
[11/20 09:33:47 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/20 09:41:20 visual_prompt]: Epoch 20 / 100: avg data time: 4.82e+00, avg batch time: 6.4593, average train loss: 0.7700
[11/20 09:42:13 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.5416, average loss: 0.7034
[11/20 09:42:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.65	
[11/20 09:42:13 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/20 09:49:44 visual_prompt]: Epoch 21 / 100: avg data time: 4.80e+00, avg batch time: 6.4397, average train loss: 0.6973
[11/20 09:50:36 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.5445, average loss: 0.6941
[11/20 09:50:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 53.96	
[11/20 09:50:36 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/20 09:58:06 visual_prompt]: Epoch 22 / 100: avg data time: 4.78e+00, avg batch time: 6.4161, average train loss: 0.7300
[11/20 09:58:59 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.5442, average loss: 0.7725
[11/20 09:58:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.90	
[11/20 09:58:59 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/20 10:06:27 visual_prompt]: Epoch 23 / 100: avg data time: 4.77e+00, avg batch time: 6.4055, average train loss: 0.7028
[11/20 10:07:20 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5409, average loss: 0.8932
[11/20 10:07:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.17	
[11/20 10:07:20 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/20 10:14:55 visual_prompt]: Epoch 24 / 100: avg data time: 4.85e+00, avg batch time: 6.4847, average train loss: 0.7128
[11/20 10:15:47 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.5441, average loss: 0.6875
[11/20 10:15:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 53.54	
[11/20 10:15:47 visual_prompt]: Stopping early.
[11/20 10:15:48 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 10:15:48 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 10:15:48 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '32', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '500', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/20 10:15:48 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 10:15:48 visual_prompt]: Training with config:
[11/20 10:15:48 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size500/val/seed0/lr0.005_wd0.0001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 500, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 32, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 10:15:48 visual_prompt]: Loading training data...
[11/20 10:15:48 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 10:15:48 visual_prompt]: Loading validation data...
[11/20 10:15:48 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 10:15:48 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 962, 768])
load_pretrained: grid-size from 14 to 31
[11/20 10:15:49 visual_prompt]: Enable all parameters update during training
[11/20 10:15:49 visual_prompt]: Total Parameters: 86387714	 Gradient Parameters: 86387714
[11/20 10:15:49 visual_prompt]: tuned percent:100.000
[11/20 10:15:49 visual_prompt]: Device used for model: 0
[11/20 10:15:49 visual_prompt]: Setting up Evaluator...
[11/20 10:15:49 visual_prompt]: Setting up Trainer...
[11/20 10:15:49 visual_prompt]: 	Setting up the optimizer...
[11/20 10:15:49 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 10:23:21 visual_prompt]: Epoch 1 / 100: avg data time: 4.81e+00, avg batch time: 6.4514, average train loss: 7.2380
[11/20 10:24:14 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.5427, average loss: 6.4181
[11/20 10:24:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 52.79	
[11/20 10:24:14 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/20 10:31:41 visual_prompt]: Epoch 2 / 100: avg data time: 4.74e+00, avg batch time: 6.3762, average train loss: 2.2196
[11/20 10:32:34 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.5444, average loss: 0.7767
[11/20 10:32:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.02	
[11/20 10:32:34 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/20 10:40:07 visual_prompt]: Epoch 3 / 100: avg data time: 4.83e+00, avg batch time: 6.4678, average train loss: 1.2920
[11/20 10:41:00 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.5417, average loss: 0.7083
[11/20 10:41:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.70	
[11/20 10:41:00 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/20 10:48:31 visual_prompt]: Epoch 4 / 100: avg data time: 4.79e+00, avg batch time: 6.4333, average train loss: 0.8236
[11/20 10:49:24 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.5430, average loss: 0.7803
[11/20 10:49:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.68	
[11/20 10:49:24 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/20 10:56:52 visual_prompt]: Epoch 5 / 100: avg data time: 4.76e+00, avg batch time: 6.3998, average train loss: 0.8073
[11/20 10:57:45 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5441, average loss: 0.6933
[11/20 10:57:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 56.83	
[11/20 10:57:45 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/20 11:05:16 visual_prompt]: Epoch 6 / 100: avg data time: 4.80e+00, avg batch time: 6.4397, average train loss: 0.7979
[11/20 11:06:09 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5426, average loss: 0.9569
[11/20 11:06:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.39	
[11/20 11:06:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/20 11:13:37 visual_prompt]: Epoch 7 / 100: avg data time: 4.76e+00, avg batch time: 6.3963, average train loss: 0.8238
[11/20 11:14:30 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5443, average loss: 0.7884
[11/20 11:14:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.26	
[11/20 11:14:30 visual_prompt]: Best epoch 7: best metric: -0.788
[11/20 11:14:30 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/20 11:22:01 visual_prompt]: Epoch 8 / 100: avg data time: 4.80e+00, avg batch time: 6.4363, average train loss: 0.7489
[11/20 11:22:54 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5430, average loss: 1.1168
[11/20 11:22:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.56	
[11/20 11:22:54 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/20 11:30:24 visual_prompt]: Epoch 9 / 100: avg data time: 4.78e+00, avg batch time: 6.4201, average train loss: 0.8048
[11/20 11:31:17 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5444, average loss: 0.9318
[11/20 11:31:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.49	
[11/20 11:31:17 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/20 11:38:51 visual_prompt]: Epoch 10 / 100: avg data time: 4.84e+00, avg batch time: 6.4836, average train loss: 0.9228
[11/20 11:39:44 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.5409, average loss: 0.7019
[11/20 11:39:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.96	
[11/20 11:39:44 visual_prompt]: Best epoch 10: best metric: -0.702
[11/20 11:39:44 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/20 11:47:15 visual_prompt]: Epoch 11 / 100: avg data time: 4.80e+00, avg batch time: 6.4341, average train loss: 0.7250
[11/20 11:48:08 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.5444, average loss: 0.7297
[11/20 11:48:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.05	
[11/20 11:48:08 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/20 11:55:40 visual_prompt]: Epoch 12 / 100: avg data time: 4.81e+00, avg batch time: 6.4455, average train loss: 0.8387
[11/20 11:56:33 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.5441, average loss: 1.0135
[11/20 11:56:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.98	
[11/20 11:56:33 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/20 12:04:15 visual_prompt]: Epoch 13 / 100: avg data time: 4.97e+00, avg batch time: 6.6047, average train loss: 0.8388
[11/20 12:05:09 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.5408, average loss: 0.6882
[11/20 12:05:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.81	
[11/20 12:05:09 visual_prompt]: Best epoch 13: best metric: -0.688
[11/20 12:05:09 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/20 12:12:42 visual_prompt]: Epoch 14 / 100: avg data time: 4.83e+00, avg batch time: 6.4691, average train loss: 0.7347
[11/20 12:13:34 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.5438, average loss: 0.6982
[11/20 12:13:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.34	
[11/20 12:13:34 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/20 12:21:06 visual_prompt]: Epoch 15 / 100: avg data time: 4.80e+00, avg batch time: 6.4438, average train loss: 0.7537
[11/20 12:21:59 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.5401, average loss: 0.6952
[11/20 12:21:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.80	
[11/20 12:21:59 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/20 12:29:28 visual_prompt]: Epoch 16 / 100: avg data time: 4.78e+00, avg batch time: 6.4199, average train loss: 0.7590
[11/20 12:30:21 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.5405, average loss: 0.6872
[11/20 12:30:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 53.94	
[11/20 12:30:21 visual_prompt]: Best epoch 16: best metric: -0.687
[11/20 12:30:21 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/20 12:38:12 visual_prompt]: Epoch 17 / 100: avg data time: 5.08e+00, avg batch time: 6.7164, average train loss: 0.7646
[11/20 12:39:07 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.5416, average loss: 0.7861
[11/20 12:39:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.28	
[11/20 12:39:07 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/20 12:46:37 visual_prompt]: Epoch 18 / 100: avg data time: 4.80e+00, avg batch time: 6.4312, average train loss: 0.7416
[11/20 12:47:30 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.5438, average loss: 0.8383
[11/20 12:47:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.28	
[11/20 12:47:30 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/20 12:55:01 visual_prompt]: Epoch 19 / 100: avg data time: 4.80e+00, avg batch time: 6.4373, average train loss: 0.7490
[11/20 12:55:54 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.5446, average loss: 0.7557
[11/20 12:55:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.48	
[11/20 12:55:54 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/20 13:03:25 visual_prompt]: Epoch 20 / 100: avg data time: 4.80e+00, avg batch time: 6.4388, average train loss: 0.7884
[11/20 13:04:18 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.5430, average loss: 0.7892
[11/20 13:04:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.98	
[11/20 13:04:18 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/20 13:11:47 visual_prompt]: Epoch 21 / 100: avg data time: 4.78e+00, avg batch time: 6.4206, average train loss: 0.7076
[11/20 13:12:40 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.5434, average loss: 0.6935
[11/20 13:12:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 53.60	
[11/20 13:12:40 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/20 13:20:06 visual_prompt]: Epoch 22 / 100: avg data time: 4.74e+00, avg batch time: 6.3753, average train loss: 0.7248
[11/20 13:20:59 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.5434, average loss: 0.7816
[11/20 13:20:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.19	
[11/20 13:20:59 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/20 13:28:26 visual_prompt]: Epoch 23 / 100: avg data time: 4.75e+00, avg batch time: 6.3860, average train loss: 0.7075
[11/20 13:29:19 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.5414, average loss: 0.8944
[11/20 13:29:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.63	
[11/20 13:29:19 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/20 13:36:46 visual_prompt]: Epoch 24 / 100: avg data time: 4.76e+00, avg batch time: 6.3896, average train loss: 0.7165
[11/20 13:37:39 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.5429, average loss: 0.6881
[11/20 13:37:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 54.21	
[11/20 13:37:39 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/20 13:45:06 visual_prompt]: Epoch 25 / 100: avg data time: 4.75e+00, avg batch time: 6.3851, average train loss: 0.7546
[11/20 13:45:59 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5411, average loss: 0.8443
[11/20 13:45:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.06	
[11/20 13:45:59 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/20 13:53:28 visual_prompt]: Epoch 26 / 100: avg data time: 4.77e+00, avg batch time: 6.4069, average train loss: 0.7188
[11/20 13:54:20 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.5449, average loss: 0.6863
[11/20 13:54:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 54.72	
[11/20 13:54:20 visual_prompt]: Best epoch 26: best metric: -0.686
[11/20 13:54:20 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/20 14:01:49 visual_prompt]: Epoch 27 / 100: avg data time: 4.78e+00, avg batch time: 6.4134, average train loss: 0.7821
[11/20 14:02:42 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.5414, average loss: 0.6848
[11/20 14:02:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 54.78	
[11/20 14:02:42 visual_prompt]: Best epoch 27: best metric: -0.685
[11/20 14:02:42 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/20 14:10:06 visual_prompt]: Epoch 28 / 100: avg data time: 4.71e+00, avg batch time: 6.3477, average train loss: 0.6983
[11/20 14:10:59 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.5450, average loss: 0.6917
[11/20 14:10:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 55.67	
[11/20 14:10:59 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.004311063011977723
[11/20 14:18:25 visual_prompt]: Epoch 29 / 100: avg data time: 4.73e+00, avg batch time: 6.3686, average train loss: 0.7284
[11/20 14:19:18 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.5413, average loss: 0.8665
[11/20 14:19:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.94	
[11/20 14:19:18 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.004253091960681222
[11/20 14:26:46 visual_prompt]: Epoch 30 / 100: avg data time: 4.77e+00, avg batch time: 6.4012, average train loss: 0.7193
[11/20 14:27:39 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.5441, average loss: 0.7902
[11/20 14:27:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.20	
[11/20 14:27:39 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.004193203929064353
[11/20 14:35:05 visual_prompt]: Epoch 31 / 100: avg data time: 4.73e+00, avg batch time: 6.3637, average train loss: 0.7753
[11/20 14:35:58 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.5418, average loss: 0.8190
[11/20 14:35:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.66	
[11/20 14:35:58 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.004131464403810421
[11/20 14:43:23 visual_prompt]: Epoch 32 / 100: avg data time: 4.72e+00, avg batch time: 6.3560, average train loss: 0.7039
[11/20 14:44:15 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5444, average loss: 0.6859
[11/20 14:44:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 55.89	
[11/20 14:44:16 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.004067940896183842
[11/20 14:51:41 visual_prompt]: Epoch 33 / 100: avg data time: 4.73e+00, avg batch time: 6.3681, average train loss: 0.7343
[11/20 14:52:34 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.5441, average loss: 0.8261
[11/20 14:52:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.27	
[11/20 14:52:34 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.004002702868207563
[11/20 14:59:59 visual_prompt]: Epoch 34 / 100: avg data time: 4.72e+00, avg batch time: 6.3558, average train loss: 0.7429
[11/20 15:00:52 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.5420, average loss: 0.7118
[11/20 15:00:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.36	
[11/20 15:00:52 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.0039358216567073594
[11/20 15:08:18 visual_prompt]: Epoch 35 / 100: avg data time: 4.73e+00, avg batch time: 6.3704, average train loss: 0.7275
[11/20 15:09:11 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.5434, average loss: 0.6864
[11/20 15:09:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.84	
[11/20 15:09:11 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.003867370395306068
[11/20 15:16:42 visual_prompt]: Epoch 36 / 100: avg data time: 4.81e+00, avg batch time: 6.4426, average train loss: 0.6925
[11/20 15:17:35 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.5427, average loss: 0.7482
[11/20 15:17:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 56.26	
[11/20 15:17:35 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.0037974239344530382
[11/20 15:25:04 visual_prompt]: Epoch 37 / 100: avg data time: 4.78e+00, avg batch time: 6.4104, average train loss: 0.7075
[11/20 15:25:57 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.5421, average loss: 0.6890
[11/20 15:25:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 54.56	
[11/20 15:25:57 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.0037260587595762708
[11/20 15:33:27 visual_prompt]: Epoch 38 / 100: avg data time: 4.79e+00, avg batch time: 6.4259, average train loss: 0.7049
[11/20 15:34:20 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.5449, average loss: 0.9226
[11/20 15:34:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.69	
[11/20 15:34:20 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.0036533529074467197
[11/20 15:41:47 visual_prompt]: Epoch 39 / 100: avg data time: 4.75e+00, avg batch time: 6.3824, average train loss: 0.7123
[11/20 15:42:40 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.5448, average loss: 0.6949
[11/20 15:42:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 54.19	
[11/20 15:42:40 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.003579385880846232
[11/20 15:50:10 visual_prompt]: Epoch 40 / 100: avg data time: 4.78e+00, avg batch time: 6.4200, average train loss: 0.6841
[11/20 15:51:03 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.5432, average loss: 0.6906
[11/20 15:51:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 54.72	
[11/20 15:51:03 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.003504238561632424
[11/20 15:58:31 visual_prompt]: Epoch 41 / 100: avg data time: 4.77e+00, avg batch time: 6.4061, average train loss: 0.6872
[11/20 15:59:24 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.5448, average loss: 0.7013
[11/20 15:59:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.43	
[11/20 15:59:24 visual_prompt]: Stopping early.
[11/20 15:59:24 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 15:59:24 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 15:59:24 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '32', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '500', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/20 15:59:24 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 15:59:24 visual_prompt]: Training with config:
[11/20 15:59:24 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size500/val/seed0/lr0.005_wd0.0/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 500, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 32, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 15:59:24 visual_prompt]: Loading training data...
[11/20 15:59:24 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 15:59:24 visual_prompt]: Loading validation data...
[11/20 15:59:24 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 15:59:24 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 962, 768])
load_pretrained: grid-size from 14 to 31
[11/20 15:59:26 visual_prompt]: Enable all parameters update during training
[11/20 15:59:26 visual_prompt]: Total Parameters: 86387714	 Gradient Parameters: 86387714
[11/20 15:59:26 visual_prompt]: tuned percent:100.000
[11/20 15:59:26 visual_prompt]: Device used for model: 0
[11/20 15:59:26 visual_prompt]: Setting up Evaluator...
[11/20 15:59:26 visual_prompt]: Setting up Trainer...
[11/20 15:59:26 visual_prompt]: 	Setting up the optimizer...
[11/20 15:59:26 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 16:06:56 visual_prompt]: Epoch 1 / 100: avg data time: 4.78e+00, avg batch time: 6.4171, average train loss: 7.2380
[11/20 16:07:49 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.5428, average loss: 6.4181
[11/20 16:07:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 52.79	
[11/20 16:07:49 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/20 16:15:16 visual_prompt]: Epoch 2 / 100: avg data time: 4.76e+00, avg batch time: 6.3867, average train loss: 5.8714
[11/20 16:16:09 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.5436, average loss: 0.8187
[11/20 16:16:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.94	
[11/20 16:16:09 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/20 16:23:42 visual_prompt]: Epoch 3 / 100: avg data time: 4.83e+00, avg batch time: 6.4585, average train loss: 1.0383
[11/20 16:24:35 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.5412, average loss: 1.1923
[11/20 16:24:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.83	
[11/20 16:24:35 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/20 16:32:05 visual_prompt]: Epoch 4 / 100: avg data time: 4.79e+00, avg batch time: 6.4212, average train loss: 0.8516
[11/20 16:32:58 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.5416, average loss: 1.1714
[11/20 16:32:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.91	
[11/20 16:32:58 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/20 16:40:24 visual_prompt]: Epoch 5 / 100: avg data time: 4.75e+00, avg batch time: 6.3760, average train loss: 1.0000
[11/20 16:41:18 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.5433, average loss: 0.8054
[11/20 16:41:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.02	
[11/20 16:41:18 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/20 16:48:46 visual_prompt]: Epoch 6 / 100: avg data time: 4.77e+00, avg batch time: 6.4020, average train loss: 1.1659
[11/20 16:49:39 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.5404, average loss: 1.3005
[11/20 16:49:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.05	
[11/20 16:49:39 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/20 16:57:07 visual_prompt]: Epoch 7 / 100: avg data time: 4.77e+00, avg batch time: 6.3985, average train loss: 1.0626
[11/20 16:58:00 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.5399, average loss: 0.7815
[11/20 16:58:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.84	
[11/20 16:58:00 visual_prompt]: Best epoch 7: best metric: -0.782
[11/20 16:58:00 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/20 17:05:28 visual_prompt]: Epoch 8 / 100: avg data time: 4.78e+00, avg batch time: 6.4055, average train loss: 0.8325
[11/20 17:06:21 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.5410, average loss: 0.7457
[11/20 17:06:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.99	
[11/20 17:06:21 visual_prompt]: Best epoch 8: best metric: -0.746
[11/20 17:06:21 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/20 17:13:52 visual_prompt]: Epoch 9 / 100: avg data time: 4.80e+00, avg batch time: 6.4341, average train loss: 1.6967
[11/20 17:14:45 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.5431, average loss: 4.8432
[11/20 17:14:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.36	
[11/20 17:14:45 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/20 17:22:14 visual_prompt]: Epoch 10 / 100: avg data time: 4.79e+00, avg batch time: 6.4161, average train loss: 1.7329
[11/20 17:23:08 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.5434, average loss: 0.8465
[11/20 17:23:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.01	
[11/20 17:23:08 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/20 17:30:39 visual_prompt]: Epoch 11 / 100: avg data time: 4.81e+00, avg batch time: 6.4368, average train loss: 0.8668
[11/20 17:31:32 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.5431, average loss: 1.5532
[11/20 17:31:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.94	
[11/20 17:31:32 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/20 17:39:00 visual_prompt]: Epoch 12 / 100: avg data time: 4.76e+00, avg batch time: 6.3936, average train loss: 1.1932
[11/20 17:39:53 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.5427, average loss: 1.8942
[11/20 17:39:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.38	
[11/20 17:39:53 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/20 17:47:24 visual_prompt]: Epoch 13 / 100: avg data time: 4.81e+00, avg batch time: 6.4386, average train loss: 1.1558
[11/20 17:48:17 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.5417, average loss: 0.7035
[11/20 17:48:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.43	
[11/20 17:48:17 visual_prompt]: Best epoch 13: best metric: -0.704
[11/20 17:48:17 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/20 17:55:49 visual_prompt]: Epoch 14 / 100: avg data time: 4.82e+00, avg batch time: 6.4476, average train loss: 0.8803
[11/20 17:56:42 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.5432, average loss: 2.0123
[11/20 17:56:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.87	
[11/20 17:56:42 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/20 18:04:09 visual_prompt]: Epoch 15 / 100: avg data time: 4.76e+00, avg batch time: 6.3850, average train loss: 0.9680
[11/20 18:05:02 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.5416, average loss: 0.8171
[11/20 18:05:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.10	
[11/20 18:05:02 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/20 18:12:29 visual_prompt]: Epoch 16 / 100: avg data time: 4.75e+00, avg batch time: 6.3822, average train loss: 0.7931
[11/20 18:13:22 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.5406, average loss: 0.6987
[11/20 18:13:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 56.44	
[11/20 18:13:22 visual_prompt]: Best epoch 16: best metric: -0.699
[11/20 18:13:22 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/20 18:20:51 visual_prompt]: Epoch 17 / 100: avg data time: 4.79e+00, avg batch time: 6.4200, average train loss: 1.0472
[11/20 18:21:45 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.5420, average loss: 0.7260
[11/20 18:21:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 56.73	
[11/20 18:21:45 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/20 18:29:12 visual_prompt]: Epoch 18 / 100: avg data time: 4.76e+00, avg batch time: 6.3927, average train loss: 0.8943
[11/20 18:30:06 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.5426, average loss: 1.6341
[11/20 18:30:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.98	
[11/20 18:30:06 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/20 18:37:38 visual_prompt]: Epoch 19 / 100: avg data time: 4.83e+00, avg batch time: 6.4591, average train loss: 1.0053
[11/20 18:38:31 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.5430, average loss: 0.7038
[11/20 18:38:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 57.12	
[11/20 18:38:31 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/20 18:46:01 visual_prompt]: Epoch 20 / 100: avg data time: 4.80e+00, avg batch time: 6.4321, average train loss: 0.8345
[11/20 18:46:54 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.5420, average loss: 0.6936
[11/20 18:46:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.70	
[11/20 18:46:54 visual_prompt]: Best epoch 20: best metric: -0.694
[11/20 18:46:54 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/20 18:54:24 visual_prompt]: Epoch 21 / 100: avg data time: 4.79e+00, avg batch time: 6.4236, average train loss: 0.8543
[11/20 18:55:17 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.5410, average loss: 1.4087
[11/20 18:55:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.79	
[11/20 18:55:17 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/20 19:02:44 visual_prompt]: Epoch 22 / 100: avg data time: 4.75e+00, avg batch time: 6.3812, average train loss: 0.9040
[11/20 19:03:37 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.5430, average loss: 0.8204
[11/20 19:03:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.84	
[11/20 19:03:37 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/20 19:11:06 visual_prompt]: Epoch 23 / 100: avg data time: 4.78e+00, avg batch time: 6.4074, average train loss: 0.9800
[11/20 19:11:59 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.5403, average loss: 1.5715
[11/20 19:11:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.59	
[11/20 19:11:59 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/20 19:19:33 visual_prompt]: Epoch 24 / 100: avg data time: 4.85e+00, avg batch time: 6.4764, average train loss: 0.9440
[11/20 19:20:26 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.5413, average loss: 0.7604
[11/20 19:20:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.97	
[11/20 19:20:26 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/20 19:27:55 visual_prompt]: Epoch 25 / 100: avg data time: 4.78e+00, avg batch time: 6.4124, average train loss: 0.7410
[11/20 19:28:49 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.5401, average loss: 1.0651
[11/20 19:28:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.98	
[11/20 19:28:49 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/20 19:36:20 visual_prompt]: Epoch 26 / 100: avg data time: 4.82e+00, avg batch time: 6.4431, average train loss: 1.0309
[11/20 19:37:13 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.5418, average loss: 1.6621
[11/20 19:37:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.62	
[11/20 19:37:13 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/20 19:44:42 visual_prompt]: Epoch 27 / 100: avg data time: 4.77e+00, avg batch time: 6.4022, average train loss: 1.0292
[11/20 19:45:35 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.5418, average loss: 0.6965
[11/20 19:45:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 57.71	
[11/20 19:45:35 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/20 19:53:03 visual_prompt]: Epoch 28 / 100: avg data time: 4.77e+00, avg batch time: 6.4002, average train loss: 0.8511
[11/20 19:53:56 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.5436, average loss: 1.0475
[11/20 19:53:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.22	
[11/20 19:53:56 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.004311063011977723
[11/20 20:01:21 visual_prompt]: Epoch 29 / 100: avg data time: 4.73e+00, avg batch time: 6.3630, average train loss: 0.8130
[11/20 20:02:14 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.5424, average loss: 0.7352
[11/20 20:02:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 58.62	
[11/20 20:02:14 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.004253091960681222
[11/20 20:09:44 visual_prompt]: Epoch 30 / 100: avg data time: 4.79e+00, avg batch time: 6.4170, average train loss: 0.9407
[11/20 20:10:37 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.5403, average loss: 0.7847
[11/20 20:10:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 58.65	
[11/20 20:10:37 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.004193203929064353
[11/20 20:18:03 visual_prompt]: Epoch 31 / 100: avg data time: 4.74e+00, avg batch time: 6.3699, average train loss: 0.7301
[11/20 20:18:56 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.5406, average loss: 1.1686
[11/20 20:18:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.81	
[11/20 20:18:56 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.004131464403810421
[11/20 20:26:24 visual_prompt]: Epoch 32 / 100: avg data time: 4.77e+00, avg batch time: 6.4008, average train loss: 0.8855
[11/20 20:27:17 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.5418, average loss: 0.6999
[11/20 20:27:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 59.49	
[11/20 20:27:17 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.004067940896183842
[11/20 20:34:44 visual_prompt]: Epoch 33 / 100: avg data time: 4.75e+00, avg batch time: 6.3801, average train loss: 0.7578
[11/20 20:35:37 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.5408, average loss: 0.9215
[11/20 20:35:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.53	
[11/20 20:35:37 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.004002702868207563
[11/20 20:43:07 visual_prompt]: Epoch 34 / 100: avg data time: 4.78e+00, avg batch time: 6.4132, average train loss: 0.8077
[11/20 20:44:00 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.5428, average loss: 0.8848
[11/20 20:44:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.16	
[11/20 20:44:00 visual_prompt]: Stopping early.
[11/20 20:44:00 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 20:44:00 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 20:44:00 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '32', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '500', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/20 20:44:00 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 20:44:00 visual_prompt]: Training with config:
[11/20 20:44:00 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size500/val/seed0/lr0.001_wd0.01/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 500, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 32, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 20:44:00 visual_prompt]: Loading training data...
[11/20 20:44:00 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 20:44:00 visual_prompt]: Loading validation data...
[11/20 20:44:00 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 20:44:00 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 962, 768])
load_pretrained: grid-size from 14 to 31
[11/20 20:44:02 visual_prompt]: Enable all parameters update during training
[11/20 20:44:02 visual_prompt]: Total Parameters: 86387714	 Gradient Parameters: 86387714
[11/20 20:44:02 visual_prompt]: tuned percent:100.000
[11/20 20:44:02 visual_prompt]: Device used for model: 0
[11/20 20:44:02 visual_prompt]: Setting up Evaluator...
[11/20 20:44:02 visual_prompt]: Setting up Trainer...
[11/20 20:44:02 visual_prompt]: 	Setting up the optimizer...
[11/20 20:44:02 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
Traceback (most recent call last):
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 99, in <module>
    main(args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 83, in main
    explore_lrwd_range(args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 79, in explore_lrwd_range
    train(cfg, args, test=False)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 44, in train
    trainer.train_classifier(train_loader, val_loader, test_loader)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/engine/trainer.py", line 187, in train_classifier
    train_loss, _ = self.forward_one_batch(X, targets, True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/engine/trainer.py", line 120, in forward_one_batch
    loss.backward()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.32 GiB (GPU 0; 44.35 GiB total capacity; 41.12 GiB already allocated; 806.75 MiB free; 43.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)

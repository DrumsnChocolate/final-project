/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
/home/s1952889/miniconda3/envs/segment_anything/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/s1952889/miniconda3/envs/segment_anything/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
{'model': {'optimizer': {'name': 'sgd', 'lr': 0.01, 'wd': 0.0005, 'momentum': 0.9}, 'finetuning': {'name': 'full'}, 'name': 'sam', 'checkpoint': 'checkpoints/sam_vit_l_0b3195.pth', 'backbone': 'vit_l', 'loss': {'reduction': 'mean', 'parts': [{'name': 'Dice', 'weight': 1}, {'name': 'Focal', 'weight': 20}]}, 'metrics': [{'name': 'Dice'}, {'name': 'Focal'}, {'name': 'IoU'}]}, 'device': 'cuda', 'data': {'name': 'ade20k', 'root': 'data/ade/ADEChallengeData2016', 'image_extension': '.jpg', 'annotation_extension': '.png', 'preprocess': [{'name': 'resize', 'dimensions': [512, 512], 'mode': 'bilinear'}], 'train': {'image_dir': 'images/training', 'annotation_dir': 'annotations/training', 'batch_size': 8}, 'val': {'image_dir': 'images/validation', 'annotation_dir': 'annotations/validation', 'batch_size': 8}, 'test': {'image_dir': 'images/validation', 'annotation_dir': 'annotations/validation', 'batch_size': 8}}, '_bases_': ['finetune/configs/_base_/datasets/ade20k.yaml', 'finetune/configs/_base_/models/sam.yaml', 'finetune/configs/_base_/finetuning/full.yaml'], 'out': 'finetune/outputs', 'schedule': {'iterations': 160000, 'val_interval': 16000, 'log_interval': 1000}, 'metrics': ['mIoU'], 'out_dir': 'outputs'}
Traceback (most recent call last):
  File "/home/s1952889/final-project/implementation/segment_anything/finetune/test.py", line 24, in <module>
    main()
  File "/home/s1952889/final-project/implementation/segment_anything/finetune/test.py", line 21, in main
    test(cfg)
  File "/home/s1952889/final-project/implementation/segment_anything/finetune/test.py", line 15, in test
    test_epoch(cfg, model, loss_function, metric_functions, dataloaders, logger)
  File "/home/s1952889/final-project/implementation/segment_anything/finetune/train.py", line 179, in test_epoch
    outputs = model(samples, foreground_points)
  File "/home/s1952889/final-project/implementation/segment_anything/finetune/models.py", line 47, in __call__
    image_embeddings = self.model.image_encoder(preprocessed_samples)
  File "/home/s1952889/miniconda3/envs/segment_anything/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s1952889/final-project/implementation/segment_anything/segment_anything/modeling/image_encoder.py", line 112, in forward
    x = blk(x)
  File "/home/s1952889/miniconda3/envs/segment_anything/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s1952889/final-project/implementation/segment_anything/segment_anything/modeling/image_encoder.py", line 174, in forward
    x = self.attn(x)
  File "/home/s1952889/miniconda3/envs/segment_anything/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s1952889/final-project/implementation/segment_anything/segment_anything/modeling/image_encoder.py", line 231, in forward
    attn = (q * self.scale) @ k.transpose(-2, -1)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.00 GiB (GPU 0; 44.35 GiB total capacity; 41.49 GiB already allocated; 1.37 GiB free; 42.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)

/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_azimuth")', 'DATA.NUMBER_CLASSES', '18', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/28 05:09:56 visual_prompt]: Rank of current process: 0. World size: 1
[09/28 05:09:56 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/28 05:09:56 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_azimuth")', 'DATA.NUMBER_CLASSES', '18', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/28 05:09:56 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/28 05:09:56 visual_prompt]: Training with config:
[09/28 05:09:56 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_azimuth")/sup_vitb16_imagenet21k/prompt50/crop224/test/seed6530/lr0.25_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 6530, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_azimuth")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 18, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/28 05:09:56 visual_prompt]: Loading training data...
2023-09-28 05:09:56.598073: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-28 05:09:56.648141: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-28 05:10:00.638120: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[09/28 05:10:10 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset trainval...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800]+test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/28 05:10:12 visual_prompt]: Number of images: 1000
[09/28 05:10:12 visual_prompt]: Number of classes: 18 / 18
[09/28 05:10:12 visual_prompt]: Loading validation data...
[09/28 05:10:12 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/28 05:10:12 visual_prompt]: Number of images: 200
[09/28 05:10:12 visual_prompt]: Number of classes: 18 / 18
[09/28 05:10:12 visual_prompt]: Loading test data...
[09/28 05:10:12 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset test...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[50%:], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/28 05:10:30 visual_prompt]: Number of images: 12150
[09/28 05:10:30 visual_prompt]: Number of classes: 18 / 18
[09/28 05:10:30 visual_prompt]: Constructing models...
[09/28 05:10:33 visual_prompt]: Total Parameters: 86273298	 Gradient Parameters: 474642
[09/28 05:10:33 visual_prompt]: tuned percent:0.550
[09/28 05:10:36 visual_prompt]: Device used for model: 0
[09/28 05:10:36 visual_prompt]: Setting up Evaluator...
[09/28 05:10:36 visual_prompt]: Setting up Trainer...
[09/28 05:10:36 visual_prompt]: 	Setting up the optimizer...
[09/28 05:10:36 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/28 05:10:49 visual_prompt]: Epoch 1 / 100: avg data time: 1.94e-01, avg batch time: 0.8001, average train loss: 3.1910
[09/28 05:10:53 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1751, average loss: 3.2023
[09/28 05:10:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 26.50	
[09/28 05:11:15 visual_prompt]: 	Test 100/190. loss: 3.059, 0.2019 s / batch. (data: 3.17e-05)max mem: 7.80438 GB 
[09/28 05:11:34 visual_prompt]: Inference (test):avg data time: 1.47e-04, avg batch time: 0.2030, average loss: 3.2147
[09/28 05:11:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.44	top5: 27.60	
[09/28 05:11:34 visual_prompt]: Best epoch 1: best metric: 0.055
[09/28 05:11:34 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/28 05:11:44 visual_prompt]: Epoch 2 / 100: avg data time: 1.05e-01, avg batch time: 0.5301, average train loss: 2.9392
[09/28 05:11:47 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1754, average loss: 2.9000
[09/28 05:11:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 33.50	
[09/28 05:12:09 visual_prompt]: 	Test 100/190. loss: 2.974, 0.2043 s / batch. (data: 3.72e-05)max mem: 7.80438 GB 
[09/28 05:12:29 visual_prompt]: Inference (test):avg data time: 3.24e-05, avg batch time: 0.2044, average loss: 2.9193
[09/28 05:12:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.63	top5: 28.02	
[09/28 05:12:29 visual_prompt]: Best epoch 2: best metric: 0.060
[09/28 05:12:29 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/28 05:12:38 visual_prompt]: Epoch 3 / 100: avg data time: 1.12e-01, avg batch time: 0.5394, average train loss: 2.9133
[09/28 05:12:41 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.1803, average loss: 2.9063
[09/28 05:12:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 29.50	
[09/28 05:13:04 visual_prompt]: 	Test 100/190. loss: 2.897, 0.2047 s / batch. (data: 2.86e-05)max mem: 7.80438 GB 
[09/28 05:13:23 visual_prompt]: Inference (test):avg data time: 3.26e-05, avg batch time: 0.2044, average loss: 2.9108
[09/28 05:13:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.97	top5: 27.64	
[09/28 05:13:23 visual_prompt]: Best epoch 3: best metric: 0.075
[09/28 05:13:23 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/28 05:13:33 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e-01, avg batch time: 0.5318, average train loss: 2.9037
[09/28 05:13:36 visual_prompt]: Inference (val):avg data time: 1.50e-05, avg batch time: 0.1834, average loss: 2.8852
[09/28 05:13:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 32.00	
[09/28 05:13:58 visual_prompt]: 	Test 100/190. loss: 2.888, 0.2059 s / batch. (data: 2.55e-05)max mem: 7.80438 GB 
[09/28 05:14:18 visual_prompt]: Inference (test):avg data time: 5.07e-05, avg batch time: 0.2048, average loss: 2.8993
[09/28 05:14:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.27	top5: 27.67	
[09/28 05:14:18 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/28 05:14:27 visual_prompt]: Epoch 5 / 100: avg data time: 1.12e-01, avg batch time: 0.5404, average train loss: 2.9054
[09/28 05:14:31 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1838, average loss: 2.8938
[09/28 05:14:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 30.50	
[09/28 05:14:53 visual_prompt]: 	Test 100/190. loss: 2.905, 0.2049 s / batch. (data: 3.15e-05)max mem: 7.80438 GB 
[09/28 05:15:12 visual_prompt]: Inference (test):avg data time: 6.04e-05, avg batch time: 0.2048, average loss: 2.9134
[09/28 05:15:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 28.40	
[09/28 05:15:12 visual_prompt]: Best epoch 5: best metric: 0.085
[09/28 05:15:12 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/28 05:15:22 visual_prompt]: Epoch 6 / 100: avg data time: 1.07e-01, avg batch time: 0.5337, average train loss: 2.9193
[09/28 05:15:25 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1855, average loss: 2.8937
[09/28 05:15:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 29.50	
[09/28 05:15:48 visual_prompt]: 	Test 100/190. loss: 2.900, 0.2049 s / batch. (data: 2.57e-05)max mem: 7.80438 GB 
[09/28 05:16:07 visual_prompt]: Inference (test):avg data time: 9.75e-05, avg batch time: 0.2048, average loss: 2.9086
[09/28 05:16:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 27.86	
[09/28 05:16:07 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/28 05:16:17 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e-01, avg batch time: 0.5308, average train loss: 2.9135
[09/28 05:16:20 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1844, average loss: 2.8984
[09/28 05:16:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.50	top5: 31.50	
[09/28 05:16:42 visual_prompt]: 	Test 100/190. loss: 2.930, 0.2057 s / batch. (data: 2.81e-05)max mem: 7.80438 GB 
[09/28 05:17:02 visual_prompt]: Inference (test):avg data time: 3.06e-05, avg batch time: 0.2045, average loss: 2.9177
[09/28 05:17:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.58	top5: 28.46	
[09/28 05:17:02 visual_prompt]: Best epoch 7: best metric: 0.095
[09/28 05:17:02 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/28 05:17:11 visual_prompt]: Epoch 8 / 100: avg data time: 1.05e-01, avg batch time: 0.5313, average train loss: 2.9046
[09/28 05:17:15 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1727, average loss: 2.8992
[09/28 05:17:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 27.50	
[09/28 05:17:37 visual_prompt]: 	Test 100/190. loss: 2.885, 0.2041 s / batch. (data: 3.70e-05)max mem: 7.80438 GB 
[09/28 05:17:56 visual_prompt]: Inference (test):avg data time: 1.31e-04, avg batch time: 0.2046, average loss: 2.9351
[09/28 05:17:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.74	
[09/28 05:17:57 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/28 05:18:06 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e-01, avg batch time: 0.5245, average train loss: 2.9212
[09/28 05:18:09 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1773, average loss: 2.8789
[09/28 05:18:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.50	top5: 36.00	
[09/28 05:18:31 visual_prompt]: 	Test 100/190. loss: 2.889, 0.2033 s / batch. (data: 3.00e-05)max mem: 7.80438 GB 
[09/28 05:18:51 visual_prompt]: Inference (test):avg data time: 2.97e-05, avg batch time: 0.2049, average loss: 2.9190
[09/28 05:18:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.70	top5: 28.75	
[09/28 05:18:51 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/28 05:19:00 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e-01, avg batch time: 0.5301, average train loss: 2.9054
[09/28 05:19:04 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1840, average loss: 2.9157
[09/28 05:19:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 3.00	top5: 32.00	
[09/28 05:19:26 visual_prompt]: 	Test 100/190. loss: 2.849, 0.2098 s / batch. (data: 2.55e-05)max mem: 7.80438 GB 
[09/28 05:19:45 visual_prompt]: Inference (test):avg data time: 2.88e-05, avg batch time: 0.2049, average loss: 2.9245
[09/28 05:19:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.53	top5: 28.12	
[09/28 05:19:45 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/28 05:19:55 visual_prompt]: Epoch 11 / 100: avg data time: 9.08e-02, avg batch time: 0.5191, average train loss: 2.9160
[09/28 05:19:58 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.1859, average loss: 2.9202
[09/28 05:19:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 32.50	
[09/28 05:20:20 visual_prompt]: 	Test 100/190. loss: 2.990, 0.2058 s / batch. (data: 3.98e-05)max mem: 7.80438 GB 
[09/28 05:20:40 visual_prompt]: Inference (test):avg data time: 3.09e-05, avg batch time: 0.2049, average loss: 2.9506
[09/28 05:20:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.17	top5: 29.33	
[09/28 05:20:40 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/28 05:20:49 visual_prompt]: Epoch 12 / 100: avg data time: 9.87e-02, avg batch time: 0.5242, average train loss: 2.9040
[09/28 05:20:53 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1751, average loss: 2.9013
[09/28 05:20:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 31.50	
[09/28 05:21:15 visual_prompt]: 	Test 100/190. loss: 2.910, 0.2052 s / batch. (data: 3.24e-05)max mem: 7.80438 GB 
[09/28 05:21:34 visual_prompt]: Inference (test):avg data time: 3.16e-05, avg batch time: 0.2047, average loss: 2.9208
[09/28 05:21:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.22	top5: 28.33	
[09/28 05:21:34 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/28 05:21:44 visual_prompt]: Epoch 13 / 100: avg data time: 9.87e-02, avg batch time: 0.5262, average train loss: 2.9170
[09/28 05:21:47 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1725, average loss: 3.0037
[09/28 05:21:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 29.00	
[09/28 05:22:09 visual_prompt]: 	Test 100/190. loss: 2.984, 0.2055 s / batch. (data: 3.58e-05)max mem: 7.80438 GB 
[09/28 05:22:29 visual_prompt]: Inference (test):avg data time: 3.18e-05, avg batch time: 0.2048, average loss: 3.0057
[09/28 05:22:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.21	top5: 27.66	
[09/28 05:22:29 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/28 05:22:38 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e-01, avg batch time: 0.5292, average train loss: 2.9159
[09/28 05:22:42 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1775, average loss: 2.8432
[09/28 05:22:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 36.00	
[09/28 05:23:04 visual_prompt]: 	Test 100/190. loss: 2.856, 0.2038 s / batch. (data: 2.96e-05)max mem: 7.80438 GB 
[09/28 05:23:24 visual_prompt]: Inference (test):avg data time: 7.18e-05, avg batch time: 0.2048, average loss: 2.8860
[09/28 05:23:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.46	top5: 30.94	
[09/28 05:23:24 visual_prompt]: Best epoch 14: best metric: 0.100
[09/28 05:23:24 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/28 05:23:33 visual_prompt]: Epoch 15 / 100: avg data time: 9.68e-02, avg batch time: 0.5248, average train loss: 2.8564
[09/28 05:23:36 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1738, average loss: 2.8410
[09/28 05:23:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 40.00	
[09/28 05:23:58 visual_prompt]: 	Test 100/190. loss: 2.857, 0.2050 s / batch. (data: 3.03e-05)max mem: 7.80438 GB 
[09/28 05:24:18 visual_prompt]: Inference (test):avg data time: 2.91e-05, avg batch time: 0.2049, average loss: 2.9153
[09/28 05:24:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.26	top5: 34.16	
[09/28 05:24:18 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/28 05:24:27 visual_prompt]: Epoch 16 / 100: avg data time: 9.80e-02, avg batch time: 0.5255, average train loss: 2.8267
[09/28 05:24:30 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1746, average loss: 2.7381
[09/28 05:24:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.50	top5: 45.00	
[09/28 05:24:53 visual_prompt]: 	Test 100/190. loss: 2.776, 0.2033 s / batch. (data: 3.00e-05)max mem: 7.80438 GB 
[09/28 05:25:12 visual_prompt]: Inference (test):avg data time: 1.34e-04, avg batch time: 0.2050, average loss: 2.8496
[09/28 05:25:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.20	top5: 36.34	
[09/28 05:25:12 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/28 05:25:22 visual_prompt]: Epoch 17 / 100: avg data time: 9.83e-02, avg batch time: 0.5259, average train loss: 2.8081
[09/28 05:25:25 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.1776, average loss: 2.7824
[09/28 05:25:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 38.50	
[09/28 05:25:47 visual_prompt]: 	Test 100/190. loss: 2.938, 0.2040 s / batch. (data: 3.08e-05)max mem: 7.80438 GB 
[09/28 05:26:07 visual_prompt]: Inference (test):avg data time: 3.02e-05, avg batch time: 0.2049, average loss: 2.8787
[09/28 05:26:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.27	top5: 35.96	
[09/28 05:26:07 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/28 05:26:16 visual_prompt]: Epoch 18 / 100: avg data time: 8.90e-02, avg batch time: 0.5157, average train loss: 2.7559
[09/28 05:26:19 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1730, average loss: 2.7291
[09/28 05:26:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.00	top5: 45.50	
[09/28 05:26:41 visual_prompt]: 	Test 100/190. loss: 2.862, 0.2060 s / batch. (data: 2.81e-05)max mem: 7.80438 GB 
[09/28 05:27:01 visual_prompt]: Inference (test):avg data time: 3.33e-05, avg batch time: 0.2048, average loss: 2.8539
[09/28 05:27:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.57	top5: 41.19	
[09/28 05:27:01 visual_prompt]: Best epoch 18: best metric: 0.130
[09/28 05:27:01 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/28 05:27:11 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e-01, avg batch time: 0.5292, average train loss: 2.7494
[09/28 05:27:14 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.1818, average loss: 2.6204
[09/28 05:27:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.00	top5: 51.00	
[09/28 05:27:36 visual_prompt]: 	Test 100/190. loss: 2.627, 0.2040 s / batch. (data: 2.55e-05)max mem: 7.80438 GB 
[09/28 05:27:56 visual_prompt]: Inference (test):avg data time: 1.14e-04, avg batch time: 0.2050, average loss: 2.8143
[09/28 05:27:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.51	top5: 42.38	
[09/28 05:27:56 visual_prompt]: Best epoch 19: best metric: 0.180
[09/28 05:27:56 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/28 05:28:05 visual_prompt]: Epoch 20 / 100: avg data time: 1.01e-01, avg batch time: 0.5314, average train loss: 2.6427
[09/28 05:28:08 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1831, average loss: 2.5945
[09/28 05:28:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.00	top5: 52.00	
[09/28 05:28:31 visual_prompt]: 	Test 100/190. loss: 2.926, 0.2054 s / batch. (data: 3.91e-05)max mem: 7.80438 GB 
[09/28 05:28:50 visual_prompt]: Inference (test):avg data time: 3.21e-05, avg batch time: 0.2047, average loss: 2.7892
[09/28 05:28:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.43	top5: 43.15	
[09/28 05:28:50 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/28 05:29:00 visual_prompt]: Epoch 21 / 100: avg data time: 1.03e-01, avg batch time: 0.5319, average train loss: 2.6368
[09/28 05:29:03 visual_prompt]: Inference (val):avg data time: 1.88e-05, avg batch time: 0.1783, average loss: 2.6312
[09/28 05:29:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.00	top5: 53.50	
[09/28 05:29:25 visual_prompt]: 	Test 100/190. loss: 2.698, 0.2049 s / batch. (data: 3.43e-05)max mem: 7.80438 GB 
[09/28 05:29:45 visual_prompt]: Inference (test):avg data time: 1.33e-04, avg batch time: 0.2049, average loss: 2.7663
[09/28 05:29:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.69	top5: 46.76	
[09/28 05:29:45 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/28 05:29:54 visual_prompt]: Epoch 22 / 100: avg data time: 1.04e-01, avg batch time: 0.5306, average train loss: 2.6108
[09/28 05:29:57 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1848, average loss: 2.6034
[09/28 05:29:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.00	top5: 55.00	
[09/28 05:30:19 visual_prompt]: 	Test 100/190. loss: 2.760, 0.2036 s / batch. (data: 2.77e-05)max mem: 7.80438 GB 
[09/28 05:30:39 visual_prompt]: Inference (test):avg data time: 3.03e-05, avg batch time: 0.2048, average loss: 2.7420
[09/28 05:30:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.28	top5: 44.57	
[09/28 05:30:39 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/28 05:30:49 visual_prompt]: Epoch 23 / 100: avg data time: 1.09e-01, avg batch time: 0.5371, average train loss: 2.6061
[09/28 05:30:52 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1799, average loss: 2.4690
[09/28 05:30:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.00	top5: 59.50	
[09/28 05:31:14 visual_prompt]: 	Test 100/190. loss: 2.474, 0.2058 s / batch. (data: 2.96e-05)max mem: 7.80438 GB 
[09/28 05:31:34 visual_prompt]: Inference (test):avg data time: 3.03e-05, avg batch time: 0.2049, average loss: 2.5998
[09/28 05:31:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.99	top5: 57.30	
[09/28 05:31:34 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/28 05:31:43 visual_prompt]: Epoch 24 / 100: avg data time: 9.92e-02, avg batch time: 0.5279, average train loss: 2.4042
[09/28 05:31:46 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1842, average loss: 2.3876
[09/28 05:31:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 20.50	top5: 67.00	
[09/28 05:32:09 visual_prompt]: 	Test 100/190. loss: 2.591, 0.2070 s / batch. (data: 2.98e-05)max mem: 7.80438 GB 
[09/28 05:32:28 visual_prompt]: Inference (test):avg data time: 3.14e-05, avg batch time: 0.2049, average loss: 2.5723
[09/28 05:32:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.33	top5: 58.27	
[09/28 05:32:28 visual_prompt]: Best epoch 24: best metric: 0.205
[09/28 05:32:28 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/28 05:32:38 visual_prompt]: Epoch 25 / 100: avg data time: 9.39e-02, avg batch time: 0.5209, average train loss: 2.3630
[09/28 05:32:41 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1779, average loss: 2.2641
[09/28 05:32:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.00	top5: 68.50	
[09/28 05:33:03 visual_prompt]: 	Test 100/190. loss: 2.721, 0.2053 s / batch. (data: 2.96e-05)max mem: 7.80438 GB 
[09/28 05:33:23 visual_prompt]: Inference (test):avg data time: 3.21e-05, avg batch time: 0.2047, average loss: 2.5051
[09/28 05:33:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.85	top5: 59.85	
[09/28 05:33:23 visual_prompt]: Best epoch 25: best metric: 0.210
[09/28 05:33:23 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/28 05:33:32 visual_prompt]: Epoch 26 / 100: avg data time: 1.03e-01, avg batch time: 0.5316, average train loss: 2.1925
[09/28 05:33:36 visual_prompt]: Inference (val):avg data time: 1.74e-05, avg batch time: 0.1841, average loss: 2.1432
[09/28 05:33:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 24.00	top5: 75.50	
[09/28 05:33:58 visual_prompt]: 	Test 100/190. loss: 2.469, 0.2049 s / batch. (data: 2.84e-05)max mem: 7.80438 GB 
[09/28 05:34:17 visual_prompt]: Inference (test):avg data time: 5.87e-05, avg batch time: 0.2051, average loss: 2.4689
[09/28 05:34:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.93	top5: 65.36	
[09/28 05:34:18 visual_prompt]: Best epoch 26: best metric: 0.240
[09/28 05:34:18 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/28 05:34:27 visual_prompt]: Epoch 27 / 100: avg data time: 1.04e-01, avg batch time: 0.5319, average train loss: 2.1040
[09/28 05:34:30 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1737, average loss: 2.4449
[09/28 05:34:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.50	top5: 63.00	
[09/28 05:34:52 visual_prompt]: 	Test 100/190. loss: 2.790, 0.2054 s / batch. (data: 3.96e-05)max mem: 7.80438 GB 
[09/28 05:35:12 visual_prompt]: Inference (test):avg data time: 3.19e-05, avg batch time: 0.2047, average loss: 2.7072
[09/28 05:35:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.62	top5: 56.02	
[09/28 05:35:12 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/28 05:35:21 visual_prompt]: Epoch 28 / 100: avg data time: 9.75e-02, avg batch time: 0.5235, average train loss: 2.0100
[09/28 05:35:25 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1850, average loss: 1.9775
[09/28 05:35:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 33.00	top5: 83.00	
[09/28 05:35:47 visual_prompt]: 	Test 100/190. loss: 2.513, 0.2059 s / batch. (data: 3.00e-05)max mem: 7.80438 GB 
[09/28 05:36:07 visual_prompt]: Inference (test):avg data time: 8.91e-05, avg batch time: 0.2051, average loss: 2.5544
[09/28 05:36:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.85	top5: 69.45	
[09/28 05:36:07 visual_prompt]: Best epoch 28: best metric: 0.330
[09/28 05:36:07 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/28 05:36:16 visual_prompt]: Epoch 29 / 100: avg data time: 1.07e-01, avg batch time: 0.5342, average train loss: 1.8142
[09/28 05:36:20 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1818, average loss: 1.8137
[09/28 05:36:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 35.00	top5: 88.00	
[09/28 05:36:42 visual_prompt]: 	Test 100/190. loss: 2.677, 0.2042 s / batch. (data: 2.81e-05)max mem: 7.80438 GB 
[09/28 05:37:01 visual_prompt]: Inference (test):avg data time: 3.06e-05, avg batch time: 0.2048, average loss: 2.6450
[09/28 05:37:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.47	top5: 70.35	
[09/28 05:37:01 visual_prompt]: Best epoch 29: best metric: 0.350
[09/28 05:37:01 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/28 05:37:11 visual_prompt]: Epoch 30 / 100: avg data time: 1.07e-01, avg batch time: 0.5333, average train loss: 1.6499
[09/28 05:37:14 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1860, average loss: 1.8727
[09/28 05:37:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.50	top5: 87.50	
[09/28 05:37:37 visual_prompt]: 	Test 100/190. loss: 2.709, 0.2144 s / batch. (data: 3.31e-05)max mem: 7.80438 GB 
[09/28 05:37:56 visual_prompt]: Inference (test):avg data time: 3.27e-05, avg batch time: 0.2049, average loss: 2.7830
[09/28 05:37:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.94	top5: 67.94	
[09/28 05:37:56 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/28 05:38:06 visual_prompt]: Epoch 31 / 100: avg data time: 1.01e-01, avg batch time: 0.5272, average train loss: 1.5938
[09/28 05:38:09 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1759, average loss: 2.0835
[09/28 05:38:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.50	top5: 84.00	
[09/28 05:38:31 visual_prompt]: 	Test 100/190. loss: 2.995, 0.2040 s / batch. (data: 2.88e-05)max mem: 7.80438 GB 
[09/28 05:38:51 visual_prompt]: Inference (test):avg data time: 2.77e-04, avg batch time: 0.2049, average loss: 2.9669
[09/28 05:38:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.33	top5: 66.39	
[09/28 05:38:51 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/28 05:39:00 visual_prompt]: Epoch 32 / 100: avg data time: 1.05e-01, avg batch time: 0.5323, average train loss: 1.4927
[09/28 05:39:03 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1839, average loss: 1.7029
[09/28 05:39:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 39.00	top5: 90.00	
[09/28 05:39:26 visual_prompt]: 	Test 100/190. loss: 2.652, 0.2045 s / batch. (data: 3.03e-05)max mem: 7.80438 GB 
[09/28 05:39:45 visual_prompt]: Inference (test):avg data time: 3.21e-05, avg batch time: 0.2051, average loss: 2.8853
[09/28 05:39:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.69	top5: 68.43	
[09/28 05:39:45 visual_prompt]: Best epoch 32: best metric: 0.390
[09/28 05:39:45 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/28 05:39:55 visual_prompt]: Epoch 33 / 100: avg data time: 1.11e-01, avg batch time: 0.5391, average train loss: 1.3349
[09/28 05:39:58 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1833, average loss: 1.3303
[09/28 05:39:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 54.00	top5: 95.00	
[09/28 05:40:20 visual_prompt]: 	Test 100/190. loss: 2.605, 0.2048 s / batch. (data: 3.08e-05)max mem: 7.80438 GB 
[09/28 05:40:40 visual_prompt]: Inference (test):avg data time: 3.28e-05, avg batch time: 0.2048, average loss: 2.8156
[09/28 05:40:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 22.65	top5: 72.91	
[09/28 05:40:40 visual_prompt]: Best epoch 33: best metric: 0.540
[09/28 05:40:40 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/28 05:40:49 visual_prompt]: Epoch 34 / 100: avg data time: 1.06e-01, avg batch time: 0.5339, average train loss: 1.1021
[09/28 05:40:53 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1723, average loss: 1.4163
[09/28 05:40:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 50.00	top5: 94.50	
[09/28 05:41:15 visual_prompt]: 	Test 100/190. loss: 2.864, 0.2044 s / batch. (data: 3.03e-05)max mem: 7.80438 GB 
[09/28 05:41:34 visual_prompt]: Inference (test):avg data time: 3.36e-05, avg batch time: 0.2050, average loss: 3.1392
[09/28 05:41:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.30	top5: 69.84	
[09/28 05:41:34 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/28 05:41:44 visual_prompt]: Epoch 35 / 100: avg data time: 9.19e-02, avg batch time: 0.5202, average train loss: 1.1623
[09/28 05:41:47 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1780, average loss: 1.1749
[09/28 05:41:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 57.00	top5: 96.50	
[09/28 05:42:09 visual_prompt]: 	Test 100/190. loss: 2.803, 0.2049 s / batch. (data: 2.84e-05)max mem: 7.80438 GB 
[09/28 05:42:29 visual_prompt]: Inference (test):avg data time: 3.28e-05, avg batch time: 0.2051, average loss: 2.9562
[09/28 05:42:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 22.77	top5: 71.40	
[09/28 05:42:29 visual_prompt]: Best epoch 35: best metric: 0.570
[09/28 05:42:29 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/28 05:42:39 visual_prompt]: Epoch 36 / 100: avg data time: 1.09e-01, avg batch time: 0.5344, average train loss: 0.9423
[09/28 05:42:42 visual_prompt]: Inference (val):avg data time: 1.92e-05, avg batch time: 0.1777, average loss: 0.9074
[09/28 05:42:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 68.00	top5: 98.00	
[09/28 05:43:04 visual_prompt]: 	Test 100/190. loss: 2.843, 0.2047 s / batch. (data: 2.55e-05)max mem: 7.80438 GB 
[09/28 05:43:23 visual_prompt]: Inference (test):avg data time: 2.96e-05, avg batch time: 0.2049, average loss: 2.9158
[09/28 05:43:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 23.77	top5: 72.88	
[09/28 05:43:23 visual_prompt]: Best epoch 36: best metric: 0.680
[09/28 05:43:23 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/28 05:43:33 visual_prompt]: Epoch 37 / 100: avg data time: 1.09e-01, avg batch time: 0.5343, average train loss: 0.7958
[09/28 05:43:36 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1774, average loss: 0.7863
[09/28 05:43:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 77.00	top5: 99.00	
[09/28 05:43:58 visual_prompt]: 	Test 100/190. loss: 3.066, 0.2044 s / batch. (data: 3.12e-05)max mem: 7.80438 GB 
[09/28 05:44:18 visual_prompt]: Inference (test):avg data time: 3.22e-05, avg batch time: 0.2050, average loss: 3.0709
[09/28 05:44:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 24.53	top5: 72.89	
[09/28 05:44:18 visual_prompt]: Best epoch 37: best metric: 0.770
[09/28 05:44:18 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/28 05:44:28 visual_prompt]: Epoch 38 / 100: avg data time: 1.04e-01, avg batch time: 0.5302, average train loss: 0.6723
[09/28 05:44:31 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1850, average loss: 0.6101
[09/28 05:44:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 81.50	top5: 100.00	
[09/28 05:44:53 visual_prompt]: 	Test 100/190. loss: 3.074, 0.2031 s / batch. (data: 3.19e-05)max mem: 7.80438 GB 
[09/28 05:45:13 visual_prompt]: Inference (test):avg data time: 3.00e-05, avg batch time: 0.2052, average loss: 3.0549
[09/28 05:45:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.43	top5: 72.53	
[09/28 05:45:13 visual_prompt]: Best epoch 38: best metric: 0.815
[09/28 05:45:13 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/28 05:45:22 visual_prompt]: Epoch 39 / 100: avg data time: 1.06e-01, avg batch time: 0.5311, average train loss: 0.5702
[09/28 05:45:25 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1840, average loss: 0.5043
[09/28 05:45:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 85.00	top5: 100.00	
[09/28 05:45:48 visual_prompt]: 	Test 100/190. loss: 2.796, 0.2043 s / batch. (data: 2.67e-05)max mem: 7.80438 GB 
[09/28 05:46:07 visual_prompt]: Inference (test):avg data time: 1.41e-04, avg batch time: 0.2049, average loss: 3.0691
[09/28 05:46:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.13	top5: 73.91	
[09/28 05:46:07 visual_prompt]: Best epoch 39: best metric: 0.850
[09/28 05:46:07 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/28 05:46:17 visual_prompt]: Epoch 40 / 100: avg data time: 1.03e-01, avg batch time: 0.5277, average train loss: 0.4473
[09/28 05:46:20 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1798, average loss: 0.4006
[09/28 05:46:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 89.50	top5: 99.50	
[09/28 05:46:42 visual_prompt]: 	Test 100/190. loss: 3.157, 0.2049 s / batch. (data: 3.05e-05)max mem: 7.80438 GB 
[09/28 05:47:02 visual_prompt]: Inference (test):avg data time: 3.34e-05, avg batch time: 0.2050, average loss: 3.1409
[09/28 05:47:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.51	top5: 73.26	
[09/28 05:47:02 visual_prompt]: Best epoch 40: best metric: 0.895
[09/28 05:47:02 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/28 05:47:11 visual_prompt]: Epoch 41 / 100: avg data time: 9.96e-02, avg batch time: 0.5248, average train loss: 0.3437
[09/28 05:47:14 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1749, average loss: 0.2836
[09/28 05:47:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 93.50	top5: 100.00	
[09/28 05:47:37 visual_prompt]: 	Test 100/190. loss: 2.681, 0.2053 s / batch. (data: 2.98e-05)max mem: 7.80438 GB 
[09/28 05:47:56 visual_prompt]: Inference (test):avg data time: 3.16e-05, avg batch time: 0.2048, average loss: 3.1083
[09/28 05:47:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.79	top5: 75.66	
[09/28 05:47:57 visual_prompt]: Best epoch 41: best metric: 0.935
[09/28 05:47:57 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/28 05:48:06 visual_prompt]: Epoch 42 / 100: avg data time: 1.04e-01, avg batch time: 0.5312, average train loss: 0.2935
[09/28 05:48:09 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1721, average loss: 0.2116
[09/28 05:48:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 94.50	top5: 100.00	
[09/28 05:48:31 visual_prompt]: 	Test 100/190. loss: 3.052, 0.2042 s / batch. (data: 3.48e-05)max mem: 7.80438 GB 
[09/28 05:48:51 visual_prompt]: Inference (test):avg data time: 3.04e-05, avg batch time: 0.2048, average loss: 3.3014
[09/28 05:48:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.03	top5: 74.08	
[09/28 05:48:51 visual_prompt]: Best epoch 42: best metric: 0.945
[09/28 05:48:51 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/28 05:49:01 visual_prompt]: Epoch 43 / 100: avg data time: 1.11e-01, avg batch time: 0.5400, average train loss: 0.2123
[09/28 05:49:04 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1820, average loss: 0.1852
[09/28 05:49:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 95.00	top5: 100.00	
[09/28 05:49:26 visual_prompt]: 	Test 100/190. loss: 3.454, 0.2037 s / batch. (data: 3.05e-05)max mem: 7.80438 GB 
[09/28 05:49:46 visual_prompt]: Inference (test):avg data time: 3.15e-05, avg batch time: 0.2050, average loss: 3.3341
[09/28 05:49:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.68	top5: 73.08	
[09/28 05:49:46 visual_prompt]: Best epoch 43: best metric: 0.950
[09/28 05:49:46 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/28 05:49:55 visual_prompt]: Epoch 44 / 100: avg data time: 1.06e-01, avg batch time: 0.5322, average train loss: 0.1819
[09/28 05:49:59 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1761, average loss: 0.1191
[09/28 05:49:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.50	top5: 100.00	
[09/28 05:50:21 visual_prompt]: 	Test 100/190. loss: 3.417, 0.2052 s / batch. (data: 3.03e-05)max mem: 7.80438 GB 
[09/28 05:50:41 visual_prompt]: Inference (test):avg data time: 1.09e-04, avg batch time: 0.2053, average loss: 3.3150
[09/28 05:50:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.33	top5: 73.34	
[09/28 05:50:41 visual_prompt]: Best epoch 44: best metric: 0.995
[09/28 05:50:41 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/28 05:50:50 visual_prompt]: Epoch 45 / 100: avg data time: 1.02e-01, avg batch time: 0.5313, average train loss: 0.1287
[09/28 05:50:53 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1848, average loss: 0.0703
[09/28 05:50:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 05:51:16 visual_prompt]: 	Test 100/190. loss: 3.400, 0.2043 s / batch. (data: 3.10e-05)max mem: 7.80438 GB 
[09/28 05:51:35 visual_prompt]: Inference (test):avg data time: 3.13e-05, avg batch time: 0.2051, average loss: 3.3779
[09/28 05:51:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.20	top5: 74.86	
[09/28 05:51:35 visual_prompt]: Best epoch 45: best metric: 1.000
[09/28 05:51:35 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/28 05:51:45 visual_prompt]: Epoch 46 / 100: avg data time: 1.05e-01, avg batch time: 0.5347, average train loss: 0.1056
[09/28 05:51:48 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.1861, average loss: 0.0656
[09/28 05:51:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.50	top5: 100.00	
[09/28 05:52:11 visual_prompt]: 	Test 100/190. loss: 3.292, 0.2042 s / batch. (data: 2.86e-05)max mem: 7.80438 GB 
[09/28 05:52:30 visual_prompt]: Inference (test):avg data time: 3.20e-05, avg batch time: 0.2050, average loss: 3.3040
[09/28 05:52:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.63	top5: 75.65	
[09/28 05:52:30 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/28 05:52:40 visual_prompt]: Epoch 47 / 100: avg data time: 1.01e-01, avg batch time: 0.5284, average train loss: 0.0785
[09/28 05:52:43 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1765, average loss: 0.1012
[09/28 05:52:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 97.50	top5: 100.00	
[09/28 05:53:05 visual_prompt]: 	Test 100/190. loss: 3.328, 0.2047 s / batch. (data: 2.55e-05)max mem: 7.80438 GB 
[09/28 05:53:25 visual_prompt]: Inference (test):avg data time: 3.06e-05, avg batch time: 0.2049, average loss: 3.4138
[09/28 05:53:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.75	top5: 75.95	
[09/28 05:53:25 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/28 05:53:34 visual_prompt]: Epoch 48 / 100: avg data time: 1.09e-01, avg batch time: 0.5358, average train loss: 0.0805
[09/28 05:53:38 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1814, average loss: 0.0406
[09/28 05:53:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 05:54:00 visual_prompt]: 	Test 100/190. loss: 3.323, 0.2058 s / batch. (data: 2.62e-05)max mem: 7.80438 GB 
[09/28 05:54:19 visual_prompt]: Inference (test):avg data time: 3.14e-05, avg batch time: 0.2047, average loss: 3.3696
[09/28 05:54:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.39	top5: 74.70	
[09/28 05:54:20 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/28 05:54:29 visual_prompt]: Epoch 49 / 100: avg data time: 1.10e-01, avg batch time: 0.5386, average train loss: 0.0532
[09/28 05:54:32 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1827, average loss: 0.0365
[09/28 05:54:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 05:54:55 visual_prompt]: 	Test 100/190. loss: 3.301, 0.2039 s / batch. (data: 3.39e-05)max mem: 7.80438 GB 
[09/28 05:55:14 visual_prompt]: Inference (test):avg data time: 3.06e-05, avg batch time: 0.2051, average loss: 3.3497
[09/28 05:55:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.05	top5: 74.67	
[09/28 05:55:14 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/28 05:55:24 visual_prompt]: Epoch 50 / 100: avg data time: 1.10e-01, avg batch time: 0.5380, average train loss: 0.0418
[09/28 05:55:27 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1726, average loss: 0.0349
[09/28 05:55:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 05:55:49 visual_prompt]: 	Test 100/190. loss: 3.374, 0.2037 s / batch. (data: 3.00e-05)max mem: 7.80438 GB 
[09/28 05:56:09 visual_prompt]: Inference (test):avg data time: 3.12e-05, avg batch time: 0.2049, average loss: 3.4044
[09/28 05:56:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.05	top5: 75.28	
[09/28 05:56:09 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/28 05:56:19 visual_prompt]: Epoch 51 / 100: avg data time: 1.01e-01, avg batch time: 0.5268, average train loss: 0.0394
[09/28 05:56:22 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1846, average loss: 0.0241
[09/28 05:56:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 05:56:44 visual_prompt]: 	Test 100/190. loss: 3.384, 0.2051 s / batch. (data: 3.48e-05)max mem: 7.80438 GB 
[09/28 05:57:04 visual_prompt]: Inference (test):avg data time: 3.26e-05, avg batch time: 0.2051, average loss: 3.3763
[09/28 05:57:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.13	top5: 75.45	
[09/28 05:57:04 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/28 05:57:13 visual_prompt]: Epoch 52 / 100: avg data time: 8.94e-02, avg batch time: 0.5160, average train loss: 0.0355
[09/28 05:57:16 visual_prompt]: Inference (val):avg data time: 4.48e-05, avg batch time: 0.1795, average loss: 0.0244
[09/28 05:57:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 05:57:39 visual_prompt]: 	Test 100/190. loss: 3.360, 0.2048 s / batch. (data: 3.05e-05)max mem: 7.80438 GB 
[09/28 05:57:58 visual_prompt]: Inference (test):avg data time: 3.15e-05, avg batch time: 0.2048, average loss: 3.4310
[09/28 05:57:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.91	top5: 74.43	
[09/28 05:57:58 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/28 05:58:08 visual_prompt]: Epoch 53 / 100: avg data time: 1.08e-01, avg batch time: 0.5349, average train loss: 0.0301
[09/28 05:58:11 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1761, average loss: 0.0230
[09/28 05:58:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 05:58:33 visual_prompt]: 	Test 100/190. loss: 3.467, 0.2050 s / batch. (data: 3.19e-05)max mem: 7.80438 GB 
[09/28 05:58:53 visual_prompt]: Inference (test):avg data time: 1.12e-04, avg batch time: 0.2051, average loss: 3.4467
[09/28 05:58:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.95	top5: 75.12	
[09/28 05:58:53 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/28 05:59:02 visual_prompt]: Epoch 54 / 100: avg data time: 1.07e-01, avg batch time: 0.5352, average train loss: 0.0265
[09/28 05:59:06 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.1849, average loss: 0.0194
[09/28 05:59:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 05:59:28 visual_prompt]: 	Test 100/190. loss: 3.382, 0.2050 s / batch. (data: 4.98e-05)max mem: 7.80438 GB 
[09/28 05:59:47 visual_prompt]: Inference (test):avg data time: 3.24e-05, avg batch time: 0.2049, average loss: 3.4055
[09/28 05:59:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.62	top5: 75.08	
[09/28 05:59:48 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/28 05:59:57 visual_prompt]: Epoch 55 / 100: avg data time: 1.10e-01, avg batch time: 0.5365, average train loss: 0.0232
[09/28 06:00:00 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1720, average loss: 0.0158
[09/28 06:00:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:00:22 visual_prompt]: 	Test 100/190. loss: 3.293, 0.2051 s / batch. (data: 3.17e-05)max mem: 7.80438 GB 
[09/28 06:00:42 visual_prompt]: Inference (test):avg data time: 1.23e-04, avg batch time: 0.2050, average loss: 3.4305
[09/28 06:00:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.63	top5: 75.45	
[09/28 06:00:42 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/28 06:00:52 visual_prompt]: Epoch 56 / 100: avg data time: 1.09e-01, avg batch time: 0.5376, average train loss: 0.0251
[09/28 06:00:55 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1850, average loss: 0.0203
[09/28 06:00:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:01:17 visual_prompt]: 	Test 100/190. loss: 3.489, 0.2054 s / batch. (data: 2.98e-05)max mem: 7.80438 GB 
[09/28 06:01:37 visual_prompt]: Inference (test):avg data time: 1.11e-04, avg batch time: 0.2054, average loss: 3.3568
[09/28 06:01:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.96	top5: 75.83	
[09/28 06:01:37 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/28 06:01:46 visual_prompt]: Epoch 57 / 100: avg data time: 1.05e-01, avg batch time: 0.5327, average train loss: 0.0266
[09/28 06:01:50 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1730, average loss: 0.0228
[09/28 06:01:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:02:12 visual_prompt]: 	Test 100/190. loss: 3.370, 0.2057 s / batch. (data: 2.91e-05)max mem: 7.80438 GB 
[09/28 06:02:32 visual_prompt]: Inference (test):avg data time: 3.16e-05, avg batch time: 0.2049, average loss: 3.4239
[09/28 06:02:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.95	top5: 75.17	
[09/28 06:02:32 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/28 06:02:41 visual_prompt]: Epoch 58 / 100: avg data time: 1.04e-01, avg batch time: 0.5321, average train loss: 0.0248
[09/28 06:02:44 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1859, average loss: 0.0176
[09/28 06:02:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:03:07 visual_prompt]: 	Test 100/190. loss: 3.481, 0.2036 s / batch. (data: 2.74e-05)max mem: 7.80438 GB 
[09/28 06:03:26 visual_prompt]: Inference (test):avg data time: 4.68e-05, avg batch time: 0.2051, average loss: 3.4499
[09/28 06:03:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.40	top5: 75.00	
[09/28 06:03:26 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/28 06:03:36 visual_prompt]: Epoch 59 / 100: avg data time: 1.09e-01, avg batch time: 0.5356, average train loss: 0.0339
[09/28 06:03:39 visual_prompt]: Inference (val):avg data time: 1.81e-05, avg batch time: 0.1825, average loss: 0.0206
[09/28 06:03:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:04:01 visual_prompt]: 	Test 100/190. loss: 3.433, 0.2044 s / batch. (data: 2.93e-05)max mem: 7.80438 GB 
[09/28 06:04:21 visual_prompt]: Inference (test):avg data time: 3.04e-05, avg batch time: 0.2051, average loss: 3.3417
[09/28 06:04:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.49	top5: 74.99	
[09/28 06:04:21 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/28 06:04:30 visual_prompt]: Epoch 60 / 100: avg data time: 1.14e-01, avg batch time: 0.5425, average train loss: 0.0354
[09/28 06:04:33 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1752, average loss: 0.0352
[09/28 06:04:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.50	top5: 100.00	
[09/28 06:04:56 visual_prompt]: 	Test 100/190. loss: 3.335, 0.2041 s / batch. (data: 3.50e-05)max mem: 7.80438 GB 
[09/28 06:05:15 visual_prompt]: Inference (test):avg data time: 3.26e-05, avg batch time: 0.2049, average loss: 3.5026
[09/28 06:05:15 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.13	top5: 74.73	
[09/28 06:05:15 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/28 06:05:25 visual_prompt]: Epoch 61 / 100: avg data time: 1.03e-01, avg batch time: 0.5330, average train loss: 0.0677
[09/28 06:05:28 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1779, average loss: 0.0461
[09/28 06:05:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.50	top5: 100.00	
[09/28 06:05:50 visual_prompt]: 	Test 100/190. loss: 3.440, 0.2055 s / batch. (data: 2.86e-05)max mem: 7.80438 GB 
[09/28 06:06:10 visual_prompt]: Inference (test):avg data time: 3.32e-05, avg batch time: 0.2051, average loss: 3.4160
[09/28 06:06:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.48	top5: 73.97	
[09/28 06:06:10 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/28 06:06:20 visual_prompt]: Epoch 62 / 100: avg data time: 1.01e-01, avg batch time: 0.5304, average train loss: 0.0679
[09/28 06:06:23 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1752, average loss: 0.0615
[09/28 06:06:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.00	top5: 100.00	
[09/28 06:06:45 visual_prompt]: 	Test 100/190. loss: 3.402, 0.2053 s / batch. (data: 3.03e-05)max mem: 7.80438 GB 
[09/28 06:07:05 visual_prompt]: Inference (test):avg data time: 3.04e-05, avg batch time: 0.2052, average loss: 3.4205
[09/28 06:07:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.13	top5: 73.80	
[09/28 06:07:05 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/28 06:07:14 visual_prompt]: Epoch 63 / 100: avg data time: 1.10e-01, avg batch time: 0.5356, average train loss: 0.0730
[09/28 06:07:17 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1776, average loss: 0.0284
[09/28 06:07:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:07:40 visual_prompt]: 	Test 100/190. loss: 3.508, 0.2039 s / batch. (data: 3.34e-05)max mem: 7.80438 GB 
[09/28 06:07:59 visual_prompt]: Inference (test):avg data time: 3.17e-05, avg batch time: 0.2049, average loss: 3.3424
[09/28 06:07:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.32	top5: 74.20	
[09/28 06:07:59 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/28 06:08:09 visual_prompt]: Epoch 64 / 100: avg data time: 1.06e-01, avg batch time: 0.5347, average train loss: 0.0491
[09/28 06:08:12 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1854, average loss: 0.0407
[09/28 06:08:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.50	top5: 100.00	
[09/28 06:08:34 visual_prompt]: 	Test 100/190. loss: 3.383, 0.2046 s / batch. (data: 2.84e-05)max mem: 7.80438 GB 
[09/28 06:08:54 visual_prompt]: Inference (test):avg data time: 3.30e-05, avg batch time: 0.2051, average loss: 3.3937
[09/28 06:08:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.62	top5: 74.72	
[09/28 06:08:54 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/28 06:09:03 visual_prompt]: Epoch 65 / 100: avg data time: 1.03e-01, avg batch time: 0.5304, average train loss: 0.0406
[09/28 06:09:07 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1856, average loss: 0.0308
[09/28 06:09:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:09:29 visual_prompt]: 	Test 100/190. loss: 3.464, 0.2055 s / batch. (data: 2.91e-05)max mem: 7.80438 GB 
[09/28 06:09:49 visual_prompt]: Inference (test):avg data time: 3.15e-05, avg batch time: 0.2052, average loss: 3.3637
[09/28 06:09:49 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.25	top5: 74.03	
[09/28 06:09:49 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/28 06:09:58 visual_prompt]: Epoch 66 / 100: avg data time: 1.13e-01, avg batch time: 0.5419, average train loss: 0.0360
[09/28 06:10:01 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1756, average loss: 0.0304
[09/28 06:10:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:10:24 visual_prompt]: 	Test 100/190. loss: 3.083, 0.2038 s / batch. (data: 2.98e-05)max mem: 7.80438 GB 
[09/28 06:10:43 visual_prompt]: Inference (test):avg data time: 3.30e-05, avg batch time: 0.2051, average loss: 3.3284
[09/28 06:10:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.37	top5: 74.52	
[09/28 06:10:43 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/28 06:10:53 visual_prompt]: Epoch 67 / 100: avg data time: 1.07e-01, avg batch time: 0.5338, average train loss: 0.0389
[09/28 06:10:56 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1848, average loss: 0.0292
[09/28 06:10:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:11:19 visual_prompt]: 	Test 100/190. loss: 3.282, 0.2041 s / batch. (data: 2.77e-05)max mem: 7.80438 GB 
[09/28 06:11:38 visual_prompt]: Inference (test):avg data time: 3.23e-05, avg batch time: 0.2047, average loss: 3.3165
[09/28 06:11:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.21	top5: 74.06	
[09/28 06:11:38 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/28 06:11:48 visual_prompt]: Epoch 68 / 100: avg data time: 1.04e-01, avg batch time: 0.5312, average train loss: 0.0307
[09/28 06:11:51 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1752, average loss: 0.0291
[09/28 06:11:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.50	top5: 100.00	
[09/28 06:12:13 visual_prompt]: 	Test 100/190. loss: 3.187, 0.2038 s / batch. (data: 3.12e-05)max mem: 7.80438 GB 
[09/28 06:12:33 visual_prompt]: Inference (test):avg data time: 3.11e-05, avg batch time: 0.2047, average loss: 3.3636
[09/28 06:12:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.26	top5: 74.71	
[09/28 06:12:33 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/28 06:12:42 visual_prompt]: Epoch 69 / 100: avg data time: 1.04e-01, avg batch time: 0.5329, average train loss: 0.0222
[09/28 06:12:46 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1825, average loss: 0.0153
[09/28 06:12:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:13:08 visual_prompt]: 	Test 100/190. loss: 3.223, 0.2048 s / batch. (data: 2.96e-05)max mem: 7.80438 GB 
[09/28 06:13:27 visual_prompt]: Inference (test):avg data time: 2.03e-04, avg batch time: 0.2050, average loss: 3.3744
[09/28 06:13:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.34	top5: 73.88	
[09/28 06:13:28 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/28 06:13:37 visual_prompt]: Epoch 70 / 100: avg data time: 1.11e-01, avg batch time: 0.5382, average train loss: 0.0179
[09/28 06:13:40 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1773, average loss: 0.0142
[09/28 06:13:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:14:03 visual_prompt]: 	Test 100/190. loss: 3.299, 0.2042 s / batch. (data: 2.69e-05)max mem: 7.80438 GB 
[09/28 06:14:22 visual_prompt]: Inference (test):avg data time: 3.00e-05, avg batch time: 0.2051, average loss: 3.4135
[09/28 06:14:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.19	top5: 74.97	
[09/28 06:14:22 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/28 06:14:32 visual_prompt]: Epoch 71 / 100: avg data time: 1.13e-01, avg batch time: 0.5434, average train loss: 0.0151
[09/28 06:14:35 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1858, average loss: 0.0106
[09/28 06:14:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:14:57 visual_prompt]: 	Test 100/190. loss: 3.301, 0.2059 s / batch. (data: 3.05e-05)max mem: 7.80438 GB 
[09/28 06:15:17 visual_prompt]: Inference (test):avg data time: 1.49e-04, avg batch time: 0.2052, average loss: 3.4249
[09/28 06:15:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.28	top5: 74.39	
[09/28 06:15:17 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/28 06:15:27 visual_prompt]: Epoch 72 / 100: avg data time: 1.07e-01, avg batch time: 0.5329, average train loss: 0.0132
[09/28 06:15:30 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1832, average loss: 0.0097
[09/28 06:15:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:15:52 visual_prompt]: 	Test 100/190. loss: 3.320, 0.2044 s / batch. (data: 2.55e-05)max mem: 7.80438 GB 
[09/28 06:16:12 visual_prompt]: Inference (test):avg data time: 3.08e-05, avg batch time: 0.2052, average loss: 3.4378
[09/28 06:16:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.47	top5: 74.70	
[09/28 06:16:12 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/28 06:16:21 visual_prompt]: Epoch 73 / 100: avg data time: 1.09e-01, avg batch time: 0.5378, average train loss: 0.0126
[09/28 06:16:24 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1735, average loss: 0.0099
[09/28 06:16:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:16:47 visual_prompt]: 	Test 100/190. loss: 3.354, 0.2055 s / batch. (data: 3.15e-05)max mem: 7.80438 GB 
[09/28 06:17:06 visual_prompt]: Inference (test):avg data time: 3.23e-05, avg batch time: 0.2047, average loss: 3.4286
[09/28 06:17:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.68	top5: 74.49	
[09/28 06:17:06 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/28 06:17:16 visual_prompt]: Epoch 74 / 100: avg data time: 1.05e-01, avg batch time: 0.5330, average train loss: 0.0119
[09/28 06:17:19 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1816, average loss: 0.0101
[09/28 06:17:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:17:41 visual_prompt]: 	Test 100/190. loss: 3.334, 0.2040 s / batch. (data: 3.05e-05)max mem: 7.80438 GB 
[09/28 06:18:01 visual_prompt]: Inference (test):avg data time: 3.12e-05, avg batch time: 0.2050, average loss: 3.4449
[09/28 06:18:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.42	top5: 74.53	
[09/28 06:18:01 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/28 06:18:11 visual_prompt]: Epoch 75 / 100: avg data time: 1.10e-01, avg batch time: 0.5366, average train loss: 0.0118
[09/28 06:18:14 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.1857, average loss: 0.0087
[09/28 06:18:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:18:36 visual_prompt]: 	Test 100/190. loss: 3.380, 0.2042 s / batch. (data: 2.98e-05)max mem: 7.80438 GB 
[09/28 06:18:56 visual_prompt]: Inference (test):avg data time: 1.28e-04, avg batch time: 0.2051, average loss: 3.4401
[09/28 06:18:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.41	top5: 74.49	
[09/28 06:18:56 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/28 06:19:05 visual_prompt]: Epoch 76 / 100: avg data time: 1.05e-01, avg batch time: 0.5326, average train loss: 0.0109
[09/28 06:19:08 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1774, average loss: 0.0083
[09/28 06:19:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:19:31 visual_prompt]: 	Test 100/190. loss: 3.361, 0.2037 s / batch. (data: 2.88e-05)max mem: 7.80438 GB 
[09/28 06:19:50 visual_prompt]: Inference (test):avg data time: 3.04e-05, avg batch time: 0.2049, average loss: 3.4301
[09/28 06:19:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.47	top5: 74.62	
[09/28 06:19:50 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/28 06:20:00 visual_prompt]: Epoch 77 / 100: avg data time: 1.05e-01, avg batch time: 0.5340, average train loss: 0.0113
[09/28 06:20:03 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1734, average loss: 0.0082
[09/28 06:20:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:20:25 visual_prompt]: 	Test 100/190. loss: 3.356, 0.2049 s / batch. (data: 2.91e-05)max mem: 7.80438 GB 
[09/28 06:20:45 visual_prompt]: Inference (test):avg data time: 3.06e-05, avg batch time: 0.2052, average loss: 3.4185
[09/28 06:20:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.64	top5: 74.56	
[09/28 06:20:45 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/28 06:20:54 visual_prompt]: Epoch 78 / 100: avg data time: 1.06e-01, avg batch time: 0.5318, average train loss: 0.0109
[09/28 06:20:58 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1797, average loss: 0.0087
[09/28 06:20:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:21:20 visual_prompt]: 	Test 100/190. loss: 3.353, 0.2056 s / batch. (data: 3.17e-05)max mem: 7.80438 GB 
[09/28 06:21:40 visual_prompt]: Inference (test):avg data time: 3.08e-05, avg batch time: 0.2049, average loss: 3.4250
[09/28 06:21:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.60	top5: 74.62	
[09/28 06:21:40 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/28 06:21:49 visual_prompt]: Epoch 79 / 100: avg data time: 1.07e-01, avg batch time: 0.5345, average train loss: 0.0107
[09/28 06:21:52 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1845, average loss: 0.0079
[09/28 06:21:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:22:15 visual_prompt]: 	Test 100/190. loss: 3.328, 0.2047 s / batch. (data: 3.10e-05)max mem: 7.80438 GB 
[09/28 06:22:34 visual_prompt]: Inference (test):avg data time: 3.12e-05, avg batch time: 0.2047, average loss: 3.4202
[09/28 06:22:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.63	top5: 74.61	
[09/28 06:22:34 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/28 06:22:44 visual_prompt]: Epoch 80 / 100: avg data time: 1.19e-01, avg batch time: 0.5454, average train loss: 0.0102
[09/28 06:22:47 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1770, average loss: 0.0074
[09/28 06:22:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:23:09 visual_prompt]: 	Test 100/190. loss: 3.332, 0.2034 s / batch. (data: 2.98e-05)max mem: 7.80438 GB 
[09/28 06:23:29 visual_prompt]: Inference (test):avg data time: 3.27e-05, avg batch time: 0.2048, average loss: 3.4201
[09/28 06:23:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.74	top5: 74.53	
[09/28 06:23:29 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/28 06:23:39 visual_prompt]: Epoch 81 / 100: avg data time: 1.15e-01, avg batch time: 0.5429, average train loss: 0.0100
[09/28 06:23:42 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1833, average loss: 0.0074
[09/28 06:23:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:24:04 visual_prompt]: 	Test 100/190. loss: 3.338, 0.2061 s / batch. (data: 3.08e-05)max mem: 7.80438 GB 
[09/28 06:24:24 visual_prompt]: Inference (test):avg data time: 3.05e-05, avg batch time: 0.2050, average loss: 3.4188
[09/28 06:24:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.76	top5: 74.63	
[09/28 06:24:24 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/28 06:24:33 visual_prompt]: Epoch 82 / 100: avg data time: 1.05e-01, avg batch time: 0.5321, average train loss: 0.0098
[09/28 06:24:37 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1751, average loss: 0.0075
[09/28 06:24:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:24:59 visual_prompt]: 	Test 100/190. loss: 3.345, 0.2040 s / batch. (data: 3.17e-05)max mem: 7.80438 GB 
[09/28 06:25:18 visual_prompt]: Inference (test):avg data time: 1.46e-04, avg batch time: 0.2052, average loss: 3.4197
[09/28 06:25:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.78	top5: 74.85	
[09/28 06:25:18 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/28 06:25:28 visual_prompt]: Epoch 83 / 100: avg data time: 1.06e-01, avg batch time: 0.5339, average train loss: 0.0097
[09/28 06:25:31 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1733, average loss: 0.0075
[09/28 06:25:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:25:53 visual_prompt]: 	Test 100/190. loss: 3.358, 0.2040 s / batch. (data: 3.93e-05)max mem: 7.80438 GB 
[09/28 06:26:13 visual_prompt]: Inference (test):avg data time: 1.04e-04, avg batch time: 0.2054, average loss: 3.4186
[09/28 06:26:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.94	top5: 74.81	
[09/28 06:26:13 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/28 06:26:23 visual_prompt]: Epoch 84 / 100: avg data time: 1.08e-01, avg batch time: 0.5360, average train loss: 0.0100
[09/28 06:26:26 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1773, average loss: 0.0074
[09/28 06:26:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:26:48 visual_prompt]: 	Test 100/190. loss: 3.358, 0.2032 s / batch. (data: 3.19e-05)max mem: 7.80438 GB 
[09/28 06:27:08 visual_prompt]: Inference (test):avg data time: 3.17e-05, avg batch time: 0.2051, average loss: 3.4134
[09/28 06:27:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.96	top5: 74.76	
[09/28 06:27:08 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/28 06:27:17 visual_prompt]: Epoch 85 / 100: avg data time: 1.04e-01, avg batch time: 0.5328, average train loss: 0.0101
[09/28 06:27:20 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1810, average loss: 0.0074
[09/28 06:27:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:27:43 visual_prompt]: 	Test 100/190. loss: 3.363, 0.2036 s / batch. (data: 2.72e-05)max mem: 7.80438 GB 
[09/28 06:28:02 visual_prompt]: Inference (test):avg data time: 3.29e-05, avg batch time: 0.2049, average loss: 3.4090
[09/28 06:28:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.91	top5: 74.78	
[09/28 06:28:02 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/28 06:28:12 visual_prompt]: Epoch 86 / 100: avg data time: 1.16e-01, avg batch time: 0.5462, average train loss: 0.0097
[09/28 06:28:15 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1785, average loss: 0.0073
[09/28 06:28:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:28:38 visual_prompt]: 	Test 100/190. loss: 3.349, 0.2043 s / batch. (data: 2.96e-05)max mem: 7.80438 GB 
[09/28 06:28:57 visual_prompt]: Inference (test):avg data time: 3.33e-05, avg batch time: 0.2049, average loss: 3.4056
[09/28 06:28:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.88	top5: 74.87	
[09/28 06:28:57 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/28 06:29:07 visual_prompt]: Epoch 87 / 100: avg data time: 1.12e-01, avg batch time: 0.5408, average train loss: 0.0096
[09/28 06:29:10 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1779, average loss: 0.0073
[09/28 06:29:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:29:32 visual_prompt]: 	Test 100/190. loss: 3.341, 0.2045 s / batch. (data: 3.05e-05)max mem: 7.80438 GB 
[09/28 06:29:52 visual_prompt]: Inference (test):avg data time: 3.03e-05, avg batch time: 0.2048, average loss: 3.4071
[09/28 06:29:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.95	top5: 74.86	
[09/28 06:29:52 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/28 06:30:01 visual_prompt]: Epoch 88 / 100: avg data time: 1.07e-01, avg batch time: 0.5325, average train loss: 0.0095
[09/28 06:30:05 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1786, average loss: 0.0073
[09/28 06:30:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:30:27 visual_prompt]: 	Test 100/190. loss: 3.341, 0.2041 s / batch. (data: 2.91e-05)max mem: 7.80438 GB 
[09/28 06:30:47 visual_prompt]: Inference (test):avg data time: 1.08e-04, avg batch time: 0.2049, average loss: 3.4071
[09/28 06:30:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.84	top5: 74.89	
[09/28 06:30:47 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/28 06:30:56 visual_prompt]: Epoch 89 / 100: avg data time: 1.12e-01, avg batch time: 0.5416, average train loss: 0.0090
[09/28 06:30:59 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1761, average loss: 0.0072
[09/28 06:30:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:31:22 visual_prompt]: 	Test 100/190. loss: 3.338, 0.2048 s / batch. (data: 3.24e-05)max mem: 7.80438 GB 
[09/28 06:31:41 visual_prompt]: Inference (test):avg data time: 6.99e-05, avg batch time: 0.2050, average loss: 3.4063
[09/28 06:31:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.85	top5: 74.96	
[09/28 06:31:41 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/28 06:31:51 visual_prompt]: Epoch 90 / 100: avg data time: 1.08e-01, avg batch time: 0.5334, average train loss: 0.0094
[09/28 06:31:54 visual_prompt]: Inference (val):avg data time: 1.85e-05, avg batch time: 0.1778, average loss: 0.0071
[09/28 06:31:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:32:16 visual_prompt]: 	Test 100/190. loss: 3.334, 0.2053 s / batch. (data: 3.03e-05)max mem: 7.80438 GB 
[09/28 06:32:36 visual_prompt]: Inference (test):avg data time: 3.34e-05, avg batch time: 0.2052, average loss: 3.4051
[09/28 06:32:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.93	top5: 74.96	
[09/28 06:32:36 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/28 06:32:46 visual_prompt]: Epoch 91 / 100: avg data time: 1.05e-01, avg batch time: 0.5303, average train loss: 0.0097
[09/28 06:32:49 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1785, average loss: 0.0071
[09/28 06:32:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:33:11 visual_prompt]: 	Test 100/190. loss: 3.336, 0.2047 s / batch. (data: 3.19e-05)max mem: 7.80438 GB 
[09/28 06:33:31 visual_prompt]: Inference (test):avg data time: 3.27e-05, avg batch time: 0.2050, average loss: 3.4055
[09/28 06:33:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.91	top5: 74.91	
[09/28 06:33:31 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/28 06:33:40 visual_prompt]: Epoch 92 / 100: avg data time: 9.49e-02, avg batch time: 0.5220, average train loss: 0.0093
[09/28 06:33:44 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1775, average loss: 0.0070
[09/28 06:33:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:34:06 visual_prompt]: 	Test 100/190. loss: 3.336, 0.2042 s / batch. (data: 2.98e-05)max mem: 7.80438 GB 
[09/28 06:34:25 visual_prompt]: Inference (test):avg data time: 3.39e-05, avg batch time: 0.2051, average loss: 3.4060
[09/28 06:34:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.90	top5: 74.91	
[09/28 06:34:25 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/28 06:34:35 visual_prompt]: Epoch 93 / 100: avg data time: 1.04e-01, avg batch time: 0.5335, average train loss: 0.0092
[09/28 06:34:38 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1745, average loss: 0.0070
[09/28 06:34:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:35:01 visual_prompt]: 	Test 100/190. loss: 3.334, 0.2049 s / batch. (data: 3.10e-05)max mem: 7.80438 GB 
[09/28 06:35:20 visual_prompt]: Inference (test):avg data time: 1.69e-04, avg batch time: 0.2051, average loss: 3.4069
[09/28 06:35:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.88	top5: 74.95	
[09/28 06:35:20 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/28 06:35:30 visual_prompt]: Epoch 94 / 100: avg data time: 1.05e-01, avg batch time: 0.5323, average train loss: 0.0093
[09/28 06:35:33 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1794, average loss: 0.0070
[09/28 06:35:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:35:55 visual_prompt]: 	Test 100/190. loss: 3.334, 0.2046 s / batch. (data: 3.19e-05)max mem: 7.80438 GB 
[09/28 06:36:15 visual_prompt]: Inference (test):avg data time: 3.21e-05, avg batch time: 0.2049, average loss: 3.4078
[09/28 06:36:15 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.88	top5: 74.94	
[09/28 06:36:15 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/28 06:36:24 visual_prompt]: Epoch 95 / 100: avg data time: 1.02e-01, avg batch time: 0.5313, average train loss: 0.0096
[09/28 06:36:28 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1769, average loss: 0.0069
[09/28 06:36:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:36:50 visual_prompt]: 	Test 100/190. loss: 3.334, 0.2040 s / batch. (data: 2.57e-05)max mem: 7.80438 GB 
[09/28 06:37:09 visual_prompt]: Inference (test):avg data time: 3.13e-05, avg batch time: 0.2049, average loss: 3.4081
[09/28 06:37:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.89	top5: 74.91	
[09/28 06:37:09 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/28 06:37:19 visual_prompt]: Epoch 96 / 100: avg data time: 1.05e-01, avg batch time: 0.5308, average train loss: 0.0094
[09/28 06:37:22 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1760, average loss: 0.0069
[09/28 06:37:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:37:44 visual_prompt]: 	Test 100/190. loss: 3.334, 0.2055 s / batch. (data: 3.39e-05)max mem: 7.80438 GB 
[09/28 06:38:04 visual_prompt]: Inference (test):avg data time: 3.08e-05, avg batch time: 0.2048, average loss: 3.4079
[09/28 06:38:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.89	top5: 74.93	
[09/28 06:38:04 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/28 06:38:13 visual_prompt]: Epoch 97 / 100: avg data time: 9.98e-02, avg batch time: 0.5263, average train loss: 0.0091
[09/28 06:38:17 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.1857, average loss: 0.0069
[09/28 06:38:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:38:39 visual_prompt]: 	Test 100/190. loss: 3.335, 0.2034 s / batch. (data: 3.00e-05)max mem: 7.80438 GB 
[09/28 06:38:58 visual_prompt]: Inference (test):avg data time: 3.26e-05, avg batch time: 0.2048, average loss: 3.4078
[09/28 06:38:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.89	top5: 74.91	
[09/28 06:38:58 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/28 06:39:08 visual_prompt]: Epoch 98 / 100: avg data time: 1.08e-01, avg batch time: 0.5343, average train loss: 0.0098
[09/28 06:39:11 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1722, average loss: 0.0069
[09/28 06:39:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:39:33 visual_prompt]: 	Test 100/190. loss: 3.335, 0.2042 s / batch. (data: 3.17e-05)max mem: 7.80438 GB 
[09/28 06:39:53 visual_prompt]: Inference (test):avg data time: 7.97e-05, avg batch time: 0.2048, average loss: 3.4079
[09/28 06:39:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.90	top5: 74.91	
[09/28 06:39:53 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/28 06:40:03 visual_prompt]: Epoch 99 / 100: avg data time: 1.10e-01, avg batch time: 0.5393, average train loss: 0.0096
[09/28 06:40:06 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1819, average loss: 0.0069
[09/28 06:40:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:40:28 visual_prompt]: 	Test 100/190. loss: 3.335, 0.2053 s / batch. (data: 5.25e-05)max mem: 7.80438 GB 
[09/28 06:40:48 visual_prompt]: Inference (test):avg data time: 3.48e-05, avg batch time: 0.2047, average loss: 3.4078
[09/28 06:40:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.90	top5: 74.91	
[09/28 06:40:48 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/28 06:40:57 visual_prompt]: Epoch 100 / 100: avg data time: 1.11e-01, avg batch time: 0.5393, average train loss: 0.0091
[09/28 06:41:00 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1802, average loss: 0.0069
[09/28 06:41:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 06:41:23 visual_prompt]: 	Test 100/190. loss: 3.335, 0.2044 s / batch. (data: 3.22e-05)max mem: 7.80438 GB 
[09/28 06:41:42 visual_prompt]: Inference (test):avg data time: 3.09e-05, avg batch time: 0.2051, average loss: 3.4078
[09/28 06:41:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.91	top5: 74.91	
[09/28 06:41:42 visual_prompt]: Rank of current process: 0. World size: 1
[09/28 06:41:42 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/28 06:41:42 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_azimuth")', 'DATA.NUMBER_CLASSES', '18', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/28 06:41:42 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/28 06:41:42 visual_prompt]: Training with config:
[09/28 06:41:42 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_azimuth")/sup_vitb16_imagenet21k/prompt50/crop224/test/seed4972/lr0.25_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 4972, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_azimuth")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 18, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/28 06:41:42 visual_prompt]: Loading training data...
[09/28 06:41:42 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset trainval...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800]+test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/28 06:41:44 visual_prompt]: Number of images: 1000
[09/28 06:41:44 visual_prompt]: Number of classes: 18 / 18
[09/28 06:41:44 visual_prompt]: Loading validation data...
[09/28 06:41:44 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/28 06:41:44 visual_prompt]: Number of images: 200
[09/28 06:41:44 visual_prompt]: Number of classes: 18 / 18
[09/28 06:41:44 visual_prompt]: Loading test data...
[09/28 06:41:44 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset test...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[50%:], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/28 06:42:00 visual_prompt]: Number of images: 12150
[09/28 06:42:00 visual_prompt]: Number of classes: 18 / 18
[09/28 06:42:00 visual_prompt]: Constructing models...
[09/28 06:42:03 visual_prompt]: Total Parameters: 86273298	 Gradient Parameters: 474642
[09/28 06:42:03 visual_prompt]: tuned percent:0.550
[09/28 06:42:03 visual_prompt]: Device used for model: 0
[09/28 06:42:03 visual_prompt]: Setting up Evaluator...
[09/28 06:42:03 visual_prompt]: Setting up Trainer...
[09/28 06:42:03 visual_prompt]: 	Setting up the optimizer...
[09/28 06:42:03 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/28 06:42:13 visual_prompt]: Epoch 1 / 100: avg data time: 1.25e-01, avg batch time: 0.5466, average train loss: 3.2021
[09/28 06:42:16 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1760, average loss: 3.2211
[09/28 06:42:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 23.50	
[09/28 06:42:38 visual_prompt]: 	Test 100/190. loss: 3.142, 0.2031 s / batch. (data: 3.03e-05)max mem: 7.80523 GB 
[09/28 06:42:58 visual_prompt]: Inference (test):avg data time: 3.27e-05, avg batch time: 0.2040, average loss: 3.1811
[09/28 06:42:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 28.71	
[09/28 06:42:58 visual_prompt]: Best epoch 1: best metric: 0.055
[09/28 06:42:58 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/28 06:43:08 visual_prompt]: Epoch 2 / 100: avg data time: 1.14e-01, avg batch time: 0.5390, average train loss: 2.9806
[09/28 06:43:11 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1762, average loss: 2.9003
[09/28 06:43:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 28.50	
[09/28 06:43:33 visual_prompt]: 	Test 100/190. loss: 2.936, 0.2033 s / batch. (data: 3.19e-05)max mem: 7.80552 GB 
[09/28 06:43:53 visual_prompt]: Inference (test):avg data time: 3.39e-05, avg batch time: 0.2045, average loss: 2.9231
[09/28 06:43:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.43	top5: 26.63	
[09/28 06:43:53 visual_prompt]: Best epoch 2: best metric: 0.085
[09/28 06:43:53 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/28 06:44:03 visual_prompt]: Epoch 3 / 100: avg data time: 1.09e-01, avg batch time: 0.5344, average train loss: 2.8984
[09/28 06:44:06 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1805, average loss: 2.8884
[09/28 06:44:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 29.50	
[09/28 06:44:28 visual_prompt]: 	Test 100/190. loss: 2.880, 0.2042 s / batch. (data: 3.08e-05)max mem: 7.80552 GB 
[09/28 06:44:48 visual_prompt]: Inference (test):avg data time: 3.47e-05, avg batch time: 0.2044, average loss: 2.9030
[09/28 06:44:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.62	top5: 27.84	
[09/28 06:44:48 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/28 06:44:57 visual_prompt]: Epoch 4 / 100: avg data time: 9.63e-02, avg batch time: 0.5232, average train loss: 2.9016
[09/28 06:45:01 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1766, average loss: 2.8961
[09/28 06:45:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 31.00	
[09/28 06:45:23 visual_prompt]: 	Test 100/190. loss: 2.902, 0.2049 s / batch. (data: 3.08e-05)max mem: 7.80552 GB 
[09/28 06:45:43 visual_prompt]: Inference (test):avg data time: 1.01e-04, avg batch time: 0.2045, average loss: 2.9040
[09/28 06:45:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.63	top5: 28.20	
[09/28 06:45:43 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/28 06:45:52 visual_prompt]: Epoch 5 / 100: avg data time: 1.09e-01, avg batch time: 0.5360, average train loss: 2.9043
[09/28 06:45:55 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1811, average loss: 2.8920
[09/28 06:45:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 34.00	
[09/28 06:46:18 visual_prompt]: 	Test 100/190. loss: 2.895, 0.2035 s / batch. (data: 2.55e-05)max mem: 7.80552 GB 
[09/28 06:46:37 visual_prompt]: Inference (test):avg data time: 3.10e-05, avg batch time: 0.2045, average loss: 2.8981
[09/28 06:46:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.87	top5: 28.07	
[09/28 06:46:37 visual_prompt]: Best epoch 5: best metric: 0.090
[09/28 06:46:37 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/28 06:46:47 visual_prompt]: Epoch 6 / 100: avg data time: 1.10e-01, avg batch time: 0.5378, average train loss: 2.9172
[09/28 06:46:50 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1785, average loss: 2.8867
[09/28 06:46:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 33.50	
[09/28 06:47:12 visual_prompt]: 	Test 100/190. loss: 2.961, 0.2033 s / batch. (data: 2.88e-05)max mem: 7.80552 GB 
[09/28 06:47:32 visual_prompt]: Inference (test):avg data time: 3.13e-05, avg batch time: 0.2044, average loss: 2.9089
[09/28 06:47:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.61	top5: 28.81	
[09/28 06:47:32 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/28 06:47:42 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e-01, avg batch time: 0.5281, average train loss: 2.9009
[09/28 06:47:45 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1806, average loss: 2.8893
[09/28 06:47:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 31.00	
[09/28 06:48:07 visual_prompt]: 	Test 100/190. loss: 2.887, 0.2041 s / batch. (data: 2.77e-05)max mem: 7.80552 GB 
[09/28 06:48:27 visual_prompt]: Inference (test):avg data time: 3.08e-05, avg batch time: 0.2050, average loss: 2.9213
[09/28 06:48:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.42	top5: 28.79	
[09/28 06:48:27 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/28 06:48:36 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e-01, avg batch time: 0.5324, average train loss: 2.9115
[09/28 06:48:40 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1762, average loss: 2.8762
[09/28 06:48:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 35.50	
[09/28 06:49:02 visual_prompt]: 	Test 100/190. loss: 2.946, 0.2043 s / batch. (data: 3.05e-05)max mem: 7.80552 GB 
[09/28 06:49:22 visual_prompt]: Inference (test):avg data time: 3.44e-05, avg batch time: 0.2047, average loss: 2.9369
[09/28 06:49:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.43	top5: 27.79	
[09/28 06:49:22 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/28 06:49:31 visual_prompt]: Epoch 9 / 100: avg data time: 1.02e-01, avg batch time: 0.5284, average train loss: 2.9035
[09/28 06:49:34 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1843, average loss: 2.8519
[09/28 06:49:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.50	top5: 36.00	
[09/28 06:49:57 visual_prompt]: 	Test 100/190. loss: 2.894, 0.2040 s / batch. (data: 3.43e-05)max mem: 7.80552 GB 
[09/28 06:50:16 visual_prompt]: Inference (test):avg data time: 1.10e-04, avg batch time: 0.2047, average loss: 2.9041
[09/28 06:50:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.41	top5: 30.35	
[09/28 06:50:16 visual_prompt]: Best epoch 9: best metric: 0.095
[09/28 06:50:16 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/28 06:50:26 visual_prompt]: Epoch 10 / 100: avg data time: 1.06e-01, avg batch time: 0.5311, average train loss: 2.8864
[09/28 06:50:29 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1813, average loss: 2.9124
[09/28 06:50:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 29.50	
[09/28 06:50:51 visual_prompt]: 	Test 100/190. loss: 2.828, 0.2046 s / batch. (data: 3.46e-05)max mem: 7.80552 GB 
[09/28 06:51:11 visual_prompt]: Inference (test):avg data time: 4.58e-05, avg batch time: 0.2043, average loss: 2.9167
[09/28 06:51:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.63	top5: 29.84	
[09/28 06:51:11 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/28 06:51:20 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e-01, avg batch time: 0.5283, average train loss: 2.8854
[09/28 06:51:24 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1847, average loss: 2.8540
[09/28 06:51:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 41.50	
[09/28 06:51:46 visual_prompt]: 	Test 100/190. loss: 2.911, 0.2054 s / batch. (data: 2.74e-05)max mem: 7.80552 GB 
[09/28 06:52:06 visual_prompt]: Inference (test):avg data time: 1.06e-04, avg batch time: 0.2046, average loss: 2.8873
[09/28 06:52:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.74	top5: 34.49	
[09/28 06:52:06 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/28 06:52:15 visual_prompt]: Epoch 12 / 100: avg data time: 1.04e-01, avg batch time: 0.5318, average train loss: 2.8663
[09/28 06:52:19 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1806, average loss: 3.0191
[09/28 06:52:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.00	top5: 30.00	
[09/28 06:52:41 visual_prompt]: 	Test 100/190. loss: 2.997, 0.2029 s / batch. (data: 2.53e-05)max mem: 7.80552 GB 
[09/28 06:53:00 visual_prompt]: Inference (test):avg data time: 5.87e-05, avg batch time: 0.2047, average loss: 3.0643
[09/28 06:53:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.07	top5: 31.25	
[09/28 06:53:00 visual_prompt]: Best epoch 12: best metric: 0.110
[09/28 06:53:00 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/28 06:53:10 visual_prompt]: Epoch 13 / 100: avg data time: 1.06e-01, avg batch time: 0.5323, average train loss: 2.8351
[09/28 06:53:13 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1827, average loss: 2.7393
[09/28 06:53:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 49.50	
[09/28 06:53:36 visual_prompt]: 	Test 100/190. loss: 2.824, 0.2049 s / batch. (data: 2.69e-05)max mem: 7.80552 GB 
[09/28 06:53:55 visual_prompt]: Inference (test):avg data time: 3.08e-05, avg batch time: 0.2048, average loss: 2.8204
[09/28 06:53:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.68	top5: 39.22	
[09/28 06:53:55 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/28 06:54:05 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e-01, avg batch time: 0.5311, average train loss: 2.8107
[09/28 06:54:08 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1818, average loss: 2.8422
[09/28 06:54:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 36.50	
[09/28 06:54:30 visual_prompt]: 	Test 100/190. loss: 2.929, 0.2061 s / batch. (data: 2.96e-05)max mem: 7.80552 GB 
[09/28 06:54:50 visual_prompt]: Inference (test):avg data time: 3.33e-05, avg batch time: 0.2047, average loss: 2.8917
[09/28 06:54:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.02	top5: 37.02	
[09/28 06:54:50 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/28 06:55:00 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e-01, avg batch time: 0.5315, average train loss: 2.7596
[09/28 06:55:03 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1823, average loss: 2.8229
[09/28 06:55:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 50.50	
[09/28 06:55:25 visual_prompt]: 	Test 100/190. loss: 3.374, 0.2044 s / batch. (data: 3.08e-05)max mem: 7.80552 GB 
[09/28 06:55:45 visual_prompt]: Inference (test):avg data time: 3.48e-05, avg batch time: 0.2048, average loss: 3.0573
[09/28 06:55:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.44	top5: 40.90	
[09/28 06:55:45 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/28 06:55:55 visual_prompt]: Epoch 16 / 100: avg data time: 9.83e-02, avg batch time: 0.5272, average train loss: 2.8299
[09/28 06:55:58 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1732, average loss: 2.6737
[09/28 06:55:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 51.00	
[09/28 06:56:20 visual_prompt]: 	Test 100/190. loss: 2.765, 0.2046 s / batch. (data: 2.74e-05)max mem: 7.80552 GB 
[09/28 06:56:40 visual_prompt]: Inference (test):avg data time: 3.39e-05, avg batch time: 0.2048, average loss: 2.7822
[09/28 06:56:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 41.58	
[09/28 06:56:40 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/28 06:56:49 visual_prompt]: Epoch 17 / 100: avg data time: 1.08e-01, avg batch time: 0.5346, average train loss: 2.6316
[09/28 06:56:53 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1773, average loss: 2.5053
[09/28 06:56:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.50	top5: 62.50	
[09/28 06:57:15 visual_prompt]: 	Test 100/190. loss: 2.725, 0.2056 s / batch. (data: 2.74e-05)max mem: 7.80552 GB 
[09/28 06:57:35 visual_prompt]: Inference (test):avg data time: 8.92e-05, avg batch time: 0.2047, average loss: 2.7093
[09/28 06:57:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.51	top5: 49.10	
[09/28 06:57:35 visual_prompt]: Best epoch 17: best metric: 0.155
[09/28 06:57:35 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/28 06:57:44 visual_prompt]: Epoch 18 / 100: avg data time: 1.00e-01, avg batch time: 0.5275, average train loss: 2.5112
[09/28 06:57:47 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1722, average loss: 2.4153
[09/28 06:57:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.00	top5: 63.00	
[09/28 06:58:09 visual_prompt]: 	Test 100/190. loss: 2.666, 0.2051 s / batch. (data: 2.62e-05)max mem: 7.80552 GB 
[09/28 06:58:29 visual_prompt]: Inference (test):avg data time: 3.01e-05, avg batch time: 0.2048, average loss: 2.5953
[09/28 06:58:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.60	top5: 56.99	
[09/28 06:58:29 visual_prompt]: Best epoch 18: best metric: 0.180
[09/28 06:58:29 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/28 06:58:39 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e-01, avg batch time: 0.5305, average train loss: 2.4633
[09/28 06:58:42 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1855, average loss: 2.3490
[09/28 06:58:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.00	top5: 65.50	
[09/28 06:59:04 visual_prompt]: 	Test 100/190. loss: 2.596, 0.2045 s / batch. (data: 2.84e-05)max mem: 7.80552 GB 
[09/28 06:59:24 visual_prompt]: Inference (test):avg data time: 2.96e-05, avg batch time: 0.2045, average loss: 2.5566
[09/28 06:59:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.28	top5: 58.21	
[09/28 06:59:24 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/28 06:59:33 visual_prompt]: Epoch 20 / 100: avg data time: 1.08e-01, avg batch time: 0.5361, average train loss: 2.3305
[09/28 06:59:37 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1762, average loss: 2.2136
[09/28 06:59:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.50	top5: 78.50	
[09/28 06:59:59 visual_prompt]: 	Test 100/190. loss: 2.460, 0.2034 s / batch. (data: 3.05e-05)max mem: 7.80552 GB 
[09/28 07:00:19 visual_prompt]: Inference (test):avg data time: 1.44e-04, avg batch time: 0.2050, average loss: 2.5298
[09/28 07:00:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.88	top5: 63.05	
[09/28 07:00:19 visual_prompt]: Best epoch 20: best metric: 0.185
[09/28 07:00:19 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/28 07:00:28 visual_prompt]: Epoch 21 / 100: avg data time: 9.42e-02, avg batch time: 0.5210, average train loss: 2.2120
[09/28 07:00:31 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.1785, average loss: 2.5439
[09/28 07:00:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.00	top5: 66.50	
[09/28 07:00:54 visual_prompt]: 	Test 100/190. loss: 2.999, 0.2051 s / batch. (data: 3.08e-05)max mem: 7.80552 GB 
[09/28 07:01:13 visual_prompt]: Inference (test):avg data time: 3.72e-05, avg batch time: 0.2046, average loss: 2.8355
[09/28 07:01:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.29	top5: 53.35	
[09/28 07:01:13 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/28 07:01:23 visual_prompt]: Epoch 22 / 100: avg data time: 1.08e-01, avg batch time: 0.5366, average train loss: 2.1052
[09/28 07:01:26 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1743, average loss: 2.2115
[09/28 07:01:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.00	top5: 76.50	
[09/28 07:01:49 visual_prompt]: 	Test 100/190. loss: 2.500, 0.2055 s / batch. (data: 2.96e-05)max mem: 7.80552 GB 
[09/28 07:02:08 visual_prompt]: Inference (test):avg data time: 1.09e-04, avg batch time: 0.2048, average loss: 2.5432
[09/28 07:02:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.32	top5: 66.72	
[09/28 07:02:08 visual_prompt]: Best epoch 22: best metric: 0.190
[09/28 07:02:08 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/28 07:02:18 visual_prompt]: Epoch 23 / 100: avg data time: 1.02e-01, avg batch time: 0.5298, average train loss: 1.9941
[09/28 07:02:21 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1771, average loss: 1.9721
[09/28 07:02:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 35.00	top5: 83.50	
[09/28 07:02:43 visual_prompt]: 	Test 100/190. loss: 2.920, 0.2050 s / batch. (data: 2.88e-05)max mem: 7.80552 GB 
[09/28 07:03:03 visual_prompt]: Inference (test):avg data time: 7.84e-05, avg batch time: 0.2050, average loss: 2.5105
[09/28 07:03:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.23	top5: 69.43	
[09/28 07:03:03 visual_prompt]: Best epoch 23: best metric: 0.350
[09/28 07:03:03 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/28 07:03:13 visual_prompt]: Epoch 24 / 100: avg data time: 1.03e-01, avg batch time: 0.5287, average train loss: 1.9440
[09/28 07:03:16 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1750, average loss: 2.0811
[09/28 07:03:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 23.00	top5: 79.50	
[09/28 07:03:38 visual_prompt]: 	Test 100/190. loss: 2.727, 0.2039 s / batch. (data: 2.74e-05)max mem: 7.80552 GB 
[09/28 07:03:58 visual_prompt]: Inference (test):avg data time: 3.19e-05, avg batch time: 0.2047, average loss: 2.6963
[09/28 07:03:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.05	top5: 64.94	
[09/28 07:03:58 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/28 07:04:07 visual_prompt]: Epoch 25 / 100: avg data time: 9.41e-02, avg batch time: 0.5196, average train loss: 1.8065
[09/28 07:04:10 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1782, average loss: 1.9779
[09/28 07:04:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 34.00	top5: 81.00	
[09/28 07:04:32 visual_prompt]: 	Test 100/190. loss: 2.857, 0.2043 s / batch. (data: 2.74e-05)max mem: 7.80552 GB 
[09/28 07:04:52 visual_prompt]: Inference (test):avg data time: 1.14e-04, avg batch time: 0.2047, average loss: 2.7463
[09/28 07:04:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.57	top5: 68.57	
[09/28 07:04:52 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/28 07:05:02 visual_prompt]: Epoch 26 / 100: avg data time: 1.05e-01, avg batch time: 0.5317, average train loss: 1.7076
[09/28 07:05:05 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1769, average loss: 1.9551
[09/28 07:05:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.50	top5: 85.50	
[09/28 07:05:27 visual_prompt]: 	Test 100/190. loss: 2.903, 0.2045 s / batch. (data: 3.08e-05)max mem: 7.80552 GB 
[09/28 07:05:47 visual_prompt]: Inference (test):avg data time: 3.37e-05, avg batch time: 0.2049, average loss: 2.7044
[09/28 07:05:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.30	top5: 68.28	
[09/28 07:05:47 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/28 07:05:57 visual_prompt]: Epoch 27 / 100: avg data time: 9.82e-02, avg batch time: 0.5261, average train loss: 1.4884
[09/28 07:06:00 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1847, average loss: 1.7116
[09/28 07:06:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 40.50	top5: 87.50	
[09/28 07:06:22 visual_prompt]: 	Test 100/190. loss: 2.976, 0.2049 s / batch. (data: 2.93e-05)max mem: 7.80552 GB 
[09/28 07:06:42 visual_prompt]: Inference (test):avg data time: 4.57e-05, avg batch time: 0.2047, average loss: 2.8660
[09/28 07:06:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 20.21	top5: 67.01	
[09/28 07:06:42 visual_prompt]: Best epoch 27: best metric: 0.405
[09/28 07:06:42 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/28 07:06:52 visual_prompt]: Epoch 28 / 100: avg data time: 1.08e-01, avg batch time: 0.5385, average train loss: 1.4101
[09/28 07:06:55 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1736, average loss: 1.4618
[09/28 07:06:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 50.50	top5: 94.50	
[09/28 07:07:17 visual_prompt]: 	Test 100/190. loss: 2.841, 0.2039 s / batch. (data: 2.62e-05)max mem: 7.80552 GB 
[09/28 07:07:37 visual_prompt]: Inference (test):avg data time: 3.38e-05, avg batch time: 0.2048, average loss: 2.7659
[09/28 07:07:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 20.38	top5: 69.67	
[09/28 07:07:37 visual_prompt]: Best epoch 28: best metric: 0.505
[09/28 07:07:37 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/28 07:07:46 visual_prompt]: Epoch 29 / 100: avg data time: 1.12e-01, avg batch time: 0.5378, average train loss: 1.2870
[09/28 07:07:50 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1760, average loss: 1.3502
[09/28 07:07:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 51.50	top5: 95.00	
[09/28 07:08:12 visual_prompt]: 	Test 100/190. loss: 2.984, 0.2047 s / batch. (data: 2.79e-05)max mem: 7.80552 GB 
[09/28 07:08:32 visual_prompt]: Inference (test):avg data time: 1.99e-04, avg batch time: 0.2050, average loss: 2.8369
[09/28 07:08:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.40	top5: 69.60	
[09/28 07:08:32 visual_prompt]: Best epoch 29: best metric: 0.515
[09/28 07:08:32 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/28 07:08:41 visual_prompt]: Epoch 30 / 100: avg data time: 1.04e-01, avg batch time: 0.5292, average train loss: 1.1345
[09/28 07:08:44 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1739, average loss: 0.9969
[09/28 07:08:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 64.00	top5: 97.50	
[09/28 07:09:07 visual_prompt]: 	Test 100/190. loss: 2.938, 0.2047 s / batch. (data: 3.03e-05)max mem: 7.80552 GB 
[09/28 07:09:26 visual_prompt]: Inference (test):avg data time: 1.41e-04, avg batch time: 0.2045, average loss: 2.9516
[09/28 07:09:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 23.20	top5: 70.38	
[09/28 07:09:26 visual_prompt]: Best epoch 30: best metric: 0.640
[09/28 07:09:26 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/28 07:09:36 visual_prompt]: Epoch 31 / 100: avg data time: 1.06e-01, avg batch time: 0.5323, average train loss: 0.9533
[09/28 07:09:39 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1813, average loss: 0.9482
[09/28 07:09:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 73.00	top5: 96.00	
[09/28 07:10:01 visual_prompt]: 	Test 100/190. loss: 3.050, 0.2059 s / batch. (data: 9.68e-05)max mem: 7.80552 GB 
[09/28 07:10:21 visual_prompt]: Inference (test):avg data time: 3.12e-05, avg batch time: 0.2046, average loss: 2.9480
[09/28 07:10:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 23.46	top5: 70.74	
[09/28 07:10:21 visual_prompt]: Best epoch 31: best metric: 0.730
[09/28 07:10:21 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/28 07:10:30 visual_prompt]: Epoch 32 / 100: avg data time: 1.02e-01, avg batch time: 0.5287, average train loss: 0.8078
[09/28 07:10:34 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1827, average loss: 0.6897
[09/28 07:10:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 78.50	top5: 99.50	
[09/28 07:10:56 visual_prompt]: 	Test 100/190. loss: 2.876, 0.2051 s / batch. (data: 3.17e-05)max mem: 7.80552 GB 
[09/28 07:11:16 visual_prompt]: Inference (test):avg data time: 3.24e-05, avg batch time: 0.2047, average loss: 2.9318
[09/28 07:11:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.07	top5: 70.63	
[09/28 07:11:16 visual_prompt]: Best epoch 32: best metric: 0.785
[09/28 07:11:16 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/28 07:11:25 visual_prompt]: Epoch 33 / 100: avg data time: 1.00e-01, avg batch time: 0.5268, average train loss: 0.6588
[09/28 07:11:28 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1799, average loss: 0.5737
[09/28 07:11:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 81.50	top5: 99.00	
[09/28 07:11:51 visual_prompt]: 	Test 100/190. loss: 3.136, 0.2059 s / batch. (data: 3.19e-05)max mem: 7.80552 GB 
[09/28 07:12:10 visual_prompt]: Inference (test):avg data time: 3.40e-05, avg batch time: 0.2048, average loss: 3.0032
[09/28 07:12:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.81	top5: 70.71	
[09/28 07:12:10 visual_prompt]: Best epoch 33: best metric: 0.815
[09/28 07:12:10 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/28 07:12:20 visual_prompt]: Epoch 34 / 100: avg data time: 1.07e-01, avg batch time: 0.5340, average train loss: 0.6130
[09/28 07:12:23 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1800, average loss: 0.6968
[09/28 07:12:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 80.00	top5: 98.00	
[09/28 07:12:45 visual_prompt]: 	Test 100/190. loss: 3.226, 0.2045 s / batch. (data: 2.93e-05)max mem: 7.80552 GB 
[09/28 07:13:05 visual_prompt]: Inference (test):avg data time: 1.13e-04, avg batch time: 0.2050, average loss: 3.0494
[09/28 07:13:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 24.61	top5: 72.57	
[09/28 07:13:05 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/28 07:13:14 visual_prompt]: Epoch 35 / 100: avg data time: 1.00e-01, avg batch time: 0.5282, average train loss: 0.5169
[09/28 07:13:18 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1813, average loss: 0.4256
[09/28 07:13:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 87.00	top5: 100.00	
[09/28 07:13:40 visual_prompt]: 	Test 100/190. loss: 3.149, 0.2046 s / batch. (data: 3.03e-05)max mem: 7.80552 GB 
[09/28 07:14:00 visual_prompt]: Inference (test):avg data time: 3.19e-05, avg batch time: 0.2049, average loss: 3.0403
[09/28 07:14:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.70	top5: 71.70	
[09/28 07:14:00 visual_prompt]: Best epoch 35: best metric: 0.870
[09/28 07:14:00 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/28 07:14:09 visual_prompt]: Epoch 36 / 100: avg data time: 1.01e-01, avg batch time: 0.5279, average train loss: 0.4071
[09/28 07:14:12 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1788, average loss: 0.3732
[09/28 07:14:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 91.50	top5: 99.50	
[09/28 07:14:35 visual_prompt]: 	Test 100/190. loss: 3.150, 0.2044 s / batch. (data: 2.41e-05)max mem: 7.80552 GB 
[09/28 07:14:54 visual_prompt]: Inference (test):avg data time: 3.15e-05, avg batch time: 0.2048, average loss: 3.1238
[09/28 07:14:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 26.81	top5: 71.87	
[09/28 07:14:54 visual_prompt]: Best epoch 36: best metric: 0.915
[09/28 07:14:54 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/28 07:15:04 visual_prompt]: Epoch 37 / 100: avg data time: 1.10e-01, avg batch time: 0.5363, average train loss: 0.3546
[09/28 07:15:07 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1796, average loss: 0.3166
[09/28 07:15:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 91.00	top5: 100.00	
[09/28 07:15:29 visual_prompt]: 	Test 100/190. loss: 3.370, 0.2043 s / batch. (data: 2.67e-05)max mem: 7.80552 GB 
[09/28 07:15:49 visual_prompt]: Inference (test):avg data time: 4.03e-05, avg batch time: 0.2050, average loss: 3.2014
[09/28 07:15:49 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 26.29	top5: 72.41	
[09/28 07:15:49 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/28 07:15:58 visual_prompt]: Epoch 38 / 100: avg data time: 1.03e-01, avg batch time: 0.5284, average train loss: 0.2999
[09/28 07:16:02 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1719, average loss: 0.2795
[09/28 07:16:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 92.50	top5: 100.00	
[09/28 07:16:24 visual_prompt]: 	Test 100/190. loss: 3.181, 0.2046 s / batch. (data: 2.91e-05)max mem: 7.80552 GB 
[09/28 07:16:43 visual_prompt]: Inference (test):avg data time: 3.21e-05, avg batch time: 0.2050, average loss: 3.1234
[09/28 07:16:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 26.27	top5: 73.08	
[09/28 07:16:44 visual_prompt]: Best epoch 38: best metric: 0.925
[09/28 07:16:44 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/28 07:16:53 visual_prompt]: Epoch 39 / 100: avg data time: 9.59e-02, avg batch time: 0.5229, average train loss: 0.2227
[09/28 07:16:56 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1847, average loss: 0.2539
[09/28 07:16:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 94.00	top5: 100.00	
[09/28 07:17:19 visual_prompt]: 	Test 100/190. loss: 3.449, 0.2057 s / batch. (data: 3.08e-05)max mem: 7.80552 GB 
[09/28 07:17:38 visual_prompt]: Inference (test):avg data time: 3.42e-05, avg batch time: 0.2049, average loss: 3.3122
[09/28 07:17:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 26.51	top5: 73.36	
[09/28 07:17:38 visual_prompt]: Best epoch 39: best metric: 0.940
[09/28 07:17:38 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/28 07:17:48 visual_prompt]: Epoch 40 / 100: avg data time: 9.95e-02, avg batch time: 0.5292, average train loss: 0.1796
[09/28 07:17:51 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1806, average loss: 0.1649
[09/28 07:17:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 95.50	top5: 100.00	
[09/28 07:18:13 visual_prompt]: 	Test 100/190. loss: 3.459, 0.2036 s / batch. (data: 2.84e-05)max mem: 7.80552 GB 
[09/28 07:18:33 visual_prompt]: Inference (test):avg data time: 5.76e-05, avg batch time: 0.2049, average loss: 3.2820
[09/28 07:18:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.23	top5: 72.18	
[09/28 07:18:33 visual_prompt]: Best epoch 40: best metric: 0.955
[09/28 07:18:33 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/28 07:18:42 visual_prompt]: Epoch 41 / 100: avg data time: 1.01e-01, avg batch time: 0.5272, average train loss: 0.1428
[09/28 07:18:46 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1809, average loss: 0.1372
[09/28 07:18:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 97.50	top5: 100.00	
[09/28 07:19:08 visual_prompt]: 	Test 100/190. loss: 3.420, 0.2035 s / batch. (data: 2.62e-05)max mem: 7.80552 GB 
[09/28 07:19:28 visual_prompt]: Inference (test):avg data time: 2.04e-04, avg batch time: 0.2053, average loss: 3.3277
[09/28 07:19:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.46	top5: 72.94	
[09/28 07:19:28 visual_prompt]: Best epoch 41: best metric: 0.975
[09/28 07:19:28 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/28 07:19:37 visual_prompt]: Epoch 42 / 100: avg data time: 1.06e-01, avg batch time: 0.5341, average train loss: 0.1091
[09/28 07:19:41 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1797, average loss: 0.0646
[09/28 07:19:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:20:03 visual_prompt]: 	Test 100/190. loss: 3.148, 0.2077 s / batch. (data: 2.50e-05)max mem: 7.80552 GB 
[09/28 07:20:23 visual_prompt]: Inference (test):avg data time: 3.06e-05, avg batch time: 0.2071, average loss: 3.2268
[09/28 07:20:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.32	top5: 72.99	
[09/28 07:20:23 visual_prompt]: Best epoch 42: best metric: 1.000
[09/28 07:20:23 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/28 07:20:32 visual_prompt]: Epoch 43 / 100: avg data time: 1.05e-01, avg batch time: 0.5387, average train loss: 0.0842
[09/28 07:20:36 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1836, average loss: 0.0610
[09/28 07:20:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:20:58 visual_prompt]: 	Test 100/190. loss: 3.304, 0.2091 s / batch. (data: 2.79e-05)max mem: 7.80552 GB 
[09/28 07:21:18 visual_prompt]: Inference (test):avg data time: 3.28e-05, avg batch time: 0.2087, average loss: 3.3134
[09/28 07:21:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.28	top5: 73.38	
[09/28 07:21:18 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/28 07:21:28 visual_prompt]: Epoch 44 / 100: avg data time: 1.06e-01, avg batch time: 0.5378, average train loss: 0.0646
[09/28 07:21:31 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1764, average loss: 0.0439
[09/28 07:21:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:21:53 visual_prompt]: 	Test 100/190. loss: 3.342, 0.2078 s / batch. (data: 7.63e-05)max mem: 7.80552 GB 
[09/28 07:22:13 visual_prompt]: Inference (test):avg data time: 3.12e-05, avg batch time: 0.2078, average loss: 3.2907
[09/28 07:22:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.95	top5: 73.22	
[09/28 07:22:13 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/28 07:22:23 visual_prompt]: Epoch 45 / 100: avg data time: 9.60e-02, avg batch time: 0.5282, average train loss: 0.0633
[09/28 07:22:26 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1857, average loss: 0.0332
[09/28 07:22:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:22:48 visual_prompt]: 	Test 100/190. loss: 3.289, 0.2060 s / batch. (data: 2.67e-05)max mem: 7.80552 GB 
[09/28 07:23:08 visual_prompt]: Inference (test):avg data time: 2.93e-05, avg batch time: 0.2061, average loss: 3.3389
[09/28 07:23:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.26	top5: 73.35	
[09/28 07:23:08 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/28 07:23:17 visual_prompt]: Epoch 46 / 100: avg data time: 9.92e-02, avg batch time: 0.5279, average train loss: 0.0426
[09/28 07:23:21 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1770, average loss: 0.0263
[09/28 07:23:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:23:43 visual_prompt]: 	Test 100/190. loss: 3.502, 0.2048 s / batch. (data: 2.60e-05)max mem: 7.80552 GB 
[09/28 07:24:02 visual_prompt]: Inference (test):avg data time: 2.98e-05, avg batch time: 0.2055, average loss: 3.3332
[09/28 07:24:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.65	top5: 73.51	
[09/28 07:24:02 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/28 07:24:12 visual_prompt]: Epoch 47 / 100: avg data time: 1.01e-01, avg batch time: 0.5276, average train loss: 0.0384
[09/28 07:24:15 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1734, average loss: 0.0231
[09/28 07:24:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:24:37 visual_prompt]: 	Test 100/190. loss: 3.256, 0.2051 s / batch. (data: 2.55e-05)max mem: 7.80552 GB 
[09/28 07:24:57 visual_prompt]: Inference (test):avg data time: 3.16e-05, avg batch time: 0.2049, average loss: 3.3404
[09/28 07:24:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.37	top5: 73.25	
[09/28 07:24:57 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/28 07:25:06 visual_prompt]: Epoch 48 / 100: avg data time: 9.06e-02, avg batch time: 0.5183, average train loss: 0.0307
[09/28 07:25:09 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1815, average loss: 0.0231
[09/28 07:25:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.50	top5: 100.00	
[09/28 07:25:32 visual_prompt]: 	Test 100/190. loss: 3.290, 0.2053 s / batch. (data: 2.72e-05)max mem: 7.80552 GB 
[09/28 07:25:51 visual_prompt]: Inference (test):avg data time: 3.12e-05, avg batch time: 0.2048, average loss: 3.3617
[09/28 07:25:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.42	top5: 73.27	
[09/28 07:25:51 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/28 07:26:01 visual_prompt]: Epoch 49 / 100: avg data time: 9.54e-02, avg batch time: 0.5224, average train loss: 0.0316
[09/28 07:26:04 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1856, average loss: 0.0181
[09/28 07:26:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:26:26 visual_prompt]: 	Test 100/190. loss: 3.357, 0.2053 s / batch. (data: 2.62e-05)max mem: 7.80552 GB 
[09/28 07:26:46 visual_prompt]: Inference (test):avg data time: 3.14e-05, avg batch time: 0.2049, average loss: 3.3532
[09/28 07:26:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.64	top5: 73.12	
[09/28 07:26:46 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/28 07:26:55 visual_prompt]: Epoch 50 / 100: avg data time: 1.04e-01, avg batch time: 0.5322, average train loss: 0.0261
[09/28 07:26:58 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1778, average loss: 0.0213
[09/28 07:26:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:27:21 visual_prompt]: 	Test 100/190. loss: 3.389, 0.2040 s / batch. (data: 2.86e-05)max mem: 7.80552 GB 
[09/28 07:27:40 visual_prompt]: Inference (test):avg data time: 4.57e-05, avg batch time: 0.2052, average loss: 3.3775
[09/28 07:27:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.82	top5: 73.35	
[09/28 07:27:40 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/28 07:27:50 visual_prompt]: Epoch 51 / 100: avg data time: 1.02e-01, avg batch time: 0.5299, average train loss: 0.0243
[09/28 07:27:53 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1791, average loss: 0.0196
[09/28 07:27:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:28:15 visual_prompt]: 	Test 100/190. loss: 3.311, 0.2049 s / batch. (data: 2.65e-05)max mem: 7.80552 GB 
[09/28 07:28:35 visual_prompt]: Inference (test):avg data time: 3.09e-05, avg batch time: 0.2051, average loss: 3.3567
[09/28 07:28:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.81	top5: 73.20	
[09/28 07:28:35 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/28 07:28:44 visual_prompt]: Epoch 52 / 100: avg data time: 9.26e-02, avg batch time: 0.5186, average train loss: 0.0210
[09/28 07:28:47 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1781, average loss: 0.0184
[09/28 07:28:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:29:09 visual_prompt]: 	Test 100/190. loss: 3.232, 0.2053 s / batch. (data: 8.94e-05)max mem: 7.80552 GB 
[09/28 07:29:29 visual_prompt]: Inference (test):avg data time: 3.16e-05, avg batch time: 0.2054, average loss: 3.3428
[09/28 07:29:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.38	top5: 73.46	
[09/28 07:29:29 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/28 07:29:39 visual_prompt]: Epoch 53 / 100: avg data time: 9.66e-02, avg batch time: 0.5244, average train loss: 0.0182
[09/28 07:29:42 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1805, average loss: 0.0139
[09/28 07:29:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:30:04 visual_prompt]: 	Test 100/190. loss: 3.306, 0.2041 s / batch. (data: 7.44e-05)max mem: 7.80552 GB 
[09/28 07:30:24 visual_prompt]: Inference (test):avg data time: 1.27e-04, avg batch time: 0.2053, average loss: 3.3231
[09/28 07:30:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.32	top5: 73.33	
[09/28 07:30:24 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/28 07:30:33 visual_prompt]: Epoch 54 / 100: avg data time: 9.94e-02, avg batch time: 0.5278, average train loss: 0.0162
[09/28 07:30:36 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1842, average loss: 0.0130
[09/28 07:30:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:30:58 visual_prompt]: 	Test 100/190. loss: 3.288, 0.2048 s / batch. (data: 2.67e-05)max mem: 7.80552 GB 
[09/28 07:31:18 visual_prompt]: Inference (test):avg data time: 1.36e-04, avg batch time: 0.2053, average loss: 3.3038
[09/28 07:31:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.26	top5: 73.56	
[09/28 07:31:18 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/28 07:31:28 visual_prompt]: Epoch 55 / 100: avg data time: 1.02e-01, avg batch time: 0.5278, average train loss: 0.0153
[09/28 07:31:31 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1771, average loss: 0.0128
[09/28 07:31:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:31:53 visual_prompt]: 	Test 100/190. loss: 3.351, 0.2047 s / batch. (data: 2.72e-05)max mem: 7.80552 GB 
[09/28 07:32:13 visual_prompt]: Inference (test):avg data time: 2.91e-05, avg batch time: 0.2053, average loss: 3.3327
[09/28 07:32:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.15	top5: 73.36	
[09/28 07:32:13 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/28 07:32:23 visual_prompt]: Epoch 56 / 100: avg data time: 1.10e-01, avg batch time: 0.5389, average train loss: 0.0153
[09/28 07:32:26 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1853, average loss: 0.0110
[09/28 07:32:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:32:48 visual_prompt]: 	Test 100/190. loss: 3.342, 0.2053 s / batch. (data: 2.53e-05)max mem: 7.80552 GB 
[09/28 07:33:08 visual_prompt]: Inference (test):avg data time: 3.09e-05, avg batch time: 0.2050, average loss: 3.3416
[09/28 07:33:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.10	top5: 73.22	
[09/28 07:33:08 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/28 07:33:17 visual_prompt]: Epoch 57 / 100: avg data time: 1.08e-01, avg batch time: 0.5344, average train loss: 0.0161
[09/28 07:33:21 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1724, average loss: 0.0148
[09/28 07:33:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:33:43 visual_prompt]: 	Test 100/190. loss: 3.372, 0.2058 s / batch. (data: 2.43e-05)max mem: 7.80552 GB 
[09/28 07:34:03 visual_prompt]: Inference (test):avg data time: 1.21e-04, avg batch time: 0.2050, average loss: 3.3459
[09/28 07:34:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.56	top5: 72.71	
[09/28 07:34:03 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/28 07:34:12 visual_prompt]: Epoch 58 / 100: avg data time: 1.12e-01, avg batch time: 0.5393, average train loss: 0.0155
[09/28 07:34:16 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1800, average loss: 0.0115
[09/28 07:34:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:34:38 visual_prompt]: 	Test 100/190. loss: 3.313, 0.2056 s / batch. (data: 3.03e-05)max mem: 7.80552 GB 
[09/28 07:34:58 visual_prompt]: Inference (test):avg data time: 2.93e-05, avg batch time: 0.2048, average loss: 3.3284
[09/28 07:34:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.04	top5: 72.95	
[09/28 07:34:58 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/28 07:35:07 visual_prompt]: Epoch 59 / 100: avg data time: 9.07e-02, avg batch time: 0.5180, average train loss: 0.0155
[09/28 07:35:10 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1741, average loss: 0.0117
[09/28 07:35:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:35:32 visual_prompt]: 	Test 100/190. loss: 3.376, 0.2051 s / batch. (data: 2.60e-05)max mem: 7.80552 GB 
[09/28 07:35:52 visual_prompt]: Inference (test):avg data time: 1.34e-04, avg batch time: 0.2051, average loss: 3.3352
[09/28 07:35:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.82	top5: 72.94	
[09/28 07:35:52 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/28 07:36:02 visual_prompt]: Epoch 60 / 100: avg data time: 1.02e-01, avg batch time: 0.5284, average train loss: 0.0136
[09/28 07:36:05 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1773, average loss: 0.0110
[09/28 07:36:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:36:27 visual_prompt]: 	Test 100/190. loss: 3.378, 0.2038 s / batch. (data: 2.62e-05)max mem: 7.80552 GB 
[09/28 07:36:47 visual_prompt]: Inference (test):avg data time: 3.07e-05, avg batch time: 0.2049, average loss: 3.3521
[09/28 07:36:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.12	top5: 72.85	
[09/28 07:36:47 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/28 07:36:56 visual_prompt]: Epoch 61 / 100: avg data time: 1.10e-01, avg batch time: 0.5362, average train loss: 0.0134
[09/28 07:36:59 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1760, average loss: 0.0096
[09/28 07:36:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:37:22 visual_prompt]: 	Test 100/190. loss: 3.359, 0.2043 s / batch. (data: 2.65e-05)max mem: 7.80552 GB 
[09/28 07:37:41 visual_prompt]: Inference (test):avg data time: 3.06e-05, avg batch time: 0.2052, average loss: 3.3479
[09/28 07:37:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.37	top5: 73.07	
[09/28 07:37:41 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/28 07:37:51 visual_prompt]: Epoch 62 / 100: avg data time: 1.09e-01, avg batch time: 0.5357, average train loss: 0.0132
[09/28 07:37:54 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1746, average loss: 0.0103
[09/28 07:37:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:38:16 visual_prompt]: 	Test 100/190. loss: 3.288, 0.2044 s / batch. (data: 2.79e-05)max mem: 7.80552 GB 
[09/28 07:38:36 visual_prompt]: Inference (test):avg data time: 3.37e-05, avg batch time: 0.2051, average loss: 3.3320
[09/28 07:38:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.01	top5: 73.42	
[09/28 07:38:36 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/28 07:38:45 visual_prompt]: Epoch 63 / 100: avg data time: 1.05e-01, avg batch time: 0.5320, average train loss: 0.0127
[09/28 07:38:49 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1777, average loss: 0.0098
[09/28 07:38:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:39:11 visual_prompt]: 	Test 100/190. loss: 3.300, 0.2034 s / batch. (data: 2.81e-05)max mem: 7.80552 GB 
[09/28 07:39:31 visual_prompt]: Inference (test):avg data time: 3.23e-05, avg batch time: 0.2052, average loss: 3.3136
[09/28 07:39:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.13	top5: 73.20	
[09/28 07:39:31 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/28 07:39:40 visual_prompt]: Epoch 64 / 100: avg data time: 1.02e-01, avg batch time: 0.5342, average train loss: 0.0128
[09/28 07:39:43 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1808, average loss: 0.0087
[09/28 07:39:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:40:06 visual_prompt]: 	Test 100/190. loss: 3.313, 0.2044 s / batch. (data: 2.72e-05)max mem: 7.80552 GB 
[09/28 07:40:25 visual_prompt]: Inference (test):avg data time: 2.96e-05, avg batch time: 0.2050, average loss: 3.3282
[09/28 07:40:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.46	top5: 72.86	
[09/28 07:40:25 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/28 07:40:35 visual_prompt]: Epoch 65 / 100: avg data time: 9.86e-02, avg batch time: 0.5260, average train loss: 0.0125
[09/28 07:40:38 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1827, average loss: 0.0107
[09/28 07:40:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:41:00 visual_prompt]: 	Test 100/190. loss: 3.369, 0.2046 s / batch. (data: 2.77e-05)max mem: 7.80552 GB 
[09/28 07:41:20 visual_prompt]: Inference (test):avg data time: 3.47e-05, avg batch time: 0.2050, average loss: 3.3135
[09/28 07:41:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.09	top5: 73.16	
[09/28 07:41:20 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/28 07:41:29 visual_prompt]: Epoch 66 / 100: avg data time: 1.10e-01, avg batch time: 0.5368, average train loss: 0.0117
[09/28 07:41:33 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1813, average loss: 0.0100
[09/28 07:41:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:41:55 visual_prompt]: 	Test 100/190. loss: 3.378, 0.2037 s / batch. (data: 7.49e-05)max mem: 7.80552 GB 
[09/28 07:42:14 visual_prompt]: Inference (test):avg data time: 3.04e-05, avg batch time: 0.2047, average loss: 3.3366
[09/28 07:42:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.58	top5: 73.04	
[09/28 07:42:14 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/28 07:42:24 visual_prompt]: Epoch 67 / 100: avg data time: 1.05e-01, avg batch time: 0.5334, average train loss: 0.0113
[09/28 07:42:27 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1837, average loss: 0.0094
[09/28 07:42:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:42:49 visual_prompt]: 	Test 100/190. loss: 3.393, 0.2044 s / batch. (data: 2.69e-05)max mem: 7.80552 GB 
[09/28 07:43:09 visual_prompt]: Inference (test):avg data time: 3.03e-05, avg batch time: 0.2049, average loss: 3.3321
[09/28 07:43:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.35	top5: 72.87	
[09/28 07:43:09 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/28 07:43:18 visual_prompt]: Epoch 68 / 100: avg data time: 1.06e-01, avg batch time: 0.5352, average train loss: 0.0111
[09/28 07:43:21 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1833, average loss: 0.0088
[09/28 07:43:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:43:44 visual_prompt]: 	Test 100/190. loss: 3.317, 0.2058 s / batch. (data: 2.62e-05)max mem: 7.80552 GB 
[09/28 07:44:03 visual_prompt]: Inference (test):avg data time: 2.95e-05, avg batch time: 0.2048, average loss: 3.3255
[09/28 07:44:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.51	top5: 72.86	
[09/28 07:44:03 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/28 07:44:13 visual_prompt]: Epoch 69 / 100: avg data time: 1.04e-01, avg batch time: 0.5312, average train loss: 0.0116
[09/28 07:44:16 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1778, average loss: 0.0079
[09/28 07:44:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:44:38 visual_prompt]: 	Test 100/190. loss: 3.363, 0.2035 s / batch. (data: 2.98e-05)max mem: 7.80552 GB 
[09/28 07:44:58 visual_prompt]: Inference (test):avg data time: 3.13e-05, avg batch time: 0.2049, average loss: 3.3170
[09/28 07:44:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.53	top5: 72.95	
[09/28 07:44:58 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/28 07:45:07 visual_prompt]: Epoch 70 / 100: avg data time: 1.01e-01, avg batch time: 0.5309, average train loss: 0.0110
[09/28 07:45:11 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1843, average loss: 0.0080
[09/28 07:45:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:45:33 visual_prompt]: 	Test 100/190. loss: 3.357, 0.2050 s / batch. (data: 2.67e-05)max mem: 7.80552 GB 
[09/28 07:45:52 visual_prompt]: Inference (test):avg data time: 2.91e-05, avg batch time: 0.2050, average loss: 3.3413
[09/28 07:45:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.21	top5: 72.66	
[09/28 07:45:52 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/28 07:46:02 visual_prompt]: Epoch 71 / 100: avg data time: 1.04e-01, avg batch time: 0.5308, average train loss: 0.0108
[09/28 07:46:05 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1724, average loss: 0.0078
[09/28 07:46:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:46:27 visual_prompt]: 	Test 100/190. loss: 3.354, 0.2051 s / batch. (data: 2.79e-05)max mem: 7.80552 GB 
[09/28 07:46:47 visual_prompt]: Inference (test):avg data time: 2.97e-05, avg batch time: 0.2051, average loss: 3.3445
[09/28 07:46:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.16	top5: 72.62	
[09/28 07:46:47 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/28 07:46:56 visual_prompt]: Epoch 72 / 100: avg data time: 9.71e-02, avg batch time: 0.5259, average train loss: 0.0104
[09/28 07:46:59 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1755, average loss: 0.0079
[09/28 07:46:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:47:22 visual_prompt]: 	Test 100/190. loss: 3.318, 0.2043 s / batch. (data: 2.77e-05)max mem: 7.80552 GB 
[09/28 07:47:41 visual_prompt]: Inference (test):avg data time: 3.06e-05, avg batch time: 0.2054, average loss: 3.3264
[09/28 07:47:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.11	top5: 72.99	
[09/28 07:47:41 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/28 07:47:51 visual_prompt]: Epoch 73 / 100: avg data time: 9.01e-02, avg batch time: 0.5194, average train loss: 0.0102
[09/28 07:47:54 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1753, average loss: 0.0076
[09/28 07:47:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:48:16 visual_prompt]: 	Test 100/190. loss: 3.297, 0.2035 s / batch. (data: 2.67e-05)max mem: 7.80552 GB 
[09/28 07:48:36 visual_prompt]: Inference (test):avg data time: 3.12e-05, avg batch time: 0.2051, average loss: 3.3548
[09/28 07:48:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.13	top5: 72.67	
[09/28 07:48:36 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/28 07:48:45 visual_prompt]: Epoch 74 / 100: avg data time: 9.59e-02, avg batch time: 0.5247, average train loss: 0.0104
[09/28 07:48:49 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1744, average loss: 0.0075
[09/28 07:48:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:49:11 visual_prompt]: 	Test 100/190. loss: 3.321, 0.2061 s / batch. (data: 2.62e-05)max mem: 7.80552 GB 
[09/28 07:49:31 visual_prompt]: Inference (test):avg data time: 3.01e-05, avg batch time: 0.2050, average loss: 3.3369
[09/28 07:49:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.07	top5: 72.61	
[09/28 07:49:31 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/28 07:49:40 visual_prompt]: Epoch 75 / 100: avg data time: 1.03e-01, avg batch time: 0.5311, average train loss: 0.0104
[09/28 07:49:43 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1823, average loss: 0.0077
[09/28 07:49:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:50:05 visual_prompt]: 	Test 100/190. loss: 3.366, 0.2044 s / batch. (data: 2.93e-05)max mem: 7.80552 GB 
[09/28 07:50:25 visual_prompt]: Inference (test):avg data time: 1.30e-04, avg batch time: 0.2050, average loss: 3.3334
[09/28 07:50:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.16	top5: 72.56	
[09/28 07:50:25 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/28 07:50:34 visual_prompt]: Epoch 76 / 100: avg data time: 1.01e-01, avg batch time: 0.5286, average train loss: 0.0115
[09/28 07:50:38 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1787, average loss: 0.0472
[09/28 07:50:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.00	top5: 100.00	
[09/28 07:51:00 visual_prompt]: 	Test 100/190. loss: 3.400, 0.2059 s / batch. (data: 2.69e-05)max mem: 7.80552 GB 
[09/28 07:51:19 visual_prompt]: Inference (test):avg data time: 1.39e-04, avg batch time: 0.2051, average loss: 3.3642
[09/28 07:51:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.65	top5: 72.55	
[09/28 07:51:19 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/28 07:51:29 visual_prompt]: Epoch 77 / 100: avg data time: 1.10e-01, avg batch time: 0.5352, average train loss: 0.0207
[09/28 07:51:32 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1786, average loss: 0.0243
[09/28 07:51:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.50	top5: 100.00	
[09/28 07:51:54 visual_prompt]: 	Test 100/190. loss: 3.298, 0.2050 s / batch. (data: 2.77e-05)max mem: 7.80552 GB 
[09/28 07:52:14 visual_prompt]: Inference (test):avg data time: 3.04e-05, avg batch time: 0.2050, average loss: 3.3593
[09/28 07:52:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.27	top5: 71.65	
[09/28 07:52:14 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/28 07:52:23 visual_prompt]: Epoch 78 / 100: avg data time: 1.02e-01, avg batch time: 0.5287, average train loss: 0.0178
[09/28 07:52:26 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1754, average loss: 0.0125
[09/28 07:52:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:52:49 visual_prompt]: 	Test 100/190. loss: 3.366, 0.2052 s / batch. (data: 2.84e-05)max mem: 7.80552 GB 
[09/28 07:53:08 visual_prompt]: Inference (test):avg data time: 3.03e-05, avg batch time: 0.2050, average loss: 3.3333
[09/28 07:53:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.61	top5: 72.52	
[09/28 07:53:08 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/28 07:53:18 visual_prompt]: Epoch 79 / 100: avg data time: 1.01e-01, avg batch time: 0.5299, average train loss: 0.0155
[09/28 07:53:21 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1820, average loss: 0.0111
[09/28 07:53:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:53:43 visual_prompt]: 	Test 100/190. loss: 3.322, 0.2067 s / batch. (data: 2.67e-05)max mem: 7.80552 GB 
[09/28 07:54:03 visual_prompt]: Inference (test):avg data time: 5.08e-05, avg batch time: 0.2050, average loss: 3.3463
[09/28 07:54:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.42	top5: 71.63	
[09/28 07:54:03 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/28 07:54:12 visual_prompt]: Epoch 80 / 100: avg data time: 9.78e-02, avg batch time: 0.5241, average train loss: 0.0124
[09/28 07:54:15 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1740, average loss: 0.0098
[09/28 07:54:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:54:38 visual_prompt]: 	Test 100/190. loss: 3.350, 0.2041 s / batch. (data: 7.30e-05)max mem: 7.80552 GB 
[09/28 07:54:57 visual_prompt]: Inference (test):avg data time: 3.19e-05, avg batch time: 0.2050, average loss: 3.3301
[09/28 07:54:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.78	top5: 72.14	
[09/28 07:54:57 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/28 07:55:07 visual_prompt]: Epoch 81 / 100: avg data time: 1.03e-01, avg batch time: 0.5319, average train loss: 0.0118
[09/28 07:55:10 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1795, average loss: 0.0088
[09/28 07:55:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:55:32 visual_prompt]: 	Test 100/190. loss: 3.327, 0.2042 s / batch. (data: 2.62e-05)max mem: 7.80552 GB 
[09/28 07:55:52 visual_prompt]: Inference (test):avg data time: 3.16e-05, avg batch time: 0.2051, average loss: 3.3117
[09/28 07:55:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.16	top5: 72.61	
[09/28 07:55:52 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/28 07:56:01 visual_prompt]: Epoch 82 / 100: avg data time: 1.01e-01, avg batch time: 0.5264, average train loss: 0.0106
[09/28 07:56:05 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1841, average loss: 0.0080
[09/28 07:56:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:56:27 visual_prompt]: 	Test 100/190. loss: 3.327, 0.2041 s / batch. (data: 2.98e-05)max mem: 7.80552 GB 
[09/28 07:56:46 visual_prompt]: Inference (test):avg data time: 3.05e-05, avg batch time: 0.2049, average loss: 3.3171
[09/28 07:56:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.10	top5: 72.56	
[09/28 07:56:46 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/28 07:56:56 visual_prompt]: Epoch 83 / 100: avg data time: 9.86e-02, avg batch time: 0.5254, average train loss: 0.0101
[09/28 07:56:59 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1746, average loss: 0.0076
[09/28 07:56:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:57:21 visual_prompt]: 	Test 100/190. loss: 3.346, 0.2046 s / batch. (data: 2.62e-05)max mem: 7.80552 GB 
[09/28 07:57:41 visual_prompt]: Inference (test):avg data time: 2.97e-05, avg batch time: 0.2049, average loss: 3.3273
[09/28 07:57:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.28	top5: 72.58	
[09/28 07:57:41 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/28 07:57:50 visual_prompt]: Epoch 84 / 100: avg data time: 1.01e-01, avg batch time: 0.5264, average train loss: 0.0101
[09/28 07:57:54 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1792, average loss: 0.0077
[09/28 07:57:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:58:16 visual_prompt]: 	Test 100/190. loss: 3.358, 0.2058 s / batch. (data: 2.74e-05)max mem: 7.80552 GB 
[09/28 07:58:35 visual_prompt]: Inference (test):avg data time: 3.14e-05, avg batch time: 0.2049, average loss: 3.3298
[09/28 07:58:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.35	top5: 72.53	
[09/28 07:58:35 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/28 07:58:45 visual_prompt]: Epoch 85 / 100: avg data time: 9.93e-02, avg batch time: 0.5266, average train loss: 0.0103
[09/28 07:58:48 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1792, average loss: 0.0083
[09/28 07:58:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 07:59:10 visual_prompt]: 	Test 100/190. loss: 3.403, 0.2051 s / batch. (data: 2.84e-05)max mem: 7.80552 GB 
[09/28 07:59:30 visual_prompt]: Inference (test):avg data time: 3.09e-05, avg batch time: 0.2047, average loss: 3.3213
[09/28 07:59:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.29	top5: 72.63	
[09/28 07:59:30 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/28 07:59:39 visual_prompt]: Epoch 86 / 100: avg data time: 1.01e-01, avg batch time: 0.5304, average train loss: 0.0103
[09/28 07:59:42 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1777, average loss: 0.0083
[09/28 07:59:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 08:00:05 visual_prompt]: 	Test 100/190. loss: 3.381, 0.2033 s / batch. (data: 3.22e-05)max mem: 7.80552 GB 
[09/28 08:00:24 visual_prompt]: Inference (test):avg data time: 2.84e-05, avg batch time: 0.2045, average loss: 3.3229
[09/28 08:00:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.32	top5: 72.64	
[09/28 08:00:24 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/28 08:00:34 visual_prompt]: Epoch 87 / 100: avg data time: 1.07e-01, avg batch time: 0.5333, average train loss: 0.0100
[09/28 08:00:37 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.1770, average loss: 0.0079
[09/28 08:00:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 08:00:59 visual_prompt]: 	Test 100/190. loss: 3.364, 0.2042 s / batch. (data: 2.86e-05)max mem: 7.80552 GB 
[09/28 08:01:19 visual_prompt]: Inference (test):avg data time: 2.92e-05, avg batch time: 0.2047, average loss: 3.3280
[09/28 08:01:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.52	top5: 72.63	
[09/28 08:01:19 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/28 08:01:28 visual_prompt]: Epoch 88 / 100: avg data time: 9.91e-02, avg batch time: 0.5245, average train loss: 0.0101
[09/28 08:01:32 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1785, average loss: 0.0078
[09/28 08:01:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 08:01:54 visual_prompt]: 	Test 100/190. loss: 3.362, 0.2036 s / batch. (data: 2.67e-05)max mem: 7.80552 GB 
[09/28 08:02:13 visual_prompt]: Inference (test):avg data time: 3.02e-05, avg batch time: 0.2049, average loss: 3.3294
[09/28 08:02:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.43	top5: 72.63	
[09/28 08:02:13 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/28 08:02:23 visual_prompt]: Epoch 89 / 100: avg data time: 9.78e-02, avg batch time: 0.5250, average train loss: 0.0096
[09/28 08:02:26 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1815, average loss: 0.0077
[09/28 08:02:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 08:02:48 visual_prompt]: 	Test 100/190. loss: 3.371, 0.2042 s / batch. (data: 2.74e-05)max mem: 7.80552 GB 
[09/28 08:03:08 visual_prompt]: Inference (test):avg data time: 2.86e-05, avg batch time: 0.2050, average loss: 3.3291
[09/28 08:03:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.41	top5: 72.58	
[09/28 08:03:08 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/28 08:03:17 visual_prompt]: Epoch 90 / 100: avg data time: 1.03e-01, avg batch time: 0.5315, average train loss: 0.0095
[09/28 08:03:21 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1788, average loss: 0.0077
[09/28 08:03:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 08:03:43 visual_prompt]: 	Test 100/190. loss: 3.372, 0.2037 s / batch. (data: 2.65e-05)max mem: 7.80552 GB 
[09/28 08:04:02 visual_prompt]: Inference (test):avg data time: 2.91e-05, avg batch time: 0.2050, average loss: 3.3255
[09/28 08:04:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.44	top5: 72.54	
[09/28 08:04:02 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/28 08:04:12 visual_prompt]: Epoch 91 / 100: avg data time: 1.10e-01, avg batch time: 0.5374, average train loss: 0.0089
[09/28 08:04:15 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1842, average loss: 0.0076
[09/28 08:04:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 08:04:38 visual_prompt]: 	Test 100/190. loss: 3.377, 0.2046 s / batch. (data: 2.74e-05)max mem: 7.80552 GB 
[09/28 08:04:57 visual_prompt]: Inference (test):avg data time: 2.95e-05, avg batch time: 0.2051, average loss: 3.3269
[09/28 08:04:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.46	top5: 72.51	
[09/28 08:04:57 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/28 08:05:07 visual_prompt]: Epoch 92 / 100: avg data time: 1.06e-01, avg batch time: 0.5346, average train loss: 0.0093
[09/28 08:05:10 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1723, average loss: 0.0075
[09/28 08:05:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 08:05:32 visual_prompt]: 	Test 100/190. loss: 3.379, 0.2051 s / batch. (data: 2.60e-05)max mem: 7.80552 GB 
[09/28 08:05:52 visual_prompt]: Inference (test):avg data time: 3.08e-05, avg batch time: 0.2050, average loss: 3.3274
[09/28 08:05:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.50	top5: 72.58	
[09/28 08:05:52 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/28 08:06:02 visual_prompt]: Epoch 93 / 100: avg data time: 1.11e-01, avg batch time: 0.5365, average train loss: 0.0093
[09/28 08:06:05 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.1853, average loss: 0.0074
[09/28 08:06:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 08:06:27 visual_prompt]: 	Test 100/190. loss: 3.378, 0.2044 s / batch. (data: 2.84e-05)max mem: 7.80552 GB 
[09/28 08:06:47 visual_prompt]: Inference (test):avg data time: 3.15e-05, avg batch time: 0.2049, average loss: 3.3265
[09/28 08:06:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.50	top5: 72.58	
[09/28 08:06:47 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/28 08:06:56 visual_prompt]: Epoch 94 / 100: avg data time: 1.06e-01, avg batch time: 0.5332, average train loss: 0.0092
[09/28 08:06:59 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1793, average loss: 0.0074
[09/28 08:06:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 08:07:22 visual_prompt]: 	Test 100/190. loss: 3.376, 0.2059 s / batch. (data: 2.67e-05)max mem: 7.80552 GB 
[09/28 08:07:41 visual_prompt]: Inference (test):avg data time: 3.14e-05, avg batch time: 0.2049, average loss: 3.3264
[09/28 08:07:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.56	top5: 72.61	
[09/28 08:07:41 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/28 08:07:51 visual_prompt]: Epoch 95 / 100: avg data time: 1.01e-01, avg batch time: 0.5260, average train loss: 0.0094
[09/28 08:07:54 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1796, average loss: 0.0074
[09/28 08:07:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 08:08:16 visual_prompt]: 	Test 100/190. loss: 3.371, 0.2057 s / batch. (data: 2.60e-05)max mem: 7.80552 GB 
[09/28 08:08:35 visual_prompt]: Inference (test):avg data time: 2.96e-05, avg batch time: 0.2051, average loss: 3.3265
[09/28 08:08:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.49	top5: 72.61	
[09/28 08:08:35 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/28 08:08:45 visual_prompt]: Epoch 96 / 100: avg data time: 1.03e-01, avg batch time: 0.5290, average train loss: 0.0091
[09/28 08:08:48 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1733, average loss: 0.0074
[09/28 08:08:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 08:09:10 visual_prompt]: 	Test 100/190. loss: 3.370, 0.2044 s / batch. (data: 9.16e-05)max mem: 7.80552 GB 
[09/28 08:09:30 visual_prompt]: Inference (test):avg data time: 3.07e-05, avg batch time: 0.2048, average loss: 3.3266
[09/28 08:09:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.48	top5: 72.57	
[09/28 08:09:30 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/28 08:09:40 visual_prompt]: Epoch 97 / 100: avg data time: 1.10e-01, avg batch time: 0.5357, average train loss: 0.0093
[09/28 08:09:43 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1826, average loss: 0.0074
[09/28 08:09:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 08:10:05 visual_prompt]: 	Test 100/190. loss: 3.369, 0.2042 s / batch. (data: 2.74e-05)max mem: 7.80552 GB 
[09/28 08:10:25 visual_prompt]: Inference (test):avg data time: 2.93e-05, avg batch time: 0.2047, average loss: 3.3265
[09/28 08:10:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.49	top5: 72.58	
[09/28 08:10:25 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/28 08:10:34 visual_prompt]: Epoch 98 / 100: avg data time: 1.08e-01, avg batch time: 0.5356, average train loss: 0.0093
[09/28 08:10:38 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.1778, average loss: 0.0074
[09/28 08:10:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 08:11:00 visual_prompt]: 	Test 100/190. loss: 3.367, 0.2054 s / batch. (data: 2.62e-05)max mem: 7.80552 GB 
[09/28 08:11:19 visual_prompt]: Inference (test):avg data time: 3.05e-05, avg batch time: 0.2048, average loss: 3.3265
[09/28 08:11:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.49	top5: 72.59	
[09/28 08:11:19 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/28 08:11:29 visual_prompt]: Epoch 99 / 100: avg data time: 1.04e-01, avg batch time: 0.5308, average train loss: 0.0091
[09/28 08:11:32 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1746, average loss: 0.0074
[09/28 08:11:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 08:11:54 visual_prompt]: 	Test 100/190. loss: 3.367, 0.2043 s / batch. (data: 2.72e-05)max mem: 7.80552 GB 
[09/28 08:12:14 visual_prompt]: Inference (test):avg data time: 3.14e-05, avg batch time: 0.2046, average loss: 3.3264
[09/28 08:12:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.49	top5: 72.58	
[09/28 08:12:14 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/28 08:12:24 visual_prompt]: Epoch 100 / 100: avg data time: 1.10e-01, avg batch time: 0.5351, average train loss: 0.0092
[09/28 08:12:27 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1761, average loss: 0.0074
[09/28 08:12:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 08:12:49 visual_prompt]: 	Test 100/190. loss: 3.367, 0.2039 s / batch. (data: 2.46e-05)max mem: 7.80552 GB 
[09/28 08:13:08 visual_prompt]: Inference (test):avg data time: 3.19e-05, avg batch time: 0.2049, average loss: 3.3264
[09/28 08:13:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.49	top5: 72.58	
[09/28 08:13:09 visual_prompt]: Rank of current process: 0. World size: 1
[09/28 08:13:09 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/28 08:13:09 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_azimuth")', 'DATA.NUMBER_CLASSES', '18', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/28 08:13:09 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/28 08:13:09 visual_prompt]: Training with config:
[09/28 08:13:09 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_azimuth")/sup_vitb16_imagenet21k/prompt50/crop224/test/seed8364/lr0.25_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 8364, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_azimuth")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 18, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/28 08:13:09 visual_prompt]: Loading training data...
[09/28 08:13:09 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset trainval...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800]+test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/28 08:13:10 visual_prompt]: Number of images: 1000
[09/28 08:13:10 visual_prompt]: Number of classes: 18 / 18
[09/28 08:13:10 visual_prompt]: Loading validation data...
[09/28 08:13:10 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/28 08:13:10 visual_prompt]: Number of images: 200
[09/28 08:13:10 visual_prompt]: Number of classes: 18 / 18
[09/28 08:13:10 visual_prompt]: Loading test data...
[09/28 08:13:10 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset test...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[50%:], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/28 08:13:27 visual_prompt]: Number of images: 12150
[09/28 08:13:27 visual_prompt]: Number of classes: 18 / 18
[09/28 08:13:27 visual_prompt]: Constructing models...
[09/28 08:13:29 visual_prompt]: Total Parameters: 86273298	 Gradient Parameters: 474642
[09/28 08:13:29 visual_prompt]: tuned percent:0.550
[09/28 08:13:29 visual_prompt]: Device used for model: 0
[09/28 08:13:29 visual_prompt]: Setting up Evaluator...
[09/28 08:13:29 visual_prompt]: Setting up Trainer...
[09/28 08:13:29 visual_prompt]: 	Setting up the optimizer...
[09/28 08:13:29 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/28 08:13:39 visual_prompt]: Epoch 1 / 100: avg data time: 1.02e-01, avg batch time: 0.5247, average train loss: 3.0634
[09/28 08:13:42 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1741, average loss: 3.0647
[09/28 08:13:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 28.50	
[09/28 08:14:04 visual_prompt]: 	Test 100/190. loss: 3.117, 0.2040 s / batch. (data: 2.72e-05)max mem: 7.81168 GB 
[09/28 08:14:24 visual_prompt]: Inference (test):avg data time: 2.90e-05, avg batch time: 0.2040, average loss: 3.0763
[09/28 08:14:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.82	top5: 28.07	
[09/28 08:14:24 visual_prompt]: Best epoch 1: best metric: 0.055
[09/28 08:14:24 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/28 08:14:33 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e-01, avg batch time: 0.5299, average train loss: 3.0237
[09/28 08:14:36 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1595, average loss: 2.9236
[09/28 08:14:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 30.50	
[09/28 08:14:59 visual_prompt]: 	Test 100/190. loss: 2.862, 0.2042 s / batch. (data: 2.67e-05)max mem: 7.81234 GB 
[09/28 08:15:18 visual_prompt]: Inference (test):avg data time: 1.29e-04, avg batch time: 0.2053, average loss: 2.9301
[09/28 08:15:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 27.41	
[09/28 08:15:18 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/28 08:15:28 visual_prompt]: Epoch 3 / 100: avg data time: 1.01e-01, avg batch time: 0.5320, average train loss: 2.9170
[09/28 08:15:31 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1853, average loss: 2.8988
[09/28 08:15:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 28.50	
[09/28 08:15:53 visual_prompt]: 	Test 100/190. loss: 2.939, 0.2049 s / batch. (data: 2.81e-05)max mem: 7.81234 GB 
[09/28 08:16:13 visual_prompt]: Inference (test):avg data time: 1.25e-04, avg batch time: 0.2052, average loss: 2.9170
[09/28 08:16:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.82	top5: 27.44	
[09/28 08:16:13 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/28 08:16:22 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e-01, avg batch time: 0.5286, average train loss: 2.9060
[09/28 08:16:25 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1767, average loss: 2.8991
[09/28 08:16:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 30.00	
[09/28 08:16:48 visual_prompt]: 	Test 100/190. loss: 2.873, 0.2057 s / batch. (data: 2.50e-05)max mem: 7.81234 GB 
[09/28 08:17:07 visual_prompt]: Inference (test):avg data time: 8.68e-05, avg batch time: 0.2052, average loss: 2.8976
[09/28 08:17:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.67	top5: 28.40	
[09/28 08:17:07 visual_prompt]: Best epoch 4: best metric: 0.090
[09/28 08:17:07 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/28 08:17:17 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e-01, avg batch time: 0.5315, average train loss: 2.9004
[09/28 08:17:20 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1825, average loss: 2.8767
[09/28 08:17:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.50	top5: 35.00	
[09/28 08:17:42 visual_prompt]: 	Test 100/190. loss: 2.929, 0.2036 s / batch. (data: 2.67e-05)max mem: 7.81234 GB 
[09/28 08:18:02 visual_prompt]: Inference (test):avg data time: 2.87e-05, avg batch time: 0.2054, average loss: 2.9089
[09/28 08:18:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.49	top5: 30.21	
[09/28 08:18:02 visual_prompt]: Best epoch 5: best metric: 0.095
[09/28 08:18:02 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/28 08:18:11 visual_prompt]: Epoch 6 / 100: avg data time: 9.53e-02, avg batch time: 0.5227, average train loss: 2.9024
[09/28 08:18:14 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1726, average loss: 2.8910
[09/28 08:18:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 27.50	
[09/28 08:18:36 visual_prompt]: 	Test 100/190. loss: 2.894, 0.2055 s / batch. (data: 2.31e-05)max mem: 7.81234 GB 
[09/28 08:18:56 visual_prompt]: Inference (test):avg data time: 2.40e-04, avg batch time: 0.2052, average loss: 2.9068
[09/28 08:18:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.91	
[09/28 08:18:56 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/28 08:19:06 visual_prompt]: Epoch 7 / 100: avg data time: 1.01e-01, avg batch time: 0.5291, average train loss: 2.9015
[09/28 08:19:09 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1783, average loss: 2.8795
[09/28 08:19:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 32.00	
[09/28 08:19:31 visual_prompt]: 	Test 100/190. loss: 2.874, 0.2045 s / batch. (data: 2.86e-05)max mem: 7.81234 GB 
[09/28 08:19:51 visual_prompt]: Inference (test):avg data time: 7.19e-05, avg batch time: 0.2051, average loss: 2.8908
[09/28 08:19:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.99	top5: 30.58	
[09/28 08:19:51 visual_prompt]: Best epoch 7: best metric: 0.100
[09/28 08:19:51 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/28 08:20:00 visual_prompt]: Epoch 8 / 100: avg data time: 9.73e-02, avg batch time: 0.5255, average train loss: 2.9146
[09/28 08:20:03 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1806, average loss: 2.8658
[09/28 08:20:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 36.50	
[09/28 08:20:26 visual_prompt]: 	Test 100/190. loss: 2.894, 0.2032 s / batch. (data: 2.88e-05)max mem: 7.81234 GB 
[09/28 08:20:45 visual_prompt]: Inference (test):avg data time: 3.05e-05, avg batch time: 0.2050, average loss: 2.9245
[09/28 08:20:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.95	top5: 29.62	
[09/28 08:20:45 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/28 08:20:55 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e-01, avg batch time: 0.5311, average train loss: 2.9078
[09/28 08:20:58 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1859, average loss: 2.8822
[09/28 08:20:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 30.50	
[09/28 08:21:20 visual_prompt]: 	Test 100/190. loss: 2.909, 0.2059 s / batch. (data: 2.74e-05)max mem: 7.81234 GB 
[09/28 08:21:40 visual_prompt]: Inference (test):avg data time: 2.90e-05, avg batch time: 0.2047, average loss: 2.9126
[09/28 08:21:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.88	top5: 30.60	
[09/28 08:21:40 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/28 08:21:49 visual_prompt]: Epoch 10 / 100: avg data time: 1.06e-01, avg batch time: 0.5308, average train loss: 2.9154
[09/28 08:21:53 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1754, average loss: 2.9316
[09/28 08:21:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 33.00	
[09/28 08:22:15 visual_prompt]: 	Test 100/190. loss: 2.928, 0.2044 s / batch. (data: 2.57e-05)max mem: 7.81234 GB 
[09/28 08:22:34 visual_prompt]: Inference (test):avg data time: 2.91e-05, avg batch time: 0.2047, average loss: 2.9273
[09/28 08:22:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.49	top5: 29.99	
[09/28 08:22:34 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/28 08:22:44 visual_prompt]: Epoch 11 / 100: avg data time: 9.05e-02, avg batch time: 0.5177, average train loss: 2.9055
[09/28 08:22:47 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1832, average loss: 2.8809
[09/28 08:22:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 36.00	
[09/28 08:23:09 visual_prompt]: 	Test 100/190. loss: 2.895, 0.2060 s / batch. (data: 2.74e-05)max mem: 7.81234 GB 
[09/28 08:23:29 visual_prompt]: Inference (test):avg data time: 2.94e-05, avg batch time: 0.2050, average loss: 2.9191
[09/28 08:23:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.46	top5: 32.12	
[09/28 08:23:29 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/28 08:23:38 visual_prompt]: Epoch 12 / 100: avg data time: 9.46e-02, avg batch time: 0.5227, average train loss: 2.8680
[09/28 08:23:41 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1838, average loss: 2.7993
[09/28 08:23:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.00	top5: 40.50	
[09/28 08:24:03 visual_prompt]: 	Test 100/190. loss: 2.873, 0.2042 s / batch. (data: 2.48e-05)max mem: 7.81234 GB 
[09/28 08:24:23 visual_prompt]: Inference (test):avg data time: 2.85e-05, avg batch time: 0.2050, average loss: 2.8706
[09/28 08:24:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.88	top5: 35.28	
[09/28 08:24:23 visual_prompt]: Best epoch 12: best metric: 0.120
[09/28 08:24:23 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/28 08:24:33 visual_prompt]: Epoch 13 / 100: avg data time: 9.73e-02, avg batch time: 0.5280, average train loss: 2.8210
[09/28 08:24:36 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1762, average loss: 2.7767
[09/28 08:24:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.00	top5: 42.00	
[09/28 08:24:58 visual_prompt]: 	Test 100/190. loss: 2.719, 0.2053 s / batch. (data: 2.72e-05)max mem: 7.81234 GB 
[09/28 08:25:18 visual_prompt]: Inference (test):avg data time: 2.92e-05, avg batch time: 0.2048, average loss: 2.8721
[09/28 08:25:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.73	top5: 37.21	
[09/28 08:25:18 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/28 08:25:27 visual_prompt]: Epoch 14 / 100: avg data time: 1.12e-01, avg batch time: 0.5373, average train loss: 2.8172
[09/28 08:25:30 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1790, average loss: 2.7113
[09/28 08:25:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.50	top5: 44.00	
[09/28 08:25:53 visual_prompt]: 	Test 100/190. loss: 2.764, 0.2037 s / batch. (data: 2.84e-05)max mem: 7.81234 GB 
[09/28 08:26:12 visual_prompt]: Inference (test):avg data time: 2.78e-05, avg batch time: 0.2048, average loss: 2.7902
[09/28 08:26:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.11	top5: 38.35	
[09/28 08:26:12 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/28 08:26:22 visual_prompt]: Epoch 15 / 100: avg data time: 9.66e-02, avg batch time: 0.5224, average train loss: 2.7230
[09/28 08:26:25 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1814, average loss: 2.7140
[09/28 08:26:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 45.50	
[09/28 08:26:47 visual_prompt]: 	Test 100/190. loss: 2.759, 0.2042 s / batch. (data: 3.24e-05)max mem: 7.81234 GB 
[09/28 08:27:07 visual_prompt]: Inference (test):avg data time: 1.55e-04, avg batch time: 0.2049, average loss: 2.8159
[09/28 08:27:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.49	top5: 42.29	
[09/28 08:27:07 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/28 08:27:16 visual_prompt]: Epoch 16 / 100: avg data time: 8.94e-02, avg batch time: 0.5171, average train loss: 2.7849
[09/28 08:27:19 visual_prompt]: Inference (val):avg data time: 1.92e-05, avg batch time: 0.1750, average loss: 2.7674
[09/28 08:27:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.00	top5: 46.50	
[09/28 08:27:41 visual_prompt]: 	Test 100/190. loss: 2.749, 0.2042 s / batch. (data: 2.77e-05)max mem: 7.81234 GB 
[09/28 08:28:01 visual_prompt]: Inference (test):avg data time: 7.92e-05, avg batch time: 0.2048, average loss: 2.8773
[09/28 08:28:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.77	top5: 36.94	
[09/28 08:28:01 visual_prompt]: Best epoch 16: best metric: 0.130
[09/28 08:28:01 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/28 08:28:10 visual_prompt]: Epoch 17 / 100: avg data time: 1.07e-01, avg batch time: 0.5326, average train loss: 2.6668
[09/28 08:28:14 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1820, average loss: 2.6249
[09/28 08:28:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.00	top5: 52.50	
[09/28 08:28:36 visual_prompt]: 	Test 100/190. loss: 2.631, 0.2043 s / batch. (data: 2.50e-05)max mem: 7.81234 GB 
[09/28 08:28:55 visual_prompt]: Inference (test):avg data time: 1.10e-04, avg batch time: 0.2052, average loss: 2.7558
[09/28 08:28:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.14	top5: 44.63	
[09/28 08:28:56 visual_prompt]: Best epoch 17: best metric: 0.140
[09/28 08:28:56 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/28 08:29:05 visual_prompt]: Epoch 18 / 100: avg data time: 1.05e-01, avg batch time: 0.5315, average train loss: 2.6134
[09/28 08:29:08 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1793, average loss: 2.4919
[09/28 08:29:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.50	top5: 57.00	
[09/28 08:29:30 visual_prompt]: 	Test 100/190. loss: 2.730, 0.2060 s / batch. (data: 2.62e-05)max mem: 7.81234 GB 
[09/28 08:29:50 visual_prompt]: Inference (test):avg data time: 2.81e-05, avg batch time: 0.2046, average loss: 2.7184
[09/28 08:29:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.18	top5: 50.21	
[09/28 08:29:50 visual_prompt]: Best epoch 18: best metric: 0.195
[09/28 08:29:50 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/28 08:30:00 visual_prompt]: Epoch 19 / 100: avg data time: 1.10e-01, avg batch time: 0.5365, average train loss: 2.5408
[09/28 08:30:03 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1817, average loss: 2.6896
[09/28 08:30:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.00	top5: 55.50	
[09/28 08:30:25 visual_prompt]: 	Test 100/190. loss: 2.746, 0.2044 s / batch. (data: 2.50e-05)max mem: 7.81234 GB 
[09/28 08:30:44 visual_prompt]: Inference (test):avg data time: 2.90e-05, avg batch time: 0.2045, average loss: 2.7931
[09/28 08:30:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.20	top5: 50.52	
[09/28 08:30:44 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/28 08:30:54 visual_prompt]: Epoch 20 / 100: avg data time: 9.74e-02, avg batch time: 0.5230, average train loss: 2.5325
[09/28 08:30:57 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1753, average loss: 2.5621
[09/28 08:30:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.00	top5: 60.50	
[09/28 08:31:19 visual_prompt]: 	Test 100/190. loss: 2.643, 0.2042 s / batch. (data: 2.67e-05)max mem: 7.81234 GB 
[09/28 08:31:39 visual_prompt]: Inference (test):avg data time: 1.46e-04, avg batch time: 0.2048, average loss: 2.6859
[09/28 08:31:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.27	top5: 55.18	
[09/28 08:31:39 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/28 08:31:48 visual_prompt]: Epoch 21 / 100: avg data time: 1.05e-01, avg batch time: 0.5311, average train loss: 2.4585
[09/28 08:31:51 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1801, average loss: 2.3458
[09/28 08:31:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.00	top5: 66.50	
[09/28 08:32:13 visual_prompt]: 	Test 100/190. loss: 2.615, 0.2037 s / batch. (data: 2.96e-05)max mem: 7.81234 GB 
[09/28 08:32:33 visual_prompt]: Inference (test):avg data time: 2.98e-05, avg batch time: 0.2048, average loss: 2.6016
[09/28 08:32:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.88	top5: 59.11	
[09/28 08:32:33 visual_prompt]: Best epoch 21: best metric: 0.210
[09/28 08:32:33 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/28 08:32:42 visual_prompt]: Epoch 22 / 100: avg data time: 1.03e-01, avg batch time: 0.5288, average train loss: 2.2833
[09/28 08:32:46 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1764, average loss: 2.2517
[09/28 08:32:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.50	top5: 72.00	
[09/28 08:33:08 visual_prompt]: 	Test 100/190. loss: 2.751, 0.2035 s / batch. (data: 2.43e-05)max mem: 7.81234 GB 
[09/28 08:33:27 visual_prompt]: Inference (test):avg data time: 2.98e-05, avg batch time: 0.2045, average loss: 2.6166
[09/28 08:33:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.03	top5: 62.55	
[09/28 08:33:28 visual_prompt]: Best epoch 22: best metric: 0.215
[09/28 08:33:28 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/28 08:33:37 visual_prompt]: Epoch 23 / 100: avg data time: 9.93e-02, avg batch time: 0.5248, average train loss: 2.3486
[09/28 08:33:40 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1792, average loss: 2.2027
[09/28 08:33:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 23.00	top5: 79.00	
[09/28 08:34:02 visual_prompt]: 	Test 100/190. loss: 2.476, 0.2062 s / batch. (data: 2.74e-05)max mem: 7.81234 GB 
[09/28 08:34:22 visual_prompt]: Inference (test):avg data time: 2.88e-05, avg batch time: 0.2047, average loss: 2.4710
[09/28 08:34:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.02	top5: 63.42	
[09/28 08:34:22 visual_prompt]: Best epoch 23: best metric: 0.230
[09/28 08:34:22 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/28 08:34:31 visual_prompt]: Epoch 24 / 100: avg data time: 1.06e-01, avg batch time: 0.5321, average train loss: 2.2239
[09/28 08:34:35 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1823, average loss: 2.2183
[09/28 08:34:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 23.50	top5: 73.50	
[09/28 08:34:57 visual_prompt]: 	Test 100/190. loss: 2.550, 0.2054 s / batch. (data: 2.88e-05)max mem: 7.81234 GB 
[09/28 08:35:16 visual_prompt]: Inference (test):avg data time: 3.09e-05, avg batch time: 0.2045, average loss: 2.4522
[09/28 08:35:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.26	top5: 65.28	
[09/28 08:35:16 visual_prompt]: Best epoch 24: best metric: 0.235
[09/28 08:35:16 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/28 08:35:26 visual_prompt]: Epoch 25 / 100: avg data time: 9.36e-02, avg batch time: 0.5205, average train loss: 2.1791
[09/28 08:35:29 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1842, average loss: 2.1798
[09/28 08:35:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 23.00	top5: 72.50	
[09/28 08:35:51 visual_prompt]: 	Test 100/190. loss: 2.559, 0.2041 s / batch. (data: 2.43e-05)max mem: 7.81234 GB 
[09/28 08:36:11 visual_prompt]: Inference (test):avg data time: 2.85e-05, avg batch time: 0.2047, average loss: 2.4473
[09/28 08:36:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.99	top5: 64.19	
[09/28 08:36:11 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/28 08:36:20 visual_prompt]: Epoch 26 / 100: avg data time: 1.06e-01, avg batch time: 0.5330, average train loss: 2.0641
[09/28 08:36:24 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1803, average loss: 2.0325
[09/28 08:36:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 26.50	top5: 81.50	
[09/28 08:36:46 visual_prompt]: 	Test 100/190. loss: 2.935, 0.2050 s / batch. (data: 2.74e-05)max mem: 7.81234 GB 
[09/28 08:37:05 visual_prompt]: Inference (test):avg data time: 2.83e-05, avg batch time: 0.2051, average loss: 2.5291
[09/28 08:37:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.35	top5: 67.54	
[09/28 08:37:05 visual_prompt]: Best epoch 26: best metric: 0.265
[09/28 08:37:05 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/28 08:37:15 visual_prompt]: Epoch 27 / 100: avg data time: 1.09e-01, avg batch time: 0.5356, average train loss: 2.0003
[09/28 08:37:18 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1776, average loss: 2.0884
[09/28 08:37:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.50	top5: 78.50	
[09/28 08:37:40 visual_prompt]: 	Test 100/190. loss: 2.643, 0.2193 s / batch. (data: 2.72e-05)max mem: 7.81234 GB 
[09/28 08:38:00 visual_prompt]: Inference (test):avg data time: 2.80e-05, avg batch time: 0.2045, average loss: 2.6798
[09/28 08:38:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.73	top5: 66.79	
[09/28 08:38:00 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/28 08:38:09 visual_prompt]: Epoch 28 / 100: avg data time: 1.05e-01, avg batch time: 0.5306, average train loss: 1.8632
[09/28 08:38:13 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1822, average loss: 1.9230
[09/28 08:38:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 32.50	top5: 85.00	
[09/28 08:38:35 visual_prompt]: 	Test 100/190. loss: 2.697, 0.2034 s / batch. (data: 2.53e-05)max mem: 7.81234 GB 
[09/28 08:38:54 visual_prompt]: Inference (test):avg data time: 2.88e-05, avg batch time: 0.2050, average loss: 2.6721
[09/28 08:38:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.19	top5: 68.63	
[09/28 08:38:54 visual_prompt]: Best epoch 28: best metric: 0.325
[09/28 08:38:54 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/28 08:39:04 visual_prompt]: Epoch 29 / 100: avg data time: 1.04e-01, avg batch time: 0.5304, average train loss: 1.7424
[09/28 08:39:07 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1852, average loss: 1.9538
[09/28 08:39:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.50	top5: 83.50	
[09/28 08:39:29 visual_prompt]: 	Test 100/190. loss: 2.762, 0.2041 s / batch. (data: 4.86e-05)max mem: 7.81234 GB 
[09/28 08:39:48 visual_prompt]: Inference (test):avg data time: 2.86e-05, avg batch time: 0.2045, average loss: 2.6681
[09/28 08:39:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.52	top5: 69.15	
[09/28 08:39:48 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/28 08:39:58 visual_prompt]: Epoch 30 / 100: avg data time: 1.09e-01, avg batch time: 0.5333, average train loss: 1.6643
[09/28 08:40:01 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1726, average loss: 1.7766
[09/28 08:40:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 35.50	top5: 90.50	
[09/28 08:40:23 visual_prompt]: 	Test 100/190. loss: 3.029, 0.2039 s / batch. (data: 2.81e-05)max mem: 7.81234 GB 
[09/28 08:40:43 visual_prompt]: Inference (test):avg data time: 1.57e-04, avg batch time: 0.2046, average loss: 2.7635
[09/28 08:40:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.86	top5: 69.54	
[09/28 08:40:43 visual_prompt]: Best epoch 30: best metric: 0.355
[09/28 08:40:43 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/28 08:40:52 visual_prompt]: Epoch 31 / 100: avg data time: 8.91e-02, avg batch time: 0.5173, average train loss: 1.5314
[09/28 08:40:56 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1769, average loss: 1.4990
[09/28 08:40:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 45.50	top5: 94.50	
[09/28 08:41:18 visual_prompt]: 	Test 100/190. loss: 3.090, 0.2043 s / batch. (data: 2.60e-05)max mem: 7.81234 GB 
[09/28 08:41:37 visual_prompt]: Inference (test):avg data time: 2.77e-05, avg batch time: 0.2043, average loss: 2.7538
[09/28 08:41:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.15	top5: 70.00	
[09/28 08:41:37 visual_prompt]: Best epoch 31: best metric: 0.455
[09/28 08:41:37 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/28 08:41:47 visual_prompt]: Epoch 32 / 100: avg data time: 9.90e-02, avg batch time: 0.5256, average train loss: 1.3877
[09/28 08:41:50 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1835, average loss: 1.6247
[09/28 08:41:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 41.50	top5: 91.00	
[09/28 08:42:12 visual_prompt]: 	Test 100/190. loss: 3.137, 0.2042 s / batch. (data: 2.84e-05)max mem: 7.81234 GB 
[09/28 08:42:31 visual_prompt]: Inference (test):avg data time: 2.81e-05, avg batch time: 0.2043, average loss: 2.9223
[09/28 08:42:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.89	top5: 69.23	
[09/28 08:42:31 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/28 08:42:41 visual_prompt]: Epoch 33 / 100: avg data time: 9.98e-02, avg batch time: 0.5270, average train loss: 1.4596
[09/28 08:42:44 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1852, average loss: 1.4254
[09/28 08:42:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 48.50	top5: 93.50	
[09/28 08:43:06 visual_prompt]: 	Test 100/190. loss: 2.735, 0.2035 s / batch. (data: 2.62e-05)max mem: 7.81234 GB 
[09/28 08:43:26 visual_prompt]: Inference (test):avg data time: 2.89e-05, avg batch time: 0.2042, average loss: 2.7526
[09/28 08:43:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.49	top5: 71.12	
[09/28 08:43:26 visual_prompt]: Best epoch 33: best metric: 0.485
[09/28 08:43:26 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/28 08:43:35 visual_prompt]: Epoch 34 / 100: avg data time: 1.00e-01, avg batch time: 0.5245, average train loss: 1.1907
[09/28 08:43:38 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1800, average loss: 1.2183
[09/28 08:43:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 55.00	top5: 95.50	
[09/28 08:44:00 visual_prompt]: 	Test 100/190. loss: 3.222, 0.2045 s / batch. (data: 2.38e-05)max mem: 7.81234 GB 
[09/28 08:44:20 visual_prompt]: Inference (test):avg data time: 1.34e-04, avg batch time: 0.2043, average loss: 3.0407
[09/28 08:44:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 22.72	top5: 71.57	
[09/28 08:44:20 visual_prompt]: Best epoch 34: best metric: 0.550
[09/28 08:44:20 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/28 08:44:29 visual_prompt]: Epoch 35 / 100: avg data time: 9.37e-02, avg batch time: 0.5191, average train loss: 1.1156
[09/28 08:44:33 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1757, average loss: 1.0363
[09/28 08:44:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 65.50	top5: 96.00	
[09/28 08:44:55 visual_prompt]: 	Test 100/190. loss: 3.258, 0.2030 s / batch. (data: 2.88e-05)max mem: 7.81234 GB 
[09/28 08:45:14 visual_prompt]: Inference (test):avg data time: 2.89e-05, avg batch time: 0.2044, average loss: 2.9517
[09/28 08:45:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 22.44	top5: 71.34	
[09/28 08:45:14 visual_prompt]: Best epoch 35: best metric: 0.655
[09/28 08:45:14 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/28 08:45:24 visual_prompt]: Epoch 36 / 100: avg data time: 1.02e-01, avg batch time: 0.5285, average train loss: 0.8803
[09/28 08:45:27 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1849, average loss: 0.9538
[09/28 08:45:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 71.50	top5: 96.50	
[09/28 08:45:49 visual_prompt]: 	Test 100/190. loss: 3.120, 0.2033 s / batch. (data: 2.26e-05)max mem: 7.81234 GB 
[09/28 08:46:09 visual_prompt]: Inference (test):avg data time: 9.65e-05, avg batch time: 0.2043, average loss: 3.0788
[09/28 08:46:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 24.61	top5: 73.16	
[09/28 08:46:09 visual_prompt]: Best epoch 36: best metric: 0.715
[09/28 08:46:09 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/28 08:46:18 visual_prompt]: Epoch 37 / 100: avg data time: 1.04e-01, avg batch time: 0.5299, average train loss: 0.7113
[09/28 08:46:21 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1741, average loss: 0.7936
[09/28 08:46:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 75.50	top5: 98.50	
[09/28 08:46:43 visual_prompt]: 	Test 100/190. loss: 3.811, 0.2034 s / batch. (data: 3.36e-05)max mem: 7.81234 GB 
[09/28 08:47:03 visual_prompt]: Inference (test):avg data time: 2.99e-05, avg batch time: 0.2041, average loss: 3.2525
[09/28 08:47:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 22.91	top5: 71.56	
[09/28 08:47:03 visual_prompt]: Best epoch 37: best metric: 0.755
[09/28 08:47:03 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/28 08:47:13 visual_prompt]: Epoch 38 / 100: avg data time: 1.02e-01, avg batch time: 0.5274, average train loss: 0.6622
[09/28 08:47:16 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1852, average loss: 0.8056
[09/28 08:47:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 69.00	top5: 99.50	
[09/28 08:47:38 visual_prompt]: 	Test 100/190. loss: 3.505, 0.2019 s / batch. (data: 3.74e-05)max mem: 7.81234 GB 
[09/28 08:47:58 visual_prompt]: Inference (test):avg data time: 4.15e-05, avg batch time: 0.2040, average loss: 3.2374
[09/28 08:47:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 24.35	top5: 73.05	
[09/28 08:47:58 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/28 08:48:07 visual_prompt]: Epoch 39 / 100: avg data time: 9.75e-02, avg batch time: 0.5252, average train loss: 0.5773
[09/28 08:48:10 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1736, average loss: 0.6043
[09/28 08:48:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 79.00	top5: 99.50	
[09/28 08:48:32 visual_prompt]: 	Test 100/190. loss: 3.456, 0.2038 s / batch. (data: 2.62e-05)max mem: 7.81234 GB 
[09/28 08:48:52 visual_prompt]: Inference (test):avg data time: 2.88e-05, avg batch time: 0.2040, average loss: 3.2090
[09/28 08:48:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.59	top5: 72.61	
[09/28 08:48:52 visual_prompt]: Best epoch 39: best metric: 0.790
[09/28 08:48:52 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/28 08:49:01 visual_prompt]: Epoch 40 / 100: avg data time: 1.05e-01, avg batch time: 0.5299, average train loss: 0.4736
[09/28 08:49:04 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1830, average loss: 0.3716
[09/28 08:49:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 88.50	top5: 100.00	
[09/28 08:49:26 visual_prompt]: 	Test 100/190. loss: 3.486, 0.2048 s / batch. (data: 2.60e-05)max mem: 7.81234 GB 
[09/28 08:49:46 visual_prompt]: Inference (test):avg data time: 2.77e-05, avg batch time: 0.2042, average loss: 3.2521
[09/28 08:49:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 26.44	top5: 73.79	
[09/28 08:49:46 visual_prompt]: Best epoch 40: best metric: 0.885
[09/28 08:49:46 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/28 08:49:55 visual_prompt]: Epoch 41 / 100: avg data time: 1.02e-01, avg batch time: 0.5262, average train loss: 0.3596
[09/28 08:49:59 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1832, average loss: 0.3028
[09/28 08:49:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 91.50	top5: 100.00	
[09/28 08:50:21 visual_prompt]: 	Test 100/190. loss: 3.394, 0.2037 s / batch. (data: 2.60e-05)max mem: 7.81234 GB 
[09/28 08:50:40 visual_prompt]: Inference (test):avg data time: 2.97e-05, avg batch time: 0.2041, average loss: 3.2205
[09/28 08:50:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.93	top5: 73.93	
[09/28 08:50:40 visual_prompt]: Best epoch 41: best metric: 0.915
[09/28 08:50:40 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/28 08:50:50 visual_prompt]: Epoch 42 / 100: avg data time: 9.64e-02, avg batch time: 0.5207, average train loss: 0.3383
[09/28 08:50:53 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1785, average loss: 0.3200
[09/28 08:50:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 90.50	top5: 100.00	
[09/28 08:51:15 visual_prompt]: 	Test 100/190. loss: 3.305, 0.2032 s / batch. (data: 2.69e-05)max mem: 7.81234 GB 
[09/28 08:51:34 visual_prompt]: Inference (test):avg data time: 2.86e-05, avg batch time: 0.2040, average loss: 3.2932
[09/28 08:51:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.18	top5: 74.81	
[09/28 08:51:34 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/28 08:51:44 visual_prompt]: Epoch 43 / 100: avg data time: 9.98e-02, avg batch time: 0.5249, average train loss: 0.2934
[09/28 08:51:47 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1781, average loss: 0.2023
[09/28 08:51:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 97.00	top5: 100.00	
[09/28 08:52:09 visual_prompt]: 	Test 100/190. loss: 3.027, 0.2033 s / batch. (data: 2.81e-05)max mem: 7.81234 GB 
[09/28 08:52:28 visual_prompt]: Inference (test):avg data time: 2.99e-05, avg batch time: 0.2040, average loss: 3.1728
[09/28 08:52:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.57	top5: 74.39	
[09/28 08:52:28 visual_prompt]: Best epoch 43: best metric: 0.970
[09/28 08:52:28 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/28 08:52:38 visual_prompt]: Epoch 44 / 100: avg data time: 1.05e-01, avg batch time: 0.5291, average train loss: 0.2281
[09/28 08:52:41 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1726, average loss: 0.1865
[09/28 08:52:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 96.50	top5: 100.00	
[09/28 08:53:03 visual_prompt]: 	Test 100/190. loss: 3.317, 0.2043 s / batch. (data: 2.29e-05)max mem: 7.81234 GB 
[09/28 08:53:23 visual_prompt]: Inference (test):avg data time: 2.84e-05, avg batch time: 0.2040, average loss: 3.2393
[09/28 08:53:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.16	top5: 75.07	
[09/28 08:53:23 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/28 08:53:32 visual_prompt]: Epoch 45 / 100: avg data time: 8.89e-02, avg batch time: 0.5165, average train loss: 0.1569
[09/28 08:53:35 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1729, average loss: 0.1124
[09/28 08:53:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 98.00	top5: 100.00	
[09/28 08:53:57 visual_prompt]: 	Test 100/190. loss: 3.453, 0.2040 s / batch. (data: 2.81e-05)max mem: 7.81234 GB 
[09/28 08:54:17 visual_prompt]: Inference (test):avg data time: 1.08e-04, avg batch time: 0.2043, average loss: 3.3640
[09/28 08:54:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.11	top5: 74.69	
[09/28 08:54:17 visual_prompt]: Best epoch 45: best metric: 0.980
[09/28 08:54:17 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/28 08:54:26 visual_prompt]: Epoch 46 / 100: avg data time: 1.02e-01, avg batch time: 0.5281, average train loss: 0.1428
[09/28 08:54:29 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1743, average loss: 0.1213
[09/28 08:54:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 97.00	top5: 100.00	
[09/28 08:54:51 visual_prompt]: 	Test 100/190. loss: 3.261, 0.2031 s / batch. (data: 3.00e-05)max mem: 7.81234 GB 
[09/28 08:55:11 visual_prompt]: Inference (test):avg data time: 2.97e-05, avg batch time: 0.2042, average loss: 3.3806
[09/28 08:55:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.07	top5: 74.77	
[09/28 08:55:11 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/28 08:55:20 visual_prompt]: Epoch 47 / 100: avg data time: 1.03e-01, avg batch time: 0.5298, average train loss: 0.1403
[09/28 08:55:24 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1812, average loss: 0.1308
[09/28 08:55:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 97.00	top5: 100.00	
[09/28 08:55:46 visual_prompt]: 	Test 100/190. loss: 3.192, 0.2042 s / batch. (data: 2.77e-05)max mem: 7.81234 GB 
[09/28 08:56:05 visual_prompt]: Inference (test):avg data time: 8.31e-05, avg batch time: 0.2044, average loss: 3.4028
[09/28 08:56:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.95	top5: 74.19	
[09/28 08:56:05 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/28 08:56:15 visual_prompt]: Epoch 48 / 100: avg data time: 1.02e-01, avg batch time: 0.5285, average train loss: 0.1373
[09/28 08:56:18 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1822, average loss: 0.3456
[09/28 08:56:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 91.00	top5: 100.00	
[09/28 08:56:40 visual_prompt]: 	Test 100/190. loss: 3.382, 0.2034 s / batch. (data: 2.84e-05)max mem: 7.81234 GB 
[09/28 08:57:00 visual_prompt]: Inference (test):avg data time: 6.39e-05, avg batch time: 0.2040, average loss: 3.4135
[09/28 08:57:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.18	top5: 73.74	
[09/28 08:57:00 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/28 08:57:09 visual_prompt]: Epoch 49 / 100: avg data time: 9.37e-02, avg batch time: 0.5204, average train loss: 0.1174
[09/28 08:57:12 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1779, average loss: 0.0808
[09/28 08:57:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 98.00	top5: 100.00	
[09/28 08:57:34 visual_prompt]: 	Test 100/190. loss: 3.314, 0.2042 s / batch. (data: 2.50e-05)max mem: 7.81234 GB 
[09/28 08:57:54 visual_prompt]: Inference (test):avg data time: 2.80e-05, avg batch time: 0.2041, average loss: 3.5092
[09/28 08:57:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.95	top5: 73.37	
[09/28 08:57:54 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/28 08:58:03 visual_prompt]: Epoch 50 / 100: avg data time: 9.93e-02, avg batch time: 0.5265, average train loss: 0.1134
[09/28 08:58:07 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1778, average loss: 0.1184
[09/28 08:58:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 97.50	top5: 100.00	
[09/28 08:58:28 visual_prompt]: 	Test 100/190. loss: 3.416, 0.2037 s / batch. (data: 2.86e-05)max mem: 7.81234 GB 
[09/28 08:58:48 visual_prompt]: Inference (test):avg data time: 2.81e-05, avg batch time: 0.2043, average loss: 3.5544
[09/28 08:58:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.92	top5: 73.22	
[09/28 08:58:48 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/28 08:58:58 visual_prompt]: Epoch 51 / 100: avg data time: 1.01e-01, avg batch time: 0.5246, average train loss: 0.0806
[09/28 08:59:01 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1756, average loss: 0.0801
[09/28 08:59:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.00	top5: 100.00	
[09/28 08:59:23 visual_prompt]: 	Test 100/190. loss: 3.222, 0.2030 s / batch. (data: 2.84e-05)max mem: 7.81234 GB 
[09/28 08:59:42 visual_prompt]: Inference (test):avg data time: 3.81e-05, avg batch time: 0.2040, average loss: 3.4958
[09/28 08:59:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.08	top5: 73.88	
[09/28 08:59:42 visual_prompt]: Best epoch 51: best metric: 0.990
[09/28 08:59:42 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/28 08:59:52 visual_prompt]: Epoch 52 / 100: avg data time: 1.02e-01, avg batch time: 0.5271, average train loss: 0.0657
[09/28 08:59:55 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1731, average loss: 0.0513
[09/28 08:59:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:00:17 visual_prompt]: 	Test 100/190. loss: 3.432, 0.2048 s / batch. (data: 2.65e-05)max mem: 7.81234 GB 
[09/28 09:00:37 visual_prompt]: Inference (test):avg data time: 3.12e-05, avg batch time: 0.2041, average loss: 3.4484
[09/28 09:00:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.49	top5: 74.26	
[09/28 09:00:37 visual_prompt]: Best epoch 52: best metric: 1.000
[09/28 09:00:37 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/28 09:00:46 visual_prompt]: Epoch 53 / 100: avg data time: 1.01e-01, avg batch time: 0.5255, average train loss: 0.0626
[09/28 09:00:49 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1846, average loss: 0.0535
[09/28 09:00:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.50	top5: 100.00	
[09/28 09:01:11 visual_prompt]: 	Test 100/190. loss: 3.390, 0.2037 s / batch. (data: 2.77e-05)max mem: 7.81234 GB 
[09/28 09:01:31 visual_prompt]: Inference (test):avg data time: 2.86e-05, avg batch time: 0.2040, average loss: 3.5027
[09/28 09:01:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.61	top5: 72.91	
[09/28 09:01:31 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/28 09:01:40 visual_prompt]: Epoch 54 / 100: avg data time: 8.83e-02, avg batch time: 0.5150, average train loss: 0.0705
[09/28 09:01:44 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1784, average loss: 0.0320
[09/28 09:01:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:02:06 visual_prompt]: 	Test 100/190. loss: 3.223, 0.2033 s / batch. (data: 2.67e-05)max mem: 7.81234 GB 
[09/28 09:02:25 visual_prompt]: Inference (test):avg data time: 2.99e-05, avg batch time: 0.2039, average loss: 3.4229
[09/28 09:02:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.34	top5: 74.62	
[09/28 09:02:25 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/28 09:02:34 visual_prompt]: Epoch 55 / 100: avg data time: 9.14e-02, avg batch time: 0.5173, average train loss: 0.0572
[09/28 09:02:38 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1817, average loss: 0.0648
[09/28 09:02:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.50	top5: 100.00	
[09/28 09:03:00 visual_prompt]: 	Test 100/190. loss: 3.498, 0.2041 s / batch. (data: 2.79e-05)max mem: 7.81234 GB 
[09/28 09:03:19 visual_prompt]: Inference (test):avg data time: 2.79e-05, avg batch time: 0.2041, average loss: 3.5072
[09/28 09:03:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.98	top5: 73.71	
[09/28 09:03:19 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/28 09:03:29 visual_prompt]: Epoch 56 / 100: avg data time: 1.08e-01, avg batch time: 0.5338, average train loss: 0.0548
[09/28 09:03:32 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1829, average loss: 0.0358
[09/28 09:03:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.50	top5: 100.00	
[09/28 09:03:54 visual_prompt]: 	Test 100/190. loss: 3.589, 0.2033 s / batch. (data: 4.01e-05)max mem: 7.81234 GB 
[09/28 09:04:14 visual_prompt]: Inference (test):avg data time: 2.85e-05, avg batch time: 0.2040, average loss: 3.5131
[09/28 09:04:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.79	top5: 73.17	
[09/28 09:04:14 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/28 09:04:23 visual_prompt]: Epoch 57 / 100: avg data time: 1.02e-01, avg batch time: 0.5262, average train loss: 0.0351
[09/28 09:04:27 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1728, average loss: 0.0141
[09/28 09:04:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:04:49 visual_prompt]: 	Test 100/190. loss: 3.431, 0.2051 s / batch. (data: 2.79e-05)max mem: 7.81234 GB 
[09/28 09:05:08 visual_prompt]: Inference (test):avg data time: 2.91e-05, avg batch time: 0.2042, average loss: 3.4861
[09/28 09:05:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.95	top5: 74.25	
[09/28 09:05:08 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/28 09:05:18 visual_prompt]: Epoch 58 / 100: avg data time: 1.10e-01, avg batch time: 0.5363, average train loss: 0.0238
[09/28 09:05:21 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1733, average loss: 0.0143
[09/28 09:05:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:05:43 visual_prompt]: 	Test 100/190. loss: 3.384, 0.2033 s / batch. (data: 2.53e-05)max mem: 7.81234 GB 
[09/28 09:06:03 visual_prompt]: Inference (test):avg data time: 2.85e-05, avg batch time: 0.2040, average loss: 3.4506
[09/28 09:06:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.46	top5: 75.04	
[09/28 09:06:03 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/28 09:06:12 visual_prompt]: Epoch 59 / 100: avg data time: 9.26e-02, avg batch time: 0.5202, average train loss: 0.0184
[09/28 09:06:15 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1770, average loss: 0.0133
[09/28 09:06:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:06:37 visual_prompt]: 	Test 100/190. loss: 3.392, 0.2032 s / batch. (data: 2.67e-05)max mem: 7.81234 GB 
[09/28 09:06:57 visual_prompt]: Inference (test):avg data time: 2.84e-05, avg batch time: 0.2041, average loss: 3.4467
[09/28 09:06:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.84	top5: 74.80	
[09/28 09:06:57 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/28 09:07:06 visual_prompt]: Epoch 60 / 100: avg data time: 1.05e-01, avg batch time: 0.5286, average train loss: 0.0155
[09/28 09:07:09 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1795, average loss: 0.0115
[09/28 09:07:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:07:31 visual_prompt]: 	Test 100/190. loss: 3.427, 0.2020 s / batch. (data: 6.41e-05)max mem: 7.81234 GB 
[09/28 09:07:51 visual_prompt]: Inference (test):avg data time: 1.03e-04, avg batch time: 0.2043, average loss: 3.4649
[09/28 09:07:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.74	top5: 74.96	
[09/28 09:07:51 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/28 09:08:00 visual_prompt]: Epoch 61 / 100: avg data time: 1.07e-01, avg batch time: 0.5320, average train loss: 0.0147
[09/28 09:08:03 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1778, average loss: 0.0110
[09/28 09:08:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:08:25 visual_prompt]: 	Test 100/190. loss: 3.413, 0.2039 s / batch. (data: 2.57e-05)max mem: 7.81234 GB 
[09/28 09:08:45 visual_prompt]: Inference (test):avg data time: 2.97e-05, avg batch time: 0.2040, average loss: 3.4593
[09/28 09:08:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.91	top5: 74.91	
[09/28 09:08:45 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/28 09:08:54 visual_prompt]: Epoch 62 / 100: avg data time: 9.53e-02, avg batch time: 0.5210, average train loss: 0.0140
[09/28 09:08:58 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1793, average loss: 0.0101
[09/28 09:08:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:09:20 visual_prompt]: 	Test 100/190. loss: 3.466, 0.2033 s / batch. (data: 3.60e-05)max mem: 7.81234 GB 
[09/28 09:09:39 visual_prompt]: Inference (test):avg data time: 2.90e-05, avg batch time: 0.2041, average loss: 3.4576
[09/28 09:09:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.80	top5: 74.81	
[09/28 09:09:39 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/28 09:09:49 visual_prompt]: Epoch 63 / 100: avg data time: 9.14e-02, avg batch time: 0.5175, average train loss: 0.0131
[09/28 09:09:52 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1770, average loss: 0.0095
[09/28 09:09:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:10:14 visual_prompt]: 	Test 100/190. loss: 3.474, 0.2042 s / batch. (data: 2.77e-05)max mem: 7.81234 GB 
[09/28 09:10:33 visual_prompt]: Inference (test):avg data time: 3.82e-05, avg batch time: 0.2041, average loss: 3.4415
[09/28 09:10:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.93	top5: 74.79	
[09/28 09:10:33 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/28 09:10:43 visual_prompt]: Epoch 64 / 100: avg data time: 1.03e-01, avg batch time: 0.5283, average train loss: 0.0125
[09/28 09:10:46 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1757, average loss: 0.0100
[09/28 09:10:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:11:08 visual_prompt]: 	Test 100/190. loss: 3.476, 0.2043 s / batch. (data: 4.67e-05)max mem: 7.81234 GB 
[09/28 09:11:28 visual_prompt]: Inference (test):avg data time: 1.44e-04, avg batch time: 0.2043, average loss: 3.4455
[09/28 09:11:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.81	top5: 74.70	
[09/28 09:11:28 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/28 09:11:37 visual_prompt]: Epoch 65 / 100: avg data time: 9.93e-02, avg batch time: 0.5262, average train loss: 0.0124
[09/28 09:11:41 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1781, average loss: 0.0088
[09/28 09:11:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:12:03 visual_prompt]: 	Test 100/190. loss: 3.462, 0.2035 s / batch. (data: 2.57e-05)max mem: 7.81234 GB 
[09/28 09:12:22 visual_prompt]: Inference (test):avg data time: 1.05e-04, avg batch time: 0.2045, average loss: 3.4476
[09/28 09:12:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.19	top5: 74.73	
[09/28 09:12:22 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/28 09:12:32 visual_prompt]: Epoch 66 / 100: avg data time: 9.50e-02, avg batch time: 0.5207, average train loss: 0.0114
[09/28 09:12:35 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1723, average loss: 0.0087
[09/28 09:12:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:12:57 visual_prompt]: 	Test 100/190. loss: 3.445, 0.2056 s / batch. (data: 2.69e-05)max mem: 7.81234 GB 
[09/28 09:13:17 visual_prompt]: Inference (test):avg data time: 2.92e-05, avg batch time: 0.2041, average loss: 3.4409
[09/28 09:13:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.25	top5: 74.67	
[09/28 09:13:17 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/28 09:13:26 visual_prompt]: Epoch 67 / 100: avg data time: 9.10e-02, avg batch time: 0.5179, average train loss: 0.0115
[09/28 09:13:29 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1734, average loss: 0.0090
[09/28 09:13:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:13:51 visual_prompt]: 	Test 100/190. loss: 3.468, 0.2034 s / batch. (data: 2.57e-05)max mem: 7.81234 GB 
[09/28 09:14:11 visual_prompt]: Inference (test):avg data time: 2.88e-05, avg batch time: 0.2041, average loss: 3.4295
[09/28 09:14:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.04	top5: 74.70	
[09/28 09:14:11 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/28 09:14:20 visual_prompt]: Epoch 68 / 100: avg data time: 9.71e-02, avg batch time: 0.5236, average train loss: 0.0114
[09/28 09:14:23 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1805, average loss: 0.0091
[09/28 09:14:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:14:45 visual_prompt]: 	Test 100/190. loss: 3.475, 0.2040 s / batch. (data: 2.69e-05)max mem: 7.81234 GB 
[09/28 09:15:05 visual_prompt]: Inference (test):avg data time: 4.50e-05, avg batch time: 0.2043, average loss: 3.4327
[09/28 09:15:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.88	top5: 74.86	
[09/28 09:15:05 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/28 09:15:15 visual_prompt]: Epoch 69 / 100: avg data time: 1.04e-01, avg batch time: 0.5300, average train loss: 0.0115
[09/28 09:15:18 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1718, average loss: 0.0083
[09/28 09:15:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:15:40 visual_prompt]: 	Test 100/190. loss: 3.450, 0.2033 s / batch. (data: 2.60e-05)max mem: 7.81234 GB 
[09/28 09:15:59 visual_prompt]: Inference (test):avg data time: 2.82e-05, avg batch time: 0.2040, average loss: 3.4336
[09/28 09:15:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.00	top5: 74.90	
[09/28 09:15:59 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/28 09:16:09 visual_prompt]: Epoch 70 / 100: avg data time: 8.81e-02, avg batch time: 0.5134, average train loss: 0.0114
[09/28 09:16:12 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1808, average loss: 0.0079
[09/28 09:16:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:16:34 visual_prompt]: 	Test 100/190. loss: 3.460, 0.2029 s / batch. (data: 2.31e-05)max mem: 7.81234 GB 
[09/28 09:16:53 visual_prompt]: Inference (test):avg data time: 1.38e-04, avg batch time: 0.2041, average loss: 3.4195
[09/28 09:16:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.18	top5: 74.88	
[09/28 09:16:54 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/28 09:17:03 visual_prompt]: Epoch 71 / 100: avg data time: 9.97e-02, avg batch time: 0.5246, average train loss: 0.0106
[09/28 09:17:06 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1831, average loss: 0.0080
[09/28 09:17:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:17:28 visual_prompt]: 	Test 100/190. loss: 3.466, 0.2042 s / batch. (data: 2.65e-05)max mem: 7.81234 GB 
[09/28 09:17:48 visual_prompt]: Inference (test):avg data time: 3.00e-05, avg batch time: 0.2040, average loss: 3.4204
[09/28 09:17:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.06	top5: 74.70	
[09/28 09:17:48 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/28 09:17:57 visual_prompt]: Epoch 72 / 100: avg data time: 1.01e-01, avg batch time: 0.5280, average train loss: 0.0112
[09/28 09:18:01 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1737, average loss: 0.0080
[09/28 09:18:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:18:23 visual_prompt]: 	Test 100/190. loss: 3.476, 0.2043 s / batch. (data: 2.38e-05)max mem: 7.81234 GB 
[09/28 09:18:42 visual_prompt]: Inference (test):avg data time: 3.03e-05, avg batch time: 0.2039, average loss: 3.4202
[09/28 09:18:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.13	top5: 74.79	
[09/28 09:18:42 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/28 09:18:51 visual_prompt]: Epoch 73 / 100: avg data time: 9.21e-02, avg batch time: 0.5176, average train loss: 0.0107
[09/28 09:18:55 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1740, average loss: 0.0079
[09/28 09:18:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:19:17 visual_prompt]: 	Test 100/190. loss: 3.501, 0.2043 s / batch. (data: 2.41e-05)max mem: 7.81234 GB 
[09/28 09:19:36 visual_prompt]: Inference (test):avg data time: 2.88e-05, avg batch time: 0.2040, average loss: 3.4186
[09/28 09:19:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.98	top5: 74.67	
[09/28 09:19:36 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/28 09:19:46 visual_prompt]: Epoch 74 / 100: avg data time: 9.85e-02, avg batch time: 0.5256, average train loss: 0.0108
[09/28 09:19:49 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1764, average loss: 0.0078
[09/28 09:19:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:20:11 visual_prompt]: 	Test 100/190. loss: 3.510, 0.2042 s / batch. (data: 2.69e-05)max mem: 7.81234 GB 
[09/28 09:20:30 visual_prompt]: Inference (test):avg data time: 2.90e-05, avg batch time: 0.2044, average loss: 3.4165
[09/28 09:20:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.09	top5: 74.68	
[09/28 09:20:31 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/28 09:20:40 visual_prompt]: Epoch 75 / 100: avg data time: 1.01e-01, avg batch time: 0.5271, average train loss: 0.0104
[09/28 09:20:43 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1811, average loss: 0.0081
[09/28 09:20:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:21:05 visual_prompt]: 	Test 100/190. loss: 3.496, 0.2036 s / batch. (data: 2.81e-05)max mem: 7.81234 GB 
[09/28 09:21:25 visual_prompt]: Inference (test):avg data time: 2.88e-05, avg batch time: 0.2040, average loss: 3.4167
[09/28 09:21:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.09	top5: 74.76	
[09/28 09:21:25 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/28 09:21:34 visual_prompt]: Epoch 76 / 100: avg data time: 1.02e-01, avg batch time: 0.5257, average train loss: 0.0104
[09/28 09:21:37 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1797, average loss: 0.0084
[09/28 09:21:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:22:00 visual_prompt]: 	Test 100/190. loss: 3.493, 0.2040 s / batch. (data: 2.74e-05)max mem: 7.81234 GB 
[09/28 09:22:19 visual_prompt]: Inference (test):avg data time: 2.98e-05, avg batch time: 0.2040, average loss: 3.4209
[09/28 09:22:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.08	top5: 74.90	
[09/28 09:22:19 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/28 09:22:28 visual_prompt]: Epoch 77 / 100: avg data time: 9.21e-02, avg batch time: 0.5172, average train loss: 0.0104
[09/28 09:22:31 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1726, average loss: 0.0081
[09/28 09:22:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:22:53 visual_prompt]: 	Test 100/190. loss: 3.481, 0.2033 s / batch. (data: 2.86e-05)max mem: 7.81234 GB 
[09/28 09:23:13 visual_prompt]: Inference (test):avg data time: 2.86e-05, avg batch time: 0.2040, average loss: 3.4023
[09/28 09:23:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.12	top5: 74.77	
[09/28 09:23:13 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/28 09:23:22 visual_prompt]: Epoch 78 / 100: avg data time: 9.93e-02, avg batch time: 0.5243, average train loss: 0.0102
[09/28 09:23:26 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1816, average loss: 0.0080
[09/28 09:23:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:23:48 visual_prompt]: 	Test 100/190. loss: 3.480, 0.2031 s / batch. (data: 2.98e-05)max mem: 7.81234 GB 
[09/28 09:24:07 visual_prompt]: Inference (test):avg data time: 2.96e-05, avg batch time: 0.2040, average loss: 3.3982
[09/28 09:24:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.17	top5: 74.72	
[09/28 09:24:07 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/28 09:24:17 visual_prompt]: Epoch 79 / 100: avg data time: 9.85e-02, avg batch time: 0.5235, average train loss: 0.0102
[09/28 09:24:20 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1788, average loss: 0.0076
[09/28 09:24:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:24:42 visual_prompt]: 	Test 100/190. loss: 3.491, 0.2034 s / batch. (data: 2.26e-05)max mem: 7.81234 GB 
[09/28 09:25:01 visual_prompt]: Inference (test):avg data time: 2.75e-05, avg batch time: 0.2039, average loss: 3.3992
[09/28 09:25:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.28	top5: 74.58	
[09/28 09:25:01 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/28 09:25:11 visual_prompt]: Epoch 80 / 100: avg data time: 9.57e-02, avg batch time: 0.5208, average train loss: 0.0098
[09/28 09:25:14 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1818, average loss: 0.0077
[09/28 09:25:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:25:36 visual_prompt]: 	Test 100/190. loss: 3.499, 0.2038 s / batch. (data: 2.81e-05)max mem: 7.81234 GB 
[09/28 09:25:55 visual_prompt]: Inference (test):avg data time: 3.02e-05, avg batch time: 0.2040, average loss: 3.4043
[09/28 09:25:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.10	top5: 74.53	
[09/28 09:25:55 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/28 09:26:05 visual_prompt]: Epoch 81 / 100: avg data time: 9.13e-02, avg batch time: 0.5159, average train loss: 0.0099
[09/28 09:26:08 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1806, average loss: 0.0078
[09/28 09:26:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:26:30 visual_prompt]: 	Test 100/190. loss: 3.504, 0.2043 s / batch. (data: 2.65e-05)max mem: 7.81234 GB 
[09/28 09:26:50 visual_prompt]: Inference (test):avg data time: 4.26e-05, avg batch time: 0.2039, average loss: 3.4064
[09/28 09:26:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.09	top5: 74.53	
[09/28 09:26:50 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/28 09:26:59 visual_prompt]: Epoch 82 / 100: avg data time: 1.05e-01, avg batch time: 0.5290, average train loss: 0.0097
[09/28 09:27:02 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1789, average loss: 0.0077
[09/28 09:27:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:27:24 visual_prompt]: 	Test 100/190. loss: 3.499, 0.2030 s / batch. (data: 2.43e-05)max mem: 7.81234 GB 
[09/28 09:27:44 visual_prompt]: Inference (test):avg data time: 4.96e-05, avg batch time: 0.2040, average loss: 3.4078
[09/28 09:27:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.23	top5: 74.55	
[09/28 09:27:44 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/28 09:27:53 visual_prompt]: Epoch 83 / 100: avg data time: 9.53e-02, avg batch time: 0.5203, average train loss: 0.0104
[09/28 09:27:57 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1801, average loss: 0.0078
[09/28 09:27:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:28:19 visual_prompt]: 	Test 100/190. loss: 3.496, 0.2035 s / batch. (data: 2.67e-05)max mem: 7.81234 GB 
[09/28 09:28:38 visual_prompt]: Inference (test):avg data time: 4.43e-05, avg batch time: 0.2042, average loss: 3.4066
[09/28 09:28:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.29	top5: 74.61	
[09/28 09:28:38 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/28 09:28:48 visual_prompt]: Epoch 84 / 100: avg data time: 1.05e-01, avg batch time: 0.5306, average train loss: 0.0097
[09/28 09:28:51 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1795, average loss: 0.0080
[09/28 09:28:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:29:13 visual_prompt]: 	Test 100/190. loss: 3.501, 0.2040 s / batch. (data: 3.17e-05)max mem: 7.81234 GB 
[09/28 09:29:33 visual_prompt]: Inference (test):avg data time: 2.92e-05, avg batch time: 0.2043, average loss: 3.4061
[09/28 09:29:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.30	top5: 74.61	
[09/28 09:29:33 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/28 09:29:42 visual_prompt]: Epoch 85 / 100: avg data time: 9.78e-02, avg batch time: 0.5237, average train loss: 0.0101
[09/28 09:29:45 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.1768, average loss: 0.0079
[09/28 09:29:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:30:07 visual_prompt]: 	Test 100/190. loss: 3.512, 0.2032 s / batch. (data: 2.48e-05)max mem: 7.81234 GB 
[09/28 09:30:27 visual_prompt]: Inference (test):avg data time: 2.84e-05, avg batch time: 0.2041, average loss: 3.4038
[09/28 09:30:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.23	top5: 74.76	
[09/28 09:30:27 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/28 09:30:36 visual_prompt]: Epoch 86 / 100: avg data time: 9.75e-02, avg batch time: 0.5241, average train loss: 0.0102
[09/28 09:30:40 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1752, average loss: 0.0079
[09/28 09:30:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:31:02 visual_prompt]: 	Test 100/190. loss: 3.507, 0.2037 s / batch. (data: 2.36e-05)max mem: 7.81234 GB 
[09/28 09:31:21 visual_prompt]: Inference (test):avg data time: 2.82e-05, avg batch time: 0.2041, average loss: 3.4027
[09/28 09:31:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.20	top5: 74.70	
[09/28 09:31:21 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/28 09:31:31 visual_prompt]: Epoch 87 / 100: avg data time: 1.01e-01, avg batch time: 0.5282, average train loss: 0.0099
[09/28 09:31:34 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1825, average loss: 0.0078
[09/28 09:31:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:31:56 visual_prompt]: 	Test 100/190. loss: 3.499, 0.2040 s / batch. (data: 2.84e-05)max mem: 7.81234 GB 
[09/28 09:32:15 visual_prompt]: Inference (test):avg data time: 2.98e-05, avg batch time: 0.2041, average loss: 3.4006
[09/28 09:32:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.15	top5: 74.75	
[09/28 09:32:16 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/28 09:32:25 visual_prompt]: Epoch 88 / 100: avg data time: 9.28e-02, avg batch time: 0.5206, average train loss: 0.0097
[09/28 09:32:28 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1773, average loss: 0.0077
[09/28 09:32:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:32:50 visual_prompt]: 	Test 100/190. loss: 3.493, 0.2034 s / batch. (data: 2.72e-05)max mem: 7.81234 GB 
[09/28 09:33:09 visual_prompt]: Inference (test):avg data time: 5.11e-05, avg batch time: 0.2042, average loss: 3.3995
[09/28 09:33:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.22	top5: 74.66	
[09/28 09:33:09 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/28 09:33:19 visual_prompt]: Epoch 89 / 100: avg data time: 1.09e-01, avg batch time: 0.5361, average train loss: 0.0100
[09/28 09:33:22 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1787, average loss: 0.0077
[09/28 09:33:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:33:44 visual_prompt]: 	Test 100/190. loss: 3.487, 0.2028 s / batch. (data: 3.15e-05)max mem: 7.81234 GB 
[09/28 09:34:04 visual_prompt]: Inference (test):avg data time: 2.85e-05, avg batch time: 0.2040, average loss: 3.3970
[09/28 09:34:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.33	top5: 74.66	
[09/28 09:34:04 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/28 09:34:13 visual_prompt]: Epoch 90 / 100: avg data time: 9.46e-02, avg batch time: 0.5197, average train loss: 0.0099
[09/28 09:34:16 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1809, average loss: 0.0077
[09/28 09:34:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:34:39 visual_prompt]: 	Test 100/190. loss: 3.487, 0.2034 s / batch. (data: 2.79e-05)max mem: 7.81234 GB 
[09/28 09:34:58 visual_prompt]: Inference (test):avg data time: 2.88e-05, avg batch time: 0.2041, average loss: 3.3938
[09/28 09:34:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.12	top5: 74.63	
[09/28 09:34:58 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/28 09:35:07 visual_prompt]: Epoch 91 / 100: avg data time: 9.65e-02, avg batch time: 0.5229, average train loss: 0.0098
[09/28 09:35:11 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1732, average loss: 0.0077
[09/28 09:35:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:35:33 visual_prompt]: 	Test 100/190. loss: 3.494, 0.2034 s / batch. (data: 2.84e-05)max mem: 7.81234 GB 
[09/28 09:35:52 visual_prompt]: Inference (test):avg data time: 1.90e-04, avg batch time: 0.2044, average loss: 3.3948
[09/28 09:35:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.19	top5: 74.63	
[09/28 09:35:52 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/28 09:36:02 visual_prompt]: Epoch 92 / 100: avg data time: 1.05e-01, avg batch time: 0.5311, average train loss: 0.0100
[09/28 09:36:05 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1776, average loss: 0.0077
[09/28 09:36:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:36:27 visual_prompt]: 	Test 100/190. loss: 3.495, 0.2034 s / batch. (data: 2.88e-05)max mem: 7.81234 GB 
[09/28 09:36:46 visual_prompt]: Inference (test):avg data time: 2.92e-05, avg batch time: 0.2041, average loss: 3.3951
[09/28 09:36:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.22	top5: 74.65	
[09/28 09:36:46 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/28 09:36:56 visual_prompt]: Epoch 93 / 100: avg data time: 1.01e-01, avg batch time: 0.5262, average train loss: 0.0100
[09/28 09:36:59 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1720, average loss: 0.0077
[09/28 09:36:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:37:21 visual_prompt]: 	Test 100/190. loss: 3.496, 0.2033 s / batch. (data: 2.53e-05)max mem: 7.81234 GB 
[09/28 09:37:41 visual_prompt]: Inference (test):avg data time: 2.88e-05, avg batch time: 0.2041, average loss: 3.3964
[09/28 09:37:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.27	top5: 74.64	
[09/28 09:37:41 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/28 09:37:50 visual_prompt]: Epoch 94 / 100: avg data time: 1.05e-01, avg batch time: 0.5312, average train loss: 0.0098
[09/28 09:37:53 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1782, average loss: 0.0077
[09/28 09:37:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:38:15 visual_prompt]: 	Test 100/190. loss: 3.498, 0.2051 s / batch. (data: 3.00e-05)max mem: 7.81234 GB 
[09/28 09:38:35 visual_prompt]: Inference (test):avg data time: 2.87e-05, avg batch time: 0.2044, average loss: 3.3970
[09/28 09:38:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.27	top5: 74.68	
[09/28 09:38:35 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/28 09:38:44 visual_prompt]: Epoch 95 / 100: avg data time: 1.02e-01, avg batch time: 0.5266, average train loss: 0.0099
[09/28 09:38:48 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1841, average loss: 0.0076
[09/28 09:38:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:39:10 visual_prompt]: 	Test 100/190. loss: 3.500, 0.2032 s / batch. (data: 7.32e-05)max mem: 7.81234 GB 
[09/28 09:39:29 visual_prompt]: Inference (test):avg data time: 3.03e-05, avg batch time: 0.2042, average loss: 3.3969
[09/28 09:39:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.22	top5: 74.67	
[09/28 09:39:29 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/28 09:39:39 visual_prompt]: Epoch 96 / 100: avg data time: 1.05e-01, avg batch time: 0.5310, average train loss: 0.0099
[09/28 09:39:42 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1848, average loss: 0.0076
[09/28 09:39:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:40:04 visual_prompt]: 	Test 100/190. loss: 3.499, 0.2033 s / batch. (data: 2.79e-05)max mem: 7.81234 GB 
[09/28 09:40:24 visual_prompt]: Inference (test):avg data time: 3.01e-05, avg batch time: 0.2043, average loss: 3.3968
[09/28 09:40:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.23	top5: 74.69	
[09/28 09:40:24 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/28 09:40:33 visual_prompt]: Epoch 97 / 100: avg data time: 9.92e-02, avg batch time: 0.5271, average train loss: 0.0098
[09/28 09:40:36 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1787, average loss: 0.0076
[09/28 09:40:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:40:58 visual_prompt]: 	Test 100/190. loss: 3.499, 0.2031 s / batch. (data: 2.79e-05)max mem: 7.81234 GB 
[09/28 09:41:18 visual_prompt]: Inference (test):avg data time: 2.92e-05, avg batch time: 0.2040, average loss: 3.3964
[09/28 09:41:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.22	top5: 74.68	
[09/28 09:41:18 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/28 09:41:27 visual_prompt]: Epoch 98 / 100: avg data time: 1.04e-01, avg batch time: 0.5314, average train loss: 0.0095
[09/28 09:41:31 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1734, average loss: 0.0076
[09/28 09:41:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:41:53 visual_prompt]: 	Test 100/190. loss: 3.499, 0.2037 s / batch. (data: 3.81e-05)max mem: 7.81234 GB 
[09/28 09:42:12 visual_prompt]: Inference (test):avg data time: 2.89e-05, avg batch time: 0.2044, average loss: 3.3962
[09/28 09:42:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.21	top5: 74.69	
[09/28 09:42:12 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/28 09:42:22 visual_prompt]: Epoch 99 / 100: avg data time: 1.00e-01, avg batch time: 0.5262, average train loss: 0.0096
[09/28 09:42:25 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1737, average loss: 0.0076
[09/28 09:42:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:42:47 visual_prompt]: 	Test 100/190. loss: 3.499, 0.2049 s / batch. (data: 2.77e-05)max mem: 7.81234 GB 
[09/28 09:43:06 visual_prompt]: Inference (test):avg data time: 4.68e-05, avg batch time: 0.2043, average loss: 3.3961
[09/28 09:43:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.21	top5: 74.69	
[09/28 09:43:06 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/28 09:43:16 visual_prompt]: Epoch 100 / 100: avg data time: 1.00e-01, avg batch time: 0.5268, average train loss: 0.0097
[09/28 09:43:19 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1814, average loss: 0.0076
[09/28 09:43:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 09:43:41 visual_prompt]: 	Test 100/190. loss: 3.499, 0.2042 s / batch. (data: 2.88e-05)max mem: 7.81234 GB 
[09/28 09:44:01 visual_prompt]: Inference (test):avg data time: 2.92e-05, avg batch time: 0.2041, average loss: 3.3961
[09/28 09:44:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 31.22	top5: 74.69	
[09/28 09:44:01 visual_prompt]: Rank of current process: 0. World size: 1
[09/28 09:44:01 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/28 09:44:01 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_azimuth")', 'DATA.NUMBER_CLASSES', '18', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/28 09:44:01 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/28 09:44:01 visual_prompt]: Training with config:
[09/28 09:44:01 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_azimuth")/sup_vitb16_imagenet21k/prompt50/crop224/test/seed7290/lr0.25_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 7290, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_azimuth")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 18, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/28 09:44:01 visual_prompt]: Loading training data...
[09/28 09:44:01 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset trainval...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800]+test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/28 09:44:02 visual_prompt]: Number of images: 1000
[09/28 09:44:02 visual_prompt]: Number of classes: 18 / 18
[09/28 09:44:02 visual_prompt]: Loading validation data...
[09/28 09:44:02 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/28 09:44:03 visual_prompt]: Number of images: 200
[09/28 09:44:03 visual_prompt]: Number of classes: 18 / 18
[09/28 09:44:03 visual_prompt]: Loading test data...
[09/28 09:44:03 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset test...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[50%:], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/28 09:44:18 visual_prompt]: Number of images: 12150
[09/28 09:44:18 visual_prompt]: Number of classes: 18 / 18
[09/28 09:44:18 visual_prompt]: Constructing models...
[09/28 09:44:20 visual_prompt]: Total Parameters: 86273298	 Gradient Parameters: 474642
[09/28 09:44:20 visual_prompt]: tuned percent:0.550
[09/28 09:44:20 visual_prompt]: Device used for model: 0
[09/28 09:44:20 visual_prompt]: Setting up Evaluator...
[09/28 09:44:20 visual_prompt]: Setting up Trainer...
[09/28 09:44:20 visual_prompt]: 	Setting up the optimizer...
[09/28 09:44:20 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/28 09:44:30 visual_prompt]: Epoch 1 / 100: avg data time: 1.02e-01, avg batch time: 0.5261, average train loss: 3.1439
[09/28 09:44:33 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1719, average loss: 3.1521
[09/28 09:44:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 30.50	
[09/28 09:44:55 visual_prompt]: 	Test 100/190. loss: 3.113, 0.2021 s / batch. (data: 2.50e-05)max mem: 7.81234 GB 
[09/28 09:45:14 visual_prompt]: Inference (test):avg data time: 2.83e-05, avg batch time: 0.2033, average loss: 3.1745
[09/28 09:45:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.47	top5: 27.81	
[09/28 09:45:14 visual_prompt]: Best epoch 1: best metric: 0.065
[09/28 09:45:14 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/28 09:45:24 visual_prompt]: Epoch 2 / 100: avg data time: 1.01e-01, avg batch time: 0.5258, average train loss: 2.9698
[09/28 09:45:27 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1843, average loss: 2.8836
[09/28 09:45:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 30.50	
[09/28 09:45:49 visual_prompt]: 	Test 100/190. loss: 2.914, 0.2020 s / batch. (data: 3.08e-05)max mem: 7.81234 GB 
[09/28 09:46:08 visual_prompt]: Inference (test):avg data time: 1.34e-04, avg batch time: 0.2037, average loss: 2.9102
[09/28 09:46:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 26.86	
[09/28 09:46:08 visual_prompt]: Best epoch 2: best metric: 0.090
[09/28 09:46:08 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/28 09:46:18 visual_prompt]: Epoch 3 / 100: avg data time: 1.01e-01, avg batch time: 0.5255, average train loss: 2.9030
[09/28 09:46:21 visual_prompt]: Inference (val):avg data time: 1.92e-05, avg batch time: 0.1728, average loss: 2.8890
[09/28 09:46:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.00	top5: 31.00	
[09/28 09:46:43 visual_prompt]: 	Test 100/190. loss: 2.891, 0.2036 s / batch. (data: 2.62e-05)max mem: 7.81234 GB 
[09/28 09:47:02 visual_prompt]: Inference (test):avg data time: 1.72e-04, avg batch time: 0.2039, average loss: 2.9030
[09/28 09:47:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.30	top5: 27.64	
[09/28 09:47:02 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/28 09:47:12 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e-01, avg batch time: 0.5263, average train loss: 2.8939
[09/28 09:47:15 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1799, average loss: 2.8871
[09/28 09:47:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 34.50	
[09/28 09:47:37 visual_prompt]: 	Test 100/190. loss: 2.902, 0.2034 s / batch. (data: 3.43e-05)max mem: 7.81234 GB 
[09/28 09:47:57 visual_prompt]: Inference (test):avg data time: 2.96e-05, avg batch time: 0.2039, average loss: 2.9088
[09/28 09:47:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 29.14	
[09/28 09:47:57 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/28 09:48:06 visual_prompt]: Epoch 5 / 100: avg data time: 1.01e-01, avg batch time: 0.5276, average train loss: 2.9024
[09/28 09:48:09 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1837, average loss: 2.8822
[09/28 09:48:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 33.00	
[09/28 09:48:31 visual_prompt]: 	Test 100/190. loss: 2.882, 0.2040 s / batch. (data: 2.86e-05)max mem: 7.81234 GB 
[09/28 09:48:51 visual_prompt]: Inference (test):avg data time: 2.88e-05, avg batch time: 0.2039, average loss: 2.8972
[09/28 09:48:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.84	top5: 28.79	
[09/28 09:48:51 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/28 09:49:00 visual_prompt]: Epoch 6 / 100: avg data time: 9.82e-02, avg batch time: 0.5244, average train loss: 2.8974
[09/28 09:49:04 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.1777, average loss: 2.8991
[09/28 09:49:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 31.00	
[09/28 09:49:26 visual_prompt]: 	Test 100/190. loss: 2.918, 0.2051 s / batch. (data: 2.91e-05)max mem: 7.81234 GB 
[09/28 09:49:45 visual_prompt]: Inference (test):avg data time: 2.85e-05, avg batch time: 0.2037, average loss: 2.9230
[09/28 09:49:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.88	top5: 28.46	
[09/28 09:49:45 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/28 09:49:55 visual_prompt]: Epoch 7 / 100: avg data time: 1.01e-01, avg batch time: 0.5262, average train loss: 2.9123
[09/28 09:49:58 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1781, average loss: 2.8741
[09/28 09:49:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 34.50	
[09/28 09:50:20 visual_prompt]: 	Test 100/190. loss: 2.900, 0.2038 s / batch. (data: 2.26e-05)max mem: 7.81234 GB 
[09/28 09:50:39 visual_prompt]: Inference (test):avg data time: 2.76e-05, avg batch time: 0.2039, average loss: 2.8973
[09/28 09:50:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.45	top5: 28.37	
[09/28 09:50:39 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/28 09:50:49 visual_prompt]: Epoch 8 / 100: avg data time: 9.90e-02, avg batch time: 0.5248, average train loss: 2.8981
[09/28 09:50:52 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1842, average loss: 2.8727
[09/28 09:50:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.50	top5: 36.50	
[09/28 09:51:14 visual_prompt]: 	Test 100/190. loss: 2.923, 0.2041 s / batch. (data: 2.74e-05)max mem: 7.81234 GB 
[09/28 09:51:34 visual_prompt]: Inference (test):avg data time: 2.82e-05, avg batch time: 0.2040, average loss: 2.9178
[09/28 09:51:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.82	top5: 30.34	
[09/28 09:51:34 visual_prompt]: Best epoch 8: best metric: 0.105
[09/28 09:51:34 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/28 09:51:43 visual_prompt]: Epoch 9 / 100: avg data time: 1.09e-01, avg batch time: 0.5340, average train loss: 2.8994
[09/28 09:51:46 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1846, average loss: 2.8405
[09/28 09:51:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 39.50	
[09/28 09:52:08 visual_prompt]: 	Test 100/190. loss: 2.898, 0.2044 s / batch. (data: 2.86e-05)max mem: 7.81234 GB 
[09/28 09:52:28 visual_prompt]: Inference (test):avg data time: 2.76e-05, avg batch time: 0.2037, average loss: 2.9183
[09/28 09:52:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.54	top5: 31.36	
[09/28 09:52:28 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/28 09:52:37 visual_prompt]: Epoch 10 / 100: avg data time: 8.91e-02, avg batch time: 0.5141, average train loss: 2.8832
[09/28 09:52:40 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1759, average loss: 2.8679
[09/28 09:52:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 37.50	
[09/28 09:53:02 visual_prompt]: 	Test 100/190. loss: 2.905, 0.2033 s / batch. (data: 2.79e-05)max mem: 7.81234 GB 
[09/28 09:53:22 visual_prompt]: Inference (test):avg data time: 2.84e-05, avg batch time: 0.2042, average loss: 2.9167
[09/28 09:53:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.78	top5: 31.93	
[09/28 09:53:22 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/28 09:53:31 visual_prompt]: Epoch 11 / 100: avg data time: 1.07e-01, avg batch time: 0.5309, average train loss: 2.8848
[09/28 09:53:35 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1730, average loss: 2.9095
[09/28 09:53:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 32.50	
[09/28 09:53:57 visual_prompt]: 	Test 100/190. loss: 2.903, 0.2030 s / batch. (data: 2.67e-05)max mem: 7.81234 GB 
[09/28 09:54:16 visual_prompt]: Inference (test):avg data time: 1.54e-04, avg batch time: 0.2041, average loss: 2.9093
[09/28 09:54:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.24	top5: 34.48	
[09/28 09:54:16 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/28 09:54:25 visual_prompt]: Epoch 12 / 100: avg data time: 8.98e-02, avg batch time: 0.5153, average train loss: 2.8568
[09/28 09:54:29 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1715, average loss: 2.8386
[09/28 09:54:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.50	top5: 38.00	
[09/28 09:54:51 visual_prompt]: 	Test 100/190. loss: 2.876, 0.2030 s / batch. (data: 2.53e-05)max mem: 7.81234 GB 
[09/28 09:55:10 visual_prompt]: Inference (test):avg data time: 2.86e-05, avg batch time: 0.2039, average loss: 2.8903
[09/28 09:55:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.51	top5: 36.86	
[09/28 09:55:10 visual_prompt]: Best epoch 12: best metric: 0.115
[09/28 09:55:10 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/28 09:55:19 visual_prompt]: Epoch 13 / 100: avg data time: 9.42e-02, avg batch time: 0.5180, average train loss: 2.8228
[09/28 09:55:23 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1731, average loss: 2.9304
[09/28 09:55:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 37.00	
[09/28 09:55:45 visual_prompt]: 	Test 100/190. loss: 3.111, 0.2047 s / batch. (data: 2.74e-05)max mem: 7.81234 GB 
[09/28 09:56:04 visual_prompt]: Inference (test):avg data time: 2.90e-05, avg batch time: 0.2039, average loss: 2.9881
[09/28 09:56:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.08	top5: 34.77	
[09/28 09:56:04 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/28 09:56:14 visual_prompt]: Epoch 14 / 100: avg data time: 9.90e-02, avg batch time: 0.5234, average train loss: 2.7788
[09/28 09:56:17 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1723, average loss: 2.6828
[09/28 09:56:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 46.50	
[09/28 09:56:39 visual_prompt]: 	Test 100/190. loss: 2.847, 0.2044 s / batch. (data: 2.74e-05)max mem: 7.81234 GB 
[09/28 09:56:59 visual_prompt]: Inference (test):avg data time: 1.43e-04, avg batch time: 0.2042, average loss: 2.7990
[09/28 09:56:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.62	top5: 39.74	
[09/28 09:56:59 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/28 09:57:08 visual_prompt]: Epoch 15 / 100: avg data time: 1.05e-01, avg batch time: 0.5304, average train loss: 2.7429
[09/28 09:57:11 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1839, average loss: 2.8356
[09/28 09:57:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 40.50	
[09/28 09:57:33 visual_prompt]: 	Test 100/190. loss: 2.800, 0.2039 s / batch. (data: 2.77e-05)max mem: 7.81234 GB 
[09/28 09:57:53 visual_prompt]: Inference (test):avg data time: 2.77e-05, avg batch time: 0.2040, average loss: 2.8879
[09/28 09:57:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.35	top5: 36.66	
[09/28 09:57:53 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/28 09:58:02 visual_prompt]: Epoch 16 / 100: avg data time: 1.01e-01, avg batch time: 0.5260, average train loss: 2.7170
[09/28 09:58:05 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1814, average loss: 2.6769
[09/28 09:58:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 50.00	
[09/28 09:58:28 visual_prompt]: 	Test 100/190. loss: 2.720, 0.2029 s / batch. (data: 2.41e-05)max mem: 7.81234 GB 
[09/28 09:58:47 visual_prompt]: Inference (test):avg data time: 9.01e-05, avg batch time: 0.2042, average loss: 2.7908
[09/28 09:58:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.69	top5: 42.30	
[09/28 09:58:47 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/28 09:58:56 visual_prompt]: Epoch 17 / 100: avg data time: 9.03e-02, avg batch time: 0.5177, average train loss: 2.6532
[09/28 09:59:00 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1786, average loss: 2.5500
[09/28 09:59:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.50	top5: 57.50	
[09/28 09:59:22 visual_prompt]: 	Test 100/190. loss: 2.734, 0.2038 s / batch. (data: 2.62e-05)max mem: 7.81234 GB 
[09/28 09:59:41 visual_prompt]: Inference (test):avg data time: 1.36e-04, avg batch time: 0.2042, average loss: 2.7439
[09/28 09:59:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.51	top5: 47.09	
[09/28 09:59:41 visual_prompt]: Best epoch 17: best metric: 0.175
[09/28 09:59:41 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/28 09:59:51 visual_prompt]: Epoch 18 / 100: avg data time: 1.05e-01, avg batch time: 0.5315, average train loss: 2.5351
[09/28 09:59:54 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1741, average loss: 3.0129
[09/28 09:59:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 38.50	
[09/28 10:00:16 visual_prompt]: 	Test 100/190. loss: 2.944, 0.2043 s / batch. (data: 4.86e-05)max mem: 7.81234 GB 
[09/28 10:00:36 visual_prompt]: Inference (test):avg data time: 2.89e-05, avg batch time: 0.2043, average loss: 3.2212
[09/28 10:00:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.71	top5: 36.37	
[09/28 10:00:36 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/28 10:00:45 visual_prompt]: Epoch 19 / 100: avg data time: 1.09e-01, avg batch time: 0.5348, average train loss: 2.5400
[09/28 10:00:48 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1585, average loss: 2.6502
[09/28 10:00:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.00	top5: 59.00	
[09/28 10:01:10 visual_prompt]: 	Test 100/190. loss: 2.900, 0.2030 s / batch. (data: 2.60e-05)max mem: 7.81234 GB 
[09/28 10:01:30 visual_prompt]: Inference (test):avg data time: 4.61e-05, avg batch time: 0.2043, average loss: 2.8852
[09/28 10:01:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.41	top5: 48.10	
[09/28 10:01:30 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/28 10:01:40 visual_prompt]: Epoch 20 / 100: avg data time: 1.06e-01, avg batch time: 0.5312, average train loss: 2.4101
[09/28 10:01:43 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1819, average loss: 2.2748
[09/28 10:01:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.00	top5: 74.00	
[09/28 10:02:05 visual_prompt]: 	Test 100/190. loss: 2.620, 0.2034 s / batch. (data: 2.36e-05)max mem: 7.81234 GB 
[09/28 10:02:24 visual_prompt]: Inference (test):avg data time: 2.83e-05, avg batch time: 0.2041, average loss: 2.5897
[09/28 10:02:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.68	top5: 60.21	
[09/28 10:02:24 visual_prompt]: Best epoch 20: best metric: 0.250
[09/28 10:02:24 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/28 10:02:34 visual_prompt]: Epoch 21 / 100: avg data time: 1.06e-01, avg batch time: 0.5323, average train loss: 2.2596
[09/28 10:02:37 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1735, average loss: 2.4672
[09/28 10:02:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 18.00	top5: 64.50	
[09/28 10:02:59 visual_prompt]: 	Test 100/190. loss: 2.585, 0.2023 s / batch. (data: 2.62e-05)max mem: 7.81234 GB 
[09/28 10:03:19 visual_prompt]: Inference (test):avg data time: 2.81e-05, avg batch time: 0.2044, average loss: 2.6957
[09/28 10:03:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.15	top5: 55.84	
[09/28 10:03:19 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/28 10:03:28 visual_prompt]: Epoch 22 / 100: avg data time: 9.99e-02, avg batch time: 0.5253, average train loss: 2.2123
[09/28 10:03:31 visual_prompt]: Inference (val):avg data time: 1.73e-05, avg batch time: 0.1815, average loss: 2.2908
[09/28 10:03:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.00	top5: 70.00	
[09/28 10:03:53 visual_prompt]: 	Test 100/190. loss: 2.574, 0.2030 s / batch. (data: 2.65e-05)max mem: 7.81234 GB 
[09/28 10:04:13 visual_prompt]: Inference (test):avg data time: 2.86e-05, avg batch time: 0.2042, average loss: 2.6074
[09/28 10:04:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.88	top5: 60.44	
[09/28 10:04:13 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/28 10:04:23 visual_prompt]: Epoch 23 / 100: avg data time: 1.03e-01, avg batch time: 0.5281, average train loss: 2.1084
[09/28 10:04:26 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1779, average loss: 2.0937
[09/28 10:04:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.00	top5: 80.50	
[09/28 10:04:48 visual_prompt]: 	Test 100/190. loss: 2.389, 0.2032 s / batch. (data: 4.67e-05)max mem: 7.81234 GB 
[09/28 10:05:08 visual_prompt]: Inference (test):avg data time: 1.66e-04, avg batch time: 0.2043, average loss: 2.5131
[09/28 10:05:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.35	top5: 65.93	
[09/28 10:05:08 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/28 10:05:17 visual_prompt]: Epoch 24 / 100: avg data time: 9.10e-02, avg batch time: 0.5178, average train loss: 1.9204
[09/28 10:05:20 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1759, average loss: 2.1663
[09/28 10:05:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.50	top5: 79.00	
[09/28 10:05:42 visual_prompt]: 	Test 100/190. loss: 2.923, 0.2029 s / batch. (data: 2.93e-05)max mem: 7.81234 GB 
[09/28 10:06:02 visual_prompt]: Inference (test):avg data time: 1.32e-04, avg batch time: 0.2041, average loss: 2.7624
[09/28 10:06:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.31	top5: 64.91	
[09/28 10:06:02 visual_prompt]: Best epoch 24: best metric: 0.275
[09/28 10:06:02 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/28 10:06:11 visual_prompt]: Epoch 25 / 100: avg data time: 1.00e-01, avg batch time: 0.5251, average train loss: 1.8631
[09/28 10:06:14 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1772, average loss: 1.9757
[09/28 10:06:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.00	top5: 81.50	
[09/28 10:06:37 visual_prompt]: 	Test 100/190. loss: 2.570, 0.2038 s / batch. (data: 3.00e-05)max mem: 7.81234 GB 
[09/28 10:06:56 visual_prompt]: Inference (test):avg data time: 2.84e-05, avg batch time: 0.2045, average loss: 2.6837
[09/28 10:06:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.29	top5: 67.14	
[09/28 10:06:56 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/28 10:07:06 visual_prompt]: Epoch 26 / 100: avg data time: 9.89e-02, avg batch time: 0.5261, average train loss: 1.8133
[09/28 10:07:09 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1835, average loss: 1.9673
[09/28 10:07:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 34.00	top5: 85.00	
[09/28 10:07:31 visual_prompt]: 	Test 100/190. loss: 2.870, 0.2040 s / batch. (data: 2.91e-05)max mem: 7.81234 GB 
[09/28 10:07:51 visual_prompt]: Inference (test):avg data time: 2.83e-05, avg batch time: 0.2041, average loss: 2.7488
[09/28 10:07:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.48	top5: 67.24	
[09/28 10:07:51 visual_prompt]: Best epoch 26: best metric: 0.340
[09/28 10:07:51 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/28 10:08:00 visual_prompt]: Epoch 27 / 100: avg data time: 9.62e-02, avg batch time: 0.5215, average train loss: 1.6779
[09/28 10:08:03 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1818, average loss: 1.8310
[09/28 10:08:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 36.00	top5: 85.00	
[09/28 10:08:25 visual_prompt]: 	Test 100/190. loss: 2.814, 0.2030 s / batch. (data: 2.62e-05)max mem: 7.81234 GB 
[09/28 10:08:45 visual_prompt]: Inference (test):avg data time: 2.81e-05, avg batch time: 0.2041, average loss: 2.7584
[09/28 10:08:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.09	top5: 66.76	
[09/28 10:08:45 visual_prompt]: Best epoch 27: best metric: 0.360
[09/28 10:08:45 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/28 10:08:54 visual_prompt]: Epoch 28 / 100: avg data time: 8.96e-02, avg batch time: 0.5156, average train loss: 1.5677
[09/28 10:08:58 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1777, average loss: 1.6244
[09/28 10:08:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 44.50	top5: 89.50	
[09/28 10:09:20 visual_prompt]: 	Test 100/190. loss: 2.956, 0.2044 s / batch. (data: 2.57e-05)max mem: 7.81234 GB 
[09/28 10:09:39 visual_prompt]: Inference (test):avg data time: 2.89e-05, avg batch time: 0.2041, average loss: 2.8978
[09/28 10:09:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 20.31	top5: 68.49	
[09/28 10:09:39 visual_prompt]: Best epoch 28: best metric: 0.445
[09/28 10:09:39 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/28 10:09:49 visual_prompt]: Epoch 29 / 100: avg data time: 1.07e-01, avg batch time: 0.5337, average train loss: 1.4213
[09/28 10:09:52 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1840, average loss: 1.5572
[09/28 10:09:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 40.00	top5: 93.00	
[09/28 10:10:14 visual_prompt]: 	Test 100/190. loss: 3.164, 0.2057 s / batch. (data: 2.86e-05)max mem: 7.81234 GB 
[09/28 10:10:34 visual_prompt]: Inference (test):avg data time: 2.86e-05, avg batch time: 0.2043, average loss: 2.9630
[09/28 10:10:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.55	top5: 70.12	
[09/28 10:10:34 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/28 10:10:43 visual_prompt]: Epoch 30 / 100: avg data time: 1.07e-01, avg batch time: 0.5343, average train loss: 1.3528
[09/28 10:10:46 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1764, average loss: 1.5904
[09/28 10:10:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 42.00	top5: 93.00	
[09/28 10:11:08 visual_prompt]: 	Test 100/190. loss: 3.421, 0.2036 s / batch. (data: 2.69e-05)max mem: 7.81234 GB 
[09/28 10:11:28 visual_prompt]: Inference (test):avg data time: 1.08e-04, avg batch time: 0.2042, average loss: 3.0957
[09/28 10:11:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.98	top5: 68.06	
[09/28 10:11:28 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/28 10:11:37 visual_prompt]: Epoch 31 / 100: avg data time: 9.56e-02, avg batch time: 0.5209, average train loss: 1.3566
[09/28 10:11:41 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1750, average loss: 1.3228
[09/28 10:11:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 52.00	top5: 96.00	
[09/28 10:12:03 visual_prompt]: 	Test 100/190. loss: 3.069, 0.2036 s / batch. (data: 3.27e-05)max mem: 7.81234 GB 
[09/28 10:12:22 visual_prompt]: Inference (test):avg data time: 2.82e-05, avg batch time: 0.2041, average loss: 3.1042
[09/28 10:12:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.09	top5: 68.18	
[09/28 10:12:22 visual_prompt]: Best epoch 31: best metric: 0.520
[09/28 10:12:22 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/28 10:12:32 visual_prompt]: Epoch 32 / 100: avg data time: 1.01e-01, avg batch time: 0.5277, average train loss: 1.1840
[09/28 10:12:35 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1722, average loss: 1.0906
[09/28 10:12:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 59.50	top5: 97.00	
[09/28 10:12:57 visual_prompt]: 	Test 100/190. loss: 3.182, 0.2120 s / batch. (data: 2.53e-05)max mem: 7.81234 GB 
[09/28 10:13:16 visual_prompt]: Inference (test):avg data time: 6.22e-05, avg batch time: 0.2041, average loss: 2.9508
[09/28 10:13:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 22.52	top5: 72.45	
[09/28 10:13:17 visual_prompt]: Best epoch 32: best metric: 0.595
[09/28 10:13:17 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/28 10:13:26 visual_prompt]: Epoch 33 / 100: avg data time: 9.73e-02, avg batch time: 0.5219, average train loss: 1.0244
[09/28 10:13:29 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1822, average loss: 1.1029
[09/28 10:13:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 60.50	top5: 96.50	
[09/28 10:13:51 visual_prompt]: 	Test 100/190. loss: 3.120, 0.2055 s / batch. (data: 2.43e-05)max mem: 7.81234 GB 
[09/28 10:14:11 visual_prompt]: Inference (test):avg data time: 2.85e-05, avg batch time: 0.2041, average loss: 3.0718
[09/28 10:14:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 23.38	top5: 70.84	
[09/28 10:14:11 visual_prompt]: Best epoch 33: best metric: 0.605
[09/28 10:14:11 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/28 10:14:20 visual_prompt]: Epoch 34 / 100: avg data time: 8.95e-02, avg batch time: 0.5168, average train loss: 0.8629
[09/28 10:14:23 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1745, average loss: 0.7745
[09/28 10:14:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 73.00	top5: 99.00	
[09/28 10:14:45 visual_prompt]: 	Test 100/190. loss: 3.273, 0.2045 s / batch. (data: 2.79e-05)max mem: 7.81234 GB 
[09/28 10:15:05 visual_prompt]: Inference (test):avg data time: 2.86e-05, avg batch time: 0.2043, average loss: 3.2310
[09/28 10:15:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 24.00	top5: 71.72	
[09/28 10:15:05 visual_prompt]: Best epoch 34: best metric: 0.730
[09/28 10:15:05 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/28 10:15:14 visual_prompt]: Epoch 35 / 100: avg data time: 8.93e-02, avg batch time: 0.5175, average train loss: 0.7635
[09/28 10:15:17 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1832, average loss: 0.6717
[09/28 10:15:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 78.00	top5: 100.00	
[09/28 10:15:39 visual_prompt]: 	Test 100/190. loss: 2.989, 0.2042 s / batch. (data: 2.74e-05)max mem: 7.81234 GB 
[09/28 10:15:59 visual_prompt]: Inference (test):avg data time: 2.86e-05, avg batch time: 0.2042, average loss: 3.0585
[09/28 10:15:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 24.50	top5: 72.07	
[09/28 10:15:59 visual_prompt]: Best epoch 35: best metric: 0.780
[09/28 10:15:59 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/28 10:16:08 visual_prompt]: Epoch 36 / 100: avg data time: 1.06e-01, avg batch time: 0.5320, average train loss: 0.5934
[09/28 10:16:12 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1761, average loss: 0.5140
[09/28 10:16:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 83.00	top5: 100.00	
[09/28 10:16:33 visual_prompt]: 	Test 100/190. loss: 3.780, 0.2033 s / batch. (data: 2.72e-05)max mem: 7.81234 GB 
[09/28 10:16:53 visual_prompt]: Inference (test):avg data time: 2.83e-05, avg batch time: 0.2040, average loss: 3.4369
[09/28 10:16:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 24.19	top5: 71.14	
[09/28 10:16:53 visual_prompt]: Best epoch 36: best metric: 0.830
[09/28 10:16:53 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/28 10:17:03 visual_prompt]: Epoch 37 / 100: avg data time: 1.04e-01, avg batch time: 0.5313, average train loss: 0.5095
[09/28 10:17:06 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1738, average loss: 0.4370
[09/28 10:17:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 86.50	top5: 100.00	
[09/28 10:17:28 visual_prompt]: 	Test 100/190. loss: 3.484, 0.2037 s / batch. (data: 2.41e-05)max mem: 7.81234 GB 
[09/28 10:17:47 visual_prompt]: Inference (test):avg data time: 2.89e-05, avg batch time: 0.2042, average loss: 3.2693
[09/28 10:17:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.99	top5: 72.36	
[09/28 10:17:47 visual_prompt]: Best epoch 37: best metric: 0.865
[09/28 10:17:47 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/28 10:17:57 visual_prompt]: Epoch 38 / 100: avg data time: 9.74e-02, avg batch time: 0.5214, average train loss: 0.4086
[09/28 10:18:00 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1801, average loss: 0.4220
[09/28 10:18:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 89.00	top5: 99.50	
[09/28 10:18:22 visual_prompt]: 	Test 100/190. loss: 3.474, 0.2032 s / batch. (data: 2.86e-05)max mem: 7.81234 GB 
[09/28 10:18:42 visual_prompt]: Inference (test):avg data time: 7.68e-05, avg batch time: 0.2040, average loss: 3.3735
[09/28 10:18:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 24.40	top5: 71.48	
[09/28 10:18:42 visual_prompt]: Best epoch 38: best metric: 0.890
[09/28 10:18:42 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/28 10:18:51 visual_prompt]: Epoch 39 / 100: avg data time: 1.03e-01, avg batch time: 0.5295, average train loss: 0.3969
[09/28 10:18:54 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1793, average loss: 0.3701
[09/28 10:18:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 89.50	top5: 100.00	
[09/28 10:19:16 visual_prompt]: 	Test 100/190. loss: 3.367, 0.2037 s / batch. (data: 2.55e-05)max mem: 7.81234 GB 
[09/28 10:19:36 visual_prompt]: Inference (test):avg data time: 2.77e-05, avg batch time: 0.2042, average loss: 3.3848
[09/28 10:19:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 24.93	top5: 72.11	
[09/28 10:19:36 visual_prompt]: Best epoch 39: best metric: 0.895
[09/28 10:19:36 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/28 10:19:46 visual_prompt]: Epoch 40 / 100: avg data time: 1.05e-01, avg batch time: 0.5333, average train loss: 0.3260
[09/28 10:19:49 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1837, average loss: 0.3552
[09/28 10:19:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 90.00	top5: 100.00	
[09/28 10:20:11 visual_prompt]: 	Test 100/190. loss: 3.700, 0.2036 s / batch. (data: 2.72e-05)max mem: 7.81234 GB 
[09/28 10:20:30 visual_prompt]: Inference (test):avg data time: 2.85e-05, avg batch time: 0.2042, average loss: 3.3923
[09/28 10:20:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.51	top5: 71.02	
[09/28 10:20:30 visual_prompt]: Best epoch 40: best metric: 0.900
[09/28 10:20:30 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/28 10:20:40 visual_prompt]: Epoch 41 / 100: avg data time: 8.43e-02, avg batch time: 0.5103, average train loss: 0.3083
[09/28 10:20:43 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1732, average loss: 0.3247
[09/28 10:20:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 90.50	top5: 100.00	
[09/28 10:21:05 visual_prompt]: 	Test 100/190. loss: 3.754, 0.2037 s / batch. (data: 4.89e-05)max mem: 7.81234 GB 
[09/28 10:21:25 visual_prompt]: Inference (test):avg data time: 3.02e-05, avg batch time: 0.2041, average loss: 3.4185
[09/28 10:21:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.46	top5: 72.25	
[09/28 10:21:25 visual_prompt]: Best epoch 41: best metric: 0.905
[09/28 10:21:25 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/28 10:21:34 visual_prompt]: Epoch 42 / 100: avg data time: 8.86e-02, avg batch time: 0.5167, average train loss: 0.2650
[09/28 10:21:37 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1823, average loss: 0.2199
[09/28 10:21:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 95.50	top5: 100.00	
[09/28 10:21:59 visual_prompt]: 	Test 100/190. loss: 3.643, 0.2036 s / batch. (data: 2.77e-05)max mem: 7.81234 GB 
[09/28 10:22:19 visual_prompt]: Inference (test):avg data time: 3.56e-05, avg batch time: 0.2040, average loss: 3.3931
[09/28 10:22:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 26.86	top5: 72.70	
[09/28 10:22:19 visual_prompt]: Best epoch 42: best metric: 0.955
[09/28 10:22:19 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/28 10:22:28 visual_prompt]: Epoch 43 / 100: avg data time: 1.05e-01, avg batch time: 0.5321, average train loss: 0.2212
[09/28 10:22:32 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1849, average loss: 0.1873
[09/28 10:22:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 96.00	top5: 100.00	
[09/28 10:22:54 visual_prompt]: 	Test 100/190. loss: 3.860, 0.2030 s / batch. (data: 7.72e-05)max mem: 7.81234 GB 
[09/28 10:23:13 visual_prompt]: Inference (test):avg data time: 9.69e-05, avg batch time: 0.2043, average loss: 3.3858
[09/28 10:23:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.63	top5: 73.32	
[09/28 10:23:13 visual_prompt]: Best epoch 43: best metric: 0.960
[09/28 10:23:13 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/28 10:23:23 visual_prompt]: Epoch 44 / 100: avg data time: 9.86e-02, avg batch time: 0.5253, average train loss: 0.1789
[09/28 10:23:26 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1841, average loss: 0.1726
[09/28 10:23:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 95.00	top5: 100.00	
[09/28 10:23:48 visual_prompt]: 	Test 100/190. loss: 4.259, 0.2041 s / batch. (data: 9.68e-05)max mem: 7.81234 GB 
[09/28 10:24:08 visual_prompt]: Inference (test):avg data time: 2.87e-05, avg batch time: 0.2041, average loss: 3.5618
[09/28 10:24:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.14	top5: 72.52	
[09/28 10:24:08 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/28 10:24:17 visual_prompt]: Epoch 45 / 100: avg data time: 1.05e-01, avg batch time: 0.5312, average train loss: 0.1560
[09/28 10:24:20 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1768, average loss: 0.1502
[09/28 10:24:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 96.00	top5: 99.50	
[09/28 10:24:42 visual_prompt]: 	Test 100/190. loss: 4.447, 0.2035 s / batch. (data: 2.69e-05)max mem: 7.81234 GB 
[09/28 10:25:02 visual_prompt]: Inference (test):avg data time: 2.81e-05, avg batch time: 0.2043, average loss: 3.5344
[09/28 10:25:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.21	top5: 72.76	
[09/28 10:25:02 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/28 10:25:11 visual_prompt]: Epoch 46 / 100: avg data time: 1.06e-01, avg batch time: 0.5319, average train loss: 0.1227
[09/28 10:25:15 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1775, average loss: 0.0609
[09/28 10:25:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:25:36 visual_prompt]: 	Test 100/190. loss: 3.593, 0.2033 s / batch. (data: 2.55e-05)max mem: 7.81234 GB 
[09/28 10:25:56 visual_prompt]: Inference (test):avg data time: 8.62e-05, avg batch time: 0.2043, average loss: 3.5330
[09/28 10:25:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.87	top5: 72.91	
[09/28 10:25:56 visual_prompt]: Best epoch 46: best metric: 1.000
[09/28 10:25:56 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/28 10:26:05 visual_prompt]: Epoch 47 / 100: avg data time: 9.48e-02, avg batch time: 0.5211, average train loss: 0.0944
[09/28 10:26:09 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1788, average loss: 0.0675
[09/28 10:26:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:26:31 visual_prompt]: 	Test 100/190. loss: 4.055, 0.2042 s / batch. (data: 2.67e-05)max mem: 7.81234 GB 
[09/28 10:26:51 visual_prompt]: Inference (test):avg data time: 1.21e-04, avg batch time: 0.2041, average loss: 3.5273
[09/28 10:26:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.74	top5: 72.58	
[09/28 10:26:51 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/28 10:27:00 visual_prompt]: Epoch 48 / 100: avg data time: 1.10e-01, avg batch time: 0.5345, average train loss: 0.0877
[09/28 10:27:03 visual_prompt]: Inference (val):avg data time: 1.91e-05, avg batch time: 0.1791, average loss: 0.0905
[09/28 10:27:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.50	top5: 100.00	
[09/28 10:27:25 visual_prompt]: 	Test 100/190. loss: 3.727, 0.2041 s / batch. (data: 2.74e-05)max mem: 7.81234 GB 
[09/28 10:27:45 visual_prompt]: Inference (test):avg data time: 2.88e-05, avg batch time: 0.2041, average loss: 3.5864
[09/28 10:27:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.59	top5: 73.05	
[09/28 10:27:45 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/28 10:27:54 visual_prompt]: Epoch 49 / 100: avg data time: 1.00e-01, avg batch time: 0.5255, average train loss: 0.0799
[09/28 10:27:58 visual_prompt]: Inference (val):avg data time: 1.87e-05, avg batch time: 0.1716, average loss: 0.0533
[09/28 10:27:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:28:20 visual_prompt]: 	Test 100/190. loss: 4.036, 0.2028 s / batch. (data: 2.81e-05)max mem: 7.81234 GB 
[09/28 10:28:39 visual_prompt]: Inference (test):avg data time: 1.51e-04, avg batch time: 0.2043, average loss: 3.5633
[09/28 10:28:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.40	top5: 72.96	
[09/28 10:28:39 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/28 10:28:49 visual_prompt]: Epoch 50 / 100: avg data time: 9.90e-02, avg batch time: 0.5247, average train loss: 0.0743
[09/28 10:28:52 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1813, average loss: 0.0529
[09/28 10:28:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.00	top5: 100.00	
[09/28 10:29:14 visual_prompt]: 	Test 100/190. loss: 3.958, 0.2038 s / batch. (data: 3.15e-05)max mem: 7.81234 GB 
[09/28 10:29:34 visual_prompt]: Inference (test):avg data time: 1.11e-04, avg batch time: 0.2044, average loss: 3.5297
[09/28 10:29:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.64	top5: 73.39	
[09/28 10:29:34 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/28 10:29:43 visual_prompt]: Epoch 51 / 100: avg data time: 1.06e-01, avg batch time: 0.5309, average train loss: 0.0788
[09/28 10:29:46 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1820, average loss: 0.0968
[09/28 10:29:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 98.50	top5: 100.00	
[09/28 10:30:08 visual_prompt]: 	Test 100/190. loss: 3.991, 0.2032 s / batch. (data: 3.19e-05)max mem: 7.81234 GB 
[09/28 10:30:28 visual_prompt]: Inference (test):avg data time: 2.84e-05, avg batch time: 0.2041, average loss: 3.5514
[09/28 10:30:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.64	top5: 72.02	
[09/28 10:30:28 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/28 10:30:37 visual_prompt]: Epoch 52 / 100: avg data time: 1.06e-01, avg batch time: 0.5318, average train loss: 0.0731
[09/28 10:30:41 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1851, average loss: 0.0309
[09/28 10:30:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:31:03 visual_prompt]: 	Test 100/190. loss: 3.806, 0.2032 s / batch. (data: 2.74e-05)max mem: 7.81234 GB 
[09/28 10:31:22 visual_prompt]: Inference (test):avg data time: 4.84e-05, avg batch time: 0.2039, average loss: 3.5132
[09/28 10:31:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.41	top5: 73.18	
[09/28 10:31:22 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/28 10:31:32 visual_prompt]: Epoch 53 / 100: avg data time: 1.04e-01, avg batch time: 0.5308, average train loss: 0.0466
[09/28 10:31:35 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1723, average loss: 0.0332
[09/28 10:31:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:31:57 visual_prompt]: 	Test 100/190. loss: 3.945, 0.2043 s / batch. (data: 2.53e-05)max mem: 7.81234 GB 
[09/28 10:32:17 visual_prompt]: Inference (test):avg data time: 2.87e-05, avg batch time: 0.2040, average loss: 3.5532
[09/28 10:32:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.94	top5: 73.84	
[09/28 10:32:17 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/28 10:32:26 visual_prompt]: Epoch 54 / 100: avg data time: 1.04e-01, avg batch time: 0.5297, average train loss: 0.0375
[09/28 10:32:29 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1841, average loss: 0.0373
[09/28 10:32:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.50	top5: 100.00	
[09/28 10:32:51 visual_prompt]: 	Test 100/190. loss: 3.975, 0.2052 s / batch. (data: 4.63e-05)max mem: 7.81234 GB 
[09/28 10:33:11 visual_prompt]: Inference (test):avg data time: 2.87e-05, avg batch time: 0.2040, average loss: 3.5361
[09/28 10:33:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.70	top5: 73.40	
[09/28 10:33:11 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/28 10:33:20 visual_prompt]: Epoch 55 / 100: avg data time: 1.01e-01, avg batch time: 0.5268, average train loss: 0.0292
[09/28 10:33:24 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1747, average loss: 0.0270
[09/28 10:33:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:33:46 visual_prompt]: 	Test 100/190. loss: 3.959, 0.2042 s / batch. (data: 2.86e-05)max mem: 7.81234 GB 
[09/28 10:34:05 visual_prompt]: Inference (test):avg data time: 2.84e-05, avg batch time: 0.2042, average loss: 3.6102
[09/28 10:34:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.21	top5: 73.16	
[09/28 10:34:05 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/28 10:34:15 visual_prompt]: Epoch 56 / 100: avg data time: 9.61e-02, avg batch time: 0.5216, average train loss: 0.0283
[09/28 10:34:18 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1736, average loss: 0.0288
[09/28 10:34:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:34:40 visual_prompt]: 	Test 100/190. loss: 3.795, 0.2033 s / batch. (data: 2.43e-05)max mem: 7.81234 GB 
[09/28 10:34:59 visual_prompt]: Inference (test):avg data time: 2.84e-05, avg batch time: 0.2042, average loss: 3.5560
[09/28 10:34:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.33	top5: 73.51	
[09/28 10:34:59 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/28 10:35:09 visual_prompt]: Epoch 57 / 100: avg data time: 1.02e-01, avg batch time: 0.5276, average train loss: 0.0308
[09/28 10:35:12 visual_prompt]: Inference (val):avg data time: 1.89e-05, avg batch time: 0.1815, average loss: 0.0302
[09/28 10:35:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:35:34 visual_prompt]: 	Test 100/190. loss: 3.897, 0.2030 s / batch. (data: 2.38e-05)max mem: 7.81234 GB 
[09/28 10:35:54 visual_prompt]: Inference (test):avg data time: 1.18e-04, avg batch time: 0.2041, average loss: 3.5852
[09/28 10:35:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.72	top5: 73.03	
[09/28 10:35:54 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/28 10:36:03 visual_prompt]: Epoch 58 / 100: avg data time: 1.02e-01, avg batch time: 0.5266, average train loss: 0.0268
[09/28 10:36:06 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1774, average loss: 0.0187
[09/28 10:36:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:36:28 visual_prompt]: 	Test 100/190. loss: 3.771, 0.2035 s / batch. (data: 2.79e-05)max mem: 7.81234 GB 
[09/28 10:36:48 visual_prompt]: Inference (test):avg data time: 1.13e-04, avg batch time: 0.2042, average loss: 3.6017
[09/28 10:36:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.91	top5: 73.22	
[09/28 10:36:48 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/28 10:36:57 visual_prompt]: Epoch 59 / 100: avg data time: 1.01e-01, avg batch time: 0.5276, average train loss: 0.0295
[09/28 10:37:01 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1733, average loss: 0.0364
[09/28 10:37:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.00	top5: 100.00	
[09/28 10:37:23 visual_prompt]: 	Test 100/190. loss: 3.857, 0.2039 s / batch. (data: 2.55e-05)max mem: 7.81234 GB 
[09/28 10:37:42 visual_prompt]: Inference (test):avg data time: 2.85e-05, avg batch time: 0.2040, average loss: 3.5874
[09/28 10:37:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.88	top5: 72.76	
[09/28 10:37:42 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/28 10:37:52 visual_prompt]: Epoch 60 / 100: avg data time: 1.04e-01, avg batch time: 0.5288, average train loss: 0.0287
[09/28 10:37:55 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1762, average loss: 0.0173
[09/28 10:37:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:38:17 visual_prompt]: 	Test 100/190. loss: 3.642, 0.2045 s / batch. (data: 2.46e-05)max mem: 7.81234 GB 
[09/28 10:38:36 visual_prompt]: Inference (test):avg data time: 2.79e-05, avg batch time: 0.2041, average loss: 3.5130
[09/28 10:38:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.11	top5: 73.09	
[09/28 10:38:36 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/28 10:38:46 visual_prompt]: Epoch 61 / 100: avg data time: 1.05e-01, avg batch time: 0.5295, average train loss: 0.0187
[09/28 10:38:49 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1806, average loss: 0.0196
[09/28 10:38:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 99.50	top5: 100.00	
[09/28 10:39:11 visual_prompt]: 	Test 100/190. loss: 3.746, 0.2032 s / batch. (data: 2.46e-05)max mem: 7.81234 GB 
[09/28 10:39:31 visual_prompt]: Inference (test):avg data time: 1.26e-04, avg batch time: 0.2042, average loss: 3.5381
[09/28 10:39:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.07	top5: 73.41	
[09/28 10:39:31 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/28 10:39:40 visual_prompt]: Epoch 62 / 100: avg data time: 9.84e-02, avg batch time: 0.5239, average train loss: 0.0161
[09/28 10:39:43 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1772, average loss: 0.0128
[09/28 10:39:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:40:05 visual_prompt]: 	Test 100/190. loss: 3.714, 0.2041 s / batch. (data: 2.53e-05)max mem: 7.81234 GB 
[09/28 10:40:25 visual_prompt]: Inference (test):avg data time: 2.84e-05, avg batch time: 0.2043, average loss: 3.5446
[09/28 10:40:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.37	top5: 73.42	
[09/28 10:40:25 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/28 10:40:34 visual_prompt]: Epoch 63 / 100: avg data time: 9.03e-02, avg batch time: 0.5174, average train loss: 0.0137
[09/28 10:40:38 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1737, average loss: 0.0119
[09/28 10:40:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:41:00 visual_prompt]: 	Test 100/190. loss: 3.814, 0.2034 s / batch. (data: 3.12e-05)max mem: 7.81234 GB 
[09/28 10:41:19 visual_prompt]: Inference (test):avg data time: 2.85e-05, avg batch time: 0.2040, average loss: 3.5441
[09/28 10:41:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.00	top5: 73.21	
[09/28 10:41:19 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/28 10:41:29 visual_prompt]: Epoch 64 / 100: avg data time: 1.07e-01, avg batch time: 0.5311, average train loss: 0.0134
[09/28 10:41:32 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1806, average loss: 0.0103
[09/28 10:41:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:41:54 visual_prompt]: 	Test 100/190. loss: 3.778, 0.2039 s / batch. (data: 2.69e-05)max mem: 7.81234 GB 
[09/28 10:42:13 visual_prompt]: Inference (test):avg data time: 1.29e-04, avg batch time: 0.2043, average loss: 3.5381
[09/28 10:42:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.03	top5: 73.37	
[09/28 10:42:13 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/28 10:42:23 visual_prompt]: Epoch 65 / 100: avg data time: 9.20e-02, avg batch time: 0.5160, average train loss: 0.0120
[09/28 10:42:26 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1823, average loss: 0.0099
[09/28 10:42:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:42:48 visual_prompt]: 	Test 100/190. loss: 3.744, 0.2030 s / batch. (data: 2.88e-05)max mem: 7.81234 GB 
[09/28 10:43:07 visual_prompt]: Inference (test):avg data time: 2.85e-05, avg batch time: 0.2043, average loss: 3.5285
[09/28 10:43:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.50	top5: 73.58	
[09/28 10:43:08 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/28 10:43:17 visual_prompt]: Epoch 66 / 100: avg data time: 1.03e-01, avg batch time: 0.5268, average train loss: 0.0109
[09/28 10:43:20 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1801, average loss: 0.0106
[09/28 10:43:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:43:42 visual_prompt]: 	Test 100/190. loss: 3.684, 0.2031 s / batch. (data: 2.62e-05)max mem: 7.81234 GB 
[09/28 10:44:02 visual_prompt]: Inference (test):avg data time: 2.81e-05, avg batch time: 0.2040, average loss: 3.5100
[09/28 10:44:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.18	top5: 73.45	
[09/28 10:44:02 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/28 10:44:11 visual_prompt]: Epoch 67 / 100: avg data time: 1.07e-01, avg batch time: 0.5320, average train loss: 0.0109
[09/28 10:44:14 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1788, average loss: 0.0100
[09/28 10:44:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:44:37 visual_prompt]: 	Test 100/190. loss: 3.672, 0.2024 s / batch. (data: 2.86e-05)max mem: 7.81234 GB 
[09/28 10:44:56 visual_prompt]: Inference (test):avg data time: 2.89e-05, avg batch time: 0.2042, average loss: 3.5195
[09/28 10:44:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.09	top5: 73.09	
[09/28 10:44:56 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/28 10:45:06 visual_prompt]: Epoch 68 / 100: avg data time: 9.52e-02, avg batch time: 0.5213, average train loss: 0.0112
[09/28 10:45:09 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1814, average loss: 0.0084
[09/28 10:45:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:45:31 visual_prompt]: 	Test 100/190. loss: 3.692, 0.2047 s / batch. (data: 2.74e-05)max mem: 7.81234 GB 
[09/28 10:45:50 visual_prompt]: Inference (test):avg data time: 2.75e-05, avg batch time: 0.2041, average loss: 3.5270
[09/28 10:45:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.19	top5: 73.41	
[09/28 10:45:51 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/28 10:46:00 visual_prompt]: Epoch 69 / 100: avg data time: 9.86e-02, avg batch time: 0.5235, average train loss: 0.0110
[09/28 10:46:03 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1770, average loss: 0.0086
[09/28 10:46:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:46:25 visual_prompt]: 	Test 100/190. loss: 3.634, 0.2040 s / batch. (data: 2.72e-05)max mem: 7.81234 GB 
[09/28 10:46:45 visual_prompt]: Inference (test):avg data time: 2.86e-05, avg batch time: 0.2042, average loss: 3.5242
[09/28 10:46:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.17	top5: 73.32	
[09/28 10:46:45 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/28 10:46:54 visual_prompt]: Epoch 70 / 100: avg data time: 1.02e-01, avg batch time: 0.5287, average train loss: 0.0099
[09/28 10:46:58 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1783, average loss: 0.0087
[09/28 10:46:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:47:20 visual_prompt]: 	Test 100/190. loss: 3.607, 0.2044 s / batch. (data: 2.77e-05)max mem: 7.81234 GB 
[09/28 10:47:39 visual_prompt]: Inference (test):avg data time: 2.91e-05, avg batch time: 0.2040, average loss: 3.5147
[09/28 10:47:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.23	top5: 73.39	
[09/28 10:47:39 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/28 10:47:49 visual_prompt]: Epoch 71 / 100: avg data time: 1.02e-01, avg batch time: 0.5269, average train loss: 0.0097
[09/28 10:47:52 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1764, average loss: 0.0092
[09/28 10:47:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:48:14 visual_prompt]: 	Test 100/190. loss: 3.600, 0.2043 s / batch. (data: 2.81e-05)max mem: 7.81234 GB 
[09/28 10:48:34 visual_prompt]: Inference (test):avg data time: 2.80e-05, avg batch time: 0.2042, average loss: 3.5148
[09/28 10:48:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.05	top5: 73.42	
[09/28 10:48:34 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/28 10:48:43 visual_prompt]: Epoch 72 / 100: avg data time: 1.07e-01, avg batch time: 0.5320, average train loss: 0.0097
[09/28 10:48:46 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1813, average loss: 0.0087
[09/28 10:48:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:49:08 visual_prompt]: 	Test 100/190. loss: 3.590, 0.2028 s / batch. (data: 2.62e-05)max mem: 7.81234 GB 
[09/28 10:49:28 visual_prompt]: Inference (test):avg data time: 2.77e-05, avg batch time: 0.2041, average loss: 3.4985
[09/28 10:49:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.14	top5: 73.43	
[09/28 10:49:28 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/28 10:49:37 visual_prompt]: Epoch 73 / 100: avg data time: 9.16e-02, avg batch time: 0.5167, average train loss: 0.0097
[09/28 10:49:41 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1772, average loss: 0.0088
[09/28 10:49:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:50:03 visual_prompt]: 	Test 100/190. loss: 3.564, 0.2033 s / batch. (data: 2.77e-05)max mem: 7.81234 GB 
[09/28 10:50:22 visual_prompt]: Inference (test):avg data time: 2.83e-05, avg batch time: 0.2040, average loss: 3.4908
[09/28 10:50:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.19	top5: 73.31	
[09/28 10:50:22 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/28 10:50:32 visual_prompt]: Epoch 74 / 100: avg data time: 1.00e-01, avg batch time: 0.5263, average train loss: 0.0097
[09/28 10:50:35 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1833, average loss: 0.0085
[09/28 10:50:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:50:57 visual_prompt]: 	Test 100/190. loss: 3.575, 0.2051 s / batch. (data: 2.60e-05)max mem: 7.81234 GB 
[09/28 10:51:16 visual_prompt]: Inference (test):avg data time: 2.97e-05, avg batch time: 0.2042, average loss: 3.4908
[09/28 10:51:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.13	top5: 73.45	
[09/28 10:51:16 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/28 10:51:26 visual_prompt]: Epoch 75 / 100: avg data time: 9.14e-02, avg batch time: 0.5207, average train loss: 0.0096
[09/28 10:51:29 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1849, average loss: 0.0083
[09/28 10:51:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:51:51 visual_prompt]: 	Test 100/190. loss: 3.557, 0.2052 s / batch. (data: 2.88e-05)max mem: 7.81234 GB 
[09/28 10:52:11 visual_prompt]: Inference (test):avg data time: 2.78e-05, avg batch time: 0.2041, average loss: 3.4929
[09/28 10:52:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.28	top5: 73.44	
[09/28 10:52:11 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/28 10:52:20 visual_prompt]: Epoch 76 / 100: avg data time: 1.06e-01, avg batch time: 0.5309, average train loss: 0.0096
[09/28 10:52:23 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1760, average loss: 0.0081
[09/28 10:52:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:52:45 visual_prompt]: 	Test 100/190. loss: 3.532, 0.2050 s / batch. (data: 2.36e-05)max mem: 7.81234 GB 
[09/28 10:53:05 visual_prompt]: Inference (test):avg data time: 5.14e-05, avg batch time: 0.2040, average loss: 3.4910
[09/28 10:53:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.24	top5: 73.45	
[09/28 10:53:05 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/28 10:53:14 visual_prompt]: Epoch 77 / 100: avg data time: 9.49e-02, avg batch time: 0.5228, average train loss: 0.0089
[09/28 10:53:17 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1849, average loss: 0.0080
[09/28 10:53:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:53:39 visual_prompt]: 	Test 100/190. loss: 3.530, 0.2036 s / batch. (data: 2.77e-05)max mem: 7.81234 GB 
[09/28 10:53:59 visual_prompt]: Inference (test):avg data time: 2.82e-05, avg batch time: 0.2040, average loss: 3.4910
[09/28 10:53:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.26	top5: 73.44	
[09/28 10:53:59 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/28 10:54:08 visual_prompt]: Epoch 78 / 100: avg data time: 9.97e-02, avg batch time: 0.5241, average train loss: 0.0092
[09/28 10:54:12 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1815, average loss: 0.0077
[09/28 10:54:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:54:33 visual_prompt]: 	Test 100/190. loss: 3.524, 0.2049 s / batch. (data: 2.77e-05)max mem: 7.81234 GB 
[09/28 10:54:53 visual_prompt]: Inference (test):avg data time: 2.75e-05, avg batch time: 0.2039, average loss: 3.4905
[09/28 10:54:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.28	top5: 73.36	
[09/28 10:54:53 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/28 10:55:02 visual_prompt]: Epoch 79 / 100: avg data time: 1.00e-01, avg batch time: 0.5245, average train loss: 0.0099
[09/28 10:55:06 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1848, average loss: 0.0078
[09/28 10:55:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:55:28 visual_prompt]: 	Test 100/190. loss: 3.507, 0.2040 s / batch. (data: 2.55e-05)max mem: 7.81234 GB 
[09/28 10:55:47 visual_prompt]: Inference (test):avg data time: 2.82e-05, avg batch time: 0.2041, average loss: 3.4882
[09/28 10:55:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.34	top5: 73.56	
[09/28 10:55:47 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/28 10:55:57 visual_prompt]: Epoch 80 / 100: avg data time: 9.42e-02, avg batch time: 0.5197, average train loss: 0.0093
[09/28 10:56:00 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1720, average loss: 0.0071
[09/28 10:56:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:56:22 visual_prompt]: 	Test 100/190. loss: 3.529, 0.2041 s / batch. (data: 2.60e-05)max mem: 7.81234 GB 
[09/28 10:56:41 visual_prompt]: Inference (test):avg data time: 4.56e-05, avg batch time: 0.2043, average loss: 3.4880
[09/28 10:56:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.24	top5: 73.58	
[09/28 10:56:41 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/28 10:56:51 visual_prompt]: Epoch 81 / 100: avg data time: 9.02e-02, avg batch time: 0.5164, average train loss: 0.0091
[09/28 10:56:54 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1788, average loss: 0.0070
[09/28 10:56:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:57:16 visual_prompt]: 	Test 100/190. loss: 3.525, 0.2033 s / batch. (data: 2.72e-05)max mem: 7.81234 GB 
[09/28 10:57:36 visual_prompt]: Inference (test):avg data time: 2.79e-05, avg batch time: 0.2042, average loss: 3.4903
[09/28 10:57:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.18	top5: 73.70	
[09/28 10:57:36 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/28 10:57:45 visual_prompt]: Epoch 82 / 100: avg data time: 9.98e-02, avg batch time: 0.5262, average train loss: 0.0093
[09/28 10:57:48 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1734, average loss: 0.0071
[09/28 10:57:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:58:10 visual_prompt]: 	Test 100/190. loss: 3.516, 0.2042 s / batch. (data: 2.41e-05)max mem: 7.81234 GB 
[09/28 10:58:30 visual_prompt]: Inference (test):avg data time: 2.89e-05, avg batch time: 0.2040, average loss: 3.4952
[09/28 10:58:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.09	top5: 73.65	
[09/28 10:58:30 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/28 10:58:40 visual_prompt]: Epoch 83 / 100: avg data time: 1.01e-01, avg batch time: 0.5254, average train loss: 0.0091
[09/28 10:58:43 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1783, average loss: 0.0070
[09/28 10:58:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:59:05 visual_prompt]: 	Test 100/190. loss: 3.519, 0.2033 s / batch. (data: 2.69e-05)max mem: 7.81234 GB 
[09/28 10:59:24 visual_prompt]: Inference (test):avg data time: 2.85e-05, avg batch time: 0.2040, average loss: 3.4975
[09/28 10:59:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.00	top5: 73.54	
[09/28 10:59:24 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/28 10:59:34 visual_prompt]: Epoch 84 / 100: avg data time: 9.24e-02, avg batch time: 0.5159, average train loss: 0.0095
[09/28 10:59:37 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1804, average loss: 0.0069
[09/28 10:59:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 10:59:59 visual_prompt]: 	Test 100/190. loss: 3.513, 0.2042 s / batch. (data: 2.86e-05)max mem: 7.81234 GB 
[09/28 11:00:19 visual_prompt]: Inference (test):avg data time: 2.89e-05, avg batch time: 0.2043, average loss: 3.4938
[09/28 11:00:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.05	top5: 73.53	
[09/28 11:00:19 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/28 11:00:28 visual_prompt]: Epoch 85 / 100: avg data time: 1.00e-01, avg batch time: 0.5264, average train loss: 0.0093
[09/28 11:00:31 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1805, average loss: 0.0071
[09/28 11:00:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 11:00:54 visual_prompt]: 	Test 100/190. loss: 3.502, 0.2033 s / batch. (data: 2.48e-05)max mem: 7.81234 GB 
[09/28 11:01:13 visual_prompt]: Inference (test):avg data time: 2.92e-05, avg batch time: 0.2041, average loss: 3.4948
[09/28 11:01:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.03	top5: 73.51	
[09/28 11:01:13 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/28 11:01:23 visual_prompt]: Epoch 86 / 100: avg data time: 9.91e-02, avg batch time: 0.5232, average train loss: 0.0090
[09/28 11:01:26 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1727, average loss: 0.0070
[09/28 11:01:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 11:01:48 visual_prompt]: 	Test 100/190. loss: 3.509, 0.2052 s / batch. (data: 2.48e-05)max mem: 7.81234 GB 
[09/28 11:02:07 visual_prompt]: Inference (test):avg data time: 1.32e-04, avg batch time: 0.2043, average loss: 3.4884
[09/28 11:02:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.99	top5: 73.61	
[09/28 11:02:07 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/28 11:02:17 visual_prompt]: Epoch 87 / 100: avg data time: 1.07e-01, avg batch time: 0.5317, average train loss: 0.0088
[09/28 11:02:20 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1783, average loss: 0.0069
[09/28 11:02:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 11:02:42 visual_prompt]: 	Test 100/190. loss: 3.505, 0.2053 s / batch. (data: 2.65e-05)max mem: 7.81234 GB 
[09/28 11:03:02 visual_prompt]: Inference (test):avg data time: 2.91e-05, avg batch time: 0.2041, average loss: 3.4888
[09/28 11:03:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.96	top5: 73.56	
[09/28 11:03:02 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/28 11:03:11 visual_prompt]: Epoch 88 / 100: avg data time: 9.09e-02, avg batch time: 0.5172, average train loss: 0.0089
[09/28 11:03:14 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1720, average loss: 0.0070
[09/28 11:03:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 11:03:36 visual_prompt]: 	Test 100/190. loss: 3.497, 0.2030 s / batch. (data: 2.65e-05)max mem: 7.81234 GB 
[09/28 11:03:56 visual_prompt]: Inference (test):avg data time: 2.89e-05, avg batch time: 0.2042, average loss: 3.4907
[09/28 11:03:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.91	top5: 73.51	
[09/28 11:03:56 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/28 11:04:05 visual_prompt]: Epoch 89 / 100: avg data time: 1.04e-01, avg batch time: 0.5281, average train loss: 0.0091
[09/28 11:04:09 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1764, average loss: 0.0070
[09/28 11:04:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 11:04:31 visual_prompt]: 	Test 100/190. loss: 3.487, 0.2029 s / batch. (data: 4.72e-05)max mem: 7.81234 GB 
[09/28 11:04:50 visual_prompt]: Inference (test):avg data time: 1.14e-04, avg batch time: 0.2041, average loss: 3.4887
[09/28 11:04:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.90	top5: 73.53	
[09/28 11:04:50 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/28 11:05:00 visual_prompt]: Epoch 90 / 100: avg data time: 1.01e-01, avg batch time: 0.5272, average train loss: 0.0094
[09/28 11:05:03 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.1827, average loss: 0.0071
[09/28 11:05:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 11:05:25 visual_prompt]: 	Test 100/190. loss: 3.483, 0.2033 s / batch. (data: 2.48e-05)max mem: 7.81234 GB 
[09/28 11:05:44 visual_prompt]: Inference (test):avg data time: 2.84e-05, avg batch time: 0.2041, average loss: 3.4886
[09/28 11:05:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.95	top5: 73.52	
[09/28 11:05:44 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/28 11:05:54 visual_prompt]: Epoch 91 / 100: avg data time: 1.06e-01, avg batch time: 0.5297, average train loss: 0.0091
[09/28 11:05:57 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1815, average loss: 0.0073
[09/28 11:05:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 11:06:19 visual_prompt]: 	Test 100/190. loss: 3.469, 0.2052 s / batch. (data: 2.36e-05)max mem: 7.81234 GB 
[09/28 11:06:39 visual_prompt]: Inference (test):avg data time: 2.88e-05, avg batch time: 0.2040, average loss: 3.4868
[09/28 11:06:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.01	top5: 73.39	
[09/28 11:06:39 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/28 11:06:48 visual_prompt]: Epoch 92 / 100: avg data time: 1.05e-01, avg batch time: 0.5315, average train loss: 0.0091
[09/28 11:06:51 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1757, average loss: 0.0073
[09/28 11:06:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 11:07:13 visual_prompt]: 	Test 100/190. loss: 3.461, 0.2026 s / batch. (data: 2.96e-05)max mem: 7.81234 GB 
[09/28 11:07:33 visual_prompt]: Inference (test):avg data time: 2.87e-05, avg batch time: 0.2041, average loss: 3.4866
[09/28 11:07:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.99	top5: 73.42	
[09/28 11:07:33 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/28 11:07:43 visual_prompt]: Epoch 93 / 100: avg data time: 1.07e-01, avg batch time: 0.5312, average train loss: 0.0089
[09/28 11:07:46 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1788, average loss: 0.0072
[09/28 11:07:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 11:08:08 visual_prompt]: 	Test 100/190. loss: 3.461, 0.2062 s / batch. (data: 2.53e-05)max mem: 7.81234 GB 
[09/28 11:08:27 visual_prompt]: Inference (test):avg data time: 2.89e-05, avg batch time: 0.2041, average loss: 3.4860
[09/28 11:08:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.01	top5: 73.36	
[09/28 11:08:27 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/28 11:08:37 visual_prompt]: Epoch 94 / 100: avg data time: 9.50e-02, avg batch time: 0.5201, average train loss: 0.0089
[09/28 11:08:40 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1802, average loss: 0.0071
[09/28 11:08:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 11:09:02 visual_prompt]: 	Test 100/190. loss: 3.461, 0.2041 s / batch. (data: 2.43e-05)max mem: 7.81234 GB 
[09/28 11:09:22 visual_prompt]: Inference (test):avg data time: 2.79e-05, avg batch time: 0.2040, average loss: 3.4858
[09/28 11:09:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.03	top5: 73.42	
[09/28 11:09:22 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/28 11:09:31 visual_prompt]: Epoch 95 / 100: avg data time: 9.77e-02, avg batch time: 0.5248, average train loss: 0.0093
[09/28 11:09:34 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1825, average loss: 0.0071
[09/28 11:09:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 11:09:56 visual_prompt]: 	Test 100/190. loss: 3.462, 0.2032 s / batch. (data: 2.84e-05)max mem: 7.81234 GB 
[09/28 11:10:16 visual_prompt]: Inference (test):avg data time: 2.84e-05, avg batch time: 0.2042, average loss: 3.4864
[09/28 11:10:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.04	top5: 73.42	
[09/28 11:10:16 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/28 11:10:25 visual_prompt]: Epoch 96 / 100: avg data time: 1.01e-01, avg batch time: 0.5267, average train loss: 0.0090
[09/28 11:10:29 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1813, average loss: 0.0071
[09/28 11:10:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 11:10:50 visual_prompt]: 	Test 100/190. loss: 3.463, 0.2033 s / batch. (data: 2.86e-05)max mem: 7.81234 GB 
[09/28 11:11:10 visual_prompt]: Inference (test):avg data time: 2.83e-05, avg batch time: 0.2042, average loss: 3.4869
[09/28 11:11:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.05	top5: 73.42	
[09/28 11:11:10 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/28 11:11:19 visual_prompt]: Epoch 97 / 100: avg data time: 9.19e-02, avg batch time: 0.5195, average train loss: 0.0088
[09/28 11:11:23 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1768, average loss: 0.0071
[09/28 11:11:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 11:11:45 visual_prompt]: 	Test 100/190. loss: 3.464, 0.2031 s / batch. (data: 2.36e-05)max mem: 7.81234 GB 
[09/28 11:12:04 visual_prompt]: Inference (test):avg data time: 2.96e-05, avg batch time: 0.2041, average loss: 3.4871
[09/28 11:12:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.05	top5: 73.42	
[09/28 11:12:04 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/28 11:12:14 visual_prompt]: Epoch 98 / 100: avg data time: 1.02e-01, avg batch time: 0.5286, average train loss: 0.0092
[09/28 11:12:17 visual_prompt]: Inference (val):avg data time: 1.89e-05, avg batch time: 0.1854, average loss: 0.0071
[09/28 11:12:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 11:12:39 visual_prompt]: 	Test 100/190. loss: 3.465, 0.2032 s / batch. (data: 2.41e-05)max mem: 7.81234 GB 
[09/28 11:12:59 visual_prompt]: Inference (test):avg data time: 2.87e-05, avg batch time: 0.2039, average loss: 3.4870
[09/28 11:12:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.03	top5: 73.44	
[09/28 11:12:59 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/28 11:13:08 visual_prompt]: Epoch 99 / 100: avg data time: 1.07e-01, avg batch time: 0.5322, average train loss: 0.0086
[09/28 11:13:12 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1771, average loss: 0.0071
[09/28 11:13:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 11:13:34 visual_prompt]: 	Test 100/190. loss: 3.466, 0.2051 s / batch. (data: 2.74e-05)max mem: 7.81234 GB 
[09/28 11:13:53 visual_prompt]: Inference (test):avg data time: 2.90e-05, avg batch time: 0.2041, average loss: 3.4869
[09/28 11:13:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.04	top5: 73.46	
[09/28 11:13:53 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/28 11:14:02 visual_prompt]: Epoch 100 / 100: avg data time: 9.04e-02, avg batch time: 0.5168, average train loss: 0.0086
[09/28 11:14:06 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1813, average loss: 0.0071
[09/28 11:14:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 11:14:28 visual_prompt]: 	Test 100/190. loss: 3.466, 0.2047 s / batch. (data: 2.62e-05)max mem: 7.81234 GB 
[09/28 11:14:47 visual_prompt]: Inference (test):avg data time: 2.86e-05, avg batch time: 0.2040, average loss: 3.4869
[09/28 11:14:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.04	top5: 73.46	
[09/28 11:14:47 visual_prompt]: Rank of current process: 0. World size: 1
[09/28 11:14:47 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/28 11:14:47 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_azimuth")', 'DATA.NUMBER_CLASSES', '18', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/28 11:14:47 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/28 11:14:47 visual_prompt]: Training with config:
[09/28 11:14:47 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_azimuth")/sup_vitb16_imagenet21k/prompt50/crop224/test/seed5877/lr0.25_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 5877, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_azimuth")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 18, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/28 11:14:47 visual_prompt]: Loading training data...
[09/28 11:14:47 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset trainval...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800]+test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/28 11:14:49 visual_prompt]: Number of images: 1000
[09/28 11:14:49 visual_prompt]: Number of classes: 18 / 18
[09/28 11:14:49 visual_prompt]: Loading validation data...
[09/28 11:14:49 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/28 11:14:49 visual_prompt]: Number of images: 200
[09/28 11:14:49 visual_prompt]: Number of classes: 18 / 18
[09/28 11:14:49 visual_prompt]: Loading test data...
[09/28 11:14:49 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_azimuth") dataset test...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[50%:], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/28 11:15:05 visual_prompt]: Number of images: 12150
[09/28 11:15:05 visual_prompt]: Number of classes: 18 / 18
[09/28 11:15:05 visual_prompt]: Constructing models...
[09/28 11:15:07 visual_prompt]: Total Parameters: 86273298	 Gradient Parameters: 474642
[09/28 11:15:07 visual_prompt]: tuned percent:0.550
[09/28 11:15:07 visual_prompt]: Device used for model: 0
[09/28 11:15:07 visual_prompt]: Setting up Evaluator...
[09/28 11:15:07 visual_prompt]: Setting up Trainer...
[09/28 11:15:07 visual_prompt]: 	Setting up the optimizer...
[09/28 11:15:07 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/28 11:15:16 visual_prompt]: Epoch 1 / 100: avg data time: 1.05e-01, avg batch time: 0.5253, average train loss: 3.0616
[09/28 11:15:20 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1829, average loss: 3.0667
[09/28 11:15:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 26.00	
[09/28 11:15:42 visual_prompt]: 	Test 100/190. loss: 3.103, 0.2037 s / batch. (data: 3.12e-05)max mem: 7.81234 GB 
[09/28 11:16:01 visual_prompt]: Inference (test):avg data time: 2.97e-05, avg batch time: 0.2030, average loss: 3.0421
[09/28 11:16:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.51	top5: 28.33	
[09/28 11:16:01 visual_prompt]: Best epoch 1: best metric: 0.055
[09/28 11:16:01 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/28 11:16:11 visual_prompt]: Epoch 2 / 100: avg data time: 1.09e-01, avg batch time: 0.5344, average train loss: 2.9390
[09/28 11:16:14 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1758, average loss: 2.9147
[09/28 11:16:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.00	top5: 26.00	
[09/28 11:16:36 visual_prompt]: 	Test 100/190. loss: 2.868, 0.2024 s / batch. (data: 2.93e-05)max mem: 7.81234 GB 
[09/28 11:16:55 visual_prompt]: Inference (test):avg data time: 1.16e-04, avg batch time: 0.2039, average loss: 2.9204
[09/28 11:16:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 27.94	
[09/28 11:16:56 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/28 11:17:05 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e-01, avg batch time: 0.5285, average train loss: 2.9214
[09/28 11:17:08 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1768, average loss: 2.8940
[09/28 11:17:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 33.00	
[09/28 11:17:30 visual_prompt]: 	Test 100/190. loss: 2.933, 0.2035 s / batch. (data: 2.65e-05)max mem: 7.81234 GB 
[09/28 11:17:50 visual_prompt]: Inference (test):avg data time: 4.54e-05, avg batch time: 0.2037, average loss: 2.9135
[09/28 11:17:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.40	top5: 27.79	
[09/28 11:17:50 visual_prompt]: Best epoch 3: best metric: 0.085
[09/28 11:17:50 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/28 11:17:59 visual_prompt]: Epoch 4 / 100: avg data time: 1.00e-01, avg batch time: 0.5256, average train loss: 2.9145
[09/28 11:18:02 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1745, average loss: 2.8909
[09/28 11:18:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 29.00	
[09/28 11:18:24 visual_prompt]: 	Test 100/190. loss: 2.885, 0.2028 s / batch. (data: 2.96e-05)max mem: 7.81234 GB 
[09/28 11:18:44 visual_prompt]: Inference (test):avg data time: 3.05e-05, avg batch time: 0.2040, average loss: 2.9090
[09/28 11:18:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.41	top5: 27.47	
[09/28 11:18:44 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/28 11:18:53 visual_prompt]: Epoch 5 / 100: avg data time: 9.33e-02, avg batch time: 0.5202, average train loss: 2.9153
[09/28 11:18:56 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1769, average loss: 2.8840
[09/28 11:18:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 4.50	top5: 29.50	
[09/28 11:19:19 visual_prompt]: 	Test 100/190. loss: 2.922, 0.2040 s / batch. (data: 2.55e-05)max mem: 7.81234 GB 
[09/28 11:19:38 visual_prompt]: Inference (test):avg data time: 3.00e-05, avg batch time: 0.2038, average loss: 2.9033
[09/28 11:19:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.50	top5: 28.17	
[09/28 11:19:38 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/28 11:19:48 visual_prompt]: Epoch 6 / 100: avg data time: 9.75e-02, avg batch time: 0.5241, average train loss: 2.9080
[09/28 11:19:51 visual_prompt]: Inference (val):avg data time: 4.38e-05, avg batch time: 0.1769, average loss: 2.9219
[09/28 11:19:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.50	top5: 34.00	
[09/28 11:20:13 visual_prompt]: 	Test 100/190. loss: 2.967, 0.2035 s / batch. (data: 2.55e-05)max mem: 7.81234 GB 
[09/28 11:20:32 visual_prompt]: Inference (test):avg data time: 3.04e-05, avg batch time: 0.2038, average loss: 2.9251
[09/28 11:20:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.40	top5: 28.71	
[09/28 11:20:32 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/28 11:20:42 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e-01, avg batch time: 0.5278, average train loss: 2.9215
[09/28 11:20:45 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1804, average loss: 2.8994
[09/28 11:20:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.00	top5: 31.50	
[09/28 11:21:07 visual_prompt]: 	Test 100/190. loss: 2.883, 0.2030 s / batch. (data: 2.86e-05)max mem: 7.81234 GB 
[09/28 11:21:27 visual_prompt]: Inference (test):avg data time: 3.00e-05, avg batch time: 0.2040, average loss: 2.9137
[09/28 11:21:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.03	top5: 28.04	
[09/28 11:21:27 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/28 11:21:36 visual_prompt]: Epoch 8 / 100: avg data time: 1.09e-01, avg batch time: 0.5337, average train loss: 2.8986
[09/28 11:21:40 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1750, average loss: 2.8514
[09/28 11:21:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 37.00	
[09/28 11:22:02 visual_prompt]: 	Test 100/190. loss: 2.904, 0.2032 s / batch. (data: 2.65e-05)max mem: 7.81234 GB 
[09/28 11:22:21 visual_prompt]: Inference (test):avg data time: 1.88e-04, avg batch time: 0.2037, average loss: 2.8954
[09/28 11:22:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.80	top5: 31.53	
[09/28 11:22:21 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/28 11:22:31 visual_prompt]: Epoch 9 / 100: avg data time: 1.05e-01, avg batch time: 0.5305, average train loss: 2.9042
[09/28 11:22:34 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1823, average loss: 2.9055
[09/28 11:22:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.00	top5: 30.00	
[09/28 11:22:56 visual_prompt]: 	Test 100/190. loss: 2.875, 0.2035 s / batch. (data: 2.86e-05)max mem: 7.81234 GB 
[09/28 11:23:15 visual_prompt]: Inference (test):avg data time: 2.94e-05, avg batch time: 0.2041, average loss: 2.9088
[09/28 11:23:15 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.21	top5: 31.42	
[09/28 11:23:15 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/28 11:23:25 visual_prompt]: Epoch 10 / 100: avg data time: 9.97e-02, avg batch time: 0.5252, average train loss: 2.9170
[09/28 11:23:28 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.1722, average loss: 2.9053
[09/28 11:23:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.00	top5: 33.00	
[09/28 11:23:50 visual_prompt]: 	Test 100/190. loss: 2.886, 0.2043 s / batch. (data: 2.93e-05)max mem: 7.81234 GB 
[09/28 11:24:10 visual_prompt]: Inference (test):avg data time: 3.00e-05, avg batch time: 0.2039, average loss: 2.9422
[09/28 11:24:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.35	top5: 27.70	
[09/28 11:24:10 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/28 11:24:19 visual_prompt]: Epoch 11 / 100: avg data time: 1.10e-01, avg batch time: 0.5346, average train loss: 2.9597
[09/28 11:24:22 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1735, average loss: 2.9681
[09/28 11:24:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 3.00	top5: 25.50	
[09/28 11:24:44 visual_prompt]: 	Test 100/190. loss: 2.918, 0.2026 s / batch. (data: 2.86e-05)max mem: 7.81234 GB 
[09/28 11:25:04 visual_prompt]: Inference (test):avg data time: 3.00e-05, avg batch time: 0.2041, average loss: 2.9511
[09/28 11:25:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.43	top5: 27.51	
[09/28 11:25:04 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/28 11:25:13 visual_prompt]: Epoch 12 / 100: avg data time: 1.08e-01, avg batch time: 0.5311, average train loss: 2.9694
[09/28 11:25:17 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1826, average loss: 2.9303
[09/28 11:25:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.00	top5: 24.00	
[09/28 11:25:38 visual_prompt]: 	Test 100/190. loss: 2.865, 0.2042 s / batch. (data: 2.65e-05)max mem: 7.81234 GB 
[09/28 11:25:58 visual_prompt]: Inference (test):avg data time: 3.01e-05, avg batch time: 0.2040, average loss: 2.9157
[09/28 11:25:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.88	top5: 28.82	
[09/28 11:25:58 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/28 11:26:07 visual_prompt]: Epoch 13 / 100: avg data time: 8.43e-02, avg batch time: 0.5107, average train loss: 2.9314
[09/28 11:26:10 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1806, average loss: 2.8983
[09/28 11:26:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.00	top5: 29.00	
[09/28 11:26:32 visual_prompt]: 	Test 100/190. loss: 2.947, 0.2033 s / batch. (data: 2.48e-05)max mem: 7.81234 GB 
[09/28 11:26:52 visual_prompt]: Inference (test):avg data time: 3.00e-05, avg batch time: 0.2038, average loss: 2.9179
[09/28 11:26:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 6.03	top5: 28.74	
[09/28 11:26:52 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/28 11:27:02 visual_prompt]: Epoch 14 / 100: avg data time: 1.13e-01, avg batch time: 0.5376, average train loss: 2.9242
[09/28 11:27:05 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1828, average loss: 2.8793
[09/28 11:27:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.00	top5: 34.00	
[09/28 11:27:27 visual_prompt]: 	Test 100/190. loss: 2.929, 0.2030 s / batch. (data: 2.69e-05)max mem: 7.81234 GB 
[09/28 11:27:47 visual_prompt]: Inference (test):avg data time: 2.97e-05, avg batch time: 0.2039, average loss: 2.9154
[09/28 11:27:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 5.61	top5: 27.93	
[09/28 11:27:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/28 11:27:56 visual_prompt]: Epoch 15 / 100: avg data time: 1.00e-01, avg batch time: 0.5237, average train loss: 2.8941
[09/28 11:27:59 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1850, average loss: 2.8953
[09/28 11:27:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.00	top5: 30.50	
[09/28 11:28:21 visual_prompt]: 	Test 100/190. loss: 2.914, 0.2047 s / batch. (data: 2.57e-05)max mem: 7.81234 GB 
[09/28 11:28:41 visual_prompt]: Inference (test):avg data time: 3.00e-05, avg batch time: 0.2038, average loss: 2.9073
[09/28 11:28:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.46	top5: 30.32	
[09/28 11:28:41 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/28 11:28:50 visual_prompt]: Epoch 16 / 100: avg data time: 1.04e-01, avg batch time: 0.5306, average train loss: 2.8838
[09/28 11:28:53 visual_prompt]: Inference (val):avg data time: 4.40e-05, avg batch time: 0.1812, average loss: 2.8092
[09/28 11:28:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.00	top5: 43.50	
[09/28 11:29:15 visual_prompt]: 	Test 100/190. loss: 2.807, 0.2032 s / batch. (data: 2.60e-05)max mem: 7.81234 GB 
[09/28 11:29:35 visual_prompt]: Inference (test):avg data time: 3.01e-05, avg batch time: 0.2040, average loss: 2.8495
[09/28 11:29:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.93	top5: 37.47	
[09/28 11:29:35 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/28 11:29:44 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e-01, avg batch time: 0.5290, average train loss: 2.8245
[09/28 11:29:47 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1767, average loss: 2.7891
[09/28 11:29:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 7.50	top5: 43.00	
[09/28 11:30:10 visual_prompt]: 	Test 100/190. loss: 2.900, 0.2028 s / batch. (data: 2.72e-05)max mem: 7.81234 GB 
[09/28 11:30:29 visual_prompt]: Inference (test):avg data time: 3.00e-05, avg batch time: 0.2041, average loss: 2.8412
[09/28 11:30:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.68	top5: 38.92	
[09/28 11:30:29 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/28 11:30:39 visual_prompt]: Epoch 18 / 100: avg data time: 1.01e-01, avg batch time: 0.5248, average train loss: 2.8134
[09/28 11:30:42 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1803, average loss: 2.7575
[09/28 11:30:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.50	top5: 50.50	
[09/28 11:31:04 visual_prompt]: 	Test 100/190. loss: 2.814, 0.2035 s / batch. (data: 2.60e-05)max mem: 7.81234 GB 
[09/28 11:31:24 visual_prompt]: Inference (test):avg data time: 3.02e-05, avg batch time: 0.2039, average loss: 2.8205
[09/28 11:31:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.41	top5: 37.78	
[09/28 11:31:24 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/28 11:31:33 visual_prompt]: Epoch 19 / 100: avg data time: 8.69e-02, avg batch time: 0.5140, average train loss: 2.7647
[09/28 11:31:36 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.1737, average loss: 2.6767
[09/28 11:31:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.50	top5: 52.50	
[09/28 11:31:58 visual_prompt]: 	Test 100/190. loss: 2.720, 0.2046 s / batch. (data: 3.08e-05)max mem: 7.81234 GB 
[09/28 11:32:18 visual_prompt]: Inference (test):avg data time: 5.20e-05, avg batch time: 0.2043, average loss: 2.7627
[09/28 11:32:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 8.65	top5: 43.89	
[09/28 11:32:18 visual_prompt]: Best epoch 19: best metric: 0.115
[09/28 11:32:18 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/28 11:32:27 visual_prompt]: Epoch 20 / 100: avg data time: 9.81e-02, avg batch time: 0.5249, average train loss: 2.6744
[09/28 11:32:30 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1744, average loss: 2.6405
[09/28 11:32:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.50	top5: 54.00	
[09/28 11:32:52 visual_prompt]: 	Test 100/190. loss: 2.620, 0.2035 s / batch. (data: 2.88e-05)max mem: 7.81234 GB 
[09/28 11:33:12 visual_prompt]: Inference (test):avg data time: 2.98e-05, avg batch time: 0.2042, average loss: 2.7377
[09/28 11:33:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 11.21	top5: 45.98	
[09/28 11:33:12 visual_prompt]: Best epoch 20: best metric: 0.145
[09/28 11:33:12 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/28 11:33:21 visual_prompt]: Epoch 21 / 100: avg data time: 1.10e-01, avg batch time: 0.5357, average train loss: 2.5590
[09/28 11:33:24 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1752, average loss: 2.6510
[09/28 11:33:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.00	top5: 51.50	
[09/28 11:33:47 visual_prompt]: 	Test 100/190. loss: 2.685, 0.2042 s / batch. (data: 2.60e-05)max mem: 7.81234 GB 
[09/28 11:34:06 visual_prompt]: Inference (test):avg data time: 9.07e-05, avg batch time: 0.2039, average loss: 2.7722
[09/28 11:34:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 10.05	top5: 46.86	
[09/28 11:34:06 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/28 11:34:16 visual_prompt]: Epoch 22 / 100: avg data time: 1.06e-01, avg batch time: 0.5297, average train loss: 2.5317
[09/28 11:34:19 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1790, average loss: 2.5769
[09/28 11:34:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 13.50	top5: 58.00	
[09/28 11:34:41 visual_prompt]: 	Test 100/190. loss: 2.818, 0.2028 s / batch. (data: 2.57e-05)max mem: 7.81234 GB 
[09/28 11:35:00 visual_prompt]: Inference (test):avg data time: 2.95e-05, avg batch time: 0.2041, average loss: 2.7718
[09/28 11:35:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 9.92	top5: 49.60	
[09/28 11:35:00 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/28 11:35:10 visual_prompt]: Epoch 23 / 100: avg data time: 9.72e-02, avg batch time: 0.5222, average train loss: 2.5152
[09/28 11:35:13 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1759, average loss: 2.4878
[09/28 11:35:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.00	top5: 56.00	
[09/28 11:35:35 visual_prompt]: 	Test 100/190. loss: 2.573, 0.2031 s / batch. (data: 2.50e-05)max mem: 7.81234 GB 
[09/28 11:35:54 visual_prompt]: Inference (test):avg data time: 2.95e-05, avg batch time: 0.2043, average loss: 2.6602
[09/28 11:35:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 12.30	top5: 54.02	
[09/28 11:35:55 visual_prompt]: Best epoch 23: best metric: 0.160
[09/28 11:35:55 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/28 11:36:04 visual_prompt]: Epoch 24 / 100: avg data time: 1.06e-01, avg batch time: 0.5306, average train loss: 2.4646
[09/28 11:36:07 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1810, average loss: 2.3496
[09/28 11:36:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.00	top5: 70.00	
[09/28 11:36:29 visual_prompt]: 	Test 100/190. loss: 2.480, 0.2052 s / batch. (data: 3.50e-05)max mem: 7.81234 GB 
[09/28 11:36:49 visual_prompt]: Inference (test):avg data time: 5.37e-05, avg batch time: 0.2042, average loss: 2.5364
[09/28 11:36:49 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 15.06	top5: 59.88	
[09/28 11:36:49 visual_prompt]: Best epoch 24: best metric: 0.170
[09/28 11:36:49 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/28 11:36:58 visual_prompt]: Epoch 25 / 100: avg data time: 9.07e-02, avg batch time: 0.5175, average train loss: 2.3310
[09/28 11:37:01 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1716, average loss: 2.3701
[09/28 11:37:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.50	top5: 70.00	
[09/28 11:37:23 visual_prompt]: 	Test 100/190. loss: 2.578, 0.2029 s / batch. (data: 3.24e-05)max mem: 7.81234 GB 
[09/28 11:37:43 visual_prompt]: Inference (test):avg data time: 3.10e-05, avg batch time: 0.2042, average loss: 2.5657
[09/28 11:37:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 14.45	top5: 61.74	
[09/28 11:37:43 visual_prompt]: Best epoch 25: best metric: 0.195
[09/28 11:37:43 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/28 11:37:52 visual_prompt]: Epoch 26 / 100: avg data time: 9.90e-02, avg batch time: 0.5257, average train loss: 2.2104
[09/28 11:37:55 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1833, average loss: 2.1615
[09/28 11:37:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 22.50	top5: 76.00	
[09/28 11:38:17 visual_prompt]: 	Test 100/190. loss: 2.483, 0.2043 s / batch. (data: 3.00e-05)max mem: 7.81234 GB 
[09/28 11:38:37 visual_prompt]: Inference (test):avg data time: 3.59e-05, avg batch time: 0.2042, average loss: 2.4990
[09/28 11:38:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.79	top5: 65.21	
[09/28 11:38:37 visual_prompt]: Best epoch 26: best metric: 0.225
[09/28 11:38:37 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/28 11:38:47 visual_prompt]: Epoch 27 / 100: avg data time: 1.06e-01, avg batch time: 0.5326, average train loss: 2.0889
[09/28 11:38:50 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1759, average loss: 2.2384
[09/28 11:38:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 23.00	top5: 73.00	
[09/28 11:39:12 visual_prompt]: 	Test 100/190. loss: 2.495, 0.2056 s / batch. (data: 2.57e-05)max mem: 7.81234 GB 
[09/28 11:39:31 visual_prompt]: Inference (test):avg data time: 3.06e-05, avg batch time: 0.2041, average loss: 2.5595
[09/28 11:39:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.74	top5: 63.96	
[09/28 11:39:31 visual_prompt]: Best epoch 27: best metric: 0.230
[09/28 11:39:31 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/28 11:39:41 visual_prompt]: Epoch 28 / 100: avg data time: 1.06e-01, avg batch time: 0.5308, average train loss: 2.1345
[09/28 11:39:44 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1792, average loss: 2.1777
[09/28 11:39:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 20.50	top5: 77.00	
[09/28 11:40:06 visual_prompt]: 	Test 100/190. loss: 2.534, 0.2055 s / batch. (data: 3.17e-05)max mem: 7.81234 GB 
[09/28 11:40:26 visual_prompt]: Inference (test):avg data time: 3.47e-05, avg batch time: 0.2042, average loss: 2.5040
[09/28 11:40:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 16.44	top5: 65.79	
[09/28 11:40:26 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/28 11:40:35 visual_prompt]: Epoch 29 / 100: avg data time: 1.09e-01, avg batch time: 0.5329, average train loss: 1.9609
[09/28 11:40:39 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1800, average loss: 2.0546
[09/28 11:40:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.50	top5: 79.00	
[09/28 11:41:01 visual_prompt]: 	Test 100/190. loss: 2.389, 0.2046 s / batch. (data: 2.67e-05)max mem: 7.81234 GB 
[09/28 11:41:20 visual_prompt]: Inference (test):avg data time: 3.09e-05, avg batch time: 0.2042, average loss: 2.5656
[09/28 11:41:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 17.65	top5: 68.21	
[09/28 11:41:20 visual_prompt]: Best epoch 29: best metric: 0.275
[09/28 11:41:20 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/28 11:41:30 visual_prompt]: Epoch 30 / 100: avg data time: 1.02e-01, avg batch time: 0.5277, average train loss: 1.8952
[09/28 11:41:33 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.1769, average loss: 1.8880
[09/28 11:41:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.00	top5: 81.50	
[09/28 11:41:55 visual_prompt]: 	Test 100/190. loss: 2.588, 0.2033 s / batch. (data: 2.93e-05)max mem: 7.81234 GB 
[09/28 11:42:15 visual_prompt]: Inference (test):avg data time: 3.02e-05, avg batch time: 0.2041, average loss: 2.4694
[09/28 11:42:15 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 20.02	top5: 70.21	
[09/28 11:42:15 visual_prompt]: Best epoch 30: best metric: 0.300
[09/28 11:42:15 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/28 11:42:24 visual_prompt]: Epoch 31 / 100: avg data time: 9.16e-02, avg batch time: 0.5194, average train loss: 1.7462
[09/28 11:42:27 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1846, average loss: 1.8211
[09/28 11:42:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 36.00	top5: 87.00	
[09/28 11:42:49 visual_prompt]: 	Test 100/190. loss: 2.618, 0.2031 s / batch. (data: 2.55e-05)max mem: 7.81234 GB 
[09/28 11:43:09 visual_prompt]: Inference (test):avg data time: 2.95e-05, avg batch time: 0.2041, average loss: 2.7851
[09/28 11:43:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 19.95	top5: 70.04	
[09/28 11:43:09 visual_prompt]: Best epoch 31: best metric: 0.360
[09/28 11:43:09 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/28 11:43:18 visual_prompt]: Epoch 32 / 100: avg data time: 9.79e-02, avg batch time: 0.5250, average train loss: 1.6764
[09/28 11:43:21 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1801, average loss: 1.7152
[09/28 11:43:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 40.00	top5: 90.50	
[09/28 11:43:44 visual_prompt]: 	Test 100/190. loss: 2.731, 0.2033 s / batch. (data: 2.57e-05)max mem: 7.81234 GB 
[09/28 11:44:03 visual_prompt]: Inference (test):avg data time: 2.93e-05, avg batch time: 0.2041, average loss: 2.7043
[09/28 11:44:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 20.97	top5: 67.89	
[09/28 11:44:03 visual_prompt]: Best epoch 32: best metric: 0.400
[09/28 11:44:03 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/28 11:44:13 visual_prompt]: Epoch 33 / 100: avg data time: 9.62e-02, avg batch time: 0.5227, average train loss: 1.5842
[09/28 11:44:16 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1728, average loss: 1.5647
[09/28 11:44:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 45.00	top5: 92.00	
[09/28 11:44:38 visual_prompt]: 	Test 100/190. loss: 2.668, 0.2029 s / batch. (data: 2.62e-05)max mem: 7.81234 GB 
[09/28 11:44:57 visual_prompt]: Inference (test):avg data time: 2.91e-05, avg batch time: 0.2043, average loss: 2.6967
[09/28 11:44:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.31	top5: 71.13	
[09/28 11:44:57 visual_prompt]: Best epoch 33: best metric: 0.450
[09/28 11:44:57 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/28 11:45:07 visual_prompt]: Epoch 34 / 100: avg data time: 1.10e-01, avg batch time: 0.5354, average train loss: 1.4718
[09/28 11:45:10 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.1815, average loss: 1.6977
[09/28 11:45:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 40.50	top5: 87.00	
[09/28 11:45:32 visual_prompt]: 	Test 100/190. loss: 3.007, 0.2047 s / batch. (data: 2.74e-05)max mem: 7.81234 GB 
[09/28 11:45:52 visual_prompt]: Inference (test):avg data time: 3.05e-05, avg batch time: 0.2040, average loss: 2.8411
[09/28 11:45:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 22.41	top5: 69.01	
[09/28 11:45:52 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/28 11:46:01 visual_prompt]: Epoch 35 / 100: avg data time: 1.10e-01, avg batch time: 0.5344, average train loss: 1.3060
[09/28 11:46:04 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1723, average loss: 1.3252
[09/28 11:46:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 49.00	top5: 97.50	
[09/28 11:46:26 visual_prompt]: 	Test 100/190. loss: 2.870, 0.2044 s / batch. (data: 3.19e-05)max mem: 7.81234 GB 
[09/28 11:46:46 visual_prompt]: Inference (test):avg data time: 3.04e-05, avg batch time: 0.2043, average loss: 2.7870
[09/28 11:46:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 22.90	top5: 73.33	
[09/28 11:46:46 visual_prompt]: Best epoch 35: best metric: 0.490
[09/28 11:46:46 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/28 11:46:56 visual_prompt]: Epoch 36 / 100: avg data time: 9.87e-02, avg batch time: 0.5243, average train loss: 1.0738
[09/28 11:46:59 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1726, average loss: 1.2052
[09/28 11:46:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 54.00	top5: 97.00	
[09/28 11:47:21 visual_prompt]: 	Test 100/190. loss: 3.538, 0.2041 s / batch. (data: 2.72e-05)max mem: 7.81234 GB 
[09/28 11:47:40 visual_prompt]: Inference (test):avg data time: 1.39e-04, avg batch time: 0.2045, average loss: 3.0339
[09/28 11:47:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.28	top5: 72.07	
[09/28 11:47:40 visual_prompt]: Best epoch 36: best metric: 0.540
[09/28 11:47:40 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/28 11:47:50 visual_prompt]: Epoch 37 / 100: avg data time: 1.01e-01, avg batch time: 0.5282, average train loss: 0.9871
[09/28 11:47:53 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1830, average loss: 1.3528
[09/28 11:47:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 53.50	top5: 92.00	
[09/28 11:48:15 visual_prompt]: 	Test 100/190. loss: 3.107, 0.2042 s / batch. (data: 2.96e-05)max mem: 7.81234 GB 
[09/28 11:48:35 visual_prompt]: Inference (test):avg data time: 3.00e-05, avg batch time: 0.2043, average loss: 3.1173
[09/28 11:48:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 21.68	top5: 71.67	
[09/28 11:48:35 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/28 11:48:44 visual_prompt]: Epoch 38 / 100: avg data time: 9.93e-02, avg batch time: 0.5253, average train loss: 0.8568
[09/28 11:48:47 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1766, average loss: 0.8735
[09/28 11:48:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 68.00	top5: 97.50	
[09/28 11:49:09 visual_prompt]: 	Test 100/190. loss: 2.958, 0.2043 s / batch. (data: 2.84e-05)max mem: 7.81234 GB 
[09/28 11:49:29 visual_prompt]: Inference (test):avg data time: 1.63e-04, avg batch time: 0.2042, average loss: 2.8371
[09/28 11:49:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 24.12	top5: 72.15	
[09/28 11:49:29 visual_prompt]: Best epoch 38: best metric: 0.680
[09/28 11:49:29 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/28 11:49:38 visual_prompt]: Epoch 39 / 100: avg data time: 1.05e-01, avg batch time: 0.5294, average train loss: 0.7268
[09/28 11:49:42 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1862, average loss: 0.7087
[09/28 11:49:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 74.00	top5: 100.00	
[09/28 11:50:04 visual_prompt]: 	Test 100/190. loss: 3.139, 0.2032 s / batch. (data: 2.65e-05)max mem: 7.81234 GB 
[09/28 11:50:23 visual_prompt]: Inference (test):avg data time: 2.97e-05, avg batch time: 0.2041, average loss: 3.1239
[09/28 11:50:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 24.02	top5: 72.21	
[09/28 11:50:23 visual_prompt]: Best epoch 39: best metric: 0.740
[09/28 11:50:23 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/28 11:50:33 visual_prompt]: Epoch 40 / 100: avg data time: 1.05e-01, avg batch time: 0.5289, average train loss: 0.6327
[09/28 11:50:36 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1829, average loss: 0.6077
[09/28 11:50:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 79.50	top5: 100.00	
[09/28 11:50:58 visual_prompt]: 	Test 100/190. loss: 3.392, 0.2040 s / batch. (data: 2.88e-05)max mem: 7.81234 GB 
[09/28 11:51:18 visual_prompt]: Inference (test):avg data time: 3.12e-05, avg batch time: 0.2040, average loss: 3.1055
[09/28 11:51:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.34	top5: 73.34	
[09/28 11:51:18 visual_prompt]: Best epoch 40: best metric: 0.795
[09/28 11:51:18 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/28 11:51:27 visual_prompt]: Epoch 41 / 100: avg data time: 9.81e-02, avg batch time: 0.5241, average train loss: 0.5279
[09/28 11:51:30 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.1717, average loss: 0.5710
[09/28 11:51:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 81.50	top5: 100.00	
[09/28 11:51:52 visual_prompt]: 	Test 100/190. loss: 3.489, 0.2050 s / batch. (data: 2.93e-05)max mem: 7.81234 GB 
[09/28 11:52:12 visual_prompt]: Inference (test):avg data time: 3.14e-05, avg batch time: 0.2040, average loss: 3.2052
[09/28 11:52:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 24.63	top5: 71.19	
[09/28 11:52:12 visual_prompt]: Best epoch 41: best metric: 0.815
[09/28 11:52:12 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/28 11:52:21 visual_prompt]: Epoch 42 / 100: avg data time: 1.10e-01, avg batch time: 0.5342, average train loss: 0.4656
[09/28 11:52:24 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1832, average loss: 0.3607
[09/28 11:52:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 92.50	top5: 100.00	
[09/28 11:52:47 visual_prompt]: 	Test 100/190. loss: 3.342, 0.2038 s / batch. (data: 2.77e-05)max mem: 7.81234 GB 
[09/28 11:53:06 visual_prompt]: Inference (test):avg data time: 5.78e-05, avg batch time: 0.2041, average loss: 3.0697
[09/28 11:53:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 26.99	top5: 73.78	
[09/28 11:53:06 visual_prompt]: Best epoch 42: best metric: 0.925
[09/28 11:53:06 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/28 11:53:16 visual_prompt]: Epoch 43 / 100: avg data time: 9.93e-02, avg batch time: 0.5270, average train loss: 0.3364
[09/28 11:53:19 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1812, average loss: 0.3316
[09/28 11:53:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 91.00	top5: 100.00	
[09/28 11:53:41 visual_prompt]: 	Test 100/190. loss: 3.420, 0.2053 s / batch. (data: 2.69e-05)max mem: 7.81234 GB 
[09/28 11:54:00 visual_prompt]: Inference (test):avg data time: 2.88e-05, avg batch time: 0.2041, average loss: 3.2436
[09/28 11:54:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 25.82	top5: 72.00	
[09/28 11:54:01 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/28 11:54:10 visual_prompt]: Epoch 44 / 100: avg data time: 1.07e-01, avg batch time: 0.5316, average train loss: 0.2617
[09/28 11:54:13 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1848, average loss: 0.2287
[09/28 11:54:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 95.50	top5: 100.00	
[09/28 11:54:35 visual_prompt]: 	Test 100/190. loss: 3.609, 0.2041 s / batch. (data: 2.74e-05)max mem: 7.81234 GB 
[09/28 11:54:55 visual_prompt]: Inference (test):avg data time: 3.05e-05, avg batch time: 0.2039, average loss: 3.1851
[09/28 11:54:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.55	top5: 73.72	
[09/28 11:54:55 visual_prompt]: Best epoch 44: best metric: 0.955
[09/28 11:54:55 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/28 11:55:04 visual_prompt]: Epoch 45 / 100: avg data time: 1.00e-01, avg batch time: 0.5245, average train loss: 0.1714
[09/28 11:55:07 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1778, average loss: 0.1500
[09/28 11:55:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 97.50	top5: 100.00	
[09/28 11:55:29 visual_prompt]: 	Test 100/190. loss: 3.616, 0.2033 s / batch. (data: 2.65e-05)max mem: 7.81234 GB 
[09/28 11:55:49 visual_prompt]: Inference (test):avg data time: 6.22e-05, avg batch time: 0.2041, average loss: 3.2785
[09/28 11:55:49 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 27.83	top5: 73.91	
[09/28 11:55:49 visual_prompt]: Best epoch 45: best metric: 0.975
[09/28 11:55:49 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/28 11:55:58 visual_prompt]: Epoch 46 / 100: avg data time: 1.06e-01, avg batch time: 0.5301, average train loss: 0.1418
[09/28 11:56:02 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1847, average loss: 0.1285
[09/28 11:56:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 97.50	top5: 100.00	
[09/28 11:56:24 visual_prompt]: 	Test 100/190. loss: 3.931, 0.2054 s / batch. (data: 2.77e-05)max mem: 7.81234 GB 
[09/28 11:56:43 visual_prompt]: Inference (test):avg data time: 2.97e-05, avg batch time: 0.2040, average loss: 3.3221
[09/28 11:56:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.31	top5: 74.26	
[09/28 11:56:43 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/28 11:56:53 visual_prompt]: Epoch 47 / 100: avg data time: 1.03e-01, avg batch time: 0.5290, average train loss: 0.1212
[09/28 11:56:56 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1834, average loss: 0.0855
[09/28 11:56:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 11:57:18 visual_prompt]: 	Test 100/190. loss: 3.646, 0.2046 s / batch. (data: 2.72e-05)max mem: 7.81234 GB 
[09/28 11:57:38 visual_prompt]: Inference (test):avg data time: 5.05e-05, avg batch time: 0.2042, average loss: 3.2850
[09/28 11:57:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.16	top5: 73.84	
[09/28 11:57:38 visual_prompt]: Best epoch 47: best metric: 1.000
[09/28 11:57:38 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/28 11:57:47 visual_prompt]: Epoch 48 / 100: avg data time: 9.82e-02, avg batch time: 0.5275, average train loss: 0.1045
[09/28 11:57:51 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1720, average loss: 0.0853
[09/28 11:57:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 98.50	top5: 100.00	
[09/28 11:58:13 visual_prompt]: 	Test 100/190. loss: 3.897, 0.2036 s / batch. (data: 3.19e-05)max mem: 7.81234 GB 
[09/28 11:58:32 visual_prompt]: Inference (test):avg data time: 3.00e-05, avg batch time: 0.2042, average loss: 3.2678
[09/28 11:58:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.62	top5: 74.42	
[09/28 11:58:32 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/28 11:58:42 visual_prompt]: Epoch 49 / 100: avg data time: 1.05e-01, avg batch time: 0.5301, average train loss: 0.0999
[09/28 11:58:45 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1799, average loss: 0.1051
[09/28 11:58:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 98.00	top5: 100.00	
[09/28 11:59:07 visual_prompt]: 	Test 100/190. loss: 3.515, 0.2048 s / batch. (data: 2.84e-05)max mem: 7.81234 GB 
[09/28 11:59:27 visual_prompt]: Inference (test):avg data time: 2.95e-05, avg batch time: 0.2042, average loss: 3.3321
[09/28 11:59:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.06	top5: 73.14	
[09/28 11:59:27 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/28 11:59:36 visual_prompt]: Epoch 50 / 100: avg data time: 1.03e-01, avg batch time: 0.5291, average train loss: 0.0790
[09/28 11:59:39 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1794, average loss: 0.0639
[09/28 11:59:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:00:01 visual_prompt]: 	Test 100/190. loss: 3.847, 0.2030 s / batch. (data: 2.98e-05)max mem: 7.81234 GB 
[09/28 12:00:21 visual_prompt]: Inference (test):avg data time: 8.75e-05, avg batch time: 0.2041, average loss: 3.3475
[09/28 12:00:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.17	top5: 72.95	
[09/28 12:00:21 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/28 12:00:30 visual_prompt]: Epoch 51 / 100: avg data time: 1.04e-01, avg batch time: 0.5312, average train loss: 0.0595
[09/28 12:00:34 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1828, average loss: 0.0468
[09/28 12:00:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:00:55 visual_prompt]: 	Test 100/190. loss: 3.727, 0.2042 s / batch. (data: 2.79e-05)max mem: 7.81234 GB 
[09/28 12:01:15 visual_prompt]: Inference (test):avg data time: 2.99e-05, avg batch time: 0.2042, average loss: 3.4154
[09/28 12:01:15 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.07	top5: 73.93	
[09/28 12:01:15 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/28 12:01:24 visual_prompt]: Epoch 52 / 100: avg data time: 9.71e-02, avg batch time: 0.5231, average train loss: 0.0431
[09/28 12:01:28 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1779, average loss: 0.0316
[09/28 12:01:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:01:50 visual_prompt]: 	Test 100/190. loss: 3.711, 0.2041 s / batch. (data: 2.93e-05)max mem: 7.81234 GB 
[09/28 12:02:09 visual_prompt]: Inference (test):avg data time: 2.98e-05, avg batch time: 0.2041, average loss: 3.4160
[09/28 12:02:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 28.70	top5: 73.44	
[09/28 12:02:09 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/28 12:02:19 visual_prompt]: Epoch 53 / 100: avg data time: 9.17e-02, avg batch time: 0.5183, average train loss: 0.0362
[09/28 12:02:22 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1756, average loss: 0.0231
[09/28 12:02:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:02:44 visual_prompt]: 	Test 100/190. loss: 3.781, 0.2056 s / batch. (data: 3.00e-05)max mem: 7.81234 GB 
[09/28 12:03:04 visual_prompt]: Inference (test):avg data time: 3.00e-05, avg batch time: 0.2041, average loss: 3.3765
[09/28 12:03:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.99	top5: 74.07	
[09/28 12:03:04 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/28 12:03:13 visual_prompt]: Epoch 54 / 100: avg data time: 1.05e-01, avg batch time: 0.5319, average train loss: 0.0270
[09/28 12:03:16 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1837, average loss: 0.0210
[09/28 12:03:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:03:38 visual_prompt]: 	Test 100/190. loss: 3.878, 0.2033 s / batch. (data: 2.74e-05)max mem: 7.81234 GB 
[09/28 12:03:58 visual_prompt]: Inference (test):avg data time: 2.99e-05, avg batch time: 0.2039, average loss: 3.3900
[09/28 12:03:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.93	top5: 73.72	
[09/28 12:03:58 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/28 12:04:08 visual_prompt]: Epoch 55 / 100: avg data time: 1.09e-01, avg batch time: 0.5339, average train loss: 0.0234
[09/28 12:04:11 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1786, average loss: 0.0185
[09/28 12:04:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:04:33 visual_prompt]: 	Test 100/190. loss: 3.841, 0.2035 s / batch. (data: 2.55e-05)max mem: 7.81234 GB 
[09/28 12:04:52 visual_prompt]: Inference (test):avg data time: 2.96e-05, avg batch time: 0.2039, average loss: 3.3870
[09/28 12:04:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.42	top5: 74.15	
[09/28 12:04:53 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/28 12:05:02 visual_prompt]: Epoch 56 / 100: avg data time: 9.66e-02, avg batch time: 0.5228, average train loss: 0.0199
[09/28 12:05:05 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1787, average loss: 0.0152
[09/28 12:05:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:05:27 visual_prompt]: 	Test 100/190. loss: 3.848, 0.2029 s / batch. (data: 4.86e-05)max mem: 7.81234 GB 
[09/28 12:05:47 visual_prompt]: Inference (test):avg data time: 2.94e-05, avg batch time: 0.2041, average loss: 3.4032
[09/28 12:05:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.65	top5: 74.39	
[09/28 12:05:47 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/28 12:05:56 visual_prompt]: Epoch 57 / 100: avg data time: 1.07e-01, avg batch time: 0.5322, average train loss: 0.0176
[09/28 12:05:59 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.1772, average loss: 0.0154
[09/28 12:05:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:06:22 visual_prompt]: 	Test 100/190. loss: 3.725, 0.2047 s / batch. (data: 3.05e-05)max mem: 7.81234 GB 
[09/28 12:06:41 visual_prompt]: Inference (test):avg data time: 2.92e-05, avg batch time: 0.2042, average loss: 3.4085
[09/28 12:06:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.44	top5: 74.14	
[09/28 12:06:41 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/28 12:06:51 visual_prompt]: Epoch 58 / 100: avg data time: 1.01e-01, avg batch time: 0.5276, average train loss: 0.0164
[09/28 12:06:54 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1807, average loss: 0.0132
[09/28 12:06:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:07:16 visual_prompt]: 	Test 100/190. loss: 3.739, 0.2034 s / batch. (data: 3.05e-05)max mem: 7.81234 GB 
[09/28 12:07:36 visual_prompt]: Inference (test):avg data time: 3.01e-05, avg batch time: 0.2041, average loss: 3.4077
[09/28 12:07:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.74	top5: 74.27	
[09/28 12:07:36 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/28 12:07:45 visual_prompt]: Epoch 59 / 100: avg data time: 9.29e-02, avg batch time: 0.5182, average train loss: 0.0156
[09/28 12:07:48 visual_prompt]: Inference (val):avg data time: 4.63e-05, avg batch time: 0.1776, average loss: 0.0121
[09/28 12:07:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:08:10 visual_prompt]: 	Test 100/190. loss: 3.767, 0.2050 s / batch. (data: 3.29e-05)max mem: 7.81234 GB 
[09/28 12:08:30 visual_prompt]: Inference (test):avg data time: 1.42e-04, avg batch time: 0.2045, average loss: 3.4116
[09/28 12:08:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.54	top5: 74.28	
[09/28 12:08:30 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/28 12:08:40 visual_prompt]: Epoch 60 / 100: avg data time: 1.10e-01, avg batch time: 0.5342, average train loss: 0.0146
[09/28 12:08:43 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1769, average loss: 0.0125
[09/28 12:08:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:09:05 visual_prompt]: 	Test 100/190. loss: 3.853, 0.2037 s / batch. (data: 4.17e-05)max mem: 7.81234 GB 
[09/28 12:09:24 visual_prompt]: Inference (test):avg data time: 3.09e-05, avg batch time: 0.2044, average loss: 3.4198
[09/28 12:09:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.45	top5: 74.19	
[09/28 12:09:24 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/28 12:09:34 visual_prompt]: Epoch 61 / 100: avg data time: 1.07e-01, avg batch time: 0.5323, average train loss: 0.0136
[09/28 12:09:37 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1795, average loss: 0.0124
[09/28 12:09:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:09:59 visual_prompt]: 	Test 100/190. loss: 3.761, 0.2044 s / batch. (data: 2.65e-05)max mem: 7.81234 GB 
[09/28 12:10:19 visual_prompt]: Inference (test):avg data time: 1.51e-04, avg batch time: 0.2043, average loss: 3.4075
[09/28 12:10:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.50	top5: 73.97	
[09/28 12:10:19 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/28 12:10:28 visual_prompt]: Epoch 62 / 100: avg data time: 9.63e-02, avg batch time: 0.5231, average train loss: 0.0133
[09/28 12:10:31 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1829, average loss: 0.0122
[09/28 12:10:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:10:53 visual_prompt]: 	Test 100/190. loss: 3.804, 0.2026 s / batch. (data: 3.08e-05)max mem: 7.81234 GB 
[09/28 12:11:13 visual_prompt]: Inference (test):avg data time: 3.06e-05, avg batch time: 0.2041, average loss: 3.4129
[09/28 12:11:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.47	top5: 74.09	
[09/28 12:11:13 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/28 12:11:22 visual_prompt]: Epoch 63 / 100: avg data time: 9.56e-02, avg batch time: 0.5229, average train loss: 0.0143
[09/28 12:11:26 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1833, average loss: 0.0117
[09/28 12:11:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:11:48 visual_prompt]: 	Test 100/190. loss: 3.758, 0.2042 s / batch. (data: 2.65e-05)max mem: 7.81234 GB 
[09/28 12:12:07 visual_prompt]: Inference (test):avg data time: 2.93e-05, avg batch time: 0.2042, average loss: 3.3870
[09/28 12:12:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.54	top5: 74.12	
[09/28 12:12:07 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/28 12:12:17 visual_prompt]: Epoch 64 / 100: avg data time: 1.01e-01, avg batch time: 0.5258, average train loss: 0.0194
[09/28 12:12:20 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1778, average loss: 0.0163
[09/28 12:12:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:12:42 visual_prompt]: 	Test 100/190. loss: 3.674, 0.2039 s / batch. (data: 2.57e-05)max mem: 7.81234 GB 
[09/28 12:13:02 visual_prompt]: Inference (test):avg data time: 2.97e-05, avg batch time: 0.2042, average loss: 3.3887
[09/28 12:13:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.32	top5: 73.79	
[09/28 12:13:02 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/28 12:13:11 visual_prompt]: Epoch 65 / 100: avg data time: 9.83e-02, avg batch time: 0.5245, average train loss: 0.0168
[09/28 12:13:14 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1860, average loss: 0.0166
[09/28 12:13:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:13:36 visual_prompt]: 	Test 100/190. loss: 3.767, 0.2046 s / batch. (data: 2.55e-05)max mem: 7.81234 GB 
[09/28 12:13:56 visual_prompt]: Inference (test):avg data time: 3.03e-05, avg batch time: 0.2041, average loss: 3.3833
[09/28 12:13:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.48	top5: 73.84	
[09/28 12:13:56 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/28 12:14:05 visual_prompt]: Epoch 66 / 100: avg data time: 8.86e-02, avg batch time: 0.5161, average train loss: 0.0149
[09/28 12:14:08 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1778, average loss: 0.0130
[09/28 12:14:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:14:30 visual_prompt]: 	Test 100/190. loss: 3.701, 0.2041 s / batch. (data: 2.88e-05)max mem: 7.81234 GB 
[09/28 12:14:50 visual_prompt]: Inference (test):avg data time: 2.94e-05, avg batch time: 0.2039, average loss: 3.3815
[09/28 12:14:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.62	top5: 73.98	
[09/28 12:14:50 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/28 12:14:59 visual_prompt]: Epoch 67 / 100: avg data time: 9.81e-02, avg batch time: 0.5231, average train loss: 0.0134
[09/28 12:15:02 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.1828, average loss: 0.0129
[09/28 12:15:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:15:24 visual_prompt]: 	Test 100/190. loss: 3.733, 0.2055 s / batch. (data: 3.29e-05)max mem: 7.81234 GB 
[09/28 12:15:44 visual_prompt]: Inference (test):avg data time: 2.92e-05, avg batch time: 0.2042, average loss: 3.3990
[09/28 12:15:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.50	top5: 73.85	
[09/28 12:15:44 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/28 12:15:53 visual_prompt]: Epoch 68 / 100: avg data time: 9.77e-02, avg batch time: 0.5235, average train loss: 0.0131
[09/28 12:15:57 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1813, average loss: 0.0141
[09/28 12:15:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:16:19 visual_prompt]: 	Test 100/190. loss: 3.724, 0.2036 s / batch. (data: 2.60e-05)max mem: 7.81234 GB 
[09/28 12:16:38 visual_prompt]: Inference (test):avg data time: 9.66e-05, avg batch time: 0.2041, average loss: 3.4069
[09/28 12:16:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.70	top5: 74.12	
[09/28 12:16:38 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/28 12:16:48 visual_prompt]: Epoch 69 / 100: avg data time: 9.95e-02, avg batch time: 0.5281, average train loss: 0.0124
[09/28 12:16:51 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1778, average loss: 0.0128
[09/28 12:16:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:17:13 visual_prompt]: 	Test 100/190. loss: 3.746, 0.2049 s / batch. (data: 2.81e-05)max mem: 7.81234 GB 
[09/28 12:17:33 visual_prompt]: Inference (test):avg data time: 1.02e-04, avg batch time: 0.2042, average loss: 3.4136
[09/28 12:17:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.63	top5: 73.71	
[09/28 12:17:33 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/28 12:17:42 visual_prompt]: Epoch 70 / 100: avg data time: 1.07e-01, avg batch time: 0.5329, average train loss: 0.0127
[09/28 12:17:45 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1784, average loss: 0.0119
[09/28 12:17:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:18:07 visual_prompt]: 	Test 100/190. loss: 3.752, 0.2035 s / batch. (data: 2.96e-05)max mem: 7.81234 GB 
[09/28 12:18:27 visual_prompt]: Inference (test):avg data time: 3.03e-05, avg batch time: 0.2042, average loss: 3.3821
[09/28 12:18:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.01	top5: 74.05	
[09/28 12:18:27 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/28 12:18:36 visual_prompt]: Epoch 71 / 100: avg data time: 1.02e-01, avg batch time: 0.5296, average train loss: 0.0129
[09/28 12:18:40 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1770, average loss: 0.0117
[09/28 12:18:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:19:02 visual_prompt]: 	Test 100/190. loss: 3.721, 0.2033 s / batch. (data: 2.38e-05)max mem: 7.81234 GB 
[09/28 12:19:21 visual_prompt]: Inference (test):avg data time: 2.92e-05, avg batch time: 0.2040, average loss: 3.3885
[09/28 12:19:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.07	top5: 74.22	
[09/28 12:19:21 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/28 12:19:31 visual_prompt]: Epoch 72 / 100: avg data time: 9.03e-02, avg batch time: 0.5163, average train loss: 0.0119
[09/28 12:19:34 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.1749, average loss: 0.0129
[09/28 12:19:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:19:56 visual_prompt]: 	Test 100/190. loss: 3.731, 0.2033 s / batch. (data: 2.88e-05)max mem: 7.81234 GB 
[09/28 12:20:15 visual_prompt]: Inference (test):avg data time: 1.11e-04, avg batch time: 0.2042, average loss: 3.3913
[09/28 12:20:15 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.66	top5: 74.15	
[09/28 12:20:15 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/28 12:20:25 visual_prompt]: Epoch 73 / 100: avg data time: 9.93e-02, avg batch time: 0.5271, average train loss: 0.0118
[09/28 12:20:28 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1746, average loss: 0.0125
[09/28 12:20:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:20:50 visual_prompt]: 	Test 100/190. loss: 3.756, 0.2042 s / batch. (data: 2.55e-05)max mem: 7.81234 GB 
[09/28 12:21:10 visual_prompt]: Inference (test):avg data time: 3.00e-05, avg batch time: 0.2043, average loss: 3.3935
[09/28 12:21:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.67	top5: 73.98	
[09/28 12:21:10 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/28 12:21:19 visual_prompt]: Epoch 74 / 100: avg data time: 8.88e-02, avg batch time: 0.5160, average train loss: 0.0112
[09/28 12:21:22 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1844, average loss: 0.0102
[09/28 12:21:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:21:44 visual_prompt]: 	Test 100/190. loss: 3.737, 0.2043 s / batch. (data: 2.98e-05)max mem: 7.81234 GB 
[09/28 12:22:04 visual_prompt]: Inference (test):avg data time: 3.02e-05, avg batch time: 0.2040, average loss: 3.3968
[09/28 12:22:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.80	top5: 74.14	
[09/28 12:22:04 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/28 12:22:13 visual_prompt]: Epoch 75 / 100: avg data time: 8.87e-02, avg batch time: 0.5156, average train loss: 0.0109
[09/28 12:22:16 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1767, average loss: 0.0110
[09/28 12:22:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:22:38 visual_prompt]: 	Test 100/190. loss: 3.730, 0.2035 s / batch. (data: 3.24e-05)max mem: 7.81234 GB 
[09/28 12:22:58 visual_prompt]: Inference (test):avg data time: 1.46e-04, avg batch time: 0.2042, average loss: 3.3984
[09/28 12:22:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.66	top5: 73.88	
[09/28 12:22:58 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/28 12:23:07 visual_prompt]: Epoch 76 / 100: avg data time: 8.97e-02, avg batch time: 0.5136, average train loss: 0.0105
[09/28 12:23:10 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1808, average loss: 0.0113
[09/28 12:23:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:23:33 visual_prompt]: 	Test 100/190. loss: 3.708, 0.2032 s / batch. (data: 3.15e-05)max mem: 7.81234 GB 
[09/28 12:23:52 visual_prompt]: Inference (test):avg data time: 3.00e-05, avg batch time: 0.2042, average loss: 3.4039
[09/28 12:23:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.59	top5: 73.88	
[09/28 12:23:52 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/28 12:24:02 visual_prompt]: Epoch 77 / 100: avg data time: 9.82e-02, avg batch time: 0.5237, average train loss: 0.0108
[09/28 12:24:05 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.1726, average loss: 0.0110
[09/28 12:24:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:24:27 visual_prompt]: 	Test 100/190. loss: 3.726, 0.2043 s / batch. (data: 2.84e-05)max mem: 7.81234 GB 
[09/28 12:24:46 visual_prompt]: Inference (test):avg data time: 3.08e-05, avg batch time: 0.2041, average loss: 3.3945
[09/28 12:24:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.84	top5: 73.84	
[09/28 12:24:46 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/28 12:24:56 visual_prompt]: Epoch 78 / 100: avg data time: 9.98e-02, avg batch time: 0.5249, average train loss: 0.0106
[09/28 12:24:59 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1720, average loss: 0.0096
[09/28 12:24:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:25:21 visual_prompt]: 	Test 100/190. loss: 3.715, 0.2053 s / batch. (data: 9.99e-05)max mem: 7.81234 GB 
[09/28 12:25:41 visual_prompt]: Inference (test):avg data time: 3.04e-05, avg batch time: 0.2042, average loss: 3.3944
[09/28 12:25:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.74	top5: 73.85	
[09/28 12:25:41 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/28 12:25:50 visual_prompt]: Epoch 79 / 100: avg data time: 1.00e-01, avg batch time: 0.5272, average train loss: 0.0100
[09/28 12:25:53 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1855, average loss: 0.0097
[09/28 12:25:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:26:15 visual_prompt]: 	Test 100/190. loss: 3.723, 0.2038 s / batch. (data: 2.86e-05)max mem: 7.81234 GB 
[09/28 12:26:35 visual_prompt]: Inference (test):avg data time: 2.98e-05, avg batch time: 0.2043, average loss: 3.3894
[09/28 12:26:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.86	top5: 73.91	
[09/28 12:26:35 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/28 12:26:44 visual_prompt]: Epoch 80 / 100: avg data time: 1.01e-01, avg batch time: 0.5282, average train loss: 0.0106
[09/28 12:26:48 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1792, average loss: 0.0103
[09/28 12:26:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:27:10 visual_prompt]: 	Test 100/190. loss: 3.711, 0.2041 s / batch. (data: 2.79e-05)max mem: 7.81234 GB 
[09/28 12:27:29 visual_prompt]: Inference (test):avg data time: 7.39e-05, avg batch time: 0.2041, average loss: 3.3774
[09/28 12:27:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.91	top5: 74.18	
[09/28 12:27:29 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/28 12:27:39 visual_prompt]: Epoch 81 / 100: avg data time: 1.12e-01, avg batch time: 0.5373, average train loss: 0.0102
[09/28 12:27:42 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1822, average loss: 0.0103
[09/28 12:27:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:28:04 visual_prompt]: 	Test 100/190. loss: 3.705, 0.2055 s / batch. (data: 2.93e-05)max mem: 7.81234 GB 
[09/28 12:28:24 visual_prompt]: Inference (test):avg data time: 3.01e-05, avg batch time: 0.2040, average loss: 3.3743
[09/28 12:28:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.16	top5: 73.97	
[09/28 12:28:24 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/28 12:28:33 visual_prompt]: Epoch 82 / 100: avg data time: 9.17e-02, avg batch time: 0.5167, average train loss: 0.0114
[09/28 12:28:36 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1746, average loss: 0.0102
[09/28 12:28:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:28:58 visual_prompt]: 	Test 100/190. loss: 3.705, 0.2034 s / batch. (data: 3.19e-05)max mem: 7.81234 GB 
[09/28 12:29:18 visual_prompt]: Inference (test):avg data time: 2.99e-05, avg batch time: 0.2040, average loss: 3.3733
[09/28 12:29:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.24	top5: 73.94	
[09/28 12:29:18 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/28 12:29:27 visual_prompt]: Epoch 83 / 100: avg data time: 1.09e-01, avg batch time: 0.5334, average train loss: 0.0095
[09/28 12:29:30 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1748, average loss: 0.0110
[09/28 12:29:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:29:53 visual_prompt]: 	Test 100/190. loss: 3.683, 0.2028 s / batch. (data: 2.84e-05)max mem: 7.81234 GB 
[09/28 12:30:12 visual_prompt]: Inference (test):avg data time: 2.96e-05, avg batch time: 0.2040, average loss: 3.3770
[09/28 12:30:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.12	top5: 73.87	
[09/28 12:30:12 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/28 12:30:22 visual_prompt]: Epoch 84 / 100: avg data time: 1.03e-01, avg batch time: 0.5307, average train loss: 0.0101
[09/28 12:30:25 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1831, average loss: 0.0113
[09/28 12:30:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:30:47 visual_prompt]: 	Test 100/190. loss: 3.681, 0.2047 s / batch. (data: 2.81e-05)max mem: 7.81234 GB 
[09/28 12:31:07 visual_prompt]: Inference (test):avg data time: 3.01e-05, avg batch time: 0.2039, average loss: 3.3770
[09/28 12:31:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.98	top5: 73.78	
[09/28 12:31:07 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/28 12:31:16 visual_prompt]: Epoch 85 / 100: avg data time: 1.04e-01, avg batch time: 0.5286, average train loss: 0.0098
[09/28 12:31:19 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1831, average loss: 0.0110
[09/28 12:31:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:31:41 visual_prompt]: 	Test 100/190. loss: 3.701, 0.2036 s / batch. (data: 2.50e-05)max mem: 7.81234 GB 
[09/28 12:32:01 visual_prompt]: Inference (test):avg data time: 3.86e-05, avg batch time: 0.2043, average loss: 3.3749
[09/28 12:32:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 30.04	top5: 73.77	
[09/28 12:32:01 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/28 12:32:11 visual_prompt]: Epoch 86 / 100: avg data time: 1.09e-01, avg batch time: 0.5356, average train loss: 0.0100
[09/28 12:32:14 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1808, average loss: 0.0105
[09/28 12:32:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:32:36 visual_prompt]: 	Test 100/190. loss: 3.689, 0.2045 s / batch. (data: 2.84e-05)max mem: 7.81234 GB 
[09/28 12:32:55 visual_prompt]: Inference (test):avg data time: 3.02e-05, avg batch time: 0.2044, average loss: 3.3757
[09/28 12:32:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.97	top5: 73.81	
[09/28 12:32:56 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/28 12:33:05 visual_prompt]: Epoch 87 / 100: avg data time: 9.76e-02, avg batch time: 0.5246, average train loss: 0.0098
[09/28 12:33:08 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1764, average loss: 0.0101
[09/28 12:33:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:33:30 visual_prompt]: 	Test 100/190. loss: 3.681, 0.2040 s / batch. (data: 2.77e-05)max mem: 7.81234 GB 
[09/28 12:33:50 visual_prompt]: Inference (test):avg data time: 3.03e-05, avg batch time: 0.2042, average loss: 3.3820
[09/28 12:33:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.80	top5: 73.95	
[09/28 12:33:50 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/28 12:33:59 visual_prompt]: Epoch 88 / 100: avg data time: 1.13e-01, avg batch time: 0.5369, average train loss: 0.0098
[09/28 12:34:02 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1778, average loss: 0.0101
[09/28 12:34:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:34:25 visual_prompt]: 	Test 100/190. loss: 3.685, 0.2045 s / batch. (data: 2.69e-05)max mem: 7.81234 GB 
[09/28 12:34:44 visual_prompt]: Inference (test):avg data time: 2.96e-05, avg batch time: 0.2042, average loss: 3.3842
[09/28 12:34:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.76	top5: 73.93	
[09/28 12:34:44 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/28 12:34:54 visual_prompt]: Epoch 89 / 100: avg data time: 9.59e-02, avg batch time: 0.5214, average train loss: 0.0094
[09/28 12:34:57 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1798, average loss: 0.0102
[09/28 12:34:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:35:19 visual_prompt]: 	Test 100/190. loss: 3.679, 0.2036 s / batch. (data: 2.69e-05)max mem: 7.81234 GB 
[09/28 12:35:39 visual_prompt]: Inference (test):avg data time: 3.04e-05, avg batch time: 0.2040, average loss: 3.3828
[09/28 12:35:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.84	top5: 73.87	
[09/28 12:35:39 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/28 12:35:48 visual_prompt]: Epoch 90 / 100: avg data time: 1.02e-01, avg batch time: 0.5284, average train loss: 0.0091
[09/28 12:35:51 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1732, average loss: 0.0101
[09/28 12:35:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:36:13 visual_prompt]: 	Test 100/190. loss: 3.680, 0.2034 s / batch. (data: 2.74e-05)max mem: 7.81234 GB 
[09/28 12:36:33 visual_prompt]: Inference (test):avg data time: 3.07e-05, avg batch time: 0.2040, average loss: 3.3825
[09/28 12:36:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.81	top5: 73.84	
[09/28 12:36:33 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/28 12:36:42 visual_prompt]: Epoch 91 / 100: avg data time: 9.60e-02, avg batch time: 0.5204, average train loss: 0.0096
[09/28 12:36:45 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1790, average loss: 0.0099
[09/28 12:36:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:37:07 visual_prompt]: 	Test 100/190. loss: 3.684, 0.2037 s / batch. (data: 3.03e-05)max mem: 7.81234 GB 
[09/28 12:37:27 visual_prompt]: Inference (test):avg data time: 2.96e-05, avg batch time: 0.2042, average loss: 3.3828
[09/28 12:37:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.88	top5: 73.78	
[09/28 12:37:27 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/28 12:37:36 visual_prompt]: Epoch 92 / 100: avg data time: 9.01e-02, avg batch time: 0.5165, average train loss: 0.0095
[09/28 12:37:39 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1585, average loss: 0.0099
[09/28 12:37:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:38:01 visual_prompt]: 	Test 100/190. loss: 3.688, 0.2041 s / batch. (data: 2.86e-05)max mem: 7.81234 GB 
[09/28 12:38:21 visual_prompt]: Inference (test):avg data time: 6.01e-05, avg batch time: 0.2045, average loss: 3.3830
[09/28 12:38:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.88	top5: 73.79	
[09/28 12:38:21 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/28 12:38:31 visual_prompt]: Epoch 93 / 100: avg data time: 1.11e-01, avg batch time: 0.5354, average train loss: 0.0094
[09/28 12:38:34 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.1813, average loss: 0.0099
[09/28 12:38:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:38:56 visual_prompt]: 	Test 100/190. loss: 3.687, 0.2055 s / batch. (data: 2.91e-05)max mem: 7.81234 GB 
[09/28 12:39:16 visual_prompt]: Inference (test):avg data time: 9.84e-05, avg batch time: 0.2044, average loss: 3.3823
[09/28 12:39:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.92	top5: 73.79	
[09/28 12:39:16 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/28 12:39:25 visual_prompt]: Epoch 94 / 100: avg data time: 1.10e-01, avg batch time: 0.5364, average train loss: 0.0090
[09/28 12:39:28 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1825, average loss: 0.0099
[09/28 12:39:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:39:50 visual_prompt]: 	Test 100/190. loss: 3.688, 0.2054 s / batch. (data: 3.00e-05)max mem: 7.81234 GB 
[09/28 12:40:10 visual_prompt]: Inference (test):avg data time: 3.10e-05, avg batch time: 0.2041, average loss: 3.3825
[09/28 12:40:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.94	top5: 73.77	
[09/28 12:40:10 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/28 12:40:20 visual_prompt]: Epoch 95 / 100: avg data time: 1.04e-01, avg batch time: 0.5315, average train loss: 0.0089
[09/28 12:40:23 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1765, average loss: 0.0099
[09/28 12:40:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:40:45 visual_prompt]: 	Test 100/190. loss: 3.687, 0.2036 s / batch. (data: 2.53e-05)max mem: 7.81234 GB 
[09/28 12:41:04 visual_prompt]: Inference (test):avg data time: 3.13e-05, avg batch time: 0.2041, average loss: 3.3827
[09/28 12:41:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.94	top5: 73.80	
[09/28 12:41:04 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/28 12:41:14 visual_prompt]: Epoch 96 / 100: avg data time: 1.07e-01, avg batch time: 0.5319, average train loss: 0.0098
[09/28 12:41:17 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1789, average loss: 0.0099
[09/28 12:41:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:41:39 visual_prompt]: 	Test 100/190. loss: 3.684, 0.2036 s / batch. (data: 3.79e-05)max mem: 7.81234 GB 
[09/28 12:41:59 visual_prompt]: Inference (test):avg data time: 2.99e-05, avg batch time: 0.2042, average loss: 3.3827
[09/28 12:41:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.93	top5: 73.77	
[09/28 12:41:59 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/28 12:42:08 visual_prompt]: Epoch 97 / 100: avg data time: 9.61e-02, avg batch time: 0.5233, average train loss: 0.0097
[09/28 12:42:11 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1835, average loss: 0.0099
[09/28 12:42:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:42:33 visual_prompt]: 	Test 100/190. loss: 3.683, 0.2037 s / batch. (data: 3.05e-05)max mem: 7.81234 GB 
[09/28 12:42:53 visual_prompt]: Inference (test):avg data time: 3.14e-05, avg batch time: 0.2042, average loss: 3.3823
[09/28 12:42:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.94	top5: 73.80	
[09/28 12:42:53 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/28 12:43:03 visual_prompt]: Epoch 98 / 100: avg data time: 1.05e-01, avg batch time: 0.5322, average train loss: 0.0094
[09/28 12:43:06 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1812, average loss: 0.0099
[09/28 12:43:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:43:28 visual_prompt]: 	Test 100/190. loss: 3.683, 0.2030 s / batch. (data: 2.93e-05)max mem: 7.81234 GB 
[09/28 12:43:47 visual_prompt]: Inference (test):avg data time: 1.07e-04, avg batch time: 0.2043, average loss: 3.3821
[09/28 12:43:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.93	top5: 73.80	
[09/28 12:43:47 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/28 12:43:57 visual_prompt]: Epoch 99 / 100: avg data time: 9.45e-02, avg batch time: 0.5209, average train loss: 0.0092
[09/28 12:44:00 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1839, average loss: 0.0099
[09/28 12:44:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:44:22 visual_prompt]: 	Test 100/190. loss: 3.683, 0.2037 s / batch. (data: 2.91e-05)max mem: 7.81234 GB 
[09/28 12:44:42 visual_prompt]: Inference (test):avg data time: 1.28e-04, avg batch time: 0.2042, average loss: 3.3820
[09/28 12:44:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.95	top5: 73.79	
[09/28 12:44:42 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/28 12:44:51 visual_prompt]: Epoch 100 / 100: avg data time: 9.77e-02, avg batch time: 0.5231, average train loss: 0.0092
[09/28 12:44:54 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.1795, average loss: 0.0099
[09/28 12:44:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 100.00	top5: 100.00	
[09/28 12:45:16 visual_prompt]: 	Test 100/190. loss: 3.683, 0.2059 s / batch. (data: 2.91e-05)max mem: 7.81234 GB 
[09/28 12:45:36 visual_prompt]: Inference (test):avg data time: 3.04e-05, avg batch time: 0.2044, average loss: 3.3820
[09/28 12:45:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_azimuth"): top1: 29.95	top5: 73.79	

/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '1', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/06 17:40:58 visual_prompt]: Rank of current process: 0. World size: 1
[11/06 17:40:58 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/06 17:40:58 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '1', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/06 17:40:58 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/06 17:40:58 visual_prompt]: Training with config:
[11/06 17:40:58 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/crop224/val/seed0/lr0.005_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 1, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/06 17:40:58 visual_prompt]: Loading training data...
[11/06 17:40:58 visual_prompt]: Constructing mammo-cbis dataset train...
[11/06 17:40:58 visual_prompt]: Loading validation data...
[11/06 17:40:58 visual_prompt]: Constructing mammo-cbis dataset val...
[11/06 17:40:58 visual_prompt]: Constructing models...
[11/06 17:41:01 visual_prompt]: Enable all parameters update during training
[11/06 17:41:01 visual_prompt]: Total Parameters: 85800194	 Gradient Parameters: 85800194
[11/06 17:41:01 visual_prompt]: tuned percent:100.000
[11/06 17:41:01 visual_prompt]: Device used for model: 0
[11/06 17:41:01 visual_prompt]: Setting up Evaluator...
[11/06 17:41:01 visual_prompt]: Setting up Trainer...
[11/06 17:41:01 visual_prompt]: 	Setting up the optimizer...
[11/06 17:41:01 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/06 17:41:25 visual_prompt]: 	Training 100/2212. train loss: 17.0073,	0.1965 s / batch. (data: 2.57e-02). ETA=12:04:03, max mem: 1.4 GB 
[11/06 17:41:46 visual_prompt]: 	Training 200/2212. train loss: 8.7706,	0.1793 s / batch. (data: 9.77e-04). ETA=11:00:28, max mem: 1.4 GB 
[11/06 17:42:06 visual_prompt]: 	Training 300/2212. train loss: 0.0001,	0.1726 s / batch. (data: 1.08e-02). ETA=10:35:26, max mem: 1.4 GB 
[11/06 17:42:25 visual_prompt]: 	Training 400/2212. train loss: 9.7364,	0.1248 s / batch. (data: 5.52e-04). ETA=7:39:18, max mem: 1.4 GB 
[11/06 17:42:46 visual_prompt]: 	Training 500/2212. train loss: 9.6752,	0.1014 s / batch. (data: 1.52e-02). ETA=6:12:51, max mem: 1.4 GB 
[11/06 17:43:07 visual_prompt]: 	Training 600/2212. train loss: 0.0000,	0.1929 s / batch. (data: 1.60e-02). ETA=11:49:22, max mem: 1.4 GB 
[11/06 17:43:28 visual_prompt]: 	Training 700/2212. train loss: 10.7484,	0.1284 s / batch. (data: 1.47e-02). ETA=7:51:49, max mem: 1.4 GB 
[11/06 17:43:48 visual_prompt]: 	Training 800/2212. train loss: 10.3468,	0.1992 s / batch. (data: 5.70e-03). ETA=12:11:49, max mem: 1.4 GB 
[11/06 17:44:09 visual_prompt]: 	Training 900/2212. train loss: 0.0000,	0.1970 s / batch. (data: 4.42e-03). ETA=12:03:09, max mem: 1.4 GB 
[11/06 17:44:28 visual_prompt]: 	Training 1000/2212. train loss: 0.0001,	0.1814 s / batch. (data: 4.78e-02). ETA=11:05:42, max mem: 1.4 GB 
[11/06 17:44:48 visual_prompt]: 	Training 1100/2212. train loss: 0.0000,	0.0512 s / batch. (data: 7.36e-04). ETA=3:07:40, max mem: 1.4 GB 
[11/06 17:45:08 visual_prompt]: 	Training 1200/2212. train loss: 0.0000,	0.1525 s / batch. (data: 1.54e-02). ETA=9:19:16, max mem: 1.4 GB 
[11/06 17:45:27 visual_prompt]: 	Training 1300/2212. train loss: 0.0000,	0.1580 s / batch. (data: 1.82e-04). ETA=9:39:07, max mem: 1.4 GB 
[11/06 17:45:49 visual_prompt]: 	Training 1400/2212. train loss: 0.0027,	0.1339 s / batch. (data: 1.80e-04). ETA=8:10:36, max mem: 1.4 GB 
[11/06 17:46:09 visual_prompt]: 	Training 1500/2212. train loss: 5.3648,	0.1425 s / batch. (data: 3.45e-02). ETA=8:41:43, max mem: 1.4 GB 
[11/06 17:46:28 visual_prompt]: 	Training 1600/2212. train loss: 5.2697,	0.1756 s / batch. (data: 1.76e-04). ETA=10:42:48, max mem: 1.4 GB 
[11/06 17:46:48 visual_prompt]: 	Training 1700/2212. train loss: 0.0000,	0.1681 s / batch. (data: 1.31e-02). ETA=10:14:59, max mem: 1.4 GB 
[11/06 17:47:09 visual_prompt]: 	Training 1800/2212. train loss: 0.0000,	0.4998 s / batch. (data: 4.06e-01). ETA=1 day, 6:27:41, max mem: 1.4 GB 
[11/06 17:47:28 visual_prompt]: 	Training 1900/2212. train loss: 8.6260,	0.1886 s / batch. (data: 2.37e-02). ETA=11:29:12, max mem: 1.4 GB 
[11/06 17:47:49 visual_prompt]: 	Training 2000/2212. train loss: 0.0002,	0.1231 s / batch. (data: 2.48e-04). ETA=7:29:39, max mem: 1.4 GB 
[11/06 17:48:08 visual_prompt]: 	Training 2100/2212. train loss: 14.9015,	0.1639 s / batch. (data: 3.37e-02). ETA=9:58:35, max mem: 1.4 GB 
[11/06 17:48:27 visual_prompt]: 	Training 2200/2212. train loss: 7.6808,	0.0946 s / batch. (data: 7.13e-05). ETA=5:45:13, max mem: 1.4 GB 
[11/06 17:48:28 visual_prompt]: Epoch 1 / 100: avg data time: 5.80e-02, avg batch time: 0.2020, average train loss: 5.2946
[11/06 17:48:48 visual_prompt]: 	Test 100/246. loss: 0.001, 0.0572 s / batch. (data: 1.93e-05)max mem: 1.44029 GB 
[11/06 17:49:07 visual_prompt]: 	Test 200/246. loss: 0.002, 0.0368 s / batch. (data: 2.53e-05)max mem: 1.44029 GB 
[11/06 17:49:14 visual_prompt]: Inference (val):avg data time: 4.11e-04, avg batch time: 0.0337, average loss: 4.3337
[11/06 17:49:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.86	
[11/06 17:49:14 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/06 17:49:37 visual_prompt]: 	Training 100/2212. train loss: 0.0110,	0.1568 s / batch. (data: 1.03e-02). ETA=9:32:06, max mem: 1.4 GB 
[11/06 17:49:57 visual_prompt]: 	Training 200/2212. train loss: 0.0058,	0.1952 s / batch. (data: 4.64e-02). ETA=11:51:51, max mem: 1.4 GB 
[11/06 17:50:17 visual_prompt]: 	Training 300/2212. train loss: 0.3867,	0.1659 s / batch. (data: 1.05e-02). ETA=10:04:46, max mem: 1.4 GB 
[11/06 17:50:36 visual_prompt]: 	Training 400/2212. train loss: 1.3777,	0.1865 s / batch. (data: 1.54e-02). ETA=11:19:34, max mem: 1.4 GB 
[11/06 17:50:55 visual_prompt]: 	Training 500/2212. train loss: 0.3260,	0.1961 s / batch. (data: 1.45e-02). ETA=11:54:13, max mem: 1.4 GB 
[11/06 17:51:14 visual_prompt]: 	Training 600/2212. train loss: 0.2823,	0.2053 s / batch. (data: 3.95e-02). ETA=12:27:21, max mem: 1.4 GB 
[11/06 17:51:33 visual_prompt]: 	Training 700/2212. train loss: 1.1778,	0.1542 s / batch. (data: 2.25e-02). ETA=9:20:53, max mem: 1.4 GB 
[11/06 17:51:54 visual_prompt]: 	Training 800/2212. train loss: 0.2036,	0.1317 s / batch. (data: 1.67e-04). ETA=7:58:58, max mem: 1.4 GB 
[11/06 17:52:14 visual_prompt]: 	Training 900/2212. train loss: 0.3994,	0.1787 s / batch. (data: 1.04e-02). ETA=10:49:33, max mem: 1.4 GB 
[11/06 17:52:35 visual_prompt]: 	Training 1000/2212. train loss: 1.7676,	0.1689 s / batch. (data: 5.33e-03). ETA=10:13:33, max mem: 1.4 GB 
[11/06 17:52:55 visual_prompt]: 	Training 1100/2212. train loss: 0.0406,	0.1981 s / batch. (data: 3.61e-02). ETA=11:59:14, max mem: 1.4 GB 
[11/06 17:53:14 visual_prompt]: 	Training 1200/2212. train loss: 0.8795,	0.1856 s / batch. (data: 1.49e-02). ETA=11:13:47, max mem: 1.4 GB 
[11/06 17:53:36 visual_prompt]: 	Training 1300/2212. train loss: 1.6760,	0.9187 s / batch. (data: 7.72e-01). ETA=2 days, 7:33:10, max mem: 1.4 GB 
[11/06 17:53:56 visual_prompt]: 	Training 1400/2212. train loss: 1.4970,	0.6038 s / batch. (data: 4.60e-01). ETA=1 day, 12:29:29, max mem: 1.4 GB 
[11/06 17:54:16 visual_prompt]: 	Training 1500/2212. train loss: 0.9697,	0.1986 s / batch. (data: 2.09e-02). ETA=12:00:00, max mem: 1.4 GB 
[11/06 17:54:36 visual_prompt]: 	Training 1600/2212. train loss: 1.3313,	0.2046 s / batch. (data: 1.08e-02). ETA=12:21:18, max mem: 1.4 GB 
[11/06 17:54:55 visual_prompt]: 	Training 1700/2212. train loss: 0.3226,	0.1521 s / batch. (data: 4.66e-04). ETA=9:10:41, max mem: 1.4 GB 
[11/06 17:55:15 visual_prompt]: 	Training 1800/2212. train loss: 0.4831,	0.1350 s / batch. (data: 2.03e-04). ETA=8:08:50, max mem: 1.4 GB 
[11/06 17:55:35 visual_prompt]: 	Training 1900/2212. train loss: 0.1575,	0.0877 s / batch. (data: 1.76e-04). ETA=5:17:28, max mem: 1.4 GB 
[11/06 17:55:54 visual_prompt]: 	Training 2000/2212. train loss: 0.6884,	0.1528 s / batch. (data: 5.34e-03). ETA=9:12:37, max mem: 1.4 GB 
[11/06 17:56:13 visual_prompt]: 	Training 2100/2212. train loss: 0.1138,	0.2040 s / batch. (data: 2.13e-03). ETA=12:17:17, max mem: 1.4 GB 
[11/06 17:56:33 visual_prompt]: 	Training 2200/2212. train loss: 0.4864,	0.0478 s / batch. (data: 7.37e-05). ETA=2:52:52, max mem: 1.4 GB 
[11/06 17:56:33 visual_prompt]: Epoch 2 / 100: avg data time: 4.91e-02, avg batch time: 0.1985, average train loss: 1.4344
[11/06 17:56:53 visual_prompt]: 	Test 100/246. loss: 0.152, 0.0561 s / batch. (data: 1.93e-05)max mem: 1.44029 GB 
[11/06 17:57:11 visual_prompt]: 	Test 200/246. loss: 0.149, 0.0627 s / batch. (data: 2.05e-05)max mem: 1.44029 GB 
[11/06 17:57:19 visual_prompt]: Inference (val):avg data time: 8.42e-04, avg batch time: 0.0410, average loss: 1.1422
[11/06 17:57:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.83	
[11/06 17:57:19 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/06 17:57:40 visual_prompt]: 	Training 100/2212. train loss: 0.4078,	0.1850 s / batch. (data: 5.53e-04). ETA=11:08:07, max mem: 1.4 GB 
[11/06 17:58:00 visual_prompt]: 	Training 200/2212. train loss: 3.2550,	0.1843 s / batch. (data: 4.50e-02). ETA=11:05:14, max mem: 1.4 GB 
[11/06 17:58:20 visual_prompt]: 	Training 300/2212. train loss: 2.6896,	0.1335 s / batch. (data: 5.29e-03). ETA=8:01:48, max mem: 1.4 GB 
[11/06 17:58:41 visual_prompt]: 	Training 400/2212. train loss: 1.0970,	0.1720 s / batch. (data: 3.71e-02). ETA=10:20:21, max mem: 1.4 GB 
[11/06 17:59:00 visual_prompt]: 	Training 500/2212. train loss: 2.0802,	0.1702 s / batch. (data: 1.65e-02). ETA=10:13:19, max mem: 1.4 GB 
[11/06 17:59:21 visual_prompt]: 	Training 600/2212. train loss: 1.2381,	0.1572 s / batch. (data: 7.12e-03). ETA=9:26:23, max mem: 1.4 GB 
[11/06 17:59:41 visual_prompt]: 	Training 700/2212. train loss: 0.2807,	0.2688 s / batch. (data: 1.43e-01). ETA=16:08:06, max mem: 1.4 GB 
[11/06 18:00:01 visual_prompt]: 	Training 800/2212. train loss: 1.7275,	0.1687 s / batch. (data: 1.07e-02). ETA=10:07:16, max mem: 1.4 GB 
[11/06 18:00:21 visual_prompt]: 	Training 900/2212. train loss: 1.1637,	0.2015 s / batch. (data: 2.05e-02). ETA=12:04:52, max mem: 1.4 GB 
[11/06 18:00:40 visual_prompt]: 	Training 1000/2212. train loss: 0.1545,	0.2080 s / batch. (data: 3.95e-02). ETA=12:28:09, max mem: 1.4 GB 
[11/06 18:01:00 visual_prompt]: 	Training 1100/2212. train loss: 0.3338,	0.1128 s / batch. (data: 1.66e-04). ETA=6:45:19, max mem: 1.4 GB 
[11/06 18:01:18 visual_prompt]: 	Training 1200/2212. train loss: 1.1778,	0.1212 s / batch. (data: 2.69e-04). ETA=7:15:27, max mem: 1.4 GB 
[11/06 18:01:39 visual_prompt]: 	Training 1300/2212. train loss: 1.6652,	0.0875 s / batch. (data: 1.56e-04). ETA=5:14:20, max mem: 1.4 GB 
[11/06 18:01:59 visual_prompt]: 	Training 1400/2212. train loss: 0.5532,	0.1238 s / batch. (data: 1.03e-02). ETA=7:24:19, max mem: 1.4 GB 
[11/06 18:02:18 visual_prompt]: 	Training 1500/2212. train loss: 0.0957,	0.1718 s / batch. (data: 1.04e-02). ETA=10:16:34, max mem: 1.4 GB 
[11/06 18:02:37 visual_prompt]: 	Training 1600/2212. train loss: 0.1149,	0.1804 s / batch. (data: 7.82e-03). ETA=10:47:07, max mem: 1.4 GB 
[11/06 18:02:57 visual_prompt]: 	Training 1700/2212. train loss: 0.9551,	0.1505 s / batch. (data: 2.11e-04). ETA=8:59:37, max mem: 1.4 GB 
[11/06 18:03:17 visual_prompt]: 	Training 1800/2212. train loss: 0.3133,	0.1607 s / batch. (data: 5.28e-03). ETA=9:35:44, max mem: 1.4 GB 
[11/06 18:03:36 visual_prompt]: 	Training 1900/2212. train loss: 0.2022,	0.1656 s / batch. (data: 6.64e-03). ETA=9:52:59, max mem: 1.4 GB 
[11/06 18:03:55 visual_prompt]: 	Training 2000/2212. train loss: 1.7955,	0.1804 s / batch. (data: 5.28e-03). ETA=10:45:40, max mem: 1.4 GB 
[11/06 18:04:17 visual_prompt]: 	Training 2100/2212. train loss: 3.6632,	0.1840 s / batch. (data: 1.07e-02). ETA=10:58:16, max mem: 1.4 GB 
[11/06 18:04:36 visual_prompt]: 	Training 2200/2212. train loss: 0.5762,	0.6233 s / batch. (data: 5.41e-01). ETA=1 day, 13:08:57, max mem: 1.4 GB 
[11/06 18:04:37 visual_prompt]: Epoch 3 / 100: avg data time: 4.89e-02, avg batch time: 0.1981, average train loss: 1.0003
[11/06 18:04:57 visual_prompt]: 	Test 100/246. loss: 0.710, 0.0556 s / batch. (data: 1.93e-05)max mem: 1.44029 GB 
[11/06 18:05:15 visual_prompt]: 	Test 200/246. loss: 0.706, 0.0595 s / batch. (data: 2.43e-05)max mem: 1.44029 GB 
[11/06 18:05:23 visual_prompt]: Inference (val):avg data time: 5.73e-04, avg batch time: 0.0408, average loss: 0.6914
[11/06 18:05:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 53.00	
[11/06 18:05:23 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/06 18:05:46 visual_prompt]: 	Training 100/2212. train loss: 1.9716,	0.1493 s / batch. (data: 1.07e-02). ETA=8:53:42, max mem: 1.4 GB 
[11/06 18:06:04 visual_prompt]: 	Training 200/2212. train loss: 0.6584,	0.1828 s / batch. (data: 1.54e-02). ETA=10:53:10, max mem: 1.4 GB 
[11/06 18:06:24 visual_prompt]: 	Training 300/2212. train loss: 0.3780,	0.1783 s / batch. (data: 1.55e-02). ETA=10:36:38, max mem: 1.4 GB 
[11/06 18:06:44 visual_prompt]: 	Training 400/2212. train loss: 1.4136,	0.2033 s / batch. (data: 6.99e-03). ETA=12:05:35, max mem: 1.4 GB 
[11/06 18:07:03 visual_prompt]: 	Training 500/2212. train loss: 0.4189,	0.1439 s / batch. (data: 6.16e-04). ETA=8:33:25, max mem: 1.4 GB 
[11/06 18:07:22 visual_prompt]: 	Training 600/2212. train loss: 0.3889,	0.1579 s / batch. (data: 6.16e-03). ETA=9:23:04, max mem: 1.4 GB 
[11/06 18:07:42 visual_prompt]: 	Training 700/2212. train loss: 0.8303,	0.1929 s / batch. (data: 1.55e-02). ETA=11:27:33, max mem: 1.4 GB 
[11/06 18:08:03 visual_prompt]: 	Training 800/2212. train loss: 1.9113,	0.1919 s / batch. (data: 1.45e-02). ETA=11:23:43, max mem: 1.4 GB 
[11/06 18:08:22 visual_prompt]: 	Training 900/2212. train loss: 0.4549,	0.1949 s / batch. (data: 1.07e-03). ETA=11:34:02, max mem: 1.4 GB 
[11/06 18:08:41 visual_prompt]: 	Training 1000/2212. train loss: 0.8969,	0.1391 s / batch. (data: 1.54e-02). ETA=8:15:10, max mem: 1.4 GB 
[11/06 18:09:01 visual_prompt]: 	Training 1100/2212. train loss: 2.6368,	0.1997 s / batch. (data: 1.04e-02). ETA=11:50:33, max mem: 1.4 GB 
[11/06 18:09:21 visual_prompt]: 	Training 1200/2212. train loss: 0.4055,	0.1291 s / batch. (data: 7.44e-03). ETA=7:38:54, max mem: 1.4 GB 
[11/06 18:09:41 visual_prompt]: 	Training 1300/2212. train loss: 0.8095,	0.1625 s / batch. (data: 1.04e-02). ETA=9:37:30, max mem: 1.4 GB 
[11/06 18:10:01 visual_prompt]: 	Training 1400/2212. train loss: 0.6338,	0.2033 s / batch. (data: 5.74e-03). ETA=12:02:12, max mem: 1.4 GB 
[11/06 18:10:21 visual_prompt]: 	Training 1500/2212. train loss: 0.5614,	0.1683 s / batch. (data: 1.95e-04). ETA=9:57:49, max mem: 1.4 GB 
[11/06 18:10:41 visual_prompt]: 	Training 1600/2212. train loss: 0.2994,	0.1328 s / batch. (data: 1.03e-02). ETA=7:51:31, max mem: 1.4 GB 
[11/06 18:11:01 visual_prompt]: 	Training 1700/2212. train loss: 0.0828,	0.1482 s / batch. (data: 1.03e-02). ETA=8:45:46, max mem: 1.4 GB 
[11/06 18:11:19 visual_prompt]: 	Training 1800/2212. train loss: 0.9597,	0.1337 s / batch. (data: 1.54e-02). ETA=7:53:58, max mem: 1.4 GB 
[11/06 18:11:40 visual_prompt]: 	Training 1900/2212. train loss: 0.6607,	0.1610 s / batch. (data: 5.40e-03). ETA=9:30:41, max mem: 1.4 GB 
[11/06 18:12:00 visual_prompt]: 	Training 2000/2212. train loss: 1.0044,	0.1931 s / batch. (data: 1.58e-02). ETA=11:24:05, max mem: 1.4 GB 
[11/06 18:12:20 visual_prompt]: 	Training 2100/2212. train loss: 0.9435,	0.1556 s / batch. (data: 5.26e-03). ETA=9:10:52, max mem: 1.4 GB 
[11/06 18:12:40 visual_prompt]: 	Training 2200/2212. train loss: 0.3127,	0.1280 s / batch. (data: 9.68e-05). ETA=7:33:01, max mem: 1.4 GB 
[11/06 18:12:41 visual_prompt]: Epoch 4 / 100: avg data time: 4.81e-02, avg batch time: 0.1982, average train loss: 0.8940
[11/06 18:13:00 visual_prompt]: 	Test 100/246. loss: 1.190, 0.0617 s / batch. (data: 2.12e-05)max mem: 1.44029 GB 
[11/06 18:13:19 visual_prompt]: 	Test 200/246. loss: 1.188, 0.0631 s / batch. (data: 1.98e-05)max mem: 1.44029 GB 
[11/06 18:13:26 visual_prompt]: Inference (val):avg data time: 9.23e-04, avg batch time: 0.0409, average loss: 0.7343
[11/06 18:13:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.18	
[11/06 18:13:26 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/06 18:13:48 visual_prompt]: 	Training 100/2212. train loss: 2.0024,	0.1536 s / batch. (data: 2.24e-02). ETA=9:03:24, max mem: 1.4 GB 
[11/06 18:14:08 visual_prompt]: 	Training 200/2212. train loss: 1.0589,	0.1428 s / batch. (data: 6.14e-04). ETA=8:24:50, max mem: 1.4 GB 
[11/06 18:14:28 visual_prompt]: 	Training 300/2212. train loss: 1.5058,	0.1896 s / batch. (data: 2.10e-02). ETA=11:10:12, max mem: 1.4 GB 
[11/06 18:14:48 visual_prompt]: 	Training 400/2212. train loss: 2.6446,	0.1342 s / batch. (data: 2.10e-04). ETA=7:54:12, max mem: 1.4 GB 
[11/06 18:15:08 visual_prompt]: 	Training 500/2212. train loss: 0.6108,	0.1639 s / batch. (data: 2.04e-04). ETA=9:38:33, max mem: 1.4 GB 
[11/06 18:15:28 visual_prompt]: 	Training 600/2212. train loss: 0.6044,	0.1743 s / batch. (data: 1.05e-02). ETA=10:15:15, max mem: 1.4 GB 
[11/06 18:15:46 visual_prompt]: 	Training 700/2212. train loss: 1.1370,	0.1918 s / batch. (data: 1.04e-02). ETA=11:16:44, max mem: 1.4 GB 
[11/06 18:16:07 visual_prompt]: 	Training 800/2212. train loss: 0.5491,	0.1804 s / batch. (data: 5.69e-03). ETA=10:36:11, max mem: 1.4 GB 
[11/06 18:16:27 visual_prompt]: 	Training 900/2212. train loss: 0.4044,	0.1775 s / batch. (data: 5.27e-03). ETA=10:25:23, max mem: 1.4 GB 
[11/06 18:16:47 visual_prompt]: 	Training 1000/2212. train loss: 0.8233,	0.8368 s / batch. (data: 7.09e-01). ETA=2 days, 1:07:33, max mem: 1.4 GB 
[11/06 18:17:06 visual_prompt]: 	Training 1100/2212. train loss: 0.5330,	0.1697 s / batch. (data: 2.00e-02). ETA=9:57:35, max mem: 1.4 GB 
[11/06 18:17:25 visual_prompt]: 	Training 1200/2212. train loss: 0.9785,	0.1514 s / batch. (data: 5.28e-03). ETA=8:52:46, max mem: 1.4 GB 
[11/06 18:17:45 visual_prompt]: 	Training 1300/2212. train loss: 0.3873,	0.1259 s / batch. (data: 1.54e-02). ETA=7:22:49, max mem: 1.4 GB 
[11/06 18:18:05 visual_prompt]: 	Training 1400/2212. train loss: 0.6895,	0.1292 s / batch. (data: 5.29e-03). ETA=7:34:09, max mem: 1.4 GB 
[11/06 18:18:25 visual_prompt]: 	Training 1500/2212. train loss: 1.2190,	1.1382 s / batch. (data: 1.03e+00). ETA=2 days, 18:39:53, max mem: 1.4 GB 
[11/06 18:18:45 visual_prompt]: 	Training 1600/2212. train loss: 0.7691,	0.1830 s / batch. (data: 5.87e-04). ETA=10:42:40, max mem: 1.4 GB 
[11/06 18:19:05 visual_prompt]: 	Training 1700/2212. train loss: 0.6337,	0.1794 s / batch. (data: 1.49e-04). ETA=10:29:54, max mem: 1.4 GB 
[11/06 18:19:26 visual_prompt]: 	Training 1800/2212. train loss: 0.7414,	0.1886 s / batch. (data: 5.48e-04). ETA=11:01:41, max mem: 1.4 GB 
[11/06 18:19:46 visual_prompt]: 	Training 1900/2212. train loss: 0.7196,	0.1596 s / batch. (data: 5.60e-03). ETA=9:19:54, max mem: 1.4 GB 
[11/06 18:20:05 visual_prompt]: 	Training 2000/2212. train loss: 0.6091,	0.1734 s / batch. (data: 2.05e-02). ETA=10:07:46, max mem: 1.4 GB 
[11/06 18:20:24 visual_prompt]: 	Training 2100/2212. train loss: 0.6745,	0.1639 s / batch. (data: 1.54e-02). ETA=9:34:26, max mem: 1.4 GB 
[11/06 18:20:44 visual_prompt]: 	Training 2200/2212. train loss: 0.7071,	0.0971 s / batch. (data: 7.77e-05). ETA=5:40:03, max mem: 1.4 GB 
[11/06 18:20:45 visual_prompt]: Epoch 5 / 100: avg data time: 4.83e-02, avg batch time: 0.1981, average train loss: 0.7450
[11/06 18:21:04 visual_prompt]: 	Test 100/246. loss: 0.748, 0.0454 s / batch. (data: 2.05e-05)max mem: 1.44029 GB 
[11/06 18:21:23 visual_prompt]: 	Test 200/246. loss: 0.748, 0.0369 s / batch. (data: 2.43e-05)max mem: 1.44029 GB 
[11/06 18:21:30 visual_prompt]: Inference (val):avg data time: 5.20e-04, avg batch time: 0.0399, average loss: 0.6893
[11/06 18:21:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.04	
[11/06 18:21:30 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/06 18:21:52 visual_prompt]: 	Training 100/2212. train loss: 2.2765,	0.1563 s / batch. (data: 1.87e-04). ETA=9:07:02, max mem: 1.4 GB 
[11/06 18:22:12 visual_prompt]: 	Training 200/2212. train loss: 0.6401,	0.1669 s / batch. (data: 1.03e-02). ETA=9:44:03, max mem: 1.4 GB 
[11/06 18:22:32 visual_prompt]: 	Training 300/2212. train loss: 0.4012,	0.1749 s / batch. (data: 1.04e-02). ETA=10:11:44, max mem: 1.4 GB 
[11/06 18:22:51 visual_prompt]: 	Training 400/2212. train loss: 0.6135,	0.1267 s / batch. (data: 1.83e-04). ETA=7:22:57, max mem: 1.4 GB 
[11/06 18:23:11 visual_prompt]: 	Training 500/2212. train loss: 0.5928,	0.1613 s / batch. (data: 1.59e-02). ETA=9:23:26, max mem: 1.4 GB 
[11/06 18:23:31 visual_prompt]: 	Training 600/2212. train loss: 0.6834,	0.1848 s / batch. (data: 1.69e-02). ETA=10:45:29, max mem: 1.4 GB 
[11/06 18:23:52 visual_prompt]: 	Training 700/2212. train loss: 0.5909,	0.1835 s / batch. (data: 2.31e-02). ETA=10:40:25, max mem: 1.4 GB 
[11/06 18:24:12 visual_prompt]: 	Training 800/2212. train loss: 0.7881,	0.1641 s / batch. (data: 1.04e-02). ETA=9:32:33, max mem: 1.4 GB 
[11/06 18:24:31 visual_prompt]: 	Training 900/2212. train loss: 0.7856,	0.1719 s / batch. (data: 1.04e-02). ETA=9:59:20, max mem: 1.4 GB 
[11/06 18:24:52 visual_prompt]: 	Training 1000/2212. train loss: 0.7910,	0.1519 s / batch. (data: 1.04e-02). ETA=8:49:31, max mem: 1.4 GB 
[11/06 18:25:11 visual_prompt]: 	Training 1100/2212. train loss: 0.5982,	0.1200 s / batch. (data: 1.82e-04). ETA=6:58:12, max mem: 1.4 GB 
[11/06 18:25:29 visual_prompt]: 	Training 1200/2212. train loss: 0.6075,	0.1651 s / batch. (data: 1.04e-02). ETA=9:34:59, max mem: 1.4 GB 
[11/06 18:25:49 visual_prompt]: 	Training 1300/2212. train loss: 0.5702,	0.1903 s / batch. (data: 1.55e-02). ETA=11:02:23, max mem: 1.4 GB 
[11/06 18:26:10 visual_prompt]: 	Training 1400/2212. train loss: 0.4948,	0.1928 s / batch. (data: 2.27e-02). ETA=11:10:51, max mem: 1.4 GB 
[11/06 18:26:30 visual_prompt]: 	Training 1500/2212. train loss: 0.6284,	0.1439 s / batch. (data: 6.21e-04). ETA=8:20:27, max mem: 1.4 GB 
[11/06 18:26:50 visual_prompt]: 	Training 1600/2212. train loss: 0.7654,	0.1765 s / batch. (data: 1.58e-02). ETA=10:13:37, max mem: 1.4 GB 
[11/06 18:27:10 visual_prompt]: 	Training 1700/2212. train loss: 0.9046,	0.1670 s / batch. (data: 1.03e-02). ETA=9:40:14, max mem: 1.4 GB 
[11/06 18:27:31 visual_prompt]: 	Training 1800/2212. train loss: 0.6844,	0.1727 s / batch. (data: 5.72e-03). ETA=9:59:34, max mem: 1.4 GB 
[11/06 18:27:50 visual_prompt]: 	Training 1900/2212. train loss: 0.5398,	0.1690 s / batch. (data: 1.04e-02). ETA=9:46:34, max mem: 1.4 GB 
[11/06 18:28:09 visual_prompt]: 	Training 2000/2212. train loss: 0.9050,	0.1344 s / batch. (data: 2.15e-04). ETA=7:46:11, max mem: 1.4 GB 
[11/06 18:28:28 visual_prompt]: 	Training 2100/2212. train loss: 0.8501,	0.4406 s / batch. (data: 3.07e-01). ETA=1 day, 1:27:46, max mem: 1.4 GB 
[11/06 18:28:48 visual_prompt]: 	Training 2200/2212. train loss: 0.6358,	0.0957 s / batch. (data: 8.68e-05). ETA=5:31:44, max mem: 1.4 GB 
[11/06 18:28:49 visual_prompt]: Epoch 6 / 100: avg data time: 4.80e-02, avg batch time: 0.1982, average train loss: 0.7096
[11/06 18:29:08 visual_prompt]: 	Test 100/246. loss: 0.761, 0.0608 s / batch. (data: 2.03e-05)max mem: 1.44029 GB 
[11/06 18:29:26 visual_prompt]: 	Test 200/246. loss: 0.761, 0.0387 s / batch. (data: 2.03e-05)max mem: 1.44029 GB 
[11/06 18:29:34 visual_prompt]: Inference (val):avg data time: 5.06e-04, avg batch time: 0.0408, average loss: 0.6889
[11/06 18:29:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.95	
[11/06 18:29:34 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/06 18:29:55 visual_prompt]: 	Training 100/2212. train loss: 0.4900,	0.1691 s / batch. (data: 5.67e-03). ETA=9:45:45, max mem: 1.4 GB 
[11/06 18:30:17 visual_prompt]: 	Training 200/2212. train loss: 0.3997,	0.1650 s / batch. (data: 2.05e-02). ETA=9:31:12, max mem: 1.4 GB 
[11/06 18:30:39 visual_prompt]: 	Training 300/2212. train loss: 0.8664,	0.1529 s / batch. (data: 1.58e-02). ETA=8:49:02, max mem: 1.4 GB 
[11/06 18:30:59 visual_prompt]: 	Training 400/2212. train loss: 0.8413,	0.1081 s / batch. (data: 8.59e-03). ETA=6:13:45, max mem: 1.4 GB 
[11/06 18:31:19 visual_prompt]: 	Training 500/2212. train loss: 0.7022,	0.1734 s / batch. (data: 1.72e-02). ETA=9:59:27, max mem: 1.4 GB 
[11/06 18:31:38 visual_prompt]: 	Training 600/2212. train loss: 0.6897,	0.1544 s / batch. (data: 1.65e-02). ETA=8:53:27, max mem: 1.4 GB 
[11/06 18:31:58 visual_prompt]: 	Training 700/2212. train loss: 0.6029,	0.1715 s / batch. (data: 1.54e-02). ETA=9:52:11, max mem: 1.4 GB 
[11/06 18:32:17 visual_prompt]: 	Training 800/2212. train loss: 0.6255,	0.1678 s / batch. (data: 1.54e-02). ETA=9:39:20, max mem: 1.4 GB 
[11/06 18:32:37 visual_prompt]: 	Training 900/2212. train loss: 0.7247,	0.1709 s / batch. (data: 1.45e-02). ETA=9:49:31, max mem: 1.4 GB 
[11/06 18:32:57 visual_prompt]: 	Training 1000/2212. train loss: 0.6855,	0.1735 s / batch. (data: 8.40e-03). ETA=9:58:13, max mem: 1.4 GB 
[11/06 18:33:16 visual_prompt]: 	Training 1100/2212. train loss: 0.7812,	0.1760 s / batch. (data: 5.34e-03). ETA=10:06:36, max mem: 1.4 GB 
[11/06 18:33:34 visual_prompt]: 	Training 1200/2212. train loss: 0.5800,	0.1294 s / batch. (data: 2.05e-02). ETA=7:25:47, max mem: 1.4 GB 
[11/06 18:33:55 visual_prompt]: 	Training 1300/2212. train loss: 0.6494,	0.1163 s / batch. (data: 5.65e-03). ETA=6:40:23, max mem: 1.4 GB 
[11/06 18:34:14 visual_prompt]: 	Training 1400/2212. train loss: 0.6007,	0.1662 s / batch. (data: 5.27e-03). ETA=9:32:12, max mem: 1.4 GB 
[11/06 18:34:33 visual_prompt]: 	Training 1500/2212. train loss: 0.9692,	0.2148 s / batch. (data: 2.60e-02). ETA=12:18:56, max mem: 1.4 GB 
[11/06 18:34:53 visual_prompt]: 	Training 1600/2212. train loss: 0.5862,	0.3438 s / batch. (data: 2.15e-01). ETA=19:42:12, max mem: 1.4 GB 
[11/06 18:35:13 visual_prompt]: 	Training 1700/2212. train loss: 0.9012,	0.1602 s / batch. (data: 1.03e-02). ETA=9:10:28, max mem: 1.4 GB 
[11/06 18:35:32 visual_prompt]: 	Training 1800/2212. train loss: 0.5723,	0.1907 s / batch. (data: 1.08e-02). ETA=10:55:01, max mem: 1.4 GB 
[11/06 18:35:52 visual_prompt]: 	Training 1900/2212. train loss: 0.8254,	0.1845 s / batch. (data: 5.31e-03). ETA=10:33:41, max mem: 1.4 GB 
[11/06 18:36:12 visual_prompt]: 	Training 2000/2212. train loss: 0.7260,	0.1634 s / batch. (data: 1.04e-02). ETA=9:20:40, max mem: 1.4 GB 
[11/06 18:36:32 visual_prompt]: 	Training 2100/2212. train loss: 0.9467,	0.2043 s / batch. (data: 1.68e-04). ETA=11:40:42, max mem: 1.4 GB 
[11/06 18:36:51 visual_prompt]: 	Training 2200/2212. train loss: 0.5506,	0.0908 s / batch. (data: 8.34e-05). ETA=5:11:22, max mem: 1.4 GB 
[11/06 18:36:52 visual_prompt]: Epoch 7 / 100: avg data time: 4.59e-02, avg batch time: 0.1982, average train loss: 0.7028
[11/06 18:37:12 visual_prompt]: 	Test 100/246. loss: 0.838, 0.0397 s / batch. (data: 1.50e-05)max mem: 1.44029 GB 
[11/06 18:37:30 visual_prompt]: 	Test 200/246. loss: 0.838, 0.0471 s / batch. (data: 1.88e-05)max mem: 1.44029 GB 
[11/06 18:37:38 visual_prompt]: Inference (val):avg data time: 1.08e-04, avg batch time: 0.0400, average loss: 0.6891
[11/06 18:37:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.06	
[11/06 18:37:38 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/06 18:38:00 visual_prompt]: 	Training 100/2212. train loss: 0.5098,	0.1303 s / batch. (data: 1.04e-02). ETA=7:26:36, max mem: 1.4 GB 
[11/06 18:38:20 visual_prompt]: 	Training 200/2212. train loss: 0.6621,	0.1862 s / batch. (data: 1.08e-02). ETA=10:37:54, max mem: 1.4 GB 
[11/06 18:38:39 visual_prompt]: 	Training 300/2212. train loss: 0.9167,	0.1653 s / batch. (data: 1.54e-02). ETA=9:25:47, max mem: 1.4 GB 
[11/06 18:38:59 visual_prompt]: 	Training 400/2212. train loss: 0.5255,	0.1845 s / batch. (data: 5.31e-03). ETA=10:31:21, max mem: 1.4 GB 
[11/06 18:39:21 visual_prompt]: 	Training 500/2212. train loss: 0.5267,	0.4257 s / batch. (data: 2.64e-01). ETA=1 day, 0:16:00, max mem: 1.4 GB 
[11/06 18:39:40 visual_prompt]: 	Training 600/2212. train loss: 0.8026,	0.1760 s / batch. (data: 1.56e-02). ETA=10:01:38, max mem: 1.4 GB 
[11/06 18:39:59 visual_prompt]: 	Training 700/2212. train loss: 0.9116,	0.1454 s / batch. (data: 2.00e-04). ETA=8:16:41, max mem: 1.4 GB 
[11/06 18:40:19 visual_prompt]: 	Training 800/2212. train loss: 0.4800,	0.1571 s / batch. (data: 1.62e-02). ETA=8:56:34, max mem: 1.4 GB 
[11/06 18:40:39 visual_prompt]: 	Training 900/2212. train loss: 0.5827,	0.1584 s / batch. (data: 1.04e-02). ETA=9:00:45, max mem: 1.4 GB 
[11/06 18:40:59 visual_prompt]: 	Training 1000/2212. train loss: 0.9213,	0.1445 s / batch. (data: 1.54e-02). ETA=8:13:05, max mem: 1.4 GB 
[11/06 18:41:18 visual_prompt]: 	Training 1100/2212. train loss: 0.5811,	0.1286 s / batch. (data: 1.54e-02). ETA=7:18:42, max mem: 1.4 GB 
[11/06 18:41:39 visual_prompt]: 	Training 1200/2212. train loss: 0.6265,	0.1767 s / batch. (data: 1.54e-02). ETA=10:02:26, max mem: 1.4 GB 
[11/06 18:41:59 visual_prompt]: 	Training 1300/2212. train loss: 0.7631,	0.2252 s / batch. (data: 2.15e-02). ETA=12:47:10, max mem: 1.4 GB 
[11/06 18:42:18 visual_prompt]: 	Training 1400/2212. train loss: 0.7888,	0.2036 s / batch. (data: 1.54e-02). ETA=11:33:18, max mem: 1.4 GB 
[11/06 18:42:39 visual_prompt]: 	Training 1500/2212. train loss: 0.5911,	0.3685 s / batch. (data: 2.72e-01). ETA=20:54:22, max mem: 1.4 GB 
[11/06 18:42:59 visual_prompt]: 	Training 1600/2212. train loss: 0.8908,	0.1648 s / batch. (data: 2.61e-02). ETA=9:20:35, max mem: 1.4 GB 
[11/06 18:43:19 visual_prompt]: 	Training 1700/2212. train loss: 0.7768,	0.1649 s / batch. (data: 5.31e-03). ETA=9:20:49, max mem: 1.4 GB 
[11/06 18:43:38 visual_prompt]: 	Training 1800/2212. train loss: 0.5397,	0.0816 s / batch. (data: 1.77e-04). ETA=4:37:16, max mem: 1.4 GB 
[11/06 18:43:57 visual_prompt]: 	Training 1900/2212. train loss: 0.6120,	0.1785 s / batch. (data: 5.98e-04). ETA=10:06:19, max mem: 1.4 GB 
[11/06 18:44:18 visual_prompt]: 	Training 2000/2212. train loss: 1.7032,	0.1637 s / batch. (data: 2.01e-04). ETA=9:15:54, max mem: 1.4 GB 
[11/06 18:44:38 visual_prompt]: 	Training 2100/2212. train loss: 0.5217,	0.1641 s / batch. (data: 6.57e-03). ETA=9:16:44, max mem: 1.4 GB 
[11/06 18:44:56 visual_prompt]: 	Training 2200/2212. train loss: 0.7590,	0.1300 s / batch. (data: 7.77e-05). ETA=7:20:54, max mem: 1.4 GB 
[11/06 18:44:57 visual_prompt]: Epoch 8 / 100: avg data time: 4.63e-02, avg batch time: 0.1983, average train loss: 0.7053
[11/06 18:45:16 visual_prompt]: 	Test 100/246. loss: 0.766, 0.0372 s / batch. (data: 2.24e-05)max mem: 1.44029 GB 
[11/06 18:45:35 visual_prompt]: 	Test 200/246. loss: 0.766, 0.0617 s / batch. (data: 2.07e-05)max mem: 1.44029 GB 
[11/06 18:45:42 visual_prompt]: Inference (val):avg data time: 3.92e-04, avg batch time: 0.0402, average loss: 0.6888
[11/06 18:45:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.01	
[11/06 18:45:42 visual_prompt]: Best epoch 8: best metric: -0.689
[11/06 18:45:42 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/06 18:46:05 visual_prompt]: 	Training 100/2212. train loss: 0.6648,	0.0960 s / batch. (data: 9.49e-03). ETA=5:25:21, max mem: 1.4 GB 
[11/06 18:46:26 visual_prompt]: 	Training 200/2212. train loss: 0.6465,	0.2025 s / batch. (data: 2.05e-02). ETA=11:26:07, max mem: 1.4 GB 
[11/06 18:46:47 visual_prompt]: 	Training 300/2212. train loss: 0.8666,	0.1440 s / batch. (data: 1.54e-02). ETA=8:07:43, max mem: 1.4 GB 
[11/06 18:47:07 visual_prompt]: 	Training 400/2212. train loss: 0.7803,	0.2299 s / batch. (data: 3.73e-02). ETA=12:58:18, max mem: 1.4 GB 
[11/06 18:47:28 visual_prompt]: 	Training 500/2212. train loss: 0.7996,	0.1258 s / batch. (data: 5.27e-03). ETA=7:05:40, max mem: 1.4 GB 
[11/06 18:47:47 visual_prompt]: 	Training 600/2212. train loss: 0.5202,	0.1538 s / batch. (data: 2.08e-02). ETA=8:40:04, max mem: 1.4 GB 
[11/06 18:48:07 visual_prompt]: 	Training 700/2212. train loss: 0.3972,	0.1566 s / batch. (data: 1.04e-02). ETA=8:49:28, max mem: 1.4 GB 
[11/06 18:48:26 visual_prompt]: 	Training 800/2212. train loss: 0.7254,	0.2024 s / batch. (data: 1.03e-02). ETA=11:23:45, max mem: 1.4 GB 
[11/06 18:48:45 visual_prompt]: 	Training 900/2212. train loss: 0.6554,	0.1742 s / batch. (data: 1.54e-02). ETA=9:48:05, max mem: 1.4 GB 
[11/06 18:49:05 visual_prompt]: 	Training 1000/2212. train loss: 0.8025,	0.1618 s / batch. (data: 6.89e-03). ETA=9:05:59, max mem: 1.4 GB 
[11/06 18:49:25 visual_prompt]: 	Training 1100/2212. train loss: 0.5062,	0.8903 s / batch. (data: 7.49e-01). ETA=2 days, 2:03:16, max mem: 1.4 GB 
[11/06 18:49:45 visual_prompt]: 	Training 1200/2212. train loss: 1.0264,	0.1249 s / batch. (data: 1.95e-04). ETA=7:01:16, max mem: 1.4 GB 
[11/06 18:50:03 visual_prompt]: 	Training 1300/2212. train loss: 0.7627,	0.1637 s / batch. (data: 1.03e-02). ETA=9:11:36, max mem: 1.4 GB 
[11/06 18:50:22 visual_prompt]: 	Training 1400/2212. train loss: 0.7527,	0.1919 s / batch. (data: 3.11e-02). ETA=10:46:28, max mem: 1.4 GB 
[11/06 18:50:42 visual_prompt]: 	Training 1500/2212. train loss: 0.9318,	0.1818 s / batch. (data: 2.82e-02). ETA=10:12:04, max mem: 1.4 GB 
[11/06 18:51:01 visual_prompt]: 	Training 1600/2212. train loss: 0.6495,	0.1291 s / batch. (data: 5.33e-03). ETA=7:14:22, max mem: 1.4 GB 
[11/06 18:51:21 visual_prompt]: 	Training 1700/2212. train loss: 1.4395,	0.1817 s / batch. (data: 6.05e-04). ETA=10:11:02, max mem: 1.4 GB 
[11/06 18:51:40 visual_prompt]: 	Training 1800/2212. train loss: 0.7318,	0.9247 s / batch. (data: 8.41e-01). ETA=2 days, 3:48:35, max mem: 1.4 GB 
[11/06 18:51:58 visual_prompt]: 	Training 1900/2212. train loss: 0.7979,	0.1450 s / batch. (data: 3.82e-03). ETA=8:07:21, max mem: 1.4 GB 
[11/06 18:52:18 visual_prompt]: 	Training 2000/2212. train loss: 0.8055,	0.1831 s / batch. (data: 1.51e-02). ETA=10:14:49, max mem: 1.4 GB 
[11/06 18:52:38 visual_prompt]: 	Training 2100/2212. train loss: 0.8031,	0.2045 s / batch. (data: 2.56e-02). ETA=11:26:32, max mem: 1.4 GB 
[11/06 18:53:00 visual_prompt]: 	Training 2200/2212. train loss: 0.5406,	0.1612 s / batch. (data: 1.24e-04). ETA=9:00:45, max mem: 1.4 GB 
[11/06 18:53:01 visual_prompt]: Epoch 9 / 100: avg data time: 4.89e-02, avg batch time: 0.1984, average train loss: 0.7082
[11/06 18:53:20 visual_prompt]: 	Test 100/246. loss: 0.927, 0.0717 s / batch. (data: 2.05e-05)max mem: 1.44029 GB 
[11/06 18:53:39 visual_prompt]: 	Test 200/246. loss: 0.927, 0.0825 s / batch. (data: 2.19e-05)max mem: 1.44029 GB 
[11/06 18:53:46 visual_prompt]: Inference (val):avg data time: 4.13e-04, avg batch time: 0.0407, average loss: 0.6947
[11/06 18:53:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.05	
[11/06 18:53:46 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/06 18:54:09 visual_prompt]: 	Training 100/2212. train loss: 0.5736,	0.1855 s / batch. (data: 2.05e-02). ETA=10:22:05, max mem: 1.4 GB 
[11/06 18:54:29 visual_prompt]: 	Training 200/2212. train loss: 0.8301,	0.5223 s / batch. (data: 4.23e-01). ETA=1 day, 5:10:24, max mem: 1.4 GB 
[11/06 18:54:49 visual_prompt]: 	Training 300/2212. train loss: 0.9691,	0.1868 s / batch. (data: 6.42e-04). ETA=10:25:39, max mem: 1.4 GB 
[11/06 18:55:08 visual_prompt]: 	Training 400/2212. train loss: 0.7929,	0.1558 s / batch. (data: 1.58e-02). ETA=8:41:45, max mem: 1.4 GB 
[11/06 18:55:27 visual_prompt]: 	Training 500/2212. train loss: 0.4732,	0.1797 s / batch. (data: 1.55e-02). ETA=10:01:18, max mem: 1.4 GB 
[11/06 18:55:47 visual_prompt]: 	Training 600/2212. train loss: 0.9828,	0.1298 s / batch. (data: 5.19e-04). ETA=7:14:12, max mem: 1.4 GB 
[11/06 18:56:08 visual_prompt]: 	Training 700/2212. train loss: 0.5239,	0.2090 s / batch. (data: 5.31e-03). ETA=11:38:49, max mem: 1.4 GB 
[11/06 18:56:28 visual_prompt]: 	Training 800/2212. train loss: 0.7443,	0.1959 s / batch. (data: 1.08e-02). ETA=10:54:39, max mem: 1.4 GB 
[11/06 18:56:48 visual_prompt]: 	Training 900/2212. train loss: 0.2362,	1.0981 s / batch. (data: 1.02e+00). ETA=2 days, 13:07:24, max mem: 1.4 GB 
[11/06 18:57:07 visual_prompt]: 	Training 1000/2212. train loss: 0.6700,	0.1531 s / batch. (data: 5.32e-03). ETA=8:30:57, max mem: 1.4 GB 
[11/06 18:57:28 visual_prompt]: 	Training 1100/2212. train loss: 0.7570,	0.1776 s / batch. (data: 5.80e-04). ETA=9:52:32, max mem: 1.4 GB 
[11/06 18:57:47 visual_prompt]: 	Training 1200/2212. train loss: 0.7626,	0.1767 s / batch. (data: 1.54e-02). ETA=9:49:08, max mem: 1.4 GB 
[11/06 18:58:07 visual_prompt]: 	Training 1300/2212. train loss: 0.7817,	0.1550 s / batch. (data: 1.54e-02). ETA=8:36:39, max mem: 1.4 GB 
[11/06 18:58:26 visual_prompt]: 	Training 1400/2212. train loss: 1.9742,	0.0887 s / batch. (data: 1.77e-04). ETA=4:55:33, max mem: 1.4 GB 
[11/06 18:58:46 visual_prompt]: 	Training 1500/2212. train loss: 0.6086,	0.1280 s / batch. (data: 1.04e-02). ETA=7:06:18, max mem: 1.4 GB 
[11/06 18:59:05 visual_prompt]: 	Training 1600/2212. train loss: 0.6166,	0.1800 s / batch. (data: 1.54e-02). ETA=9:59:12, max mem: 1.4 GB 
[11/06 18:59:25 visual_prompt]: 	Training 1700/2212. train loss: 0.5784,	0.1315 s / batch. (data: 1.97e-04). ETA=7:17:22, max mem: 1.4 GB 
[11/06 18:59:45 visual_prompt]: 	Training 1800/2212. train loss: 0.9025,	0.1296 s / batch. (data: 7.29e-03). ETA=7:10:53, max mem: 1.4 GB 
[11/06 19:00:04 visual_prompt]: 	Training 1900/2212. train loss: 0.6273,	0.1623 s / batch. (data: 1.04e-02). ETA=8:59:31, max mem: 1.4 GB 
[11/06 19:00:24 visual_prompt]: 	Training 2000/2212. train loss: 0.6165,	0.1723 s / batch. (data: 2.25e-04). ETA=9:32:15, max mem: 1.4 GB 
[11/06 19:00:45 visual_prompt]: 	Training 2100/2212. train loss: 0.7117,	0.1702 s / batch. (data: 2.00e-04). ETA=9:24:52, max mem: 1.4 GB 
[11/06 19:01:04 visual_prompt]: 	Training 2200/2212. train loss: 0.6600,	0.1200 s / batch. (data: 1.03e-04). ETA=6:38:16, max mem: 1.4 GB 
[11/06 19:01:05 visual_prompt]: Epoch 10 / 100: avg data time: 4.78e-02, avg batch time: 0.1983, average train loss: 0.7021
[11/06 19:01:25 visual_prompt]: 	Test 100/246. loss: 0.641, 0.0463 s / batch. (data: 2.31e-05)max mem: 1.44029 GB 
[11/06 19:01:43 visual_prompt]: 	Test 200/246. loss: 0.641, 0.0387 s / batch. (data: 2.34e-05)max mem: 1.44029 GB 
[11/06 19:01:51 visual_prompt]: Inference (val):avg data time: 5.86e-04, avg batch time: 0.0399, average loss: 0.6998
[11/06 19:01:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.95	
[11/06 19:01:51 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/06 19:02:12 visual_prompt]: 	Training 100/2212. train loss: 0.5598,	0.1598 s / batch. (data: 1.03e-02). ETA=8:50:03, max mem: 1.4 GB 
[11/06 19:02:33 visual_prompt]: 	Training 200/2212. train loss: 0.6380,	0.2048 s / batch. (data: 1.58e-02). ETA=11:18:56, max mem: 1.4 GB 
[11/06 19:02:54 visual_prompt]: 	Training 300/2212. train loss: 0.8742,	0.1554 s / batch. (data: 5.29e-03). ETA=8:34:47, max mem: 1.4 GB 
[11/06 19:03:14 visual_prompt]: 	Training 400/2212. train loss: 0.8680,	0.1814 s / batch. (data: 1.03e-02). ETA=10:00:46, max mem: 1.4 GB 
[11/06 19:03:34 visual_prompt]: 	Training 500/2212. train loss: 0.6961,	0.1117 s / batch. (data: 6.25e-04). ETA=6:09:37, max mem: 1.4 GB 
[11/06 19:03:53 visual_prompt]: 	Training 600/2212. train loss: 0.7149,	0.0941 s / batch. (data: 1.35e-02). ETA=5:11:08, max mem: 1.4 GB 
[11/06 19:04:14 visual_prompt]: 	Training 700/2212. train loss: 1.0688,	0.7666 s / batch. (data: 6.83e-01). ETA=1 day, 18:14:34, max mem: 1.4 GB 
[11/06 19:04:32 visual_prompt]: 	Training 800/2212. train loss: 0.6716,	0.1578 s / batch. (data: 1.54e-02). ETA=8:41:26, max mem: 1.4 GB 
[11/06 19:04:51 visual_prompt]: 	Training 900/2212. train loss: 0.9117,	0.1533 s / batch. (data: 5.30e-03). ETA=8:26:11, max mem: 1.4 GB 
[11/06 19:05:10 visual_prompt]: 	Training 1000/2212. train loss: 0.6506,	0.1866 s / batch. (data: 2.52e-02). ETA=10:15:57, max mem: 1.4 GB 
[11/06 19:05:31 visual_prompt]: 	Training 1100/2212. train loss: 0.5555,	0.1487 s / batch. (data: 2.06e-04). ETA=8:10:39, max mem: 1.4 GB 
[11/06 19:05:50 visual_prompt]: 	Training 1200/2212. train loss: 0.7581,	0.1415 s / batch. (data: 1.03e-02). ETA=7:46:36, max mem: 1.4 GB 
[11/06 19:06:10 visual_prompt]: 	Training 1300/2212. train loss: 0.6278,	0.1787 s / batch. (data: 5.63e-04). ETA=9:49:07, max mem: 1.4 GB 
[11/06 19:06:30 visual_prompt]: 	Training 1400/2212. train loss: 2.1061,	0.1085 s / batch. (data: 2.29e-04). ETA=5:57:32, max mem: 1.4 GB 
[11/06 19:06:50 visual_prompt]: 	Training 1500/2212. train loss: 0.3091,	0.1848 s / batch. (data: 1.07e-02). ETA=10:08:26, max mem: 1.4 GB 
[11/06 19:07:10 visual_prompt]: 	Training 1600/2212. train loss: 1.0879,	0.1885 s / batch. (data: 2.92e-02). ETA=10:20:16, max mem: 1.4 GB 
[11/06 19:07:29 visual_prompt]: 	Training 1700/2212. train loss: 0.4020,	0.1633 s / batch. (data: 1.54e-02). ETA=8:57:05, max mem: 1.4 GB 
[11/06 19:07:49 visual_prompt]: 	Training 1800/2212. train loss: 0.4553,	0.1735 s / batch. (data: 1.95e-02). ETA=9:30:36, max mem: 1.4 GB 
[11/06 19:08:09 visual_prompt]: 	Training 1900/2212. train loss: 0.5299,	0.1642 s / batch. (data: 1.04e-02). ETA=8:59:42, max mem: 1.4 GB 
[11/06 19:08:29 visual_prompt]: 	Training 2000/2212. train loss: 0.7184,	0.1955 s / batch. (data: 1.04e-02). ETA=10:42:05, max mem: 1.4 GB 
[11/06 19:08:48 visual_prompt]: 	Training 2100/2212. train loss: 0.4503,	0.1960 s / batch. (data: 1.04e-02). ETA=10:43:29, max mem: 1.4 GB 
[11/06 19:09:08 visual_prompt]: 	Training 2200/2212. train loss: 0.8330,	0.0898 s / batch. (data: 7.15e-05). ETA=4:54:36, max mem: 1.4 GB 
[11/06 19:09:09 visual_prompt]: Epoch 11 / 100: avg data time: 4.78e-02, avg batch time: 0.1982, average train loss: 0.7126
[11/06 19:09:29 visual_prompt]: 	Test 100/246. loss: 0.767, 0.0687 s / batch. (data: 2.41e-02)max mem: 1.44029 GB 
[11/06 19:09:47 visual_prompt]: 	Test 200/246. loss: 0.767, 0.0635 s / batch. (data: 1.98e-05)max mem: 1.44029 GB 
[11/06 19:09:54 visual_prompt]: Inference (val):avg data time: 8.57e-04, avg batch time: 0.0410, average loss: 0.6887
[11/06 19:09:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[11/06 19:09:54 visual_prompt]: Best epoch 11: best metric: -0.689
[11/06 19:09:54 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/06 19:10:15 visual_prompt]: 	Training 100/2212. train loss: 0.5227,	0.1699 s / batch. (data: 1.11e-02). ETA=9:17:01, max mem: 1.4 GB 
[11/06 19:10:34 visual_prompt]: 	Training 200/2212. train loss: 0.2015,	0.1348 s / batch. (data: 5.65e-03). ETA=7:21:54, max mem: 1.4 GB 
[11/06 19:10:54 visual_prompt]: 	Training 300/2212. train loss: 0.7183,	0.1885 s / batch. (data: 2.60e-02). ETA=10:17:33, max mem: 1.4 GB 
[11/06 19:11:14 visual_prompt]: 	Training 400/2212. train loss: 0.3812,	0.1434 s / batch. (data: 3.01e-02). ETA=7:49:36, max mem: 1.4 GB 
[11/06 19:11:35 visual_prompt]: 	Training 500/2212. train loss: 0.6161,	0.1915 s / batch. (data: 6.17e-04). ETA=10:26:39, max mem: 1.4 GB 
[11/06 19:11:55 visual_prompt]: 	Training 600/2212. train loss: 0.8490,	0.1654 s / batch. (data: 5.51e-03). ETA=9:01:03, max mem: 1.4 GB 
[11/06 19:12:16 visual_prompt]: 	Training 700/2212. train loss: 0.7026,	0.1623 s / batch. (data: 5.69e-03). ETA=8:50:44, max mem: 1.4 GB 
[11/06 19:12:36 visual_prompt]: 	Training 800/2212. train loss: 1.1023,	0.1538 s / batch. (data: 1.54e-02). ETA=8:22:35, max mem: 1.4 GB 
[11/06 19:12:56 visual_prompt]: 	Training 900/2212. train loss: 0.7598,	1.0625 s / batch. (data: 9.40e-01). ETA=2 days, 9:50:22, max mem: 1.4 GB 
[11/06 19:13:16 visual_prompt]: 	Training 1000/2212. train loss: 0.5182,	0.1016 s / batch. (data: 1.71e-04). ETA=5:31:46, max mem: 1.4 GB 
[11/06 19:13:35 visual_prompt]: 	Training 1100/2212. train loss: 0.7913,	0.4085 s / batch. (data: 2.81e-01). ETA=22:12:58, max mem: 1.4 GB 
[11/06 19:13:54 visual_prompt]: 	Training 1200/2212. train loss: 1.1663,	0.1892 s / batch. (data: 1.08e-02). ETA=10:17:02, max mem: 1.4 GB 
[11/06 19:14:13 visual_prompt]: 	Training 1300/2212. train loss: 0.4215,	0.1747 s / batch. (data: 2.05e-02). ETA=9:29:25, max mem: 1.4 GB 
[11/06 19:14:33 visual_prompt]: 	Training 1400/2212. train loss: 0.5854,	0.1593 s / batch. (data: 1.04e-02). ETA=8:38:50, max mem: 1.4 GB 
[11/06 19:14:54 visual_prompt]: 	Training 1500/2212. train loss: 0.4297,	0.4275 s / batch. (data: 2.98e-01). ETA=23:12:01, max mem: 1.4 GB 
[11/06 19:15:13 visual_prompt]: 	Training 1600/2212. train loss: 0.5904,	0.1687 s / batch. (data: 1.62e-02). ETA=9:09:10, max mem: 1.4 GB 
[11/06 19:15:34 visual_prompt]: 	Training 1700/2212. train loss: 0.8420,	0.1921 s / batch. (data: 2.31e-02). ETA=10:24:48, max mem: 1.4 GB 
[11/06 19:15:54 visual_prompt]: 	Training 1800/2212. train loss: 0.8315,	0.1404 s / batch. (data: 1.03e-02). ETA=7:36:19, max mem: 1.4 GB 
[11/06 19:16:13 visual_prompt]: 	Training 1900/2212. train loss: 0.6329,	0.2013 s / batch. (data: 2.22e-02). ETA=10:54:01, max mem: 1.4 GB 
[11/06 19:16:32 visual_prompt]: 	Training 2000/2212. train loss: 0.9280,	0.1490 s / batch. (data: 1.10e-02). ETA=8:03:52, max mem: 1.4 GB 
[11/06 19:16:53 visual_prompt]: 	Training 2100/2212. train loss: 0.7737,	0.6810 s / batch. (data: 5.89e-01). ETA=1 day, 12:50:30, max mem: 1.4 GB 
[11/06 19:17:12 visual_prompt]: 	Training 2200/2212. train loss: 0.2947,	0.0915 s / batch. (data: 7.77e-05). ETA=4:56:47, max mem: 1.4 GB 
[11/06 19:17:13 visual_prompt]: Epoch 12 / 100: avg data time: 4.70e-02, avg batch time: 0.1983, average train loss: 0.7091
[11/06 19:17:33 visual_prompt]: 	Test 100/246. loss: 0.608, 0.0721 s / batch. (data: 2.88e-05)max mem: 1.44029 GB 
[11/06 19:17:51 visual_prompt]: 	Test 200/246. loss: 0.608, 0.0675 s / batch. (data: 2.24e-05)max mem: 1.44029 GB 
[11/06 19:17:59 visual_prompt]: Inference (val):avg data time: 2.68e-04, avg batch time: 0.0407, average loss: 0.7057
[11/06 19:17:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.49	
[11/06 19:17:59 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/06 19:18:21 visual_prompt]: 	Training 100/2212. train loss: 0.6781,	0.1592 s / batch. (data: 1.58e-02). ETA=8:36:10, max mem: 1.4 GB 
[11/06 19:18:41 visual_prompt]: 	Training 200/2212. train loss: 0.5952,	0.1999 s / batch. (data: 1.58e-02). ETA=10:48:01, max mem: 1.4 GB 
[11/06 19:19:01 visual_prompt]: 	Training 300/2212. train loss: 0.6473,	0.0520 s / batch. (data: 1.70e-04). ETA=2:48:17, max mem: 1.4 GB 
[11/06 19:19:19 visual_prompt]: 	Training 400/2212. train loss: 1.0364,	0.1123 s / batch. (data: 1.67e-04). ETA=6:03:29, max mem: 1.4 GB 
[11/06 19:19:38 visual_prompt]: 	Training 500/2212. train loss: 0.6946,	0.1859 s / batch. (data: 1.63e-02). ETA=10:01:28, max mem: 1.4 GB 
[11/06 19:19:59 visual_prompt]: 	Training 600/2212. train loss: 0.5273,	0.1806 s / batch. (data: 6.78e-03). ETA=9:44:01, max mem: 1.4 GB 
[11/06 19:20:20 visual_prompt]: 	Training 700/2212. train loss: 0.8498,	0.1329 s / batch. (data: 7.29e-03). ETA=7:09:34, max mem: 1.4 GB 
[11/06 19:20:41 visual_prompt]: 	Training 800/2212. train loss: 0.7060,	0.1227 s / batch. (data: 1.91e-04). ETA=6:36:29, max mem: 1.4 GB 
[11/06 19:21:00 visual_prompt]: 	Training 900/2212. train loss: 0.5326,	0.1370 s / batch. (data: 1.54e-02). ETA=7:22:24, max mem: 1.4 GB 
[11/06 19:21:19 visual_prompt]: 	Training 1000/2212. train loss: 0.5190,	0.1496 s / batch. (data: 6.47e-03). ETA=8:02:45, max mem: 1.4 GB 
[11/06 19:21:39 visual_prompt]: 	Training 1100/2212. train loss: 0.7932,	0.1998 s / batch. (data: 1.07e-02). ETA=10:44:33, max mem: 1.4 GB 
[11/06 19:22:00 visual_prompt]: 	Training 1200/2212. train loss: 0.6742,	0.1970 s / batch. (data: 1.59e-02). ETA=10:35:14, max mem: 1.4 GB 
[11/06 19:22:20 visual_prompt]: 	Training 1300/2212. train loss: 0.9948,	0.1612 s / batch. (data: 4.35e-03). ETA=8:39:33, max mem: 1.4 GB 
[11/06 19:22:39 visual_prompt]: 	Training 1400/2212. train loss: 0.4940,	0.1108 s / batch. (data: 1.83e-04). ETA=5:56:52, max mem: 1.4 GB 
[11/06 19:23:00 visual_prompt]: 	Training 1500/2212. train loss: 0.4261,	0.2003 s / batch. (data: 4.67e-02). ETA=10:44:54, max mem: 1.4 GB 
[11/06 19:23:19 visual_prompt]: 	Training 1600/2212. train loss: 0.6757,	0.1695 s / batch. (data: 1.54e-02). ETA=9:05:28, max mem: 1.4 GB 
[11/06 19:23:39 visual_prompt]: 	Training 1700/2212. train loss: 0.6922,	0.1943 s / batch. (data: 3.02e-02). ETA=10:24:49, max mem: 1.4 GB 
[11/06 19:23:57 visual_prompt]: 	Training 1800/2212. train loss: 1.0629,	0.1621 s / batch. (data: 6.49e-03). ETA=8:41:01, max mem: 1.4 GB 
[11/06 19:24:19 visual_prompt]: 	Training 1900/2212. train loss: 0.6008,	0.1607 s / batch. (data: 5.38e-03). ETA=8:36:07, max mem: 1.4 GB 
[11/06 19:24:38 visual_prompt]: 	Training 2000/2212. train loss: 0.5681,	0.1914 s / batch. (data: 4.01e-02). ETA=10:14:31, max mem: 1.4 GB 
[11/06 19:24:57 visual_prompt]: 	Training 2100/2212. train loss: 0.7812,	0.1522 s / batch. (data: 1.03e-02). ETA=8:08:31, max mem: 1.4 GB 
[11/06 19:25:16 visual_prompt]: 	Training 2200/2212. train loss: 0.6901,	0.0898 s / batch. (data: 8.39e-05). ETA=4:48:03, max mem: 1.4 GB 
[11/06 19:25:17 visual_prompt]: Epoch 13 / 100: avg data time: 4.76e-02, avg batch time: 0.1982, average train loss: 0.7063
[11/06 19:25:37 visual_prompt]: 	Test 100/246. loss: 0.841, 0.0624 s / batch. (data: 1.96e-05)max mem: 1.44029 GB 
[11/06 19:25:55 visual_prompt]: 	Test 200/246. loss: 0.841, 0.0393 s / batch. (data: 2.03e-05)max mem: 1.44029 GB 
[11/06 19:26:03 visual_prompt]: Inference (val):avg data time: 5.79e-04, avg batch time: 0.0402, average loss: 0.6892
[11/06 19:26:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.50	
[11/06 19:26:03 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/06 19:26:24 visual_prompt]: 	Training 100/2212. train loss: 0.9179,	0.1481 s / batch. (data: 1.03e-02). ETA=7:54:55, max mem: 1.4 GB 
[11/06 19:26:45 visual_prompt]: 	Training 200/2212. train loss: 0.7964,	0.1399 s / batch. (data: 5.72e-03). ETA=7:28:14, max mem: 1.4 GB 
[11/06 19:27:06 visual_prompt]: 	Training 300/2212. train loss: 0.6403,	0.1465 s / batch. (data: 2.20e-04). ETA=7:49:05, max mem: 1.4 GB 
[11/06 19:27:25 visual_prompt]: 	Training 400/2212. train loss: 0.7959,	0.0986 s / batch. (data: 1.51e-02). ETA=5:15:39, max mem: 1.4 GB 
[11/06 19:27:45 visual_prompt]: 	Training 500/2212. train loss: 0.8063,	0.1814 s / batch. (data: 9.00e-03). ETA=9:40:16, max mem: 1.4 GB 
[11/06 19:28:06 visual_prompt]: 	Training 600/2212. train loss: 0.4153,	0.2080 s / batch. (data: 1.07e-02). ETA=11:05:08, max mem: 1.4 GB 
[11/06 19:28:27 visual_prompt]: 	Training 700/2212. train loss: 0.7547,	0.1751 s / batch. (data: 1.04e-02). ETA=9:19:40, max mem: 1.4 GB 
[11/06 19:28:47 visual_prompt]: 	Training 800/2212. train loss: 0.6737,	0.2001 s / batch. (data: 3.23e-02). ETA=10:39:00, max mem: 1.4 GB 
[11/06 19:29:06 visual_prompt]: 	Training 900/2212. train loss: 0.7794,	0.1693 s / batch. (data: 1.90e-04). ETA=9:00:20, max mem: 1.4 GB 
[11/06 19:29:25 visual_prompt]: 	Training 1000/2212. train loss: 0.5906,	0.1960 s / batch. (data: 2.47e-02). ETA=10:25:20, max mem: 1.4 GB 
[11/06 19:29:44 visual_prompt]: 	Training 1100/2212. train loss: 1.3446,	0.2057 s / batch. (data: 2.05e-02). ETA=10:56:07, max mem: 1.4 GB 
[11/06 19:30:04 visual_prompt]: 	Training 1200/2212. train loss: 1.1575,	0.1218 s / batch. (data: 9.43e-03). ETA=6:28:21, max mem: 1.4 GB 
[11/06 19:30:23 visual_prompt]: 	Training 1300/2212. train loss: 0.4896,	0.1476 s / batch. (data: 5.29e-03). ETA=7:50:04, max mem: 1.4 GB 
[11/06 19:30:43 visual_prompt]: 	Training 1400/2212. train loss: 1.2204,	0.3453 s / batch. (data: 2.13e-01). ETA=18:19:23, max mem: 1.4 GB 
[11/06 19:31:03 visual_prompt]: 	Training 1500/2212. train loss: 0.5909,	0.1893 s / batch. (data: 2.59e-02). ETA=10:02:27, max mem: 1.4 GB 
[11/06 19:31:23 visual_prompt]: 	Training 1600/2212. train loss: 0.3325,	0.1478 s / batch. (data: 5.30e-03). ETA=7:50:11, max mem: 1.4 GB 
[11/06 19:31:42 visual_prompt]: 	Training 1700/2212. train loss: 0.3346,	0.1932 s / batch. (data: 5.08e-03). ETA=10:14:07, max mem: 1.4 GB 
[11/06 19:32:03 visual_prompt]: 	Training 1800/2212. train loss: 0.9730,	0.1630 s / batch. (data: 1.04e-02). ETA=8:37:55, max mem: 1.4 GB 
[11/06 19:32:22 visual_prompt]: 	Training 1900/2212. train loss: 0.8770,	0.1838 s / batch. (data: 1.58e-02). ETA=9:43:38, max mem: 1.4 GB 
[11/06 19:32:41 visual_prompt]: 	Training 2000/2212. train loss: 0.6216,	0.1704 s / batch. (data: 1.07e-02). ETA=9:01:00, max mem: 1.4 GB 
[11/06 19:33:01 visual_prompt]: 	Training 2100/2212. train loss: 0.7338,	0.1537 s / batch. (data: 5.33e-03). ETA=8:07:45, max mem: 1.4 GB 
[11/06 19:33:20 visual_prompt]: 	Training 2200/2212. train loss: 0.9485,	0.1069 s / batch. (data: 9.13e-05). ETA=5:39:05, max mem: 1.4 GB 
[11/06 19:33:21 visual_prompt]: Epoch 14 / 100: avg data time: 4.78e-02, avg batch time: 0.1982, average train loss: 0.7135
[11/06 19:33:41 visual_prompt]: 	Test 100/246. loss: 0.796, 0.0555 s / batch. (data: 1.91e-05)max mem: 1.44029 GB 
[11/06 19:33:59 visual_prompt]: 	Test 200/246. loss: 0.796, 0.0412 s / batch. (data: 2.53e-05)max mem: 1.44029 GB 
[11/06 19:34:07 visual_prompt]: Inference (val):avg data time: 5.47e-04, avg batch time: 0.0413, average loss: 0.6884
[11/06 19:34:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.01	
[11/06 19:34:07 visual_prompt]: Best epoch 14: best metric: -0.688
[11/06 19:34:07 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/06 19:34:29 visual_prompt]: 	Training 100/2212. train loss: 0.9827,	0.8566 s / batch. (data: 7.61e-01). ETA=1 day, 21:14:26, max mem: 1.4 GB 
[11/06 19:34:50 visual_prompt]: 	Training 200/2212. train loss: 0.9674,	0.8671 s / batch. (data: 7.78e-01). ETA=1 day, 21:46:25, max mem: 1.4 GB 
[11/06 19:35:09 visual_prompt]: 	Training 300/2212. train loss: 0.4364,	0.1813 s / batch. (data: 3.92e-02). ETA=9:33:46, max mem: 1.4 GB 
[11/06 19:35:30 visual_prompt]: 	Training 400/2212. train loss: 0.8346,	0.1811 s / batch. (data: 2.08e-02). ETA=9:32:57, max mem: 1.4 GB 
[11/06 19:35:51 visual_prompt]: 	Training 500/2212. train loss: 0.6326,	0.1016 s / batch. (data: 5.28e-03). ETA=5:21:24, max mem: 1.4 GB 
[11/06 19:36:12 visual_prompt]: 	Training 600/2212. train loss: 0.7905,	0.1585 s / batch. (data: 2.33e-02). ETA=8:20:51, max mem: 1.4 GB 
[11/06 19:36:32 visual_prompt]: 	Training 700/2212. train loss: 0.8404,	0.2063 s / batch. (data: 6.55e-02). ETA=10:51:35, max mem: 1.4 GB 
[11/06 19:36:50 visual_prompt]: 	Training 800/2212. train loss: 0.7943,	0.1858 s / batch. (data: 1.59e-02). ETA=9:46:36, max mem: 1.4 GB 
[11/06 19:37:10 visual_prompt]: 	Training 900/2212. train loss: 0.7568,	0.1789 s / batch. (data: 5.28e-03). ETA=9:24:35, max mem: 1.4 GB 
[11/06 19:37:30 visual_prompt]: 	Training 1000/2212. train loss: 0.7058,	0.0990 s / batch. (data: 1.61e-02). ETA=5:12:18, max mem: 1.4 GB 
[11/06 19:37:50 visual_prompt]: 	Training 1100/2212. train loss: 0.6971,	0.1308 s / batch. (data: 5.27e-03). ETA=6:52:13, max mem: 1.4 GB 
[11/06 19:38:09 visual_prompt]: 	Training 1200/2212. train loss: 0.7374,	0.1872 s / batch. (data: 2.23e-02). ETA=9:49:42, max mem: 1.4 GB 
[11/06 19:38:30 visual_prompt]: 	Training 1300/2212. train loss: 0.5946,	0.1659 s / batch. (data: 2.03e-04). ETA=8:42:17, max mem: 1.4 GB 
[11/06 19:38:49 visual_prompt]: 	Training 1400/2212. train loss: 0.7423,	0.1737 s / batch. (data: 1.04e-02). ETA=9:06:47, max mem: 1.4 GB 
[11/06 19:39:10 visual_prompt]: 	Training 1500/2212. train loss: 0.7588,	0.4412 s / batch. (data: 3.04e-01). ETA=23:07:47, max mem: 1.4 GB 
[11/06 19:39:30 visual_prompt]: 	Training 1600/2212. train loss: 0.4084,	0.1914 s / batch. (data: 2.05e-02). ETA=10:01:50, max mem: 1.4 GB 
[11/06 19:39:49 visual_prompt]: 	Training 1700/2212. train loss: 0.9098,	0.1886 s / batch. (data: 5.70e-03). ETA=9:52:44, max mem: 1.4 GB 
[11/06 19:40:08 visual_prompt]: 	Training 1800/2212. train loss: 0.1303,	0.1733 s / batch. (data: 2.04e-02). ETA=9:04:22, max mem: 1.4 GB 
[11/06 19:40:28 visual_prompt]: 	Training 1900/2212. train loss: 0.3608,	0.1695 s / batch. (data: 1.46e-02). ETA=8:52:08, max mem: 1.4 GB 
[11/06 19:40:48 visual_prompt]: 	Training 2000/2212. train loss: 0.6531,	0.3999 s / batch. (data: 2.65e-01). ETA=20:54:42, max mem: 1.4 GB 
[11/06 19:41:06 visual_prompt]: 	Training 2100/2212. train loss: 0.4810,	0.1951 s / batch. (data: 5.75e-03). ETA=10:11:37, max mem: 1.4 GB 
[11/06 19:41:24 visual_prompt]: 	Training 2200/2212. train loss: 0.5046,	0.1521 s / batch. (data: 8.11e-05). ETA=7:56:34, max mem: 1.4 GB 
[11/06 19:41:25 visual_prompt]: Epoch 15 / 100: avg data time: 4.86e-02, avg batch time: 0.1984, average train loss: 0.7081
[11/06 19:41:45 visual_prompt]: 	Test 100/246. loss: 0.982, 0.0636 s / batch. (data: 1.57e-05)max mem: 1.44029 GB 
[11/06 19:42:03 visual_prompt]: 	Test 200/246. loss: 0.982, 0.0512 s / batch. (data: 2.00e-05)max mem: 1.44029 GB 
[11/06 19:42:11 visual_prompt]: Inference (val):avg data time: 8.72e-04, avg batch time: 0.0411, average loss: 0.7007
[11/06 19:42:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.52	
[11/06 19:42:11 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/06 19:42:33 visual_prompt]: 	Training 100/2212. train loss: 0.4627,	0.1509 s / batch. (data: 1.80e-02). ETA=7:52:44, max mem: 1.4 GB 
[11/06 19:42:53 visual_prompt]: 	Training 200/2212. train loss: 0.9713,	0.2456 s / batch. (data: 9.52e-02). ETA=12:48:41, max mem: 1.4 GB 
[11/06 19:43:14 visual_prompt]: 	Training 300/2212. train loss: 0.5494,	0.1916 s / batch. (data: 2.56e-02). ETA=9:59:25, max mem: 1.4 GB 
[11/06 19:43:35 visual_prompt]: 	Training 400/2212. train loss: 0.8426,	0.1990 s / batch. (data: 1.59e-02). ETA=10:22:15, max mem: 1.4 GB 
[11/06 19:43:55 visual_prompt]: 	Training 500/2212. train loss: 0.7331,	0.1124 s / batch. (data: 2.52e-04). ETA=5:51:21, max mem: 1.4 GB 
[11/06 19:44:15 visual_prompt]: 	Training 600/2212. train loss: 0.2765,	0.2078 s / batch. (data: 1.58e-02). ETA=10:49:14, max mem: 1.4 GB 
[11/06 19:44:37 visual_prompt]: 	Training 700/2212. train loss: 0.7017,	0.1733 s / batch. (data: 7.73e-03). ETA=9:01:09, max mem: 1.4 GB 
[11/06 19:45:00 visual_prompt]: 	Training 800/2212. train loss: 0.7397,	0.1642 s / batch. (data: 5.33e-03). ETA=8:32:17, max mem: 1.4 GB 
[11/06 19:45:21 visual_prompt]: 	Training 900/2212. train loss: 0.6238,	0.3844 s / batch. (data: 2.35e-01). ETA=19:58:50, max mem: 1.4 GB 
[11/06 19:45:44 visual_prompt]: 	Training 1000/2212. train loss: 0.8348,	0.5469 s / batch. (data: 4.27e-01). ETA=1 day, 4:24:47, max mem: 1.4 GB 
[11/06 19:46:05 visual_prompt]: 	Training 1100/2212. train loss: 0.6684,	0.1855 s / batch. (data: 1.55e-02). ETA=9:37:54, max mem: 1.4 GB 
[11/06 19:46:27 visual_prompt]: 	Training 1200/2212. train loss: 0.7489,	0.2172 s / batch. (data: 2.28e-02). ETA=11:16:08, max mem: 1.4 GB 
[11/06 19:46:49 visual_prompt]: 	Training 1300/2212. train loss: 0.7815,	0.1939 s / batch. (data: 1.04e-02). ETA=10:03:29, max mem: 1.4 GB 
[11/06 19:47:12 visual_prompt]: 	Training 1400/2212. train loss: 0.8815,	0.0882 s / batch. (data: 1.97e-04). ETA=4:34:16, max mem: 1.4 GB 
[11/06 19:47:33 visual_prompt]: 	Training 1500/2212. train loss: 0.6301,	0.1723 s / batch. (data: 5.96e-04). ETA=8:55:29, max mem: 1.4 GB 
[11/06 19:47:55 visual_prompt]: 	Training 1600/2212. train loss: 0.7592,	0.0601 s / batch. (data: 1.85e-04). ETA=3:06:43, max mem: 1.4 GB 
[11/06 19:48:15 visual_prompt]: 	Training 1700/2212. train loss: 0.6397,	0.1703 s / batch. (data: 6.71e-03). ETA=8:48:41, max mem: 1.4 GB 
[11/06 19:48:36 visual_prompt]: 	Training 1800/2212. train loss: 0.7558,	0.2032 s / batch. (data: 1.04e-02). ETA=10:30:42, max mem: 1.4 GB 
[11/06 19:48:56 visual_prompt]: 	Training 1900/2212. train loss: 0.6205,	0.1950 s / batch. (data: 1.65e-02). ETA=10:04:46, max mem: 1.4 GB 
[11/06 19:49:17 visual_prompt]: 	Training 2000/2212. train loss: 0.7808,	0.1810 s / batch. (data: 1.04e-02). ETA=9:21:01, max mem: 1.4 GB 
[11/06 19:49:40 visual_prompt]: 	Training 2100/2212. train loss: 0.5883,	0.1438 s / batch. (data: 6.09e-04). ETA=7:25:41, max mem: 1.4 GB 
[11/06 19:50:02 visual_prompt]: 	Training 2200/2212. train loss: 0.5178,	0.1781 s / batch. (data: 9.06e-05). ETA=9:11:39, max mem: 1.4 GB 
[11/06 19:50:03 visual_prompt]: Epoch 16 / 100: avg data time: 5.75e-02, avg batch time: 0.2134, average train loss: 0.6977
[11/06 19:50:24 visual_prompt]: 	Test 100/246. loss: 0.991, 0.0376 s / batch. (data: 2.17e-05)max mem: 1.44029 GB 
[11/06 19:50:44 visual_prompt]: 	Test 200/246. loss: 0.991, 0.0393 s / batch. (data: 2.17e-05)max mem: 1.44029 GB 
[11/06 19:50:53 visual_prompt]: Inference (val):avg data time: 2.84e-04, avg batch time: 0.0376, average loss: 0.7017
[11/06 19:50:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.06	
[11/06 19:50:53 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/06 19:51:16 visual_prompt]: 	Training 100/2212. train loss: 0.7785,	0.2016 s / batch. (data: 1.08e-02). ETA=10:23:52, max mem: 1.4 GB 
[11/06 19:51:37 visual_prompt]: 	Training 200/2212. train loss: 0.7107,	0.1117 s / batch. (data: 2.10e-04). ETA=5:45:37, max mem: 1.4 GB 
[11/06 19:51:58 visual_prompt]: 	Training 300/2212. train loss: 0.6778,	0.1531 s / batch. (data: 5.33e-03). ETA=7:53:25, max mem: 1.4 GB 
[11/06 19:52:18 visual_prompt]: 	Training 400/2212. train loss: 0.6544,	0.2055 s / batch. (data: 4.55e-02). ETA=10:34:56, max mem: 1.4 GB 
[11/06 19:52:40 visual_prompt]: 	Training 500/2212. train loss: 0.6068,	0.2114 s / batch. (data: 1.55e-02). ETA=10:52:45, max mem: 1.4 GB 
[11/06 19:53:03 visual_prompt]: 	Training 600/2212. train loss: 1.0049,	0.1351 s / batch. (data: 1.57e-02). ETA=6:56:56, max mem: 1.4 GB 
[11/06 19:53:23 visual_prompt]: 	Training 700/2212. train loss: 0.5563,	0.1719 s / batch. (data: 2.42e-04). ETA=8:50:12, max mem: 1.4 GB 
[11/06 19:53:44 visual_prompt]: 	Training 800/2212. train loss: 0.2006,	0.5355 s / batch. (data: 4.59e-01). ETA=1 day, 3:31:11, max mem: 1.4 GB 
[11/06 19:54:04 visual_prompt]: 	Training 900/2212. train loss: 0.5483,	0.2215 s / batch. (data: 5.76e-03). ETA=11:22:41, max mem: 1.4 GB 
[11/06 19:54:25 visual_prompt]: 	Training 1000/2212. train loss: 0.7298,	0.1709 s / batch. (data: 7.38e-03). ETA=8:46:26, max mem: 1.4 GB 
[11/06 19:54:47 visual_prompt]: 	Training 1100/2212. train loss: 0.6831,	0.1616 s / batch. (data: 2.06e-04). ETA=8:17:32, max mem: 1.4 GB 
[11/06 19:55:07 visual_prompt]: 	Training 1200/2212. train loss: 0.8113,	0.1710 s / batch. (data: 2.04e-04). ETA=8:46:00, max mem: 1.4 GB 
[11/06 19:55:29 visual_prompt]: 	Training 1300/2212. train loss: 0.6625,	0.1767 s / batch. (data: 7.37e-03). ETA=9:03:20, max mem: 1.4 GB 
[11/06 19:55:51 visual_prompt]: 	Training 1400/2212. train loss: 0.5984,	0.1729 s / batch. (data: 2.05e-02). ETA=8:51:15, max mem: 1.4 GB 
[11/06 19:56:14 visual_prompt]: 	Training 1500/2212. train loss: 0.5696,	0.1687 s / batch. (data: 2.32e-04). ETA=8:38:19, max mem: 1.4 GB 
[11/06 19:56:36 visual_prompt]: 	Training 1600/2212. train loss: 0.8443,	0.1502 s / batch. (data: 2.00e-04). ETA=7:41:16, max mem: 1.4 GB 
[11/06 19:56:56 visual_prompt]: 	Training 1700/2212. train loss: 0.9127,	0.1837 s / batch. (data: 8.76e-03). ETA=9:23:37, max mem: 1.4 GB 
[11/06 19:57:17 visual_prompt]: 	Training 1800/2212. train loss: 1.0271,	0.2437 s / batch. (data: 2.05e-02). ETA=12:27:14, max mem: 1.4 GB 
[11/06 19:57:39 visual_prompt]: 	Training 1900/2212. train loss: 0.6181,	0.1747 s / batch. (data: 1.58e-02). ETA=8:55:25, max mem: 1.4 GB 
[11/06 19:58:02 visual_prompt]: 	Training 2000/2212. train loss: 0.4726,	0.2038 s / batch. (data: 3.23e-02). ETA=10:24:26, max mem: 1.4 GB 
[11/06 19:58:23 visual_prompt]: 	Training 2100/2212. train loss: 0.5086,	0.1973 s / batch. (data: 1.59e-02). ETA=10:03:57, max mem: 1.4 GB 
[11/06 19:58:45 visual_prompt]: 	Training 2200/2212. train loss: 0.9925,	0.1310 s / batch. (data: 9.39e-05). ETA=6:40:50, max mem: 1.4 GB 
[11/06 19:58:46 visual_prompt]: Epoch 17 / 100: avg data time: 4.91e-02, avg batch time: 0.2139, average train loss: 0.7059
[11/06 19:59:07 visual_prompt]: 	Test 100/246. loss: 0.937, 0.0461 s / batch. (data: 2.07e-05)max mem: 1.44029 GB 
[11/06 19:59:27 visual_prompt]: 	Test 200/246. loss: 0.937, 0.0648 s / batch. (data: 2.03e-05)max mem: 1.44029 GB 
[11/06 19:59:35 visual_prompt]: Inference (val):avg data time: 2.95e-04, avg batch time: 0.0452, average loss: 0.6957
[11/06 19:59:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.78	
[11/06 19:59:35 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/06 19:59:59 visual_prompt]: 	Training 100/2212. train loss: 0.8682,	0.1811 s / batch. (data: 5.32e-03). ETA=9:13:48, max mem: 1.4 GB 
[11/06 20:00:21 visual_prompt]: 	Training 200/2212. train loss: 0.5353,	0.1566 s / batch. (data: 1.59e-02). ETA=7:58:39, max mem: 1.4 GB 
[11/06 20:00:42 visual_prompt]: 	Training 300/2212. train loss: 0.5404,	0.2015 s / batch. (data: 1.55e-02). ETA=10:15:39, max mem: 1.4 GB 
[11/06 20:01:06 visual_prompt]: 	Training 400/2212. train loss: 0.5664,	0.1654 s / batch. (data: 2.79e-04). ETA=8:25:05, max mem: 1.4 GB 
[11/06 20:01:28 visual_prompt]: 	Training 500/2212. train loss: 0.7738,	1.3962 s / batch. (data: 1.32e+00). ETA=2 days, 23:00:38, max mem: 1.4 GB 
[11/06 20:01:49 visual_prompt]: 	Training 600/2212. train loss: 0.7251,	0.1983 s / batch. (data: 2.08e-02). ETA=10:04:54, max mem: 1.4 GB 
[11/06 20:02:12 visual_prompt]: 	Training 700/2212. train loss: 0.8341,	0.2616 s / batch. (data: 5.78e-03). ETA=13:17:20, max mem: 1.4 GB 
[11/06 20:02:34 visual_prompt]: 	Training 800/2212. train loss: 0.7486,	0.1693 s / batch. (data: 5.34e-03). ETA=8:35:55, max mem: 1.4 GB 
[11/06 20:02:56 visual_prompt]: 	Training 900/2212. train loss: 0.7135,	0.2266 s / batch. (data: 1.04e-02). ETA=11:29:54, max mem: 1.4 GB 
[11/06 20:03:19 visual_prompt]: 	Training 1000/2212. train loss: 0.8818,	0.1866 s / batch. (data: 1.04e-02). ETA=9:27:44, max mem: 1.4 GB 
[11/06 20:03:40 visual_prompt]: 	Training 1100/2212. train loss: 0.6277,	0.1076 s / batch. (data: 1.38e-02). ETA=5:27:18, max mem: 1.4 GB 
[11/06 20:04:01 visual_prompt]: 	Training 1200/2212. train loss: 0.8052,	0.1279 s / batch. (data: 2.20e-04). ETA=6:28:41, max mem: 1.4 GB 
[11/06 20:04:23 visual_prompt]: 	Training 1300/2212. train loss: 0.5989,	0.2081 s / batch. (data: 5.78e-03). ETA=10:32:10, max mem: 1.4 GB 
[11/06 20:04:45 visual_prompt]: 	Training 1400/2212. train loss: 0.7587,	0.1936 s / batch. (data: 5.34e-03). ETA=9:47:45, max mem: 1.4 GB 
[11/06 20:05:08 visual_prompt]: 	Training 1500/2212. train loss: 0.8765,	0.2185 s / batch. (data: 6.08e-03). ETA=11:03:12, max mem: 1.4 GB 
[11/06 20:05:30 visual_prompt]: 	Training 1600/2212. train loss: 0.6073,	0.2071 s / batch. (data: 1.02e-02). ETA=10:28:17, max mem: 1.4 GB 
[11/06 20:05:52 visual_prompt]: 	Training 1700/2212. train loss: 0.6059,	0.2234 s / batch. (data: 3.46e-02). ETA=11:17:10, max mem: 1.4 GB 
[11/06 20:06:15 visual_prompt]: 	Training 1800/2212. train loss: 0.8174,	0.2383 s / batch. (data: 5.78e-03). ETA=12:01:53, max mem: 1.4 GB 
[11/06 20:06:36 visual_prompt]: 	Training 1900/2212. train loss: 0.5963,	0.2044 s / batch. (data: 2.94e-03). ETA=10:18:59, max mem: 1.4 GB 
[11/06 20:06:57 visual_prompt]: 	Training 2000/2212. train loss: 0.8074,	0.2344 s / batch. (data: 1.08e-02). ETA=11:49:21, max mem: 1.4 GB 
[11/06 20:07:19 visual_prompt]: 	Training 2100/2212. train loss: 0.5862,	0.2159 s / batch. (data: 2.74e-02). ETA=10:53:10, max mem: 1.4 GB 
[11/06 20:07:41 visual_prompt]: 	Training 2200/2212. train loss: 0.5618,	0.1641 s / batch. (data: 9.30e-05). ETA=8:16:01, max mem: 1.4 GB 
[11/06 20:07:42 visual_prompt]: Epoch 18 / 100: avg data time: 4.98e-02, avg batch time: 0.2200, average train loss: 0.6982
[11/06 20:08:04 visual_prompt]: 	Test 100/246. loss: 0.844, 0.0706 s / batch. (data: 2.43e-05)max mem: 1.44029 GB 
[11/06 20:08:24 visual_prompt]: 	Test 200/246. loss: 0.844, 0.0735 s / batch. (data: 2.29e-05)max mem: 1.44029 GB 
[11/06 20:08:32 visual_prompt]: Inference (val):avg data time: 1.95e-04, avg batch time: 0.0476, average loss: 0.6893
[11/06 20:08:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.95	
[11/06 20:08:32 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/06 20:08:56 visual_prompt]: 	Training 100/2212. train loss: 0.9035,	0.1933 s / batch. (data: 5.31e-03). ETA=9:43:54, max mem: 1.4 GB 
[11/06 20:09:18 visual_prompt]: 	Training 200/2212. train loss: 0.5679,	0.1737 s / batch. (data: 2.50e-04). ETA=8:44:23, max mem: 1.4 GB 
[11/06 20:09:39 visual_prompt]: 	Training 300/2212. train loss: 1.1631,	0.1659 s / batch. (data: 1.03e-02). ETA=8:20:33, max mem: 1.4 GB 
[11/06 20:10:01 visual_prompt]: 	Training 400/2212. train loss: 0.5566,	0.7950 s / batch. (data: 6.91e-01). ETA=1 day, 15:58:07, max mem: 1.4 GB 
[11/06 20:10:21 visual_prompt]: 	Training 500/2212. train loss: 0.6033,	0.2094 s / batch. (data: 5.75e-03). ETA=10:31:19, max mem: 1.4 GB 
[11/06 20:10:44 visual_prompt]: 	Training 600/2212. train loss: 0.8827,	0.1393 s / batch. (data: 1.87e-02). ETA=6:59:45, max mem: 1.4 GB 
[11/06 20:11:05 visual_prompt]: 	Training 700/2212. train loss: 0.8296,	0.2065 s / batch. (data: 1.08e-02). ETA=10:21:42, max mem: 1.4 GB 
[11/06 20:11:27 visual_prompt]: 	Training 800/2212. train loss: 0.5638,	0.1993 s / batch. (data: 2.34e-04). ETA=9:59:48, max mem: 1.4 GB 
[11/06 20:11:48 visual_prompt]: 	Training 900/2212. train loss: 0.5569,	0.3764 s / batch. (data: 2.40e-01). ETA=18:52:08, max mem: 1.4 GB 
[11/06 20:12:09 visual_prompt]: 	Training 1000/2212. train loss: 0.5466,	0.2103 s / batch. (data: 2.05e-02). ETA=10:32:19, max mem: 1.4 GB 
[11/06 20:12:32 visual_prompt]: 	Training 1100/2212. train loss: 0.9052,	0.1790 s / batch. (data: 5.31e-03). ETA=8:57:48, max mem: 1.4 GB 
[11/06 20:12:52 visual_prompt]: 	Training 1200/2212. train loss: 1.3848,	0.1812 s / batch. (data: 1.04e-02). ETA=9:04:15, max mem: 1.4 GB 
[11/06 20:13:16 visual_prompt]: 	Training 1300/2212. train loss: 0.8316,	0.1255 s / batch. (data: 5.31e-03). ETA=6:16:39, max mem: 1.4 GB 
[11/06 20:13:35 visual_prompt]: 	Training 1400/2212. train loss: 0.7402,	0.1859 s / batch. (data: 1.04e-02). ETA=9:17:43, max mem: 1.4 GB 
[11/06 20:13:57 visual_prompt]: 	Training 1500/2212. train loss: 0.7360,	0.2150 s / batch. (data: 3.20e-02). ETA=10:44:38, max mem: 1.4 GB 
[11/06 20:14:19 visual_prompt]: 	Training 1600/2212. train loss: 0.8149,	0.3605 s / batch. (data: 2.79e-01). ETA=18:00:20, max mem: 1.4 GB 
[11/06 20:14:40 visual_prompt]: 	Training 1700/2212. train loss: 0.4888,	0.1556 s / batch. (data: 7.05e-03). ETA=7:45:58, max mem: 1.4 GB 
[11/06 20:15:01 visual_prompt]: 	Training 1800/2212. train loss: 0.6148,	0.1621 s / batch. (data: 9.38e-03). ETA=8:05:09, max mem: 1.4 GB 
[11/06 20:15:22 visual_prompt]: 	Training 1900/2212. train loss: 0.5154,	0.2100 s / batch. (data: 1.54e-02). ETA=10:28:05, max mem: 1.4 GB 
[11/06 20:15:43 visual_prompt]: 	Training 2000/2212. train loss: 0.6448,	0.1397 s / batch. (data: 2.36e-04). ETA=6:57:41, max mem: 1.4 GB 
[11/06 20:16:05 visual_prompt]: 	Training 2100/2212. train loss: 0.5808,	0.1705 s / batch. (data: 1.59e-02). ETA=8:29:26, max mem: 1.4 GB 
[11/06 20:16:26 visual_prompt]: 	Training 2200/2212. train loss: 0.3812,	0.1206 s / batch. (data: 9.78e-05). ETA=6:00:04, max mem: 1.4 GB 
[11/06 20:16:27 visual_prompt]: Epoch 19 / 100: avg data time: 4.91e-02, avg batch time: 0.2144, average train loss: 0.7023
[11/06 20:16:48 visual_prompt]: 	Test 100/246. loss: 0.432, 0.0796 s / batch. (data: 2.36e-05)max mem: 1.44029 GB 
[11/06 20:17:07 visual_prompt]: 	Test 200/246. loss: 0.432, 0.0793 s / batch. (data: 2.41e-05)max mem: 1.44029 GB 
[11/06 20:17:15 visual_prompt]: Inference (val):avg data time: 6.46e-04, avg batch time: 0.0440, average loss: 0.7701
[11/06 20:17:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.02	
[11/06 20:17:15 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/06 20:17:38 visual_prompt]: 	Training 100/2212. train loss: 0.5124,	0.2194 s / batch. (data: 1.55e-02). ETA=10:54:48, max mem: 1.4 GB 
[11/06 20:18:01 visual_prompt]: 	Training 200/2212. train loss: 1.0433,	0.1030 s / batch. (data: 2.31e-04). ETA=5:07:12, max mem: 1.4 GB 
[11/06 20:18:23 visual_prompt]: 	Training 300/2212. train loss: 0.5681,	0.1891 s / batch. (data: 1.40e-02). ETA=9:23:49, max mem: 1.4 GB 
[11/06 20:18:45 visual_prompt]: 	Training 400/2212. train loss: 0.6607,	0.3676 s / batch. (data: 2.40e-01). ETA=18:15:17, max mem: 1.4 GB 
[11/06 20:19:06 visual_prompt]: 	Training 500/2212. train loss: 0.4971,	0.2347 s / batch. (data: 1.83e-02). ETA=11:38:52, max mem: 1.4 GB 
[11/06 20:19:28 visual_prompt]: 	Training 600/2212. train loss: 0.8525,	0.2119 s / batch. (data: 4.84e-03). ETA=10:30:33, max mem: 1.4 GB 
[11/06 20:19:49 visual_prompt]: 	Training 700/2212. train loss: 0.6120,	0.1675 s / batch. (data: 2.25e-04). ETA=8:18:06, max mem: 1.4 GB 
[11/06 20:20:09 visual_prompt]: 	Training 800/2212. train loss: 0.5183,	0.2260 s / batch. (data: 1.59e-02). ETA=11:11:55, max mem: 1.4 GB 
[11/06 20:20:31 visual_prompt]: 	Training 900/2212. train loss: 0.7786,	0.1561 s / batch. (data: 1.17e-02). ETA=7:43:51, max mem: 1.4 GB 
[11/06 20:20:52 visual_prompt]: 	Training 1000/2212. train loss: 0.9230,	0.2126 s / batch. (data: 2.60e-02). ETA=10:31:14, max mem: 1.4 GB 
[11/06 20:21:13 visual_prompt]: 	Training 1100/2212. train loss: 0.4839,	0.2032 s / batch. (data: 2.43e-02). ETA=10:02:55, max mem: 1.4 GB 
[11/06 20:21:35 visual_prompt]: 	Training 1200/2212. train loss: 0.7820,	0.2150 s / batch. (data: 1.55e-02). ETA=10:37:50, max mem: 1.4 GB 
[11/06 20:21:57 visual_prompt]: 	Training 1300/2212. train loss: 0.8541,	0.1301 s / batch. (data: 2.37e-04). ETA=6:25:34, max mem: 1.4 GB 
[11/06 20:22:19 visual_prompt]: 	Training 1400/2212. train loss: 0.4822,	0.1580 s / batch. (data: 6.45e-04). ETA=7:48:05, max mem: 1.4 GB 
[11/06 20:22:41 visual_prompt]: 	Training 1500/2212. train loss: 0.6898,	0.1793 s / batch. (data: 1.80e-02). ETA=8:50:55, max mem: 1.4 GB 
[11/06 20:23:02 visual_prompt]: 	Training 1600/2212. train loss: 0.8665,	0.1875 s / batch. (data: 1.24e-03). ETA=9:14:56, max mem: 1.4 GB 
[11/06 20:23:23 visual_prompt]: 	Training 1700/2212. train loss: 0.8411,	0.1943 s / batch. (data: 1.04e-02). ETA=9:34:47, max mem: 1.4 GB 
[11/06 20:23:44 visual_prompt]: 	Training 1800/2212. train loss: 0.6116,	0.1797 s / batch. (data: 7.70e-03). ETA=8:51:08, max mem: 1.4 GB 
[11/06 20:24:06 visual_prompt]: 	Training 1900/2212. train loss: 0.8567,	0.1391 s / batch. (data: 1.61e-02). ETA=6:51:02, max mem: 1.4 GB 
[11/06 20:24:27 visual_prompt]: 	Training 2000/2212. train loss: 0.5968,	0.2402 s / batch. (data: 3.27e-02). ETA=11:49:11, max mem: 1.4 GB 
[11/06 20:24:50 visual_prompt]: 	Training 2100/2212. train loss: 0.6830,	0.1927 s / batch. (data: 8.23e-03). ETA=9:28:39, max mem: 1.4 GB 
[11/06 20:25:11 visual_prompt]: 	Training 2200/2212. train loss: 0.5685,	0.0987 s / batch. (data: 1.02e-04). ETA=4:51:06, max mem: 1.4 GB 
[11/06 20:25:12 visual_prompt]: Epoch 20 / 100: avg data time: 4.69e-02, avg batch time: 0.2153, average train loss: 0.7037
[11/06 20:25:33 visual_prompt]: 	Test 100/246. loss: 0.688, 0.0417 s / batch. (data: 2.07e-05)max mem: 1.44029 GB 
[11/06 20:25:53 visual_prompt]: 	Test 200/246. loss: 0.688, 0.0728 s / batch. (data: 2.43e-05)max mem: 1.44029 GB 
[11/06 20:26:01 visual_prompt]: Inference (val):avg data time: 5.80e-04, avg batch time: 0.0459, average loss: 0.6937
[11/06 20:26:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.95	
[11/06 20:26:01 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/06 20:26:26 visual_prompt]: 	Training 100/2212. train loss: 0.5834,	1.5881 s / batch. (data: 1.50e+00). ETA=3 days, 6:01:09, max mem: 1.4 GB 
[11/06 20:26:46 visual_prompt]: 	Training 200/2212. train loss: 0.6685,	0.1853 s / batch. (data: 2.05e-04). ETA=9:05:49, max mem: 1.4 GB 
[11/06 20:27:09 visual_prompt]: 	Training 300/2212. train loss: 1.0859,	0.1584 s / batch. (data: 1.53e-02). ETA=7:46:24, max mem: 1.4 GB 
[11/06 20:27:31 visual_prompt]: 	Training 400/2212. train loss: 0.8037,	0.1692 s / batch. (data: 6.60e-04). ETA=8:17:52, max mem: 1.4 GB 
[11/06 20:27:51 visual_prompt]: 	Training 500/2212. train loss: 0.7896,	0.1309 s / batch. (data: 2.40e-04). ETA=6:25:01, max mem: 1.4 GB 
[11/06 20:28:13 visual_prompt]: 	Training 600/2212. train loss: 0.8306,	0.1770 s / batch. (data: 4.51e-04). ETA=8:40:07, max mem: 1.4 GB 
[11/06 20:28:35 visual_prompt]: 	Training 700/2212. train loss: 0.5770,	0.1997 s / batch. (data: 5.33e-03). ETA=9:46:33, max mem: 1.4 GB 
[11/06 20:28:57 visual_prompt]: 	Training 800/2212. train loss: 0.6874,	0.2277 s / batch. (data: 6.25e-04). ETA=11:08:26, max mem: 1.4 GB 
[11/06 20:29:19 visual_prompt]: 	Training 900/2212. train loss: 0.7887,	0.1800 s / batch. (data: 1.37e-02). ETA=8:48:19, max mem: 1.4 GB 
[11/06 20:29:39 visual_prompt]: 	Training 1000/2212. train loss: 0.6271,	0.2111 s / batch. (data: 1.04e-02). ETA=10:19:07, max mem: 1.4 GB 
[11/06 20:29:59 visual_prompt]: 	Training 1100/2212. train loss: 0.7528,	0.1959 s / batch. (data: 3.63e-02). ETA=9:34:05, max mem: 1.4 GB 
[11/06 20:30:21 visual_prompt]: 	Training 1200/2212. train loss: 0.7071,	0.1807 s / batch. (data: 1.55e-02). ETA=8:49:11, max mem: 1.4 GB 
[11/06 20:30:43 visual_prompt]: 	Training 1300/2212. train loss: 0.6523,	0.6334 s / batch. (data: 4.95e-01). ETA=1 day, 6:54:19, max mem: 1.4 GB 
[11/06 20:31:04 visual_prompt]: 	Training 1400/2212. train loss: 0.6152,	0.2014 s / batch. (data: 2.47e-02). ETA=9:49:24, max mem: 1.4 GB 
[11/06 20:31:26 visual_prompt]: 	Training 1500/2212. train loss: 0.5682,	0.2129 s / batch. (data: 1.39e-02). ETA=10:22:27, max mem: 1.4 GB 
[11/06 20:31:46 visual_prompt]: 	Training 1600/2212. train loss: 0.5715,	0.1925 s / batch. (data: 6.54e-04). ETA=9:22:40, max mem: 1.4 GB 
[11/06 20:32:07 visual_prompt]: 	Training 1700/2212. train loss: 0.7787,	0.1285 s / batch. (data: 5.32e-03). ETA=6:15:22, max mem: 1.4 GB 
[11/06 20:32:28 visual_prompt]: 	Training 1800/2212. train loss: 0.6419,	0.1267 s / batch. (data: 1.04e-02). ETA=6:09:48, max mem: 1.4 GB 
[11/06 20:32:49 visual_prompt]: 	Training 1900/2212. train loss: 0.6088,	0.2008 s / batch. (data: 2.05e-02). ETA=9:45:44, max mem: 1.4 GB 
[11/06 20:33:10 visual_prompt]: 	Training 2000/2212. train loss: 0.7846,	0.2570 s / batch. (data: 1.31e-01). ETA=12:29:24, max mem: 1.4 GB 
[11/06 20:33:31 visual_prompt]: 	Training 2100/2212. train loss: 0.9442,	0.1882 s / batch. (data: 1.04e-02). ETA=9:08:37, max mem: 1.4 GB 
[11/06 20:33:51 visual_prompt]: 	Training 2200/2212. train loss: 0.7141,	0.1627 s / batch. (data: 1.31e-04). ETA=7:54:01, max mem: 1.4 GB 
[11/06 20:33:52 visual_prompt]: Epoch 21 / 100: avg data time: 4.74e-02, avg batch time: 0.2130, average train loss: 0.6952
[11/06 20:34:13 visual_prompt]: 	Test 100/246. loss: 0.909, 0.0470 s / batch. (data: 2.34e-05)max mem: 1.44029 GB 
[11/06 20:34:33 visual_prompt]: 	Test 200/246. loss: 0.909, 0.0549 s / batch. (data: 2.31e-05)max mem: 1.44029 GB 
[11/06 20:34:41 visual_prompt]: Inference (val):avg data time: 4.91e-04, avg batch time: 0.0450, average loss: 0.6931
[11/06 20:34:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.09	
[11/06 20:34:41 visual_prompt]: Stopping early.
[11/06 20:34:41 visual_prompt]: Rank of current process: 0. World size: 1
[11/06 20:34:41 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/06 20:34:41 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '1', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/06 20:34:41 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/06 20:34:41 visual_prompt]: Training with config:
[11/06 20:34:41 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/crop224/val/seed0/lr0.005_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 1, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/06 20:34:41 visual_prompt]: Loading training data...
[11/06 20:34:41 visual_prompt]: Constructing mammo-cbis dataset train...
[11/06 20:34:41 visual_prompt]: Loading validation data...
[11/06 20:34:41 visual_prompt]: Constructing mammo-cbis dataset val...
[11/06 20:34:41 visual_prompt]: Constructing models...
[11/06 20:34:45 visual_prompt]: Enable all parameters update during training
[11/06 20:34:45 visual_prompt]: Total Parameters: 85800194	 Gradient Parameters: 85800194
[11/06 20:34:45 visual_prompt]: tuned percent:100.000
[11/06 20:34:45 visual_prompt]: Device used for model: 0
[11/06 20:34:45 visual_prompt]: Setting up Evaluator...
[11/06 20:34:45 visual_prompt]: Setting up Trainer...
[11/06 20:34:45 visual_prompt]: 	Setting up the optimizer...
[11/06 20:34:45 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/06 20:35:09 visual_prompt]: 	Training 100/2212. train loss: 17.0073,	0.2123 s / batch. (data: 2.05e-02). ETA=13:02:13, max mem: 2.7 GB 
[11/06 20:35:30 visual_prompt]: 	Training 200/2212. train loss: 8.7706,	0.1805 s / batch. (data: 2.50e-02). ETA=11:04:41, max mem: 2.7 GB 
[11/06 20:35:51 visual_prompt]: 	Training 300/2212. train loss: 0.0001,	0.1797 s / batch. (data: 1.58e-02). ETA=11:01:40, max mem: 2.7 GB 
[11/06 20:36:12 visual_prompt]: 	Training 400/2212. train loss: 9.7364,	0.1227 s / batch. (data: 2.01e-04). ETA=7:31:42, max mem: 2.7 GB 
[11/06 20:36:34 visual_prompt]: 	Training 500/2212. train loss: 9.6752,	0.1370 s / batch. (data: 1.04e-02). ETA=8:24:02, max mem: 2.7 GB 
[11/06 20:36:55 visual_prompt]: 	Training 600/2212. train loss: 0.0000,	0.2225 s / batch. (data: 7.30e-03). ETA=13:38:13, max mem: 2.7 GB 
[11/06 20:37:17 visual_prompt]: 	Training 700/2212. train loss: 10.7484,	0.1756 s / batch. (data: 1.65e-02). ETA=10:45:23, max mem: 2.7 GB 
[11/06 20:37:39 visual_prompt]: 	Training 800/2212. train loss: 10.3468,	0.2075 s / batch. (data: 6.80e-04). ETA=12:42:10, max mem: 2.7 GB 
[11/06 20:38:00 visual_prompt]: 	Training 900/2212. train loss: 0.0000,	0.1995 s / batch. (data: 1.55e-02). ETA=12:12:23, max mem: 2.7 GB 
[11/06 20:38:20 visual_prompt]: 	Training 1000/2212. train loss: 0.0001,	0.1790 s / batch. (data: 5.15e-03). ETA=10:57:05, max mem: 2.7 GB 
[11/06 20:38:41 visual_prompt]: 	Training 1100/2212. train loss: 0.0000,	0.1119 s / batch. (data: 6.80e-04). ETA=6:50:30, max mem: 2.7 GB 
[11/06 20:39:02 visual_prompt]: 	Training 1200/2212. train loss: 0.0000,	0.1804 s / batch. (data: 5.33e-03). ETA=11:01:32, max mem: 2.7 GB 
[11/06 20:39:22 visual_prompt]: 	Training 1300/2212. train loss: 0.0000,	0.2166 s / batch. (data: 1.08e-02). ETA=13:13:55, max mem: 2.7 GB 
[11/06 20:39:44 visual_prompt]: 	Training 1400/2212. train loss: 0.0027,	0.1277 s / batch. (data: 1.04e-02). ETA=7:47:38, max mem: 2.7 GB 
[11/06 20:40:05 visual_prompt]: 	Training 1500/2212. train loss: 5.3648,	0.1721 s / batch. (data: 5.31e-03). ETA=10:30:08, max mem: 2.7 GB 
[11/06 20:40:25 visual_prompt]: 	Training 1600/2212. train loss: 5.2697,	0.1866 s / batch. (data: 1.04e-02). ETA=11:22:46, max mem: 2.7 GB 
[11/06 20:40:46 visual_prompt]: 	Training 1700/2212. train loss: 0.0000,	0.2135 s / batch. (data: 3.06e-02). ETA=13:01:10, max mem: 2.7 GB 
[11/06 20:41:07 visual_prompt]: 	Training 1800/2212. train loss: 0.0000,	0.1921 s / batch. (data: 1.55e-02). ETA=11:42:27, max mem: 2.7 GB 
[11/06 20:41:28 visual_prompt]: 	Training 1900/2212. train loss: 8.6260,	0.2123 s / batch. (data: 1.14e-02). ETA=12:55:57, max mem: 2.7 GB 
[11/06 20:41:50 visual_prompt]: 	Training 2000/2212. train loss: 0.0002,	0.1677 s / batch. (data: 2.06e-04). ETA=10:12:37, max mem: 2.7 GB 
[11/06 20:42:10 visual_prompt]: 	Training 2100/2212. train loss: 14.9015,	0.1973 s / batch. (data: 5.65e-03). ETA=12:00:29, max mem: 2.7 GB 
[11/06 20:42:30 visual_prompt]: 	Training 2200/2212. train loss: 7.6808,	0.0523 s / batch. (data: 8.82e-05). ETA=3:11:01, max mem: 2.7 GB 
[11/06 20:42:31 visual_prompt]: Epoch 1 / 100: avg data time: 4.43e-02, avg batch time: 0.2105, average train loss: 5.2946
[11/06 20:42:52 visual_prompt]: 	Test 100/246. loss: 0.001, 0.0650 s / batch. (data: 2.29e-05)max mem: 2.72052 GB 
[11/06 20:43:11 visual_prompt]: 	Test 200/246. loss: 0.002, 0.0628 s / batch. (data: 2.46e-05)max mem: 2.72052 GB 
[11/06 20:43:20 visual_prompt]: Inference (val):avg data time: 2.15e-04, avg batch time: 0.0451, average loss: 4.3337
[11/06 20:43:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.86	
[11/06 20:43:20 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/06 20:43:45 visual_prompt]: 	Training 100/2212. train loss: 0.0110,	0.1887 s / batch. (data: 1.04e-02). ETA=11:28:34, max mem: 2.7 GB 
[11/06 20:44:06 visual_prompt]: 	Training 200/2212. train loss: 0.0058,	0.2275 s / batch. (data: 5.71e-04). ETA=13:49:42, max mem: 2.7 GB 
[11/06 20:44:28 visual_prompt]: 	Training 300/2212. train loss: 0.3867,	0.2071 s / batch. (data: 1.04e-02). ETA=12:34:47, max mem: 2.7 GB 
[11/06 20:44:49 visual_prompt]: 	Training 400/2212. train loss: 1.3777,	0.1905 s / batch. (data: 2.76e-04). ETA=11:34:06, max mem: 2.7 GB 
[11/06 20:45:08 visual_prompt]: 	Training 500/2212. train loss: 0.3260,	0.1999 s / batch. (data: 2.49e-02). ETA=12:07:50, max mem: 2.7 GB 
[11/06 20:45:29 visual_prompt]: 	Training 600/2212. train loss: 0.2823,	0.2373 s / batch. (data: 8.91e-04). ETA=14:23:52, max mem: 2.7 GB 
[11/06 20:45:50 visual_prompt]: 	Training 700/2212. train loss: 1.1778,	0.2935 s / batch. (data: 1.72e-01). ETA=17:47:51, max mem: 2.7 GB 
[11/06 20:46:12 visual_prompt]: 	Training 800/2212. train loss: 0.2036,	0.1778 s / batch. (data: 2.20e-04). ETA=10:46:40, max mem: 2.7 GB 
[11/06 20:46:34 visual_prompt]: 	Training 900/2212. train loss: 0.3994,	0.2083 s / batch. (data: 1.70e-02). ETA=12:37:13, max mem: 2.7 GB 
[11/06 20:46:57 visual_prompt]: 	Training 1000/2212. train loss: 1.7676,	0.1694 s / batch. (data: 1.04e-02). ETA=10:15:19, max mem: 2.7 GB 
[11/06 20:47:18 visual_prompt]: 	Training 1100/2212. train loss: 0.0406,	0.1696 s / batch. (data: 1.06e-02). ETA=10:15:56, max mem: 2.7 GB 
[11/06 20:47:39 visual_prompt]: 	Training 1200/2212. train loss: 0.8795,	0.1837 s / batch. (data: 1.04e-02). ETA=11:06:56, max mem: 2.7 GB 
[11/06 20:48:02 visual_prompt]: 	Training 1300/2212. train loss: 1.6760,	0.5760 s / batch. (data: 4.17e-01). ETA=1 day, 10:49:56, max mem: 2.7 GB 
[11/06 20:48:23 visual_prompt]: 	Training 1400/2212. train loss: 1.4970,	0.5435 s / batch. (data: 3.96e-01). ETA=1 day, 8:51:03, max mem: 2.7 GB 
[11/06 20:48:46 visual_prompt]: 	Training 1500/2212. train loss: 0.9697,	0.2371 s / batch. (data: 1.08e-02). ETA=14:19:31, max mem: 2.7 GB 
[11/06 20:49:07 visual_prompt]: 	Training 1600/2212. train loss: 1.3313,	0.2395 s / batch. (data: 3.85e-02). ETA=14:27:46, max mem: 2.7 GB 
[11/06 20:49:28 visual_prompt]: 	Training 1700/2212. train loss: 0.3226,	0.2255 s / batch. (data: 1.60e-02). ETA=13:36:39, max mem: 2.7 GB 
[11/06 20:49:49 visual_prompt]: 	Training 1800/2212. train loss: 0.4831,	0.1854 s / batch. (data: 1.97e-02). ETA=11:11:03, max mem: 2.7 GB 
[11/06 20:50:10 visual_prompt]: 	Training 1900/2212. train loss: 0.1575,	0.0911 s / batch. (data: 2.14e-04). ETA=5:29:28, max mem: 2.7 GB 
[11/06 20:50:31 visual_prompt]: 	Training 2000/2212. train loss: 0.6884,	0.1789 s / batch. (data: 2.06e-02). ETA=10:47:05, max mem: 2.7 GB 
[11/06 20:50:52 visual_prompt]: 	Training 2100/2212. train loss: 0.1138,	0.2105 s / batch. (data: 1.32e-02). ETA=12:40:58, max mem: 2.7 GB 
[11/06 20:51:13 visual_prompt]: 	Training 2200/2212. train loss: 0.4864,	0.0862 s / batch. (data: 1.01e-04). ETA=5:11:26, max mem: 2.7 GB 
[11/06 20:51:13 visual_prompt]: Epoch 2 / 100: avg data time: 4.65e-02, avg batch time: 0.2141, average train loss: 1.4344
[11/06 20:51:34 visual_prompt]: 	Test 100/246. loss: 0.152, 0.0468 s / batch. (data: 2.38e-05)max mem: 2.72052 GB 
[11/06 20:51:54 visual_prompt]: 	Test 200/246. loss: 0.149, 0.0470 s / batch. (data: 3.93e-05)max mem: 2.72052 GB 
[11/06 20:52:02 visual_prompt]: Inference (val):avg data time: 9.40e-05, avg batch time: 0.0476, average loss: 1.1422
[11/06 20:52:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.83	
[11/06 20:52:02 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/06 20:52:25 visual_prompt]: 	Training 100/2212. train loss: 0.4078,	0.1301 s / batch. (data: 2.07e-04). ETA=7:49:40, max mem: 2.7 GB 
[11/06 20:52:47 visual_prompt]: 	Training 200/2212. train loss: 3.2550,	0.1942 s / batch. (data: 1.04e-02). ETA=11:41:02, max mem: 2.7 GB 
[11/06 20:53:08 visual_prompt]: 	Training 300/2212. train loss: 2.6896,	0.1459 s / batch. (data: 1.04e-02). ETA=8:46:25, max mem: 2.7 GB 
[11/06 20:53:30 visual_prompt]: 	Training 400/2212. train loss: 1.0970,	0.1697 s / batch. (data: 2.05e-02). ETA=10:11:50, max mem: 2.7 GB 
[11/06 20:53:50 visual_prompt]: 	Training 500/2212. train loss: 2.0802,	0.1909 s / batch. (data: 1.55e-02). ETA=11:28:12, max mem: 2.7 GB 
[11/06 20:54:13 visual_prompt]: 	Training 600/2212. train loss: 1.2381,	0.1919 s / batch. (data: 2.60e-02). ETA=11:31:17, max mem: 2.7 GB 
[11/06 20:54:34 visual_prompt]: 	Training 700/2212. train loss: 0.2807,	0.2538 s / batch. (data: 1.45e-01). ETA=15:13:55, max mem: 2.7 GB 
[11/06 20:54:55 visual_prompt]: 	Training 800/2212. train loss: 1.7275,	0.1578 s / batch. (data: 2.22e-04). ETA=9:28:08, max mem: 2.7 GB 
[11/06 20:55:16 visual_prompt]: 	Training 900/2212. train loss: 1.1637,	0.1871 s / batch. (data: 6.96e-04). ETA=11:13:17, max mem: 2.7 GB 
[11/06 20:55:36 visual_prompt]: 	Training 1000/2212. train loss: 0.1545,	0.1917 s / batch. (data: 1.04e-02). ETA=11:29:15, max mem: 2.7 GB 
[11/06 20:55:58 visual_prompt]: 	Training 1100/2212. train loss: 0.3338,	0.1393 s / batch. (data: 8.05e-03). ETA=8:20:38, max mem: 2.7 GB 
[11/06 20:56:17 visual_prompt]: 	Training 1200/2212. train loss: 1.1778,	0.1749 s / batch. (data: 1.55e-02). ETA=10:28:22, max mem: 2.7 GB 
[11/06 20:56:39 visual_prompt]: 	Training 1300/2212. train loss: 1.6652,	0.1410 s / batch. (data: 7.00e-04). ETA=8:26:18, max mem: 2.7 GB 
[11/06 20:57:01 visual_prompt]: 	Training 1400/2212. train loss: 0.5532,	0.0569 s / batch. (data: 1.97e-04). ETA=3:24:13, max mem: 2.7 GB 
[11/06 20:57:21 visual_prompt]: 	Training 1500/2212. train loss: 0.0957,	0.1928 s / batch. (data: 1.55e-02). ETA=11:31:49, max mem: 2.7 GB 
[11/06 20:57:42 visual_prompt]: 	Training 1600/2212. train loss: 0.1149,	0.1890 s / batch. (data: 1.04e-02). ETA=11:17:56, max mem: 2.7 GB 
[11/06 20:58:05 visual_prompt]: 	Training 1700/2212. train loss: 0.9551,	0.2054 s / batch. (data: 2.56e-02). ETA=12:16:17, max mem: 2.7 GB 
[11/06 20:58:27 visual_prompt]: 	Training 1800/2212. train loss: 0.3133,	0.2041 s / batch. (data: 1.54e-02). ETA=12:11:09, max mem: 2.7 GB 
[11/06 20:58:47 visual_prompt]: 	Training 1900/2212. train loss: 0.2022,	0.2047 s / batch. (data: 2.42e-04). ETA=12:12:57, max mem: 2.7 GB 
[11/06 20:59:07 visual_prompt]: 	Training 2000/2212. train loss: 1.7955,	0.1831 s / batch. (data: 4.04e-03). ETA=10:55:23, max mem: 2.7 GB 
[11/06 20:59:31 visual_prompt]: 	Training 2100/2212. train loss: 3.6632,	0.2080 s / batch. (data: 1.42e-02). ETA=12:24:07, max mem: 2.7 GB 
[11/06 20:59:51 visual_prompt]: 	Training 2200/2212. train loss: 0.5762,	0.6078 s / batch. (data: 5.16e-01). ETA=1 day, 12:13:44, max mem: 2.7 GB 
[11/06 20:59:52 visual_prompt]: Epoch 3 / 100: avg data time: 4.76e-02, avg batch time: 0.2123, average train loss: 1.0003
[11/06 21:00:14 visual_prompt]: 	Test 100/246. loss: 0.710, 0.0671 s / batch. (data: 1.88e-05)max mem: 2.72052 GB 
[11/06 21:00:34 visual_prompt]: 	Test 200/246. loss: 0.706, 0.0913 s / batch. (data: 2.34e-05)max mem: 2.72052 GB 
[11/06 21:00:42 visual_prompt]: Inference (val):avg data time: 2.40e-04, avg batch time: 0.0431, average loss: 0.6914
[11/06 21:00:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 53.00	
[11/06 21:00:42 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/06 21:01:06 visual_prompt]: 	Training 100/2212. train loss: 1.9716,	0.1689 s / batch. (data: 7.29e-04). ETA=10:03:51, max mem: 2.7 GB 
[11/06 21:01:26 visual_prompt]: 	Training 200/2212. train loss: 0.6584,	0.1922 s / batch. (data: 3.29e-02). ETA=11:26:34, max mem: 2.7 GB 
[11/06 21:01:48 visual_prompt]: 	Training 300/2212. train loss: 0.3780,	0.1922 s / batch. (data: 2.44e-02). ETA=11:26:24, max mem: 2.7 GB 
[11/06 21:02:09 visual_prompt]: 	Training 400/2212. train loss: 1.4136,	0.2132 s / batch. (data: 2.44e-02). ETA=12:40:54, max mem: 2.7 GB 
[11/06 21:02:29 visual_prompt]: 	Training 500/2212. train loss: 0.4189,	0.1586 s / batch. (data: 1.04e-02). ETA=9:25:58, max mem: 2.7 GB 
[11/06 21:02:50 visual_prompt]: 	Training 600/2212. train loss: 0.3889,	0.1547 s / batch. (data: 1.07e-03). ETA=9:11:35, max mem: 2.7 GB 
[11/06 21:03:10 visual_prompt]: 	Training 700/2212. train loss: 0.8303,	0.2042 s / batch. (data: 3.24e-02). ETA=12:07:56, max mem: 2.7 GB 
[11/06 21:03:33 visual_prompt]: 	Training 800/2212. train loss: 1.9113,	0.1829 s / batch. (data: 2.35e-04). ETA=10:51:37, max mem: 2.7 GB 
[11/06 21:03:53 visual_prompt]: 	Training 900/2212. train loss: 0.4549,	0.2074 s / batch. (data: 3.51e-03). ETA=12:18:36, max mem: 2.7 GB 
[11/06 21:04:14 visual_prompt]: 	Training 1000/2212. train loss: 0.8969,	0.1700 s / batch. (data: 9.65e-03). ETA=10:05:04, max mem: 2.7 GB 
[11/06 21:04:35 visual_prompt]: 	Training 1100/2212. train loss: 2.6368,	0.1972 s / batch. (data: 1.38e-02). ETA=11:41:27, max mem: 2.7 GB 
[11/06 21:04:56 visual_prompt]: 	Training 1200/2212. train loss: 0.4055,	0.1510 s / batch. (data: 2.10e-04). ETA=8:56:56, max mem: 2.7 GB 
[11/06 21:05:17 visual_prompt]: 	Training 1300/2212. train loss: 0.8095,	0.2073 s / batch. (data: 1.55e-02). ETA=12:16:43, max mem: 2.7 GB 
[11/06 21:05:38 visual_prompt]: 	Training 1400/2212. train loss: 0.6338,	0.2137 s / batch. (data: 1.04e-02). ETA=12:39:17, max mem: 2.7 GB 
[11/06 21:05:59 visual_prompt]: 	Training 1500/2212. train loss: 0.5614,	0.1696 s / batch. (data: 2.16e-04). ETA=10:02:05, max mem: 2.7 GB 
[11/06 21:06:20 visual_prompt]: 	Training 1600/2212. train loss: 0.2994,	0.1788 s / batch. (data: 2.93e-02). ETA=10:34:31, max mem: 2.7 GB 
[11/06 21:06:42 visual_prompt]: 	Training 1700/2212. train loss: 0.0828,	0.1641 s / batch. (data: 5.29e-03). ETA=9:42:11, max mem: 2.7 GB 
[11/06 21:07:02 visual_prompt]: 	Training 1800/2212. train loss: 0.9597,	0.1372 s / batch. (data: 2.13e-04). ETA=8:06:35, max mem: 2.7 GB 
[11/06 21:07:23 visual_prompt]: 	Training 1900/2212. train loss: 0.6607,	0.1975 s / batch. (data: 2.38e-04). ETA=11:39:53, max mem: 2.7 GB 
[11/06 21:07:45 visual_prompt]: 	Training 2000/2212. train loss: 1.0044,	0.1791 s / batch. (data: 1.04e-02). ETA=10:34:19, max mem: 2.7 GB 
[11/06 21:08:06 visual_prompt]: 	Training 2100/2212. train loss: 0.9435,	0.1893 s / batch. (data: 6.94e-04). ETA=11:10:25, max mem: 2.7 GB 
[11/06 21:08:27 visual_prompt]: 	Training 2200/2212. train loss: 0.3127,	0.1331 s / batch. (data: 8.80e-05). ETA=7:51:02, max mem: 2.7 GB 
[11/06 21:08:28 visual_prompt]: Epoch 4 / 100: avg data time: 4.46e-02, avg batch time: 0.2109, average train loss: 0.8940
[11/06 21:08:49 visual_prompt]: 	Test 100/246. loss: 1.190, 0.0620 s / batch. (data: 2.29e-05)max mem: 2.72052 GB 
[11/06 21:09:09 visual_prompt]: 	Test 200/246. loss: 1.188, 0.0650 s / batch. (data: 2.34e-05)max mem: 2.72052 GB 
[11/06 21:09:17 visual_prompt]: Inference (val):avg data time: 5.24e-04, avg batch time: 0.0467, average loss: 0.7343
[11/06 21:09:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.18	
[11/06 21:09:17 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/06 21:09:40 visual_prompt]: 	Training 100/2212. train loss: 2.0024,	0.3694 s / batch. (data: 2.48e-01). ETA=21:46:37, max mem: 2.7 GB 
[11/06 21:10:01 visual_prompt]: 	Training 200/2212. train loss: 1.0589,	0.1599 s / batch. (data: 6.43e-03). ETA=9:25:13, max mem: 2.7 GB 
[11/06 21:10:22 visual_prompt]: 	Training 300/2212. train loss: 1.5058,	0.1956 s / batch. (data: 1.55e-02). ETA=11:31:16, max mem: 2.7 GB 
[11/06 21:10:44 visual_prompt]: 	Training 400/2212. train loss: 2.6446,	0.1376 s / batch. (data: 6.11e-04). ETA=8:06:09, max mem: 2.7 GB 
[11/06 21:11:06 visual_prompt]: 	Training 500/2212. train loss: 0.6108,	0.1722 s / batch. (data: 5.32e-03). ETA=10:08:09, max mem: 2.7 GB 
[11/06 21:11:27 visual_prompt]: 	Training 600/2212. train loss: 0.6044,	0.1858 s / batch. (data: 1.57e-02). ETA=10:55:39, max mem: 2.7 GB 
[11/06 21:11:46 visual_prompt]: 	Training 700/2212. train loss: 1.1370,	0.2093 s / batch. (data: 2.75e-04). ETA=12:18:17, max mem: 2.7 GB 
[11/06 21:12:08 visual_prompt]: 	Training 800/2212. train loss: 0.5491,	0.2081 s / batch. (data: 3.70e-02). ETA=12:13:48, max mem: 2.7 GB 
[11/06 21:12:29 visual_prompt]: 	Training 900/2212. train loss: 0.4044,	0.2223 s / batch. (data: 5.73e-03). ETA=13:03:29, max mem: 2.7 GB 
[11/06 21:12:50 visual_prompt]: 	Training 1000/2212. train loss: 0.8233,	0.2401 s / batch. (data: 6.44e-02). ETA=14:05:42, max mem: 2.7 GB 
[11/06 21:13:10 visual_prompt]: 	Training 1100/2212. train loss: 0.5330,	0.1646 s / batch. (data: 5.32e-03). ETA=9:39:38, max mem: 2.7 GB 
[11/06 21:13:31 visual_prompt]: 	Training 1200/2212. train loss: 0.9785,	0.1393 s / batch. (data: 1.54e-02). ETA=8:10:12, max mem: 2.7 GB 
[11/06 21:13:51 visual_prompt]: 	Training 1300/2212. train loss: 0.3873,	0.1734 s / batch. (data: 2.13e-04). ETA=10:10:00, max mem: 2.7 GB 
[11/06 21:14:12 visual_prompt]: 	Training 1400/2212. train loss: 0.6895,	0.1737 s / batch. (data: 1.55e-02). ETA=10:10:46, max mem: 2.7 GB 
[11/06 21:14:34 visual_prompt]: 	Training 1500/2212. train loss: 1.2190,	0.9975 s / batch. (data: 8.43e-01). ETA=2 days, 10:25:32, max mem: 2.7 GB 
[11/06 21:14:56 visual_prompt]: 	Training 1600/2212. train loss: 0.7691,	0.1640 s / batch. (data: 1.08e-02). ETA=9:35:56, max mem: 2.7 GB 
[11/06 21:15:17 visual_prompt]: 	Training 1700/2212. train loss: 0.6337,	0.2216 s / batch. (data: 3.27e-02). ETA=12:58:09, max mem: 2.7 GB 
[11/06 21:15:39 visual_prompt]: 	Training 1800/2212. train loss: 0.7414,	0.2216 s / batch. (data: 1.09e-02). ETA=12:57:45, max mem: 2.7 GB 
[11/06 21:16:00 visual_prompt]: 	Training 1900/2212. train loss: 0.7196,	0.1905 s / batch. (data: 5.32e-03). ETA=11:08:06, max mem: 2.7 GB 
[11/06 21:16:20 visual_prompt]: 	Training 2000/2212. train loss: 0.6091,	0.1801 s / batch. (data: 3.64e-03). ETA=10:31:24, max mem: 2.7 GB 
[11/06 21:16:40 visual_prompt]: 	Training 2100/2212. train loss: 0.6745,	0.1975 s / batch. (data: 1.73e-02). ETA=11:32:09, max mem: 2.7 GB 
[11/06 21:17:01 visual_prompt]: 	Training 2200/2212. train loss: 0.7071,	0.1094 s / batch. (data: 7.63e-05). ETA=6:23:04, max mem: 2.7 GB 
[11/06 21:17:02 visual_prompt]: Epoch 5 / 100: avg data time: 4.46e-02, avg batch time: 0.2102, average train loss: 0.7450
[11/06 21:17:23 visual_prompt]: 	Test 100/246. loss: 0.748, 0.0630 s / batch. (data: 2.41e-05)max mem: 2.72052 GB 
[11/06 21:17:42 visual_prompt]: 	Test 200/246. loss: 0.748, 0.0387 s / batch. (data: 3.27e-05)max mem: 2.72052 GB 
[11/06 21:17:50 visual_prompt]: Inference (val):avg data time: 2.36e-04, avg batch time: 0.0444, average loss: 0.6893
[11/06 21:17:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.04	
[11/06 21:17:50 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/06 21:18:13 visual_prompt]: 	Training 100/2212. train loss: 2.2765,	0.1782 s / batch. (data: 2.13e-04). ETA=10:23:49, max mem: 2.7 GB 
[11/06 21:18:35 visual_prompt]: 	Training 200/2212. train loss: 0.6401,	0.1736 s / batch. (data: 2.55e-02). ETA=10:07:17, max mem: 2.7 GB 
[11/06 21:18:56 visual_prompt]: 	Training 300/2212. train loss: 0.4012,	0.1729 s / batch. (data: 1.04e-02). ETA=10:04:32, max mem: 2.7 GB 
[11/06 21:19:16 visual_prompt]: 	Training 400/2212. train loss: 0.6135,	0.1731 s / batch. (data: 2.08e-02). ETA=10:05:00, max mem: 2.7 GB 
[11/06 21:19:38 visual_prompt]: 	Training 500/2212. train loss: 0.5928,	0.1593 s / batch. (data: 4.49e-03). ETA=9:16:32, max mem: 2.7 GB 
[11/06 21:19:59 visual_prompt]: 	Training 600/2212. train loss: 0.6834,	0.1782 s / batch. (data: 7.05e-03). ETA=10:22:09, max mem: 2.7 GB 
[11/06 21:20:20 visual_prompt]: 	Training 700/2212. train loss: 0.5909,	0.1826 s / batch. (data: 2.26e-04). ETA=10:37:32, max mem: 2.7 GB 
[11/06 21:20:41 visual_prompt]: 	Training 800/2212. train loss: 0.7881,	0.1857 s / batch. (data: 5.36e-03). ETA=10:47:54, max mem: 2.7 GB 
[11/06 21:21:02 visual_prompt]: 	Training 900/2212. train loss: 0.7856,	0.2041 s / batch. (data: 1.04e-02). ETA=11:51:56, max mem: 2.7 GB 
[11/06 21:21:24 visual_prompt]: 	Training 1000/2212. train loss: 0.7910,	0.1726 s / batch. (data: 5.32e-03). ETA=10:01:46, max mem: 2.7 GB 
[11/06 21:21:44 visual_prompt]: 	Training 1100/2212. train loss: 0.5982,	0.1832 s / batch. (data: 1.56e-02). ETA=10:38:18, max mem: 2.7 GB 
[11/06 21:22:04 visual_prompt]: 	Training 1200/2212. train loss: 0.6075,	0.1930 s / batch. (data: 1.55e-02). ETA=11:12:08, max mem: 2.7 GB 
[11/06 21:22:25 visual_prompt]: 	Training 1300/2212. train loss: 0.5702,	0.1840 s / batch. (data: 1.04e-02). ETA=10:40:29, max mem: 2.7 GB 
[11/06 21:22:46 visual_prompt]: 	Training 1400/2212. train loss: 0.4948,	0.2010 s / batch. (data: 1.64e-02). ETA=11:39:22, max mem: 2.7 GB 
[11/06 21:23:07 visual_prompt]: 	Training 1500/2212. train loss: 0.6284,	0.1658 s / batch. (data: 6.57e-04). ETA=9:36:26, max mem: 2.7 GB 
[11/06 21:23:28 visual_prompt]: 	Training 1600/2212. train loss: 0.7654,	0.1936 s / batch. (data: 1.55e-02). ETA=11:13:00, max mem: 2.7 GB 
[11/06 21:23:50 visual_prompt]: 	Training 1700/2212. train loss: 0.9046,	0.2059 s / batch. (data: 2.05e-02). ETA=11:55:16, max mem: 2.7 GB 
[11/06 21:24:12 visual_prompt]: 	Training 1800/2212. train loss: 0.6844,	0.1977 s / batch. (data: 5.72e-03). ETA=11:26:25, max mem: 2.7 GB 
[11/06 21:24:32 visual_prompt]: 	Training 1900/2212. train loss: 0.5398,	0.2059 s / batch. (data: 1.58e-02). ETA=11:54:38, max mem: 2.7 GB 
[11/06 21:24:52 visual_prompt]: 	Training 2000/2212. train loss: 0.9050,	0.1713 s / batch. (data: 4.89e-02). ETA=9:54:23, max mem: 2.7 GB 
[11/06 21:25:12 visual_prompt]: 	Training 2100/2212. train loss: 0.8501,	0.3464 s / batch. (data: 2.49e-01). ETA=20:01:13, max mem: 2.7 GB 
[11/06 21:25:33 visual_prompt]: 	Training 2200/2212. train loss: 0.6358,	0.0945 s / batch. (data: 1.18e-04). ETA=5:27:40, max mem: 2.7 GB 
[11/06 21:25:34 visual_prompt]: Epoch 6 / 100: avg data time: 4.55e-02, avg batch time: 0.2095, average train loss: 0.7096
[11/06 21:25:55 visual_prompt]: 	Test 100/246. loss: 0.761, 0.0783 s / batch. (data: 2.50e-05)max mem: 2.72052 GB 
[11/06 21:26:14 visual_prompt]: 	Test 200/246. loss: 0.761, 0.0485 s / batch. (data: 2.22e-05)max mem: 2.72052 GB 
[11/06 21:26:22 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.0432, average loss: 0.6889
[11/06 21:26:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.95	
[11/06 21:26:22 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/06 21:26:45 visual_prompt]: 	Training 100/2212. train loss: 0.4900,	0.2064 s / batch. (data: 2.56e-02). ETA=11:54:46, max mem: 2.7 GB 
[11/06 21:27:08 visual_prompt]: 	Training 200/2212. train loss: 0.3997,	0.1420 s / batch. (data: 2.19e-04). ETA=8:11:46, max mem: 2.7 GB 
[11/06 21:27:31 visual_prompt]: 	Training 300/2212. train loss: 0.8664,	0.1724 s / batch. (data: 2.09e-02). ETA=9:56:29, max mem: 2.7 GB 
[11/06 21:27:52 visual_prompt]: 	Training 400/2212. train loss: 0.8413,	0.1603 s / batch. (data: 6.87e-04). ETA=9:14:34, max mem: 2.7 GB 
[11/06 21:28:13 visual_prompt]: 	Training 500/2212. train loss: 0.7022,	0.1786 s / batch. (data: 1.25e-02). ETA=10:17:21, max mem: 2.7 GB 
[11/06 21:28:34 visual_prompt]: 	Training 600/2212. train loss: 0.6897,	0.1421 s / batch. (data: 6.64e-03). ETA=8:10:55, max mem: 2.7 GB 
[11/06 21:28:55 visual_prompt]: 	Training 700/2212. train loss: 0.6029,	0.2062 s / batch. (data: 2.20e-03). ETA=11:52:16, max mem: 2.7 GB 
[11/06 21:29:15 visual_prompt]: 	Training 800/2212. train loss: 0.6255,	0.1670 s / batch. (data: 1.86e-04). ETA=9:36:30, max mem: 2.7 GB 
[11/06 21:29:36 visual_prompt]: 	Training 900/2212. train loss: 0.7247,	0.1553 s / batch. (data: 5.31e-03). ETA=8:55:43, max mem: 2.7 GB 
[11/06 21:29:57 visual_prompt]: 	Training 1000/2212. train loss: 0.6855,	0.1566 s / batch. (data: 1.03e-02). ETA=9:00:07, max mem: 2.7 GB 
[11/06 21:30:17 visual_prompt]: 	Training 1100/2212. train loss: 0.7812,	0.1895 s / batch. (data: 2.12e-02). ETA=10:53:07, max mem: 2.7 GB 
[11/06 21:30:37 visual_prompt]: 	Training 1200/2212. train loss: 0.5800,	0.1061 s / batch. (data: 3.71e-03). ETA=6:05:26, max mem: 2.7 GB 
[11/06 21:30:58 visual_prompt]: 	Training 1300/2212. train loss: 0.6494,	0.1081 s / batch. (data: 1.99e-04). ETA=6:12:13, max mem: 2.7 GB 
[11/06 21:31:18 visual_prompt]: 	Training 1400/2212. train loss: 0.6007,	0.2083 s / batch. (data: 1.69e-02). ETA=11:57:01, max mem: 2.7 GB 
[11/06 21:31:39 visual_prompt]: 	Training 1500/2212. train loss: 0.9692,	0.1840 s / batch. (data: 1.04e-02). ETA=10:32:54, max mem: 2.7 GB 
[11/06 21:32:00 visual_prompt]: 	Training 1600/2212. train loss: 0.5862,	0.1639 s / batch. (data: 8.81e-03). ETA=9:23:34, max mem: 2.7 GB 
[11/06 21:32:21 visual_prompt]: 	Training 1700/2212. train loss: 0.9012,	0.1762 s / batch. (data: 2.78e-03). ETA=10:05:27, max mem: 2.7 GB 
[11/06 21:32:41 visual_prompt]: 	Training 1800/2212. train loss: 0.5723,	0.1827 s / batch. (data: 2.49e-04). ETA=10:27:44, max mem: 2.7 GB 
[11/06 21:33:02 visual_prompt]: 	Training 1900/2212. train loss: 0.8254,	0.2324 s / batch. (data: 2.45e-02). ETA=13:17:56, max mem: 2.7 GB 
[11/06 21:33:23 visual_prompt]: 	Training 2000/2212. train loss: 0.7260,	0.1705 s / batch. (data: 1.04e-02). ETA=9:45:17, max mem: 2.7 GB 
[11/06 21:33:44 visual_prompt]: 	Training 2100/2212. train loss: 0.9467,	0.1876 s / batch. (data: 1.04e-02). ETA=10:43:31, max mem: 2.7 GB 
[11/06 21:34:05 visual_prompt]: 	Training 2200/2212. train loss: 0.5506,	0.1355 s / batch. (data: 8.46e-05). ETA=7:44:30, max mem: 2.7 GB 
[11/06 21:34:06 visual_prompt]: Epoch 7 / 100: avg data time: 4.39e-02, avg batch time: 0.2096, average train loss: 0.7028
[11/06 21:34:27 visual_prompt]: 	Test 100/246. loss: 0.838, 0.0822 s / batch. (data: 2.34e-05)max mem: 2.72052 GB 
[11/06 21:34:47 visual_prompt]: 	Test 200/246. loss: 0.838, 0.0472 s / batch. (data: 2.00e-05)max mem: 2.72052 GB 
[11/06 21:34:54 visual_prompt]: Inference (val):avg data time: 4.08e-04, avg batch time: 0.0455, average loss: 0.6891
[11/06 21:34:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.06	
[11/06 21:34:54 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/06 21:35:18 visual_prompt]: 	Training 100/2212. train loss: 0.5098,	0.1797 s / batch. (data: 5.34e-03). ETA=10:15:47, max mem: 2.7 GB 
[11/06 21:35:39 visual_prompt]: 	Training 200/2212. train loss: 0.6621,	0.1237 s / batch. (data: 8.55e-03). ETA=7:03:52, max mem: 2.7 GB 
[11/06 21:36:00 visual_prompt]: 	Training 300/2212. train loss: 0.9167,	0.1580 s / batch. (data: 1.21e-02). ETA=9:00:55, max mem: 2.7 GB 
[11/06 21:36:20 visual_prompt]: 	Training 400/2212. train loss: 0.5255,	0.1953 s / batch. (data: 2.05e-02). ETA=11:08:09, max mem: 2.7 GB 
[11/06 21:36:43 visual_prompt]: 	Training 500/2212. train loss: 0.5267,	0.4135 s / batch. (data: 3.17e-01). ETA=23:34:23, max mem: 2.7 GB 
[11/06 21:37:04 visual_prompt]: 	Training 600/2212. train loss: 0.8026,	0.1816 s / batch. (data: 7.64e-04). ETA=10:20:39, max mem: 2.7 GB 
[11/06 21:37:24 visual_prompt]: 	Training 700/2212. train loss: 0.9116,	0.2254 s / batch. (data: 2.05e-02). ETA=12:50:17, max mem: 2.7 GB 
[11/06 21:37:45 visual_prompt]: 	Training 800/2212. train loss: 0.4800,	0.2077 s / batch. (data: 6.40e-04). ETA=11:49:14, max mem: 2.7 GB 
[11/06 21:38:06 visual_prompt]: 	Training 900/2212. train loss: 0.5827,	0.1781 s / batch. (data: 1.48e-02). ETA=10:08:05, max mem: 2.7 GB 
[11/06 21:38:27 visual_prompt]: 	Training 1000/2212. train loss: 0.9213,	0.1799 s / batch. (data: 5.72e-03). ETA=10:13:45, max mem: 2.7 GB 
[11/06 21:38:48 visual_prompt]: 	Training 1100/2212. train loss: 0.5811,	0.1636 s / batch. (data: 5.32e-03). ETA=9:17:54, max mem: 2.7 GB 
[11/06 21:39:09 visual_prompt]: 	Training 1200/2212. train loss: 0.6265,	0.2252 s / batch. (data: 2.56e-02). ETA=12:47:39, max mem: 2.7 GB 
[11/06 21:39:30 visual_prompt]: 	Training 1300/2212. train loss: 0.7631,	0.1989 s / batch. (data: 2.22e-04). ETA=11:17:33, max mem: 2.7 GB 
[11/06 21:39:51 visual_prompt]: 	Training 1400/2212. train loss: 0.7888,	0.1775 s / batch. (data: 2.16e-04). ETA=10:04:22, max mem: 2.7 GB 
[11/06 21:40:13 visual_prompt]: 	Training 1500/2212. train loss: 0.5911,	0.6082 s / batch. (data: 5.09e-01). ETA=1 day, 10:30:13, max mem: 2.7 GB 
[11/06 21:40:34 visual_prompt]: 	Training 1600/2212. train loss: 0.8908,	0.1600 s / batch. (data: 2.20e-04). ETA=9:04:21, max mem: 2.7 GB 
[11/06 21:40:55 visual_prompt]: 	Training 1700/2212. train loss: 0.7768,	0.1800 s / batch. (data: 9.65e-03). ETA=10:12:00, max mem: 2.7 GB 
[11/06 21:41:15 visual_prompt]: 	Training 1800/2212. train loss: 0.5397,	0.0845 s / batch. (data: 6.33e-04). ETA=4:47:08, max mem: 2.7 GB 
[11/06 21:41:36 visual_prompt]: 	Training 1900/2212. train loss: 0.6120,	0.2035 s / batch. (data: 1.59e-02). ETA=11:31:13, max mem: 2.7 GB 
[11/06 21:41:57 visual_prompt]: 	Training 2000/2212. train loss: 1.7032,	0.1862 s / batch. (data: 2.05e-02). ETA=10:32:02, max mem: 2.7 GB 
[11/06 21:42:18 visual_prompt]: 	Training 2100/2212. train loss: 0.5217,	0.1916 s / batch. (data: 1.42e-02). ETA=10:50:15, max mem: 2.7 GB 
[11/06 21:42:38 visual_prompt]: 	Training 2200/2212. train loss: 0.7590,	0.0865 s / batch. (data: 8.56e-05). ETA=4:53:25, max mem: 2.7 GB 
[11/06 21:42:38 visual_prompt]: Epoch 8 / 100: avg data time: 4.53e-02, avg batch time: 0.2096, average train loss: 0.7053
[11/06 21:42:59 visual_prompt]: 	Test 100/246. loss: 0.766, 0.0630 s / batch. (data: 1.74e-05)max mem: 2.72052 GB 
[11/06 21:43:18 visual_prompt]: 	Test 200/246. loss: 0.766, 0.0565 s / batch. (data: 2.29e-05)max mem: 2.72052 GB 
[11/06 21:43:26 visual_prompt]: Inference (val):avg data time: 1.58e-04, avg batch time: 0.0449, average loss: 0.6888
[11/06 21:43:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.01	
[11/06 21:43:26 visual_prompt]: Best epoch 8: best metric: -0.689
[11/06 21:43:26 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/06 21:43:50 visual_prompt]: 	Training 100/2212. train loss: 0.6648,	0.1393 s / batch. (data: 1.54e-02). ETA=7:52:05, max mem: 2.7 GB 
[11/06 21:44:13 visual_prompt]: 	Training 200/2212. train loss: 0.6465,	0.1774 s / batch. (data: 2.05e-02). ETA=10:01:10, max mem: 2.7 GB 
[11/06 21:44:35 visual_prompt]: 	Training 300/2212. train loss: 0.8666,	0.1796 s / batch. (data: 6.96e-04). ETA=10:08:07, max mem: 2.7 GB 
[11/06 21:44:56 visual_prompt]: 	Training 400/2212. train loss: 0.7803,	0.2110 s / batch. (data: 5.80e-03). ETA=11:54:07, max mem: 2.7 GB 
[11/06 21:45:18 visual_prompt]: 	Training 500/2212. train loss: 0.7996,	0.1045 s / batch. (data: 5.29e-03). ETA=5:53:40, max mem: 2.7 GB 
[11/06 21:45:39 visual_prompt]: 	Training 600/2212. train loss: 0.5202,	0.1713 s / batch. (data: 1.60e-02). ETA=9:39:13, max mem: 2.7 GB 
[11/06 21:46:00 visual_prompt]: 	Training 700/2212. train loss: 0.3972,	0.1595 s / batch. (data: 1.04e-02). ETA=8:59:08, max mem: 2.7 GB 
[11/06 21:46:19 visual_prompt]: 	Training 800/2212. train loss: 0.7254,	0.1883 s / batch. (data: 2.18e-04). ETA=10:36:06, max mem: 2.7 GB 
[11/06 21:46:40 visual_prompt]: 	Training 900/2212. train loss: 0.6554,	0.2031 s / batch. (data: 2.05e-02). ETA=11:25:43, max mem: 2.7 GB 
[11/06 21:47:01 visual_prompt]: 	Training 1000/2212. train loss: 0.8025,	0.2062 s / batch. (data: 4.94e-02). ETA=11:36:03, max mem: 2.7 GB 
[11/06 21:47:23 visual_prompt]: 	Training 1100/2212. train loss: 0.5062,	0.8678 s / batch. (data: 7.08e-01). ETA=2 days, 0:47:31, max mem: 2.7 GB 
[11/06 21:47:43 visual_prompt]: 	Training 1200/2212. train loss: 1.0264,	0.1794 s / batch. (data: 5.32e-03). ETA=10:04:48, max mem: 2.7 GB 
[11/06 21:48:02 visual_prompt]: 	Training 1300/2212. train loss: 0.7627,	0.2037 s / batch. (data: 1.59e-02). ETA=11:26:29, max mem: 2.7 GB 
[11/06 21:48:22 visual_prompt]: 	Training 1400/2212. train loss: 0.7527,	0.1751 s / batch. (data: 2.91e-04). ETA=9:49:49, max mem: 2.7 GB 
[11/06 21:48:43 visual_prompt]: 	Training 1500/2212. train loss: 0.9318,	0.1878 s / batch. (data: 1.54e-02). ETA=10:32:22, max mem: 2.7 GB 
[11/06 21:49:04 visual_prompt]: 	Training 1600/2212. train loss: 0.6495,	0.1692 s / batch. (data: 1.55e-02). ETA=9:29:24, max mem: 2.7 GB 
[11/06 21:49:25 visual_prompt]: 	Training 1700/2212. train loss: 1.4395,	0.1951 s / batch. (data: 5.34e-03). ETA=10:56:14, max mem: 2.7 GB 
[11/06 21:49:45 visual_prompt]: 	Training 1800/2212. train loss: 0.7318,	0.7499 s / batch. (data: 6.44e-01). ETA=1 day, 18:01:03, max mem: 2.7 GB 
[11/06 21:50:03 visual_prompt]: 	Training 1900/2212. train loss: 0.7979,	0.1630 s / batch. (data: 8.33e-03). ETA=9:07:37, max mem: 2.7 GB 
[11/06 21:50:25 visual_prompt]: 	Training 2000/2212. train loss: 0.8055,	0.1024 s / batch. (data: 1.04e-02). ETA=5:43:50, max mem: 2.7 GB 
[11/06 21:50:46 visual_prompt]: 	Training 2100/2212. train loss: 0.8031,	0.1923 s / batch. (data: 1.16e-02). ETA=10:45:35, max mem: 2.7 GB 
[11/06 21:51:09 visual_prompt]: 	Training 2200/2212. train loss: 0.5406,	0.1859 s / batch. (data: 1.08e-04). ETA=10:23:35, max mem: 2.7 GB 
[11/06 21:51:10 visual_prompt]: Epoch 9 / 100: avg data time: 4.71e-02, avg batch time: 0.2096, average train loss: 0.7082
[11/06 21:51:33 visual_prompt]: 	Test 100/246. loss: 0.927, 0.0929 s / batch. (data: 2.24e-05)max mem: 2.72052 GB 
[11/06 21:51:52 visual_prompt]: 	Test 200/246. loss: 0.927, 0.0644 s / batch. (data: 2.07e-05)max mem: 2.72052 GB 
[11/06 21:52:00 visual_prompt]: Inference (val):avg data time: 2.52e-04, avg batch time: 0.0442, average loss: 0.6947
[11/06 21:52:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.05	
[11/06 21:52:00 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/06 21:52:24 visual_prompt]: 	Training 100/2212. train loss: 0.5736,	0.4242 s / batch. (data: 3.02e-01). ETA=23:42:20, max mem: 2.7 GB 
[11/06 21:52:45 visual_prompt]: 	Training 200/2212. train loss: 0.8301,	0.6075 s / batch. (data: 4.70e-01). ETA=1 day, 9:56:00, max mem: 2.7 GB 
[11/06 21:53:06 visual_prompt]: 	Training 300/2212. train loss: 0.9691,	0.1852 s / batch. (data: 2.26e-04). ETA=10:20:29, max mem: 2.7 GB 
[11/06 21:53:26 visual_prompt]: 	Training 400/2212. train loss: 0.7929,	0.1399 s / batch. (data: 5.30e-03). ETA=7:48:27, max mem: 2.7 GB 
[11/06 21:53:46 visual_prompt]: 	Training 500/2212. train loss: 0.4732,	0.1984 s / batch. (data: 3.36e-02). ETA=11:03:53, max mem: 2.7 GB 
[11/06 21:54:08 visual_prompt]: 	Training 600/2212. train loss: 0.9828,	0.1955 s / batch. (data: 6.04e-03). ETA=10:53:56, max mem: 2.7 GB 
[11/06 21:54:30 visual_prompt]: 	Training 700/2212. train loss: 0.5239,	0.8627 s / batch. (data: 6.93e-01). ETA=2 days, 0:04:10, max mem: 2.7 GB 
[11/06 21:54:51 visual_prompt]: 	Training 800/2212. train loss: 0.7443,	0.2242 s / batch. (data: 1.08e-02). ETA=12:29:00, max mem: 2.7 GB 
[11/06 21:55:12 visual_prompt]: 	Training 900/2212. train loss: 0.2362,	1.1213 s / batch. (data: 1.01e+00). ETA=2 days, 14:25:02, max mem: 2.7 GB 
[11/06 21:55:33 visual_prompt]: 	Training 1000/2212. train loss: 0.6700,	0.0760 s / batch. (data: 2.04e-04). ETA=4:13:39, max mem: 2.7 GB 
[11/06 21:55:54 visual_prompt]: 	Training 1100/2212. train loss: 0.7570,	0.2192 s / batch. (data: 2.05e-02). ETA=12:11:21, max mem: 2.7 GB 
[11/06 21:56:14 visual_prompt]: 	Training 1200/2212. train loss: 0.7626,	0.2040 s / batch. (data: 1.08e-02). ETA=11:20:15, max mem: 2.7 GB 
[11/06 21:56:35 visual_prompt]: 	Training 1300/2212. train loss: 0.7817,	0.1830 s / batch. (data: 3.09e-02). ETA=10:09:52, max mem: 2.7 GB 
[11/06 21:56:56 visual_prompt]: 	Training 1400/2212. train loss: 1.9742,	0.0884 s / batch. (data: 1.79e-04). ETA=4:54:30, max mem: 2.7 GB 
[11/06 21:57:16 visual_prompt]: 	Training 1500/2212. train loss: 0.6086,	0.1761 s / batch. (data: 1.62e-02). ETA=9:46:25, max mem: 2.7 GB 
[11/06 21:57:37 visual_prompt]: 	Training 1600/2212. train loss: 0.6166,	0.1870 s / batch. (data: 2.94e-02). ETA=10:22:32, max mem: 2.7 GB 
[11/06 21:57:58 visual_prompt]: 	Training 1700/2212. train loss: 0.5784,	0.1383 s / batch. (data: 5.33e-03). ETA=7:39:55, max mem: 2.7 GB 
[11/06 21:58:20 visual_prompt]: 	Training 1800/2212. train loss: 0.9025,	0.1648 s / batch. (data: 1.55e-02). ETA=9:07:47, max mem: 2.7 GB 
[11/06 21:58:40 visual_prompt]: 	Training 1900/2212. train loss: 0.6273,	0.1938 s / batch. (data: 2.55e-04). ETA=10:43:52, max mem: 2.7 GB 
[11/06 21:59:00 visual_prompt]: 	Training 2000/2212. train loss: 0.6165,	0.2273 s / batch. (data: 2.55e-02). ETA=12:35:07, max mem: 2.7 GB 
[11/06 21:59:22 visual_prompt]: 	Training 2100/2212. train loss: 0.7117,	0.1626 s / batch. (data: 2.56e-02). ETA=8:59:46, max mem: 2.7 GB 
[11/06 21:59:42 visual_prompt]: 	Training 2200/2212. train loss: 0.6600,	0.2244 s / batch. (data: 9.47e-02). ETA=12:24:45, max mem: 2.7 GB 
[11/06 21:59:43 visual_prompt]: Epoch 10 / 100: avg data time: 4.67e-02, avg batch time: 0.2095, average train loss: 0.7021
[11/06 22:00:04 visual_prompt]: 	Test 100/246. loss: 0.641, 0.0462 s / batch. (data: 2.48e-05)max mem: 2.72052 GB 
[11/06 22:00:24 visual_prompt]: 	Test 200/246. loss: 0.641, 0.0318 s / batch. (data: 3.67e-05)max mem: 2.72052 GB 
[11/06 22:00:32 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.0452, average loss: 0.6998
[11/06 22:00:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.95	
[11/06 22:00:32 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/06 22:00:56 visual_prompt]: 	Training 100/2212. train loss: 0.5598,	0.1590 s / batch. (data: 2.52e-04). ETA=8:47:12, max mem: 2.7 GB 
[11/06 22:01:17 visual_prompt]: 	Training 200/2212. train loss: 0.6380,	0.2053 s / batch. (data: 2.13e-04). ETA=11:20:31, max mem: 2.7 GB 
[11/06 22:01:39 visual_prompt]: 	Training 300/2212. train loss: 0.8742,	0.1205 s / batch. (data: 5.34e-03). ETA=6:39:10, max mem: 2.7 GB 
[11/06 22:02:01 visual_prompt]: 	Training 400/2212. train loss: 0.8680,	0.1901 s / batch. (data: 5.78e-03). ETA=10:29:23, max mem: 2.7 GB 
[11/06 22:02:22 visual_prompt]: 	Training 500/2212. train loss: 0.6961,	0.1409 s / batch. (data: 5.70e-04). ETA=7:46:27, max mem: 2.7 GB 
[11/06 22:02:41 visual_prompt]: 	Training 600/2212. train loss: 0.7149,	0.0530 s / batch. (data: 4.03e-04). ETA=2:55:16, max mem: 2.7 GB 
[11/06 22:03:02 visual_prompt]: 	Training 700/2212. train loss: 1.0688,	0.8879 s / batch. (data: 7.78e-01). ETA=2 days, 0:55:36, max mem: 2.7 GB 
[11/06 22:03:19 visual_prompt]: 	Training 800/2212. train loss: 0.6716,	0.1781 s / batch. (data: 5.35e-03). ETA=9:48:30, max mem: 2.7 GB 
[11/06 22:03:39 visual_prompt]: 	Training 900/2212. train loss: 0.9117,	0.1772 s / batch. (data: 3.73e-02). ETA=9:45:19, max mem: 2.7 GB 
[11/06 22:03:58 visual_prompt]: 	Training 1000/2212. train loss: 0.6506,	0.1277 s / batch. (data: 2.08e-04). ETA=7:01:29, max mem: 2.7 GB 
[11/06 22:04:19 visual_prompt]: 	Training 1100/2212. train loss: 0.5555,	0.1739 s / batch. (data: 1.55e-02). ETA=9:33:41, max mem: 2.7 GB 
[11/06 22:04:37 visual_prompt]: 	Training 1200/2212. train loss: 0.7581,	0.1543 s / batch. (data: 2.15e-04). ETA=8:28:46, max mem: 2.7 GB 
[11/06 22:04:58 visual_prompt]: 	Training 1300/2212. train loss: 0.6278,	0.1810 s / batch. (data: 5.26e-04). ETA=9:56:44, max mem: 2.7 GB 
[11/06 22:05:19 visual_prompt]: 	Training 1400/2212. train loss: 2.1061,	0.1328 s / batch. (data: 1.20e-02). ETA=7:17:22, max mem: 2.7 GB 
[11/06 22:05:39 visual_prompt]: 	Training 1500/2212. train loss: 0.3091,	0.1707 s / batch. (data: 5.32e-03). ETA=9:22:09, max mem: 2.7 GB 
[11/06 22:05:59 visual_prompt]: 	Training 1600/2212. train loss: 1.0879,	0.1678 s / batch. (data: 1.54e-02). ETA=9:12:11, max mem: 2.7 GB 
[11/06 22:06:19 visual_prompt]: 	Training 1700/2212. train loss: 0.4020,	0.1776 s / batch. (data: 2.05e-02). ETA=9:44:11, max mem: 2.7 GB 
[11/06 22:06:39 visual_prompt]: 	Training 1800/2212. train loss: 0.4553,	0.1883 s / batch. (data: 1.04e-02). ETA=10:19:05, max mem: 2.7 GB 
[11/06 22:07:00 visual_prompt]: 	Training 1900/2212. train loss: 0.5299,	0.1881 s / batch. (data: 1.54e-02). ETA=10:18:06, max mem: 2.7 GB 
[11/06 22:07:20 visual_prompt]: 	Training 2000/2212. train loss: 0.7184,	0.1658 s / batch. (data: 1.54e-02). ETA=9:04:29, max mem: 2.7 GB 
[11/06 22:07:40 visual_prompt]: 	Training 2100/2212. train loss: 0.4503,	0.3611 s / batch. (data: 2.02e-01). ETA=19:45:35, max mem: 2.7 GB 
[11/06 22:07:59 visual_prompt]: 	Training 2200/2212. train loss: 0.8330,	0.1106 s / batch. (data: 7.37e-05). ETA=6:02:56, max mem: 2.7 GB 
[11/06 22:08:00 visual_prompt]: Epoch 11 / 100: avg data time: 4.50e-02, avg batch time: 0.2026, average train loss: 0.7126
[11/06 22:08:20 visual_prompt]: 	Test 100/246. loss: 0.767, 0.0534 s / batch. (data: 2.10e-05)max mem: 2.72052 GB 
[11/06 22:08:38 visual_prompt]: 	Test 200/246. loss: 0.767, 0.0730 s / batch. (data: 2.46e-05)max mem: 2.72052 GB 
[11/06 22:08:45 visual_prompt]: Inference (val):avg data time: 6.28e-04, avg batch time: 0.0397, average loss: 0.6887
[11/06 22:08:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[11/06 22:08:45 visual_prompt]: Best epoch 11: best metric: -0.689
[11/06 22:08:45 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/06 22:09:06 visual_prompt]: 	Training 100/2212. train loss: 0.5227,	0.1644 s / batch. (data: 1.53e-02). ETA=8:59:11, max mem: 2.7 GB 
[11/06 22:09:25 visual_prompt]: 	Training 200/2212. train loss: 0.2015,	0.1481 s / batch. (data: 6.12e-04). ETA=8:05:23, max mem: 2.7 GB 
[11/06 22:09:45 visual_prompt]: 	Training 300/2212. train loss: 0.7183,	0.1517 s / batch. (data: 1.90e-04). ETA=8:16:55, max mem: 2.7 GB 
[11/06 22:10:05 visual_prompt]: 	Training 400/2212. train loss: 0.3812,	0.1819 s / batch. (data: 1.04e-02). ETA=9:55:32, max mem: 2.7 GB 
[11/06 22:10:27 visual_prompt]: 	Training 500/2212. train loss: 0.6161,	0.2528 s / batch. (data: 1.60e-02). ETA=13:47:30, max mem: 2.7 GB 
[11/06 22:10:48 visual_prompt]: 	Training 600/2212. train loss: 0.8490,	0.1666 s / batch. (data: 1.49e-02). ETA=9:04:53, max mem: 2.7 GB 
[11/06 22:11:09 visual_prompt]: 	Training 700/2212. train loss: 0.7026,	0.1928 s / batch. (data: 1.58e-02). ETA=10:30:13, max mem: 2.7 GB 
[11/06 22:11:30 visual_prompt]: 	Training 800/2212. train loss: 1.1023,	0.1863 s / batch. (data: 2.50e-02). ETA=10:08:39, max mem: 2.7 GB 
[11/06 22:11:52 visual_prompt]: 	Training 900/2212. train loss: 0.7598,	0.6292 s / batch. (data: 5.15e-01). ETA=1 day, 10:15:10, max mem: 2.7 GB 
[11/06 22:12:12 visual_prompt]: 	Training 1000/2212. train loss: 0.5182,	0.1659 s / batch. (data: 5.78e-04). ETA=9:01:35, max mem: 2.7 GB 
[11/06 22:12:32 visual_prompt]: 	Training 1100/2212. train loss: 0.7913,	0.1803 s / batch. (data: 1.28e-02). ETA=9:48:18, max mem: 2.7 GB 
[11/06 22:12:53 visual_prompt]: 	Training 1200/2212. train loss: 1.1663,	0.2212 s / batch. (data: 6.98e-04). ETA=12:01:27, max mem: 2.7 GB 
[11/06 22:13:13 visual_prompt]: 	Training 1300/2212. train loss: 0.4215,	0.2187 s / batch. (data: 1.69e-02). ETA=11:52:51, max mem: 2.7 GB 
[11/06 22:13:35 visual_prompt]: 	Training 1400/2212. train loss: 0.5854,	0.1690 s / batch. (data: 5.32e-03). ETA=9:10:36, max mem: 2.7 GB 
[11/06 22:13:57 visual_prompt]: 	Training 1500/2212. train loss: 0.4297,	0.5770 s / batch. (data: 4.52e-01). ETA=1 day, 7:18:44, max mem: 2.7 GB 
[11/06 22:14:17 visual_prompt]: 	Training 1600/2212. train loss: 0.5904,	0.1948 s / batch. (data: 2.05e-02). ETA=10:33:56, max mem: 2.7 GB 
[11/06 22:14:38 visual_prompt]: 	Training 1700/2212. train loss: 0.8420,	0.1623 s / batch. (data: 5.38e-03). ETA=8:47:52, max mem: 2.7 GB 
[11/06 22:15:00 visual_prompt]: 	Training 1800/2212. train loss: 0.8315,	0.1642 s / batch. (data: 2.43e-02). ETA=8:53:43, max mem: 2.7 GB 
[11/06 22:15:20 visual_prompt]: 	Training 1900/2212. train loss: 0.6329,	0.1506 s / batch. (data: 6.89e-04). ETA=8:09:20, max mem: 2.7 GB 
[11/06 22:15:41 visual_prompt]: 	Training 2000/2212. train loss: 0.9280,	0.1315 s / batch. (data: 6.48e-04). ETA=7:06:58, max mem: 2.7 GB 
[11/06 22:16:03 visual_prompt]: 	Training 2100/2212. train loss: 0.7737,	0.7595 s / batch. (data: 6.73e-01). ETA=1 day, 17:05:34, max mem: 2.7 GB 
[11/06 22:16:24 visual_prompt]: 	Training 2200/2212. train loss: 0.2947,	0.0984 s / batch. (data: 1.04e-04). ETA=5:19:18, max mem: 2.7 GB 
[11/06 22:16:25 visual_prompt]: Epoch 12 / 100: avg data time: 4.38e-02, avg batch time: 0.2078, average train loss: 0.7091
[11/06 22:16:47 visual_prompt]: 	Test 100/246. loss: 0.608, 0.0399 s / batch. (data: 2.03e-05)max mem: 2.72052 GB 
[11/06 22:17:07 visual_prompt]: 	Test 200/246. loss: 0.608, 0.0596 s / batch. (data: 3.00e-05)max mem: 2.72052 GB 
[11/06 22:17:15 visual_prompt]: Inference (val):avg data time: 1.86e-04, avg batch time: 0.0411, average loss: 0.7057
[11/06 22:17:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.49	
[11/06 22:17:15 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/06 22:17:40 visual_prompt]: 	Training 100/2212. train loss: 0.6781,	0.1262 s / batch. (data: 1.68e-02). ETA=6:49:14, max mem: 2.7 GB 
[11/06 22:18:01 visual_prompt]: 	Training 200/2212. train loss: 0.5952,	0.2339 s / batch. (data: 7.26e-04). ETA=12:37:59, max mem: 2.7 GB 
[11/06 22:18:22 visual_prompt]: 	Training 300/2212. train loss: 0.6473,	0.0823 s / batch. (data: 1.53e-04). ETA=4:26:30, max mem: 2.7 GB 
[11/06 22:18:42 visual_prompt]: 	Training 400/2212. train loss: 1.0364,	0.1699 s / batch. (data: 5.73e-03). ETA=9:10:09, max mem: 2.7 GB 
[11/06 22:19:02 visual_prompt]: 	Training 500/2212. train loss: 0.6946,	0.2133 s / batch. (data: 1.87e-02). ETA=11:30:22, max mem: 2.7 GB 
[11/06 22:19:24 visual_prompt]: 	Training 600/2212. train loss: 0.5273,	0.1851 s / batch. (data: 7.05e-03). ETA=9:58:36, max mem: 2.7 GB 
[11/06 22:19:47 visual_prompt]: 	Training 700/2212. train loss: 0.8498,	0.1957 s / batch. (data: 1.59e-02). ETA=10:32:32, max mem: 2.7 GB 
[11/06 22:20:09 visual_prompt]: 	Training 800/2212. train loss: 0.7060,	0.1280 s / batch. (data: 1.04e-02). ETA=6:53:34, max mem: 2.7 GB 
[11/06 22:20:29 visual_prompt]: 	Training 900/2212. train loss: 0.5326,	0.1796 s / batch. (data: 1.04e-02). ETA=9:39:59, max mem: 2.7 GB 
[11/06 22:20:50 visual_prompt]: 	Training 1000/2212. train loss: 0.5190,	0.1850 s / batch. (data: 2.35e-02). ETA=9:56:59, max mem: 2.7 GB 
[11/06 22:21:12 visual_prompt]: 	Training 1100/2212. train loss: 0.7932,	0.1880 s / batch. (data: 1.09e-02). ETA=10:06:30, max mem: 2.7 GB 
[11/06 22:21:34 visual_prompt]: 	Training 1200/2212. train loss: 0.6742,	0.2003 s / batch. (data: 1.63e-02). ETA=10:45:43, max mem: 2.7 GB 
[11/06 22:21:55 visual_prompt]: 	Training 1300/2212. train loss: 0.9948,	0.1392 s / batch. (data: 1.60e-02). ETA=7:28:28, max mem: 2.7 GB 
[11/06 22:22:16 visual_prompt]: 	Training 1400/2212. train loss: 0.4940,	0.1630 s / batch. (data: 2.71e-04). ETA=8:44:59, max mem: 2.7 GB 
[11/06 22:22:38 visual_prompt]: 	Training 1500/2212. train loss: 0.4261,	0.1915 s / batch. (data: 5.82e-03). ETA=10:16:24, max mem: 2.7 GB 
[11/06 22:22:59 visual_prompt]: 	Training 1600/2212. train loss: 0.6757,	0.1993 s / batch. (data: 1.55e-02). ETA=10:41:23, max mem: 2.7 GB 
[11/06 22:23:20 visual_prompt]: 	Training 1700/2212. train loss: 0.6922,	0.2277 s / batch. (data: 1.09e-02). ETA=12:12:19, max mem: 2.7 GB 
[11/06 22:23:41 visual_prompt]: 	Training 1800/2212. train loss: 1.0629,	0.2216 s / batch. (data: 2.06e-02). ETA=11:52:19, max mem: 2.7 GB 
[11/06 22:24:03 visual_prompt]: 	Training 1900/2212. train loss: 0.6008,	0.1981 s / batch. (data: 2.70e-04). ETA=10:36:21, max mem: 2.7 GB 
[11/06 22:24:24 visual_prompt]: 	Training 2000/2212. train loss: 0.5681,	0.2007 s / batch. (data: 2.49e-04). ETA=10:44:28, max mem: 2.7 GB 
[11/06 22:24:45 visual_prompt]: 	Training 2100/2212. train loss: 0.7812,	0.1912 s / batch. (data: 1.04e-02). ETA=10:13:28, max mem: 2.7 GB 
[11/06 22:25:06 visual_prompt]: 	Training 2200/2212. train loss: 0.6901,	0.1599 s / batch. (data: 1.42e-04). ETA=8:32:59, max mem: 2.7 GB 
[11/06 22:25:08 visual_prompt]: Epoch 13 / 100: avg data time: 4.53e-02, avg batch time: 0.2137, average train loss: 0.7063
[11/06 22:25:30 visual_prompt]: 	Test 100/246. loss: 0.841, 0.0994 s / batch. (data: 2.10e-05)max mem: 2.72052 GB 
[11/06 22:25:50 visual_prompt]: 	Test 200/246. loss: 0.841, 0.0788 s / batch. (data: 4.20e-05)max mem: 2.72052 GB 
[11/06 22:25:58 visual_prompt]: Inference (val):avg data time: 2.11e-04, avg batch time: 0.0649, average loss: 0.6892
[11/06 22:25:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.50	
[11/06 22:25:58 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/06 22:26:22 visual_prompt]: 	Training 100/2212. train loss: 0.9179,	0.1917 s / batch. (data: 2.85e-04). ETA=10:14:22, max mem: 2.7 GB 
[11/06 22:26:45 visual_prompt]: 	Training 200/2212. train loss: 0.7964,	0.1721 s / batch. (data: 1.04e-02). ETA=9:11:22, max mem: 2.7 GB 
[11/06 22:27:08 visual_prompt]: 	Training 300/2212. train loss: 0.6403,	0.1383 s / batch. (data: 1.04e-02). ETA=7:22:44, max mem: 2.7 GB 
[11/06 22:27:29 visual_prompt]: 	Training 400/2212. train loss: 0.7959,	0.1125 s / batch. (data: 1.61e-02). ETA=6:00:11, max mem: 2.7 GB 
[11/06 22:27:51 visual_prompt]: 	Training 500/2212. train loss: 0.8063,	0.2157 s / batch. (data: 1.89e-02). ETA=11:29:58, max mem: 2.7 GB 
[11/06 22:28:14 visual_prompt]: 	Training 600/2212. train loss: 0.4153,	0.2042 s / batch. (data: 5.24e-04). ETA=10:52:48, max mem: 2.7 GB 
[11/06 22:28:36 visual_prompt]: 	Training 700/2212. train loss: 0.7547,	0.1941 s / batch. (data: 1.05e-02). ETA=10:20:09, max mem: 2.7 GB 
[11/06 22:28:58 visual_prompt]: 	Training 800/2212. train loss: 0.6737,	0.2103 s / batch. (data: 1.55e-02). ETA=11:11:35, max mem: 2.7 GB 
[11/06 22:29:19 visual_prompt]: 	Training 900/2212. train loss: 0.7794,	0.2045 s / batch. (data: 6.63e-04). ETA=10:52:43, max mem: 2.7 GB 
[11/06 22:29:40 visual_prompt]: 	Training 1000/2212. train loss: 0.5906,	0.2465 s / batch. (data: 1.09e-02). ETA=13:06:36, max mem: 2.7 GB 
[11/06 22:30:01 visual_prompt]: 	Training 1100/2212. train loss: 1.3446,	0.2361 s / batch. (data: 3.21e-02). ETA=12:32:50, max mem: 2.7 GB 
[11/06 22:30:23 visual_prompt]: 	Training 1200/2212. train loss: 1.1575,	0.2720 s / batch. (data: 1.43e-01). ETA=14:26:57, max mem: 2.7 GB 
[11/06 22:30:43 visual_prompt]: 	Training 1300/2212. train loss: 0.4896,	0.1574 s / batch. (data: 2.69e-04). ETA=8:21:28, max mem: 2.7 GB 
[11/06 22:31:05 visual_prompt]: 	Training 1400/2212. train loss: 1.2204,	0.2483 s / batch. (data: 1.40e-02). ETA=13:10:41, max mem: 2.7 GB 
[11/06 22:31:27 visual_prompt]: 	Training 1500/2212. train loss: 0.5909,	0.2312 s / batch. (data: 5.36e-03). ETA=12:15:48, max mem: 2.7 GB 
[11/06 22:31:49 visual_prompt]: 	Training 1600/2212. train loss: 0.3325,	0.1950 s / batch. (data: 2.06e-02). ETA=10:20:17, max mem: 2.7 GB 
[11/06 22:32:10 visual_prompt]: 	Training 1700/2212. train loss: 0.3346,	0.2408 s / batch. (data: 6.66e-04). ETA=12:45:35, max mem: 2.7 GB 
[11/06 22:32:32 visual_prompt]: 	Training 1800/2212. train loss: 0.9730,	0.2340 s / batch. (data: 6.84e-04). ETA=12:23:37, max mem: 2.7 GB 
[11/06 22:32:53 visual_prompt]: 	Training 1900/2212. train loss: 0.8770,	0.2170 s / batch. (data: 7.01e-04). ETA=11:29:15, max mem: 2.7 GB 
[11/06 22:33:14 visual_prompt]: 	Training 2000/2212. train loss: 0.6216,	0.2014 s / batch. (data: 1.55e-02). ETA=10:39:12, max mem: 2.7 GB 
[11/06 22:33:36 visual_prompt]: 	Training 2100/2212. train loss: 0.7338,	0.2357 s / batch. (data: 2.17e-02). ETA=12:27:44, max mem: 2.7 GB 
[11/06 22:33:57 visual_prompt]: 	Training 2200/2212. train loss: 0.9485,	0.1680 s / batch. (data: 1.58e-04). ETA=8:52:33, max mem: 2.7 GB 
[11/06 22:33:58 visual_prompt]: Epoch 14 / 100: avg data time: 2.99e-02, avg batch time: 0.2169, average train loss: 0.7135
[11/06 22:34:20 visual_prompt]: 	Test 100/246. loss: 0.796, 0.0915 s / batch. (data: 2.74e-05)max mem: 2.72052 GB 
[11/06 22:34:39 visual_prompt]: 	Test 200/246. loss: 0.796, 0.0841 s / batch. (data: 3.89e-05)max mem: 2.72052 GB 
[11/06 22:34:48 visual_prompt]: Inference (val):avg data time: 1.29e-04, avg batch time: 0.0649, average loss: 0.6884
[11/06 22:34:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.01	
[11/06 22:34:48 visual_prompt]: Best epoch 14: best metric: -0.688
[11/06 22:34:48 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/06 22:35:13 visual_prompt]: 	Training 100/2212. train loss: 0.9827,	0.8469 s / batch. (data: 7.30e-01). ETA=1 day, 20:43:44, max mem: 2.7 GB 
[11/06 22:35:35 visual_prompt]: 	Training 200/2212. train loss: 0.9674,	0.8680 s / batch. (data: 7.32e-01). ETA=1 day, 21:49:07, max mem: 2.7 GB 
[11/06 22:35:57 visual_prompt]: 	Training 300/2212. train loss: 0.4364,	0.2064 s / batch. (data: 1.36e-02). ETA=10:53:30, max mem: 2.7 GB 
[11/06 22:36:19 visual_prompt]: 	Training 400/2212. train loss: 0.8346,	0.2022 s / batch. (data: 1.69e-02). ETA=10:39:45, max mem: 2.7 GB 
[11/06 22:36:42 visual_prompt]: 	Training 500/2212. train loss: 0.6326,	0.1783 s / batch. (data: 1.55e-02). ETA=9:23:57, max mem: 2.7 GB 
[11/06 22:37:04 visual_prompt]: 	Training 600/2212. train loss: 0.7905,	0.1810 s / batch. (data: 1.64e-02). ETA=9:32:07, max mem: 2.7 GB 
[11/06 22:37:26 visual_prompt]: 	Training 700/2212. train loss: 0.8404,	0.2643 s / batch. (data: 1.02e-01). ETA=13:54:59, max mem: 2.7 GB 
[11/06 22:37:46 visual_prompt]: 	Training 800/2212. train loss: 0.7943,	0.2275 s / batch. (data: 1.59e-02). ETA=11:58:07, max mem: 2.7 GB 
[11/06 22:38:08 visual_prompt]: 	Training 900/2212. train loss: 0.7568,	0.2102 s / batch. (data: 1.30e-02). ETA=11:03:10, max mem: 2.7 GB 
[11/06 22:38:29 visual_prompt]: 	Training 1000/2212. train loss: 0.7058,	0.1680 s / batch. (data: 6.78e-04). ETA=8:49:57, max mem: 2.7 GB 
[11/06 22:38:51 visual_prompt]: 	Training 1100/2212. train loss: 0.6971,	0.1765 s / batch. (data: 1.55e-02). ETA=9:16:17, max mem: 2.7 GB 
[11/06 22:39:12 visual_prompt]: 	Training 1200/2212. train loss: 0.7374,	0.2105 s / batch. (data: 1.05e-02). ETA=11:03:07, max mem: 2.7 GB 
[11/06 22:39:34 visual_prompt]: 	Training 1300/2212. train loss: 0.5946,	0.2015 s / batch. (data: 5.37e-03). ETA=10:34:28, max mem: 2.7 GB 
[11/06 22:39:55 visual_prompt]: 	Training 1400/2212. train loss: 0.7423,	0.1939 s / batch. (data: 2.49e-02). ETA=10:10:14, max mem: 2.7 GB 
[11/06 22:40:17 visual_prompt]: 	Training 1500/2212. train loss: 0.7588,	0.2662 s / batch. (data: 9.82e-02). ETA=13:57:13, max mem: 2.7 GB 
[11/06 22:40:38 visual_prompt]: 	Training 1600/2212. train loss: 0.4084,	0.2273 s / batch. (data: 3.58e-02). ETA=11:54:36, max mem: 2.7 GB 
[11/06 22:40:59 visual_prompt]: 	Training 1700/2212. train loss: 0.9098,	0.2129 s / batch. (data: 1.08e-02). ETA=11:09:02, max mem: 2.7 GB 
[11/06 22:41:19 visual_prompt]: 	Training 1800/2212. train loss: 0.1303,	0.2133 s / batch. (data: 1.55e-02). ETA=11:09:46, max mem: 2.7 GB 
[11/06 22:41:40 visual_prompt]: 	Training 1900/2212. train loss: 0.3608,	0.1780 s / batch. (data: 1.04e-02). ETA=9:18:39, max mem: 2.7 GB 
[11/06 22:42:01 visual_prompt]: 	Training 2000/2212. train loss: 0.6531,	0.1926 s / batch. (data: 2.09e-02). ETA=10:04:08, max mem: 2.7 GB 
[11/06 22:42:19 visual_prompt]: 	Training 2100/2212. train loss: 0.4810,	0.1639 s / batch. (data: 6.45e-03). ETA=8:34:01, max mem: 2.7 GB 
[11/06 22:42:38 visual_prompt]: 	Training 2200/2212. train loss: 0.5046,	0.1478 s / batch. (data: 8.37e-05). ETA=7:43:17, max mem: 2.7 GB 
[11/06 22:42:39 visual_prompt]: Epoch 15 / 100: avg data time: 3.06e-02, avg batch time: 0.2128, average train loss: 0.7081
[11/06 22:42:59 visual_prompt]: 	Test 100/246. loss: 0.982, 0.0867 s / batch. (data: 3.39e-05)max mem: 2.72052 GB 
[11/06 22:43:17 visual_prompt]: 	Test 200/246. loss: 0.982, 0.0831 s / batch. (data: 2.12e-05)max mem: 2.72052 GB 
[11/06 22:43:25 visual_prompt]: Inference (val):avg data time: 6.08e-04, avg batch time: 0.0605, average loss: 0.7007
[11/06 22:43:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.52	
[11/06 22:43:25 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/06 22:43:48 visual_prompt]: 	Training 100/2212. train loss: 0.4627,	0.1976 s / batch. (data: 1.04e-02). ETA=10:18:53, max mem: 2.7 GB 
[11/06 22:44:08 visual_prompt]: 	Training 200/2212. train loss: 0.9713,	0.1698 s / batch. (data: 1.04e-02). ETA=8:51:39, max mem: 2.7 GB 
[11/06 22:44:28 visual_prompt]: 	Training 300/2212. train loss: 0.5494,	0.2166 s / batch. (data: 3.91e-02). ETA=11:17:33, max mem: 2.7 GB 
[11/06 22:44:47 visual_prompt]: 	Training 400/2212. train loss: 0.8426,	0.1911 s / batch. (data: 5.79e-03). ETA=9:57:34, max mem: 2.7 GB 
[11/06 22:45:06 visual_prompt]: 	Training 500/2212. train loss: 0.7331,	0.1495 s / batch. (data: 5.71e-03). ETA=7:47:15, max mem: 2.7 GB 
[11/06 22:45:26 visual_prompt]: 	Training 600/2212. train loss: 0.2765,	0.1885 s / batch. (data: 1.07e-02). ETA=9:48:42, max mem: 2.7 GB 
[11/06 22:45:46 visual_prompt]: 	Training 700/2212. train loss: 0.7017,	0.1656 s / batch. (data: 1.04e-02). ETA=8:37:02, max mem: 2.7 GB 
[11/06 22:46:07 visual_prompt]: 	Training 800/2212. train loss: 0.7397,	0.1344 s / batch. (data: 1.54e-02). ETA=6:59:26, max mem: 2.7 GB 
[11/06 22:46:27 visual_prompt]: 	Training 900/2212. train loss: 0.6238,	0.2014 s / batch. (data: 2.81e-02). ETA=10:28:14, max mem: 2.7 GB 
[11/06 22:46:47 visual_prompt]: 	Training 1000/2212. train loss: 0.8348,	0.2644 s / batch. (data: 1.45e-01). ETA=13:44:06, max mem: 2.7 GB 
[11/06 22:47:09 visual_prompt]: 	Training 1100/2212. train loss: 0.6684,	0.1932 s / batch. (data: 2.06e-02). ETA=10:01:47, max mem: 2.7 GB 
[11/06 22:47:30 visual_prompt]: 	Training 1200/2212. train loss: 0.7489,	0.1643 s / batch. (data: 5.66e-03). ETA=8:31:39, max mem: 2.7 GB 
[11/06 22:47:51 visual_prompt]: 	Training 1300/2212. train loss: 0.7815,	0.2161 s / batch. (data: 7.38e-04). ETA=11:12:37, max mem: 2.7 GB 
[11/06 22:48:13 visual_prompt]: 	Training 1400/2212. train loss: 0.8815,	0.1158 s / batch. (data: 1.16e-02). ETA=6:00:18, max mem: 2.7 GB 
[11/06 22:48:34 visual_prompt]: 	Training 1500/2212. train loss: 0.6301,	0.2146 s / batch. (data: 5.76e-03). ETA=11:07:15, max mem: 2.7 GB 
[11/06 22:48:56 visual_prompt]: 	Training 1600/2212. train loss: 0.7592,	0.1220 s / batch. (data: 7.32e-04). ETA=6:19:08, max mem: 2.7 GB 
[11/06 22:49:16 visual_prompt]: 	Training 1700/2212. train loss: 0.6397,	0.2127 s / batch. (data: 6.70e-04). ETA=11:00:38, max mem: 2.7 GB 
[11/06 22:49:37 visual_prompt]: 	Training 1800/2212. train loss: 0.7558,	0.1263 s / batch. (data: 3.96e-03). ETA=6:32:04, max mem: 2.7 GB 
[11/06 22:49:57 visual_prompt]: 	Training 1900/2212. train loss: 0.6205,	0.1911 s / batch. (data: 6.69e-04). ETA=9:52:41, max mem: 2.7 GB 
[11/06 22:50:17 visual_prompt]: 	Training 2000/2212. train loss: 0.7808,	0.1969 s / batch. (data: 1.09e-02). ETA=10:10:18, max mem: 2.7 GB 
[11/06 22:50:40 visual_prompt]: 	Training 2100/2212. train loss: 0.5883,	0.1557 s / batch. (data: 2.62e-04). ETA=8:02:30, max mem: 2.7 GB 
[11/06 22:51:01 visual_prompt]: 	Training 2200/2212. train loss: 0.5178,	0.1725 s / batch. (data: 9.94e-05). ETA=8:54:20, max mem: 2.7 GB 
[11/06 22:51:03 visual_prompt]: Epoch 16 / 100: avg data time: 3.01e-02, avg batch time: 0.2069, average train loss: 0.6977
[11/06 22:51:24 visual_prompt]: 	Test 100/246. loss: 0.991, 0.0813 s / batch. (data: 2.91e-05)max mem: 2.72052 GB 
[11/06 22:51:44 visual_prompt]: 	Test 200/246. loss: 0.991, 0.0730 s / batch. (data: 2.17e-05)max mem: 2.72052 GB 
[11/06 22:51:52 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.0639, average loss: 0.7017
[11/06 22:51:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.06	
[11/06 22:51:52 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/06 22:52:15 visual_prompt]: 	Training 100/2212. train loss: 0.7785,	0.1832 s / batch. (data: 2.04e-02). ETA=9:26:53, max mem: 2.7 GB 
[11/06 22:52:36 visual_prompt]: 	Training 200/2212. train loss: 0.7107,	0.1907 s / batch. (data: 7.46e-04). ETA=9:49:48, max mem: 2.7 GB 
[11/06 22:52:58 visual_prompt]: 	Training 300/2212. train loss: 0.6778,	0.2169 s / batch. (data: 2.19e-02). ETA=11:10:29, max mem: 2.7 GB 
[11/06 22:53:18 visual_prompt]: 	Training 400/2212. train loss: 0.6544,	0.1945 s / batch. (data: 5.83e-03). ETA=10:01:06, max mem: 2.7 GB 
[11/06 22:53:40 visual_prompt]: 	Training 500/2212. train loss: 0.6068,	0.1901 s / batch. (data: 2.06e-02). ETA=9:47:13, max mem: 2.7 GB 
[11/06 22:54:03 visual_prompt]: 	Training 600/2212. train loss: 1.0049,	0.1554 s / batch. (data: 1.02e-03). ETA=7:59:45, max mem: 2.7 GB 
[11/06 22:54:23 visual_prompt]: 	Training 700/2212. train loss: 0.5563,	0.2395 s / batch. (data: 2.92e-02). ETA=12:18:55, max mem: 2.7 GB 
[11/06 22:54:43 visual_prompt]: 	Training 800/2212. train loss: 0.2006,	0.5842 s / batch. (data: 4.76e-01). ETA=1 day, 6:01:30, max mem: 2.7 GB 
[11/06 22:55:04 visual_prompt]: 	Training 900/2212. train loss: 0.5483,	0.2262 s / batch. (data: 5.40e-03). ETA=11:37:02, max mem: 2.7 GB 
[11/06 22:55:25 visual_prompt]: 	Training 1000/2212. train loss: 0.7298,	0.1869 s / batch. (data: 2.50e-04). ETA=9:35:48, max mem: 2.7 GB 
[11/06 22:55:46 visual_prompt]: 	Training 1100/2212. train loss: 0.6831,	0.2066 s / batch. (data: 1.04e-02). ETA=10:36:07, max mem: 2.7 GB 
[11/06 22:56:07 visual_prompt]: 	Training 1200/2212. train loss: 0.8113,	0.1619 s / batch. (data: 2.42e-04). ETA=8:18:10, max mem: 2.7 GB 
[11/06 22:56:28 visual_prompt]: 	Training 1300/2212. train loss: 0.6625,	0.1925 s / batch. (data: 1.04e-02). ETA=9:51:53, max mem: 2.7 GB 
[11/06 22:56:50 visual_prompt]: 	Training 1400/2212. train loss: 0.5984,	0.1694 s / batch. (data: 1.21e-02). ETA=8:40:43, max mem: 2.7 GB 
[11/06 22:57:12 visual_prompt]: 	Training 1500/2212. train loss: 0.5696,	0.2354 s / batch. (data: 1.55e-02). ETA=12:03:01, max mem: 2.7 GB 
[11/06 22:57:34 visual_prompt]: 	Training 1600/2212. train loss: 0.8443,	0.1548 s / batch. (data: 2.49e-04). ETA=7:55:19, max mem: 2.7 GB 
[11/06 22:57:54 visual_prompt]: 	Training 1700/2212. train loss: 0.9127,	0.1883 s / batch. (data: 2.60e-04). ETA=9:37:41, max mem: 2.7 GB 
[11/06 22:58:15 visual_prompt]: 	Training 1800/2212. train loss: 1.0271,	0.1976 s / batch. (data: 1.55e-02). ETA=10:06:01, max mem: 2.7 GB 
[11/06 22:58:37 visual_prompt]: 	Training 1900/2212. train loss: 0.6181,	0.1994 s / batch. (data: 2.05e-02). ETA=10:11:18, max mem: 2.7 GB 
[11/06 22:58:59 visual_prompt]: 	Training 2000/2212. train loss: 0.4726,	0.2272 s / batch. (data: 1.55e-02). ETA=11:36:03, max mem: 2.7 GB 
[11/06 22:59:21 visual_prompt]: 	Training 2100/2212. train loss: 0.5086,	0.1934 s / batch. (data: 6.68e-04). ETA=9:52:15, max mem: 2.7 GB 
[11/06 22:59:42 visual_prompt]: 	Training 2200/2212. train loss: 0.9925,	0.1619 s / batch. (data: 1.14e-04). ETA=8:15:27, max mem: 2.7 GB 
[11/06 22:59:43 visual_prompt]: Epoch 17 / 100: avg data time: 2.81e-02, avg batch time: 0.2129, average train loss: 0.7059
[11/06 23:00:04 visual_prompt]: 	Test 100/246. loss: 0.937, 0.1009 s / batch. (data: 2.91e-05)max mem: 2.72052 GB 
[11/06 23:00:24 visual_prompt]: 	Test 200/246. loss: 0.937, 0.0488 s / batch. (data: 3.62e-05)max mem: 2.72052 GB 
[11/06 23:00:32 visual_prompt]: Inference (val):avg data time: 6.15e-05, avg batch time: 0.0617, average loss: 0.6957
[11/06 23:00:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.78	
[11/06 23:00:32 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/06 23:00:56 visual_prompt]: 	Training 100/2212. train loss: 0.8682,	0.2036 s / batch. (data: 2.67e-04). ETA=10:22:37, max mem: 2.7 GB 
[11/06 23:01:18 visual_prompt]: 	Training 200/2212. train loss: 0.5353,	0.2078 s / batch. (data: 3.66e-02). ETA=10:35:09, max mem: 2.7 GB 
[11/06 23:01:39 visual_prompt]: 	Training 300/2212. train loss: 0.5404,	0.2400 s / batch. (data: 2.88e-04). ETA=12:13:05, max mem: 2.7 GB 
[11/06 23:02:01 visual_prompt]: 	Training 400/2212. train loss: 0.5664,	0.2020 s / batch. (data: 5.36e-03). ETA=10:16:40, max mem: 2.7 GB 
[11/06 23:02:23 visual_prompt]: 	Training 500/2212. train loss: 0.7738,	0.7380 s / batch. (data: 6.40e-01). ETA=1 day, 13:32:01, max mem: 2.7 GB 
[11/06 23:02:44 visual_prompt]: 	Training 600/2212. train loss: 0.7251,	0.1865 s / batch. (data: 1.08e-02). ETA=9:28:53, max mem: 2.7 GB 
[11/06 23:03:06 visual_prompt]: 	Training 700/2212. train loss: 0.8341,	0.2609 s / batch. (data: 1.09e-02). ETA=13:15:15, max mem: 2.7 GB 
[11/06 23:03:26 visual_prompt]: 	Training 800/2212. train loss: 0.7486,	0.2201 s / batch. (data: 1.04e-02). ETA=11:10:37, max mem: 2.7 GB 
[11/06 23:03:48 visual_prompt]: 	Training 900/2212. train loss: 0.7135,	0.2252 s / batch. (data: 1.09e-02). ETA=11:25:34, max mem: 2.7 GB 
[11/06 23:04:09 visual_prompt]: 	Training 1000/2212. train loss: 0.8818,	0.1951 s / batch. (data: 5.22e-02). ETA=9:53:40, max mem: 2.7 GB 
[11/06 23:04:31 visual_prompt]: 	Training 1100/2212. train loss: 0.6277,	0.1899 s / batch. (data: 8.00e-03). ETA=9:37:34, max mem: 2.7 GB 
[11/06 23:04:51 visual_prompt]: 	Training 1200/2212. train loss: 0.8052,	0.1609 s / batch. (data: 5.38e-03). ETA=8:09:09, max mem: 2.7 GB 
[11/06 23:05:12 visual_prompt]: 	Training 1300/2212. train loss: 0.5989,	0.2105 s / batch. (data: 1.04e-02). ETA=10:39:26, max mem: 2.7 GB 
[11/06 23:05:33 visual_prompt]: 	Training 1400/2212. train loss: 0.7587,	0.2037 s / batch. (data: 5.37e-02). ETA=10:18:24, max mem: 2.7 GB 
[11/06 23:05:55 visual_prompt]: 	Training 1500/2212. train loss: 0.8765,	0.2335 s / batch. (data: 1.55e-02). ETA=11:48:40, max mem: 2.7 GB 
[11/06 23:06:16 visual_prompt]: 	Training 1600/2212. train loss: 0.6073,	0.2501 s / batch. (data: 1.09e-02). ETA=12:38:44, max mem: 2.7 GB 
[11/06 23:06:37 visual_prompt]: 	Training 1700/2212. train loss: 0.6059,	0.2551 s / batch. (data: 2.10e-02). ETA=12:53:16, max mem: 2.7 GB 
[11/06 23:07:00 visual_prompt]: 	Training 1800/2212. train loss: 0.8174,	0.2159 s / batch. (data: 2.10e-02). ETA=10:54:10, max mem: 2.7 GB 
[11/06 23:07:20 visual_prompt]: 	Training 1900/2212. train loss: 0.5963,	0.2242 s / batch. (data: 5.62e-03). ETA=11:18:59, max mem: 2.7 GB 
[11/06 23:07:41 visual_prompt]: 	Training 2000/2212. train loss: 0.8074,	0.2261 s / batch. (data: 1.58e-02). ETA=11:24:14, max mem: 2.7 GB 
[11/06 23:08:01 visual_prompt]: 	Training 2100/2212. train loss: 0.5862,	0.1664 s / batch. (data: 2.82e-04). ETA=8:23:14, max mem: 2.7 GB 
[11/06 23:08:22 visual_prompt]: 	Training 2200/2212. train loss: 0.5618,	0.1106 s / batch. (data: 1.09e-04). ETA=5:34:15, max mem: 2.7 GB 
[11/06 23:08:23 visual_prompt]: Epoch 18 / 100: avg data time: 2.85e-02, avg batch time: 0.2131, average train loss: 0.6982
[11/06 23:08:44 visual_prompt]: 	Test 100/246. loss: 0.844, 0.0670 s / batch. (data: 4.20e-05)max mem: 2.72052 GB 
[11/06 23:09:04 visual_prompt]: 	Test 200/246. loss: 0.844, 0.0897 s / batch. (data: 2.74e-05)max mem: 2.72052 GB 
[11/06 23:09:12 visual_prompt]: Inference (val):avg data time: 2.22e-04, avg batch time: 0.0641, average loss: 0.6893
[11/06 23:09:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.95	
[11/06 23:09:12 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/06 23:09:36 visual_prompt]: 	Training 100/2212. train loss: 0.9035,	0.1825 s / batch. (data: 5.37e-03). ETA=9:11:22, max mem: 2.7 GB 
[11/06 23:09:58 visual_prompt]: 	Training 200/2212. train loss: 0.5679,	0.1675 s / batch. (data: 3.22e-04). ETA=8:25:40, max mem: 2.7 GB 
[11/06 23:10:18 visual_prompt]: 	Training 300/2212. train loss: 1.1631,	0.2172 s / batch. (data: 7.73e-04). ETA=10:55:36, max mem: 2.7 GB 
[11/06 23:10:39 visual_prompt]: 	Training 400/2212. train loss: 0.5566,	0.5403 s / batch. (data: 3.73e-01). ETA=1 day, 3:09:42, max mem: 2.7 GB 
[11/06 23:11:00 visual_prompt]: 	Training 500/2212. train loss: 0.6033,	0.2332 s / batch. (data: 1.59e-02). ETA=11:43:09, max mem: 2.7 GB 
[11/06 23:11:23 visual_prompt]: 	Training 600/2212. train loss: 0.8827,	0.1454 s / batch. (data: 1.03e-03). ETA=7:18:04, max mem: 2.7 GB 
[11/06 23:11:43 visual_prompt]: 	Training 700/2212. train loss: 0.8296,	0.2404 s / batch. (data: 3.28e-02). ETA=12:03:51, max mem: 2.7 GB 
[11/06 23:12:05 visual_prompt]: 	Training 800/2212. train loss: 0.5638,	0.2147 s / batch. (data: 2.92e-04). ETA=10:46:16, max mem: 2.7 GB 
[11/06 23:12:26 visual_prompt]: 	Training 900/2212. train loss: 0.5569,	0.1645 s / batch. (data: 5.34e-03). ETA=8:14:47, max mem: 2.7 GB 
[11/06 23:12:47 visual_prompt]: 	Training 1000/2212. train loss: 0.5466,	0.2252 s / batch. (data: 2.57e-02). ETA=11:17:11, max mem: 2.7 GB 
[11/06 23:13:10 visual_prompt]: 	Training 1100/2212. train loss: 0.9052,	0.1988 s / batch. (data: 5.40e-03). ETA=9:57:17, max mem: 2.7 GB 
[11/06 23:13:30 visual_prompt]: 	Training 1200/2212. train loss: 1.3848,	0.1850 s / batch. (data: 1.12e-02). ETA=9:15:25, max mem: 2.7 GB 
[11/06 23:13:53 visual_prompt]: 	Training 1300/2212. train loss: 0.8316,	0.1912 s / batch. (data: 2.90e-02). ETA=9:33:51, max mem: 2.7 GB 
[11/06 23:14:13 visual_prompt]: 	Training 1400/2212. train loss: 0.7402,	0.1984 s / batch. (data: 2.98e-02). ETA=9:55:07, max mem: 2.7 GB 
[11/06 23:14:34 visual_prompt]: 	Training 1500/2212. train loss: 0.7360,	0.2120 s / batch. (data: 5.40e-03). ETA=10:35:36, max mem: 2.7 GB 
[11/06 23:14:56 visual_prompt]: 	Training 1600/2212. train loss: 0.8149,	0.2791 s / batch. (data: 1.60e-01). ETA=13:56:17, max mem: 2.7 GB 
[11/06 23:15:17 visual_prompt]: 	Training 1700/2212. train loss: 0.4888,	0.2041 s / batch. (data: 6.98e-04). ETA=10:11:06, max mem: 2.7 GB 
[11/06 23:15:38 visual_prompt]: 	Training 1800/2212. train loss: 0.6148,	0.2106 s / batch. (data: 1.08e-02). ETA=10:30:12, max mem: 2.7 GB 
[11/06 23:16:00 visual_prompt]: 	Training 1900/2212. train loss: 0.5154,	0.2109 s / batch. (data: 3.59e-02). ETA=10:30:49, max mem: 2.7 GB 
[11/06 23:16:20 visual_prompt]: 	Training 2000/2212. train loss: 0.6448,	0.2257 s / batch. (data: 1.60e-02). ETA=11:14:51, max mem: 2.7 GB 
[11/06 23:16:41 visual_prompt]: 	Training 2100/2212. train loss: 0.5808,	0.1909 s / batch. (data: 7.00e-04). ETA=9:30:25, max mem: 2.7 GB 
[11/06 23:17:02 visual_prompt]: 	Training 2200/2212. train loss: 0.3812,	0.1509 s / batch. (data: 1.19e-04). ETA=7:30:31, max mem: 2.7 GB 
[11/06 23:17:04 visual_prompt]: Epoch 19 / 100: avg data time: 2.80e-02, avg batch time: 0.2130, average train loss: 0.7023
[11/06 23:17:25 visual_prompt]: 	Test 100/246. loss: 0.432, 0.1028 s / batch. (data: 2.84e-05)max mem: 2.72052 GB 
[11/06 23:17:45 visual_prompt]: 	Test 200/246. loss: 0.432, 0.0697 s / batch. (data: 2.57e-05)max mem: 2.72052 GB 
[11/06 23:17:52 visual_prompt]: Inference (val):avg data time: 6.13e-05, avg batch time: 0.0637, average loss: 0.7701
[11/06 23:17:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.02	
[11/06 23:17:52 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/06 23:18:16 visual_prompt]: 	Training 100/2212. train loss: 0.5124,	0.2205 s / batch. (data: 2.68e-04). ETA=10:57:58, max mem: 2.7 GB 
[11/06 23:18:39 visual_prompt]: 	Training 200/2212. train loss: 1.0433,	0.1111 s / batch. (data: 2.24e-04). ETA=5:31:30, max mem: 2.7 GB 
[11/06 23:19:00 visual_prompt]: 	Training 300/2212. train loss: 0.5681,	0.1824 s / batch. (data: 1.05e-02). ETA=9:03:43, max mem: 2.7 GB 
[11/06 23:19:22 visual_prompt]: 	Training 400/2212. train loss: 0.6607,	0.2253 s / batch. (data: 2.53e-02). ETA=11:11:09, max mem: 2.7 GB 
[11/06 23:19:43 visual_prompt]: 	Training 500/2212. train loss: 0.4971,	0.2080 s / batch. (data: 2.45e-04). ETA=10:19:21, max mem: 2.7 GB 
[11/06 23:20:04 visual_prompt]: 	Training 600/2212. train loss: 0.8525,	0.1975 s / batch. (data: 4.57e-04). ETA=9:47:51, max mem: 2.7 GB 
[11/06 23:20:25 visual_prompt]: 	Training 700/2212. train loss: 0.6120,	0.1954 s / batch. (data: 5.32e-03). ETA=9:41:13, max mem: 2.7 GB 
[11/06 23:20:45 visual_prompt]: 	Training 800/2212. train loss: 0.5183,	0.2082 s / batch. (data: 4.71e-03). ETA=10:19:01, max mem: 2.7 GB 
[11/06 23:21:07 visual_prompt]: 	Training 900/2212. train loss: 0.7786,	0.2069 s / batch. (data: 1.55e-02). ETA=10:14:35, max mem: 2.7 GB 
[11/06 23:21:27 visual_prompt]: 	Training 1000/2212. train loss: 0.9230,	0.1824 s / batch. (data: 6.35e-03). ETA=9:01:31, max mem: 2.7 GB 
[11/06 23:21:48 visual_prompt]: 	Training 1100/2212. train loss: 0.4839,	0.2267 s / batch. (data: 5.41e-03). ETA=11:12:50, max mem: 2.7 GB 
[11/06 23:22:10 visual_prompt]: 	Training 1200/2212. train loss: 0.7820,	0.2266 s / batch. (data: 1.55e-02). ETA=11:11:59, max mem: 2.7 GB 
[11/06 23:22:32 visual_prompt]: 	Training 1300/2212. train loss: 0.8541,	0.1950 s / batch. (data: 4.18e-02). ETA=9:38:05, max mem: 2.7 GB 
[11/06 23:22:53 visual_prompt]: 	Training 1400/2212. train loss: 0.4822,	0.1723 s / batch. (data: 6.48e-04). ETA=8:30:32, max mem: 2.7 GB 
[11/06 23:23:15 visual_prompt]: 	Training 1500/2212. train loss: 0.6898,	0.1811 s / batch. (data: 1.04e-02). ETA=8:56:18, max mem: 2.7 GB 
[11/06 23:23:35 visual_prompt]: 	Training 1600/2212. train loss: 0.8665,	0.2456 s / batch. (data: 1.09e-02). ETA=12:06:49, max mem: 2.7 GB 
[11/06 23:23:56 visual_prompt]: 	Training 1700/2212. train loss: 0.8411,	0.2215 s / batch. (data: 3.79e-02). ETA=10:55:15, max mem: 2.7 GB 
[11/06 23:24:17 visual_prompt]: 	Training 1800/2212. train loss: 0.6116,	0.1855 s / batch. (data: 1.04e-02). ETA=9:08:20, max mem: 2.7 GB 
[11/06 23:24:38 visual_prompt]: 	Training 1900/2212. train loss: 0.8567,	0.2086 s / batch. (data: 5.79e-03). ETA=10:16:23, max mem: 2.7 GB 
[11/06 23:25:00 visual_prompt]: 	Training 2000/2212. train loss: 0.5968,	0.1931 s / batch. (data: 3.58e-03). ETA=9:30:11, max mem: 2.7 GB 
[11/06 23:25:22 visual_prompt]: 	Training 2100/2212. train loss: 0.6830,	0.2134 s / batch. (data: 1.05e-02). ETA=10:29:45, max mem: 2.7 GB 
[11/06 23:25:42 visual_prompt]: 	Training 2200/2212. train loss: 0.5685,	0.1555 s / batch. (data: 1.18e-04). ETA=7:38:36, max mem: 2.7 GB 
[11/06 23:25:43 visual_prompt]: Epoch 20 / 100: avg data time: 2.70e-02, avg batch time: 0.2128, average train loss: 0.7037
[11/06 23:26:05 visual_prompt]: 	Test 100/246. loss: 0.688, 0.0931 s / batch. (data: 3.77e-05)max mem: 2.72052 GB 
[11/06 23:26:24 visual_prompt]: 	Test 200/246. loss: 0.688, 0.0691 s / batch. (data: 3.22e-05)max mem: 2.72052 GB 
[11/06 23:26:33 visual_prompt]: Inference (val):avg data time: 2.54e-04, avg batch time: 0.0643, average loss: 0.6937
[11/06 23:26:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.95	
[11/06 23:26:33 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/06 23:26:57 visual_prompt]: 	Training 100/2212. train loss: 0.5834,	1.0838 s / batch. (data: 9.48e-01). ETA=2 days, 5:14:40, max mem: 2.7 GB 
[11/06 23:27:18 visual_prompt]: 	Training 200/2212. train loss: 0.6685,	0.2223 s / batch. (data: 1.59e-02). ETA=10:54:51, max mem: 2.7 GB 
[11/06 23:27:41 visual_prompt]: 	Training 300/2212. train loss: 1.0859,	0.1983 s / batch. (data: 1.05e-02). ETA=9:44:00, max mem: 2.7 GB 
[11/06 23:28:03 visual_prompt]: 	Training 400/2212. train loss: 0.8037,	0.2312 s / batch. (data: 4.06e-02). ETA=11:20:25, max mem: 2.7 GB 
[11/06 23:28:23 visual_prompt]: 	Training 500/2212. train loss: 0.7896,	0.2082 s / batch. (data: 1.09e-02). ETA=10:12:23, max mem: 2.7 GB 
[11/06 23:28:44 visual_prompt]: 	Training 600/2212. train loss: 0.8306,	0.2676 s / batch. (data: 3.00e-02). ETA=13:06:30, max mem: 2.7 GB 
[11/06 23:29:07 visual_prompt]: 	Training 700/2212. train loss: 0.5770,	0.2234 s / batch. (data: 1.59e-02). ETA=10:56:16, max mem: 2.7 GB 
[11/06 23:29:29 visual_prompt]: 	Training 800/2212. train loss: 0.6874,	0.2376 s / batch. (data: 5.33e-03). ETA=11:37:35, max mem: 2.7 GB 
[11/06 23:29:50 visual_prompt]: 	Training 900/2212. train loss: 0.7887,	0.2160 s / batch. (data: 1.05e-02). ETA=10:33:55, max mem: 2.7 GB 
[11/06 23:30:11 visual_prompt]: 	Training 1000/2212. train loss: 0.6271,	0.2210 s / batch. (data: 1.05e-02). ETA=10:48:10, max mem: 2.7 GB 
[11/06 23:30:31 visual_prompt]: 	Training 1100/2212. train loss: 0.7528,	0.2095 s / batch. (data: 1.55e-02). ETA=10:14:06, max mem: 2.7 GB 
[11/06 23:30:53 visual_prompt]: 	Training 1200/2212. train loss: 0.7071,	0.1982 s / batch. (data: 4.06e-02). ETA=9:40:36, max mem: 2.7 GB 
[11/06 23:31:14 visual_prompt]: 	Training 1300/2212. train loss: 0.6523,	0.1979 s / batch. (data: 2.06e-02). ETA=9:39:29, max mem: 2.7 GB 
[11/06 23:31:36 visual_prompt]: 	Training 1400/2212. train loss: 0.6152,	0.2432 s / batch. (data: 1.04e-02). ETA=11:51:36, max mem: 2.7 GB 
[11/06 23:31:58 visual_prompt]: 	Training 1500/2212. train loss: 0.5682,	0.2464 s / batch. (data: 1.55e-02). ETA=12:00:32, max mem: 2.7 GB 
[11/06 23:32:18 visual_prompt]: 	Training 1600/2212. train loss: 0.5715,	0.2554 s / batch. (data: 5.43e-03). ETA=12:26:20, max mem: 2.7 GB 
[11/06 23:32:38 visual_prompt]: 	Training 1700/2212. train loss: 0.7787,	0.1609 s / batch. (data: 5.38e-03). ETA=7:50:02, max mem: 2.7 GB 
[11/06 23:32:59 visual_prompt]: 	Training 1800/2212. train loss: 0.6419,	0.1347 s / batch. (data: 1.55e-02). ETA=6:33:08, max mem: 2.7 GB 
[11/06 23:33:20 visual_prompt]: 	Training 1900/2212. train loss: 0.6088,	0.2233 s / batch. (data: 1.18e-02). ETA=10:51:22, max mem: 2.7 GB 
[11/06 23:33:41 visual_prompt]: 	Training 2000/2212. train loss: 0.7846,	0.1809 s / batch. (data: 1.01e-02). ETA=8:47:22, max mem: 2.7 GB 
[11/06 23:34:03 visual_prompt]: 	Training 2100/2212. train loss: 0.9442,	0.2028 s / batch. (data: 1.39e-02). ETA=9:51:00, max mem: 2.7 GB 
[11/06 23:34:23 visual_prompt]: 	Training 2200/2212. train loss: 0.7141,	0.1363 s / batch. (data: 1.54e-04). ETA=6:37:01, max mem: 2.7 GB 
[11/06 23:34:24 visual_prompt]: Epoch 21 / 100: avg data time: 2.72e-02, avg batch time: 0.2131, average train loss: 0.6952
[11/06 23:34:46 visual_prompt]: 	Test 100/246. loss: 0.909, 0.0827 s / batch. (data: 2.60e-05)max mem: 2.72052 GB 
[11/06 23:35:05 visual_prompt]: 	Test 200/246. loss: 0.909, 0.0621 s / batch. (data: 2.46e-05)max mem: 2.72052 GB 
[11/06 23:35:13 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.0629, average loss: 0.6931
[11/06 23:35:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.09	
[11/06 23:35:13 visual_prompt]: Stopping early.
[11/06 23:35:13 visual_prompt]: Rank of current process: 0. World size: 1
[11/06 23:35:13 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/06 23:35:13 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '1', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/06 23:35:13 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/06 23:35:13 visual_prompt]: Training with config:
[11/06 23:35:13 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/crop224/val/seed0/lr0.005_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 1, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/06 23:35:13 visual_prompt]: Loading training data...
[11/06 23:35:13 visual_prompt]: Constructing mammo-cbis dataset train...
[11/06 23:35:13 visual_prompt]: Loading validation data...
[11/06 23:35:13 visual_prompt]: Constructing mammo-cbis dataset val...
[11/06 23:35:13 visual_prompt]: Constructing models...
[11/06 23:35:18 visual_prompt]: Enable all parameters update during training
[11/06 23:35:18 visual_prompt]: Total Parameters: 85800194	 Gradient Parameters: 85800194
[11/06 23:35:18 visual_prompt]: tuned percent:100.000
[11/06 23:35:18 visual_prompt]: Device used for model: 0
[11/06 23:35:18 visual_prompt]: Setting up Evaluator...
[11/06 23:35:18 visual_prompt]: Setting up Trainer...
[11/06 23:35:18 visual_prompt]: 	Setting up the optimizer...
[11/06 23:35:18 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/06 23:35:42 visual_prompt]: 	Training 100/2212. train loss: 17.0073,	0.1641 s / batch. (data: 6.70e-03). ETA=10:04:51, max mem: 4.0 GB 
[11/06 23:36:04 visual_prompt]: 	Training 200/2212. train loss: 8.7706,	0.1662 s / batch. (data: 2.43e-04). ETA=10:12:07, max mem: 4.0 GB 
[11/06 23:36:25 visual_prompt]: 	Training 300/2212. train loss: 0.0001,	0.1957 s / batch. (data: 5.81e-03). ETA=12:00:40, max mem: 4.0 GB 
[11/06 23:36:45 visual_prompt]: 	Training 400/2212. train loss: 9.7364,	0.1328 s / batch. (data: 3.39e-03). ETA=8:08:41, max mem: 4.0 GB 
[11/06 23:37:08 visual_prompt]: 	Training 500/2212. train loss: 9.6752,	0.1511 s / batch. (data: 1.06e-02). ETA=9:15:47, max mem: 4.0 GB 
[11/06 23:37:30 visual_prompt]: 	Training 600/2212. train loss: 0.0000,	0.2521 s / batch. (data: 2.91e-02). ETA=15:26:44, max mem: 4.0 GB 
[11/06 23:37:52 visual_prompt]: 	Training 700/2212. train loss: 10.7484,	0.2379 s / batch. (data: 5.83e-03). ETA=14:34:12, max mem: 4.0 GB 
[11/06 23:38:14 visual_prompt]: 	Training 800/2212. train loss: 10.3468,	0.2508 s / batch. (data: 5.38e-03). ETA=15:21:11, max mem: 4.0 GB 
[11/06 23:38:36 visual_prompt]: 	Training 900/2212. train loss: 0.0000,	0.2286 s / batch. (data: 2.06e-02). ETA=13:59:13, max mem: 4.0 GB 
[11/06 23:38:56 visual_prompt]: 	Training 1000/2212. train loss: 0.0001,	0.2172 s / batch. (data: 1.21e-02). ETA=13:17:14, max mem: 4.0 GB 
[11/06 23:39:17 visual_prompt]: 	Training 1100/2212. train loss: 0.0000,	0.1169 s / batch. (data: 1.28e-02). ETA=7:08:43, max mem: 4.0 GB 
[11/06 23:39:37 visual_prompt]: 	Training 1200/2212. train loss: 0.0000,	0.2032 s / batch. (data: 1.55e-02). ETA=12:25:11, max mem: 4.0 GB 
[11/06 23:39:58 visual_prompt]: 	Training 1300/2212. train loss: 0.0000,	0.2145 s / batch. (data: 5.79e-03). ETA=13:06:15, max mem: 4.0 GB 
[11/06 23:40:20 visual_prompt]: 	Training 1400/2212. train loss: 0.0027,	0.1259 s / batch. (data: 3.05e-04). ETA=7:41:04, max mem: 4.0 GB 
[11/06 23:40:41 visual_prompt]: 	Training 1500/2212. train loss: 5.3648,	0.2219 s / batch. (data: 7.33e-04). ETA=13:32:22, max mem: 4.0 GB 
[11/06 23:41:02 visual_prompt]: 	Training 1600/2212. train loss: 5.2697,	0.1980 s / batch. (data: 2.57e-02). ETA=12:04:39, max mem: 4.0 GB 
[11/06 23:41:23 visual_prompt]: 	Training 1700/2212. train loss: 0.0000,	0.1995 s / batch. (data: 2.06e-02). ETA=12:09:53, max mem: 4.0 GB 
[11/06 23:41:45 visual_prompt]: 	Training 1800/2212. train loss: 0.0000,	0.1963 s / batch. (data: 1.55e-02). ETA=11:57:38, max mem: 4.0 GB 
[11/06 23:42:06 visual_prompt]: 	Training 1900/2212. train loss: 8.6260,	0.1975 s / batch. (data: 9.71e-03). ETA=12:01:47, max mem: 4.0 GB 
[11/06 23:42:28 visual_prompt]: 	Training 2000/2212. train loss: 0.0002,	0.1874 s / batch. (data: 5.37e-03). ETA=11:24:28, max mem: 4.0 GB 
[11/06 23:42:48 visual_prompt]: 	Training 2100/2212. train loss: 14.9015,	0.2061 s / batch. (data: 1.55e-02). ETA=12:32:40, max mem: 4.0 GB 
[11/06 23:43:08 visual_prompt]: 	Training 2200/2212. train loss: 7.6808,	0.1124 s / batch. (data: 1.16e-04). ETA=6:50:05, max mem: 4.0 GB 
[11/06 23:43:09 visual_prompt]: Epoch 1 / 100: avg data time: 2.75e-02, avg batch time: 0.2130, average train loss: 5.2946
[11/06 23:43:30 visual_prompt]: 	Test 100/246. loss: 0.001, 0.0669 s / batch. (data: 2.91e-05)max mem: 4.00127 GB 
[11/06 23:43:50 visual_prompt]: 	Test 200/246. loss: 0.002, 0.0917 s / batch. (data: 3.22e-05)max mem: 4.00127 GB 
[11/06 23:43:58 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.0645, average loss: 4.3337
[11/06 23:43:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.86	
[11/06 23:43:58 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/06 23:44:23 visual_prompt]: 	Training 100/2212. train loss: 0.0110,	0.1552 s / batch. (data: 2.65e-04). ETA=9:26:21, max mem: 4.0 GB 
[11/06 23:44:44 visual_prompt]: 	Training 200/2212. train loss: 0.0058,	0.2240 s / batch. (data: 3.29e-04). ETA=13:36:47, max mem: 4.0 GB 
[11/06 23:45:06 visual_prompt]: 	Training 300/2212. train loss: 0.3867,	0.1761 s / batch. (data: 3.31e-02). ETA=10:42:00, max mem: 4.0 GB 
[11/06 23:45:26 visual_prompt]: 	Training 400/2212. train loss: 1.3777,	0.2024 s / batch. (data: 5.43e-03). ETA=12:17:17, max mem: 4.0 GB 
[11/06 23:45:46 visual_prompt]: 	Training 500/2212. train loss: 0.3260,	0.2040 s / batch. (data: 2.06e-02). ETA=12:23:02, max mem: 4.0 GB 
[11/06 23:46:07 visual_prompt]: 	Training 600/2212. train loss: 0.2823,	0.1531 s / batch. (data: 1.20e-03). ETA=9:17:05, max mem: 4.0 GB 
[11/06 23:46:28 visual_prompt]: 	Training 700/2212. train loss: 1.1778,	0.1322 s / batch. (data: 2.32e-04). ETA=8:00:51, max mem: 4.0 GB 
[11/06 23:46:50 visual_prompt]: 	Training 800/2212. train loss: 0.2036,	0.1927 s / batch. (data: 5.80e-03). ETA=11:40:39, max mem: 4.0 GB 
[11/06 23:47:11 visual_prompt]: 	Training 900/2212. train loss: 0.3994,	0.2354 s / batch. (data: 1.67e-02). ETA=14:15:28, max mem: 4.0 GB 
[11/06 23:47:34 visual_prompt]: 	Training 1000/2212. train loss: 1.7676,	0.2131 s / batch. (data: 2.56e-02). ETA=12:54:18, max mem: 4.0 GB 
[11/06 23:47:55 visual_prompt]: 	Training 1100/2212. train loss: 0.0406,	0.1912 s / batch. (data: 2.56e-02). ETA=11:34:10, max mem: 4.0 GB 
[11/06 23:48:16 visual_prompt]: 	Training 1200/2212. train loss: 0.8795,	0.1737 s / batch. (data: 7.07e-04). ETA=10:30:26, max mem: 4.0 GB 
[11/06 23:48:39 visual_prompt]: 	Training 1300/2212. train loss: 1.6760,	0.4980 s / batch. (data: 3.49e-01). ETA=1 day, 6:06:55, max mem: 4.0 GB 
[11/06 23:49:00 visual_prompt]: 	Training 1400/2212. train loss: 1.4970,	0.2081 s / batch. (data: 4.38e-02). ETA=12:34:34, max mem: 4.0 GB 
[11/06 23:49:22 visual_prompt]: 	Training 1500/2212. train loss: 0.9697,	0.2152 s / batch. (data: 1.55e-02). ETA=13:00:09, max mem: 4.0 GB 
[11/06 23:49:44 visual_prompt]: 	Training 1600/2212. train loss: 1.3313,	0.2737 s / batch. (data: 2.10e-02). ETA=16:31:32, max mem: 4.0 GB 
[11/06 23:50:04 visual_prompt]: 	Training 1700/2212. train loss: 0.3226,	0.2230 s / batch. (data: 5.83e-03). ETA=13:27:29, max mem: 4.0 GB 
[11/06 23:50:25 visual_prompt]: 	Training 1800/2212. train loss: 0.4831,	0.2055 s / batch. (data: 2.71e-04). ETA=12:23:58, max mem: 4.0 GB 
[11/06 23:50:46 visual_prompt]: 	Training 1900/2212. train loss: 0.1575,	0.1115 s / batch. (data: 2.59e-04). ETA=6:43:30, max mem: 4.0 GB 
[11/06 23:51:07 visual_prompt]: 	Training 2000/2212. train loss: 0.6884,	0.2086 s / batch. (data: 5.39e-03). ETA=12:34:16, max mem: 4.0 GB 
[11/06 23:51:27 visual_prompt]: 	Training 2100/2212. train loss: 0.1138,	0.2247 s / batch. (data: 5.82e-03). ETA=13:32:04, max mem: 4.0 GB 
[11/06 23:51:48 visual_prompt]: 	Training 2200/2212. train loss: 0.4864,	0.1323 s / batch. (data: 1.14e-04). ETA=7:58:10, max mem: 4.0 GB 
[11/06 23:51:49 visual_prompt]: Epoch 2 / 100: avg data time: 3.05e-02, avg batch time: 0.2130, average train loss: 1.4344
[11/06 23:52:11 visual_prompt]: 	Test 100/246. loss: 0.152, 0.0639 s / batch. (data: 3.00e-05)max mem: 4.00127 GB 
[11/06 23:52:30 visual_prompt]: 	Test 200/246. loss: 0.149, 0.1129 s / batch. (data: 3.10e-05)max mem: 4.00127 GB 
[11/06 23:52:38 visual_prompt]: Inference (val):avg data time: 6.24e-05, avg batch time: 0.0654, average loss: 1.1422
[11/06 23:52:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.83	
[11/06 23:52:38 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/06 23:53:02 visual_prompt]: 	Training 100/2212. train loss: 0.4078,	0.1778 s / batch. (data: 7.61e-04). ETA=10:42:15, max mem: 4.0 GB 
[11/06 23:53:23 visual_prompt]: 	Training 200/2212. train loss: 3.2550,	0.2299 s / batch. (data: 5.35e-03). ETA=13:49:49, max mem: 4.0 GB 
[11/06 23:53:45 visual_prompt]: 	Training 300/2212. train loss: 2.6896,	0.2079 s / batch. (data: 1.55e-02). ETA=12:30:12, max mem: 4.0 GB 
[11/06 23:54:07 visual_prompt]: 	Training 400/2212. train loss: 1.0970,	0.1830 s / batch. (data: 1.55e-02). ETA=11:00:07, max mem: 4.0 GB 
[11/06 23:54:28 visual_prompt]: 	Training 500/2212. train loss: 2.0802,	0.2086 s / batch. (data: 1.84e-02). ETA=12:31:47, max mem: 4.0 GB 
[11/06 23:54:51 visual_prompt]: 	Training 600/2212. train loss: 1.2381,	0.1787 s / batch. (data: 5.58e-04). ETA=10:43:51, max mem: 4.0 GB 
[11/06 23:55:12 visual_prompt]: 	Training 700/2212. train loss: 0.2807,	0.1901 s / batch. (data: 1.53e-02). ETA=11:24:30, max mem: 4.0 GB 
[11/06 23:55:33 visual_prompt]: 	Training 800/2212. train loss: 1.7275,	0.1870 s / batch. (data: 1.55e-02). ETA=11:12:58, max mem: 4.0 GB 
[11/06 23:55:55 visual_prompt]: 	Training 900/2212. train loss: 1.1637,	0.2154 s / batch. (data: 2.13e-03). ETA=12:54:58, max mem: 4.0 GB 
[11/06 23:56:15 visual_prompt]: 	Training 1000/2212. train loss: 0.1545,	0.1786 s / batch. (data: 5.80e-03). ETA=10:42:22, max mem: 4.0 GB 
[11/06 23:56:36 visual_prompt]: 	Training 1100/2212. train loss: 0.3338,	0.1619 s / batch. (data: 2.21e-02). ETA=9:42:05, max mem: 4.0 GB 
[11/06 23:56:57 visual_prompt]: 	Training 1200/2212. train loss: 1.1778,	0.2186 s / batch. (data: 1.05e-02). ETA=13:05:26, max mem: 4.0 GB 
[11/06 23:57:19 visual_prompt]: 	Training 1300/2212. train loss: 1.6652,	0.1449 s / batch. (data: 1.08e-02). ETA=8:40:14, max mem: 4.0 GB 
[11/06 23:57:39 visual_prompt]: 	Training 1400/2212. train loss: 0.5532,	0.1091 s / batch. (data: 2.16e-04). ETA=6:31:46, max mem: 4.0 GB 
[11/06 23:57:59 visual_prompt]: 	Training 1500/2212. train loss: 0.0957,	0.2664 s / batch. (data: 1.59e-02). ETA=15:55:40, max mem: 4.0 GB 
[11/06 23:58:19 visual_prompt]: 	Training 1600/2212. train loss: 0.1149,	0.2001 s / batch. (data: 1.54e-02). ETA=11:57:39, max mem: 4.0 GB 
[11/06 23:58:39 visual_prompt]: 	Training 1700/2212. train loss: 0.9551,	0.1910 s / batch. (data: 1.04e-02). ETA=11:24:35, max mem: 4.0 GB 
[11/06 23:59:00 visual_prompt]: 	Training 1800/2212. train loss: 0.3133,	0.2046 s / batch. (data: 2.05e-02). ETA=12:13:12, max mem: 4.0 GB 
[11/06 23:59:20 visual_prompt]: 	Training 1900/2212. train loss: 0.2022,	0.2052 s / batch. (data: 5.21e-03). ETA=12:14:59, max mem: 4.0 GB 
[11/06 23:59:40 visual_prompt]: 	Training 2000/2212. train loss: 1.7955,	0.2206 s / batch. (data: 2.56e-02). ETA=13:09:50, max mem: 4.0 GB 
[11/07 00:00:02 visual_prompt]: 	Training 2100/2212. train loss: 3.6632,	0.2202 s / batch. (data: 1.08e-02). ETA=13:07:51, max mem: 4.0 GB 
[11/07 00:00:22 visual_prompt]: 	Training 2200/2212. train loss: 0.5762,	0.4602 s / batch. (data: 3.43e-01). ETA=1 day, 3:25:52, max mem: 4.0 GB 
[11/07 00:00:23 visual_prompt]: Epoch 3 / 100: avg data time: 2.99e-02, avg batch time: 0.2098, average train loss: 1.0003
[11/07 00:00:43 visual_prompt]: 	Test 100/246. loss: 0.710, 0.0993 s / batch. (data: 2.29e-05)max mem: 4.00127 GB 
[11/07 00:01:01 visual_prompt]: 	Test 200/246. loss: 0.706, 0.1090 s / batch. (data: 2.29e-05)max mem: 4.00127 GB 
[11/07 00:01:09 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.0559, average loss: 0.6914
[11/07 00:01:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 53.00	
[11/07 00:01:09 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/07 00:01:32 visual_prompt]: 	Training 100/2212. train loss: 1.9716,	0.1680 s / batch. (data: 1.82e-04). ETA=10:00:32, max mem: 4.0 GB 
[11/07 00:01:51 visual_prompt]: 	Training 200/2212. train loss: 0.6584,	0.1777 s / batch. (data: 1.81e-03). ETA=10:35:01, max mem: 4.0 GB 
[11/07 00:02:12 visual_prompt]: 	Training 300/2212. train loss: 0.3780,	0.1598 s / batch. (data: 2.37e-04). ETA=9:30:30, max mem: 4.0 GB 
[11/07 00:02:31 visual_prompt]: 	Training 400/2212. train loss: 1.4136,	0.2122 s / batch. (data: 7.20e-04). ETA=12:37:27, max mem: 4.0 GB 
[11/07 00:02:50 visual_prompt]: 	Training 500/2212. train loss: 0.4189,	0.1625 s / batch. (data: 5.34e-03). ETA=9:39:47, max mem: 4.0 GB 
[11/07 00:03:10 visual_prompt]: 	Training 600/2212. train loss: 0.3889,	0.1446 s / batch. (data: 6.72e-04). ETA=8:35:30, max mem: 4.0 GB 
[11/07 00:03:30 visual_prompt]: 	Training 700/2212. train loss: 0.8303,	0.2081 s / batch. (data: 1.04e-02). ETA=12:21:40, max mem: 4.0 GB 
[11/07 00:03:51 visual_prompt]: 	Training 800/2212. train loss: 1.9113,	0.1608 s / batch. (data: 1.04e-02). ETA=9:32:54, max mem: 4.0 GB 
[11/07 00:04:10 visual_prompt]: 	Training 900/2212. train loss: 0.4549,	0.1968 s / batch. (data: 5.34e-03). ETA=11:40:56, max mem: 4.0 GB 
[11/07 00:04:30 visual_prompt]: 	Training 1000/2212. train loss: 0.8969,	0.1927 s / batch. (data: 5.73e-03). ETA=11:25:55, max mem: 4.0 GB 
[11/07 00:04:49 visual_prompt]: 	Training 1100/2212. train loss: 2.6368,	0.1971 s / batch. (data: 5.32e-03). ETA=11:41:22, max mem: 4.0 GB 
[11/07 00:05:09 visual_prompt]: 	Training 1200/2212. train loss: 0.4055,	0.1803 s / batch. (data: 6.86e-04). ETA=10:41:13, max mem: 4.0 GB 
[11/07 00:05:29 visual_prompt]: 	Training 1300/2212. train loss: 0.8095,	0.1826 s / batch. (data: 1.54e-02). ETA=10:48:55, max mem: 4.0 GB 
[11/07 00:05:49 visual_prompt]: 	Training 1400/2212. train loss: 0.6338,	0.2207 s / batch. (data: 1.59e-02). ETA=13:04:11, max mem: 4.0 GB 
[11/07 00:06:10 visual_prompt]: 	Training 1500/2212. train loss: 0.5614,	0.1593 s / batch. (data: 1.43e-02). ETA=9:25:37, max mem: 4.0 GB 
[11/07 00:06:29 visual_prompt]: 	Training 1600/2212. train loss: 0.2994,	0.1766 s / batch. (data: 1.78e-02). ETA=10:26:45, max mem: 4.0 GB 
[11/07 00:06:49 visual_prompt]: 	Training 1700/2212. train loss: 0.0828,	0.1559 s / batch. (data: 1.03e-02). ETA=9:12:58, max mem: 4.0 GB 
[11/07 00:07:08 visual_prompt]: 	Training 1800/2212. train loss: 0.9597,	0.1889 s / batch. (data: 1.04e-02). ETA=11:09:58, max mem: 4.0 GB 
[11/07 00:07:29 visual_prompt]: 	Training 1900/2212. train loss: 0.6607,	0.1780 s / batch. (data: 1.04e-02). ETA=10:30:49, max mem: 4.0 GB 
[11/07 00:07:50 visual_prompt]: 	Training 2000/2212. train loss: 1.0044,	0.1871 s / batch. (data: 1.05e-02). ETA=11:02:56, max mem: 4.0 GB 
[11/07 00:08:12 visual_prompt]: 	Training 2100/2212. train loss: 0.9435,	0.1907 s / batch. (data: 2.81e-03). ETA=11:15:08, max mem: 4.0 GB 
[11/07 00:08:33 visual_prompt]: 	Training 2200/2212. train loss: 0.3127,	0.1719 s / batch. (data: 1.22e-04). ETA=10:08:22, max mem: 4.0 GB 
[11/07 00:08:34 visual_prompt]: Epoch 4 / 100: avg data time: 3.04e-02, avg batch time: 0.2010, average train loss: 0.8940
[11/07 00:08:54 visual_prompt]: 	Test 100/246. loss: 1.190, 0.0444 s / batch. (data: 2.17e-05)max mem: 4.00127 GB 
[11/07 00:09:12 visual_prompt]: 	Test 200/246. loss: 1.188, 0.0875 s / batch. (data: 2.34e-05)max mem: 4.00127 GB 
[11/07 00:09:20 visual_prompt]: Inference (val):avg data time: 3.82e-04, avg batch time: 0.0584, average loss: 0.7343
[11/07 00:09:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.18	
[11/07 00:09:20 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/07 00:09:43 visual_prompt]: 	Training 100/2212. train loss: 2.0024,	0.1892 s / batch. (data: 2.31e-04). ETA=11:09:26, max mem: 4.0 GB 
[11/07 00:10:04 visual_prompt]: 	Training 200/2212. train loss: 1.0589,	0.1483 s / batch. (data: 2.62e-04). ETA=8:44:16, max mem: 4.0 GB 
[11/07 00:10:25 visual_prompt]: 	Training 300/2212. train loss: 1.5058,	0.1855 s / batch. (data: 2.49e-04). ETA=10:55:42, max mem: 4.0 GB 
[11/07 00:10:48 visual_prompt]: 	Training 400/2212. train loss: 2.6446,	0.1825 s / batch. (data: 2.63e-04). ETA=10:44:50, max mem: 4.0 GB 
[11/07 00:11:09 visual_prompt]: 	Training 500/2212. train loss: 0.6108,	0.1595 s / batch. (data: 1.04e-02). ETA=9:23:15, max mem: 4.0 GB 
[11/07 00:11:31 visual_prompt]: 	Training 600/2212. train loss: 0.6044,	0.1981 s / batch. (data: 5.95e-04). ETA=11:39:05, max mem: 4.0 GB 
[11/07 00:11:50 visual_prompt]: 	Training 700/2212. train loss: 1.1370,	0.1666 s / batch. (data: 2.66e-04). ETA=9:47:42, max mem: 4.0 GB 
[11/07 00:12:13 visual_prompt]: 	Training 800/2212. train loss: 0.5491,	0.2105 s / batch. (data: 6.43e-02). ETA=12:22:09, max mem: 4.0 GB 
[11/07 00:12:34 visual_prompt]: 	Training 900/2212. train loss: 0.4044,	0.2211 s / batch. (data: 6.85e-04). ETA=12:59:11, max mem: 4.0 GB 
[11/07 00:12:55 visual_prompt]: 	Training 1000/2212. train loss: 0.8233,	0.4460 s / batch. (data: 2.70e-01). ETA=1 day, 2:11:09, max mem: 4.0 GB 
[11/07 00:13:16 visual_prompt]: 	Training 1100/2212. train loss: 0.5330,	0.2203 s / batch. (data: 5.79e-03). ETA=12:55:41, max mem: 4.0 GB 
[11/07 00:13:37 visual_prompt]: 	Training 1200/2212. train loss: 0.9785,	0.2257 s / batch. (data: 2.16e-02). ETA=13:14:16, max mem: 4.0 GB 
[11/07 00:13:57 visual_prompt]: 	Training 1300/2212. train loss: 0.3873,	0.1580 s / batch. (data: 6.18e-04). ETA=9:15:45, max mem: 4.0 GB 
[11/07 00:14:19 visual_prompt]: 	Training 1400/2212. train loss: 0.6895,	0.1986 s / batch. (data: 5.80e-03). ETA=11:38:20, max mem: 4.0 GB 
[11/07 00:14:40 visual_prompt]: 	Training 1500/2212. train loss: 1.2190,	0.7389 s / batch. (data: 5.96e-01). ETA=1 day, 19:16:41, max mem: 4.0 GB 
[11/07 00:15:02 visual_prompt]: 	Training 1600/2212. train loss: 0.7691,	0.1870 s / batch. (data: 5.78e-03). ETA=10:56:57, max mem: 4.0 GB 
[11/07 00:15:24 visual_prompt]: 	Training 1700/2212. train loss: 0.6337,	0.1826 s / batch. (data: 7.01e-04). ETA=10:41:07, max mem: 4.0 GB 
[11/07 00:15:46 visual_prompt]: 	Training 1800/2212. train loss: 0.7414,	0.2533 s / batch. (data: 2.11e-02). ETA=14:49:00, max mem: 4.0 GB 
[11/07 00:16:08 visual_prompt]: 	Training 1900/2212. train loss: 0.7196,	0.1937 s / batch. (data: 1.09e-02). ETA=11:19:25, max mem: 4.0 GB 
[11/07 00:16:28 visual_prompt]: 	Training 2000/2212. train loss: 0.6091,	0.1769 s / batch. (data: 2.06e-02). ETA=10:20:16, max mem: 4.0 GB 
[11/07 00:16:49 visual_prompt]: 	Training 2100/2212. train loss: 0.6745,	0.2234 s / batch. (data: 1.95e-02). ETA=13:02:56, max mem: 4.0 GB 
[11/07 00:17:10 visual_prompt]: 	Training 2200/2212. train loss: 0.7071,	0.0882 s / batch. (data: 1.17e-04). ETA=5:08:58, max mem: 4.0 GB 
[11/07 00:17:11 visual_prompt]: Epoch 5 / 100: avg data time: 2.89e-02, avg batch time: 0.2128, average train loss: 0.7450
[11/07 00:17:32 visual_prompt]: 	Test 100/246. loss: 0.748, 0.1010 s / batch. (data: 2.96e-05)max mem: 4.00127 GB 
[11/07 00:17:52 visual_prompt]: 	Test 200/246. loss: 0.748, 0.0713 s / batch. (data: 4.55e-05)max mem: 4.00127 GB 
[11/07 00:18:00 visual_prompt]: Inference (val):avg data time: 4.12e-04, avg batch time: 0.0642, average loss: 0.6893
[11/07 00:18:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.04	
[11/07 00:18:00 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/07 00:18:23 visual_prompt]: 	Training 100/2212. train loss: 2.2765,	0.2467 s / batch. (data: 2.56e-02). ETA=14:23:30, max mem: 4.0 GB 
[11/07 00:18:46 visual_prompt]: 	Training 200/2212. train loss: 0.6401,	0.2078 s / batch. (data: 1.04e-02). ETA=12:07:10, max mem: 4.0 GB 
[11/07 00:19:07 visual_prompt]: 	Training 300/2212. train loss: 0.4012,	0.1807 s / batch. (data: 2.99e-04). ETA=10:31:59, max mem: 4.0 GB 
[11/07 00:19:27 visual_prompt]: 	Training 400/2212. train loss: 0.6135,	0.1828 s / batch. (data: 1.04e-02). ETA=10:39:05, max mem: 4.0 GB 
[11/07 00:19:49 visual_prompt]: 	Training 500/2212. train loss: 0.5928,	0.1959 s / batch. (data: 1.04e-02). ETA=11:24:38, max mem: 4.0 GB 
[11/07 00:20:11 visual_prompt]: 	Training 600/2212. train loss: 0.6834,	0.1962 s / batch. (data: 5.71e-03). ETA=11:25:12, max mem: 4.0 GB 
[11/07 00:20:33 visual_prompt]: 	Training 700/2212. train loss: 0.5909,	0.2113 s / batch. (data: 2.20e-02). ETA=12:17:44, max mem: 4.0 GB 
[11/07 00:20:54 visual_prompt]: 	Training 800/2212. train loss: 0.7881,	0.1577 s / batch. (data: 1.04e-02). ETA=9:10:21, max mem: 4.0 GB 
[11/07 00:21:15 visual_prompt]: 	Training 900/2212. train loss: 0.7856,	0.2022 s / batch. (data: 2.68e-04). ETA=11:45:10, max mem: 4.0 GB 
[11/07 00:21:37 visual_prompt]: 	Training 1000/2212. train loss: 0.7910,	0.2227 s / batch. (data: 3.37e-02). ETA=12:56:17, max mem: 4.0 GB 
[11/07 00:21:58 visual_prompt]: 	Training 1100/2212. train loss: 0.5982,	0.2225 s / batch. (data: 1.56e-02). ETA=12:55:06, max mem: 4.0 GB 
[11/07 00:22:17 visual_prompt]: 	Training 1200/2212. train loss: 0.6075,	0.2274 s / batch. (data: 5.35e-03). ETA=13:12:02, max mem: 4.0 GB 
[11/07 00:22:39 visual_prompt]: 	Training 1300/2212. train loss: 0.5702,	0.1742 s / batch. (data: 3.04e-04). ETA=10:06:18, max mem: 4.0 GB 
[11/07 00:23:01 visual_prompt]: 	Training 1400/2212. train loss: 0.4948,	0.2251 s / batch. (data: 2.05e-02). ETA=13:03:17, max mem: 4.0 GB 
[11/07 00:23:22 visual_prompt]: 	Training 1500/2212. train loss: 0.6284,	0.1635 s / batch. (data: 6.75e-03). ETA=9:28:24, max mem: 4.0 GB 
[11/07 00:23:43 visual_prompt]: 	Training 1600/2212. train loss: 0.7654,	0.1839 s / batch. (data: 3.49e-02). ETA=10:39:12, max mem: 4.0 GB 
[11/07 00:24:05 visual_prompt]: 	Training 1700/2212. train loss: 0.9046,	0.2063 s / batch. (data: 8.28e-03). ETA=11:56:48, max mem: 4.0 GB 
[11/07 00:24:27 visual_prompt]: 	Training 1800/2212. train loss: 0.6844,	0.2183 s / batch. (data: 1.08e-02). ETA=12:38:08, max mem: 4.0 GB 
[11/07 00:24:48 visual_prompt]: 	Training 1900/2212. train loss: 0.5398,	0.2210 s / batch. (data: 1.55e-02). ETA=12:47:01, max mem: 4.0 GB 
[11/07 00:25:08 visual_prompt]: 	Training 2000/2212. train loss: 0.9050,	0.1644 s / batch. (data: 1.17e-02). ETA=9:30:27, max mem: 4.0 GB 
[11/07 00:25:28 visual_prompt]: 	Training 2100/2212. train loss: 0.8501,	0.3139 s / batch. (data: 1.37e-01). ETA=18:08:30, max mem: 4.0 GB 
[11/07 00:25:50 visual_prompt]: 	Training 2200/2212. train loss: 0.6358,	0.1181 s / batch. (data: 1.47e-04). ETA=6:49:21, max mem: 4.0 GB 
[11/07 00:25:51 visual_prompt]: Epoch 6 / 100: avg data time: 2.94e-02, avg batch time: 0.2130, average train loss: 0.7096
[11/07 00:26:12 visual_prompt]: 	Test 100/246. loss: 0.761, 0.0861 s / batch. (data: 2.72e-05)max mem: 4.00127 GB 
[11/07 00:26:32 visual_prompt]: 	Test 200/246. loss: 0.761, 0.0653 s / batch. (data: 3.19e-05)max mem: 4.00127 GB 
[11/07 00:26:40 visual_prompt]: Inference (val):avg data time: 2.80e-04, avg batch time: 0.0636, average loss: 0.6889
[11/07 00:26:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.95	
[11/07 00:26:40 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/07 00:27:03 visual_prompt]: 	Training 100/2212. train loss: 0.4900,	0.2587 s / batch. (data: 1.59e-02). ETA=14:55:58, max mem: 4.0 GB 
[11/07 00:27:26 visual_prompt]: 	Training 200/2212. train loss: 0.3997,	0.1632 s / batch. (data: 7.03e-04). ETA=9:25:05, max mem: 4.0 GB 
[11/07 00:27:49 visual_prompt]: 	Training 300/2212. train loss: 0.8664,	0.1807 s / batch. (data: 1.08e-02). ETA=10:25:11, max mem: 4.0 GB 
[11/07 00:28:11 visual_prompt]: 	Training 400/2212. train loss: 0.8413,	0.1856 s / batch. (data: 2.62e-04). ETA=10:42:06, max mem: 4.0 GB 
[11/07 00:28:33 visual_prompt]: 	Training 500/2212. train loss: 0.7022,	0.1779 s / batch. (data: 2.40e-04). ETA=10:14:55, max mem: 4.0 GB 
[11/07 00:28:54 visual_prompt]: 	Training 600/2212. train loss: 0.6897,	0.1779 s / batch. (data: 6.93e-03). ETA=10:14:47, max mem: 4.0 GB 
[11/07 00:29:15 visual_prompt]: 	Training 700/2212. train loss: 0.6029,	0.2132 s / batch. (data: 1.59e-02). ETA=12:16:22, max mem: 4.0 GB 
[11/07 00:29:36 visual_prompt]: 	Training 800/2212. train loss: 0.6255,	0.2331 s / batch. (data: 1.55e-02). ETA=13:24:47, max mem: 4.0 GB 
[11/07 00:29:57 visual_prompt]: 	Training 900/2212. train loss: 0.7247,	0.1833 s / batch. (data: 2.64e-04). ETA=10:32:28, max mem: 4.0 GB 
[11/07 00:30:18 visual_prompt]: 	Training 1000/2212. train loss: 0.6855,	0.2213 s / batch. (data: 2.35e-03). ETA=12:43:12, max mem: 4.0 GB 
[11/07 00:30:39 visual_prompt]: 	Training 1100/2212. train loss: 0.7812,	0.1744 s / batch. (data: 5.39e-03). ETA=10:01:01, max mem: 4.0 GB 
[11/07 00:30:58 visual_prompt]: 	Training 1200/2212. train loss: 0.5800,	0.1683 s / batch. (data: 1.04e-02). ETA=9:39:49, max mem: 4.0 GB 
[11/07 00:31:20 visual_prompt]: 	Training 1300/2212. train loss: 0.6494,	0.1243 s / batch. (data: 1.01e-02). ETA=7:08:10, max mem: 4.0 GB 
[11/07 00:31:41 visual_prompt]: 	Training 1400/2212. train loss: 0.6007,	0.2162 s / batch. (data: 5.54e-03). ETA=12:24:10, max mem: 4.0 GB 
[11/07 00:32:02 visual_prompt]: 	Training 1500/2212. train loss: 0.9692,	0.2291 s / batch. (data: 1.59e-02). ETA=13:08:16, max mem: 4.0 GB 
[11/07 00:32:23 visual_prompt]: 	Training 1600/2212. train loss: 0.5862,	0.2190 s / batch. (data: 1.55e-02). ETA=12:32:57, max mem: 4.0 GB 
[11/07 00:32:45 visual_prompt]: 	Training 1700/2212. train loss: 0.9012,	0.2295 s / batch. (data: 5.80e-03). ETA=13:08:47, max mem: 4.0 GB 
[11/07 00:33:05 visual_prompt]: 	Training 1800/2212. train loss: 0.5723,	0.2112 s / batch. (data: 7.50e-04). ETA=12:05:31, max mem: 4.0 GB 
[11/07 00:33:27 visual_prompt]: 	Training 1900/2212. train loss: 0.8254,	0.2652 s / batch. (data: 2.10e-02). ETA=15:10:41, max mem: 4.0 GB 
[11/07 00:33:48 visual_prompt]: 	Training 2000/2212. train loss: 0.7260,	0.1803 s / batch. (data: 5.45e-03). ETA=10:18:44, max mem: 4.0 GB 
[11/07 00:34:09 visual_prompt]: 	Training 2100/2212. train loss: 0.9467,	0.2216 s / batch. (data: 1.55e-02). ETA=12:40:09, max mem: 4.0 GB 
[11/07 00:34:30 visual_prompt]: 	Training 2200/2212. train loss: 0.5506,	0.1432 s / batch. (data: 9.04e-05). ETA=8:10:53, max mem: 4.0 GB 
[11/07 00:34:31 visual_prompt]: Epoch 7 / 100: avg data time: 2.72e-02, avg batch time: 0.2129, average train loss: 0.7028
[11/07 00:34:52 visual_prompt]: 	Test 100/246. loss: 0.838, 0.0799 s / batch. (data: 2.62e-05)max mem: 4.00127 GB 
[11/07 00:35:12 visual_prompt]: 	Test 200/246. loss: 0.838, 0.0610 s / batch. (data: 4.36e-05)max mem: 4.00127 GB 
[11/07 00:35:20 visual_prompt]: Inference (val):avg data time: 1.40e-04, avg batch time: 0.0638, average loss: 0.6891
[11/07 00:35:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.06	
[11/07 00:35:20 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/07 00:35:44 visual_prompt]: 	Training 100/2212. train loss: 0.5098,	0.2358 s / batch. (data: 1.55e-02). ETA=13:27:59, max mem: 4.0 GB 
[11/07 00:36:05 visual_prompt]: 	Training 200/2212. train loss: 0.6621,	0.1641 s / batch. (data: 1.04e-02). ETA=9:22:13, max mem: 4.0 GB 
[11/07 00:36:26 visual_prompt]: 	Training 300/2212. train loss: 0.9167,	0.2037 s / batch. (data: 1.55e-02). ETA=11:37:13, max mem: 4.0 GB 
[11/07 00:36:48 visual_prompt]: 	Training 400/2212. train loss: 0.5255,	0.2319 s / batch. (data: 2.18e-04). ETA=13:13:40, max mem: 4.0 GB 
[11/07 00:37:11 visual_prompt]: 	Training 500/2212. train loss: 0.5267,	0.3847 s / batch. (data: 2.63e-01). ETA=21:55:41, max mem: 4.0 GB 
[11/07 00:37:32 visual_prompt]: 	Training 600/2212. train loss: 0.8026,	0.1794 s / batch. (data: 1.12e-03). ETA=10:13:22, max mem: 4.0 GB 
[11/07 00:37:52 visual_prompt]: 	Training 700/2212. train loss: 0.9116,	0.2020 s / batch. (data: 3.10e-02). ETA=11:30:08, max mem: 4.0 GB 
[11/07 00:38:13 visual_prompt]: 	Training 800/2212. train loss: 0.4800,	0.1997 s / batch. (data: 5.81e-03). ETA=11:21:54, max mem: 4.0 GB 
[11/07 00:38:35 visual_prompt]: 	Training 900/2212. train loss: 0.5827,	0.1817 s / batch. (data: 2.70e-04). ETA=10:20:21, max mem: 4.0 GB 
[11/07 00:38:56 visual_prompt]: 	Training 1000/2212. train loss: 0.9213,	0.1744 s / batch. (data: 1.56e-02). ETA=9:54:57, max mem: 4.0 GB 
[11/07 00:39:17 visual_prompt]: 	Training 1100/2212. train loss: 0.5811,	0.1698 s / batch. (data: 2.52e-02). ETA=9:39:00, max mem: 4.0 GB 
[11/07 00:39:39 visual_prompt]: 	Training 1200/2212. train loss: 0.6265,	0.2418 s / batch. (data: 1.86e-02). ETA=13:44:06, max mem: 4.0 GB 
[11/07 00:40:01 visual_prompt]: 	Training 1300/2212. train loss: 0.7631,	0.2426 s / batch. (data: 1.19e-02). ETA=13:46:41, max mem: 4.0 GB 
[11/07 00:40:21 visual_prompt]: 	Training 1400/2212. train loss: 0.7888,	0.2262 s / batch. (data: 5.38e-03). ETA=12:50:08, max mem: 4.0 GB 
[11/07 00:40:44 visual_prompt]: 	Training 1500/2212. train loss: 0.5911,	0.3034 s / batch. (data: 1.98e-01). ETA=17:12:46, max mem: 4.0 GB 
[11/07 00:41:05 visual_prompt]: 	Training 1600/2212. train loss: 0.8908,	0.1869 s / batch. (data: 1.50e-02). ETA=10:35:54, max mem: 4.0 GB 
[11/07 00:41:26 visual_prompt]: 	Training 1700/2212. train loss: 0.7768,	0.1829 s / batch. (data: 1.05e-02). ETA=10:21:50, max mem: 4.0 GB 
[11/07 00:41:47 visual_prompt]: 	Training 1800/2212. train loss: 0.5397,	0.1432 s / batch. (data: 1.05e-02). ETA=8:06:47, max mem: 4.0 GB 
[11/07 00:42:08 visual_prompt]: 	Training 1900/2212. train loss: 0.6120,	0.2589 s / batch. (data: 1.59e-02). ETA=14:39:36, max mem: 4.0 GB 
[11/07 00:42:30 visual_prompt]: 	Training 2000/2212. train loss: 1.7032,	0.2115 s / batch. (data: 5.39e-03). ETA=11:58:03, max mem: 4.0 GB 
[11/07 00:42:51 visual_prompt]: 	Training 2100/2212. train loss: 0.5217,	0.1756 s / batch. (data: 1.67e-02). ETA=9:55:56, max mem: 4.0 GB 
[11/07 00:43:11 visual_prompt]: 	Training 2200/2212. train loss: 0.7590,	0.0491 s / batch. (data: 1.38e-04). ETA=2:46:33, max mem: 4.0 GB 
[11/07 00:43:12 visual_prompt]: Epoch 8 / 100: avg data time: 2.79e-02, avg batch time: 0.2132, average train loss: 0.7053
[11/07 00:43:33 visual_prompt]: 	Test 100/246. loss: 0.766, 0.0909 s / batch. (data: 2.67e-05)max mem: 4.00127 GB 
[11/07 00:43:53 visual_prompt]: 	Test 200/246. loss: 0.766, 0.0995 s / batch. (data: 1.93e-05)max mem: 4.00127 GB 
[11/07 00:44:01 visual_prompt]: Inference (val):avg data time: 7.64e-05, avg batch time: 0.0621, average loss: 0.6888
[11/07 00:44:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.01	
[11/07 00:44:01 visual_prompt]: Best epoch 8: best metric: -0.689
[11/07 00:44:01 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/07 00:44:25 visual_prompt]: 	Training 100/2212. train loss: 0.6648,	0.1143 s / batch. (data: 1.85e-04). ETA=6:27:34, max mem: 4.0 GB 
[11/07 00:44:48 visual_prompt]: 	Training 200/2212. train loss: 0.6465,	0.1728 s / batch. (data: 5.36e-03). ETA=9:45:40, max mem: 4.0 GB 
[11/07 00:45:11 visual_prompt]: 	Training 300/2212. train loss: 0.8666,	0.1643 s / batch. (data: 2.56e-04). ETA=9:16:18, max mem: 4.0 GB 
[11/07 00:45:32 visual_prompt]: 	Training 400/2212. train loss: 0.7803,	0.2083 s / batch. (data: 1.55e-02). ETA=11:45:13, max mem: 4.0 GB 
[11/07 00:45:54 visual_prompt]: 	Training 500/2212. train loss: 0.7996,	0.1772 s / batch. (data: 6.82e-04). ETA=9:59:25, max mem: 4.0 GB 
[11/07 00:46:16 visual_prompt]: 	Training 600/2212. train loss: 0.5202,	0.2051 s / batch. (data: 1.90e-02). ETA=11:33:27, max mem: 4.0 GB 
[11/07 00:46:37 visual_prompt]: 	Training 700/2212. train loss: 0.3972,	0.1806 s / batch. (data: 2.05e-02). ETA=10:10:20, max mem: 4.0 GB 
[11/07 00:46:57 visual_prompt]: 	Training 800/2212. train loss: 0.7254,	0.2227 s / batch. (data: 1.09e-02). ETA=12:32:22, max mem: 4.0 GB 
[11/07 00:47:18 visual_prompt]: 	Training 900/2212. train loss: 0.6554,	0.1911 s / batch. (data: 2.58e-04). ETA=10:45:25, max mem: 4.0 GB 
[11/07 00:47:39 visual_prompt]: 	Training 1000/2212. train loss: 0.8025,	0.1769 s / batch. (data: 5.36e-03). ETA=9:57:11, max mem: 4.0 GB 
[11/07 00:48:00 visual_prompt]: 	Training 1100/2212. train loss: 0.5062,	0.4220 s / batch. (data: 2.05e-01). ETA=23:43:42, max mem: 4.0 GB 
[11/07 00:48:21 visual_prompt]: 	Training 1200/2212. train loss: 1.0264,	0.1825 s / batch. (data: 1.42e-02). ETA=10:15:23, max mem: 4.0 GB 
[11/07 00:48:41 visual_prompt]: 	Training 1300/2212. train loss: 0.7627,	0.1983 s / batch. (data: 1.26e-02). ETA=11:08:17, max mem: 4.0 GB 
[11/07 00:49:01 visual_prompt]: 	Training 1400/2212. train loss: 0.7527,	0.1966 s / batch. (data: 1.37e-02). ETA=11:02:20, max mem: 4.0 GB 
[11/07 00:49:21 visual_prompt]: 	Training 1500/2212. train loss: 0.9318,	0.1864 s / batch. (data: 6.49e-04). ETA=10:27:29, max mem: 4.0 GB 
[11/07 00:49:42 visual_prompt]: 	Training 1600/2212. train loss: 0.6495,	0.1915 s / batch. (data: 1.47e-02). ETA=10:44:17, max mem: 4.0 GB 
[11/07 00:50:02 visual_prompt]: 	Training 1700/2212. train loss: 1.4395,	0.2060 s / batch. (data: 7.36e-03). ETA=11:32:45, max mem: 4.0 GB 
[11/07 00:50:21 visual_prompt]: 	Training 1800/2212. train loss: 0.7318,	0.3068 s / batch. (data: 1.83e-01). ETA=17:11:29, max mem: 4.0 GB 
[11/07 00:50:39 visual_prompt]: 	Training 1900/2212. train loss: 0.7979,	0.1503 s / batch. (data: 1.57e-02). ETA=8:25:00, max mem: 4.0 GB 
[11/07 00:51:00 visual_prompt]: 	Training 2000/2212. train loss: 0.8055,	0.1905 s / batch. (data: 5.41e-03). ETA=10:39:41, max mem: 4.0 GB 
[11/07 00:51:22 visual_prompt]: 	Training 2100/2212. train loss: 0.8031,	0.2123 s / batch. (data: 5.42e-03). ETA=11:52:34, max mem: 4.0 GB 
[11/07 00:51:44 visual_prompt]: 	Training 2200/2212. train loss: 0.5406,	0.1682 s / batch. (data: 8.70e-05). ETA=9:24:25, max mem: 4.0 GB 
[11/07 00:51:45 visual_prompt]: Epoch 9 / 100: avg data time: 3.11e-02, avg batch time: 0.2097, average train loss: 0.7082
[11/07 00:52:05 visual_prompt]: 	Test 100/246. loss: 0.927, 0.0714 s / batch. (data: 2.53e-05)max mem: 4.00127 GB 
[11/07 00:52:25 visual_prompt]: 	Test 200/246. loss: 0.927, 0.1009 s / batch. (data: 3.74e-05)max mem: 4.00127 GB 
[11/07 00:52:33 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.0585, average loss: 0.6947
[11/07 00:52:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.05	
[11/07 00:52:33 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/07 00:52:57 visual_prompt]: 	Training 100/2212. train loss: 0.5736,	0.1665 s / batch. (data: 1.55e-02). ETA=9:18:20, max mem: 4.0 GB 
[11/07 00:53:19 visual_prompt]: 	Training 200/2212. train loss: 0.8301,	0.4356 s / batch. (data: 3.12e-01). ETA=1 day, 0:19:48, max mem: 4.0 GB 
[11/07 00:53:40 visual_prompt]: 	Training 300/2212. train loss: 0.9691,	0.1991 s / batch. (data: 4.07e-03). ETA=11:06:52, max mem: 4.0 GB 
[11/07 00:54:01 visual_prompt]: 	Training 400/2212. train loss: 0.7929,	0.1914 s / batch. (data: 5.36e-03). ETA=10:40:41, max mem: 4.0 GB 
[11/07 00:54:21 visual_prompt]: 	Training 500/2212. train loss: 0.4732,	0.2301 s / batch. (data: 1.04e-02). ETA=12:49:55, max mem: 4.0 GB 
[11/07 00:54:43 visual_prompt]: 	Training 600/2212. train loss: 0.9828,	0.1942 s / batch. (data: 5.79e-03). ETA=10:49:37, max mem: 4.0 GB 
[11/07 00:55:05 visual_prompt]: 	Training 700/2212. train loss: 0.5239,	0.1614 s / batch. (data: 2.73e-04). ETA=8:59:41, max mem: 4.0 GB 
[11/07 00:55:27 visual_prompt]: 	Training 800/2212. train loss: 0.7443,	0.2793 s / batch. (data: 4.55e-02). ETA=15:33:10, max mem: 4.0 GB 
[11/07 00:55:48 visual_prompt]: 	Training 900/2212. train loss: 0.2362,	0.7634 s / batch. (data: 6.35e-01). ETA=1 day, 18:29:29, max mem: 4.0 GB 
[11/07 00:56:09 visual_prompt]: 	Training 1000/2212. train loss: 0.6700,	0.1672 s / batch. (data: 1.52e-02). ETA=9:18:15, max mem: 4.0 GB 
[11/07 00:56:31 visual_prompt]: 	Training 1100/2212. train loss: 0.7570,	0.1730 s / batch. (data: 2.06e-02). ETA=9:37:17, max mem: 4.0 GB 
[11/07 00:56:52 visual_prompt]: 	Training 1200/2212. train loss: 0.7626,	0.1976 s / batch. (data: 2.92e-04). ETA=10:58:53, max mem: 4.0 GB 
[11/07 00:57:13 visual_prompt]: 	Training 1300/2212. train loss: 0.7817,	0.1860 s / batch. (data: 1.55e-02). ETA=10:20:07, max mem: 4.0 GB 
[11/07 00:57:34 visual_prompt]: 	Training 1400/2212. train loss: 1.9742,	0.1847 s / batch. (data: 2.28e-04). ETA=10:15:25, max mem: 4.0 GB 
[11/07 00:57:55 visual_prompt]: 	Training 1500/2212. train loss: 0.6086,	0.2004 s / batch. (data: 3.59e-02). ETA=11:07:15, max mem: 4.0 GB 
[11/07 00:58:16 visual_prompt]: 	Training 1600/2212. train loss: 0.6166,	0.1981 s / batch. (data: 1.55e-02). ETA=10:59:21, max mem: 4.0 GB 
[11/07 00:58:37 visual_prompt]: 	Training 1700/2212. train loss: 0.5784,	0.1532 s / batch. (data: 7.48e-03). ETA=8:29:44, max mem: 4.0 GB 
[11/07 00:58:59 visual_prompt]: 	Training 1800/2212. train loss: 0.9025,	0.1619 s / batch. (data: 1.04e-02). ETA=8:58:18, max mem: 4.0 GB 
[11/07 00:59:20 visual_prompt]: 	Training 1900/2212. train loss: 0.6273,	0.2375 s / batch. (data: 1.09e-02). ETA=13:09:21, max mem: 4.0 GB 
[11/07 00:59:41 visual_prompt]: 	Training 2000/2212. train loss: 0.6165,	0.2166 s / batch. (data: 1.55e-02). ETA=11:59:29, max mem: 4.0 GB 
[11/07 01:00:03 visual_prompt]: 	Training 2100/2212. train loss: 0.7117,	0.2024 s / batch. (data: 6.91e-04). ETA=11:12:03, max mem: 4.0 GB 
[11/07 01:00:23 visual_prompt]: 	Training 2200/2212. train loss: 0.6600,	0.1163 s / batch. (data: 1.42e-04). ETA=6:25:52, max mem: 4.0 GB 
[11/07 01:00:25 visual_prompt]: Epoch 10 / 100: avg data time: 2.84e-02, avg batch time: 0.2131, average train loss: 0.7021
[11/07 01:00:46 visual_prompt]: 	Test 100/246. loss: 0.641, 0.0862 s / batch. (data: 3.05e-05)max mem: 4.00127 GB 
[11/07 01:01:06 visual_prompt]: 	Test 200/246. loss: 0.641, 0.0485 s / batch. (data: 2.91e-05)max mem: 4.00127 GB 
[11/07 01:01:14 visual_prompt]: Inference (val):avg data time: 2.43e-04, avg batch time: 0.0636, average loss: 0.6998
[11/07 01:01:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.95	
[11/07 01:01:14 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/07 01:01:38 visual_prompt]: 	Training 100/2212. train loss: 0.5598,	0.2089 s / batch. (data: 1.55e-02). ETA=11:32:52, max mem: 4.0 GB 
[11/07 01:02:00 visual_prompt]: 	Training 200/2212. train loss: 0.6380,	0.2329 s / batch. (data: 1.78e-02). ETA=12:52:01, max mem: 4.0 GB 
[11/07 01:02:22 visual_prompt]: 	Training 300/2212. train loss: 0.8742,	0.2121 s / batch. (data: 2.06e-02). ETA=11:42:38, max mem: 4.0 GB 
[11/07 01:02:44 visual_prompt]: 	Training 400/2212. train loss: 0.8680,	0.1382 s / batch. (data: 1.04e-02). ETA=7:37:39, max mem: 4.0 GB 
[11/07 01:03:05 visual_prompt]: 	Training 500/2212. train loss: 0.6961,	0.1183 s / batch. (data: 5.37e-03). ETA=6:31:31, max mem: 4.0 GB 
[11/07 01:03:26 visual_prompt]: 	Training 600/2212. train loss: 0.7149,	0.1288 s / batch. (data: 6.10e-04). ETA=7:06:02, max mem: 4.0 GB 
[11/07 01:03:49 visual_prompt]: 	Training 700/2212. train loss: 1.0688,	0.8823 s / batch. (data: 7.65e-01). ETA=2 days, 0:37:12, max mem: 4.0 GB 
[11/07 01:04:08 visual_prompt]: 	Training 800/2212. train loss: 0.6716,	0.2084 s / batch. (data: 1.55e-02). ETA=11:28:44, max mem: 4.0 GB 
[11/07 01:04:29 visual_prompt]: 	Training 900/2212. train loss: 0.9117,	0.1467 s / batch. (data: 6.41e-03). ETA=8:04:33, max mem: 4.0 GB 
[11/07 01:04:49 visual_prompt]: 	Training 1000/2212. train loss: 0.6506,	0.1866 s / batch. (data: 3.93e-02). ETA=10:15:58, max mem: 4.0 GB 
[11/07 01:05:11 visual_prompt]: 	Training 1100/2212. train loss: 0.5555,	0.2235 s / batch. (data: 2.56e-02). ETA=12:17:33, max mem: 4.0 GB 
[11/07 01:05:31 visual_prompt]: 	Training 1200/2212. train loss: 0.7581,	0.2134 s / batch. (data: 2.30e-02). ETA=11:43:46, max mem: 4.0 GB 
[11/07 01:05:53 visual_prompt]: 	Training 1300/2212. train loss: 0.6278,	0.2353 s / batch. (data: 5.75e-03). ETA=12:55:34, max mem: 4.0 GB 
[11/07 01:06:14 visual_prompt]: 	Training 1400/2212. train loss: 2.1061,	0.1870 s / batch. (data: 2.44e-02). ETA=10:16:13, max mem: 4.0 GB 
[11/07 01:06:36 visual_prompt]: 	Training 1500/2212. train loss: 0.3091,	0.2286 s / batch. (data: 5.86e-03). ETA=12:32:52, max mem: 4.0 GB 
[11/07 01:06:57 visual_prompt]: 	Training 1600/2212. train loss: 1.0879,	0.1806 s / batch. (data: 5.81e-03). ETA=9:54:16, max mem: 4.0 GB 
[11/07 01:07:18 visual_prompt]: 	Training 1700/2212. train loss: 0.4020,	0.2011 s / batch. (data: 4.55e-03). ETA=11:01:33, max mem: 4.0 GB 
[11/07 01:07:40 visual_prompt]: 	Training 1800/2212. train loss: 0.4553,	0.2140 s / batch. (data: 7.11e-04). ETA=11:43:39, max mem: 4.0 GB 
[11/07 01:08:01 visual_prompt]: 	Training 1900/2212. train loss: 0.5299,	0.2275 s / batch. (data: 2.43e-02). ETA=12:27:46, max mem: 4.0 GB 
[11/07 01:08:23 visual_prompt]: 	Training 2000/2212. train loss: 0.7184,	0.2146 s / batch. (data: 2.46e-02). ETA=11:44:56, max mem: 4.0 GB 
[11/07 01:08:43 visual_prompt]: 	Training 2100/2212. train loss: 0.4503,	0.2202 s / batch. (data: 2.06e-02). ETA=12:03:02, max mem: 4.0 GB 
[11/07 01:09:04 visual_prompt]: 	Training 2200/2212. train loss: 0.8330,	0.1456 s / batch. (data: 1.18e-04). ETA=7:57:47, max mem: 4.0 GB 
[11/07 01:09:05 visual_prompt]: Epoch 11 / 100: avg data time: 2.83e-02, avg batch time: 0.2132, average train loss: 0.7126
[11/07 01:09:27 visual_prompt]: 	Test 100/246. loss: 0.767, 0.0472 s / batch. (data: 3.58e-05)max mem: 4.00127 GB 
[11/07 01:09:47 visual_prompt]: 	Test 200/246. loss: 0.767, 0.0974 s / batch. (data: 3.72e-05)max mem: 4.00127 GB 
[11/07 01:09:55 visual_prompt]: Inference (val):avg data time: 6.04e-05, avg batch time: 0.0641, average loss: 0.6887
[11/07 01:09:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[11/07 01:09:55 visual_prompt]: Best epoch 11: best metric: -0.689
[11/07 01:09:55 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/07 01:10:17 visual_prompt]: 	Training 100/2212. train loss: 0.5227,	0.2110 s / batch. (data: 5.30e-03). ETA=11:31:49, max mem: 4.0 GB 
[11/07 01:10:37 visual_prompt]: 	Training 200/2212. train loss: 0.2015,	0.2280 s / batch. (data: 1.03e-03). ETA=12:27:18, max mem: 4.0 GB 
[11/07 01:11:00 visual_prompt]: 	Training 300/2212. train loss: 0.7183,	0.1702 s / batch. (data: 7.39e-03). ETA=9:17:37, max mem: 4.0 GB 
[11/07 01:11:21 visual_prompt]: 	Training 400/2212. train loss: 0.3812,	0.2243 s / batch. (data: 2.57e-02). ETA=12:14:25, max mem: 4.0 GB 
[11/07 01:11:43 visual_prompt]: 	Training 500/2212. train loss: 0.6161,	0.2101 s / batch. (data: 5.77e-03). ETA=11:27:36, max mem: 4.0 GB 
[11/07 01:12:05 visual_prompt]: 	Training 600/2212. train loss: 0.8490,	0.2128 s / batch. (data: 3.06e-02). ETA=11:36:07, max mem: 4.0 GB 
[11/07 01:12:27 visual_prompt]: 	Training 700/2212. train loss: 0.7026,	0.2077 s / batch. (data: 1.59e-02). ETA=11:19:02, max mem: 4.0 GB 
[11/07 01:12:49 visual_prompt]: 	Training 800/2212. train loss: 1.1023,	0.2073 s / batch. (data: 6.84e-04). ETA=11:17:15, max mem: 4.0 GB 
[11/07 01:13:10 visual_prompt]: 	Training 900/2212. train loss: 0.7598,	0.6284 s / batch. (data: 4.73e-01). ETA=1 day, 10:12:33, max mem: 4.0 GB 
[11/07 01:13:31 visual_prompt]: 	Training 1000/2212. train loss: 0.5182,	0.1677 s / batch. (data: 1.05e-02). ETA=9:07:21, max mem: 4.0 GB 
[11/07 01:13:52 visual_prompt]: 	Training 1100/2212. train loss: 0.7913,	0.1783 s / batch. (data: 2.69e-04). ETA=9:41:49, max mem: 4.0 GB 
[11/07 01:14:13 visual_prompt]: 	Training 1200/2212. train loss: 1.1663,	0.2349 s / batch. (data: 1.09e-02). ETA=12:46:09, max mem: 4.0 GB 
[11/07 01:14:33 visual_prompt]: 	Training 1300/2212. train loss: 0.4215,	0.2744 s / batch. (data: 1.09e-02). ETA=14:54:17, max mem: 4.0 GB 
[11/07 01:14:55 visual_prompt]: 	Training 1400/2212. train loss: 0.5854,	0.1888 s / batch. (data: 1.66e-02). ETA=10:14:58, max mem: 4.0 GB 
[11/07 01:15:17 visual_prompt]: 	Training 1500/2212. train loss: 0.4297,	0.2766 s / batch. (data: 1.50e-01). ETA=15:00:38, max mem: 4.0 GB 
[11/07 01:15:38 visual_prompt]: 	Training 1600/2212. train loss: 0.5904,	0.2369 s / batch. (data: 1.04e-02). ETA=12:51:08, max mem: 4.0 GB 
[11/07 01:15:59 visual_prompt]: 	Training 1700/2212. train loss: 0.8420,	0.2197 s / batch. (data: 3.03e-04). ETA=11:54:44, max mem: 4.0 GB 
[11/07 01:16:21 visual_prompt]: 	Training 1800/2212. train loss: 0.8315,	0.1335 s / batch. (data: 2.27e-04). ETA=7:14:08, max mem: 4.0 GB 
[11/07 01:16:42 visual_prompt]: 	Training 1900/2212. train loss: 0.6329,	0.2589 s / batch. (data: 1.59e-02). ETA=14:01:11, max mem: 4.0 GB 
[11/07 01:17:03 visual_prompt]: 	Training 2000/2212. train loss: 0.9280,	0.1955 s / batch. (data: 2.06e-02). ETA=10:35:04, max mem: 4.0 GB 
[11/07 01:17:24 visual_prompt]: 	Training 2100/2212. train loss: 0.7737,	0.3932 s / batch. (data: 2.41e-01). ETA=21:16:30, max mem: 4.0 GB 
[11/07 01:17:45 visual_prompt]: 	Training 2200/2212. train loss: 0.2947,	0.0909 s / batch. (data: 1.16e-04). ETA=4:54:55, max mem: 4.0 GB 
[11/07 01:17:46 visual_prompt]: Epoch 12 / 100: avg data time: 2.82e-02, avg batch time: 0.2132, average train loss: 0.7091
[11/07 01:18:08 visual_prompt]: 	Test 100/246. loss: 0.608, 0.0660 s / batch. (data: 5.13e-05)max mem: 4.00127 GB 
[11/07 01:18:27 visual_prompt]: 	Test 200/246. loss: 0.608, 0.0574 s / batch. (data: 4.63e-05)max mem: 4.00127 GB 
[11/07 01:18:35 visual_prompt]: Inference (val):avg data time: 5.37e-05, avg batch time: 0.0645, average loss: 0.7057
[11/07 01:18:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.49	
[11/07 01:18:36 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/07 01:19:00 visual_prompt]: 	Training 100/2212. train loss: 0.6781,	0.2162 s / batch. (data: 2.89e-02). ETA=11:40:56, max mem: 4.0 GB 
[11/07 01:19:21 visual_prompt]: 	Training 200/2212. train loss: 0.5952,	0.2583 s / batch. (data: 5.82e-03). ETA=13:57:14, max mem: 4.0 GB 
[11/07 01:19:43 visual_prompt]: 	Training 300/2212. train loss: 0.6473,	0.1422 s / batch. (data: 1.45e-02). ETA=7:40:36, max mem: 4.0 GB 
[11/07 01:20:02 visual_prompt]: 	Training 400/2212. train loss: 1.0364,	0.1700 s / batch. (data: 2.60e-02). ETA=9:10:16, max mem: 4.0 GB 
[11/07 01:20:23 visual_prompt]: 	Training 500/2212. train loss: 0.6946,	0.2380 s / batch. (data: 1.05e-02). ETA=12:50:11, max mem: 4.0 GB 
[11/07 01:20:45 visual_prompt]: 	Training 600/2212. train loss: 0.5273,	0.1939 s / batch. (data: 7.95e-03). ETA=10:27:12, max mem: 4.0 GB 
[11/07 01:21:08 visual_prompt]: 	Training 700/2212. train loss: 0.8498,	0.2181 s / batch. (data: 5.80e-03). ETA=11:44:55, max mem: 4.0 GB 
[11/07 01:21:30 visual_prompt]: 	Training 800/2212. train loss: 0.7060,	0.1697 s / batch. (data: 2.58e-04). ETA=9:08:25, max mem: 4.0 GB 
[11/07 01:21:51 visual_prompt]: 	Training 900/2212. train loss: 0.5326,	0.1886 s / batch. (data: 6.80e-03). ETA=10:09:06, max mem: 4.0 GB 
[11/07 01:22:11 visual_prompt]: 	Training 1000/2212. train loss: 0.5190,	0.1911 s / batch. (data: 2.63e-02). ETA=10:16:53, max mem: 4.0 GB 
[11/07 01:22:33 visual_prompt]: 	Training 1100/2212. train loss: 0.7932,	0.1991 s / batch. (data: 6.89e-04). ETA=10:42:08, max mem: 4.0 GB 
[11/07 01:22:55 visual_prompt]: 	Training 1200/2212. train loss: 0.6742,	0.1948 s / batch. (data: 1.86e-03). ETA=10:27:59, max mem: 4.0 GB 
[11/07 01:23:16 visual_prompt]: 	Training 1300/2212. train loss: 0.9948,	0.2182 s / batch. (data: 1.05e-02). ETA=11:43:16, max mem: 4.0 GB 
[11/07 01:23:37 visual_prompt]: 	Training 1400/2212. train loss: 0.4940,	0.2210 s / batch. (data: 3.17e-02). ETA=11:51:47, max mem: 4.0 GB 
[11/07 01:23:59 visual_prompt]: 	Training 1500/2212. train loss: 0.4261,	0.2147 s / batch. (data: 4.00e-03). ETA=11:31:12, max mem: 4.0 GB 
[11/07 01:24:20 visual_prompt]: 	Training 1600/2212. train loss: 0.6757,	0.2237 s / batch. (data: 1.86e-02). ETA=11:59:49, max mem: 4.0 GB 
[11/07 01:24:41 visual_prompt]: 	Training 1700/2212. train loss: 0.6922,	0.2185 s / batch. (data: 5.78e-03). ETA=11:42:46, max mem: 4.0 GB 
[11/07 01:25:01 visual_prompt]: 	Training 1800/2212. train loss: 1.0629,	0.1876 s / batch. (data: 5.34e-03). ETA=10:03:02, max mem: 4.0 GB 
[11/07 01:25:24 visual_prompt]: 	Training 1900/2212. train loss: 0.6008,	0.2151 s / batch. (data: 5.35e-03). ETA=11:31:06, max mem: 4.0 GB 
[11/07 01:25:44 visual_prompt]: 	Training 2000/2212. train loss: 0.5681,	0.2147 s / batch. (data: 1.04e-02). ETA=11:29:14, max mem: 4.0 GB 
[11/07 01:26:04 visual_prompt]: 	Training 2100/2212. train loss: 0.7812,	0.2089 s / batch. (data: 1.04e-02). ETA=11:10:31, max mem: 4.0 GB 
[11/07 01:26:25 visual_prompt]: 	Training 2200/2212. train loss: 0.6901,	0.1492 s / batch. (data: 1.37e-04). ETA=7:58:38, max mem: 4.0 GB 
[11/07 01:26:27 visual_prompt]: Epoch 13 / 100: avg data time: 2.84e-02, avg batch time: 0.2129, average train loss: 0.7063
[11/07 01:26:48 visual_prompt]: 	Test 100/246. loss: 0.841, 0.1044 s / batch. (data: 2.98e-05)max mem: 4.00127 GB 
[11/07 01:27:08 visual_prompt]: 	Test 200/246. loss: 0.841, 0.0827 s / batch. (data: 2.29e-05)max mem: 4.00127 GB 
[11/07 01:27:16 visual_prompt]: Inference (val):avg data time: 1.29e-04, avg batch time: 0.0631, average loss: 0.6892
[11/07 01:27:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.50	
[11/07 01:27:16 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/07 01:27:39 visual_prompt]: 	Training 100/2212. train loss: 0.9179,	0.1506 s / batch. (data: 2.33e-02). ETA=8:02:54, max mem: 4.0 GB 
[11/07 01:28:01 visual_prompt]: 	Training 200/2212. train loss: 0.7964,	0.1941 s / batch. (data: 1.55e-02). ETA=10:22:00, max mem: 4.0 GB 
[11/07 01:28:24 visual_prompt]: 	Training 300/2212. train loss: 0.6403,	0.1836 s / batch. (data: 5.97e-04). ETA=9:47:58, max mem: 4.0 GB 
[11/07 01:28:45 visual_prompt]: 	Training 400/2212. train loss: 0.7959,	0.1511 s / batch. (data: 3.05e-02). ETA=8:03:39, max mem: 4.0 GB 
[11/07 01:29:06 visual_prompt]: 	Training 500/2212. train loss: 0.8063,	0.2126 s / batch. (data: 1.55e-02). ETA=11:20:15, max mem: 4.0 GB 
[11/07 01:29:29 visual_prompt]: 	Training 600/2212. train loss: 0.4153,	0.2418 s / batch. (data: 2.61e-02). ETA=12:53:17, max mem: 4.0 GB 
[11/07 01:29:51 visual_prompt]: 	Training 700/2212. train loss: 0.7547,	0.1981 s / batch. (data: 5.37e-03). ETA=10:33:00, max mem: 4.0 GB 
[11/07 01:30:13 visual_prompt]: 	Training 800/2212. train loss: 0.6737,	0.2582 s / batch. (data: 1.04e-02). ETA=13:44:45, max mem: 4.0 GB 
[11/07 01:30:33 visual_prompt]: 	Training 900/2212. train loss: 0.7794,	0.2102 s / batch. (data: 1.55e-02). ETA=11:11:00, max mem: 4.0 GB 
[11/07 01:30:54 visual_prompt]: 	Training 1000/2212. train loss: 0.5906,	0.2323 s / batch. (data: 2.54e-02). ETA=12:21:18, max mem: 4.0 GB 
[11/07 01:31:14 visual_prompt]: 	Training 1100/2212. train loss: 1.3446,	0.2250 s / batch. (data: 2.51e-02). ETA=11:57:40, max mem: 4.0 GB 
[11/07 01:31:36 visual_prompt]: 	Training 1200/2212. train loss: 1.1575,	0.3985 s / batch. (data: 2.77e-01). ETA=21:10:20, max mem: 4.0 GB 
[11/07 01:31:56 visual_prompt]: 	Training 1300/2212. train loss: 0.4896,	0.1808 s / batch. (data: 1.04e-02). ETA=9:35:56, max mem: 4.0 GB 
[11/07 01:32:18 visual_prompt]: 	Training 1400/2212. train loss: 1.2204,	0.1855 s / batch. (data: 8.07e-03). ETA=9:50:46, max mem: 4.0 GB 
[11/07 01:32:39 visual_prompt]: 	Training 1500/2212. train loss: 0.5909,	0.2229 s / batch. (data: 2.19e-02). ETA=11:49:14, max mem: 4.0 GB 
[11/07 01:33:00 visual_prompt]: 	Training 1600/2212. train loss: 0.3325,	0.1948 s / batch. (data: 1.55e-02). ETA=10:19:36, max mem: 4.0 GB 
[11/07 01:33:21 visual_prompt]: 	Training 1700/2212. train loss: 0.3346,	0.1934 s / batch. (data: 1.04e-02). ETA=10:14:45, max mem: 4.0 GB 
[11/07 01:33:42 visual_prompt]: 	Training 1800/2212. train loss: 0.9730,	0.2047 s / batch. (data: 6.39e-04). ETA=10:50:32, max mem: 4.0 GB 
[11/07 01:34:04 visual_prompt]: 	Training 1900/2212. train loss: 0.8770,	0.1895 s / batch. (data: 2.91e-04). ETA=10:01:46, max mem: 4.0 GB 
[11/07 01:34:24 visual_prompt]: 	Training 2000/2212. train loss: 0.6216,	0.2033 s / batch. (data: 2.64e-04). ETA=10:45:17, max mem: 4.0 GB 
[11/07 01:34:45 visual_prompt]: 	Training 2100/2212. train loss: 0.7338,	0.1935 s / batch. (data: 2.54e-02). ETA=10:13:50, max mem: 4.0 GB 
[11/07 01:35:06 visual_prompt]: 	Training 2200/2212. train loss: 0.9485,	0.1638 s / batch. (data: 1.93e-04). ETA=8:39:15, max mem: 4.0 GB 
[11/07 01:35:07 visual_prompt]: Epoch 14 / 100: avg data time: 2.78e-02, avg batch time: 0.2129, average train loss: 0.7135
[11/07 01:35:28 visual_prompt]: 	Test 100/246. loss: 0.796, 0.0747 s / batch. (data: 2.72e-05)max mem: 4.00127 GB 
[11/07 01:35:48 visual_prompt]: 	Test 200/246. loss: 0.796, 0.0922 s / batch. (data: 3.05e-05)max mem: 4.00127 GB 
[11/07 01:35:56 visual_prompt]: Inference (val):avg data time: 2.20e-04, avg batch time: 0.0631, average loss: 0.6884
[11/07 01:35:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.01	
[11/07 01:35:56 visual_prompt]: Best epoch 14: best metric: -0.688
[11/07 01:35:56 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/07 01:36:20 visual_prompt]: 	Training 100/2212. train loss: 0.9827,	0.8694 s / batch. (data: 7.71e-01). ETA=1 day, 21:54:58, max mem: 4.0 GB 
[11/07 01:36:43 visual_prompt]: 	Training 200/2212. train loss: 0.9674,	0.8879 s / batch. (data: 7.92e-01). ETA=1 day, 22:52:02, max mem: 4.0 GB 
[11/07 01:37:04 visual_prompt]: 	Training 300/2212. train loss: 0.4364,	0.2139 s / batch. (data: 1.54e-02). ETA=11:17:03, max mem: 4.0 GB 
[11/07 01:37:26 visual_prompt]: 	Training 400/2212. train loss: 0.8346,	0.1762 s / batch. (data: 1.55e-02). ETA=9:17:35, max mem: 4.0 GB 
[11/07 01:37:49 visual_prompt]: 	Training 500/2212. train loss: 0.6326,	0.1698 s / batch. (data: 1.17e-02). ETA=8:56:48, max mem: 4.0 GB 
[11/07 01:38:10 visual_prompt]: 	Training 600/2212. train loss: 0.7905,	0.1811 s / batch. (data: 5.64e-03). ETA=9:32:23, max mem: 4.0 GB 
[11/07 01:38:31 visual_prompt]: 	Training 700/2212. train loss: 0.8404,	0.2000 s / batch. (data: 1.55e-02). ETA=10:31:52, max mem: 4.0 GB 
[11/07 01:38:52 visual_prompt]: 	Training 800/2212. train loss: 0.7943,	0.1963 s / batch. (data: 8.03e-03). ETA=10:19:42, max mem: 4.0 GB 
[11/07 01:39:13 visual_prompt]: 	Training 900/2212. train loss: 0.7568,	0.2067 s / batch. (data: 2.80e-02). ETA=10:52:07, max mem: 4.0 GB 
[11/07 01:39:35 visual_prompt]: 	Training 1000/2212. train loss: 0.7058,	0.1929 s / batch. (data: 1.08e-02). ETA=10:08:18, max mem: 4.0 GB 
[11/07 01:39:56 visual_prompt]: 	Training 1100/2212. train loss: 0.6971,	0.1514 s / batch. (data: 2.51e-04). ETA=7:57:22, max mem: 4.0 GB 
[11/07 01:40:17 visual_prompt]: 	Training 1200/2212. train loss: 0.7374,	0.2124 s / batch. (data: 1.04e-02). ETA=11:09:14, max mem: 4.0 GB 
[11/07 01:40:38 visual_prompt]: 	Training 1300/2212. train loss: 0.5946,	0.2020 s / batch. (data: 2.47e-04). ETA=10:36:01, max mem: 4.0 GB 
[11/07 01:40:59 visual_prompt]: 	Training 1400/2212. train loss: 0.7423,	0.1912 s / batch. (data: 7.19e-04). ETA=10:01:39, max mem: 4.0 GB 
[11/07 01:41:21 visual_prompt]: 	Training 1500/2212. train loss: 0.7588,	0.1777 s / batch. (data: 5.50e-02). ETA=9:18:50, max mem: 4.0 GB 
[11/07 01:41:43 visual_prompt]: 	Training 1600/2212. train loss: 0.4084,	0.1897 s / batch. (data: 2.70e-04). ETA=9:56:18, max mem: 4.0 GB 
[11/07 01:42:03 visual_prompt]: 	Training 1700/2212. train loss: 0.9098,	0.1725 s / batch. (data: 2.39e-04). ETA=9:02:04, max mem: 4.0 GB 
[11/07 01:42:24 visual_prompt]: 	Training 1800/2212. train loss: 0.1303,	0.2146 s / batch. (data: 1.04e-02). ETA=11:14:00, max mem: 4.0 GB 
[11/07 01:42:45 visual_prompt]: 	Training 1900/2212. train loss: 0.3608,	0.2018 s / batch. (data: 2.06e-02). ETA=10:33:29, max mem: 4.0 GB 
[11/07 01:43:07 visual_prompt]: 	Training 2000/2212. train loss: 0.6531,	0.3724 s / batch. (data: 2.83e-01). ETA=19:28:11, max mem: 4.0 GB 
[11/07 01:43:26 visual_prompt]: 	Training 2100/2212. train loss: 0.4810,	0.2286 s / batch. (data: 2.57e-02). ETA=11:56:41, max mem: 4.0 GB 
[11/07 01:43:46 visual_prompt]: 	Training 2200/2212. train loss: 0.5046,	0.1559 s / batch. (data: 1.24e-04). ETA=8:08:33, max mem: 4.0 GB 
[11/07 01:43:47 visual_prompt]: Epoch 15 / 100: avg data time: 2.95e-02, avg batch time: 0.2128, average train loss: 0.7081
[11/07 01:44:08 visual_prompt]: 	Test 100/246. loss: 0.982, 0.0965 s / batch. (data: 2.12e-05)max mem: 4.00127 GB 
[11/07 01:44:28 visual_prompt]: 	Test 200/246. loss: 0.982, 0.0598 s / batch. (data: 3.74e-05)max mem: 4.00127 GB 
[11/07 01:44:36 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.0637, average loss: 0.7007
[11/07 01:44:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.52	
[11/07 01:44:36 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/07 01:45:00 visual_prompt]: 	Training 100/2212. train loss: 0.4627,	0.2166 s / batch. (data: 2.56e-02). ETA=11:18:19, max mem: 4.0 GB 
[11/07 01:45:21 visual_prompt]: 	Training 200/2212. train loss: 0.9713,	0.1744 s / batch. (data: 2.91e-04). ETA=9:05:57, max mem: 4.0 GB 
[11/07 01:45:42 visual_prompt]: 	Training 300/2212. train loss: 0.5494,	0.2501 s / batch. (data: 5.78e-03). ETA=13:02:25, max mem: 4.0 GB 
[11/07 01:46:02 visual_prompt]: 	Training 400/2212. train loss: 0.8426,	0.1410 s / batch. (data: 2.42e-04). ETA=7:20:59, max mem: 4.0 GB 
[11/07 01:46:23 visual_prompt]: 	Training 500/2212. train loss: 0.7331,	0.1879 s / batch. (data: 7.26e-04). ETA=9:47:20, max mem: 4.0 GB 
[11/07 01:46:43 visual_prompt]: 	Training 600/2212. train loss: 0.2765,	0.1808 s / batch. (data: 6.61e-04). ETA=9:24:40, max mem: 4.0 GB 
[11/07 01:47:05 visual_prompt]: 	Training 700/2212. train loss: 0.7017,	0.2345 s / batch. (data: 2.52e-04). ETA=12:12:00, max mem: 4.0 GB 
[11/07 01:47:26 visual_prompt]: 	Training 800/2212. train loss: 0.7397,	0.1471 s / batch. (data: 5.34e-03). ETA=7:39:06, max mem: 4.0 GB 
[11/07 01:47:48 visual_prompt]: 	Training 900/2212. train loss: 0.6238,	0.2422 s / batch. (data: 1.56e-02). ETA=12:35:19, max mem: 4.0 GB 
[11/07 01:48:10 visual_prompt]: 	Training 1000/2212. train loss: 0.8348,	0.3830 s / batch. (data: 2.56e-01). ETA=19:53:42, max mem: 4.0 GB 
[11/07 01:48:32 visual_prompt]: 	Training 1100/2212. train loss: 0.6684,	0.1985 s / batch. (data: 5.40e-03). ETA=10:18:32, max mem: 4.0 GB 
[11/07 01:48:53 visual_prompt]: 	Training 1200/2212. train loss: 0.7489,	0.2210 s / batch. (data: 8.72e-03). ETA=11:28:13, max mem: 4.0 GB 
[11/07 01:49:15 visual_prompt]: 	Training 1300/2212. train loss: 0.7815,	0.2094 s / batch. (data: 2.50e-04). ETA=10:51:38, max mem: 4.0 GB 
[11/07 01:49:37 visual_prompt]: 	Training 1400/2212. train loss: 0.8815,	0.1757 s / batch. (data: 1.04e-02). ETA=9:06:25, max mem: 4.0 GB 
[11/07 01:49:58 visual_prompt]: 	Training 1500/2212. train loss: 0.6301,	0.2168 s / batch. (data: 1.37e-02). ETA=11:13:58, max mem: 4.0 GB 
[11/07 01:50:20 visual_prompt]: 	Training 1600/2212. train loss: 0.7592,	0.0830 s / batch. (data: 2.42e-04). ETA=4:17:45, max mem: 4.0 GB 
[11/07 01:50:40 visual_prompt]: 	Training 1700/2212. train loss: 0.6397,	0.2075 s / batch. (data: 2.48e-02). ETA=10:44:13, max mem: 4.0 GB 
[11/07 01:51:01 visual_prompt]: 	Training 1800/2212. train loss: 0.7558,	0.1370 s / batch. (data: 2.54e-04). ETA=7:05:09, max mem: 4.0 GB 
[11/07 01:51:21 visual_prompt]: 	Training 1900/2212. train loss: 0.6205,	0.2366 s / batch. (data: 5.01e-03). ETA=12:13:51, max mem: 4.0 GB 
[11/07 01:51:42 visual_prompt]: 	Training 2000/2212. train loss: 0.7808,	0.1944 s / batch. (data: 5.48e-03). ETA=10:02:38, max mem: 4.0 GB 
[11/07 01:52:04 visual_prompt]: 	Training 2100/2212. train loss: 0.5883,	0.1998 s / batch. (data: 6.98e-04). ETA=10:18:59, max mem: 4.0 GB 
[11/07 01:52:26 visual_prompt]: 	Training 2200/2212. train loss: 0.5178,	0.1241 s / batch. (data: 9.06e-05). ETA=6:24:24, max mem: 4.0 GB 
[11/07 01:52:27 visual_prompt]: Epoch 16 / 100: avg data time: 3.04e-02, avg batch time: 0.2130, average train loss: 0.6977
[11/07 01:52:49 visual_prompt]: 	Test 100/246. loss: 0.991, 0.0996 s / batch. (data: 3.08e-05)max mem: 4.00127 GB 
[11/07 01:53:08 visual_prompt]: 	Test 200/246. loss: 0.991, 0.0627 s / batch. (data: 3.60e-05)max mem: 4.00127 GB 
[11/07 01:53:16 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.0607, average loss: 0.7017
[11/07 01:53:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.06	
[11/07 01:53:16 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/07 01:53:39 visual_prompt]: 	Training 100/2212. train loss: 0.7785,	0.1331 s / batch. (data: 5.34e-03). ETA=6:51:50, max mem: 4.0 GB 
[11/07 01:54:01 visual_prompt]: 	Training 200/2212. train loss: 0.7107,	0.1568 s / batch. (data: 5.33e-03). ETA=8:05:01, max mem: 4.0 GB 
[11/07 01:54:22 visual_prompt]: 	Training 300/2212. train loss: 0.6778,	0.2287 s / batch. (data: 2.06e-02). ETA=11:46:57, max mem: 4.0 GB 
[11/07 01:54:42 visual_prompt]: 	Training 400/2212. train loss: 0.6544,	0.2078 s / batch. (data: 1.59e-02). ETA=10:42:03, max mem: 4.0 GB 
[11/07 01:55:04 visual_prompt]: 	Training 500/2212. train loss: 0.6068,	0.2404 s / batch. (data: 5.81e-03). ETA=12:22:27, max mem: 4.0 GB 
[11/07 01:55:27 visual_prompt]: 	Training 600/2212. train loss: 1.0049,	0.1399 s / batch. (data: 6.50e-04). ETA=7:11:41, max mem: 4.0 GB 
[11/07 01:55:47 visual_prompt]: 	Training 700/2212. train loss: 0.5563,	0.2041 s / batch. (data: 2.60e-02). ETA=10:29:42, max mem: 4.0 GB 
[11/07 01:56:08 visual_prompt]: 	Training 800/2212. train loss: 0.2006,	0.5751 s / batch. (data: 4.60e-01). ETA=1 day, 5:33:16, max mem: 4.0 GB 
[11/07 01:56:28 visual_prompt]: 	Training 900/2212. train loss: 0.5483,	0.1851 s / batch. (data: 1.05e-02). ETA=9:30:34, max mem: 4.0 GB 
[11/07 01:56:50 visual_prompt]: 	Training 1000/2212. train loss: 0.7298,	0.1815 s / batch. (data: 2.66e-04). ETA=9:18:58, max mem: 4.0 GB 
[11/07 01:57:11 visual_prompt]: 	Training 1100/2212. train loss: 0.6831,	0.1790 s / batch. (data: 2.51e-04). ETA=9:11:01, max mem: 4.0 GB 
[11/07 01:57:32 visual_prompt]: 	Training 1200/2212. train loss: 0.8113,	0.1878 s / batch. (data: 2.06e-02). ETA=9:37:52, max mem: 4.0 GB 
[11/07 01:57:53 visual_prompt]: 	Training 1300/2212. train loss: 0.6625,	0.2000 s / batch. (data: 6.93e-04). ETA=10:15:10, max mem: 4.0 GB 
[11/07 01:58:15 visual_prompt]: 	Training 1400/2212. train loss: 0.5984,	0.1939 s / batch. (data: 1.04e-02). ETA=9:55:55, max mem: 4.0 GB 
[11/07 01:58:37 visual_prompt]: 	Training 1500/2212. train loss: 0.5696,	0.2346 s / batch. (data: 2.10e-02). ETA=12:00:43, max mem: 4.0 GB 
[11/07 01:58:59 visual_prompt]: 	Training 1600/2212. train loss: 0.8443,	0.2080 s / batch. (data: 6.83e-04). ETA=10:38:39, max mem: 4.0 GB 
[11/07 01:59:19 visual_prompt]: 	Training 1700/2212. train loss: 0.9127,	0.2136 s / batch. (data: 7.18e-04). ETA=10:55:20, max mem: 4.0 GB 
[11/07 01:59:40 visual_prompt]: 	Training 1800/2212. train loss: 1.0271,	0.2281 s / batch. (data: 3.35e-02). ETA=11:39:33, max mem: 4.0 GB 
[11/07 02:00:02 visual_prompt]: 	Training 1900/2212. train loss: 0.6181,	0.1988 s / batch. (data: 1.04e-02). ETA=10:09:14, max mem: 4.0 GB 
[11/07 02:00:24 visual_prompt]: 	Training 2000/2212. train loss: 0.4726,	0.1990 s / batch. (data: 5.36e-03). ETA=10:09:35, max mem: 4.0 GB 
[11/07 02:00:45 visual_prompt]: 	Training 2100/2212. train loss: 0.5086,	0.2147 s / batch. (data: 1.72e-02). ETA=10:57:25, max mem: 4.0 GB 
[11/07 02:01:06 visual_prompt]: 	Training 2200/2212. train loss: 0.9925,	0.1326 s / batch. (data: 1.04e-04). ETA=6:45:53, max mem: 4.0 GB 
[11/07 02:01:07 visual_prompt]: Epoch 17 / 100: avg data time: 2.85e-02, avg batch time: 0.2129, average train loss: 0.7059
[11/07 02:01:29 visual_prompt]: 	Test 100/246. loss: 0.937, 0.0913 s / batch. (data: 3.00e-05)max mem: 4.00127 GB 
[11/07 02:01:48 visual_prompt]: 	Test 200/246. loss: 0.937, 0.0563 s / batch. (data: 3.41e-05)max mem: 4.00127 GB 
[11/07 02:01:56 visual_prompt]: Inference (val):avg data time: 4.85e-05, avg batch time: 0.0619, average loss: 0.6957
[11/07 02:01:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.78	
[11/07 02:01:56 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/07 02:02:21 visual_prompt]: 	Training 100/2212. train loss: 0.8682,	0.2215 s / batch. (data: 1.56e-02). ETA=11:17:28, max mem: 4.0 GB 
[11/07 02:02:42 visual_prompt]: 	Training 200/2212. train loss: 0.5353,	0.1839 s / batch. (data: 1.09e-02). ETA=9:22:06, max mem: 4.0 GB 
[11/07 02:03:03 visual_prompt]: 	Training 300/2212. train loss: 0.5404,	0.2157 s / batch. (data: 2.60e-02). ETA=10:58:51, max mem: 4.0 GB 
[11/07 02:03:26 visual_prompt]: 	Training 400/2212. train loss: 0.5664,	0.1986 s / batch. (data: 1.04e-02). ETA=10:06:27, max mem: 4.0 GB 
[11/07 02:03:48 visual_prompt]: 	Training 500/2212. train loss: 0.7738,	1.0344 s / batch. (data: 9.69e-01). ETA=2 days, 4:36:39, max mem: 4.0 GB 
[11/07 02:04:08 visual_prompt]: 	Training 600/2212. train loss: 0.7251,	0.2297 s / batch. (data: 3.65e-02). ETA=11:40:42, max mem: 4.0 GB 
[11/07 02:04:30 visual_prompt]: 	Training 700/2212. train loss: 0.8341,	0.2085 s / batch. (data: 2.05e-02). ETA=10:35:38, max mem: 4.0 GB 
[11/07 02:04:51 visual_prompt]: 	Training 800/2212. train loss: 0.7486,	0.2072 s / batch. (data: 2.14e-02). ETA=10:31:23, max mem: 4.0 GB 
[11/07 02:05:12 visual_prompt]: 	Training 900/2212. train loss: 0.7135,	0.1991 s / batch. (data: 2.19e-02). ETA=10:06:08, max mem: 4.0 GB 
[11/07 02:05:34 visual_prompt]: 	Training 1000/2212. train loss: 0.8818,	0.1612 s / batch. (data: 2.19e-04). ETA=8:10:36, max mem: 4.0 GB 
[11/07 02:05:55 visual_prompt]: 	Training 1100/2212. train loss: 0.6277,	0.1661 s / batch. (data: 6.75e-04). ETA=8:25:21, max mem: 4.0 GB 
[11/07 02:06:15 visual_prompt]: 	Training 1200/2212. train loss: 0.8052,	0.1566 s / batch. (data: 2.45e-04). ETA=7:56:10, max mem: 4.0 GB 
[11/07 02:06:36 visual_prompt]: 	Training 1300/2212. train loss: 0.5989,	0.2448 s / batch. (data: 1.59e-02). ETA=12:23:37, max mem: 4.0 GB 
[11/07 02:06:57 visual_prompt]: 	Training 1400/2212. train loss: 0.7587,	0.1994 s / batch. (data: 5.35e-03). ETA=10:05:34, max mem: 4.0 GB 
[11/07 02:07:19 visual_prompt]: 	Training 1500/2212. train loss: 0.8765,	0.1913 s / batch. (data: 6.66e-04). ETA=9:40:39, max mem: 4.0 GB 
[11/07 02:07:40 visual_prompt]: 	Training 1600/2212. train loss: 0.6073,	0.2217 s / batch. (data: 5.37e-03). ETA=11:12:23, max mem: 4.0 GB 
[11/07 02:08:02 visual_prompt]: 	Training 1700/2212. train loss: 0.6059,	0.2419 s / batch. (data: 2.10e-02). ETA=12:13:13, max mem: 4.0 GB 
[11/07 02:08:24 visual_prompt]: 	Training 1800/2212. train loss: 0.8174,	0.2350 s / batch. (data: 5.76e-03). ETA=11:51:56, max mem: 4.0 GB 
[11/07 02:08:45 visual_prompt]: 	Training 1900/2212. train loss: 0.5963,	0.2538 s / batch. (data: 1.55e-02). ETA=12:48:29, max mem: 4.0 GB 
[11/07 02:09:05 visual_prompt]: 	Training 2000/2212. train loss: 0.8074,	0.1823 s / batch. (data: 2.74e-04). ETA=9:11:46, max mem: 4.0 GB 
[11/07 02:09:26 visual_prompt]: 	Training 2100/2212. train loss: 0.5862,	0.1919 s / batch. (data: 2.52e-04). ETA=9:40:31, max mem: 4.0 GB 
[11/07 02:09:47 visual_prompt]: 	Training 2200/2212. train loss: 0.5618,	0.1536 s / batch. (data: 1.17e-04). ETA=7:44:23, max mem: 4.0 GB 
[11/07 02:09:48 visual_prompt]: Epoch 18 / 100: avg data time: 2.84e-02, avg batch time: 0.2130, average train loss: 0.6982
[11/07 02:10:09 visual_prompt]: 	Test 100/246. loss: 0.844, 0.0703 s / batch. (data: 3.03e-05)max mem: 4.00127 GB 
[11/07 02:10:29 visual_prompt]: 	Test 200/246. loss: 0.844, 0.0593 s / batch. (data: 3.24e-05)max mem: 4.00127 GB 
[11/07 02:10:37 visual_prompt]: Inference (val):avg data time: 6.52e-05, avg batch time: 0.0647, average loss: 0.6893
[11/07 02:10:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.95	
[11/07 02:10:37 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/07 02:11:00 visual_prompt]: 	Training 100/2212. train loss: 0.9035,	0.1922 s / batch. (data: 5.35e-03). ETA=9:40:48, max mem: 4.0 GB 
[11/07 02:11:22 visual_prompt]: 	Training 200/2212. train loss: 0.5679,	0.1813 s / batch. (data: 5.40e-03). ETA=9:07:31, max mem: 4.0 GB 
[11/07 02:11:42 visual_prompt]: 	Training 300/2212. train loss: 1.1631,	0.2022 s / batch. (data: 7.56e-04). ETA=10:10:12, max mem: 4.0 GB 
[11/07 02:12:04 visual_prompt]: 	Training 400/2212. train loss: 0.5566,	0.5851 s / batch. (data: 4.57e-01). ETA=1 day, 5:24:54, max mem: 4.0 GB 
[11/07 02:12:24 visual_prompt]: 	Training 500/2212. train loss: 0.6033,	0.2000 s / batch. (data: 5.36e-03). ETA=10:02:53, max mem: 4.0 GB 
[11/07 02:12:47 visual_prompt]: 	Training 600/2212. train loss: 0.8827,	0.1753 s / batch. (data: 1.57e-02). ETA=8:48:17, max mem: 4.0 GB 
[11/07 02:13:07 visual_prompt]: 	Training 700/2212. train loss: 0.8296,	0.1997 s / batch. (data: 1.04e-02). ETA=10:01:23, max mem: 4.0 GB 
[11/07 02:13:29 visual_prompt]: 	Training 800/2212. train loss: 0.5638,	0.2018 s / batch. (data: 6.76e-04). ETA=10:07:14, max mem: 4.0 GB 
[11/07 02:13:50 visual_prompt]: 	Training 900/2212. train loss: 0.5569,	0.2426 s / batch. (data: 7.48e-02). ETA=12:09:38, max mem: 4.0 GB 
[11/07 02:14:11 visual_prompt]: 	Training 1000/2212. train loss: 0.5466,	0.2081 s / batch. (data: 5.45e-03). ETA=10:25:35, max mem: 4.0 GB 
[11/07 02:14:34 visual_prompt]: 	Training 1100/2212. train loss: 0.9052,	0.2041 s / batch. (data: 2.02e-02). ETA=10:13:07, max mem: 4.0 GB 
[11/07 02:14:54 visual_prompt]: 	Training 1200/2212. train loss: 1.3848,	0.1954 s / batch. (data: 5.35e-03). ETA=9:46:39, max mem: 4.0 GB 
[11/07 02:15:17 visual_prompt]: 	Training 1300/2212. train loss: 0.8316,	0.1134 s / batch. (data: 2.69e-04). ETA=5:40:29, max mem: 4.0 GB 
[11/07 02:15:37 visual_prompt]: 	Training 1400/2212. train loss: 0.7402,	0.2139 s / batch. (data: 1.79e-02). ETA=10:41:46, max mem: 4.0 GB 
[11/07 02:15:58 visual_prompt]: 	Training 1500/2212. train loss: 0.7360,	0.2049 s / batch. (data: 2.05e-02). ETA=10:14:15, max mem: 4.0 GB 
[11/07 02:16:21 visual_prompt]: 	Training 1600/2212. train loss: 0.8149,	0.5262 s / batch. (data: 4.11e-01). ETA=1 day, 2:16:48, max mem: 4.0 GB 
[11/07 02:16:42 visual_prompt]: 	Training 1700/2212. train loss: 0.4888,	0.2482 s / batch. (data: 1.09e-02). ETA=12:23:09, max mem: 4.0 GB 
[11/07 02:17:02 visual_prompt]: 	Training 1800/2212. train loss: 0.6148,	0.1875 s / batch. (data: 1.51e-02). ETA=9:21:13, max mem: 4.0 GB 
[11/07 02:17:24 visual_prompt]: 	Training 1900/2212. train loss: 0.5154,	0.1692 s / batch. (data: 5.77e-03). ETA=8:26:14, max mem: 4.0 GB 
[11/07 02:17:44 visual_prompt]: 	Training 2000/2212. train loss: 0.6448,	0.1727 s / batch. (data: 1.55e-02). ETA=8:36:25, max mem: 4.0 GB 
[11/07 02:18:06 visual_prompt]: 	Training 2100/2212. train loss: 0.5808,	0.1870 s / batch. (data: 1.75e-02). ETA=9:18:51, max mem: 4.0 GB 
[11/07 02:18:26 visual_prompt]: 	Training 2200/2212. train loss: 0.3812,	0.1734 s / batch. (data: 1.17e-04). ETA=8:37:53, max mem: 4.0 GB 
[11/07 02:18:28 visual_prompt]: Epoch 19 / 100: avg data time: 2.84e-02, avg batch time: 0.2128, average train loss: 0.7023
[11/07 02:18:49 visual_prompt]: 	Test 100/246. loss: 0.432, 0.0641 s / batch. (data: 5.05e-05)max mem: 4.00127 GB 
[11/07 02:19:08 visual_prompt]: 	Test 200/246. loss: 0.432, 0.0651 s / batch. (data: 4.01e-05)max mem: 4.00127 GB 
[11/07 02:19:16 visual_prompt]: Inference (val):avg data time: 6.72e-05, avg batch time: 0.0644, average loss: 0.7701
[11/07 02:19:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.02	
[11/07 02:19:16 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/07 02:19:40 visual_prompt]: 	Training 100/2212. train loss: 0.5124,	0.2434 s / batch. (data: 1.55e-02). ETA=12:06:31, max mem: 4.0 GB 
[11/07 02:20:03 visual_prompt]: 	Training 200/2212. train loss: 1.0433,	0.1673 s / batch. (data: 1.34e-02). ETA=8:18:57, max mem: 4.0 GB 
[11/07 02:20:24 visual_prompt]: 	Training 300/2212. train loss: 0.5681,	0.2003 s / batch. (data: 1.04e-02). ETA=9:57:07, max mem: 4.0 GB 
[11/07 02:20:46 visual_prompt]: 	Training 400/2212. train loss: 0.6607,	0.1830 s / batch. (data: 1.05e-02). ETA=9:05:09, max mem: 4.0 GB 
[11/07 02:21:07 visual_prompt]: 	Training 500/2212. train loss: 0.4971,	0.1995 s / batch. (data: 2.49e-02). ETA=9:54:06, max mem: 4.0 GB 
[11/07 02:21:29 visual_prompt]: 	Training 600/2212. train loss: 0.8525,	0.2232 s / batch. (data: 2.09e-02). ETA=11:04:25, max mem: 4.0 GB 
[11/07 02:21:50 visual_prompt]: 	Training 700/2212. train loss: 0.6120,	0.2097 s / batch. (data: 1.04e-02). ETA=10:23:53, max mem: 4.0 GB 
[11/07 02:22:10 visual_prompt]: 	Training 800/2212. train loss: 0.5183,	0.2466 s / batch. (data: 5.84e-03). ETA=12:12:59, max mem: 4.0 GB 
[11/07 02:22:31 visual_prompt]: 	Training 900/2212. train loss: 0.7786,	0.1467 s / batch. (data: 6.74e-04). ETA=7:15:55, max mem: 4.0 GB 
[11/07 02:22:52 visual_prompt]: 	Training 1000/2212. train loss: 0.9230,	0.2250 s / batch. (data: 3.92e-02). ETA=11:08:10, max mem: 4.0 GB 
[11/07 02:23:13 visual_prompt]: 	Training 1100/2212. train loss: 0.4839,	0.1948 s / batch. (data: 1.55e-02). ETA=9:38:06, max mem: 4.0 GB 
[11/07 02:23:34 visual_prompt]: 	Training 1200/2212. train loss: 0.7820,	0.1978 s / batch. (data: 5.34e-03). ETA=9:46:48, max mem: 4.0 GB 
[11/07 02:23:56 visual_prompt]: 	Training 1300/2212. train loss: 0.8541,	0.1678 s / batch. (data: 6.33e-04). ETA=8:17:24, max mem: 4.0 GB 
[11/07 02:24:18 visual_prompt]: 	Training 1400/2212. train loss: 0.4822,	0.2072 s / batch. (data: 5.36e-03). ETA=10:14:01, max mem: 4.0 GB 
[11/07 02:24:39 visual_prompt]: 	Training 1500/2212. train loss: 0.6898,	0.1315 s / batch. (data: 1.55e-02). ETA=6:29:25, max mem: 4.0 GB 
[11/07 02:24:59 visual_prompt]: 	Training 1600/2212. train loss: 0.8665,	0.2488 s / batch. (data: 7.72e-04). ETA=12:16:18, max mem: 4.0 GB 
[11/07 02:25:20 visual_prompt]: 	Training 1700/2212. train loss: 0.8411,	0.2182 s / batch. (data: 2.51e-04). ETA=10:45:30, max mem: 4.0 GB 
[11/07 02:25:41 visual_prompt]: 	Training 1800/2212. train loss: 0.6116,	0.2439 s / batch. (data: 2.65e-04). ETA=12:01:07, max mem: 4.0 GB 
[11/07 02:26:02 visual_prompt]: 	Training 1900/2212. train loss: 0.8567,	0.1682 s / batch. (data: 2.13e-04). ETA=8:17:03, max mem: 4.0 GB 
[11/07 02:26:24 visual_prompt]: 	Training 2000/2212. train loss: 0.5968,	0.2370 s / batch. (data: 2.05e-02). ETA=11:39:42, max mem: 4.0 GB 
[11/07 02:26:46 visual_prompt]: 	Training 2100/2212. train loss: 0.6830,	0.1913 s / batch. (data: 1.05e-02). ETA=9:24:30, max mem: 4.0 GB 
[11/07 02:27:07 visual_prompt]: 	Training 2200/2212. train loss: 0.5685,	0.1235 s / batch. (data: 1.04e-04). ETA=6:04:13, max mem: 4.0 GB 
[11/07 02:27:08 visual_prompt]: Epoch 20 / 100: avg data time: 2.81e-02, avg batch time: 0.2129, average train loss: 0.7037
[11/07 02:27:29 visual_prompt]: 	Test 100/246. loss: 0.688, 0.0955 s / batch. (data: 2.46e-05)max mem: 4.00127 GB 
[11/07 02:27:49 visual_prompt]: 	Test 200/246. loss: 0.688, 0.0521 s / batch. (data: 2.62e-05)max mem: 4.00127 GB 
[11/07 02:27:56 visual_prompt]: Inference (val):avg data time: 4.37e-04, avg batch time: 0.0629, average loss: 0.6937
[11/07 02:27:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.95	
[11/07 02:27:57 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/07 02:28:21 visual_prompt]: 	Training 100/2212. train loss: 0.5834,	1.0969 s / batch. (data: 9.42e-01). ETA=2 days, 5:53:15, max mem: 4.0 GB 
[11/07 02:28:42 visual_prompt]: 	Training 200/2212. train loss: 0.6685,	0.2370 s / batch. (data: 1.09e-02). ETA=11:38:15, max mem: 4.0 GB 
[11/07 02:29:05 visual_prompt]: 	Training 300/2212. train loss: 1.0859,	0.1559 s / batch. (data: 1.55e-02). ETA=7:38:54, max mem: 4.0 GB 
[11/07 02:29:27 visual_prompt]: 	Training 400/2212. train loss: 0.8037,	0.2037 s / batch. (data: 1.75e-02). ETA=9:59:23, max mem: 4.0 GB 
[11/07 02:29:47 visual_prompt]: 	Training 500/2212. train loss: 0.7896,	0.1860 s / batch. (data: 5.38e-03). ETA=9:07:01, max mem: 4.0 GB 
[11/07 02:30:08 visual_prompt]: 	Training 600/2212. train loss: 0.8306,	0.2100 s / batch. (data: 2.60e-02). ETA=10:17:10, max mem: 4.0 GB 
[11/07 02:30:30 visual_prompt]: 	Training 700/2212. train loss: 0.5770,	0.2273 s / batch. (data: 1.59e-02). ETA=11:07:52, max mem: 4.0 GB 
[11/07 02:30:53 visual_prompt]: 	Training 800/2212. train loss: 0.6874,	0.2333 s / batch. (data: 1.59e-02). ETA=11:25:00, max mem: 4.0 GB 
[11/07 02:31:14 visual_prompt]: 	Training 900/2212. train loss: 0.7887,	0.1994 s / batch. (data: 1.55e-02). ETA=9:45:12, max mem: 4.0 GB 
[11/07 02:31:35 visual_prompt]: 	Training 1000/2212. train loss: 0.6271,	0.2493 s / batch. (data: 6.92e-04). ETA=12:11:06, max mem: 4.0 GB 
[11/07 02:31:55 visual_prompt]: 	Training 1100/2212. train loss: 0.7528,	0.2169 s / batch. (data: 7.97e-03). ETA=10:35:44, max mem: 4.0 GB 
[11/07 02:32:16 visual_prompt]: 	Training 1200/2212. train loss: 0.7071,	0.2058 s / batch. (data: 7.48e-03). ETA=10:02:54, max mem: 4.0 GB 
[11/07 02:32:38 visual_prompt]: 	Training 1300/2212. train loss: 0.6523,	0.1983 s / batch. (data: 1.50e-02). ETA=9:40:29, max mem: 4.0 GB 
[11/07 02:32:59 visual_prompt]: 	Training 1400/2212. train loss: 0.6152,	0.2224 s / batch. (data: 1.15e-02). ETA=10:50:50, max mem: 4.0 GB 
[11/07 02:33:21 visual_prompt]: 	Training 1500/2212. train loss: 0.5682,	0.2413 s / batch. (data: 2.91e-02). ETA=11:45:45, max mem: 4.0 GB 
[11/07 02:33:41 visual_prompt]: 	Training 1600/2212. train loss: 0.5715,	0.2049 s / batch. (data: 2.12e-04). ETA=9:58:47, max mem: 4.0 GB 
[11/07 02:34:02 visual_prompt]: 	Training 1700/2212. train loss: 0.7787,	0.1487 s / batch. (data: 9.54e-03). ETA=7:14:28, max mem: 4.0 GB 
[11/07 02:34:23 visual_prompt]: 	Training 1800/2212. train loss: 0.6419,	0.1857 s / batch. (data: 1.38e-02). ETA=9:02:15, max mem: 4.0 GB 
[11/07 02:34:44 visual_prompt]: 	Training 1900/2212. train loss: 0.6088,	0.2250 s / batch. (data: 5.79e-03). ETA=10:56:29, max mem: 4.0 GB 
[11/07 02:35:05 visual_prompt]: 	Training 2000/2212. train loss: 0.7846,	0.1776 s / batch. (data: 2.43e-04). ETA=8:37:54, max mem: 4.0 GB 
[11/07 02:35:26 visual_prompt]: 	Training 2100/2212. train loss: 0.9442,	0.2171 s / batch. (data: 1.43e-02). ETA=10:32:38, max mem: 4.0 GB 
[11/07 02:35:47 visual_prompt]: 	Training 2200/2212. train loss: 0.7141,	0.1282 s / batch. (data: 1.36e-04). ETA=6:13:25, max mem: 4.0 GB 
[11/07 02:35:48 visual_prompt]: Epoch 21 / 100: avg data time: 2.77e-02, avg batch time: 0.2129, average train loss: 0.6952
[11/07 02:36:09 visual_prompt]: 	Test 100/246. loss: 0.909, 0.0627 s / batch. (data: 2.93e-05)max mem: 4.00127 GB 
[11/07 02:36:29 visual_prompt]: 	Test 200/246. loss: 0.909, 0.0738 s / batch. (data: 2.41e-05)max mem: 4.00127 GB 
[11/07 02:36:37 visual_prompt]: Inference (val):avg data time: 1.08e-04, avg batch time: 0.0619, average loss: 0.6931
[11/07 02:36:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.09	
[11/07 02:36:37 visual_prompt]: Stopping early.
[11/07 02:36:37 visual_prompt]: Rank of current process: 0. World size: 1
[11/07 02:36:37 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/07 02:36:37 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '1', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/07 02:36:37 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/07 02:36:37 visual_prompt]: Training with config:
[11/07 02:36:37 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/crop224/val/seed0/lr0.005_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 1, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/07 02:36:37 visual_prompt]: Loading training data...
[11/07 02:36:37 visual_prompt]: Constructing mammo-cbis dataset train...
[11/07 02:36:37 visual_prompt]: Loading validation data...
[11/07 02:36:37 visual_prompt]: Constructing mammo-cbis dataset val...
[11/07 02:36:37 visual_prompt]: Constructing models...
[11/07 02:36:38 visual_prompt]: Enable all parameters update during training
[11/07 02:36:38 visual_prompt]: Total Parameters: 85800194	 Gradient Parameters: 85800194
[11/07 02:36:38 visual_prompt]: tuned percent:100.000
[11/07 02:36:39 visual_prompt]: Device used for model: 0
[11/07 02:36:39 visual_prompt]: Setting up Evaluator...
[11/07 02:36:39 visual_prompt]: Setting up Trainer...
[11/07 02:36:39 visual_prompt]: 	Setting up the optimizer...
[11/07 02:36:39 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/07 02:37:02 visual_prompt]: 	Training 100/2212. train loss: 17.0073,	0.1913 s / batch. (data: 1.68e-02). ETA=11:44:45, max mem: 5.0 GB 
[11/07 02:37:23 visual_prompt]: 	Training 200/2212. train loss: 8.7706,	0.1466 s / batch. (data: 6.91e-04). ETA=8:59:52, max mem: 5.0 GB 
[11/07 02:37:44 visual_prompt]: 	Training 300/2212. train loss: 0.0001,	0.1680 s / batch. (data: 1.04e-02). ETA=10:18:42, max mem: 5.0 GB 
[11/07 02:38:04 visual_prompt]: 	Training 400/2212. train loss: 9.7364,	0.1801 s / batch. (data: 2.33e-04). ETA=11:02:52, max mem: 5.0 GB 
[11/07 02:38:26 visual_prompt]: 	Training 500/2212. train loss: 9.6752,	0.1053 s / batch. (data: 6.20e-04). ETA=6:27:19, max mem: 5.0 GB 
[11/07 02:38:48 visual_prompt]: 	Training 600/2212. train loss: 0.0000,	0.1888 s / batch. (data: 1.06e-02). ETA=11:34:02, max mem: 5.0 GB 
[11/07 02:39:10 visual_prompt]: 	Training 700/2212. train loss: 10.7484,	0.1590 s / batch. (data: 6.84e-04). ETA=9:44:25, max mem: 5.0 GB 
[11/07 02:39:31 visual_prompt]: 	Training 800/2212. train loss: 10.3468,	0.2037 s / batch. (data: 1.08e-02). ETA=12:28:08, max mem: 5.0 GB 
[11/07 02:39:52 visual_prompt]: 	Training 900/2212. train loss: 0.0000,	0.1724 s / batch. (data: 1.04e-02). ETA=10:33:04, max mem: 5.0 GB 
[11/07 02:40:12 visual_prompt]: 	Training 1000/2212. train loss: 0.0001,	0.2285 s / batch. (data: 1.01e-01). ETA=13:58:35, max mem: 5.0 GB 
[11/07 02:40:33 visual_prompt]: 	Training 1100/2212. train loss: 0.0000,	0.0868 s / batch. (data: 6.58e-04). ETA=5:18:14, max mem: 5.0 GB 
[11/07 02:40:53 visual_prompt]: 	Training 1200/2212. train loss: 0.0000,	0.1434 s / batch. (data: 2.73e-04). ETA=8:45:38, max mem: 5.0 GB 
[11/07 02:41:13 visual_prompt]: 	Training 1300/2212. train loss: 0.0000,	0.1701 s / batch. (data: 5.91e-03). ETA=10:23:15, max mem: 5.0 GB 
[11/07 02:41:35 visual_prompt]: 	Training 1400/2212. train loss: 0.0027,	0.1179 s / batch. (data: 2.81e-04). ETA=7:11:55, max mem: 5.0 GB 
[11/07 02:41:56 visual_prompt]: 	Training 1500/2212. train loss: 5.3648,	0.1548 s / batch. (data: 1.51e-02). ETA=9:26:45, max mem: 5.0 GB 
[11/07 02:42:16 visual_prompt]: 	Training 1600/2212. train loss: 5.2697,	0.1556 s / batch. (data: 2.52e-04). ETA=9:29:22, max mem: 5.0 GB 
[11/07 02:42:36 visual_prompt]: 	Training 1700/2212. train loss: 0.0000,	0.1639 s / batch. (data: 2.55e-04). ETA=9:59:27, max mem: 5.0 GB 
[11/07 02:42:58 visual_prompt]: 	Training 1800/2212. train loss: 0.0000,	0.4778 s / batch. (data: 3.53e-01). ETA=1 day, 5:07:03, max mem: 5.0 GB 
[11/07 02:43:18 visual_prompt]: 	Training 1900/2212. train loss: 8.6260,	0.2667 s / batch. (data: 9.65e-02). ETA=16:14:51, max mem: 5.0 GB 
[11/07 02:43:40 visual_prompt]: 	Training 2000/2212. train loss: 0.0002,	0.1621 s / batch. (data: 5.34e-03). ETA=9:52:09, max mem: 5.0 GB 
[11/07 02:44:00 visual_prompt]: 	Training 2100/2212. train loss: 14.9015,	0.2085 s / batch. (data: 1.08e-02). ETA=12:41:28, max mem: 5.0 GB 
[11/07 02:44:19 visual_prompt]: 	Training 2200/2212. train loss: 7.6808,	0.1096 s / batch. (data: 1.07e-04). ETA=6:40:00, max mem: 5.0 GB 
[11/07 02:44:20 visual_prompt]: Epoch 1 / 100: avg data time: 5.91e-02, avg batch time: 0.2086, average train loss: 5.2946
[11/07 02:44:42 visual_prompt]: 	Test 100/246. loss: 0.001, 0.1169 s / batch. (data: 2.19e-05)max mem: 4.96204 GB 
[11/07 02:45:01 visual_prompt]: 	Test 200/246. loss: 0.002, 0.0691 s / batch. (data: 2.36e-05)max mem: 4.96204 GB 
[11/07 02:45:10 visual_prompt]: Inference (val):avg data time: 2.58e-04, avg batch time: 0.0639, average loss: 4.3337
[11/07 02:45:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.86	
[11/07 02:45:10 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/07 02:45:34 visual_prompt]: 	Training 100/2212. train loss: 0.3689,	0.1563 s / batch. (data: 1.45e-02). ETA=9:30:22, max mem: 5.0 GB 
[11/07 02:45:55 visual_prompt]: 	Training 200/2212. train loss: 0.0000,	0.2718 s / batch. (data: 1.68e-01). ETA=16:31:12, max mem: 5.0 GB 
[11/07 02:46:16 visual_prompt]: 	Training 300/2212. train loss: 0.5153,	0.1569 s / batch. (data: 2.60e-04). ETA=9:31:52, max mem: 5.0 GB 
[11/07 02:46:36 visual_prompt]: 	Training 400/2212. train loss: 0.3900,	0.1579 s / batch. (data: 1.04e-02). ETA=9:35:21, max mem: 5.0 GB 
[11/07 02:46:56 visual_prompt]: 	Training 500/2212. train loss: 0.0014,	0.1723 s / batch. (data: 2.05e-02). ETA=10:27:28, max mem: 5.0 GB 
[11/07 02:47:15 visual_prompt]: 	Training 600/2212. train loss: 4.3828,	0.1478 s / batch. (data: 1.59e-02). ETA=8:57:50, max mem: 5.0 GB 
[11/07 02:47:36 visual_prompt]: 	Training 700/2212. train loss: 0.0196,	0.5324 s / batch. (data: 4.16e-01). ETA=1 day, 8:16:51, max mem: 5.0 GB 
[11/07 02:47:57 visual_prompt]: 	Training 800/2212. train loss: 6.6721,	0.1439 s / batch. (data: 1.55e-02). ETA=8:43:26, max mem: 5.0 GB 
[11/07 02:48:19 visual_prompt]: 	Training 900/2212. train loss: 1.2908,	0.1554 s / batch. (data: 2.69e-04). ETA=9:24:46, max mem: 5.0 GB 
[11/07 02:48:41 visual_prompt]: 	Training 1000/2212. train loss: 0.0051,	0.1145 s / batch. (data: 6.98e-03). ETA=6:56:09, max mem: 5.0 GB 
[11/07 02:49:01 visual_prompt]: 	Training 1100/2212. train loss: 0.0000,	0.1613 s / batch. (data: 5.39e-03). ETA=9:45:52, max mem: 5.0 GB 
[11/07 02:49:22 visual_prompt]: 	Training 1200/2212. train loss: 4.2503,	0.1451 s / batch. (data: 6.28e-03). ETA=8:46:35, max mem: 5.0 GB 
[11/07 02:49:44 visual_prompt]: 	Training 1300/2212. train loss: 3.8137,	0.5803 s / batch. (data: 4.66e-01). ETA=1 day, 11:05:20, max mem: 5.0 GB 
[11/07 02:50:05 visual_prompt]: 	Training 1400/2212. train loss: 0.3844,	0.7658 s / batch. (data: 6.35e-01). ETA=1 day, 22:17:06, max mem: 5.0 GB 
[11/07 02:50:27 visual_prompt]: 	Training 1500/2212. train loss: 3.2151,	0.1539 s / batch. (data: 6.12e-04). ETA=9:18:00, max mem: 5.0 GB 
[11/07 02:50:48 visual_prompt]: 	Training 1600/2212. train loss: 0.0269,	0.1469 s / batch. (data: 2.27e-04). ETA=8:52:12, max mem: 5.0 GB 
[11/07 02:51:08 visual_prompt]: 	Training 1700/2212. train loss: 0.0022,	0.2020 s / batch. (data: 1.55e-02). ETA=12:11:31, max mem: 5.0 GB 
[11/07 02:51:28 visual_prompt]: 	Training 1800/2212. train loss: 0.0017,	0.1343 s / batch. (data: 2.33e-04). ETA=8:06:04, max mem: 5.0 GB 
[11/07 02:51:49 visual_prompt]: 	Training 1900/2212. train loss: 0.0878,	0.1240 s / batch. (data: 2.40e-04). ETA=7:28:46, max mem: 5.0 GB 
[11/07 02:52:10 visual_prompt]: 	Training 2000/2212. train loss: 2.1136,	0.1646 s / batch. (data: 1.66e-02). ETA=9:55:23, max mem: 5.0 GB 
[11/07 02:52:30 visual_prompt]: 	Training 2100/2212. train loss: 0.0077,	0.2055 s / batch. (data: 1.04e-02). ETA=12:22:45, max mem: 5.0 GB 
[11/07 02:52:50 visual_prompt]: 	Training 2200/2212. train loss: 0.6759,	0.1075 s / batch. (data: 1.14e-04). ETA=6:28:35, max mem: 5.0 GB 
[11/07 02:52:51 visual_prompt]: Epoch 2 / 100: avg data time: 6.01e-02, avg batch time: 0.2087, average train loss: 2.5537
[11/07 02:53:13 visual_prompt]: 	Test 100/246. loss: 0.190, 0.0808 s / batch. (data: 2.29e-05)max mem: 4.96204 GB 
[11/07 02:53:32 visual_prompt]: 	Test 200/246. loss: 0.186, 0.0680 s / batch. (data: 4.53e-05)max mem: 4.96204 GB 
[11/07 02:53:40 visual_prompt]: Inference (val):avg data time: 1.05e-04, avg batch time: 0.0631, average loss: 1.0798
[11/07 02:53:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.39	
[11/07 02:53:40 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/07 02:54:03 visual_prompt]: 	Training 100/2212. train loss: 0.0109,	0.1229 s / batch. (data: 9.50e-03). ETA=7:23:49, max mem: 5.0 GB 
[11/07 02:54:24 visual_prompt]: 	Training 200/2212. train loss: 9.2851,	0.1870 s / batch. (data: 1.04e-02). ETA=11:15:09, max mem: 5.0 GB 
[11/07 02:54:46 visual_prompt]: 	Training 300/2212. train loss: 6.4251,	0.1228 s / batch. (data: 5.33e-03). ETA=7:23:07, max mem: 5.0 GB 
[11/07 02:55:07 visual_prompt]: 	Training 400/2212. train loss: 6.5468,	0.1933 s / batch. (data: 2.15e-02). ETA=11:36:59, max mem: 5.0 GB 
[11/07 02:55:28 visual_prompt]: 	Training 500/2212. train loss: 15.5075,	0.1934 s / batch. (data: 2.05e-02). ETA=11:37:02, max mem: 5.0 GB 
[11/07 02:55:50 visual_prompt]: 	Training 600/2212. train loss: 8.3329,	0.1769 s / batch. (data: 5.62e-03). ETA=10:37:20, max mem: 5.0 GB 
[11/07 02:56:11 visual_prompt]: 	Training 700/2212. train loss: 0.0105,	0.3602 s / batch. (data: 2.62e-01). ETA=21:37:14, max mem: 5.0 GB 
[11/07 02:56:32 visual_prompt]: 	Training 800/2212. train loss: 0.3313,	0.1647 s / batch. (data: 5.33e-03). ETA=9:52:58, max mem: 5.0 GB 
[11/07 02:56:52 visual_prompt]: 	Training 900/2212. train loss: 0.2723,	0.1903 s / batch. (data: 5.77e-03). ETA=11:24:42, max mem: 5.0 GB 
[11/07 02:57:13 visual_prompt]: 	Training 1000/2212. train loss: 0.2738,	0.1302 s / batch. (data: 5.34e-03). ETA=7:48:14, max mem: 5.0 GB 
[11/07 02:57:34 visual_prompt]: 	Training 1100/2212. train loss: 0.1572,	0.0923 s / batch. (data: 6.95e-03). ETA=5:31:39, max mem: 5.0 GB 
[11/07 02:57:53 visual_prompt]: 	Training 1200/2212. train loss: 14.7523,	0.1772 s / batch. (data: 7.32e-03). ETA=10:36:35, max mem: 5.0 GB 
[11/07 02:58:15 visual_prompt]: 	Training 1300/2212. train loss: 8.3634,	0.0762 s / batch. (data: 2.22e-04). ETA=4:33:31, max mem: 5.0 GB 
[11/07 02:58:36 visual_prompt]: 	Training 1400/2212. train loss: 3.2679,	0.0777 s / batch. (data: 2.77e-04). ETA=4:38:59, max mem: 5.0 GB 
[11/07 02:58:56 visual_prompt]: 	Training 1500/2212. train loss: 5.1215,	0.1401 s / batch. (data: 5.35e-03). ETA=8:22:44, max mem: 5.0 GB 
[11/07 02:59:16 visual_prompt]: 	Training 1600/2212. train loss: 0.0056,	0.1984 s / batch. (data: 2.06e-02). ETA=11:51:22, max mem: 5.0 GB 
[11/07 02:59:37 visual_prompt]: 	Training 1700/2212. train loss: 4.2977,	0.1560 s / batch. (data: 5.37e-03). ETA=9:19:15, max mem: 5.0 GB 
[11/07 02:59:58 visual_prompt]: 	Training 1800/2212. train loss: 3.2950,	0.1848 s / batch. (data: 2.06e-02). ETA=11:02:14, max mem: 5.0 GB 
[11/07 03:00:18 visual_prompt]: 	Training 1900/2212. train loss: 0.0142,	0.1818 s / batch. (data: 2.42e-04). ETA=10:51:13, max mem: 5.0 GB 
[11/07 03:00:38 visual_prompt]: 	Training 2000/2212. train loss: 2.4958,	0.1810 s / batch. (data: 2.06e-02). ETA=10:47:51, max mem: 5.0 GB 
[11/07 03:01:01 visual_prompt]: 	Training 2100/2212. train loss: 1.1018,	0.1468 s / batch. (data: 2.36e-04). ETA=8:45:17, max mem: 5.0 GB 
[11/07 03:01:21 visual_prompt]: 	Training 2200/2212. train loss: 0.0502,	0.8799 s / batch. (data: 7.70e-01). ETA=2 days, 4:26:54, max mem: 5.0 GB 
[11/07 03:01:22 visual_prompt]: Epoch 3 / 100: avg data time: 5.90e-02, avg batch time: 0.2086, average train loss: 2.9114
[11/07 03:01:43 visual_prompt]: 	Test 100/246. loss: 3.648, 0.1096 s / batch. (data: 2.17e-05)max mem: 4.96204 GB 
[11/07 03:02:03 visual_prompt]: 	Test 200/246. loss: 3.592, 0.0949 s / batch. (data: 2.60e-05)max mem: 4.96204 GB 
[11/07 03:02:11 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.0641, average loss: 1.5914
[11/07 03:02:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.40	
[11/07 03:02:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/07 03:02:36 visual_prompt]: 	Training 100/2212. train loss: 0.0000,	0.1440 s / batch. (data: 2.57e-04). ETA=8:34:37, max mem: 5.0 GB 
[11/07 03:02:55 visual_prompt]: 	Training 200/2212. train loss: 0.0253,	0.1664 s / batch. (data: 1.04e-02). ETA=9:54:24, max mem: 5.0 GB 
[11/07 03:03:16 visual_prompt]: 	Training 300/2212. train loss: 0.0139,	0.1478 s / batch. (data: 2.75e-04). ETA=8:47:41, max mem: 5.0 GB 
[11/07 03:03:37 visual_prompt]: 	Training 400/2212. train loss: 1.8779,	0.1632 s / batch. (data: 6.94e-04). ETA=9:42:38, max mem: 5.0 GB 
[11/07 03:03:57 visual_prompt]: 	Training 500/2212. train loss: 0.0050,	0.1138 s / batch. (data: 5.34e-03). ETA=6:46:05, max mem: 5.0 GB 
[11/07 03:04:18 visual_prompt]: 	Training 600/2212. train loss: 0.0097,	0.1470 s / batch. (data: 4.83e-04). ETA=8:44:11, max mem: 5.0 GB 
[11/07 03:04:38 visual_prompt]: 	Training 700/2212. train loss: 0.0001,	0.2130 s / batch. (data: 3.14e-02). ETA=12:39:04, max mem: 5.0 GB 
[11/07 03:05:00 visual_prompt]: 	Training 800/2212. train loss: 5.6948,	0.1682 s / batch. (data: 2.35e-04). ETA=9:59:22, max mem: 5.0 GB 
[11/07 03:05:20 visual_prompt]: 	Training 900/2212. train loss: 0.0154,	0.2104 s / batch. (data: 1.04e-02). ETA=12:29:24, max mem: 5.0 GB 
[11/07 03:05:41 visual_prompt]: 	Training 1000/2212. train loss: 0.3096,	0.1293 s / batch. (data: 1.21e-02). ETA=7:40:18, max mem: 5.0 GB 
[11/07 03:06:01 visual_prompt]: 	Training 1100/2212. train loss: 16.7800,	0.1480 s / batch. (data: 2.48e-04). ETA=8:46:41, max mem: 5.0 GB 
[11/07 03:06:23 visual_prompt]: 	Training 1200/2212. train loss: 0.3824,	0.1861 s / batch. (data: 6.76e-04). ETA=11:01:47, max mem: 5.0 GB 
[11/07 03:06:43 visual_prompt]: 	Training 1300/2212. train loss: 8.9291,	0.1814 s / batch. (data: 2.58e-02). ETA=10:44:53, max mem: 5.0 GB 
[11/07 03:07:05 visual_prompt]: 	Training 1400/2212. train loss: 0.0880,	0.1766 s / batch. (data: 5.76e-03). ETA=10:27:15, max mem: 5.0 GB 
[11/07 03:07:26 visual_prompt]: 	Training 1500/2212. train loss: 4.4185,	0.1652 s / batch. (data: 2.40e-04). ETA=9:46:32, max mem: 5.0 GB 
[11/07 03:07:46 visual_prompt]: 	Training 1600/2212. train loss: 0.5203,	0.1685 s / batch. (data: 6.26e-03). ETA=9:58:09, max mem: 5.0 GB 
[11/07 03:08:08 visual_prompt]: 	Training 1700/2212. train loss: 0.0000,	0.1878 s / batch. (data: 1.59e-02). ETA=11:06:14, max mem: 5.0 GB 
[11/07 03:08:27 visual_prompt]: 	Training 1800/2212. train loss: 1.0152,	0.1572 s / batch. (data: 9.21e-03). ETA=9:17:31, max mem: 5.0 GB 
[11/07 03:08:48 visual_prompt]: 	Training 1900/2212. train loss: 0.6079,	0.1959 s / batch. (data: 5.39e-03). ETA=11:34:18, max mem: 5.0 GB 
[11/07 03:09:10 visual_prompt]: 	Training 2000/2212. train loss: 0.1803,	0.1725 s / batch. (data: 1.31e-02). ETA=10:11:15, max mem: 5.0 GB 
[11/07 03:09:31 visual_prompt]: 	Training 2100/2212. train loss: 3.0730,	0.1993 s / batch. (data: 5.71e-04). ETA=11:45:46, max mem: 5.0 GB 
[11/07 03:09:52 visual_prompt]: 	Training 2200/2212. train loss: 0.0003,	0.1472 s / batch. (data: 9.63e-05). ETA=8:41:06, max mem: 5.0 GB 
[11/07 03:09:53 visual_prompt]: Epoch 4 / 100: avg data time: 5.91e-02, avg batch time: 0.2086, average train loss: 2.3108
[11/07 03:10:14 visual_prompt]: 	Test 100/246. loss: 0.676, 0.0627 s / batch. (data: 3.50e-05)max mem: 4.96204 GB 
[11/07 03:10:34 visual_prompt]: 	Test 200/246. loss: 0.462, 0.0570 s / batch. (data: 2.38e-05)max mem: 4.96204 GB 
[11/07 03:10:42 visual_prompt]: Inference (val):avg data time: 6.10e-05, avg batch time: 0.0638, average loss: 0.7307
[11/07 03:10:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 54.63	
[11/07 03:10:42 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/07 03:11:05 visual_prompt]: 	Training 100/2212. train loss: 0.6308,	0.2102 s / batch. (data: 9.30e-02). ETA=12:23:32, max mem: 5.0 GB 
[11/07 03:11:26 visual_prompt]: 	Training 200/2212. train loss: 3.8406,	0.1974 s / batch. (data: 4.57e-02). ETA=11:38:03, max mem: 5.0 GB 
[11/07 03:11:47 visual_prompt]: 	Training 300/2212. train loss: 0.1868,	0.1978 s / batch. (data: 6.83e-04). ETA=11:39:03, max mem: 5.0 GB 
[11/07 03:12:09 visual_prompt]: 	Training 400/2212. train loss: 3.1069,	0.1716 s / batch. (data: 5.36e-03). ETA=10:06:02, max mem: 5.0 GB 
[11/07 03:12:30 visual_prompt]: 	Training 500/2212. train loss: 3.7195,	0.2004 s / batch. (data: 6.61e-04). ETA=11:47:45, max mem: 5.0 GB 
[11/07 03:12:51 visual_prompt]: 	Training 600/2212. train loss: 0.9116,	0.1748 s / batch. (data: 1.59e-02). ETA=10:16:45, max mem: 5.0 GB 
[11/07 03:13:10 visual_prompt]: 	Training 700/2212. train loss: 0.2825,	0.1495 s / batch. (data: 6.76e-04). ETA=8:47:32, max mem: 5.0 GB 
[11/07 03:13:32 visual_prompt]: 	Training 800/2212. train loss: 2.1396,	0.1610 s / batch. (data: 7.79e-04). ETA=9:27:47, max mem: 5.0 GB 
[11/07 03:13:53 visual_prompt]: 	Training 900/2212. train loss: 0.1721,	0.1524 s / batch. (data: 1.73e-02). ETA=8:57:02, max mem: 5.0 GB 
[11/07 03:14:13 visual_prompt]: 	Training 1000/2212. train loss: 0.6555,	0.5869 s / batch. (data: 4.30e-01). ETA=1 day, 10:27:20, max mem: 5.0 GB 
[11/07 03:14:34 visual_prompt]: 	Training 1100/2212. train loss: 0.2483,	0.1471 s / batch. (data: 2.22e-02). ETA=8:37:49, max mem: 5.0 GB 
[11/07 03:14:54 visual_prompt]: 	Training 1200/2212. train loss: 0.3868,	0.1539 s / batch. (data: 5.34e-03). ETA=9:01:45, max mem: 5.0 GB 
[11/07 03:15:14 visual_prompt]: 	Training 1300/2212. train loss: 0.8175,	0.1669 s / batch. (data: 6.97e-04). ETA=9:46:59, max mem: 5.0 GB 
[11/07 03:15:35 visual_prompt]: 	Training 1400/2212. train loss: 0.0317,	0.1260 s / batch. (data: 2.62e-04). ETA=7:23:02, max mem: 5.0 GB 
[11/07 03:15:57 visual_prompt]: 	Training 1500/2212. train loss: 2.9419,	1.2204 s / batch. (data: 1.09e+00). ETA=2 days, 23:28:34, max mem: 5.0 GB 
[11/07 03:16:18 visual_prompt]: 	Training 1600/2212. train loss: 0.6011,	0.1488 s / batch. (data: 6.65e-04). ETA=8:42:46, max mem: 5.0 GB 
[11/07 03:16:39 visual_prompt]: 	Training 1700/2212. train loss: 1.5865,	0.1938 s / batch. (data: 5.39e-03). ETA=11:20:16, max mem: 5.0 GB 
[11/07 03:17:01 visual_prompt]: 	Training 1800/2212. train loss: 0.5778,	0.1689 s / batch. (data: 1.04e-02). ETA=9:52:35, max mem: 5.0 GB 
[11/07 03:17:22 visual_prompt]: 	Training 1900/2212. train loss: 0.6728,	0.2053 s / batch. (data: 7.96e-03). ETA=12:00:09, max mem: 5.0 GB 
[11/07 03:17:42 visual_prompt]: 	Training 2000/2212. train loss: 0.3148,	0.1583 s / batch. (data: 3.19e-04). ETA=9:14:50, max mem: 5.0 GB 
[11/07 03:18:02 visual_prompt]: 	Training 2100/2212. train loss: 2.3398,	0.1411 s / batch. (data: 2.91e-04). ETA=8:14:32, max mem: 5.0 GB 
[11/07 03:18:23 visual_prompt]: 	Training 2200/2212. train loss: 0.4832,	0.1272 s / batch. (data: 1.39e-04). ETA=7:25:40, max mem: 5.0 GB 
[11/07 03:18:24 visual_prompt]: Epoch 5 / 100: avg data time: 5.96e-02, avg batch time: 0.2088, average train loss: 1.1601
[11/07 03:18:45 visual_prompt]: 	Test 100/246. loss: 0.955, 0.1052 s / batch. (data: 2.72e-05)max mem: 4.96204 GB 
[11/07 03:19:05 visual_prompt]: 	Test 200/246. loss: 0.930, 0.0652 s / batch. (data: 4.34e-05)max mem: 4.96204 GB 
[11/07 03:19:13 visual_prompt]: Inference (val):avg data time: 1.12e-04, avg batch time: 0.0639, average loss: 0.6943
[11/07 03:19:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.90	
[11/07 03:19:13 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/07 03:19:37 visual_prompt]: 	Training 100/2212. train loss: 8.0524,	0.1518 s / batch. (data: 5.35e-03). ETA=8:51:30, max mem: 5.0 GB 
[11/07 03:19:59 visual_prompt]: 	Training 200/2212. train loss: 2.8874,	0.1565 s / batch. (data: 7.97e-03). ETA=9:07:44, max mem: 5.0 GB 
[11/07 03:20:20 visual_prompt]: 	Training 300/2212. train loss: 0.6369,	0.1700 s / batch. (data: 1.05e-02). ETA=9:54:40, max mem: 5.0 GB 
[11/07 03:20:40 visual_prompt]: 	Training 400/2212. train loss: 0.7445,	0.1348 s / batch. (data: 6.70e-04). ETA=7:51:20, max mem: 5.0 GB 
[11/07 03:21:01 visual_prompt]: 	Training 500/2212. train loss: 0.5567,	0.1557 s / batch. (data: 6.10e-04). ETA=9:03:53, max mem: 5.0 GB 
[11/07 03:21:23 visual_prompt]: 	Training 600/2212. train loss: 0.5795,	0.1486 s / batch. (data: 1.41e-02). ETA=8:38:50, max mem: 5.0 GB 
[11/07 03:21:44 visual_prompt]: 	Training 700/2212. train loss: 0.6469,	0.1704 s / batch. (data: 7.89e-03). ETA=9:54:46, max mem: 5.0 GB 
[11/07 03:22:05 visual_prompt]: 	Training 800/2212. train loss: 1.1498,	0.1783 s / batch. (data: 1.55e-02). ETA=10:22:12, max mem: 5.0 GB 
[11/07 03:22:26 visual_prompt]: 	Training 900/2212. train loss: 0.3076,	0.1642 s / batch. (data: 8.07e-03). ETA=9:32:36, max mem: 5.0 GB 
[11/07 03:22:48 visual_prompt]: 	Training 1000/2212. train loss: 0.7418,	0.0977 s / batch. (data: 1.94e-02). ETA=5:40:37, max mem: 5.0 GB 
[11/07 03:23:08 visual_prompt]: 	Training 1100/2212. train loss: 0.5077,	0.1807 s / batch. (data: 3.62e-02). ETA=10:29:25, max mem: 5.0 GB 
[11/07 03:23:27 visual_prompt]: 	Training 1200/2212. train loss: 0.6441,	0.1911 s / batch. (data: 6.81e-04). ETA=11:05:30, max mem: 5.0 GB 
[11/07 03:23:48 visual_prompt]: 	Training 1300/2212. train loss: 0.6599,	0.1874 s / batch. (data: 1.55e-02). ETA=10:52:21, max mem: 5.0 GB 
[11/07 03:24:09 visual_prompt]: 	Training 1400/2212. train loss: 0.5307,	0.1982 s / batch. (data: 2.05e-02). ETA=11:29:29, max mem: 5.0 GB 
[11/07 03:24:31 visual_prompt]: 	Training 1500/2212. train loss: 0.9448,	0.0945 s / batch. (data: 2.22e-04). ETA=5:28:43, max mem: 5.0 GB 
[11/07 03:24:52 visual_prompt]: 	Training 1600/2212. train loss: 0.6908,	0.0846 s / batch. (data: 5.29e-03). ETA=4:54:01, max mem: 5.0 GB 
[11/07 03:25:13 visual_prompt]: 	Training 1700/2212. train loss: 1.1090,	0.1487 s / batch. (data: 5.36e-03). ETA=8:36:30, max mem: 5.0 GB 
[11/07 03:25:35 visual_prompt]: 	Training 1800/2212. train loss: 0.8322,	0.1757 s / batch. (data: 1.08e-02). ETA=10:10:12, max mem: 5.0 GB 
[11/07 03:25:55 visual_prompt]: 	Training 1900/2212. train loss: 0.3297,	0.1533 s / batch. (data: 5.35e-03). ETA=8:51:53, max mem: 5.0 GB 
[11/07 03:26:14 visual_prompt]: 	Training 2000/2212. train loss: 1.0436,	0.1493 s / batch. (data: 2.32e-04). ETA=8:38:04, max mem: 5.0 GB 
[11/07 03:26:35 visual_prompt]: 	Training 2100/2212. train loss: 0.8285,	0.5662 s / batch. (data: 4.57e-01). ETA=1 day, 8:43:04, max mem: 5.0 GB 
[11/07 03:26:56 visual_prompt]: 	Training 2200/2212. train loss: 0.6114,	0.0904 s / batch. (data: 1.26e-04). ETA=5:13:17, max mem: 5.0 GB 
[11/07 03:26:56 visual_prompt]: Epoch 6 / 100: avg data time: 5.91e-02, avg batch time: 0.2094, average train loss: 0.8265
[11/07 03:27:18 visual_prompt]: 	Test 100/246. loss: 0.819, 0.0827 s / batch. (data: 5.32e-05)max mem: 4.96204 GB 
[11/07 03:27:38 visual_prompt]: 	Test 200/246. loss: 0.786, 0.1129 s / batch. (data: 2.62e-05)max mem: 4.96204 GB 
[11/07 03:27:46 visual_prompt]: Inference (val):avg data time: 9.42e-05, avg batch time: 0.0645, average loss: 0.6877
[11/07 03:27:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 53.26	
[11/07 03:27:46 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/07 03:28:08 visual_prompt]: 	Training 100/2212. train loss: 0.4933,	0.1936 s / batch. (data: 5.76e-03). ETA=11:10:39, max mem: 5.0 GB 
[11/07 03:28:31 visual_prompt]: 	Training 200/2212. train loss: 0.4095,	0.1486 s / batch. (data: 2.60e-02). ETA=8:34:24, max mem: 5.0 GB 
[11/07 03:28:54 visual_prompt]: 	Training 300/2212. train loss: 0.8333,	0.1017 s / batch. (data: 2.66e-04). ETA=5:51:55, max mem: 5.0 GB 
[11/07 03:29:16 visual_prompt]: 	Training 400/2212. train loss: 1.3346,	0.1623 s / batch. (data: 2.38e-03). ETA=9:21:31, max mem: 5.0 GB 
[11/07 03:29:37 visual_prompt]: 	Training 500/2212. train loss: 1.0666,	0.1770 s / batch. (data: 7.07e-03). ETA=10:12:01, max mem: 5.0 GB 
[11/07 03:29:57 visual_prompt]: 	Training 600/2212. train loss: 0.7474,	0.1727 s / batch. (data: 2.51e-02). ETA=9:56:40, max mem: 5.0 GB 
[11/07 03:30:18 visual_prompt]: 	Training 700/2212. train loss: 0.5928,	0.1494 s / batch. (data: 2.30e-04). ETA=8:35:51, max mem: 5.0 GB 
[11/07 03:30:38 visual_prompt]: 	Training 800/2212. train loss: 0.5319,	0.1498 s / batch. (data: 2.68e-04). ETA=8:37:09, max mem: 5.0 GB 
[11/07 03:30:59 visual_prompt]: 	Training 900/2212. train loss: 0.7339,	0.1424 s / batch. (data: 2.35e-04). ETA=8:11:30, max mem: 5.0 GB 
[11/07 03:31:20 visual_prompt]: 	Training 1000/2212. train loss: 0.8815,	0.1877 s / batch. (data: 1.08e-02). ETA=10:47:26, max mem: 5.0 GB 
[11/07 03:31:40 visual_prompt]: 	Training 1100/2212. train loss: 0.8823,	0.1769 s / batch. (data: 5.37e-03). ETA=10:09:40, max mem: 5.0 GB 
[11/07 03:32:00 visual_prompt]: 	Training 1200/2212. train loss: 0.5064,	0.1236 s / batch. (data: 2.15e-04). ETA=7:05:54, max mem: 5.0 GB 
[11/07 03:32:21 visual_prompt]: 	Training 1300/2212. train loss: 0.5952,	0.1517 s / batch. (data: 1.09e-02). ETA=8:42:31, max mem: 5.0 GB 
[11/07 03:32:41 visual_prompt]: 	Training 1400/2212. train loss: 0.6303,	0.1933 s / batch. (data: 3.31e-02). ETA=11:05:18, max mem: 5.0 GB 
[11/07 03:33:02 visual_prompt]: 	Training 1500/2212. train loss: 0.9800,	0.1497 s / batch. (data: 7.61e-03). ETA=8:35:03, max mem: 5.0 GB 
[11/07 03:33:22 visual_prompt]: 	Training 1600/2212. train loss: 0.6629,	0.2853 s / batch. (data: 1.84e-01). ETA=16:21:02, max mem: 5.0 GB 
[11/07 03:33:44 visual_prompt]: 	Training 1700/2212. train loss: 0.5867,	0.1812 s / batch. (data: 5.73e-03). ETA=10:22:40, max mem: 5.0 GB 
[11/07 03:34:04 visual_prompt]: 	Training 1800/2212. train loss: 0.4883,	0.1921 s / batch. (data: 5.34e-03). ETA=10:59:48, max mem: 5.0 GB 
[11/07 03:34:24 visual_prompt]: 	Training 1900/2212. train loss: 0.8419,	0.1680 s / batch. (data: 1.55e-02). ETA=9:36:56, max mem: 5.0 GB 
[11/07 03:34:46 visual_prompt]: 	Training 2000/2212. train loss: 0.7212,	0.1474 s / batch. (data: 5.43e-03). ETA=8:26:02, max mem: 5.0 GB 
[11/07 03:35:06 visual_prompt]: 	Training 2100/2212. train loss: 1.0538,	0.1552 s / batch. (data: 5.35e-03). ETA=8:52:28, max mem: 5.0 GB 
[11/07 03:35:27 visual_prompt]: 	Training 2200/2212. train loss: 0.6517,	0.1178 s / batch. (data: 1.38e-04). ETA=6:43:53, max mem: 5.0 GB 
[11/07 03:35:28 visual_prompt]: Epoch 7 / 100: avg data time: 5.77e-02, avg batch time: 0.2089, average train loss: 0.7113
[11/07 03:35:49 visual_prompt]: 	Test 100/246. loss: 0.818, 0.0398 s / batch. (data: 4.20e-05)max mem: 4.96204 GB 
[11/07 03:36:09 visual_prompt]: 	Test 200/246. loss: 0.794, 0.0703 s / batch. (data: 2.72e-05)max mem: 4.96204 GB 
[11/07 03:36:17 visual_prompt]: Inference (val):avg data time: 4.57e-04, avg batch time: 0.0647, average loss: 0.6860
[11/07 03:36:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 54.77	
[11/07 03:36:17 visual_prompt]: Best epoch 7: best metric: -0.686
[11/07 03:36:17 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/07 03:36:41 visual_prompt]: 	Training 100/2212. train loss: 0.5381,	0.1400 s / batch. (data: 2.45e-04). ETA=7:59:47, max mem: 5.0 GB 
[11/07 03:37:02 visual_prompt]: 	Training 200/2212. train loss: 0.6611,	0.1954 s / batch. (data: 7.22e-04). ETA=11:09:09, max mem: 5.0 GB 
[11/07 03:37:22 visual_prompt]: 	Training 300/2212. train loss: 0.8252,	0.1618 s / batch. (data: 3.04e-04). ETA=9:13:59, max mem: 5.0 GB 
[11/07 03:37:43 visual_prompt]: 	Training 400/2212. train loss: 0.4668,	0.1791 s / batch. (data: 5.36e-03). ETA=10:12:43, max mem: 5.0 GB 
[11/07 03:38:06 visual_prompt]: 	Training 500/2212. train loss: 0.5776,	0.6222 s / batch. (data: 5.02e-01). ETA=1 day, 11:28:09, max mem: 5.0 GB 
[11/07 03:38:26 visual_prompt]: 	Training 600/2212. train loss: 0.5247,	0.1844 s / batch. (data: 1.08e-02). ETA=10:30:23, max mem: 5.0 GB 
[11/07 03:38:47 visual_prompt]: 	Training 700/2212. train loss: 0.5023,	0.1957 s / batch. (data: 1.04e-02). ETA=11:08:32, max mem: 5.0 GB 
[11/07 03:39:07 visual_prompt]: 	Training 800/2212. train loss: 0.5908,	0.1412 s / batch. (data: 2.29e-04). ETA=8:02:14, max mem: 5.0 GB 
[11/07 03:39:28 visual_prompt]: 	Training 900/2212. train loss: 0.7290,	0.1563 s / batch. (data: 1.38e-02). ETA=8:53:26, max mem: 5.0 GB 
[11/07 03:39:50 visual_prompt]: 	Training 1000/2212. train loss: 0.9254,	0.1231 s / batch. (data: 2.64e-04). ETA=6:59:59, max mem: 5.0 GB 
[11/07 03:40:10 visual_prompt]: 	Training 1100/2212. train loss: 0.4358,	0.1819 s / batch. (data: 1.04e-02). ETA=10:20:19, max mem: 5.0 GB 
[11/07 03:40:31 visual_prompt]: 	Training 1200/2212. train loss: 0.6279,	0.1583 s / batch. (data: 2.54e-04). ETA=8:59:28, max mem: 5.0 GB 
[11/07 03:40:52 visual_prompt]: 	Training 1300/2212. train loss: 0.7632,	0.1596 s / batch. (data: 5.37e-03). ETA=9:03:53, max mem: 5.0 GB 
[11/07 03:41:12 visual_prompt]: 	Training 1400/2212. train loss: 0.7113,	0.2041 s / batch. (data: 3.23e-02). ETA=11:34:54, max mem: 5.0 GB 
[11/07 03:41:35 visual_prompt]: 	Training 1500/2212. train loss: 0.5616,	0.6086 s / batch. (data: 5.11e-01). ETA=1 day, 10:31:25, max mem: 5.0 GB 
[11/07 03:41:56 visual_prompt]: 	Training 1600/2212. train loss: 1.0124,	0.1482 s / batch. (data: 1.01e-02). ETA=8:24:09, max mem: 5.0 GB 
[11/07 03:42:16 visual_prompt]: 	Training 1700/2212. train loss: 0.8476,	0.2369 s / batch. (data: 1.32e-01). ETA=13:25:31, max mem: 5.0 GB 
[11/07 03:42:37 visual_prompt]: 	Training 1800/2212. train loss: 0.6657,	0.0948 s / batch. (data: 2.27e-04). ETA=5:22:04, max mem: 5.0 GB 
[11/07 03:42:57 visual_prompt]: 	Training 1900/2212. train loss: 0.6288,	0.1957 s / batch. (data: 5.77e-03). ETA=11:04:47, max mem: 5.0 GB 
[11/07 03:43:19 visual_prompt]: 	Training 2000/2212. train loss: 1.6548,	0.1284 s / batch. (data: 5.34e-03). ETA=7:16:04, max mem: 5.0 GB 
[11/07 03:43:39 visual_prompt]: 	Training 2100/2212. train loss: 0.5465,	0.1336 s / batch. (data: 1.55e-02). ETA=7:33:31, max mem: 5.0 GB 
[11/07 03:43:58 visual_prompt]: 	Training 2200/2212. train loss: 0.6867,	0.1241 s / batch. (data: 1.52e-04). ETA=7:01:03, max mem: 5.0 GB 
[11/07 03:43:59 visual_prompt]: Epoch 8 / 100: avg data time: 5.83e-02, avg batch time: 0.2089, average train loss: 0.7033
[11/07 03:44:21 visual_prompt]: 	Test 100/246. loss: 0.781, 0.0660 s / batch. (data: 3.72e-05)max mem: 4.96204 GB 
[11/07 03:44:41 visual_prompt]: 	Test 200/246. loss: 0.774, 0.0722 s / batch. (data: 3.29e-05)max mem: 4.96204 GB 
[11/07 03:44:49 visual_prompt]: Inference (val):avg data time: 4.21e-04, avg batch time: 0.0642, average loss: 0.6867
[11/07 03:44:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.44	
[11/07 03:44:49 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/07 03:45:13 visual_prompt]: 	Training 100/2212. train loss: 0.5960,	0.2310 s / batch. (data: 1.18e-01). ETA=13:03:03, max mem: 5.0 GB 
[11/07 03:45:35 visual_prompt]: 	Training 200/2212. train loss: 0.7040,	0.1559 s / batch. (data: 5.39e-03). ETA=8:48:05, max mem: 5.0 GB 
[11/07 03:45:58 visual_prompt]: 	Training 300/2212. train loss: 0.9414,	0.1229 s / batch. (data: 2.40e-04). ETA=6:56:18, max mem: 5.0 GB 
[11/07 03:46:18 visual_prompt]: 	Training 400/2212. train loss: 0.9078,	0.1725 s / batch. (data: 5.40e-03). ETA=9:44:01, max mem: 5.0 GB 
[11/07 03:46:40 visual_prompt]: 	Training 500/2212. train loss: 0.7228,	0.0780 s / batch. (data: 1.11e-02). ETA=4:23:50, max mem: 5.0 GB 
[11/07 03:47:01 visual_prompt]: 	Training 600/2212. train loss: 1.1072,	0.1748 s / batch. (data: 1.59e-02). ETA=9:51:08, max mem: 5.0 GB 
[11/07 03:47:22 visual_prompt]: 	Training 700/2212. train loss: 0.4215,	0.1581 s / batch. (data: 1.04e-02). ETA=8:54:15, max mem: 5.0 GB 
[11/07 03:47:41 visual_prompt]: 	Training 800/2212. train loss: 0.7263,	0.1945 s / batch. (data: 1.55e-02). ETA=10:57:13, max mem: 5.0 GB 
[11/07 03:48:02 visual_prompt]: 	Training 900/2212. train loss: 0.6117,	0.1487 s / batch. (data: 5.35e-03). ETA=8:22:06, max mem: 5.0 GB 
[11/07 03:48:22 visual_prompt]: 	Training 1000/2212. train loss: 0.8748,	0.1532 s / batch. (data: 5.34e-03). ETA=8:37:04, max mem: 5.0 GB 
[11/07 03:48:44 visual_prompt]: 	Training 1100/2212. train loss: 0.4502,	0.8926 s / batch. (data: 7.81e-01). ETA=2 days, 2:11:14, max mem: 5.0 GB 
[11/07 03:49:04 visual_prompt]: 	Training 1200/2212. train loss: 1.9776,	0.1687 s / batch. (data: 1.04e-02). ETA=9:28:50, max mem: 5.0 GB 
[11/07 03:49:24 visual_prompt]: 	Training 1300/2212. train loss: 0.9315,	0.1850 s / batch. (data: 7.33e-04). ETA=10:23:18, max mem: 5.0 GB 
[11/07 03:49:43 visual_prompt]: 	Training 1400/2212. train loss: 0.9700,	0.1727 s / batch. (data: 1.55e-02). ETA=9:41:48, max mem: 5.0 GB 
[11/07 03:50:04 visual_prompt]: 	Training 1500/2212. train loss: 0.8354,	0.1835 s / batch. (data: 7.25e-04). ETA=10:17:39, max mem: 5.0 GB 
[11/07 03:50:25 visual_prompt]: 	Training 1600/2212. train loss: 0.6112,	0.1647 s / batch. (data: 2.33e-02). ETA=9:14:23, max mem: 5.0 GB 
[11/07 03:50:46 visual_prompt]: 	Training 1700/2212. train loss: 1.4100,	0.1788 s / batch. (data: 5.35e-03). ETA=10:01:32, max mem: 5.0 GB 
[11/07 03:51:06 visual_prompt]: 	Training 1800/2212. train loss: 0.6494,	1.1508 s / batch. (data: 1.06e+00). ETA=2 days, 16:28:34, max mem: 5.0 GB 
[11/07 03:51:25 visual_prompt]: 	Training 1900/2212. train loss: 0.8497,	0.1461 s / batch. (data: 2.35e-04). ETA=8:10:52, max mem: 5.0 GB 
[11/07 03:51:46 visual_prompt]: 	Training 2000/2212. train loss: 0.8602,	0.1445 s / batch. (data: 1.76e-02). ETA=8:05:17, max mem: 5.0 GB 
[11/07 03:52:07 visual_prompt]: 	Training 2100/2212. train loss: 0.7933,	0.2567 s / batch. (data: 9.95e-02). ETA=14:21:46, max mem: 5.0 GB 
[11/07 03:52:30 visual_prompt]: 	Training 2200/2212. train loss: 0.4362,	0.1382 s / batch. (data: 1.37e-04). ETA=7:43:50, max mem: 5.0 GB 
[11/07 03:52:31 visual_prompt]: Epoch 9 / 100: avg data time: 5.99e-02, avg batch time: 0.2089, average train loss: 0.7048
[11/07 03:52:52 visual_prompt]: 	Test 100/246. loss: 0.983, 0.0851 s / batch. (data: 2.91e-05)max mem: 4.96204 GB 
[11/07 03:53:12 visual_prompt]: 	Test 200/246. loss: 0.969, 0.0654 s / batch. (data: 3.60e-05)max mem: 4.96204 GB 
[11/07 03:53:20 visual_prompt]: Inference (val):avg data time: 2.68e-04, avg batch time: 0.0614, average loss: 0.6960
[11/07 03:53:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.92	
[11/07 03:53:20 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/07 03:53:44 visual_prompt]: 	Training 100/2212. train loss: 0.5162,	0.2244 s / batch. (data: 1.12e-01). ETA=12:32:21, max mem: 5.0 GB 
[11/07 03:54:05 visual_prompt]: 	Training 200/2212. train loss: 0.8644,	0.4369 s / batch. (data: 2.98e-01). ETA=1 day, 0:24:23, max mem: 5.0 GB 
[11/07 03:54:26 visual_prompt]: 	Training 300/2212. train loss: 0.9734,	0.1965 s / batch. (data: 6.85e-04). ETA=10:58:24, max mem: 5.0 GB 
[11/07 03:54:46 visual_prompt]: 	Training 400/2212. train loss: 1.5546,	0.1102 s / batch. (data: 2.20e-03). ETA=6:08:56, max mem: 5.0 GB 
[11/07 03:55:06 visual_prompt]: 	Training 500/2212. train loss: 0.1165,	0.1741 s / batch. (data: 5.76e-03). ETA=9:42:28, max mem: 5.0 GB 
[11/07 03:55:28 visual_prompt]: 	Training 600/2212. train loss: 0.7048,	0.1901 s / batch. (data: 1.01e-02). ETA=10:36:00, max mem: 5.0 GB 
[11/07 03:55:49 visual_prompt]: 	Training 700/2212. train loss: 0.6826,	0.5004 s / batch. (data: 3.73e-01). ETA=1 day, 3:52:47, max mem: 5.0 GB 
[11/07 03:56:11 visual_prompt]: 	Training 800/2212. train loss: 0.7562,	0.2056 s / batch. (data: 6.69e-04). ETA=11:27:02, max mem: 5.0 GB 
[11/07 03:56:32 visual_prompt]: 	Training 900/2212. train loss: 0.1490,	1.6522 s / batch. (data: 1.54e+00). ETA=3 days, 19:58:04, max mem: 5.0 GB 
[11/07 03:56:52 visual_prompt]: 	Training 1000/2212. train loss: 0.4434,	0.1588 s / batch. (data: 2.50e-04). ETA=8:50:08, max mem: 5.0 GB 
[11/07 03:57:14 visual_prompt]: 	Training 1100/2212. train loss: 0.6106,	0.1456 s / batch. (data: 2.42e-04). ETA=8:05:44, max mem: 5.0 GB 
[11/07 03:57:34 visual_prompt]: 	Training 1200/2212. train loss: 1.1842,	0.1685 s / batch. (data: 1.05e-02). ETA=9:21:52, max mem: 5.0 GB 
[11/07 03:57:54 visual_prompt]: 	Training 1300/2212. train loss: 1.7408,	0.1413 s / batch. (data: 2.24e-04). ETA=7:50:56, max mem: 5.0 GB 
[11/07 03:58:15 visual_prompt]: 	Training 1400/2212. train loss: 2.0710,	0.1234 s / batch. (data: 6.66e-04). ETA=6:50:59, max mem: 5.0 GB 
[11/07 03:58:36 visual_prompt]: 	Training 1500/2212. train loss: 0.7412,	0.1489 s / batch. (data: 5.43e-03). ETA=8:15:44, max mem: 5.0 GB 
[11/07 03:58:56 visual_prompt]: 	Training 1600/2212. train loss: 0.6461,	0.1916 s / batch. (data: 2.19e-02). ETA=10:37:36, max mem: 5.0 GB 
[11/07 03:59:17 visual_prompt]: 	Training 1700/2212. train loss: 0.1907,	0.1167 s / batch. (data: 2.38e-04). ETA=6:28:17, max mem: 5.0 GB 
[11/07 03:59:39 visual_prompt]: 	Training 1800/2212. train loss: 1.1446,	0.1271 s / batch. (data: 5.36e-03). ETA=7:02:28, max mem: 5.0 GB 
[11/07 03:59:58 visual_prompt]: 	Training 1900/2212. train loss: 0.6498,	0.2140 s / batch. (data: 5.76e-03). ETA=11:51:02, max mem: 5.0 GB 
[11/07 04:00:19 visual_prompt]: 	Training 2000/2212. train loss: 0.6350,	0.1528 s / batch. (data: 1.05e-02). ETA=8:27:23, max mem: 5.0 GB 
[11/07 04:00:41 visual_prompt]: 	Training 2100/2212. train loss: 0.6342,	0.1299 s / batch. (data: 9.86e-03). ETA=7:11:22, max mem: 5.0 GB 
[11/07 04:01:01 visual_prompt]: 	Training 2200/2212. train loss: 0.5056,	0.5272 s / batch. (data: 4.31e-01). ETA=1 day, 5:09:16, max mem: 5.0 GB 
[11/07 04:01:02 visual_prompt]: Epoch 10 / 100: avg data time: 5.86e-02, avg batch time: 0.2089, average train loss: 0.7223
[11/07 04:01:24 visual_prompt]: 	Test 100/246. loss: 0.560, 0.1049 s / batch. (data: 2.43e-05)max mem: 4.96204 GB 
[11/07 04:01:43 visual_prompt]: 	Test 200/246. loss: 0.537, 0.0596 s / batch. (data: 3.24e-05)max mem: 4.96204 GB 
[11/07 04:01:52 visual_prompt]: Inference (val):avg data time: 1.64e-04, avg batch time: 0.0627, average loss: 0.7170
[11/07 04:01:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 55.47	
[11/07 04:01:52 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/07 04:02:15 visual_prompt]: 	Training 100/2212. train loss: 0.4312,	0.1702 s / batch. (data: 2.67e-04). ETA=9:24:25, max mem: 5.0 GB 
[11/07 04:02:37 visual_prompt]: 	Training 200/2212. train loss: 0.8918,	0.1938 s / batch. (data: 1.09e-02). ETA=10:42:28, max mem: 5.0 GB 
[11/07 04:02:59 visual_prompt]: 	Training 300/2212. train loss: 0.9318,	0.1659 s / batch. (data: 1.04e-02). ETA=9:09:42, max mem: 5.0 GB 
[11/07 04:03:20 visual_prompt]: 	Training 400/2212. train loss: 0.9753,	0.1906 s / batch. (data: 6.97e-04). ETA=10:31:08, max mem: 5.0 GB 
[11/07 04:03:41 visual_prompt]: 	Training 500/2212. train loss: 0.7405,	0.1560 s / batch. (data: 6.91e-04). ETA=8:36:25, max mem: 5.0 GB 
[11/07 04:04:01 visual_prompt]: 	Training 600/2212. train loss: 0.6467,	0.1040 s / batch. (data: 6.23e-04). ETA=5:44:08, max mem: 5.0 GB 
[11/07 04:04:24 visual_prompt]: 	Training 700/2212. train loss: 0.8933,	0.8152 s / batch. (data: 7.01e-01). ETA=1 day, 20:55:17, max mem: 5.0 GB 
[11/07 04:04:42 visual_prompt]: 	Training 800/2212. train loss: 0.7081,	0.1538 s / batch. (data: 2.42e-04). ETA=8:28:14, max mem: 5.0 GB 
[11/07 04:05:03 visual_prompt]: 	Training 900/2212. train loss: 0.7241,	0.1007 s / batch. (data: 2.26e-04). ETA=5:32:30, max mem: 5.0 GB 
[11/07 04:05:23 visual_prompt]: 	Training 1000/2212. train loss: 0.5655,	0.1636 s / batch. (data: 7.99e-03). ETA=9:00:04, max mem: 5.0 GB 
[11/07 04:05:44 visual_prompt]: 	Training 1100/2212. train loss: 0.4373,	0.1475 s / batch. (data: 6.65e-04). ETA=8:06:45, max mem: 5.0 GB 
[11/07 04:06:04 visual_prompt]: 	Training 1200/2212. train loss: 0.8494,	0.1640 s / batch. (data: 5.78e-03). ETA=9:00:53, max mem: 5.0 GB 
[11/07 04:06:25 visual_prompt]: 	Training 1300/2212. train loss: 0.5278,	0.1855 s / batch. (data: 5.78e-03). ETA=10:11:36, max mem: 5.0 GB 
[11/07 04:06:46 visual_prompt]: 	Training 1400/2212. train loss: 1.8662,	0.4162 s / batch. (data: 2.89e-01). ETA=22:51:19, max mem: 5.0 GB 
[11/07 04:07:07 visual_prompt]: 	Training 1500/2212. train loss: 0.1914,	0.1867 s / batch. (data: 1.55e-02). ETA=10:14:49, max mem: 5.0 GB 
[11/07 04:07:28 visual_prompt]: 	Training 1600/2212. train loss: 0.4682,	0.1511 s / batch. (data: 2.45e-04). ETA=8:17:19, max mem: 5.0 GB 
[11/07 04:07:49 visual_prompt]: 	Training 1700/2212. train loss: 0.2202,	0.1532 s / batch. (data: 2.40e-04). ETA=8:24:04, max mem: 5.0 GB 
[11/07 04:08:09 visual_prompt]: 	Training 1800/2212. train loss: 0.3854,	0.1913 s / batch. (data: 1.04e-02). ETA=10:29:04, max mem: 5.0 GB 
[11/07 04:08:31 visual_prompt]: 	Training 1900/2212. train loss: 0.5018,	0.1665 s / batch. (data: 3.23e-02). ETA=9:07:11, max mem: 5.0 GB 
[11/07 04:08:52 visual_prompt]: 	Training 2000/2212. train loss: 0.5685,	0.1352 s / batch. (data: 2.44e-04). ETA=7:24:13, max mem: 5.0 GB 
[11/07 04:09:12 visual_prompt]: 	Training 2100/2212. train loss: 0.4459,	0.1570 s / batch. (data: 1.04e-02). ETA=8:35:27, max mem: 5.0 GB 
[11/07 04:09:33 visual_prompt]: 	Training 2200/2212. train loss: 0.7022,	0.0979 s / batch. (data: 1.17e-04). ETA=5:21:20, max mem: 5.0 GB 
[11/07 04:09:34 visual_prompt]: Epoch 11 / 100: avg data time: 5.95e-02, avg batch time: 0.2088, average train loss: 0.7170
[11/07 04:09:55 visual_prompt]: 	Test 100/246. loss: 0.748, 0.1331 s / batch. (data: 6.13e-05)max mem: 4.96204 GB 
[11/07 04:10:15 visual_prompt]: 	Test 200/246. loss: 0.718, 0.0993 s / batch. (data: 2.67e-05)max mem: 4.96204 GB 
[11/07 04:10:23 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.0635, average loss: 0.6867
[11/07 04:10:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 55.42	
[11/07 04:10:23 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/07 04:10:45 visual_prompt]: 	Training 100/2212. train loss: 0.5137,	0.1678 s / batch. (data: 5.34e-03). ETA=9:10:12, max mem: 5.0 GB 
[11/07 04:11:05 visual_prompt]: 	Training 200/2212. train loss: 0.3725,	0.0886 s / batch. (data: 2.33e-04). ETA=4:50:23, max mem: 5.0 GB 
[11/07 04:11:27 visual_prompt]: 	Training 300/2212. train loss: 0.9190,	0.1866 s / batch. (data: 2.48e-02). ETA=10:11:15, max mem: 5.0 GB 
[11/07 04:11:48 visual_prompt]: 	Training 400/2212. train loss: 1.0141,	0.1598 s / batch. (data: 1.04e-02). ETA=8:43:10, max mem: 5.0 GB 
[11/07 04:12:09 visual_prompt]: 	Training 500/2212. train loss: 0.5680,	0.1771 s / batch. (data: 5.36e-03). ETA=9:39:33, max mem: 5.0 GB 
[11/07 04:12:31 visual_prompt]: 	Training 600/2212. train loss: 0.7194,	0.1577 s / batch. (data: 9.16e-03). ETA=8:36:00, max mem: 5.0 GB 
[11/07 04:12:52 visual_prompt]: 	Training 700/2212. train loss: 0.6565,	0.1703 s / batch. (data: 6.73e-04). ETA=9:16:40, max mem: 5.0 GB 
[11/07 04:13:14 visual_prompt]: 	Training 800/2212. train loss: 1.0247,	0.1877 s / batch. (data: 6.94e-04). ETA=10:13:12, max mem: 5.0 GB 
[11/07 04:13:35 visual_prompt]: 	Training 900/2212. train loss: 0.6395,	0.9310 s / batch. (data: 8.28e-01). ETA=2 days, 2:40:45, max mem: 5.0 GB 
[11/07 04:13:56 visual_prompt]: 	Training 1000/2212. train loss: 0.5032,	0.1441 s / batch. (data: 2.41e-04). ETA=7:50:26, max mem: 5.0 GB 
[11/07 04:14:16 visual_prompt]: 	Training 1100/2212. train loss: 0.9333,	0.5351 s / batch. (data: 4.20e-01). ETA=1 day, 5:05:57, max mem: 5.0 GB 
[11/07 04:14:36 visual_prompt]: 	Training 1200/2212. train loss: 1.1090,	0.1939 s / batch. (data: 1.59e-02). ETA=10:32:12, max mem: 5.0 GB 
[11/07 04:14:56 visual_prompt]: 	Training 1300/2212. train loss: 0.7957,	0.2089 s / batch. (data: 1.04e-02). ETA=11:20:50, max mem: 5.0 GB 
[11/07 04:15:17 visual_prompt]: 	Training 1400/2212. train loss: 0.5366,	0.1278 s / batch. (data: 2.35e-04). ETA=6:56:18, max mem: 5.0 GB 
[11/07 04:15:39 visual_prompt]: 	Training 1500/2212. train loss: 0.3787,	0.4025 s / batch. (data: 2.68e-01). ETA=21:50:28, max mem: 5.0 GB 
[11/07 04:15:59 visual_prompt]: 	Training 1600/2212. train loss: 0.3962,	0.1749 s / batch. (data: 1.99e-02). ETA=9:29:15, max mem: 5.0 GB 
[11/07 04:16:20 visual_prompt]: 	Training 1700/2212. train loss: 1.0494,	0.1950 s / batch. (data: 3.41e-02). ETA=10:34:08, max mem: 5.0 GB 
[11/07 04:16:42 visual_prompt]: 	Training 1800/2212. train loss: 0.3303,	0.0818 s / batch. (data: 8.11e-03). ETA=4:25:56, max mem: 5.0 GB 
[11/07 04:17:02 visual_prompt]: 	Training 1900/2212. train loss: 0.1529,	0.1424 s / batch. (data: 1.55e-02). ETA=7:42:45, max mem: 5.0 GB 
[11/07 04:17:22 visual_prompt]: 	Training 2000/2212. train loss: 0.9263,	0.1526 s / batch. (data: 6.79e-04). ETA=8:15:31, max mem: 5.0 GB 
[11/07 04:17:44 visual_prompt]: 	Training 2100/2212. train loss: 1.1611,	0.9756 s / batch. (data: 8.90e-01). ETA=2 days, 4:47:02, max mem: 5.0 GB 
[11/07 04:18:04 visual_prompt]: 	Training 2200/2212. train loss: 0.6348,	0.0880 s / batch. (data: 1.52e-04). ETA=4:45:32, max mem: 5.0 GB 
[11/07 04:18:06 visual_prompt]: Epoch 12 / 100: avg data time: 5.87e-02, avg batch time: 0.2090, average train loss: 0.7211
[11/07 04:18:27 visual_prompt]: 	Test 100/246. loss: 0.537, 0.0622 s / batch. (data: 2.84e-05)max mem: 4.96204 GB 
[11/07 04:18:47 visual_prompt]: 	Test 200/246. loss: 0.505, 0.1009 s / batch. (data: 2.86e-05)max mem: 4.96204 GB 
[11/07 04:18:55 visual_prompt]: Inference (val):avg data time: 1.21e-04, avg batch time: 0.0625, average loss: 0.7435
[11/07 04:18:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.30	
[11/07 04:18:55 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/07 04:19:19 visual_prompt]: 	Training 100/2212. train loss: 0.8348,	0.0966 s / batch. (data: 2.87e-04). ETA=5:13:06, max mem: 5.0 GB 
[11/07 04:19:40 visual_prompt]: 	Training 200/2212. train loss: 0.9509,	0.1662 s / batch. (data: 6.25e-03). ETA=8:58:34, max mem: 5.0 GB 
[11/07 04:20:01 visual_prompt]: 	Training 300/2212. train loss: 0.7179,	0.0937 s / batch. (data: 6.83e-04). ETA=5:03:23, max mem: 5.0 GB 
[11/07 04:20:20 visual_prompt]: 	Training 400/2212. train loss: 0.7148,	0.1598 s / batch. (data: 7.32e-04). ETA=8:37:22, max mem: 5.0 GB 
[11/07 04:20:41 visual_prompt]: 	Training 500/2212. train loss: 0.8544,	0.2152 s / batch. (data: 1.55e-02). ETA=11:36:20, max mem: 5.0 GB 
[11/07 04:21:02 visual_prompt]: 	Training 600/2212. train loss: 0.4549,	0.2211 s / batch. (data: 3.34e-02). ETA=11:55:00, max mem: 5.0 GB 
[11/07 04:21:24 visual_prompt]: 	Training 700/2212. train loss: 0.5320,	0.1320 s / batch. (data: 2.31e-04). ETA=7:06:41, max mem: 5.0 GB 
[11/07 04:21:46 visual_prompt]: 	Training 800/2212. train loss: 0.6787,	0.1813 s / batch. (data: 9.27e-03). ETA=9:45:51, max mem: 5.0 GB 
[11/07 04:22:06 visual_prompt]: 	Training 900/2212. train loss: 0.5202,	0.1739 s / batch. (data: 1.04e-02). ETA=9:21:40, max mem: 5.0 GB 
[11/07 04:22:26 visual_prompt]: 	Training 1000/2212. train loss: 0.3952,	0.1164 s / batch. (data: 5.36e-03). ETA=6:15:45, max mem: 5.0 GB 
[11/07 04:22:48 visual_prompt]: 	Training 1100/2212. train loss: 0.8370,	0.1863 s / batch. (data: 6.48e-04). ETA=10:01:04, max mem: 5.0 GB 
[11/07 04:23:10 visual_prompt]: 	Training 1200/2212. train loss: 0.5227,	0.1281 s / batch. (data: 8.45e-03). ETA=6:52:52, max mem: 5.0 GB 
[11/07 04:23:30 visual_prompt]: 	Training 1300/2212. train loss: 0.9590,	0.2064 s / batch. (data: 2.05e-02). ETA=11:05:10, max mem: 5.0 GB 
[11/07 04:23:51 visual_prompt]: 	Training 1400/2212. train loss: 0.9405,	0.1346 s / batch. (data: 1.04e-02). ETA=7:13:37, max mem: 5.0 GB 
[11/07 04:24:13 visual_prompt]: 	Training 1500/2212. train loss: 0.5106,	0.2165 s / batch. (data: 6.77e-04). ETA=11:37:05, max mem: 5.0 GB 
[11/07 04:24:33 visual_prompt]: 	Training 1600/2212. train loss: 1.0378,	0.1621 s / batch. (data: 5.39e-03). ETA=8:41:38, max mem: 5.0 GB 
[11/07 04:24:54 visual_prompt]: 	Training 1700/2212. train loss: 0.7319,	0.1453 s / batch. (data: 5.37e-03). ETA=7:47:21, max mem: 5.0 GB 
[11/07 04:25:14 visual_prompt]: 	Training 1800/2212. train loss: 0.7993,	0.1356 s / batch. (data: 2.65e-04). ETA=7:15:58, max mem: 5.0 GB 
[11/07 04:25:35 visual_prompt]: 	Training 1900/2212. train loss: 0.3683,	0.2580 s / batch. (data: 1.34e-01). ETA=13:49:00, max mem: 5.0 GB 
[11/07 04:25:56 visual_prompt]: 	Training 2000/2212. train loss: 0.3823,	0.1569 s / batch. (data: 2.09e-02). ETA=8:23:55, max mem: 5.0 GB 
[11/07 04:26:16 visual_prompt]: 	Training 2100/2212. train loss: 0.6845,	0.1648 s / batch. (data: 9.75e-03). ETA=8:48:59, max mem: 5.0 GB 
[11/07 04:26:36 visual_prompt]: 	Training 2200/2212. train loss: 0.5594,	0.1227 s / batch. (data: 1.17e-04). ETA=6:33:38, max mem: 5.0 GB 
[11/07 04:26:37 visual_prompt]: Epoch 13 / 100: avg data time: 5.83e-02, avg batch time: 0.2091, average train loss: 0.6989
[11/07 04:26:59 visual_prompt]: 	Test 100/246. loss: 0.825, 0.0619 s / batch. (data: 2.31e-05)max mem: 4.96204 GB 
[11/07 04:27:19 visual_prompt]: 	Test 200/246. loss: 0.801, 0.0865 s / batch. (data: 2.65e-05)max mem: 4.96204 GB 
[11/07 04:27:27 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.0622, average loss: 0.6869
[11/07 04:27:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 52.98	
[11/07 04:27:27 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/07 04:27:50 visual_prompt]: 	Training 100/2212. train loss: 0.5547,	0.1923 s / batch. (data: 2.02e-02). ETA=10:16:35, max mem: 5.0 GB 
[11/07 04:28:12 visual_prompt]: 	Training 200/2212. train loss: 0.8110,	0.1602 s / batch. (data: 5.35e-03). ETA=8:33:20, max mem: 5.0 GB 
[11/07 04:28:34 visual_prompt]: 	Training 300/2212. train loss: 0.7759,	0.1301 s / batch. (data: 6.30e-03). ETA=6:56:30, max mem: 5.0 GB 
[11/07 04:28:54 visual_prompt]: 	Training 400/2212. train loss: 1.0000,	0.0778 s / batch. (data: 2.15e-04). ETA=4:09:07, max mem: 5.0 GB 
[11/07 04:29:15 visual_prompt]: 	Training 500/2212. train loss: 0.7927,	0.1543 s / batch. (data: 2.47e-04). ETA=8:13:44, max mem: 5.0 GB 
[11/07 04:29:37 visual_prompt]: 	Training 600/2212. train loss: 0.5388,	0.1618 s / batch. (data: 5.83e-03). ETA=8:37:21, max mem: 5.0 GB 
[11/07 04:29:59 visual_prompt]: 	Training 700/2212. train loss: 1.2196,	0.2164 s / batch. (data: 5.77e-03). ETA=11:31:35, max mem: 5.0 GB 
[11/07 04:30:21 visual_prompt]: 	Training 800/2212. train loss: 0.6616,	0.1638 s / batch. (data: 1.04e-02). ETA=8:43:16, max mem: 5.0 GB 
[11/07 04:30:40 visual_prompt]: 	Training 900/2212. train loss: 0.4680,	0.1875 s / batch. (data: 6.71e-03). ETA=9:58:25, max mem: 5.0 GB 
[11/07 04:31:01 visual_prompt]: 	Training 1000/2212. train loss: 0.8206,	0.1719 s / batch. (data: 1.69e-02). ETA=9:08:36, max mem: 5.0 GB 
[11/07 04:31:20 visual_prompt]: 	Training 1100/2212. train loss: 0.9574,	0.2004 s / batch. (data: 2.98e-02). ETA=10:39:10, max mem: 5.0 GB 
[11/07 04:31:42 visual_prompt]: 	Training 1200/2212. train loss: 1.0985,	0.1212 s / batch. (data: 7.50e-03). ETA=6:26:20, max mem: 5.0 GB 
[11/07 04:32:02 visual_prompt]: 	Training 1300/2212. train loss: 0.6645,	0.1725 s / batch. (data: 5.37e-03). ETA=9:09:39, max mem: 5.0 GB 
[11/07 04:32:23 visual_prompt]: 	Training 1400/2212. train loss: 0.7154,	0.7004 s / batch. (data: 5.89e-01). ETA=1 day, 13:10:07, max mem: 5.0 GB 
[11/07 04:32:44 visual_prompt]: 	Training 1500/2212. train loss: 0.4450,	0.1593 s / batch. (data: 1.04e-02). ETA=8:26:51, max mem: 5.0 GB 
[11/07 04:33:04 visual_prompt]: 	Training 1600/2212. train loss: 1.1486,	0.1453 s / batch. (data: 5.36e-03). ETA=7:42:13, max mem: 5.0 GB 
[11/07 04:33:25 visual_prompt]: 	Training 1700/2212. train loss: 0.4859,	0.1605 s / batch. (data: 6.53e-04). ETA=8:30:12, max mem: 5.0 GB 
[11/07 04:33:46 visual_prompt]: 	Training 1800/2212. train loss: 0.9934,	0.1923 s / batch. (data: 5.34e-03). ETA=10:11:09, max mem: 5.0 GB 
[11/07 04:34:07 visual_prompt]: 	Training 1900/2212. train loss: 0.9182,	0.1314 s / batch. (data: 2.45e-04). ETA=6:57:18, max mem: 5.0 GB 
[11/07 04:34:27 visual_prompt]: 	Training 2000/2212. train loss: 0.7116,	0.1320 s / batch. (data: 2.38e-04). ETA=6:58:49, max mem: 5.0 GB 
[11/07 04:34:47 visual_prompt]: 	Training 2100/2212. train loss: 1.0061,	0.1352 s / batch. (data: 2.49e-04). ETA=7:08:54, max mem: 5.0 GB 
[11/07 04:35:08 visual_prompt]: 	Training 2200/2212. train loss: 1.3990,	0.1215 s / batch. (data: 1.50e-04). ETA=6:25:08, max mem: 5.0 GB 
[11/07 04:35:09 visual_prompt]: Epoch 14 / 100: avg data time: 5.94e-02, avg batch time: 0.2089, average train loss: 0.7048
[11/07 04:35:30 visual_prompt]: 	Test 100/246. loss: 0.628, 0.0888 s / batch. (data: 3.91e-05)max mem: 4.96204 GB 
[11/07 04:35:50 visual_prompt]: 	Test 200/246. loss: 0.615, 0.0652 s / batch. (data: 2.34e-05)max mem: 4.96204 GB 
[11/07 04:35:58 visual_prompt]: Inference (val):avg data time: 3.43e-04, avg batch time: 0.0638, average loss: 0.7054
[11/07 04:35:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 51.83	
[11/07 04:35:58 visual_prompt]: Stopping early.
[11/07 04:35:58 visual_prompt]: Rank of current process: 0. World size: 1
[11/07 04:35:58 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/07 04:35:58 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '1', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/07 04:35:58 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/07 04:35:58 visual_prompt]: Training with config:
[11/07 04:35:58 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/crop224/val/seed0/lr0.001_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 1, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/07 04:35:58 visual_prompt]: Loading training data...
[11/07 04:35:58 visual_prompt]: Constructing mammo-cbis dataset train...
[11/07 04:35:58 visual_prompt]: Loading validation data...
[11/07 04:35:58 visual_prompt]: Constructing mammo-cbis dataset val...
[11/07 04:35:58 visual_prompt]: Constructing models...
[11/07 04:36:00 visual_prompt]: Enable all parameters update during training
[11/07 04:36:00 visual_prompt]: Total Parameters: 85800194	 Gradient Parameters: 85800194
[11/07 04:36:00 visual_prompt]: tuned percent:100.000
[11/07 04:36:00 visual_prompt]: Device used for model: 0
[11/07 04:36:00 visual_prompt]: Setting up Evaluator...
[11/07 04:36:00 visual_prompt]: Setting up Trainer...
[11/07 04:36:00 visual_prompt]: 	Setting up the optimizer...
[11/07 04:36:00 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/07 04:36:24 visual_prompt]: 	Training 100/2212. train loss: 17.0073,	0.2008 s / batch. (data: 7.08e-04). ETA=12:20:00, max mem: 5.0 GB 
[11/07 04:36:46 visual_prompt]: 	Training 200/2212. train loss: 8.7706,	0.1967 s / batch. (data: 6.76e-04). ETA=12:04:27, max mem: 5.0 GB 
[11/07 04:37:07 visual_prompt]: 	Training 300/2212. train loss: 0.0001,	0.2104 s / batch. (data: 4.61e-02). ETA=12:54:42, max mem: 5.0 GB 
[11/07 04:37:28 visual_prompt]: 	Training 400/2212. train loss: 9.7364,	0.1620 s / batch. (data: 6.71e-04). ETA=9:56:15, max mem: 5.0 GB 
[11/07 04:37:50 visual_prompt]: 	Training 500/2212. train loss: 9.6752,	0.1815 s / batch. (data: 6.69e-04). ETA=11:07:43, max mem: 5.0 GB 
[11/07 04:38:12 visual_prompt]: 	Training 600/2212. train loss: 0.0000,	0.2526 s / batch. (data: 2.15e-02). ETA=15:28:37, max mem: 5.0 GB 
[11/07 04:38:34 visual_prompt]: 	Training 700/2212. train loss: 10.7484,	0.2182 s / batch. (data: 1.82e-02). ETA=13:21:44, max mem: 5.0 GB 
[11/07 04:38:56 visual_prompt]: 	Training 800/2212. train loss: 10.3468,	0.1889 s / batch. (data: 6.82e-03). ETA=11:33:58, max mem: 5.0 GB 
[11/07 04:39:18 visual_prompt]: 	Training 900/2212. train loss: 0.0000,	0.2107 s / batch. (data: 5.40e-03). ETA=12:53:45, max mem: 5.0 GB 
[11/07 04:39:38 visual_prompt]: 	Training 1000/2212. train loss: 0.0001,	0.1996 s / batch. (data: 1.04e-02). ETA=12:12:29, max mem: 5.0 GB 
[11/07 04:39:59 visual_prompt]: 	Training 1100/2212. train loss: 0.0000,	0.1016 s / batch. (data: 3.25e-04). ETA=6:12:34, max mem: 5.0 GB 
[11/07 04:40:20 visual_prompt]: 	Training 1200/2212. train loss: 0.0000,	0.2106 s / batch. (data: 1.04e-02). ETA=12:52:02, max mem: 5.0 GB 
[11/07 04:40:40 visual_prompt]: 	Training 1300/2212. train loss: 0.0000,	0.2164 s / batch. (data: 1.04e-02). ETA=13:13:06, max mem: 5.0 GB 
[11/07 04:41:03 visual_prompt]: 	Training 1400/2212. train loss: 0.0027,	0.1633 s / batch. (data: 5.37e-03). ETA=9:58:11, max mem: 5.0 GB 
[11/07 04:41:24 visual_prompt]: 	Training 1500/2212. train loss: 5.3648,	0.2185 s / batch. (data: 5.36e-03). ETA=13:20:08, max mem: 5.0 GB 
[11/07 04:41:44 visual_prompt]: 	Training 1600/2212. train loss: 5.2697,	0.1700 s / batch. (data: 5.41e-03). ETA=10:22:05, max mem: 5.0 GB 
[11/07 04:42:06 visual_prompt]: 	Training 1700/2212. train loss: 0.0000,	0.2198 s / batch. (data: 2.10e-02). ETA=13:24:06, max mem: 5.0 GB 
[11/07 04:42:27 visual_prompt]: 	Training 1800/2212. train loss: 0.0000,	0.2155 s / batch. (data: 1.55e-02). ETA=13:08:08, max mem: 5.0 GB 
[11/07 04:42:48 visual_prompt]: 	Training 1900/2212. train loss: 8.6260,	0.2187 s / batch. (data: 1.50e-02). ETA=13:19:16, max mem: 5.0 GB 
[11/07 04:43:10 visual_prompt]: 	Training 2000/2212. train loss: 0.0002,	0.1966 s / batch. (data: 5.78e-03). ETA=11:58:22, max mem: 5.0 GB 
[11/07 04:43:31 visual_prompt]: 	Training 2100/2212. train loss: 14.9015,	0.2268 s / batch. (data: 7.69e-04). ETA=13:48:13, max mem: 5.0 GB 
[11/07 04:43:51 visual_prompt]: 	Training 2200/2212. train loss: 7.6808,	0.1049 s / batch. (data: 1.18e-04). ETA=6:22:59, max mem: 5.0 GB 
[11/07 04:43:52 visual_prompt]: Epoch 1 / 100: avg data time: 2.83e-02, avg batch time: 0.2132, average train loss: 5.2946
[11/07 04:44:13 visual_prompt]: 	Test 100/246. loss: 0.001, 0.0953 s / batch. (data: 3.15e-05)max mem: 4.96204 GB 
[11/07 04:44:33 visual_prompt]: 	Test 200/246. loss: 0.002, 0.0533 s / batch. (data: 3.00e-05)max mem: 4.96204 GB 
[11/07 04:44:41 visual_prompt]: Inference (val):avg data time: 3.93e-04, avg batch time: 0.0647, average loss: 4.3337
[11/07 04:44:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.86	
[11/07 04:44:41 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/07 04:45:06 visual_prompt]: 	Training 100/2212. train loss: 0.1393,	0.1958 s / batch. (data: 2.06e-02). ETA=11:54:13, max mem: 5.0 GB 
[11/07 04:45:27 visual_prompt]: 	Training 200/2212. train loss: 0.8012,	0.2491 s / batch. (data: 1.16e-03). ETA=15:08:11, max mem: 5.0 GB 
[11/07 04:45:49 visual_prompt]: 	Training 300/2212. train loss: 0.0788,	0.1849 s / batch. (data: 5.40e-03). ETA=11:14:01, max mem: 5.0 GB 
[11/07 04:46:10 visual_prompt]: 	Training 400/2212. train loss: 0.6753,	0.2007 s / batch. (data: 9.84e-03). ETA=12:11:06, max mem: 5.0 GB 
[11/07 04:46:30 visual_prompt]: 	Training 500/2212. train loss: 0.0789,	0.2390 s / batch. (data: 1.55e-02). ETA=14:30:12, max mem: 5.0 GB 
[11/07 04:46:50 visual_prompt]: 	Training 600/2212. train loss: 0.3582,	0.1789 s / batch. (data: 6.63e-04). ETA=10:51:04, max mem: 5.0 GB 
[11/07 04:47:11 visual_prompt]: 	Training 700/2212. train loss: 0.2549,	0.1640 s / batch. (data: 7.96e-03). ETA=9:56:43, max mem: 5.0 GB 
[11/07 04:47:33 visual_prompt]: 	Training 800/2212. train loss: 0.0845,	0.1742 s / batch. (data: 1.04e-02). ETA=10:33:36, max mem: 5.0 GB 
[11/07 04:47:55 visual_prompt]: 	Training 900/2212. train loss: 0.8504,	0.2129 s / batch. (data: 2.56e-02). ETA=12:53:57, max mem: 5.0 GB 
[11/07 04:48:18 visual_prompt]: 	Training 1000/2212. train loss: 0.7034,	0.2020 s / batch. (data: 1.04e-02). ETA=12:13:46, max mem: 5.0 GB 
[11/07 04:48:39 visual_prompt]: 	Training 1100/2212. train loss: 0.0461,	0.2041 s / batch. (data: 2.64e-04). ETA=12:21:15, max mem: 5.0 GB 
[11/07 04:48:59 visual_prompt]: 	Training 1200/2212. train loss: 0.4415,	0.1759 s / batch. (data: 2.44e-04). ETA=10:38:29, max mem: 5.0 GB 
[11/07 04:49:23 visual_prompt]: 	Training 1300/2212. train loss: 0.2197,	0.9300 s / batch. (data: 7.70e-01). ETA=2 days, 8:14:04, max mem: 5.0 GB 
[11/07 04:49:44 visual_prompt]: 	Training 1400/2212. train loss: 0.6966,	0.1914 s / batch. (data: 2.79e-04). ETA=11:34:12, max mem: 5.0 GB 
[11/07 04:50:06 visual_prompt]: 	Training 1500/2212. train loss: 1.3098,	0.1819 s / batch. (data: 1.79e-02). ETA=10:59:18, max mem: 5.0 GB 
[11/07 04:50:27 visual_prompt]: 	Training 1600/2212. train loss: 1.9869,	0.2088 s / batch. (data: 2.05e-02). ETA=12:36:29, max mem: 5.0 GB 
[11/07 04:50:48 visual_prompt]: 	Training 1700/2212. train loss: 0.3484,	0.1886 s / batch. (data: 1.55e-02). ETA=11:23:04, max mem: 5.0 GB 
[11/07 04:51:08 visual_prompt]: 	Training 1800/2212. train loss: 0.1993,	0.2100 s / batch. (data: 1.04e-02). ETA=12:40:10, max mem: 5.0 GB 
[11/07 04:51:30 visual_prompt]: 	Training 1900/2212. train loss: 0.2669,	0.1438 s / batch. (data: 5.37e-03). ETA=8:40:18, max mem: 5.0 GB 
[11/07 04:51:51 visual_prompt]: 	Training 2000/2212. train loss: 0.7119,	0.1753 s / batch. (data: 7.98e-03). ETA=10:33:47, max mem: 5.0 GB 
[11/07 04:52:11 visual_prompt]: 	Training 2100/2212. train loss: 0.1638,	0.1993 s / batch. (data: 8.25e-03). ETA=12:00:23, max mem: 5.0 GB 
[11/07 04:52:32 visual_prompt]: 	Training 2200/2212. train loss: 0.2001,	0.0585 s / batch. (data: 8.94e-05). ETA=3:31:17, max mem: 5.0 GB 
[11/07 04:52:33 visual_prompt]: Epoch 2 / 100: avg data time: 2.98e-02, avg batch time: 0.2133, average train loss: 1.0977
[11/07 04:52:55 visual_prompt]: 	Test 100/246. loss: 0.269, 0.0890 s / batch. (data: 2.43e-05)max mem: 4.96204 GB 
[11/07 04:53:14 visual_prompt]: 	Test 200/246. loss: 0.294, 0.0615 s / batch. (data: 3.60e-05)max mem: 4.96204 GB 
[11/07 04:53:22 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.0628, average loss: 0.8391
[11/07 04:53:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.00	
[11/07 04:53:22 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/07 04:53:46 visual_prompt]: 	Training 100/2212. train loss: 0.4928,	0.1642 s / batch. (data: 2.82e-04). ETA=9:52:49, max mem: 5.0 GB 
[11/07 04:54:08 visual_prompt]: 	Training 200/2212. train loss: 2.2591,	0.2006 s / batch. (data: 2.49e-02). ETA=12:04:01, max mem: 5.0 GB 
[11/07 04:54:30 visual_prompt]: 	Training 300/2212. train loss: 1.3486,	0.1700 s / batch. (data: 2.40e-04). ETA=10:13:31, max mem: 5.0 GB 
[11/07 04:54:51 visual_prompt]: 	Training 400/2212. train loss: 1.4217,	0.1837 s / batch. (data: 9.30e-03). ETA=11:02:38, max mem: 5.0 GB 
[11/07 04:55:13 visual_prompt]: 	Training 500/2212. train loss: 2.2470,	0.2163 s / batch. (data: 1.04e-02). ETA=12:59:41, max mem: 5.0 GB 
[11/07 04:55:35 visual_prompt]: 	Training 600/2212. train loss: 1.1179,	0.1662 s / batch. (data: 7.84e-04). ETA=9:58:42, max mem: 5.0 GB 
[11/07 04:55:56 visual_prompt]: 	Training 700/2212. train loss: 1.3942,	0.1710 s / batch. (data: 6.91e-03). ETA=10:15:45, max mem: 5.0 GB 
[11/07 04:56:18 visual_prompt]: 	Training 800/2212. train loss: 2.6094,	0.1803 s / batch. (data: 1.74e-02). ETA=10:48:57, max mem: 5.0 GB 
[11/07 04:56:39 visual_prompt]: 	Training 900/2212. train loss: 1.1515,	0.2088 s / batch. (data: 8.01e-03). ETA=12:31:12, max mem: 5.0 GB 
[11/07 04:57:00 visual_prompt]: 	Training 1000/2212. train loss: 0.1815,	0.1931 s / batch. (data: 1.21e-02). ETA=11:34:16, max mem: 5.0 GB 
[11/07 04:57:21 visual_prompt]: 	Training 1100/2212. train loss: 0.6591,	0.1565 s / batch. (data: 2.50e-04). ETA=9:22:30, max mem: 5.0 GB 
[11/07 04:57:41 visual_prompt]: 	Training 1200/2212. train loss: 0.7950,	0.2078 s / batch. (data: 5.80e-03). ETA=12:26:26, max mem: 5.0 GB 
[11/07 04:58:03 visual_prompt]: 	Training 1300/2212. train loss: 1.2591,	0.1131 s / batch. (data: 1.87e-04). ETA=6:46:08, max mem: 5.0 GB 
[11/07 04:58:25 visual_prompt]: 	Training 1400/2212. train loss: 0.3342,	0.1282 s / batch. (data: 5.39e-03). ETA=7:40:10, max mem: 5.0 GB 
[11/07 04:58:45 visual_prompt]: 	Training 1500/2212. train loss: 0.1076,	0.2269 s / batch. (data: 1.05e-02). ETA=13:34:09, max mem: 5.0 GB 
[11/07 04:59:06 visual_prompt]: 	Training 1600/2212. train loss: 0.2285,	0.2206 s / batch. (data: 1.31e-02). ETA=13:11:00, max mem: 5.0 GB 
[11/07 04:59:27 visual_prompt]: 	Training 1700/2212. train loss: 1.1235,	0.2206 s / batch. (data: 1.56e-02). ETA=13:10:38, max mem: 5.0 GB 
[11/07 04:59:49 visual_prompt]: 	Training 1800/2212. train loss: 0.3517,	0.2178 s / batch. (data: 5.37e-03). ETA=13:00:18, max mem: 5.0 GB 
[11/07 05:00:09 visual_prompt]: 	Training 1900/2212. train loss: 0.2734,	0.1901 s / batch. (data: 7.96e-03). ETA=11:20:41, max mem: 5.0 GB 
[11/07 05:00:30 visual_prompt]: 	Training 2000/2212. train loss: 1.6778,	0.2414 s / batch. (data: 2.11e-02). ETA=14:24:04, max mem: 5.0 GB 
[11/07 05:00:53 visual_prompt]: 	Training 2100/2212. train loss: 2.6383,	0.2441 s / batch. (data: 5.85e-03). ETA=14:33:11, max mem: 5.0 GB 
[11/07 05:01:13 visual_prompt]: 	Training 2200/2212. train loss: 0.6687,	0.4060 s / batch. (data: 2.80e-01). ETA=1 day, 0:11:57, max mem: 5.0 GB 
[11/07 05:01:14 visual_prompt]: Epoch 3 / 100: avg data time: 2.92e-02, avg batch time: 0.2133, average train loss: 0.9122
[11/07 05:01:36 visual_prompt]: 	Test 100/246. loss: 0.486, 0.0910 s / batch. (data: 3.60e-05)max mem: 4.96204 GB 
[11/07 05:01:56 visual_prompt]: 	Test 200/246. loss: 0.468, 0.0687 s / batch. (data: 2.93e-05)max mem: 4.96204 GB 
[11/07 05:02:04 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.0643, average loss: 0.7461
[11/07 05:02:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.24	
[11/07 05:02:04 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/07 05:02:29 visual_prompt]: 	Training 100/2212. train loss: 1.6867,	0.2203 s / batch. (data: 3.80e-02). ETA=13:07:29, max mem: 5.0 GB 
[11/07 05:02:49 visual_prompt]: 	Training 200/2212. train loss: 0.6990,	0.2097 s / batch. (data: 3.08e-02). ETA=12:29:04, max mem: 5.0 GB 
[11/07 05:03:11 visual_prompt]: 	Training 300/2212. train loss: 0.3313,	0.1698 s / batch. (data: 1.04e-02). ETA=10:06:28, max mem: 5.0 GB 
[11/07 05:03:32 visual_prompt]: 	Training 400/2212. train loss: 1.0073,	0.2143 s / batch. (data: 2.13e-02). ETA=12:44:57, max mem: 5.0 GB 
[11/07 05:03:53 visual_prompt]: 	Training 500/2212. train loss: 0.6463,	0.1654 s / batch. (data: 2.26e-02). ETA=9:49:58, max mem: 5.0 GB 
[11/07 05:04:14 visual_prompt]: 	Training 600/2212. train loss: 0.3259,	0.1916 s / batch. (data: 1.64e-02). ETA=11:23:21, max mem: 5.0 GB 
[11/07 05:04:35 visual_prompt]: 	Training 700/2212. train loss: 1.3500,	0.2157 s / batch. (data: 2.41e-04). ETA=12:48:49, max mem: 5.0 GB 
[11/07 05:04:57 visual_prompt]: 	Training 800/2212. train loss: 1.5442,	0.1702 s / batch. (data: 5.42e-03). ETA=10:06:20, max mem: 5.0 GB 
[11/07 05:05:18 visual_prompt]: 	Training 900/2212. train loss: 0.2893,	0.2439 s / batch. (data: 2.06e-02). ETA=14:28:42, max mem: 5.0 GB 
[11/07 05:05:38 visual_prompt]: 	Training 1000/2212. train loss: 1.2215,	0.1960 s / batch. (data: 9.91e-03). ETA=11:37:27, max mem: 5.0 GB 
[11/07 05:06:00 visual_prompt]: 	Training 1100/2212. train loss: 2.3748,	0.2006 s / batch. (data: 1.55e-02). ETA=11:53:48, max mem: 5.0 GB 
[11/07 05:06:21 visual_prompt]: 	Training 1200/2212. train loss: 0.2733,	0.1851 s / batch. (data: 5.38e-03). ETA=10:58:17, max mem: 5.0 GB 
[11/07 05:06:42 visual_prompt]: 	Training 1300/2212. train loss: 1.2201,	0.2102 s / batch. (data: 5.38e-03). ETA=12:26:59, max mem: 5.0 GB 
[11/07 05:07:04 visual_prompt]: 	Training 1400/2212. train loss: 0.8250,	0.2428 s / batch. (data: 2.10e-02). ETA=14:22:36, max mem: 5.0 GB 
[11/07 05:07:25 visual_prompt]: 	Training 1500/2212. train loss: 0.4628,	0.1707 s / batch. (data: 2.50e-04). ETA=10:06:10, max mem: 5.0 GB 
[11/07 05:07:46 visual_prompt]: 	Training 1600/2212. train loss: 0.2615,	0.2087 s / batch. (data: 1.55e-02). ETA=12:20:39, max mem: 5.0 GB 
[11/07 05:08:08 visual_prompt]: 	Training 1700/2212. train loss: 0.0936,	0.1791 s / batch. (data: 2.56e-04). ETA=10:35:30, max mem: 5.0 GB 
[11/07 05:08:28 visual_prompt]: 	Training 1800/2212. train loss: 0.9399,	0.1756 s / batch. (data: 1.80e-02). ETA=10:22:49, max mem: 5.0 GB 
[11/07 05:08:50 visual_prompt]: 	Training 1900/2212. train loss: 0.6914,	0.2217 s / batch. (data: 2.33e-04). ETA=13:05:43, max mem: 5.0 GB 
[11/07 05:09:11 visual_prompt]: 	Training 2000/2212. train loss: 1.1759,	0.1747 s / batch. (data: 2.41e-04). ETA=10:18:58, max mem: 5.0 GB 
[11/07 05:09:33 visual_prompt]: 	Training 2100/2212. train loss: 1.2894,	0.1893 s / batch. (data: 7.07e-04). ETA=11:10:23, max mem: 5.0 GB 
[11/07 05:09:54 visual_prompt]: 	Training 2200/2212. train loss: 0.1614,	0.1550 s / batch. (data: 1.15e-04). ETA=9:08:35, max mem: 5.0 GB 
[11/07 05:09:56 visual_prompt]: Epoch 4 / 100: avg data time: 2.90e-02, avg batch time: 0.2132, average train loss: 0.8940
[11/07 05:10:17 visual_prompt]: 	Test 100/246. loss: 0.925, 0.0707 s / batch. (data: 4.98e-05)max mem: 4.96204 GB 
[11/07 05:10:37 visual_prompt]: 	Test 200/246. loss: 0.931, 0.0854 s / batch. (data: 2.72e-05)max mem: 4.96204 GB 
[11/07 05:10:45 visual_prompt]: Inference (val):avg data time: 2.49e-04, avg batch time: 0.0639, average loss: 0.6937
[11/07 05:10:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.44	
[11/07 05:10:45 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/07 05:11:09 visual_prompt]: 	Training 100/2212. train loss: 2.0238,	0.1953 s / batch. (data: 2.83e-04). ETA=11:31:03, max mem: 5.0 GB 
[11/07 05:11:30 visual_prompt]: 	Training 200/2212. train loss: 1.0386,	0.1492 s / batch. (data: 5.32e-03). ETA=8:47:23, max mem: 5.0 GB 
[11/07 05:11:52 visual_prompt]: 	Training 300/2212. train loss: 1.4904,	0.2136 s / batch. (data: 2.56e-02). ETA=12:34:56, max mem: 5.0 GB 
[11/07 05:12:14 visual_prompt]: 	Training 400/2212. train loss: 2.6809,	0.1578 s / batch. (data: 9.82e-03). ETA=9:17:24, max mem: 5.0 GB 
[11/07 05:12:36 visual_prompt]: 	Training 500/2212. train loss: 0.8951,	0.1791 s / batch. (data: 5.77e-03). ETA=10:32:12, max mem: 5.0 GB 
[11/07 05:12:57 visual_prompt]: 	Training 600/2212. train loss: 0.2169,	0.2087 s / batch. (data: 7.02e-04). ETA=12:16:24, max mem: 5.0 GB 
[11/07 05:13:17 visual_prompt]: 	Training 700/2212. train loss: 0.8593,	0.1927 s / batch. (data: 2.53e-02). ETA=11:19:43, max mem: 5.0 GB 
[11/07 05:13:39 visual_prompt]: 	Training 800/2212. train loss: 0.9078,	0.2106 s / batch. (data: 1.55e-02). ETA=12:22:31, max mem: 5.0 GB 
[11/07 05:14:00 visual_prompt]: 	Training 900/2212. train loss: 0.5003,	0.1694 s / batch. (data: 2.46e-04). ETA=9:57:08, max mem: 5.0 GB 
[11/07 05:14:21 visual_prompt]: 	Training 1000/2212. train loss: 0.7297,	0.2320 s / batch. (data: 7.01e-02). ETA=13:37:11, max mem: 5.0 GB 
[11/07 05:14:42 visual_prompt]: 	Training 1100/2212. train loss: 0.6109,	0.1973 s / batch. (data: 4.13e-02). ETA=11:34:30, max mem: 5.0 GB 
[11/07 05:15:03 visual_prompt]: 	Training 1200/2212. train loss: 0.6024,	0.1731 s / batch. (data: 2.65e-04). ETA=10:09:09, max mem: 5.0 GB 
[11/07 05:15:24 visual_prompt]: 	Training 1300/2212. train loss: 0.4197,	0.1811 s / batch. (data: 6.61e-04). ETA=10:36:51, max mem: 5.0 GB 
[11/07 05:15:45 visual_prompt]: 	Training 1400/2212. train loss: 0.0003,	0.2313 s / batch. (data: 1.54e-02). ETA=13:33:15, max mem: 5.0 GB 
[11/07 05:16:07 visual_prompt]: 	Training 1500/2212. train loss: 1.2953,	0.7841 s / batch. (data: 6.42e-01). ETA=1 day, 21:55:20, max mem: 5.0 GB 
[11/07 05:16:29 visual_prompt]: 	Training 1600/2212. train loss: 0.5169,	0.1953 s / batch. (data: 2.72e-04). ETA=11:25:55, max mem: 5.0 GB 
[11/07 05:16:50 visual_prompt]: 	Training 1700/2212. train loss: 0.7794,	0.2103 s / batch. (data: 3.04e-02). ETA=12:18:13, max mem: 5.0 GB 
[11/07 05:17:12 visual_prompt]: 	Training 1800/2212. train loss: 0.8215,	0.2308 s / batch. (data: 2.44e-02). ETA=13:30:03, max mem: 5.0 GB 
[11/07 05:17:34 visual_prompt]: 	Training 1900/2212. train loss: 0.6410,	0.2303 s / batch. (data: 1.59e-02). ETA=13:27:51, max mem: 5.0 GB 
[11/07 05:17:55 visual_prompt]: 	Training 2000/2212. train loss: 0.2228,	0.1717 s / batch. (data: 1.08e-02). ETA=10:02:03, max mem: 5.0 GB 
[11/07 05:18:15 visual_prompt]: 	Training 2100/2212. train loss: 1.4759,	0.2167 s / batch. (data: 5.72e-03). ETA=12:39:25, max mem: 5.0 GB 
[11/07 05:18:36 visual_prompt]: 	Training 2200/2212. train loss: 0.4495,	0.0991 s / batch. (data: 1.25e-04). ETA=5:47:06, max mem: 5.0 GB 
[11/07 05:18:37 visual_prompt]: Epoch 5 / 100: avg data time: 2.93e-02, avg batch time: 0.2133, average train loss: 0.8782
[11/07 05:18:58 visual_prompt]: 	Test 100/246. loss: 0.840, 0.1055 s / batch. (data: 2.57e-05)max mem: 4.96204 GB 
[11/07 05:19:18 visual_prompt]: 	Test 200/246. loss: 0.842, 0.0827 s / batch. (data: 2.29e-05)max mem: 4.96204 GB 
[11/07 05:19:26 visual_prompt]: Inference (val):avg data time: 3.36e-04, avg batch time: 0.0631, average loss: 0.6888
[11/07 05:19:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.37	
[11/07 05:19:26 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/07 05:19:50 visual_prompt]: 	Training 100/2212. train loss: 3.4754,	0.2004 s / batch. (data: 5.38e-03). ETA=11:41:33, max mem: 5.0 GB 
[11/07 05:20:13 visual_prompt]: 	Training 200/2212. train loss: 1.6310,	0.1928 s / batch. (data: 2.57e-02). ETA=11:14:41, max mem: 5.0 GB 
[11/07 05:20:34 visual_prompt]: 	Training 300/2212. train loss: 0.4592,	0.1844 s / batch. (data: 6.79e-03). ETA=10:44:49, max mem: 5.0 GB 
[11/07 05:20:54 visual_prompt]: 	Training 400/2212. train loss: 0.8283,	0.1877 s / batch. (data: 2.51e-04). ETA=10:56:10, max mem: 5.0 GB 
[11/07 05:21:16 visual_prompt]: 	Training 500/2212. train loss: 0.4875,	0.1741 s / batch. (data: 1.08e-02). ETA=10:08:23, max mem: 5.0 GB 
[11/07 05:21:38 visual_prompt]: 	Training 600/2212. train loss: 0.4917,	0.1902 s / batch. (data: 1.57e-02). ETA=11:04:13, max mem: 5.0 GB 
[11/07 05:22:00 visual_prompt]: 	Training 700/2212. train loss: 0.7122,	0.2117 s / batch. (data: 3.01e-02). ETA=12:19:07, max mem: 5.0 GB 
[11/07 05:22:21 visual_prompt]: 	Training 800/2212. train loss: 1.0167,	0.1954 s / batch. (data: 6.92e-03). ETA=11:21:45, max mem: 5.0 GB 
[11/07 05:22:42 visual_prompt]: 	Training 900/2212. train loss: 0.6651,	0.2015 s / batch. (data: 5.39e-03). ETA=11:42:35, max mem: 5.0 GB 
[11/07 05:23:04 visual_prompt]: 	Training 1000/2212. train loss: 0.7879,	0.1914 s / batch. (data: 1.37e-02). ETA=11:07:07, max mem: 5.0 GB 
[11/07 05:23:25 visual_prompt]: 	Training 1100/2212. train loss: 0.2936,	0.1815 s / batch. (data: 1.55e-02). ETA=10:32:18, max mem: 5.0 GB 
[11/07 05:23:45 visual_prompt]: 	Training 1200/2212. train loss: 0.3209,	0.2121 s / batch. (data: 3.73e-02). ETA=12:18:30, max mem: 5.0 GB 
[11/07 05:24:06 visual_prompt]: 	Training 1300/2212. train loss: 0.4799,	0.2070 s / batch. (data: 1.55e-02). ETA=12:00:21, max mem: 5.0 GB 
[11/07 05:24:28 visual_prompt]: 	Training 1400/2212. train loss: 0.3625,	0.2169 s / batch. (data: 1.04e-02). ETA=12:34:45, max mem: 5.0 GB 
[11/07 05:24:49 visual_prompt]: 	Training 1500/2212. train loss: 0.9777,	0.1585 s / batch. (data: 2.80e-04). ETA=9:11:15, max mem: 5.0 GB 
[11/07 05:25:11 visual_prompt]: 	Training 1600/2212. train loss: 0.3323,	0.1799 s / batch. (data: 1.08e-02). ETA=10:25:21, max mem: 5.0 GB 
[11/07 05:25:32 visual_prompt]: 	Training 1700/2212. train loss: 0.6734,	0.1840 s / batch. (data: 1.55e-02). ETA=10:39:02, max mem: 5.0 GB 
[11/07 05:25:55 visual_prompt]: 	Training 1800/2212. train loss: 0.9524,	0.2003 s / batch. (data: 1.04e-02). ETA=11:35:27, max mem: 5.0 GB 
[11/07 05:26:15 visual_prompt]: 	Training 1900/2212. train loss: 1.0611,	0.2165 s / batch. (data: 8.86e-03). ETA=12:31:26, max mem: 5.0 GB 
[11/07 05:26:36 visual_prompt]: 	Training 2000/2212. train loss: 0.7980,	0.1573 s / batch. (data: 2.03e-04). ETA=9:05:39, max mem: 5.0 GB 
[11/07 05:26:55 visual_prompt]: 	Training 2100/2212. train loss: 0.7706,	0.1360 s / batch. (data: 1.04e-02). ETA=7:51:41, max mem: 5.0 GB 
[11/07 05:27:17 visual_prompt]: 	Training 2200/2212. train loss: 0.8025,	0.1505 s / batch. (data: 1.12e-04). ETA=8:41:29, max mem: 5.0 GB 
[11/07 05:27:18 visual_prompt]: Epoch 6 / 100: avg data time: 2.95e-02, avg batch time: 0.2132, average train loss: 0.8033
[11/07 05:27:40 visual_prompt]: 	Test 100/246. loss: 1.030, 0.1116 s / batch. (data: 2.88e-05)max mem: 4.96204 GB 
[11/07 05:27:59 visual_prompt]: 	Test 200/246. loss: 1.026, 0.1117 s / batch. (data: 2.36e-05)max mem: 4.96204 GB 
[11/07 05:28:07 visual_prompt]: Inference (val):avg data time: 4.76e-05, avg batch time: 0.0636, average loss: 0.7057
[11/07 05:28:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.21	
[11/07 05:28:07 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/07 05:28:30 visual_prompt]: 	Training 100/2212. train loss: 0.1194,	0.1844 s / batch. (data: 7.37e-04). ETA=10:38:41, max mem: 5.0 GB 
[11/07 05:28:54 visual_prompt]: 	Training 200/2212. train loss: 0.2359,	0.1526 s / batch. (data: 5.37e-03). ETA=8:48:27, max mem: 5.0 GB 
[11/07 05:29:17 visual_prompt]: 	Training 300/2212. train loss: 0.9825,	0.2199 s / batch. (data: 7.11e-04). ETA=12:40:59, max mem: 5.0 GB 
[11/07 05:29:39 visual_prompt]: 	Training 400/2212. train loss: 1.2486,	0.1775 s / batch. (data: 7.05e-04). ETA=10:13:54, max mem: 5.0 GB 
[11/07 05:30:01 visual_prompt]: 	Training 500/2212. train loss: 1.4225,	0.2199 s / batch. (data: 6.85e-04). ETA=12:40:22, max mem: 5.0 GB 
[11/07 05:30:21 visual_prompt]: 	Training 600/2212. train loss: 0.8155,	0.1722 s / batch. (data: 9.93e-03). ETA=9:54:59, max mem: 5.0 GB 
[11/07 05:30:43 visual_prompt]: 	Training 700/2212. train loss: 0.1980,	0.1942 s / batch. (data: 2.49e-02). ETA=11:10:53, max mem: 5.0 GB 
[11/07 05:31:04 visual_prompt]: 	Training 800/2212. train loss: 0.3860,	0.1937 s / batch. (data: 2.45e-04). ETA=11:08:36, max mem: 5.0 GB 
[11/07 05:31:25 visual_prompt]: 	Training 900/2212. train loss: 0.6925,	0.2067 s / batch. (data: 1.55e-02). ETA=11:53:22, max mem: 5.0 GB 
[11/07 05:31:46 visual_prompt]: 	Training 1000/2212. train loss: 0.5898,	0.1908 s / batch. (data: 1.34e-02). ETA=10:58:05, max mem: 5.0 GB 
[11/07 05:32:07 visual_prompt]: 	Training 1100/2212. train loss: 0.7698,	0.1591 s / batch. (data: 2.83e-04). ETA=9:08:18, max mem: 5.0 GB 
[11/07 05:32:27 visual_prompt]: 	Training 1200/2212. train loss: 0.4889,	0.1590 s / batch. (data: 2.11e-04). ETA=9:07:41, max mem: 5.0 GB 
[11/07 05:32:48 visual_prompt]: 	Training 1300/2212. train loss: 0.6733,	0.1280 s / batch. (data: 7.04e-04). ETA=7:20:52, max mem: 5.0 GB 
[11/07 05:33:09 visual_prompt]: 	Training 1400/2212. train loss: 0.4502,	0.1972 s / batch. (data: 2.39e-04). ETA=11:18:38, max mem: 5.0 GB 
[11/07 05:33:30 visual_prompt]: 	Training 1500/2212. train loss: 1.6218,	0.1909 s / batch. (data: 1.05e-02). ETA=10:56:47, max mem: 5.0 GB 
[11/07 05:33:51 visual_prompt]: 	Training 1600/2212. train loss: 0.4768,	0.2289 s / batch. (data: 9.20e-03). ETA=13:07:18, max mem: 5.0 GB 
[11/07 05:34:12 visual_prompt]: 	Training 1700/2212. train loss: 0.8390,	0.2712 s / batch. (data: 5.82e-03). ETA=15:32:12, max mem: 5.0 GB 
[11/07 05:34:33 visual_prompt]: 	Training 1800/2212. train loss: 0.9950,	0.2133 s / batch. (data: 2.98e-04). ETA=12:12:41, max mem: 5.0 GB 
[11/07 05:34:55 visual_prompt]: 	Training 1900/2212. train loss: 0.8383,	0.2477 s / batch. (data: 3.14e-02). ETA=14:10:39, max mem: 5.0 GB 
[11/07 05:35:16 visual_prompt]: 	Training 2000/2212. train loss: 0.9636,	0.1659 s / batch. (data: 1.04e-02). ETA=9:29:13, max mem: 5.0 GB 
[11/07 05:35:37 visual_prompt]: 	Training 2100/2212. train loss: 1.0157,	0.1912 s / batch. (data: 5.37e-03). ETA=10:55:50, max mem: 5.0 GB 
[11/07 05:35:58 visual_prompt]: 	Training 2200/2212. train loss: 0.7969,	0.1342 s / batch. (data: 1.21e-04). ETA=7:40:05, max mem: 5.0 GB 
[11/07 05:35:59 visual_prompt]: Epoch 7 / 100: avg data time: 2.78e-02, avg batch time: 0.2131, average train loss: 0.7624
[11/07 05:36:20 visual_prompt]: 	Test 100/246. loss: 0.898, 0.0658 s / batch. (data: 2.38e-05)max mem: 4.96204 GB 
[11/07 05:36:40 visual_prompt]: 	Test 200/246. loss: 0.892, 0.0771 s / batch. (data: 3.15e-05)max mem: 4.96204 GB 
[11/07 05:36:48 visual_prompt]: Inference (val):avg data time: 1.28e-04, avg batch time: 0.0627, average loss: 0.6903
[11/07 05:36:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.25	
[11/07 05:36:48 visual_prompt]: Best epoch 7: best metric: -0.690
[11/07 05:36:48 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/07 05:37:12 visual_prompt]: 	Training 100/2212. train loss: 0.1556,	0.1876 s / batch. (data: 2.45e-04). ETA=10:42:44, max mem: 5.0 GB 
[11/07 05:37:34 visual_prompt]: 	Training 200/2212. train loss: 0.8309,	0.2149 s / batch. (data: 6.93e-04). ETA=12:16:08, max mem: 5.0 GB 
[11/07 05:37:55 visual_prompt]: 	Training 300/2212. train loss: 0.8390,	0.1691 s / batch. (data: 1.60e-02). ETA=9:38:50, max mem: 5.0 GB 
[11/07 05:38:16 visual_prompt]: 	Training 400/2212. train loss: 0.3260,	0.2088 s / batch. (data: 5.33e-03). ETA=11:54:25, max mem: 5.0 GB 
[11/07 05:38:39 visual_prompt]: 	Training 500/2212. train loss: 0.4819,	0.4401 s / batch. (data: 2.95e-01). ETA=1 day, 1:05:18, max mem: 5.0 GB 
[11/07 05:39:00 visual_prompt]: 	Training 600/2212. train loss: 0.6748,	0.1926 s / batch. (data: 7.00e-04). ETA=10:58:28, max mem: 5.0 GB 
[11/07 05:39:21 visual_prompt]: 	Training 700/2212. train loss: 0.2610,	0.2331 s / batch. (data: 2.57e-02). ETA=13:16:31, max mem: 5.0 GB 
[11/07 05:39:42 visual_prompt]: 	Training 800/2212. train loss: 0.3941,	0.2024 s / batch. (data: 1.04e-02). ETA=11:31:12, max mem: 5.0 GB 
[11/07 05:40:04 visual_prompt]: 	Training 900/2212. train loss: 0.4259,	0.1824 s / batch. (data: 2.76e-04). ETA=10:22:33, max mem: 5.0 GB 
[11/07 05:40:25 visual_prompt]: 	Training 1000/2212. train loss: 1.1341,	0.2008 s / batch. (data: 6.84e-04). ETA=11:25:06, max mem: 5.0 GB 
[11/07 05:40:46 visual_prompt]: 	Training 1100/2212. train loss: 0.6023,	0.1285 s / batch. (data: 2.37e-04). ETA=7:18:19, max mem: 5.0 GB 
[11/07 05:41:08 visual_prompt]: 	Training 1200/2212. train loss: 0.6748,	0.2291 s / batch. (data: 5.38e-03). ETA=13:00:57, max mem: 5.0 GB 
[11/07 05:41:29 visual_prompt]: 	Training 1300/2212. train loss: 0.9671,	0.2552 s / batch. (data: 1.61e-02). ETA=14:29:27, max mem: 5.0 GB 
[11/07 05:41:50 visual_prompt]: 	Training 1400/2212. train loss: 0.8380,	0.2514 s / batch. (data: 1.55e-02). ETA=14:16:10, max mem: 5.0 GB 
[11/07 05:42:12 visual_prompt]: 	Training 1500/2212. train loss: 1.2837,	0.1162 s / batch. (data: 3.04e-04). ETA=6:35:28, max mem: 5.0 GB 
[11/07 05:42:34 visual_prompt]: 	Training 1600/2212. train loss: 1.1890,	0.2144 s / batch. (data: 2.06e-02). ETA=12:09:32, max mem: 5.0 GB 
[11/07 05:42:55 visual_prompt]: 	Training 1700/2212. train loss: 1.4702,	0.1809 s / batch. (data: 2.71e-04). ETA=10:15:00, max mem: 5.0 GB 
[11/07 05:43:16 visual_prompt]: 	Training 1800/2212. train loss: 0.1903,	0.1304 s / batch. (data: 7.98e-03). ETA=7:23:02, max mem: 5.0 GB 
[11/07 05:43:37 visual_prompt]: 	Training 1900/2212. train loss: 0.4417,	0.2579 s / batch. (data: 5.79e-03). ETA=14:36:12, max mem: 5.0 GB 
[11/07 05:43:59 visual_prompt]: 	Training 2000/2212. train loss: 0.9705,	0.1858 s / batch. (data: 5.38e-03). ETA=10:30:43, max mem: 5.0 GB 
[11/07 05:44:20 visual_prompt]: 	Training 2100/2212. train loss: 0.4146,	0.1782 s / batch. (data: 1.75e-02). ETA=10:04:36, max mem: 5.0 GB 
[11/07 05:44:39 visual_prompt]: 	Training 2200/2212. train loss: 0.9488,	0.0516 s / batch. (data: 1.50e-04). ETA=2:55:02, max mem: 5.0 GB 
[11/07 05:44:40 visual_prompt]: Epoch 8 / 100: avg data time: 2.77e-02, avg batch time: 0.2134, average train loss: 0.7614
[11/07 05:45:02 visual_prompt]: 	Test 100/246. loss: 0.733, 0.0862 s / batch. (data: 2.03e-05)max mem: 4.96204 GB 
[11/07 05:45:22 visual_prompt]: 	Test 200/246. loss: 0.733, 0.1286 s / batch. (data: 2.48e-05)max mem: 4.96204 GB 
[11/07 05:45:30 visual_prompt]: Inference (val):avg data time: 2.16e-04, avg batch time: 0.0632, average loss: 0.6901
[11/07 05:45:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.75	
[11/07 05:45:30 visual_prompt]: Best epoch 8: best metric: -0.690
[11/07 05:45:30 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/07 05:45:54 visual_prompt]: 	Training 100/2212. train loss: 1.0796,	0.1806 s / batch. (data: 1.05e-02). ETA=10:12:04, max mem: 5.0 GB 
[11/07 05:46:17 visual_prompt]: 	Training 200/2212. train loss: 0.7517,	0.1866 s / batch. (data: 2.86e-04). ETA=10:32:18, max mem: 5.0 GB 
[11/07 05:46:40 visual_prompt]: 	Training 300/2212. train loss: 0.7709,	0.2107 s / batch. (data: 2.39e-04). ETA=11:53:32, max mem: 5.0 GB 
[11/07 05:47:01 visual_prompt]: 	Training 400/2212. train loss: 0.6345,	0.2321 s / batch. (data: 1.09e-02). ETA=13:05:32, max mem: 5.0 GB 
[11/07 05:47:23 visual_prompt]: 	Training 500/2212. train loss: 0.9164,	0.1472 s / batch. (data: 5.34e-03). ETA=8:18:08, max mem: 5.0 GB 
[11/07 05:47:45 visual_prompt]: 	Training 600/2212. train loss: 0.3013,	0.1895 s / batch. (data: 8.44e-04). ETA=10:40:41, max mem: 5.0 GB 
[11/07 05:48:06 visual_prompt]: 	Training 700/2212. train loss: 0.3901,	0.1812 s / batch. (data: 1.55e-02). ETA=10:12:31, max mem: 5.0 GB 
[11/07 05:48:26 visual_prompt]: 	Training 800/2212. train loss: 0.7721,	0.2053 s / batch. (data: 1.55e-02). ETA=11:33:31, max mem: 5.0 GB 
[11/07 05:48:47 visual_prompt]: 	Training 900/2212. train loss: 0.7586,	0.1914 s / batch. (data: 1.55e-02). ETA=10:46:13, max mem: 5.0 GB 
[11/07 05:49:08 visual_prompt]: 	Training 1000/2212. train loss: 1.0669,	0.1917 s / batch. (data: 1.42e-02). ETA=10:46:59, max mem: 5.0 GB 
[11/07 05:49:30 visual_prompt]: 	Training 1100/2212. train loss: 0.3990,	0.6153 s / batch. (data: 4.22e-01). ETA=1 day, 10:35:39, max mem: 5.0 GB 
[11/07 05:49:51 visual_prompt]: 	Training 1200/2212. train loss: 0.4463,	0.2023 s / batch. (data: 1.31e-02). ETA=11:22:15, max mem: 5.0 GB 
[11/07 05:50:10 visual_prompt]: 	Training 1300/2212. train loss: 0.2597,	0.1537 s / batch. (data: 7.92e-04). ETA=8:38:02, max mem: 5.0 GB 
[11/07 05:50:31 visual_prompt]: 	Training 1400/2212. train loss: 0.6847,	0.2235 s / batch. (data: 2.61e-02). ETA=12:32:59, max mem: 5.0 GB 
[11/07 05:50:52 visual_prompt]: 	Training 1500/2212. train loss: 1.1391,	0.2139 s / batch. (data: 6.78e-04). ETA=12:00:01, max mem: 5.0 GB 
[11/07 05:51:13 visual_prompt]: 	Training 1600/2212. train loss: 0.5599,	0.1915 s / batch. (data: 2.58e-02). ETA=10:44:28, max mem: 5.0 GB 
[11/07 05:51:34 visual_prompt]: 	Training 1700/2212. train loss: 1.4337,	0.2280 s / batch. (data: 7.07e-04). ETA=12:46:50, max mem: 5.0 GB 
[11/07 05:51:54 visual_prompt]: 	Training 1800/2212. train loss: 0.9363,	0.4562 s / batch. (data: 2.96e-01). ETA=1 day, 1:33:44, max mem: 5.0 GB 
[11/07 05:52:14 visual_prompt]: 	Training 1900/2212. train loss: 0.9229,	0.2373 s / batch. (data: 2.47e-02). ETA=13:17:12, max mem: 5.0 GB 
[11/07 05:52:36 visual_prompt]: 	Training 2000/2212. train loss: 0.5587,	0.1895 s / batch. (data: 2.40e-04). ETA=10:36:29, max mem: 5.0 GB 
[11/07 05:52:57 visual_prompt]: 	Training 2100/2212. train loss: 1.0290,	0.2102 s / batch. (data: 5.40e-03). ETA=11:45:26, max mem: 5.0 GB 
[11/07 05:53:20 visual_prompt]: 	Training 2200/2212. train loss: 0.6088,	0.2238 s / batch. (data: 1.18e-04). ETA=12:30:59, max mem: 5.0 GB 
[11/07 05:53:22 visual_prompt]: Epoch 9 / 100: avg data time: 3.10e-02, avg batch time: 0.2132, average train loss: 0.7356
[11/07 05:53:43 visual_prompt]: 	Test 100/246. loss: 1.048, 0.0977 s / batch. (data: 2.84e-05)max mem: 4.96204 GB 
[11/07 05:54:03 visual_prompt]: 	Test 200/246. loss: 1.048, 0.1088 s / batch. (data: 3.36e-05)max mem: 4.96204 GB 
[11/07 05:54:11 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.0627, average loss: 0.7098
[11/07 05:54:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.92	
[11/07 05:54:11 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/07 05:54:35 visual_prompt]: 	Training 100/2212. train loss: 0.6151,	0.2310 s / batch. (data: 2.06e-02). ETA=12:54:34, max mem: 5.0 GB 
[11/07 05:54:57 visual_prompt]: 	Training 200/2212. train loss: 0.7149,	0.3857 s / batch. (data: 2.55e-01). ETA=21:32:47, max mem: 5.0 GB 
[11/07 05:55:18 visual_prompt]: 	Training 300/2212. train loss: 1.0263,	0.2088 s / batch. (data: 5.32e-03). ETA=11:39:33, max mem: 5.0 GB 
[11/07 05:55:39 visual_prompt]: 	Training 400/2212. train loss: 1.0508,	0.1887 s / batch. (data: 1.55e-02). ETA=10:31:52, max mem: 5.0 GB 
[11/07 05:56:00 visual_prompt]: 	Training 500/2212. train loss: 0.4159,	0.1678 s / batch. (data: 5.77e-04). ETA=9:21:31, max mem: 5.0 GB 
[11/07 05:56:21 visual_prompt]: 	Training 600/2212. train loss: 0.9675,	0.2566 s / batch. (data: 2.62e-02). ETA=14:18:26, max mem: 5.0 GB 
[11/07 05:56:43 visual_prompt]: 	Training 700/2212. train loss: 0.5522,	0.2192 s / batch. (data: 3.12e-02). ETA=12:12:59, max mem: 5.0 GB 
[11/07 05:57:06 visual_prompt]: 	Training 800/2212. train loss: 0.4319,	0.2102 s / batch. (data: 7.35e-04). ETA=11:42:31, max mem: 5.0 GB 
[11/07 05:57:27 visual_prompt]: 	Training 900/2212. train loss: 0.1828,	1.1060 s / batch. (data: 9.66e-01). ETA=2 days, 13:33:49, max mem: 5.0 GB 
[11/07 05:57:48 visual_prompt]: 	Training 1000/2212. train loss: 0.4579,	0.1729 s / batch. (data: 3.72e-04). ETA=9:37:17, max mem: 5.0 GB 
[11/07 05:58:10 visual_prompt]: 	Training 1100/2212. train loss: 0.6470,	0.2060 s / batch. (data: 6.85e-04). ETA=11:27:13, max mem: 5.0 GB 
[11/07 05:58:30 visual_prompt]: 	Training 1200/2212. train loss: 1.0791,	0.2184 s / batch. (data: 2.07e-02). ETA=12:08:24, max mem: 5.0 GB 
[11/07 05:58:51 visual_prompt]: 	Training 1300/2212. train loss: 0.9148,	0.1966 s / batch. (data: 2.06e-02). ETA=10:55:14, max mem: 5.0 GB 
[11/07 05:59:12 visual_prompt]: 	Training 1400/2212. train loss: 1.5989,	0.1592 s / batch. (data: 5.40e-03). ETA=8:50:21, max mem: 5.0 GB 
[11/07 05:59:33 visual_prompt]: 	Training 1500/2212. train loss: 0.5250,	0.1793 s / batch. (data: 7.81e-03). ETA=9:57:03, max mem: 5.0 GB 
[11/07 05:59:55 visual_prompt]: 	Training 1600/2212. train loss: 0.6246,	0.2222 s / batch. (data: 5.22e-02). ETA=12:19:24, max mem: 5.0 GB 
[11/07 06:00:16 visual_prompt]: 	Training 1700/2212. train loss: 0.3874,	0.2087 s / batch. (data: 2.23e-04). ETA=11:34:15, max mem: 5.0 GB 
[11/07 06:00:37 visual_prompt]: 	Training 1800/2212. train loss: 1.1034,	0.1610 s / batch. (data: 5.33e-03). ETA=8:55:23, max mem: 5.0 GB 
[11/07 06:00:58 visual_prompt]: 	Training 1900/2212. train loss: 0.7878,	0.1853 s / batch. (data: 5.36e-03). ETA=10:15:51, max mem: 5.0 GB 
[11/07 06:01:19 visual_prompt]: 	Training 2000/2212. train loss: 0.6634,	0.2261 s / batch. (data: 2.43e-04). ETA=12:30:52, max mem: 5.0 GB 
[11/07 06:01:42 visual_prompt]: 	Training 2100/2212. train loss: 0.6341,	0.2237 s / batch. (data: 1.09e-02). ETA=12:22:37, max mem: 5.0 GB 
[11/07 06:02:02 visual_prompt]: 	Training 2200/2212. train loss: 0.5619,	0.1180 s / batch. (data: 1.51e-04). ETA=6:31:29, max mem: 5.0 GB 
[11/07 06:02:03 visual_prompt]: Epoch 10 / 100: avg data time: 2.85e-02, avg batch time: 0.2133, average train loss: 0.7254
[11/07 06:02:24 visual_prompt]: 	Test 100/246. loss: 0.613, 0.0968 s / batch. (data: 2.98e-05)max mem: 4.96204 GB 
[11/07 06:02:44 visual_prompt]: 	Test 200/246. loss: 0.613, 0.1086 s / batch. (data: 3.10e-05)max mem: 4.96204 GB 
[11/07 06:02:52 visual_prompt]: Inference (val):avg data time: 1.05e-04, avg batch time: 0.0649, average loss: 0.7048
[11/07 06:02:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.03	
[11/07 06:02:52 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/07 06:03:16 visual_prompt]: 	Training 100/2212. train loss: 0.2875,	0.2163 s / batch. (data: 1.55e-02). ETA=11:57:11, max mem: 5.0 GB 
[11/07 06:03:38 visual_prompt]: 	Training 200/2212. train loss: 0.7569,	0.2270 s / batch. (data: 1.79e-02). ETA=12:32:19, max mem: 5.0 GB 
[11/07 06:04:01 visual_prompt]: 	Training 300/2212. train loss: 0.7046,	0.2270 s / batch. (data: 5.37e-03). ETA=12:31:54, max mem: 5.0 GB 
[11/07 06:04:23 visual_prompt]: 	Training 400/2212. train loss: 0.9635,	0.1957 s / batch. (data: 2.54e-02). ETA=10:47:58, max mem: 5.0 GB 
[11/07 06:04:44 visual_prompt]: 	Training 500/2212. train loss: 0.9682,	0.1530 s / batch. (data: 7.00e-04). ETA=8:26:26, max mem: 5.0 GB 
[11/07 06:05:05 visual_prompt]: 	Training 600/2212. train loss: 0.5917,	0.1170 s / batch. (data: 5.32e-04). ETA=6:27:09, max mem: 5.0 GB 
[11/07 06:05:27 visual_prompt]: 	Training 700/2212. train loss: 1.0594,	0.9268 s / batch. (data: 8.28e-01). ETA=2 days, 3:04:15, max mem: 5.0 GB 
[11/07 06:05:47 visual_prompt]: 	Training 800/2212. train loss: 0.6491,	0.2193 s / batch. (data: 1.04e-02). ETA=12:04:38, max mem: 5.0 GB 
[11/07 06:06:08 visual_prompt]: 	Training 900/2212. train loss: 0.8829,	0.1812 s / batch. (data: 5.76e-03). ETA=9:58:20, max mem: 5.0 GB 
[11/07 06:06:28 visual_prompt]: 	Training 1000/2212. train loss: 0.7404,	0.1967 s / batch. (data: 3.25e-02). ETA=10:49:24, max mem: 5.0 GB 
[11/07 06:06:50 visual_prompt]: 	Training 1100/2212. train loss: 0.4195,	0.2780 s / batch. (data: 3.11e-02). ETA=15:17:12, max mem: 5.0 GB 
[11/07 06:07:10 visual_prompt]: 	Training 1200/2212. train loss: 0.7125,	0.1995 s / batch. (data: 1.66e-02). ETA=10:58:06, max mem: 5.0 GB 
[11/07 06:07:32 visual_prompt]: 	Training 1300/2212. train loss: 0.7946,	0.1820 s / batch. (data: 2.63e-04). ETA=9:59:54, max mem: 5.0 GB 
[11/07 06:07:53 visual_prompt]: 	Training 1400/2212. train loss: 1.7125,	0.1749 s / batch. (data: 1.55e-02). ETA=9:36:14, max mem: 5.0 GB 
[11/07 06:08:15 visual_prompt]: 	Training 1500/2212. train loss: 0.4543,	0.2061 s / batch. (data: 6.41e-04). ETA=11:18:47, max mem: 5.0 GB 
[11/07 06:08:36 visual_prompt]: 	Training 1600/2212. train loss: 1.1059,	0.2149 s / batch. (data: 5.36e-03). ETA=11:47:16, max mem: 5.0 GB 
[11/07 06:08:57 visual_prompt]: 	Training 1700/2212. train loss: 0.4332,	0.1789 s / batch. (data: 2.60e-04). ETA=9:48:23, max mem: 5.0 GB 
[11/07 06:09:18 visual_prompt]: 	Training 1800/2212. train loss: 0.3884,	0.2101 s / batch. (data: 2.34e-02). ETA=11:30:50, max mem: 5.0 GB 
[11/07 06:09:40 visual_prompt]: 	Training 1900/2212. train loss: 0.4469,	0.1861 s / batch. (data: 1.04e-02). ETA=10:11:25, max mem: 5.0 GB 
[11/07 06:10:01 visual_prompt]: 	Training 2000/2212. train loss: 0.7371,	0.2142 s / batch. (data: 5.40e-03). ETA=11:43:28, max mem: 5.0 GB 
[11/07 06:10:22 visual_prompt]: 	Training 2100/2212. train loss: 0.3644,	0.2153 s / batch. (data: 1.55e-02). ETA=11:46:57, max mem: 5.0 GB 
[11/07 06:10:43 visual_prompt]: 	Training 2200/2212. train loss: 0.7895,	0.1084 s / batch. (data: 1.24e-04). ETA=5:55:41, max mem: 5.0 GB 
[11/07 06:10:44 visual_prompt]: Epoch 11 / 100: avg data time: 2.92e-02, avg batch time: 0.2133, average train loss: 0.7167
[11/07 06:11:06 visual_prompt]: 	Test 100/246. loss: 0.733, 0.0786 s / batch. (data: 2.69e-05)max mem: 4.96204 GB 
[11/07 06:11:25 visual_prompt]: 	Test 200/246. loss: 0.733, 0.0957 s / batch. (data: 2.38e-05)max mem: 4.96204 GB 
[11/07 06:11:34 visual_prompt]: Inference (val):avg data time: 9.83e-05, avg batch time: 0.0636, average loss: 0.6901
[11/07 06:11:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.56	
[11/07 06:11:34 visual_prompt]: Best epoch 11: best metric: -0.690
[11/07 06:11:34 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/07 06:11:56 visual_prompt]: 	Training 100/2212. train loss: 0.3699,	0.1725 s / batch. (data: 1.55e-02). ETA=9:25:39, max mem: 5.0 GB 
[11/07 06:12:16 visual_prompt]: 	Training 200/2212. train loss: 0.2541,	0.1619 s / batch. (data: 1.05e-02). ETA=8:50:34, max mem: 5.0 GB 
[11/07 06:12:39 visual_prompt]: 	Training 300/2212. train loss: 0.9950,	0.1897 s / batch. (data: 9.66e-03). ETA=10:21:23, max mem: 5.0 GB 
[11/07 06:13:00 visual_prompt]: 	Training 400/2212. train loss: 0.2365,	0.1973 s / batch. (data: 2.06e-02). ETA=10:46:02, max mem: 5.0 GB 
[11/07 06:13:22 visual_prompt]: 	Training 500/2212. train loss: 0.4296,	0.1711 s / batch. (data: 6.33e-04). ETA=9:19:52, max mem: 5.0 GB 
[11/07 06:13:44 visual_prompt]: 	Training 600/2212. train loss: 1.0286,	0.1704 s / batch. (data: 7.88e-04). ETA=9:17:33, max mem: 5.0 GB 
[11/07 06:14:06 visual_prompt]: 	Training 700/2212. train loss: 0.6107,	0.1595 s / batch. (data: 2.52e-04). ETA=8:41:36, max mem: 5.0 GB 
[11/07 06:14:28 visual_prompt]: 	Training 800/2212. train loss: 1.2170,	0.2320 s / batch. (data: 5.69e-03). ETA=12:38:06, max mem: 5.0 GB 
[11/07 06:14:49 visual_prompt]: 	Training 900/2212. train loss: 0.7222,	0.6334 s / batch. (data: 4.88e-01). ETA=1 day, 10:28:44, max mem: 5.0 GB 
[11/07 06:15:10 visual_prompt]: 	Training 1000/2212. train loss: 0.3871,	0.1994 s / batch. (data: 1.04e-02). ETA=10:50:52, max mem: 5.0 GB 
[11/07 06:15:31 visual_prompt]: 	Training 1100/2212. train loss: 0.8877,	0.2075 s / batch. (data: 2.06e-02). ETA=11:17:03, max mem: 5.0 GB 
[11/07 06:15:52 visual_prompt]: 	Training 1200/2212. train loss: 1.1470,	0.2472 s / batch. (data: 1.60e-02). ETA=13:26:13, max mem: 5.0 GB 
[11/07 06:16:12 visual_prompt]: 	Training 1300/2212. train loss: 0.4174,	0.2202 s / batch. (data: 5.37e-03). ETA=11:57:51, max mem: 5.0 GB 
[11/07 06:16:34 visual_prompt]: 	Training 1400/2212. train loss: 0.4779,	0.1925 s / batch. (data: 1.25e-02). ETA=10:27:09, max mem: 5.0 GB 
[11/07 06:16:56 visual_prompt]: 	Training 1500/2212. train loss: 0.4254,	0.4923 s / batch. (data: 3.53e-01). ETA=1 day, 2:42:57, max mem: 5.0 GB 
[11/07 06:17:17 visual_prompt]: 	Training 1600/2212. train loss: 0.7787,	0.1917 s / batch. (data: 6.61e-04). ETA=10:23:49, max mem: 5.0 GB 
[11/07 06:17:38 visual_prompt]: 	Training 1700/2212. train loss: 0.5578,	0.2306 s / batch. (data: 1.65e-02). ETA=12:30:09, max mem: 5.0 GB 
[11/07 06:18:00 visual_prompt]: 	Training 1800/2212. train loss: 0.8393,	0.1423 s / batch. (data: 6.66e-04). ETA=7:42:37, max mem: 5.0 GB 
[11/07 06:18:21 visual_prompt]: 	Training 1900/2212. train loss: 0.5474,	0.2368 s / batch. (data: 1.08e-02). ETA=12:49:19, max mem: 5.0 GB 
[11/07 06:18:41 visual_prompt]: 	Training 2000/2212. train loss: 1.0062,	0.1399 s / batch. (data: 1.04e-02). ETA=7:34:21, max mem: 5.0 GB 
[11/07 06:19:03 visual_prompt]: 	Training 2100/2212. train loss: 0.9365,	0.1773 s / batch. (data: 5.38e-03). ETA=9:35:36, max mem: 5.0 GB 
[11/07 06:19:24 visual_prompt]: 	Training 2200/2212. train loss: 0.3871,	0.1084 s / batch. (data: 8.54e-05). ETA=5:51:51, max mem: 5.0 GB 
[11/07 06:19:25 visual_prompt]: Epoch 12 / 100: avg data time: 2.94e-02, avg batch time: 0.2131, average train loss: 0.7121
[11/07 06:19:47 visual_prompt]: 	Test 100/246. loss: 0.405, 0.0537 s / batch. (data: 2.62e-05)max mem: 4.96204 GB 
[11/07 06:20:06 visual_prompt]: 	Test 200/246. loss: 0.404, 0.0755 s / batch. (data: 4.63e-05)max mem: 4.96204 GB 
[11/07 06:20:14 visual_prompt]: Inference (val):avg data time: 2.46e-04, avg batch time: 0.0637, average loss: 0.7860
[11/07 06:20:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.87	
[11/07 06:20:15 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/07 06:20:39 visual_prompt]: 	Training 100/2212. train loss: 0.7230,	0.2146 s / batch. (data: 4.44e-02). ETA=11:35:57, max mem: 5.0 GB 
[11/07 06:21:01 visual_prompt]: 	Training 200/2212. train loss: 0.6570,	0.1995 s / batch. (data: 1.55e-02). ETA=10:46:43, max mem: 5.0 GB 
[11/07 06:21:22 visual_prompt]: 	Training 300/2212. train loss: 0.6309,	0.0982 s / batch. (data: 2.41e-04). ETA=5:17:57, max mem: 5.0 GB 
[11/07 06:21:42 visual_prompt]: 	Training 400/2212. train loss: 1.0584,	0.1617 s / batch. (data: 9.13e-03). ETA=8:43:22, max mem: 5.0 GB 
[11/07 06:22:03 visual_prompt]: 	Training 500/2212. train loss: 0.6756,	0.1906 s / batch. (data: 2.78e-04). ETA=10:16:47, max mem: 5.0 GB 
[11/07 06:22:25 visual_prompt]: 	Training 600/2212. train loss: 0.5066,	0.1981 s / batch. (data: 6.38e-03). ETA=10:40:50, max mem: 5.0 GB 
[11/07 06:22:47 visual_prompt]: 	Training 700/2212. train loss: 0.8665,	0.1977 s / batch. (data: 2.75e-04). ETA=10:38:58, max mem: 5.0 GB 
[11/07 06:23:09 visual_prompt]: 	Training 800/2212. train loss: 0.6133,	0.2374 s / batch. (data: 1.05e-02). ETA=12:47:06, max mem: 5.0 GB 
[11/07 06:23:30 visual_prompt]: 	Training 900/2212. train loss: 0.4481,	0.1909 s / batch. (data: 1.69e-02). ETA=10:16:22, max mem: 5.0 GB 
[11/07 06:23:51 visual_prompt]: 	Training 1000/2212. train loss: 0.4962,	0.2272 s / batch. (data: 1.04e-02). ETA=12:13:25, max mem: 5.0 GB 
[11/07 06:24:12 visual_prompt]: 	Training 1100/2212. train loss: 0.8152,	0.2015 s / batch. (data: 6.93e-04). ETA=10:50:03, max mem: 5.0 GB 
[11/07 06:24:34 visual_prompt]: 	Training 1200/2212. train loss: 0.6355,	0.2002 s / batch. (data: 2.78e-04). ETA=10:45:31, max mem: 5.0 GB 
[11/07 06:24:55 visual_prompt]: 	Training 1300/2212. train loss: 1.1509,	0.1852 s / batch. (data: 1.04e-02). ETA=9:56:48, max mem: 5.0 GB 
[11/07 06:25:17 visual_prompt]: 	Training 1400/2212. train loss: 0.5052,	0.2040 s / batch. (data: 1.55e-02). ETA=10:57:03, max mem: 5.0 GB 
[11/07 06:25:39 visual_prompt]: 	Training 1500/2212. train loss: 0.5306,	0.1982 s / batch. (data: 7.44e-03). ETA=10:37:55, max mem: 5.0 GB 
[11/07 06:26:00 visual_prompt]: 	Training 1600/2212. train loss: 0.6638,	0.2183 s / batch. (data: 1.04e-02). ETA=11:42:17, max mem: 5.0 GB 
[11/07 06:26:21 visual_prompt]: 	Training 1700/2212. train loss: 0.6068,	0.2178 s / batch. (data: 6.85e-04). ETA=11:40:30, max mem: 5.0 GB 
[11/07 06:26:41 visual_prompt]: 	Training 1800/2212. train loss: 1.0547,	0.2079 s / batch. (data: 2.05e-02). ETA=11:08:23, max mem: 5.0 GB 
[11/07 06:27:03 visual_prompt]: 	Training 1900/2212. train loss: 0.6676,	0.1857 s / batch. (data: 1.46e-02). ETA=9:56:31, max mem: 5.0 GB 
[11/07 06:27:24 visual_prompt]: 	Training 2000/2212. train loss: 0.5656,	0.2041 s / batch. (data: 2.63e-04). ETA=10:55:21, max mem: 5.0 GB 
[11/07 06:27:44 visual_prompt]: 	Training 2100/2212. train loss: 0.7913,	0.2042 s / batch. (data: 1.55e-02). ETA=10:55:19, max mem: 5.0 GB 
[11/07 06:28:05 visual_prompt]: 	Training 2200/2212. train loss: 0.7278,	0.1773 s / batch. (data: 1.10e-04). ETA=9:28:49, max mem: 5.0 GB 
[11/07 06:28:06 visual_prompt]: Epoch 13 / 100: avg data time: 2.88e-02, avg batch time: 0.2132, average train loss: 0.7075
[11/07 06:28:28 visual_prompt]: 	Test 100/246. loss: 0.886, 0.0661 s / batch. (data: 3.39e-05)max mem: 4.96204 GB 
[11/07 06:28:47 visual_prompt]: 	Test 200/246. loss: 0.886, 0.0870 s / batch. (data: 2.43e-05)max mem: 4.96204 GB 
[11/07 06:28:56 visual_prompt]: Inference (val):avg data time: 9.86e-05, avg batch time: 0.0626, average loss: 0.6915
[11/07 06:28:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.27	
[11/07 06:28:56 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/07 06:29:19 visual_prompt]: 	Training 100/2212. train loss: 0.9027,	0.1763 s / batch. (data: 1.60e-02). ETA=9:25:09, max mem: 5.0 GB 
[11/07 06:29:41 visual_prompt]: 	Training 200/2212. train loss: 0.7642,	0.1813 s / batch. (data: 8.07e-03). ETA=9:40:58, max mem: 5.0 GB 
[11/07 06:30:04 visual_prompt]: 	Training 300/2212. train loss: 0.5944,	0.2206 s / batch. (data: 6.08e-03). ETA=11:46:21, max mem: 5.0 GB 
[11/07 06:30:25 visual_prompt]: 	Training 400/2212. train loss: 0.8178,	0.1270 s / batch. (data: 6.34e-04). ETA=6:46:21, max mem: 5.0 GB 
[11/07 06:30:47 visual_prompt]: 	Training 500/2212. train loss: 0.6737,	0.2103 s / batch. (data: 6.88e-03). ETA=11:12:43, max mem: 5.0 GB 
[11/07 06:31:09 visual_prompt]: 	Training 600/2212. train loss: 0.4576,	0.2359 s / batch. (data: 1.80e-03). ETA=12:34:20, max mem: 5.0 GB 
[11/07 06:31:31 visual_prompt]: 	Training 700/2212. train loss: 0.8349,	0.2002 s / batch. (data: 1.04e-02). ETA=10:39:43, max mem: 5.0 GB 
[11/07 06:31:53 visual_prompt]: 	Training 800/2212. train loss: 0.6085,	0.2128 s / batch. (data: 7.26e-03). ETA=11:19:36, max mem: 5.0 GB 
[11/07 06:32:14 visual_prompt]: 	Training 900/2212. train loss: 0.6647,	0.2431 s / batch. (data: 3.11e-02). ETA=12:56:08, max mem: 5.0 GB 
[11/07 06:32:34 visual_prompt]: 	Training 1000/2212. train loss: 0.5699,	0.2188 s / batch. (data: 1.54e-02). ETA=11:38:10, max mem: 5.0 GB 
[11/07 06:32:55 visual_prompt]: 	Training 1100/2212. train loss: 1.1023,	0.2770 s / batch. (data: 4.10e-02). ETA=14:43:14, max mem: 5.0 GB 
[11/07 06:33:16 visual_prompt]: 	Training 1200/2212. train loss: 0.9069,	0.2403 s / batch. (data: 8.59e-02). ETA=12:45:59, max mem: 5.0 GB 
[11/07 06:33:37 visual_prompt]: 	Training 1300/2212. train loss: 0.6878,	0.1908 s / batch. (data: 1.55e-02). ETA=10:07:55, max mem: 5.0 GB 
[11/07 06:33:58 visual_prompt]: 	Training 1400/2212. train loss: 0.8294,	0.1875 s / batch. (data: 1.55e-02). ETA=9:56:59, max mem: 5.0 GB 
[11/07 06:34:19 visual_prompt]: 	Training 1500/2212. train loss: 0.5525,	0.2097 s / batch. (data: 2.88e-04). ETA=11:07:25, max mem: 5.0 GB 
[11/07 06:34:41 visual_prompt]: 	Training 1600/2212. train loss: 1.3303,	0.2240 s / batch. (data: 2.77e-04). ETA=11:52:35, max mem: 5.0 GB 
[11/07 06:35:01 visual_prompt]: 	Training 1700/2212. train loss: 0.5754,	0.2555 s / batch. (data: 2.56e-02). ETA=13:32:23, max mem: 5.0 GB 
[11/07 06:35:23 visual_prompt]: 	Training 1800/2212. train loss: 1.2945,	0.2608 s / batch. (data: 1.09e-02). ETA=13:48:41, max mem: 5.0 GB 
[11/07 06:35:44 visual_prompt]: 	Training 1900/2212. train loss: 1.2849,	0.1713 s / batch. (data: 5.36e-03). ETA=9:04:07, max mem: 5.0 GB 
[11/07 06:36:05 visual_prompt]: 	Training 2000/2212. train loss: 0.7339,	0.1809 s / batch. (data: 7.10e-03). ETA=9:34:15, max mem: 5.0 GB 
[11/07 06:36:26 visual_prompt]: 	Training 2100/2212. train loss: 0.6208,	0.1860 s / batch. (data: 1.04e-02). ETA=9:50:11, max mem: 5.0 GB 
[11/07 06:36:46 visual_prompt]: 	Training 2200/2212. train loss: 0.7996,	0.1704 s / batch. (data: 1.44e-04). ETA=9:00:23, max mem: 5.0 GB 
[11/07 06:36:48 visual_prompt]: Epoch 14 / 100: avg data time: 2.84e-02, avg batch time: 0.2134, average train loss: 0.7077
[11/07 06:37:09 visual_prompt]: 	Test 100/246. loss: 0.800, 0.0388 s / batch. (data: 3.70e-05)max mem: 4.96204 GB 
[11/07 06:37:29 visual_prompt]: 	Test 200/246. loss: 0.800, 0.0711 s / batch. (data: 2.19e-05)max mem: 4.96204 GB 
[11/07 06:37:37 visual_prompt]: Inference (val):avg data time: 3.45e-04, avg batch time: 0.0643, average loss: 0.6884
[11/07 06:37:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.55	
[11/07 06:37:37 visual_prompt]: Best epoch 14: best metric: -0.688
[11/07 06:37:37 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/07 06:38:02 visual_prompt]: 	Training 100/2212. train loss: 0.9677,	0.9184 s / batch. (data: 8.14e-01). ETA=2 days, 0:30:08, max mem: 5.0 GB 
[11/07 06:38:24 visual_prompt]: 	Training 200/2212. train loss: 0.7287,	0.8985 s / batch. (data: 7.61e-01). ETA=1 day, 23:25:43, max mem: 5.0 GB 
[11/07 06:38:45 visual_prompt]: 	Training 300/2212. train loss: 0.4303,	0.2417 s / batch. (data: 2.44e-02). ETA=12:45:15, max mem: 5.0 GB 
[11/07 06:39:08 visual_prompt]: 	Training 400/2212. train loss: 0.8677,	0.1779 s / batch. (data: 5.87e-03). ETA=9:22:53, max mem: 5.0 GB 
[11/07 06:39:30 visual_prompt]: 	Training 500/2212. train loss: 0.7920,	0.1871 s / batch. (data: 1.08e-02). ETA=9:51:37, max mem: 5.0 GB 
[11/07 06:39:52 visual_prompt]: 	Training 600/2212. train loss: 0.7992,	0.1951 s / batch. (data: 1.03e-02). ETA=10:16:39, max mem: 5.0 GB 
[11/07 06:40:13 visual_prompt]: 	Training 700/2212. train loss: 0.8823,	0.1978 s / batch. (data: 1.81e-02). ETA=10:24:48, max mem: 5.0 GB 
[11/07 06:40:34 visual_prompt]: 	Training 800/2212. train loss: 0.7864,	0.2040 s / batch. (data: 5.76e-03). ETA=10:44:11, max mem: 5.0 GB 
[11/07 06:40:55 visual_prompt]: 	Training 900/2212. train loss: 0.5326,	0.2020 s / batch. (data: 1.81e-02). ETA=10:37:33, max mem: 5.0 GB 
[11/07 06:41:17 visual_prompt]: 	Training 1000/2212. train loss: 0.6584,	0.1876 s / batch. (data: 7.02e-04). ETA=9:51:39, max mem: 5.0 GB 
[11/07 06:41:39 visual_prompt]: 	Training 1100/2212. train loss: 0.7092,	0.1682 s / batch. (data: 2.54e-04). ETA=8:50:09, max mem: 5.0 GB 
[11/07 06:42:01 visual_prompt]: 	Training 1200/2212. train loss: 0.8292,	0.2575 s / batch. (data: 2.57e-02). ETA=13:31:22, max mem: 5.0 GB 
[11/07 06:42:23 visual_prompt]: 	Training 1300/2212. train loss: 0.4521,	0.1852 s / batch. (data: 2.51e-04). ETA=9:43:15, max mem: 5.0 GB 
[11/07 06:42:45 visual_prompt]: 	Training 1400/2212. train loss: 0.7945,	0.1658 s / batch. (data: 3.95e-03). ETA=8:41:57, max mem: 5.0 GB 
[11/07 06:43:08 visual_prompt]: 	Training 1500/2212. train loss: 0.7751,	0.2180 s / batch. (data: 2.59e-02). ETA=11:25:40, max mem: 5.0 GB 
[11/07 06:43:30 visual_prompt]: 	Training 1600/2212. train loss: 0.6166,	0.2270 s / batch. (data: 1.55e-02). ETA=11:53:35, max mem: 5.0 GB 
[11/07 06:43:51 visual_prompt]: 	Training 1700/2212. train loss: 0.7998,	0.1974 s / batch. (data: 5.41e-03). ETA=10:20:20, max mem: 5.0 GB 
[11/07 06:44:13 visual_prompt]: 	Training 1800/2212. train loss: 0.5993,	0.2127 s / batch. (data: 2.56e-02). ETA=11:08:05, max mem: 5.0 GB 
[11/07 06:44:35 visual_prompt]: 	Training 1900/2212. train loss: 0.4660,	0.2023 s / batch. (data: 5.37e-03). ETA=10:35:08, max mem: 5.0 GB 
[11/07 06:44:58 visual_prompt]: 	Training 2000/2212. train loss: 0.6272,	0.1944 s / batch. (data: 6.21e-03). ETA=10:09:47, max mem: 5.0 GB 
[11/07 06:45:18 visual_prompt]: 	Training 2100/2212. train loss: 0.5943,	0.2248 s / batch. (data: 1.55e-02). ETA=11:44:42, max mem: 5.0 GB 
[11/07 06:45:39 visual_prompt]: 	Training 2200/2212. train loss: 1.1496,	0.1443 s / batch. (data: 1.19e-04). ETA=7:32:06, max mem: 5.0 GB 
[11/07 06:45:40 visual_prompt]: Epoch 15 / 100: avg data time: 3.28e-02, avg batch time: 0.2183, average train loss: 0.7036
[11/07 06:46:02 visual_prompt]: 	Test 100/246. loss: 0.932, 0.0669 s / batch. (data: 6.39e-05)max mem: 4.96204 GB 
[11/07 06:46:23 visual_prompt]: 	Test 200/246. loss: 0.932, 0.0957 s / batch. (data: 2.22e-05)max mem: 4.96204 GB 
[11/07 06:46:32 visual_prompt]: Inference (val):avg data time: 6.86e-05, avg batch time: 0.0665, average loss: 0.6952
[11/07 06:46:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.74	
[11/07 06:46:32 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/07 06:46:57 visual_prompt]: 	Training 100/2212. train loss: 0.4694,	0.2043 s / batch. (data: 2.59e-02). ETA=10:40:01, max mem: 5.0 GB 
[11/07 06:47:19 visual_prompt]: 	Training 200/2212. train loss: 1.0083,	0.1952 s / batch. (data: 1.63e-02). ETA=10:10:54, max mem: 5.0 GB 
[11/07 06:47:41 visual_prompt]: 	Training 300/2212. train loss: 0.3909,	0.1997 s / batch. (data: 5.40e-03). ETA=10:24:46, max mem: 5.0 GB 
[11/07 06:48:02 visual_prompt]: 	Training 400/2212. train loss: 0.8952,	0.1415 s / batch. (data: 2.13e-04). ETA=7:22:29, max mem: 5.0 GB 
[11/07 06:48:23 visual_prompt]: 	Training 500/2212. train loss: 0.6695,	0.1871 s / batch. (data: 5.75e-03). ETA=9:44:39, max mem: 5.0 GB 
[11/07 06:48:44 visual_prompt]: 	Training 600/2212. train loss: 0.5559,	0.2547 s / batch. (data: 1.09e-02). ETA=13:15:28, max mem: 5.0 GB 
[11/07 06:49:07 visual_prompt]: 	Training 700/2212. train loss: 0.7890,	0.1972 s / batch. (data: 7.00e-04). ETA=10:15:40, max mem: 5.0 GB 
[11/07 06:49:29 visual_prompt]: 	Training 800/2212. train loss: 0.6486,	0.1740 s / batch. (data: 7.30e-04). ETA=9:02:57, max mem: 5.0 GB 
[11/07 06:49:51 visual_prompt]: 	Training 900/2212. train loss: 0.5895,	0.2186 s / batch. (data: 1.05e-02). ETA=11:21:49, max mem: 5.0 GB 
[11/07 06:50:14 visual_prompt]: 	Training 1000/2212. train loss: 0.9913,	0.2284 s / batch. (data: 8.94e-02). ETA=11:51:48, max mem: 5.0 GB 
[11/07 06:50:37 visual_prompt]: 	Training 1100/2212. train loss: 0.6420,	0.2026 s / batch. (data: 2.06e-02). ETA=10:31:06, max mem: 5.0 GB 
[11/07 06:50:59 visual_prompt]: 	Training 1200/2212. train loss: 0.9289,	0.2396 s / batch. (data: 2.88e-02). ETA=12:26:04, max mem: 5.0 GB 
[11/07 06:51:22 visual_prompt]: 	Training 1300/2212. train loss: 0.8247,	0.2306 s / batch. (data: 2.07e-02). ETA=11:57:36, max mem: 5.0 GB 
[11/07 06:51:45 visual_prompt]: 	Training 1400/2212. train loss: 0.9930,	0.1282 s / batch. (data: 2.69e-04). ETA=6:38:44, max mem: 5.0 GB 
[11/07 06:52:07 visual_prompt]: 	Training 1500/2212. train loss: 0.7594,	0.1533 s / batch. (data: 3.22e-04). ETA=7:56:40, max mem: 5.0 GB 
[11/07 06:52:30 visual_prompt]: 	Training 1600/2212. train loss: 0.6898,	0.1226 s / batch. (data: 5.38e-03). ETA=6:20:50, max mem: 5.0 GB 
[11/07 06:52:51 visual_prompt]: 	Training 1700/2212. train loss: 0.7022,	0.2258 s / batch. (data: 5.80e-03). ETA=11:41:01, max mem: 5.0 GB 
[11/07 06:53:13 visual_prompt]: 	Training 1800/2212. train loss: 0.8212,	0.1750 s / batch. (data: 2.83e-04). ETA=9:03:16, max mem: 5.0 GB 
[11/07 06:53:33 visual_prompt]: 	Training 1900/2212. train loss: 0.6050,	0.1829 s / batch. (data: 7.03e-03). ETA=9:27:24, max mem: 5.0 GB 
[11/07 06:53:55 visual_prompt]: 	Training 2000/2212. train loss: 0.8570,	0.2015 s / batch. (data: 2.54e-02). ETA=10:24:49, max mem: 5.0 GB 
[11/07 06:54:18 visual_prompt]: 	Training 2100/2212. train loss: 0.7165,	0.1413 s / batch. (data: 2.71e-04). ETA=7:17:58, max mem: 5.0 GB 
[11/07 06:54:40 visual_prompt]: 	Training 2200/2212. train loss: 0.4928,	0.1263 s / batch. (data: 1.45e-04). ETA=6:31:06, max mem: 5.0 GB 
[11/07 06:54:42 visual_prompt]: Epoch 16 / 100: avg data time: 3.23e-02, avg batch time: 0.2214, average train loss: 0.6984
[11/07 06:55:04 visual_prompt]: 	Test 100/246. loss: 1.003, 0.1128 s / batch. (data: 3.81e-05)max mem: 4.96204 GB 
[11/07 06:55:24 visual_prompt]: 	Test 200/246. loss: 1.003, 0.0527 s / batch. (data: 3.15e-05)max mem: 4.96204 GB 
[11/07 06:55:33 visual_prompt]: Inference (val):avg data time: 1.97e-04, avg batch time: 0.0695, average loss: 0.7033
[11/07 06:55:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.17	
[11/07 06:55:33 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/07 06:55:57 visual_prompt]: 	Training 100/2212. train loss: 0.6363,	0.1623 s / batch. (data: 5.37e-03). ETA=8:22:15, max mem: 5.0 GB 
[11/07 06:56:20 visual_prompt]: 	Training 200/2212. train loss: 0.6554,	0.1396 s / batch. (data: 2.86e-04). ETA=7:11:54, max mem: 5.0 GB 
[11/07 06:56:42 visual_prompt]: 	Training 300/2212. train loss: 0.7995,	0.1988 s / batch. (data: 8.10e-04). ETA=10:14:36, max mem: 5.0 GB 
[11/07 06:57:03 visual_prompt]: 	Training 400/2212. train loss: 0.7175,	0.1817 s / batch. (data: 5.40e-03). ETA=9:21:30, max mem: 5.0 GB 
[11/07 06:57:26 visual_prompt]: 	Training 500/2212. train loss: 0.5609,	0.2256 s / batch. (data: 5.34e-03). ETA=11:36:37, max mem: 5.0 GB 
[11/07 06:57:49 visual_prompt]: 	Training 600/2212. train loss: 0.7060,	0.1525 s / batch. (data: 7.01e-04). ETA=7:50:53, max mem: 5.0 GB 
[11/07 06:58:11 visual_prompt]: 	Training 700/2212. train loss: 0.4648,	0.1891 s / batch. (data: 9.23e-03). ETA=9:43:26, max mem: 5.0 GB 
[11/07 06:58:32 visual_prompt]: 	Training 800/2212. train loss: 0.7927,	0.4600 s / batch. (data: 3.47e-01). ETA=23:38:28, max mem: 5.0 GB 
[11/07 06:58:54 visual_prompt]: 	Training 900/2212. train loss: 0.7785,	0.2363 s / batch. (data: 5.82e-03). ETA=12:08:18, max mem: 5.0 GB 
[11/07 06:59:16 visual_prompt]: 	Training 1000/2212. train loss: 0.7806,	0.1935 s / batch. (data: 1.53e-02). ETA=9:56:04, max mem: 5.0 GB 
[11/07 06:59:38 visual_prompt]: 	Training 1100/2212. train loss: 0.5921,	0.1954 s / batch. (data: 1.55e-02). ETA=10:01:37, max mem: 5.0 GB 
[11/07 07:00:00 visual_prompt]: 	Training 1200/2212. train loss: 0.8094,	0.2082 s / batch. (data: 2.06e-02). ETA=10:40:44, max mem: 5.0 GB 
[11/07 07:00:22 visual_prompt]: 	Training 1300/2212. train loss: 0.7803,	0.1522 s / batch. (data: 9.35e-03). ETA=7:48:04, max mem: 5.0 GB 
[11/07 07:00:45 visual_prompt]: 	Training 1400/2212. train loss: 0.5151,	0.2322 s / batch. (data: 2.43e-02). ETA=11:53:32, max mem: 5.0 GB 
[11/07 07:01:08 visual_prompt]: 	Training 1500/2212. train loss: 0.4629,	0.2168 s / batch. (data: 1.09e-02). ETA=11:05:50, max mem: 5.0 GB 
[11/07 07:01:31 visual_prompt]: 	Training 1600/2212. train loss: 0.8753,	0.2028 s / batch. (data: 2.56e-02). ETA=10:22:33, max mem: 5.0 GB 
[11/07 07:01:52 visual_prompt]: 	Training 1700/2212. train loss: 0.9717,	0.2201 s / batch. (data: 3.36e-02). ETA=11:15:30, max mem: 5.0 GB 
[11/07 07:02:14 visual_prompt]: 	Training 1800/2212. train loss: 0.9516,	0.2495 s / batch. (data: 1.76e-02). ETA=12:45:04, max mem: 5.0 GB 
[11/07 07:02:37 visual_prompt]: 	Training 1900/2212. train loss: 0.5035,	0.1701 s / batch. (data: 5.34e-03). ETA=8:41:24, max mem: 5.0 GB 
[11/07 07:03:00 visual_prompt]: 	Training 2000/2212. train loss: 0.4312,	0.2150 s / batch. (data: 2.82e-04). ETA=10:58:31, max mem: 5.0 GB 
[11/07 07:03:22 visual_prompt]: 	Training 2100/2212. train loss: 0.5092,	0.2091 s / batch. (data: 2.15e-04). ETA=10:40:05, max mem: 5.0 GB 
[11/07 07:03:44 visual_prompt]: 	Training 2200/2212. train loss: 0.8056,	0.1784 s / batch. (data: 1.20e-04). ETA=9:05:50, max mem: 5.0 GB 
[11/07 07:03:45 visual_prompt]: Epoch 17 / 100: avg data time: 3.04e-02, avg batch time: 0.2226, average train loss: 0.6962
[11/07 07:04:08 visual_prompt]: 	Test 100/246. loss: 0.874, 0.1032 s / batch. (data: 3.50e-05)max mem: 4.96204 GB 
[11/07 07:04:28 visual_prompt]: 	Test 200/246. loss: 0.874, 0.0943 s / batch. (data: 2.43e-05)max mem: 4.96204 GB 
[11/07 07:04:37 visual_prompt]: Inference (val):avg data time: 5.03e-04, avg batch time: 0.0666, average loss: 0.6907
[11/07 07:04:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.05	
[11/07 07:04:37 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/07 07:05:03 visual_prompt]: 	Training 100/2212. train loss: 0.7501,	0.2423 s / batch. (data: 2.63e-02). ETA=12:21:04, max mem: 5.0 GB 
[11/07 07:05:24 visual_prompt]: 	Training 200/2212. train loss: 0.5358,	0.1307 s / batch. (data: 6.38e-04). ETA=6:39:30, max mem: 5.0 GB 
[11/07 07:05:47 visual_prompt]: 	Training 300/2212. train loss: 0.5017,	0.2149 s / batch. (data: 4.57e-03). ETA=10:56:28, max mem: 5.0 GB 
[11/07 07:06:10 visual_prompt]: 	Training 400/2212. train loss: 0.5999,	0.2332 s / batch. (data: 2.06e-02). ETA=11:51:56, max mem: 5.0 GB 
[11/07 07:06:33 visual_prompt]: 	Training 500/2212. train loss: 0.6620,	0.7881 s / batch. (data: 6.61e-01). ETA=1 day, 16:04:52, max mem: 5.0 GB 
[11/07 07:06:54 visual_prompt]: 	Training 600/2212. train loss: 0.6789,	0.1826 s / batch. (data: 6.04e-03). ETA=9:16:56, max mem: 5.0 GB 
[11/07 07:07:16 visual_prompt]: 	Training 700/2212. train loss: 0.9681,	0.2580 s / batch. (data: 5.90e-03). ETA=13:06:23, max mem: 5.0 GB 
[11/07 07:07:38 visual_prompt]: 	Training 800/2212. train loss: 0.6214,	0.2329 s / batch. (data: 1.45e-02). ETA=11:49:41, max mem: 5.0 GB 
[11/07 07:08:00 visual_prompt]: 	Training 900/2212. train loss: 0.6857,	0.1485 s / batch. (data: 8.15e-03). ETA=7:32:07, max mem: 5.0 GB 
[11/07 07:08:23 visual_prompt]: 	Training 1000/2212. train loss: 0.9058,	0.1734 s / batch. (data: 5.37e-03). ETA=8:47:47, max mem: 5.0 GB 
[11/07 07:08:44 visual_prompt]: 	Training 1100/2212. train loss: 0.6486,	0.1555 s / batch. (data: 9.92e-03). ETA=7:53:00, max mem: 5.0 GB 
[11/07 07:09:05 visual_prompt]: 	Training 1200/2212. train loss: 0.8900,	0.1675 s / batch. (data: 1.63e-02). ETA=8:29:14, max mem: 5.0 GB 
[11/07 07:09:27 visual_prompt]: 	Training 1300/2212. train loss: 0.5236,	0.2495 s / batch. (data: 1.51e-02). ETA=12:37:55, max mem: 5.0 GB 
[11/07 07:09:49 visual_prompt]: 	Training 1400/2212. train loss: 0.6347,	0.2319 s / batch. (data: 1.69e-02). ETA=11:44:04, max mem: 5.0 GB 
[11/07 07:10:12 visual_prompt]: 	Training 1500/2212. train loss: 0.9492,	0.2799 s / batch. (data: 3.00e-03). ETA=14:09:21, max mem: 5.0 GB 
[11/07 07:10:34 visual_prompt]: 	Training 1600/2212. train loss: 0.5987,	0.2372 s / batch. (data: 5.81e-03). ETA=11:59:38, max mem: 5.0 GB 
[11/07 07:10:57 visual_prompt]: 	Training 1700/2212. train loss: 0.6019,	0.1979 s / batch. (data: 3.04e-02). ETA=9:59:49, max mem: 5.0 GB 
[11/07 07:11:20 visual_prompt]: 	Training 1800/2212. train loss: 0.8767,	0.2540 s / batch. (data: 5.37e-03). ETA=12:49:37, max mem: 5.0 GB 
[11/07 07:11:41 visual_prompt]: 	Training 1900/2212. train loss: 0.5725,	0.1897 s / batch. (data: 2.50e-04). ETA=9:34:30, max mem: 5.0 GB 
[11/07 07:12:03 visual_prompt]: 	Training 2000/2212. train loss: 0.8071,	0.2060 s / batch. (data: 1.55e-02). ETA=10:23:27, max mem: 5.0 GB 
[11/07 07:12:24 visual_prompt]: 	Training 2100/2212. train loss: 0.5753,	0.2198 s / batch. (data: 7.29e-04). ETA=11:04:43, max mem: 5.0 GB 
[11/07 07:12:46 visual_prompt]: 	Training 2200/2212. train loss: 0.5579,	0.1470 s / batch. (data: 1.19e-04). ETA=7:24:17, max mem: 5.0 GB 
[11/07 07:12:47 visual_prompt]: Epoch 18 / 100: avg data time: 3.04e-02, avg batch time: 0.2216, average train loss: 0.6959
[11/07 07:13:10 visual_prompt]: 	Test 100/246. loss: 0.822, 0.0579 s / batch. (data: 4.77e-05)max mem: 4.96204 GB 
[11/07 07:13:30 visual_prompt]: 	Test 200/246. loss: 0.822, 0.0933 s / batch. (data: 3.00e-05)max mem: 4.96204 GB 
[11/07 07:13:39 visual_prompt]: Inference (val):avg data time: 6.83e-05, avg batch time: 0.0684, average loss: 0.6886
[11/07 07:13:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.04	
[11/07 07:13:39 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/07 07:14:03 visual_prompt]: 	Training 100/2212. train loss: 0.8436,	0.2200 s / batch. (data: 1.04e-02). ETA=11:04:36, max mem: 5.0 GB 
[11/07 07:14:25 visual_prompt]: 	Training 200/2212. train loss: 0.7897,	0.1994 s / batch. (data: 4.61e-02). ETA=10:02:01, max mem: 5.0 GB 
[11/07 07:14:47 visual_prompt]: 	Training 300/2212. train loss: 0.8875,	0.2095 s / batch. (data: 2.31e-03). ETA=10:32:10, max mem: 5.0 GB 
[11/07 07:15:09 visual_prompt]: 	Training 400/2212. train loss: 0.6162,	0.8067 s / batch. (data: 6.67e-01). ETA=1 day, 16:33:16, max mem: 5.0 GB 
[11/07 07:15:30 visual_prompt]: 	Training 500/2212. train loss: 0.7398,	0.2314 s / batch. (data: 5.40e-03). ETA=11:37:41, max mem: 5.0 GB 
[11/07 07:15:54 visual_prompt]: 	Training 600/2212. train loss: 0.8358,	0.1902 s / batch. (data: 2.55e-02). ETA=9:33:05, max mem: 5.0 GB 
[11/07 07:16:16 visual_prompt]: 	Training 700/2212. train loss: 0.7556,	0.2167 s / batch. (data: 5.67e-03). ETA=10:52:42, max mem: 5.0 GB 
[11/07 07:16:38 visual_prompt]: 	Training 800/2212. train loss: 0.5152,	0.2206 s / batch. (data: 5.36e-03). ETA=11:03:50, max mem: 5.0 GB 
[11/07 07:17:01 visual_prompt]: 	Training 900/2212. train loss: 0.5930,	0.1622 s / batch. (data: 7.77e-03). ETA=8:07:53, max mem: 5.0 GB 
[11/07 07:17:22 visual_prompt]: 	Training 1000/2212. train loss: 0.5242,	0.2230 s / batch. (data: 2.81e-04). ETA=11:10:33, max mem: 5.0 GB 
[11/07 07:17:46 visual_prompt]: 	Training 1100/2212. train loss: 0.9529,	0.1948 s / batch. (data: 1.55e-02). ETA=9:45:23, max mem: 5.0 GB 
[11/07 07:18:08 visual_prompt]: 	Training 1200/2212. train loss: 0.9198,	0.2060 s / batch. (data: 1.66e-02). ETA=10:18:36, max mem: 5.0 GB 
[11/07 07:18:31 visual_prompt]: 	Training 1300/2212. train loss: 0.7943,	0.1283 s / batch. (data: 1.68e-02). ETA=6:24:58, max mem: 5.0 GB 
[11/07 07:18:52 visual_prompt]: 	Training 1400/2212. train loss: 0.6733,	0.1783 s / batch. (data: 2.06e-02). ETA=8:54:52, max mem: 5.0 GB 
[11/07 07:19:14 visual_prompt]: 	Training 1500/2212. train loss: 0.8200,	0.2381 s / batch. (data: 1.04e-02). ETA=11:53:43, max mem: 5.0 GB 
[11/07 07:19:37 visual_prompt]: 	Training 1600/2212. train loss: 0.6503,	0.3141 s / batch. (data: 2.16e-01). ETA=15:41:07, max mem: 5.0 GB 
[11/07 07:19:59 visual_prompt]: 	Training 1700/2212. train loss: 0.5133,	0.1937 s / batch. (data: 5.38e-03). ETA=9:40:10, max mem: 5.0 GB 
[11/07 07:20:21 visual_prompt]: 	Training 1800/2212. train loss: 0.6679,	0.2124 s / batch. (data: 2.48e-02). ETA=10:35:52, max mem: 5.0 GB 
[11/07 07:20:43 visual_prompt]: 	Training 1900/2212. train loss: 0.4675,	0.2466 s / batch. (data: 7.10e-04). ETA=12:17:43, max mem: 5.0 GB 
[11/07 07:21:05 visual_prompt]: 	Training 2000/2212. train loss: 0.6179,	0.2250 s / batch. (data: 1.56e-02). ETA=11:12:44, max mem: 5.0 GB 
[11/07 07:21:27 visual_prompt]: 	Training 2100/2212. train loss: 0.6366,	0.2085 s / batch. (data: 5.87e-03). ETA=10:23:03, max mem: 5.0 GB 
[11/07 07:21:49 visual_prompt]: 	Training 2200/2212. train loss: 0.5362,	0.1131 s / batch. (data: 1.02e-04). ETA=5:37:54, max mem: 5.0 GB 
[11/07 07:21:50 visual_prompt]: Epoch 19 / 100: avg data time: 3.05e-02, avg batch time: 0.2221, average train loss: 0.6951
[11/07 07:22:12 visual_prompt]: 	Test 100/246. loss: 0.443, 0.0612 s / batch. (data: 3.72e-05)max mem: 4.96204 GB 
[11/07 07:22:33 visual_prompt]: 	Test 200/246. loss: 0.443, 0.0992 s / batch. (data: 3.36e-05)max mem: 4.96204 GB 
[11/07 07:22:42 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.0678, average loss: 0.7636
[11/07 07:22:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.22	
[11/07 07:22:42 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/07 07:23:06 visual_prompt]: 	Training 100/2212. train loss: 0.5643,	0.2132 s / batch. (data: 5.39e-03). ETA=10:36:20, max mem: 5.0 GB 
[11/07 07:23:30 visual_prompt]: 	Training 200/2212. train loss: 0.9191,	0.1760 s / batch. (data: 1.59e-02). ETA=8:44:52, max mem: 5.0 GB 
[11/07 07:23:52 visual_prompt]: 	Training 300/2212. train loss: 0.6327,	0.1873 s / batch. (data: 1.70e-02). ETA=9:18:26, max mem: 5.0 GB 
[11/07 07:24:15 visual_prompt]: 	Training 400/2212. train loss: 0.6536,	0.1728 s / batch. (data: 5.41e-03). ETA=8:34:46, max mem: 5.0 GB 
[11/07 07:24:37 visual_prompt]: 	Training 500/2212. train loss: 0.5253,	0.1410 s / batch. (data: 2.66e-04). ETA=6:59:51, max mem: 5.0 GB 
[11/07 07:24:59 visual_prompt]: 	Training 600/2212. train loss: 0.7961,	0.2196 s / batch. (data: 5.97e-03). ETA=10:53:34, max mem: 5.0 GB 
[11/07 07:25:21 visual_prompt]: 	Training 700/2212. train loss: 0.6373,	0.2290 s / batch. (data: 4.64e-02). ETA=11:21:12, max mem: 5.0 GB 
[11/07 07:25:42 visual_prompt]: 	Training 800/2212. train loss: 0.5331,	0.2170 s / batch. (data: 2.46e-02). ETA=10:45:03, max mem: 5.0 GB 
[11/07 07:26:04 visual_prompt]: 	Training 900/2212. train loss: 0.7667,	0.1874 s / batch. (data: 5.37e-03). ETA=9:16:53, max mem: 5.0 GB 
[11/07 07:26:25 visual_prompt]: 	Training 1000/2212. train loss: 1.0764,	0.2159 s / batch. (data: 1.15e-02). ETA=10:41:05, max mem: 5.0 GB 
[11/07 07:26:47 visual_prompt]: 	Training 1100/2212. train loss: 0.5108,	0.2061 s / batch. (data: 1.55e-02). ETA=10:11:41, max mem: 5.0 GB 
[11/07 07:27:09 visual_prompt]: 	Training 1200/2212. train loss: 0.7710,	0.1982 s / batch. (data: 2.39e-02). ETA=9:47:56, max mem: 5.0 GB 
[11/07 07:27:32 visual_prompt]: 	Training 1300/2212. train loss: 0.8242,	0.1867 s / batch. (data: 7.10e-04). ETA=9:13:27, max mem: 5.0 GB 
[11/07 07:27:55 visual_prompt]: 	Training 1400/2212. train loss: 0.5682,	0.1908 s / batch. (data: 1.94e-02). ETA=9:25:21, max mem: 5.0 GB 
[11/07 07:28:17 visual_prompt]: 	Training 1500/2212. train loss: 0.7445,	0.1941 s / batch. (data: 9.93e-03). ETA=9:34:51, max mem: 5.0 GB 
[11/07 07:28:38 visual_prompt]: 	Training 1600/2212. train loss: 0.9278,	0.2026 s / batch. (data: 1.05e-02). ETA=9:59:31, max mem: 5.0 GB 
[11/07 07:29:00 visual_prompt]: 	Training 1700/2212. train loss: 0.8476,	0.2306 s / batch. (data: 2.06e-02). ETA=11:22:07, max mem: 5.0 GB 
[11/07 07:29:22 visual_prompt]: 	Training 1800/2212. train loss: 0.5936,	0.2281 s / batch. (data: 1.09e-02). ETA=11:14:16, max mem: 5.0 GB 
[11/07 07:29:44 visual_prompt]: 	Training 1900/2212. train loss: 0.8725,	0.1911 s / batch. (data: 3.47e-02). ETA=9:24:36, max mem: 5.0 GB 
[11/07 07:30:06 visual_prompt]: 	Training 2000/2212. train loss: 0.6131,	0.2504 s / batch. (data: 2.14e-02). ETA=12:19:23, max mem: 5.0 GB 
[11/07 07:30:29 visual_prompt]: 	Training 2100/2212. train loss: 0.6718,	0.2025 s / batch. (data: 7.98e-03). ETA=9:57:37, max mem: 5.0 GB 
[11/07 07:30:51 visual_prompt]: 	Training 2200/2212. train loss: 0.6736,	0.1550 s / batch. (data: 1.58e-04). ETA=7:37:18, max mem: 5.0 GB 
[11/07 07:30:52 visual_prompt]: Epoch 20 / 100: avg data time: 2.93e-02, avg batch time: 0.2216, average train loss: 0.6938
[11/07 07:31:14 visual_prompt]: 	Test 100/246. loss: 0.647, 0.0842 s / batch. (data: 3.60e-05)max mem: 4.96204 GB 
[11/07 07:31:35 visual_prompt]: 	Test 200/246. loss: 0.646, 0.0736 s / batch. (data: 3.31e-05)max mem: 4.96204 GB 
[11/07 07:31:43 visual_prompt]: Inference (val):avg data time: 1.13e-04, avg batch time: 0.0679, average loss: 0.6985
[11/07 07:31:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.20	
[11/07 07:31:43 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.0009397368756032445
[11/07 07:32:09 visual_prompt]: 	Training 100/2212. train loss: 0.6487,	0.9866 s / batch. (data: 8.07e-01). ETA=2 days, 0:28:15, max mem: 5.0 GB 
[11/07 07:32:31 visual_prompt]: 	Training 200/2212. train loss: 0.6634,	0.1838 s / batch. (data: 5.77e-03). ETA=9:01:21, max mem: 5.0 GB 
[11/07 07:32:55 visual_prompt]: 	Training 300/2212. train loss: 1.0681,	0.1502 s / batch. (data: 1.88e-02). ETA=7:22:09, max mem: 5.0 GB 
[11/07 07:33:18 visual_prompt]: 	Training 400/2212. train loss: 0.8451,	0.2340 s / batch. (data: 2.71e-02). ETA=11:28:36, max mem: 5.0 GB 
[11/07 07:33:39 visual_prompt]: 	Training 500/2212. train loss: 0.7397,	0.1960 s / batch. (data: 1.04e-02). ETA=9:36:33, max mem: 5.0 GB 
[11/07 07:34:01 visual_prompt]: 	Training 600/2212. train loss: 0.9434,	0.2618 s / batch. (data: 2.67e-02). ETA=12:49:38, max mem: 5.0 GB 
[11/07 07:34:24 visual_prompt]: 	Training 700/2212. train loss: 0.5902,	0.2242 s / batch. (data: 2.10e-02). ETA=10:58:34, max mem: 5.0 GB 
[11/07 07:34:48 visual_prompt]: 	Training 800/2212. train loss: 0.7074,	0.2290 s / batch. (data: 7.24e-04). ETA=11:12:25, max mem: 5.0 GB 
[11/07 07:35:10 visual_prompt]: 	Training 900/2212. train loss: 0.8259,	0.1776 s / batch. (data: 3.05e-04). ETA=8:41:16, max mem: 5.0 GB 
[11/07 07:35:31 visual_prompt]: 	Training 1000/2212. train loss: 0.6558,	0.2668 s / batch. (data: 5.65e-02). ETA=13:02:22, max mem: 5.0 GB 
[11/07 07:35:52 visual_prompt]: 	Training 1100/2212. train loss: 0.7166,	0.2122 s / batch. (data: 5.40e-03). ETA=10:22:06, max mem: 5.0 GB 
[11/07 07:36:15 visual_prompt]: 	Training 1200/2212. train loss: 0.7590,	0.2131 s / batch. (data: 5.35e-03). ETA=10:24:16, max mem: 5.0 GB 
[11/07 07:36:38 visual_prompt]: 	Training 1300/2212. train loss: 0.6703,	0.7122 s / batch. (data: 5.63e-01). ETA=1 day, 10:45:09, max mem: 5.0 GB 
[11/07 07:36:59 visual_prompt]: 	Training 1400/2212. train loss: 0.6080,	0.2176 s / batch. (data: 2.10e-02). ETA=10:36:41, max mem: 5.0 GB 
[11/07 07:37:22 visual_prompt]: 	Training 1500/2212. train loss: 0.5322,	0.2442 s / batch. (data: 5.42e-03). ETA=11:54:01, max mem: 5.0 GB 
[11/07 07:37:43 visual_prompt]: 	Training 1600/2212. train loss: 0.5405,	0.1900 s / batch. (data: 2.13e-04). ETA=9:15:20, max mem: 5.0 GB 
[11/07 07:38:04 visual_prompt]: 	Training 1700/2212. train loss: 0.7534,	0.1501 s / batch. (data: 5.33e-03). ETA=7:18:22, max mem: 5.0 GB 
[11/07 07:38:26 visual_prompt]: 	Training 1800/2212. train loss: 0.6612,	0.1424 s / batch. (data: 2.57e-04). ETA=6:55:48, max mem: 5.0 GB 
[11/07 07:38:48 visual_prompt]: 	Training 1900/2212. train loss: 0.5902,	0.2575 s / batch. (data: 1.63e-02). ETA=12:31:19, max mem: 5.0 GB 
[11/07 07:39:10 visual_prompt]: 	Training 2000/2212. train loss: 0.7598,	0.1901 s / batch. (data: 1.56e-02). ETA=9:14:16, max mem: 5.0 GB 
[11/07 07:39:32 visual_prompt]: 	Training 2100/2212. train loss: 0.8507,	0.1865 s / batch. (data: 2.90e-02). ETA=9:03:35, max mem: 5.0 GB 
[11/07 07:39:54 visual_prompt]: 	Training 2200/2212. train loss: 0.5738,	0.1374 s / batch. (data: 1.51e-04). ETA=6:40:17, max mem: 5.0 GB 
[11/07 07:39:55 visual_prompt]: Epoch 21 / 100: avg data time: 3.03e-02, avg batch time: 0.2221, average train loss: 0.6930
[11/07 07:40:17 visual_prompt]: 	Test 100/246. loss: 0.825, 0.0805 s / batch. (data: 2.79e-05)max mem: 4.96204 GB 
[11/07 07:40:37 visual_prompt]: 	Test 200/246. loss: 0.825, 0.0734 s / batch. (data: 4.48e-05)max mem: 4.96204 GB 
[11/07 07:40:46 visual_prompt]: Inference (val):avg data time: 4.92e-05, avg batch time: 0.0682, average loss: 0.6887
[11/07 07:40:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.56	
[11/07 07:40:46 visual_prompt]: Stopping early.
[11/07 07:40:46 visual_prompt]: Rank of current process: 0. World size: 1
[11/07 07:40:46 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/07 07:40:46 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '1', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/07 07:40:46 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/07 07:40:46 visual_prompt]: Training with config:
[11/07 07:40:46 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/crop224/val/seed0/lr0.001_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 1, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/07 07:40:46 visual_prompt]: Loading training data...
[11/07 07:40:46 visual_prompt]: Constructing mammo-cbis dataset train...
[11/07 07:40:46 visual_prompt]: Loading validation data...
[11/07 07:40:46 visual_prompt]: Constructing mammo-cbis dataset val...
[11/07 07:40:46 visual_prompt]: Constructing models...
[11/07 07:40:51 visual_prompt]: Enable all parameters update during training
[11/07 07:40:51 visual_prompt]: Total Parameters: 85800194	 Gradient Parameters: 85800194
[11/07 07:40:51 visual_prompt]: tuned percent:100.000
[11/07 07:40:51 visual_prompt]: Device used for model: 0
[11/07 07:40:51 visual_prompt]: Setting up Evaluator...
[11/07 07:40:51 visual_prompt]: Setting up Trainer...
[11/07 07:40:51 visual_prompt]: 	Setting up the optimizer...
[11/07 07:40:51 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/07 07:41:16 visual_prompt]: 	Training 100/2212. train loss: 17.0073,	0.2506 s / batch. (data: 1.59e-02). ETA=15:23:24, max mem: 5.0 GB 
[11/07 07:41:39 visual_prompt]: 	Training 200/2212. train loss: 8.7706,	0.1854 s / batch. (data: 1.60e-02). ETA=11:23:00, max mem: 5.0 GB 
[11/07 07:42:00 visual_prompt]: 	Training 300/2212. train loss: 0.0001,	0.1880 s / batch. (data: 1.55e-02). ETA=11:32:04, max mem: 5.0 GB 
[11/07 07:42:22 visual_prompt]: 	Training 400/2212. train loss: 9.7364,	0.1659 s / batch. (data: 6.95e-04). ETA=10:10:31, max mem: 5.0 GB 
[11/07 07:42:45 visual_prompt]: 	Training 500/2212. train loss: 9.6752,	0.1955 s / batch. (data: 5.78e-03). ETA=11:59:03, max mem: 5.0 GB 
[11/07 07:43:08 visual_prompt]: 	Training 600/2212. train loss: 0.0000,	0.2457 s / batch. (data: 2.12e-02). ETA=15:03:19, max mem: 5.0 GB 
[11/07 07:43:31 visual_prompt]: 	Training 700/2212. train loss: 10.7484,	0.2140 s / batch. (data: 2.44e-02). ETA=13:06:36, max mem: 5.0 GB 
[11/07 07:43:54 visual_prompt]: 	Training 800/2212. train loss: 10.3468,	0.2525 s / batch. (data: 2.61e-02). ETA=15:27:34, max mem: 5.0 GB 
[11/07 07:44:16 visual_prompt]: 	Training 900/2212. train loss: 0.0000,	0.2685 s / batch. (data: 2.06e-02). ETA=16:25:44, max mem: 5.0 GB 
[11/07 07:44:37 visual_prompt]: 	Training 1000/2212. train loss: 0.0001,	0.2255 s / batch. (data: 2.06e-02). ETA=13:47:41, max mem: 5.0 GB 
[11/07 07:44:59 visual_prompt]: 	Training 1100/2212. train loss: 0.0000,	0.0600 s / batch. (data: 2.47e-04). ETA=3:39:59, max mem: 5.0 GB 
[11/07 07:45:20 visual_prompt]: 	Training 1200/2212. train loss: 0.0000,	0.2079 s / batch. (data: 1.80e-02). ETA=12:42:11, max mem: 5.0 GB 
[11/07 07:45:42 visual_prompt]: 	Training 1300/2212. train loss: 0.0000,	0.2351 s / batch. (data: 1.55e-02). ETA=14:21:35, max mem: 5.0 GB 
[11/07 07:46:05 visual_prompt]: 	Training 1400/2212. train loss: 0.0027,	0.1796 s / batch. (data: 3.16e-04). ETA=10:57:58, max mem: 5.0 GB 
[11/07 07:46:27 visual_prompt]: 	Training 1500/2212. train loss: 5.3648,	0.2152 s / batch. (data: 9.12e-03). ETA=13:07:48, max mem: 5.0 GB 
[11/07 07:46:48 visual_prompt]: 	Training 1600/2212. train loss: 5.2697,	0.1903 s / batch. (data: 2.51e-02). ETA=11:36:21, max mem: 5.0 GB 
[11/07 07:47:11 visual_prompt]: 	Training 1700/2212. train loss: 0.0000,	0.2171 s / batch. (data: 3.95e-03). ETA=13:14:03, max mem: 5.0 GB 
[11/07 07:47:33 visual_prompt]: 	Training 1800/2212. train loss: 0.0000,	0.2103 s / batch. (data: 1.05e-02). ETA=12:48:57, max mem: 5.0 GB 
[11/07 07:47:55 visual_prompt]: 	Training 1900/2212. train loss: 8.6260,	0.2101 s / batch. (data: 3.40e-04). ETA=12:47:52, max mem: 5.0 GB 
[11/07 07:48:18 visual_prompt]: 	Training 2000/2212. train loss: 0.0002,	0.1880 s / batch. (data: 8.04e-03). ETA=11:26:57, max mem: 5.0 GB 
[11/07 07:48:39 visual_prompt]: 	Training 2100/2212. train loss: 14.9015,	0.2267 s / batch. (data: 7.26e-04). ETA=13:47:52, max mem: 5.0 GB 
[11/07 07:49:00 visual_prompt]: 	Training 2200/2212. train loss: 7.6808,	0.1288 s / batch. (data: 1.18e-04). ETA=7:50:17, max mem: 5.0 GB 
[11/07 07:49:01 visual_prompt]: Epoch 1 / 100: avg data time: 2.95e-02, avg batch time: 0.2213, average train loss: 5.2946
[11/07 07:49:23 visual_prompt]: 	Test 100/246. loss: 0.001, 0.1109 s / batch. (data: 2.29e-05)max mem: 4.96204 GB 
[11/07 07:49:44 visual_prompt]: 	Test 200/246. loss: 0.002, 0.0912 s / batch. (data: 2.81e-05)max mem: 4.96204 GB 
[11/07 07:49:52 visual_prompt]: Inference (val):avg data time: 1.86e-04, avg batch time: 0.0677, average loss: 4.3337
[11/07 07:49:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.86	
[11/07 07:49:52 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/07 07:50:18 visual_prompt]: 	Training 100/2212. train loss: 0.1393,	0.1871 s / batch. (data: 1.47e-02). ETA=11:22:32, max mem: 5.0 GB 
[11/07 07:50:40 visual_prompt]: 	Training 200/2212. train loss: 0.8012,	0.2070 s / batch. (data: 9.07e-03). ETA=12:34:38, max mem: 5.0 GB 
[11/07 07:51:03 visual_prompt]: 	Training 300/2212. train loss: 0.0788,	0.1846 s / batch. (data: 2.79e-04). ETA=11:12:41, max mem: 5.0 GB 
[11/07 07:51:24 visual_prompt]: 	Training 400/2212. train loss: 0.6753,	0.2071 s / batch. (data: 5.38e-03). ETA=12:34:32, max mem: 5.0 GB 
[11/07 07:51:45 visual_prompt]: 	Training 500/2212. train loss: 0.0789,	0.2110 s / batch. (data: 2.47e-04). ETA=12:48:16, max mem: 5.0 GB 
[11/07 07:52:06 visual_prompt]: 	Training 600/2212. train loss: 0.3582,	0.1970 s / batch. (data: 5.77e-03). ETA=11:57:04, max mem: 5.0 GB 
[11/07 07:52:28 visual_prompt]: 	Training 700/2212. train loss: 0.2549,	0.1861 s / batch. (data: 1.01e-02). ETA=11:17:04, max mem: 5.0 GB 
[11/07 07:52:51 visual_prompt]: 	Training 800/2212. train loss: 0.0845,	0.1606 s / batch. (data: 2.56e-04). ETA=9:44:04, max mem: 5.0 GB 
[11/07 07:53:14 visual_prompt]: 	Training 900/2212. train loss: 0.8504,	0.2126 s / batch. (data: 1.09e-02). ETA=12:52:41, max mem: 5.0 GB 
[11/07 07:53:37 visual_prompt]: 	Training 1000/2212. train loss: 0.7034,	0.2046 s / batch. (data: 1.55e-02). ETA=12:23:23, max mem: 5.0 GB 
[11/07 07:53:59 visual_prompt]: 	Training 1100/2212. train loss: 0.0461,	0.2169 s / batch. (data: 1.04e-02). ETA=13:07:29, max mem: 5.0 GB 
[11/07 07:54:20 visual_prompt]: 	Training 1200/2212. train loss: 0.4415,	0.2463 s / batch. (data: 2.08e-02). ETA=14:54:01, max mem: 5.0 GB 
[11/07 07:54:44 visual_prompt]: 	Training 1300/2212. train loss: 0.2197,	0.5467 s / batch. (data: 3.70e-01). ETA=1 day, 9:03:34, max mem: 5.0 GB 
[11/07 07:55:06 visual_prompt]: 	Training 1400/2212. train loss: 0.6966,	0.1901 s / batch. (data: 1.05e-02). ETA=11:29:18, max mem: 5.0 GB 
[11/07 07:55:29 visual_prompt]: 	Training 1500/2212. train loss: 1.3098,	0.2164 s / batch. (data: 1.55e-02). ETA=13:04:29, max mem: 5.0 GB 
[11/07 07:55:51 visual_prompt]: 	Training 1600/2212. train loss: 1.9869,	0.2375 s / batch. (data: 1.09e-02). ETA=14:20:29, max mem: 5.0 GB 
[11/07 07:56:13 visual_prompt]: 	Training 1700/2212. train loss: 0.3484,	0.2082 s / batch. (data: 1.43e-02). ETA=12:33:53, max mem: 5.0 GB 
[11/07 07:56:34 visual_prompt]: 	Training 1800/2212. train loss: 0.1993,	0.2465 s / batch. (data: 1.58e-02). ETA=14:52:26, max mem: 5.0 GB 
[11/07 07:56:56 visual_prompt]: 	Training 1900/2212. train loss: 0.2669,	0.1341 s / batch. (data: 1.19e-02). ETA=8:05:16, max mem: 5.0 GB 
[11/07 07:57:18 visual_prompt]: 	Training 2000/2212. train loss: 0.7119,	0.1918 s / batch. (data: 1.05e-02). ETA=11:33:29, max mem: 5.0 GB 
[11/07 07:57:39 visual_prompt]: 	Training 2100/2212. train loss: 0.1638,	0.1883 s / batch. (data: 5.39e-03). ETA=11:20:40, max mem: 5.0 GB 
[11/07 07:58:01 visual_prompt]: 	Training 2200/2212. train loss: 0.2001,	0.1122 s / batch. (data: 1.14e-04). ETA=6:45:17, max mem: 5.0 GB 
[11/07 07:58:02 visual_prompt]: Epoch 2 / 100: avg data time: 3.11e-02, avg batch time: 0.2212, average train loss: 1.0977
[11/07 07:58:24 visual_prompt]: 	Test 100/246. loss: 0.269, 0.0550 s / batch. (data: 3.17e-05)max mem: 4.96204 GB 
[11/07 07:58:45 visual_prompt]: 	Test 200/246. loss: 0.294, 0.1030 s / batch. (data: 2.96e-05)max mem: 4.96204 GB 
[11/07 07:58:53 visual_prompt]: Inference (val):avg data time: 9.83e-05, avg batch time: 0.0688, average loss: 0.8391
[11/07 07:58:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.00	
[11/07 07:58:53 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/07 07:59:17 visual_prompt]: 	Training 100/2212. train loss: 0.4928,	0.2007 s / batch. (data: 6.85e-04). ETA=12:04:43, max mem: 5.0 GB 
[11/07 07:59:39 visual_prompt]: 	Training 200/2212. train loss: 2.2591,	0.2312 s / batch. (data: 1.05e-02). ETA=13:54:38, max mem: 5.0 GB 
[11/07 08:00:02 visual_prompt]: 	Training 300/2212. train loss: 1.3486,	0.2051 s / batch. (data: 1.04e-02). ETA=12:20:09, max mem: 5.0 GB 
[11/07 08:00:25 visual_prompt]: 	Training 400/2212. train loss: 1.4217,	0.2057 s / batch. (data: 3.92e-02). ETA=12:21:46, max mem: 5.0 GB 
[11/07 08:00:46 visual_prompt]: 	Training 500/2212. train loss: 2.2470,	0.2291 s / batch. (data: 1.55e-02). ETA=13:45:47, max mem: 5.0 GB 
[11/07 08:01:10 visual_prompt]: 	Training 600/2212. train loss: 1.1179,	0.2015 s / batch. (data: 2.95e-02). ETA=12:06:03, max mem: 5.0 GB 
[11/07 08:01:32 visual_prompt]: 	Training 700/2212. train loss: 1.3942,	0.1870 s / batch. (data: 3.98e-02). ETA=11:13:26, max mem: 5.0 GB 
[11/07 08:01:54 visual_prompt]: 	Training 800/2212. train loss: 2.6094,	0.2045 s / batch. (data: 2.06e-02). ETA=12:16:12, max mem: 5.0 GB 
[11/07 08:02:16 visual_prompt]: 	Training 900/2212. train loss: 1.1515,	0.2066 s / batch. (data: 1.55e-02). ETA=12:23:25, max mem: 5.0 GB 
[11/07 08:02:37 visual_prompt]: 	Training 1000/2212. train loss: 0.1815,	0.1752 s / batch. (data: 5.37e-03). ETA=10:29:59, max mem: 5.0 GB 
[11/07 08:02:59 visual_prompt]: 	Training 1100/2212. train loss: 0.6591,	0.1578 s / batch. (data: 5.37e-03). ETA=9:27:10, max mem: 5.0 GB 
[11/07 08:03:20 visual_prompt]: 	Training 1200/2212. train loss: 0.7950,	0.1684 s / batch. (data: 1.05e-02). ETA=10:05:00, max mem: 5.0 GB 
[11/07 08:03:43 visual_prompt]: 	Training 1300/2212. train loss: 1.2591,	0.1246 s / batch. (data: 1.36e-02). ETA=7:27:19, max mem: 5.0 GB 
[11/07 08:04:05 visual_prompt]: 	Training 1400/2212. train loss: 0.3342,	0.1252 s / batch. (data: 6.69e-04). ETA=7:29:25, max mem: 5.0 GB 
[11/07 08:04:26 visual_prompt]: 	Training 1500/2212. train loss: 0.1076,	0.1874 s / batch. (data: 1.04e-02). ETA=11:12:16, max mem: 5.0 GB 
[11/07 08:04:48 visual_prompt]: 	Training 1600/2212. train loss: 0.2285,	0.2060 s / batch. (data: 2.10e-02). ETA=12:18:51, max mem: 5.0 GB 
[11/07 08:05:09 visual_prompt]: 	Training 1700/2212. train loss: 1.1235,	0.2173 s / batch. (data: 2.06e-02). ETA=12:58:56, max mem: 5.0 GB 
[11/07 08:05:32 visual_prompt]: 	Training 1800/2212. train loss: 0.3517,	0.2401 s / batch. (data: 1.55e-02). ETA=14:20:07, max mem: 5.0 GB 
[11/07 08:05:54 visual_prompt]: 	Training 1900/2212. train loss: 0.2734,	0.2231 s / batch. (data: 1.49e-02). ETA=13:18:53, max mem: 5.0 GB 
[11/07 08:06:15 visual_prompt]: 	Training 2000/2212. train loss: 1.6778,	0.2480 s / batch. (data: 7.36e-04). ETA=14:47:48, max mem: 5.0 GB 
[11/07 08:06:39 visual_prompt]: 	Training 2100/2212. train loss: 2.6383,	0.2425 s / batch. (data: 2.57e-02). ETA=14:27:32, max mem: 5.0 GB 
[11/07 08:07:00 visual_prompt]: 	Training 2200/2212. train loss: 0.6687,	0.4349 s / batch. (data: 2.76e-01). ETA=1 day, 1:55:19, max mem: 5.0 GB 
[11/07 08:07:02 visual_prompt]: Epoch 3 / 100: avg data time: 3.07e-02, avg batch time: 0.2209, average train loss: 0.9122
[11/07 08:07:24 visual_prompt]: 	Test 100/246. loss: 0.486, 0.0783 s / batch. (data: 3.50e-05)max mem: 4.96204 GB 
[11/07 08:07:45 visual_prompt]: 	Test 200/246. loss: 0.468, 0.0647 s / batch. (data: 2.91e-05)max mem: 4.96204 GB 
[11/07 08:07:53 visual_prompt]: Inference (val):avg data time: 4.98e-05, avg batch time: 0.0680, average loss: 0.7461
[11/07 08:07:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.24	
[11/07 08:07:53 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/07 08:08:19 visual_prompt]: 	Training 100/2212. train loss: 1.6867,	0.1879 s / batch. (data: 2.54e-04). ETA=11:11:39, max mem: 5.0 GB 
[11/07 08:08:41 visual_prompt]: 	Training 200/2212. train loss: 0.6990,	0.1983 s / batch. (data: 2.06e-02). ETA=11:48:23, max mem: 5.0 GB 
[11/07 08:09:03 visual_prompt]: 	Training 300/2212. train loss: 0.3313,	0.2376 s / batch. (data: 5.39e-03). ETA=14:08:30, max mem: 5.0 GB 
[11/07 08:09:25 visual_prompt]: 	Training 400/2212. train loss: 1.0073,	0.2129 s / batch. (data: 7.18e-04). ETA=12:39:50, max mem: 5.0 GB 
[11/07 08:09:46 visual_prompt]: 	Training 500/2212. train loss: 0.6463,	0.1652 s / batch. (data: 8.93e-03). ETA=9:49:22, max mem: 5.0 GB 
[11/07 08:10:08 visual_prompt]: 	Training 600/2212. train loss: 0.3259,	0.1851 s / batch. (data: 7.25e-03). ETA=11:00:14, max mem: 5.0 GB 
[11/07 08:10:30 visual_prompt]: 	Training 700/2212. train loss: 1.3500,	0.2192 s / batch. (data: 9.60e-03). ETA=13:01:21, max mem: 5.0 GB 
[11/07 08:10:54 visual_prompt]: 	Training 800/2212. train loss: 1.5442,	0.1899 s / batch. (data: 1.05e-02). ETA=11:16:24, max mem: 5.0 GB 
[11/07 08:11:15 visual_prompt]: 	Training 900/2212. train loss: 0.2893,	0.2394 s / batch. (data: 1.55e-02). ETA=14:12:27, max mem: 5.0 GB 
[11/07 08:11:37 visual_prompt]: 	Training 1000/2212. train loss: 1.2215,	0.2108 s / batch. (data: 3.07e-02). ETA=12:30:27, max mem: 5.0 GB 
[11/07 08:11:59 visual_prompt]: 	Training 1100/2212. train loss: 2.3748,	0.2054 s / batch. (data: 2.43e-04). ETA=12:10:47, max mem: 5.0 GB 
[11/07 08:12:21 visual_prompt]: 	Training 1200/2212. train loss: 0.2733,	0.1757 s / batch. (data: 8.03e-03). ETA=10:24:39, max mem: 5.0 GB 
[11/07 08:12:43 visual_prompt]: 	Training 1300/2212. train loss: 1.2201,	0.2306 s / batch. (data: 2.45e-02). ETA=13:39:30, max mem: 5.0 GB 
[11/07 08:13:05 visual_prompt]: 	Training 1400/2212. train loss: 0.8250,	0.2443 s / batch. (data: 2.10e-02). ETA=14:27:50, max mem: 5.0 GB 
[11/07 08:13:27 visual_prompt]: 	Training 1500/2212. train loss: 0.4628,	0.1512 s / batch. (data: 5.33e-03). ETA=8:56:56, max mem: 5.0 GB 
[11/07 08:13:49 visual_prompt]: 	Training 1600/2212. train loss: 0.2615,	0.1819 s / batch. (data: 1.55e-02). ETA=10:45:31, max mem: 5.0 GB 
[11/07 08:14:11 visual_prompt]: 	Training 1700/2212. train loss: 0.0936,	0.1550 s / batch. (data: 2.20e-04). ETA=9:09:43, max mem: 5.0 GB 
[11/07 08:14:32 visual_prompt]: 	Training 1800/2212. train loss: 0.9399,	0.1726 s / batch. (data: 2.96e-04). ETA=10:12:12, max mem: 5.0 GB 
[11/07 08:14:54 visual_prompt]: 	Training 1900/2212. train loss: 0.6914,	0.1909 s / batch. (data: 2.43e-04). ETA=11:16:36, max mem: 5.0 GB 
[11/07 08:15:17 visual_prompt]: 	Training 2000/2212. train loss: 1.1759,	0.2140 s / batch. (data: 1.04e-02). ETA=12:37:58, max mem: 5.0 GB 
[11/07 08:15:39 visual_prompt]: 	Training 2100/2212. train loss: 1.2894,	0.1486 s / batch. (data: 2.65e-04). ETA=8:46:21, max mem: 5.0 GB 
[11/07 08:16:02 visual_prompt]: 	Training 2200/2212. train loss: 0.1614,	0.1701 s / batch. (data: 1.02e-04). ETA=10:01:53, max mem: 5.0 GB 
[11/07 08:16:03 visual_prompt]: Epoch 4 / 100: avg data time: 3.23e-02, avg batch time: 0.2212, average train loss: 0.8940
[11/07 08:16:25 visual_prompt]: 	Test 100/246. loss: 0.925, 0.0992 s / batch. (data: 2.36e-05)max mem: 4.96204 GB 
[11/07 08:16:45 visual_prompt]: 	Test 200/246. loss: 0.931, 0.0890 s / batch. (data: 2.69e-05)max mem: 4.96204 GB 
[11/07 08:16:54 visual_prompt]: Inference (val):avg data time: 8.09e-05, avg batch time: 0.0664, average loss: 0.6937
[11/07 08:16:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.44	
[11/07 08:16:54 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/07 08:17:18 visual_prompt]: 	Training 100/2212. train loss: 2.0238,	0.2447 s / batch. (data: 2.07e-02). ETA=14:25:33, max mem: 5.0 GB 
[11/07 08:17:40 visual_prompt]: 	Training 200/2212. train loss: 1.0386,	0.1784 s / batch. (data: 2.05e-02). ETA=10:30:47, max mem: 5.0 GB 
[11/07 08:18:02 visual_prompt]: 	Training 300/2212. train loss: 1.4904,	0.2361 s / batch. (data: 1.55e-02). ETA=13:54:24, max mem: 5.0 GB 
[11/07 08:18:25 visual_prompt]: 	Training 400/2212. train loss: 2.6809,	0.1901 s / batch. (data: 4.09e-03). ETA=11:11:26, max mem: 5.0 GB 
[11/07 08:18:47 visual_prompt]: 	Training 500/2212. train loss: 0.8951,	0.1719 s / batch. (data: 1.55e-02). ETA=10:06:59, max mem: 5.0 GB 
[11/07 08:19:09 visual_prompt]: 	Training 600/2212. train loss: 0.2169,	0.2083 s / batch. (data: 1.58e-02). ETA=12:15:08, max mem: 5.0 GB 
[11/07 08:19:29 visual_prompt]: 	Training 700/2212. train loss: 0.8593,	0.2036 s / batch. (data: 6.65e-04). ETA=11:58:11, max mem: 5.0 GB 
[11/07 08:19:53 visual_prompt]: 	Training 800/2212. train loss: 0.9078,	0.2012 s / batch. (data: 1.95e-02). ETA=11:49:29, max mem: 5.0 GB 
[11/07 08:20:15 visual_prompt]: 	Training 900/2212. train loss: 0.5003,	0.2334 s / batch. (data: 2.36e-04). ETA=13:42:37, max mem: 5.0 GB 
[11/07 08:20:36 visual_prompt]: 	Training 1000/2212. train loss: 0.7297,	0.5011 s / batch. (data: 3.36e-01). ETA=1 day, 5:25:04, max mem: 5.0 GB 
[11/07 08:20:57 visual_prompt]: 	Training 1100/2212. train loss: 0.6109,	0.1140 s / batch. (data: 1.31e-03). ETA=6:41:22, max mem: 5.0 GB 
[11/07 08:21:19 visual_prompt]: 	Training 1200/2212. train loss: 0.6024,	0.2219 s / batch. (data: 2.98e-02). ETA=13:00:46, max mem: 5.0 GB 
[11/07 08:21:40 visual_prompt]: 	Training 1300/2212. train loss: 0.4197,	0.1801 s / batch. (data: 6.26e-04). ETA=10:33:23, max mem: 5.0 GB 
[11/07 08:22:02 visual_prompt]: 	Training 1400/2212. train loss: 0.0003,	0.1690 s / batch. (data: 1.42e-02). ETA=9:54:15, max mem: 5.0 GB 
[11/07 08:22:25 visual_prompt]: 	Training 1500/2212. train loss: 1.2953,	0.9027 s / batch. (data: 7.47e-01). ETA=2 days, 4:52:14, max mem: 5.0 GB 
[11/07 08:22:47 visual_prompt]: 	Training 1600/2212. train loss: 0.5169,	0.1884 s / batch. (data: 3.14e-02). ETA=11:01:46, max mem: 5.0 GB 
[11/07 08:23:10 visual_prompt]: 	Training 1700/2212. train loss: 0.7794,	0.2026 s / batch. (data: 2.56e-04). ETA=11:51:15, max mem: 5.0 GB 
[11/07 08:23:32 visual_prompt]: 	Training 1800/2212. train loss: 0.8215,	0.2225 s / batch. (data: 6.90e-04). ETA=13:00:40, max mem: 5.0 GB 
[11/07 08:23:54 visual_prompt]: 	Training 1900/2212. train loss: 0.6410,	0.2245 s / batch. (data: 5.71e-04). ETA=13:07:27, max mem: 5.0 GB 
[11/07 08:24:16 visual_prompt]: 	Training 2000/2212. train loss: 0.2228,	0.1882 s / batch. (data: 2.78e-04). ETA=10:59:43, max mem: 5.0 GB 
[11/07 08:24:37 visual_prompt]: 	Training 2100/2212. train loss: 1.4759,	0.1843 s / batch. (data: 5.68e-03). ETA=10:45:58, max mem: 5.0 GB 
[11/07 08:24:58 visual_prompt]: 	Training 2200/2212. train loss: 0.4495,	0.1101 s / batch. (data: 1.06e-04). ETA=6:25:27, max mem: 5.0 GB 
[11/07 08:24:59 visual_prompt]: Epoch 5 / 100: avg data time: 3.24e-02, avg batch time: 0.2195, average train loss: 0.8782
[11/07 08:25:21 visual_prompt]: 	Test 100/246. loss: 0.840, 0.1006 s / batch. (data: 2.34e-05)max mem: 4.96204 GB 
[11/07 08:25:42 visual_prompt]: 	Test 200/246. loss: 0.842, 0.0691 s / batch. (data: 2.10e-05)max mem: 4.96204 GB 
[11/07 08:25:50 visual_prompt]: Inference (val):avg data time: 4.24e-04, avg batch time: 0.0670, average loss: 0.6888
[11/07 08:25:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.37	
[11/07 08:25:50 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/07 08:26:14 visual_prompt]: 	Training 100/2212. train loss: 3.4754,	0.2282 s / batch. (data: 1.09e-02). ETA=13:18:44, max mem: 5.0 GB 
[11/07 08:26:37 visual_prompt]: 	Training 200/2212. train loss: 1.6310,	0.1952 s / batch. (data: 2.61e-04). ETA=11:23:09, max mem: 5.0 GB 
[11/07 08:26:59 visual_prompt]: 	Training 300/2212. train loss: 0.4592,	0.1844 s / batch. (data: 5.18e-03). ETA=10:44:48, max mem: 5.0 GB 
[11/07 08:27:20 visual_prompt]: 	Training 400/2212. train loss: 0.8283,	0.2091 s / batch. (data: 1.54e-02). ETA=12:10:59, max mem: 5.0 GB 
[11/07 08:27:42 visual_prompt]: 	Training 500/2212. train loss: 0.4875,	0.1928 s / batch. (data: 7.53e-04). ETA=11:13:36, max mem: 5.0 GB 
[11/07 08:28:05 visual_prompt]: 	Training 600/2212. train loss: 0.4917,	0.1826 s / batch. (data: 6.26e-04). ETA=10:37:43, max mem: 5.0 GB 
[11/07 08:28:27 visual_prompt]: 	Training 700/2212. train loss: 0.7122,	0.2190 s / batch. (data: 2.59e-04). ETA=12:44:37, max mem: 5.0 GB 
[11/07 08:28:49 visual_prompt]: 	Training 800/2212. train loss: 1.0167,	0.2055 s / batch. (data: 5.32e-03). ETA=11:56:58, max mem: 5.0 GB 
[11/07 08:29:11 visual_prompt]: 	Training 900/2212. train loss: 0.6651,	0.2205 s / batch. (data: 3.04e-04). ETA=12:48:51, max mem: 5.0 GB 
[11/07 08:29:34 visual_prompt]: 	Training 1000/2212. train loss: 0.7879,	0.2213 s / batch. (data: 1.04e-02). ETA=12:51:27, max mem: 5.0 GB 
[11/07 08:29:55 visual_prompt]: 	Training 1100/2212. train loss: 0.2936,	0.1763 s / batch. (data: 2.26e-04). ETA=10:14:23, max mem: 5.0 GB 
[11/07 08:30:15 visual_prompt]: 	Training 1200/2212. train loss: 0.3209,	0.1680 s / batch. (data: 2.18e-04). ETA=9:45:12, max mem: 5.0 GB 
[11/07 08:30:38 visual_prompt]: 	Training 1300/2212. train loss: 0.4799,	0.2055 s / batch. (data: 2.05e-02). ETA=11:55:21, max mem: 5.0 GB 
[11/07 08:31:00 visual_prompt]: 	Training 1400/2212. train loss: 0.3625,	0.2032 s / batch. (data: 6.60e-03). ETA=11:46:49, max mem: 5.0 GB 
[11/07 08:31:22 visual_prompt]: 	Training 1500/2212. train loss: 0.9777,	0.1896 s / batch. (data: 1.55e-02). ETA=10:59:25, max mem: 5.0 GB 
[11/07 08:31:44 visual_prompt]: 	Training 1600/2212. train loss: 0.3323,	0.1160 s / batch. (data: 1.21e-02). ETA=6:43:05, max mem: 5.0 GB 
[11/07 08:32:07 visual_prompt]: 	Training 1700/2212. train loss: 0.6734,	0.2304 s / batch. (data: 6.39e-03). ETA=13:20:25, max mem: 5.0 GB 
[11/07 08:32:30 visual_prompt]: 	Training 1800/2212. train loss: 0.9524,	0.2253 s / batch. (data: 6.76e-04). ETA=13:02:11, max mem: 5.0 GB 
[11/07 08:32:51 visual_prompt]: 	Training 1900/2212. train loss: 1.0611,	0.2109 s / batch. (data: 1.54e-02). ETA=12:11:49, max mem: 5.0 GB 
[11/07 08:33:11 visual_prompt]: 	Training 2000/2212. train loss: 0.7980,	0.2033 s / batch. (data: 4.07e-02). ETA=11:45:07, max mem: 5.0 GB 
[11/07 08:33:32 visual_prompt]: 	Training 2100/2212. train loss: 0.7706,	0.1877 s / batch. (data: 2.56e-04). ETA=10:50:46, max mem: 5.0 GB 
[11/07 08:33:54 visual_prompt]: 	Training 2200/2212. train loss: 0.8025,	0.1550 s / batch. (data: 1.04e-04). ETA=8:57:10, max mem: 5.0 GB 
[11/07 08:33:55 visual_prompt]: Epoch 6 / 100: avg data time: 3.26e-02, avg batch time: 0.2194, average train loss: 0.8033
[11/07 08:34:17 visual_prompt]: 	Test 100/246. loss: 1.030, 0.1128 s / batch. (data: 2.57e-05)max mem: 4.96204 GB 
[11/07 08:34:38 visual_prompt]: 	Test 200/246. loss: 1.026, 0.0958 s / batch. (data: 2.12e-05)max mem: 4.96204 GB 
[11/07 08:34:46 visual_prompt]: Inference (val):avg data time: 6.13e-05, avg batch time: 0.0656, average loss: 0.7057
[11/07 08:34:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.21	
[11/07 08:34:46 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/07 08:35:10 visual_prompt]: 	Training 100/2212. train loss: 0.1194,	0.2465 s / batch. (data: 1.99e-02). ETA=14:13:54, max mem: 5.0 GB 
[11/07 08:35:34 visual_prompt]: 	Training 200/2212. train loss: 0.2359,	0.1678 s / batch. (data: 6.93e-04). ETA=9:40:53, max mem: 5.0 GB 
[11/07 08:35:58 visual_prompt]: 	Training 300/2212. train loss: 0.9825,	0.1721 s / batch. (data: 1.03e-02). ETA=9:55:42, max mem: 5.0 GB 
[11/07 08:36:21 visual_prompt]: 	Training 400/2212. train loss: 1.2486,	0.1764 s / batch. (data: 3.87e-02). ETA=10:10:17, max mem: 5.0 GB 
[11/07 08:36:42 visual_prompt]: 	Training 500/2212. train loss: 1.4225,	0.2536 s / batch. (data: 1.59e-02). ETA=14:36:40, max mem: 5.0 GB 
[11/07 08:37:04 visual_prompt]: 	Training 600/2212. train loss: 0.8155,	0.1610 s / batch. (data: 6.06e-04). ETA=9:16:21, max mem: 5.0 GB 
[11/07 08:37:26 visual_prompt]: 	Training 700/2212. train loss: 0.1980,	0.2079 s / batch. (data: 2.52e-04). ETA=11:58:00, max mem: 5.0 GB 
[11/07 08:37:47 visual_prompt]: 	Training 800/2212. train loss: 0.3860,	0.2190 s / batch. (data: 2.93e-02). ETA=12:35:58, max mem: 5.0 GB 
[11/07 08:38:09 visual_prompt]: 	Training 900/2212. train loss: 0.6925,	0.2061 s / batch. (data: 1.55e-02). ETA=11:51:03, max mem: 5.0 GB 
[11/07 08:38:31 visual_prompt]: 	Training 1000/2212. train loss: 0.5898,	0.1693 s / batch. (data: 6.28e-04). ETA=9:43:47, max mem: 5.0 GB 
[11/07 08:38:52 visual_prompt]: 	Training 1100/2212. train loss: 0.7698,	0.1477 s / batch. (data: 2.46e-04). ETA=8:29:07, max mem: 5.0 GB 
[11/07 08:39:13 visual_prompt]: 	Training 1200/2212. train loss: 0.4889,	0.1789 s / batch. (data: 1.89e-02). ETA=10:16:29, max mem: 5.0 GB 
[11/07 08:39:35 visual_prompt]: 	Training 1300/2212. train loss: 0.6733,	0.1601 s / batch. (data: 3.25e-02). ETA=9:11:12, max mem: 5.0 GB 
[11/07 08:39:56 visual_prompt]: 	Training 1400/2212. train loss: 0.4502,	0.2408 s / batch. (data: 2.05e-02). ETA=13:49:01, max mem: 5.0 GB 
[11/07 08:40:18 visual_prompt]: 	Training 1500/2212. train loss: 1.6218,	0.1983 s / batch. (data: 2.33e-04). ETA=11:22:05, max mem: 5.0 GB 
[11/07 08:40:39 visual_prompt]: 	Training 1600/2212. train loss: 0.4768,	0.2271 s / batch. (data: 6.22e-02). ETA=13:00:53, max mem: 5.0 GB 
[11/07 08:41:02 visual_prompt]: 	Training 1700/2212. train loss: 0.8390,	0.2092 s / batch. (data: 1.59e-02). ETA=11:58:58, max mem: 5.0 GB 
[11/07 08:41:23 visual_prompt]: 	Training 1800/2212. train loss: 0.9950,	0.2312 s / batch. (data: 5.79e-03). ETA=13:14:07, max mem: 5.0 GB 
[11/07 08:41:45 visual_prompt]: 	Training 1900/2212. train loss: 0.8383,	0.1911 s / batch. (data: 1.77e-02). ETA=10:56:06, max mem: 5.0 GB 
[11/07 08:42:07 visual_prompt]: 	Training 2000/2212. train loss: 0.9636,	0.1875 s / batch. (data: 5.33e-03). ETA=10:43:25, max mem: 5.0 GB 
[11/07 08:42:29 visual_prompt]: 	Training 2100/2212. train loss: 1.0157,	0.2081 s / batch. (data: 5.32e-03). ETA=11:54:02, max mem: 5.0 GB 
[11/07 08:42:50 visual_prompt]: 	Training 2200/2212. train loss: 0.7969,	0.1383 s / batch. (data: 1.02e-04). ETA=7:54:17, max mem: 5.0 GB 
[11/07 08:42:51 visual_prompt]: Epoch 7 / 100: avg data time: 3.10e-02, avg batch time: 0.2193, average train loss: 0.7624
[11/07 08:43:14 visual_prompt]: 	Test 100/246. loss: 0.898, 0.0945 s / batch. (data: 2.48e-05)max mem: 4.96204 GB 
[11/07 08:43:34 visual_prompt]: 	Test 200/246. loss: 0.892, 0.0654 s / batch. (data: 1.98e-05)max mem: 4.96204 GB 
[11/07 08:43:42 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.0642, average loss: 0.6903
[11/07 08:43:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.25	
[11/07 08:43:42 visual_prompt]: Best epoch 7: best metric: -0.690
[11/07 08:43:42 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/07 08:44:07 visual_prompt]: 	Training 100/2212. train loss: 0.1556,	0.2381 s / batch. (data: 2.21e-02). ETA=13:35:53, max mem: 5.0 GB 
[11/07 08:44:29 visual_prompt]: 	Training 200/2212. train loss: 0.8309,	0.2425 s / batch. (data: 2.60e-02). ETA=13:50:41, max mem: 5.0 GB 
[11/07 08:44:50 visual_prompt]: 	Training 300/2212. train loss: 0.8390,	0.1733 s / batch. (data: 5.30e-03). ETA=9:53:13, max mem: 5.0 GB 
[11/07 08:45:12 visual_prompt]: 	Training 400/2212. train loss: 0.3260,	0.2586 s / batch. (data: 1.84e-02). ETA=14:44:48, max mem: 5.0 GB 
[11/07 08:45:36 visual_prompt]: 	Training 500/2212. train loss: 0.4819,	0.3948 s / batch. (data: 2.76e-01). ETA=22:30:20, max mem: 5.0 GB 
[11/07 08:45:58 visual_prompt]: 	Training 600/2212. train loss: 0.6748,	0.1798 s / batch. (data: 5.72e-03). ETA=10:14:43, max mem: 5.0 GB 
[11/07 08:46:19 visual_prompt]: 	Training 700/2212. train loss: 0.2610,	0.2296 s / batch. (data: 1.54e-02). ETA=13:04:34, max mem: 5.0 GB 
[11/07 08:46:40 visual_prompt]: 	Training 800/2212. train loss: 0.3941,	0.1635 s / batch. (data: 2.26e-04). ETA=9:18:30, max mem: 5.0 GB 
[11/07 08:47:03 visual_prompt]: 	Training 900/2212. train loss: 0.4259,	0.1854 s / batch. (data: 1.04e-02). ETA=10:33:00, max mem: 5.0 GB 
[11/07 08:47:25 visual_prompt]: 	Training 1000/2212. train loss: 1.1341,	0.1978 s / batch. (data: 6.88e-04). ETA=11:14:53, max mem: 5.0 GB 
[11/07 08:47:46 visual_prompt]: 	Training 1100/2212. train loss: 0.6023,	0.1877 s / batch. (data: 1.57e-02). ETA=10:40:06, max mem: 5.0 GB 
[11/07 08:48:09 visual_prompt]: 	Training 1200/2212. train loss: 0.6748,	0.1952 s / batch. (data: 2.60e-04). ETA=11:05:22, max mem: 5.0 GB 
[11/07 08:48:31 visual_prompt]: 	Training 1300/2212. train loss: 0.9671,	0.2666 s / batch. (data: 7.20e-04). ETA=15:08:15, max mem: 5.0 GB 
[11/07 08:48:52 visual_prompt]: 	Training 1400/2212. train loss: 0.8380,	0.2807 s / batch. (data: 2.87e-02). ETA=15:55:48, max mem: 5.0 GB 
[11/07 08:49:15 visual_prompt]: 	Training 1500/2212. train loss: 1.2837,	0.1561 s / batch. (data: 5.40e-03). ETA=8:51:07, max mem: 5.0 GB 
[11/07 08:49:37 visual_prompt]: 	Training 1600/2212. train loss: 1.1890,	0.1901 s / batch. (data: 5.33e-03). ETA=10:46:33, max mem: 5.0 GB 
[11/07 08:49:59 visual_prompt]: 	Training 1700/2212. train loss: 1.4702,	0.1992 s / batch. (data: 5.36e-03). ETA=11:17:27, max mem: 5.0 GB 
[11/07 08:50:21 visual_prompt]: 	Training 1800/2212. train loss: 0.1903,	0.1401 s / batch. (data: 1.04e-02). ETA=7:55:58, max mem: 5.0 GB 
[11/07 08:50:42 visual_prompt]: 	Training 1900/2212. train loss: 0.4417,	0.2657 s / batch. (data: 1.08e-02). ETA=15:02:33, max mem: 5.0 GB 
[11/07 08:51:04 visual_prompt]: 	Training 2000/2212. train loss: 0.9705,	0.2229 s / batch. (data: 5.85e-03). ETA=12:36:47, max mem: 5.0 GB 
[11/07 08:51:26 visual_prompt]: 	Training 2100/2212. train loss: 0.4146,	0.1897 s / batch. (data: 1.55e-02). ETA=10:43:47, max mem: 5.0 GB 
[11/07 08:51:46 visual_prompt]: 	Training 2200/2212. train loss: 0.9488,	0.0909 s / batch. (data: 1.18e-04). ETA=5:08:22, max mem: 5.0 GB 
[11/07 08:51:47 visual_prompt]: Epoch 8 / 100: avg data time: 3.08e-02, avg batch time: 0.2193, average train loss: 0.7614
[11/07 08:52:10 visual_prompt]: 	Test 100/246. loss: 0.733, 0.1245 s / batch. (data: 2.62e-05)max mem: 4.96204 GB 
[11/07 08:52:30 visual_prompt]: 	Test 200/246. loss: 0.733, 0.0909 s / batch. (data: 2.26e-05)max mem: 4.96204 GB 
[11/07 08:52:38 visual_prompt]: Inference (val):avg data time: 5.77e-05, avg batch time: 0.0663, average loss: 0.6901
[11/07 08:52:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.75	
[11/07 08:52:38 visual_prompt]: Best epoch 8: best metric: -0.690
[11/07 08:52:38 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/07 08:53:03 visual_prompt]: 	Training 100/2212. train loss: 1.0796,	0.1959 s / batch. (data: 2.06e-02). ETA=11:04:00, max mem: 5.0 GB 
[11/07 08:53:27 visual_prompt]: 	Training 200/2212. train loss: 0.7517,	0.1825 s / batch. (data: 2.54e-04). ETA=10:18:21, max mem: 5.0 GB 
[11/07 08:53:50 visual_prompt]: 	Training 300/2212. train loss: 0.7709,	0.1899 s / batch. (data: 7.12e-04). ETA=10:43:17, max mem: 5.0 GB 
[11/07 08:54:12 visual_prompt]: 	Training 400/2212. train loss: 0.6345,	0.2108 s / batch. (data: 5.35e-03). ETA=11:53:41, max mem: 5.0 GB 
[11/07 08:54:35 visual_prompt]: 	Training 500/2212. train loss: 0.9164,	0.1652 s / batch. (data: 1.07e-02). ETA=9:18:48, max mem: 5.0 GB 
[11/07 08:54:57 visual_prompt]: 	Training 600/2212. train loss: 0.3013,	0.1899 s / batch. (data: 8.07e-03). ETA=10:42:15, max mem: 5.0 GB 
[11/07 08:55:19 visual_prompt]: 	Training 700/2212. train loss: 0.3901,	0.1846 s / batch. (data: 1.04e-02). ETA=10:23:56, max mem: 5.0 GB 
[11/07 08:55:40 visual_prompt]: 	Training 800/2212. train loss: 0.7721,	0.2194 s / batch. (data: 2.05e-02). ETA=12:21:15, max mem: 5.0 GB 
[11/07 08:56:01 visual_prompt]: 	Training 900/2212. train loss: 0.7586,	0.2057 s / batch. (data: 5.32e-03). ETA=11:34:27, max mem: 5.0 GB 
[11/07 08:56:23 visual_prompt]: 	Training 1000/2212. train loss: 1.0669,	0.1800 s / batch. (data: 2.43e-02). ETA=10:07:31, max mem: 5.0 GB 
[11/07 08:56:45 visual_prompt]: 	Training 1100/2212. train loss: 0.3990,	0.7775 s / batch. (data: 6.04e-01). ETA=1 day, 19:42:51, max mem: 5.0 GB 
[11/07 08:57:07 visual_prompt]: 	Training 1200/2212. train loss: 0.4463,	0.2076 s / batch. (data: 2.05e-02). ETA=11:40:00, max mem: 5.0 GB 
[11/07 08:57:27 visual_prompt]: 	Training 1300/2212. train loss: 0.2597,	0.2133 s / batch. (data: 1.55e-02). ETA=11:58:51, max mem: 5.0 GB 
[11/07 08:57:48 visual_prompt]: 	Training 1400/2212. train loss: 0.6847,	0.2314 s / batch. (data: 2.56e-02). ETA=12:59:20, max mem: 5.0 GB 
[11/07 08:58:09 visual_prompt]: 	Training 1500/2212. train loss: 1.1391,	0.2287 s / batch. (data: 2.41e-02). ETA=12:50:03, max mem: 5.0 GB 
[11/07 08:58:31 visual_prompt]: 	Training 1600/2212. train loss: 0.5599,	0.1600 s / batch. (data: 2.64e-04). ETA=8:58:22, max mem: 5.0 GB 
[11/07 08:58:53 visual_prompt]: 	Training 1700/2212. train loss: 1.4337,	0.1990 s / batch. (data: 1.55e-02). ETA=11:09:10, max mem: 5.0 GB 
[11/07 08:59:14 visual_prompt]: 	Training 1800/2212. train loss: 0.9363,	0.5757 s / batch. (data: 4.20e-01). ETA=1 day, 8:15:22, max mem: 5.0 GB 
[11/07 08:59:34 visual_prompt]: 	Training 1900/2212. train loss: 0.9229,	0.1409 s / batch. (data: 1.43e-02). ETA=7:53:30, max mem: 5.0 GB 
[11/07 08:59:56 visual_prompt]: 	Training 2000/2212. train loss: 0.5587,	0.1791 s / batch. (data: 5.78e-03). ETA=10:01:35, max mem: 5.0 GB 
[11/07 09:00:18 visual_prompt]: 	Training 2100/2212. train loss: 1.0290,	0.1746 s / batch. (data: 5.37e-03). ETA=9:46:04, max mem: 5.0 GB 
[11/07 09:00:42 visual_prompt]: 	Training 2200/2212. train loss: 0.6088,	0.2081 s / batch. (data: 3.62e-02). ETA=11:38:05, max mem: 5.0 GB 
[11/07 09:00:43 visual_prompt]: Epoch 9 / 100: avg data time: 3.43e-02, avg batch time: 0.2193, average train loss: 0.7356
[11/07 09:01:05 visual_prompt]: 	Test 100/246. loss: 1.048, 0.0524 s / batch. (data: 1.91e-05)max mem: 4.96204 GB 
[11/07 09:01:26 visual_prompt]: 	Test 200/246. loss: 1.048, 0.0933 s / batch. (data: 2.24e-05)max mem: 4.96204 GB 
[11/07 09:01:34 visual_prompt]: Inference (val):avg data time: 5.84e-05, avg batch time: 0.0662, average loss: 0.7098
[11/07 09:01:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.92	
[11/07 09:01:34 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/07 09:01:59 visual_prompt]: 	Training 100/2212. train loss: 0.6151,	0.1972 s / batch. (data: 1.46e-02). ETA=11:01:16, max mem: 5.0 GB 
[11/07 09:02:22 visual_prompt]: 	Training 200/2212. train loss: 0.7149,	0.5964 s / batch. (data: 4.32e-01). ETA=1 day, 9:18:41, max mem: 5.0 GB 
[11/07 09:02:43 visual_prompt]: 	Training 300/2212. train loss: 1.0263,	0.2400 s / batch. (data: 7.43e-04). ETA=13:23:57, max mem: 5.0 GB 
[11/07 09:03:05 visual_prompt]: 	Training 400/2212. train loss: 1.0508,	0.2157 s / batch. (data: 6.93e-04). ETA=12:02:19, max mem: 5.0 GB 
[11/07 09:03:26 visual_prompt]: 	Training 500/2212. train loss: 0.4159,	0.2256 s / batch. (data: 5.33e-03). ETA=12:34:56, max mem: 5.0 GB 
[11/07 09:03:48 visual_prompt]: 	Training 600/2212. train loss: 0.9675,	0.2431 s / batch. (data: 3.08e-02). ETA=13:33:08, max mem: 5.0 GB 
[11/07 09:04:11 visual_prompt]: 	Training 700/2212. train loss: 0.5522,	0.4382 s / batch. (data: 2.33e-01). ETA=1 day, 0:24:56, max mem: 5.0 GB 
[11/07 09:04:34 visual_prompt]: 	Training 800/2212. train loss: 0.4319,	0.2308 s / batch. (data: 1.59e-02). ETA=12:51:18, max mem: 5.0 GB 
[11/07 09:04:55 visual_prompt]: 	Training 900/2212. train loss: 0.1828,	0.9528 s / batch. (data: 8.29e-01). ETA=2 days, 5:02:21, max mem: 5.0 GB 
[11/07 09:05:17 visual_prompt]: 	Training 1000/2212. train loss: 0.4579,	0.1800 s / batch. (data: 5.33e-03). ETA=10:00:53, max mem: 5.0 GB 
[11/07 09:05:39 visual_prompt]: 	Training 1100/2212. train loss: 0.6470,	0.1931 s / batch. (data: 2.39e-04). ETA=10:44:14, max mem: 5.0 GB 
[11/07 09:06:01 visual_prompt]: 	Training 1200/2212. train loss: 1.0791,	0.2024 s / batch. (data: 6.99e-03). ETA=11:14:59, max mem: 5.0 GB 
[11/07 09:06:22 visual_prompt]: 	Training 1300/2212. train loss: 0.9148,	0.1809 s / batch. (data: 2.18e-02). ETA=10:02:51, max mem: 5.0 GB 
[11/07 09:06:44 visual_prompt]: 	Training 1400/2212. train loss: 1.5989,	0.1832 s / batch. (data: 4.19e-02). ETA=10:10:25, max mem: 5.0 GB 
[11/07 09:07:05 visual_prompt]: 	Training 1500/2212. train loss: 0.5250,	0.2081 s / batch. (data: 1.08e-02). ETA=11:33:04, max mem: 5.0 GB 
[11/07 09:07:27 visual_prompt]: 	Training 1600/2212. train loss: 0.6246,	0.1749 s / batch. (data: 2.39e-04). ETA=9:42:06, max mem: 5.0 GB 
[11/07 09:07:49 visual_prompt]: 	Training 1700/2212. train loss: 0.3874,	0.1642 s / batch. (data: 7.13e-03). ETA=9:06:08, max mem: 5.0 GB 
[11/07 09:08:11 visual_prompt]: 	Training 1800/2212. train loss: 1.1034,	0.1803 s / batch. (data: 5.32e-03). ETA=9:59:36, max mem: 5.0 GB 
[11/07 09:08:32 visual_prompt]: 	Training 1900/2212. train loss: 0.7878,	0.1960 s / batch. (data: 5.32e-03). ETA=10:51:17, max mem: 5.0 GB 
[11/07 09:08:54 visual_prompt]: 	Training 2000/2212. train loss: 0.6634,	0.2336 s / batch. (data: 5.35e-03). ETA=12:55:45, max mem: 5.0 GB 
[11/07 09:09:17 visual_prompt]: 	Training 2100/2212. train loss: 0.6341,	0.2429 s / batch. (data: 1.83e-02). ETA=13:26:29, max mem: 5.0 GB 
[11/07 09:09:38 visual_prompt]: 	Training 2200/2212. train loss: 0.5619,	0.1962 s / batch. (data: 3.58e-02). ETA=10:51:03, max mem: 5.0 GB 
[11/07 09:09:39 visual_prompt]: Epoch 10 / 100: avg data time: 3.16e-02, avg batch time: 0.2193, average train loss: 0.7254
[11/07 09:10:01 visual_prompt]: 	Test 100/246. loss: 0.613, 0.0661 s / batch. (data: 3.22e-05)max mem: 4.96204 GB 
[11/07 09:10:22 visual_prompt]: 	Test 200/246. loss: 0.613, 0.0618 s / batch. (data: 2.69e-05)max mem: 4.96204 GB 
[11/07 09:10:30 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.0645, average loss: 0.7048
[11/07 09:10:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.03	
[11/07 09:10:30 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/07 09:10:55 visual_prompt]: 	Training 100/2212. train loss: 0.2875,	0.2127 s / batch. (data: 3.06e-02). ETA=11:45:21, max mem: 5.0 GB 
[11/07 09:11:17 visual_prompt]: 	Training 200/2212. train loss: 0.7569,	0.2521 s / batch. (data: 1.81e-02). ETA=13:55:46, max mem: 5.0 GB 
[11/07 09:11:40 visual_prompt]: 	Training 300/2212. train loss: 0.7046,	0.2084 s / batch. (data: 2.05e-02). ETA=11:30:31, max mem: 5.0 GB 
[11/07 09:12:03 visual_prompt]: 	Training 400/2212. train loss: 0.9635,	0.2313 s / batch. (data: 1.09e-02). ETA=12:45:52, max mem: 5.0 GB 
[11/07 09:12:25 visual_prompt]: 	Training 500/2212. train loss: 0.9682,	0.1756 s / batch. (data: 6.69e-04). ETA=9:41:15, max mem: 5.0 GB 
[11/07 09:12:46 visual_prompt]: 	Training 600/2212. train loss: 0.5917,	0.1040 s / batch. (data: 5.66e-03). ETA=5:44:06, max mem: 5.0 GB 
[11/07 09:13:09 visual_prompt]: 	Training 700/2212. train loss: 1.0594,	0.8783 s / batch. (data: 7.82e-01). ETA=2 days, 0:23:55, max mem: 5.0 GB 
[11/07 09:13:29 visual_prompt]: 	Training 800/2212. train loss: 0.6491,	0.1927 s / batch. (data: 1.52e-02). ETA=10:36:53, max mem: 5.0 GB 
slurmstepd-ctit085: error: *** JOB 246249 ON ctit085 CANCELLED AT 2023-11-07T09:13:42 ***
/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)

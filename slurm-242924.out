/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/23 17:21:39 visual_prompt]: Rank of current process: 0. World size: 1
[10/23 17:21:39 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/23 17:21:39 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/23 17:21:39 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/23 17:21:39 visual_prompt]: Training with config:
[10/23 17:21:39 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr50.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/23 17:21:39 visual_prompt]: Loading training data...
[10/23 17:21:39 visual_prompt]: Constructing mammo-cbis dataset train...
[10/23 17:21:39 visual_prompt]: Loading validation data...
[10/23 17:21:39 visual_prompt]: Constructing mammo-cbis dataset val...
[10/23 17:21:39 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/23 17:21:42 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/23 17:21:42 visual_prompt]: tuned percent:0.534
[10/23 17:21:45 visual_prompt]: Device used for model: 0
[10/23 17:21:45 visual_prompt]: Setting up Evaluator...
[10/23 17:21:45 visual_prompt]: Setting up Trainer...
[10/23 17:21:45 visual_prompt]: 	Setting up the optimizer...
[10/23 17:21:45 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/23 17:27:14 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8836 s / batch. (data: 3.13e-04). ETA=3:23:13, max mem: 7.6 GB 
[10/23 17:29:17 visual_prompt]: Epoch 1 / 100: avg data time: 2.35e+00, avg batch time: 3.2542, average train loss: 1.3980
[10/23 17:30:09 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.4562, average loss: 1.3816
[10/23 17:30:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/23 17:30:09 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[10/23 17:35:14 visual_prompt]: 	Training 100/139. train loss: 6.9658,	0.8910 s / batch. (data: 1.28e-02). ETA=3:22:51, max mem: 7.6 GB 
[10/23 17:37:08 visual_prompt]: Epoch 2 / 100: avg data time: 2.14e+00, avg batch time: 3.0160, average train loss: 15.3217
[10/23 17:37:55 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4569, average loss: 8.1286
[10/23 17:37:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.87	
[10/23 17:37:55 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[10/23 17:43:14 visual_prompt]: 	Training 100/139. train loss: 3.6293,	0.8800 s / batch. (data: 1.20e-02). ETA=3:18:19, max mem: 7.6 GB 
[10/23 17:45:04 visual_prompt]: Epoch 3 / 100: avg data time: 2.21e+00, avg batch time: 3.0865, average train loss: 22.2693
[10/23 17:45:51 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4605, average loss: 2.9344
[10/23 17:45:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.62	
[10/23 17:45:51 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[10/23 17:50:55 visual_prompt]: 	Training 100/139. train loss: 120.3848,	0.8854 s / batch. (data: 2.78e-04). ETA=3:17:29, max mem: 7.6 GB 
[10/23 17:52:50 visual_prompt]: Epoch 4 / 100: avg data time: 2.14e+00, avg batch time: 3.0121, average train loss: 31.6118
[10/23 17:53:37 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4564, average loss: 45.7926
[10/23 17:53:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.10	
[10/23 17:53:37 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[10/23 17:58:50 visual_prompt]: 	Training 100/139. train loss: 16.6890,	11.8377 s / batch. (data: 1.09e+01). ETA=1 day, 19:32:58, max mem: 7.6 GB 
[10/23 18:00:37 visual_prompt]: Epoch 5 / 100: avg data time: 2.14e+00, avg batch time: 3.0177, average train loss: 52.2669
[10/23 18:01:24 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.4581, average loss: 114.1092
[10/23 18:01:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.17	
[10/23 18:01:24 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[10/23 18:06:30 visual_prompt]: 	Training 100/139. train loss: 118.4967,	2.0240 s / batch. (data: 1.16e+00). ETA=7:22:04, max mem: 7.6 GB 
[10/23 18:08:22 visual_prompt]: Epoch 6 / 100: avg data time: 2.14e+00, avg batch time: 3.0080, average train loss: 79.6528
[10/23 18:09:09 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4580, average loss: 4.3124
[10/23 18:09:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 45.13	
[10/23 18:09:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[10/23 18:14:23 visual_prompt]: 	Training 100/139. train loss: 24.4904,	0.8725 s / batch. (data: 7.96e-03). ETA=3:08:32, max mem: 7.6 GB 
[10/23 18:16:14 visual_prompt]: Epoch 7 / 100: avg data time: 2.18e+00, avg batch time: 3.0527, average train loss: 70.9733
[10/23 18:17:01 visual_prompt]: Inference (val):avg data time: 2.03e-03, avg batch time: 0.4658, average loss: 26.0986
[10/23 18:17:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 52.26	
[10/23 18:17:01 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[10/23 18:22:12 visual_prompt]: 	Training 100/139. train loss: 249.0732,	0.8756 s / batch. (data: 3.36e-04). ETA=3:07:10, max mem: 7.6 GB 
[10/23 18:24:03 visual_prompt]: Epoch 8 / 100: avg data time: 2.16e+00, avg batch time: 3.0356, average train loss: 105.7476
[10/23 18:24:50 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4581, average loss: 39.6652
[10/23 18:24:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.14	
[10/23 18:24:50 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[10/23 18:30:00 visual_prompt]: 	Training 100/139. train loss: 285.8554,	0.8800 s / batch. (data: 3.09e-04). ETA=3:06:05, max mem: 7.6 GB 
[10/23 18:31:48 visual_prompt]: Epoch 9 / 100: avg data time: 2.14e+00, avg batch time: 3.0093, average train loss: 123.3656
[10/23 18:32:35 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.4608, average loss: 86.2684
[10/23 18:32:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.14	
[10/23 18:32:35 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[10/23 18:37:45 visual_prompt]: 	Training 100/139. train loss: 291.2468,	0.8553 s / batch. (data: 2.77e-04). ETA=2:58:53, max mem: 7.6 GB 
[10/23 18:39:34 visual_prompt]: Epoch 10 / 100: avg data time: 2.14e+00, avg batch time: 3.0137, average train loss: 127.8158
[10/23 18:40:21 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4576, average loss: 25.4786
[10/23 18:40:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.55	
[10/23 18:40:21 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[10/23 18:45:41 visual_prompt]: 	Training 100/139. train loss: 278.8935,	0.8880 s / batch. (data: 2.99e-04). ETA=3:03:39, max mem: 7.6 GB 
[10/23 18:47:36 visual_prompt]: Epoch 11 / 100: avg data time: 2.25e+00, avg batch time: 3.1247, average train loss: 143.9970
[10/23 18:48:23 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.4580, average loss: 143.8029
[10/23 18:48:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.20	
[10/23 18:48:23 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[10/23 18:53:31 visual_prompt]: 	Training 100/139. train loss: 66.8981,	0.8998 s / batch. (data: 7.77e-03). ETA=3:04:01, max mem: 7.6 GB 
[10/23 18:55:21 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 3.0069, average train loss: 131.3576
[10/23 18:56:08 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4594, average loss: 8.3210
[10/23 18:56:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.36	
[10/23 18:56:08 visual_prompt]: Best epoch 12: best metric: -8.321
[10/23 18:56:08 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[10/23 19:01:18 visual_prompt]: 	Training 100/139. train loss: 34.1035,	0.9048 s / batch. (data: 5.48e-03). ETA=3:02:57, max mem: 7.6 GB 
[10/23 19:03:07 visual_prompt]: Epoch 13 / 100: avg data time: 2.14e+00, avg batch time: 3.0126, average train loss: 145.8790
[10/23 19:03:54 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4576, average loss: 432.5201
[10/23 19:03:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.02	
[10/23 19:03:54 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[10/23 19:09:08 visual_prompt]: 	Training 100/139. train loss: 103.4728,	0.9040 s / batch. (data: 7.32e-04). ETA=3:00:41, max mem: 7.6 GB 
[10/23 19:10:55 visual_prompt]: Epoch 14 / 100: avg data time: 2.16e+00, avg batch time: 3.0316, average train loss: 124.5345
[10/23 19:11:42 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4571, average loss: 112.5647
[10/23 19:11:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.65	
[10/23 19:11:42 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[10/23 19:16:57 visual_prompt]: 	Training 100/139. train loss: 166.5592,	9.5428 s / batch. (data: 8.67e+00). ETA=1 day, 7:25:20, max mem: 7.6 GB 
[10/23 19:18:42 visual_prompt]: Epoch 15 / 100: avg data time: 2.15e+00, avg batch time: 3.0173, average train loss: 167.1316
[10/23 19:19:29 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4608, average loss: 379.3663
[10/23 19:19:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.04	
[10/23 19:19:29 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[10/23 19:24:36 visual_prompt]: 	Training 100/139. train loss: 171.4099,	0.8880 s / batch. (data: 7.96e-03). ETA=2:53:22, max mem: 7.6 GB 
[10/23 19:26:26 visual_prompt]: Epoch 16 / 100: avg data time: 2.13e+00, avg batch time: 2.9987, average train loss: 159.7660
[10/23 19:27:26 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4628, average loss: 295.0053
[10/23 19:27:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.28	
[10/23 19:27:26 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[10/23 19:32:35 visual_prompt]: 	Training 100/139. train loss: 238.4927,	4.4920 s / batch. (data: 3.62e+00). ETA=14:26:39, max mem: 7.6 GB 
[10/23 19:34:26 visual_prompt]: Epoch 17 / 100: avg data time: 2.15e+00, avg batch time: 3.0207, average train loss: 170.5435
[10/23 19:35:13 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.4606, average loss: 180.4650
[10/23 19:35:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.34	
[10/23 19:35:13 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[10/23 19:40:23 visual_prompt]: 	Training 100/139. train loss: 57.3006,	2.2106 s / batch. (data: 1.29e+00). ETA=7:01:22, max mem: 7.6 GB 
[10/23 19:42:12 visual_prompt]: Epoch 18 / 100: avg data time: 2.15e+00, avg batch time: 3.0191, average train loss: 182.2928
[10/23 19:42:59 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.4593, average loss: 296.4349
[10/23 19:42:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.69	
[10/23 19:42:59 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[10/23 19:48:11 visual_prompt]: 	Training 100/139. train loss: 8.9000,	8.4760 s / batch. (data: 7.58e+00). ETA=1 day, 2:36:01, max mem: 7.6 GB 
[10/23 19:49:57 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0012, average train loss: 164.8395
[10/23 19:50:44 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.4562, average loss: 25.0131
[10/23 19:50:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.27	
[10/23 19:50:44 visual_prompt]: Stopping early.
[10/23 19:50:44 visual_prompt]: Rank of current process: 0. World size: 1
[10/23 19:50:44 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/23 19:50:44 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/23 19:50:44 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/23 19:50:44 visual_prompt]: Training with config:
[10/23 19:50:44 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr50.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/23 19:50:44 visual_prompt]: Loading training data...
[10/23 19:50:44 visual_prompt]: Constructing mammo-cbis dataset train...
[10/23 19:50:44 visual_prompt]: Loading validation data...
[10/23 19:50:44 visual_prompt]: Constructing mammo-cbis dataset val...
[10/23 19:50:44 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/23 19:50:47 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/23 19:50:47 visual_prompt]: tuned percent:0.534
[10/23 19:50:47 visual_prompt]: Device used for model: 0
[10/23 19:50:47 visual_prompt]: Setting up Evaluator...
[10/23 19:50:47 visual_prompt]: Setting up Trainer...
[10/23 19:50:47 visual_prompt]: 	Setting up the optimizer...
[10/23 19:50:47 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/23 19:55:55 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.9120 s / batch. (data: 1.09e-02). ETA=3:29:44, max mem: 7.6 GB 
[10/23 19:57:44 visual_prompt]: Epoch 1 / 100: avg data time: 2.13e+00, avg batch time: 3.0015, average train loss: 1.3980
[10/23 19:58:32 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.4583, average loss: 1.3816
[10/23 19:58:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/23 19:58:32 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[10/23 20:03:41 visual_prompt]: 	Training 100/139. train loss: 0.9255,	0.9098 s / batch. (data: 2.18e-02). ETA=3:27:08, max mem: 7.6 GB 
[10/23 20:05:33 visual_prompt]: Epoch 2 / 100: avg data time: 2.16e+00, avg batch time: 3.0286, average train loss: 15.0200
[10/23 20:06:20 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4651, average loss: 16.0142
[10/23 20:06:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.00	
[10/23 20:06:20 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[10/23 20:11:24 visual_prompt]: 	Training 100/139. train loss: 9.3527,	0.8824 s / batch. (data: 1.59e-02). ETA=3:18:51, max mem: 7.6 GB 
[10/23 20:13:17 visual_prompt]: Epoch 3 / 100: avg data time: 2.13e+00, avg batch time: 2.9980, average train loss: 18.9681
[10/23 20:14:04 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4601, average loss: 54.9080
[10/23 20:14:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.11	
[10/23 20:14:04 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[10/23 20:19:07 visual_prompt]: 	Training 100/139. train loss: 2.2068,	0.8703 s / batch. (data: 1.20e-02). ETA=3:14:07, max mem: 7.6 GB 
[10/23 20:21:01 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 2.9994, average train loss: 35.2718
[10/23 20:21:48 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.4596, average loss: 72.9777
[10/23 20:21:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.23	
[10/23 20:21:48 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[10/23 20:26:58 visual_prompt]: 	Training 100/139. train loss: 7.0184,	12.1465 s / batch. (data: 1.12e+01). ETA=1 day, 20:41:08, max mem: 7.6 GB 
[10/23 20:28:46 visual_prompt]: Epoch 5 / 100: avg data time: 2.14e+00, avg batch time: 3.0051, average train loss: 48.7799
[10/23 20:29:33 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4585, average loss: 6.3174
[10/23 20:29:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.54	
[10/23 20:29:33 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[10/23 20:34:39 visual_prompt]: 	Training 100/139. train loss: 80.2957,	0.8800 s / batch. (data: 2.82e-04). ETA=3:12:12, max mem: 7.6 GB 
[10/23 20:36:30 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 3.0008, average train loss: 61.2381
[10/23 20:37:18 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4578, average loss: 57.7756
[10/23 20:37:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.61	
[10/23 20:37:18 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[10/23 20:42:25 visual_prompt]: 	Training 100/139. train loss: 43.2675,	0.8751 s / batch. (data: 1.04e-02). ETA=3:09:06, max mem: 7.6 GB 
[10/23 20:44:14 visual_prompt]: Epoch 7 / 100: avg data time: 2.13e+00, avg batch time: 2.9954, average train loss: 82.0096
[10/23 20:45:02 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4626, average loss: 72.9051
[10/23 20:45:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.19	
[10/23 20:45:02 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[10/23 20:50:09 visual_prompt]: 	Training 100/139. train loss: 23.5145,	0.8643 s / batch. (data: 5.41e-03). ETA=3:04:45, max mem: 7.6 GB 
[10/23 20:52:00 visual_prompt]: Epoch 8 / 100: avg data time: 2.14e+00, avg batch time: 3.0071, average train loss: 95.7963
[10/23 20:52:47 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4605, average loss: 52.4908
[10/23 20:52:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.74	
[10/23 20:52:47 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[10/23 20:57:54 visual_prompt]: 	Training 100/139. train loss: 181.6529,	0.8948 s / batch. (data: 2.68e-02). ETA=3:09:13, max mem: 7.6 GB 
[10/23 20:59:43 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9915, average train loss: 90.6534
[10/23 21:00:30 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4576, average loss: 130.6086
[10/23 21:00:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.54	
[10/23 21:00:30 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[10/23 21:05:42 visual_prompt]: 	Training 100/139. train loss: 49.8937,	0.8641 s / batch. (data: 2.81e-04). ETA=3:00:43, max mem: 7.6 GB 
[10/23 21:07:32 visual_prompt]: Epoch 10 / 100: avg data time: 2.17e+00, avg batch time: 3.0378, average train loss: 109.3111
[10/23 21:08:20 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4586, average loss: 102.0381
[10/23 21:08:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.95	
[10/23 21:08:20 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[10/23 21:13:31 visual_prompt]: 	Training 100/139. train loss: 25.8953,	0.8720 s / batch. (data: 2.72e-04). ETA=3:00:21, max mem: 7.6 GB 
[10/23 21:15:21 visual_prompt]: Epoch 11 / 100: avg data time: 2.17e+00, avg batch time: 3.0326, average train loss: 114.7841
[10/23 21:16:08 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4629, average loss: 42.0390
[10/23 21:16:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.75	
[10/23 21:16:08 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[10/23 21:21:14 visual_prompt]: 	Training 100/139. train loss: 185.8117,	0.8800 s / batch. (data: 7.98e-03). ETA=2:59:58, max mem: 7.6 GB 
[10/23 21:23:05 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 2.9982, average train loss: 137.5071
[10/23 21:23:52 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.4595, average loss: 151.7092
[10/23 21:23:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.55	
[10/23 21:23:52 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[10/23 21:29:00 visual_prompt]: 	Training 100/139. train loss: 34.8932,	0.8670 s / batch. (data: 2.88e-04). ETA=2:55:18, max mem: 7.6 GB 
[10/23 21:30:50 visual_prompt]: Epoch 13 / 100: avg data time: 2.14e+00, avg batch time: 3.0018, average train loss: 134.4728
[10/23 21:31:37 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4602, average loss: 364.0551
[10/23 21:31:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.99	
[10/23 21:31:37 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[10/23 21:36:49 visual_prompt]: 	Training 100/139. train loss: 16.9787,	0.8955 s / batch. (data: 2.70e-03). ETA=2:58:59, max mem: 7.6 GB 
[10/23 21:38:36 visual_prompt]: Epoch 14 / 100: avg data time: 2.14e+00, avg batch time: 3.0130, average train loss: 140.7186
[10/23 21:39:23 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4628, average loss: 19.3022
[10/23 21:39:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.32	
[10/23 21:39:23 visual_prompt]: Best epoch 14: best metric: -19.302
[10/23 21:39:23 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[10/23 21:44:36 visual_prompt]: 	Training 100/139. train loss: 142.9858,	9.4200 s / batch. (data: 8.54e+00). ETA=1 day, 7:01:04, max mem: 7.6 GB 
[10/23 21:46:22 visual_prompt]: Epoch 15 / 100: avg data time: 2.14e+00, avg batch time: 3.0100, average train loss: 132.9112
[10/23 21:47:09 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.4605, average loss: 30.5107
[10/23 21:47:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.39	
[10/23 21:47:09 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[10/23 21:52:17 visual_prompt]: 	Training 100/139. train loss: 198.7462,	0.8960 s / batch. (data: 2.97e-04). ETA=2:54:56, max mem: 7.6 GB 
[10/23 21:54:07 visual_prompt]: Epoch 16 / 100: avg data time: 2.14e+00, avg batch time: 3.0045, average train loss: 150.4824
[10/23 21:54:54 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4598, average loss: 13.1493
[10/23 21:54:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.24	
[10/23 21:54:54 visual_prompt]: Best epoch 16: best metric: -13.149
[10/23 21:54:54 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[10/23 22:00:03 visual_prompt]: 	Training 100/139. train loss: 104.4997,	4.6680 s / batch. (data: 3.79e+00). ETA=15:00:36, max mem: 7.6 GB 
[10/23 22:01:52 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 3.0000, average train loss: 116.1914
[10/23 22:02:39 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4605, average loss: 137.0198
[10/23 22:02:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.24	
[10/23 22:02:39 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[10/23 22:07:55 visual_prompt]: 	Training 100/139. train loss: 225.5371,	5.2008 s / batch. (data: 4.33e+00). ETA=16:31:22, max mem: 7.6 GB 
[10/23 22:09:43 visual_prompt]: Epoch 18 / 100: avg data time: 2.19e+00, avg batch time: 3.0534, average train loss: 110.3439
[10/23 22:10:32 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4563, average loss: 2.1420
[10/23 22:10:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.27	
[10/23 22:10:32 visual_prompt]: Best epoch 18: best metric: -2.142
[10/23 22:10:32 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[10/23 22:15:45 visual_prompt]: 	Training 100/139. train loss: 309.2334,	10.7240 s / batch. (data: 9.83e+00). ETA=1 day, 9:39:19, max mem: 7.6 GB 
[10/23 22:17:30 visual_prompt]: Epoch 19 / 100: avg data time: 2.14e+00, avg batch time: 3.0090, average train loss: 101.8692
[10/23 22:18:17 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4615, average loss: 172.4784
[10/23 22:18:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.63	
[10/23 22:18:17 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[10/23 22:23:23 visual_prompt]: 	Training 100/139. train loss: 49.6417,	0.8640 s / batch. (data: 2.80e-04). ETA=2:40:41, max mem: 7.6 GB 
[10/23 22:25:14 visual_prompt]: Epoch 20 / 100: avg data time: 2.13e+00, avg batch time: 3.0010, average train loss: 135.0022
[10/23 22:26:02 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.4604, average loss: 106.0303
[10/23 22:26:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.08	
[10/23 22:26:02 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[10/23 22:31:10 visual_prompt]: 	Training 100/139. train loss: 54.2624,	0.8680 s / batch. (data: 2.93e-04). ETA=2:39:25, max mem: 7.6 GB 
[10/23 22:32:59 visual_prompt]: Epoch 21 / 100: avg data time: 2.13e+00, avg batch time: 2.9977, average train loss: 136.5498
[10/23 22:33:46 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.4591, average loss: 51.2642
[10/23 22:33:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.44	
[10/23 22:33:46 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[10/23 22:38:49 visual_prompt]: 	Training 100/139. train loss: 111.3928,	0.8706 s / batch. (data: 2.96e-04). ETA=2:37:52, max mem: 7.6 GB 
[10/23 22:40:42 visual_prompt]: Epoch 22 / 100: avg data time: 2.13e+00, avg batch time: 2.9962, average train loss: 113.2468
[10/23 22:41:30 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4591, average loss: 136.4668
[10/23 22:41:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 37.10	
[10/23 22:41:30 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[10/23 22:46:36 visual_prompt]: 	Training 100/139. train loss: 20.6130,	0.8860 s / batch. (data: 9.94e-03). ETA=2:38:37, max mem: 7.6 GB 
[10/23 22:48:28 visual_prompt]: Epoch 23 / 100: avg data time: 2.14e+00, avg batch time: 3.0048, average train loss: 120.0155
[10/23 22:49:15 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4598, average loss: 197.9776
[10/23 22:49:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.22	
[10/23 22:49:15 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[10/23 22:54:27 visual_prompt]: 	Training 100/139. train loss: 152.2474,	0.8640 s / batch. (data: 2.69e-04). ETA=2:32:41, max mem: 7.6 GB 
[10/23 22:56:14 visual_prompt]: Epoch 24 / 100: avg data time: 2.15e+00, avg batch time: 3.0148, average train loss: 123.7812
[10/23 22:57:03 visual_prompt]: Inference (val):avg data time: 4.85e-04, avg batch time: 0.4617, average loss: 89.9725
[10/23 22:57:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.51	
[10/23 22:57:03 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[10/23 23:02:16 visual_prompt]: 	Training 100/139. train loss: 4.7496,	0.8680 s / batch. (data: 3.03e-04). ETA=2:31:22, max mem: 7.6 GB 
[10/23 23:04:06 visual_prompt]: Epoch 25 / 100: avg data time: 2.18e+00, avg batch time: 3.0445, average train loss: 88.2113
[10/23 23:04:54 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4611, average loss: 97.2868
[10/23 23:04:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.58	
[10/23 23:04:54 visual_prompt]: Stopping early.
[10/23 23:04:54 visual_prompt]: Rank of current process: 0. World size: 1
[10/23 23:04:54 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/23 23:04:54 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/23 23:04:54 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/23 23:04:54 visual_prompt]: Training with config:
[10/23 23:04:54 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr50.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/23 23:04:54 visual_prompt]: Loading training data...
[10/23 23:04:54 visual_prompt]: Constructing mammo-cbis dataset train...
[10/23 23:04:54 visual_prompt]: Loading validation data...
[10/23 23:04:54 visual_prompt]: Constructing mammo-cbis dataset val...
[10/23 23:04:54 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/23 23:04:57 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/23 23:04:57 visual_prompt]: tuned percent:0.534
[10/23 23:04:57 visual_prompt]: Device used for model: 0
[10/23 23:04:57 visual_prompt]: Setting up Evaluator...
[10/23 23:04:57 visual_prompt]: Setting up Trainer...
[10/23 23:04:57 visual_prompt]: 	Setting up the optimizer...
[10/23 23:04:57 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/23 23:10:04 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8906 s / batch. (data: 1.68e-02). ETA=3:24:50, max mem: 7.6 GB 
[10/23 23:11:53 visual_prompt]: Epoch 1 / 100: avg data time: 2.13e+00, avg batch time: 2.9956, average train loss: 1.3980
[10/23 23:12:41 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.4584, average loss: 1.3816
[10/23 23:12:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/23 23:12:41 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[10/23 23:17:46 visual_prompt]: 	Training 100/139. train loss: 7.3918,	0.9045 s / batch. (data: 1.55e-02). ETA=3:25:56, max mem: 7.6 GB 
[10/23 23:19:45 visual_prompt]: Epoch 2 / 100: avg data time: 2.18e+00, avg batch time: 3.0524, average train loss: 22.3836
[10/23 23:20:42 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4578, average loss: 30.4611
[10/23 23:20:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.05	
[10/23 23:20:42 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[10/23 23:26:40 visual_prompt]: 	Training 100/139. train loss: 4.5824,	0.8760 s / batch. (data: 2.98e-04). ETA=3:17:25, max mem: 7.6 GB 
[10/23 23:28:52 visual_prompt]: Epoch 3 / 100: avg data time: 2.66e+00, avg batch time: 3.5211, average train loss: 25.7036
[10/23 23:29:39 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4563, average loss: 18.2279
[10/23 23:29:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.51	
[10/23 23:29:39 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[10/23 23:34:41 visual_prompt]: 	Training 100/139. train loss: 23.0020,	0.8797 s / batch. (data: 1.56e-02). ETA=3:16:12, max mem: 7.6 GB 
[10/23 23:36:37 visual_prompt]: Epoch 4 / 100: avg data time: 2.14e+00, avg batch time: 3.0072, average train loss: 31.5612
[10/23 23:37:24 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4573, average loss: 2.0338
[10/23 23:37:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.59	
[10/23 23:37:24 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[10/23 23:42:37 visual_prompt]: 	Training 100/139. train loss: 52.6386,	12.2560 s / batch. (data: 1.14e+01). ETA=1 day, 21:05:18, max mem: 7.6 GB 
[10/23 23:44:25 visual_prompt]: Epoch 5 / 100: avg data time: 2.16e+00, avg batch time: 3.0258, average train loss: 49.2920
[10/23 23:45:12 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.4604, average loss: 24.9531
[10/23 23:45:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.26	
[10/23 23:45:12 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[10/23 23:50:22 visual_prompt]: 	Training 100/139. train loss: 7.8614,	1.0504 s / batch. (data: 1.73e-01). ETA=3:49:25, max mem: 7.6 GB 
[10/23 23:52:19 visual_prompt]: Epoch 6 / 100: avg data time: 2.20e+00, avg batch time: 3.0706, average train loss: 60.8620
[10/23 23:53:07 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.4592, average loss: 50.5547
[10/23 23:53:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.48	
[10/23 23:53:07 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[10/23 23:58:14 visual_prompt]: 	Training 100/139. train loss: 47.4052,	0.9073 s / batch. (data: 2.32e-02). ETA=3:16:03, max mem: 7.6 GB 
[10/24 00:00:04 visual_prompt]: Epoch 7 / 100: avg data time: 2.14e+00, avg batch time: 3.0056, average train loss: 44.3429
[10/24 00:00:52 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4577, average loss: 21.7749
[10/24 00:00:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.31	
[10/24 00:00:52 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[10/24 00:06:25 visual_prompt]: 	Training 100/139. train loss: 175.5486,	0.8570 s / batch. (data: 2.88e-04). ETA=3:03:13, max mem: 7.6 GB 
[10/24 00:08:19 visual_prompt]: Epoch 8 / 100: avg data time: 2.35e+00, avg batch time: 3.2113, average train loss: 70.3594
[10/24 00:09:06 visual_prompt]: Inference (val):avg data time: 7.23e-04, avg batch time: 0.4604, average loss: 64.1171
[10/24 00:09:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.11	
[10/24 00:09:06 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[10/24 00:14:18 visual_prompt]: 	Training 100/139. train loss: 66.6982,	0.8744 s / batch. (data: 2.88e-04). ETA=3:04:54, max mem: 7.6 GB 
[10/24 00:16:06 visual_prompt]: Epoch 9 / 100: avg data time: 2.15e+00, avg batch time: 3.0157, average train loss: 94.5196
[10/24 00:16:53 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.4557, average loss: 18.3106
[10/24 00:16:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.04	
[10/24 00:16:53 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[10/24 00:21:59 visual_prompt]: 	Training 100/139. train loss: 250.6877,	0.8760 s / batch. (data: 2.99e-04). ETA=3:03:12, max mem: 7.6 GB 
[10/24 00:23:50 visual_prompt]: Epoch 10 / 100: avg data time: 2.14e+00, avg batch time: 3.0009, average train loss: 146.9457
[10/24 00:24:38 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.4588, average loss: 125.6813
[10/24 00:24:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.21	
[10/24 00:24:38 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[10/24 00:29:46 visual_prompt]: 	Training 100/139. train loss: 38.3460,	0.8720 s / batch. (data: 2.80e-04). ETA=3:00:21, max mem: 7.6 GB 
[10/24 00:31:37 visual_prompt]: Epoch 11 / 100: avg data time: 2.15e+00, avg batch time: 3.0148, average train loss: 145.2341
[10/24 00:32:25 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4548, average loss: 29.0576
[10/24 00:32:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.05	
[10/24 00:32:25 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[10/24 00:37:57 visual_prompt]: 	Training 100/139. train loss: 49.6464,	0.8571 s / batch. (data: 3.01e-04). ETA=2:55:17, max mem: 7.6 GB 
[10/24 00:40:03 visual_prompt]: Epoch 12 / 100: avg data time: 2.43e+00, avg batch time: 3.2913, average train loss: 88.1033
[10/24 00:40:50 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4535, average loss: 63.3996
[10/24 00:40:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.02	
[10/24 00:40:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[10/24 00:46:04 visual_prompt]: 	Training 100/139. train loss: 158.4286,	0.9080 s / batch. (data: 7.96e-03). ETA=3:03:35, max mem: 7.6 GB 
[10/24 00:47:53 visual_prompt]: Epoch 13 / 100: avg data time: 2.17e+00, avg batch time: 3.0390, average train loss: 102.1398
[10/24 00:48:40 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.4573, average loss: 157.5631
[10/24 00:48:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.03	
[10/24 00:48:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[10/24 00:53:51 visual_prompt]: 	Training 100/139. train loss: 38.8410,	0.8760 s / batch. (data: 7.78e-04). ETA=2:55:05, max mem: 7.6 GB 
[10/24 00:55:39 visual_prompt]: Epoch 14 / 100: avg data time: 2.15e+00, avg batch time: 3.0122, average train loss: 84.2828
[10/24 00:56:26 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4567, average loss: 218.8557
[10/24 00:56:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.12	
[10/24 00:56:26 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[10/24 01:01:40 visual_prompt]: 	Training 100/139. train loss: 110.3843,	9.5142 s / batch. (data: 8.66e+00). ETA=1 day, 7:19:41, max mem: 7.6 GB 
[10/24 01:03:26 visual_prompt]: Epoch 15 / 100: avg data time: 2.15e+00, avg batch time: 3.0205, average train loss: 132.7606
[10/24 01:04:14 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4571, average loss: 109.1536
[10/24 01:04:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.14	
[10/24 01:04:14 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[10/24 01:09:30 visual_prompt]: 	Training 100/139. train loss: 71.1932,	0.8850 s / batch. (data: 1.04e-02). ETA=2:52:47, max mem: 7.6 GB 
[10/24 01:11:21 visual_prompt]: Epoch 16 / 100: avg data time: 2.21e+00, avg batch time: 3.0713, average train loss: 80.7711
[10/24 01:12:08 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4607, average loss: 264.2005
[10/24 01:12:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.19	
[10/24 01:12:08 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[10/24 01:17:15 visual_prompt]: 	Training 100/139. train loss: 78.6142,	0.8720 s / batch. (data: 3.14e-04). ETA=2:48:14, max mem: 7.6 GB 
[10/24 01:19:07 visual_prompt]: Epoch 17 / 100: avg data time: 2.14e+00, avg batch time: 3.0108, average train loss: 114.3806
[10/24 01:19:55 visual_prompt]: Inference (val):avg data time: 7.32e-04, avg batch time: 0.4563, average loss: 583.6720
[10/24 01:19:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.16	
[10/24 01:19:55 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[10/24 01:25:03 visual_prompt]: 	Training 100/139. train loss: 169.5276,	0.8740 s / batch. (data: 1.03e-02). ETA=2:46:35, max mem: 7.6 GB 
[10/24 01:26:53 visual_prompt]: Epoch 18 / 100: avg data time: 2.14e+00, avg batch time: 3.0044, average train loss: 139.4621
[10/24 01:27:40 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.4557, average loss: 293.0351
[10/24 01:27:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.04	
[10/24 01:27:40 visual_prompt]: Stopping early.
[10/24 01:27:40 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 01:27:40 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 01:27:40 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 01:27:40 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 01:27:40 visual_prompt]: Training with config:
[10/24 01:27:40 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr50.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 01:27:40 visual_prompt]: Loading training data...
[10/24 01:27:40 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 01:27:40 visual_prompt]: Loading validation data...
[10/24 01:27:40 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 01:27:40 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/24 01:27:49 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/24 01:27:49 visual_prompt]: tuned percent:0.534
[10/24 01:27:49 visual_prompt]: Device used for model: 0
[10/24 01:27:49 visual_prompt]: Setting up Evaluator...
[10/24 01:27:49 visual_prompt]: Setting up Trainer...
[10/24 01:27:49 visual_prompt]: 	Setting up the optimizer...
[10/24 01:27:49 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 01:32:56 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8602 s / batch. (data: 5.41e-03). ETA=3:17:50, max mem: 7.6 GB 
[10/24 01:34:46 visual_prompt]: Epoch 1 / 100: avg data time: 2.14e+00, avg batch time: 3.0026, average train loss: 1.3980
[10/24 01:35:34 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.4576, average loss: 1.3816
[10/24 01:35:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/24 01:35:34 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[10/24 01:40:39 visual_prompt]: 	Training 100/139. train loss: 3.3088,	0.8881 s / batch. (data: 2.43e-04). ETA=3:22:11, max mem: 7.6 GB 
[10/24 01:42:31 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 3.0004, average train loss: 14.6969
[10/24 01:43:18 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.4605, average loss: 47.3309
[10/24 01:43:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.35	
[10/24 01:43:18 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[10/24 01:48:30 visual_prompt]: 	Training 100/139. train loss: 13.0796,	0.8920 s / batch. (data: 2.86e-04). ETA=3:21:01, max mem: 7.6 GB 
[10/24 01:50:20 visual_prompt]: Epoch 3 / 100: avg data time: 2.17e+00, avg batch time: 3.0369, average train loss: 20.7344
[10/24 01:51:08 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4610, average loss: 7.3558
[10/24 01:51:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.16	
[10/24 01:51:08 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[10/24 01:56:11 visual_prompt]: 	Training 100/139. train loss: 11.4683,	0.8622 s / batch. (data: 4.38e-04). ETA=3:12:18, max mem: 7.6 GB 
[10/24 01:58:06 visual_prompt]: Epoch 4 / 100: avg data time: 2.14e+00, avg batch time: 3.0077, average train loss: 33.0877
[10/24 01:58:54 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4557, average loss: 3.5603
[10/24 01:58:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.14	
[10/24 01:58:54 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[10/24 02:04:04 visual_prompt]: 	Training 100/139. train loss: 132.5649,	11.7614 s / batch. (data: 1.09e+01). ETA=1 day, 19:16:08, max mem: 7.6 GB 
[10/24 02:05:52 visual_prompt]: Epoch 5 / 100: avg data time: 2.15e+00, avg batch time: 3.0095, average train loss: 44.5079
[10/24 02:06:39 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4584, average loss: 68.5540
[10/24 02:06:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.51	
[10/24 02:06:39 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[10/24 02:11:47 visual_prompt]: 	Training 100/139. train loss: 68.6510,	0.8539 s / batch. (data: 2.64e-04). ETA=3:06:30, max mem: 7.6 GB 
[10/24 02:13:38 visual_prompt]: Epoch 6 / 100: avg data time: 2.15e+00, avg batch time: 3.0074, average train loss: 59.2751
[10/24 02:14:25 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4588, average loss: 46.3667
[10/24 02:14:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.28	
[10/24 02:14:25 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[10/24 02:19:34 visual_prompt]: 	Training 100/139. train loss: 148.1025,	0.8630 s / batch. (data: 7.95e-03). ETA=3:06:29, max mem: 7.6 GB 
[10/24 02:21:23 visual_prompt]: Epoch 7 / 100: avg data time: 2.15e+00, avg batch time: 3.0091, average train loss: 65.5394
[10/24 02:22:10 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4607, average loss: 198.7904
[10/24 02:22:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.12	
[10/24 02:22:11 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[10/24 02:27:18 visual_prompt]: 	Training 100/139. train loss: 13.5356,	0.8702 s / batch. (data: 3.13e-04). ETA=3:06:01, max mem: 7.6 GB 
[10/24 02:29:09 visual_prompt]: Epoch 8 / 100: avg data time: 2.15e+00, avg batch time: 3.0079, average train loss: 67.6883
[10/24 02:29:56 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.4573, average loss: 83.3239
[10/24 02:29:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.71	
[10/24 02:29:56 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[10/24 02:35:05 visual_prompt]: 	Training 100/139. train loss: 24.9465,	0.8969 s / batch. (data: 8.77e-03). ETA=3:09:39, max mem: 7.6 GB 
[10/24 02:36:53 visual_prompt]: Epoch 9 / 100: avg data time: 2.14e+00, avg batch time: 2.9974, average train loss: 45.3302
[10/24 02:37:40 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.4551, average loss: 48.8535
[10/24 02:37:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.96	
[10/24 02:37:40 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[10/24 02:42:47 visual_prompt]: 	Training 100/139. train loss: 70.8550,	0.8536 s / batch. (data: 3.04e-04). ETA=2:58:32, max mem: 7.6 GB 
[10/24 02:44:37 visual_prompt]: Epoch 10 / 100: avg data time: 2.14e+00, avg batch time: 3.0002, average train loss: 63.7315
[10/24 02:45:24 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4591, average loss: 44.4111
[10/24 02:45:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.36	
[10/24 02:45:24 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[10/24 02:50:31 visual_prompt]: 	Training 100/139. train loss: 46.0173,	0.8640 s / batch. (data: 7.95e-03). ETA=2:58:42, max mem: 7.6 GB 
[10/24 02:52:23 visual_prompt]: Epoch 11 / 100: avg data time: 2.15e+00, avg batch time: 3.0077, average train loss: 78.8664
[10/24 02:53:10 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4580, average loss: 96.2849
[10/24 02:53:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.14	
[10/24 02:53:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[10/24 02:58:16 visual_prompt]: 	Training 100/139. train loss: 102.4414,	0.8685 s / batch. (data: 2.91e-04). ETA=2:57:37, max mem: 7.6 GB 
[10/24 03:00:07 visual_prompt]: Epoch 12 / 100: avg data time: 2.14e+00, avg batch time: 2.9999, average train loss: 120.3756
[10/24 03:00:54 visual_prompt]: Inference (val):avg data time: 4.98e-04, avg batch time: 0.4594, average loss: 4.7352
[10/24 03:00:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.15	
[10/24 03:00:54 visual_prompt]: Best epoch 12: best metric: -4.735
[10/24 03:00:54 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[10/24 03:06:03 visual_prompt]: 	Training 100/139. train loss: 148.3417,	0.8923 s / batch. (data: 1.64e-02). ETA=3:00:24, max mem: 7.6 GB 
[10/24 03:07:52 visual_prompt]: Epoch 13 / 100: avg data time: 2.14e+00, avg batch time: 3.0077, average train loss: 74.2423
[10/24 03:08:40 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.4586, average loss: 104.4458
[10/24 03:08:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.27	
[10/24 03:08:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[10/24 03:13:50 visual_prompt]: 	Training 100/139. train loss: 31.5267,	0.8744 s / batch. (data: 8.03e-04). ETA=2:54:46, max mem: 7.6 GB 
[10/24 03:15:37 visual_prompt]: Epoch 14 / 100: avg data time: 2.14e+00, avg batch time: 3.0039, average train loss: 73.0184
[10/24 03:16:24 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4582, average loss: 234.9228
[10/24 03:16:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.51	
[10/24 03:16:24 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[10/24 03:21:41 visual_prompt]: 	Training 100/139. train loss: 70.5586,	9.4160 s / batch. (data: 8.54e+00). ETA=1 day, 7:00:17, max mem: 7.6 GB 
[10/24 03:23:26 visual_prompt]: Epoch 15 / 100: avg data time: 2.17e+00, avg batch time: 3.0317, average train loss: 62.6490
[10/24 03:24:13 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4634, average loss: 196.1515
[10/24 03:24:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.41	
[10/24 03:24:13 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[10/24 03:29:20 visual_prompt]: 	Training 100/139. train loss: 116.9952,	0.9200 s / batch. (data: 1.20e-02). ETA=2:59:38, max mem: 7.6 GB 
[10/24 03:31:10 visual_prompt]: Epoch 16 / 100: avg data time: 2.14e+00, avg batch time: 2.9961, average train loss: 78.6608
[10/24 03:31:57 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4599, average loss: 87.0466
[10/24 03:31:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.13	
[10/24 03:31:57 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[10/24 03:37:03 visual_prompt]: 	Training 100/139. train loss: 187.6907,	3.3812 s / batch. (data: 2.54e+00). ETA=10:52:21, max mem: 7.6 GB 
[10/24 03:38:54 visual_prompt]: Epoch 17 / 100: avg data time: 2.14e+00, avg batch time: 2.9973, average train loss: 77.6079
[10/24 03:39:41 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4583, average loss: 93.9980
[10/24 03:39:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.73	
[10/24 03:39:41 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[10/24 03:44:49 visual_prompt]: 	Training 100/139. train loss: 17.7394,	1.4900 s / batch. (data: 6.26e-01). ETA=4:44:00, max mem: 7.6 GB 
[10/24 03:46:39 visual_prompt]: Epoch 18 / 100: avg data time: 2.14e+00, avg batch time: 3.0045, average train loss: 58.8235
[10/24 03:47:26 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4572, average loss: 74.3518
[10/24 03:47:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.76	
[10/24 03:47:26 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[10/24 03:52:38 visual_prompt]: 	Training 100/139. train loss: 87.2641,	10.9759 s / batch. (data: 1.01e+01). ETA=1 day, 10:26:46, max mem: 7.6 GB 
[10/24 03:54:24 visual_prompt]: Epoch 19 / 100: avg data time: 2.15e+00, avg batch time: 3.0061, average train loss: 68.4920
[10/24 03:55:11 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.4585, average loss: 84.4474
[10/24 03:55:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.54	
[10/24 03:55:11 visual_prompt]: Stopping early.
[10/24 03:55:12 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 03:55:12 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 03:55:12 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 03:55:12 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 03:55:12 visual_prompt]: Training with config:
[10/24 03:55:12 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr25.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 03:55:12 visual_prompt]: Loading training data...
[10/24 03:55:12 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 03:55:12 visual_prompt]: Loading validation data...
[10/24 03:55:12 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 03:55:12 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/24 03:55:14 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/24 03:55:14 visual_prompt]: tuned percent:0.534
[10/24 03:55:14 visual_prompt]: Device used for model: 0
[10/24 03:55:14 visual_prompt]: Setting up Evaluator...
[10/24 03:55:14 visual_prompt]: Setting up Trainer...
[10/24 03:55:14 visual_prompt]: 	Setting up the optimizer...
[10/24 03:55:14 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 04:00:23 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8920 s / batch. (data: 2.87e-04). ETA=3:25:09, max mem: 7.6 GB 
[10/24 04:02:15 visual_prompt]: Epoch 1 / 100: avg data time: 2.15e+00, avg batch time: 3.0256, average train loss: 1.3980
[10/24 04:03:02 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.4586, average loss: 1.3816
[10/24 04:03:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/24 04:03:02 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[10/24 04:08:07 visual_prompt]: 	Training 100/139. train loss: 1.5686,	0.8840 s / batch. (data: 2.32e-04). ETA=3:21:16, max mem: 7.6 GB 
[10/24 04:09:58 visual_prompt]: Epoch 2 / 100: avg data time: 2.12e+00, avg batch time: 2.9924, average train loss: 8.2640
[10/24 04:10:46 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.4539, average loss: 3.0011
[10/24 04:10:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.98	
[10/24 04:10:46 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[10/24 04:15:52 visual_prompt]: 	Training 100/139. train loss: 8.0978,	0.8720 s / batch. (data: 2.81e-04). ETA=3:16:31, max mem: 7.6 GB 
[10/24 04:17:42 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9940, average train loss: 10.5749
[10/24 04:18:29 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.4584, average loss: 7.3076
[10/24 04:18:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.06	
[10/24 04:18:29 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[10/24 04:23:31 visual_prompt]: 	Training 100/139. train loss: 34.4224,	0.9074 s / batch. (data: 2.75e-04). ETA=3:22:23, max mem: 7.6 GB 
[10/24 04:25:26 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 2.9981, average train loss: 18.2033
[10/24 04:26:13 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.4555, average loss: 16.8463
[10/24 04:26:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.27	
[10/24 04:26:13 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[10/24 04:31:22 visual_prompt]: 	Training 100/139. train loss: 3.3737,	8.9776 s / batch. (data: 8.11e+00). ETA=1 day, 9:01:39, max mem: 7.6 GB 
[10/24 04:33:12 visual_prompt]: Epoch 5 / 100: avg data time: 2.14e+00, avg batch time: 3.0110, average train loss: 26.7002
[10/24 04:33:59 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4589, average loss: 29.6429
[10/24 04:33:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.26	
[10/24 04:33:59 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[10/24 04:39:03 visual_prompt]: 	Training 100/139. train loss: 37.7305,	0.8706 s / batch. (data: 3.03e-04). ETA=3:10:09, max mem: 7.6 GB 
[10/24 04:40:56 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 2.9993, average train loss: 37.3126
[10/24 04:41:43 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.4560, average loss: 30.7406
[10/24 04:41:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.84	
[10/24 04:41:43 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[10/24 04:46:51 visual_prompt]: 	Training 100/139. train loss: 142.8260,	0.8768 s / batch. (data: 2.42e-04). ETA=3:09:29, max mem: 7.6 GB 
[10/24 04:48:39 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9922, average train loss: 42.6225
[10/24 04:49:27 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4573, average loss: 19.3437
[10/24 04:49:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.95	
[10/24 04:49:27 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[10/24 04:54:34 visual_prompt]: 	Training 100/139. train loss: 56.9626,	0.8800 s / batch. (data: 7.95e-03). ETA=3:08:07, max mem: 7.6 GB 
[10/24 04:56:24 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0012, average train loss: 47.1032
[10/24 04:57:11 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4568, average loss: 18.8657
[10/24 04:57:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.50	
[10/24 04:57:11 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[10/24 05:02:22 visual_prompt]: 	Training 100/139. train loss: 3.2009,	0.8740 s / batch. (data: 2.49e-04). ETA=3:04:49, max mem: 7.6 GB 
[10/24 05:04:10 visual_prompt]: Epoch 9 / 100: avg data time: 2.14e+00, avg batch time: 3.0099, average train loss: 63.1768
[10/24 05:04:57 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4574, average loss: 58.9496
[10/24 05:04:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.21	
[10/24 05:04:57 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[10/24 05:10:03 visual_prompt]: 	Training 100/139. train loss: 3.7980,	0.8689 s / batch. (data: 2.85e-04). ETA=3:01:44, max mem: 7.6 GB 
[10/24 05:11:53 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9928, average train loss: 62.7223
[10/24 05:12:41 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.4551, average loss: 37.3636
[10/24 05:12:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.53	
[10/24 05:12:41 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[10/24 05:17:50 visual_prompt]: 	Training 100/139. train loss: 190.2850,	0.8788 s / batch. (data: 2.91e-04). ETA=3:01:46, max mem: 7.6 GB 
[10/24 05:19:39 visual_prompt]: Epoch 11 / 100: avg data time: 2.14e+00, avg batch time: 3.0054, average train loss: 80.5206
[10/24 05:20:26 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4610, average loss: 99.5680
[10/24 05:20:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.63	
[10/24 05:20:27 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[10/24 05:25:33 visual_prompt]: 	Training 100/139. train loss: 59.3253,	0.8800 s / batch. (data: 3.64e-04). ETA=2:59:58, max mem: 7.6 GB 
[10/24 05:27:23 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 2.9954, average train loss: 67.0742
[10/24 05:28:10 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4554, average loss: 21.6802
[10/24 05:28:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.36	
[10/24 05:28:10 visual_prompt]: Best epoch 12: best metric: -21.680
[10/24 05:28:10 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[10/24 05:33:20 visual_prompt]: 	Training 100/139. train loss: 9.5571,	0.8948 s / batch. (data: 5.36e-03). ETA=3:00:55, max mem: 7.6 GB 
[10/24 05:35:08 visual_prompt]: Epoch 13 / 100: avg data time: 2.14e+00, avg batch time: 3.0056, average train loss: 59.6519
[10/24 05:35:55 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4557, average loss: 44.2847
[10/24 05:35:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.88	
[10/24 05:35:55 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[10/24 05:41:05 visual_prompt]: 	Training 100/139. train loss: 4.7747,	0.8984 s / batch. (data: 7.36e-04). ETA=2:59:34, max mem: 7.6 GB 
[10/24 05:42:52 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 2.9951, average train loss: 70.5977
[10/24 05:43:39 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.4574, average loss: 28.2995
[10/24 05:43:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.22	
[10/24 05:43:39 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[10/24 05:48:51 visual_prompt]: 	Training 100/139. train loss: 64.3491,	9.2480 s / batch. (data: 8.39e+00). ETA=1 day, 6:27:05, max mem: 7.6 GB 
[10/24 05:50:36 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 2.9989, average train loss: 67.9332
[10/24 05:51:23 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.4548, average loss: 102.9595
[10/24 05:51:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.67	
[10/24 05:51:23 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[10/24 05:56:30 visual_prompt]: 	Training 100/139. train loss: 73.9576,	0.8960 s / batch. (data: 7.96e-03). ETA=2:54:57, max mem: 7.6 GB 
[10/24 05:58:19 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9911, average train loss: 56.1537
[10/24 05:59:06 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.4542, average loss: 14.0811
[10/24 05:59:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.99	
[10/24 05:59:06 visual_prompt]: Best epoch 16: best metric: -14.081
[10/24 05:59:06 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[10/24 06:04:17 visual_prompt]: 	Training 100/139. train loss: 74.9376,	1.7960 s / batch. (data: 9.41e-01). ETA=5:46:30, max mem: 7.6 GB 
[10/24 06:06:08 visual_prompt]: Epoch 17 / 100: avg data time: 2.17e+00, avg batch time: 3.0362, average train loss: 68.7440
[10/24 06:06:56 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4569, average loss: 12.2231
[10/24 06:06:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.64	
[10/24 06:06:56 visual_prompt]: Best epoch 17: best metric: -12.223
[10/24 06:06:56 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[10/24 06:12:03 visual_prompt]: 	Training 100/139. train loss: 16.7960,	1.3560 s / batch. (data: 4.82e-01). ETA=4:18:28, max mem: 7.6 GB 
[10/24 06:13:53 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0006, average train loss: 59.6577
[10/24 06:14:40 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4553, average loss: 15.4010
[10/24 06:14:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.21	
[10/24 06:14:40 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[10/24 06:19:55 visual_prompt]: 	Training 100/139. train loss: 53.3968,	11.2200 s / batch. (data: 1.03e+01). ETA=1 day, 11:12:43, max mem: 7.6 GB 
[10/24 06:21:38 visual_prompt]: Epoch 19 / 100: avg data time: 2.14e+00, avg batch time: 3.0083, average train loss: 82.7684
[10/24 06:22:26 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4611, average loss: 138.0756
[10/24 06:22:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.03	
[10/24 06:22:26 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[10/24 06:27:37 visual_prompt]: 	Training 100/139. train loss: 166.4270,	0.9171 s / batch. (data: 2.96e-04). ETA=2:50:33, max mem: 7.6 GB 
[10/24 06:29:29 visual_prompt]: Epoch 20 / 100: avg data time: 2.18e+00, avg batch time: 3.0448, average train loss: 69.1845
[10/24 06:30:16 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.4602, average loss: 140.6116
[10/24 06:30:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.28	
[10/24 06:30:16 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[10/24 06:35:24 visual_prompt]: 	Training 100/139. train loss: 42.0835,	0.8840 s / batch. (data: 3.07e-04). ETA=2:42:21, max mem: 7.6 GB 
[10/24 06:37:13 visual_prompt]: Epoch 21 / 100: avg data time: 2.13e+00, avg batch time: 2.9975, average train loss: 70.0924
[10/24 06:38:00 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.4556, average loss: 36.7362
[10/24 06:38:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.82	
[10/24 06:38:00 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[10/24 06:43:08 visual_prompt]: 	Training 100/139. train loss: 68.6553,	0.8866 s / batch. (data: 2.43e-04). ETA=2:40:46, max mem: 7.6 GB 
[10/24 06:45:00 visual_prompt]: Epoch 22 / 100: avg data time: 2.16e+00, avg batch time: 3.0239, average train loss: 64.6085
[10/24 06:45:48 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4588, average loss: 8.9493
[10/24 06:45:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.15	
[10/24 06:45:48 visual_prompt]: Best epoch 22: best metric: -8.949
[10/24 06:45:48 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[10/24 06:50:53 visual_prompt]: 	Training 100/139. train loss: 97.4785,	0.8960 s / batch. (data: 1.19e-02). ETA=2:40:24, max mem: 7.6 GB 
[10/24 06:52:45 visual_prompt]: Epoch 23 / 100: avg data time: 2.13e+00, avg batch time: 3.0013, average train loss: 76.5242
[10/24 06:53:32 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4590, average loss: 36.9175
[10/24 06:53:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.50	
[10/24 06:53:32 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[10/24 06:58:43 visual_prompt]: 	Training 100/139. train loss: 38.3145,	0.8840 s / batch. (data: 7.97e-04). ETA=2:36:12, max mem: 7.6 GB 
[10/24 07:00:30 visual_prompt]: Epoch 24 / 100: avg data time: 2.14e+00, avg batch time: 3.0050, average train loss: 78.1622
[10/24 07:01:17 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4580, average loss: 25.5605
[10/24 07:01:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.08	
[10/24 07:01:17 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[10/24 07:06:26 visual_prompt]: 	Training 100/139. train loss: 146.5514,	0.8670 s / batch. (data: 3.00e-04). ETA=2:31:12, max mem: 7.6 GB 
[10/24 07:08:15 visual_prompt]: Epoch 25 / 100: avg data time: 2.14e+00, avg batch time: 3.0043, average train loss: 75.1050
[10/24 07:09:02 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4550, average loss: 60.1119
[10/24 07:09:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.06	
[10/24 07:09:02 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[10/24 07:14:08 visual_prompt]: 	Training 100/139. train loss: 39.5464,	0.8696 s / batch. (data: 5.50e-03). ETA=2:29:38, max mem: 7.6 GB 
[10/24 07:15:58 visual_prompt]: Epoch 26 / 100: avg data time: 2.13e+00, avg batch time: 2.9947, average train loss: 81.3624
[10/24 07:16:46 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.4587, average loss: 7.2279
[10/24 07:16:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.31	rocauc: 39.56	
[10/24 07:16:46 visual_prompt]: Best epoch 26: best metric: -7.228
[10/24 07:16:46 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[10/24 07:21:57 visual_prompt]: 	Training 100/139. train loss: 68.6620,	9.4228 s / batch. (data: 8.55e+00). ETA=1 day, 2:39:40, max mem: 7.6 GB 
[10/24 07:23:44 visual_prompt]: Epoch 27 / 100: avg data time: 2.14e+00, avg batch time: 3.0118, average train loss: 67.3887
[10/24 07:24:32 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4558, average loss: 205.9958
[10/24 07:24:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.13	
[10/24 07:24:32 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[10/24 07:29:36 visual_prompt]: 	Training 100/139. train loss: 103.5647,	0.8570 s / batch. (data: 3.13e-04). ETA=2:23:30, max mem: 7.6 GB 
[10/24 07:31:28 visual_prompt]: Epoch 28 / 100: avg data time: 2.13e+00, avg batch time: 2.9949, average train loss: 70.0262
[10/24 07:32:15 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4574, average loss: 14.2620
[10/24 07:32:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.50	
[10/24 07:32:15 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[10/24 07:37:19 visual_prompt]: 	Training 100/139. train loss: 52.7830,	0.8840 s / batch. (data: 2.91e-04). ETA=2:25:58, max mem: 7.6 GB 
[10/24 07:39:11 visual_prompt]: Epoch 29 / 100: avg data time: 2.12e+00, avg batch time: 2.9923, average train loss: 62.9429
[10/24 07:40:04 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4574, average loss: 27.2768
[10/24 07:40:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.79	
[10/24 07:40:04 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[10/24 07:45:13 visual_prompt]: 	Training 100/139. train loss: 24.5457,	0.8723 s / batch. (data: 2.91e-04). ETA=2:22:01, max mem: 7.6 GB 
[10/24 07:47:02 visual_prompt]: Epoch 30 / 100: avg data time: 2.14e+00, avg batch time: 3.0055, average train loss: 54.0780
[10/24 07:47:49 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4569, average loss: 54.0660
[10/24 07:47:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.04	
[10/24 07:47:49 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[10/24 07:53:02 visual_prompt]: 	Training 100/139. train loss: 66.6540,	0.8596 s / batch. (data: 7.99e-03). ETA=2:17:58, max mem: 7.6 GB 
[10/24 07:54:46 visual_prompt]: Epoch 31 / 100: avg data time: 2.13e+00, avg batch time: 2.9984, average train loss: 66.2732
[10/24 07:55:33 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4571, average loss: 88.4020
[10/24 07:55:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.09	
[10/24 07:55:33 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[10/24 08:00:43 visual_prompt]: 	Training 100/139. train loss: 73.9498,	6.4624 s / batch. (data: 5.61e+00). ETA=17:02:15, max mem: 7.6 GB 
[10/24 08:02:30 visual_prompt]: Epoch 32 / 100: avg data time: 2.13e+00, avg batch time: 2.9941, average train loss: 57.0966
[10/24 08:03:17 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.4603, average loss: 101.3944
[10/24 08:03:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.29	
[10/24 08:03:17 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[10/24 08:08:22 visual_prompt]: 	Training 100/139. train loss: 60.6372,	0.8775 s / batch. (data: 5.41e-03). ETA=2:16:46, max mem: 7.6 GB 
[10/24 08:10:15 visual_prompt]: Epoch 33 / 100: avg data time: 2.14e+00, avg batch time: 3.0097, average train loss: 66.2812
[10/24 08:11:03 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4564, average loss: 58.3273
[10/24 08:11:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.21	
[10/24 08:11:03 visual_prompt]: Stopping early.
[10/24 08:11:03 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 08:11:03 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 08:11:03 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 08:11:03 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 08:11:03 visual_prompt]: Training with config:
[10/24 08:11:03 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr25.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 08:11:03 visual_prompt]: Loading training data...
[10/24 08:11:03 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 08:11:03 visual_prompt]: Loading validation data...
[10/24 08:11:03 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 08:11:03 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/24 08:11:05 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/24 08:11:05 visual_prompt]: tuned percent:0.534
[10/24 08:11:05 visual_prompt]: Device used for model: 0
[10/24 08:11:05 visual_prompt]: Setting up Evaluator...
[10/24 08:11:05 visual_prompt]: Setting up Trainer...
[10/24 08:11:05 visual_prompt]: 	Setting up the optimizer...
[10/24 08:11:05 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 08:16:11 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8797 s / batch. (data: 5.42e-03). ETA=3:22:19, max mem: 7.6 GB 
[10/24 08:18:03 visual_prompt]: Epoch 1 / 100: avg data time: 2.13e+00, avg batch time: 3.0013, average train loss: 1.3980
[10/24 08:18:50 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4557, average loss: 1.3816
[10/24 08:18:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/24 08:18:50 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[10/24 08:23:55 visual_prompt]: 	Training 100/139. train loss: 0.7101,	0.8770 s / batch. (data: 1.06e-02). ETA=3:19:40, max mem: 7.6 GB 
[10/24 08:25:47 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 2.9984, average train loss: 5.9956
[10/24 08:26:34 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.4567, average loss: 15.2626
[10/24 08:26:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.72	
[10/24 08:26:34 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[10/24 08:31:40 visual_prompt]: 	Training 100/139. train loss: 15.5067,	0.9013 s / batch. (data: 1.55e-02). ETA=3:23:07, max mem: 7.6 GB 
[10/24 08:33:31 visual_prompt]: Epoch 3 / 100: avg data time: 2.13e+00, avg batch time: 2.9988, average train loss: 10.6451
[10/24 08:34:18 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.4636, average loss: 32.0054
[10/24 08:34:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.71	
[10/24 08:34:18 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[10/24 08:39:21 visual_prompt]: 	Training 100/139. train loss: 7.8960,	0.9000 s / batch. (data: 1.20e-02). ETA=3:20:45, max mem: 7.6 GB 
[10/24 08:41:17 visual_prompt]: Epoch 4 / 100: avg data time: 2.14e+00, avg batch time: 3.0105, average train loss: 20.5271
[10/24 08:42:04 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4553, average loss: 6.3715
[10/24 08:42:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.45	
[10/24 08:42:04 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[10/24 08:47:13 visual_prompt]: 	Training 100/139. train loss: 40.7385,	11.0522 s / batch. (data: 1.02e+01). ETA=1 day, 16:39:35, max mem: 7.6 GB 
[10/24 08:49:02 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0024, average train loss: 21.4847
[10/24 08:49:49 visual_prompt]: Inference (val):avg data time: 5.30e-04, avg batch time: 0.4584, average loss: 5.9540
[10/24 08:49:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.18	
[10/24 08:49:49 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[10/24 08:54:54 visual_prompt]: 	Training 100/139. train loss: 20.9920,	0.8681 s / batch. (data: 3.14e-04). ETA=3:09:36, max mem: 7.6 GB 
[10/24 08:56:46 visual_prompt]: Epoch 6 / 100: avg data time: 2.12e+00, avg batch time: 2.9951, average train loss: 27.0252
[10/24 08:57:33 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.4551, average loss: 54.0825
[10/24 08:57:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.59	
[10/24 08:57:33 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[10/24 09:02:38 visual_prompt]: 	Training 100/139. train loss: 28.7745,	0.9160 s / batch. (data: 2.14e-02). ETA=3:17:57, max mem: 7.6 GB 
[10/24 09:04:29 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9941, average train loss: 37.6212
[10/24 09:05:16 visual_prompt]: Inference (val):avg data time: 7.24e-04, avg batch time: 0.4593, average loss: 1.9970
[10/24 09:05:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.23	
[10/24 09:05:16 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[10/24 09:10:25 visual_prompt]: 	Training 100/139. train loss: 15.5294,	0.8543 s / batch. (data: 2.96e-04). ETA=3:02:38, max mem: 7.6 GB 
[10/24 09:12:17 visual_prompt]: Epoch 8 / 100: avg data time: 2.16e+00, avg batch time: 3.0254, average train loss: 41.6842
[10/24 09:13:05 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4574, average loss: 2.3485
[10/24 09:13:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.86	
[10/24 09:13:05 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[10/24 09:18:14 visual_prompt]: 	Training 100/139. train loss: 35.5764,	0.8931 s / batch. (data: 2.72e-02). ETA=3:08:51, max mem: 7.6 GB 
[10/24 09:20:02 visual_prompt]: Epoch 9 / 100: avg data time: 2.13e+00, avg batch time: 2.9938, average train loss: 49.2450
[10/24 09:20:49 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.4605, average loss: 19.4740
[10/24 09:20:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.00	
[10/24 09:20:49 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[10/24 09:25:57 visual_prompt]: 	Training 100/139. train loss: 10.9091,	0.8920 s / batch. (data: 2.98e-04). ETA=3:06:33, max mem: 7.6 GB 
[10/24 09:27:46 visual_prompt]: Epoch 10 / 100: avg data time: 2.13e+00, avg batch time: 3.0001, average train loss: 39.0609
[10/24 09:28:33 visual_prompt]: Inference (val):avg data time: 7.37e-04, avg batch time: 0.4565, average loss: 38.7019
[10/24 09:28:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.67	
[10/24 09:28:33 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[10/24 09:33:41 visual_prompt]: 	Training 100/139. train loss: 13.4967,	0.8589 s / batch. (data: 5.41e-03). ETA=2:57:39, max mem: 7.6 GB 
[10/24 09:35:33 visual_prompt]: Epoch 11 / 100: avg data time: 2.15e+00, avg batch time: 3.0195, average train loss: 63.4035
[10/24 09:36:20 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.4579, average loss: 58.2809
[10/24 09:36:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.97	
[10/24 09:36:20 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[10/24 09:41:26 visual_prompt]: 	Training 100/139. train loss: 48.0405,	0.8640 s / batch. (data: 7.96e-03). ETA=2:56:42, max mem: 7.6 GB 
[10/24 09:43:17 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9935, average train loss: 66.5892
[10/24 09:44:04 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4563, average loss: 14.6409
[10/24 09:44:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.78	
[10/24 09:44:04 visual_prompt]: Best epoch 12: best metric: -14.641
[10/24 09:44:04 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[10/24 09:49:13 visual_prompt]: 	Training 100/139. train loss: 73.7635,	0.8880 s / batch. (data: 1.19e-02). ETA=2:59:33, max mem: 7.6 GB 
[10/24 09:51:01 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0026, average train loss: 53.5464
[10/24 09:51:48 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.4576, average loss: 22.3736
[10/24 09:51:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.11	
[10/24 09:51:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[10/24 09:57:00 visual_prompt]: 	Training 100/139. train loss: 50.5001,	0.8720 s / batch. (data: 2.88e-04). ETA=2:54:17, max mem: 7.6 GB 
[10/24 09:58:48 visual_prompt]: Epoch 14 / 100: avg data time: 2.15e+00, avg batch time: 3.0139, average train loss: 49.3710
[10/24 09:59:35 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4588, average loss: 117.8173
[10/24 09:59:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.64	
[10/24 09:59:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[10/24 10:04:47 visual_prompt]: 	Training 100/139. train loss: 70.0655,	9.3857 s / batch. (data: 8.52e+00). ETA=1 day, 6:54:17, max mem: 7.6 GB 
[10/24 10:06:33 visual_prompt]: Epoch 15 / 100: avg data time: 2.14e+00, avg batch time: 3.0047, average train loss: 60.6113
[10/24 10:07:20 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4586, average loss: 44.2307
[10/24 10:07:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.42	
[10/24 10:07:20 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[10/24 10:12:27 visual_prompt]: 	Training 100/139. train loss: 101.2358,	0.8514 s / batch. (data: 2.42e-04). ETA=2:46:14, max mem: 7.6 GB 
[10/24 10:14:22 visual_prompt]: Epoch 16 / 100: avg data time: 2.17e+00, avg batch time: 3.0352, average train loss: 68.0554
[10/24 10:15:09 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4548, average loss: 35.9065
[10/24 10:15:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.00	
[10/24 10:15:09 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[10/24 10:20:14 visual_prompt]: 	Training 100/139. train loss: 62.2980,	0.8677 s / batch. (data: 2.95e-04). ETA=2:47:24, max mem: 7.6 GB 
[10/24 10:22:08 visual_prompt]: Epoch 17 / 100: avg data time: 2.14e+00, avg batch time: 3.0119, average train loss: 72.4114
[10/24 10:22:55 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.4582, average loss: 17.9404
[10/24 10:22:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.87	
[10/24 10:22:55 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[10/24 10:28:03 visual_prompt]: 	Training 100/139. train loss: 432.1643,	4.1840 s / batch. (data: 3.33e+00). ETA=13:17:32, max mem: 7.6 GB 
[10/24 10:29:52 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 2.9984, average train loss: 70.6083
[10/24 10:30:39 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4539, average loss: 198.6532
[10/24 10:30:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.48	
[10/24 10:30:39 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[10/24 10:35:53 visual_prompt]: 	Training 100/139. train loss: 17.2283,	11.0570 s / batch. (data: 1.02e+01). ETA=1 day, 10:42:02, max mem: 7.6 GB 
[10/24 10:37:37 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0015, average train loss: 60.3624
[10/24 10:38:24 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4546, average loss: 43.8188
[10/24 10:38:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.91	
[10/24 10:38:24 visual_prompt]: Stopping early.
[10/24 10:38:24 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 10:38:24 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 10:38:24 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 10:38:24 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 10:38:24 visual_prompt]: Training with config:
[10/24 10:38:24 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr25.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 10:38:24 visual_prompt]: Loading training data...
[10/24 10:38:24 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 10:38:24 visual_prompt]: Loading validation data...
[10/24 10:38:24 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 10:38:24 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/24 10:38:27 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/24 10:38:27 visual_prompt]: tuned percent:0.534
[10/24 10:38:27 visual_prompt]: Device used for model: 0
[10/24 10:38:27 visual_prompt]: Setting up Evaluator...
[10/24 10:38:27 visual_prompt]: Setting up Trainer...
[10/24 10:38:27 visual_prompt]: 	Setting up the optimizer...
[10/24 10:38:27 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 10:43:35 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8855 s / batch. (data: 1.09e-02). ETA=3:23:40, max mem: 7.6 GB 
[10/24 10:45:25 visual_prompt]: Epoch 1 / 100: avg data time: 2.13e+00, avg batch time: 3.0069, average train loss: 1.3980
[10/24 10:46:12 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.4647, average loss: 1.3816
[10/24 10:46:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/24 10:46:12 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[10/24 10:51:17 visual_prompt]: 	Training 100/139. train loss: 3.6095,	0.8814 s / batch. (data: 5.41e-03). ETA=3:20:40, max mem: 7.6 GB 
[10/24 10:53:09 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 3.0012, average train loss: 8.7793
[10/24 10:53:57 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4648, average loss: 17.9089
[10/24 10:53:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.87	
[10/24 10:53:57 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[10/24 10:59:03 visual_prompt]: 	Training 100/139. train loss: 27.5985,	0.8720 s / batch. (data: 2.64e-04). ETA=3:16:31, max mem: 7.6 GB 
[10/24 11:00:53 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9966, average train loss: 15.1278
[10/24 11:01:41 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4610, average loss: 36.5833
[10/24 11:01:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.58	
[10/24 11:01:41 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[10/24 11:06:43 visual_prompt]: 	Training 100/139. train loss: 14.5298,	0.8800 s / batch. (data: 3.23e-04). ETA=3:16:16, max mem: 7.6 GB 
[10/24 11:08:38 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 3.0046, average train loss: 13.5571
[10/24 11:09:32 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4670, average loss: 17.9078
[10/24 11:09:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.54	
[10/24 11:09:32 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[10/24 11:14:43 visual_prompt]: 	Training 100/139. train loss: 44.8680,	11.8200 s / batch. (data: 1.09e+01). ETA=1 day, 19:29:04, max mem: 7.6 GB 
[10/24 11:16:31 visual_prompt]: Epoch 5 / 100: avg data time: 2.14e+00, avg batch time: 3.0088, average train loss: 17.8426
[10/24 11:17:18 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4626, average loss: 10.1244
[10/24 11:17:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.85	
[10/24 11:17:18 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[10/24 11:22:24 visual_prompt]: 	Training 100/139. train loss: 2.6961,	0.9024 s / batch. (data: 5.39e-03). ETA=3:17:05, max mem: 7.6 GB 
[10/24 11:24:15 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 2.9999, average train loss: 19.9342
[10/24 11:25:02 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4637, average loss: 9.2045
[10/24 11:25:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.80	
[10/24 11:25:02 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[10/24 11:30:12 visual_prompt]: 	Training 100/139. train loss: 35.8083,	3.0108 s / batch. (data: 2.14e+00). ETA=10:50:38, max mem: 7.6 GB 
[10/24 11:32:00 visual_prompt]: Epoch 7 / 100: avg data time: 2.13e+00, avg batch time: 3.0019, average train loss: 34.9492
[10/24 11:32:47 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.4636, average loss: 33.5401
[10/24 11:32:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.92	
[10/24 11:32:47 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[10/24 11:37:55 visual_prompt]: 	Training 100/139. train loss: 3.6477,	0.9134 s / batch. (data: 2.06e-02). ETA=3:15:16, max mem: 7.6 GB 
[10/24 11:39:46 visual_prompt]: Epoch 8 / 100: avg data time: 2.14e+00, avg batch time: 3.0117, average train loss: 33.9524
[10/24 11:40:33 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.4617, average loss: 4.4035
[10/24 11:40:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.74	
[10/24 11:40:33 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[10/24 11:45:43 visual_prompt]: 	Training 100/139. train loss: 22.5412,	0.8744 s / batch. (data: 2.83e-04). ETA=3:04:54, max mem: 7.6 GB 
[10/24 11:47:30 visual_prompt]: Epoch 9 / 100: avg data time: 2.13e+00, avg batch time: 2.9972, average train loss: 50.8399
[10/24 11:48:17 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.4631, average loss: 50.0796
[10/24 11:48:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.19	
[10/24 11:48:17 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[10/24 11:53:24 visual_prompt]: 	Training 100/139. train loss: 26.2840,	0.8734 s / batch. (data: 1.05e-02). ETA=3:02:40, max mem: 7.6 GB 
[10/24 11:55:13 visual_prompt]: Epoch 10 / 100: avg data time: 2.13e+00, avg batch time: 2.9961, average train loss: 74.9429
[10/24 11:56:01 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4619, average loss: 0.9842
[10/24 11:56:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.09	
[10/24 11:56:01 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[10/24 12:01:08 visual_prompt]: 	Training 100/139. train loss: 53.3264,	0.8664 s / batch. (data: 2.94e-04). ETA=2:59:11, max mem: 7.6 GB 
[10/24 12:02:58 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0023, average train loss: 52.9468
[10/24 12:03:45 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.4628, average loss: 106.9399
[10/24 12:03:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.47	
[10/24 12:03:45 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[10/24 12:08:53 visual_prompt]: 	Training 100/139. train loss: 94.9973,	0.8758 s / batch. (data: 2.92e-04). ETA=2:59:07, max mem: 7.6 GB 
[10/24 12:10:42 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9965, average train loss: 38.0316
[10/24 12:11:29 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4649, average loss: 33.3500
[10/24 12:11:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.77	
[10/24 12:11:29 visual_prompt]: Best epoch 12: best metric: -33.350
[10/24 12:11:29 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[10/24 12:16:39 visual_prompt]: 	Training 100/139. train loss: 11.7466,	0.8760 s / batch. (data: 7.96e-03). ETA=2:57:07, max mem: 7.6 GB 
[10/24 12:18:27 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0065, average train loss: 27.2905
[10/24 12:19:15 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4610, average loss: 19.4682
[10/24 12:19:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.44	
[10/24 12:19:15 visual_prompt]: Best epoch 13: best metric: -19.468
[10/24 12:19:15 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[10/24 12:24:26 visual_prompt]: 	Training 100/139. train loss: 78.1935,	0.8626 s / batch. (data: 2.92e-04). ETA=2:52:25, max mem: 7.6 GB 
[10/24 12:26:13 visual_prompt]: Epoch 14 / 100: avg data time: 2.14e+00, avg batch time: 3.0081, average train loss: 42.8098
[10/24 12:27:00 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4602, average loss: 40.1805
[10/24 12:27:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.93	
[10/24 12:27:00 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[10/24 12:32:13 visual_prompt]: 	Training 100/139. train loss: 73.9683,	9.5697 s / batch. (data: 8.67e+00). ETA=1 day, 7:30:39, max mem: 7.6 GB 
[10/24 12:33:58 visual_prompt]: Epoch 15 / 100: avg data time: 2.14e+00, avg batch time: 3.0063, average train loss: 45.8262
[10/24 12:34:46 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4622, average loss: 47.4537
[10/24 12:34:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.77	
[10/24 12:34:46 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[10/24 12:39:54 visual_prompt]: 	Training 100/139. train loss: 17.4508,	0.8967 s / batch. (data: 1.05e-02). ETA=2:55:05, max mem: 7.6 GB 
[10/24 12:41:43 visual_prompt]: Epoch 16 / 100: avg data time: 2.13e+00, avg batch time: 2.9981, average train loss: 46.5228
[10/24 12:42:30 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4687, average loss: 2.5338
[10/24 12:42:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.87	
[10/24 12:42:30 visual_prompt]: Best epoch 16: best metric: -2.534
[10/24 12:42:30 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[10/24 12:47:37 visual_prompt]: 	Training 100/139. train loss: 21.2264,	3.8104 s / batch. (data: 2.91e+00). ETA=12:15:08, max mem: 7.6 GB 
[10/24 12:49:27 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 2.9982, average train loss: 46.1818
[10/24 12:50:14 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4623, average loss: 57.6435
[10/24 12:50:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.52	
[10/24 12:50:14 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[10/24 12:55:24 visual_prompt]: 	Training 100/139. train loss: 41.4179,	1.2480 s / batch. (data: 3.62e-01). ETA=3:57:53, max mem: 7.6 GB 
[10/24 12:57:16 visual_prompt]: Epoch 18 / 100: avg data time: 2.16e+00, avg batch time: 3.0308, average train loss: 66.2825
[10/24 12:58:03 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.4645, average loss: 44.5824
[10/24 12:58:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.61	
[10/24 12:58:03 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[10/24 13:03:16 visual_prompt]: 	Training 100/139. train loss: 8.5981,	10.3011 s / batch. (data: 9.40e+00). ETA=1 day, 8:19:41, max mem: 7.6 GB 
[10/24 13:05:01 visual_prompt]: Epoch 19 / 100: avg data time: 2.14e+00, avg batch time: 3.0062, average train loss: 52.5682
[10/24 13:05:48 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4644, average loss: 67.9704
[10/24 13:05:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.44	
[10/24 13:05:48 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[10/24 13:10:55 visual_prompt]: 	Training 100/139. train loss: 38.9702,	0.8828 s / batch. (data: 3.57e-04). ETA=2:44:10, max mem: 7.6 GB 
[10/24 13:12:46 visual_prompt]: Epoch 20 / 100: avg data time: 2.13e+00, avg batch time: 2.9996, average train loss: 44.8029
[10/24 13:13:33 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.4620, average loss: 56.3157
[10/24 13:13:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.34	
[10/24 13:13:33 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[10/24 13:18:39 visual_prompt]: 	Training 100/139. train loss: 27.1249,	0.8797 s / batch. (data: 2.90e-04). ETA=2:41:34, max mem: 7.6 GB 
[10/24 13:20:30 visual_prompt]: Epoch 21 / 100: avg data time: 2.13e+00, avg batch time: 2.9981, average train loss: 76.2536
[10/24 13:21:17 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.4636, average loss: 42.7339
[10/24 13:21:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.77	
[10/24 13:21:17 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[10/24 13:26:21 visual_prompt]: 	Training 100/139. train loss: 65.6409,	0.8960 s / batch. (data: 2.76e-04). ETA=2:42:29, max mem: 7.6 GB 
[10/24 13:28:14 visual_prompt]: Epoch 22 / 100: avg data time: 2.13e+00, avg batch time: 2.9982, average train loss: 64.5280
[10/24 13:29:01 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4639, average loss: 49.9473
[10/24 13:29:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.53	
[10/24 13:29:01 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[10/24 13:34:06 visual_prompt]: 	Training 100/139. train loss: 53.2441,	0.8588 s / batch. (data: 3.30e-04). ETA=2:33:45, max mem: 7.6 GB 
[10/24 13:35:58 visual_prompt]: Epoch 23 / 100: avg data time: 2.13e+00, avg batch time: 2.9985, average train loss: 41.8651
[10/24 13:36:45 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.4615, average loss: 28.6827
[10/24 13:36:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.18	
[10/24 13:36:45 visual_prompt]: Stopping early.
[10/24 13:36:45 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 13:36:45 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 13:36:45 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 13:36:45 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 13:36:45 visual_prompt]: Training with config:
[10/24 13:36:45 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr25.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 13:36:45 visual_prompt]: Loading training data...
[10/24 13:36:45 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 13:36:46 visual_prompt]: Loading validation data...
[10/24 13:36:46 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 13:36:46 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/24 13:36:48 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/24 13:36:48 visual_prompt]: tuned percent:0.534
[10/24 13:36:48 visual_prompt]: Device used for model: 0
[10/24 13:36:48 visual_prompt]: Setting up Evaluator...
[10/24 13:36:48 visual_prompt]: Setting up Trainer...
[10/24 13:36:48 visual_prompt]: 	Setting up the optimizer...
[10/24 13:36:48 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 13:41:54 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.9100 s / batch. (data: 5.41e-03). ETA=3:29:18, max mem: 7.6 GB 
[10/24 13:43:46 visual_prompt]: Epoch 1 / 100: avg data time: 2.13e+00, avg batch time: 3.0068, average train loss: 1.3980
[10/24 13:44:34 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.4634, average loss: 1.3816
[10/24 13:44:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/24 13:44:34 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[10/24 13:49:39 visual_prompt]: 	Training 100/139. train loss: 20.0589,	0.8731 s / batch. (data: 3.14e-04). ETA=3:18:46, max mem: 7.6 GB 
[10/24 13:51:30 visual_prompt]: Epoch 2 / 100: avg data time: 2.12e+00, avg batch time: 2.9933, average train loss: 8.9190
[10/24 13:52:17 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.4601, average loss: 1.9243
[10/24 13:52:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.88	
[10/24 13:52:17 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[10/24 13:57:24 visual_prompt]: 	Training 100/139. train loss: 6.2814,	0.9040 s / batch. (data: 2.92e-04). ETA=3:23:43, max mem: 7.6 GB 
[10/24 13:59:14 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9987, average train loss: 18.6562
[10/24 14:00:03 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.4645, average loss: 25.2331
[10/24 14:00:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.24	
[10/24 14:00:03 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[10/24 14:05:04 visual_prompt]: 	Training 100/139. train loss: 10.5324,	0.8675 s / batch. (data: 2.99e-04). ETA=3:13:30, max mem: 7.6 GB 
[10/24 14:06:59 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 2.9945, average train loss: 21.9145
[10/24 14:07:47 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4608, average loss: 13.3256
[10/24 14:07:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.26	
[10/24 14:07:47 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[10/24 14:12:56 visual_prompt]: 	Training 100/139. train loss: 34.8144,	11.1311 s / batch. (data: 1.03e+01). ETA=1 day, 16:57:00, max mem: 7.6 GB 
[10/24 14:14:44 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0045, average train loss: 22.4213
[10/24 14:15:32 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4581, average loss: 18.8706
[10/24 14:15:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.87	
[10/24 14:15:32 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[10/24 14:20:37 visual_prompt]: 	Training 100/139. train loss: 43.9742,	0.8791 s / batch. (data: 5.45e-03). ETA=3:12:00, max mem: 7.6 GB 
[10/24 14:22:31 visual_prompt]: Epoch 6 / 100: avg data time: 2.14e+00, avg batch time: 3.0156, average train loss: 45.4141
[10/24 14:23:18 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4598, average loss: 11.0383
[10/24 14:23:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.29	
[10/24 14:23:18 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[10/24 14:28:27 visual_prompt]: 	Training 100/139. train loss: 43.0118,	0.8630 s / batch. (data: 2.76e-04). ETA=3:06:30, max mem: 7.6 GB 
[10/24 14:30:16 visual_prompt]: Epoch 7 / 100: avg data time: 2.13e+00, avg batch time: 3.0067, average train loss: 31.4132
[10/24 14:31:04 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.4596, average loss: 32.8384
[10/24 14:31:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.55	
[10/24 14:31:04 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[10/24 14:36:11 visual_prompt]: 	Training 100/139. train loss: 2.2776,	0.8840 s / batch. (data: 3.07e-04). ETA=3:08:59, max mem: 7.6 GB 
[10/24 14:38:02 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0086, average train loss: 26.5107
[10/24 14:38:49 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.4636, average loss: 15.6407
[10/24 14:38:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.02	
[10/24 14:38:49 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[10/24 14:43:58 visual_prompt]: 	Training 100/139. train loss: 32.8679,	0.8643 s / batch. (data: 2.90e-04). ETA=3:02:46, max mem: 7.6 GB 
[10/24 14:45:46 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9959, average train loss: 25.9408
[10/24 14:46:33 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.4608, average loss: 3.5083
[10/24 14:46:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.48	
[10/24 14:46:33 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[10/24 14:51:40 visual_prompt]: 	Training 100/139. train loss: 15.7520,	0.9017 s / batch. (data: 9.63e-03). ETA=3:08:35, max mem: 7.6 GB 
[10/24 14:53:30 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9967, average train loss: 32.6737
[10/24 14:54:17 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.4634, average loss: 75.5382
[10/24 14:54:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.82	
[10/24 14:54:17 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[10/24 14:59:25 visual_prompt]: 	Training 100/139. train loss: 29.0940,	0.8828 s / batch. (data: 2.84e-04). ETA=3:02:35, max mem: 7.6 GB 
[10/24 15:01:15 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0052, average train loss: 33.5843
[10/24 15:02:02 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4598, average loss: 4.6997
[10/24 15:02:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.62	
[10/24 15:02:02 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[10/24 15:07:10 visual_prompt]: 	Training 100/139. train loss: 19.5744,	0.9028 s / batch. (data: 3.31e-04). ETA=3:04:38, max mem: 7.6 GB 
[10/24 15:08:59 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 3.0011, average train loss: 26.1493
[10/24 15:09:46 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4621, average loss: 22.4545
[10/24 15:09:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.78	
[10/24 15:09:46 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[10/24 15:14:56 visual_prompt]: 	Training 100/139. train loss: 35.3173,	0.8934 s / batch. (data: 1.57e-02). ETA=3:00:38, max mem: 7.6 GB 
[10/24 15:16:45 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0094, average train loss: 20.7935
[10/24 15:17:32 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4621, average loss: 23.0159
[10/24 15:17:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.49	
[10/24 15:17:32 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[10/24 15:22:44 visual_prompt]: 	Training 100/139. train loss: 87.6216,	0.8814 s / batch. (data: 7.37e-04). ETA=2:56:10, max mem: 7.6 GB 
[10/24 15:24:31 visual_prompt]: Epoch 14 / 100: avg data time: 2.14e+00, avg batch time: 3.0134, average train loss: 37.1945
[10/24 15:25:18 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.4592, average loss: 5.5738
[10/24 15:25:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.91	
[10/24 15:25:18 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[10/24 15:30:32 visual_prompt]: 	Training 100/139. train loss: 12.6344,	8.7209 s / batch. (data: 7.84e+00). ETA=1 day, 4:42:57, max mem: 7.6 GB 
[10/24 15:32:17 visual_prompt]: Epoch 15 / 100: avg data time: 2.14e+00, avg batch time: 3.0133, average train loss: 27.7940
[10/24 15:33:05 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4592, average loss: 15.5479
[10/24 15:33:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.77	
[10/24 15:33:05 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[10/24 15:38:11 visual_prompt]: 	Training 100/139. train loss: 29.7170,	0.8777 s / batch. (data: 2.85e-04). ETA=2:51:22, max mem: 7.6 GB 
[10/24 15:40:01 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9966, average train loss: 29.6555
[10/24 15:40:48 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.4596, average loss: 11.2868
[10/24 15:40:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.48	
[10/24 15:40:48 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[10/24 15:45:55 visual_prompt]: 	Training 100/139. train loss: 5.5991,	2.9133 s / batch. (data: 2.04e+00). ETA=9:22:03, max mem: 7.6 GB 
[10/24 15:47:46 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 3.0059, average train loss: 21.3315
[10/24 15:48:34 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4609, average loss: 10.0635
[10/24 15:48:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.42	
[10/24 15:48:34 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[10/24 15:53:41 visual_prompt]: 	Training 100/139. train loss: 4.7374,	0.8854 s / batch. (data: 5.41e-03). ETA=2:48:46, max mem: 7.6 GB 
[10/24 15:55:32 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0048, average train loss: 26.2444
[10/24 15:56:19 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4621, average loss: 108.2687
[10/24 15:56:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.88	
[10/24 15:56:19 visual_prompt]: Stopping early.
[10/24 15:56:19 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 15:56:19 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 15:56:19 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 15:56:19 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 15:56:19 visual_prompt]: Training with config:
[10/24 15:56:19 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr10.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 15:56:19 visual_prompt]: Loading training data...
[10/24 15:56:19 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 15:56:19 visual_prompt]: Loading validation data...
[10/24 15:56:19 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 15:56:19 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/24 15:56:22 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/24 15:56:22 visual_prompt]: tuned percent:0.534
[10/24 15:56:22 visual_prompt]: Device used for model: 0
[10/24 15:56:22 visual_prompt]: Setting up Evaluator...
[10/24 15:56:22 visual_prompt]: Setting up Trainer...
[10/24 15:56:22 visual_prompt]: 	Setting up the optimizer...
[10/24 15:56:22 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 16:01:29 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8920 s / batch. (data: 2.74e-04). ETA=3:25:09, max mem: 7.6 GB 
[10/24 16:03:19 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9999, average train loss: 1.3980
[10/24 16:04:06 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4623, average loss: 1.3816
[10/24 16:04:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/24 16:04:06 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[10/24 16:09:12 visual_prompt]: 	Training 100/139. train loss: 1.4531,	0.8641 s / batch. (data: 2.88e-04). ETA=3:16:44, max mem: 7.6 GB 
[10/24 16:11:04 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 3.0015, average train loss: 3.4965
[10/24 16:11:51 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.4622, average loss: 1.9190
[10/24 16:11:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.15	
[10/24 16:11:51 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[10/24 16:16:59 visual_prompt]: 	Training 100/139. train loss: 1.1362,	0.9076 s / batch. (data: 2.94e-04). ETA=3:24:32, max mem: 7.6 GB 
[10/24 16:18:52 visual_prompt]: Epoch 3 / 100: avg data time: 2.15e+00, avg batch time: 3.0283, average train loss: 4.4386
[10/24 16:19:39 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.4570, average loss: 6.3889
[10/24 16:19:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.82	
[10/24 16:19:39 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[10/24 16:24:41 visual_prompt]: 	Training 100/139. train loss: 4.4796,	0.8688 s / batch. (data: 2.79e-04). ETA=3:13:46, max mem: 7.6 GB 
[10/24 16:26:36 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 2.9938, average train loss: 6.2214
[10/24 16:27:23 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.4613, average loss: 9.5295
[10/24 16:27:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.18	
[10/24 16:27:23 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[10/24 16:32:33 visual_prompt]: 	Training 100/139. train loss: 10.2594,	11.2738 s / batch. (data: 1.04e+01). ETA=1 day, 17:28:29, max mem: 7.6 GB 
[10/24 16:34:20 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0029, average train loss: 8.3463
[10/24 16:35:08 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.4625, average loss: 22.7192
[10/24 16:35:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.50	
[10/24 16:35:08 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[10/24 16:40:13 visual_prompt]: 	Training 100/139. train loss: 1.9223,	0.8981 s / batch. (data: 3.04e-04). ETA=3:16:09, max mem: 7.6 GB 
[10/24 16:42:05 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 3.0015, average train loss: 12.7123
[10/24 16:42:52 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.4637, average loss: 8.0334
[10/24 16:42:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.31	
[10/24 16:42:52 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[10/24 16:48:00 visual_prompt]: 	Training 100/139. train loss: 5.7634,	1.5457 s / batch. (data: 6.61e-01). ETA=5:34:01, max mem: 7.6 GB 
[10/24 16:49:49 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9990, average train loss: 13.5732
[10/24 16:50:36 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4621, average loss: 34.0866
[10/24 16:50:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.73	
[10/24 16:50:36 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[10/24 16:55:47 visual_prompt]: 	Training 100/139. train loss: 0.8032,	0.8856 s / batch. (data: 5.43e-03). ETA=3:09:19, max mem: 7.6 GB 
[10/24 16:57:40 visual_prompt]: Epoch 8 / 100: avg data time: 2.17e+00, avg batch time: 3.0489, average train loss: 15.7479
[10/24 16:58:28 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.4646, average loss: 3.7730
[10/24 16:58:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.38	
[10/24 16:58:28 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[10/24 17:03:39 visual_prompt]: 	Training 100/139. train loss: 23.0385,	0.8663 s / batch. (data: 3.47e-04). ETA=3:03:11, max mem: 7.6 GB 
[10/24 17:05:26 visual_prompt]: Epoch 9 / 100: avg data time: 2.13e+00, avg batch time: 3.0054, average train loss: 17.3996
[10/24 17:06:13 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4629, average loss: 12.6325
[10/24 17:06:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.44	
[10/24 17:06:13 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[10/24 17:11:21 visual_prompt]: 	Training 100/139. train loss: 16.1652,	0.8711 s / batch. (data: 7.96e-03). ETA=3:02:11, max mem: 7.6 GB 
[10/24 17:13:11 visual_prompt]: Epoch 10 / 100: avg data time: 2.14e+00, avg batch time: 3.0100, average train loss: 22.7116
[10/24 17:13:59 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4609, average loss: 86.6868
[10/24 17:13:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.78	
[10/24 17:13:59 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[10/24 17:19:07 visual_prompt]: 	Training 100/139. train loss: 14.5760,	0.8626 s / batch. (data: 2.98e-04). ETA=2:58:24, max mem: 7.6 GB 
[10/24 17:20:57 visual_prompt]: Epoch 11 / 100: avg data time: 2.14e+00, avg batch time: 3.0111, average train loss: 32.1918
[10/24 17:21:44 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.4623, average loss: 29.1643
[10/24 17:21:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.51	
[10/24 17:21:44 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[10/24 17:26:52 visual_prompt]: 	Training 100/139. train loss: 35.5133,	1.1311 s / batch. (data: 2.41e-01). ETA=3:51:19, max mem: 7.6 GB 
[10/24 17:28:42 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 3.0012, average train loss: 23.8449
[10/24 17:29:29 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.4627, average loss: 13.9235
[10/24 17:29:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.86	
[10/24 17:29:29 visual_prompt]: Best epoch 12: best metric: -13.923
[10/24 17:29:29 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[10/24 17:34:44 visual_prompt]: 	Training 100/139. train loss: 19.1007,	0.8946 s / batch. (data: 8.44e-03). ETA=3:00:53, max mem: 7.6 GB 
[10/24 17:36:40 visual_prompt]: Epoch 13 / 100: avg data time: 2.23e+00, avg batch time: 3.0974, average train loss: 26.5038
[10/24 17:37:27 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.4632, average loss: 37.4082
[10/24 17:37:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.70	
[10/24 17:37:27 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[10/24 17:42:40 visual_prompt]: 	Training 100/139. train loss: 26.8538,	0.8869 s / batch. (data: 1.05e-02). ETA=2:57:16, max mem: 7.6 GB 
[10/24 17:44:27 visual_prompt]: Epoch 14 / 100: avg data time: 2.15e+00, avg batch time: 3.0215, average train loss: 34.2192
[10/24 17:45:14 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.4648, average loss: 28.3718
[10/24 17:45:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.77	
[10/24 17:45:14 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[10/24 17:50:30 visual_prompt]: 	Training 100/139. train loss: 28.4739,	9.3280 s / batch. (data: 8.45e+00). ETA=1 day, 6:42:54, max mem: 7.6 GB 
[10/24 17:52:20 visual_prompt]: Epoch 15 / 100: avg data time: 2.19e+00, avg batch time: 3.0640, average train loss: 30.1051
[10/24 17:53:09 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4631, average loss: 41.0321
[10/24 17:53:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.43	
[10/24 17:53:09 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[10/24 17:59:00 visual_prompt]: 	Training 100/139. train loss: 10.8299,	0.8883 s / batch. (data: 2.16e-04). ETA=2:53:26, max mem: 7.6 GB 
[10/24 18:01:15 visual_prompt]: Epoch 16 / 100: avg data time: 2.63e+00, avg batch time: 3.4950, average train loss: 27.0358
[10/24 18:02:09 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4610, average loss: 31.4445
[10/24 18:02:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.38	
[10/24 18:02:09 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[10/24 18:07:38 visual_prompt]: 	Training 100/139. train loss: 10.9053,	5.7640 s / batch. (data: 4.89e+00). ETA=18:32:03, max mem: 7.6 GB 
[10/24 18:09:37 visual_prompt]: Epoch 17 / 100: avg data time: 2.35e+00, avg batch time: 3.2221, average train loss: 20.6429
[10/24 18:10:26 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4627, average loss: 28.7534
[10/24 18:10:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.13	
[10/24 18:10:26 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[10/24 18:15:37 visual_prompt]: 	Training 100/139. train loss: 24.2994,	1.1873 s / batch. (data: 2.96e-01). ETA=3:46:19, max mem: 7.6 GB 
[10/24 18:17:29 visual_prompt]: Epoch 18 / 100: avg data time: 2.17e+00, avg batch time: 3.0409, average train loss: 23.9786
[10/24 18:18:16 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4632, average loss: 43.0022
[10/24 18:18:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.91	
[10/24 18:18:16 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[10/24 18:23:32 visual_prompt]: 	Training 100/139. train loss: 33.4592,	10.6083 s / batch. (data: 9.73e+00). ETA=1 day, 9:17:32, max mem: 7.6 GB 
[10/24 18:25:17 visual_prompt]: Epoch 19 / 100: avg data time: 2.15e+00, avg batch time: 3.0243, average train loss: 32.3087
[10/24 18:26:04 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4624, average loss: 33.5319
[10/24 18:26:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.41	
[10/24 18:26:04 visual_prompt]: Stopping early.
[10/24 18:26:04 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 18:26:04 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 18:26:04 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 18:26:04 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 18:26:04 visual_prompt]: Training with config:
[10/24 18:26:04 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr10.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 18:26:04 visual_prompt]: Loading training data...
[10/24 18:26:04 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 18:26:04 visual_prompt]: Loading validation data...
[10/24 18:26:04 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 18:26:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/24 18:26:19 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/24 18:26:19 visual_prompt]: tuned percent:0.534
[10/24 18:26:20 visual_prompt]: Device used for model: 0
[10/24 18:26:20 visual_prompt]: Setting up Evaluator...
[10/24 18:26:20 visual_prompt]: Setting up Trainer...
[10/24 18:26:20 visual_prompt]: 	Setting up the optimizer...
[10/24 18:26:20 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 18:31:26 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.9000 s / batch. (data: 3.02e-04). ETA=3:26:59, max mem: 7.6 GB 
[10/24 18:33:18 visual_prompt]: Epoch 1 / 100: avg data time: 2.13e+00, avg batch time: 3.0066, average train loss: 1.3980
[10/24 18:34:05 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.4584, average loss: 1.3816
[10/24 18:34:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/24 18:34:05 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[10/24 18:39:21 visual_prompt]: 	Training 100/139. train loss: 1.1053,	0.9026 s / batch. (data: 3.25e-02). ETA=3:25:30, max mem: 7.6 GB 
[10/24 18:41:12 visual_prompt]: Epoch 2 / 100: avg data time: 2.20e+00, avg batch time: 3.0739, average train loss: 3.2443
[10/24 18:42:00 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4639, average loss: 1.4417
[10/24 18:42:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.02	
[10/24 18:42:00 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[10/24 18:47:09 visual_prompt]: 	Training 100/139. train loss: 6.5960,	0.8895 s / batch. (data: 2.55e-02). ETA=3:20:27, max mem: 7.6 GB 
[10/24 18:48:59 visual_prompt]: Epoch 3 / 100: avg data time: 2.14e+00, avg batch time: 3.0159, average train loss: 4.6322
[10/24 18:49:47 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4592, average loss: 11.9650
[10/24 18:49:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.97	
[10/24 18:49:47 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[10/24 18:54:50 visual_prompt]: 	Training 100/139. train loss: 3.2051,	0.8799 s / batch. (data: 1.15e-02). ETA=3:16:16, max mem: 7.6 GB 
[10/24 18:56:44 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 3.0037, average train loss: 5.4655
[10/24 18:57:32 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4613, average loss: 4.3391
[10/24 18:57:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.31	
[10/24 18:57:32 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[10/24 19:02:40 visual_prompt]: 	Training 100/139. train loss: 8.2045,	8.0453 s / batch. (data: 7.15e+00). ETA=1 day, 5:35:52, max mem: 7.6 GB 
[10/24 19:04:30 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0072, average train loss: 5.6450
[10/24 19:05:17 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.4605, average loss: 8.1188
[10/24 19:05:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.14	
[10/24 19:05:17 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[10/24 19:10:24 visual_prompt]: 	Training 100/139. train loss: 3.2201,	0.8720 s / batch. (data: 2.95e-04). ETA=3:10:27, max mem: 7.6 GB 
[10/24 19:12:16 visual_prompt]: Epoch 6 / 100: avg data time: 2.14e+00, avg batch time: 3.0129, average train loss: 7.0567
[10/24 19:13:05 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4583, average loss: 6.8879
[10/24 19:13:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.39	
[10/24 19:13:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[10/24 19:18:13 visual_prompt]: 	Training 100/139. train loss: 12.0577,	0.8677 s / batch. (data: 1.11e-02). ETA=3:07:30, max mem: 7.6 GB 
[10/24 19:20:03 visual_prompt]: Epoch 7 / 100: avg data time: 2.13e+00, avg batch time: 3.0079, average train loss: 12.7402
[10/24 19:20:50 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4594, average loss: 11.2956
[10/24 19:20:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.45	
[10/24 19:20:50 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[10/24 19:26:04 visual_prompt]: 	Training 100/139. train loss: 7.8469,	0.9120 s / batch. (data: 7.96e-03). ETA=3:14:58, max mem: 7.6 GB 
[10/24 19:27:54 visual_prompt]: Epoch 8 / 100: avg data time: 2.18e+00, avg batch time: 3.0500, average train loss: 9.8229
[10/24 19:28:42 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4617, average loss: 8.3744
[10/24 19:28:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.87	
[10/24 19:28:42 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[10/24 19:33:52 visual_prompt]: 	Training 100/139. train loss: 119.0844,	0.8610 s / batch. (data: 2.41e-04). ETA=3:02:03, max mem: 7.6 GB 
[10/24 19:35:39 visual_prompt]: Epoch 9 / 100: avg data time: 2.13e+00, avg batch time: 3.0052, average train loss: 22.5341
[10/24 19:36:27 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4635, average loss: 16.7284
[10/24 19:36:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.49	
[10/24 19:36:27 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[10/24 19:41:34 visual_prompt]: 	Training 100/139. train loss: 1.3925,	0.8787 s / batch. (data: 4.30e-04). ETA=3:03:46, max mem: 7.6 GB 
[10/24 19:43:24 visual_prompt]: Epoch 10 / 100: avg data time: 2.13e+00, avg batch time: 3.0012, average train loss: 28.1083
[10/24 19:44:11 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4620, average loss: 28.2718
[10/24 19:44:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.09	
[10/24 19:44:11 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[10/24 19:49:19 visual_prompt]: 	Training 100/139. train loss: 12.7750,	0.8882 s / batch. (data: 1.05e-02). ETA=3:03:42, max mem: 7.6 GB 
[10/24 19:51:09 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0022, average train loss: 23.6544
[10/24 19:51:56 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4620, average loss: 21.5683
[10/24 19:51:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.77	
[10/24 19:51:56 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[10/24 19:57:02 visual_prompt]: 	Training 100/139. train loss: 20.7124,	0.8772 s / batch. (data: 2.98e-04). ETA=2:59:24, max mem: 7.6 GB 
[10/24 19:58:53 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 2.9984, average train loss: 30.0003
[10/24 19:59:40 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4592, average loss: 42.8581
[10/24 19:59:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.62	
[10/24 19:59:40 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[10/24 20:04:49 visual_prompt]: 	Training 100/139. train loss: 13.6219,	0.8857 s / batch. (data: 5.35e-03). ETA=2:59:04, max mem: 7.6 GB 
[10/24 20:06:38 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0049, average train loss: 24.8046
[10/24 20:07:25 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4577, average loss: 7.9567
[10/24 20:07:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.65	
[10/24 20:07:25 visual_prompt]: Best epoch 13: best metric: -7.957
[10/24 20:07:25 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[10/24 20:12:36 visual_prompt]: 	Training 100/139. train loss: 7.9576,	0.8642 s / batch. (data: 2.94e-04). ETA=2:52:43, max mem: 7.6 GB 
[10/24 20:14:22 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 2.9999, average train loss: 20.5599
[10/24 20:15:10 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4592, average loss: 19.4327
[10/24 20:15:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.34	
[10/24 20:15:10 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[10/24 20:20:22 visual_prompt]: 	Training 100/139. train loss: 62.9787,	9.6040 s / batch. (data: 8.67e+00). ETA=1 day, 7:37:25, max mem: 7.6 GB 
[10/24 20:22:07 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0050, average train loss: 21.7238
[10/24 20:22:55 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.4572, average loss: 12.9086
[10/24 20:22:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.01	
[10/24 20:22:55 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[10/24 20:28:02 visual_prompt]: 	Training 100/139. train loss: 18.0735,	0.8800 s / batch. (data: 2.95e-04). ETA=2:51:49, max mem: 7.6 GB 
[10/24 20:29:52 visual_prompt]: Epoch 16 / 100: avg data time: 2.13e+00, avg batch time: 3.0006, average train loss: 19.0259
[10/24 20:30:39 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.4583, average loss: 7.6055
[10/24 20:30:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.03	
[10/24 20:30:39 visual_prompt]: Best epoch 16: best metric: -7.606
[10/24 20:30:39 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[10/24 20:35:46 visual_prompt]: 	Training 100/139. train loss: 9.7591,	2.6109 s / batch. (data: 1.73e+00). ETA=8:23:43, max mem: 7.6 GB 
[10/24 20:37:37 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 3.0027, average train loss: 15.8410
[10/24 20:38:24 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.4618, average loss: 32.1409
[10/24 20:38:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.74	
[10/24 20:38:24 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[10/24 20:43:32 visual_prompt]: 	Training 100/139. train loss: 15.4025,	1.3110 s / batch. (data: 4.47e-01). ETA=4:09:53, max mem: 7.6 GB 
[10/24 20:45:21 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0011, average train loss: 16.8019
[10/24 20:46:09 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4593, average loss: 0.6937
[10/24 20:46:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 51.14	
[10/24 20:46:09 visual_prompt]: Best epoch 18: best metric: -0.694
[10/24 20:46:09 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[10/24 20:51:22 visual_prompt]: 	Training 100/139. train loss: 28.2124,	11.0039 s / batch. (data: 1.01e+01). ETA=1 day, 10:32:02, max mem: 7.6 GB 
[10/24 20:53:07 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0074, average train loss: 19.5524
[10/24 20:53:54 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4614, average loss: 30.1253
[10/24 20:53:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.03	
[10/24 20:53:54 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[10/24 20:59:07 visual_prompt]: 	Training 100/139. train loss: 18.2020,	0.8801 s / batch. (data: 2.87e-04). ETA=2:43:41, max mem: 7.6 GB 
[10/24 21:01:01 visual_prompt]: Epoch 20 / 100: avg data time: 2.19e+00, avg batch time: 3.0651, average train loss: 23.7465
[10/24 21:01:48 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4621, average loss: 14.0408
[10/24 21:01:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.61	
[10/24 21:01:48 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[10/24 21:06:56 visual_prompt]: 	Training 100/139. train loss: 45.9024,	0.8990 s / batch. (data: 7.02e-03). ETA=2:45:07, max mem: 7.6 GB 
[10/24 21:08:57 visual_prompt]: Epoch 21 / 100: avg data time: 2.22e+00, avg batch time: 3.0882, average train loss: 14.9080
[10/24 21:09:44 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4612, average loss: 18.2179
[10/24 21:09:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.53	
[10/24 21:09:44 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[10/24 21:14:53 visual_prompt]: 	Training 100/139. train loss: 0.8686,	0.8802 s / batch. (data: 5.42e-03). ETA=2:39:37, max mem: 7.6 GB 
[10/24 21:16:46 visual_prompt]: Epoch 22 / 100: avg data time: 2.16e+00, avg batch time: 3.0332, average train loss: 19.6363
[10/24 21:17:34 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.4602, average loss: 4.5926
[10/24 21:17:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.17	
[10/24 21:17:34 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[10/24 21:22:39 visual_prompt]: 	Training 100/139. train loss: 9.3366,	1.2760 s / batch. (data: 3.83e-01). ETA=3:48:26, max mem: 7.6 GB 
[10/24 21:24:31 visual_prompt]: Epoch 23 / 100: avg data time: 2.13e+00, avg batch time: 3.0029, average train loss: 22.3012
[10/24 21:25:18 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4628, average loss: 17.7909
[10/24 21:25:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.30	
[10/24 21:25:18 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[10/24 21:30:29 visual_prompt]: 	Training 100/139. train loss: 0.9827,	0.9159 s / batch. (data: 5.43e-03). ETA=2:41:51, max mem: 7.6 GB 
[10/24 21:32:16 visual_prompt]: Epoch 24 / 100: avg data time: 2.13e+00, avg batch time: 3.0060, average train loss: 16.0912
[10/24 21:33:04 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.4590, average loss: 22.4444
[10/24 21:33:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.96	
[10/24 21:33:04 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[10/24 21:38:12 visual_prompt]: 	Training 100/139. train loss: 17.1888,	0.9128 s / batch. (data: 1.05e-02). ETA=2:39:11, max mem: 7.6 GB 
[10/24 21:40:01 visual_prompt]: Epoch 25 / 100: avg data time: 2.13e+00, avg batch time: 3.0025, average train loss: 20.7804
[10/24 21:40:49 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.4630, average loss: 3.6706
[10/24 21:40:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.02	
[10/24 21:40:49 visual_prompt]: Stopping early.
[10/24 21:40:49 visual_prompt]: Rank of current process: 0. World size: 1
[10/24 21:40:49 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/24 21:40:49 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/24 21:40:49 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/24 21:40:49 visual_prompt]: Training with config:
[10/24 21:40:49 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr10.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/24 21:40:49 visual_prompt]: Loading training data...
[10/24 21:40:49 visual_prompt]: Constructing mammo-cbis dataset train...
[10/24 21:40:49 visual_prompt]: Loading validation data...
[10/24 21:40:49 visual_prompt]: Constructing mammo-cbis dataset val...
[10/24 21:40:49 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/24 21:40:51 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/24 21:40:51 visual_prompt]: tuned percent:0.534
[10/24 21:40:52 visual_prompt]: Device used for model: 0
[10/24 21:40:52 visual_prompt]: Setting up Evaluator...
[10/24 21:40:52 visual_prompt]: Setting up Trainer...
[10/24 21:40:52 visual_prompt]: 	Setting up the optimizer...
[10/24 21:40:52 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/24 21:45:57 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8808 s / batch. (data: 2.97e-04). ETA=3:22:34, max mem: 7.6 GB 
[10/24 21:47:49 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 3.0012, average train loss: 1.3980
[10/24 21:48:36 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.4599, average loss: 1.3816
[10/24 21:48:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/24 21:48:36 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[10/24 21:53:43 visual_prompt]: 	Training 100/139. train loss: 1.1852,	0.8880 s / batch. (data: 3.02e-04). ETA=3:22:10, max mem: 7.6 GB 
[10/24 21:55:34 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 3.0044, average train loss: 3.1551
[10/24 21:56:21 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4637, average loss: 1.9061
[10/24 21:56:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.76	
[10/24 21:56:21 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[10/24 22:01:29 visual_prompt]: 	Training 100/139. train loss: 4.9029,	0.8829 s / batch. (data: 2.91e-04). ETA=3:18:59, max mem: 7.6 GB 
[10/24 22:03:18 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9999, average train loss: 3.5950
[10/24 22:04:06 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4604, average loss: 3.9901
[10/24 22:04:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.47	
[10/24 22:04:06 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[10/24 22:09:08 visual_prompt]: 	Training 100/139. train loss: 1.0698,	0.8827 s / batch. (data: 6.57e-03). ETA=3:16:52, max mem: 7.6 GB 
[10/24 22:11:03 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 2.9983, average train loss: 4.1824
[10/24 22:11:50 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4577, average loss: 4.6534
[10/24 22:11:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.22	
[10/24 22:11:50 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[10/24 22:17:01 visual_prompt]: 	Training 100/139. train loss: 1.0907,	11.6174 s / batch. (data: 1.07e+01). ETA=1 day, 18:44:21, max mem: 7.6 GB 
[10/24 22:18:48 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0043, average train loss: 9.1647
[10/24 22:19:35 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.4620, average loss: 2.8540
[10/24 22:19:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.14	
[10/24 22:19:35 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[10/24 22:24:41 visual_prompt]: 	Training 100/139. train loss: 10.4912,	0.8840 s / batch. (data: 2.83e-04). ETA=3:13:04, max mem: 7.6 GB 
[10/24 22:26:36 visual_prompt]: Epoch 6 / 100: avg data time: 2.14e+00, avg batch time: 3.0240, average train loss: 10.7795
[10/24 22:27:25 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4599, average loss: 28.2096
[10/24 22:27:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.68	
[10/24 22:27:25 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[10/24 22:32:34 visual_prompt]: 	Training 100/139. train loss: 17.9136,	0.8933 s / batch. (data: 2.81e-03). ETA=3:13:02, max mem: 7.6 GB 
[10/24 22:34:23 visual_prompt]: Epoch 7 / 100: avg data time: 2.13e+00, avg batch time: 3.0058, average train loss: 11.8818
[10/24 22:35:10 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.4611, average loss: 15.2616
[10/24 22:35:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.04	
[10/24 22:35:10 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[10/24 22:40:19 visual_prompt]: 	Training 100/139. train loss: 1.6386,	0.9206 s / batch. (data: 3.22e-02). ETA=3:16:49, max mem: 7.6 GB 
[10/24 22:42:09 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0134, average train loss: 14.5041
[10/24 22:42:56 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.4601, average loss: 57.7887
[10/24 22:42:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.00	
[10/24 22:42:56 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[10/24 22:48:06 visual_prompt]: 	Training 100/139. train loss: 48.6009,	0.8762 s / batch. (data: 2.89e-04). ETA=3:05:17, max mem: 7.6 GB 
[10/24 22:49:53 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9995, average train loss: 20.1452
[10/24 22:50:41 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4642, average loss: 7.6488
[10/24 22:50:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.81	
[10/24 22:50:41 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[10/24 22:55:47 visual_prompt]: 	Training 100/139. train loss: 2.1718,	0.8768 s / batch. (data: 1.07e-02). ETA=3:03:23, max mem: 7.6 GB 
[10/24 22:57:37 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9946, average train loss: 19.7395
[10/24 22:58:24 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4634, average loss: 15.0150
[10/24 22:58:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.86	
[10/24 22:58:24 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[10/24 23:03:32 visual_prompt]: 	Training 100/139. train loss: 19.7278,	0.8662 s / batch. (data: 2.93e-04). ETA=2:59:09, max mem: 7.6 GB 
[10/24 23:05:23 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0118, average train loss: 13.5729
[10/24 23:06:10 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4622, average loss: 3.3195
[10/24 23:06:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.87	
[10/24 23:06:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[10/24 23:11:19 visual_prompt]: 	Training 100/139. train loss: 0.8904,	0.8697 s / batch. (data: 2.86e-04). ETA=2:57:52, max mem: 7.6 GB 
[10/24 23:13:07 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9955, average train loss: 13.7975
[10/24 23:13:54 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.4595, average loss: 4.9177
[10/24 23:13:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.49	
[10/24 23:13:54 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[10/24 23:19:03 visual_prompt]: 	Training 100/139. train loss: 8.9004,	0.8891 s / batch. (data: 2.06e-02). ETA=2:59:46, max mem: 7.6 GB 
[10/24 23:20:56 visual_prompt]: Epoch 13 / 100: avg data time: 2.15e+00, avg batch time: 3.0324, average train loss: 9.5108
[10/24 23:21:43 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4661, average loss: 7.5658
[10/24 23:21:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.81	
[10/24 23:21:43 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[10/24 23:26:53 visual_prompt]: 	Training 100/139. train loss: 132.1361,	0.8852 s / batch. (data: 5.43e-03). ETA=2:56:56, max mem: 7.6 GB 
[10/24 23:28:40 visual_prompt]: Epoch 14 / 100: avg data time: 2.12e+00, avg batch time: 3.0003, average train loss: 27.6424
[10/24 23:29:27 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.4626, average loss: 27.1823
[10/24 23:29:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.23	
[10/24 23:29:27 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[10/24 23:34:41 visual_prompt]: 	Training 100/139. train loss: 4.5876,	9.2800 s / batch. (data: 8.40e+00). ETA=1 day, 6:33:24, max mem: 7.6 GB 
[10/24 23:36:25 visual_prompt]: Epoch 15 / 100: avg data time: 2.12e+00, avg batch time: 3.0018, average train loss: 14.7396
[10/24 23:37:12 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4646, average loss: 1.9299
[10/24 23:37:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.79	
[10/24 23:37:12 visual_prompt]: Best epoch 15: best metric: -1.930
[10/24 23:37:12 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[10/24 23:42:28 visual_prompt]: 	Training 100/139. train loss: 38.2253,	0.8879 s / batch. (data: 7.98e-03). ETA=2:53:22, max mem: 7.6 GB 
[10/24 23:44:18 visual_prompt]: Epoch 16 / 100: avg data time: 2.19e+00, avg batch time: 3.0645, average train loss: 16.4854
[10/24 23:45:05 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.4601, average loss: 1.2389
[10/24 23:45:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.53	
[10/24 23:45:05 visual_prompt]: Best epoch 16: best metric: -1.239
[10/24 23:45:05 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[10/24 23:50:12 visual_prompt]: 	Training 100/139. train loss: 21.5891,	2.3720 s / batch. (data: 1.48e+00). ETA=7:37:38, max mem: 7.6 GB 
[10/24 23:52:02 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 2.9980, average train loss: 14.8741
[10/24 23:52:49 visual_prompt]: Inference (val):avg data time: 1.48e-03, avg batch time: 0.4632, average loss: 0.7350
[10/24 23:52:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 45.15	
[10/24 23:52:49 visual_prompt]: Best epoch 17: best metric: -0.735
[10/24 23:52:49 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[10/24 23:57:57 visual_prompt]: 	Training 100/139. train loss: 35.5515,	2.2518 s / batch. (data: 1.38e+00). ETA=7:09:13, max mem: 7.6 GB 
[10/24 23:59:47 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0022, average train loss: 18.6060
[10/25 00:00:34 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4613, average loss: 42.3565
[10/25 00:00:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.96	
[10/25 00:00:34 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[10/25 00:05:47 visual_prompt]: 	Training 100/139. train loss: 8.0253,	11.1054 s / batch. (data: 1.02e+01). ETA=1 day, 10:51:09, max mem: 7.6 GB 
[10/25 00:07:32 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0030, average train loss: 17.4480
[10/25 00:08:19 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4606, average loss: 29.1852
[10/25 00:08:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.70	
[10/25 00:08:19 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[10/25 00:13:29 visual_prompt]: 	Training 100/139. train loss: 15.8891,	0.8805 s / batch. (data: 4.20e-04). ETA=2:43:45, max mem: 7.6 GB 
[10/25 00:15:20 visual_prompt]: Epoch 20 / 100: avg data time: 2.15e+00, avg batch time: 3.0264, average train loss: 18.2950
[10/25 00:16:07 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4648, average loss: 36.7540
[10/25 00:16:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.72	
[10/25 00:16:07 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[10/25 00:21:21 visual_prompt]: 	Training 100/139. train loss: 18.4329,	0.8983 s / batch. (data: 3.44e-04). ETA=2:44:59, max mem: 7.6 GB 
[10/25 00:23:16 visual_prompt]: Epoch 21 / 100: avg data time: 2.21e+00, avg batch time: 3.0865, average train loss: 17.2436
[10/25 00:24:05 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4673, average loss: 7.5111
[10/25 00:24:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.56	
[10/25 00:24:05 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[10/25 00:29:22 visual_prompt]: 	Training 100/139. train loss: 19.1381,	0.8835 s / batch. (data: 2.98e-04). ETA=2:40:12, max mem: 7.6 GB 
[10/25 00:31:19 visual_prompt]: Epoch 22 / 100: avg data time: 2.24e+00, avg batch time: 3.1222, average train loss: 27.6310
[10/25 00:32:08 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.4596, average loss: 26.6849
[10/25 00:32:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.83	
[10/25 00:32:08 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[10/25 00:37:13 visual_prompt]: 	Training 100/139. train loss: 8.9935,	0.8906 s / batch. (data: 7.26e-03). ETA=2:39:26, max mem: 7.6 GB 
[10/25 00:39:06 visual_prompt]: Epoch 23 / 100: avg data time: 2.13e+00, avg batch time: 3.0070, average train loss: 17.1603
[10/25 00:39:53 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4617, average loss: 9.3571
[10/25 00:39:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.27	
[10/25 00:39:53 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[10/25 00:45:06 visual_prompt]: 	Training 100/139. train loss: 10.6816,	0.8733 s / batch. (data: 2.95e-04). ETA=2:34:19, max mem: 7.6 GB 
[10/25 00:46:53 visual_prompt]: Epoch 24 / 100: avg data time: 2.14e+00, avg batch time: 3.0175, average train loss: 19.0851
[10/25 00:47:40 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4661, average loss: 49.4330
[10/25 00:47:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.73	
[10/25 00:47:40 visual_prompt]: Stopping early.
[10/25 00:47:40 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 00:47:40 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 00:47:40 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 00:47:40 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 00:47:40 visual_prompt]: Training with config:
[10/25 00:47:40 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr10.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 00:47:40 visual_prompt]: Loading training data...
[10/25 00:47:40 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 00:47:41 visual_prompt]: Loading validation data...
[10/25 00:47:41 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 00:47:41 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/25 00:47:50 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/25 00:47:50 visual_prompt]: tuned percent:0.534
[10/25 00:47:50 visual_prompt]: Device used for model: 0
[10/25 00:47:50 visual_prompt]: Setting up Evaluator...
[10/25 00:47:50 visual_prompt]: Setting up Trainer...
[10/25 00:47:50 visual_prompt]: 	Setting up the optimizer...
[10/25 00:47:50 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 00:53:07 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8647 s / batch. (data: 3.12e-04). ETA=3:18:52, max mem: 7.6 GB 
[10/25 00:55:07 visual_prompt]: Epoch 1 / 100: avg data time: 2.27e+00, avg batch time: 3.1434, average train loss: 1.3980
[10/25 00:55:56 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.4596, average loss: 1.3816
[10/25 00:55:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/25 00:55:56 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[10/25 01:01:10 visual_prompt]: 	Training 100/139. train loss: 1.0883,	0.8840 s / batch. (data: 2.88e-04). ETA=3:21:16, max mem: 7.6 GB 
[10/25 01:03:06 visual_prompt]: Epoch 2 / 100: avg data time: 2.22e+00, avg batch time: 3.0913, average train loss: 3.1619
[10/25 01:03:54 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4605, average loss: 0.7238
[10/25 01:03:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 55.33	
[10/25 01:03:54 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[10/25 01:08:59 visual_prompt]: 	Training 100/139. train loss: 10.6819,	0.8886 s / batch. (data: 1.05e-02). ETA=3:20:15, max mem: 7.6 GB 
[10/25 01:10:50 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9968, average train loss: 3.8436
[10/25 01:11:38 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.4581, average loss: 6.9468
[10/25 01:11:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.66	
[10/25 01:11:38 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[10/25 01:16:39 visual_prompt]: 	Training 100/139. train loss: 4.2293,	0.8800 s / batch. (data: 3.04e-04). ETA=3:16:16, max mem: 7.6 GB 
[10/25 01:18:35 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 3.0049, average train loss: 5.8712
[10/25 01:19:23 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4614, average loss: 1.0664
[10/25 01:19:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.72	
[10/25 01:19:23 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[10/25 01:24:33 visual_prompt]: 	Training 100/139. train loss: 1.0970,	11.6440 s / batch. (data: 1.08e+01). ETA=1 day, 18:50:13, max mem: 7.6 GB 
[10/25 01:26:21 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0042, average train loss: 4.3635
[10/25 01:27:08 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.4615, average loss: 2.8609
[10/25 01:27:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.78	
[10/25 01:27:08 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[10/25 01:32:13 visual_prompt]: 	Training 100/139. train loss: 3.5966,	2.2000 s / batch. (data: 1.33e+00). ETA=8:00:30, max mem: 7.6 GB 
[10/25 01:34:06 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 3.0087, average train loss: 5.2356
[10/25 01:34:57 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.4607, average loss: 0.7460
[10/25 01:34:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.95	
[10/25 01:34:57 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[10/25 01:40:05 visual_prompt]: 	Training 100/139. train loss: 3.1820,	0.8883 s / batch. (data: 2.94e-04). ETA=3:11:58, max mem: 7.6 GB 
[10/25 01:41:54 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9952, average train loss: 8.5311
[10/25 01:42:41 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4588, average loss: 3.0423
[10/25 01:42:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.15	
[10/25 01:42:41 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[10/25 01:47:49 visual_prompt]: 	Training 100/139. train loss: 22.6924,	0.8972 s / batch. (data: 2.06e-02). ETA=3:11:47, max mem: 7.6 GB 
[10/25 01:49:38 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0014, average train loss: 13.3780
[10/25 01:50:25 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.4617, average loss: 37.5027
[10/25 01:50:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.20	
[10/25 01:50:25 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[10/25 01:55:34 visual_prompt]: 	Training 100/139. train loss: 6.4119,	0.8960 s / batch. (data: 2.99e-04). ETA=3:09:28, max mem: 7.6 GB 
[10/25 01:57:22 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9976, average train loss: 10.3118
[10/25 01:58:09 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4587, average loss: 15.0629
[10/25 01:58:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.45	
[10/25 01:58:09 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[10/25 02:03:19 visual_prompt]: 	Training 100/139. train loss: 2.5603,	0.8930 s / batch. (data: 3.10e-04). ETA=3:06:46, max mem: 7.6 GB 
[10/25 02:05:14 visual_prompt]: Epoch 10 / 100: avg data time: 2.18e+00, avg batch time: 3.0538, average train loss: 12.7021
[10/25 02:06:03 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.4593, average loss: 3.1271
[10/25 02:06:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.20	
[10/25 02:06:03 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[10/25 02:11:20 visual_prompt]: 	Training 100/139. train loss: 24.4342,	0.8951 s / batch. (data: 2.70e-04). ETA=3:05:07, max mem: 7.6 GB 
[10/25 02:13:18 visual_prompt]: Epoch 11 / 100: avg data time: 2.26e+00, avg batch time: 3.1279, average train loss: 15.6081
[10/25 02:14:05 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4577, average loss: 26.2365
[10/25 02:14:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.08	
[10/25 02:14:05 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[10/25 02:19:14 visual_prompt]: 	Training 100/139. train loss: 21.7574,	2.4742 s / batch. (data: 1.61e+00). ETA=8:26:00, max mem: 7.6 GB 
[10/25 02:21:03 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 3.0030, average train loss: 8.5438
[10/25 02:21:50 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.4582, average loss: 32.0082
[10/25 02:21:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.25	
[10/25 02:21:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[10/25 02:26:59 visual_prompt]: 	Training 100/139. train loss: 14.0434,	0.8687 s / batch. (data: 2.81e-04). ETA=2:55:39, max mem: 7.6 GB 
[10/25 02:28:48 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0036, average train loss: 11.4995
[10/25 02:29:35 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4584, average loss: 8.7845
[10/25 02:29:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.93	
[10/25 02:29:35 visual_prompt]: Best epoch 13: best metric: -8.785
[10/25 02:29:35 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[10/25 02:34:45 visual_prompt]: 	Training 100/139. train loss: 35.1117,	0.8917 s / batch. (data: 5.41e-03). ETA=2:58:14, max mem: 7.6 GB 
[10/25 02:36:32 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0021, average train loss: 12.1280
[10/25 02:37:20 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4576, average loss: 7.4098
[10/25 02:37:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.30	
[10/25 02:37:20 visual_prompt]: Best epoch 14: best metric: -7.410
[10/25 02:37:20 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[10/25 02:42:32 visual_prompt]: 	Training 100/139. train loss: 5.2567,	9.4509 s / batch. (data: 8.58e+00). ETA=1 day, 7:07:10, max mem: 7.6 GB 
[10/25 02:44:17 visual_prompt]: Epoch 15 / 100: avg data time: 2.12e+00, avg batch time: 2.9977, average train loss: 10.6113
[10/25 02:45:04 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.4581, average loss: 2.2799
[10/25 02:45:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.19	
[10/25 02:45:04 visual_prompt]: Best epoch 15: best metric: -2.280
[10/25 02:45:04 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[10/25 02:50:10 visual_prompt]: 	Training 100/139. train loss: 16.4395,	0.8883 s / batch. (data: 3.07e-04). ETA=2:53:26, max mem: 7.6 GB 
[10/25 02:52:01 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9977, average train loss: 9.3767
[10/25 02:52:48 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4592, average loss: 4.8307
[10/25 02:52:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.27	
[10/25 02:52:48 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[10/25 02:57:55 visual_prompt]: 	Training 100/139. train loss: 3.4124,	2.6720 s / batch. (data: 1.78e+00). ETA=8:35:30, max mem: 7.6 GB 
[10/25 02:59:45 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 2.9978, average train loss: 5.8636
[10/25 03:00:32 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4608, average loss: 8.2371
[10/25 03:00:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.44	
[10/25 03:00:32 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[10/25 03:05:41 visual_prompt]: 	Training 100/139. train loss: 1.5541,	4.1880 s / batch. (data: 3.31e+00). ETA=13:18:18, max mem: 7.6 GB 
[10/25 03:07:29 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0021, average train loss: 5.3816
[10/25 03:08:16 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4599, average loss: 11.0985
[10/25 03:08:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.96	
[10/25 03:08:16 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[10/25 03:13:30 visual_prompt]: 	Training 100/139. train loss: 3.4718,	11.2488 s / batch. (data: 1.03e+01). ETA=1 day, 11:18:09, max mem: 7.6 GB 
[10/25 03:15:14 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0044, average train loss: 6.7311
[10/25 03:16:01 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4621, average loss: 3.9113
[10/25 03:16:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.39	
[10/25 03:16:01 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[10/25 03:21:08 visual_prompt]: 	Training 100/139. train loss: 1.3600,	0.8614 s / batch. (data: 3.01e-04). ETA=2:40:12, max mem: 7.6 GB 
[10/25 03:22:58 visual_prompt]: Epoch 20 / 100: avg data time: 2.12e+00, avg batch time: 2.9968, average train loss: 7.0603
[10/25 03:23:45 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.4626, average loss: 6.9359
[10/25 03:23:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.56	
[10/25 03:23:45 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[10/25 03:28:51 visual_prompt]: 	Training 100/139. train loss: 1.7023,	0.8840 s / batch. (data: 2.99e-04). ETA=2:42:21, max mem: 7.6 GB 
[10/25 03:30:42 visual_prompt]: Epoch 21 / 100: avg data time: 2.12e+00, avg batch time: 2.9959, average train loss: 9.6645
[10/25 03:31:29 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.4564, average loss: 6.0789
[10/25 03:31:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.38	
[10/25 03:31:29 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[10/25 03:36:32 visual_prompt]: 	Training 100/139. train loss: 7.8817,	0.8660 s / batch. (data: 5.40e-03). ETA=2:37:02, max mem: 7.6 GB 
[10/25 03:38:26 visual_prompt]: Epoch 22 / 100: avg data time: 2.12e+00, avg batch time: 2.9981, average train loss: 6.4586
[10/25 03:39:13 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4576, average loss: 4.7314
[10/25 03:39:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.94	
[10/25 03:39:13 visual_prompt]: Stopping early.
[10/25 03:39:14 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 03:39:14 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 03:39:14 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 03:39:14 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 03:39:14 visual_prompt]: Training with config:
[10/25 03:39:14 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr5.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 03:39:14 visual_prompt]: Loading training data...
[10/25 03:39:14 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 03:39:14 visual_prompt]: Loading validation data...
[10/25 03:39:14 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 03:39:14 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/25 03:39:23 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/25 03:39:23 visual_prompt]: tuned percent:0.534
[10/25 03:39:23 visual_prompt]: Device used for model: 0
[10/25 03:39:23 visual_prompt]: Setting up Evaluator...
[10/25 03:39:23 visual_prompt]: Setting up Trainer...
[10/25 03:39:23 visual_prompt]: 	Setting up the optimizer...
[10/25 03:39:23 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 03:44:28 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.9083 s / batch. (data: 1.60e-02). ETA=3:28:54, max mem: 7.6 GB 
[10/25 03:46:19 visual_prompt]: Epoch 1 / 100: avg data time: 2.11e+00, avg batch time: 2.9951, average train loss: 1.3980
[10/25 03:47:07 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4629, average loss: 1.3816
[10/25 03:47:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/25 03:47:07 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[10/25 03:52:11 visual_prompt]: 	Training 100/139. train loss: 0.7006,	0.8851 s / batch. (data: 2.61e-04). ETA=3:21:31, max mem: 7.6 GB 
[10/25 03:54:03 visual_prompt]: Epoch 2 / 100: avg data time: 2.12e+00, avg batch time: 2.9968, average train loss: 1.6556
[10/25 03:54:51 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4674, average loss: 1.9795
[10/25 03:54:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.74	
[10/25 03:54:51 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[10/25 03:59:57 visual_prompt]: 	Training 100/139. train loss: 2.1354,	0.8839 s / batch. (data: 2.83e-04). ETA=3:19:11, max mem: 7.6 GB 
[10/25 04:01:48 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9982, average train loss: 1.1669
[10/25 04:02:35 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.4658, average loss: 4.5155
[10/25 04:02:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.64	
[10/25 04:02:35 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[10/25 04:07:38 visual_prompt]: 	Training 100/139. train loss: 1.5440,	0.8982 s / batch. (data: 1.04e-02). ETA=3:20:20, max mem: 7.6 GB 
[10/25 04:09:33 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 3.0050, average train loss: 2.5864
[10/25 04:10:20 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4674, average loss: 14.6525
[10/25 04:10:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.71	
[10/25 04:10:20 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[10/25 04:15:29 visual_prompt]: 	Training 100/139. train loss: 5.3338,	11.4087 s / batch. (data: 1.05e+01). ETA=1 day, 17:58:16, max mem: 7.6 GB 
[10/25 04:17:18 visual_prompt]: Epoch 5 / 100: avg data time: 2.12e+00, avg batch time: 3.0028, average train loss: 3.4845
[10/25 04:18:05 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4619, average loss: 0.7828
[10/25 04:18:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.86	
[10/25 04:18:05 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[10/25 04:23:11 visual_prompt]: 	Training 100/139. train loss: 4.5417,	0.8680 s / batch. (data: 2.56e-04). ETA=3:09:35, max mem: 7.6 GB 
[10/25 04:25:03 visual_prompt]: Epoch 6 / 100: avg data time: 2.12e+00, avg batch time: 3.0024, average train loss: 4.6685
[10/25 04:25:50 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4701, average loss: 3.7478
[10/25 04:25:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.21	
[10/25 04:25:50 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[10/25 04:31:01 visual_prompt]: 	Training 100/139. train loss: 1.9542,	0.8948 s / batch. (data: 1.56e-02). ETA=3:13:21, max mem: 7.6 GB 
[10/25 04:32:48 visual_prompt]: Epoch 7 / 100: avg data time: 2.13e+00, avg batch time: 3.0089, average train loss: 5.4998
[10/25 04:33:36 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.4666, average loss: 0.7248
[10/25 04:33:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.86	
[10/25 04:33:36 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[10/25 04:38:44 visual_prompt]: 	Training 100/139. train loss: 0.7423,	0.8948 s / batch. (data: 1.04e-02). ETA=3:11:18, max mem: 7.6 GB 
[10/25 04:40:34 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0121, average train loss: 6.5914
[10/25 04:41:22 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4641, average loss: 24.2060
[10/25 04:41:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.92	
[10/25 04:41:22 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[10/25 04:46:31 visual_prompt]: 	Training 100/139. train loss: 7.2980,	0.8800 s / batch. (data: 3.04e-04). ETA=3:06:05, max mem: 7.6 GB 
[10/25 04:48:19 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9982, average train loss: 9.3622
[10/25 04:49:06 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.4646, average loss: 3.9607
[10/25 04:49:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.29	
[10/25 04:49:06 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[10/25 04:54:12 visual_prompt]: 	Training 100/139. train loss: 6.6298,	0.8880 s / batch. (data: 3.16e-04). ETA=3:05:43, max mem: 7.6 GB 
[10/25 04:56:02 visual_prompt]: Epoch 10 / 100: avg data time: 2.11e+00, avg batch time: 2.9953, average train loss: 9.8768
[10/25 04:56:50 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4680, average loss: 21.7512
[10/25 04:56:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.50	
[10/25 04:56:50 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[10/25 05:01:55 visual_prompt]: 	Training 100/139. train loss: 8.8773,	0.8806 s / batch. (data: 5.42e-03). ETA=3:02:08, max mem: 7.6 GB 
[10/25 05:03:47 visual_prompt]: Epoch 11 / 100: avg data time: 2.12e+00, avg batch time: 2.9978, average train loss: 10.6028
[10/25 05:04:34 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4660, average loss: 4.2865
[10/25 05:04:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.89	
[10/25 05:04:34 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[10/25 05:09:40 visual_prompt]: 	Training 100/139. train loss: 8.6870,	0.8692 s / batch. (data: 2.93e-04). ETA=2:57:46, max mem: 7.6 GB 
[10/25 05:11:31 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 3.0001, average train loss: 11.2337
[10/25 05:12:18 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.4649, average loss: 23.7712
[10/25 05:12:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.97	
[10/25 05:12:18 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[10/25 05:17:28 visual_prompt]: 	Training 100/139. train loss: 4.8808,	0.8840 s / batch. (data: 7.95e-03). ETA=2:58:44, max mem: 7.6 GB 
[10/25 05:19:16 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0055, average train loss: 10.7818
[10/25 05:20:03 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4665, average loss: 5.2187
[10/25 05:20:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.23	
[10/25 05:20:03 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[10/25 05:25:13 visual_prompt]: 	Training 100/139. train loss: 24.3979,	0.9091 s / batch. (data: 1.05e-02). ETA=3:01:42, max mem: 7.6 GB 
[10/25 05:27:00 visual_prompt]: Epoch 14 / 100: avg data time: 2.12e+00, avg batch time: 2.9986, average train loss: 14.7520
[10/25 05:27:48 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.4656, average loss: 29.2766
[10/25 05:27:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.62	
[10/25 05:27:48 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[10/25 05:33:02 visual_prompt]: 	Training 100/139. train loss: 9.5772,	9.2807 s / batch. (data: 8.40e+00). ETA=1 day, 6:33:33, max mem: 7.6 GB 
[10/25 05:34:46 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0075, average train loss: 14.7950
[10/25 05:35:33 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4641, average loss: 10.2832
[10/25 05:35:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.86	
[10/25 05:35:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[10/25 05:40:41 visual_prompt]: 	Training 100/139. train loss: 12.1289,	0.8794 s / batch. (data: 8.50e-03). ETA=2:51:41, max mem: 7.6 GB 
[10/25 05:42:31 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 3.0051, average train loss: 9.3583
[10/25 05:43:19 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4653, average loss: 9.4685
[10/25 05:43:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.20	
[10/25 05:43:19 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[10/25 05:48:27 visual_prompt]: 	Training 100/139. train loss: 15.6044,	4.7719 s / batch. (data: 3.90e+00). ETA=15:20:40, max mem: 7.6 GB 
[10/25 05:50:16 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 3.0047, average train loss: 12.1490
[10/25 05:51:04 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.4609, average loss: 7.3088
[10/25 05:51:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.67	
[10/25 05:51:04 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[10/25 05:56:12 visual_prompt]: 	Training 100/139. train loss: 8.8272,	2.8512 s / batch. (data: 1.96e+00). ETA=9:03:28, max mem: 7.6 GB 
[10/25 05:58:02 visual_prompt]: Epoch 18 / 100: avg data time: 2.12e+00, avg batch time: 3.0076, average train loss: 11.3804
[10/25 05:58:49 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4670, average loss: 9.6468
[10/25 05:58:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.54	
[10/25 05:58:49 visual_prompt]: Stopping early.
[10/25 05:58:49 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 05:58:49 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 05:58:49 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 05:58:49 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 05:58:49 visual_prompt]: Training with config:
[10/25 05:58:49 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr5.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 05:58:49 visual_prompt]: Loading training data...
[10/25 05:58:49 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 05:58:49 visual_prompt]: Loading validation data...
[10/25 05:58:49 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 05:58:49 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/25 05:58:52 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/25 05:58:52 visual_prompt]: tuned percent:0.534
[10/25 05:58:52 visual_prompt]: Device used for model: 0
[10/25 05:58:52 visual_prompt]: Setting up Evaluator...
[10/25 05:58:52 visual_prompt]: Setting up Trainer...
[10/25 05:58:52 visual_prompt]: 	Setting up the optimizer...
[10/25 05:58:52 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 06:03:59 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8970 s / batch. (data: 5.83e-03). ETA=3:26:17, max mem: 7.6 GB 
[10/25 06:05:49 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9967, average train loss: 1.3980
[10/25 06:06:36 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4650, average loss: 1.3816
[10/25 06:06:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/25 06:06:36 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[10/25 06:11:42 visual_prompt]: 	Training 100/139. train loss: 0.8087,	0.8720 s / batch. (data: 2.46e-04). ETA=3:18:32, max mem: 7.6 GB 
[10/25 06:13:32 visual_prompt]: Epoch 2 / 100: avg data time: 2.11e+00, avg batch time: 2.9945, average train loss: 2.0714
[10/25 06:14:20 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4655, average loss: 0.8581
[10/25 06:14:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.43	
[10/25 06:14:20 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[10/25 06:19:27 visual_prompt]: 	Training 100/139. train loss: 1.5074,	0.8774 s / batch. (data: 2.80e-04). ETA=3:17:44, max mem: 7.6 GB 
[10/25 06:21:17 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 3.0032, average train loss: 1.2101
[10/25 06:22:05 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4635, average loss: 1.5898
[10/25 06:22:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.82	
[10/25 06:22:05 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[10/25 06:27:07 visual_prompt]: 	Training 100/139. train loss: 0.6392,	0.9160 s / batch. (data: 3.96e-03). ETA=3:24:19, max mem: 7.6 GB 
[10/25 06:29:02 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 3.0050, average train loss: 1.4111
[10/25 06:29:50 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.4628, average loss: 0.9740
[10/25 06:29:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.21	
[10/25 06:29:50 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[10/25 06:35:00 visual_prompt]: 	Training 100/139. train loss: 1.2430,	11.6800 s / batch. (data: 1.08e+01). ETA=1 day, 18:58:09, max mem: 7.6 GB 
[10/25 06:36:47 visual_prompt]: Epoch 5 / 100: avg data time: 2.12e+00, avg batch time: 3.0037, average train loss: 2.5874
[10/25 06:37:35 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4574, average loss: 4.8940
[10/25 06:37:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.87	
[10/25 06:37:35 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[10/25 06:42:40 visual_prompt]: 	Training 100/139. train loss: 0.8172,	0.8641 s / batch. (data: 2.92e-04). ETA=3:08:43, max mem: 7.6 GB 
[10/25 06:44:32 visual_prompt]: Epoch 6 / 100: avg data time: 2.12e+00, avg batch time: 2.9999, average train loss: 3.3459
[10/25 06:45:19 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4625, average loss: 2.0606
[10/25 06:45:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.59	
[10/25 06:45:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[10/25 06:50:27 visual_prompt]: 	Training 100/139. train loss: 1.8868,	0.9280 s / batch. (data: 7.95e-03). ETA=3:20:32, max mem: 7.6 GB 
[10/25 06:52:16 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 3.0000, average train loss: 4.9162
[10/25 06:53:03 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.4614, average loss: 1.1096
[10/25 06:53:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.72	
[10/25 06:53:03 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[10/25 06:58:11 visual_prompt]: 	Training 100/139. train loss: 9.1884,	0.9290 s / batch. (data: 9.01e-03). ETA=3:18:36, max mem: 7.6 GB 
[10/25 07:00:02 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0076, average train loss: 7.1010
[10/25 07:00:49 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4615, average loss: 5.6222
[10/25 07:00:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.45	
[10/25 07:00:49 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[10/25 07:05:59 visual_prompt]: 	Training 100/139. train loss: 7.0241,	0.8795 s / batch. (data: 6.98e-03). ETA=3:05:58, max mem: 7.6 GB 
[10/25 07:07:46 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 3.0024, average train loss: 6.3126
[10/25 07:08:34 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4645, average loss: 8.1286
[10/25 07:08:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.54	
[10/25 07:08:34 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[10/25 07:13:40 visual_prompt]: 	Training 100/139. train loss: 7.5828,	1.8119 s / batch. (data: 9.33e-01). ETA=6:18:57, max mem: 7.6 GB 
[10/25 07:15:30 visual_prompt]: Epoch 10 / 100: avg data time: 2.11e+00, avg batch time: 2.9959, average train loss: 11.7081
[10/25 07:16:18 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.4635, average loss: 2.8508
[10/25 07:16:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.51	
[10/25 07:16:18 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[10/25 07:21:25 visual_prompt]: 	Training 100/139. train loss: 20.9642,	0.8862 s / batch. (data: 1.20e-02). ETA=3:03:17, max mem: 7.6 GB 
[10/25 07:23:15 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0052, average train loss: 8.1778
[10/25 07:24:03 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.4647, average loss: 11.5839
[10/25 07:24:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.25	
[10/25 07:24:03 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[10/25 07:29:10 visual_prompt]: 	Training 100/139. train loss: 11.9324,	1.5312 s / batch. (data: 6.50e-01). ETA=5:13:09, max mem: 7.6 GB 
[10/25 07:31:00 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9989, average train loss: 12.8001
[10/25 07:31:47 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4611, average loss: 22.5790
[10/25 07:31:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.94	
[10/25 07:31:47 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[10/25 07:36:57 visual_prompt]: 	Training 100/139. train loss: 8.3765,	0.8900 s / batch. (data: 5.54e-03). ETA=2:59:57, max mem: 7.6 GB 
[10/25 07:38:45 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0094, average train loss: 11.4449
[10/25 07:39:33 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.4655, average loss: 18.0561
[10/25 07:39:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.83	
[10/25 07:39:33 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[10/25 07:44:44 visual_prompt]: 	Training 100/139. train loss: 2.3247,	0.8952 s / batch. (data: 7.22e-04). ETA=2:58:55, max mem: 7.6 GB 
[10/25 07:46:30 visual_prompt]: Epoch 14 / 100: avg data time: 2.12e+00, avg batch time: 3.0020, average train loss: 10.4090
[10/25 07:47:18 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4635, average loss: 11.7436
[10/25 07:47:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.30	
[10/25 07:47:18 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[10/25 07:52:31 visual_prompt]: 	Training 100/139. train loss: 11.0986,	9.3401 s / batch. (data: 8.46e+00). ETA=1 day, 6:45:18, max mem: 7.6 GB 
[10/25 07:54:16 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0072, average train loss: 13.3230
[10/25 07:55:03 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4628, average loss: 8.0519
[10/25 07:55:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.18	
[10/25 07:55:03 visual_prompt]: Best epoch 15: best metric: -8.052
[10/25 07:55:03 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[10/25 08:00:11 visual_prompt]: 	Training 100/139. train loss: 36.6545,	0.8621 s / batch. (data: 5.41e-03). ETA=2:48:19, max mem: 7.6 GB 
[10/25 08:02:00 visual_prompt]: Epoch 16 / 100: avg data time: 2.11e+00, avg batch time: 2.9955, average train loss: 10.2832
[10/25 08:02:47 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4658, average loss: 5.4835
[10/25 08:02:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.71	
[10/25 08:02:47 visual_prompt]: Best epoch 16: best metric: -5.483
[10/25 08:02:47 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[10/25 08:07:54 visual_prompt]: 	Training 100/139. train loss: 25.1091,	3.0412 s / batch. (data: 2.15e+00). ETA=9:46:44, max mem: 7.6 GB 
[10/25 08:09:44 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 3.0008, average train loss: 9.1313
[10/25 08:10:31 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4614, average loss: 3.2099
[10/25 08:10:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.00	
[10/25 08:10:31 visual_prompt]: Best epoch 17: best metric: -3.210
[10/25 08:10:32 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[10/25 08:15:40 visual_prompt]: 	Training 100/139. train loss: 10.9866,	3.1615 s / batch. (data: 2.26e+00). ETA=10:02:38, max mem: 7.6 GB 
[10/25 08:17:29 visual_prompt]: Epoch 18 / 100: avg data time: 2.12e+00, avg batch time: 3.0026, average train loss: 9.3648
[10/25 08:18:16 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.4610, average loss: 6.6664
[10/25 08:18:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.59	
[10/25 08:18:16 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[10/25 08:23:27 visual_prompt]: 	Training 100/139. train loss: 9.8026,	8.8080 s / batch. (data: 7.93e+00). ETA=1 day, 3:38:32, max mem: 7.6 GB 
[10/25 08:25:14 visual_prompt]: Epoch 19 / 100: avg data time: 2.12e+00, avg batch time: 3.0010, average train loss: 7.8341
[10/25 08:26:01 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4675, average loss: 15.2769
[10/25 08:26:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.82	
[10/25 08:26:01 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[10/25 08:31:06 visual_prompt]: 	Training 100/139. train loss: 30.3781,	0.9046 s / batch. (data: 2.46e-02). ETA=2:48:14, max mem: 7.6 GB 
[10/25 08:32:58 visual_prompt]: Epoch 20 / 100: avg data time: 2.12e+00, avg batch time: 3.0008, average train loss: 12.2573
[10/25 08:33:45 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4633, average loss: 0.9014
[10/25 08:33:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.00	
[10/25 08:33:45 visual_prompt]: Best epoch 20: best metric: -0.901
[10/25 08:33:45 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[10/25 08:38:52 visual_prompt]: 	Training 100/139. train loss: 7.0550,	0.9043 s / batch. (data: 2.82e-02). ETA=2:46:04, max mem: 7.6 GB 
[10/25 08:40:42 visual_prompt]: Epoch 21 / 100: avg data time: 2.12e+00, avg batch time: 2.9995, average train loss: 10.9781
[10/25 08:41:30 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4641, average loss: 3.9865
[10/25 08:41:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.50	
[10/25 08:41:30 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[10/25 08:46:33 visual_prompt]: 	Training 100/139. train loss: 8.4689,	0.8880 s / batch. (data: 5.55e-03). ETA=2:41:02, max mem: 7.6 GB 
[10/25 08:48:27 visual_prompt]: Epoch 22 / 100: avg data time: 2.12e+00, avg batch time: 3.0018, average train loss: 7.7959
[10/25 08:49:14 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.4618, average loss: 4.8519
[10/25 08:49:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.48	
[10/25 08:49:14 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[10/25 08:54:19 visual_prompt]: 	Training 100/139. train loss: 2.8799,	0.8840 s / batch. (data: 2.99e-04). ETA=2:38:15, max mem: 7.6 GB 
[10/25 08:56:12 visual_prompt]: Epoch 23 / 100: avg data time: 2.12e+00, avg batch time: 3.0018, average train loss: 8.1940
[10/25 08:56:59 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4649, average loss: 38.2692
[10/25 08:56:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.05	
[10/25 08:56:59 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[10/25 09:02:11 visual_prompt]: 	Training 100/139. train loss: 5.9151,	0.9017 s / batch. (data: 1.56e-02). ETA=2:39:21, max mem: 7.6 GB 
[10/25 09:03:58 visual_prompt]: Epoch 24 / 100: avg data time: 2.13e+00, avg batch time: 3.0093, average train loss: 16.6776
[10/25 09:04:45 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.4626, average loss: 0.8837
[10/25 09:04:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.31	rocauc: 42.22	
[10/25 09:04:45 visual_prompt]: Best epoch 24: best metric: -0.884
[10/25 09:04:45 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[10/25 09:09:53 visual_prompt]: 	Training 100/139. train loss: 31.9043,	0.9120 s / batch. (data: 2.98e-04). ETA=2:39:03, max mem: 7.6 GB 
[10/25 09:11:42 visual_prompt]: Epoch 25 / 100: avg data time: 2.12e+00, avg batch time: 3.0010, average train loss: 9.3560
[10/25 09:12:30 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.4620, average loss: 15.1751
[10/25 09:12:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.83	
[10/25 09:12:30 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[10/25 09:17:39 visual_prompt]: 	Training 100/139. train loss: 9.7081,	0.8663 s / batch. (data: 2.15e-04). ETA=2:29:04, max mem: 7.6 GB 
[10/25 09:19:29 visual_prompt]: Epoch 26 / 100: avg data time: 2.13e+00, avg batch time: 3.0146, average train loss: 11.1134
[10/25 09:20:16 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.4626, average loss: 0.8577
[10/25 09:20:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.64	
[10/25 09:20:16 visual_prompt]: Best epoch 26: best metric: -0.858
[10/25 09:20:16 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[10/25 09:25:28 visual_prompt]: 	Training 100/139. train loss: 1.5675,	9.4827 s / batch. (data: 8.61e+00). ETA=1 day, 2:49:51, max mem: 7.6 GB 
[10/25 09:27:15 visual_prompt]: Epoch 27 / 100: avg data time: 2.14e+00, avg batch time: 3.0137, average train loss: 11.0667
[10/25 09:28:05 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.4688, average loss: 12.9289
[10/25 09:28:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.79	
[10/25 09:28:05 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[10/25 09:33:23 visual_prompt]: 	Training 100/139. train loss: 7.7710,	0.8909 s / batch. (data: 3.29e-04). ETA=2:29:10, max mem: 7.6 GB 
[10/25 09:35:18 visual_prompt]: Epoch 28 / 100: avg data time: 2.24e+00, avg batch time: 3.1185, average train loss: 6.7892
[10/25 09:36:06 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.4617, average loss: 24.6573
[10/25 09:36:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.72	
[10/25 09:36:06 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[10/25 09:41:12 visual_prompt]: 	Training 100/139. train loss: 5.2820,	0.9480 s / batch. (data: 4.52e-02). ETA=2:36:32, max mem: 7.6 GB 
[10/25 09:43:05 visual_prompt]: Epoch 29 / 100: avg data time: 2.13e+00, avg batch time: 3.0141, average train loss: 10.6483
[10/25 09:43:52 visual_prompt]: Inference (val):avg data time: 7.67e-04, avg batch time: 0.4650, average loss: 4.4518
[10/25 09:43:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.17	
[10/25 09:43:52 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[10/25 09:49:03 visual_prompt]: 	Training 100/139. train loss: 9.2403,	0.8840 s / batch. (data: 2.88e-04). ETA=2:23:55, max mem: 7.6 GB 
[10/25 09:50:52 visual_prompt]: Epoch 30 / 100: avg data time: 2.14e+00, avg batch time: 3.0178, average train loss: 8.4837
[10/25 09:51:39 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.4674, average loss: 7.7603
[10/25 09:51:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.69	
[10/25 09:51:39 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[10/25 09:56:51 visual_prompt]: 	Training 100/139. train loss: 2.2778,	0.8700 s / batch. (data: 4.35e-04). ETA=2:19:38, max mem: 7.6 GB 
[10/25 09:58:37 visual_prompt]: Epoch 31 / 100: avg data time: 2.13e+00, avg batch time: 3.0075, average train loss: 6.5908
[10/25 09:59:25 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4647, average loss: 5.9496
[10/25 09:59:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.50	
[10/25 09:59:25 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[10/25 10:04:36 visual_prompt]: 	Training 100/139. train loss: 5.5532,	7.0480 s / batch. (data: 6.16e+00). ETA=18:34:52, max mem: 7.6 GB 
[10/25 10:06:22 visual_prompt]: Epoch 32 / 100: avg data time: 2.12e+00, avg batch time: 3.0008, average train loss: 5.7457
[10/25 10:07:09 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4637, average loss: 4.9461
[10/25 10:07:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.31	rocauc: 50.56	
[10/25 10:07:09 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[10/25 10:12:16 visual_prompt]: 	Training 100/139. train loss: 6.4269,	0.8806 s / batch. (data: 5.39e-03). ETA=2:17:15, max mem: 7.6 GB 
[10/25 10:14:07 visual_prompt]: Epoch 33 / 100: avg data time: 2.12e+00, avg batch time: 3.0056, average train loss: 9.7206
[10/25 10:14:55 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4643, average loss: 6.7837
[10/25 10:14:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.14	
[10/25 10:14:55 visual_prompt]: Stopping early.
[10/25 10:14:55 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 10:14:55 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 10:14:55 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 10:14:55 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 10:14:55 visual_prompt]: Training with config:
[10/25 10:14:55 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr5.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 10:14:55 visual_prompt]: Loading training data...
[10/25 10:14:55 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 10:14:55 visual_prompt]: Loading validation data...
[10/25 10:14:55 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 10:14:55 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/25 10:15:24 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/25 10:15:24 visual_prompt]: tuned percent:0.534
[10/25 10:15:24 visual_prompt]: Device used for model: 0
[10/25 10:15:24 visual_prompt]: Setting up Evaluator...
[10/25 10:15:24 visual_prompt]: Setting up Trainer...
[10/25 10:15:24 visual_prompt]: 	Setting up the optimizer...
[10/25 10:15:25 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 10:20:31 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.9080 s / batch. (data: 7.93e-04). ETA=3:28:50, max mem: 7.6 GB 
[10/25 10:22:22 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 3.0021, average train loss: 1.3980
[10/25 10:23:09 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.4642, average loss: 1.3816
[10/25 10:23:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/25 10:23:09 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[10/25 10:28:15 visual_prompt]: 	Training 100/139. train loss: 0.7949,	0.9040 s / batch. (data: 3.03e-04). ETA=3:25:49, max mem: 7.6 GB 
[10/25 10:30:08 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 3.0081, average train loss: 2.0479
[10/25 10:30:55 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4682, average loss: 0.9525
[10/25 10:30:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.64	
[10/25 10:30:55 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[10/25 10:36:04 visual_prompt]: 	Training 100/139. train loss: 0.6266,	0.9000 s / batch. (data: 2.82e-04). ETA=3:22:49, max mem: 7.6 GB 
[10/25 10:37:52 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9996, average train loss: 1.3484
[10/25 10:38:40 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.4682, average loss: 3.0657
[10/25 10:38:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.78	
[10/25 10:38:40 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[10/25 10:43:49 visual_prompt]: 	Training 100/139. train loss: 1.6252,	0.8836 s / batch. (data: 2.98e-04). ETA=3:17:05, max mem: 7.6 GB 
[10/25 10:45:46 visual_prompt]: Epoch 4 / 100: avg data time: 2.18e+00, avg batch time: 3.0629, average train loss: 2.1174
[10/25 10:46:33 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.4673, average loss: 2.3400
[10/25 10:46:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.67	
[10/25 10:46:33 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[10/25 10:51:53 visual_prompt]: 	Training 100/139. train loss: 1.1885,	11.0964 s / batch. (data: 1.02e+01). ETA=1 day, 16:49:20, max mem: 7.6 GB 
[10/25 10:53:41 visual_prompt]: Epoch 5 / 100: avg data time: 2.20e+00, avg batch time: 3.0784, average train loss: 2.4838
[10/25 10:54:29 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.4668, average loss: 4.1743
[10/25 10:54:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.44	
[10/25 10:54:29 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[10/25 10:59:45 visual_prompt]: 	Training 100/139. train loss: 2.0260,	0.9119 s / batch. (data: 1.05e-02). ETA=3:19:10, max mem: 7.6 GB 
[10/25 11:01:39 visual_prompt]: Epoch 6 / 100: avg data time: 2.21e+00, avg batch time: 3.0908, average train loss: 3.1588
[10/25 11:02:39 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4626, average loss: 2.8812
[10/25 11:02:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.73	
[10/25 11:02:39 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[10/25 11:07:58 visual_prompt]: 	Training 100/139. train loss: 7.7367,	7.8158 s / batch. (data: 6.92e+00). ETA=1 day, 4:08:59, max mem: 7.6 GB 
[10/25 11:09:48 visual_prompt]: Epoch 7 / 100: avg data time: 2.20e+00, avg batch time: 3.0806, average train loss: 3.8888
[10/25 11:10:35 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4645, average loss: 7.6601
[10/25 11:10:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.38	
[10/25 11:10:35 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[10/25 11:16:03 visual_prompt]: 	Training 100/139. train loss: 2.9556,	0.8961 s / batch. (data: 3.64e-04). ETA=3:11:34, max mem: 7.6 GB 
[10/25 11:17:55 visual_prompt]: Epoch 8 / 100: avg data time: 2.28e+00, avg batch time: 3.1633, average train loss: 8.8354
[10/25 11:18:43 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.4646, average loss: 11.8528
[10/25 11:18:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.52	
[10/25 11:18:43 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[10/25 11:24:02 visual_prompt]: 	Training 100/139. train loss: 6.6744,	0.8931 s / batch. (data: 2.91e-04). ETA=3:08:52, max mem: 7.6 GB 
[10/25 11:25:52 visual_prompt]: Epoch 9 / 100: avg data time: 2.21e+00, avg batch time: 3.0874, average train loss: 5.6211
[10/25 11:26:48 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4646, average loss: 1.7682
[10/25 11:26:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.27	
[10/25 11:26:48 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[10/25 11:31:57 visual_prompt]: 	Training 100/139. train loss: 17.9064,	0.8896 s / batch. (data: 1.01e-02). ETA=3:06:03, max mem: 7.6 GB 
[10/25 11:33:46 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 3.0035, average train loss: 11.0878
[10/25 11:34:33 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.4653, average loss: 3.2783
[10/25 11:34:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.05	
[10/25 11:34:33 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[10/25 11:39:46 visual_prompt]: 	Training 100/139. train loss: 0.9864,	0.8845 s / batch. (data: 2.84e-04). ETA=3:02:57, max mem: 7.6 GB 
[10/25 11:41:37 visual_prompt]: Epoch 11 / 100: avg data time: 2.17e+00, avg batch time: 3.0490, average train loss: 4.0839
[10/25 11:42:25 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.4646, average loss: 3.0704
[10/25 11:42:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.66	
[10/25 11:42:25 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[10/25 11:48:08 visual_prompt]: 	Training 100/139. train loss: 4.4321,	4.0683 s / batch. (data: 3.21e+00). ETA=13:52:01, max mem: 7.6 GB 
[10/25 11:50:07 visual_prompt]: Epoch 12 / 100: avg data time: 2.45e+00, avg batch time: 3.3223, average train loss: 9.1017
[10/25 11:51:00 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4660, average loss: 0.6878
[10/25 11:51:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 50.85	
[10/25 11:51:00 visual_prompt]: Best epoch 12: best metric: -0.688
[10/25 11:51:00 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[10/25 11:56:19 visual_prompt]: 	Training 100/139. train loss: 16.0523,	0.9039 s / batch. (data: 2.98e-04). ETA=3:02:45, max mem: 7.6 GB 
[10/25 11:58:12 visual_prompt]: Epoch 13 / 100: avg data time: 2.23e+00, avg batch time: 3.1072, average train loss: 6.5344
[10/25 11:59:11 visual_prompt]: Inference (val):avg data time: 1.74e-04, avg batch time: 0.4694, average loss: 15.8172
[10/25 11:59:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.25	
[10/25 11:59:11 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[10/25 12:04:32 visual_prompt]: 	Training 100/139. train loss: 18.1718,	0.8752 s / batch. (data: 2.12e-04). ETA=2:54:56, max mem: 7.6 GB 
[10/25 12:06:29 visual_prompt]: Epoch 14 / 100: avg data time: 2.28e+00, avg batch time: 3.1542, average train loss: 8.8814
[10/25 12:07:23 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.4630, average loss: 16.8271
[10/25 12:07:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.29	
[10/25 12:07:23 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[10/25 12:13:08 visual_prompt]: 	Training 100/139. train loss: 1.8775,	9.4668 s / batch. (data: 8.59e+00). ETA=1 day, 7:10:19, max mem: 7.6 GB 
[10/25 12:14:56 visual_prompt]: Epoch 15 / 100: avg data time: 2.38e+00, avg batch time: 3.2595, average train loss: 2.8841
[10/25 12:15:44 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4678, average loss: 5.8796
[10/25 12:15:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.44	
[10/25 12:15:44 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[10/25 12:20:54 visual_prompt]: 	Training 100/139. train loss: 3.0465,	0.9168 s / batch. (data: 1.27e-02). ETA=2:59:00, max mem: 7.6 GB 
[10/25 12:22:44 visual_prompt]: Epoch 16 / 100: avg data time: 2.14e+00, avg batch time: 3.0212, average train loss: 4.7978
[10/25 12:23:31 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4695, average loss: 10.0999
[10/25 12:23:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.28	
[10/25 12:23:31 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[10/25 12:28:41 visual_prompt]: 	Training 100/139. train loss: 12.5572,	5.4114 s / batch. (data: 4.54e+00). ETA=17:24:02, max mem: 7.6 GB 
[10/25 12:30:35 visual_prompt]: Epoch 17 / 100: avg data time: 2.16e+00, avg batch time: 3.0446, average train loss: 9.4158
[10/25 12:31:22 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.4636, average loss: 5.0898
[10/25 12:31:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.04	
[10/25 12:31:22 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[10/25 12:36:31 visual_prompt]: 	Training 100/139. train loss: 8.1252,	0.8960 s / batch. (data: 4.98e-03). ETA=2:50:47, max mem: 7.6 GB 
[10/25 12:38:21 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0153, average train loss: 6.0812
[10/25 12:39:14 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.4676, average loss: 6.2386
[10/25 12:39:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.08	
[10/25 12:39:14 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[10/25 12:44:27 visual_prompt]: 	Training 100/139. train loss: 1.2586,	10.7017 s / batch. (data: 9.83e+00). ETA=1 day, 9:35:08, max mem: 7.6 GB 
[10/25 12:46:12 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0097, average train loss: 4.4439
[10/25 12:47:00 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.4601, average loss: 7.1923
[10/25 12:47:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.46	
[10/25 12:47:00 visual_prompt]: Stopping early.
[10/25 12:47:00 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 12:47:00 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 12:47:00 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 12:47:00 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 12:47:00 visual_prompt]: Training with config:
[10/25 12:47:00 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr5.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 12:47:00 visual_prompt]: Loading training data...
[10/25 12:47:00 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 12:47:00 visual_prompt]: Loading validation data...
[10/25 12:47:00 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 12:47:00 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/25 12:47:10 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/25 12:47:10 visual_prompt]: tuned percent:0.534
[10/25 12:47:10 visual_prompt]: Device used for model: 0
[10/25 12:47:10 visual_prompt]: Setting up Evaluator...
[10/25 12:47:10 visual_prompt]: Setting up Trainer...
[10/25 12:47:10 visual_prompt]: 	Setting up the optimizer...
[10/25 12:47:10 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 12:52:18 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8789 s / batch. (data: 7.77e-04). ETA=3:22:08, max mem: 7.6 GB 
[10/25 12:54:08 visual_prompt]: Epoch 1 / 100: avg data time: 2.13e+00, avg batch time: 3.0053, average train loss: 1.3980
[10/25 12:54:55 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4673, average loss: 1.3816
[10/25 12:54:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/25 12:54:55 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[10/25 13:00:03 visual_prompt]: 	Training 100/139. train loss: 0.7719,	0.8831 s / batch. (data: 5.43e-03). ETA=3:21:04, max mem: 7.6 GB 
[10/25 13:01:53 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 3.0044, average train loss: 1.9650
[10/25 13:02:41 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.4625, average loss: 0.9467
[10/25 13:02:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.50	
[10/25 13:02:41 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[10/25 13:07:48 visual_prompt]: 	Training 100/139. train loss: 0.6172,	0.8882 s / batch. (data: 8.15e-03). ETA=3:20:10, max mem: 7.6 GB 
[10/25 13:09:38 visual_prompt]: Epoch 3 / 100: avg data time: 2.13e+00, avg batch time: 3.0039, average train loss: 1.3962
[10/25 13:10:26 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4653, average loss: 3.0120
[10/25 13:10:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.15	
[10/25 13:10:26 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[10/25 13:15:35 visual_prompt]: 	Training 100/139. train loss: 0.9651,	0.8624 s / batch. (data: 2.81e-04). ETA=3:12:22, max mem: 7.6 GB 
[10/25 13:17:39 visual_prompt]: Epoch 4 / 100: avg data time: 2.24e+00, avg batch time: 3.1157, average train loss: 2.3180
[10/25 13:18:35 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.4614, average loss: 0.7087
[10/25 13:18:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 56.02	
[10/25 13:18:35 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[10/25 13:24:03 visual_prompt]: 	Training 100/139. train loss: 1.7781,	7.2800 s / batch. (data: 6.40e+00). ETA=1 day, 2:46:56, max mem: 7.6 GB 
[10/25 13:26:00 visual_prompt]: Epoch 5 / 100: avg data time: 2.33e+00, avg batch time: 3.2061, average train loss: 3.1425
[10/25 13:26:49 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.4646, average loss: 1.3023
[10/25 13:26:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.52	
[10/25 13:26:49 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[10/25 13:32:56 visual_prompt]: 	Training 100/139. train loss: 4.5913,	8.7052 s / batch. (data: 7.82e+00). ETA=1 day, 7:41:21, max mem: 7.6 GB 
[10/25 13:34:51 visual_prompt]: Epoch 6 / 100: avg data time: 2.59e+00, avg batch time: 3.4630, average train loss: 2.2601
[10/25 13:35:44 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.4632, average loss: 0.7282
[10/25 13:35:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.06	
[10/25 13:35:44 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[10/25 13:41:00 visual_prompt]: 	Training 100/139. train loss: 0.6191,	3.2678 s / batch. (data: 2.40e+00). ETA=11:46:10, max mem: 7.6 GB 
[10/25 13:42:53 visual_prompt]: Epoch 7 / 100: avg data time: 2.21e+00, avg batch time: 3.0820, average train loss: 2.0240
[10/25 13:43:40 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4631, average loss: 0.7337
[10/25 13:43:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.56	
[10/25 13:43:40 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[10/25 13:48:49 visual_prompt]: 	Training 100/139. train loss: 5.2419,	0.8750 s / batch. (data: 2.78e-04). ETA=3:07:03, max mem: 7.6 GB 
[10/25 13:50:39 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0119, average train loss: 4.6515
[10/25 13:51:26 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.4657, average loss: 4.8357
[10/25 13:51:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.13	
[10/25 13:51:26 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[10/25 13:56:38 visual_prompt]: 	Training 100/139. train loss: 6.9228,	0.8685 s / batch. (data: 2.79e-04). ETA=3:03:39, max mem: 7.6 GB 
[10/25 13:58:25 visual_prompt]: Epoch 9 / 100: avg data time: 2.14e+00, avg batch time: 3.0130, average train loss: 7.1370
[10/25 13:59:13 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4638, average loss: 22.8689
[10/25 13:59:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.90	
[10/25 13:59:13 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[10/25 14:04:25 visual_prompt]: 	Training 100/139. train loss: 2.7128,	0.8840 s / batch. (data: 2.86e-04). ETA=3:04:53, max mem: 7.6 GB 
[10/25 14:06:16 visual_prompt]: Epoch 10 / 100: avg data time: 2.17e+00, avg batch time: 3.0455, average train loss: 4.1629
[10/25 14:07:04 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4642, average loss: 1.0707
[10/25 14:07:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.41	
[10/25 14:07:04 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[10/25 14:12:14 visual_prompt]: 	Training 100/139. train loss: 0.6688,	0.8840 s / batch. (data: 3.00e-04). ETA=3:02:50, max mem: 7.6 GB 
[10/25 14:14:04 visual_prompt]: Epoch 11 / 100: avg data time: 2.14e+00, avg batch time: 3.0175, average train loss: 2.9840
[10/25 14:14:51 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4632, average loss: 3.9801
[10/25 14:14:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.71	
[10/25 14:14:51 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[10/25 14:19:58 visual_prompt]: 	Training 100/139. train loss: 1.7143,	2.2064 s / batch. (data: 1.31e+00). ETA=7:31:15, max mem: 7.6 GB 
[10/25 14:21:48 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9971, average train loss: 3.4650
[10/25 14:22:35 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.4647, average loss: 0.8028
[10/25 14:22:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.80	
[10/25 14:22:35 visual_prompt]: Best epoch 12: best metric: -0.803
[10/25 14:22:35 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[10/25 14:27:45 visual_prompt]: 	Training 100/139. train loss: 2.1192,	0.8699 s / batch. (data: 2.73e-04). ETA=2:55:53, max mem: 7.6 GB 
[10/25 14:29:34 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0090, average train loss: 5.7944
[10/25 14:30:21 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.4679, average loss: 2.8887
[10/25 14:30:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.48	
[10/25 14:30:21 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[10/25 14:35:32 visual_prompt]: 	Training 100/139. train loss: 1.9691,	0.8757 s / batch. (data: 2.82e-04). ETA=2:55:01, max mem: 7.6 GB 
[10/25 14:37:19 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0083, average train loss: 2.0418
[10/25 14:38:07 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4629, average loss: 4.8010
[10/25 14:38:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.12	
[10/25 14:38:07 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[10/25 14:43:34 visual_prompt]: 	Training 100/139. train loss: 0.8933,	9.6243 s / batch. (data: 8.76e+00). ETA=1 day, 7:41:26, max mem: 7.6 GB 
[10/25 14:45:19 visual_prompt]: Epoch 15 / 100: avg data time: 2.23e+00, avg batch time: 3.1095, average train loss: 3.6381
[10/25 14:46:07 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4651, average loss: 10.6906
[10/25 14:46:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.15	
[10/25 14:46:07 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[10/25 14:51:14 visual_prompt]: 	Training 100/139. train loss: 2.2976,	0.8678 s / batch. (data: 2.96e-04). ETA=2:49:26, max mem: 7.6 GB 
[10/25 14:53:06 visual_prompt]: Epoch 16 / 100: avg data time: 2.14e+00, avg batch time: 3.0146, average train loss: 2.9911
[10/25 14:53:53 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4621, average loss: 3.1650
[10/25 14:53:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.31	
[10/25 14:53:53 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[10/25 14:59:01 visual_prompt]: 	Training 100/139. train loss: 3.3039,	4.0400 s / batch. (data: 3.13e+00). ETA=12:59:26, max mem: 7.6 GB 
[10/25 15:00:51 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 3.0037, average train loss: 3.1504
[10/25 15:01:38 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4620, average loss: 1.8630
[10/25 15:01:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.98	
[10/25 15:01:38 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[10/25 15:06:47 visual_prompt]: 	Training 100/139. train loss: 3.2506,	0.8720 s / batch. (data: 2.99e-04). ETA=2:46:13, max mem: 7.6 GB 
[10/25 15:08:37 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0069, average train loss: 5.4771
[10/25 15:09:24 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.4675, average loss: 6.3148
[10/25 15:09:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.24	
[10/25 15:09:24 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[10/25 15:14:36 visual_prompt]: 	Training 100/139. train loss: 4.1481,	9.9760 s / batch. (data: 9.10e+00). ETA=1 day, 7:18:28, max mem: 7.6 GB 
[10/25 15:16:22 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0064, average train loss: 2.9775
[10/25 15:17:10 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4639, average loss: 5.1572
[10/25 15:17:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.70	
[10/25 15:17:10 visual_prompt]: Stopping early.
[10/25 15:17:10 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 15:17:10 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 15:17:10 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 15:17:10 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 15:17:10 visual_prompt]: Training with config:
[10/25 15:17:10 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr2.5_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 15:17:10 visual_prompt]: Loading training data...
[10/25 15:17:10 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 15:17:10 visual_prompt]: Loading validation data...
[10/25 15:17:10 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 15:17:10 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/25 15:17:30 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/25 15:17:30 visual_prompt]: tuned percent:0.534
[10/25 15:17:30 visual_prompt]: Device used for model: 0
[10/25 15:17:30 visual_prompt]: Setting up Evaluator...
[10/25 15:17:30 visual_prompt]: Setting up Trainer...
[10/25 15:17:30 visual_prompt]: 	Setting up the optimizer...
[10/25 15:17:30 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 15:22:37 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8616 s / batch. (data: 2.85e-04). ETA=3:18:10, max mem: 7.6 GB 
[10/25 15:24:27 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9986, average train loss: 1.3980
[10/25 15:25:14 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.4635, average loss: 1.3816
[10/25 15:25:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/25 15:25:14 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[10/25 15:30:19 visual_prompt]: 	Training 100/139. train loss: 0.7021,	0.9050 s / batch. (data: 1.79e-02). ETA=3:26:03, max mem: 7.6 GB 
[10/25 15:32:13 visual_prompt]: Epoch 2 / 100: avg data time: 2.14e+00, avg batch time: 3.0143, average train loss: 1.2087
[10/25 15:33:02 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4647, average loss: 0.8962
[10/25 15:33:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.30	
[10/25 15:33:02 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[10/25 15:38:10 visual_prompt]: 	Training 100/139. train loss: 0.7673,	0.8880 s / batch. (data: 2.97e-04). ETA=3:20:07, max mem: 7.6 GB 
[10/25 15:40:04 visual_prompt]: Epoch 3 / 100: avg data time: 2.16e+00, avg batch time: 3.0363, average train loss: 0.7926
[10/25 15:40:52 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4632, average loss: 2.4804
[10/25 15:40:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.39	
[10/25 15:40:52 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[10/25 15:45:58 visual_prompt]: 	Training 100/139. train loss: 1.3713,	0.8707 s / batch. (data: 9.28e-04). ETA=3:14:12, max mem: 7.6 GB 
[10/25 15:47:52 visual_prompt]: Epoch 4 / 100: avg data time: 2.14e+00, avg batch time: 3.0206, average train loss: 1.0255
[10/25 15:48:39 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4628, average loss: 1.5193
[10/25 15:48:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.85	
[10/25 15:48:39 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[10/25 15:53:52 visual_prompt]: 	Training 100/139. train loss: 1.5704,	12.6000 s / batch. (data: 1.17e+01). ETA=1 day, 22:21:14, max mem: 7.6 GB 
[10/25 15:55:40 visual_prompt]: Epoch 5 / 100: avg data time: 2.15e+00, avg batch time: 3.0274, average train loss: 1.1880
[10/25 15:56:28 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4579, average loss: 1.5290
[10/25 15:56:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.75	
[10/25 15:56:28 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[10/25 16:01:34 visual_prompt]: 	Training 100/139. train loss: 0.6334,	0.8800 s / batch. (data: 1.19e-02). ETA=3:12:12, max mem: 7.6 GB 
[10/25 16:03:25 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 3.0044, average train loss: 1.6985
[10/25 16:04:13 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.4611, average loss: 1.0501
[10/25 16:04:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.56	
[10/25 16:04:13 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[10/25 16:09:22 visual_prompt]: 	Training 100/139. train loss: 1.3814,	0.8720 s / batch. (data: 2.86e-04). ETA=3:08:26, max mem: 7.6 GB 
[10/25 16:11:10 visual_prompt]: Epoch 7 / 100: avg data time: 2.13e+00, avg batch time: 3.0003, average train loss: 2.2536
[10/25 16:11:57 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4629, average loss: 3.2250
[10/25 16:11:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.70	
[10/25 16:11:57 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[10/25 16:17:06 visual_prompt]: 	Training 100/139. train loss: 1.2671,	0.9040 s / batch. (data: 1.20e-02). ETA=3:13:15, max mem: 7.6 GB 
[10/25 16:18:56 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0074, average train loss: 3.0375
[10/25 16:19:43 visual_prompt]: Inference (val):avg data time: 7.33e-04, avg batch time: 0.4657, average loss: 6.1915
[10/25 16:19:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.66	
[10/25 16:19:43 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[10/25 16:24:52 visual_prompt]: 	Training 100/139. train loss: 1.4155,	0.8840 s / batch. (data: 2.90e-04). ETA=3:06:56, max mem: 7.6 GB 
[10/25 16:26:40 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9973, average train loss: 2.8837
[10/25 16:27:27 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4667, average loss: 0.7351
[10/25 16:27:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.17	
[10/25 16:27:27 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[10/25 16:32:33 visual_prompt]: 	Training 100/139. train loss: 12.7233,	0.8674 s / batch. (data: 3.11e-04). ETA=3:01:24, max mem: 7.6 GB 
[10/25 16:34:23 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9956, average train loss: 4.6004
[10/25 16:35:11 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.4628, average loss: 0.8269
[10/25 16:35:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 55.21	
[10/25 16:35:11 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[10/25 16:40:19 visual_prompt]: 	Training 100/139. train loss: 2.5645,	0.9080 s / batch. (data: 2.90e-04). ETA=3:07:48, max mem: 7.6 GB 
[10/25 16:42:08 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0041, average train loss: 5.2591
[10/25 16:42:56 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4619, average loss: 14.3545
[10/25 16:42:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.10	
[10/25 16:42:56 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[10/25 16:48:13 visual_prompt]: 	Training 100/139. train loss: 7.5089,	0.9206 s / batch. (data: 1.05e-02). ETA=3:08:16, max mem: 7.6 GB 
[10/25 16:50:03 visual_prompt]: Epoch 12 / 100: avg data time: 2.20e+00, avg batch time: 3.0752, average train loss: 7.6851
[10/25 16:50:51 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4660, average loss: 11.3950
[10/25 16:50:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.52	
[10/25 16:50:51 visual_prompt]: Best epoch 12: best metric: -11.395
[10/25 16:50:51 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[10/25 16:56:07 visual_prompt]: 	Training 100/139. train loss: 8.9514,	0.8744 s / batch. (data: 1.20e-02). ETA=2:56:48, max mem: 7.6 GB 
[10/25 16:58:00 visual_prompt]: Epoch 13 / 100: avg data time: 2.21e+00, avg batch time: 3.0868, average train loss: 6.5364
[10/25 16:58:48 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4623, average loss: 2.3378
[10/25 16:58:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.03	
[10/25 16:58:48 visual_prompt]: Best epoch 13: best metric: -2.338
[10/25 16:58:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[10/25 17:03:58 visual_prompt]: 	Training 100/139. train loss: 1.4142,	0.8910 s / batch. (data: 3.90e-04). ETA=2:58:05, max mem: 7.6 GB 
[10/25 17:05:45 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0002, average train loss: 6.3496
[10/25 17:06:32 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4619, average loss: 8.6437
[10/25 17:06:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.19	
[10/25 17:06:32 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[10/25 17:11:45 visual_prompt]: 	Training 100/139. train loss: 1.1557,	9.4407 s / batch. (data: 8.57e+00). ETA=1 day, 7:05:09, max mem: 7.6 GB 
[10/25 17:13:29 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0021, average train loss: 4.8060
[10/25 17:14:17 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4615, average loss: 6.7742
[10/25 17:14:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.38	
[10/25 17:14:17 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[10/25 17:19:23 visual_prompt]: 	Training 100/139. train loss: 7.4492,	0.8848 s / batch. (data: 2.82e-04). ETA=2:52:45, max mem: 7.6 GB 
[10/25 17:21:13 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9969, average train loss: 5.2293
[10/25 17:22:01 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4626, average loss: 0.8791
[10/25 17:22:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.13	
[10/25 17:22:01 visual_prompt]: Best epoch 16: best metric: -0.879
[10/25 17:22:01 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[10/25 17:27:09 visual_prompt]: 	Training 100/139. train loss: 5.2217,	4.6600 s / batch. (data: 3.77e+00). ETA=14:59:04, max mem: 7.6 GB 
[10/25 17:28:58 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 2.9985, average train loss: 3.4772
[10/25 17:29:46 visual_prompt]: Inference (val):avg data time: 6.32e-04, avg batch time: 0.4633, average loss: 0.8903
[10/25 17:29:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.68	
[10/25 17:29:46 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[10/25 17:34:53 visual_prompt]: 	Training 100/139. train loss: 4.5351,	1.7641 s / batch. (data: 8.59e-01). ETA=5:36:15, max mem: 7.6 GB 
[10/25 17:36:43 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0018, average train loss: 4.9586
[10/25 17:37:30 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4624, average loss: 0.6948
[10/25 17:37:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 43.50	rocauc: 44.22	
[10/25 17:37:30 visual_prompt]: Best epoch 18: best metric: -0.695
[10/25 17:37:30 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[10/25 17:42:44 visual_prompt]: 	Training 100/139. train loss: 3.5330,	11.4565 s / batch. (data: 1.06e+01). ETA=1 day, 11:57:15, max mem: 7.6 GB 
[10/25 17:44:29 visual_prompt]: Epoch 19 / 100: avg data time: 2.14e+00, avg batch time: 3.0149, average train loss: 4.5665
[10/25 17:45:17 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4611, average loss: 7.0421
[10/25 17:45:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.14	
[10/25 17:45:17 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[10/25 17:50:24 visual_prompt]: 	Training 100/139. train loss: 6.7053,	0.8889 s / batch. (data: 5.46e-03). ETA=2:45:18, max mem: 7.6 GB 
[10/25 17:52:15 visual_prompt]: Epoch 20 / 100: avg data time: 2.13e+00, avg batch time: 3.0077, average train loss: 6.0068
[10/25 17:53:03 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4577, average loss: 2.4697
[10/25 17:53:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[10/25 17:53:03 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[10/25 17:58:12 visual_prompt]: 	Training 100/139. train loss: 2.0977,	0.8861 s / batch. (data: 3.88e-04). ETA=2:42:45, max mem: 7.6 GB 
[10/25 18:00:04 visual_prompt]: Epoch 21 / 100: avg data time: 2.15e+00, avg batch time: 3.0272, average train loss: 5.1597
[10/25 18:00:51 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.4643, average loss: 3.3707
[10/25 18:00:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.78	
[10/25 18:00:51 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[10/25 18:05:55 visual_prompt]: 	Training 100/139. train loss: 9.0427,	0.9129 s / batch. (data: 5.43e-03). ETA=2:45:33, max mem: 7.6 GB 
[10/25 18:07:49 visual_prompt]: Epoch 22 / 100: avg data time: 2.13e+00, avg batch time: 3.0092, average train loss: 6.5773
[10/25 18:08:36 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.4658, average loss: 5.5224
[10/25 18:08:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.53	
[10/25 18:08:36 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[10/25 18:13:42 visual_prompt]: 	Training 100/139. train loss: 2.4208,	0.8927 s / batch. (data: 8.67e-03). ETA=2:39:49, max mem: 7.6 GB 
[10/25 18:15:34 visual_prompt]: Epoch 23 / 100: avg data time: 2.13e+00, avg batch time: 3.0053, average train loss: 4.3969
[10/25 18:16:21 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4622, average loss: 2.8592
[10/25 18:16:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.82	
[10/25 18:16:21 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[10/25 18:21:33 visual_prompt]: 	Training 100/139. train loss: 1.5227,	0.8839 s / batch. (data: 7.99e-04). ETA=2:36:11, max mem: 7.6 GB 
[10/25 18:23:19 visual_prompt]: Epoch 24 / 100: avg data time: 2.13e+00, avg batch time: 3.0074, average train loss: 4.5654
[10/25 18:24:07 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4653, average loss: 4.2285
[10/25 18:24:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.29	
[10/25 18:24:07 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[10/25 18:29:15 visual_prompt]: 	Training 100/139. train loss: 11.7271,	0.8612 s / batch. (data: 2.77e-04). ETA=2:30:12, max mem: 7.6 GB 
[10/25 18:31:04 visual_prompt]: Epoch 25 / 100: avg data time: 2.12e+00, avg batch time: 2.9990, average train loss: 4.7844
[10/25 18:31:51 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4650, average loss: 9.2652
[10/25 18:31:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.27	
[10/25 18:31:51 visual_prompt]: Stopping early.
[10/25 18:31:51 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 18:31:51 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 18:31:51 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 18:31:51 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 18:31:51 visual_prompt]: Training with config:
[10/25 18:31:51 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr2.5_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 18:31:51 visual_prompt]: Loading training data...
[10/25 18:31:51 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 18:31:51 visual_prompt]: Loading validation data...
[10/25 18:31:51 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 18:31:51 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/25 18:31:54 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/25 18:31:54 visual_prompt]: tuned percent:0.534
[10/25 18:31:54 visual_prompt]: Device used for model: 0
[10/25 18:31:54 visual_prompt]: Setting up Evaluator...
[10/25 18:31:54 visual_prompt]: Setting up Trainer...
[10/25 18:31:54 visual_prompt]: 	Setting up the optimizer...
[10/25 18:31:54 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 18:37:01 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.9034 s / batch. (data: 1.10e-02). ETA=3:27:47, max mem: 7.6 GB 
[10/25 18:38:51 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9987, average train loss: 1.3980
[10/25 18:39:38 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4600, average loss: 1.3816
[10/25 18:39:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/25 18:39:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[10/25 18:44:43 visual_prompt]: 	Training 100/139. train loss: 0.8009,	0.8735 s / batch. (data: 2.45e-04). ETA=3:18:53, max mem: 7.6 GB 
[10/25 18:46:35 visual_prompt]: Epoch 2 / 100: avg data time: 2.12e+00, avg batch time: 2.9990, average train loss: 1.2462
[10/25 18:47:23 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.4627, average loss: 1.1469
[10/25 18:47:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.33	
[10/25 18:47:23 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[10/25 18:52:31 visual_prompt]: 	Training 100/139. train loss: 0.7808,	0.8691 s / batch. (data: 2.65e-04). ETA=3:15:51, max mem: 7.6 GB 
[10/25 18:54:20 visual_prompt]: Epoch 3 / 100: avg data time: 2.13e+00, avg batch time: 3.0027, average train loss: 0.8876
[10/25 18:55:08 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4638, average loss: 1.7404
[10/25 18:55:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.73	
[10/25 18:55:08 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[10/25 19:00:11 visual_prompt]: 	Training 100/139. train loss: 0.6700,	0.9018 s / batch. (data: 2.81e-04). ETA=3:21:09, max mem: 7.6 GB 
[10/25 19:02:05 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 3.0010, average train loss: 0.9571
[10/25 19:02:52 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4657, average loss: 0.7630
[10/25 19:02:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.60	
[10/25 19:02:52 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[10/25 19:08:02 visual_prompt]: 	Training 100/139. train loss: 1.0019,	11.6320 s / batch. (data: 1.07e+01). ETA=1 day, 18:47:33, max mem: 7.6 GB 
[10/25 19:09:50 visual_prompt]: Epoch 5 / 100: avg data time: 2.12e+00, avg batch time: 3.0018, average train loss: 0.8831
[10/25 19:10:37 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.4630, average loss: 1.6277
[10/25 19:10:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.37	
[10/25 19:10:37 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[10/25 19:15:44 visual_prompt]: 	Training 100/139. train loss: 0.6567,	5.1320 s / batch. (data: 4.24e+00). ETA=18:40:54, max mem: 7.6 GB 
[10/25 19:17:34 visual_prompt]: Epoch 6 / 100: avg data time: 2.12e+00, avg batch time: 2.9980, average train loss: 1.8984
[10/25 19:18:21 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.4659, average loss: 0.7653
[10/25 19:18:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.81	
[10/25 19:18:21 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[10/25 19:23:29 visual_prompt]: 	Training 100/139. train loss: 1.7814,	0.8907 s / batch. (data: 1.55e-02). ETA=3:12:28, max mem: 7.6 GB 
[10/25 19:25:18 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9949, average train loss: 1.8310
[10/25 19:26:05 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4665, average loss: 2.8785
[10/25 19:26:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.98	
[10/25 19:26:05 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[10/25 19:31:13 visual_prompt]: 	Training 100/139. train loss: 0.9865,	0.8843 s / batch. (data: 2.90e-04). ETA=3:09:02, max mem: 7.6 GB 
[10/25 19:33:03 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0022, average train loss: 2.5029
[10/25 19:33:50 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4632, average loss: 0.8730
[10/25 19:33:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.11	
[10/25 19:33:50 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[10/25 19:39:00 visual_prompt]: 	Training 100/139. train loss: 3.3333,	0.8880 s / batch. (data: 2.51e-04). ETA=3:07:47, max mem: 7.6 GB 
[10/25 19:40:47 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9971, average train loss: 2.4393
[10/25 19:41:34 visual_prompt]: Inference (val):avg data time: 1.28e-03, avg batch time: 0.4640, average loss: 1.4763
[10/25 19:41:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.44	
[10/25 19:41:34 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[10/25 19:46:40 visual_prompt]: 	Training 100/139. train loss: 5.2910,	0.8734 s / batch. (data: 5.42e-03). ETA=3:02:39, max mem: 7.6 GB 
[10/25 19:48:31 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9962, average train loss: 4.3982
[10/25 19:49:18 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.4601, average loss: 3.2186
[10/25 19:49:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.48	
[10/25 19:49:18 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[10/25 19:54:26 visual_prompt]: 	Training 100/139. train loss: 0.7921,	0.8800 s / batch. (data: 2.90e-04). ETA=3:02:00, max mem: 7.6 GB 
[10/25 19:56:15 visual_prompt]: Epoch 11 / 100: avg data time: 2.12e+00, avg batch time: 2.9985, average train loss: 3.7789
[10/25 19:57:03 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4643, average loss: 5.5541
[10/25 19:57:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.02	
[10/25 19:57:03 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[10/25 20:02:09 visual_prompt]: 	Training 100/139. train loss: 3.1185,	1.7920 s / batch. (data: 9.25e-01). ETA=6:06:29, max mem: 7.6 GB 
[10/25 20:04:00 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9999, average train loss: 3.7951
[10/25 20:04:47 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.4637, average loss: 9.8874
[10/25 20:04:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.84	
[10/25 20:04:47 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[10/25 20:09:57 visual_prompt]: 	Training 100/139. train loss: 14.3386,	0.8679 s / batch. (data: 2.86e-04). ETA=2:55:29, max mem: 7.6 GB 
[10/25 20:11:45 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0063, average train loss: 6.0798
[10/25 20:12:33 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.4667, average loss: 1.0950
[10/25 20:12:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.59	
[10/25 20:12:33 visual_prompt]: Best epoch 13: best metric: -1.095
[10/25 20:12:33 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[10/25 20:17:42 visual_prompt]: 	Training 100/139. train loss: 4.3807,	0.8996 s / batch. (data: 7.66e-04). ETA=2:59:49, max mem: 7.6 GB 
[10/25 20:19:30 visual_prompt]: Epoch 14 / 100: avg data time: 2.12e+00, avg batch time: 2.9987, average train loss: 2.7068
[10/25 20:20:17 visual_prompt]: Inference (val):avg data time: 5.02e-04, avg batch time: 0.4647, average loss: 1.5516
[10/25 20:20:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.36	
[10/25 20:20:17 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[10/25 20:25:30 visual_prompt]: 	Training 100/139. train loss: 0.6802,	9.6201 s / batch. (data: 8.74e+00). ETA=1 day, 7:40:36, max mem: 7.6 GB 
[10/25 20:27:15 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0092, average train loss: 3.7032
[10/25 20:28:03 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.4637, average loss: 2.1572
[10/25 20:28:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.69	
[10/25 20:28:03 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[10/25 20:33:09 visual_prompt]: 	Training 100/139. train loss: 2.6463,	0.9169 s / batch. (data: 8.85e-03). ETA=2:59:01, max mem: 7.6 GB 
[10/25 20:34:59 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9957, average train loss: 2.8094
[10/25 20:35:47 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4615, average loss: 0.6938
[10/25 20:35:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 48.36	
[10/25 20:35:47 visual_prompt]: Best epoch 16: best metric: -0.694
[10/25 20:35:47 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[10/25 20:40:55 visual_prompt]: 	Training 100/139. train loss: 9.3224,	5.6520 s / batch. (data: 4.76e+00). ETA=18:10:27, max mem: 7.6 GB 
[10/25 20:42:43 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 2.9964, average train loss: 3.9065
[10/25 20:43:31 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4649, average loss: 0.6901
[10/25 20:43:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.76	
[10/25 20:43:31 visual_prompt]: Best epoch 17: best metric: -0.690
[10/25 20:43:31 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[10/25 20:48:39 visual_prompt]: 	Training 100/139. train loss: 1.9873,	3.2757 s / batch. (data: 2.39e+00). ETA=10:24:24, max mem: 7.6 GB 
[10/25 20:50:28 visual_prompt]: Epoch 18 / 100: avg data time: 2.12e+00, avg batch time: 3.0001, average train loss: 3.9303
[10/25 20:51:15 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4601, average loss: 4.5312
[10/25 20:51:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.22	
[10/25 20:51:15 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[10/25 20:56:29 visual_prompt]: 	Training 100/139. train loss: 8.4278,	10.9486 s / batch. (data: 1.01e+01). ETA=1 day, 10:21:36, max mem: 7.6 GB 
[10/25 20:58:13 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0032, average train loss: 5.7389
[10/25 20:59:00 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4627, average loss: 7.5660
[10/25 20:59:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.14	
[10/25 20:59:00 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[10/25 21:04:06 visual_prompt]: 	Training 100/139. train loss: 12.3130,	0.8829 s / batch. (data: 2.98e-04). ETA=2:44:12, max mem: 7.6 GB 
[10/25 21:05:57 visual_prompt]: Epoch 20 / 100: avg data time: 2.12e+00, avg batch time: 2.9982, average train loss: 3.2469
[10/25 21:06:45 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.4648, average loss: 3.8889
[10/25 21:06:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.55	
[10/25 21:06:45 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[10/25 21:11:51 visual_prompt]: 	Training 100/139. train loss: 0.9632,	0.8840 s / batch. (data: 1.20e-02). ETA=2:42:21, max mem: 7.6 GB 
[10/25 21:13:41 visual_prompt]: Epoch 21 / 100: avg data time: 2.12e+00, avg batch time: 2.9932, average train loss: 1.8881
[10/25 21:14:28 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.4654, average loss: 1.5091
[10/25 21:14:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.52	
[10/25 21:14:28 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[10/25 21:19:31 visual_prompt]: 	Training 100/139. train loss: 2.6667,	0.8939 s / batch. (data: 1.55e-02). ETA=2:42:06, max mem: 7.6 GB 
[10/25 21:21:25 visual_prompt]: Epoch 22 / 100: avg data time: 2.12e+00, avg batch time: 2.9964, average train loss: 4.0926
[10/25 21:22:12 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4620, average loss: 4.8761
[10/25 21:22:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.08	
[10/25 21:22:12 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[10/25 21:27:18 visual_prompt]: 	Training 100/139. train loss: 0.6965,	0.8916 s / batch. (data: 1.55e-02). ETA=2:39:37, max mem: 7.6 GB 
[10/25 21:29:10 visual_prompt]: Epoch 23 / 100: avg data time: 2.13e+00, avg batch time: 3.0050, average train loss: 2.9922
[10/25 21:29:57 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4619, average loss: 5.7380
[10/25 21:29:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.28	
[10/25 21:29:57 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[10/25 21:35:09 visual_prompt]: 	Training 100/139. train loss: 2.3410,	0.8793 s / batch. (data: 2.85e-04). ETA=2:35:23, max mem: 7.6 GB 
[10/25 21:36:55 visual_prompt]: Epoch 24 / 100: avg data time: 2.13e+00, avg batch time: 3.0055, average train loss: 5.1052
[10/25 21:37:43 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.4620, average loss: 2.1583
[10/25 21:37:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.44	
[10/25 21:37:43 visual_prompt]: Stopping early.
[10/25 21:37:43 visual_prompt]: Rank of current process: 0. World size: 1
[10/25 21:37:43 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/25 21:37:43 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/25 21:37:43 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/25 21:37:43 visual_prompt]: Training with config:
[10/25 21:37:43 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr2.5_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/25 21:37:43 visual_prompt]: Loading training data...
[10/25 21:37:43 visual_prompt]: Constructing mammo-cbis dataset train...
[10/25 21:37:43 visual_prompt]: Loading validation data...
[10/25 21:37:43 visual_prompt]: Constructing mammo-cbis dataset val...
[10/25 21:37:43 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/25 21:37:45 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/25 21:37:45 visual_prompt]: tuned percent:0.534
[10/25 21:37:46 visual_prompt]: Device used for model: 0
[10/25 21:37:46 visual_prompt]: Setting up Evaluator...
[10/25 21:37:46 visual_prompt]: Setting up Trainer...
[10/25 21:37:46 visual_prompt]: 	Setting up the optimizer...
[10/25 21:37:46 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/25 21:42:52 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8840 s / batch. (data: 2.92e-04). ETA=3:23:18, max mem: 7.6 GB 
[10/25 21:44:43 visual_prompt]: Epoch 1 / 100: avg data time: 2.13e+00, avg batch time: 2.9997, average train loss: 1.3980
[10/25 21:45:30 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4639, average loss: 1.3816
[10/25 21:45:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/25 21:45:30 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[10/25 21:50:35 visual_prompt]: 	Training 100/139. train loss: 0.8196,	0.9142 s / batch. (data: 1.01e-02). ETA=3:28:08, max mem: 7.6 GB 
[10/25 21:52:27 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 3.0001, average train loss: 1.2522
[10/25 21:53:14 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4622, average loss: 1.0942
[10/25 21:53:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.89	
[10/25 21:53:14 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[10/25 21:58:21 visual_prompt]: 	Training 100/139. train loss: 0.6402,	0.8916 s / batch. (data: 5.32e-03). ETA=3:20:56, max mem: 7.6 GB 
[10/25 22:00:11 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9952, average train loss: 0.8994
[10/25 22:00:58 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4586, average loss: 1.3035
[10/25 22:00:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.89	
[10/25 22:00:58 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[10/25 22:06:01 visual_prompt]: 	Training 100/139. train loss: 1.5827,	0.8779 s / batch. (data: 5.42e-03). ETA=3:15:48, max mem: 7.6 GB 
[10/25 22:07:55 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 3.0000, average train loss: 1.1106
[10/25 22:08:43 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4645, average loss: 1.1378
[10/25 22:08:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.30	
[10/25 22:08:43 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[10/25 22:13:52 visual_prompt]: 	Training 100/139. train loss: 0.9241,	11.5433 s / batch. (data: 1.07e+01). ETA=1 day, 18:27:59, max mem: 7.6 GB 
[10/25 22:15:40 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 2.9986, average train loss: 1.4027
[10/25 22:16:27 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4596, average loss: 1.2101
[10/25 22:16:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.13	
[10/25 22:16:27 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[10/25 22:21:33 visual_prompt]: 	Training 100/139. train loss: 0.6815,	0.9041 s / batch. (data: 1.64e-02). ETA=3:17:28, max mem: 7.6 GB 
[10/25 22:23:24 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 2.9998, average train loss: 1.3337
[10/25 22:24:11 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4625, average loss: 0.7115
[10/25 22:24:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.08	
[10/25 22:24:11 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[10/25 22:29:18 visual_prompt]: 	Training 100/139. train loss: 1.0799,	0.9096 s / batch. (data: 5.18e-02). ETA=3:16:33, max mem: 7.6 GB 
[10/25 22:31:09 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9994, average train loss: 1.0828
[10/25 22:31:56 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.4610, average loss: 0.9450
[10/25 22:31:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.04	
[10/25 22:31:56 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[10/25 22:37:04 visual_prompt]: 	Training 100/139. train loss: 0.7671,	0.8680 s / batch. (data: 2.96e-04). ETA=3:05:33, max mem: 7.6 GB 
[10/25 22:38:54 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0078, average train loss: 1.5284
[10/25 22:39:41 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.4610, average loss: 4.4740
[10/25 22:39:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.39	
[10/25 22:39:41 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[10/25 22:44:53 visual_prompt]: 	Training 100/139. train loss: 3.8341,	0.8966 s / batch. (data: 3.21e-04). ETA=3:09:36, max mem: 7.6 GB 
[10/25 22:46:41 visual_prompt]: Epoch 9 / 100: avg data time: 2.14e+00, avg batch time: 3.0180, average train loss: 2.4614
[10/25 22:47:29 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4615, average loss: 1.4755
[10/25 22:47:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.83	
[10/25 22:47:29 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[10/25 22:52:38 visual_prompt]: 	Training 100/139. train loss: 1.4533,	0.9040 s / batch. (data: 3.39e-04). ETA=3:09:04, max mem: 7.6 GB 
[10/25 22:54:26 visual_prompt]: Epoch 10 / 100: avg data time: 2.13e+00, avg batch time: 3.0004, average train loss: 2.8426
[10/25 22:55:14 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.4598, average loss: 1.7777
[10/25 22:55:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.05	
[10/25 22:55:14 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[10/25 23:00:21 visual_prompt]: 	Training 100/139. train loss: 1.3262,	0.8840 s / batch. (data: 2.89e-04). ETA=3:02:50, max mem: 7.6 GB 
[10/25 23:02:11 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0019, average train loss: 1.8136
[10/25 23:02:58 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4652, average loss: 1.0337
[10/25 23:02:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.11	
[10/25 23:02:58 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[10/25 23:08:06 visual_prompt]: 	Training 100/139. train loss: 1.7405,	0.9012 s / batch. (data: 2.53e-02). ETA=3:04:18, max mem: 7.6 GB 
[10/25 23:09:55 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9987, average train loss: 2.1693
[10/25 23:10:43 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.4639, average loss: 0.6945
[10/25 23:10:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 54.83	
[10/25 23:10:43 visual_prompt]: Best epoch 12: best metric: -0.695
[10/25 23:10:43 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[10/25 23:15:52 visual_prompt]: 	Training 100/139. train loss: 1.2125,	0.8712 s / batch. (data: 2.45e-04). ETA=2:56:09, max mem: 7.6 GB 
[10/25 23:17:40 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0051, average train loss: 3.8394
[10/25 23:18:28 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4604, average loss: 1.8089
[10/25 23:18:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.76	
[10/25 23:18:28 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[10/25 23:23:38 visual_prompt]: 	Training 100/139. train loss: 2.0798,	0.8853 s / batch. (data: 3.03e-04). ETA=2:56:57, max mem: 7.6 GB 
[10/25 23:25:25 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 2.9998, average train loss: 2.9551
[10/25 23:26:12 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4618, average loss: 2.8069
[10/25 23:26:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.51	
[10/25 23:26:12 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[10/25 23:31:25 visual_prompt]: 	Training 100/139. train loss: 0.7080,	9.5399 s / batch. (data: 8.65e+00). ETA=1 day, 7:24:45, max mem: 7.6 GB 
[10/25 23:33:09 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0008, average train loss: 1.6609
[10/25 23:33:57 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4626, average loss: 0.6966
[10/25 23:33:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.85	
[10/25 23:33:57 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[10/25 23:39:03 visual_prompt]: 	Training 100/139. train loss: 1.0426,	0.9000 s / batch. (data: 2.40e-02). ETA=2:55:43, max mem: 7.6 GB 
[10/25 23:40:53 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9916, average train loss: 1.5565
[10/25 23:41:40 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.4655, average loss: 1.9539
[10/25 23:41:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.72	
[10/25 23:41:40 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[10/25 23:46:47 visual_prompt]: 	Training 100/139. train loss: 1.9468,	3.7680 s / batch. (data: 2.87e+00). ETA=12:06:58, max mem: 7.6 GB 
[10/25 23:48:37 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 2.9941, average train loss: 2.7099
[10/25 23:49:24 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4647, average loss: 1.6636
[10/25 23:49:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.61	
[10/25 23:49:24 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[10/25 23:54:33 visual_prompt]: 	Training 100/139. train loss: 0.7379,	2.5702 s / batch. (data: 1.69e+00). ETA=8:09:55, max mem: 7.6 GB 
[10/25 23:56:21 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0028, average train loss: 1.9831
[10/25 23:57:09 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.4626, average loss: 2.2140
[10/25 23:57:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.99	
[10/25 23:57:09 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[10/26 00:02:21 visual_prompt]: 	Training 100/139. train loss: 0.5446,	11.1680 s / batch. (data: 1.03e+01). ETA=1 day, 11:02:56, max mem: 7.6 GB 
[10/26 00:04:06 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0024, average train loss: 1.2386
[10/26 00:04:53 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.4619, average loss: 1.8802
[10/26 00:04:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.77	
[10/26 00:04:53 visual_prompt]: Stopping early.
[10/26 00:04:54 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 00:04:54 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 00:04:54 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 00:04:54 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 00:04:54 visual_prompt]: Training with config:
[10/26 00:04:54 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr2.5_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 00:04:54 visual_prompt]: Loading training data...
[10/26 00:04:54 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 00:04:54 visual_prompt]: Loading validation data...
[10/26 00:04:54 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 00:04:54 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/26 00:04:56 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/26 00:04:56 visual_prompt]: tuned percent:0.534
[10/26 00:04:56 visual_prompt]: Device used for model: 0
[10/26 00:04:56 visual_prompt]: Setting up Evaluator...
[10/26 00:04:56 visual_prompt]: Setting up Trainer...
[10/26 00:04:56 visual_prompt]: 	Setting up the optimizer...
[10/26 00:04:56 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 00:10:03 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8753 s / batch. (data: 2.57e-04). ETA=3:21:18, max mem: 7.6 GB 
[10/26 00:11:52 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9906, average train loss: 1.3980
[10/26 00:12:40 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4596, average loss: 1.3816
[10/26 00:12:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/26 00:12:40 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[10/26 00:17:45 visual_prompt]: 	Training 100/139. train loss: 0.8240,	0.9119 s / batch. (data: 1.20e-02). ETA=3:27:38, max mem: 7.6 GB 
[10/26 00:19:36 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 2.9967, average train loss: 1.2549
[10/26 00:20:24 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4588, average loss: 1.1638
[10/26 00:20:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.55	
[10/26 00:20:24 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[10/26 00:25:29 visual_prompt]: 	Training 100/139. train loss: 0.6296,	0.8800 s / batch. (data: 2.97e-04). ETA=3:18:19, max mem: 7.6 GB 
[10/26 00:27:20 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9936, average train loss: 0.9046
[10/26 00:28:07 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4594, average loss: 1.3689
[10/26 00:28:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.99	
[10/26 00:28:07 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[10/26 00:33:09 visual_prompt]: 	Training 100/139. train loss: 0.8604,	0.8645 s / batch. (data: 2.95e-04). ETA=3:12:49, max mem: 7.6 GB 
[10/26 00:35:04 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 2.9952, average train loss: 1.1421
[10/26 00:35:51 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.4608, average loss: 1.0921
[10/26 00:35:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.15	
[10/26 00:35:51 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[10/26 00:41:01 visual_prompt]: 	Training 100/139. train loss: 0.9341,	11.5043 s / batch. (data: 1.06e+01). ETA=1 day, 18:19:22, max mem: 7.6 GB 
[10/26 00:42:48 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0021, average train loss: 1.1407
[10/26 00:43:36 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.4581, average loss: 1.7102
[10/26 00:43:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.26	
[10/26 00:43:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[10/26 00:48:41 visual_prompt]: 	Training 100/139. train loss: 0.5581,	1.5125 s / batch. (data: 6.49e-01). ETA=5:30:21, max mem: 7.6 GB 
[10/26 00:50:32 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 2.9968, average train loss: 1.3382
[10/26 00:51:20 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.4605, average loss: 1.0002
[10/26 00:51:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[10/26 00:51:20 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[10/26 00:56:27 visual_prompt]: 	Training 100/139. train loss: 0.5994,	0.8747 s / batch. (data: 2.57e-04). ETA=3:09:01, max mem: 7.6 GB 
[10/26 00:58:16 visual_prompt]: Epoch 7 / 100: avg data time: 2.13e+00, avg batch time: 2.9937, average train loss: 1.5899
[10/26 00:59:03 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4613, average loss: 0.7312
[10/26 00:59:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.88	
[10/26 00:59:03 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[10/26 01:04:11 visual_prompt]: 	Training 100/139. train loss: 0.6520,	0.8920 s / batch. (data: 2.93e-04). ETA=3:10:41, max mem: 7.6 GB 
[10/26 01:06:01 visual_prompt]: Epoch 8 / 100: avg data time: 2.14e+00, avg batch time: 3.0054, average train loss: 1.7096
[10/26 01:06:48 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.4594, average loss: 0.7900
[10/26 01:06:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.55	
[10/26 01:06:48 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[10/26 01:11:59 visual_prompt]: 	Training 100/139. train loss: 3.8040,	0.8719 s / batch. (data: 7.96e-03). ETA=3:04:23, max mem: 7.6 GB 
[10/26 01:13:46 visual_prompt]: Epoch 9 / 100: avg data time: 2.13e+00, avg batch time: 3.0001, average train loss: 1.8671
[10/26 01:14:33 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4584, average loss: 0.7770
[10/26 01:14:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.93	
[10/26 01:14:33 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[10/26 01:19:41 visual_prompt]: 	Training 100/139. train loss: 7.0390,	0.8948 s / batch. (data: 2.89e-04). ETA=3:07:09, max mem: 7.6 GB 
[10/26 01:21:29 visual_prompt]: Epoch 10 / 100: avg data time: 2.13e+00, avg batch time: 2.9955, average train loss: 3.5695
[10/26 01:22:17 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.4607, average loss: 2.3662
[10/26 01:22:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.64	
[10/26 01:22:17 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[10/26 01:27:23 visual_prompt]: 	Training 100/139. train loss: 1.1079,	0.8807 s / batch. (data: 4.37e-03). ETA=3:02:09, max mem: 7.6 GB 
[10/26 01:29:13 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 2.9961, average train loss: 5.4688
[10/26 01:30:01 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.4617, average loss: 10.1258
[10/26 01:30:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.50	
[10/26 01:30:01 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[10/26 01:35:08 visual_prompt]: 	Training 100/139. train loss: 9.7209,	0.8880 s / batch. (data: 2.79e-04). ETA=3:01:36, max mem: 7.6 GB 
[10/26 01:36:58 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 2.9991, average train loss: 5.5558
[10/26 01:37:45 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4617, average loss: 9.1782
[10/26 01:37:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.81	
[10/26 01:37:45 visual_prompt]: Best epoch 12: best metric: -9.178
[10/26 01:37:45 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[10/26 01:42:54 visual_prompt]: 	Training 100/139. train loss: 0.8740,	0.8816 s / batch. (data: 2.93e-04). ETA=2:58:16, max mem: 7.6 GB 
[10/26 01:44:43 visual_prompt]: Epoch 13 / 100: avg data time: 2.14e+00, avg batch time: 3.0052, average train loss: 1.9978
[10/26 01:45:30 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4605, average loss: 0.8966
[10/26 01:45:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.78	
[10/26 01:45:30 visual_prompt]: Best epoch 13: best metric: -0.897
[10/26 01:45:30 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[10/26 01:50:40 visual_prompt]: 	Training 100/139. train loss: 0.6778,	0.8946 s / batch. (data: 1.59e-02). ETA=2:58:48, max mem: 7.6 GB 
[10/26 01:52:27 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0026, average train loss: 1.3066
[10/26 01:53:15 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.4622, average loss: 0.6861
[10/26 01:53:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 55.35	
[10/26 01:53:15 visual_prompt]: Best epoch 14: best metric: -0.686
[10/26 01:53:15 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[10/26 01:58:28 visual_prompt]: 	Training 100/139. train loss: 1.0901,	9.5321 s / batch. (data: 8.64e+00). ETA=1 day, 7:23:13, max mem: 7.6 GB 
[10/26 02:00:12 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0018, average train loss: 2.2870
[10/26 02:00:59 visual_prompt]: Inference (val):avg data time: 7.24e-04, avg batch time: 0.4620, average loss: 0.6854
[10/26 02:00:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.49	
[10/26 02:00:59 visual_prompt]: Best epoch 15: best metric: -0.685
[10/26 02:00:59 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[10/26 02:06:05 visual_prompt]: 	Training 100/139. train loss: 5.9291,	0.8798 s / batch. (data: 1.05e-02). ETA=2:51:47, max mem: 7.6 GB 
[10/26 02:07:55 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9927, average train loss: 2.3705
[10/26 02:08:43 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4574, average loss: 6.0609
[10/26 02:08:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.41	
[10/26 02:08:43 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[10/26 02:13:50 visual_prompt]: 	Training 100/139. train loss: 1.4675,	4.6448 s / batch. (data: 3.72e+00). ETA=14:56:08, max mem: 7.6 GB 
[10/26 02:15:40 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 2.9983, average train loss: 1.6201
[10/26 02:16:27 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4591, average loss: 1.3136
[10/26 02:16:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.23	
[10/26 02:16:27 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[10/26 02:21:35 visual_prompt]: 	Training 100/139. train loss: 0.7508,	1.5471 s / batch. (data: 6.91e-01). ETA=4:54:54, max mem: 7.6 GB 
[10/26 02:23:24 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0015, average train loss: 1.1807
[10/26 02:24:12 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.4616, average loss: 0.8516
[10/26 02:24:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.85	
[10/26 02:24:12 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[10/26 02:29:23 visual_prompt]: 	Training 100/139. train loss: 0.6973,	8.3710 s / batch. (data: 7.50e+00). ETA=1 day, 2:16:15, max mem: 7.6 GB 
[10/26 02:31:09 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0005, average train loss: 1.0832
[10/26 02:31:56 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4603, average loss: 3.4511
[10/26 02:31:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.34	
[10/26 02:31:56 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[10/26 02:37:02 visual_prompt]: 	Training 100/139. train loss: 1.1709,	0.8720 s / batch. (data: 3.05e-04). ETA=2:42:10, max mem: 7.6 GB 
[10/26 02:38:53 visual_prompt]: Epoch 20 / 100: avg data time: 2.13e+00, avg batch time: 2.9970, average train loss: 2.1203
[10/26 02:39:40 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4579, average loss: 1.3943
[10/26 02:39:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.67	
[10/26 02:39:40 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[10/26 02:44:47 visual_prompt]: 	Training 100/139. train loss: 1.8944,	0.9068 s / batch. (data: 2.26e-02). ETA=2:46:32, max mem: 7.6 GB 
[10/26 02:46:37 visual_prompt]: Epoch 21 / 100: avg data time: 2.13e+00, avg batch time: 2.9985, average train loss: 1.1052
[10/26 02:47:25 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4600, average loss: 2.0512
[10/26 02:47:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.10	
[10/26 02:47:25 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[10/26 02:52:28 visual_prompt]: 	Training 100/139. train loss: 2.3011,	0.8863 s / batch. (data: 2.93e-04). ETA=2:40:43, max mem: 7.6 GB 
[10/26 02:54:22 visual_prompt]: Epoch 22 / 100: avg data time: 2.13e+00, avg batch time: 2.9984, average train loss: 1.8484
[10/26 02:55:09 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4636, average loss: 1.4274
[10/26 02:55:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.91	
[10/26 02:55:09 visual_prompt]: Stopping early.
[10/26 02:55:09 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 02:55:09 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 02:55:09 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 02:55:09 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 02:55:09 visual_prompt]: Training with config:
[10/26 02:55:09 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr1.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 02:55:09 visual_prompt]: Loading training data...
[10/26 02:55:09 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 02:55:09 visual_prompt]: Loading validation data...
[10/26 02:55:09 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 02:55:09 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/26 02:55:11 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/26 02:55:11 visual_prompt]: tuned percent:0.534
[10/26 02:55:12 visual_prompt]: Device used for model: 0
[10/26 02:55:12 visual_prompt]: Setting up Evaluator...
[10/26 02:55:12 visual_prompt]: Setting up Trainer...
[10/26 02:55:12 visual_prompt]: 	Setting up the optimizer...
[10/26 02:55:12 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 03:00:18 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8796 s / batch. (data: 2.55e-04). ETA=3:22:17, max mem: 7.6 GB 
[10/26 03:02:09 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9988, average train loss: 1.3980
[10/26 03:02:56 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.4622, average loss: 1.3816
[10/26 03:02:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/26 03:02:56 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[10/26 03:08:02 visual_prompt]: 	Training 100/139. train loss: 0.7410,	0.8922 s / batch. (data: 9.52e-03). ETA=3:23:07, max mem: 7.6 GB 
[10/26 03:09:53 visual_prompt]: Epoch 2 / 100: avg data time: 2.12e+00, avg batch time: 2.9984, average train loss: 0.9593
[10/26 03:10:40 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4623, average loss: 0.7543
[10/26 03:10:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.16	
[10/26 03:10:40 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[10/26 03:15:45 visual_prompt]: 	Training 100/139. train loss: 0.6851,	0.8882 s / batch. (data: 4.13e-03). ETA=3:20:09, max mem: 7.6 GB 
[10/26 03:17:37 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9976, average train loss: 0.7363
[10/26 03:18:24 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4624, average loss: 0.8256
[10/26 03:18:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.18	
[10/26 03:18:24 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[10/26 03:23:26 visual_prompt]: 	Training 100/139. train loss: 0.8709,	0.9040 s / batch. (data: 2.93e-04). ETA=3:21:38, max mem: 7.6 GB 
[10/26 03:25:21 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 2.9959, average train loss: 0.7411
[10/26 03:26:08 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.4591, average loss: 0.6977
[10/26 03:26:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.22	
[10/26 03:26:08 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[10/26 03:31:19 visual_prompt]: 	Training 100/139. train loss: 1.0033,	11.5127 s / batch. (data: 1.06e+01). ETA=1 day, 18:21:14, max mem: 7.6 GB 
[10/26 03:33:07 visual_prompt]: Epoch 5 / 100: avg data time: 2.14e+00, avg batch time: 3.0096, average train loss: 0.8091
[10/26 03:33:54 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4600, average loss: 0.7570
[10/26 03:33:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.95	
[10/26 03:33:54 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[10/26 03:39:00 visual_prompt]: 	Training 100/139. train loss: 0.6565,	0.8800 s / batch. (data: 1.14e-02). ETA=3:12:12, max mem: 7.6 GB 
[10/26 03:40:52 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 3.0030, average train loss: 1.0316
[10/26 03:41:39 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4603, average loss: 0.6969
[10/26 03:41:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.47	
[10/26 03:41:39 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[10/26 03:46:47 visual_prompt]: 	Training 100/139. train loss: 0.8871,	0.8800 s / batch. (data: 3.02e-04). ETA=3:10:10, max mem: 7.6 GB 
[10/26 03:48:36 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9970, average train loss: 0.9873
[10/26 03:49:23 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4616, average loss: 1.0774
[10/26 03:49:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.74	
[10/26 03:49:23 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[10/26 03:54:31 visual_prompt]: 	Training 100/139. train loss: 0.6911,	0.8643 s / batch. (data: 3.10e-04). ETA=3:04:46, max mem: 7.6 GB 
[10/26 03:56:21 visual_prompt]: Epoch 8 / 100: avg data time: 2.14e+00, avg batch time: 3.0093, average train loss: 0.9158
[10/26 03:57:09 visual_prompt]: Inference (val):avg data time: 4.81e-04, avg batch time: 0.4651, average loss: 3.1260
[10/26 03:57:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.32	
[10/26 03:57:09 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[10/26 04:02:19 visual_prompt]: 	Training 100/139. train loss: 1.2429,	0.8713 s / batch. (data: 1.05e-02). ETA=3:04:14, max mem: 7.6 GB 
[10/26 04:04:06 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9986, average train loss: 1.4113
[10/26 04:04:53 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.4610, average loss: 0.7904
[10/26 04:04:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.91	
[10/26 04:04:53 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[10/26 04:10:00 visual_prompt]: 	Training 100/139. train loss: 0.8482,	0.8680 s / batch. (data: 7.93e-03). ETA=3:01:32, max mem: 7.6 GB 
[10/26 04:11:49 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9968, average train loss: 1.1525
[10/26 04:12:37 visual_prompt]: Inference (val):avg data time: 4.92e-04, avg batch time: 0.4598, average loss: 1.0921
[10/26 04:12:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 53.50	
[10/26 04:12:37 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[10/26 04:17:44 visual_prompt]: 	Training 100/139. train loss: 0.6773,	0.9113 s / batch. (data: 5.42e-03). ETA=3:08:29, max mem: 7.6 GB 
[10/26 04:19:34 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0005, average train loss: 1.5340
[10/26 04:20:21 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4607, average loss: 1.1128
[10/26 04:20:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.63	
[10/26 04:20:21 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[10/26 04:25:29 visual_prompt]: 	Training 100/139. train loss: 1.2717,	0.8799 s / batch. (data: 2.88e-04). ETA=2:59:57, max mem: 7.6 GB 
[10/26 04:27:18 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9993, average train loss: 1.2373
[10/26 04:28:06 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4633, average loss: 0.9979
[10/26 04:28:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.58	
[10/26 04:28:06 visual_prompt]: Best epoch 12: best metric: -0.998
[10/26 04:28:06 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[10/26 04:33:15 visual_prompt]: 	Training 100/139. train loss: 0.7719,	0.8678 s / batch. (data: 2.66e-04). ETA=2:55:27, max mem: 7.6 GB 
[10/26 04:35:04 visual_prompt]: Epoch 13 / 100: avg data time: 2.14e+00, avg batch time: 3.0090, average train loss: 1.2696
[10/26 04:35:52 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.4618, average loss: 1.4242
[10/26 04:35:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.02	
[10/26 04:35:52 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[10/26 04:41:02 visual_prompt]: 	Training 100/139. train loss: 0.8195,	0.8910 s / batch. (data: 5.38e-03). ETA=2:58:06, max mem: 7.6 GB 
[10/26 04:42:49 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0057, average train loss: 1.2089
[10/26 04:43:37 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.4618, average loss: 0.8122
[10/26 04:43:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.68	
[10/26 04:43:37 visual_prompt]: Best epoch 14: best metric: -0.812
[10/26 04:43:37 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[10/26 04:48:50 visual_prompt]: 	Training 100/139. train loss: 1.4547,	9.4801 s / batch. (data: 8.61e+00). ETA=1 day, 7:12:57, max mem: 7.6 GB 
[10/26 04:50:34 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0016, average train loss: 1.9500
[10/26 04:51:22 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4584, average loss: 0.8038
[10/26 04:51:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.38	
[10/26 04:51:22 visual_prompt]: Best epoch 15: best metric: -0.804
[10/26 04:51:22 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[10/26 04:56:28 visual_prompt]: 	Training 100/139. train loss: 1.2531,	0.8880 s / batch. (data: 7.96e-03). ETA=2:53:23, max mem: 7.6 GB 
[10/26 04:58:18 visual_prompt]: Epoch 16 / 100: avg data time: 2.13e+00, avg batch time: 2.9986, average train loss: 1.2379
[10/26 04:59:06 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4618, average loss: 1.1520
[10/26 04:59:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.71	
[10/26 04:59:06 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[10/26 05:04:14 visual_prompt]: 	Training 100/139. train loss: 0.7021,	5.0527 s / batch. (data: 4.19e+00). ETA=16:14:50, max mem: 7.6 GB 
[10/26 05:06:03 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 3.0024, average train loss: 1.2352
[10/26 05:06:51 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4613, average loss: 0.7847
[10/26 05:06:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.09	
[10/26 05:06:51 visual_prompt]: Best epoch 17: best metric: -0.785
[10/26 05:06:51 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[10/26 05:11:59 visual_prompt]: 	Training 100/139. train loss: 0.9636,	0.8848 s / batch. (data: 2.92e-04). ETA=2:48:39, max mem: 7.6 GB 
[10/26 05:13:48 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0030, average train loss: 1.7639
[10/26 05:14:35 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.4595, average loss: 23.7672
[10/26 05:14:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.26	
[10/26 05:14:35 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[10/26 05:19:49 visual_prompt]: 	Training 100/139. train loss: 0.4952,	11.0674 s / batch. (data: 1.02e+01). ETA=1 day, 10:43:59, max mem: 7.6 GB 
[10/26 05:21:33 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0044, average train loss: 1.4583
[10/26 05:22:21 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.4629, average loss: 3.4058
[10/26 05:22:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.09	
[10/26 05:22:21 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[10/26 05:27:27 visual_prompt]: 	Training 100/139. train loss: 0.6902,	0.8607 s / batch. (data: 2.88e-04). ETA=2:40:04, max mem: 7.6 GB 
[10/26 05:29:17 visual_prompt]: Epoch 20 / 100: avg data time: 2.13e+00, avg batch time: 2.9971, average train loss: 1.4165
[10/26 05:30:05 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4584, average loss: 0.6972
[10/26 05:30:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.13	
[10/26 05:30:05 visual_prompt]: Best epoch 20: best metric: -0.697
[10/26 05:30:05 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[10/26 05:35:11 visual_prompt]: 	Training 100/139. train loss: 1.9711,	0.8800 s / batch. (data: 2.80e-04). ETA=2:41:37, max mem: 7.6 GB 
[10/26 05:37:02 visual_prompt]: Epoch 21 / 100: avg data time: 2.13e+00, avg batch time: 3.0016, average train loss: 2.4532
[10/26 05:37:49 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4603, average loss: 3.0718
[10/26 05:37:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.04	
[10/26 05:37:49 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[10/26 05:42:52 visual_prompt]: 	Training 100/139. train loss: 2.2665,	0.8712 s / batch. (data: 2.89e-04). ETA=2:37:59, max mem: 7.6 GB 
[10/26 05:44:46 visual_prompt]: Epoch 22 / 100: avg data time: 2.12e+00, avg batch time: 2.9951, average train loss: 1.4471
[10/26 05:45:33 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4571, average loss: 0.9875
[10/26 05:45:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.56	
[10/26 05:45:33 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[10/26 05:50:38 visual_prompt]: 	Training 100/139. train loss: 0.9293,	0.8770 s / batch. (data: 2.94e-04). ETA=2:37:00, max mem: 7.6 GB 
[10/26 05:52:31 visual_prompt]: Epoch 23 / 100: avg data time: 2.13e+00, avg batch time: 3.0047, average train loss: 1.4502
[10/26 05:53:18 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.4612, average loss: 4.7187
[10/26 05:53:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.61	
[10/26 05:53:18 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[10/26 05:58:30 visual_prompt]: 	Training 100/139. train loss: 0.7931,	0.8841 s / batch. (data: 2.29e-04). ETA=2:36:13, max mem: 7.6 GB 
[10/26 06:00:16 visual_prompt]: Epoch 24 / 100: avg data time: 2.13e+00, avg batch time: 3.0065, average train loss: 1.5885
[10/26 06:01:04 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.4616, average loss: 0.9086
[10/26 06:01:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.91	
[10/26 06:01:04 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[10/26 06:06:11 visual_prompt]: 	Training 100/139. train loss: 0.8692,	0.8760 s / batch. (data: 5.42e-03). ETA=2:32:46, max mem: 7.6 GB 
[10/26 06:08:00 visual_prompt]: Epoch 25 / 100: avg data time: 2.12e+00, avg batch time: 2.9966, average train loss: 0.9993
[10/26 06:08:48 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4640, average loss: 2.1210
[10/26 06:08:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.26	
[10/26 06:08:48 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[10/26 06:13:55 visual_prompt]: 	Training 100/139. train loss: 1.6137,	0.8834 s / batch. (data: 1.14e-02). ETA=2:32:00, max mem: 7.6 GB 
[10/26 06:15:44 visual_prompt]: Epoch 26 / 100: avg data time: 2.12e+00, avg batch time: 2.9942, average train loss: 1.2895
[10/26 06:16:31 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4599, average loss: 1.1427
[10/26 06:16:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.64	
[10/26 06:16:31 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[10/26 06:21:42 visual_prompt]: 	Training 100/139. train loss: 0.7122,	9.2751 s / batch. (data: 8.39e+00). ETA=1 day, 2:14:35, max mem: 7.6 GB 
[10/26 06:23:29 visual_prompt]: Epoch 27 / 100: avg data time: 2.13e+00, avg batch time: 3.0078, average train loss: 1.2496
[10/26 06:24:17 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.4594, average loss: 1.7111
[10/26 06:24:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.45	
[10/26 06:24:17 visual_prompt]: Stopping early.
[10/26 06:24:18 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 06:24:18 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 06:24:18 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 06:24:18 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 06:24:18 visual_prompt]: Training with config:
[10/26 06:24:18 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr1.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 06:24:18 visual_prompt]: Loading training data...
[10/26 06:24:18 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 06:24:18 visual_prompt]: Loading validation data...
[10/26 06:24:18 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 06:24:18 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/26 06:24:20 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/26 06:24:20 visual_prompt]: tuned percent:0.534
[10/26 06:24:21 visual_prompt]: Device used for model: 0
[10/26 06:24:21 visual_prompt]: Setting up Evaluator...
[10/26 06:24:21 visual_prompt]: Setting up Trainer...
[10/26 06:24:21 visual_prompt]: 	Setting up the optimizer...
[10/26 06:24:21 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 06:29:27 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.9258 s / batch. (data: 1.06e-03). ETA=3:32:55, max mem: 7.6 GB 
[10/26 06:31:17 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9950, average train loss: 1.3980
[10/26 06:32:04 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4600, average loss: 1.3816
[10/26 06:32:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/26 06:32:04 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[10/26 06:37:10 visual_prompt]: 	Training 100/139. train loss: 0.7200,	0.8882 s / batch. (data: 2.93e-04). ETA=3:22:14, max mem: 7.6 GB 
[10/26 06:39:01 visual_prompt]: Epoch 2 / 100: avg data time: 2.12e+00, avg batch time: 2.9957, average train loss: 0.9858
[10/26 06:39:48 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.4616, average loss: 0.8108
[10/26 06:39:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.76	
[10/26 06:39:48 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[10/26 06:44:55 visual_prompt]: 	Training 100/139. train loss: 0.6939,	0.9120 s / batch. (data: 2.91e-04). ETA=3:25:31, max mem: 7.6 GB 
[10/26 06:46:45 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9965, average train loss: 0.8016
[10/26 06:47:32 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.4566, average loss: 0.9717
[10/26 06:47:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.21	
[10/26 06:47:32 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[10/26 06:52:33 visual_prompt]: 	Training 100/139. train loss: 1.2038,	0.8624 s / batch. (data: 2.95e-04). ETA=3:12:21, max mem: 7.6 GB 
[10/26 06:54:28 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 2.9945, average train loss: 0.8114
[10/26 06:55:15 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.4580, average loss: 0.6916
[10/26 06:55:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 57.87	
[10/26 06:55:15 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[10/26 07:00:25 visual_prompt]: 	Training 100/139. train loss: 1.3243,	10.8376 s / batch. (data: 9.97e+00). ETA=1 day, 15:52:12, max mem: 7.6 GB 
[10/26 07:02:13 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0044, average train loss: 0.8644
[10/26 07:03:00 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4594, average loss: 0.7176
[10/26 07:03:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.55	
[10/26 07:03:00 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[10/26 07:08:06 visual_prompt]: 	Training 100/139. train loss: 0.6234,	0.9084 s / batch. (data: 1.57e-02). ETA=3:18:24, max mem: 7.6 GB 
[10/26 07:09:57 visual_prompt]: Epoch 6 / 100: avg data time: 2.12e+00, avg batch time: 2.9977, average train loss: 0.9246
[10/26 07:10:45 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4595, average loss: 0.6926
[10/26 07:10:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.99	
[10/26 07:10:45 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[10/26 07:15:52 visual_prompt]: 	Training 100/139. train loss: 0.7532,	0.8920 s / batch. (data: 2.85e-04). ETA=3:12:45, max mem: 7.6 GB 
[10/26 07:17:40 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9898, average train loss: 0.7706
[10/26 07:18:28 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.4608, average loss: 0.7299
[10/26 07:18:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.39	
[10/26 07:18:28 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[10/26 07:23:35 visual_prompt]: 	Training 100/139. train loss: 0.6929,	0.9000 s / batch. (data: 7.95e-03). ETA=3:12:24, max mem: 7.6 GB 
[10/26 07:25:25 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0020, average train loss: 1.0830
[10/26 07:26:12 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4639, average loss: 0.9035
[10/26 07:26:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.50	
[10/26 07:26:12 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[10/26 07:31:22 visual_prompt]: 	Training 100/139. train loss: 0.9273,	0.8794 s / batch. (data: 2.77e-04). ETA=3:05:57, max mem: 7.6 GB 
[10/26 07:33:09 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9966, average train loss: 0.9820
[10/26 07:33:56 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.4555, average loss: 0.7709
[10/26 07:33:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.40	
[10/26 07:33:56 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[10/26 07:39:02 visual_prompt]: 	Training 100/139. train loss: 1.2031,	0.8865 s / batch. (data: 5.42e-03). ETA=3:05:24, max mem: 7.6 GB 
[10/26 07:40:52 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9923, average train loss: 1.1216
[10/26 07:41:39 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4621, average loss: 1.1171
[10/26 07:41:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.19	
[10/26 07:41:39 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[10/26 07:46:45 visual_prompt]: 	Training 100/139. train loss: 0.7641,	0.8880 s / batch. (data: 3.01e-04). ETA=3:03:40, max mem: 7.6 GB 
[10/26 07:48:35 visual_prompt]: Epoch 11 / 100: avg data time: 2.12e+00, avg batch time: 2.9921, average train loss: 0.9092
[10/26 07:49:23 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4613, average loss: 0.7366
[10/26 07:49:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.87	
[10/26 07:49:23 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[10/26 07:54:28 visual_prompt]: 	Training 100/139. train loss: 0.7452,	1.2920 s / batch. (data: 4.09e-01). ETA=4:24:14, max mem: 7.6 GB 
[10/26 07:56:19 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9939, average train loss: 0.9308
[10/26 07:57:06 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4595, average loss: 0.7050
[10/26 07:57:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.05	
[10/26 07:57:06 visual_prompt]: Best epoch 12: best metric: -0.705
[10/26 07:57:06 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[10/26 08:02:15 visual_prompt]: 	Training 100/139. train loss: 0.7638,	0.8887 s / batch. (data: 2.30e-04). ETA=2:59:41, max mem: 7.6 GB 
[10/26 08:04:03 visual_prompt]: Epoch 13 / 100: avg data time: 2.12e+00, avg batch time: 2.9972, average train loss: 0.9359
[10/26 08:04:50 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4595, average loss: 0.7086
[10/26 08:04:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.23	
[10/26 08:04:50 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[10/26 08:10:00 visual_prompt]: 	Training 100/139. train loss: 0.7420,	0.9160 s / batch. (data: 7.25e-04). ETA=3:03:05, max mem: 7.6 GB 
[10/26 08:11:47 visual_prompt]: Epoch 14 / 100: avg data time: 2.12e+00, avg batch time: 2.9979, average train loss: 1.2503
[10/26 08:12:34 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4600, average loss: 1.6155
[10/26 08:12:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.71	
[10/26 08:12:34 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[10/26 08:17:47 visual_prompt]: 	Training 100/139. train loss: 0.7683,	9.5680 s / batch. (data: 8.68e+00). ETA=1 day, 7:30:19, max mem: 7.6 GB 
[10/26 08:19:32 visual_prompt]: Epoch 15 / 100: avg data time: 2.12e+00, avg batch time: 2.9995, average train loss: 1.1127
[10/26 08:20:19 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.4608, average loss: 0.9706
[10/26 08:20:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.58	
[10/26 08:20:19 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[10/26 08:25:25 visual_prompt]: 	Training 100/139. train loss: 0.7826,	0.9163 s / batch. (data: 2.56e-02). ETA=2:58:54, max mem: 7.6 GB 
[10/26 08:27:15 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9935, average train loss: 0.8691
[10/26 08:28:02 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4589, average loss: 0.8548
[10/26 08:28:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.43	
[10/26 08:28:02 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[10/26 08:33:08 visual_prompt]: 	Training 100/139. train loss: 0.7608,	3.0800 s / batch. (data: 2.19e+00). ETA=9:54:13, max mem: 7.6 GB 
[10/26 08:34:59 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 2.9939, average train loss: 0.8056
[10/26 08:35:46 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.4622, average loss: 0.7540
[10/26 08:35:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.31	
[10/26 08:35:46 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[10/26 08:40:54 visual_prompt]: 	Training 100/139. train loss: 0.6873,	3.0000 s / batch. (data: 2.12e+00). ETA=9:31:51, max mem: 7.6 GB 
[10/26 08:42:43 visual_prompt]: Epoch 18 / 100: avg data time: 2.12e+00, avg batch time: 2.9987, average train loss: 0.8560
[10/26 08:43:30 visual_prompt]: Inference (val):avg data time: 6.47e-04, avg batch time: 0.4627, average loss: 1.0237
[10/26 08:43:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.15	
[10/26 08:43:30 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[10/26 08:48:44 visual_prompt]: 	Training 100/139. train loss: 0.7887,	10.9759 s / batch. (data: 1.01e+01). ETA=1 day, 10:26:45, max mem: 7.6 GB 
[10/26 08:50:27 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0029, average train loss: 0.9223
[10/26 08:51:15 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.4611, average loss: 0.6933
[10/26 08:51:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.03	
[10/26 08:51:15 visual_prompt]: Best epoch 19: best metric: -0.693
[10/26 08:51:15 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[10/26 08:56:19 visual_prompt]: 	Training 100/139. train loss: 0.6241,	0.8760 s / batch. (data: 2.97e-04). ETA=2:42:55, max mem: 7.6 GB 
[10/26 08:58:11 visual_prompt]: Epoch 20 / 100: avg data time: 2.12e+00, avg batch time: 2.9919, average train loss: 0.9429
[10/26 08:58:58 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4564, average loss: 0.8041
[10/26 08:58:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.67	
[10/26 08:58:58 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[10/26 09:04:04 visual_prompt]: 	Training 100/139. train loss: 0.8663,	0.8810 s / batch. (data: 7.96e-03). ETA=2:41:48, max mem: 7.6 GB 
[10/26 09:05:54 visual_prompt]: Epoch 21 / 100: avg data time: 2.12e+00, avg batch time: 2.9936, average train loss: 0.8885
[10/26 09:06:41 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.4569, average loss: 1.5617
[10/26 09:06:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.50	
[10/26 09:06:41 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[10/26 09:11:44 visual_prompt]: 	Training 100/139. train loss: 0.6668,	0.8800 s / batch. (data: 2.87e-04). ETA=2:39:35, max mem: 7.6 GB 
[10/26 09:13:38 visual_prompt]: Epoch 22 / 100: avg data time: 2.12e+00, avg batch time: 2.9985, average train loss: 0.9420
[10/26 09:14:25 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4539, average loss: 1.7386
[10/26 09:14:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.53	
[10/26 09:14:25 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[10/26 09:19:31 visual_prompt]: 	Training 100/139. train loss: 0.7684,	1.5800 s / batch. (data: 7.06e-01). ETA=4:42:52, max mem: 7.6 GB 
[10/26 09:21:22 visual_prompt]: Epoch 23 / 100: avg data time: 2.12e+00, avg batch time: 2.9981, average train loss: 1.1908
[10/26 09:22:10 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.4577, average loss: 0.7394
[10/26 09:22:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.72	
[10/26 09:22:10 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[10/26 09:27:20 visual_prompt]: 	Training 100/139. train loss: 0.7049,	0.8844 s / batch. (data: 5.90e-03). ETA=2:36:17, max mem: 7.6 GB 
[10/26 09:29:07 visual_prompt]: Epoch 24 / 100: avg data time: 2.13e+00, avg batch time: 3.0005, average train loss: 0.9487
[10/26 09:29:54 visual_prompt]: Inference (val):avg data time: 4.83e-04, avg batch time: 0.4608, average loss: 0.7168
[10/26 09:29:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.85	
[10/26 09:29:54 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[10/26 09:35:02 visual_prompt]: 	Training 100/139. train loss: 0.6970,	0.8899 s / batch. (data: 5.43e-03). ETA=2:35:11, max mem: 7.6 GB 
[10/26 09:36:51 visual_prompt]: Epoch 25 / 100: avg data time: 2.12e+00, avg batch time: 2.9972, average train loss: 0.7817
[10/26 09:37:38 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4601, average loss: 0.7087
[10/26 09:37:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.12	
[10/26 09:37:38 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[10/26 09:42:45 visual_prompt]: 	Training 100/139. train loss: 0.5754,	2.3520 s / batch. (data: 1.48e+00). ETA=6:44:44, max mem: 7.6 GB 
[10/26 09:44:34 visual_prompt]: Epoch 26 / 100: avg data time: 2.12e+00, avg batch time: 2.9931, average train loss: 0.8331
[10/26 09:45:22 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4612, average loss: 0.6931
[10/26 09:45:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.88	
[10/26 09:45:22 visual_prompt]: Best epoch 26: best metric: -0.693
[10/26 09:45:22 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[10/26 09:50:34 visual_prompt]: 	Training 100/139. train loss: 1.0361,	9.4000 s / batch. (data: 8.53e+00). ETA=1 day, 2:35:48, max mem: 7.6 GB 
[10/26 09:52:21 visual_prompt]: Epoch 27 / 100: avg data time: 2.14e+00, avg batch time: 3.0180, average train loss: 0.9117
[10/26 09:53:09 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4593, average loss: 0.7061
[10/26 09:53:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.20	
[10/26 09:53:09 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[10/26 09:58:13 visual_prompt]: 	Training 100/139. train loss: 0.8767,	0.8649 s / batch. (data: 2.80e-04). ETA=2:24:49, max mem: 7.6 GB 
[10/26 10:00:05 visual_prompt]: Epoch 28 / 100: avg data time: 2.12e+00, avg batch time: 2.9967, average train loss: 0.8004
[10/26 10:00:52 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4608, average loss: 1.2841
[10/26 10:00:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.93	
[10/26 10:00:52 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[10/26 10:05:55 visual_prompt]: 	Training 100/139. train loss: 0.6796,	0.8956 s / batch. (data: 1.04e-02). ETA=2:27:53, max mem: 7.6 GB 
[10/26 10:07:48 visual_prompt]: Epoch 29 / 100: avg data time: 2.11e+00, avg batch time: 2.9897, average train loss: 0.8211
[10/26 10:08:36 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.4595, average loss: 0.8018
[10/26 10:08:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.63	
[10/26 10:08:36 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[10/26 10:13:45 visual_prompt]: 	Training 100/139. train loss: 0.7286,	0.9072 s / batch. (data: 2.96e-04). ETA=2:27:42, max mem: 7.6 GB 
[10/26 10:15:33 visual_prompt]: Epoch 30 / 100: avg data time: 2.13e+00, avg batch time: 3.0044, average train loss: 1.0815
[10/26 10:16:21 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.4623, average loss: 0.6982
[10/26 10:16:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.46	
[10/26 10:16:21 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[10/26 10:21:33 visual_prompt]: 	Training 100/139. train loss: 0.8247,	0.9043 s / batch. (data: 2.34e-04). ETA=2:25:08, max mem: 7.6 GB 
[10/26 10:23:18 visual_prompt]: Epoch 31 / 100: avg data time: 2.13e+00, avg batch time: 3.0018, average train loss: 0.8380
[10/26 10:24:05 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.4627, average loss: 0.6884
[10/26 10:24:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.46	
[10/26 10:24:05 visual_prompt]: Best epoch 31: best metric: -0.688
[10/26 10:24:05 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[10/26 10:29:15 visual_prompt]: 	Training 100/139. train loss: 0.9264,	7.4219 s / batch. (data: 6.55e+00). ETA=19:34:00, max mem: 7.6 GB 
[10/26 10:31:01 visual_prompt]: Epoch 32 / 100: avg data time: 2.12e+00, avg batch time: 2.9931, average train loss: 0.8364
[10/26 10:31:49 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4584, average loss: 1.6162
[10/26 10:31:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.75	
[10/26 10:31:49 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[10/26 10:36:56 visual_prompt]: 	Training 100/139. train loss: 0.7361,	0.8960 s / batch. (data: 7.69e-04). ETA=2:19:39, max mem: 7.6 GB 
[10/26 10:38:46 visual_prompt]: Epoch 33 / 100: avg data time: 2.13e+00, avg batch time: 3.0017, average train loss: 0.9762
[10/26 10:39:33 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.4634, average loss: 0.6881
[10/26 10:39:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.88	
[10/26 10:39:33 visual_prompt]: Best epoch 33: best metric: -0.688
[10/26 10:39:33 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[10/26 10:44:42 visual_prompt]: 	Training 100/139. train loss: 0.6994,	3.5114 s / batch. (data: 2.60e+00). ETA=8:59:10, max mem: 7.6 GB 
[10/26 10:46:31 visual_prompt]: Epoch 34 / 100: avg data time: 2.13e+00, avg batch time: 3.0007, average train loss: 0.7849
[10/26 10:47:18 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4584, average loss: 0.9819
[10/26 10:47:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.00	
[10/26 10:47:18 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[10/26 10:52:27 visual_prompt]: 	Training 100/139. train loss: 0.3764,	0.8831 s / batch. (data: 4.41e-04). ETA=2:13:33, max mem: 7.6 GB 
[10/26 10:54:15 visual_prompt]: Epoch 35 / 100: avg data time: 2.12e+00, avg batch time: 2.9956, average train loss: 1.2223
[10/26 10:55:02 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.4634, average loss: 0.9885
[10/26 10:55:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.69	
[10/26 10:55:02 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[10/26 11:00:12 visual_prompt]: 	Training 100/139. train loss: 0.6447,	0.9151 s / batch. (data: 1.55e-02). ETA=2:16:16, max mem: 7.6 GB 
[10/26 11:01:59 visual_prompt]: Epoch 36 / 100: avg data time: 2.13e+00, avg batch time: 3.0025, average train loss: 0.9840
[10/26 11:02:47 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4612, average loss: 0.7382
[10/26 11:02:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.98	
[10/26 11:02:47 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[10/26 11:07:53 visual_prompt]: 	Training 100/139. train loss: 1.1714,	0.8880 s / batch. (data: 2.74e-04). ETA=2:10:10, max mem: 7.6 GB 
[10/26 11:09:43 visual_prompt]: Epoch 37 / 100: avg data time: 2.12e+00, avg batch time: 2.9983, average train loss: 0.8395
[10/26 11:10:31 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4592, average loss: 0.7616
[10/26 11:10:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.77	
[10/26 11:10:31 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[10/26 11:15:40 visual_prompt]: 	Training 100/139. train loss: 0.6837,	6.8317 s / batch. (data: 5.94e+00). ETA=16:25:41, max mem: 7.6 GB 
[10/26 11:17:27 visual_prompt]: Epoch 38 / 100: avg data time: 2.12e+00, avg batch time: 2.9943, average train loss: 0.8953
[10/26 11:18:14 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4583, average loss: 1.3395
[10/26 11:18:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.85	
[10/26 11:18:14 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[10/26 11:23:26 visual_prompt]: 	Training 100/139. train loss: 0.6679,	0.8759 s / batch. (data: 2.97e-04). ETA=2:04:21, max mem: 7.6 GB 
[10/26 11:25:11 visual_prompt]: Epoch 39 / 100: avg data time: 2.12e+00, avg batch time: 2.9980, average train loss: 1.2093
[10/26 11:25:58 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.4614, average loss: 1.5052
[10/26 11:25:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.60	
[10/26 11:25:58 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[10/26 11:31:06 visual_prompt]: 	Training 100/139. train loss: 0.6636,	0.8723 s / batch. (data: 2.84e-04). ETA=2:01:48, max mem: 7.6 GB 
[10/26 11:32:55 visual_prompt]: Epoch 40 / 100: avg data time: 2.12e+00, avg batch time: 2.9978, average train loss: 0.9446
[10/26 11:33:42 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4590, average loss: 1.2511
[10/26 11:33:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.57	
[10/26 11:33:42 visual_prompt]: Stopping early.
[10/26 11:33:43 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 11:33:43 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 11:33:43 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 11:33:43 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 11:33:43 visual_prompt]: Training with config:
[10/26 11:33:43 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr1.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 11:33:43 visual_prompt]: Loading training data...
[10/26 11:33:43 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 11:33:43 visual_prompt]: Loading validation data...
[10/26 11:33:43 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 11:33:43 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/26 11:33:45 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/26 11:33:45 visual_prompt]: tuned percent:0.534
[10/26 11:33:45 visual_prompt]: Device used for model: 0
[10/26 11:33:45 visual_prompt]: Setting up Evaluator...
[10/26 11:33:45 visual_prompt]: Setting up Trainer...
[10/26 11:33:45 visual_prompt]: 	Setting up the optimizer...
[10/26 11:33:45 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 11:38:52 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8840 s / batch. (data: 7.82e-04). ETA=3:23:19, max mem: 7.6 GB 
[10/26 11:40:42 visual_prompt]: Epoch 1 / 100: avg data time: 2.13e+00, avg batch time: 2.9969, average train loss: 1.3980
[10/26 11:41:29 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4599, average loss: 1.3816
[10/26 11:41:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/26 11:41:29 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[10/26 11:46:34 visual_prompt]: 	Training 100/139. train loss: 0.7000,	0.8740 s / batch. (data: 2.43e-04). ETA=3:18:59, max mem: 7.6 GB 
[10/26 11:48:26 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 3.0002, average train loss: 0.9919
[10/26 11:49:13 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4617, average loss: 0.8202
[10/26 11:49:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.94	
[10/26 11:49:13 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[10/26 11:54:18 visual_prompt]: 	Training 100/139. train loss: 0.6866,	0.8760 s / batch. (data: 2.79e-04). ETA=3:17:25, max mem: 7.6 GB 
[10/26 11:56:10 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9933, average train loss: 0.8172
[10/26 11:56:57 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.4630, average loss: 1.0466
[10/26 11:56:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.39	
[10/26 11:56:57 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[10/26 12:02:10 visual_prompt]: 	Training 100/139. train loss: 1.2900,	0.8800 s / batch. (data: 2.89e-04). ETA=3:16:16, max mem: 7.6 GB 
[10/26 12:04:14 visual_prompt]: Epoch 4 / 100: avg data time: 2.27e+00, avg batch time: 3.1413, average train loss: 0.8468
[10/26 12:05:02 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4581, average loss: 0.7134
[10/26 12:05:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.78	
[10/26 12:05:02 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[10/26 12:10:12 visual_prompt]: 	Training 100/139. train loss: 1.0609,	11.1519 s / batch. (data: 1.02e+01). ETA=1 day, 17:01:36, max mem: 7.6 GB 
[10/26 12:12:00 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0004, average train loss: 0.9098
[10/26 12:12:47 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4603, average loss: 0.7816
[10/26 12:12:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.88	
[10/26 12:12:47 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[10/26 12:17:52 visual_prompt]: 	Training 100/139. train loss: 1.1206,	0.8767 s / batch. (data: 3.06e-04). ETA=3:11:28, max mem: 7.6 GB 
[10/26 12:19:44 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 2.9979, average train loss: 1.0373
[10/26 12:20:31 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4622, average loss: 0.7073
[10/26 12:20:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 57.31	
[10/26 12:20:31 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[10/26 12:25:38 visual_prompt]: 	Training 100/139. train loss: 1.0744,	2.4720 s / batch. (data: 1.61e+00). ETA=8:54:11, max mem: 7.6 GB 
[10/26 12:27:27 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9922, average train loss: 0.9015
[10/26 12:28:14 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4625, average loss: 0.6849
[10/26 12:28:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 58.71	
[10/26 12:28:14 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[10/26 12:33:22 visual_prompt]: 	Training 100/139. train loss: 0.7434,	0.8902 s / batch. (data: 5.41e-03). ETA=3:10:18, max mem: 7.6 GB 
[10/26 12:35:12 visual_prompt]: Epoch 8 / 100: avg data time: 2.14e+00, avg batch time: 3.0065, average train loss: 0.8828
[10/26 12:36:00 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4602, average loss: 0.7329
[10/26 12:36:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.36	
[10/26 12:36:00 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[10/26 12:41:08 visual_prompt]: 	Training 100/139. train loss: 3.3814,	0.8761 s / batch. (data: 2.90e-04). ETA=3:05:16, max mem: 7.6 GB 
[10/26 12:42:56 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9941, average train loss: 1.2668
[10/26 12:43:43 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4581, average loss: 1.0637
[10/26 12:43:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.78	
[10/26 12:43:43 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[10/26 12:48:50 visual_prompt]: 	Training 100/139. train loss: 2.0190,	0.8646 s / batch. (data: 2.89e-04). ETA=3:00:50, max mem: 7.6 GB 
[10/26 12:50:39 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9938, average train loss: 1.1734
[10/26 12:51:27 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4618, average loss: 1.5400
[10/26 12:51:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.07	
[10/26 12:51:27 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[10/26 12:56:33 visual_prompt]: 	Training 100/139. train loss: 0.6924,	0.8640 s / batch. (data: 2.83e-04). ETA=2:58:42, max mem: 7.6 GB 
[10/26 12:58:23 visual_prompt]: Epoch 11 / 100: avg data time: 2.12e+00, avg batch time: 2.9941, average train loss: 1.0898
[10/26 12:59:10 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.4611, average loss: 1.0443
[10/26 12:59:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.74	
[10/26 12:59:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[10/26 13:04:17 visual_prompt]: 	Training 100/139. train loss: 1.0570,	0.9066 s / batch. (data: 2.25e-02). ETA=3:05:24, max mem: 7.6 GB 
[10/26 13:06:06 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9907, average train loss: 1.0604
[10/26 13:06:53 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.4653, average loss: 0.9458
[10/26 13:06:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.49	
[10/26 13:06:53 visual_prompt]: Best epoch 12: best metric: -0.946
[10/26 13:06:53 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[10/26 13:12:02 visual_prompt]: 	Training 100/139. train loss: 0.7901,	0.8887 s / batch. (data: 7.96e-03). ETA=2:59:41, max mem: 7.6 GB 
[10/26 13:13:51 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0006, average train loss: 0.9574
[10/26 13:14:40 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.4615, average loss: 0.7455
[10/26 13:14:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.10	
[10/26 13:14:40 visual_prompt]: Best epoch 13: best metric: -0.745
[10/26 13:14:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[10/26 13:19:50 visual_prompt]: 	Training 100/139. train loss: 0.6774,	0.8779 s / batch. (data: 5.35e-03). ETA=2:55:28, max mem: 7.6 GB 
[10/26 13:21:37 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 2.9988, average train loss: 0.8228
[10/26 13:22:24 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4627, average loss: 0.8388
[10/26 13:22:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.55	
[10/26 13:22:24 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[10/26 13:27:36 visual_prompt]: 	Training 100/139. train loss: 0.6457,	9.5180 s / batch. (data: 8.64e+00). ETA=1 day, 7:20:26, max mem: 7.6 GB 
[10/26 13:29:21 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 2.9958, average train loss: 0.7998
[10/26 13:30:08 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.4598, average loss: 1.0479
[10/26 13:30:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.62	
[10/26 13:30:08 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[10/26 13:35:16 visual_prompt]: 	Training 100/139. train loss: 0.7559,	0.8799 s / batch. (data: 2.72e-04). ETA=2:51:48, max mem: 7.6 GB 
[10/26 13:37:04 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9907, average train loss: 0.9644
[10/26 13:37:51 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4613, average loss: 0.9678
[10/26 13:37:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.20	
[10/26 13:37:51 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[10/26 13:43:00 visual_prompt]: 	Training 100/139. train loss: 1.5481,	7.8599 s / batch. (data: 6.96e+00). ETA=1 day, 1:16:26, max mem: 7.6 GB 
[10/26 13:44:47 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 2.9930, average train loss: 0.8526
[10/26 13:45:35 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.4601, average loss: 0.7389
[10/26 13:45:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.67	
[10/26 13:45:35 visual_prompt]: Best epoch 17: best metric: -0.739
[10/26 13:45:35 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[10/26 13:50:43 visual_prompt]: 	Training 100/139. train loss: 0.6980,	0.8751 s / batch. (data: 3.08e-04). ETA=2:46:48, max mem: 7.6 GB 
[10/26 13:52:33 visual_prompt]: Epoch 18 / 100: avg data time: 2.14e+00, avg batch time: 3.0066, average train loss: 0.7967
[10/26 13:53:20 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4583, average loss: 0.9676
[10/26 13:53:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.72	
[10/26 13:53:20 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[10/26 13:58:33 visual_prompt]: 	Training 100/139. train loss: 0.7556,	10.7679 s / batch. (data: 9.89e+00). ETA=1 day, 9:47:36, max mem: 7.6 GB 
[10/26 14:00:17 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0000, average train loss: 0.7932
[10/26 14:01:04 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4619, average loss: 1.0863
[10/26 14:01:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.12	
[10/26 14:01:04 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[10/26 14:06:11 visual_prompt]: 	Training 100/139. train loss: 2.9489,	0.8842 s / batch. (data: 5.41e-03). ETA=2:44:26, max mem: 7.6 GB 
[10/26 14:08:01 visual_prompt]: Epoch 20 / 100: avg data time: 2.13e+00, avg batch time: 2.9994, average train loss: 1.0727
[10/26 14:08:48 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4639, average loss: 0.7077
[10/26 14:08:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 59.36	
[10/26 14:08:48 visual_prompt]: Best epoch 20: best metric: -0.708
[10/26 14:08:48 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[10/26 14:13:54 visual_prompt]: 	Training 100/139. train loss: 0.9493,	0.8746 s / batch. (data: 4.09e-03). ETA=2:40:37, max mem: 7.6 GB 
[10/26 14:15:44 visual_prompt]: Epoch 21 / 100: avg data time: 2.12e+00, avg batch time: 2.9925, average train loss: 0.9188
[10/26 14:16:32 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4584, average loss: 0.7733
[10/26 14:16:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.75	
[10/26 14:16:32 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[10/26 14:21:36 visual_prompt]: 	Training 100/139. train loss: 1.0905,	0.8760 s / batch. (data: 2.86e-04). ETA=2:38:51, max mem: 7.6 GB 
[10/26 14:23:28 visual_prompt]: Epoch 22 / 100: avg data time: 2.12e+00, avg batch time: 2.9926, average train loss: 1.0881
[10/26 14:24:15 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.4636, average loss: 2.5827
[10/26 14:24:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.00	
[10/26 14:24:15 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[10/26 14:29:21 visual_prompt]: 	Training 100/139. train loss: 0.7867,	0.8835 s / batch. (data: 7.43e-03). ETA=2:38:10, max mem: 7.6 GB 
[10/26 14:31:13 visual_prompt]: Epoch 23 / 100: avg data time: 2.13e+00, avg batch time: 3.0016, average train loss: 1.0537
[10/26 14:32:00 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4604, average loss: 0.8230
[10/26 14:32:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.08	
[10/26 14:32:00 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[10/26 14:37:11 visual_prompt]: 	Training 100/139. train loss: 0.6703,	0.8600 s / batch. (data: 3.19e-04). ETA=2:31:58, max mem: 7.6 GB 
[10/26 14:38:58 visual_prompt]: Epoch 24 / 100: avg data time: 2.14e+00, avg batch time: 3.0077, average train loss: 0.9414
[10/26 14:39:45 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4646, average loss: 0.7533
[10/26 14:39:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.27	
[10/26 14:39:45 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[10/26 14:44:52 visual_prompt]: 	Training 100/139. train loss: 0.6957,	0.8863 s / batch. (data: 5.40e-03). ETA=2:34:33, max mem: 7.6 GB 
[10/26 14:46:42 visual_prompt]: Epoch 25 / 100: avg data time: 2.13e+00, avg batch time: 2.9974, average train loss: 0.8629
[10/26 14:47:30 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4631, average loss: 0.8532
[10/26 14:47:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.84	
[10/26 14:47:30 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[10/26 14:52:37 visual_prompt]: 	Training 100/139. train loss: 0.5452,	0.8749 s / batch. (data: 3.59e-04). ETA=2:30:33, max mem: 7.6 GB 
[10/26 14:54:26 visual_prompt]: Epoch 26 / 100: avg data time: 2.12e+00, avg batch time: 2.9936, average train loss: 0.9115
[10/26 14:55:13 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4568, average loss: 0.6990
[10/26 14:55:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.53	
[10/26 14:55:13 visual_prompt]: Best epoch 26: best metric: -0.699
[10/26 14:55:13 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[10/26 15:00:24 visual_prompt]: 	Training 100/139. train loss: 0.8141,	9.2553 s / batch. (data: 8.38e+00). ETA=1 day, 2:11:14, max mem: 7.6 GB 
[10/26 15:02:11 visual_prompt]: Epoch 27 / 100: avg data time: 2.14e+00, avg batch time: 3.0065, average train loss: 0.8107
[10/26 15:02:58 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4624, average loss: 0.6814
[10/26 15:02:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 57.62	
[10/26 15:02:58 visual_prompt]: Best epoch 27: best metric: -0.681
[10/26 15:02:58 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[10/26 15:08:04 visual_prompt]: 	Training 100/139. train loss: 0.6938,	0.8793 s / batch. (data: 2.05e-03). ETA=2:27:14, max mem: 7.6 GB 
[10/26 15:09:55 visual_prompt]: Epoch 28 / 100: avg data time: 2.13e+00, avg batch time: 2.9976, average train loss: 0.7839
[10/26 15:10:42 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.4613, average loss: 1.0946
[10/26 15:10:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.39	
[10/26 15:10:42 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[10/26 15:15:46 visual_prompt]: 	Training 100/139. train loss: 0.7931,	0.8720 s / batch. (data: 7.96e-03). ETA=2:23:59, max mem: 7.6 GB 
[10/26 15:17:40 visual_prompt]: Epoch 29 / 100: avg data time: 2.13e+00, avg batch time: 3.0007, average train loss: 0.7988
[10/26 15:18:27 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4618, average loss: 0.8469
[10/26 15:18:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.17	
[10/26 15:18:27 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[10/26 15:23:37 visual_prompt]: 	Training 100/139. train loss: 0.7153,	0.8757 s / batch. (data: 1.52e-02). ETA=2:22:34, max mem: 7.6 GB 
[10/26 15:25:25 visual_prompt]: Epoch 30 / 100: avg data time: 2.14e+00, avg batch time: 3.0077, average train loss: 0.8863
[10/26 15:26:12 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4623, average loss: 0.6767
[10/26 15:26:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 59.00	
[10/26 15:26:12 visual_prompt]: Best epoch 30: best metric: -0.677
[10/26 15:26:12 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[10/26 15:31:25 visual_prompt]: 	Training 100/139. train loss: 0.8107,	0.8760 s / batch. (data: 2.86e-04). ETA=2:20:36, max mem: 7.6 GB 
[10/26 15:33:10 visual_prompt]: Epoch 31 / 100: avg data time: 2.13e+00, avg batch time: 3.0007, average train loss: 0.7726
[10/26 15:33:57 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.4612, average loss: 0.8633
[10/26 15:33:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.18	
[10/26 15:33:57 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[10/26 15:39:07 visual_prompt]: 	Training 100/139. train loss: 0.9225,	6.2650 s / batch. (data: 5.36e+00). ETA=16:31:00, max mem: 7.6 GB 
[10/26 15:40:53 visual_prompt]: Epoch 32 / 100: avg data time: 2.12e+00, avg batch time: 2.9946, average train loss: 0.7879
[10/26 15:41:40 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4623, average loss: 1.0440
[10/26 15:41:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.26	
[10/26 15:41:40 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[10/26 15:46:47 visual_prompt]: 	Training 100/139. train loss: 0.6897,	0.8759 s / batch. (data: 8.30e-03). ETA=2:16:31, max mem: 7.6 GB 
[10/26 15:48:38 visual_prompt]: Epoch 33 / 100: avg data time: 2.13e+00, avg batch time: 3.0036, average train loss: 0.8746
[10/26 15:49:25 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4648, average loss: 0.6914
[10/26 15:49:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.46	
[10/26 15:49:25 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[10/26 15:54:35 visual_prompt]: 	Training 100/139. train loss: 0.7644,	6.4127 s / batch. (data: 5.55e+00). ETA=16:24:40, max mem: 7.6 GB 
[10/26 15:56:22 visual_prompt]: Epoch 34 / 100: avg data time: 2.13e+00, avg batch time: 2.9983, average train loss: 0.7497
[10/26 15:57:09 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.4586, average loss: 0.8402
[10/26 15:57:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.42	
[10/26 15:57:09 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[10/26 16:02:18 visual_prompt]: 	Training 100/139. train loss: 0.4237,	0.8800 s / batch. (data: 2.74e-04). ETA=2:13:05, max mem: 7.6 GB 
[10/26 16:04:06 visual_prompt]: Epoch 35 / 100: avg data time: 2.12e+00, avg batch time: 2.9940, average train loss: 0.8288
[10/26 16:04:53 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4605, average loss: 0.7878
[10/26 16:04:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.58	
[10/26 16:04:53 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[10/26 16:10:02 visual_prompt]: 	Training 100/139. train loss: 0.8194,	0.8976 s / batch. (data: 2.15e-02). ETA=2:13:39, max mem: 7.6 GB 
[10/26 16:11:50 visual_prompt]: Epoch 36 / 100: avg data time: 2.13e+00, avg batch time: 3.0013, average train loss: 0.8491
[10/26 16:12:37 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4651, average loss: 0.9080
[10/26 16:12:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.26	
[10/26 16:12:37 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[10/26 16:17:45 visual_prompt]: 	Training 100/139. train loss: 0.9203,	0.8716 s / batch. (data: 2.95e-04). ETA=2:07:46, max mem: 7.6 GB 
[10/26 16:19:35 visual_prompt]: Epoch 37 / 100: avg data time: 2.13e+00, avg batch time: 3.0014, average train loss: 0.7725
[10/26 16:20:22 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.4620, average loss: 0.7343
[10/26 16:20:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.63	
[10/26 16:20:22 visual_prompt]: Stopping early.
[10/26 16:20:22 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 16:20:22 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 16:20:22 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 16:20:22 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 16:20:22 visual_prompt]: Training with config:
[10/26 16:20:22 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr1.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 16:20:22 visual_prompt]: Loading training data...
[10/26 16:20:22 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 16:20:22 visual_prompt]: Loading validation data...
[10/26 16:20:22 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 16:20:22 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/26 16:20:24 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/26 16:20:24 visual_prompt]: tuned percent:0.534
[10/26 16:20:25 visual_prompt]: Device used for model: 0
[10/26 16:20:25 visual_prompt]: Setting up Evaluator...
[10/26 16:20:25 visual_prompt]: Setting up Trainer...
[10/26 16:20:25 visual_prompt]: 	Setting up the optimizer...
[10/26 16:20:25 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 16:25:31 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8960 s / batch. (data: 3.06e-04). ETA=3:26:04, max mem: 7.6 GB 
[10/26 16:27:21 visual_prompt]: Epoch 1 / 100: avg data time: 2.11e+00, avg batch time: 2.9926, average train loss: 1.3980
[10/26 16:28:08 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4610, average loss: 1.3816
[10/26 16:28:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/26 16:28:08 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[10/26 16:33:15 visual_prompt]: 	Training 100/139. train loss: 0.7000,	0.8760 s / batch. (data: 7.98e-03). ETA=3:19:27, max mem: 7.6 GB 
[10/26 16:35:05 visual_prompt]: Epoch 2 / 100: avg data time: 2.12e+00, avg batch time: 2.9989, average train loss: 0.9921
[10/26 16:35:52 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4583, average loss: 0.8203
[10/26 16:35:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.15	
[10/26 16:35:52 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[10/26 16:40:56 visual_prompt]: 	Training 100/139. train loss: 0.6859,	0.8964 s / batch. (data: 2.97e-04). ETA=3:22:01, max mem: 7.6 GB 
[10/26 16:42:48 visual_prompt]: Epoch 3 / 100: avg data time: 2.11e+00, avg batch time: 2.9910, average train loss: 0.8189
[10/26 16:43:35 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.4596, average loss: 1.0512
[10/26 16:43:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.28	
[10/26 16:43:35 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[10/26 16:48:37 visual_prompt]: 	Training 100/139. train loss: 1.2880,	0.8992 s / batch. (data: 1.05e-02). ETA=3:20:34, max mem: 7.6 GB 
[10/26 16:50:32 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 2.9967, average train loss: 0.8495
[10/26 16:51:19 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4579, average loss: 0.7098
[10/26 16:51:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 57.88	
[10/26 16:51:19 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[10/26 16:56:29 visual_prompt]: 	Training 100/139. train loss: 0.9325,	11.5878 s / batch. (data: 1.07e+01). ETA=1 day, 18:37:48, max mem: 7.6 GB 
[10/26 16:58:18 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0076, average train loss: 0.9129
[10/26 16:59:05 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.4586, average loss: 0.8644
[10/26 16:59:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.91	
[10/26 16:59:05 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[10/26 17:04:10 visual_prompt]: 	Training 100/139. train loss: 1.0907,	1.1078 s / batch. (data: 2.02e-01). ETA=4:01:57, max mem: 7.6 GB 
[10/26 17:06:02 visual_prompt]: Epoch 6 / 100: avg data time: 2.12e+00, avg batch time: 2.9983, average train loss: 1.0500
[10/26 17:06:49 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.4585, average loss: 0.7232
[10/26 17:06:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.51	
[10/26 17:06:49 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[10/26 17:11:56 visual_prompt]: 	Training 100/139. train loss: 1.0995,	0.8967 s / batch. (data: 1.55e-02). ETA=3:13:46, max mem: 7.6 GB 
[10/26 17:13:46 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9989, average train loss: 0.9243
[10/26 17:14:33 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4561, average loss: 0.6856
[10/26 17:14:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.38	
[10/26 17:14:33 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[10/26 17:19:40 visual_prompt]: 	Training 100/139. train loss: 0.6573,	0.8720 s / batch. (data: 2.89e-04). ETA=3:06:25, max mem: 7.6 GB 
[10/26 17:21:31 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0041, average train loss: 0.8753
[10/26 17:22:18 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4581, average loss: 0.8221
[10/26 17:22:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.78	
[10/26 17:22:18 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[10/26 17:27:27 visual_prompt]: 	Training 100/139. train loss: 2.1509,	0.8760 s / batch. (data: 2.84e-04). ETA=3:05:14, max mem: 7.6 GB 
[10/26 17:29:15 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9955, average train loss: 1.1995
[10/26 17:30:02 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4586, average loss: 0.6893
[10/26 17:30:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 58.05	
[10/26 17:30:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[10/26 17:35:08 visual_prompt]: 	Training 100/139. train loss: 1.8814,	0.8899 s / batch. (data: 1.02e-02). ETA=3:06:07, max mem: 7.6 GB 
[10/26 17:36:58 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9927, average train loss: 1.2606
[10/26 17:37:46 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4588, average loss: 2.0221
[10/26 17:37:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.40	
[10/26 17:37:46 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[10/26 17:42:53 visual_prompt]: 	Training 100/139. train loss: 0.6893,	0.9059 s / batch. (data: 2.18e-02). ETA=3:07:21, max mem: 7.6 GB 
[10/26 17:44:42 visual_prompt]: Epoch 11 / 100: avg data time: 2.12e+00, avg batch time: 2.9984, average train loss: 1.2382
[10/26 17:45:30 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4594, average loss: 1.0566
[10/26 17:45:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.23	
[10/26 17:45:30 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[10/26 17:50:37 visual_prompt]: 	Training 100/139. train loss: 1.3549,	1.2245 s / batch. (data: 3.61e-01). ETA=4:10:26, max mem: 7.6 GB 
[10/26 17:52:27 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9981, average train loss: 1.2787
[10/26 17:53:14 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4583, average loss: 2.6617
[10/26 17:53:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.06	
[10/26 17:53:14 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[10/26 17:58:24 visual_prompt]: 	Training 100/139. train loss: 0.9511,	0.9168 s / batch. (data: 2.88e-02). ETA=3:05:22, max mem: 7.6 GB 
[10/26 18:00:12 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0062, average train loss: 1.3094
[10/26 18:00:59 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4618, average loss: 0.6762
[10/26 18:00:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 59.42	
[10/26 18:00:59 visual_prompt]: Best epoch 13: best metric: -0.676
[10/26 18:00:59 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[10/26 18:06:09 visual_prompt]: 	Training 100/139. train loss: 0.6469,	0.8865 s / batch. (data: 7.44e-04). ETA=2:57:11, max mem: 7.6 GB 
[10/26 18:07:56 visual_prompt]: Epoch 14 / 100: avg data time: 2.12e+00, avg batch time: 2.9990, average train loss: 0.9179
[10/26 18:08:43 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4584, average loss: 1.0973
[10/26 18:08:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.91	
[10/26 18:08:43 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[10/26 18:13:56 visual_prompt]: 	Training 100/139. train loss: 0.5387,	9.4110 s / batch. (data: 8.52e+00). ETA=1 day, 6:59:18, max mem: 7.6 GB 
[10/26 18:15:40 visual_prompt]: Epoch 15 / 100: avg data time: 2.12e+00, avg batch time: 2.9999, average train loss: 0.8933
[10/26 18:16:28 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.4563, average loss: 0.8044
[10/26 18:16:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.33	
[10/26 18:16:28 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[10/26 18:21:34 visual_prompt]: 	Training 100/139. train loss: 0.8476,	0.8720 s / batch. (data: 2.50e-04). ETA=2:50:15, max mem: 7.6 GB 
[10/26 18:23:24 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9932, average train loss: 0.8378
[10/26 18:24:11 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4592, average loss: 0.8429
[10/26 18:24:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.68	
[10/26 18:24:11 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[10/26 18:29:19 visual_prompt]: 	Training 100/139. train loss: 1.7229,	4.3119 s / batch. (data: 3.41e+00). ETA=13:51:55, max mem: 7.6 GB 
[10/26 18:31:08 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 2.9948, average train loss: 0.8683
[10/26 18:31:55 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.4561, average loss: 0.7541
[10/26 18:31:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 58.06	
[10/26 18:31:55 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[10/26 18:37:01 visual_prompt]: 	Training 100/139. train loss: 0.6755,	0.8587 s / batch. (data: 3.17e-04). ETA=2:43:40, max mem: 7.6 GB 
[10/26 18:38:52 visual_prompt]: Epoch 18 / 100: avg data time: 2.12e+00, avg batch time: 2.9999, average train loss: 0.8180
[10/26 18:39:39 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.4611, average loss: 1.3678
[10/26 18:39:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.81	
[10/26 18:39:39 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[10/26 18:44:52 visual_prompt]: 	Training 100/139. train loss: 0.5624,	10.9346 s / batch. (data: 1.01e+01). ETA=1 day, 10:18:59, max mem: 7.6 GB 
[10/26 18:46:37 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0026, average train loss: 0.9189
[10/26 18:47:24 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4603, average loss: 1.8173
[10/26 18:47:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.58	
[10/26 18:47:24 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[10/26 18:52:30 visual_prompt]: 	Training 100/139. train loss: 0.6287,	0.8880 s / batch. (data: 1.20e-02). ETA=2:45:09, max mem: 7.6 GB 
[10/26 18:54:21 visual_prompt]: Epoch 20 / 100: avg data time: 2.12e+00, avg batch time: 3.0012, average train loss: 1.3559
[10/26 18:55:09 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4614, average loss: 0.7625
[10/26 18:55:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 60.72	
[10/26 18:55:09 visual_prompt]: Stopping early.
[10/26 18:55:10 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 18:55:10 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 18:55:10 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 18:55:10 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 18:55:10 visual_prompt]: Training with config:
[10/26 18:55:10 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr0.5_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 18:55:10 visual_prompt]: Loading training data...
[10/26 18:55:10 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 18:55:10 visual_prompt]: Loading validation data...
[10/26 18:55:10 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 18:55:10 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/26 18:55:12 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/26 18:55:12 visual_prompt]: tuned percent:0.534
[10/26 18:55:12 visual_prompt]: Device used for model: 0
[10/26 18:55:12 visual_prompt]: Setting up Evaluator...
[10/26 18:55:12 visual_prompt]: Setting up Trainer...
[10/26 18:55:12 visual_prompt]: 	Setting up the optimizer...
[10/26 18:55:12 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 19:00:18 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8956 s / batch. (data: 3.11e-04). ETA=3:25:58, max mem: 7.6 GB 
[10/26 19:02:08 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9915, average train loss: 1.3980
[10/26 19:02:55 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4639, average loss: 1.3816
[10/26 19:02:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/26 19:02:55 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[10/26 19:08:03 visual_prompt]: 	Training 100/139. train loss: 0.7535,	0.8760 s / batch. (data: 7.96e-03). ETA=3:19:26, max mem: 7.6 GB 
[10/26 19:09:52 visual_prompt]: Epoch 2 / 100: avg data time: 2.12e+00, avg batch time: 2.9937, average train loss: 0.8711
[10/26 19:10:39 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4637, average loss: 0.7052
[10/26 19:10:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.93	
[10/26 19:10:39 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[10/26 19:15:46 visual_prompt]: 	Training 100/139. train loss: 0.7317,	0.9000 s / batch. (data: 2.73e-04). ETA=3:22:49, max mem: 7.6 GB 
[10/26 19:17:36 visual_prompt]: Epoch 3 / 100: avg data time: 2.13e+00, avg batch time: 2.9949, average train loss: 0.7403
[10/26 19:18:23 visual_prompt]: Inference (val):avg data time: 4.81e-04, avg batch time: 0.4615, average loss: 0.7072
[10/26 19:18:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.59	
[10/26 19:18:23 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[10/26 19:23:26 visual_prompt]: 	Training 100/139. train loss: 0.6543,	0.8680 s / batch. (data: 1.20e-02). ETA=3:13:36, max mem: 7.6 GB 
[10/26 19:25:20 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 2.9968, average train loss: 0.7232
[10/26 19:26:07 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.4668, average loss: 0.8487
[10/26 19:26:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.17	
[10/26 19:26:07 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[10/26 19:31:17 visual_prompt]: 	Training 100/139. train loss: 0.6919,	10.5245 s / batch. (data: 9.63e+00). ETA=1 day, 14:43:06, max mem: 7.6 GB 
[10/26 19:33:05 visual_prompt]: Epoch 5 / 100: avg data time: 2.14e+00, avg batch time: 3.0038, average train loss: 0.7388
[10/26 19:33:52 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4639, average loss: 0.7003
[10/26 19:33:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.76	
[10/26 19:33:52 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[10/26 19:38:58 visual_prompt]: 	Training 100/139. train loss: 0.6637,	0.8920 s / batch. (data: 3.54e-04). ETA=3:14:49, max mem: 7.6 GB 
[10/26 19:40:49 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 2.9990, average train loss: 0.7444
[10/26 19:41:36 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4610, average loss: 0.7254
[10/26 19:41:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.74	
[10/26 19:41:36 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[10/26 19:46:43 visual_prompt]: 	Training 100/139. train loss: 0.6247,	3.2282 s / batch. (data: 2.37e+00). ETA=11:37:37, max mem: 7.6 GB 
[10/26 19:48:33 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9953, average train loss: 0.7521
[10/26 19:49:20 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4590, average loss: 0.7722
[10/26 19:49:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.49	
[10/26 19:49:20 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[10/26 19:54:28 visual_prompt]: 	Training 100/139. train loss: 0.6852,	0.8686 s / batch. (data: 5.43e-03). ETA=3:05:42, max mem: 7.6 GB 
[10/26 19:56:18 visual_prompt]: Epoch 8 / 100: avg data time: 2.14e+00, avg batch time: 3.0052, average train loss: 0.7640
[10/26 19:57:06 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4625, average loss: 0.8288
[10/26 19:57:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.30	
[10/26 19:57:06 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[10/26 20:02:15 visual_prompt]: 	Training 100/139. train loss: 0.9783,	0.8781 s / batch. (data: 2.95e-04). ETA=3:05:40, max mem: 7.6 GB 
[10/26 20:04:02 visual_prompt]: Epoch 9 / 100: avg data time: 2.13e+00, avg batch time: 2.9977, average train loss: 0.9137
[10/26 20:04:50 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.4602, average loss: 0.7564
[10/26 20:04:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.58	
[10/26 20:04:50 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[10/26 20:09:56 visual_prompt]: 	Training 100/139. train loss: 1.1258,	0.8800 s / batch. (data: 7.95e-03). ETA=3:04:03, max mem: 7.6 GB 
[10/26 20:11:46 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9942, average train loss: 0.8925
[10/26 20:12:33 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4635, average loss: 0.7017
[10/26 20:12:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.65	
[10/26 20:12:33 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[10/26 20:17:40 visual_prompt]: 	Training 100/139. train loss: 1.0676,	0.8709 s / batch. (data: 2.85e-04). ETA=3:00:08, max mem: 7.6 GB 
[10/26 20:19:30 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0001, average train loss: 0.8281
[10/26 20:20:18 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4625, average loss: 0.6905
[10/26 20:20:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.57	
[10/26 20:20:18 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[10/26 20:25:25 visual_prompt]: 	Training 100/139. train loss: 1.1326,	1.9990 s / batch. (data: 1.10e+00). ETA=6:48:49, max mem: 7.6 GB 
[10/26 20:27:14 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 2.9961, average train loss: 0.9489
[10/26 20:28:02 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4613, average loss: 0.7138
[10/26 20:28:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.84	
[10/26 20:28:02 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[10/26 20:33:10 visual_prompt]: 	Training 100/139. train loss: 1.2931,	0.8659 s / batch. (data: 3.04e-04). ETA=2:55:05, max mem: 7.6 GB 
[10/26 20:34:59 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 2.9993, average train loss: 0.8727
[10/26 20:35:46 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.4582, average loss: 0.6919
[10/26 20:35:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.51	
[10/26 20:35:46 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[10/26 20:40:57 visual_prompt]: 	Training 100/139. train loss: 0.6275,	0.8744 s / batch. (data: 2.90e-04). ETA=2:54:46, max mem: 7.6 GB 
[10/26 20:42:44 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0019, average train loss: 0.7816
[10/26 20:43:31 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.4616, average loss: 0.7728
[10/26 20:43:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.09	
[10/26 20:43:31 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[10/26 20:48:44 visual_prompt]: 	Training 100/139. train loss: 0.6574,	9.5756 s / batch. (data: 8.69e+00). ETA=1 day, 7:31:49, max mem: 7.6 GB 
[10/26 20:50:29 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0030, average train loss: 0.8099
[10/26 20:51:16 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.4599, average loss: 0.9009
[10/26 20:51:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.22	
[10/26 20:51:16 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[10/26 20:56:24 visual_prompt]: 	Training 100/139. train loss: 0.6866,	0.8640 s / batch. (data: 2.83e-04). ETA=2:48:41, max mem: 7.6 GB 
[10/26 20:58:13 visual_prompt]: Epoch 16 / 100: avg data time: 2.13e+00, avg batch time: 2.9981, average train loss: 0.7624
[10/26 20:59:00 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.4624, average loss: 0.7083
[10/26 20:59:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.43	
[10/26 20:59:00 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[10/26 21:04:09 visual_prompt]: 	Training 100/139. train loss: 0.6909,	5.6328 s / batch. (data: 4.77e+00). ETA=18:06:45, max mem: 7.6 GB 
[10/26 21:05:58 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 3.0000, average train loss: 0.7715
[10/26 21:06:45 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4615, average loss: 0.6887
[10/26 21:06:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.20	
[10/26 21:06:45 visual_prompt]: Best epoch 17: best metric: -0.689
[10/26 21:06:45 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[10/26 21:11:52 visual_prompt]: 	Training 100/139. train loss: 0.7487,	1.2197 s / batch. (data: 3.46e-01). ETA=3:52:29, max mem: 7.6 GB 
[10/26 21:13:42 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0008, average train loss: 0.7955
[10/26 21:14:30 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4606, average loss: 1.0103
[10/26 21:14:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.82	
[10/26 21:14:30 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[10/26 21:19:42 visual_prompt]: 	Training 100/139. train loss: 0.5503,	11.0360 s / batch. (data: 1.01e+01). ETA=1 day, 10:38:04, max mem: 7.6 GB 
[10/26 21:21:27 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0026, average train loss: 0.8712
[10/26 21:22:14 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.4606, average loss: 1.8427
[10/26 21:22:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.98	
[10/26 21:22:14 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[10/26 21:27:21 visual_prompt]: 	Training 100/139. train loss: 0.6647,	0.8570 s / batch. (data: 3.84e-04). ETA=2:39:23, max mem: 7.6 GB 
[10/26 21:29:11 visual_prompt]: Epoch 20 / 100: avg data time: 2.12e+00, avg batch time: 2.9952, average train loss: 0.8740
[10/26 21:29:58 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4583, average loss: 0.6914
[10/26 21:29:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.69	
[10/26 21:29:58 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[10/26 21:35:04 visual_prompt]: 	Training 100/139. train loss: 0.9960,	0.8602 s / batch. (data: 3.03e-04). ETA=2:37:59, max mem: 7.6 GB 
[10/26 21:36:54 visual_prompt]: Epoch 21 / 100: avg data time: 2.12e+00, avg batch time: 2.9924, average train loss: 0.8977
[10/26 21:37:42 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4613, average loss: 1.4575
[10/26 21:37:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.17	
[10/26 21:37:42 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[10/26 21:42:44 visual_prompt]: 	Training 100/139. train loss: 1.2412,	0.8720 s / batch. (data: 2.83e-04). ETA=2:38:08, max mem: 7.6 GB 
[10/26 21:44:38 visual_prompt]: Epoch 22 / 100: avg data time: 2.12e+00, avg batch time: 2.9946, average train loss: 1.0362
[10/26 21:45:25 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.4639, average loss: 0.7308
[10/26 21:45:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.87	
[10/26 21:45:25 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[10/26 21:50:30 visual_prompt]: 	Training 100/139. train loss: 0.6707,	0.8719 s / batch. (data: 5.42e-03). ETA=2:36:06, max mem: 7.6 GB 
[10/26 21:52:22 visual_prompt]: Epoch 23 / 100: avg data time: 2.13e+00, avg batch time: 2.9993, average train loss: 0.8345
[10/26 21:53:10 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.4605, average loss: 0.6906
[10/26 21:53:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.52	
[10/26 21:53:10 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[10/26 21:58:19 visual_prompt]: 	Training 100/139. train loss: 0.8113,	0.8968 s / batch. (data: 5.42e-03). ETA=2:38:28, max mem: 7.6 GB 
[10/26 22:00:07 visual_prompt]: Epoch 24 / 100: avg data time: 2.13e+00, avg batch time: 3.0046, average train loss: 0.8918
[10/26 22:00:55 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4600, average loss: 0.7215
[10/26 22:00:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.19	
[10/26 22:00:55 visual_prompt]: Stopping early.
[10/26 22:00:55 visual_prompt]: Rank of current process: 0. World size: 1
[10/26 22:00:55 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 22:00:55 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 22:00:55 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 22:00:55 visual_prompt]: Training with config:
[10/26 22:00:55 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr0.5_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 22:00:55 visual_prompt]: Loading training data...
[10/26 22:00:55 visual_prompt]: Constructing mammo-cbis dataset train...
[10/26 22:00:55 visual_prompt]: Loading validation data...
[10/26 22:00:55 visual_prompt]: Constructing mammo-cbis dataset val...
[10/26 22:00:55 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/26 22:00:57 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/26 22:00:57 visual_prompt]: tuned percent:0.534
[10/26 22:00:57 visual_prompt]: Device used for model: 0
[10/26 22:00:57 visual_prompt]: Setting up Evaluator...
[10/26 22:00:57 visual_prompt]: Setting up Trainer...
[10/26 22:00:57 visual_prompt]: 	Setting up the optimizer...
[10/26 22:00:57 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/26 22:06:04 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8596 s / batch. (data: 2.99e-04). ETA=3:17:43, max mem: 7.6 GB 
[10/26 22:07:54 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9940, average train loss: 1.3980
[10/26 22:08:41 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4612, average loss: 1.3816
[10/26 22:08:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/26 22:08:41 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[10/26 22:13:47 visual_prompt]: 	Training 100/139. train loss: 0.7653,	0.8840 s / batch. (data: 2.99e-04). ETA=3:21:16, max mem: 7.6 GB 
[10/26 22:15:37 visual_prompt]: Epoch 2 / 100: avg data time: 2.12e+00, avg batch time: 2.9922, average train loss: 0.8791
[10/26 22:16:25 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4630, average loss: 0.7176
[10/26 22:16:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.21	
[10/26 22:16:25 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[10/26 22:21:30 visual_prompt]: 	Training 100/139. train loss: 0.7791,	0.8827 s / batch. (data: 1.55e-02). ETA=3:18:56, max mem: 7.6 GB 
[10/26 22:23:21 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9917, average train loss: 0.7447
[10/26 22:24:08 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4628, average loss: 0.8510
[10/26 22:24:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.69	
[10/26 22:24:08 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[10/26 22:29:10 visual_prompt]: 	Training 100/139. train loss: 0.6551,	0.8880 s / batch. (data: 3.01e-04). ETA=3:18:03, max mem: 7.6 GB 
[10/26 22:31:04 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 2.9952, average train loss: 0.7364
[10/26 22:31:52 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.4615, average loss: 0.8820
[10/26 22:31:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.27	
[10/26 22:31:52 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[10/26 22:37:01 visual_prompt]: 	Training 100/139. train loss: 0.8026,	11.5884 s / batch. (data: 1.07e+01). ETA=1 day, 18:37:56, max mem: 7.6 GB 
[10/26 22:38:49 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 2.9994, average train loss: 0.8025
[10/26 22:39:36 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4620, average loss: 0.7629
[10/26 22:39:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.37	
[10/26 22:39:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[10/26 22:44:41 visual_prompt]: 	Training 100/139. train loss: 0.9281,	0.8883 s / batch. (data: 1.18e-02). ETA=3:14:00, max mem: 7.6 GB 
[10/26 22:46:32 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 2.9950, average train loss: 0.8532
[10/26 22:47:20 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4594, average loss: 0.7437
[10/26 22:47:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.67	
[10/26 22:47:20 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[10/26 22:52:26 visual_prompt]: 	Training 100/139. train loss: 0.6266,	0.8762 s / batch. (data: 2.60e-04). ETA=3:09:20, max mem: 7.6 GB 
[10/26 22:54:15 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9898, average train loss: 0.7387
[10/26 22:55:03 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4616, average loss: 0.8687
[10/26 22:55:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.75	
[10/26 22:55:03 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[10/26 23:00:10 visual_prompt]: 	Training 100/139. train loss: 0.7640,	0.8683 s / batch. (data: 2.84e-04). ETA=3:05:37, max mem: 7.6 GB 
[10/26 23:02:00 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0030, average train loss: 0.7815
[10/26 23:02:48 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4580, average loss: 1.4123
[10/26 23:02:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.44	
[10/26 23:02:48 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[10/26 23:07:57 visual_prompt]: 	Training 100/139. train loss: 0.7779,	0.8954 s / batch. (data: 7.95e-03). ETA=3:09:21, max mem: 7.6 GB 
[10/26 23:09:44 visual_prompt]: Epoch 9 / 100: avg data time: 2.13e+00, avg batch time: 2.9972, average train loss: 0.7959
[10/26 23:10:32 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4625, average loss: 0.9713
[10/26 23:10:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.52	
[10/26 23:10:32 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[10/26 23:15:38 visual_prompt]: 	Training 100/139. train loss: 0.7790,	0.8714 s / batch. (data: 2.87e-04). ETA=3:02:14, max mem: 7.6 GB 
[10/26 23:17:28 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9917, average train loss: 0.8067
[10/26 23:18:15 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4641, average loss: 0.6881
[10/26 23:18:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.79	
[10/26 23:18:15 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[10/26 23:23:22 visual_prompt]: 	Training 100/139. train loss: 0.7421,	0.8880 s / batch. (data: 7.96e-03). ETA=3:03:40, max mem: 7.6 GB 
[10/26 23:25:11 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 2.9949, average train loss: 0.7703
[10/26 23:25:59 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4640, average loss: 0.6894
[10/26 23:25:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.62	
[10/26 23:25:59 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[10/26 23:31:05 visual_prompt]: 	Training 100/139. train loss: 0.8642,	1.1789 s / batch. (data: 3.23e-01). ETA=4:01:06, max mem: 7.6 GB 
[10/26 23:32:55 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 2.9939, average train loss: 0.8161
[10/26 23:33:42 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.4620, average loss: 0.9750
[10/26 23:33:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.19	
[10/26 23:33:42 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[10/26 23:38:51 visual_prompt]: 	Training 100/139. train loss: 0.7039,	0.8840 s / batch. (data: 7.96e-03). ETA=2:58:44, max mem: 7.6 GB 
[10/26 23:40:39 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0002, average train loss: 0.8500
[10/26 23:41:27 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4659, average loss: 0.6879
[10/26 23:41:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.20	
[10/26 23:41:27 visual_prompt]: Best epoch 13: best metric: -0.688
[10/26 23:41:27 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[10/26 23:46:36 visual_prompt]: 	Training 100/139. train loss: 0.6582,	0.8657 s / batch. (data: 2.93e-04). ETA=2:53:01, max mem: 7.6 GB 
[10/26 23:48:23 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 2.9972, average train loss: 0.7886
[10/26 23:49:11 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.4618, average loss: 0.7470
[10/26 23:49:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.68	
[10/26 23:49:11 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[10/26 23:54:22 visual_prompt]: 	Training 100/139. train loss: 0.6616,	9.6481 s / batch. (data: 8.76e+00). ETA=1 day, 7:46:08, max mem: 7.6 GB 
[10/26 23:56:07 visual_prompt]: Epoch 15 / 100: avg data time: 2.12e+00, avg batch time: 2.9950, average train loss: 0.7731
[10/26 23:56:54 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4604, average loss: 0.7572
[10/26 23:56:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.70	
[10/26 23:56:54 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[10/27 00:02:01 visual_prompt]: 	Training 100/139. train loss: 0.7328,	0.8630 s / batch. (data: 2.90e-04). ETA=2:48:30, max mem: 7.6 GB 
[10/27 00:03:50 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9910, average train loss: 0.7578
[10/27 00:04:38 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4602, average loss: 0.6943
[10/27 00:04:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.50	
[10/27 00:04:38 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[10/27 00:09:45 visual_prompt]: 	Training 100/139. train loss: 0.8215,	4.7960 s / batch. (data: 3.90e+00). ETA=15:25:18, max mem: 7.6 GB 
[10/27 00:11:34 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 2.9926, average train loss: 0.7478
[10/27 00:12:21 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4663, average loss: 0.8094
[10/27 00:12:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.47	
[10/27 00:12:21 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[10/27 00:17:30 visual_prompt]: 	Training 100/139. train loss: 0.7580,	3.9320 s / batch. (data: 3.06e+00). ETA=12:29:30, max mem: 7.6 GB 
[10/27 00:19:18 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 2.9981, average train loss: 0.7409
[10/27 00:20:05 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.4630, average loss: 0.7288
[10/27 00:20:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.75	
[10/27 00:20:05 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[10/27 00:25:18 visual_prompt]: 	Training 100/139. train loss: 0.5282,	10.8720 s / batch. (data: 1.00e+01). ETA=1 day, 10:07:11, max mem: 7.6 GB 
[10/27 00:27:03 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0026, average train loss: 0.7486
[10/27 00:27:50 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4615, average loss: 0.7211
[10/27 00:27:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.86	
[10/27 00:27:50 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[10/27 00:32:56 visual_prompt]: 	Training 100/139. train loss: 0.8407,	0.8929 s / batch. (data: 2.07e-02). ETA=2:46:04, max mem: 7.6 GB 
[10/27 00:34:46 visual_prompt]: Epoch 20 / 100: avg data time: 2.13e+00, avg batch time: 2.9955, average train loss: 0.7556
[10/27 00:35:34 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4603, average loss: 0.8402
[10/27 00:35:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.92	
[10/27 00:35:34 visual_prompt]: Stopping early.
[10/27 00:35:34 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 00:35:34 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 00:35:34 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 00:35:34 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 00:35:34 visual_prompt]: Training with config:
[10/27 00:35:34 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr0.5_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 00:35:34 visual_prompt]: Loading training data...
[10/27 00:35:34 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 00:35:34 visual_prompt]: Loading validation data...
[10/27 00:35:34 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 00:35:34 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/27 00:35:36 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/27 00:35:36 visual_prompt]: tuned percent:0.534
[10/27 00:35:37 visual_prompt]: Device used for model: 0
[10/27 00:35:37 visual_prompt]: Setting up Evaluator...
[10/27 00:35:37 visual_prompt]: Setting up Trainer...
[10/27 00:35:37 visual_prompt]: 	Setting up the optimizer...
[10/27 00:35:37 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 00:40:44 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8988 s / batch. (data: 5.81e-03). ETA=3:26:43, max mem: 7.6 GB 
[10/27 00:42:33 visual_prompt]: Epoch 1 / 100: avg data time: 2.13e+00, avg batch time: 2.9983, average train loss: 1.3980
[10/27 00:43:21 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4605, average loss: 1.3816
[10/27 00:43:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/27 00:43:21 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[10/27 00:48:25 visual_prompt]: 	Training 100/139. train loss: 0.7647,	0.8593 s / batch. (data: 4.43e-04). ETA=3:15:38, max mem: 7.6 GB 
[10/27 00:50:18 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 2.9977, average train loss: 0.8798
[10/27 00:51:05 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.4617, average loss: 0.7193
[10/27 00:51:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.24	
[10/27 00:51:05 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[10/27 00:56:11 visual_prompt]: 	Training 100/139. train loss: 0.7840,	0.8749 s / batch. (data: 2.95e-04). ETA=3:17:10, max mem: 7.6 GB 
[10/27 00:58:02 visual_prompt]: Epoch 3 / 100: avg data time: 2.13e+00, avg batch time: 3.0009, average train loss: 0.7468
[10/27 00:58:50 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4592, average loss: 0.8733
[10/27 00:58:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.64	
[10/27 00:58:50 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[10/27 01:03:52 visual_prompt]: 	Training 100/139. train loss: 0.6712,	0.8720 s / batch. (data: 7.96e-03). ETA=3:14:30, max mem: 7.6 GB 
[10/27 01:05:46 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 2.9972, average train loss: 0.7402
[10/27 01:06:34 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.4616, average loss: 0.8297
[10/27 01:06:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.41	
[10/27 01:06:34 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[10/27 01:11:43 visual_prompt]: 	Training 100/139. train loss: 0.8506,	11.3280 s / batch. (data: 1.04e+01). ETA=1 day, 17:40:28, max mem: 7.6 GB 
[10/27 01:13:31 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0021, average train loss: 0.8240
[10/27 01:14:19 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4584, average loss: 0.7100
[10/27 01:14:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.15	
[10/27 01:14:19 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[10/27 01:19:24 visual_prompt]: 	Training 100/139. train loss: 0.9913,	0.8663 s / batch. (data: 2.87e-03). ETA=3:09:12, max mem: 7.6 GB 
[10/27 01:21:15 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 2.9974, average train loss: 0.8820
[10/27 01:22:03 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4635, average loss: 0.8541
[10/27 01:22:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.77	
[10/27 01:22:03 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[10/27 01:27:09 visual_prompt]: 	Training 100/139. train loss: 0.6515,	0.8608 s / batch. (data: 3.01e-04). ETA=3:06:01, max mem: 7.6 GB 
[10/27 01:28:59 visual_prompt]: Epoch 7 / 100: avg data time: 2.13e+00, avg batch time: 2.9937, average train loss: 0.7521
[10/27 01:29:47 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4606, average loss: 0.8822
[10/27 01:29:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.62	
[10/27 01:29:47 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[10/27 01:34:55 visual_prompt]: 	Training 100/139. train loss: 0.7592,	0.8880 s / batch. (data: 7.25e-03). ETA=3:09:50, max mem: 7.6 GB 
[10/27 01:36:45 visual_prompt]: Epoch 8 / 100: avg data time: 2.14e+00, avg batch time: 3.0094, average train loss: 0.7649
[10/27 01:37:32 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4630, average loss: 0.8114
[10/27 01:37:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.64	
[10/27 01:37:32 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[10/27 01:42:41 visual_prompt]: 	Training 100/139. train loss: 1.7179,	0.8798 s / batch. (data: 1.17e-02). ETA=3:06:02, max mem: 7.6 GB 
[10/27 01:44:29 visual_prompt]: Epoch 9 / 100: avg data time: 2.13e+00, avg batch time: 2.9974, average train loss: 0.9847
[10/27 01:45:16 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4592, average loss: 2.3026
[10/27 01:45:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.34	
[10/27 01:45:16 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[10/27 01:50:24 visual_prompt]: 	Training 100/139. train loss: 1.0114,	0.8692 s / batch. (data: 2.97e-04). ETA=3:01:47, max mem: 7.6 GB 
[10/27 01:52:13 visual_prompt]: Epoch 10 / 100: avg data time: 2.13e+00, avg batch time: 2.9988, average train loss: 0.9993
[10/27 01:53:01 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.4566, average loss: 1.1791
[10/27 01:53:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.63	
[10/27 01:53:01 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[10/27 01:58:08 visual_prompt]: 	Training 100/139. train loss: 0.7974,	0.8754 s / batch. (data: 2.86e-04). ETA=3:01:03, max mem: 7.6 GB 
[10/27 01:59:58 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 2.9982, average train loss: 0.8349
[10/27 02:00:45 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4594, average loss: 0.7216
[10/27 02:00:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 59.93	
[10/27 02:00:45 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[10/27 02:05:53 visual_prompt]: 	Training 100/139. train loss: 1.1450,	2.6142 s / batch. (data: 1.75e+00). ETA=8:54:39, max mem: 7.6 GB 
[10/27 02:07:42 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 3.0001, average train loss: 0.8830
[10/27 02:08:30 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.4584, average loss: 0.7082
[10/27 02:08:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 59.24	
[10/27 02:08:30 visual_prompt]: Best epoch 12: best metric: -0.708
[10/27 02:08:30 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[10/27 02:13:40 visual_prompt]: 	Training 100/139. train loss: 0.8120,	0.8605 s / batch. (data: 2.83e-04). ETA=2:53:59, max mem: 7.6 GB 
[10/27 02:15:28 visual_prompt]: Epoch 13 / 100: avg data time: 2.14e+00, avg batch time: 3.0099, average train loss: 0.8121
[10/27 02:16:16 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4604, average loss: 0.6874
[10/27 02:16:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 57.22	
[10/27 02:16:16 visual_prompt]: Best epoch 13: best metric: -0.687
[10/27 02:16:16 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[10/27 02:21:26 visual_prompt]: 	Training 100/139. train loss: 0.6552,	0.8735 s / batch. (data: 5.36e-03). ETA=2:54:36, max mem: 7.6 GB 
[10/27 02:23:13 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0024, average train loss: 0.7593
[10/27 02:24:01 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4603, average loss: 0.7586
[10/27 02:24:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.07	
[10/27 02:24:01 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[10/27 02:29:14 visual_prompt]: 	Training 100/139. train loss: 0.6036,	9.6097 s / batch. (data: 8.73e+00). ETA=1 day, 7:38:32, max mem: 7.6 GB 
[10/27 02:30:58 visual_prompt]: Epoch 15 / 100: avg data time: 2.14e+00, avg batch time: 3.0020, average train loss: 0.7743
[10/27 02:31:45 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.4611, average loss: 0.7209
[10/27 02:31:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.26	
[10/27 02:31:45 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[10/27 02:36:52 visual_prompt]: 	Training 100/139. train loss: 0.8093,	0.8736 s / batch. (data: 2.78e-04). ETA=2:50:33, max mem: 7.6 GB 
[10/27 02:38:42 visual_prompt]: Epoch 16 / 100: avg data time: 2.13e+00, avg batch time: 2.9959, average train loss: 0.8407
[10/27 02:39:29 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.4580, average loss: 0.8140
[10/27 02:39:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.20	
[10/27 02:39:29 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[10/27 02:44:37 visual_prompt]: 	Training 100/139. train loss: 0.8125,	3.7560 s / batch. (data: 2.88e+00). ETA=12:04:39, max mem: 7.6 GB 
[10/27 02:46:26 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 2.9976, average train loss: 0.8259
[10/27 02:47:14 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.4583, average loss: 0.6902
[10/27 02:47:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 52.84	
[10/27 02:47:14 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[10/27 02:52:23 visual_prompt]: 	Training 100/139. train loss: 0.7871,	2.9004 s / batch. (data: 2.04e+00). ETA=9:12:51, max mem: 7.6 GB 
[10/27 02:54:12 visual_prompt]: Epoch 18 / 100: avg data time: 2.14e+00, avg batch time: 3.0068, average train loss: 0.8507
[10/27 02:54:59 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4579, average loss: 0.6969
[10/27 02:54:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 54.97	
[10/27 02:54:59 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[10/27 03:00:12 visual_prompt]: 	Training 100/139. train loss: 0.5378,	11.0930 s / batch. (data: 1.02e+01). ETA=1 day, 10:48:49, max mem: 7.6 GB 
[10/27 03:01:57 visual_prompt]: Epoch 19 / 100: avg data time: 2.14e+00, avg batch time: 3.0049, average train loss: 0.7798
[10/27 03:02:44 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4586, average loss: 0.7503
[10/27 03:02:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.03	
[10/27 03:02:44 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[10/27 03:07:50 visual_prompt]: 	Training 100/139. train loss: 1.4817,	0.8708 s / batch. (data: 3.70e-04). ETA=2:41:56, max mem: 7.6 GB 
[10/27 03:09:41 visual_prompt]: Epoch 20 / 100: avg data time: 2.13e+00, avg batch time: 2.9981, average train loss: 0.8562
[10/27 03:10:29 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.4608, average loss: 0.7283
[10/27 03:10:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.69	
[10/27 03:10:29 visual_prompt]: Stopping early.
[10/27 03:10:29 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 03:10:29 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 03:10:29 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 03:10:29 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 03:10:29 visual_prompt]: Training with config:
[10/27 03:10:29 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr0.5_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 03:10:29 visual_prompt]: Loading training data...
[10/27 03:10:29 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 03:10:29 visual_prompt]: Loading validation data...
[10/27 03:10:29 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 03:10:29 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/27 03:10:31 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/27 03:10:31 visual_prompt]: tuned percent:0.534
[10/27 03:10:31 visual_prompt]: Device used for model: 0
[10/27 03:10:31 visual_prompt]: Setting up Evaluator...
[10/27 03:10:31 visual_prompt]: Setting up Trainer...
[10/27 03:10:31 visual_prompt]: 	Setting up the optimizer...
[10/27 03:10:31 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 03:15:37 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8600 s / batch. (data: 2.92e-04). ETA=3:17:47, max mem: 7.6 GB 
[10/27 03:17:28 visual_prompt]: Epoch 1 / 100: avg data time: 2.13e+00, avg batch time: 2.9927, average train loss: 1.3980
[10/27 03:18:15 visual_prompt]: Inference (val):avg data time: 2.04e-03, avg batch time: 0.4625, average loss: 1.3816
[10/27 03:18:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/27 03:18:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[10/27 03:23:20 visual_prompt]: 	Training 100/139. train loss: 0.7647,	0.8652 s / batch. (data: 3.00e-04). ETA=3:16:59, max mem: 7.6 GB 
[10/27 03:25:11 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 2.9949, average train loss: 0.8799
[10/27 03:25:58 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4614, average loss: 0.7195
[10/27 03:25:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.22	
[10/27 03:25:58 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[10/27 03:31:03 visual_prompt]: 	Training 100/139. train loss: 0.7846,	0.8634 s / batch. (data: 1.75e-03). ETA=3:14:34, max mem: 7.6 GB 
[10/27 03:32:55 visual_prompt]: Epoch 3 / 100: avg data time: 2.13e+00, avg batch time: 2.9959, average train loss: 0.7471
[10/27 03:33:42 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4589, average loss: 0.8757
[10/27 03:33:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.61	
[10/27 03:33:42 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[10/27 03:38:45 visual_prompt]: 	Training 100/139. train loss: 0.6727,	0.8920 s / batch. (data: 2.85e-04). ETA=3:18:57, max mem: 7.6 GB 
[10/27 03:40:39 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 2.9977, average train loss: 0.7409
[10/27 03:41:26 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.4614, average loss: 0.8223
[10/27 03:41:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.26	
[10/27 03:41:26 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[10/27 03:46:36 visual_prompt]: 	Training 100/139. train loss: 0.8499,	11.6566 s / batch. (data: 1.08e+01). ETA=1 day, 18:52:59, max mem: 7.6 GB 
[10/27 03:48:23 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 2.9996, average train loss: 0.8268
[10/27 03:49:11 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.4617, average loss: 0.7063
[10/27 03:49:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.09	
[10/27 03:49:11 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[10/27 03:54:17 visual_prompt]: 	Training 100/139. train loss: 0.9993,	2.4520 s / batch. (data: 1.58e+00). ETA=8:55:33, max mem: 7.6 GB 
[10/27 03:56:08 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 3.0027, average train loss: 0.8863
[10/27 03:56:56 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4602, average loss: 0.8545
[10/27 03:56:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.82	
[10/27 03:56:56 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[10/27 04:02:03 visual_prompt]: 	Training 100/139. train loss: 0.6658,	0.8711 s / batch. (data: 1.20e-03). ETA=3:08:15, max mem: 7.6 GB 
[10/27 04:03:52 visual_prompt]: Epoch 7 / 100: avg data time: 2.13e+00, avg batch time: 2.9957, average train loss: 0.7529
[10/27 04:04:40 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.4585, average loss: 0.8707
[10/27 04:04:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.57	
[10/27 04:04:40 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[10/27 04:09:48 visual_prompt]: 	Training 100/139. train loss: 0.7741,	0.8761 s / batch. (data: 2.79e-04). ETA=3:07:18, max mem: 7.6 GB 
[10/27 04:11:37 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0025, average train loss: 0.7673
[10/27 04:12:24 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4601, average loss: 0.7864
[10/27 04:12:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.46	
[10/27 04:12:24 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[10/27 04:17:34 visual_prompt]: 	Training 100/139. train loss: 1.7436,	0.8640 s / batch. (data: 3.04e-04). ETA=3:02:42, max mem: 7.6 GB 
[10/27 04:19:21 visual_prompt]: Epoch 9 / 100: avg data time: 2.13e+00, avg batch time: 3.0006, average train loss: 0.9965
[10/27 04:20:09 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4572, average loss: 2.4103
[10/27 04:20:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.39	
[10/27 04:20:09 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[10/27 04:25:16 visual_prompt]: 	Training 100/139. train loss: 1.0133,	0.8760 s / batch. (data: 6.09e-03). ETA=3:03:12, max mem: 7.6 GB 
[10/27 04:27:05 visual_prompt]: Epoch 10 / 100: avg data time: 2.13e+00, avg batch time: 2.9936, average train loss: 1.0294
[10/27 04:27:53 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4619, average loss: 1.3228
[10/27 04:27:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.42	
[10/27 04:27:53 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[10/27 04:33:01 visual_prompt]: 	Training 100/139. train loss: 0.6903,	0.8853 s / batch. (data: 2.84e-04). ETA=3:03:06, max mem: 7.6 GB 
[10/27 04:34:50 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0032, average train loss: 0.8567
[10/27 04:35:37 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4621, average loss: 0.7600
[10/27 04:35:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 59.99	
[10/27 04:35:37 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[10/27 04:40:45 visual_prompt]: 	Training 100/139. train loss: 0.9894,	0.8720 s / batch. (data: 2.67e-04). ETA=2:58:20, max mem: 7.6 GB 
[10/27 04:42:34 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 2.9950, average train loss: 0.9174
[10/27 04:43:21 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4621, average loss: 0.6678
[10/27 04:43:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 60.67	
[10/27 04:43:21 visual_prompt]: Best epoch 12: best metric: -0.668
[10/27 04:43:21 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[10/27 04:48:31 visual_prompt]: 	Training 100/139. train loss: 0.7789,	0.8943 s / batch. (data: 2.73e-04). ETA=3:00:49, max mem: 7.6 GB 
[10/27 04:50:19 visual_prompt]: Epoch 13 / 100: avg data time: 2.14e+00, avg batch time: 3.0062, average train loss: 0.7806
[10/27 04:51:07 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4586, average loss: 0.6774
[10/27 04:51:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 60.65	
[10/27 04:51:07 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[10/27 04:56:17 visual_prompt]: 	Training 100/139. train loss: 0.6845,	0.8901 s / batch. (data: 3.24e-04). ETA=2:57:55, max mem: 7.6 GB 
[10/27 04:58:04 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0038, average train loss: 0.7631
[10/27 04:58:52 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.4611, average loss: 0.8126
[10/27 04:58:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.72	
[10/27 04:58:52 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[10/27 05:04:04 visual_prompt]: 	Training 100/139. train loss: 0.5295,	9.3649 s / batch. (data: 8.50e+00). ETA=1 day, 6:50:11, max mem: 7.6 GB 
[10/27 05:05:49 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0000, average train loss: 0.7669
[10/27 05:06:36 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4609, average loss: 0.7542
[10/27 05:06:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 62.52	
[10/27 05:06:36 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[10/27 05:11:43 visual_prompt]: 	Training 100/139. train loss: 0.8746,	0.8732 s / batch. (data: 8.89e-03). ETA=2:50:29, max mem: 7.6 GB 
[10/27 05:13:32 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9919, average train loss: 0.8356
[10/27 05:14:19 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.4637, average loss: 0.8638
[10/27 05:14:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.21	
[10/27 05:14:19 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[10/27 05:19:26 visual_prompt]: 	Training 100/139. train loss: 0.7090,	2.4452 s / batch. (data: 1.58e+00). ETA=7:51:45, max mem: 7.6 GB 
[10/27 05:21:16 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 2.9986, average train loss: 0.7705
[10/27 05:22:04 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4644, average loss: 0.7175
[10/27 05:22:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 61.22	
[10/27 05:22:04 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[10/27 05:27:11 visual_prompt]: 	Training 100/139. train loss: 0.8399,	2.2126 s / batch. (data: 1.33e+00). ETA=7:01:46, max mem: 7.6 GB 
[10/27 05:29:00 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 2.9954, average train loss: 0.8505
[10/27 05:29:48 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4602, average loss: 0.6807
[10/27 05:29:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 60.58	
[10/27 05:29:48 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[10/27 05:35:00 visual_prompt]: 	Training 100/139. train loss: 0.5448,	11.1277 s / batch. (data: 1.02e+01). ETA=1 day, 10:55:21, max mem: 7.6 GB 
[10/27 05:36:45 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 2.9992, average train loss: 0.7406
[10/27 05:37:32 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4623, average loss: 0.6976
[10/27 05:37:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 59.82	
[10/27 05:37:32 visual_prompt]: Stopping early.
[10/27 05:37:32 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 05:37:32 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 05:37:32 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 05:37:32 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 05:37:32 visual_prompt]: Training with config:
[10/27 05:37:32 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr0.25_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 05:37:32 visual_prompt]: Loading training data...
[10/27 05:37:32 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 05:37:32 visual_prompt]: Loading validation data...
[10/27 05:37:32 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 05:37:32 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/27 05:37:34 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/27 05:37:34 visual_prompt]: tuned percent:0.534
[10/27 05:37:35 visual_prompt]: Device used for model: 0
[10/27 05:37:35 visual_prompt]: Setting up Evaluator...
[10/27 05:37:35 visual_prompt]: Setting up Trainer...
[10/27 05:37:35 visual_prompt]: 	Setting up the optimizer...
[10/27 05:37:35 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 05:42:41 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8935 s / batch. (data: 1.09e-02). ETA=3:25:29, max mem: 7.6 GB 
[10/27 05:44:31 visual_prompt]: Epoch 1 / 100: avg data time: 2.13e+00, avg batch time: 2.9973, average train loss: 1.3980
[10/27 05:45:19 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4633, average loss: 1.3816
[10/27 05:45:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/27 05:45:19 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[10/27 05:50:25 visual_prompt]: 	Training 100/139. train loss: 0.7701,	0.8656 s / batch. (data: 2.79e-04). ETA=3:17:05, max mem: 7.6 GB 
[10/27 05:52:16 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 3.0021, average train loss: 0.8131
[10/27 05:53:03 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4597, average loss: 0.6867
[10/27 05:53:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 54.92	
[10/27 05:53:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[10/27 05:58:11 visual_prompt]: 	Training 100/139. train loss: 0.7031,	0.9032 s / batch. (data: 6.45e-03). ETA=3:23:33, max mem: 7.6 GB 
[10/27 06:00:01 visual_prompt]: Epoch 3 / 100: avg data time: 2.13e+00, avg batch time: 3.0002, average train loss: 0.7183
[10/27 06:00:48 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.4612, average loss: 0.6875
[10/27 06:00:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.67	
[10/27 06:00:48 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[10/27 06:05:50 visual_prompt]: 	Training 100/139. train loss: 0.7183,	0.8659 s / batch. (data: 5.36e-03). ETA=3:13:08, max mem: 7.6 GB 
[10/27 06:07:45 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 3.0008, average train loss: 0.7202
[10/27 06:08:33 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.4626, average loss: 0.7639
[10/27 06:08:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.51	
[10/27 06:08:33 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[10/27 06:13:43 visual_prompt]: 	Training 100/139. train loss: 0.6933,	11.5040 s / batch. (data: 1.06e+01). ETA=1 day, 18:19:18, max mem: 7.6 GB 
[10/27 06:15:31 visual_prompt]: Epoch 5 / 100: avg data time: 2.14e+00, avg batch time: 3.0072, average train loss: 0.7254
[10/27 06:16:18 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.4627, average loss: 0.6882
[10/27 06:16:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.76	
[10/27 06:16:18 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[10/27 06:21:24 visual_prompt]: 	Training 100/139. train loss: 0.6950,	0.8772 s / batch. (data: 9.20e-03). ETA=3:11:36, max mem: 7.6 GB 
[10/27 06:23:16 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 3.0031, average train loss: 0.7278
[10/27 06:24:03 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.4605, average loss: 0.7169
[10/27 06:24:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.48	
[10/27 06:24:03 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[10/27 06:29:10 visual_prompt]: 	Training 100/139. train loss: 0.6330,	1.3808 s / batch. (data: 5.16e-01). ETA=4:58:23, max mem: 7.6 GB 
[10/27 06:31:00 visual_prompt]: Epoch 7 / 100: avg data time: 2.13e+00, avg batch time: 2.9971, average train loss: 0.7091
[10/27 06:31:47 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4614, average loss: 0.7930
[10/27 06:31:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.24	
[10/27 06:31:47 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[10/27 06:36:54 visual_prompt]: 	Training 100/139. train loss: 0.9188,	0.8640 s / batch. (data: 2.81e-04). ETA=3:04:42, max mem: 7.6 GB 
[10/27 06:38:45 visual_prompt]: Epoch 8 / 100: avg data time: 2.14e+00, avg batch time: 3.0061, average train loss: 0.7357
[10/27 06:39:32 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4608, average loss: 0.9652
[10/27 06:39:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.14	
[10/27 06:39:32 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[10/27 06:44:43 visual_prompt]: 	Training 100/139. train loss: 0.7051,	0.8648 s / batch. (data: 3.07e-04). ETA=3:02:53, max mem: 7.6 GB 
[10/27 06:46:30 visual_prompt]: Epoch 9 / 100: avg data time: 2.14e+00, avg batch time: 3.0055, average train loss: 0.7309
[10/27 06:47:17 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4597, average loss: 0.8010
[10/27 06:47:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.90	
[10/27 06:47:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[10/27 06:52:26 visual_prompt]: 	Training 100/139. train loss: 0.7114,	0.8856 s / batch. (data: 2.89e-04). ETA=3:05:12, max mem: 7.6 GB 
[10/27 06:54:15 visual_prompt]: Epoch 10 / 100: avg data time: 2.13e+00, avg batch time: 3.0013, average train loss: 0.7377
[10/27 06:55:02 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.4646, average loss: 0.6899
[10/27 06:55:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.30	
[10/27 06:55:02 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[10/27 07:00:10 visual_prompt]: 	Training 100/139. train loss: 0.7019,	0.8573 s / batch. (data: 2.83e-04). ETA=2:57:18, max mem: 7.6 GB 
[10/27 07:02:00 visual_prompt]: Epoch 11 / 100: avg data time: 2.14e+00, avg batch time: 3.0037, average train loss: 0.7399
[10/27 07:02:47 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4617, average loss: 0.8498
[10/27 07:02:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.73	
[10/27 07:02:47 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[10/27 07:07:54 visual_prompt]: 	Training 100/139. train loss: 0.7637,	0.8733 s / batch. (data: 7.96e-03). ETA=2:58:36, max mem: 7.6 GB 
[10/27 07:09:45 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 3.0024, average train loss: 0.7462
[10/27 07:10:32 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4604, average loss: 0.7689
[10/27 07:10:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.87	
[10/27 07:10:32 visual_prompt]: Best epoch 12: best metric: -0.769
[10/27 07:10:32 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[10/27 07:15:42 visual_prompt]: 	Training 100/139. train loss: 1.1221,	0.8707 s / batch. (data: 2.35e-04). ETA=2:56:03, max mem: 7.6 GB 
[10/27 07:17:31 visual_prompt]: Epoch 13 / 100: avg data time: 2.14e+00, avg batch time: 3.0114, average train loss: 0.7685
[10/27 07:18:18 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4594, average loss: 0.6924
[10/27 07:18:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.72	
[10/27 07:18:18 visual_prompt]: Best epoch 13: best metric: -0.692
[10/27 07:18:18 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[10/27 07:23:28 visual_prompt]: 	Training 100/139. train loss: 0.8498,	0.8805 s / batch. (data: 8.37e-04). ETA=2:56:00, max mem: 7.6 GB 
[10/27 07:25:16 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0046, average train loss: 0.7466
[10/27 07:26:03 visual_prompt]: Inference (val):avg data time: 4.32e-04, avg batch time: 0.4626, average loss: 0.6997
[10/27 07:26:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.49	
[10/27 07:26:03 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[10/27 07:31:17 visual_prompt]: 	Training 100/139. train loss: 0.6667,	9.4089 s / batch. (data: 8.55e+00). ETA=1 day, 6:58:53, max mem: 7.6 GB 
[10/27 07:33:02 visual_prompt]: Epoch 15 / 100: avg data time: 2.14e+00, avg batch time: 3.0106, average train loss: 0.7203
[10/27 07:33:49 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4649, average loss: 0.7485
[10/27 07:33:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.91	
[10/27 07:33:49 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[10/27 07:38:57 visual_prompt]: 	Training 100/139. train loss: 0.6844,	0.8860 s / batch. (data: 3.11e-04). ETA=2:52:59, max mem: 7.6 GB 
[10/27 07:40:46 visual_prompt]: Epoch 16 / 100: avg data time: 2.13e+00, avg batch time: 2.9985, average train loss: 0.7245
[10/27 07:41:33 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4591, average loss: 0.6992
[10/27 07:41:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.68	
[10/27 07:41:33 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[10/27 07:46:40 visual_prompt]: 	Training 100/139. train loss: 0.7717,	4.4996 s / batch. (data: 3.60e+00). ETA=14:28:07, max mem: 7.6 GB 
[10/27 07:48:30 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 2.9983, average train loss: 0.7226
[10/27 07:49:17 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.4603, average loss: 0.7731
[10/27 07:49:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.99	
[10/27 07:49:17 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[10/27 07:54:26 visual_prompt]: 	Training 100/139. train loss: 0.7905,	2.6365 s / batch. (data: 1.76e+00). ETA=8:22:34, max mem: 7.6 GB 
[10/27 07:56:15 visual_prompt]: Epoch 18 / 100: avg data time: 2.14e+00, avg batch time: 3.0052, average train loss: 0.8248
[10/27 07:57:03 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4613, average loss: 0.7420
[10/27 07:57:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.77	
[10/27 07:57:03 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[10/27 08:02:17 visual_prompt]: 	Training 100/139. train loss: 0.4961,	11.2367 s / batch. (data: 1.04e+01). ETA=1 day, 11:15:52, max mem: 7.6 GB 
[10/27 08:04:01 visual_prompt]: Epoch 19 / 100: avg data time: 2.14e+00, avg batch time: 3.0064, average train loss: 0.7471
[10/27 08:04:48 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.4586, average loss: 0.7895
[10/27 08:04:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.36	
[10/27 08:04:48 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[10/27 08:09:54 visual_prompt]: 	Training 100/139. train loss: 1.3195,	0.8800 s / batch. (data: 2.96e-04). ETA=2:43:40, max mem: 7.6 GB 
[10/27 08:11:45 visual_prompt]: Epoch 20 / 100: avg data time: 2.13e+00, avg batch time: 3.0011, average train loss: 0.7954
[10/27 08:12:33 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4626, average loss: 0.7209
[10/27 08:12:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.03	
[10/27 08:12:33 visual_prompt]: Stopping early.
[10/27 08:12:33 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 08:12:33 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 08:12:33 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 08:12:33 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 08:12:33 visual_prompt]: Training with config:
[10/27 08:12:33 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr0.25_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 08:12:33 visual_prompt]: Loading training data...
[10/27 08:12:33 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 08:12:33 visual_prompt]: Loading validation data...
[10/27 08:12:33 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 08:12:33 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/27 08:12:35 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/27 08:12:35 visual_prompt]: tuned percent:0.534
[10/27 08:12:35 visual_prompt]: Device used for model: 0
[10/27 08:12:35 visual_prompt]: Setting up Evaluator...
[10/27 08:12:35 visual_prompt]: Setting up Trainer...
[10/27 08:12:35 visual_prompt]: 	Setting up the optimizer...
[10/27 08:12:35 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 08:17:43 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.9080 s / batch. (data: 7.38e-04). ETA=3:28:50, max mem: 7.6 GB 
[10/27 08:19:33 visual_prompt]: Epoch 1 / 100: avg data time: 2.14e+00, avg batch time: 3.0059, average train loss: 1.3980
[10/27 08:20:21 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4584, average loss: 1.3816
[10/27 08:20:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/27 08:20:21 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[10/27 08:25:27 visual_prompt]: 	Training 100/139. train loss: 0.7850,	0.8560 s / batch. (data: 3.03e-04). ETA=3:14:53, max mem: 7.6 GB 
[10/27 08:27:17 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 2.9936, average train loss: 0.8177
[10/27 08:28:04 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4609, average loss: 0.6857
[10/27 08:28:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 56.16	
[10/27 08:28:04 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[10/27 08:33:10 visual_prompt]: 	Training 100/139. train loss: 0.7405,	0.8974 s / batch. (data: 5.42e-03). ETA=3:22:14, max mem: 7.6 GB 
[10/27 08:35:00 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9932, average train loss: 0.7295
[10/27 08:35:48 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4649, average loss: 0.6864
[10/27 08:35:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[10/27 08:35:48 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[10/27 08:40:50 visual_prompt]: 	Training 100/139. train loss: 0.6526,	0.8669 s / batch. (data: 2.35e-04). ETA=3:13:21, max mem: 7.6 GB 
[10/27 08:42:44 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 2.9965, average train loss: 0.7331
[10/27 08:43:32 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4608, average loss: 0.7810
[10/27 08:43:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.14	
[10/27 08:43:32 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[10/27 08:48:41 visual_prompt]: 	Training 100/139. train loss: 0.6567,	11.7085 s / batch. (data: 1.08e+01). ETA=1 day, 19:04:27, max mem: 7.6 GB 
[10/27 08:50:29 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0019, average train loss: 0.7576
[10/27 08:51:16 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.4633, average loss: 0.7380
[10/27 08:51:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.84	
[10/27 08:51:16 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[10/27 08:56:22 visual_prompt]: 	Training 100/139. train loss: 0.5931,	0.8760 s / batch. (data: 3.03e-04). ETA=3:11:20, max mem: 7.6 GB 
[10/27 08:58:13 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 2.9990, average train loss: 0.7560
[10/27 08:59:01 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4638, average loss: 0.7794
[10/27 08:59:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.96	
[10/27 08:59:01 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[10/27 09:04:10 visual_prompt]: 	Training 100/139. train loss: 0.6347,	1.1880 s / batch. (data: 2.88e-01). ETA=4:16:43, max mem: 7.6 GB 
[10/27 09:05:57 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9926, average train loss: 0.7193
[10/27 09:06:44 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.4617, average loss: 0.9636
[10/27 09:06:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.03	
[10/27 09:06:44 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[10/27 09:11:51 visual_prompt]: 	Training 100/139. train loss: 0.9802,	0.8608 s / batch. (data: 5.41e-03). ETA=3:04:01, max mem: 7.6 GB 
[10/27 09:13:42 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0033, average train loss: 0.7370
[10/27 09:14:29 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4592, average loss: 0.9781
[10/27 09:14:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.84	
[10/27 09:14:29 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[10/27 09:19:39 visual_prompt]: 	Training 100/139. train loss: 0.6941,	0.8920 s / batch. (data: 2.97e-04). ETA=3:08:37, max mem: 7.6 GB 
[10/27 09:21:26 visual_prompt]: Epoch 9 / 100: avg data time: 2.13e+00, avg batch time: 2.9967, average train loss: 0.7455
[10/27 09:22:13 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4614, average loss: 1.0680
[10/27 09:22:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.43	
[10/27 09:22:13 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[10/27 09:27:19 visual_prompt]: 	Training 100/139. train loss: 0.7555,	0.8726 s / batch. (data: 1.60e-02). ETA=3:02:30, max mem: 7.6 GB 
[10/27 09:29:10 visual_prompt]: Epoch 10 / 100: avg data time: 2.13e+00, avg batch time: 2.9948, average train loss: 0.7597
[10/27 09:29:57 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.4640, average loss: 0.6863
[10/27 09:29:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.59	
[10/27 09:29:57 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[10/27 09:35:04 visual_prompt]: 	Training 100/139. train loss: 0.8632,	0.8681 s / batch. (data: 2.88e-04). ETA=2:59:33, max mem: 7.6 GB 
[10/27 09:36:54 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0025, average train loss: 0.7564
[10/27 09:37:42 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.4672, average loss: 0.9386
[10/27 09:37:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.49	
[10/27 09:37:42 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[10/27 09:42:48 visual_prompt]: 	Training 100/139. train loss: 0.7383,	0.8880 s / batch. (data: 2.96e-04). ETA=3:01:36, max mem: 7.6 GB 
[10/27 09:44:38 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9918, average train loss: 0.7723
[10/27 09:45:25 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4637, average loss: 0.6873
[10/27 09:45:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 59.18	
[10/27 09:45:25 visual_prompt]: Best epoch 12: best metric: -0.687
[10/27 09:45:25 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[10/27 09:50:34 visual_prompt]: 	Training 100/139. train loss: 0.7448,	0.8869 s / batch. (data: 6.72e-03). ETA=2:59:19, max mem: 7.6 GB 
[10/27 09:52:22 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0012, average train loss: 0.7312
[10/27 09:53:10 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.4579, average loss: 0.6905
[10/27 09:53:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.63	
[10/27 09:53:10 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[10/27 09:58:21 visual_prompt]: 	Training 100/139. train loss: 1.1432,	0.9084 s / batch. (data: 7.13e-04). ETA=3:01:34, max mem: 7.6 GB 
[10/27 10:00:07 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 2.9993, average train loss: 0.7482
[10/27 10:00:54 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.4599, average loss: 0.6850
[10/27 10:00:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 60.02	
[10/27 10:00:54 visual_prompt]: Best epoch 14: best metric: -0.685
[10/27 10:00:54 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[10/27 10:06:06 visual_prompt]: 	Training 100/139. train loss: 0.6598,	9.4515 s / batch. (data: 8.58e+00). ETA=1 day, 7:07:17, max mem: 7.6 GB 
[10/27 10:07:50 visual_prompt]: Epoch 15 / 100: avg data time: 2.12e+00, avg batch time: 2.9948, average train loss: 0.7142
[10/27 10:08:38 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.4619, average loss: 0.6942
[10/27 10:08:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.32	
[10/27 10:08:38 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[10/27 10:13:45 visual_prompt]: 	Training 100/139. train loss: 0.6713,	0.8800 s / batch. (data: 5.41e-03). ETA=2:51:48, max mem: 7.6 GB 
[10/27 10:15:34 visual_prompt]: Epoch 16 / 100: avg data time: 2.13e+00, avg batch time: 2.9971, average train loss: 0.7364
[10/27 10:16:22 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.4618, average loss: 0.7770
[10/27 10:16:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.64	
[10/27 10:16:22 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[10/27 10:21:26 visual_prompt]: 	Training 100/139. train loss: 0.7609,	1.4079 s / batch. (data: 5.27e-01). ETA=4:31:38, max mem: 7.6 GB 
[10/27 10:23:18 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 2.9943, average train loss: 0.7154
[10/27 10:24:05 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4627, average loss: 0.6878
[10/27 10:24:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.03	
[10/27 10:24:05 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[10/27 10:29:12 visual_prompt]: 	Training 100/139. train loss: 0.7110,	1.1209 s / batch. (data: 2.48e-01). ETA=3:33:39, max mem: 7.6 GB 
[10/27 10:31:02 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 2.9978, average train loss: 0.7512
[10/27 10:31:50 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.4636, average loss: 0.9401
[10/27 10:31:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.68	
[10/27 10:31:50 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[10/27 10:37:02 visual_prompt]: 	Training 100/139. train loss: 0.5330,	11.0280 s / batch. (data: 1.01e+01). ETA=1 day, 10:36:34, max mem: 7.6 GB 
[10/27 10:38:47 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0043, average train loss: 0.7390
[10/27 10:39:35 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4625, average loss: 0.7448
[10/27 10:39:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.87	
[10/27 10:39:35 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[10/27 10:44:39 visual_prompt]: 	Training 100/139. train loss: 0.7926,	0.9000 s / batch. (data: 2.81e-04). ETA=2:47:22, max mem: 7.6 GB 
[10/27 10:46:31 visual_prompt]: Epoch 20 / 100: avg data time: 2.12e+00, avg batch time: 2.9933, average train loss: 0.7244
[10/27 10:47:18 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4603, average loss: 0.7052
[10/27 10:47:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.01	
[10/27 10:47:18 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[10/27 10:52:24 visual_prompt]: 	Training 100/139. train loss: 0.8563,	0.8663 s / batch. (data: 2.86e-04). ETA=2:39:06, max mem: 7.6 GB 
[10/27 10:54:14 visual_prompt]: Epoch 21 / 100: avg data time: 2.12e+00, avg batch time: 2.9930, average train loss: 0.7114
[10/27 10:55:02 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4572, average loss: 0.6963
[10/27 10:55:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.22	
[10/27 10:55:02 visual_prompt]: Stopping early.
[10/27 10:55:02 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 10:55:02 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 10:55:02 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 10:55:02 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 10:55:02 visual_prompt]: Training with config:
[10/27 10:55:02 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr0.25_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 10:55:02 visual_prompt]: Loading training data...
[10/27 10:55:02 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 10:55:02 visual_prompt]: Loading validation data...
[10/27 10:55:02 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 10:55:02 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/27 10:55:04 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/27 10:55:04 visual_prompt]: tuned percent:0.534
[10/27 10:55:04 visual_prompt]: Device used for model: 0
[10/27 10:55:04 visual_prompt]: Setting up Evaluator...
[10/27 10:55:04 visual_prompt]: Setting up Trainer...
[10/27 10:55:04 visual_prompt]: 	Setting up the optimizer...
[10/27 10:55:05 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 11:00:12 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8808 s / batch. (data: 7.72e-04). ETA=3:22:34, max mem: 7.6 GB 
[10/27 11:02:01 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9939, average train loss: 1.3980
[10/27 11:02:48 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4638, average loss: 1.3816
[10/27 11:02:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/27 11:02:48 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[10/27 11:07:53 visual_prompt]: 	Training 100/139. train loss: 0.7866,	0.8578 s / batch. (data: 2.88e-04). ETA=3:15:18, max mem: 7.6 GB 
[10/27 11:09:45 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 2.9966, average train loss: 0.8183
[10/27 11:10:32 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4618, average loss: 0.6864
[10/27 11:10:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 56.08	
[10/27 11:10:32 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[10/27 11:15:38 visual_prompt]: 	Training 100/139. train loss: 0.7506,	0.9000 s / batch. (data: 7.95e-03). ETA=3:22:49, max mem: 7.6 GB 
[10/27 11:17:29 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9948, average train loss: 0.7297
[10/27 11:18:16 visual_prompt]: Inference (val):avg data time: 8.03e-04, avg batch time: 0.4604, average loss: 0.6868
[10/27 11:18:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.28	
[10/27 11:18:16 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[10/27 11:23:18 visual_prompt]: 	Training 100/139. train loss: 0.6552,	0.8959 s / batch. (data: 5.43e-03). ETA=3:19:50, max mem: 7.6 GB 
[10/27 11:25:13 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 2.9998, average train loss: 0.7356
[10/27 11:26:01 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4604, average loss: 0.7903
[10/27 11:26:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.48	
[10/27 11:26:01 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[10/27 11:31:11 visual_prompt]: 	Training 100/139. train loss: 0.6479,	11.7037 s / batch. (data: 1.08e+01). ETA=1 day, 19:03:24, max mem: 7.6 GB 
[10/27 11:32:59 visual_prompt]: Epoch 5 / 100: avg data time: 2.14e+00, avg batch time: 3.0098, average train loss: 0.7549
[10/27 11:33:46 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4626, average loss: 0.7323
[10/27 11:33:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.66	
[10/27 11:33:47 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[10/27 11:38:53 visual_prompt]: 	Training 100/139. train loss: 0.5958,	0.8708 s / batch. (data: 7.97e-03). ETA=3:10:11, max mem: 7.6 GB 
[10/27 11:40:43 visual_prompt]: Epoch 6 / 100: avg data time: 2.12e+00, avg batch time: 2.9954, average train loss: 0.7600
[10/27 11:41:30 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.4624, average loss: 0.7895
[10/27 11:41:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.59	
[10/27 11:41:30 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[10/27 11:46:38 visual_prompt]: 	Training 100/139. train loss: 0.6436,	0.8840 s / batch. (data: 2.89e-04). ETA=3:11:01, max mem: 7.6 GB 
[10/27 11:48:26 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9913, average train loss: 0.7210
[10/27 11:49:14 visual_prompt]: Inference (val):avg data time: 4.06e-04, avg batch time: 0.4583, average loss: 0.9090
[10/27 11:49:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.06	
[10/27 11:49:14 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[10/27 11:54:21 visual_prompt]: 	Training 100/139. train loss: 1.0741,	0.8801 s / batch. (data: 7.01e-04). ETA=3:08:08, max mem: 7.6 GB 
[10/27 11:56:11 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0035, average train loss: 0.7445
[10/27 11:56:59 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.4631, average loss: 0.9775
[10/27 11:56:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.45	
[10/27 11:56:59 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[10/27 12:02:07 visual_prompt]: 	Training 100/139. train loss: 0.7019,	0.8697 s / batch. (data: 1.20e-02). ETA=3:03:54, max mem: 7.6 GB 
[10/27 12:03:55 visual_prompt]: Epoch 9 / 100: avg data time: 2.13e+00, avg batch time: 2.9959, average train loss: 0.7589
[10/27 12:04:43 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4630, average loss: 1.4526
[10/27 12:04:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.18	
[10/27 12:04:43 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[10/27 12:09:50 visual_prompt]: 	Training 100/139. train loss: 0.8010,	0.8720 s / batch. (data: 2.97e-04). ETA=3:02:23, max mem: 7.6 GB 
[10/27 12:11:39 visual_prompt]: Epoch 10 / 100: avg data time: 2.13e+00, avg batch time: 2.9972, average train loss: 0.8009
[10/27 12:12:27 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.4584, average loss: 0.6788
[10/27 12:12:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 58.90	
[10/27 12:12:27 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[10/27 12:17:34 visual_prompt]: 	Training 100/139. train loss: 0.7335,	0.8843 s / batch. (data: 3.02e-04). ETA=3:02:53, max mem: 7.6 GB 
[10/27 12:19:24 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0018, average train loss: 0.7810
[10/27 12:20:11 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4637, average loss: 0.8250
[10/27 12:20:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.29	
[10/27 12:20:11 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[10/27 12:25:18 visual_prompt]: 	Training 100/139. train loss: 0.9040,	0.8760 s / batch. (data: 2.99e-04). ETA=2:59:09, max mem: 7.6 GB 
[10/27 12:27:08 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 2.9954, average train loss: 0.7941
[10/27 12:27:55 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4638, average loss: 0.8398
[10/27 12:27:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.45	
[10/27 12:27:55 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[10/27 12:33:04 visual_prompt]: 	Training 100/139. train loss: 1.0299,	0.8920 s / batch. (data: 1.20e-02). ETA=3:00:21, max mem: 7.6 GB 
[10/27 12:34:53 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0034, average train loss: 0.7885
[10/27 12:35:40 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4604, average loss: 0.6939
[10/27 12:35:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 60.46	
[10/27 12:35:40 visual_prompt]: Best epoch 13: best metric: -0.694
[10/27 12:35:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[10/27 12:40:51 visual_prompt]: 	Training 100/139. train loss: 1.4157,	0.8763 s / batch. (data: 5.42e-03). ETA=2:55:09, max mem: 7.6 GB 
[10/27 12:42:37 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0001, average train loss: 0.7602
[10/27 12:43:25 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4632, average loss: 0.7877
[10/27 12:43:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.83	
[10/27 12:43:25 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[10/27 12:48:37 visual_prompt]: 	Training 100/139. train loss: 0.5250,	9.4520 s / batch. (data: 8.56e+00). ETA=1 day, 7:07:24, max mem: 7.6 GB 
[10/27 12:50:22 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0032, average train loss: 0.7144
[10/27 12:51:10 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4628, average loss: 0.7710
[10/27 12:51:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.95	
[10/27 12:51:10 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[10/27 12:56:15 visual_prompt]: 	Training 100/139. train loss: 0.6827,	0.8762 s / batch. (data: 5.44e-03). ETA=2:51:04, max mem: 7.6 GB 
[10/27 12:58:06 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9900, average train loss: 0.7394
[10/27 12:58:53 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4603, average loss: 0.7203
[10/27 12:58:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 60.50	
[10/27 12:58:53 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[10/27 13:04:02 visual_prompt]: 	Training 100/139. train loss: 0.8167,	5.3241 s / batch. (data: 4.46e+00). ETA=17:07:12, max mem: 7.6 GB 
[10/27 13:05:50 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 3.0015, average train loss: 0.6952
[10/27 13:06:37 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.4664, average loss: 0.7710
[10/27 13:06:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.86	
[10/27 13:06:38 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[10/27 13:11:46 visual_prompt]: 	Training 100/139. train loss: 0.8855,	2.4752 s / batch. (data: 1.60e+00). ETA=7:51:49, max mem: 7.6 GB 
[10/27 13:13:34 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 2.9986, average train loss: 0.7247
[10/27 13:14:22 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4603, average loss: 0.7052
[10/27 13:14:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 59.78	
[10/27 13:14:22 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[10/27 13:19:35 visual_prompt]: 	Training 100/139. train loss: 0.5101,	11.1040 s / batch. (data: 1.02e+01). ETA=1 day, 10:50:53, max mem: 7.6 GB 
[10/27 13:21:19 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0032, average train loss: 0.7161
[10/27 13:22:07 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.4588, average loss: 0.7371
[10/27 13:22:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 60.38	
[10/27 13:22:07 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[10/27 13:27:13 visual_prompt]: 	Training 100/139. train loss: 1.3026,	0.8680 s / batch. (data: 2.87e-04). ETA=2:41:26, max mem: 7.6 GB 
[10/27 13:29:03 visual_prompt]: Epoch 20 / 100: avg data time: 2.13e+00, avg batch time: 2.9971, average train loss: 0.7943
[10/27 13:29:51 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4613, average loss: 0.6712
[10/27 13:29:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.38	
[10/27 13:29:51 visual_prompt]: Best epoch 20: best metric: -0.671
[10/27 13:29:51 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[10/27 13:34:57 visual_prompt]: 	Training 100/139. train loss: 0.6044,	0.9107 s / batch. (data: 3.03e-04). ETA=2:47:16, max mem: 7.6 GB 
[10/27 13:36:47 visual_prompt]: Epoch 21 / 100: avg data time: 2.12e+00, avg batch time: 2.9938, average train loss: 0.7094
[10/27 13:37:35 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.4624, average loss: 0.6956
[10/27 13:37:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 59.05	
[10/27 13:37:35 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[10/27 13:42:37 visual_prompt]: 	Training 100/139. train loss: 0.6616,	0.8600 s / batch. (data: 2.90e-04). ETA=2:35:57, max mem: 7.6 GB 
[10/27 13:44:31 visual_prompt]: Epoch 22 / 100: avg data time: 2.13e+00, avg batch time: 2.9957, average train loss: 0.7618
[10/27 13:45:19 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.4657, average loss: 0.7717
[10/27 13:45:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 62.23	
[10/27 13:45:19 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[10/27 13:50:24 visual_prompt]: 	Training 100/139. train loss: 0.6536,	0.8627 s / batch. (data: 3.01e-04). ETA=2:34:27, max mem: 7.6 GB 
[10/27 13:52:16 visual_prompt]: Epoch 23 / 100: avg data time: 2.13e+00, avg batch time: 3.0000, average train loss: 0.7333
[10/27 13:53:03 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.4625, average loss: 0.6745
[10/27 13:53:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 62.80	
[10/27 13:53:03 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[10/27 13:58:14 visual_prompt]: 	Training 100/139. train loss: 0.6487,	0.8880 s / batch. (data: 7.87e-04). ETA=2:36:55, max mem: 7.6 GB 
[10/27 14:00:01 visual_prompt]: Epoch 24 / 100: avg data time: 2.14e+00, avg batch time: 3.0085, average train loss: 0.7051
[10/27 14:00:49 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4647, average loss: 0.8909
[10/27 14:00:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.19	
[10/27 14:00:49 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[10/27 14:05:58 visual_prompt]: 	Training 100/139. train loss: 0.8132,	0.8908 s / batch. (data: 2.57e-04). ETA=2:35:21, max mem: 7.6 GB 
[10/27 14:07:46 visual_prompt]: Epoch 25 / 100: avg data time: 2.13e+00, avg batch time: 2.9985, average train loss: 0.8084
[10/27 14:08:33 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4610, average loss: 0.6823
[10/27 14:08:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 62.23	
[10/27 14:08:33 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[10/27 14:13:40 visual_prompt]: 	Training 100/139. train loss: 0.6201,	1.0234 s / batch. (data: 1.52e-01). ETA=2:56:06, max mem: 7.6 GB 
[10/27 14:15:30 visual_prompt]: Epoch 26 / 100: avg data time: 2.13e+00, avg batch time: 2.9967, average train loss: 0.7092
[10/27 14:16:17 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4623, average loss: 0.6860
[10/27 14:16:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 63.69	
[10/27 14:16:17 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[10/27 14:21:29 visual_prompt]: 	Training 100/139. train loss: 0.8213,	9.4006 s / batch. (data: 8.51e+00). ETA=1 day, 2:35:54, max mem: 7.6 GB 
[10/27 14:23:16 visual_prompt]: Epoch 27 / 100: avg data time: 2.14e+00, avg batch time: 3.0094, average train loss: 0.7222
[10/27 14:24:03 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.4600, average loss: 0.6859
[10/27 14:24:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 63.20	
[10/27 14:24:03 visual_prompt]: Stopping early.
[10/27 14:24:03 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 14:24:03 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 14:24:03 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 14:24:03 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 14:24:03 visual_prompt]: Training with config:
[10/27 14:24:03 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr0.25_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 14:24:03 visual_prompt]: Loading training data...
[10/27 14:24:03 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 14:24:03 visual_prompt]: Loading validation data...
[10/27 14:24:03 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 14:24:03 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/27 14:24:06 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/27 14:24:06 visual_prompt]: tuned percent:0.534
[10/27 14:24:06 visual_prompt]: Device used for model: 0
[10/27 14:24:06 visual_prompt]: Setting up Evaluator...
[10/27 14:24:06 visual_prompt]: Setting up Trainer...
[10/27 14:24:06 visual_prompt]: 	Setting up the optimizer...
[10/27 14:24:06 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 14:29:12 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8800 s / batch. (data: 3.09e-04). ETA=3:22:24, max mem: 7.6 GB 
[10/27 14:31:02 visual_prompt]: Epoch 1 / 100: avg data time: 2.13e+00, avg batch time: 2.9964, average train loss: 1.3980
[10/27 14:31:50 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4612, average loss: 1.3816
[10/27 14:31:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/27 14:31:50 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[10/27 14:36:54 visual_prompt]: 	Training 100/139. train loss: 0.7866,	0.8870 s / batch. (data: 2.73e-04). ETA=3:21:57, max mem: 7.6 GB 
[10/27 14:38:46 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 2.9974, average train loss: 0.8184
[10/27 14:39:34 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.4633, average loss: 0.6864
[10/27 14:39:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 56.08	
[10/27 14:39:34 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[10/27 14:44:39 visual_prompt]: 	Training 100/139. train loss: 0.7505,	0.8920 s / batch. (data: 2.80e-04). ETA=3:21:01, max mem: 7.6 GB 
[10/27 14:46:30 visual_prompt]: Epoch 3 / 100: avg data time: 2.13e+00, avg batch time: 2.9977, average train loss: 0.7302
[10/27 14:47:18 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4622, average loss: 0.6868
[10/27 14:47:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.24	
[10/27 14:47:18 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[10/27 14:52:20 visual_prompt]: 	Training 100/139. train loss: 0.6557,	0.8602 s / batch. (data: 3.26e-04). ETA=3:11:51, max mem: 7.6 GB 
[10/27 14:54:15 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 2.9993, average train loss: 0.7360
[10/27 14:55:02 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4595, average loss: 0.7907
[10/27 14:55:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.48	
[10/27 14:55:02 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[10/27 15:00:11 visual_prompt]: 	Training 100/139. train loss: 0.6480,	11.6788 s / batch. (data: 1.08e+01). ETA=1 day, 18:57:54, max mem: 7.6 GB 
[10/27 15:01:59 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 2.9993, average train loss: 0.7553
[10/27 15:02:46 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.4628, average loss: 0.7347
[10/27 15:02:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.79	
[10/27 15:02:47 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[10/27 15:07:52 visual_prompt]: 	Training 100/139. train loss: 0.5972,	0.8760 s / batch. (data: 3.10e-04). ETA=3:11:20, max mem: 7.6 GB 
[10/27 15:09:44 visual_prompt]: Epoch 6 / 100: avg data time: 2.14e+00, avg batch time: 3.0042, average train loss: 0.7610
[10/27 15:10:32 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.4619, average loss: 0.7869
[10/27 15:10:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.77	
[10/27 15:10:32 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[10/27 15:15:41 visual_prompt]: 	Training 100/139. train loss: 0.6412,	0.8880 s / batch. (data: 2.82e-04). ETA=3:11:54, max mem: 7.6 GB 
[10/27 15:17:29 visual_prompt]: Epoch 7 / 100: avg data time: 2.13e+00, avg batch time: 3.0000, average train loss: 0.7216
[10/27 15:18:16 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4608, average loss: 0.9138
[10/27 15:18:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.38	
[10/27 15:18:16 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[10/27 15:23:24 visual_prompt]: 	Training 100/139. train loss: 1.0823,	0.8840 s / batch. (data: 2.92e-04). ETA=3:08:59, max mem: 7.6 GB 
[10/27 15:25:14 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0025, average train loss: 0.7470
[10/27 15:26:01 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4645, average loss: 0.9829
[10/27 15:26:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.51	
[10/27 15:26:01 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[10/27 15:31:10 visual_prompt]: 	Training 100/139. train loss: 0.6938,	0.8799 s / batch. (data: 4.23e-04). ETA=3:06:04, max mem: 7.6 GB 
[10/27 15:32:58 visual_prompt]: Epoch 9 / 100: avg data time: 2.13e+00, avg batch time: 2.9982, average train loss: 0.7630
[10/27 15:33:45 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.4640, average loss: 1.4788
[10/27 15:33:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.85	
[10/27 15:33:45 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[10/27 15:38:52 visual_prompt]: 	Training 100/139. train loss: 0.8318,	0.9000 s / batch. (data: 2.63e-04). ETA=3:08:14, max mem: 7.6 GB 
[10/27 15:40:41 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9948, average train loss: 0.8055
[10/27 15:41:29 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4603, average loss: 0.6801
[10/27 15:41:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.22	
[10/27 15:41:29 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[10/27 15:46:34 visual_prompt]: 	Training 100/139. train loss: 0.7065,	0.9042 s / batch. (data: 1.64e-02). ETA=3:07:00, max mem: 7.6 GB 
[10/27 15:48:26 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 2.9989, average train loss: 0.7770
[10/27 15:49:13 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4611, average loss: 0.7911
[10/27 15:49:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.57	
[10/27 15:49:13 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[10/27 15:54:21 visual_prompt]: 	Training 100/139. train loss: 0.9392,	2.5769 s / batch. (data: 1.72e+00). ETA=8:47:01, max mem: 7.6 GB 
[10/27 15:56:10 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 3.0004, average train loss: 0.7870
[10/27 15:56:58 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4621, average loss: 0.8382
[10/27 15:56:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.16	
[10/27 15:56:58 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[10/27 16:02:07 visual_prompt]: 	Training 100/139. train loss: 1.0298,	0.8840 s / batch. (data: 2.89e-04). ETA=2:58:44, max mem: 7.6 GB 
[10/27 16:03:55 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0043, average train loss: 0.7909
[10/27 16:04:43 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4635, average loss: 0.6980
[10/27 16:04:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 60.49	
[10/27 16:04:43 visual_prompt]: Best epoch 13: best metric: -0.698
[10/27 16:04:43 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[10/27 16:09:54 visual_prompt]: 	Training 100/139. train loss: 1.3848,	0.8914 s / batch. (data: 5.94e-03). ETA=2:58:10, max mem: 7.6 GB 
[10/27 16:11:40 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0009, average train loss: 0.7600
[10/27 16:12:27 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4665, average loss: 0.8118
[10/27 16:12:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.29	
[10/27 16:12:27 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[10/27 16:17:39 visual_prompt]: 	Training 100/139. train loss: 0.5339,	9.3640 s / batch. (data: 8.48e+00). ETA=1 day, 6:50:01, max mem: 7.6 GB 
[10/27 16:19:25 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0021, average train loss: 0.7214
[10/27 16:20:12 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.4605, average loss: 0.8127
[10/27 16:20:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.87	
[10/27 16:20:12 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[10/27 16:25:18 visual_prompt]: 	Training 100/139. train loss: 0.7105,	0.8960 s / batch. (data: 7.95e-03). ETA=2:54:56, max mem: 7.6 GB 
[10/27 16:27:08 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9920, average train loss: 0.7402
[10/27 16:27:56 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.4585, average loss: 0.6964
[10/27 16:27:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 61.51	
[10/27 16:27:56 visual_prompt]: Best epoch 16: best metric: -0.696
[10/27 16:27:56 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[10/27 16:33:02 visual_prompt]: 	Training 100/139. train loss: 0.8371,	4.1679 s / batch. (data: 3.29e+00). ETA=13:24:08, max mem: 7.6 GB 
[10/27 16:34:52 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 2.9928, average train loss: 0.6961
[10/27 16:35:39 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.4588, average loss: 0.7402
[10/27 16:35:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.98	
[10/27 16:35:39 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[10/27 16:40:47 visual_prompt]: 	Training 100/139. train loss: 0.8356,	2.3640 s / batch. (data: 1.48e+00). ETA=7:30:36, max mem: 7.6 GB 
[10/27 16:42:36 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 2.9970, average train loss: 0.7081
[10/27 16:43:23 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4602, average loss: 0.6906
[10/27 16:43:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 62.16	
[10/27 16:43:23 visual_prompt]: Best epoch 18: best metric: -0.691
[10/27 16:43:23 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[10/27 16:48:35 visual_prompt]: 	Training 100/139. train loss: 0.5038,	10.9883 s / batch. (data: 1.01e+01). ETA=1 day, 10:29:06, max mem: 7.6 GB 
[10/27 16:50:20 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0033, average train loss: 0.7157
[10/27 16:51:08 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4666, average loss: 0.7176
[10/27 16:51:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.42	
[10/27 16:51:08 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[10/27 16:56:14 visual_prompt]: 	Training 100/139. train loss: 1.3662,	0.8965 s / batch. (data: 5.41e-03). ETA=2:46:43, max mem: 7.6 GB 
[10/27 16:58:05 visual_prompt]: Epoch 20 / 100: avg data time: 2.13e+00, avg batch time: 2.9963, average train loss: 0.7998
[10/27 16:58:52 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.4608, average loss: 0.6646
[10/27 16:58:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 62.97	
[10/27 16:58:52 visual_prompt]: Best epoch 20: best metric: -0.665
[10/27 16:58:52 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[10/27 17:03:57 visual_prompt]: 	Training 100/139. train loss: 0.6358,	0.8794 s / batch. (data: 3.15e-04). ETA=2:41:30, max mem: 7.6 GB 
[10/27 17:05:48 visual_prompt]: Epoch 21 / 100: avg data time: 2.12e+00, avg batch time: 2.9911, average train loss: 0.7068
[10/27 17:06:35 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4610, average loss: 0.6642
[10/27 17:06:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 62.81	
[10/27 17:06:35 visual_prompt]: Best epoch 21: best metric: -0.664
[10/27 17:06:35 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[10/27 17:11:39 visual_prompt]: 	Training 100/139. train loss: 0.6317,	0.8721 s / batch. (data: 1.15e-02). ETA=2:38:09, max mem: 7.6 GB 
[10/27 17:13:32 visual_prompt]: Epoch 22 / 100: avg data time: 2.13e+00, avg batch time: 2.9982, average train loss: 0.7573
[10/27 17:14:20 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4638, average loss: 0.8118
[10/27 17:14:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 64.11	
[10/27 17:14:20 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[10/27 17:19:24 visual_prompt]: 	Training 100/139. train loss: 0.6811,	0.8808 s / batch. (data: 1.13e-02). ETA=2:37:42, max mem: 7.6 GB 
[10/27 17:21:17 visual_prompt]: Epoch 23 / 100: avg data time: 2.13e+00, avg batch time: 2.9997, average train loss: 0.7355
[10/27 17:22:04 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.4609, average loss: 0.6805
[10/27 17:22:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 63.55	
[10/27 17:22:04 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[10/27 17:27:15 visual_prompt]: 	Training 100/139. train loss: 0.6605,	0.8760 s / batch. (data: 3.20e-04). ETA=2:34:48, max mem: 7.6 GB 
[10/27 17:29:02 visual_prompt]: Epoch 24 / 100: avg data time: 2.14e+00, avg batch time: 3.0067, average train loss: 0.7075
[10/27 17:29:49 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.4616, average loss: 0.9825
[10/27 17:29:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.41	
[10/27 17:29:49 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[10/27 17:34:57 visual_prompt]: 	Training 100/139. train loss: 0.8360,	0.8920 s / batch. (data: 2.88e-04). ETA=2:35:34, max mem: 7.6 GB 
[10/27 17:36:46 visual_prompt]: Epoch 25 / 100: avg data time: 2.13e+00, avg batch time: 2.9973, average train loss: 0.8320
[10/27 17:37:33 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4604, average loss: 0.6746
[10/27 17:37:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 63.13	
[10/27 17:37:33 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[10/27 17:42:40 visual_prompt]: 	Training 100/139. train loss: 0.6640,	0.8643 s / batch. (data: 2.96e-04). ETA=2:28:44, max mem: 7.6 GB 
[10/27 17:44:29 visual_prompt]: Epoch 26 / 100: avg data time: 2.12e+00, avg batch time: 2.9940, average train loss: 0.7096
[10/27 17:45:17 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.4597, average loss: 0.6914
[10/27 17:45:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 65.86	
[10/27 17:45:17 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[10/27 17:50:28 visual_prompt]: 	Training 100/139. train loss: 0.8176,	9.6872 s / batch. (data: 8.82e+00). ETA=1 day, 3:24:33, max mem: 7.6 GB 
[10/27 17:52:15 visual_prompt]: Epoch 27 / 100: avg data time: 2.14e+00, avg batch time: 3.0081, average train loss: 0.7340
[10/27 17:53:02 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.4625, average loss: 0.6891
[10/27 17:53:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 64.61	
[10/27 17:53:02 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[10/27 17:58:08 visual_prompt]: 	Training 100/139. train loss: 0.6530,	0.8652 s / batch. (data: 2.89e-04). ETA=2:24:52, max mem: 7.6 GB 
[10/27 17:59:59 visual_prompt]: Epoch 28 / 100: avg data time: 2.13e+00, avg batch time: 2.9993, average train loss: 0.7271
[10/27 18:00:47 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.4649, average loss: 0.6953
[10/27 18:00:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 65.91	
[10/27 18:00:47 visual_prompt]: Stopping early.
[10/27 18:00:47 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 18:00:47 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 18:00:47 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 18:00:47 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 18:00:47 visual_prompt]: Training with config:
[10/27 18:00:47 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr0.1_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 18:00:47 visual_prompt]: Loading training data...
[10/27 18:00:47 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 18:00:47 visual_prompt]: Loading validation data...
[10/27 18:00:47 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 18:00:47 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/27 18:00:49 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/27 18:00:49 visual_prompt]: tuned percent:0.534
[10/27 18:00:49 visual_prompt]: Device used for model: 0
[10/27 18:00:49 visual_prompt]: Setting up Evaluator...
[10/27 18:00:49 visual_prompt]: Setting up Trainer...
[10/27 18:00:49 visual_prompt]: 	Setting up the optimizer...
[10/27 18:00:49 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 18:05:56 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8640 s / batch. (data: 3.14e-04). ETA=3:18:43, max mem: 7.6 GB 
[10/27 18:07:46 visual_prompt]: Epoch 1 / 100: avg data time: 2.13e+00, avg batch time: 2.9987, average train loss: 1.3980
[10/27 18:08:34 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4596, average loss: 1.3816
[10/27 18:08:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/27 18:08:34 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[10/27 18:13:40 visual_prompt]: 	Training 100/139. train loss: 0.7980,	0.8866 s / batch. (data: 1.04e-02). ETA=3:21:52, max mem: 7.6 GB 
[10/27 18:15:31 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 2.9985, average train loss: 0.7916
[10/27 18:16:18 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4637, average loss: 0.6923
[10/27 18:16:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 54.04	
[10/27 18:16:18 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[10/27 18:21:23 visual_prompt]: 	Training 100/139. train loss: 0.7281,	0.8679 s / batch. (data: 5.34e-03). ETA=3:15:35, max mem: 7.6 GB 
[10/27 18:23:14 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9924, average train loss: 0.7302
[10/27 18:24:02 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.4634, average loss: 0.6854
[10/27 18:24:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.68	
[10/27 18:24:02 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[10/27 18:29:04 visual_prompt]: 	Training 100/139. train loss: 0.7009,	0.8640 s / batch. (data: 3.20e-04). ETA=3:12:43, max mem: 7.6 GB 
[10/27 18:30:59 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 2.9997, average train loss: 0.7184
[10/27 18:31:46 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4635, average loss: 0.7530
[10/27 18:31:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.19	
[10/27 18:31:46 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[10/27 18:36:56 visual_prompt]: 	Training 100/139. train loss: 0.7102,	11.6963 s / batch. (data: 1.08e+01). ETA=1 day, 19:01:46, max mem: 7.6 GB 
[10/27 18:38:43 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 2.9984, average train loss: 0.7377
[10/27 18:39:30 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.4611, average loss: 0.6900
[10/27 18:39:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 56.07	
[10/27 18:39:30 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[10/27 18:44:36 visual_prompt]: 	Training 100/139. train loss: 0.6960,	2.3124 s / batch. (data: 1.43e+00). ETA=8:25:03, max mem: 7.6 GB 
[10/27 18:46:28 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 3.0017, average train loss: 0.7129
[10/27 18:47:15 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4634, average loss: 0.7086
[10/27 18:47:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.18	
[10/27 18:47:15 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[10/27 18:52:23 visual_prompt]: 	Training 100/139. train loss: 0.6252,	0.8720 s / batch. (data: 2.82e-04). ETA=3:08:26, max mem: 7.6 GB 
[10/27 18:54:11 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9941, average train loss: 0.7096
[10/27 18:54:59 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4649, average loss: 0.8618
[10/27 18:54:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.59	
[10/27 18:54:59 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[10/27 19:00:07 visual_prompt]: 	Training 100/139. train loss: 0.6989,	0.9080 s / batch. (data: 2.83e-04). ETA=3:14:06, max mem: 7.6 GB 
[10/27 19:01:56 visual_prompt]: Epoch 8 / 100: avg data time: 2.14e+00, avg batch time: 3.0054, average train loss: 0.7131
[10/27 19:02:44 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4623, average loss: 0.6897
[10/27 19:02:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.18	
[10/27 19:02:44 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[10/27 19:07:52 visual_prompt]: 	Training 100/139. train loss: 0.7871,	0.9280 s / batch. (data: 3.55e-04). ETA=3:16:13, max mem: 7.6 GB 
[10/27 19:09:40 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9938, average train loss: 0.7082
[10/27 19:10:28 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4592, average loss: 0.7019
[10/27 19:10:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.17	
[10/27 19:10:28 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[10/27 19:15:32 visual_prompt]: 	Training 100/139. train loss: 0.6923,	2.8869 s / batch. (data: 2.02e+00). ETA=10:03:47, max mem: 7.6 GB 
[10/27 19:17:23 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9898, average train loss: 0.7004
[10/27 19:18:11 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4600, average loss: 0.6885
[10/27 19:18:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.88	
[10/27 19:18:11 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[10/27 19:23:17 visual_prompt]: 	Training 100/139. train loss: 0.6851,	0.8673 s / batch. (data: 2.97e-04). ETA=2:59:23, max mem: 7.6 GB 
[10/27 19:25:08 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 2.9977, average train loss: 0.7093
[10/27 19:25:55 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4638, average loss: 0.7382
[10/27 19:25:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.70	
[10/27 19:25:55 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[10/27 19:31:02 visual_prompt]: 	Training 100/139. train loss: 0.7486,	1.9270 s / batch. (data: 1.07e+00). ETA=6:34:06, max mem: 7.6 GB 
[10/27 19:32:51 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 2.9952, average train loss: 0.7213
[10/27 19:33:39 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4627, average loss: 0.7071
[10/27 19:33:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.09	
[10/27 19:33:39 visual_prompt]: Best epoch 12: best metric: -0.707
[10/27 19:33:39 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[10/27 19:38:48 visual_prompt]: 	Training 100/139. train loss: 0.6915,	0.9159 s / batch. (data: 5.49e-03). ETA=3:05:11, max mem: 7.6 GB 
[10/27 19:40:37 visual_prompt]: Epoch 13 / 100: avg data time: 2.14e+00, avg batch time: 3.0064, average train loss: 0.7251
[10/27 19:41:24 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4636, average loss: 0.6896
[10/27 19:41:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.95	
[10/27 19:41:24 visual_prompt]: Best epoch 13: best metric: -0.690
[10/27 19:41:24 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[10/27 19:46:34 visual_prompt]: 	Training 100/139. train loss: 0.6957,	0.8600 s / batch. (data: 3.44e-04). ETA=2:51:53, max mem: 7.6 GB 
[10/27 19:48:21 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0020, average train loss: 0.7088
[10/27 19:49:09 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.4628, average loss: 0.7887
[10/27 19:49:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.00	
[10/27 19:49:09 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[10/27 19:54:22 visual_prompt]: 	Training 100/139. train loss: 0.6617,	9.0427 s / batch. (data: 8.16e+00). ETA=1 day, 5:46:32, max mem: 7.6 GB 
[10/27 19:56:06 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0024, average train loss: 0.7136
[10/27 19:56:54 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4598, average loss: 0.7032
[10/27 19:56:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.69	
[10/27 19:56:54 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[10/27 20:02:00 visual_prompt]: 	Training 100/139. train loss: 0.6867,	0.8821 s / batch. (data: 1.05e-02). ETA=2:52:14, max mem: 7.6 GB 
[10/27 20:03:50 visual_prompt]: Epoch 16 / 100: avg data time: 2.13e+00, avg batch time: 2.9946, average train loss: 0.7116
[10/27 20:04:37 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4594, average loss: 0.7967
[10/27 20:04:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.41	
[10/27 20:04:37 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[10/27 20:09:45 visual_prompt]: 	Training 100/139. train loss: 0.6861,	4.9476 s / batch. (data: 4.05e+00). ETA=15:54:33, max mem: 7.6 GB 
[10/27 20:11:34 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 2.9970, average train loss: 0.7179
[10/27 20:12:21 visual_prompt]: Inference (val):avg data time: 6.42e-04, avg batch time: 0.4640, average loss: 0.7345
[10/27 20:12:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.38	
[10/27 20:12:21 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[10/27 20:17:29 visual_prompt]: 	Training 100/139. train loss: 0.7382,	0.8874 s / batch. (data: 3.25e-04). ETA=2:49:09, max mem: 7.6 GB 
[10/27 20:19:19 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0033, average train loss: 0.7201
[10/27 20:20:06 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4590, average loss: 1.0071
[10/27 20:20:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.70	
[10/27 20:20:06 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[10/27 20:25:19 visual_prompt]: 	Training 100/139. train loss: 0.7222,	11.0640 s / batch. (data: 1.02e+01). ETA=1 day, 10:43:20, max mem: 7.6 GB 
[10/27 20:27:04 visual_prompt]: Epoch 19 / 100: avg data time: 2.14e+00, avg batch time: 3.0041, average train loss: 0.7238
[10/27 20:27:51 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4588, average loss: 0.7402
[10/27 20:27:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.34	
[10/27 20:27:51 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[10/27 20:32:56 visual_prompt]: 	Training 100/139. train loss: 0.7478,	0.8874 s / batch. (data: 3.06e-04). ETA=2:45:02, max mem: 7.6 GB 
[10/27 20:34:48 visual_prompt]: Epoch 20 / 100: avg data time: 2.13e+00, avg batch time: 2.9966, average train loss: 0.6991
[10/27 20:35:35 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4619, average loss: 0.7058
[10/27 20:35:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.09	
[10/27 20:35:35 visual_prompt]: Stopping early.
[10/27 20:35:35 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 20:35:35 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 20:35:35 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 20:35:35 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 20:35:35 visual_prompt]: Training with config:
[10/27 20:35:35 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr0.1_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 20:35:35 visual_prompt]: Loading training data...
[10/27 20:35:35 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 20:35:35 visual_prompt]: Loading validation data...
[10/27 20:35:35 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 20:35:35 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/27 20:35:38 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/27 20:35:38 visual_prompt]: tuned percent:0.534
[10/27 20:35:38 visual_prompt]: Device used for model: 0
[10/27 20:35:38 visual_prompt]: Setting up Evaluator...
[10/27 20:35:38 visual_prompt]: Setting up Trainer...
[10/27 20:35:38 visual_prompt]: 	Setting up the optimizer...
[10/27 20:35:38 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 20:40:45 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8709 s / batch. (data: 5.83e-03). ETA=3:20:18, max mem: 7.6 GB 
[10/27 20:42:34 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9926, average train loss: 1.3980
[10/27 20:43:22 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4598, average loss: 1.3816
[10/27 20:43:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/27 20:43:22 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[10/27 20:48:27 visual_prompt]: 	Training 100/139. train loss: 0.8029,	0.8921 s / batch. (data: 2.38e-04). ETA=3:23:06, max mem: 7.6 GB 
[10/27 20:50:19 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 2.9997, average train loss: 0.7933
[10/27 20:51:06 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4582, average loss: 0.6930
[10/27 20:51:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 53.86	
[10/27 20:51:06 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[10/27 20:56:11 visual_prompt]: 	Training 100/139. train loss: 0.7441,	0.8759 s / batch. (data: 3.02e-04). ETA=3:17:24, max mem: 7.6 GB 
[10/27 20:58:03 visual_prompt]: Epoch 3 / 100: avg data time: 2.13e+00, avg batch time: 2.9993, average train loss: 0.7412
[10/27 20:58:50 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4588, average loss: 0.6854
[10/27 20:58:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.49	
[10/27 20:58:50 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[10/27 21:03:53 visual_prompt]: 	Training 100/139. train loss: 0.7402,	0.8875 s / batch. (data: 3.06e-04). ETA=3:17:57, max mem: 7.6 GB 
[10/27 21:05:47 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 3.0000, average train loss: 0.7391
[10/27 21:06:35 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4576, average loss: 0.7351
[10/27 21:06:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.51	
[10/27 21:06:35 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[10/27 21:11:46 visual_prompt]: 	Training 100/139. train loss: 0.6726,	11.6960 s / batch. (data: 1.08e+01). ETA=1 day, 19:01:41, max mem: 7.6 GB 
[10/27 21:13:33 visual_prompt]: Epoch 5 / 100: avg data time: 2.14e+00, avg batch time: 3.0092, average train loss: 0.7445
[10/27 21:14:20 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.4567, average loss: 0.7411
[10/27 21:14:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.09	
[10/27 21:14:20 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[10/27 21:19:27 visual_prompt]: 	Training 100/139. train loss: 0.6369,	0.8760 s / batch. (data: 2.96e-04). ETA=3:11:20, max mem: 7.6 GB 
[10/27 21:21:17 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 2.9977, average train loss: 0.7309
[10/27 21:22:05 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.4601, average loss: 0.6842
[10/27 21:22:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.54	
[10/27 21:22:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[10/27 21:27:12 visual_prompt]: 	Training 100/139. train loss: 0.6776,	0.8906 s / batch. (data: 5.35e-03). ETA=3:12:26, max mem: 7.6 GB 
[10/27 21:29:00 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9909, average train loss: 0.7122
[10/27 21:29:48 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.4613, average loss: 0.8264
[10/27 21:29:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.51	
[10/27 21:29:48 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[10/27 21:34:55 visual_prompt]: 	Training 100/139. train loss: 0.6664,	0.8679 s / batch. (data: 2.85e-04). ETA=3:05:33, max mem: 7.6 GB 
[10/27 21:36:45 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0017, average train loss: 0.7317
[10/27 21:37:33 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4602, average loss: 1.0279
[10/27 21:37:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.22	
[10/27 21:37:33 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[10/27 21:42:42 visual_prompt]: 	Training 100/139. train loss: 0.7424,	0.8800 s / batch. (data: 2.92e-04). ETA=3:06:05, max mem: 7.6 GB 
[10/27 21:44:29 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9944, average train loss: 0.7577
[10/27 21:45:16 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.4586, average loss: 0.6783
[10/27 21:45:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 59.55	
[10/27 21:45:16 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[10/27 21:50:22 visual_prompt]: 	Training 100/139. train loss: 0.6839,	0.8839 s / batch. (data: 1.18e-02). ETA=3:04:51, max mem: 7.6 GB 
[10/27 21:52:12 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9920, average train loss: 0.7117
[10/27 21:53:00 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4617, average loss: 0.6942
[10/27 21:53:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 59.84	
[10/27 21:53:00 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[10/27 21:58:07 visual_prompt]: 	Training 100/139. train loss: 0.7008,	0.9161 s / batch. (data: 1.20e-02). ETA=3:09:28, max mem: 7.6 GB 
[10/27 21:59:57 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 2.9980, average train loss: 0.7248
[10/27 22:00:44 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.4612, average loss: 0.7496
[10/27 22:00:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.37	
[10/27 22:00:44 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[10/27 22:05:50 visual_prompt]: 	Training 100/139. train loss: 0.6638,	0.8768 s / batch. (data: 2.89e-04). ETA=2:59:19, max mem: 7.6 GB 
[10/27 22:07:40 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9961, average train loss: 0.7307
[10/27 22:08:28 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4627, average loss: 0.6756
[10/27 22:08:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 60.32	
[10/27 22:08:28 visual_prompt]: Best epoch 12: best metric: -0.676
[10/27 22:08:28 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[10/27 22:13:36 visual_prompt]: 	Training 100/139. train loss: 0.7280,	0.9000 s / batch. (data: 2.88e-04). ETA=3:01:58, max mem: 7.6 GB 
[10/27 22:15:25 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 2.9992, average train loss: 0.7030
[10/27 22:16:12 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4553, average loss: 0.6808
[10/27 22:16:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.73	
[10/27 22:16:12 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[10/27 22:21:21 visual_prompt]: 	Training 100/139. train loss: 0.9337,	0.8840 s / batch. (data: 8.08e-04). ETA=2:56:41, max mem: 7.6 GB 
[10/27 22:23:09 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0005, average train loss: 0.6970
[10/27 22:23:57 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4574, average loss: 0.6838
[10/27 22:23:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.73	
[10/27 22:23:57 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[10/27 22:29:09 visual_prompt]: 	Training 100/139. train loss: 0.6323,	9.4592 s / batch. (data: 8.59e+00). ETA=1 day, 7:08:49, max mem: 7.6 GB 
[10/27 22:30:54 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0018, average train loss: 0.7178
[10/27 22:31:41 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4600, average loss: 0.7429
[10/27 22:31:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.25	
[10/27 22:31:41 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[10/27 22:36:49 visual_prompt]: 	Training 100/139. train loss: 0.6930,	0.8920 s / batch. (data: 2.65e-04). ETA=2:54:10, max mem: 7.6 GB 
[10/27 22:38:37 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9928, average train loss: 0.7152
[10/27 22:39:25 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.4593, average loss: 0.7376
[10/27 22:39:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.28	
[10/27 22:39:25 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[10/27 22:44:31 visual_prompt]: 	Training 100/139. train loss: 0.7007,	3.2790 s / batch. (data: 2.38e+00). ETA=10:32:38, max mem: 7.6 GB 
[10/27 22:46:22 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 3.0007, average train loss: 0.7206
[10/27 22:47:09 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4573, average loss: 0.7290
[10/27 22:47:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.15	
[10/27 22:47:09 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[10/27 22:52:17 visual_prompt]: 	Training 100/139. train loss: 0.6865,	0.8840 s / batch. (data: 3.06e-04). ETA=2:48:30, max mem: 7.6 GB 
[10/27 22:54:07 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0038, average train loss: 0.7001
[10/27 22:54:54 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.4610, average loss: 0.8115
[10/27 22:54:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.65	
[10/27 22:54:54 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[10/27 23:00:07 visual_prompt]: 	Training 100/139. train loss: 0.6665,	10.9480 s / batch. (data: 1.01e+01). ETA=1 day, 10:21:30, max mem: 7.6 GB 
[10/27 23:01:52 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0026, average train loss: 0.7187
[10/27 23:02:39 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.4610, average loss: 0.7105
[10/27 23:02:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.72	
[10/27 23:02:39 visual_prompt]: Stopping early.
[10/27 23:02:39 visual_prompt]: Rank of current process: 0. World size: 1
[10/27 23:02:39 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/27 23:02:39 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/27 23:02:39 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/27 23:02:39 visual_prompt]: Training with config:
[10/27 23:02:39 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr0.1_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/27 23:02:39 visual_prompt]: Loading training data...
[10/27 23:02:39 visual_prompt]: Constructing mammo-cbis dataset train...
[10/27 23:02:39 visual_prompt]: Loading validation data...
[10/27 23:02:39 visual_prompt]: Constructing mammo-cbis dataset val...
[10/27 23:02:39 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/27 23:02:42 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/27 23:02:42 visual_prompt]: tuned percent:0.534
[10/27 23:02:42 visual_prompt]: Device used for model: 0
[10/27 23:02:42 visual_prompt]: Setting up Evaluator...
[10/27 23:02:42 visual_prompt]: Setting up Trainer...
[10/27 23:02:42 visual_prompt]: 	Setting up the optimizer...
[10/27 23:02:42 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/27 23:07:50 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8959 s / batch. (data: 2.89e-04). ETA=3:26:04, max mem: 7.6 GB 
[10/27 23:09:39 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 3.0010, average train loss: 1.3980
[10/27 23:10:26 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4595, average loss: 1.3816
[10/27 23:10:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/27 23:10:26 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[10/27 23:15:32 visual_prompt]: 	Training 100/139. train loss: 0.8034,	0.9069 s / batch. (data: 6.87e-03). ETA=3:26:28, max mem: 7.6 GB 
[10/27 23:17:22 visual_prompt]: Epoch 2 / 100: avg data time: 2.11e+00, avg batch time: 2.9907, average train loss: 0.7935
[10/27 23:18:10 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4607, average loss: 0.6930
[10/27 23:18:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 53.88	
[10/27 23:18:10 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[10/27 23:23:16 visual_prompt]: 	Training 100/139. train loss: 0.7420,	0.8840 s / batch. (data: 2.81e-04). ETA=3:19:13, max mem: 7.6 GB 
[10/27 23:25:06 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9939, average train loss: 0.7414
[10/27 23:25:53 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4583, average loss: 0.6851
[10/27 23:25:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.61	
[10/27 23:25:53 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[10/27 23:30:56 visual_prompt]: 	Training 100/139. train loss: 0.7396,	0.8941 s / batch. (data: 2.94e-04). ETA=3:19:25, max mem: 7.6 GB 
[10/27 23:32:51 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 3.0017, average train loss: 0.7408
[10/27 23:33:38 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.4576, average loss: 0.7313
[10/27 23:33:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.51	
[10/27 23:33:38 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[10/27 23:38:47 visual_prompt]: 	Training 100/139. train loss: 0.6753,	11.0762 s / batch. (data: 1.02e+01). ETA=1 day, 16:44:53, max mem: 7.6 GB 
[10/27 23:40:36 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0055, average train loss: 0.7387
[10/27 23:41:23 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4596, average loss: 0.7416
[10/27 23:41:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.60	
[10/27 23:41:23 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[10/27 23:46:29 visual_prompt]: 	Training 100/139. train loss: 0.6637,	1.7989 s / batch. (data: 9.23e-01). ETA=6:32:54, max mem: 7.6 GB 
[10/27 23:48:20 visual_prompt]: Epoch 6 / 100: avg data time: 2.12e+00, avg batch time: 3.0003, average train loss: 0.7311
[10/27 23:49:08 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4618, average loss: 0.7132
[10/27 23:49:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.84	
[10/27 23:49:08 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[10/27 23:54:15 visual_prompt]: 	Training 100/139. train loss: 0.6431,	1.1564 s / batch. (data: 2.89e-01). ETA=4:09:53, max mem: 7.6 GB 
[10/27 23:56:04 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9935, average train loss: 0.7215
[10/27 23:56:51 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4568, average loss: 0.7869
[10/27 23:56:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.65	
[10/27 23:56:51 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[10/28 00:01:59 visual_prompt]: 	Training 100/139. train loss: 0.7560,	0.8760 s / batch. (data: 2.90e-04). ETA=3:07:16, max mem: 7.6 GB 
[10/28 00:03:49 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0033, average train loss: 0.7358
[10/28 00:04:36 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4602, average loss: 1.0590
[10/28 00:04:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.90	
[10/28 00:04:36 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[10/28 00:09:46 visual_prompt]: 	Training 100/139. train loss: 0.7216,	0.8758 s / batch. (data: 2.88e-04). ETA=3:05:11, max mem: 7.6 GB 
[10/28 00:11:33 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9999, average train loss: 0.7577
[10/28 00:12:21 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4612, average loss: 0.7047
[10/28 00:12:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.50	
[10/28 00:12:21 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[10/28 00:17:27 visual_prompt]: 	Training 100/139. train loss: 0.7151,	0.8768 s / batch. (data: 7.96e-03). ETA=3:03:23, max mem: 7.6 GB 
[10/28 00:19:17 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9942, average train loss: 0.7236
[10/28 00:20:04 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4601, average loss: 0.6967
[10/28 00:20:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.85	rocauc: 57.98	
[10/28 00:20:04 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[10/28 00:25:12 visual_prompt]: 	Training 100/139. train loss: 0.7367,	0.8920 s / batch. (data: 2.91e-04). ETA=3:04:29, max mem: 7.6 GB 
[10/28 00:27:01 visual_prompt]: Epoch 11 / 100: avg data time: 2.12e+00, avg batch time: 2.9997, average train loss: 0.7208
[10/28 00:27:49 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.4610, average loss: 0.8388
[10/28 00:27:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.19	
[10/28 00:27:49 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[10/28 00:32:56 visual_prompt]: 	Training 100/139. train loss: 0.6726,	0.8880 s / batch. (data: 5.43e-03). ETA=3:01:36, max mem: 7.6 GB 
[10/28 00:34:46 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9989, average train loss: 0.7493
[10/28 00:35:33 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4593, average loss: 0.6759
[10/28 00:35:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 60.37	
[10/28 00:35:33 visual_prompt]: Best epoch 12: best metric: -0.676
[10/28 00:35:33 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[10/28 00:40:41 visual_prompt]: 	Training 100/139. train loss: 0.8240,	0.8804 s / batch. (data: 2.91e-04). ETA=2:58:00, max mem: 7.6 GB 
[10/28 00:42:30 visual_prompt]: Epoch 13 / 100: avg data time: 2.12e+00, avg batch time: 2.9988, average train loss: 0.7007
[10/28 00:43:17 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.4577, average loss: 0.6731
[10/28 00:43:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 60.59	
[10/28 00:43:17 visual_prompt]: Best epoch 13: best metric: -0.673
[10/28 00:43:17 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[10/28 00:48:26 visual_prompt]: 	Training 100/139. train loss: 1.0521,	0.8790 s / batch. (data: 2.61e-04). ETA=2:55:41, max mem: 7.6 GB 
[10/28 00:50:15 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0017, average train loss: 0.7024
[10/28 00:51:02 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4610, average loss: 0.6731
[10/28 00:51:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 60.49	
[10/28 00:51:02 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[10/28 00:56:15 visual_prompt]: 	Training 100/139. train loss: 0.5447,	9.2250 s / batch. (data: 8.37e+00). ETA=1 day, 6:22:33, max mem: 7.6 GB 
[10/28 00:57:59 visual_prompt]: Epoch 15 / 100: avg data time: 2.12e+00, avg batch time: 3.0006, average train loss: 0.6996
[10/28 00:58:47 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.4600, average loss: 0.6683
[10/28 00:58:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 61.78	
[10/28 00:58:47 visual_prompt]: Best epoch 15: best metric: -0.668
[10/28 00:58:47 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[10/28 01:03:54 visual_prompt]: 	Training 100/139. train loss: 0.6914,	0.8863 s / batch. (data: 1.05e-02). ETA=2:53:02, max mem: 7.6 GB 
[10/28 01:05:43 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9952, average train loss: 0.7592
[10/28 01:06:30 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4598, average loss: 0.6707
[10/28 01:06:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 60.79	
[10/28 01:06:30 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[10/28 01:11:37 visual_prompt]: 	Training 100/139. train loss: 0.6296,	4.2401 s / batch. (data: 3.38e+00). ETA=13:38:02, max mem: 7.6 GB 
[10/28 01:13:27 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 2.9955, average train loss: 0.7101
[10/28 01:14:14 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.4614, average loss: 0.7691
[10/28 01:14:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.23	
[10/28 01:14:14 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[10/28 01:19:23 visual_prompt]: 	Training 100/139. train loss: 0.6799,	2.1080 s / batch. (data: 1.20e+00). ETA=6:41:49, max mem: 7.6 GB 
[10/28 01:21:12 visual_prompt]: Epoch 18 / 100: avg data time: 2.12e+00, avg batch time: 3.0011, average train loss: 0.7171
[10/28 01:21:59 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4596, average loss: 0.7972
[10/28 01:21:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.29	
[10/28 01:21:59 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[10/28 01:27:13 visual_prompt]: 	Training 100/139. train loss: 0.5261,	11.0737 s / batch. (data: 1.02e+01). ETA=1 day, 10:45:10, max mem: 7.6 GB 
[10/28 01:28:57 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0051, average train loss: 0.7129
[10/28 01:29:44 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4563, average loss: 0.7811
[10/28 01:29:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.40	
[10/28 01:29:44 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[10/28 01:34:50 visual_prompt]: 	Training 100/139. train loss: 0.7853,	0.8723 s / batch. (data: 5.38e-03). ETA=2:42:14, max mem: 7.6 GB 
[10/28 01:36:41 visual_prompt]: Epoch 20 / 100: avg data time: 2.12e+00, avg batch time: 2.9982, average train loss: 0.6999
[10/28 01:37:29 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.4583, average loss: 0.6777
[10/28 01:37:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 61.37	
[10/28 01:37:29 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[10/28 01:42:34 visual_prompt]: 	Training 100/139. train loss: 0.8373,	0.8720 s / batch. (data: 9.67e-03). ETA=2:40:09, max mem: 7.6 GB 
[10/28 01:44:25 visual_prompt]: Epoch 21 / 100: avg data time: 2.12e+00, avg batch time: 2.9934, average train loss: 0.6866
[10/28 01:45:12 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.4627, average loss: 0.6788
[10/28 01:45:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.39	
[10/28 01:45:12 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[10/28 01:50:17 visual_prompt]: 	Training 100/139. train loss: 0.6055,	0.9066 s / batch. (data: 2.51e-04). ETA=2:44:24, max mem: 7.6 GB 
[10/28 01:52:10 visual_prompt]: Epoch 22 / 100: avg data time: 2.13e+00, avg batch time: 3.0024, average train loss: 0.7065
[10/28 01:52:57 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4609, average loss: 0.6641
[10/28 01:52:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 61.82	
[10/28 01:52:57 visual_prompt]: Best epoch 22: best metric: -0.664
[10/28 01:52:57 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[10/28 01:58:03 visual_prompt]: 	Training 100/139. train loss: 0.6980,	0.9000 s / batch. (data: 3.08e-04). ETA=2:41:07, max mem: 7.6 GB 
[10/28 01:59:55 visual_prompt]: Epoch 23 / 100: avg data time: 2.13e+00, avg batch time: 3.0046, average train loss: 0.6986
[10/28 02:00:42 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4594, average loss: 0.6771
[10/28 02:00:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 61.21	
[10/28 02:00:42 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[10/28 02:05:53 visual_prompt]: 	Training 100/139. train loss: 0.6719,	0.9072 s / batch. (data: 8.17e-04). ETA=2:40:18, max mem: 7.6 GB 
[10/28 02:07:40 visual_prompt]: Epoch 24 / 100: avg data time: 2.13e+00, avg batch time: 3.0053, average train loss: 0.6874
[10/28 02:08:27 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.4581, average loss: 0.6842
[10/28 02:08:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 61.75	
[10/28 02:08:27 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[10/28 02:13:34 visual_prompt]: 	Training 100/139. train loss: 0.6885,	0.8720 s / batch. (data: 2.89e-04). ETA=2:32:04, max mem: 7.6 GB 
[10/28 02:15:24 visual_prompt]: Epoch 25 / 100: avg data time: 2.12e+00, avg batch time: 2.9943, average train loss: 0.7047
[10/28 02:16:11 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4583, average loss: 0.7121
[10/28 02:16:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 62.33	
[10/28 02:16:11 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[10/28 02:21:17 visual_prompt]: 	Training 100/139. train loss: 0.5910,	0.8966 s / batch. (data: 1.05e-02). ETA=2:34:17, max mem: 7.6 GB 
[10/28 02:23:07 visual_prompt]: Epoch 26 / 100: avg data time: 2.12e+00, avg batch time: 2.9925, average train loss: 0.6968
[10/28 02:23:54 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4581, average loss: 0.6573
[10/28 02:23:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 64.39	
[10/28 02:23:54 visual_prompt]: Best epoch 26: best metric: -0.657
[10/28 02:23:54 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[10/28 02:29:06 visual_prompt]: 	Training 100/139. train loss: 0.7278,	9.4235 s / batch. (data: 8.55e+00). ETA=1 day, 2:39:47, max mem: 7.6 GB 
[10/28 02:30:52 visual_prompt]: Epoch 27 / 100: avg data time: 2.13e+00, avg batch time: 3.0053, average train loss: 0.6740
[10/28 02:31:40 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.4562, average loss: 0.7036
[10/28 02:31:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 62.18	
[10/28 02:31:40 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[10/28 02:36:43 visual_prompt]: 	Training 100/139. train loss: 0.6039,	0.8760 s / batch. (data: 2.82e-04). ETA=2:26:41, max mem: 7.6 GB 
[10/28 02:38:36 visual_prompt]: Epoch 28 / 100: avg data time: 2.12e+00, avg batch time: 2.9941, average train loss: 0.6839
[10/28 02:39:23 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4594, average loss: 0.6608
[10/28 02:39:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 67.48	
[10/28 02:39:23 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[10/28 02:44:26 visual_prompt]: 	Training 100/139. train loss: 0.5367,	0.8760 s / batch. (data: 2.41e-04). ETA=2:24:39, max mem: 7.6 GB 
[10/28 02:46:19 visual_prompt]: Epoch 29 / 100: avg data time: 2.11e+00, avg batch time: 2.9913, average train loss: 0.6741
[10/28 02:47:06 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.4596, average loss: 0.6553
[10/28 02:47:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 66.52	
[10/28 02:47:06 visual_prompt]: Best epoch 29: best metric: -0.655
[10/28 02:47:06 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[10/28 02:52:16 visual_prompt]: 	Training 100/139. train loss: 0.7203,	0.8800 s / batch. (data: 2.85e-04). ETA=2:23:16, max mem: 7.6 GB 
[10/28 02:54:05 visual_prompt]: Epoch 30 / 100: avg data time: 2.13e+00, avg batch time: 3.0080, average train loss: 0.6634
[10/28 02:54:52 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.4586, average loss: 0.7379
[10/28 02:54:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 65.36	
[10/28 02:54:52 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[10/28 03:00:04 visual_prompt]: 	Training 100/139. train loss: 0.7233,	0.8796 s / batch. (data: 2.54e-04). ETA=2:21:10, max mem: 7.6 GB 
[10/28 03:01:49 visual_prompt]: Epoch 31 / 100: avg data time: 2.12e+00, avg batch time: 2.9978, average train loss: 0.6835
[10/28 03:02:36 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4572, average loss: 0.6527
[10/28 03:02:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 65.01	
[10/28 03:02:36 visual_prompt]: Best epoch 31: best metric: -0.653
[10/28 03:02:36 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[10/28 03:07:46 visual_prompt]: 	Training 100/139. train loss: 0.8481,	4.7744 s / batch. (data: 3.89e+00). ETA=12:35:13, max mem: 7.6 GB 
[10/28 03:09:32 visual_prompt]: Epoch 32 / 100: avg data time: 2.12e+00, avg batch time: 2.9942, average train loss: 0.6686
[10/28 03:10:20 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4602, average loss: 0.6586
[10/28 03:10:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 65.38	
[10/28 03:10:20 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[10/28 03:15:26 visual_prompt]: 	Training 100/139. train loss: 0.6939,	0.9008 s / batch. (data: 2.48e-02). ETA=2:20:24, max mem: 7.6 GB 
[10/28 03:17:17 visual_prompt]: Epoch 33 / 100: avg data time: 2.12e+00, avg batch time: 2.9993, average train loss: 0.6506
[10/28 03:18:04 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4587, average loss: 0.7107
[10/28 03:18:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 65.59	
[10/28 03:18:04 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[10/28 03:23:13 visual_prompt]: 	Training 100/139. train loss: 0.6038,	4.9960 s / batch. (data: 4.07e+00). ETA=12:47:08, max mem: 7.6 GB 
[10/28 03:25:01 visual_prompt]: Epoch 34 / 100: avg data time: 2.12e+00, avg batch time: 2.9992, average train loss: 0.6924
[10/28 03:25:48 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4595, average loss: 0.6714
[10/28 03:25:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 63.80	
[10/28 03:25:48 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[10/28 03:30:56 visual_prompt]: 	Training 100/139. train loss: 0.5703,	0.8896 s / batch. (data: 1.20e-02). ETA=2:14:32, max mem: 7.6 GB 
[10/28 03:32:45 visual_prompt]: Epoch 35 / 100: avg data time: 2.12e+00, avg batch time: 2.9937, average train loss: 0.6485
[10/28 03:33:32 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4578, average loss: 0.7041
[10/28 03:33:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 65.42	
[10/28 03:33:32 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[10/28 03:38:42 visual_prompt]: 	Training 100/139. train loss: 0.6338,	0.8588 s / batch. (data: 2.96e-04). ETA=2:07:53, max mem: 7.6 GB 
[10/28 03:40:30 visual_prompt]: Epoch 36 / 100: avg data time: 2.13e+00, avg batch time: 3.0039, average train loss: 0.6681
[10/28 03:41:17 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.4612, average loss: 0.7456
[10/28 03:41:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 65.66	
[10/28 03:41:17 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[10/28 03:46:23 visual_prompt]: 	Training 100/139. train loss: 0.6034,	0.8836 s / batch. (data: 2.89e-04). ETA=2:09:32, max mem: 7.6 GB 
[10/28 03:48:14 visual_prompt]: Epoch 37 / 100: avg data time: 2.12e+00, avg batch time: 2.9955, average train loss: 0.6737
[10/28 03:49:01 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.4563, average loss: 0.7004
[10/28 03:49:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 65.98	
[10/28 03:49:01 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[10/28 03:54:11 visual_prompt]: 	Training 100/139. train loss: 0.9035,	7.8360 s / batch. (data: 6.90e+00). ETA=18:50:36, max mem: 7.6 GB 
[10/28 03:55:57 visual_prompt]: Epoch 38 / 100: avg data time: 2.11e+00, avg batch time: 2.9929, average train loss: 0.6533
[10/28 03:56:44 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4630, average loss: 0.6569
[10/28 03:56:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 68.84	
[10/28 03:56:44 visual_prompt]: Stopping early.
[10/28 03:56:45 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 03:56:45 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 03:56:45 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 03:56:45 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 03:56:45 visual_prompt]: Training with config:
[10/28 03:56:45 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr0.1_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 03:56:45 visual_prompt]: Loading training data...
[10/28 03:56:45 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 03:56:45 visual_prompt]: Loading validation data...
[10/28 03:56:45 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 03:56:45 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/28 03:56:47 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/28 03:56:47 visual_prompt]: tuned percent:0.534
[10/28 03:56:47 visual_prompt]: Device used for model: 0
[10/28 03:56:47 visual_prompt]: Setting up Evaluator...
[10/28 03:56:47 visual_prompt]: Setting up Trainer...
[10/28 03:56:47 visual_prompt]: 	Setting up the optimizer...
[10/28 03:56:47 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 04:01:54 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8840 s / batch. (data: 3.16e-04). ETA=3:23:19, max mem: 7.6 GB 
[10/28 04:03:44 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9946, average train loss: 1.3980
[10/28 04:04:31 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.4675, average loss: 1.3816
[10/28 04:04:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/28 04:04:31 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[10/28 04:09:37 visual_prompt]: 	Training 100/139. train loss: 0.8034,	0.8999 s / batch. (data: 7.96e-03). ETA=3:24:54, max mem: 7.6 GB 
[10/28 04:11:27 visual_prompt]: Epoch 2 / 100: avg data time: 2.12e+00, avg batch time: 2.9957, average train loss: 0.7935
[10/28 04:12:15 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4642, average loss: 0.6930
[10/28 04:12:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 53.88	
[10/28 04:12:15 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[10/28 04:17:21 visual_prompt]: 	Training 100/139. train loss: 0.7422,	0.8577 s / batch. (data: 2.93e-04). ETA=3:13:17, max mem: 7.6 GB 
[10/28 04:19:12 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9979, average train loss: 0.7415
[10/28 04:19:59 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.4644, average loss: 0.6851
[10/28 04:19:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.65	
[10/28 04:19:59 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[10/28 04:25:01 visual_prompt]: 	Training 100/139. train loss: 0.7392,	0.8961 s / batch. (data: 7.99e-03). ETA=3:19:52, max mem: 7.6 GB 
[10/28 04:26:56 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 2.9988, average train loss: 0.7410
[10/28 04:27:43 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.4665, average loss: 0.7311
[10/28 04:27:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.56	
[10/28 04:27:43 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[10/28 04:32:54 visual_prompt]: 	Training 100/139. train loss: 0.6652,	11.7440 s / batch. (data: 1.08e+01). ETA=1 day, 19:12:17, max mem: 7.6 GB 
[10/28 04:34:42 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0088, average train loss: 0.7412
[10/28 04:35:29 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.4631, average loss: 0.7508
[10/28 04:35:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.47	
[10/28 04:35:29 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[10/28 04:40:36 visual_prompt]: 	Training 100/139. train loss: 0.6109,	0.9000 s / batch. (data: 2.34e-04). ETA=3:16:34, max mem: 7.6 GB 
[10/28 04:42:27 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 3.0045, average train loss: 0.7385
[10/28 04:43:14 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4620, average loss: 0.6838
[10/28 04:43:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.00	
[10/28 04:43:14 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[10/28 04:48:23 visual_prompt]: 	Training 100/139. train loss: 0.6518,	0.8800 s / batch. (data: 2.74e-03). ETA=3:10:10, max mem: 7.6 GB 
[10/28 04:50:11 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9986, average train loss: 0.7164
[10/28 04:50:58 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.4703, average loss: 0.8278
[10/28 04:50:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.31	
[10/28 04:50:58 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[10/28 04:56:06 visual_prompt]: 	Training 100/139. train loss: 0.7085,	0.8827 s / batch. (data: 5.41e-03). ETA=3:08:41, max mem: 7.6 GB 
[10/28 04:57:56 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0042, average train loss: 0.7383
[10/28 04:58:44 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.4655, average loss: 1.0683
[10/28 04:58:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.89	
[10/28 04:58:44 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[10/28 05:03:54 visual_prompt]: 	Training 100/139. train loss: 0.7002,	0.8760 s / batch. (data: 1.20e-02). ETA=3:05:14, max mem: 7.6 GB 
[10/28 05:05:40 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9976, average train loss: 0.7564
[10/28 05:06:28 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4680, average loss: 0.7008
[10/28 05:06:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.23	
[10/28 05:06:28 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[10/28 05:11:35 visual_prompt]: 	Training 100/139. train loss: 0.7098,	0.8838 s / batch. (data: 5.42e-03). ETA=3:04:50, max mem: 7.6 GB 
[10/28 05:13:24 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9962, average train loss: 0.7230
[10/28 05:14:12 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4654, average loss: 0.6893
[10/28 05:14:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 59.21	
[10/28 05:14:12 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[10/28 05:19:19 visual_prompt]: 	Training 100/139. train loss: 0.7515,	0.8594 s / batch. (data: 5.42e-03). ETA=2:57:45, max mem: 7.6 GB 
[10/28 05:21:09 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0007, average train loss: 0.7205
[10/28 05:21:56 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.4652, average loss: 0.8292
[10/28 05:21:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.64	
[10/28 05:21:56 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[10/28 05:27:03 visual_prompt]: 	Training 100/139. train loss: 0.6768,	1.1600 s / batch. (data: 2.76e-01). ETA=3:57:14, max mem: 7.6 GB 
[10/28 05:28:53 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9975, average train loss: 0.7503
[10/28 05:29:41 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4642, average loss: 0.6757
[10/28 05:29:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 60.22	
[10/28 05:29:41 visual_prompt]: Best epoch 12: best metric: -0.676
[10/28 05:29:41 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[10/28 05:34:49 visual_prompt]: 	Training 100/139. train loss: 0.8163,	0.9080 s / batch. (data: 2.71e-04). ETA=3:03:35, max mem: 7.6 GB 
[10/28 05:36:39 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0058, average train loss: 0.7014
[10/28 05:37:26 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4693, average loss: 0.6721
[10/28 05:37:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 60.65	
[10/28 05:37:26 visual_prompt]: Best epoch 13: best metric: -0.672
[10/28 05:37:26 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[10/28 05:42:36 visual_prompt]: 	Training 100/139. train loss: 1.1298,	0.8840 s / batch. (data: 2.75e-04). ETA=2:56:42, max mem: 7.6 GB 
[10/28 05:44:23 visual_prompt]: Epoch 14 / 100: avg data time: 2.12e+00, avg batch time: 3.0007, average train loss: 0.7034
[10/28 05:45:10 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.4665, average loss: 0.6719
[10/28 05:45:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 60.39	
[10/28 05:45:10 visual_prompt]: Best epoch 14: best metric: -0.672
[10/28 05:45:10 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[10/28 05:50:24 visual_prompt]: 	Training 100/139. train loss: 0.5579,	9.1481 s / batch. (data: 8.27e+00). ETA=1 day, 6:07:21, max mem: 7.6 GB 
[10/28 05:52:08 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0022, average train loss: 0.7001
[10/28 05:52:55 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.4646, average loss: 0.6670
[10/28 05:52:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 61.27	
[10/28 05:52:55 visual_prompt]: Best epoch 15: best metric: -0.667
[10/28 05:52:55 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[10/28 05:58:02 visual_prompt]: 	Training 100/139. train loss: 0.6880,	0.9121 s / batch. (data: 8.12e-03). ETA=2:58:05, max mem: 7.6 GB 
[10/28 05:59:51 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9925, average train loss: 0.7608
[10/28 06:00:39 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.4663, average loss: 0.6692
[10/28 06:00:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 60.59	
[10/28 06:00:39 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[10/28 06:05:46 visual_prompt]: 	Training 100/139. train loss: 0.6212,	3.9520 s / batch. (data: 3.07e+00). ETA=12:42:28, max mem: 7.6 GB 
[10/28 06:07:36 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 2.9980, average train loss: 0.7086
[10/28 06:08:23 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4643, average loss: 0.7691
[10/28 06:08:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.92	
[10/28 06:08:23 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[10/28 06:13:30 visual_prompt]: 	Training 100/139. train loss: 0.6816,	1.2680 s / batch. (data: 3.81e-01). ETA=4:01:42, max mem: 7.6 GB 
[10/28 06:15:20 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0025, average train loss: 0.7155
[10/28 06:16:08 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4665, average loss: 0.8150
[10/28 06:16:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 61.25	
[10/28 06:16:08 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[10/28 06:21:19 visual_prompt]: 	Training 100/139. train loss: 0.5194,	9.6545 s / batch. (data: 8.77e+00). ETA=1 day, 6:17:56, max mem: 7.6 GB 
[10/28 06:23:05 visual_prompt]: Epoch 19 / 100: avg data time: 2.12e+00, avg batch time: 3.0003, average train loss: 0.7126
[10/28 06:23:52 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.4703, average loss: 0.7935
[10/28 06:23:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.73	
[10/28 06:23:52 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[10/28 06:28:59 visual_prompt]: 	Training 100/139. train loss: 0.8113,	0.9271 s / batch. (data: 1.51e-02). ETA=2:52:25, max mem: 7.6 GB 
[10/28 06:30:49 visual_prompt]: Epoch 20 / 100: avg data time: 2.12e+00, avg batch time: 2.9970, average train loss: 0.7009
[10/28 06:31:36 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4659, average loss: 0.6745
[10/28 06:31:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 61.28	
[10/28 06:31:36 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[10/28 06:36:43 visual_prompt]: 	Training 100/139. train loss: 0.8840,	0.8834 s / batch. (data: 1.05e-02). ETA=2:42:15, max mem: 7.6 GB 
[10/28 06:38:33 visual_prompt]: Epoch 21 / 100: avg data time: 2.12e+00, avg batch time: 2.9975, average train loss: 0.6859
[10/28 06:39:20 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4691, average loss: 0.6745
[10/28 06:39:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 61.29	
[10/28 06:39:20 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[10/28 06:44:24 visual_prompt]: 	Training 100/139. train loss: 0.5955,	0.8904 s / batch. (data: 8.25e-03). ETA=2:41:28, max mem: 7.6 GB 
[10/28 06:46:17 visual_prompt]: Epoch 22 / 100: avg data time: 2.12e+00, avg batch time: 2.9975, average train loss: 0.7038
[10/28 06:47:05 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4635, average loss: 0.6643
[10/28 06:47:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 61.11	
[10/28 06:47:05 visual_prompt]: Best epoch 22: best metric: -0.664
[10/28 06:47:05 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[10/28 06:52:10 visual_prompt]: 	Training 100/139. train loss: 0.7120,	0.8720 s / batch. (data: 2.95e-04). ETA=2:36:06, max mem: 7.6 GB 
[10/28 06:54:02 visual_prompt]: Epoch 23 / 100: avg data time: 2.13e+00, avg batch time: 3.0012, average train loss: 0.6959
[10/28 06:54:49 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4627, average loss: 0.6903
[10/28 06:54:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 59.82	
[10/28 06:54:49 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[10/28 07:00:01 visual_prompt]: 	Training 100/139. train loss: 0.6824,	0.8760 s / batch. (data: 3.20e-04). ETA=2:34:48, max mem: 7.6 GB 
[10/28 07:01:48 visual_prompt]: Epoch 24 / 100: avg data time: 2.13e+00, avg batch time: 3.0074, average train loss: 0.6908
[10/28 07:02:35 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4648, average loss: 0.6854
[10/28 07:02:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 62.08	
[10/28 07:02:35 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[10/28 07:07:42 visual_prompt]: 	Training 100/139. train loss: 0.6961,	0.8880 s / batch. (data: 2.92e-04). ETA=2:34:51, max mem: 7.6 GB 
[10/28 07:09:32 visual_prompt]: Epoch 25 / 100: avg data time: 2.12e+00, avg batch time: 2.9982, average train loss: 0.7016
[10/28 07:10:19 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4702, average loss: 0.7061
[10/28 07:10:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 60.89	
[10/28 07:10:19 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[10/28 07:15:27 visual_prompt]: 	Training 100/139. train loss: 0.5878,	0.8640 s / batch. (data: 5.41e-03). ETA=2:28:41, max mem: 7.6 GB 
[10/28 07:17:15 visual_prompt]: Epoch 26 / 100: avg data time: 2.11e+00, avg batch time: 2.9914, average train loss: 0.6925
[10/28 07:18:02 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.4611, average loss: 0.6621
[10/28 07:18:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 62.92	
[10/28 07:18:02 visual_prompt]: Best epoch 26: best metric: -0.662
[10/28 07:18:02 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[10/28 07:23:14 visual_prompt]: 	Training 100/139. train loss: 0.7063,	9.2126 s / batch. (data: 8.35e+00). ETA=1 day, 2:04:00, max mem: 7.6 GB 
[10/28 07:25:01 visual_prompt]: Epoch 27 / 100: avg data time: 2.14e+00, avg batch time: 3.0116, average train loss: 0.6719
[10/28 07:25:49 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4656, average loss: 0.6869
[10/28 07:25:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 62.56	
[10/28 07:25:49 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[10/28 07:30:53 visual_prompt]: 	Training 100/139. train loss: 0.6606,	0.8715 s / batch. (data: 3.02e-04). ETA=2:25:55, max mem: 7.6 GB 
[10/28 07:32:45 visual_prompt]: Epoch 28 / 100: avg data time: 2.12e+00, avg batch time: 2.9987, average train loss: 0.6813
[10/28 07:33:33 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4637, average loss: 0.6745
[10/28 07:33:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 64.93	
[10/28 07:33:33 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[10/28 07:38:36 visual_prompt]: 	Training 100/139. train loss: 0.5224,	0.9185 s / batch. (data: 2.55e-02). ETA=2:31:40, max mem: 7.6 GB 
[10/28 07:40:29 visual_prompt]: Epoch 29 / 100: avg data time: 2.12e+00, avg batch time: 2.9930, average train loss: 0.6664
[10/28 07:41:16 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4669, average loss: 0.6744
[10/28 07:41:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 65.06	
[10/28 07:41:16 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[10/28 07:46:26 visual_prompt]: 	Training 100/139. train loss: 0.6935,	0.8760 s / batch. (data: 7.96e-03). ETA=2:22:37, max mem: 7.6 GB 
[10/28 07:48:14 visual_prompt]: Epoch 30 / 100: avg data time: 2.13e+00, avg batch time: 3.0051, average train loss: 0.6648
[10/28 07:49:02 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.4650, average loss: 0.7715
[10/28 07:49:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 62.87	
[10/28 07:49:02 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[10/28 07:54:13 visual_prompt]: 	Training 100/139. train loss: 0.7501,	0.8613 s / batch. (data: 2.83e-04). ETA=2:18:14, max mem: 7.6 GB 
[10/28 07:55:59 visual_prompt]: Epoch 31 / 100: avg data time: 2.12e+00, avg batch time: 2.9987, average train loss: 0.6821
[10/28 07:56:46 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4661, average loss: 0.6574
[10/28 07:56:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 64.26	
[10/28 07:56:46 visual_prompt]: Best epoch 31: best metric: -0.657
[10/28 07:56:46 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[10/28 08:01:57 visual_prompt]: 	Training 100/139. train loss: 0.8693,	6.0145 s / batch. (data: 5.13e+00). ETA=15:51:23, max mem: 7.6 GB 
[10/28 08:03:43 visual_prompt]: Epoch 32 / 100: avg data time: 2.12e+00, avg batch time: 2.9959, average train loss: 0.6672
[10/28 08:04:30 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.4645, average loss: 0.6694
[10/28 08:04:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 62.23	
[10/28 08:04:30 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[10/28 08:09:36 visual_prompt]: 	Training 100/139. train loss: 0.6947,	0.8824 s / batch. (data: 2.93e-04). ETA=2:17:32, max mem: 7.6 GB 
[10/28 08:11:28 visual_prompt]: Epoch 33 / 100: avg data time: 2.13e+00, avg batch time: 3.0040, average train loss: 0.6464
[10/28 08:12:15 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.4686, average loss: 0.7184
[10/28 08:12:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 65.27	
[10/28 08:12:15 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[10/28 08:17:23 visual_prompt]: 	Training 100/139. train loss: 0.5473,	3.0629 s / batch. (data: 2.17e+00). ETA=7:50:18, max mem: 7.6 GB 
[10/28 08:19:13 visual_prompt]: Epoch 34 / 100: avg data time: 2.13e+00, avg batch time: 3.0034, average train loss: 0.6910
[10/28 08:20:00 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.4683, average loss: 0.6840
[10/28 08:20:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 61.65	
[10/28 08:20:00 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[10/28 08:25:08 visual_prompt]: 	Training 100/139. train loss: 0.5532,	0.8770 s / batch. (data: 8.24e-03). ETA=2:12:37, max mem: 7.6 GB 
[10/28 08:26:57 visual_prompt]: Epoch 35 / 100: avg data time: 2.12e+00, avg batch time: 3.0000, average train loss: 0.6449
[10/28 08:27:45 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.4658, average loss: 0.7282
[10/28 08:27:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 63.51	
[10/28 08:27:45 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[10/28 08:32:54 visual_prompt]: 	Training 100/139. train loss: 0.6748,	0.8802 s / batch. (data: 7.97e-03). ETA=2:11:04, max mem: 7.6 GB 
[10/28 08:34:42 visual_prompt]: Epoch 36 / 100: avg data time: 2.13e+00, avg batch time: 3.0053, average train loss: 0.6534
[10/28 08:35:30 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4673, average loss: 0.7738
[10/28 08:35:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 64.50	
[10/28 08:35:30 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[10/28 08:40:38 visual_prompt]: 	Training 100/139. train loss: 0.6008,	0.8720 s / batch. (data: 2.90e-04). ETA=2:07:50, max mem: 7.6 GB 
[10/28 08:42:27 visual_prompt]: Epoch 37 / 100: avg data time: 2.13e+00, avg batch time: 3.0030, average train loss: 0.6823
[10/28 08:43:15 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4690, average loss: 0.6974
[10/28 08:43:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 62.83	
[10/28 08:43:15 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[10/28 08:48:27 visual_prompt]: 	Training 100/139. train loss: 0.9010,	10.2891 s / batch. (data: 9.43e+00). ETA=1 day, 0:44:32, max mem: 7.6 GB 
[10/28 08:50:13 visual_prompt]: Epoch 38 / 100: avg data time: 2.13e+00, avg batch time: 3.0048, average train loss: 0.6503
[10/28 08:51:00 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.4658, average loss: 0.6729
[10/28 08:51:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 67.01	
[10/28 08:51:00 visual_prompt]: Stopping early.
[10/28 08:51:00 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 08:51:00 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 08:51:00 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 08:51:00 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 08:51:00 visual_prompt]: Training with config:
[10/28 08:51:00 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr0.05_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 08:51:00 visual_prompt]: Loading training data...
[10/28 08:51:00 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 08:51:00 visual_prompt]: Loading validation data...
[10/28 08:51:00 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 08:51:00 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/28 08:51:03 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/28 08:51:03 visual_prompt]: tuned percent:0.534
[10/28 08:51:03 visual_prompt]: Device used for model: 0
[10/28 08:51:03 visual_prompt]: Setting up Evaluator...
[10/28 08:51:03 visual_prompt]: Setting up Trainer...
[10/28 08:51:03 visual_prompt]: 	Setting up the optimizer...
[10/28 08:51:03 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 08:56:09 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.9118 s / batch. (data: 7.24e-04). ETA=3:29:42, max mem: 7.6 GB 
[10/28 08:57:59 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9960, average train loss: 1.3980
[10/28 08:58:47 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.4652, average loss: 1.3816
[10/28 08:58:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/28 08:58:47 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[10/28 09:03:53 visual_prompt]: 	Training 100/139. train loss: 0.7287,	0.8744 s / batch. (data: 1.05e-02). ETA=3:19:05, max mem: 7.6 GB 
[10/28 09:05:45 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 3.0051, average train loss: 0.7374
[10/28 09:06:32 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4699, average loss: 0.6841
[10/28 09:06:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 56.13	
[10/28 09:06:32 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[10/28 09:11:38 visual_prompt]: 	Training 100/139. train loss: 0.6948,	0.8728 s / batch. (data: 7.96e-03). ETA=3:16:42, max mem: 7.6 GB 
[10/28 09:13:28 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9927, average train loss: 0.7176
[10/28 09:14:15 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.4687, average loss: 0.6891
[10/28 09:14:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 58.11	
[10/28 09:14:15 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[10/28 09:19:17 visual_prompt]: 	Training 100/139. train loss: 0.7004,	0.8782 s / batch. (data: 1.02e-02). ETA=3:15:53, max mem: 7.6 GB 
[10/28 09:21:12 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 2.9960, average train loss: 0.7177
[10/28 09:21:59 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.4635, average loss: 0.7430
[10/28 09:21:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.79	
[10/28 09:21:59 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[10/28 09:27:10 visual_prompt]: 	Training 100/139. train loss: 0.6844,	10.8955 s / batch. (data: 1.00e+01). ETA=1 day, 16:04:59, max mem: 7.6 GB 
[10/28 09:28:58 visual_prompt]: Epoch 5 / 100: avg data time: 2.14e+00, avg batch time: 3.0130, average train loss: 0.7239
[10/28 09:29:45 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4676, average loss: 0.6839
[10/28 09:29:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.21	
[10/28 09:29:45 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[10/28 09:34:50 visual_prompt]: 	Training 100/139. train loss: 0.6831,	1.0759 s / batch. (data: 1.99e-01). ETA=3:55:00, max mem: 7.6 GB 
[10/28 09:36:42 visual_prompt]: Epoch 6 / 100: avg data time: 2.12e+00, avg batch time: 2.9980, average train loss: 0.7143
[10/28 09:37:29 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4613, average loss: 0.6944
[10/28 09:37:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.86	
[10/28 09:37:29 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[10/28 09:42:36 visual_prompt]: 	Training 100/139. train loss: 0.6178,	0.9320 s / batch. (data: 5.81e-02). ETA=3:21:24, max mem: 7.6 GB 
[10/28 09:44:25 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9893, average train loss: 0.7046
[10/28 09:45:12 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.4692, average loss: 0.9382
[10/28 09:45:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.81	
[10/28 09:45:12 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[10/28 09:50:20 visual_prompt]: 	Training 100/139. train loss: 0.6796,	0.8678 s / batch. (data: 2.80e-04). ETA=3:05:31, max mem: 7.6 GB 
[10/28 09:52:11 visual_prompt]: Epoch 8 / 100: avg data time: 2.14e+00, avg batch time: 3.0092, average train loss: 0.7093
[10/28 09:52:58 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.4639, average loss: 0.6871
[10/28 09:52:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.69	
[10/28 09:52:58 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[10/28 09:58:08 visual_prompt]: 	Training 100/139. train loss: 0.7709,	0.8800 s / batch. (data: 3.01e-04). ETA=3:06:05, max mem: 7.6 GB 
[10/28 09:59:55 visual_prompt]: Epoch 9 / 100: avg data time: 2.13e+00, avg batch time: 3.0022, average train loss: 0.7089
[10/28 10:00:43 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4641, average loss: 0.7025
[10/28 10:00:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.77	
[10/28 10:00:43 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[10/28 10:05:50 visual_prompt]: 	Training 100/139. train loss: 0.6941,	0.8803 s / batch. (data: 1.06e-02). ETA=3:04:06, max mem: 7.6 GB 
[10/28 10:07:39 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9942, average train loss: 0.6948
[10/28 10:08:26 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4675, average loss: 0.6884
[10/28 10:08:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.59	
[10/28 10:08:26 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[10/28 10:13:33 visual_prompt]: 	Training 100/139. train loss: 0.6866,	0.8800 s / batch. (data: 2.81e-04). ETA=3:02:00, max mem: 7.6 GB 
[10/28 10:15:24 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0033, average train loss: 0.7015
[10/28 10:16:11 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4664, average loss: 0.6977
[10/28 10:16:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.29	
[10/28 10:16:11 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[10/28 10:21:18 visual_prompt]: 	Training 100/139. train loss: 0.7282,	1.4921 s / batch. (data: 5.99e-01). ETA=5:05:09, max mem: 7.6 GB 
[10/28 10:23:08 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9947, average train loss: 0.6969
[10/28 10:23:55 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.4680, average loss: 0.6941
[10/28 10:23:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.01	
[10/28 10:23:55 visual_prompt]: Best epoch 12: best metric: -0.694
[10/28 10:23:55 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[10/28 10:29:04 visual_prompt]: 	Training 100/139. train loss: 0.6950,	0.8760 s / batch. (data: 7.95e-03). ETA=2:57:07, max mem: 7.6 GB 
[10/28 10:30:53 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0049, average train loss: 0.7078
[10/28 10:31:40 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.4670, average loss: 0.6981
[10/28 10:31:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.38	
[10/28 10:31:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[10/28 10:36:50 visual_prompt]: 	Training 100/139. train loss: 0.7182,	0.8882 s / batch. (data: 7.55e-04). ETA=2:57:31, max mem: 7.6 GB 
[10/28 10:38:37 visual_prompt]: Epoch 14 / 100: avg data time: 2.12e+00, avg batch time: 2.9973, average train loss: 0.7046
[10/28 10:39:24 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4661, average loss: 0.6993
[10/28 10:39:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.09	
[10/28 10:39:24 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[10/28 10:44:36 visual_prompt]: 	Training 100/139. train loss: 0.6657,	9.4640 s / batch. (data: 8.57e+00). ETA=1 day, 7:09:45, max mem: 7.6 GB 
[10/28 10:46:21 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0009, average train loss: 0.7033
[10/28 10:47:08 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4678, average loss: 0.6892
[10/28 10:47:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.36	
[10/28 10:47:08 visual_prompt]: Best epoch 15: best metric: -0.689
[10/28 10:47:08 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[10/28 10:52:15 visual_prompt]: 	Training 100/139. train loss: 0.6895,	0.8602 s / batch. (data: 2.41e-04). ETA=2:47:57, max mem: 7.6 GB 
[10/28 10:54:05 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9944, average train loss: 0.6957
[10/28 10:54:52 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4644, average loss: 0.7268
[10/28 10:54:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.99	
[10/28 10:54:52 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[10/28 10:59:58 visual_prompt]: 	Training 100/139. train loss: 0.6854,	2.7552 s / batch. (data: 1.86e+00). ETA=8:51:34, max mem: 7.6 GB 
[10/28 11:01:48 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 2.9935, average train loss: 0.7057
[10/28 11:02:35 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.4653, average loss: 0.6901
[10/28 11:02:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.57	
[10/28 11:02:35 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[10/28 11:07:43 visual_prompt]: 	Training 100/139. train loss: 0.7857,	1.7520 s / batch. (data: 8.47e-01). ETA=5:33:57, max mem: 7.6 GB 
[10/28 11:09:33 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0032, average train loss: 0.6979
[10/28 11:10:20 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4684, average loss: 0.8126
[10/28 11:10:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.00	
[10/28 11:10:20 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[10/28 11:15:33 visual_prompt]: 	Training 100/139. train loss: 0.5750,	11.0241 s / batch. (data: 1.01e+01). ETA=1 day, 10:35:49, max mem: 7.6 GB 
[10/28 11:17:17 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 2.9999, average train loss: 0.6981
[10/28 11:18:05 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.4664, average loss: 0.7230
[10/28 11:18:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.22	
[10/28 11:18:05 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[10/28 11:23:10 visual_prompt]: 	Training 100/139. train loss: 0.7869,	0.8924 s / batch. (data: 7.96e-03). ETA=2:45:58, max mem: 7.6 GB 
[10/28 11:25:01 visual_prompt]: Epoch 20 / 100: avg data time: 2.12e+00, avg batch time: 2.9954, average train loss: 0.6952
[10/28 11:25:48 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4643, average loss: 0.7628
[10/28 11:25:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.18	
[10/28 11:25:48 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[10/28 11:30:55 visual_prompt]: 	Training 100/139. train loss: 0.6945,	0.8637 s / batch. (data: 3.33e-04). ETA=2:38:38, max mem: 7.6 GB 
[10/28 11:32:45 visual_prompt]: Epoch 21 / 100: avg data time: 2.12e+00, avg batch time: 2.9930, average train loss: 0.6961
[10/28 11:33:32 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4664, average loss: 0.6940
[10/28 11:33:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.31	
[10/28 11:33:32 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[10/28 11:38:35 visual_prompt]: 	Training 100/139. train loss: 0.6992,	0.8880 s / batch. (data: 2.89e-04). ETA=2:41:02, max mem: 7.6 GB 
[10/28 11:40:28 visual_prompt]: Epoch 22 / 100: avg data time: 2.12e+00, avg batch time: 2.9942, average train loss: 0.6980
[10/28 11:41:15 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4666, average loss: 0.7111
[10/28 11:41:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.18	
[10/28 11:41:15 visual_prompt]: Stopping early.
[10/28 11:41:16 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 11:41:16 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 11:41:16 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 11:41:16 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 11:41:16 visual_prompt]: Training with config:
[10/28 11:41:16 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr0.05_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 11:41:16 visual_prompt]: Loading training data...
[10/28 11:41:16 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 11:41:16 visual_prompt]: Loading validation data...
[10/28 11:41:16 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 11:41:16 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/28 11:41:18 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/28 11:41:18 visual_prompt]: tuned percent:0.534
[10/28 11:41:18 visual_prompt]: Device used for model: 0
[10/28 11:41:18 visual_prompt]: Setting up Evaluator...
[10/28 11:41:18 visual_prompt]: Setting up Trainer...
[10/28 11:41:18 visual_prompt]: 	Setting up the optimizer...
[10/28 11:41:18 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 11:46:25 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.9275 s / batch. (data: 5.88e-03). ETA=3:33:19, max mem: 7.6 GB 
[10/28 11:48:15 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9974, average train loss: 1.3980
[10/28 11:49:02 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4703, average loss: 1.3816
[10/28 11:49:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/28 11:49:02 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[10/28 11:54:08 visual_prompt]: 	Training 100/139. train loss: 0.7308,	0.9069 s / batch. (data: 2.70e-02). ETA=3:26:29, max mem: 7.6 GB 
[10/28 11:56:00 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 3.0010, average train loss: 0.7381
[10/28 11:56:47 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.4688, average loss: 0.6840
[10/28 11:56:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 56.12	
[10/28 11:56:47 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[10/28 12:01:55 visual_prompt]: 	Training 100/139. train loss: 0.6987,	0.8808 s / batch. (data: 2.99e-04). ETA=3:18:29, max mem: 7.6 GB 
[10/28 12:03:43 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9947, average train loss: 0.7206
[10/28 12:04:31 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4667, average loss: 0.6866
[10/28 12:04:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 57.96	
[10/28 12:04:31 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[10/28 12:09:33 visual_prompt]: 	Training 100/139. train loss: 0.6968,	0.8634 s / batch. (data: 1.79e-03). ETA=3:12:34, max mem: 7.6 GB 
[10/28 12:11:27 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 2.9978, average train loss: 0.7228
[10/28 12:12:15 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4660, average loss: 0.7492
[10/28 12:12:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.24	
[10/28 12:12:15 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[10/28 12:17:26 visual_prompt]: 	Training 100/139. train loss: 0.6712,	11.1232 s / batch. (data: 1.02e+01). ETA=1 day, 16:55:15, max mem: 7.6 GB 
[10/28 12:19:14 visual_prompt]: Epoch 5 / 100: avg data time: 2.14e+00, avg batch time: 3.0167, average train loss: 0.7319
[10/28 12:20:02 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.4695, average loss: 0.6883
[10/28 12:20:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.86	
[10/28 12:20:02 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[10/28 12:25:08 visual_prompt]: 	Training 100/139. train loss: 0.7424,	0.9080 s / batch. (data: 7.97e-03). ETA=3:18:19, max mem: 7.6 GB 
[10/28 12:26:58 visual_prompt]: Epoch 6 / 100: avg data time: 2.12e+00, avg batch time: 2.9979, average train loss: 0.7226
[10/28 12:27:46 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.4687, average loss: 0.6836
[10/28 12:27:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 58.06	
[10/28 12:27:46 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[10/28 12:32:53 visual_prompt]: 	Training 100/139. train loss: 0.6261,	2.1789 s / batch. (data: 1.29e+00). ETA=7:50:51, max mem: 7.6 GB 
[10/28 12:34:42 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9928, average train loss: 0.7144
[10/28 12:35:29 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4701, average loss: 1.0940
[10/28 12:35:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.58	
[10/28 12:35:29 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[10/28 12:40:37 visual_prompt]: 	Training 100/139. train loss: 0.6546,	0.8760 s / batch. (data: 3.07e-04). ETA=3:07:16, max mem: 7.6 GB 
[10/28 12:42:27 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0060, average train loss: 0.7193
[10/28 12:43:15 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.4686, average loss: 0.7954
[10/28 12:43:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.03	
[10/28 12:43:15 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[10/28 12:48:24 visual_prompt]: 	Training 100/139. train loss: 0.7486,	0.8776 s / batch. (data: 1.09e-02). ETA=3:05:34, max mem: 7.6 GB 
[10/28 12:50:12 visual_prompt]: Epoch 9 / 100: avg data time: 2.13e+00, avg batch time: 3.0017, average train loss: 0.7326
[10/28 12:50:59 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.4706, average loss: 0.6828
[10/28 12:50:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.53	
[10/28 12:50:59 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[10/28 12:56:07 visual_prompt]: 	Training 100/139. train loss: 0.6923,	0.8886 s / batch. (data: 2.23e-02). ETA=3:05:51, max mem: 7.6 GB 
[10/28 12:57:56 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9968, average train loss: 0.6989
[10/28 12:58:43 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.4688, average loss: 0.6889
[10/28 12:58:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 58.65	
[10/28 12:58:43 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[10/28 13:03:50 visual_prompt]: 	Training 100/139. train loss: 0.7223,	0.9254 s / batch. (data: 1.33e-02). ETA=3:11:24, max mem: 7.6 GB 
[10/28 13:05:40 visual_prompt]: Epoch 11 / 100: avg data time: 2.12e+00, avg batch time: 2.9964, average train loss: 0.7164
[10/28 13:06:27 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4705, average loss: 0.6743
[10/28 13:06:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 60.31	
[10/28 13:06:27 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[10/28 13:11:34 visual_prompt]: 	Training 100/139. train loss: 0.6909,	0.8810 s / batch. (data: 5.41e-03). ETA=3:00:11, max mem: 7.6 GB 
[10/28 13:13:23 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9932, average train loss: 0.7189
[10/28 13:14:11 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4673, average loss: 0.7054
[10/28 13:14:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 60.03	
[10/28 13:14:11 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[10/28 13:19:20 visual_prompt]: 	Training 100/139. train loss: 0.7566,	0.8575 s / batch. (data: 3.11e-04). ETA=2:53:23, max mem: 7.6 GB 
[10/28 13:21:09 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0050, average train loss: 0.7319
[10/28 13:21:56 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4664, average loss: 0.7609
[10/28 13:21:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.91	
[10/28 13:21:56 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[10/28 13:27:05 visual_prompt]: 	Training 100/139. train loss: 0.7922,	0.8732 s / batch. (data: 2.95e-04). ETA=2:54:32, max mem: 7.6 GB 
[10/28 13:28:53 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 2.9990, average train loss: 0.6952
[10/28 13:29:40 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.4696, average loss: 0.7386
[10/28 13:29:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.25	
[10/28 13:29:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[10/28 13:34:53 visual_prompt]: 	Training 100/139. train loss: 0.5963,	9.5440 s / batch. (data: 8.67e+00). ETA=1 day, 7:25:34, max mem: 7.6 GB 
[10/28 13:36:38 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0026, average train loss: 0.7068
[10/28 13:37:25 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4695, average loss: 0.6863
[10/28 13:37:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 60.72	
[10/28 13:37:25 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[10/28 13:42:33 visual_prompt]: 	Training 100/139. train loss: 0.7122,	0.8709 s / batch. (data: 3.08e-04). ETA=2:50:02, max mem: 7.6 GB 
[10/28 13:44:22 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9976, average train loss: 0.6959
[10/28 13:45:09 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.4673, average loss: 0.7820
[10/28 13:45:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.20	
[10/28 13:45:09 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[10/28 13:50:16 visual_prompt]: 	Training 100/139. train loss: 0.6469,	3.5840 s / batch. (data: 2.69e+00). ETA=11:31:28, max mem: 7.6 GB 
[10/28 13:52:06 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 2.9992, average train loss: 0.7106
[10/28 13:52:53 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.4681, average loss: 0.7329
[10/28 13:52:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.86	
[10/28 13:52:53 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[10/28 13:58:02 visual_prompt]: 	Training 100/139. train loss: 0.6674,	1.9560 s / batch. (data: 1.07e+00). ETA=6:12:51, max mem: 7.6 GB 
[10/28 13:59:51 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0049, average train loss: 0.6857
[10/28 14:00:39 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4675, average loss: 0.7298
[10/28 14:00:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.59	
[10/28 14:00:39 visual_prompt]: Stopping early.
[10/28 14:00:39 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 14:00:39 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 14:00:39 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 14:00:39 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 14:00:39 visual_prompt]: Training with config:
[10/28 14:00:39 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr0.05_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 14:00:39 visual_prompt]: Loading training data...
[10/28 14:00:39 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 14:00:39 visual_prompt]: Loading validation data...
[10/28 14:00:39 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 14:00:39 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/28 14:00:41 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/28 14:00:41 visual_prompt]: tuned percent:0.534
[10/28 14:00:41 visual_prompt]: Device used for model: 0
[10/28 14:00:41 visual_prompt]: Setting up Evaluator...
[10/28 14:00:41 visual_prompt]: Setting up Trainer...
[10/28 14:00:41 visual_prompt]: 	Setting up the optimizer...
[10/28 14:00:41 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 14:05:48 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.8755 s / batch. (data: 5.37e-03). ETA=3:21:21, max mem: 7.6 GB 
[10/28 14:07:38 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9943, average train loss: 1.3980
[10/28 14:08:25 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.4711, average loss: 1.3816
[10/28 14:08:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/28 14:08:25 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[10/28 14:13:30 visual_prompt]: 	Training 100/139. train loss: 0.7311,	0.8840 s / batch. (data: 3.11e-04). ETA=3:21:16, max mem: 7.6 GB 
[10/28 14:15:22 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 2.9988, average train loss: 0.7382
[10/28 14:16:09 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4641, average loss: 0.6840
[10/28 14:16:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 56.13	
[10/28 14:16:09 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[10/28 14:21:14 visual_prompt]: 	Training 100/139. train loss: 0.6992,	0.8849 s / batch. (data: 2.91e-04). ETA=3:19:25, max mem: 7.6 GB 
[10/28 14:23:06 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9966, average train loss: 0.7210
[10/28 14:23:53 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.4672, average loss: 0.6864
[10/28 14:23:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 57.94	
[10/28 14:23:53 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[10/28 14:28:55 visual_prompt]: 	Training 100/139. train loss: 0.6967,	0.8840 s / batch. (data: 3.24e-04). ETA=3:17:10, max mem: 7.6 GB 
[10/28 14:30:50 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 2.9969, average train loss: 0.7233
[10/28 14:31:37 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4677, average loss: 0.7475
[10/28 14:31:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.23	
[10/28 14:31:37 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[10/28 14:36:47 visual_prompt]: 	Training 100/139. train loss: 0.6616,	11.6471 s / batch. (data: 1.08e+01). ETA=1 day, 18:50:54, max mem: 7.6 GB 
[10/28 14:38:35 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0025, average train loss: 0.7318
[10/28 14:39:22 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4690, average loss: 0.6807
[10/28 14:39:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.73	
[10/28 14:39:22 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[10/28 14:44:29 visual_prompt]: 	Training 100/139. train loss: 0.7485,	0.8823 s / batch. (data: 3.52e-04). ETA=3:12:41, max mem: 7.6 GB 
[10/28 14:46:19 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 3.0015, average train loss: 0.7240
[10/28 14:47:07 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4678, average loss: 0.6861
[10/28 14:47:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.13	
[10/28 14:47:07 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[10/28 14:52:14 visual_prompt]: 	Training 100/139. train loss: 0.6279,	0.8746 s / batch. (data: 2.97e-04). ETA=3:09:00, max mem: 7.6 GB 
[10/28 14:54:03 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9935, average train loss: 0.7157
[10/28 14:54:50 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.4662, average loss: 1.0838
[10/28 14:54:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.63	
[10/28 14:54:50 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[10/28 14:59:58 visual_prompt]: 	Training 100/139. train loss: 0.6417,	0.8949 s / batch. (data: 2.84e-04). ETA=3:11:18, max mem: 7.6 GB 
[10/28 15:01:48 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0025, average train loss: 0.7210
[10/28 15:02:35 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4665, average loss: 0.8203
[10/28 15:02:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.57	
[10/28 15:02:35 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[10/28 15:07:42 visual_prompt]: 	Training 100/139. train loss: 0.7757,	0.8800 s / batch. (data: 2.78e-04). ETA=3:06:05, max mem: 7.6 GB 
[10/28 15:09:31 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9957, average train loss: 0.7377
[10/28 15:10:19 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4696, average loss: 0.6784
[10/28 15:10:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 57.98	
[10/28 15:10:19 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[10/28 15:15:26 visual_prompt]: 	Training 100/139. train loss: 0.7010,	0.8948 s / batch. (data: 1.47e-02). ETA=3:07:08, max mem: 7.6 GB 
[10/28 15:17:15 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9950, average train loss: 0.7012
[10/28 15:18:02 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.4671, average loss: 0.6808
[10/28 15:18:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 58.09	
[10/28 15:18:02 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[10/28 15:23:09 visual_prompt]: 	Training 100/139. train loss: 0.7015,	0.8631 s / batch. (data: 2.86e-04). ETA=2:58:30, max mem: 7.6 GB 
[10/28 15:24:59 visual_prompt]: Epoch 11 / 100: avg data time: 2.12e+00, avg batch time: 2.9970, average train loss: 0.7195
[10/28 15:25:47 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4701, average loss: 0.6722
[10/28 15:25:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 59.82	
[10/28 15:25:47 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[10/28 15:30:52 visual_prompt]: 	Training 100/139. train loss: 0.6588,	1.1888 s / batch. (data: 3.18e-01). ETA=4:03:08, max mem: 7.6 GB 
[10/28 15:32:43 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9968, average train loss: 0.7291
[10/28 15:33:30 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4661, average loss: 0.7055
[10/28 15:33:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 59.81	
[10/28 15:33:30 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[10/28 15:38:39 visual_prompt]: 	Training 100/139. train loss: 0.7590,	0.8880 s / batch. (data: 2.94e-04). ETA=2:59:33, max mem: 7.6 GB 
[10/28 15:40:28 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0017, average train loss: 0.7244
[10/28 15:41:15 visual_prompt]: Inference (val):avg data time: 7.30e-04, avg batch time: 0.4672, average loss: 0.7450
[10/28 15:41:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.95	
[10/28 15:41:15 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[10/28 15:46:25 visual_prompt]: 	Training 100/139. train loss: 0.8319,	0.9040 s / batch. (data: 2.91e-04). ETA=3:00:41, max mem: 7.6 GB 
[10/28 15:48:12 visual_prompt]: Epoch 14 / 100: avg data time: 2.12e+00, avg batch time: 2.9988, average train loss: 0.6918
[10/28 15:49:00 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4647, average loss: 0.7207
[10/28 15:49:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 59.38	
[10/28 15:49:00 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[10/28 15:54:11 visual_prompt]: 	Training 100/139. train loss: 0.5844,	9.5560 s / batch. (data: 8.68e+00). ETA=1 day, 7:27:56, max mem: 7.6 GB 
[10/28 15:55:57 visual_prompt]: Epoch 15 / 100: avg data time: 2.12e+00, avg batch time: 2.9987, average train loss: 0.7070
[10/28 15:56:44 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4676, average loss: 0.6935
[10/28 15:56:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 60.17	
[10/28 15:56:44 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[10/28 16:01:51 visual_prompt]: 	Training 100/139. train loss: 0.7163,	0.8721 s / batch. (data: 3.20e-04). ETA=2:50:16, max mem: 7.6 GB 
[10/28 16:03:40 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9934, average train loss: 0.6957
[10/28 16:04:27 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4691, average loss: 0.7643
[10/28 16:04:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.55	
[10/28 16:04:27 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[10/28 16:09:35 visual_prompt]: 	Training 100/139. train loss: 0.6289,	3.8631 s / batch. (data: 3.00e+00). ETA=12:25:18, max mem: 7.6 GB 
[10/28 16:11:24 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 2.9987, average train loss: 0.7089
[10/28 16:12:12 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.4636, average loss: 0.7188
[10/28 16:12:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.59	
[10/28 16:12:12 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[10/28 16:17:19 visual_prompt]: 	Training 100/139. train loss: 0.6638,	1.7960 s / batch. (data: 9.05e-01). ETA=5:42:20, max mem: 7.6 GB 
[10/28 16:19:09 visual_prompt]: Epoch 18 / 100: avg data time: 2.12e+00, avg batch time: 2.9985, average train loss: 0.6839
[10/28 16:19:56 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4687, average loss: 0.7139
[10/28 16:19:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 61.57	
[10/28 16:19:56 visual_prompt]: Stopping early.
[10/28 16:19:56 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 16:19:56 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 16:19:56 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 16:19:56 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 16:19:56 visual_prompt]: Training with config:
[10/28 16:19:56 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/val/seed0/lr0.05_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 16:19:56 visual_prompt]: Loading training data...
[10/28 16:19:56 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 16:19:56 visual_prompt]: Loading validation data...
[10/28 16:19:56 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 16:19:56 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/28 16:19:59 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/28 16:19:59 visual_prompt]: tuned percent:0.534
[10/28 16:19:59 visual_prompt]: Device used for model: 0
[10/28 16:19:59 visual_prompt]: Setting up Evaluator...
[10/28 16:19:59 visual_prompt]: Setting up Trainer...
[10/28 16:19:59 visual_prompt]: 	Setting up the optimizer...
[10/28 16:19:59 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 16:25:06 visual_prompt]: 	Training 100/139. train loss: 1.2978,	0.9000 s / batch. (data: 7.87e-04). ETA=3:26:59, max mem: 7.6 GB 
[10/28 16:26:55 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9928, average train loss: 1.3980
[10/28 16:27:42 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4680, average loss: 1.3816
[10/28 16:27:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.22	
[10/28 16:27:42 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[10/28 16:32:47 visual_prompt]: 	Training 100/139. train loss: 0.7311,	0.8879 s / batch. (data: 7.95e-03). ETA=3:22:10, max mem: 7.6 GB 
[10/28 16:34:39 visual_prompt]: Epoch 2 / 100: avg data time: 2.12e+00, avg batch time: 2.9992, average train loss: 0.7382
[10/28 16:35:27 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.4691, average loss: 0.6840
[10/28 16:35:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 56.13	
[10/28 16:35:27 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[10/28 16:40:33 visual_prompt]: 	Training 100/139. train loss: 0.6992,	0.8759 s / batch. (data: 2.87e-04). ETA=3:17:24, max mem: 7.6 GB 
[10/28 16:42:23 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9970, average train loss: 0.7210
[10/28 16:43:11 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4700, average loss: 0.6864
[10/28 16:43:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 57.93	
[10/28 16:43:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[10/28 16:48:13 visual_prompt]: 	Training 100/139. train loss: 0.6967,	0.8693 s / batch. (data: 2.91e-04). ETA=3:13:53, max mem: 7.6 GB 
[10/28 16:50:08 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 2.9994, average train loss: 0.7234
[10/28 16:50:55 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4661, average loss: 0.7479
[10/28 16:50:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.24	
[10/28 16:50:55 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[10/28 16:56:05 visual_prompt]: 	Training 100/139. train loss: 0.6630,	11.9121 s / batch. (data: 1.10e+01). ETA=1 day, 19:49:23, max mem: 7.6 GB 
[10/28 16:57:53 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0023, average train loss: 0.7338
[10/28 16:58:40 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.4682, average loss: 0.6856
[10/28 16:58:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.52	
[10/28 16:58:40 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[10/28 17:03:47 visual_prompt]: 	Training 100/139. train loss: 0.7420,	0.8736 s / batch. (data: 2.56e-04). ETA=3:10:49, max mem: 7.6 GB 
[10/28 17:05:37 visual_prompt]: Epoch 6 / 100: avg data time: 2.12e+00, avg batch time: 2.9981, average train loss: 0.7244
[10/28 17:06:24 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4661, average loss: 0.6831
[10/28 17:06:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.88	
[10/28 17:06:24 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[10/28 17:11:32 visual_prompt]: 	Training 100/139. train loss: 0.6622,	0.8960 s / batch. (data: 2.90e-04). ETA=3:13:37, max mem: 7.6 GB 
[10/28 17:13:21 visual_prompt]: Epoch 7 / 100: avg data time: 2.12e+00, avg batch time: 2.9935, average train loss: 0.7135
[10/28 17:14:08 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4682, average loss: 0.8492
[10/28 17:14:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.53	
[10/28 17:14:08 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[10/28 17:19:16 visual_prompt]: 	Training 100/139. train loss: 0.6357,	0.8920 s / batch. (data: 2.69e-04). ETA=3:10:41, max mem: 7.6 GB 
[10/28 17:21:06 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 3.0041, average train loss: 0.7330
[10/28 17:21:53 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4661, average loss: 0.9556
[10/28 17:21:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.62	
[10/28 17:21:53 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[10/28 17:27:04 visual_prompt]: 	Training 100/139. train loss: 0.7958,	0.8800 s / batch. (data: 1.05e-02). ETA=3:06:05, max mem: 7.6 GB 
[10/28 17:28:50 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9988, average train loss: 0.7620
[10/28 17:29:38 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4671, average loss: 0.6791
[10/28 17:29:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 57.25	
[10/28 17:29:38 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[10/28 17:34:45 visual_prompt]: 	Training 100/139. train loss: 0.6985,	0.8701 s / batch. (data: 2.90e-04). ETA=3:01:59, max mem: 7.6 GB 
[10/28 17:36:34 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9964, average train loss: 0.7082
[10/28 17:37:22 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4704, average loss: 0.6813
[10/28 17:37:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 58.01	
[10/28 17:37:22 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[10/28 17:42:29 visual_prompt]: 	Training 100/139. train loss: 0.7231,	0.9040 s / batch. (data: 5.40e-03). ETA=3:06:58, max mem: 7.6 GB 
[10/28 17:44:19 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0009, average train loss: 0.7258
[10/28 17:45:06 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4663, average loss: 0.6751
[10/28 17:45:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 58.80	
[10/28 17:45:06 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[10/28 17:50:13 visual_prompt]: 	Training 100/139. train loss: 0.6853,	0.9023 s / batch. (data: 2.89e-04). ETA=3:04:32, max mem: 7.6 GB 
[10/28 17:52:03 visual_prompt]: Epoch 12 / 100: avg data time: 2.12e+00, avg batch time: 2.9955, average train loss: 0.7223
[10/28 17:52:50 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.4666, average loss: 0.7055
[10/28 17:52:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 59.42	
[10/28 17:52:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[10/28 17:58:00 visual_prompt]: 	Training 100/139. train loss: 0.7544,	0.8726 s / batch. (data: 2.99e-04). ETA=2:56:26, max mem: 7.6 GB 
[10/28 17:59:49 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 3.0072, average train loss: 0.7209
[10/28 18:00:36 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.4675, average loss: 0.7707
[10/28 18:00:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.88	
[10/28 18:00:36 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[10/28 18:05:46 visual_prompt]: 	Training 100/139. train loss: 0.8726,	0.9082 s / batch. (data: 5.42e-03). ETA=3:01:31, max mem: 7.6 GB 
[10/28 18:07:34 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0033, average train loss: 0.6947
[10/28 18:08:21 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.4668, average loss: 0.7051
[10/28 18:08:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 58.60	
[10/28 18:08:21 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[10/28 18:13:35 visual_prompt]: 	Training 100/139. train loss: 0.6354,	9.4040 s / batch. (data: 8.55e+00). ETA=1 day, 6:57:55, max mem: 7.6 GB 
[10/28 18:15:19 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0049, average train loss: 0.7173
[10/28 18:16:06 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4671, average loss: 0.7229
[10/28 18:16:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 58.67	
[10/28 18:16:06 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[10/28 18:21:14 visual_prompt]: 	Training 100/139. train loss: 0.7106,	0.9295 s / batch. (data: 1.04e-02). ETA=3:01:29, max mem: 7.6 GB 
[10/28 18:23:03 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9961, average train loss: 0.7053
[10/28 18:23:50 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.4691, average loss: 0.7442
[10/28 18:23:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.07	
[10/28 18:23:50 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[10/28 18:28:58 visual_prompt]: 	Training 100/139. train loss: 0.6369,	3.5418 s / batch. (data: 2.67e+00). ETA=11:23:19, max mem: 7.6 GB 
[10/28 18:30:48 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 3.0024, average train loss: 0.7141
[10/28 18:31:35 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4674, average loss: 0.7181
[10/28 18:31:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.77	
[10/28 18:31:35 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[10/28 18:36:43 visual_prompt]: 	Training 100/139. train loss: 0.6736,	2.0164 s / batch. (data: 1.15e+00). ETA=6:24:21, max mem: 7.6 GB 
[10/28 18:38:33 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0030, average train loss: 0.6914
[10/28 18:39:20 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4679, average loss: 0.7088
[10/28 18:39:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 59.43	
[10/28 18:39:20 visual_prompt]: Stopping early.
[10/28 18:39:21 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 18:39:21 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 18:39:21 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 18:39:21 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 18:39:21 visual_prompt]: Training with config:
[10/28 18:39:21 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/test/seed9805/lr0.1_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 9805, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 18:39:21 visual_prompt]: Loading training data...
[10/28 18:39:21 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 18:39:21 visual_prompt]: Loading validation data...
[10/28 18:39:21 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 18:39:21 visual_prompt]: Loading test data...
[10/28 18:39:21 visual_prompt]: Constructing mammo-cbis dataset test...
[10/28 18:39:21 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/28 18:39:23 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/28 18:39:23 visual_prompt]: tuned percent:0.534
[10/28 18:39:23 visual_prompt]: Device used for model: 0
[10/28 18:39:23 visual_prompt]: Setting up Evaluator...
[10/28 18:39:23 visual_prompt]: Setting up Trainer...
[10/28 18:39:23 visual_prompt]: 	Setting up the optimizer...
[10/28 18:39:23 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 18:44:29 visual_prompt]: 	Training 100/139. train loss: 1.0666,	0.8589 s / batch. (data: 3.11e-04). ETA=3:17:32, max mem: 7.6 GB 
[10/28 18:46:20 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9952, average train loss: 0.9036
[10/28 18:47:07 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4658, average loss: 0.8644
[10/28 18:47:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.98	
[10/28 18:49:12 visual_prompt]: Inference (test):avg data time: 3.61e-05, avg batch time: 0.4796, average loss: 0.8799
[10/28 18:49:12 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.40	rocauc: 46.94	
[10/28 18:49:12 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[10/28 18:54:19 visual_prompt]: 	Training 100/139. train loss: 0.7024,	2.4120 s / batch. (data: 1.54e+00). ETA=9:09:10, max mem: 7.6 GB 
[10/28 18:56:09 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 2.9994, average train loss: 0.9101
[10/28 18:56:57 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4652, average loss: 0.7355
[10/28 18:56:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.86	
[10/28 18:58:57 visual_prompt]: Inference (test):avg data time: 3.96e-05, avg batch time: 0.4796, average loss: 0.6974
[10/28 18:58:57 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 53.08	
[10/28 18:58:57 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[10/28 19:04:06 visual_prompt]: 	Training 100/139. train loss: 0.6691,	0.8689 s / batch. (data: 2.84e-04). ETA=3:15:49, max mem: 7.6 GB 
[10/28 19:05:54 visual_prompt]: Epoch 3 / 100: avg data time: 2.12e+00, avg batch time: 2.9985, average train loss: 0.7440
[10/28 19:06:41 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4661, average loss: 0.6939
[10/28 19:06:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.00	
[10/28 19:08:42 visual_prompt]: Inference (test):avg data time: 2.13e-04, avg batch time: 0.4784, average loss: 0.6759
[10/28 19:08:42 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 51.94	
[10/28 19:08:42 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[10/28 19:13:48 visual_prompt]: 	Training 100/139. train loss: 0.8794,	0.8780 s / batch. (data: 1.55e-02). ETA=3:15:50, max mem: 7.6 GB 
[10/28 19:15:39 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 3.0038, average train loss: 0.7237
[10/28 19:16:27 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4675, average loss: 0.7061
[10/28 19:16:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.54	
[10/28 19:18:27 visual_prompt]: Inference (test):avg data time: 3.79e-05, avg batch time: 0.4794, average loss: 0.6798
[10/28 19:18:27 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 52.03	
[10/28 19:18:27 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[10/28 19:23:39 visual_prompt]: 	Training 100/139. train loss: 0.6532,	0.8880 s / batch. (data: 2.46e-04). ETA=3:16:00, max mem: 7.6 GB 
[10/28 19:25:24 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0014, average train loss: 0.7443
[10/28 19:26:12 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4659, average loss: 0.8386
[10/28 19:26:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.04	
[10/28 19:28:12 visual_prompt]: Inference (test):avg data time: 3.86e-05, avg batch time: 0.4786, average loss: 0.8726
[10/28 19:28:12 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 54.49	
[10/28 19:28:12 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[10/28 19:33:19 visual_prompt]: 	Training 100/139. train loss: 1.0454,	0.8800 s / batch. (data: 3.07e-04). ETA=3:12:12, max mem: 7.6 GB 
[10/28 19:35:08 visual_prompt]: Epoch 6 / 100: avg data time: 2.12e+00, avg batch time: 2.9957, average train loss: 0.7313
[10/28 19:35:56 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4695, average loss: 0.7371
[10/28 19:35:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.45	
[10/28 19:37:56 visual_prompt]: Inference (test):avg data time: 3.72e-05, avg batch time: 0.4808, average loss: 0.6984
[10/28 19:37:56 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 56.37	
[10/28 19:37:56 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[10/28 19:43:04 visual_prompt]: 	Training 100/139. train loss: 0.7630,	6.4640 s / batch. (data: 5.58e+00). ETA=23:16:52, max mem: 7.6 GB 
[10/28 19:44:54 visual_prompt]: Epoch 7 / 100: avg data time: 2.13e+00, avg batch time: 3.0037, average train loss: 0.7264
[10/28 19:45:41 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4645, average loss: 0.8153
[10/28 19:45:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.88	
[10/28 19:47:41 visual_prompt]: Inference (test):avg data time: 3.92e-05, avg batch time: 0.4781, average loss: 0.7600
[10/28 19:47:41 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 58.30	
[10/28 19:47:41 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[10/28 19:52:53 visual_prompt]: 	Training 100/139. train loss: 0.7519,	0.8820 s / batch. (data: 3.01e-04). ETA=3:08:33, max mem: 7.6 GB 
[10/28 19:54:38 visual_prompt]: Epoch 8 / 100: avg data time: 2.12e+00, avg batch time: 2.9966, average train loss: 0.7283
[10/28 19:55:25 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4635, average loss: 0.8027
[10/28 19:55:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.57	
[10/28 19:57:25 visual_prompt]: Inference (test):avg data time: 3.99e-05, avg batch time: 0.4799, average loss: 0.8300
[10/28 19:57:25 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 58.30	
[10/28 19:57:25 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[10/28 20:02:34 visual_prompt]: 	Training 100/139. train loss: 0.6844,	0.8697 s / batch. (data: 5.46e-03). ETA=3:03:54, max mem: 7.6 GB 
[10/28 20:04:22 visual_prompt]: Epoch 9 / 100: avg data time: 2.12e+00, avg batch time: 2.9949, average train loss: 0.7279
[10/28 20:05:09 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4687, average loss: 0.7355
[10/28 20:05:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.20	
[10/28 20:07:09 visual_prompt]: Inference (test):avg data time: 3.77e-05, avg batch time: 0.4787, average loss: 0.6953
[10/28 20:07:09 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.61	
[10/28 20:07:09 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[10/28 20:12:19 visual_prompt]: 	Training 100/139. train loss: 0.6937,	0.8840 s / batch. (data: 7.80e-04). ETA=3:04:53, max mem: 7.6 GB 
[10/28 20:14:06 visual_prompt]: Epoch 10 / 100: avg data time: 2.13e+00, avg batch time: 2.9991, average train loss: 0.7401
[10/28 20:14:54 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4681, average loss: 1.0762
[10/28 20:14:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.78	
[10/28 20:16:54 visual_prompt]: Inference (test):avg data time: 3.69e-05, avg batch time: 0.4801, average loss: 1.1428
[10/28 20:16:54 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 60.52	
[10/28 20:16:54 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[10/28 20:22:05 visual_prompt]: 	Training 100/139. train loss: 0.7073,	0.9040 s / batch. (data: 2.89e-04). ETA=3:06:58, max mem: 7.6 GB 
[10/28 20:23:51 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0030, average train loss: 0.7650
[10/28 20:24:38 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.4658, average loss: 0.9051
[10/28 20:24:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.68	
[10/28 20:26:39 visual_prompt]: Inference (test):avg data time: 3.63e-05, avg batch time: 0.4830, average loss: 0.8319
[10/28 20:26:39 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 60.82	
[10/28 20:26:39 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[10/28 20:31:49 visual_prompt]: 	Training 100/139. train loss: 0.9161,	0.8657 s / batch. (data: 2.61e-04). ETA=2:57:03, max mem: 7.6 GB 
[10/28 20:33:35 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 2.9985, average train loss: 0.7630
[10/28 20:34:23 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4646, average loss: 1.0681
[10/28 20:34:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.69	
[10/28 20:36:23 visual_prompt]: Inference (test):avg data time: 3.77e-05, avg batch time: 0.4810, average loss: 0.9763
[10/28 20:36:23 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 61.12	
[10/28 20:36:23 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[10/28 20:41:32 visual_prompt]: 	Training 100/139. train loss: 0.6711,	0.8760 s / batch. (data: 3.03e-04). ETA=2:57:07, max mem: 7.6 GB 
[10/28 20:43:20 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 2.9998, average train loss: 0.7724
[10/28 20:44:07 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4679, average loss: 0.6755
[10/28 20:44:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 60.32	
[10/28 20:46:08 visual_prompt]: Inference (test):avg data time: 3.63e-05, avg batch time: 0.4764, average loss: 0.6771
[10/28 20:46:08 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.69	rocauc: 60.97	
[10/28 20:46:08 visual_prompt]: Best epoch 13: best metric: -0.675
[10/28 20:46:08 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[10/28 20:51:17 visual_prompt]: 	Training 100/139. train loss: 0.7181,	0.8807 s / batch. (data: 2.80e-04). ETA=2:56:02, max mem: 7.6 GB 
[10/28 20:53:05 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0044, average train loss: 0.7210
[10/28 20:53:53 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4708, average loss: 0.6875
[10/28 20:53:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 60.20	
[10/28 20:55:53 visual_prompt]: Inference (test):avg data time: 3.07e-04, avg batch time: 0.4803, average loss: 0.6926
[10/28 20:55:53 visual_prompt]: Classification results with test_mammo-cbis: top1: 54.26	rocauc: 62.27	
[10/28 20:55:53 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[10/28 21:00:58 visual_prompt]: 	Training 100/139. train loss: 0.7341,	0.8757 s / batch. (data: 8.67e-03). ETA=2:53:00, max mem: 7.6 GB 
[10/28 21:02:49 visual_prompt]: Epoch 15 / 100: avg data time: 2.12e+00, avg batch time: 2.9948, average train loss: 0.7278
[10/28 21:03:37 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4655, average loss: 0.6716
[10/28 21:03:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 60.36	
[10/28 21:05:37 visual_prompt]: Inference (test):avg data time: 3.77e-05, avg batch time: 0.4801, average loss: 0.6581
[10/28 21:05:37 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.16	rocauc: 62.06	
[10/28 21:05:37 visual_prompt]: Best epoch 15: best metric: -0.672
[10/28 21:05:37 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[10/28 21:10:45 visual_prompt]: 	Training 100/139. train loss: 0.7951,	0.8727 s / batch. (data: 7.29e-04). ETA=2:50:24, max mem: 7.6 GB 
[10/28 21:12:34 visual_prompt]: Epoch 16 / 100: avg data time: 2.13e+00, avg batch time: 3.0015, average train loss: 0.6996
[10/28 21:13:22 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4663, average loss: 0.6837
[10/28 21:13:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 59.26	
[10/28 21:15:21 visual_prompt]: Inference (test):avg data time: 3.84e-05, avg batch time: 0.4782, average loss: 0.6857
[10/28 21:15:21 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.98	rocauc: 60.35	
[10/28 21:15:21 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[10/28 21:20:24 visual_prompt]: 	Training 100/139. train loss: 0.8681,	0.9040 s / batch. (data: 5.41e-03). ETA=2:54:24, max mem: 7.6 GB 
[10/28 21:22:18 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 2.9985, average train loss: 0.7712
[10/28 21:23:05 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.4699, average loss: 0.6765
[10/28 21:23:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 59.96	
[10/28 21:25:05 visual_prompt]: Inference (test):avg data time: 3.87e-05, avg batch time: 0.4836, average loss: 0.6714
[10/28 21:25:05 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.69	rocauc: 62.59	
[10/28 21:25:05 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[10/28 21:30:13 visual_prompt]: 	Training 100/139. train loss: 0.6358,	0.9104 s / batch. (data: 1.55e-02). ETA=2:53:31, max mem: 7.6 GB 
[10/28 21:32:02 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 2.9987, average train loss: 0.7007
[10/28 21:32:50 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.4660, average loss: 0.8117
[10/28 21:32:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.20	
[10/28 21:34:50 visual_prompt]: Inference (test):avg data time: 3.87e-05, avg batch time: 0.4809, average loss: 0.7460
[10/28 21:34:50 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 63.46	
[10/28 21:34:50 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[10/28 21:39:57 visual_prompt]: 	Training 100/139. train loss: 0.7073,	0.9119 s / batch. (data: 2.01e-02). ETA=2:51:43, max mem: 7.6 GB 
[10/28 21:41:47 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0022, average train loss: 0.6988
[10/28 21:42:35 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4660, average loss: 0.6829
[10/28 21:42:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.58	
[10/28 21:44:35 visual_prompt]: Inference (test):avg data time: 3.72e-05, avg batch time: 0.4798, average loss: 0.6816
[10/28 21:44:35 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.84	rocauc: 63.24	
[10/28 21:44:35 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[10/28 21:49:39 visual_prompt]: 	Training 100/139. train loss: 0.5856,	0.8831 s / batch. (data: 3.02e-04). ETA=2:44:14, max mem: 7.6 GB 
[10/28 21:51:32 visual_prompt]: Epoch 20 / 100: avg data time: 2.12e+00, avg batch time: 2.9961, average train loss: 0.7109
[10/28 21:52:19 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.4717, average loss: 0.6758
[10/28 21:52:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 62.06	
[10/28 21:54:19 visual_prompt]: Inference (test):avg data time: 3.66e-05, avg batch time: 0.4786, average loss: 0.6819
[10/28 21:54:19 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.60	rocauc: 62.73	
[10/28 21:54:19 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[10/28 21:59:28 visual_prompt]: 	Training 100/139. train loss: 0.7484,	9.5474 s / batch. (data: 8.64e+00). ETA=1 day, 5:13:32, max mem: 7.6 GB 
[10/28 22:01:16 visual_prompt]: Epoch 21 / 100: avg data time: 2.13e+00, avg batch time: 2.9978, average train loss: 0.6866
[10/28 22:02:04 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4662, average loss: 0.6886
[10/28 22:02:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 62.61	
[10/28 22:04:04 visual_prompt]: Inference (test):avg data time: 3.85e-05, avg batch time: 0.4794, average loss: 0.7011
[10/28 22:04:04 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.21	rocauc: 62.88	
[10/28 22:04:04 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[10/28 22:09:11 visual_prompt]: 	Training 100/139. train loss: 0.8662,	0.8885 s / batch. (data: 7.03e-03). ETA=2:41:07, max mem: 7.6 GB 
[10/28 22:11:01 visual_prompt]: Epoch 22 / 100: avg data time: 2.13e+00, avg batch time: 3.0006, average train loss: 0.6961
[10/28 22:11:48 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4674, average loss: 0.9240
[10/28 22:11:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.75	
[10/28 22:13:49 visual_prompt]: Inference (test):avg data time: 3.61e-05, avg batch time: 0.4804, average loss: 0.8448
[10/28 22:13:49 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 63.07	
[10/28 22:13:49 visual_prompt]: Stopping early.
[10/28 22:13:49 visual_prompt]: Rank of current process: 0. World size: 1
[10/28 22:13:49 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/28 22:13:49 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/28 22:13:49 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/28 22:13:49 visual_prompt]: Training with config:
[10/28 22:13:49 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/test/seed875/lr0.1_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 875, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/28 22:13:49 visual_prompt]: Loading training data...
[10/28 22:13:49 visual_prompt]: Constructing mammo-cbis dataset train...
[10/28 22:13:49 visual_prompt]: Loading validation data...
[10/28 22:13:49 visual_prompt]: Constructing mammo-cbis dataset val...
[10/28 22:13:49 visual_prompt]: Loading test data...
[10/28 22:13:49 visual_prompt]: Constructing mammo-cbis dataset test...
[10/28 22:13:49 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/28 22:13:51 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/28 22:13:51 visual_prompt]: tuned percent:0.534
[10/28 22:13:51 visual_prompt]: Device used for model: 0
[10/28 22:13:51 visual_prompt]: Setting up Evaluator...
[10/28 22:13:51 visual_prompt]: Setting up Trainer...
[10/28 22:13:51 visual_prompt]: 	Setting up the optimizer...
[10/28 22:13:51 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/28 22:19:00 visual_prompt]: 	Training 100/139. train loss: 0.7618,	7.8348 s / batch. (data: 6.94e+00). ETA=1 day, 6:01:59, max mem: 7.6 GB 
[10/28 22:20:48 visual_prompt]: Epoch 1 / 100: avg data time: 2.13e+00, avg batch time: 2.9983, average train loss: 0.7834
[10/28 22:21:36 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4687, average loss: 0.7031
[10/28 22:21:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 55.68	
[10/28 22:23:36 visual_prompt]: Inference (test):avg data time: 3.70e-05, avg batch time: 0.4826, average loss: 0.6968
[10/28 22:23:36 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.98	rocauc: 52.92	
[10/28 22:23:36 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[10/28 22:28:44 visual_prompt]: 	Training 100/139. train loss: 0.6888,	3.2240 s / batch. (data: 2.35e+00). ETA=12:14:03, max mem: 7.6 GB 
[10/28 22:30:34 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 3.0033, average train loss: 0.7407
[10/28 22:31:21 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.4641, average loss: 0.6999
[10/28 22:31:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.15	
[10/28 22:33:21 visual_prompt]: Inference (test):avg data time: 3.95e-05, avg batch time: 0.4833, average loss: 0.6790
[10/28 22:33:21 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 55.12	
[10/28 22:33:21 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[10/28 22:38:30 visual_prompt]: 	Training 100/139. train loss: 0.6694,	0.8941 s / batch. (data: 1.05e-02). ETA=3:21:30, max mem: 7.6 GB 
[10/28 22:40:19 visual_prompt]: Epoch 3 / 100: avg data time: 2.13e+00, avg batch time: 3.0004, average train loss: 0.7192
[10/28 22:41:06 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4747, average loss: 0.7096
[10/28 22:41:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 59.33	
[10/28 22:43:06 visual_prompt]: Inference (test):avg data time: 3.81e-05, avg batch time: 0.4821, average loss: 0.7247
[10/28 22:43:06 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.24	rocauc: 57.23	
[10/28 22:43:06 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[10/28 22:48:10 visual_prompt]: 	Training 100/139. train loss: 0.7172,	0.8720 s / batch. (data: 1.19e-02). ETA=3:14:30, max mem: 7.6 GB 
[10/28 22:50:02 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 2.9938, average train loss: 0.7369
[10/28 22:50:50 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.4690, average loss: 0.7000
[10/28 22:50:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.80	
[10/28 22:52:50 visual_prompt]: Inference (test):avg data time: 4.00e-05, avg batch time: 0.4855, average loss: 0.6763
[10/28 22:52:50 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.24	
[10/28 22:52:50 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[10/28 22:58:01 visual_prompt]: 	Training 100/139. train loss: 0.7586,	0.8583 s / batch. (data: 2.95e-04). ETA=3:09:27, max mem: 7.6 GB 
[10/28 22:59:47 visual_prompt]: Epoch 5 / 100: avg data time: 2.12e+00, avg batch time: 3.0004, average train loss: 0.7503
[10/28 23:00:35 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.4708, average loss: 0.6799
[10/28 23:00:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.50	
[10/28 23:02:35 visual_prompt]: Inference (test):avg data time: 3.83e-05, avg batch time: 0.4842, average loss: 0.6691
[10/28 23:02:35 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.69	rocauc: 57.48	
[10/28 23:02:35 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[10/28 23:07:43 visual_prompt]: 	Training 100/139. train loss: 0.6773,	0.9036 s / batch. (data: 1.26e-02). ETA=3:17:21, max mem: 7.6 GB 
[10/28 23:09:32 visual_prompt]: Epoch 6 / 100: avg data time: 2.12e+00, avg batch time: 2.9992, average train loss: 0.7221
[10/28 23:10:19 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4696, average loss: 0.7947
[10/28 23:10:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.11	
[10/28 23:12:19 visual_prompt]: Inference (test):avg data time: 2.79e-04, avg batch time: 0.4836, average loss: 0.8311
[10/28 23:12:19 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 58.15	
[10/28 23:12:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[10/28 23:17:25 visual_prompt]: 	Training 100/139. train loss: 0.6056,	3.9396 s / batch. (data: 3.09e+00). ETA=14:11:20, max mem: 7.6 GB 
[10/28 23:19:17 visual_prompt]: Epoch 7 / 100: avg data time: 2.13e+00, avg batch time: 3.0020, average train loss: 0.7571
[10/28 23:20:04 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4713, average loss: 0.7828
[10/28 23:20:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.67	
[10/28 23:22:05 visual_prompt]: Inference (test):avg data time: 3.56e-05, avg batch time: 0.4796, average loss: 0.7364
[10/28 23:22:05 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 58.68	
[10/28 23:22:05 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[10/28 23:27:12 visual_prompt]: 	Training 100/139. train loss: 0.7596,	0.9063 s / batch. (data: 1.03e-02). ETA=3:13:45, max mem: 7.6 GB 
[10/28 23:29:01 visual_prompt]: Epoch 8 / 100: avg data time: 2.12e+00, avg batch time: 2.9978, average train loss: 0.8139
[10/28 23:29:49 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.4737, average loss: 0.8147
[10/28 23:29:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.50	
[10/28 23:31:49 visual_prompt]: Inference (test):avg data time: 3.80e-05, avg batch time: 0.4810, average loss: 0.7615
[10/28 23:31:49 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.46	
[10/28 23:31:49 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[10/28 23:36:57 visual_prompt]: 	Training 100/139. train loss: 0.7351,	0.8895 s / batch. (data: 2.15e-02). ETA=3:08:06, max mem: 7.6 GB 
[10/28 23:38:46 visual_prompt]: Epoch 9 / 100: avg data time: 2.13e+00, avg batch time: 3.0015, average train loss: 0.7437
[10/28 23:39:34 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.4693, average loss: 0.7109
[10/28 23:39:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.13	
[10/28 23:41:34 visual_prompt]: Inference (test):avg data time: 3.67e-05, avg batch time: 0.4817, average loss: 0.6826
[10/28 23:41:34 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 56.44	
[10/28 23:41:34 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[10/28 23:46:51 visual_prompt]: 	Training 100/139. train loss: 0.7373,	9.2418 s / batch. (data: 8.36e+00). ETA=1 day, 8:12:54, max mem: 7.6 GB 
[10/28 23:48:32 visual_prompt]: Epoch 10 / 100: avg data time: 2.13e+00, avg batch time: 3.0076, average train loss: 0.7462
[10/28 23:49:20 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.4697, average loss: 0.7594
[10/28 23:49:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.68	
[10/28 23:51:20 visual_prompt]: Inference (test):avg data time: 3.81e-05, avg batch time: 0.4834, average loss: 0.7154
[10/28 23:51:20 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.42	
[10/28 23:51:20 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[10/28 23:56:25 visual_prompt]: 	Training 100/139. train loss: 0.6784,	0.8760 s / batch. (data: 2.98e-04). ETA=3:01:10, max mem: 7.6 GB 
[10/28 23:58:17 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0002, average train loss: 0.7289
[10/28 23:59:05 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.4667, average loss: 0.6942
[10/28 23:59:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.41	rocauc: 57.55	
[10/29 00:01:05 visual_prompt]: Inference (test):avg data time: 3.80e-05, avg batch time: 0.4840, average loss: 0.7016
[10/29 00:01:05 visual_prompt]: Classification results with test_mammo-cbis: top1: 51.32	rocauc: 58.56	
[10/29 00:01:05 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[10/29 00:06:16 visual_prompt]: 	Training 100/139. train loss: 0.8438,	0.8790 s / batch. (data: 7.56e-04). ETA=2:59:45, max mem: 7.6 GB 
[10/29 00:08:03 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 3.0021, average train loss: 0.7012
[10/29 00:08:50 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.4696, average loss: 0.7113
[10/29 00:08:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 60.08	
[10/29 00:10:50 visual_prompt]: Inference (test):avg data time: 3.88e-05, avg batch time: 0.4817, average loss: 0.7273
[10/29 00:10:50 visual_prompt]: Classification results with test_mammo-cbis: top1: 44.19	rocauc: 60.36	
[10/29 00:10:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[10/29 00:15:57 visual_prompt]: 	Training 100/139. train loss: 0.7669,	0.8999 s / batch. (data: 7.95e-03). ETA=3:01:58, max mem: 7.6 GB 
[10/29 00:17:49 visual_prompt]: Epoch 13 / 100: avg data time: 2.14e+00, avg batch time: 3.0119, average train loss: 0.7201
[10/29 00:18:37 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.4667, average loss: 0.6754
[10/29 00:18:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 58.38	
[10/29 00:20:37 visual_prompt]: Inference (test):avg data time: 3.73e-05, avg batch time: 0.4811, average loss: 0.6653
[10/29 00:20:37 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.16	rocauc: 60.09	
[10/29 00:20:37 visual_prompt]: Best epoch 13: best metric: -0.675
[10/29 00:20:37 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[10/29 00:25:49 visual_prompt]: 	Training 100/139. train loss: 0.8286,	0.8887 s / batch. (data: 1.35e-03). ETA=2:57:38, max mem: 7.6 GB 
[10/29 00:27:35 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0032, average train loss: 0.7188
[10/29 00:28:22 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4709, average loss: 0.6774
[10/29 00:28:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 58.91	
[10/29 00:30:23 visual_prompt]: Inference (test):avg data time: 3.79e-05, avg batch time: 0.4820, average loss: 0.6799
[10/29 00:30:23 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.98	rocauc: 58.50	
[10/29 00:30:23 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[10/29 00:35:33 visual_prompt]: 	Training 100/139. train loss: 0.7212,	0.8881 s / batch. (data: 2.47e-04). ETA=2:55:27, max mem: 7.6 GB 
[10/29 00:37:20 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0020, average train loss: 0.7234
[10/29 00:38:07 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.4669, average loss: 0.6700
[10/29 00:38:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 60.17	
[10/29 00:40:08 visual_prompt]: Inference (test):avg data time: 4.05e-05, avg batch time: 0.4789, average loss: 0.6636
[10/29 00:40:08 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.53	rocauc: 60.23	
[10/29 00:40:08 visual_prompt]: Best epoch 15: best metric: -0.670
[10/29 00:40:08 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[10/29 00:45:13 visual_prompt]: 	Training 100/139. train loss: 0.7104,	0.8784 s / batch. (data: 2.96e-04). ETA=2:51:30, max mem: 7.6 GB 
[10/29 00:47:04 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9939, average train loss: 0.7344
[10/29 00:47:51 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4686, average loss: 0.6895
[10/29 00:47:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 60.81	
[10/29 00:49:52 visual_prompt]: Inference (test):avg data time: 3.93e-05, avg batch time: 0.4792, average loss: 0.7024
[10/29 00:49:52 visual_prompt]: Classification results with test_mammo-cbis: top1: 53.80	rocauc: 59.76	
[10/29 00:49:52 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[10/29 00:54:57 visual_prompt]: 	Training 100/139. train loss: 0.7940,	2.0040 s / batch. (data: 1.13e+00). ETA=6:26:37, max mem: 7.6 GB 
[10/29 00:56:48 visual_prompt]: Epoch 17 / 100: avg data time: 2.12e+00, avg batch time: 2.9977, average train loss: 0.7387
[10/29 00:57:36 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.4673, average loss: 0.7147
[10/29 00:57:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 62.28	
[10/29 00:59:36 visual_prompt]: Inference (test):avg data time: 3.99e-05, avg batch time: 0.4813, average loss: 0.6830
[10/29 00:59:36 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.53	rocauc: 61.81	
[10/29 00:59:36 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[10/29 01:04:42 visual_prompt]: 	Training 100/139. train loss: 0.8008,	0.8611 s / batch. (data: 2.78e-04). ETA=2:44:08, max mem: 7.6 GB 
[10/29 01:06:33 visual_prompt]: Epoch 18 / 100: avg data time: 2.12e+00, avg batch time: 2.9957, average train loss: 0.7119
[10/29 01:07:20 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4667, average loss: 0.6871
[10/29 01:07:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.76	
[10/29 01:09:20 visual_prompt]: Inference (test):avg data time: 4.09e-05, avg batch time: 0.4816, average loss: 0.7029
[10/29 01:09:20 visual_prompt]: Classification results with test_mammo-cbis: top1: 54.57	rocauc: 61.23	
[10/29 01:09:20 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[10/29 01:14:29 visual_prompt]: 	Training 100/139. train loss: 0.6562,	0.8921 s / batch. (data: 2.40e-04). ETA=2:47:58, max mem: 7.6 GB 
[10/29 01:16:18 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0012, average train loss: 0.7027
[10/29 01:17:05 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4659, average loss: 0.6694
[10/29 01:17:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 62.44	
[10/29 01:19:06 visual_prompt]: Inference (test):avg data time: 3.89e-05, avg batch time: 0.4810, average loss: 0.6807
[10/29 01:19:06 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.29	rocauc: 61.23	
[10/29 01:19:06 visual_prompt]: Best epoch 19: best metric: -0.669
[10/29 01:19:06 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[10/29 01:24:12 visual_prompt]: 	Training 100/139. train loss: 0.5904,	0.8722 s / batch. (data: 3.08e-04). ETA=2:42:12, max mem: 7.6 GB 
[10/29 01:26:02 visual_prompt]: Epoch 20 / 100: avg data time: 2.12e+00, avg batch time: 2.9935, average train loss: 0.6836
[10/29 01:26:49 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4706, average loss: 0.7078
[10/29 01:26:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 62.88	
[10/29 01:28:49 visual_prompt]: Inference (test):avg data time: 3.80e-05, avg batch time: 0.4816, average loss: 0.7367
[10/29 01:28:49 visual_prompt]: Classification results with test_mammo-cbis: top1: 50.85	rocauc: 61.27	
[10/29 01:28:49 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[10/29 01:33:53 visual_prompt]: 	Training 100/139. train loss: 0.9572,	0.8857 s / batch. (data: 1.05e-02). ETA=2:42:40, max mem: 7.6 GB 
[10/29 01:35:46 visual_prompt]: Epoch 21 / 100: avg data time: 2.12e+00, avg batch time: 2.9983, average train loss: 0.6947
[10/29 01:36:34 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4696, average loss: 0.8400
[10/29 01:36:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 63.17	
[10/29 01:38:34 visual_prompt]: Inference (test):avg data time: 3.70e-05, avg batch time: 0.4805, average loss: 0.9032
[10/29 01:38:34 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.24	rocauc: 60.08	
[10/29 01:38:34 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[10/29 01:43:41 visual_prompt]: 	Training 100/139. train loss: 0.6062,	0.9116 s / batch. (data: 1.55e-02). ETA=2:45:18, max mem: 7.6 GB 
[10/29 01:45:31 visual_prompt]: Epoch 22 / 100: avg data time: 2.13e+00, avg batch time: 3.0019, average train loss: 0.6958
[10/29 01:46:19 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4671, average loss: 0.6610
[10/29 01:46:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 63.06	
[10/29 01:48:19 visual_prompt]: Inference (test):avg data time: 4.07e-05, avg batch time: 0.4830, average loss: 0.6520
[10/29 01:48:19 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.24	rocauc: 63.57	
[10/29 01:48:19 visual_prompt]: Best epoch 22: best metric: -0.661
[10/29 01:48:19 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[10/29 01:53:23 visual_prompt]: 	Training 100/139. train loss: 0.6601,	0.9040 s / batch. (data: 2.63e-02). ETA=2:41:51, max mem: 7.6 GB 
[10/29 01:55:16 visual_prompt]: Epoch 23 / 100: avg data time: 2.12e+00, avg batch time: 2.9951, average train loss: 0.6974
[10/29 01:56:03 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4707, average loss: 0.6803
[10/29 01:56:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 63.87	
[10/29 01:58:03 visual_prompt]: Inference (test):avg data time: 3.85e-05, avg batch time: 0.4820, average loss: 0.6992
[10/29 01:58:03 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.05	rocauc: 63.05	
[10/29 01:58:03 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[10/29 02:03:11 visual_prompt]: 	Training 100/139. train loss: 0.7181,	0.8731 s / batch. (data: 2.68e-04). ETA=2:34:17, max mem: 7.6 GB 
[10/29 02:05:00 visual_prompt]: Epoch 24 / 100: avg data time: 2.12e+00, avg batch time: 2.9945, average train loss: 0.7005
[10/29 02:05:47 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.4700, average loss: 0.6811
[10/29 02:05:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 63.76	
[10/29 02:07:47 visual_prompt]: Inference (test):avg data time: 3.86e-05, avg batch time: 0.4798, average loss: 0.7009
[10/29 02:07:47 visual_prompt]: Classification results with test_mammo-cbis: top1: 55.19	rocauc: 62.52	
[10/29 02:07:47 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[10/29 02:12:58 visual_prompt]: 	Training 100/139. train loss: 0.7783,	0.8596 s / batch. (data: 2.94e-04). ETA=2:29:55, max mem: 7.6 GB 
[10/29 02:14:46 visual_prompt]: Epoch 25 / 100: avg data time: 2.13e+00, avg batch time: 3.0076, average train loss: 0.6898
[10/29 02:15:33 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.4666, average loss: 0.6592
[10/29 02:15:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 64.12	
[10/29 02:17:33 visual_prompt]: Inference (test):avg data time: 3.78e-05, avg batch time: 0.4825, average loss: 0.6545
[10/29 02:17:33 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.62	rocauc: 62.23	
[10/29 02:17:33 visual_prompt]: Best epoch 25: best metric: -0.659
[10/29 02:17:33 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[10/29 02:22:35 visual_prompt]: 	Training 100/139. train loss: 0.7132,	0.9047 s / batch. (data: 2.06e-02). ETA=2:35:41, max mem: 7.6 GB 
[10/29 02:24:29 visual_prompt]: Epoch 26 / 100: avg data time: 2.12e+00, avg batch time: 2.9934, average train loss: 0.6993
[10/29 02:25:17 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.4677, average loss: 0.6670
[10/29 02:25:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.17	
[10/29 02:27:17 visual_prompt]: Inference (test):avg data time: 3.92e-05, avg batch time: 0.4827, average loss: 0.6806
[10/29 02:27:17 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.69	rocauc: 63.18	
[10/29 02:27:17 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[10/29 02:32:23 visual_prompt]: 	Training 100/139. train loss: 0.6528,	0.8760 s / batch. (data: 4.59e-04). ETA=2:28:42, max mem: 7.6 GB 
[10/29 02:34:14 visual_prompt]: Epoch 27 / 100: avg data time: 2.12e+00, avg batch time: 2.9945, average train loss: 0.6817
[10/29 02:35:01 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4695, average loss: 0.7611
[10/29 02:35:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 65.26	
[10/29 02:37:01 visual_prompt]: Inference (test):avg data time: 3.83e-05, avg batch time: 0.4827, average loss: 0.8177
[10/29 02:37:01 visual_prompt]: Classification results with test_mammo-cbis: top1: 42.95	rocauc: 62.63	
[10/29 02:37:01 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[10/29 02:42:07 visual_prompt]: 	Training 100/139. train loss: 0.7662,	0.8740 s / batch. (data: 2.80e-04). ETA=2:26:21, max mem: 7.6 GB 
[10/29 02:43:59 visual_prompt]: Epoch 28 / 100: avg data time: 2.13e+00, avg batch time: 3.0036, average train loss: 0.7000
[10/29 02:44:46 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4687, average loss: 0.7723
[10/29 02:44:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 64.81	
[10/29 02:46:47 visual_prompt]: Inference (test):avg data time: 3.83e-05, avg batch time: 0.4831, average loss: 0.7271
[10/29 02:46:47 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.78	rocauc: 63.12	
[10/29 02:46:47 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[10/29 02:51:58 visual_prompt]: 	Training 100/139. train loss: 0.7238,	0.8742 s / batch. (data: 7.96e-03). ETA=2:24:21, max mem: 7.6 GB 
[10/29 02:53:45 visual_prompt]: Epoch 29 / 100: avg data time: 2.14e+00, avg batch time: 3.0097, average train loss: 0.6864
[10/29 02:54:32 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4679, average loss: 0.6538
[10/29 02:54:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 63.70	
[10/29 02:56:33 visual_prompt]: Inference (test):avg data time: 3.75e-05, avg batch time: 0.4822, average loss: 0.6628
[10/29 02:56:33 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.38	rocauc: 62.28	
[10/29 02:56:33 visual_prompt]: Best epoch 29: best metric: -0.654
[10/29 02:56:33 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[10/29 02:01:36 visual_prompt]: 	Training 100/139. train loss: 0.6156,	0.8768 s / batch. (data: 3.11e-04). ETA=2:22:45, max mem: 7.6 GB 
[10/29 02:03:30 visual_prompt]: Epoch 30 / 100: avg data time: 2.13e+00, avg batch time: 3.0021, average train loss: 0.6680
[10/29 02:04:18 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.4681, average loss: 0.8016
[10/29 02:04:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 63.01	
[10/29 02:06:18 visual_prompt]: Inference (test):avg data time: 3.70e-05, avg batch time: 0.4835, average loss: 0.7478
[10/29 02:06:18 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.00	rocauc: 63.35	
[10/29 02:06:18 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[10/29 02:11:26 visual_prompt]: 	Training 100/139. train loss: 0.5584,	0.8880 s / batch. (data: 2.90e-04). ETA=2:22:31, max mem: 7.6 GB 
[10/29 02:13:15 visual_prompt]: Epoch 31 / 100: avg data time: 2.13e+00, avg batch time: 3.0003, average train loss: 0.6728
[10/29 02:14:02 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.4716, average loss: 0.7712
[10/29 02:14:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 64.21	
[10/29 02:16:03 visual_prompt]: Inference (test):avg data time: 3.85e-05, avg batch time: 0.4819, average loss: 0.8295
[10/29 02:16:03 visual_prompt]: Classification results with test_mammo-cbis: top1: 43.57	rocauc: 61.93	
[10/29 02:16:03 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[10/29 02:21:09 visual_prompt]: 	Training 100/139. train loss: 0.6347,	0.8782 s / batch. (data: 5.35e-03). ETA=2:18:54, max mem: 7.6 GB 
[10/29 02:23:00 visual_prompt]: Epoch 32 / 100: avg data time: 2.13e+00, avg batch time: 2.9989, average train loss: 0.6865
[10/29 02:23:47 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4670, average loss: 0.6599
[10/29 02:23:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 63.75	
[10/29 02:25:47 visual_prompt]: Inference (test):avg data time: 3.80e-05, avg batch time: 0.4798, average loss: 0.6722
[10/29 02:25:47 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.98	rocauc: 62.30	
[10/29 02:25:47 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[10/29 02:30:53 visual_prompt]: 	Training 100/139. train loss: 0.4282,	1.4431 s / batch. (data: 5.70e-01). ETA=3:44:55, max mem: 7.6 GB 
[10/29 02:32:44 visual_prompt]: Epoch 33 / 100: avg data time: 2.12e+00, avg batch time: 2.9943, average train loss: 0.6784
[10/29 02:33:31 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4665, average loss: 0.6644
[10/29 02:33:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 62.79	
[10/29 02:35:31 visual_prompt]: Inference (test):avg data time: 3.79e-05, avg batch time: 0.4781, average loss: 0.6429
[10/29 02:35:31 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.26	rocauc: 64.21	
[10/29 02:35:31 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[10/29 02:40:43 visual_prompt]: 	Training 100/139. train loss: 0.4752,	0.8960 s / batch. (data: 2.89e-04). ETA=2:17:34, max mem: 7.6 GB 
[10/29 02:42:29 visual_prompt]: Epoch 34 / 100: avg data time: 2.13e+00, avg batch time: 3.0062, average train loss: 0.6649
[10/29 02:43:17 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4736, average loss: 0.6566
[10/29 02:43:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 63.80	
[10/29 02:45:17 visual_prompt]: Inference (test):avg data time: 3.93e-05, avg batch time: 0.4806, average loss: 0.6561
[10/29 02:45:17 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.24	rocauc: 62.35	
[10/29 02:45:17 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[10/29 02:50:25 visual_prompt]: 	Training 100/139. train loss: 0.8439,	6.7971 s / batch. (data: 5.92e+00). ETA=17:07:56, max mem: 7.6 GB 
[10/29 02:52:15 visual_prompt]: Epoch 35 / 100: avg data time: 2.13e+00, avg batch time: 3.0061, average train loss: 0.6707
[10/29 02:53:03 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.4713, average loss: 0.7046
[10/29 02:53:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 65.17	
[10/29 02:55:03 visual_prompt]: Inference (test):avg data time: 4.06e-05, avg batch time: 0.4795, average loss: 0.6731
[10/29 02:55:03 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.24	rocauc: 63.55	
[10/29 02:55:03 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[10/29 03:00:11 visual_prompt]: 	Training 100/139. train loss: 0.7559,	0.8604 s / batch. (data: 2.88e-04). ETA=2:08:08, max mem: 7.6 GB 
[10/29 03:02:00 visual_prompt]: Epoch 36 / 100: avg data time: 2.12e+00, avg batch time: 2.9994, average train loss: 0.6484
[10/29 03:02:48 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.4677, average loss: 0.7070
[10/29 03:02:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 62.74	
[10/29 03:04:48 visual_prompt]: Inference (test):avg data time: 3.87e-05, avg batch time: 0.4813, average loss: 0.7436
[10/29 03:04:48 visual_prompt]: Classification results with test_mammo-cbis: top1: 55.35	rocauc: 60.38	
[10/29 03:04:48 visual_prompt]: Stopping early.
[10/29 03:04:48 visual_prompt]: Rank of current process: 0. World size: 1
[10/29 03:04:48 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/29 03:04:48 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/29 03:04:48 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/29 03:04:48 visual_prompt]: Training with config:
[10/29 03:04:48 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/test/seed4536/lr0.1_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 4536, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/29 03:04:48 visual_prompt]: Loading training data...
[10/29 03:04:48 visual_prompt]: Constructing mammo-cbis dataset train...
[10/29 03:04:48 visual_prompt]: Loading validation data...
[10/29 03:04:48 visual_prompt]: Constructing mammo-cbis dataset val...
[10/29 03:04:48 visual_prompt]: Loading test data...
[10/29 03:04:48 visual_prompt]: Constructing mammo-cbis dataset test...
[10/29 03:04:48 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/29 03:04:51 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/29 03:04:51 visual_prompt]: tuned percent:0.534
[10/29 03:04:51 visual_prompt]: Device used for model: 0
[10/29 03:04:51 visual_prompt]: Setting up Evaluator...
[10/29 03:04:51 visual_prompt]: Setting up Trainer...
[10/29 03:04:51 visual_prompt]: 	Setting up the optimizer...
[10/29 03:04:51 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/29 03:10:01 visual_prompt]: 	Training 100/139. train loss: 1.4935,	0.8760 s / batch. (data: 2.91e-04). ETA=3:21:28, max mem: 7.6 GB 
[10/29 03:11:48 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9983, average train loss: 1.4712
[10/29 03:12:35 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4653, average loss: 1.4544
[10/29 03:12:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.06	
[10/29 03:14:35 visual_prompt]: Inference (test):avg data time: 3.71e-05, avg batch time: 0.4791, average loss: 1.3336
[10/29 03:14:35 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 48.12	
[10/29 03:14:35 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[10/29 03:19:45 visual_prompt]: 	Training 100/139. train loss: 0.6751,	0.8760 s / batch. (data: 7.98e-03). ETA=3:19:27, max mem: 7.6 GB 
[10/29 03:21:33 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 3.0031, average train loss: 0.8873
[10/29 03:22:20 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.4654, average loss: 0.7120
[10/29 03:22:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.59	
[10/29 03:24:21 visual_prompt]: Inference (test):avg data time: 4.04e-05, avg batch time: 0.4789, average loss: 0.6854
[10/29 03:24:21 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 50.78	
[10/29 03:24:21 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[10/29 03:29:24 visual_prompt]: 	Training 100/139. train loss: 0.6686,	0.8679 s / batch. (data: 2.41e-04). ETA=3:15:35, max mem: 7.6 GB 
[10/29 03:31:18 visual_prompt]: Epoch 3 / 100: avg data time: 2.13e+00, avg batch time: 2.9996, average train loss: 0.7333
[10/29 03:32:05 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.4711, average loss: 0.6888
[10/29 03:32:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.03	
[10/29 03:34:05 visual_prompt]: Inference (test):avg data time: 3.76e-05, avg batch time: 0.4798, average loss: 0.6765
[10/29 03:34:05 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 51.52	
[10/29 03:34:05 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[10/29 03:39:11 visual_prompt]: 	Training 100/139. train loss: 0.6286,	0.9000 s / batch. (data: 2.91e-04). ETA=3:20:44, max mem: 7.6 GB 
[10/29 03:41:02 visual_prompt]: Epoch 4 / 100: avg data time: 2.13e+00, avg batch time: 2.9995, average train loss: 0.7564
[10/29 03:41:50 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4715, average loss: 0.6849
[10/29 03:41:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 54.87	
[10/29 03:43:50 visual_prompt]: Inference (test):avg data time: 3.83e-05, avg batch time: 0.4816, average loss: 0.6802
[10/29 03:43:50 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.71	rocauc: 54.27	
[10/29 03:43:50 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[10/29 03:48:59 visual_prompt]: 	Training 100/139. train loss: 0.6986,	0.8878 s / batch. (data: 3.33e-04). ETA=3:15:58, max mem: 7.6 GB 
[10/29 03:50:48 visual_prompt]: Epoch 5 / 100: avg data time: 2.13e+00, avg batch time: 3.0039, average train loss: 0.7466
[10/29 03:51:35 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4663, average loss: 0.7051
[10/29 03:51:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.26	
[10/29 03:53:36 visual_prompt]: Inference (test):avg data time: 7.23e-04, avg batch time: 0.4799, average loss: 0.6788
[10/29 03:53:36 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 56.57	
[10/29 03:53:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[10/29 03:58:42 visual_prompt]: 	Training 100/139. train loss: 0.7000,	0.8880 s / batch. (data: 1.20e-02). ETA=3:13:57, max mem: 7.6 GB 
[10/29 04:00:32 visual_prompt]: Epoch 6 / 100: avg data time: 2.12e+00, avg batch time: 2.9958, average train loss: 0.7234
[10/29 04:01:20 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4666, average loss: 0.6991
[10/29 04:01:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.24	
[10/29 04:03:20 visual_prompt]: Inference (test):avg data time: 3.65e-05, avg batch time: 0.4791, average loss: 0.6740
[10/29 04:03:20 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 57.90	
[10/29 04:03:20 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[10/29 04:08:27 visual_prompt]: 	Training 100/139. train loss: 0.6936,	0.8703 s / batch. (data: 2.88e-04). ETA=3:08:03, max mem: 7.6 GB 
[10/29 04:10:17 visual_prompt]: Epoch 7 / 100: avg data time: 2.13e+00, avg batch time: 2.9986, average train loss: 0.7168
[10/29 04:11:04 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4693, average loss: 0.7328
[10/29 04:11:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.70	
[10/29 04:13:04 visual_prompt]: Inference (test):avg data time: 4.06e-05, avg batch time: 0.4782, average loss: 0.7526
[10/29 04:13:04 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.78	rocauc: 59.39	
[10/29 04:13:04 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[10/29 04:18:10 visual_prompt]: 	Training 100/139. train loss: 0.8855,	0.8943 s / batch. (data: 3.17e-02). ETA=3:11:11, max mem: 7.6 GB 
[10/29 04:20:01 visual_prompt]: Epoch 8 / 100: avg data time: 2.12e+00, avg batch time: 2.9978, average train loss: 0.7359
[10/29 04:20:49 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.4694, average loss: 0.9169
[10/29 04:20:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.66	
[10/29 04:22:49 visual_prompt]: Inference (test):avg data time: 3.74e-05, avg batch time: 0.4828, average loss: 0.9739
[10/29 04:22:49 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 58.33	
[10/29 04:22:49 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[10/29 04:27:57 visual_prompt]: 	Training 100/139. train loss: 0.6926,	0.8920 s / batch. (data: 2.88e-04). ETA=3:08:37, max mem: 7.6 GB 
[10/29 04:29:47 visual_prompt]: Epoch 9 / 100: avg data time: 2.14e+00, avg batch time: 3.0095, average train loss: 0.7218
[10/29 04:30:35 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4681, average loss: 0.7788
[10/29 04:30:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.31	
[10/29 04:32:35 visual_prompt]: Inference (test):avg data time: 4.00e-05, avg batch time: 0.4817, average loss: 0.7297
[10/29 04:32:35 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.98	
[10/29 04:32:35 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[10/29 04:37:43 visual_prompt]: 	Training 100/139. train loss: 0.9313,	1.7277 s / batch. (data: 8.56e-01). ETA=6:01:21, max mem: 7.6 GB 
[10/29 04:39:32 visual_prompt]: Epoch 10 / 100: avg data time: 2.12e+00, avg batch time: 2.9984, average train loss: 0.7088
[10/29 04:40:20 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.4646, average loss: 0.6754
[10/29 04:40:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 58.62	
[10/29 04:42:20 visual_prompt]: Inference (test):avg data time: 3.83e-05, avg batch time: 0.4800, average loss: 0.6608
[10/29 04:42:20 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.78	rocauc: 61.21	
[10/29 04:42:20 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[10/29 04:47:26 visual_prompt]: 	Training 100/139. train loss: 0.6463,	0.8735 s / batch. (data: 2.85e-04). ETA=3:00:39, max mem: 7.6 GB 
[10/29 04:49:16 visual_prompt]: Epoch 11 / 100: avg data time: 2.12e+00, avg batch time: 2.9954, average train loss: 0.7650
[10/29 04:50:04 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.4660, average loss: 0.8404
[10/29 04:50:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.35	
[10/29 04:52:04 visual_prompt]: Inference (test):avg data time: 3.76e-05, avg batch time: 0.4784, average loss: 0.8835
[10/29 04:52:04 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 61.07	
[10/29 04:52:04 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[10/29 04:57:08 visual_prompt]: 	Training 100/139. train loss: 0.7574,	0.8851 s / batch. (data: 2.45e-04). ETA=3:01:00, max mem: 7.6 GB 
[10/29 04:59:01 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 2.9995, average train loss: 0.7132
[10/29 04:59:48 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4655, average loss: 0.6984
[10/29 04:59:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.19	rocauc: 59.97	
[10/29 05:01:49 visual_prompt]: Inference (test):avg data time: 3.68e-05, avg batch time: 0.4788, average loss: 0.7116
[10/29 05:01:49 visual_prompt]: Classification results with test_mammo-cbis: top1: 48.22	rocauc: 61.01	
[10/29 05:01:49 visual_prompt]: Best epoch 12: best metric: -0.698
[10/29 05:01:49 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[10/29 05:06:56 visual_prompt]: 	Training 100/139. train loss: 0.6595,	2.6108 s / batch. (data: 1.75e+00). ETA=8:47:54, max mem: 7.6 GB 
[10/29 05:08:46 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 2.9990, average train loss: 0.7397
[10/29 05:09:33 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4641, average loss: 0.6864
[10/29 05:09:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 60.08	
[10/29 05:11:33 visual_prompt]: Inference (test):avg data time: 3.77e-05, avg batch time: 0.4792, average loss: 0.6931
[10/29 05:11:33 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.91	rocauc: 60.91	
[10/29 05:11:33 visual_prompt]: Best epoch 13: best metric: -0.686
[10/29 05:11:33 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[10/29 05:16:37 visual_prompt]: 	Training 100/139. train loss: 0.6314,	0.8800 s / batch. (data: 2.87e-04). ETA=2:55:53, max mem: 7.6 GB 
[10/29 05:18:31 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0061, average train loss: 0.6987
[10/29 05:19:18 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.4673, average loss: 0.7919
[10/29 05:19:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.20	
[10/29 05:21:19 visual_prompt]: Inference (test):avg data time: 3.88e-05, avg batch time: 0.4803, average loss: 0.7431
[10/29 05:21:19 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 60.10	
[10/29 05:21:19 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[10/29 05:26:26 visual_prompt]: 	Training 100/139. train loss: 0.7524,	0.8921 s / batch. (data: 2.59e-04). ETA=2:56:14, max mem: 7.6 GB 
[10/29 05:28:18 visual_prompt]: Epoch 15 / 100: avg data time: 2.14e+00, avg batch time: 3.0119, average train loss: 0.7326
[10/29 05:29:05 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.4671, average loss: 0.6778
[10/29 05:29:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 58.79	
[10/29 05:31:05 visual_prompt]: Inference (test):avg data time: 3.13e-04, avg batch time: 0.4781, average loss: 0.6628
[10/29 05:31:05 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.00	rocauc: 60.98	
[10/29 05:31:05 visual_prompt]: Best epoch 15: best metric: -0.678
[10/29 05:31:05 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[10/29 05:36:11 visual_prompt]: 	Training 100/139. train loss: 0.6911,	1.4680 s / batch. (data: 5.85e-01). ETA=4:46:37, max mem: 7.6 GB 
[10/29 05:38:02 visual_prompt]: Epoch 16 / 100: avg data time: 2.12e+00, avg batch time: 2.9985, average train loss: 0.7136
[10/29 05:38:50 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4680, average loss: 0.6749
[10/29 05:38:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 60.09	
[10/29 05:40:50 visual_prompt]: Inference (test):avg data time: 3.90e-05, avg batch time: 0.4783, average loss: 0.6690
[10/29 05:40:50 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.79	rocauc: 62.02	
[10/29 05:40:50 visual_prompt]: Best epoch 16: best metric: -0.675
[10/29 05:40:50 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[10/29 05:45:56 visual_prompt]: 	Training 100/139. train loss: 0.5431,	0.8880 s / batch. (data: 2.93e-04). ETA=2:51:19, max mem: 7.6 GB 
[10/29 05:47:48 visual_prompt]: Epoch 17 / 100: avg data time: 2.14e+00, avg batch time: 3.0074, average train loss: 0.6946
[10/29 05:48:36 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.4652, average loss: 0.6911
[10/29 05:48:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 60.81	
[10/29 05:50:36 visual_prompt]: Inference (test):avg data time: 3.99e-05, avg batch time: 0.4792, average loss: 0.6598
[10/29 05:50:36 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.40	rocauc: 62.82	
[10/29 05:50:36 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[10/29 05:55:45 visual_prompt]: 	Training 100/139. train loss: 1.0626,	0.8702 s / batch. (data: 2.85e-04). ETA=2:45:52, max mem: 7.6 GB 
[10/29 05:57:34 visual_prompt]: Epoch 18 / 100: avg data time: 2.13e+00, avg batch time: 3.0056, average train loss: 0.7208
[10/29 05:58:21 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.4671, average loss: 0.7088
[10/29 05:58:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 60.97	
[10/29 06:00:21 visual_prompt]: Inference (test):avg data time: 3.69e-05, avg batch time: 0.4785, average loss: 0.6810
[10/29 06:00:21 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.69	rocauc: 60.19	
[10/29 06:00:21 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[10/29 06:05:29 visual_prompt]: 	Training 100/139. train loss: 0.9311,	0.8846 s / batch. (data: 1.09e-02). ETA=2:46:34, max mem: 7.6 GB 
[10/29 06:07:18 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 2.9993, average train loss: 0.7227
[10/29 06:08:06 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.4659, average loss: 0.6743
[10/29 06:08:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 61.87	
[10/29 06:10:06 visual_prompt]: Inference (test):avg data time: 3.89e-05, avg batch time: 0.4797, average loss: 0.6745
[10/29 06:10:06 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.55	rocauc: 63.37	
[10/29 06:10:06 visual_prompt]: Best epoch 19: best metric: -0.674
[10/29 06:10:06 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[10/29 06:15:16 visual_prompt]: 	Training 100/139. train loss: 0.5550,	0.8835 s / batch. (data: 2.83e-04). ETA=2:44:18, max mem: 7.6 GB 
[10/29 06:17:03 visual_prompt]: Epoch 20 / 100: avg data time: 2.13e+00, avg batch time: 2.9998, average train loss: 0.7087
[10/29 06:17:50 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.4670, average loss: 0.6758
[10/29 06:17:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 62.48	
[10/29 06:19:51 visual_prompt]: Inference (test):avg data time: 3.74e-05, avg batch time: 0.4805, average loss: 0.6557
[10/29 06:19:51 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.24	rocauc: 62.27	
[10/29 06:19:51 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[10/29 06:24:56 visual_prompt]: 	Training 100/139. train loss: 0.6029,	0.9016 s / batch. (data: 2.49e-02). ETA=2:45:35, max mem: 7.6 GB 
[10/29 06:26:47 visual_prompt]: Epoch 21 / 100: avg data time: 2.12e+00, avg batch time: 2.9950, average train loss: 0.7274
[10/29 06:27:34 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4656, average loss: 0.8235
[10/29 06:27:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.57	
[10/29 06:29:35 visual_prompt]: Inference (test):avg data time: 3.85e-05, avg batch time: 0.4805, average loss: 0.8643
[10/29 06:29:35 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 62.11	
[10/29 06:29:35 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[10/29 06:34:43 visual_prompt]: 	Training 100/139. train loss: 0.9060,	4.5360 s / batch. (data: 3.66e+00). ETA=13:42:36, max mem: 7.6 GB 
[10/29 06:36:32 visual_prompt]: Epoch 22 / 100: avg data time: 2.13e+00, avg batch time: 2.9996, average train loss: 0.6910
[10/29 06:37:19 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.4653, average loss: 0.6662
[10/29 06:37:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 61.88	
[10/29 06:39:19 visual_prompt]: Inference (test):avg data time: 3.68e-05, avg batch time: 0.4816, average loss: 0.6487
[10/29 06:39:19 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.71	rocauc: 63.71	
[10/29 06:39:19 visual_prompt]: Best epoch 22: best metric: -0.666
[10/29 06:39:19 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[10/29 06:44:29 visual_prompt]: 	Training 100/139. train loss: 0.6920,	0.8840 s / batch. (data: 7.59e-04). ETA=2:38:16, max mem: 7.6 GB 
[10/29 06:46:17 visual_prompt]: Epoch 23 / 100: avg data time: 2.13e+00, avg batch time: 3.0070, average train loss: 0.6943
[10/29 06:47:05 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.4670, average loss: 0.7623
[10/29 06:47:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.41	
[10/29 06:49:05 visual_prompt]: Inference (test):avg data time: 3.82e-05, avg batch time: 0.4787, average loss: 0.7966
[10/29 06:49:05 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 63.96	
[10/29 06:49:05 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[10/29 06:54:12 visual_prompt]: 	Training 100/139. train loss: 0.8393,	0.8920 s / batch. (data: 2.83e-04). ETA=2:37:37, max mem: 7.6 GB 
[10/29 06:56:02 visual_prompt]: Epoch 24 / 100: avg data time: 2.12e+00, avg batch time: 2.9971, average train loss: 0.6861
[10/29 06:56:50 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4713, average loss: 0.6591
[10/29 06:56:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 63.03	
[10/29 06:58:50 visual_prompt]: Inference (test):avg data time: 3.63e-05, avg batch time: 0.4785, average loss: 0.6489
[10/29 06:58:50 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.24	rocauc: 65.04	
[10/29 06:58:50 visual_prompt]: Best epoch 24: best metric: -0.659
[10/29 06:58:50 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[10/29 07:03:59 visual_prompt]: 	Training 100/139. train loss: 0.6151,	4.7711 s / batch. (data: 3.86e+00). ETA=13:52:04, max mem: 7.6 GB 
[10/29 07:05:46 visual_prompt]: Epoch 25 / 100: avg data time: 2.12e+00, avg batch time: 2.9969, average train loss: 0.6829
[10/29 07:06:34 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4688, average loss: 0.6625
[10/29 07:06:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 62.62	
[10/29 07:08:34 visual_prompt]: Inference (test):avg data time: 3.80e-05, avg batch time: 0.4779, average loss: 0.6446
[10/29 07:08:34 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.64	rocauc: 64.30	
[10/29 07:08:34 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[10/29 07:13:39 visual_prompt]: 	Training 100/139. train loss: 0.7013,	0.8960 s / batch. (data: 1.59e-02). ETA=2:34:10, max mem: 7.6 GB 
[10/29 07:15:31 visual_prompt]: Epoch 26 / 100: avg data time: 2.13e+00, avg batch time: 3.0015, average train loss: 0.6750
[10/29 07:16:18 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.4650, average loss: 0.6563
[10/29 07:16:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 63.79	
[10/29 07:18:18 visual_prompt]: Inference (test):avg data time: 4.16e-05, avg batch time: 0.4812, average loss: 0.6453
[10/29 07:18:18 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.48	rocauc: 64.77	
[10/29 07:18:18 visual_prompt]: Best epoch 26: best metric: -0.656
[10/29 07:18:18 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[10/29 07:23:18 visual_prompt]: 	Training 100/139. train loss: 0.6392,	0.8759 s / batch. (data: 1.21e-02). ETA=2:28:42, max mem: 7.6 GB 
[10/29 07:25:15 visual_prompt]: Epoch 27 / 100: avg data time: 2.12e+00, avg batch time: 2.9961, average train loss: 0.6867
[10/29 07:26:02 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4654, average loss: 0.6663
[10/29 07:26:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 61.49	
[10/29 07:28:03 visual_prompt]: Inference (test):avg data time: 3.83e-05, avg batch time: 0.4797, average loss: 0.6467
[10/29 07:28:03 visual_prompt]: Classification results with test_mammo-cbis: top1: 64.19	rocauc: 63.49	
[10/29 07:28:03 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[10/29 07:33:09 visual_prompt]: 	Training 100/139. train loss: 0.7377,	0.8811 s / batch. (data: 9.67e-03). ETA=2:27:32, max mem: 7.6 GB 
[10/29 07:35:00 visual_prompt]: Epoch 28 / 100: avg data time: 2.13e+00, avg batch time: 3.0022, average train loss: 0.6867
[10/29 07:35:47 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.4666, average loss: 0.7122
[10/29 07:35:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 63.74	
[10/29 07:37:48 visual_prompt]: Inference (test):avg data time: 3.62e-05, avg batch time: 0.4813, average loss: 0.7270
[10/29 07:37:48 visual_prompt]: Classification results with test_mammo-cbis: top1: 47.13	rocauc: 65.26	
[10/29 07:37:48 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[10/29 07:43:00 visual_prompt]: 	Training 100/139. train loss: 0.8737,	0.8985 s / batch. (data: 2.81e-04). ETA=2:28:22, max mem: 7.6 GB 
[10/29 07:44:45 visual_prompt]: Epoch 29 / 100: avg data time: 2.13e+00, avg batch time: 3.0032, average train loss: 0.6696
[10/29 07:45:33 visual_prompt]: Inference (val):avg data time: 7.29e-04, avg batch time: 0.4694, average loss: 0.7002
[10/29 07:45:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 62.75	
[10/29 07:47:33 visual_prompt]: Inference (test):avg data time: 3.95e-05, avg batch time: 0.4793, average loss: 0.6575
[10/29 07:47:33 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.33	rocauc: 64.81	
[10/29 07:47:33 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[10/29 07:52:40 visual_prompt]: 	Training 100/139. train loss: 0.7149,	0.8879 s / batch. (data: 2.92e-04). ETA=2:24:34, max mem: 7.6 GB 
[10/29 07:54:30 visual_prompt]: Epoch 30 / 100: avg data time: 2.13e+00, avg batch time: 2.9975, average train loss: 0.6623
[10/29 07:55:17 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.4687, average loss: 0.6584
[10/29 07:55:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 62.43	
[10/29 07:57:17 visual_prompt]: Inference (test):avg data time: 3.78e-05, avg batch time: 0.4792, average loss: 0.6386
[10/29 07:57:17 visual_prompt]: Classification results with test_mammo-cbis: top1: 64.19	rocauc: 64.37	
[10/29 07:57:17 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[10/29 08:02:22 visual_prompt]: 	Training 100/139. train loss: 0.5265,	0.8689 s / batch. (data: 3.46e-04). ETA=2:19:27, max mem: 7.6 GB 
[10/29 08:04:14 visual_prompt]: Epoch 31 / 100: avg data time: 2.13e+00, avg batch time: 2.9992, average train loss: 0.6605
[10/29 08:05:02 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.4663, average loss: 0.7165
[10/29 08:05:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 63.12	
[10/29 08:07:02 visual_prompt]: Inference (test):avg data time: 3.87e-05, avg batch time: 0.4805, average loss: 0.6722
[10/29 08:07:02 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.41	rocauc: 64.40	
[10/29 08:07:02 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[10/29 08:12:07 visual_prompt]: 	Training 100/139. train loss: 0.8361,	0.8780 s / batch. (data: 8.02e-03). ETA=2:18:53, max mem: 7.6 GB 
[10/29 08:13:58 visual_prompt]: Epoch 32 / 100: avg data time: 2.12e+00, avg batch time: 2.9918, average train loss: 0.6970
[10/29 08:14:45 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4661, average loss: 0.6788
[10/29 08:14:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 64.01	
[10/29 08:16:45 visual_prompt]: Inference (test):avg data time: 3.80e-05, avg batch time: 0.4824, average loss: 0.6924
[10/29 08:16:45 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.76	rocauc: 64.71	
[10/29 08:16:45 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[10/29 08:21:53 visual_prompt]: 	Training 100/139. train loss: 0.6117,	7.2360 s / batch. (data: 6.34e+00). ETA=18:47:51, max mem: 7.6 GB 
[10/29 08:23:43 visual_prompt]: Epoch 33 / 100: avg data time: 2.13e+00, avg batch time: 3.0060, average train loss: 0.6680
[10/29 08:24:30 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.4698, average loss: 0.7253
[10/29 08:24:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 65.10	
[10/29 08:26:31 visual_prompt]: Inference (test):avg data time: 3.91e-05, avg batch time: 0.4797, average loss: 0.7548
[10/29 08:26:31 visual_prompt]: Classification results with test_mammo-cbis: top1: 42.02	rocauc: 65.30	
[10/29 08:26:31 visual_prompt]: Stopping early.
[10/29 08:26:31 visual_prompt]: Rank of current process: 0. World size: 1
[10/29 08:26:31 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/29 08:26:31 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/29 08:26:31 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/29 08:26:31 visual_prompt]: Training with config:
[10/29 08:26:31 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/test/seed3172/lr0.1_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 3172, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/29 08:26:31 visual_prompt]: Loading training data...
[10/29 08:26:31 visual_prompt]: Constructing mammo-cbis dataset train...
[10/29 08:26:31 visual_prompt]: Loading validation data...
[10/29 08:26:31 visual_prompt]: Constructing mammo-cbis dataset val...
[10/29 08:26:31 visual_prompt]: Loading test data...
[10/29 08:26:31 visual_prompt]: Constructing mammo-cbis dataset test...
[10/29 08:26:31 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/29 08:26:33 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/29 08:26:33 visual_prompt]: tuned percent:0.534
[10/29 08:26:33 visual_prompt]: Device used for model: 0
[10/29 08:26:33 visual_prompt]: Setting up Evaluator...
[10/29 08:26:33 visual_prompt]: Setting up Trainer...
[10/29 08:26:33 visual_prompt]: 	Setting up the optimizer...
[10/29 08:26:33 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/29 08:31:43 visual_prompt]: 	Training 100/139. train loss: 1.3664,	7.9960 s / batch. (data: 7.11e+00). ETA=1 day, 6:39:05, max mem: 7.6 GB 
[10/29 08:33:31 visual_prompt]: Epoch 1 / 100: avg data time: 2.13e+00, avg batch time: 3.0016, average train loss: 1.0946
[10/29 08:34:18 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4674, average loss: 1.0071
[10/29 08:34:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.21	
[10/29 08:36:18 visual_prompt]: Inference (test):avg data time: 3.89e-05, avg batch time: 0.4811, average loss: 1.0788
[10/29 08:36:18 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.24	rocauc: 50.06	
[10/29 08:36:18 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[10/29 08:41:25 visual_prompt]: 	Training 100/139. train loss: 0.7260,	0.8560 s / batch. (data: 2.88e-04). ETA=3:14:54, max mem: 7.6 GB 
[10/29 08:43:15 visual_prompt]: Epoch 2 / 100: avg data time: 2.12e+00, avg batch time: 2.9953, average train loss: 0.8851
[10/29 08:44:02 visual_prompt]: Inference (val):avg data time: 6.89e-04, avg batch time: 0.4674, average loss: 0.6895
[10/29 08:44:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 51.60	
[10/29 08:46:03 visual_prompt]: Inference (test):avg data time: 4.07e-05, avg batch time: 0.4822, average loss: 0.6785
[10/29 08:46:03 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.69	rocauc: 53.75	
[10/29 08:46:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[10/29 08:51:08 visual_prompt]: 	Training 100/139. train loss: 0.6998,	0.8752 s / batch. (data: 1.05e-02). ETA=3:17:14, max mem: 7.6 GB 
[10/29 08:53:00 visual_prompt]: Epoch 3 / 100: avg data time: 2.13e+00, avg batch time: 2.9997, average train loss: 0.7311
[10/29 08:53:47 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.4677, average loss: 0.7118
[10/29 08:53:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 53.43	
[10/29 08:55:47 visual_prompt]: Inference (test):avg data time: 3.77e-05, avg batch time: 0.4796, average loss: 0.7207
[10/29 08:55:47 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 56.26	
[10/29 08:55:47 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[10/29 09:00:57 visual_prompt]: 	Training 100/139. train loss: 0.6931,	0.8904 s / batch. (data: 1.03e-02). ETA=3:18:35, max mem: 7.6 GB 
[10/29 09:02:44 visual_prompt]: Epoch 4 / 100: avg data time: 2.12e+00, avg batch time: 2.9965, average train loss: 0.7402
[10/29 09:03:31 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4674, average loss: 0.8367
[10/29 09:03:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.47	
[10/29 09:05:32 visual_prompt]: Inference (test):avg data time: 3.97e-05, avg batch time: 0.4804, average loss: 0.7791
[10/29 09:05:32 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 56.79	
[10/29 09:05:32 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[10/29 09:10:42 visual_prompt]: 	Training 100/139. train loss: 0.7952,	0.8611 s / batch. (data: 3.11e-04). ETA=3:10:03, max mem: 7.6 GB 
[10/29 09:12:31 visual_prompt]: Epoch 5 / 100: avg data time: 2.14e+00, avg batch time: 3.0114, average train loss: 0.7582
[10/29 09:13:18 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4688, average loss: 0.6867
[10/29 09:13:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 55.23	
[10/29 09:15:19 visual_prompt]: Inference (test):avg data time: 3.64e-05, avg batch time: 0.4811, average loss: 0.6842
[10/29 09:15:19 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.22	rocauc: 57.72	
[10/29 09:15:19 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[10/29 09:20:27 visual_prompt]: 	Training 100/139. train loss: 0.6638,	0.8800 s / batch. (data: 2.97e-04). ETA=3:12:12, max mem: 7.6 GB 
[10/29 09:22:17 visual_prompt]: Epoch 6 / 100: avg data time: 2.14e+00, avg batch time: 3.0102, average train loss: 0.7438
[10/29 09:23:05 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4689, average loss: 0.6841
[10/29 09:23:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.51	
[10/29 09:25:05 visual_prompt]: Inference (test):avg data time: 3.91e-05, avg batch time: 0.4799, average loss: 0.6803
[10/29 09:25:05 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.38	rocauc: 58.54	
[10/29 09:25:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[10/29 09:30:11 visual_prompt]: 	Training 100/139. train loss: 0.7674,	0.8982 s / batch. (data: 1.55e-02). ETA=3:14:05, max mem: 7.6 GB 
[10/29 09:32:02 visual_prompt]: Epoch 7 / 100: avg data time: 2.13e+00, avg batch time: 3.0018, average train loss: 0.7440
[10/29 09:32:49 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4664, average loss: 0.7372
[10/29 09:32:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.96	
[10/29 09:34:50 visual_prompt]: Inference (test):avg data time: 3.70e-05, avg batch time: 0.4816, average loss: 0.7552
[10/29 09:34:50 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 59.47	
[10/29 09:34:50 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[10/29 09:39:56 visual_prompt]: 	Training 100/139. train loss: 0.6127,	0.8760 s / batch. (data: 2.43e-04). ETA=3:07:17, max mem: 7.6 GB 
[10/29 09:41:47 visual_prompt]: Epoch 8 / 100: avg data time: 2.12e+00, avg batch time: 2.9988, average train loss: 0.6967
[10/29 09:42:34 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.4703, average loss: 0.8822
[10/29 09:42:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.11	
[10/29 09:44:35 visual_prompt]: Inference (test):avg data time: 3.95e-05, avg batch time: 0.4837, average loss: 0.8087
[10/29 09:44:35 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 60.63	
[10/29 09:44:35 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[10/29 09:49:49 visual_prompt]: 	Training 100/139. train loss: 0.7983,	5.6952 s / batch. (data: 4.83e+00). ETA=20:04:20, max mem: 7.6 GB 
[10/29 09:51:33 visual_prompt]: Epoch 9 / 100: avg data time: 2.14e+00, avg batch time: 3.0088, average train loss: 0.7606
[10/29 09:52:20 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4684, average loss: 0.7054
[10/29 09:52:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 57.46	
[10/29 09:54:21 visual_prompt]: Inference (test):avg data time: 4.10e-05, avg batch time: 0.4808, average loss: 0.7187
[10/29 09:54:21 visual_prompt]: Classification results with test_mammo-cbis: top1: 46.98	rocauc: 58.97	
[10/29 09:54:21 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[10/29 09:59:29 visual_prompt]: 	Training 100/139. train loss: 0.7240,	0.8870 s / batch. (data: 5.55e-03). ETA=3:05:30, max mem: 7.6 GB 
[10/29 10:01:19 visual_prompt]: Epoch 10 / 100: avg data time: 2.13e+00, avg batch time: 3.0054, average train loss: 0.7582
[10/29 10:02:06 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4694, average loss: 0.6768
[10/29 10:02:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 58.61	
[10/29 10:04:06 visual_prompt]: Inference (test):avg data time: 3.79e-05, avg batch time: 0.4819, average loss: 0.6736
[10/29 10:04:06 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.69	rocauc: 59.26	
[10/29 10:04:06 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[10/29 10:09:10 visual_prompt]: 	Training 100/139. train loss: 0.9493,	1.8327 s / batch. (data: 9.66e-01). ETA=6:19:03, max mem: 7.6 GB 
[10/29 10:11:04 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0037, average train loss: 0.7467
[10/29 10:11:51 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4659, average loss: 0.7352
[10/29 10:11:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.88	
[10/29 10:13:52 visual_prompt]: Inference (test):avg data time: 3.66e-05, avg batch time: 0.4830, average loss: 0.6968
[10/29 10:13:52 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 59.44	
[10/29 10:13:52 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[10/29 10:18:57 visual_prompt]: 	Training 100/139. train loss: 0.6291,	0.8907 s / batch. (data: 5.50e-04). ETA=3:02:10, max mem: 7.6 GB 
[10/29 10:20:50 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 3.0040, average train loss: 0.7119
[10/29 10:21:37 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4639, average loss: 0.6887
[10/29 10:21:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 60.63	
[10/29 10:23:37 visual_prompt]: Inference (test):avg data time: 3.80e-05, avg batch time: 0.4828, average loss: 0.6640
[10/29 10:23:37 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.31	rocauc: 61.09	
[10/29 10:23:37 visual_prompt]: Best epoch 12: best metric: -0.689
[10/29 10:23:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[10/29 10:28:42 visual_prompt]: 	Training 100/139. train loss: 0.7118,	0.8880 s / batch. (data: 1.20e-02). ETA=2:59:32, max mem: 7.6 GB 
[10/29 10:30:34 visual_prompt]: Epoch 13 / 100: avg data time: 2.13e+00, avg batch time: 2.9986, average train loss: 0.7842
[10/29 10:31:21 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.4654, average loss: 0.6779
[10/29 10:31:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 61.44	
[10/29 10:33:22 visual_prompt]: Inference (test):avg data time: 3.94e-05, avg batch time: 0.4814, average loss: 0.6855
[10/29 10:33:22 visual_prompt]: Classification results with test_mammo-cbis: top1: 55.19	rocauc: 60.58	
[10/29 10:33:22 visual_prompt]: Best epoch 13: best metric: -0.678
[10/29 10:33:22 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[10/29 10:38:27 visual_prompt]: 	Training 100/139. train loss: 0.7785,	0.8721 s / batch. (data: 2.62e-04). ETA=2:54:18, max mem: 7.6 GB 
[10/29 10:40:18 visual_prompt]: Epoch 14 / 100: avg data time: 2.12e+00, avg batch time: 2.9946, average train loss: 0.7656
[10/29 10:41:06 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.4698, average loss: 0.6713
[10/29 10:41:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 60.81	
[10/29 10:43:06 visual_prompt]: Inference (test):avg data time: 3.68e-05, avg batch time: 0.4806, average loss: 0.6696
[10/29 10:43:06 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.16	rocauc: 60.81	
[10/29 10:43:06 visual_prompt]: Best epoch 14: best metric: -0.671
[10/29 10:43:06 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[10/29 10:48:16 visual_prompt]: 	Training 100/139. train loss: 0.6630,	0.8792 s / batch. (data: 3.04e-04). ETA=2:53:41, max mem: 7.6 GB 
[10/29 10:50:03 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 2.9999, average train loss: 0.7224
[10/29 10:50:51 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.4706, average loss: 0.7186
[10/29 10:50:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 62.07	
[10/29 10:52:51 visual_prompt]: Inference (test):avg data time: 3.97e-05, avg batch time: 0.4793, average loss: 0.6844
[10/29 10:52:51 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.47	rocauc: 61.42	
[10/29 10:52:51 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[10/29 10:58:01 visual_prompt]: 	Training 100/139. train loss: 0.7139,	0.8803 s / batch. (data: 7.96e-03). ETA=2:51:52, max mem: 7.6 GB 
[10/29 10:59:48 visual_prompt]: Epoch 16 / 100: avg data time: 2.13e+00, avg batch time: 3.0020, average train loss: 0.7044
[10/29 11:00:36 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4706, average loss: 0.8688
[10/29 11:00:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.28	
[10/29 11:02:36 visual_prompt]: Inference (test):avg data time: 3.84e-05, avg batch time: 0.4835, average loss: 0.9202
[10/29 11:02:36 visual_prompt]: Classification results with test_mammo-cbis: top1: 40.93	rocauc: 61.69	
[10/29 11:02:36 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[10/29 11:07:43 visual_prompt]: 	Training 100/139. train loss: 0.7669,	0.8960 s / batch. (data: 1.44e-02). ETA=2:52:52, max mem: 7.6 GB 
[10/29 11:09:34 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 3.0060, average train loss: 0.7092
[10/29 11:10:21 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.4674, average loss: 0.6772
[10/29 11:10:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.89	
[10/29 11:12:22 visual_prompt]: Inference (test):avg data time: 3.85e-05, avg batch time: 0.4829, average loss: 0.6614
[10/29 11:12:22 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.78	rocauc: 61.19	
[10/29 11:12:22 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[10/29 11:17:25 visual_prompt]: 	Training 100/139. train loss: 0.7413,	0.8793 s / batch. (data: 2.99e-04). ETA=2:47:36, max mem: 7.6 GB 
[10/29 11:19:18 visual_prompt]: Epoch 18 / 100: avg data time: 2.12e+00, avg batch time: 2.9939, average train loss: 0.7070
[10/29 11:20:05 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.4674, average loss: 0.6705
[10/29 11:20:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 61.08	
[10/29 11:22:06 visual_prompt]: Inference (test):avg data time: 3.63e-05, avg batch time: 0.4828, average loss: 0.6534
[10/29 11:22:06 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.17	rocauc: 61.75	
[10/29 11:22:06 visual_prompt]: Best epoch 18: best metric: -0.671
[10/29 11:22:06 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[10/29 11:27:17 visual_prompt]: 	Training 100/139. train loss: 0.7071,	0.8640 s / batch. (data: 5.41e-03). ETA=2:42:40, max mem: 7.6 GB 
[10/29 11:29:04 visual_prompt]: Epoch 19 / 100: avg data time: 2.14e+00, avg batch time: 3.0094, average train loss: 0.6707
[10/29 11:29:51 visual_prompt]: Inference (val):avg data time: 4.71e-04, avg batch time: 0.4697, average loss: 0.7008
[10/29 11:29:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.26	
[10/29 11:31:52 visual_prompt]: Inference (test):avg data time: 3.68e-05, avg batch time: 0.4817, average loss: 0.6692
[10/29 11:31:52 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.78	rocauc: 63.74	
[10/29 11:31:52 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[10/29 11:37:00 visual_prompt]: 	Training 100/139. train loss: 0.6180,	0.8893 s / batch. (data: 2.03e-02). ETA=2:45:23, max mem: 7.6 GB 
[10/29 11:38:49 visual_prompt]: Epoch 20 / 100: avg data time: 2.13e+00, avg batch time: 3.0031, average train loss: 0.6832
[10/29 11:39:36 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.4680, average loss: 0.6585
[10/29 11:39:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 64.03	
[10/29 11:41:37 visual_prompt]: Inference (test):avg data time: 3.95e-05, avg batch time: 0.4799, average loss: 0.6688
[10/29 11:41:37 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.29	rocauc: 63.00	
[10/29 11:41:37 visual_prompt]: Best epoch 20: best metric: -0.658
[10/29 11:41:37 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[10/29 11:46:44 visual_prompt]: 	Training 100/139. train loss: 0.6562,	0.9189 s / batch. (data: 2.05e-02). ETA=2:48:45, max mem: 7.6 GB 
[10/29 11:48:34 visual_prompt]: Epoch 21 / 100: avg data time: 2.13e+00, avg batch time: 3.0039, average train loss: 0.6698
[10/29 11:49:22 visual_prompt]: Inference (val):avg data time: 1.03e-03, avg batch time: 0.4662, average loss: 0.6634
[10/29 11:49:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 63.43	
[10/29 11:51:22 visual_prompt]: Inference (test):avg data time: 3.67e-05, avg batch time: 0.4806, average loss: 0.6507
[10/29 11:51:22 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.57	rocauc: 62.01	
[10/29 11:51:22 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[10/29 11:56:30 visual_prompt]: 	Training 100/139. train loss: 0.6336,	0.8687 s / batch. (data: 5.41e-03). ETA=2:37:32, max mem: 7.6 GB 
[10/29 11:58:20 visual_prompt]: Epoch 22 / 100: avg data time: 2.13e+00, avg batch time: 3.0018, average train loss: 0.6909
[10/29 11:59:07 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.4691, average loss: 0.6547
[10/29 11:59:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.62	
[10/29 12:01:07 visual_prompt]: Inference (test):avg data time: 3.99e-05, avg batch time: 0.4781, average loss: 0.6687
[10/29 12:01:07 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.53	rocauc: 62.62	
[10/29 12:01:07 visual_prompt]: Best epoch 22: best metric: -0.655
[10/29 12:01:07 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[10/29 12:06:08 visual_prompt]: 	Training 100/139. train loss: 0.6180,	0.8785 s / batch. (data: 5.40e-03). ETA=2:37:17, max mem: 7.6 GB 
[10/29 12:08:04 visual_prompt]: Epoch 23 / 100: avg data time: 2.12e+00, avg batch time: 2.9983, average train loss: 0.6946
[10/29 12:08:51 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4665, average loss: 0.7056
[10/29 12:08:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 64.67	
[10/29 12:10:52 visual_prompt]: Inference (test):avg data time: 4.02e-05, avg batch time: 0.4792, average loss: 0.7331
[10/29 12:10:52 visual_prompt]: Classification results with test_mammo-cbis: top1: 50.08	rocauc: 63.58	
[10/29 12:10:52 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[10/29 12:16:00 visual_prompt]: 	Training 100/139. train loss: 0.5967,	0.9120 s / batch. (data: 2.87e-04). ETA=2:41:09, max mem: 7.6 GB 
[10/29 12:17:49 visual_prompt]: Epoch 24 / 100: avg data time: 2.13e+00, avg batch time: 3.0023, average train loss: 0.6697
[10/29 12:18:37 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.4682, average loss: 0.6531
[10/29 12:18:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 65.70	
[10/29 12:20:37 visual_prompt]: Inference (test):avg data time: 3.94e-05, avg batch time: 0.4839, average loss: 0.6652
[10/29 12:20:37 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.78	rocauc: 63.31	
[10/29 12:20:37 visual_prompt]: Best epoch 24: best metric: -0.653
[10/29 12:20:37 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[10/29 12:25:39 visual_prompt]: 	Training 100/139. train loss: 0.7847,	1.0127 s / batch. (data: 1.55e-01). ETA=2:56:37, max mem: 7.6 GB 
[10/29 12:27:34 visual_prompt]: Epoch 25 / 100: avg data time: 2.12e+00, avg batch time: 2.9996, average train loss: 0.7068
[10/29 12:28:22 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4695, average loss: 0.6699
[10/29 12:28:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 65.70	
[10/29 12:30:22 visual_prompt]: Inference (test):avg data time: 3.81e-05, avg batch time: 0.4828, average loss: 0.6953
[10/29 12:30:22 visual_prompt]: Classification results with test_mammo-cbis: top1: 56.28	rocauc: 63.67	
[10/29 12:30:22 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[10/29 12:35:31 visual_prompt]: 	Training 100/139. train loss: 0.7621,	0.8842 s / batch. (data: 2.86e-04). ETA=2:32:09, max mem: 7.6 GB 
[10/29 12:37:19 visual_prompt]: Epoch 26 / 100: avg data time: 2.12e+00, avg batch time: 2.9979, average train loss: 0.6632
[10/29 12:38:06 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4690, average loss: 0.6604
[10/29 12:38:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 65.20	
[10/29 12:40:07 visual_prompt]: Inference (test):avg data time: 3.87e-05, avg batch time: 0.4826, average loss: 0.6704
[10/29 12:40:07 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.22	rocauc: 63.83	
[10/29 12:40:07 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[10/29 12:45:15 visual_prompt]: 	Training 100/139. train loss: 0.6614,	3.2460 s / batch. (data: 2.38e+00). ETA=9:11:03, max mem: 7.6 GB 
[10/29 12:47:04 visual_prompt]: Epoch 27 / 100: avg data time: 2.12e+00, avg batch time: 2.9983, average train loss: 0.6726
[10/29 12:47:51 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4691, average loss: 0.6650
[10/29 12:47:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 64.94	
[10/29 12:49:52 visual_prompt]: Inference (test):avg data time: 6.08e-04, avg batch time: 0.4831, average loss: 0.6795
[10/29 12:49:52 visual_prompt]: Classification results with test_mammo-cbis: top1: 57.67	rocauc: 63.41	
[10/29 12:49:52 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[10/29 12:54:58 visual_prompt]: 	Training 100/139. train loss: 0.7161,	0.8880 s / batch. (data: 1.61e-02). ETA=2:28:41, max mem: 7.6 GB 
[10/29 12:56:49 visual_prompt]: Epoch 28 / 100: avg data time: 2.13e+00, avg batch time: 3.0038, average train loss: 0.7304
[10/29 12:57:37 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.4698, average loss: 0.7552
[10/29 12:57:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 64.80	
[10/29 12:59:37 visual_prompt]: Inference (test):avg data time: 4.03e-05, avg batch time: 0.4780, average loss: 0.7074
[10/29 12:59:37 visual_prompt]: Classification results with test_mammo-cbis: top1: 62.33	rocauc: 64.00	
[10/29 12:59:37 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[10/29 13:04:46 visual_prompt]: 	Training 100/139. train loss: 0.6163,	6.4767 s / batch. (data: 5.61e+00). ETA=17:49:30, max mem: 7.6 GB 
[10/29 13:06:34 visual_prompt]: Epoch 29 / 100: avg data time: 2.12e+00, avg batch time: 2.9974, average train loss: 0.7056
[10/29 13:07:21 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.4718, average loss: 0.8116
[10/29 13:07:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 64.70	
[10/29 13:09:22 visual_prompt]: Inference (test):avg data time: 3.90e-05, avg batch time: 0.4821, average loss: 0.8554
[10/29 13:09:22 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.40	rocauc: 63.91	
[10/29 13:09:22 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[10/29 13:14:31 visual_prompt]: 	Training 100/139. train loss: 0.7821,	0.8956 s / batch. (data: 1.05e-02). ETA=2:25:48, max mem: 7.6 GB 
[10/29 13:16:20 visual_prompt]: Epoch 30 / 100: avg data time: 2.13e+00, avg batch time: 3.0074, average train loss: 0.6590
[10/29 13:17:08 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.4682, average loss: 0.7575
[10/29 13:17:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 67.36	
[10/29 13:19:08 visual_prompt]: Inference (test):avg data time: 3.76e-05, avg batch time: 0.4801, average loss: 0.7902
[10/29 13:19:08 visual_prompt]: Classification results with test_mammo-cbis: top1: 49.92	rocauc: 65.35	
[10/29 13:19:08 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[10/29 13:24:11 visual_prompt]: 	Training 100/139. train loss: 0.8603,	0.8821 s / batch. (data: 1.20e-02). ETA=2:21:34, max mem: 7.6 GB 
[10/29 13:26:04 visual_prompt]: Epoch 31 / 100: avg data time: 2.12e+00, avg batch time: 2.9968, average train loss: 0.7190
[10/29 13:26:52 visual_prompt]: Inference (val):avg data time: 7.20e-04, avg batch time: 0.4706, average loss: 1.0159
[10/29 13:26:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 67.17	
[10/29 13:28:52 visual_prompt]: Inference (test):avg data time: 3.98e-05, avg batch time: 0.4831, average loss: 1.1213
[10/29 13:28:52 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.09	rocauc: 62.30	
[10/29 13:28:52 visual_prompt]: Stopping early.
[10/29 13:28:52 visual_prompt]: Rank of current process: 0. World size: 1
[10/29 13:28:52 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA TITAN Xp
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/29 13:28:52 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '16', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '384', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/29 13:28:52 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/29 13:28:53 visual_prompt]: Training with config:
[10/29 13:28:53 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop384/test/seed8393/lr0.1_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 8393, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 384, 'NO_TEST': False, 'BATCH_SIZE': 16, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/29 13:28:53 visual_prompt]: Loading training data...
[10/29 13:28:53 visual_prompt]: Constructing mammo-cbis dataset train...
[10/29 13:28:53 visual_prompt]: Loading validation data...
[10/29 13:28:53 visual_prompt]: Constructing mammo-cbis dataset val...
[10/29 13:28:53 visual_prompt]: Loading test data...
[10/29 13:28:53 visual_prompt]: Constructing mammo-cbis dataset test...
[10/29 13:28:53 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 577, 768])
load_pretrained: grid-size from 14 to 24
[10/29 13:28:55 visual_prompt]: Total Parameters: 86552834	 Gradient Parameters: 462338
[10/29 13:28:55 visual_prompt]: tuned percent:0.534
[10/29 13:28:55 visual_prompt]: Device used for model: 0
[10/29 13:28:55 visual_prompt]: Setting up Evaluator...
[10/29 13:28:55 visual_prompt]: Setting up Trainer...
[10/29 13:28:55 visual_prompt]: 	Setting up the optimizer...
[10/29 13:28:55 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[10/29 13:34:03 visual_prompt]: 	Training 100/139. train loss: 1.7740,	0.8726 s / batch. (data: 4.14e-04). ETA=3:20:41, max mem: 7.6 GB 
[10/29 13:35:52 visual_prompt]: Epoch 1 / 100: avg data time: 2.12e+00, avg batch time: 2.9947, average train loss: 1.2502
[10/29 13:36:39 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4616, average loss: 1.0840
[10/29 13:36:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.20	
[10/29 13:38:39 visual_prompt]: Inference (test):avg data time: 4.07e-05, avg batch time: 0.4712, average loss: 1.0200
[10/29 13:38:39 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 49.31	
[10/29 13:38:39 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[10/29 13:43:47 visual_prompt]: 	Training 100/139. train loss: 0.6696,	0.8563 s / batch. (data: 2.42e-04). ETA=3:14:58, max mem: 7.6 GB 
[10/29 13:45:37 visual_prompt]: Epoch 2 / 100: avg data time: 2.13e+00, avg batch time: 3.0026, average train loss: 0.8737
[10/29 13:46:24 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.4594, average loss: 0.6908
[10/29 13:46:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.42	
[10/29 13:48:24 visual_prompt]: Inference (test):avg data time: 3.89e-05, avg batch time: 0.4695, average loss: 0.6756
[10/29 13:48:24 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 51.77	
[10/29 13:48:24 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[10/29 13:53:32 visual_prompt]: 	Training 100/139. train loss: 0.7527,	0.8747 s / batch. (data: 2.81e-04). ETA=3:17:07, max mem: 7.6 GB 
[10/29 13:55:21 visual_prompt]: Epoch 3 / 100: avg data time: 2.13e+00, avg batch time: 2.9987, average train loss: 0.7150
[10/29 13:56:09 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4579, average loss: 0.7298
[10/29 13:56:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.55	
[10/29 13:58:09 visual_prompt]: Inference (test):avg data time: 3.96e-05, avg batch time: 0.4717, average loss: 0.6973
[10/29 13:58:09 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 54.46	
[10/29 13:58:09 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[10/29 14:03:18 visual_prompt]: 	Training 100/139. train loss: 0.6703,	0.8869 s / batch. (data: 7.36e-03). ETA=3:17:49, max mem: 7.6 GB 
[10/29 14:05:08 visual_prompt]: Epoch 4 / 100: avg data time: 2.14e+00, avg batch time: 3.0084, average train loss: 0.7151
[10/29 14:05:55 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4632, average loss: 0.7115
[10/29 14:05:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.73	
[10/29 14:07:55 visual_prompt]: Inference (test):avg data time: 3.91e-05, avg batch time: 0.4704, average loss: 0.6864
[10/29 14:07:55 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 53.79	
[10/29 14:07:55 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[10/29 14:13:02 visual_prompt]: 	Training 100/139. train loss: 0.8183,	0.8600 s / batch. (data: 3.07e-04). ETA=3:09:49, max mem: 7.6 GB 
[10/29 14:14:53 visual_prompt]: Epoch 5 / 100: avg data time: 2.14e+00, avg batch time: 3.0067, average train loss: 0.7283
[10/29 14:15:41 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.4573, average loss: 1.0430
[10/29 14:15:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.09	
[10/29 14:17:41 visual_prompt]: Inference (test):avg data time: 4.11e-05, avg batch time: 0.4727, average loss: 0.9597
[10/29 14:17:41 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 55.06	
[10/29 14:17:41 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[10/29 14:22:48 visual_prompt]: 	Training 100/139. train loss: 0.7182,	0.8835 s / batch. (data: 1.56e-02). ETA=3:12:57, max mem: 7.6 GB 
[10/29 14:24:38 visual_prompt]: Epoch 6 / 100: avg data time: 2.13e+00, avg batch time: 2.9994, average train loss: 0.7438
[10/29 14:25:26 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4583, average loss: 0.7069
[10/29 14:25:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 59.24	
[10/29 14:27:26 visual_prompt]: Inference (test):avg data time: 4.03e-05, avg batch time: 0.4709, average loss: 0.7190
[10/29 14:27:26 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.86	rocauc: 57.67	
[10/29 14:27:26 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[10/29 14:32:32 visual_prompt]: 	Training 100/139. train loss: 0.7013,	1.4640 s / batch. (data: 5.92e-01). ETA=5:16:22, max mem: 7.6 GB 
[10/29 14:34:23 visual_prompt]: Epoch 7 / 100: avg data time: 2.13e+00, avg batch time: 2.9988, average train loss: 0.7412
[10/29 14:35:10 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4581, average loss: 0.6878
[10/29 14:35:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.16	
[10/29 14:37:10 visual_prompt]: Inference (test):avg data time: 3.74e-05, avg batch time: 0.4722, average loss: 0.6904
[10/29 14:37:10 visual_prompt]: Classification results with test_mammo-cbis: top1: 53.49	rocauc: 59.90	
[10/29 14:37:10 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[10/29 14:42:14 visual_prompt]: 	Training 100/139. train loss: 0.6511,	0.8816 s / batch. (data: 1.19e-02). ETA=3:08:27, max mem: 7.6 GB 
[10/29 14:44:07 visual_prompt]: Epoch 8 / 100: avg data time: 2.13e+00, avg batch time: 2.9990, average train loss: 0.7466
[10/29 14:44:55 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.4574, average loss: 0.6809
[10/29 14:44:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 59.02	
[10/29 14:46:55 visual_prompt]: Inference (test):avg data time: 3.73e-05, avg batch time: 0.4717, average loss: 0.6644
[10/29 14:46:55 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.22	rocauc: 59.98	
[10/29 14:46:55 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[10/29 14:52:01 visual_prompt]: 	Training 100/139. train loss: 0.6738,	0.9080 s / batch. (data: 7.96e-03). ETA=3:12:00, max mem: 7.6 GB 
[10/29 14:53:53 visual_prompt]: Epoch 9 / 100: avg data time: 2.14e+00, avg batch time: 3.0060, average train loss: 0.7378
[10/29 14:54:40 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4558, average loss: 0.7003
[10/29 14:54:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 58.85	
[10/29 14:56:41 visual_prompt]: Inference (test):avg data time: 3.75e-05, avg batch time: 0.4718, average loss: 0.6726
[10/29 14:56:41 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.07	rocauc: 60.05	
[10/29 14:56:41 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[10/29 15:01:53 visual_prompt]: 	Training 100/139. train loss: 0.6472,	0.8505 s / batch. (data: 3.05e-04). ETA=2:57:53, max mem: 7.6 GB 
[10/29 15:03:39 visual_prompt]: Epoch 10 / 100: avg data time: 2.14e+00, avg batch time: 3.0066, average train loss: 0.7816
[10/29 15:04:26 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.4629, average loss: 0.6755
[10/29 15:04:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 59.86	
[10/29 15:06:27 visual_prompt]: Inference (test):avg data time: 3.25e-04, avg batch time: 0.4715, average loss: 0.6687
[10/29 15:06:27 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.53	rocauc: 59.50	
[10/29 15:06:27 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[10/29 15:11:35 visual_prompt]: 	Training 100/139. train loss: 0.8299,	0.9120 s / batch. (data: 7.97e-03). ETA=3:08:37, max mem: 7.6 GB 
[10/29 15:13:24 visual_prompt]: Epoch 11 / 100: avg data time: 2.13e+00, avg batch time: 3.0019, average train loss: 0.7240
[10/29 15:14:11 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.4564, average loss: 0.7227
[10/29 15:14:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 58.99	
[10/29 15:16:12 visual_prompt]: Inference (test):avg data time: 3.85e-05, avg batch time: 0.4688, average loss: 0.6890
[10/29 15:16:12 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.91	rocauc: 59.22	
[10/29 15:16:12 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[10/29 15:21:21 visual_prompt]: 	Training 100/139. train loss: 0.6056,	0.8766 s / batch. (data: 2.88e-04). ETA=2:59:16, max mem: 7.6 GB 
[10/29 15:23:10 visual_prompt]: Epoch 12 / 100: avg data time: 2.13e+00, avg batch time: 3.0044, average train loss: 0.7223
[10/29 15:23:57 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.4625, average loss: 0.6765
[10/29 15:23:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 59.86	
[10/29 15:25:58 visual_prompt]: Inference (test):avg data time: 3.91e-05, avg batch time: 0.4756, average loss: 0.6769
[10/29 15:25:58 visual_prompt]: Classification results with test_mammo-cbis: top1: 56.90	rocauc: 59.93	
[10/29 15:25:58 visual_prompt]: Best epoch 12: best metric: -0.676
[10/29 15:25:58 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[10/29 15:31:07 visual_prompt]: 	Training 100/139. train loss: 0.6657,	0.8945 s / batch. (data: 1.09e-02). ETA=3:00:51, max mem: 7.6 GB 
[10/29 15:32:56 visual_prompt]: Epoch 13 / 100: avg data time: 2.14e+00, avg batch time: 3.0092, average train loss: 0.7313
[10/29 15:33:43 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4589, average loss: 0.6699
[10/29 15:33:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.20	
[10/29 15:35:44 visual_prompt]: Inference (test):avg data time: 3.76e-05, avg batch time: 0.4708, average loss: 0.6666
[10/29 15:35:44 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.62	rocauc: 61.22	
[10/29 15:35:44 visual_prompt]: Best epoch 13: best metric: -0.670
[10/29 15:35:44 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[10/29 15:40:50 visual_prompt]: 	Training 100/139. train loss: 0.6848,	1.9112 s / batch. (data: 1.04e+00). ETA=6:22:01, max mem: 7.6 GB 
[10/29 15:42:41 visual_prompt]: Epoch 14 / 100: avg data time: 2.13e+00, avg batch time: 3.0008, average train loss: 0.7202
[10/29 15:43:28 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4576, average loss: 0.6723
[10/29 15:43:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 59.65	
[10/29 15:45:29 visual_prompt]: Inference (test):avg data time: 3.83e-05, avg batch time: 0.4720, average loss: 0.6665
[10/29 15:45:29 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.84	rocauc: 60.68	
[10/29 15:45:29 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[10/29 15:50:32 visual_prompt]: 	Training 100/139. train loss: 1.1747,	0.8679 s / batch. (data: 3.42e-04). ETA=2:51:28, max mem: 7.6 GB 
[10/29 15:52:26 visual_prompt]: Epoch 15 / 100: avg data time: 2.13e+00, avg batch time: 3.0025, average train loss: 0.7400
[10/29 15:53:14 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.4577, average loss: 0.6686
[10/29 15:53:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 61.88	
[10/29 15:55:14 visual_prompt]: Inference (test):avg data time: 3.88e-05, avg batch time: 0.4713, average loss: 0.6716
[10/29 15:55:14 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.45	rocauc: 61.02	
[10/29 15:55:14 visual_prompt]: Best epoch 15: best metric: -0.669
[10/29 15:55:14 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[10/29 16:00:20 visual_prompt]: 	Training 100/139. train loss: 0.7309,	4.3349 s / batch. (data: 3.46e+00). ETA=14:06:22, max mem: 7.6 GB 
[10/29 16:02:11 visual_prompt]: Epoch 16 / 100: avg data time: 2.13e+00, avg batch time: 3.0020, average train loss: 0.7319
[10/29 16:02:59 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.4616, average loss: 0.6918
[10/29 16:02:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 61.08	
[10/29 16:04:59 visual_prompt]: Inference (test):avg data time: 3.79e-05, avg batch time: 0.4716, average loss: 0.6659
[10/29 16:04:59 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.09	rocauc: 61.01	
[10/29 16:04:59 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[10/29 16:10:05 visual_prompt]: 	Training 100/139. train loss: 0.7942,	0.8661 s / batch. (data: 2.95e-04). ETA=2:47:05, max mem: 7.6 GB 
[10/29 16:11:56 visual_prompt]: Epoch 17 / 100: avg data time: 2.13e+00, avg batch time: 3.0021, average train loss: 0.7142
[10/29 16:12:44 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.4596, average loss: 0.6907
[10/29 16:12:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.21	
[10/29 16:14:44 visual_prompt]: Inference (test):avg data time: 3.94e-05, avg batch time: 0.4723, average loss: 0.6687
[10/29 16:14:44 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.62	rocauc: 60.36	
[10/29 16:14:44 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[10/29 16:19:53 visual_prompt]: 	Training 100/139. train loss: 0.6617,	0.8721 s / batch. (data: 2.98e-04). ETA=2:46:13, max mem: 7.6 GB 
[10/29 16:21:42 visual_prompt]: Epoch 18 / 100: avg data time: 2.14e+00, avg batch time: 3.0066, average train loss: 0.7200
[10/29 16:22:30 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4606, average loss: 0.6642
[10/29 16:22:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 61.93	
[10/29 16:24:30 visual_prompt]: Inference (test):avg data time: 3.72e-05, avg batch time: 0.4715, average loss: 0.6561
[10/29 16:24:30 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.09	rocauc: 61.79	
[10/29 16:24:30 visual_prompt]: Best epoch 18: best metric: -0.664
[10/29 16:24:30 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[10/29 16:29:39 visual_prompt]: 	Training 100/139. train loss: 0.6119,	0.8991 s / batch. (data: 7.46e-04). ETA=2:49:18, max mem: 7.6 GB 
[10/29 16:31:28 visual_prompt]: Epoch 19 / 100: avg data time: 2.13e+00, avg batch time: 3.0053, average train loss: 0.6946
[10/29 16:32:15 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.4555, average loss: 0.6933
[10/29 16:32:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 62.04	
[10/29 16:34:16 visual_prompt]: Inference (test):avg data time: 3.92e-05, avg batch time: 0.4722, average loss: 0.6706
[10/29 16:34:16 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.53	rocauc: 60.69	
[10/29 16:34:16 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[10/29 16:39:26 visual_prompt]: 	Training 100/139. train loss: 0.6047,	0.8656 s / batch. (data: 2.83e-04). ETA=2:40:58, max mem: 7.6 GB 
[10/29 16:41:14 visual_prompt]: Epoch 20 / 100: avg data time: 2.14e+00, avg batch time: 3.0063, average train loss: 0.6908
[10/29 16:42:01 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4584, average loss: 0.7265
[10/29 16:42:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 60.28	
[10/29 16:44:01 visual_prompt]: Inference (test):avg data time: 3.73e-05, avg batch time: 0.4716, average loss: 0.6826
[10/29 16:44:01 visual_prompt]: Classification results with test_mammo-cbis: top1: 60.93	rocauc: 61.53	
[10/29 16:44:01 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[10/29 16:49:11 visual_prompt]: 	Training 100/139. train loss: 0.6764,	0.8830 s / batch. (data: 3.04e-04). ETA=2:42:11, max mem: 7.6 GB 
[10/29 16:50:59 visual_prompt]: Epoch 21 / 100: avg data time: 2.14e+00, avg batch time: 3.0059, average train loss: 0.7051
[10/29 16:51:47 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.4601, average loss: 0.6955
[10/29 16:51:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 61.49	
[10/29 16:53:47 visual_prompt]: Inference (test):avg data time: 3.98e-05, avg batch time: 0.4710, average loss: 0.6633
[10/29 16:53:47 visual_prompt]: Classification results with test_mammo-cbis: top1: 61.40	rocauc: 62.64	
[10/29 16:53:47 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[10/29 16:58:55 visual_prompt]: 	Training 100/139. train loss: 0.6784,	0.8800 s / batch. (data: 2.91e-04). ETA=2:39:35, max mem: 7.6 GB 
[10/29 17:00:45 visual_prompt]: Epoch 22 / 100: avg data time: 2.13e+00, avg batch time: 3.0041, average train loss: 0.6867
[10/29 17:01:32 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.4574, average loss: 0.9503
[10/29 17:01:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 61.16	
[10/29 17:03:32 visual_prompt]: Inference (test):avg data time: 3.88e-05, avg batch time: 0.4700, average loss: 1.0311
[10/29 17:03:32 visual_prompt]: Classification results with test_mammo-cbis: top1: 41.09	rocauc: 60.79	
[10/29 17:03:32 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[10/29 17:08:37 visual_prompt]: 	Training 100/139. train loss: 0.6776,	0.8954 s / batch. (data: 5.36e-03). ETA=2:40:18, max mem: 7.6 GB 
[10/29 17:10:31 visual_prompt]: Epoch 23 / 100: avg data time: 2.14e+00, avg batch time: 3.0091, average train loss: 0.7004
[10/29 17:11:18 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.4601, average loss: 0.8319
[10/29 17:11:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.85	
[10/29 17:13:18 visual_prompt]: Inference (test):avg data time: 3.67e-05, avg batch time: 0.4707, average loss: 0.7692
[10/29 17:13:18 visual_prompt]: Classification results with test_mammo-cbis: top1: 59.38	rocauc: 62.64	
[10/29 17:13:18 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[10/29 17:18:29 visual_prompt]: 	Training 100/139. train loss: 0.6055,	0.8960 s / batch. (data: 2.67e-04). ETA=2:38:20, max mem: 7.6 GB 
[10/29 17:20:15 visual_prompt]: Epoch 24 / 100: avg data time: 2.13e+00, avg batch time: 2.9987, average train loss: 0.6984
[10/29 17:21:03 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.4621, average loss: 0.6674
[10/29 17:21:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.01	
[10/29 17:23:03 visual_prompt]: Inference (test):avg data time: 3.86e-05, avg batch time: 0.4719, average loss: 0.6487
[10/29 17:23:03 visual_prompt]: Classification results with test_mammo-cbis: top1: 63.10	rocauc: 62.61	
[10/29 17:23:03 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[10/29 17:28:13 visual_prompt]: 	Training 100/139. train loss: 0.7455,	0.8767 s / batch. (data: 5.39e-03). ETA=2:32:53, max mem: 7.6 GB 
[10/29 17:30:00 visual_prompt]: Epoch 25 / 100: avg data time: 2.13e+00, avg batch time: 3.0015, average train loss: 0.7023
[10/29 17:30:47 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.4581, average loss: 0.6744
[10/29 17:30:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 61.70	
[10/29 17:32:48 visual_prompt]: Inference (test):avg data time: 3.92e-05, avg batch time: 0.4706, average loss: 0.6776
[10/29 17:32:48 visual_prompt]: Classification results with test_mammo-cbis: top1: 58.29	rocauc: 63.10	
[10/29 17:32:48 visual_prompt]: Stopping early.
/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)

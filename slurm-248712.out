/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/23 00:59:52 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 00:59:54 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 00:59:54 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/23 00:59:54 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 00:59:54 visual_prompt]: Training with config:
[11/23 00:59:54 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/23 00:59:54 visual_prompt]: Loading training data...
[11/23 00:59:54 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 00:59:54 visual_prompt]: Loading validation data...
[11/23 00:59:54 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 00:59:54 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 01:00:03 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 01:00:03 visual_prompt]: tuned percent:0.525
[11/23 01:00:04 visual_prompt]: Device used for model: 0
[11/23 01:00:04 visual_prompt]: Setting up Evaluator...
[11/23 01:00:04 visual_prompt]: Setting up Trainer...
[11/23 01:00:04 visual_prompt]: 	Setting up the optimizer...
[11/23 01:00:04 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 01:01:49 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8283 s / batch. (data: 4.84e-04). ETA=12:42:03, max mem: 20.9 GB 
[11/23 01:03:27 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8360 s / batch. (data: 1.03e-02). ETA=12:47:41, max mem: 20.9 GB 
[11/23 01:05:07 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3200 s / batch. (data: 4.93e-01). ETA=20:09:59, max mem: 20.9 GB 
[11/23 01:06:42 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8357 s / batch. (data: 5.41e-03). ETA=12:44:38, max mem: 20.9 GB 
[11/23 01:08:21 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8281 s / batch. (data: 5.39e-03). ETA=12:36:18, max mem: 20.9 GB 
[11/23 01:09:12 visual_prompt]: Epoch 1 / 100: avg data time: 1.60e-01, avg batch time: 0.9913, average train loss: 1.5403
[11/23 01:10:06 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3115, average loss: 1.5201
[11/23 01:10:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 01:10:06 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/23 01:11:46 visual_prompt]: 	Training 100/553. train loss: 11.0660,	1.0625 s / batch. (data: 2.42e-01). ETA=16:07:40, max mem: 20.9 GB 
[11/23 01:13:21 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.9680 s / batch. (data: 1.37e-01). ETA=14:40:01, max mem: 20.9 GB 
[11/23 01:15:00 visual_prompt]: 	Training 300/553. train loss: 16.4932,	0.9888 s / batch. (data: 1.70e-01). ETA=14:57:17, max mem: 20.9 GB 
[11/23 01:16:35 visual_prompt]: 	Training 400/553. train loss: 6.3848,	0.8312 s / batch. (data: 2.98e-04). ETA=12:32:54, max mem: 20.9 GB 
[11/23 01:18:13 visual_prompt]: 	Training 500/553. train loss: 1.8677,	0.8368 s / batch. (data: 3.21e-04). ETA=12:36:35, max mem: 20.9 GB 
[11/23 01:19:02 visual_prompt]: Epoch 2 / 100: avg data time: 1.36e-01, avg batch time: 0.9680, average train loss: 14.2649
[11/23 01:19:56 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3113, average loss: 67.7520
[11/23 01:19:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.35	
[11/23 01:19:56 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/23 01:21:35 visual_prompt]: 	Training 100/553. train loss: 36.6844,	0.8450 s / batch. (data: 1.08e-02). ETA=12:41:50, max mem: 20.9 GB 
[11/23 01:23:12 visual_prompt]: 	Training 200/553. train loss: 39.5236,	0.8348 s / batch. (data: 2.93e-04). ETA=12:31:12, max mem: 20.9 GB 
[11/23 01:24:47 visual_prompt]: 	Training 300/553. train loss: 87.4285,	0.8440 s / batch. (data: 3.04e-04). ETA=12:38:05, max mem: 20.9 GB 
[11/23 01:26:23 visual_prompt]: 	Training 400/553. train loss: 14.2160,	0.8271 s / batch. (data: 3.11e-04). ETA=12:21:34, max mem: 20.9 GB 
[11/23 01:28:01 visual_prompt]: 	Training 500/553. train loss: 37.0757,	1.1080 s / batch. (data: 2.78e-01). ETA=16:31:31, max mem: 20.9 GB 
[11/23 01:28:50 visual_prompt]: Epoch 3 / 100: avg data time: 1.35e-01, avg batch time: 0.9646, average train loss: 39.4319
[11/23 01:29:44 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3084, average loss: 28.8792
[11/23 01:29:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.40	
[11/23 01:29:44 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/23 01:31:27 visual_prompt]: 	Training 100/553. train loss: 151.6634,	0.8734 s / batch. (data: 2.54e-02). ETA=12:59:23, max mem: 20.9 GB 
[11/23 01:33:07 visual_prompt]: 	Training 200/553. train loss: 0.4510,	0.8262 s / batch. (data: 3.11e-04). ETA=12:15:53, max mem: 20.9 GB 
[11/23 01:34:45 visual_prompt]: 	Training 300/553. train loss: 7.5754,	1.2880 s / batch. (data: 4.54e-01). ETA=19:05:01, max mem: 20.9 GB 
[11/23 01:36:18 visual_prompt]: 	Training 400/553. train loss: 67.7845,	1.2680 s / batch. (data: 4.30e-01). ETA=18:45:10, max mem: 20.9 GB 
[11/23 01:37:56 visual_prompt]: 	Training 500/553. train loss: 0.0000,	2.9759 s / batch. (data: 2.17e+00). ETA=1 day, 19:55:39, max mem: 20.9 GB 
[11/23 01:38:49 visual_prompt]: Epoch 4 / 100: avg data time: 1.56e-01, avg batch time: 0.9841, average train loss: 53.2946
[11/23 01:39:44 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3112, average loss: 34.9306
[11/23 01:39:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.06	
[11/23 01:39:44 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/23 01:41:23 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8280 s / batch. (data: 2.89e-04). ETA=12:11:14, max mem: 20.9 GB 
[11/23 01:43:00 visual_prompt]: 	Training 200/553. train loss: 35.7910,	1.1909 s / batch. (data: 3.61e-01). ETA=17:29:45, max mem: 20.9 GB 
[11/23 01:44:37 visual_prompt]: 	Training 300/553. train loss: 13.4560,	0.8471 s / batch. (data: 3.27e-04). ETA=12:25:18, max mem: 20.9 GB 
[11/23 01:46:14 visual_prompt]: 	Training 400/553. train loss: 74.0575,	0.8314 s / batch. (data: 5.42e-03). ETA=12:10:05, max mem: 20.9 GB 
[11/23 01:47:51 visual_prompt]: 	Training 500/553. train loss: 51.1401,	0.8414 s / batch. (data: 9.33e-03). ETA=12:17:25, max mem: 20.9 GB 
[11/23 01:48:42 visual_prompt]: Epoch 5 / 100: avg data time: 1.45e-01, avg batch time: 0.9730, average train loss: 70.3916
[11/23 01:49:36 visual_prompt]: Inference (val):avg data time: 2.77e-04, avg batch time: 0.3115, average loss: 43.8182
[11/23 01:49:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.23	
[11/23 01:49:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/23 01:51:18 visual_prompt]: 	Training 100/553. train loss: 363.2985,	0.8453 s / batch. (data: 1.60e-02). ETA=12:18:42, max mem: 20.9 GB 
[11/23 01:52:53 visual_prompt]: 	Training 200/553. train loss: 621.9333,	0.8432 s / batch. (data: 1.12e-02). ETA=12:15:28, max mem: 20.9 GB 
[11/23 01:54:27 visual_prompt]: 	Training 300/553. train loss: 82.2972,	0.8720 s / batch. (data: 7.97e-03). ETA=12:39:09, max mem: 20.9 GB 
[11/23 01:56:07 visual_prompt]: 	Training 400/553. train loss: 33.9150,	0.8163 s / batch. (data: 5.40e-03). ETA=11:49:17, max mem: 20.9 GB 
[11/23 01:57:42 visual_prompt]: 	Training 500/553. train loss: 96.2137,	0.8199 s / batch. (data: 3.40e-04). ETA=11:51:02, max mem: 20.9 GB 
[11/23 01:58:31 visual_prompt]: Epoch 6 / 100: avg data time: 1.40e-01, avg batch time: 0.9670, average train loss: 103.8233
[11/23 01:59:26 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3103, average loss: 91.2159
[11/23 01:59:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.52	
[11/23 01:59:26 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/23 02:01:04 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8017 s / batch. (data: 3.08e-04). ETA=11:33:14, max mem: 20.9 GB 
[11/23 02:02:40 visual_prompt]: 	Training 200/553. train loss: 64.2342,	0.8290 s / batch. (data: 7.95e-03). ETA=11:55:25, max mem: 20.9 GB 
[11/23 02:04:18 visual_prompt]: 	Training 300/553. train loss: 5.0139,	1.4510 s / batch. (data: 6.43e-01). ETA=20:49:52, max mem: 20.9 GB 
[11/23 02:05:55 visual_prompt]: 	Training 400/553. train loss: 52.8119,	1.5314 s / batch. (data: 7.18e-01). ETA=21:56:34, max mem: 20.9 GB 
[11/23 02:07:31 visual_prompt]: 	Training 500/553. train loss: 68.9438,	0.8252 s / batch. (data: 5.41e-03). ETA=11:48:04, max mem: 20.9 GB 
[11/23 02:08:20 visual_prompt]: Epoch 7 / 100: avg data time: 1.39e-01, avg batch time: 0.9660, average train loss: 96.4033
[11/23 02:09:16 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3095, average loss: 174.5468
[11/23 02:09:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.70	
[11/23 02:09:16 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/23 02:10:54 visual_prompt]: 	Training 100/553. train loss: 53.3281,	0.8160 s / batch. (data: 3.60e-04). ETA=11:38:06, max mem: 20.9 GB 
[11/23 02:12:33 visual_prompt]: 	Training 200/553. train loss: 583.0183,	0.8236 s / batch. (data: 5.44e-03). ETA=11:43:11, max mem: 20.9 GB 
[11/23 02:14:09 visual_prompt]: 	Training 300/553. train loss: 62.8790,	0.8097 s / batch. (data: 2.91e-04). ETA=11:29:57, max mem: 20.9 GB 
[11/23 02:15:46 visual_prompt]: 	Training 400/553. train loss: 97.0186,	0.8560 s / batch. (data: 3.30e-04). ETA=12:08:00, max mem: 20.9 GB 
[11/23 02:17:22 visual_prompt]: 	Training 500/553. train loss: 524.1109,	1.1880 s / batch. (data: 3.43e-01). ETA=16:48:23, max mem: 20.9 GB 
[11/23 02:18:13 visual_prompt]: Epoch 8 / 100: avg data time: 1.44e-01, avg batch time: 0.9707, average train loss: 112.1865
[11/23 02:19:07 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3101, average loss: 16.1529
[11/23 02:19:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.73	
[11/23 02:19:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/23 02:20:47 visual_prompt]: 	Training 100/553. train loss: 0.3753,	0.8180 s / batch. (data: 3.07e-04). ETA=11:32:12, max mem: 20.9 GB 
[11/23 02:22:22 visual_prompt]: 	Training 200/553. train loss: 171.0539,	0.8760 s / batch. (data: 1.42e-02). ETA=12:19:52, max mem: 20.9 GB 
[11/23 02:23:59 visual_prompt]: 	Training 300/553. train loss: 55.7412,	1.5533 s / batch. (data: 7.16e-01). ETA=21:49:19, max mem: 20.9 GB 
[11/23 02:25:36 visual_prompt]: 	Training 400/553. train loss: 35.6600,	0.8204 s / batch. (data: 7.76e-04). ETA=11:30:08, max mem: 20.9 GB 
[11/23 02:27:12 visual_prompt]: 	Training 500/553. train loss: 203.6946,	0.8239 s / batch. (data: 1.05e-02). ETA=11:31:47, max mem: 20.9 GB 
[11/23 02:28:02 visual_prompt]: Epoch 9 / 100: avg data time: 1.39e-01, avg batch time: 0.9661, average train loss: 156.6385
[11/23 02:28:57 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3100, average loss: 34.7765
[11/23 02:28:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.50	
[11/23 02:28:57 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/23 02:30:39 visual_prompt]: 	Training 100/553. train loss: 35.8805,	0.8153 s / batch. (data: 3.02e-04). ETA=11:22:24, max mem: 20.9 GB 
[11/23 02:32:13 visual_prompt]: 	Training 200/553. train loss: 283.7655,	0.8280 s / batch. (data: 3.05e-04). ETA=11:31:41, max mem: 20.9 GB 
[11/23 02:33:49 visual_prompt]: 	Training 300/553. train loss: 90.8150,	0.8406 s / batch. (data: 2.84e-04). ETA=11:40:49, max mem: 20.9 GB 
[11/23 02:35:23 visual_prompt]: 	Training 400/553. train loss: 170.0476,	0.8203 s / batch. (data: 3.14e-04). ETA=11:22:32, max mem: 20.9 GB 
[11/23 02:37:00 visual_prompt]: 	Training 500/553. train loss: 16.0459,	0.8625 s / batch. (data: 2.24e-02). ETA=11:56:10, max mem: 20.9 GB 
[11/23 02:37:50 visual_prompt]: Epoch 10 / 100: avg data time: 1.38e-01, avg batch time: 0.9647, average train loss: 156.2801
[11/23 02:38:45 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3123, average loss: 7.4814
[11/23 02:38:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 57.60	
[11/23 02:38:45 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/23 02:40:27 visual_prompt]: 	Training 100/553. train loss: 436.6214,	0.8185 s / batch. (data: 5.40e-03). ETA=11:17:32, max mem: 20.9 GB 
[11/23 02:42:04 visual_prompt]: 	Training 200/553. train loss: 283.7959,	0.8403 s / batch. (data: 5.41e-03). ETA=11:34:12, max mem: 20.9 GB 
[11/23 02:43:39 visual_prompt]: 	Training 300/553. train loss: 7.6936,	2.0778 s / batch. (data: 1.24e+00). ETA=1 day, 4:33:06, max mem: 20.9 GB 
[11/23 02:45:14 visual_prompt]: 	Training 400/553. train loss: 61.5182,	0.8234 s / batch. (data: 5.39e-03). ETA=11:17:32, max mem: 20.9 GB 
[11/23 02:46:48 visual_prompt]: 	Training 500/553. train loss: 224.7789,	0.8630 s / batch. (data: 2.29e-02). ETA=11:48:38, max mem: 20.9 GB 
[11/23 02:47:38 visual_prompt]: Epoch 11 / 100: avg data time: 1.38e-01, avg batch time: 0.9644, average train loss: 171.8130
[11/23 02:48:33 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3100, average loss: 178.9753
[11/23 02:48:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.97	
[11/23 02:48:33 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/23 02:50:14 visual_prompt]: 	Training 100/553. train loss: 185.6118,	0.8185 s / batch. (data: 3.01e-04). ETA=11:10:01, max mem: 20.9 GB 
[11/23 02:51:51 visual_prompt]: 	Training 200/553. train loss: 41.3509,	1.8374 s / batch. (data: 1.01e+00). ETA=1 day, 1:01:02, max mem: 20.9 GB 
[11/23 02:53:25 visual_prompt]: 	Training 300/553. train loss: 61.1303,	0.8229 s / batch. (data: 3.02e-04). ETA=11:10:51, max mem: 20.9 GB 
[11/23 02:55:02 visual_prompt]: 	Training 400/553. train loss: 41.7388,	0.8440 s / batch. (data: 2.75e-04). ETA=11:26:41, max mem: 20.9 GB 
[11/23 02:56:37 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8520 s / batch. (data: 7.94e-04). ETA=11:31:46, max mem: 20.9 GB 
[11/23 02:57:27 visual_prompt]: Epoch 12 / 100: avg data time: 1.39e-01, avg batch time: 0.9653, average train loss: 194.8883
[11/23 02:58:21 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3100, average loss: 327.9139
[11/23 02:58:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.36	
[11/23 02:58:21 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/23 03:00:03 visual_prompt]: 	Training 100/553. train loss: 219.3569,	0.8553 s / batch. (data: 7.97e-03). ETA=11:32:18, max mem: 20.9 GB 
[11/23 03:01:35 visual_prompt]: 	Training 200/553. train loss: 397.0056,	0.8262 s / batch. (data: 1.05e-02). ETA=11:07:21, max mem: 20.9 GB 
[11/23 03:03:12 visual_prompt]: 	Training 300/553. train loss: 206.8777,	1.5963 s / batch. (data: 7.89e-01). ETA=21:26:41, max mem: 20.9 GB 
[11/23 03:04:47 visual_prompt]: 	Training 400/553. train loss: 107.5091,	0.8428 s / batch. (data: 7.96e-03). ETA=11:17:55, max mem: 20.9 GB 
[11/23 03:06:24 visual_prompt]: 	Training 500/553. train loss: 72.6317,	0.8250 s / batch. (data: 2.92e-04). ETA=11:02:16, max mem: 20.9 GB 
[11/23 03:07:14 visual_prompt]: Epoch 13 / 100: avg data time: 1.38e-01, avg batch time: 0.9638, average train loss: 154.9497
[11/23 03:08:09 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3093, average loss: 150.0563
[11/23 03:08:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.86	
[11/23 03:08:09 visual_prompt]: Best epoch 13: best metric: -150.056
[11/23 03:08:09 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/23 03:09:50 visual_prompt]: 	Training 100/553. train loss: 67.0006,	0.8320 s / batch. (data: 3.97e-04). ETA=11:05:43, max mem: 20.9 GB 
[11/23 03:11:26 visual_prompt]: 	Training 200/553. train loss: 235.3682,	0.8702 s / batch. (data: 5.04e-02). ETA=11:34:51, max mem: 20.9 GB 
[11/23 03:13:02 visual_prompt]: 	Training 300/553. train loss: 105.3834,	0.8274 s / batch. (data: 3.04e-04). ETA=10:59:20, max mem: 20.9 GB 
[11/23 03:14:37 visual_prompt]: 	Training 400/553. train loss: 222.6599,	0.8144 s / batch. (data: 7.95e-03). ETA=10:47:34, max mem: 20.9 GB 
[11/23 03:16:13 visual_prompt]: 	Training 500/553. train loss: 442.3081,	0.8160 s / batch. (data: 2.92e-04). ETA=10:47:31, max mem: 20.9 GB 
[11/23 03:17:03 visual_prompt]: Epoch 14 / 100: avg data time: 1.39e-01, avg batch time: 0.9656, average train loss: 190.7561
[11/23 03:17:58 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3107, average loss: 204.7610
[11/23 03:17:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.46	
[11/23 03:17:58 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/23 03:19:37 visual_prompt]: 	Training 100/553. train loss: 86.5963,	0.8280 s / batch. (data: 1.20e-02). ETA=10:54:54, max mem: 20.9 GB 
[11/23 03:21:11 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8440 s / batch. (data: 3.13e-04). ETA=11:06:11, max mem: 20.9 GB 
[11/23 03:22:50 visual_prompt]: 	Training 300/553. train loss: 107.6311,	0.8480 s / batch. (data: 3.08e-04). ETA=11:07:53, max mem: 20.9 GB 
[11/23 03:24:23 visual_prompt]: 	Training 400/553. train loss: 29.8066,	1.0569 s / batch. (data: 2.46e-01). ETA=13:50:40, max mem: 20.9 GB 
[11/23 03:26:00 visual_prompt]: 	Training 500/553. train loss: 232.2905,	0.8430 s / batch. (data: 1.72e-02). ETA=11:01:07, max mem: 20.9 GB 
[11/23 03:26:51 visual_prompt]: Epoch 15 / 100: avg data time: 1.37e-01, avg batch time: 0.9638, average train loss: 186.5540
[11/23 03:27:45 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3101, average loss: 120.4946
[11/23 03:27:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.99	
[11/23 03:27:45 visual_prompt]: Best epoch 15: best metric: -120.495
[11/23 03:27:45 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/23 03:29:25 visual_prompt]: 	Training 100/553. train loss: 43.8713,	0.8320 s / batch. (data: 7.95e-03). ETA=10:50:24, max mem: 20.9 GB 
[11/23 03:31:00 visual_prompt]: 	Training 200/553. train loss: 317.8667,	0.8313 s / batch. (data: 4.16e-04). ETA=10:48:29, max mem: 20.9 GB 
[11/23 03:32:37 visual_prompt]: 	Training 300/553. train loss: 10.5811,	0.8400 s / batch. (data: 8.51e-03). ETA=10:53:50, max mem: 20.9 GB 
[11/23 03:34:13 visual_prompt]: 	Training 400/553. train loss: 195.0448,	0.8492 s / batch. (data: 1.10e-02). ETA=10:59:39, max mem: 20.9 GB 
[11/23 03:35:48 visual_prompt]: 	Training 500/553. train loss: 188.4037,	1.0202 s / batch. (data: 2.03e-01). ETA=13:10:45, max mem: 20.9 GB 
[11/23 03:36:38 visual_prompt]: Epoch 16 / 100: avg data time: 1.37e-01, avg batch time: 0.9639, average train loss: 190.2302
[11/23 03:37:33 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3090, average loss: 2.1358
[11/23 03:37:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 52.24	
[11/23 03:37:33 visual_prompt]: Best epoch 16: best metric: -2.136
[11/23 03:37:33 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/23 03:39:12 visual_prompt]: 	Training 100/553. train loss: 243.7455,	0.8362 s / batch. (data: 1.59e-02). ETA=10:46:00, max mem: 20.9 GB 
[11/23 03:40:49 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8332 s / batch. (data: 1.55e-02). ETA=10:42:18, max mem: 20.9 GB 
[11/23 03:42:25 visual_prompt]: 	Training 300/553. train loss: 380.4587,	0.8320 s / batch. (data: 7.96e-03). ETA=10:39:57, max mem: 20.9 GB 
[11/23 03:44:01 visual_prompt]: 	Training 400/553. train loss: 251.5679,	1.0081 s / batch. (data: 2.01e-01). ETA=12:53:44, max mem: 20.9 GB 
[11/23 03:45:36 visual_prompt]: 	Training 500/553. train loss: 26.9485,	1.5014 s / batch. (data: 6.60e-01). ETA=19:09:52, max mem: 20.9 GB 
[11/23 03:46:27 visual_prompt]: Epoch 17 / 100: avg data time: 1.39e-01, avg batch time: 0.9661, average train loss: 160.8307
[11/23 03:47:22 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3098, average loss: 67.9313
[11/23 03:47:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.46	
[11/23 03:47:22 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/23 03:49:02 visual_prompt]: 	Training 100/553. train loss: 337.4742,	0.8320 s / batch. (data: 1.20e-02). ETA=10:35:06, max mem: 20.9 GB 
[11/23 03:50:40 visual_prompt]: 	Training 200/553. train loss: 31.8491,	0.8679 s / batch. (data: 1.09e-02). ETA=11:00:59, max mem: 20.9 GB 
[11/23 03:52:16 visual_prompt]: 	Training 300/553. train loss: 227.8670,	0.8400 s / batch. (data: 2.77e-04). ETA=10:38:23, max mem: 20.9 GB 
[11/23 03:53:51 visual_prompt]: 	Training 400/553. train loss: 127.6485,	0.8506 s / batch. (data: 2.87e-04). ETA=10:45:00, max mem: 20.9 GB 
[11/23 03:55:26 visual_prompt]: 	Training 500/553. train loss: 38.8039,	0.8467 s / batch. (data: 1.47e-02). ETA=10:40:40, max mem: 20.9 GB 
[11/23 03:56:15 visual_prompt]: Epoch 18 / 100: avg data time: 1.38e-01, avg batch time: 0.9644, average train loss: 172.3706
[11/23 03:57:10 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3091, average loss: 23.0419
[11/23 03:57:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.30	
[11/23 03:57:10 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/23 03:58:50 visual_prompt]: 	Training 100/553. train loss: 30.3725,	0.9437 s / batch. (data: 1.21e-01). ETA=11:51:40, max mem: 20.9 GB 
[11/23 04:00:26 visual_prompt]: 	Training 200/553. train loss: 76.1996,	0.8400 s / batch. (data: 3.14e-04). ETA=10:32:02, max mem: 20.9 GB 
[11/23 04:02:02 visual_prompt]: 	Training 300/553. train loss: 526.4801,	0.8402 s / batch. (data: 3.33e-04). ETA=10:30:47, max mem: 20.9 GB 
[11/23 04:03:39 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8243 s / batch. (data: 2.93e-04). ETA=10:17:27, max mem: 20.9 GB 
[11/23 04:05:11 visual_prompt]: 	Training 500/553. train loss: 9.3277,	0.8489 s / batch. (data: 1.56e-02). ETA=10:34:29, max mem: 20.9 GB 
[11/23 04:06:01 visual_prompt]: Epoch 19 / 100: avg data time: 1.33e-01, avg batch time: 0.9604, average train loss: 195.7430
[11/23 04:06:56 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3091, average loss: 76.6227
[11/23 04:06:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.08	
[11/23 04:06:56 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/23 04:08:34 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8223 s / batch. (data: 3.19e-04). ETA=10:12:29, max mem: 20.9 GB 
[11/23 04:10:11 visual_prompt]: 	Training 200/553. train loss: 80.9129,	0.8185 s / batch. (data: 3.20e-04). ETA=10:08:20, max mem: 20.9 GB 
[11/23 04:11:48 visual_prompt]: 	Training 300/553. train loss: 117.1381,	0.8146 s / batch. (data: 3.58e-04). ETA=10:04:03, max mem: 20.9 GB 
[11/23 04:13:23 visual_prompt]: 	Training 400/553. train loss: 17.3723,	0.8320 s / batch. (data: 2.88e-04). ETA=10:15:32, max mem: 20.9 GB 
[11/23 04:14:59 visual_prompt]: 	Training 500/553. train loss: 100.9234,	0.8280 s / batch. (data: 3.02e-04). ETA=10:11:14, max mem: 20.9 GB 
[11/23 04:15:50 visual_prompt]: Epoch 20 / 100: avg data time: 1.39e-01, avg batch time: 0.9662, average train loss: 160.8579
[11/23 04:16:45 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3113, average loss: 58.2357
[11/23 04:16:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.21	
[11/23 04:16:45 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/23 04:18:28 visual_prompt]: 	Training 100/553. train loss: 113.3547,	0.8370 s / batch. (data: 5.87e-03). ETA=10:15:46, max mem: 20.9 GB 
[11/23 04:20:02 visual_prompt]: 	Training 200/553. train loss: 94.7154,	0.8474 s / batch. (data: 1.60e-02). ETA=10:21:58, max mem: 20.9 GB 
[11/23 04:21:37 visual_prompt]: 	Training 300/553. train loss: 420.3132,	0.9520 s / batch. (data: 1.19e-01). ETA=11:37:10, max mem: 20.9 GB 
[11/23 04:23:13 visual_prompt]: 	Training 400/553. train loss: 258.5422,	0.8358 s / batch. (data: 2.95e-04). ETA=10:10:42, max mem: 20.9 GB 
[11/23 04:24:50 visual_prompt]: 	Training 500/553. train loss: 24.6469,	0.8160 s / batch. (data: 3.05e-04). ETA=9:54:51, max mem: 20.9 GB 
[11/23 04:25:40 visual_prompt]: Epoch 21 / 100: avg data time: 1.40e-01, avg batch time: 0.9662, average train loss: 152.4561
[11/23 04:26:34 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3099, average loss: 8.9101
[11/23 04:26:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.52	
[11/23 04:26:34 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/23 04:28:13 visual_prompt]: 	Training 100/553. train loss: 76.3283,	0.8199 s / batch. (data: 5.41e-03). ETA=9:55:35, max mem: 20.9 GB 
[11/23 04:29:49 visual_prompt]: 	Training 200/553. train loss: 141.7162,	0.8280 s / batch. (data: 3.04e-04). ETA=10:00:07, max mem: 20.9 GB 
[11/23 04:31:23 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8352 s / batch. (data: 5.41e-03). ETA=10:03:55, max mem: 20.9 GB 
[11/23 04:33:00 visual_prompt]: 	Training 400/553. train loss: 18.4423,	0.8183 s / batch. (data: 4.52e-04). ETA=9:50:20, max mem: 20.9 GB 
[11/23 04:34:37 visual_prompt]: 	Training 500/553. train loss: 34.7550,	0.8172 s / batch. (data: 3.08e-04). ETA=9:48:12, max mem: 20.9 GB 
[11/23 04:35:28 visual_prompt]: Epoch 22 / 100: avg data time: 1.39e-01, avg batch time: 0.9650, average train loss: 170.8602
[11/23 04:36:22 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3095, average loss: 246.9585
[11/23 04:36:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.10	
[11/23 04:36:22 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/23 04:38:03 visual_prompt]: 	Training 100/553. train loss: 186.1240,	0.8288 s / batch. (data: 1.06e-02). ETA=9:54:27, max mem: 20.9 GB 
[11/23 04:39:40 visual_prompt]: 	Training 200/553. train loss: 272.6903,	0.8440 s / batch. (data: 7.96e-03). ETA=10:03:55, max mem: 20.9 GB 
[11/23 04:41:18 visual_prompt]: 	Training 300/553. train loss: 47.7774,	0.8322 s / batch. (data: 3.19e-04). ETA=9:54:04, max mem: 20.9 GB 
[11/23 04:42:52 visual_prompt]: 	Training 400/553. train loss: 123.1810,	0.8418 s / batch. (data: 5.46e-03). ETA=9:59:31, max mem: 20.9 GB 
[11/23 04:44:26 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8040 s / batch. (data: 3.41e-04). ETA=9:31:16, max mem: 20.9 GB 
[11/23 04:45:16 visual_prompt]: Epoch 23 / 100: avg data time: 1.38e-01, avg batch time: 0.9647, average train loss: 160.6898
[11/23 04:46:10 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.3114, average loss: 47.8060
[11/23 04:46:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.12	
[11/23 04:46:11 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/23 04:47:47 visual_prompt]: 	Training 100/553. train loss: 14.8031,	0.8240 s / batch. (data: 3.08e-04). ETA=9:43:23, max mem: 20.9 GB 
[11/23 04:49:23 visual_prompt]: 	Training 200/553. train loss: 150.9551,	0.8214 s / batch. (data: 2.86e-04). ETA=9:40:10, max mem: 20.9 GB 
[11/23 04:50:59 visual_prompt]: 	Training 300/553. train loss: 100.0307,	0.8772 s / batch. (data: 3.87e-02). ETA=10:18:07, max mem: 20.9 GB 
[11/23 04:52:36 visual_prompt]: 	Training 400/553. train loss: 40.2626,	0.8398 s / batch. (data: 3.00e-04). ETA=9:50:22, max mem: 20.9 GB 
[11/23 04:54:14 visual_prompt]: 	Training 500/553. train loss: 169.1389,	0.8155 s / batch. (data: 3.23e-04). ETA=9:31:57, max mem: 20.9 GB 
[11/23 04:55:04 visual_prompt]: Epoch 24 / 100: avg data time: 1.38e-01, avg batch time: 0.9650, average train loss: 156.1355
[11/23 04:55:59 visual_prompt]: Inference (val):avg data time: 2.58e-04, avg batch time: 0.3127, average loss: 125.1586
[11/23 04:55:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.33	
[11/23 04:55:59 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/23 04:57:41 visual_prompt]: 	Training 100/553. train loss: 52.7218,	0.8280 s / batch. (data: 3.60e-04). ETA=9:38:37, max mem: 20.9 GB 
[11/23 04:59:14 visual_prompt]: 	Training 200/553. train loss: 223.0438,	0.8561 s / batch. (data: 2.10e-02). ETA=9:56:49, max mem: 20.9 GB 
[11/23 05:00:50 visual_prompt]: 	Training 300/553. train loss: 104.7876,	0.8486 s / batch. (data: 2.06e-02). ETA=9:50:08, max mem: 20.9 GB 
[11/23 05:02:25 visual_prompt]: 	Training 400/553. train loss: 683.1397,	0.8358 s / batch. (data: 3.37e-04). ETA=9:39:53, max mem: 20.9 GB 
[11/23 05:04:01 visual_prompt]: 	Training 500/553. train loss: 55.3867,	1.0479 s / batch. (data: 2.10e-01). ETA=12:05:15, max mem: 20.9 GB 
[11/23 05:04:52 visual_prompt]: Epoch 25 / 100: avg data time: 1.36e-01, avg batch time: 0.9633, average train loss: 164.1294
[11/23 05:05:46 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3097, average loss: 7.3253
[11/23 05:05:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.97	
[11/23 05:05:46 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/23 05:07:26 visual_prompt]: 	Training 100/553. train loss: 129.7458,	0.8169 s / batch. (data: 2.86e-04). ETA=9:23:18, max mem: 20.9 GB 
[11/23 05:09:03 visual_prompt]: 	Training 200/553. train loss: 1.3798,	1.5436 s / batch. (data: 7.27e-01). ETA=17:41:51, max mem: 20.9 GB 
[11/23 05:10:41 visual_prompt]: 	Training 300/553. train loss: 7.9059,	0.8493 s / batch. (data: 1.09e-02). ETA=9:42:48, max mem: 20.9 GB 
[11/23 05:12:15 visual_prompt]: 	Training 400/553. train loss: 55.3619,	0.8357 s / batch. (data: 3.25e-04). ETA=9:32:05, max mem: 20.9 GB 
[11/23 05:13:50 visual_prompt]: 	Training 500/553. train loss: 29.2409,	0.8405 s / batch. (data: 2.91e-04). ETA=9:33:58, max mem: 20.9 GB 
[11/23 05:14:40 visual_prompt]: Epoch 26 / 100: avg data time: 1.37e-01, avg batch time: 0.9639, average train loss: 145.6865
[11/23 05:15:34 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3104, average loss: 113.1753
[11/23 05:15:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.51	
[11/23 05:15:34 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/23 05:17:15 visual_prompt]: 	Training 100/553. train loss: 7.8318,	0.8306 s / batch. (data: 3.73e-04). ETA=9:25:07, max mem: 20.9 GB 
[11/23 05:18:50 visual_prompt]: 	Training 200/553. train loss: 111.5258,	1.1822 s / batch. (data: 3.57e-01). ETA=13:22:20, max mem: 20.9 GB 
[11/23 05:20:26 visual_prompt]: 	Training 300/553. train loss: 89.1500,	0.8245 s / batch. (data: 7.96e-03). ETA=9:18:11, max mem: 20.9 GB 
[11/23 05:22:03 visual_prompt]: 	Training 400/553. train loss: 120.9062,	0.8202 s / batch. (data: 2.98e-04). ETA=9:13:55, max mem: 20.9 GB 
[11/23 05:23:39 visual_prompt]: 	Training 500/553. train loss: 298.4228,	0.8364 s / batch. (data: 5.41e-03). ETA=9:23:30, max mem: 20.9 GB 
[11/23 05:24:27 visual_prompt]: Epoch 27 / 100: avg data time: 1.37e-01, avg batch time: 0.9640, average train loss: 150.7523
[11/23 05:25:22 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3107, average loss: 162.6716
[11/23 05:25:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.07	
[11/23 05:25:22 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/23 05:27:01 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8078 s / batch. (data: 3.58e-04). ETA=9:02:10, max mem: 20.9 GB 
[11/23 05:28:37 visual_prompt]: 	Training 200/553. train loss: 48.0571,	0.8320 s / batch. (data: 1.20e-02). ETA=9:17:00, max mem: 20.9 GB 
[11/23 05:30:14 visual_prompt]: 	Training 300/553. train loss: 1.7146,	0.8595 s / batch. (data: 1.32e-02). ETA=9:33:59, max mem: 20.9 GB 
[11/23 05:31:49 visual_prompt]: 	Training 400/553. train loss: 7.2133,	0.8310 s / batch. (data: 2.95e-04). ETA=9:13:35, max mem: 20.9 GB 
[11/23 05:33:23 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8197 s / batch. (data: 5.42e-03). ETA=9:04:39, max mem: 20.9 GB 
[11/23 05:34:15 visual_prompt]: Epoch 28 / 100: avg data time: 1.37e-01, avg batch time: 0.9638, average train loss: 158.9682
[11/23 05:35:09 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3117, average loss: 344.8593
[11/23 05:35:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.29	
[11/23 05:35:10 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/23 05:36:55 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8116 s / batch. (data: 3.05e-04). ETA=8:57:13, max mem: 20.9 GB 
[11/23 05:38:30 visual_prompt]: 	Training 200/553. train loss: 63.6507,	1.5899 s / batch. (data: 7.82e-01). ETA=17:29:45, max mem: 20.9 GB 
[11/23 05:40:03 visual_prompt]: 	Training 300/553. train loss: 61.6685,	0.8134 s / batch. (data: 3.02e-04). ETA=8:55:43, max mem: 20.9 GB 
[11/23 05:41:35 visual_prompt]: 	Training 400/553. train loss: 166.5122,	1.2133 s / batch. (data: 3.54e-01). ETA=13:17:04, max mem: 20.9 GB 
[11/23 05:43:11 visual_prompt]: 	Training 500/553. train loss: 145.6787,	0.8321 s / batch. (data: 1.20e-02). ETA=9:05:13, max mem: 20.9 GB 
[11/23 05:44:01 visual_prompt]: Epoch 29 / 100: avg data time: 1.35e-01, avg batch time: 0.9618, average train loss: 138.1748
[11/23 05:44:56 visual_prompt]: Inference (val):avg data time: 3.35e-04, avg batch time: 0.3099, average loss: 183.6311
[11/23 05:44:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.50	
[11/23 05:44:56 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/23 05:46:35 visual_prompt]: 	Training 100/553. train loss: 19.7214,	0.8389 s / batch. (data: 2.82e-04). ETA=9:07:32, max mem: 20.9 GB 
[11/23 05:48:12 visual_prompt]: 	Training 200/553. train loss: 509.7855,	0.8820 s / batch. (data: 5.41e-03). ETA=9:34:14, max mem: 20.9 GB 
[11/23 05:49:46 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8560 s / batch. (data: 7.96e-03). ETA=9:15:52, max mem: 20.9 GB 
[11/23 05:51:23 visual_prompt]: 	Training 400/553. train loss: 28.2085,	1.0739 s / batch. (data: 2.40e-01). ETA=11:35:35, max mem: 20.9 GB 
[11/23 05:52:58 visual_prompt]: 	Training 500/553. train loss: 231.7779,	1.3057 s / batch. (data: 4.77e-01). ETA=14:03:32, max mem: 20.9 GB 
[11/23 05:53:50 visual_prompt]: Epoch 30 / 100: avg data time: 1.39e-01, avg batch time: 0.9649, average train loss: 124.2348
[11/23 05:54:45 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3102, average loss: 68.2561
[11/23 05:54:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.27	
[11/23 05:54:45 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[11/23 05:56:26 visual_prompt]: 	Training 100/553. train loss: 158.6663,	0.8440 s / batch. (data: 3.04e-04). ETA=9:03:08, max mem: 20.9 GB 
[11/23 05:58:04 visual_prompt]: 	Training 200/553. train loss: 74.8282,	0.8235 s / batch. (data: 2.71e-04). ETA=8:48:32, max mem: 20.9 GB 
[11/23 05:59:37 visual_prompt]: 	Training 300/553. train loss: 216.2123,	0.8234 s / batch. (data: 2.82e-04). ETA=8:47:07, max mem: 20.9 GB 
[11/23 06:01:12 visual_prompt]: 	Training 400/553. train loss: 41.8518,	1.0444 s / batch. (data: 2.27e-01). ETA=11:06:50, max mem: 20.9 GB 
[11/23 06:02:47 visual_prompt]: 	Training 500/553. train loss: 106.7536,	0.8120 s / batch. (data: 3.04e-04). ETA=8:37:06, max mem: 20.9 GB 
[11/23 06:03:36 visual_prompt]: Epoch 31 / 100: avg data time: 1.34e-01, avg batch time: 0.9611, average train loss: 147.2976
[11/23 06:04:31 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3102, average loss: 26.1778
[11/23 06:04:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.19	
[11/23 06:04:31 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[11/23 06:06:12 visual_prompt]: 	Training 100/553. train loss: 12.1126,	0.8400 s / batch. (data: 3.07e-04). ETA=8:52:48, max mem: 20.9 GB 
[11/23 06:07:47 visual_prompt]: 	Training 200/553. train loss: 213.4811,	0.8231 s / batch. (data: 7.57e-04). ETA=8:40:40, max mem: 20.9 GB 
[11/23 06:09:26 visual_prompt]: 	Training 300/553. train loss: 59.8548,	0.8320 s / batch. (data: 3.06e-04). ETA=8:44:56, max mem: 20.9 GB 
[11/23 06:11:02 visual_prompt]: 	Training 400/553. train loss: 17.2170,	0.8280 s / batch. (data: 2.97e-04). ETA=8:41:02, max mem: 20.9 GB 
[11/23 06:12:34 visual_prompt]: 	Training 500/553. train loss: 19.9192,	0.8292 s / batch. (data: 5.40e-03). ETA=8:40:25, max mem: 20.9 GB 
[11/23 06:13:22 visual_prompt]: Epoch 32 / 100: avg data time: 1.35e-01, avg batch time: 0.9601, average train loss: 150.7823
[11/23 06:14:16 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3091, average loss: 6.9319
[11/23 06:14:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.54	
[11/23 06:14:16 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[11/23 06:15:54 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8160 s / batch. (data: 2.95e-04). ETA=8:30:03, max mem: 20.9 GB 
[11/23 06:17:30 visual_prompt]: 	Training 200/553. train loss: 62.5337,	0.9015 s / batch. (data: 9.45e-02). ETA=9:21:59, max mem: 20.9 GB 
[11/23 06:19:05 visual_prompt]: 	Training 300/553. train loss: 8.7674,	0.8188 s / batch. (data: 2.74e-04). ETA=8:29:04, max mem: 20.9 GB 
[11/23 06:20:40 visual_prompt]: 	Training 400/553. train loss: 132.0480,	0.8239 s / batch. (data: 1.05e-02). ETA=8:30:52, max mem: 20.9 GB 
[11/23 06:22:15 visual_prompt]: 	Training 500/553. train loss: 26.6511,	0.8478 s / batch. (data: 1.05e-02). ETA=8:44:16, max mem: 20.9 GB 
[11/23 06:23:04 visual_prompt]: Epoch 33 / 100: avg data time: 1.27e-01, avg batch time: 0.9534, average train loss: 159.6651
[11/23 06:23:58 visual_prompt]: Inference (val):avg data time: 4.97e-04, avg batch time: 0.3116, average loss: 174.1229
[11/23 06:23:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.58	
[11/23 06:23:58 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[11/23 06:25:38 visual_prompt]: 	Training 100/553. train loss: 52.6122,	0.8160 s / batch. (data: 3.28e-04). ETA=8:22:31, max mem: 20.9 GB 
[11/23 06:27:10 visual_prompt]: 	Training 200/553. train loss: 68.4719,	0.8242 s / batch. (data: 1.23e-02). ETA=8:26:11, max mem: 20.9 GB 
[11/23 06:28:46 visual_prompt]: 	Training 300/553. train loss: 172.0752,	0.8107 s / batch. (data: 3.23e-04). ETA=8:16:34, max mem: 20.9 GB 
[11/23 06:30:22 visual_prompt]: 	Training 400/553. train loss: 610.7329,	0.8400 s / batch. (data: 2.80e-04). ETA=8:33:07, max mem: 20.9 GB 
[11/23 06:31:57 visual_prompt]: 	Training 500/553. train loss: 380.0856,	1.4240 s / batch. (data: 5.59e-01). ETA=14:27:27, max mem: 20.9 GB 
[11/23 06:32:47 visual_prompt]: Epoch 34 / 100: avg data time: 1.29e-01, avg batch time: 0.9563, average train loss: 164.0386
[11/23 06:33:41 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3112, average loss: 78.3900
[11/23 06:33:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 55.08	
[11/23 06:33:41 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[11/23 06:35:23 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8099 s / batch. (data: 8.33e-03). ETA=8:11:17, max mem: 20.9 GB 
[11/23 06:37:00 visual_prompt]: 	Training 200/553. train loss: 61.8306,	0.8200 s / batch. (data: 3.26e-04). ETA=8:16:03, max mem: 20.9 GB 
[11/23 06:38:34 visual_prompt]: 	Training 300/553. train loss: 18.9457,	0.8345 s / batch. (data: 2.90e-04). ETA=8:23:26, max mem: 20.9 GB 
[11/23 06:40:09 visual_prompt]: 	Training 400/553. train loss: 100.3453,	0.8320 s / batch. (data: 3.14e-04). ETA=8:20:33, max mem: 20.9 GB 
[11/23 06:41:44 visual_prompt]: 	Training 500/553. train loss: 194.8438,	0.8522 s / batch. (data: 8.18e-03). ETA=8:31:17, max mem: 20.9 GB 
[11/23 06:42:35 visual_prompt]: Epoch 35 / 100: avg data time: 1.39e-01, avg batch time: 0.9656, average train loss: 160.5242
[11/23 06:43:30 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.3113, average loss: 12.3208
[11/23 06:43:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.55	
[11/23 06:43:30 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[11/23 06:45:09 visual_prompt]: 	Training 100/553. train loss: 15.6176,	0.8274 s / batch. (data: 3.09e-04). ETA=8:14:19, max mem: 20.9 GB 
[11/23 06:46:46 visual_prompt]: 	Training 200/553. train loss: 491.0982,	0.8246 s / batch. (data: 3.32e-04). ETA=8:11:16, max mem: 20.9 GB 
[11/23 06:48:23 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8522 s / batch. (data: 2.42e-02). ETA=8:26:17, max mem: 20.9 GB 
[11/23 06:49:59 visual_prompt]: 	Training 400/553. train loss: 8.7644,	0.8205 s / batch. (data: 2.79e-04). ETA=8:06:03, max mem: 20.9 GB 
[11/23 06:51:35 visual_prompt]: 	Training 500/553. train loss: 37.5856,	0.8723 s / batch. (data: 5.38e-02). ETA=8:35:18, max mem: 20.9 GB 
[11/23 06:52:22 visual_prompt]: Epoch 36 / 100: avg data time: 1.36e-01, avg batch time: 0.9628, average train loss: 152.7814
[11/23 06:53:17 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3111, average loss: 30.2378
[11/23 06:53:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.42	
[11/23 06:53:17 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[11/23 06:54:57 visual_prompt]: 	Training 100/553. train loss: 75.8718,	0.8189 s / batch. (data: 2.94e-04). ETA=8:01:41, max mem: 20.9 GB 
[11/23 06:56:32 visual_prompt]: 	Training 200/553. train loss: 103.2251,	0.8370 s / batch. (data: 2.78e-04). ETA=8:10:54, max mem: 20.9 GB 
[11/23 06:58:08 visual_prompt]: 	Training 300/553. train loss: 166.5088,	1.1331 s / batch. (data: 3.16e-01). ETA=11:02:43, max mem: 20.9 GB 
[11/23 06:59:47 visual_prompt]: 	Training 400/553. train loss: 6.2100,	1.8117 s / batch. (data: 9.85e-01). ETA=17:36:33, max mem: 20.9 GB 
[11/23 07:01:19 visual_prompt]: 	Training 500/553. train loss: 125.8673,	0.8356 s / batch. (data: 6.47e-03). ETA=8:05:54, max mem: 20.9 GB 
[11/23 07:02:11 visual_prompt]: Epoch 37 / 100: avg data time: 1.38e-01, avg batch time: 0.9651, average train loss: 124.8053
[11/23 07:03:05 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3097, average loss: 122.9183
[11/23 07:03:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.28	
[11/23 07:03:05 visual_prompt]: Stopping early.
[11/23 07:03:05 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 07:03:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 07:03:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/23 07:03:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 07:03:05 visual_prompt]: Training with config:
[11/23 07:03:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/23 07:03:05 visual_prompt]: Loading training data...
[11/23 07:03:05 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 07:03:05 visual_prompt]: Loading validation data...
[11/23 07:03:05 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 07:03:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 07:03:10 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 07:03:10 visual_prompt]: tuned percent:0.525
[11/23 07:03:10 visual_prompt]: Device used for model: 0
[11/23 07:03:10 visual_prompt]: Setting up Evaluator...
[11/23 07:03:10 visual_prompt]: Setting up Trainer...
[11/23 07:03:10 visual_prompt]: 	Setting up the optimizer...
[11/23 07:03:10 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 07:04:50 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8359 s / batch. (data: 4.60e-04). ETA=12:49:03, max mem: 20.9 GB 
[11/23 07:06:24 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8760 s / batch. (data: 2.86e-04). ETA=13:24:28, max mem: 20.9 GB 
[11/23 07:08:02 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8550 s / batch. (data: 3.69e-02). ETA=13:03:42, max mem: 20.9 GB 
[11/23 07:09:37 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8254 s / batch. (data: 3.05e-04). ETA=12:35:14, max mem: 20.9 GB 
[11/23 07:11:15 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8720 s / batch. (data: 1.19e-02). ETA=13:16:28, max mem: 20.9 GB 
[11/23 07:12:06 visual_prompt]: Epoch 1 / 100: avg data time: 1.34e-01, avg batch time: 0.9680, average train loss: 1.5403
[11/23 07:13:00 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3116, average loss: 1.5201
[11/23 07:13:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 07:13:00 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/23 07:14:39 visual_prompt]: 	Training 100/553. train loss: 10.6743,	0.8360 s / batch. (data: 3.24e-04). ETA=12:41:24, max mem: 20.9 GB 
[11/23 07:16:15 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8272 s / batch. (data: 1.12e-02). ETA=12:32:03, max mem: 20.9 GB 
[11/23 07:17:53 visual_prompt]: 	Training 300/553. train loss: 0.8119,	1.0164 s / batch. (data: 1.79e-01). ETA=15:22:22, max mem: 20.9 GB 
[11/23 07:19:28 visual_prompt]: 	Training 400/553. train loss: 11.9164,	0.8440 s / batch. (data: 7.97e-03). ETA=12:44:28, max mem: 20.9 GB 
[11/23 07:21:05 visual_prompt]: 	Training 500/553. train loss: 8.2818,	0.8200 s / batch. (data: 2.98e-04). ETA=12:21:23, max mem: 20.9 GB 
[11/23 07:21:55 visual_prompt]: Epoch 2 / 100: avg data time: 1.34e-01, avg batch time: 0.9662, average train loss: 15.2291
[11/23 07:22:49 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3093, average loss: 15.8872
[11/23 07:22:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.27	
[11/23 07:22:49 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/23 07:24:28 visual_prompt]: 	Training 100/553. train loss: 37.7481,	0.8203 s / batch. (data: 5.41e-03). ETA=12:19:31, max mem: 20.9 GB 
[11/23 07:26:04 visual_prompt]: 	Training 200/553. train loss: 27.9482,	1.0840 s / batch. (data: 2.41e-01). ETA=16:15:31, max mem: 20.9 GB 
[11/23 07:27:40 visual_prompt]: 	Training 300/553. train loss: 10.3654,	0.8507 s / batch. (data: 2.95e-04). ETA=12:44:08, max mem: 20.9 GB 
[11/23 07:29:16 visual_prompt]: 	Training 400/553. train loss: 96.2913,	0.8327 s / batch. (data: 7.96e-03). ETA=12:26:33, max mem: 20.9 GB 
[11/23 07:30:53 visual_prompt]: 	Training 500/553. train loss: 67.0479,	1.0800 s / batch. (data: 2.54e-01). ETA=16:06:30, max mem: 20.9 GB 
[11/23 07:31:42 visual_prompt]: Epoch 3 / 100: avg data time: 1.33e-01, avg batch time: 0.9638, average train loss: 40.2478
[11/23 07:32:37 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3103, average loss: 14.2773
[11/23 07:32:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.73	
[11/23 07:32:37 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/23 07:34:18 visual_prompt]: 	Training 100/553. train loss: 16.8942,	0.8600 s / batch. (data: 3.05e-04). ETA=12:47:25, max mem: 20.9 GB 
[11/23 07:35:54 visual_prompt]: 	Training 200/553. train loss: 4.3071,	0.8400 s / batch. (data: 1.19e-02). ETA=12:28:09, max mem: 20.9 GB 
[11/23 07:37:31 visual_prompt]: 	Training 300/553. train loss: 86.3309,	1.1198 s / batch. (data: 3.01e-01). ETA=16:35:33, max mem: 20.9 GB 
[11/23 07:39:03 visual_prompt]: 	Training 400/553. train loss: 79.9603,	0.9092 s / batch. (data: 9.56e-02). ETA=13:26:45, max mem: 20.9 GB 
[11/23 07:40:41 visual_prompt]: 	Training 500/553. train loss: 0.0007,	3.3659 s / batch. (data: 2.55e+00). ETA=2 days, 1:41:07, max mem: 20.9 GB 
[11/23 07:41:32 visual_prompt]: Epoch 4 / 100: avg data time: 1.39e-01, avg batch time: 0.9677, average train loss: 63.0865
[11/23 07:42:27 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3106, average loss: 14.3394
[11/23 07:42:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.73	
[11/23 07:42:27 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/23 07:44:04 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8360 s / batch. (data: 3.08e-04). ETA=12:18:18, max mem: 20.9 GB 
[11/23 07:45:41 visual_prompt]: 	Training 200/553. train loss: 38.8275,	1.0153 s / batch. (data: 1.92e-01). ETA=14:54:58, max mem: 20.9 GB 
[11/23 07:47:18 visual_prompt]: 	Training 300/553. train loss: 151.9556,	0.8240 s / batch. (data: 5.37e-03). ETA=12:04:57, max mem: 20.9 GB 
[11/23 07:48:53 visual_prompt]: 	Training 400/553. train loss: 228.9831,	0.8320 s / batch. (data: 2.81e-04). ETA=12:10:35, max mem: 20.9 GB 
[11/23 07:50:30 visual_prompt]: 	Training 500/553. train loss: 162.6530,	0.8236 s / batch. (data: 7.96e-03). ETA=12:01:50, max mem: 20.9 GB 
[11/23 07:51:21 visual_prompt]: Epoch 5 / 100: avg data time: 1.36e-01, avg batch time: 0.9655, average train loss: 59.6810
[11/23 07:52:15 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3104, average loss: 53.9044
[11/23 07:52:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.30	
[11/23 07:52:15 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/23 07:53:57 visual_prompt]: 	Training 100/553. train loss: 39.3588,	0.8400 s / batch. (data: 7.68e-04). ETA=12:14:06, max mem: 20.9 GB 
[11/23 07:55:32 visual_prompt]: 	Training 200/553. train loss: 64.8807,	0.8320 s / batch. (data: 2.81e-04). ETA=12:05:43, max mem: 20.9 GB 
[11/23 07:57:07 visual_prompt]: 	Training 300/553. train loss: 19.3553,	0.8280 s / batch. (data: 3.62e-04). ETA=12:00:48, max mem: 20.9 GB 
[11/23 07:58:46 visual_prompt]: 	Training 400/553. train loss: 41.5402,	0.8278 s / batch. (data: 5.42e-03). ETA=11:59:19, max mem: 20.9 GB 
[11/23 08:00:21 visual_prompt]: 	Training 500/553. train loss: 67.3078,	0.8112 s / batch. (data: 3.15e-04). ETA=11:43:30, max mem: 20.9 GB 
[11/23 08:01:11 visual_prompt]: Epoch 6 / 100: avg data time: 1.39e-01, avg batch time: 0.9681, average train loss: 76.6691
[11/23 08:02:05 visual_prompt]: Inference (val):avg data time: 3.14e-04, avg batch time: 0.3110, average loss: 44.6269
[11/23 08:02:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.56	
[11/23 08:02:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/23 08:03:43 visual_prompt]: 	Training 100/553. train loss: 801.0915,	0.8322 s / batch. (data: 8.11e-03). ETA=11:59:36, max mem: 20.9 GB 
[11/23 08:05:20 visual_prompt]: 	Training 200/553. train loss: 8.7732,	0.8243 s / batch. (data: 1.47e-03). ETA=11:51:21, max mem: 20.9 GB 
[11/23 08:06:59 visual_prompt]: 	Training 300/553. train loss: 77.1621,	1.7385 s / batch. (data: 9.26e-01). ETA=1 day, 0:57:27, max mem: 20.9 GB 
[11/23 08:08:35 visual_prompt]: 	Training 400/553. train loss: 87.9645,	1.7440 s / batch. (data: 9.25e-01). ETA=1 day, 0:59:18, max mem: 20.9 GB 
[11/23 08:10:10 visual_prompt]: 	Training 500/553. train loss: 22.3223,	0.8598 s / batch. (data: 7.81e-03). ETA=12:17:46, max mem: 20.9 GB 
[11/23 08:10:58 visual_prompt]: Epoch 7 / 100: avg data time: 1.36e-01, avg batch time: 0.9632, average train loss: 118.3119
[11/23 08:11:53 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3103, average loss: 200.2819
[11/23 08:11:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.85	
[11/23 08:11:53 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/23 08:13:30 visual_prompt]: 	Training 100/553. train loss: 38.9741,	0.8318 s / batch. (data: 3.09e-04). ETA=11:51:36, max mem: 20.9 GB 
[11/23 08:15:08 visual_prompt]: 	Training 200/553. train loss: 10.0451,	0.8238 s / batch. (data: 3.38e-04). ETA=11:43:22, max mem: 20.9 GB 
[11/23 08:16:44 visual_prompt]: 	Training 300/553. train loss: 94.3741,	0.8314 s / batch. (data: 2.87e-04). ETA=11:48:28, max mem: 20.9 GB 
[11/23 08:18:20 visual_prompt]: 	Training 400/553. train loss: 1.2774,	0.8280 s / batch. (data: 3.02e-04). ETA=11:44:11, max mem: 20.9 GB 
[11/23 08:19:56 visual_prompt]: 	Training 500/553. train loss: 130.4558,	1.3377 s / batch. (data: 5.12e-01). ETA=18:55:28, max mem: 20.9 GB 
[11/23 08:20:46 visual_prompt]: Epoch 8 / 100: avg data time: 1.36e-01, avg batch time: 0.9638, average train loss: 105.7661
[11/23 08:21:40 visual_prompt]: Inference (val):avg data time: 3.70e-04, avg batch time: 0.3112, average loss: 36.9941
[11/23 08:21:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.93	
[11/23 08:21:40 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/23 08:23:18 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8323 s / batch. (data: 7.41e-04). ETA=11:44:18, max mem: 20.9 GB 
[11/23 08:24:54 visual_prompt]: 	Training 200/553. train loss: 0.3531,	0.8472 s / batch. (data: 1.11e-02). ETA=11:55:32, max mem: 20.9 GB 
[11/23 08:26:30 visual_prompt]: 	Training 300/553. train loss: 52.0310,	1.3080 s / batch. (data: 4.94e-01). ETA=18:22:32, max mem: 20.9 GB 
[11/23 08:28:07 visual_prompt]: 	Training 400/553. train loss: 45.0630,	0.8360 s / batch. (data: 1.20e-02). ETA=11:43:18, max mem: 20.9 GB 
[11/23 08:29:44 visual_prompt]: 	Training 500/553. train loss: 277.6619,	0.8510 s / batch. (data: 1.10e-02). ETA=11:54:30, max mem: 20.9 GB 
[11/23 08:30:33 visual_prompt]: Epoch 9 / 100: avg data time: 1.38e-01, avg batch time: 0.9644, average train loss: 126.2693
[11/23 08:31:28 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.3125, average loss: 152.5892
[11/23 08:31:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.18	
[11/23 08:31:28 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/23 08:33:09 visual_prompt]: 	Training 100/553. train loss: 187.3826,	0.8200 s / batch. (data: 4.54e-04). ETA=11:26:21, max mem: 20.9 GB 
[11/23 08:34:44 visual_prompt]: 	Training 200/553. train loss: 37.7419,	0.8080 s / batch. (data: 3.13e-04). ETA=11:14:58, max mem: 20.9 GB 
[11/23 08:36:19 visual_prompt]: 	Training 300/553. train loss: 90.5083,	0.8266 s / batch. (data: 3.11e-04). ETA=11:29:10, max mem: 20.9 GB 
[11/23 08:37:53 visual_prompt]: 	Training 400/553. train loss: 265.5869,	0.8280 s / batch. (data: 2.88e-04). ETA=11:28:56, max mem: 20.9 GB 
[11/23 08:39:30 visual_prompt]: 	Training 500/553. train loss: 52.3765,	0.8399 s / batch. (data: 1.05e-02). ETA=11:37:27, max mem: 20.9 GB 
[11/23 08:40:21 visual_prompt]: Epoch 10 / 100: avg data time: 1.37e-01, avg batch time: 0.9642, average train loss: 146.8459
[11/23 08:41:16 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3100, average loss: 202.4706
[11/23 08:41:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.96	
[11/23 08:41:16 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/23 08:42:57 visual_prompt]: 	Training 100/553. train loss: 170.1281,	0.8349 s / batch. (data: 3.01e-04). ETA=11:31:10, max mem: 20.9 GB 
[11/23 08:44:34 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8176 s / batch. (data: 5.40e-03). ETA=11:15:30, max mem: 20.9 GB 
[11/23 08:46:10 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.0155 s / batch. (data: 1.16e+00). ETA=1 day, 3:41:47, max mem: 20.9 GB 
[11/23 08:47:44 visual_prompt]: 	Training 400/553. train loss: 144.2417,	0.8201 s / batch. (data: 2.87e-04). ETA=11:14:48, max mem: 20.9 GB 
[11/23 08:49:19 visual_prompt]: 	Training 500/553. train loss: 227.5852,	0.8320 s / batch. (data: 2.77e-04). ETA=11:23:14, max mem: 20.9 GB 
[11/23 08:50:08 visual_prompt]: Epoch 11 / 100: avg data time: 1.36e-01, avg batch time: 0.9631, average train loss: 167.5750
[11/23 08:51:03 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3099, average loss: 24.0446
[11/23 08:51:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.40	
[11/23 08:51:03 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/23 08:52:44 visual_prompt]: 	Training 100/553. train loss: 120.9846,	0.8224 s / batch. (data: 2.87e-04). ETA=11:13:13, max mem: 20.9 GB 
[11/23 08:54:21 visual_prompt]: 	Training 200/553. train loss: 51.3779,	1.1440 s / batch. (data: 3.22e-01). ETA=15:34:34, max mem: 20.9 GB 
[11/23 08:55:55 visual_prompt]: 	Training 300/553. train loss: 18.5274,	0.8640 s / batch. (data: 7.95e-03). ETA=11:44:23, max mem: 20.9 GB 
[11/23 08:57:32 visual_prompt]: 	Training 400/553. train loss: 151.8916,	0.8280 s / batch. (data: 3.12e-04). ETA=11:13:39, max mem: 20.9 GB 
[11/23 08:59:07 visual_prompt]: 	Training 500/553. train loss: 212.2830,	0.8394 s / batch. (data: 5.41e-03). ETA=11:21:34, max mem: 20.9 GB 
[11/23 08:59:57 visual_prompt]: Epoch 12 / 100: avg data time: 1.37e-01, avg batch time: 0.9653, average train loss: 174.7716
[11/23 09:00:51 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3115, average loss: 324.7250
[11/23 09:00:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.69	
[11/23 09:00:51 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/23 09:02:33 visual_prompt]: 	Training 100/553. train loss: 46.6780,	0.8428 s / batch. (data: 7.99e-03). ETA=11:22:08, max mem: 20.9 GB 
[11/23 09:04:06 visual_prompt]: 	Training 200/553. train loss: 108.3145,	0.8105 s / batch. (data: 2.99e-04). ETA=10:54:38, max mem: 20.9 GB 
[11/23 09:05:43 visual_prompt]: 	Training 300/553. train loss: 56.2402,	1.5774 s / batch. (data: 7.48e-01). ETA=21:11:31, max mem: 20.9 GB 
[11/23 09:07:17 visual_prompt]: 	Training 400/553. train loss: 401.1780,	0.8328 s / batch. (data: 3.50e-04). ETA=11:09:54, max mem: 20.9 GB 
[11/23 09:08:54 visual_prompt]: 	Training 500/553. train loss: 278.3896,	0.8426 s / batch. (data: 2.69e-04). ETA=11:16:24, max mem: 20.9 GB 
[11/23 09:09:45 visual_prompt]: Epoch 13 / 100: avg data time: 1.38e-01, avg batch time: 0.9639, average train loss: 174.0920
[11/23 09:10:39 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3109, average loss: 44.2031
[11/23 09:10:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 50.94	
[11/23 09:10:39 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/23 09:12:20 visual_prompt]: 	Training 100/553. train loss: 1101.5603,	0.8292 s / batch. (data: 2.91e-04). ETA=11:03:31, max mem: 20.9 GB 
[11/23 09:13:55 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8252 s / batch. (data: 1.29e-02). ETA=10:58:56, max mem: 20.9 GB 
[11/23 09:15:32 visual_prompt]: 	Training 300/553. train loss: 54.3812,	0.8160 s / batch. (data: 2.83e-04). ETA=10:50:15, max mem: 20.9 GB 
[11/23 09:17:07 visual_prompt]: 	Training 400/553. train loss: 266.3464,	0.8360 s / batch. (data: 3.16e-04). ETA=11:04:45, max mem: 20.9 GB 
[11/23 09:18:43 visual_prompt]: 	Training 500/553. train loss: 163.6314,	0.8150 s / batch. (data: 3.02e-04). ETA=10:46:42, max mem: 20.9 GB 
[11/23 09:19:33 visual_prompt]: Epoch 14 / 100: avg data time: 1.39e-01, avg batch time: 0.9654, average train loss: 158.8360
[11/23 09:20:28 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3096, average loss: 98.8413
[11/23 09:20:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.20	
[11/23 09:20:28 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/23 09:22:07 visual_prompt]: 	Training 100/553. train loss: 106.0826,	0.8440 s / batch. (data: 3.07e-04). ETA=11:07:35, max mem: 20.9 GB 
[11/23 09:23:42 visual_prompt]: 	Training 200/553. train loss: 797.1801,	0.8247 s / batch. (data: 2.95e-04). ETA=10:50:55, max mem: 20.9 GB 
[11/23 09:25:20 visual_prompt]: 	Training 300/553. train loss: 390.2479,	0.8745 s / batch. (data: 1.03e-02). ETA=11:28:48, max mem: 20.9 GB 
[11/23 09:26:54 visual_prompt]: 	Training 400/553. train loss: 66.4679,	1.0240 s / batch. (data: 1.89e-01). ETA=13:24:48, max mem: 20.9 GB 
[11/23 09:28:30 visual_prompt]: 	Training 500/553. train loss: 56.4750,	0.8400 s / batch. (data: 2.97e-04). ETA=10:58:49, max mem: 20.9 GB 
[11/23 09:29:22 visual_prompt]: Epoch 15 / 100: avg data time: 1.38e-01, avg batch time: 0.9653, average train loss: 165.2661
[11/23 09:30:16 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3098, average loss: 518.4259
[11/23 09:30:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.15	
[11/23 09:30:16 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/23 09:31:55 visual_prompt]: 	Training 100/553. train loss: 103.7124,	0.8360 s / batch. (data: 3.00e-04). ETA=10:53:33, max mem: 20.9 GB 
[11/23 09:33:31 visual_prompt]: 	Training 200/553. train loss: 263.4484,	0.8521 s / batch. (data: 8.94e-03). ETA=11:04:41, max mem: 20.9 GB 
[11/23 09:35:08 visual_prompt]: 	Training 300/553. train loss: 192.0815,	0.8283 s / batch. (data: 1.01e-03). ETA=10:44:44, max mem: 20.9 GB 
[11/23 09:36:43 visual_prompt]: 	Training 400/553. train loss: 154.5278,	0.8498 s / batch. (data: 2.76e-04). ETA=11:00:02, max mem: 20.9 GB 
[11/23 09:38:18 visual_prompt]: 	Training 500/553. train loss: 77.0727,	1.0193 s / batch. (data: 2.01e-01). ETA=13:10:01, max mem: 20.9 GB 
[11/23 09:39:09 visual_prompt]: Epoch 16 / 100: avg data time: 1.37e-01, avg batch time: 0.9633, average train loss: 160.8979
[11/23 09:40:04 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3099, average loss: 42.0164
[11/23 09:40:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 51.17	
[11/23 09:40:04 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/23 09:41:43 visual_prompt]: 	Training 100/553. train loss: 53.9767,	0.8560 s / batch. (data: 7.97e-03). ETA=11:01:17, max mem: 20.9 GB 
[11/23 09:43:20 visual_prompt]: 	Training 200/553. train loss: 253.7905,	0.8360 s / batch. (data: 7.96e-03). ETA=10:44:26, max mem: 20.9 GB 
[11/23 09:44:56 visual_prompt]: 	Training 300/553. train loss: 176.7908,	0.8551 s / batch. (data: 2.96e-04). ETA=10:57:43, max mem: 20.9 GB 
[11/23 09:46:31 visual_prompt]: 	Training 400/553. train loss: 53.5881,	0.8242 s / batch. (data: 1.66e-02). ETA=10:32:36, max mem: 20.9 GB 
[11/23 09:48:06 visual_prompt]: 	Training 500/553. train loss: 101.5773,	1.0971 s / batch. (data: 2.74e-01). ETA=14:00:15, max mem: 20.9 GB 
[11/23 09:48:58 visual_prompt]: Epoch 17 / 100: avg data time: 1.38e-01, avg batch time: 0.9657, average train loss: 177.7189
[11/23 09:49:53 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3111, average loss: 180.0793
[11/23 09:49:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.05	
[11/23 09:49:53 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/23 09:51:33 visual_prompt]: 	Training 100/553. train loss: 42.3787,	0.8400 s / batch. (data: 2.93e-04). ETA=10:41:10, max mem: 20.9 GB 
[11/23 09:53:11 visual_prompt]: 	Training 200/553. train loss: 41.7151,	0.8360 s / batch. (data: 2.93e-04). ETA=10:36:44, max mem: 20.9 GB 
[11/23 09:54:47 visual_prompt]: 	Training 300/553. train loss: 76.5457,	0.8157 s / batch. (data: 3.09e-04). ETA=10:19:55, max mem: 20.9 GB 
[11/23 09:56:22 visual_prompt]: 	Training 400/553. train loss: 110.9185,	0.8106 s / batch. (data: 2.90e-04). ETA=10:14:40, max mem: 20.9 GB 
[11/23 09:57:57 visual_prompt]: 	Training 500/553. train loss: 114.4731,	0.8108 s / batch. (data: 2.75e-04). ETA=10:13:29, max mem: 20.9 GB 
[11/23 09:58:46 visual_prompt]: Epoch 18 / 100: avg data time: 1.37e-01, avg batch time: 0.9647, average train loss: 154.6931
[11/23 09:59:41 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3091, average loss: 218.2068
[11/23 09:59:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.01	
[11/23 09:59:41 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/23 10:01:22 visual_prompt]: 	Training 100/553. train loss: 100.4358,	0.8178 s / batch. (data: 3.82e-04). ETA=10:16:42, max mem: 20.9 GB 
[11/23 10:02:59 visual_prompt]: 	Training 200/553. train loss: 326.8077,	0.8345 s / batch. (data: 2.97e-04). ETA=10:27:54, max mem: 20.9 GB 
[11/23 10:04:37 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8289 s / batch. (data: 5.08e-04). ETA=10:22:18, max mem: 20.9 GB 
[11/23 10:06:16 visual_prompt]: 	Training 400/553. train loss: 211.2019,	0.8161 s / batch. (data: 8.17e-04). ETA=10:11:22, max mem: 20.9 GB 
[11/23 10:07:48 visual_prompt]: 	Training 500/553. train loss: 568.7255,	0.8237 s / batch. (data: 3.23e-04). ETA=10:15:38, max mem: 20.9 GB 
[11/23 10:08:39 visual_prompt]: Epoch 19 / 100: avg data time: 1.46e-01, avg batch time: 0.9729, average train loss: 181.0746
[11/23 10:09:34 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3104, average loss: 410.1010
[11/23 10:09:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 36.76	
[11/23 10:09:34 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/23 10:11:12 visual_prompt]: 	Training 100/553. train loss: 10.1556,	0.8734 s / batch. (data: 2.14e-02). ETA=10:50:33, max mem: 20.9 GB 
[11/23 10:12:50 visual_prompt]: 	Training 200/553. train loss: 81.5022,	0.8815 s / batch. (data: 2.54e-02). ETA=10:55:06, max mem: 20.9 GB 
[11/23 10:14:26 visual_prompt]: 	Training 300/553. train loss: 175.6327,	0.8247 s / batch. (data: 5.40e-03). ETA=10:11:33, max mem: 20.9 GB 
[11/23 10:16:02 visual_prompt]: 	Training 400/553. train loss: 416.4282,	0.8118 s / batch. (data: 2.97e-04). ETA=10:00:39, max mem: 20.9 GB 
[11/23 10:17:37 visual_prompt]: 	Training 500/553. train loss: 294.9010,	0.8480 s / batch. (data: 2.97e-04). ETA=10:25:59, max mem: 20.9 GB 
[11/23 10:18:29 visual_prompt]: Epoch 20 / 100: avg data time: 1.40e-01, avg batch time: 0.9663, average train loss: 159.3108
[11/23 10:19:24 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3124, average loss: 33.0233
[11/23 10:19:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.73	
[11/23 10:19:24 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/23 10:21:05 visual_prompt]: 	Training 100/553. train loss: 51.4833,	0.8240 s / batch. (data: 3.01e-04). ETA=10:06:11, max mem: 20.9 GB 
[11/23 10:22:41 visual_prompt]: 	Training 200/553. train loss: 244.3890,	0.8336 s / batch. (data: 5.39e-03). ETA=10:11:51, max mem: 20.9 GB 
[11/23 10:24:18 visual_prompt]: 	Training 300/553. train loss: 32.4769,	0.9379 s / batch. (data: 1.31e-01). ETA=11:26:51, max mem: 20.9 GB 
[11/23 10:25:54 visual_prompt]: 	Training 400/553. train loss: 14.3152,	0.8360 s / batch. (data: 3.15e-04). ETA=10:10:51, max mem: 20.9 GB 
[11/23 10:27:34 visual_prompt]: 	Training 500/553. train loss: 38.1298,	0.8505 s / batch. (data: 2.23e-02). ETA=10:20:00, max mem: 20.9 GB 
[11/23 10:28:24 visual_prompt]: Epoch 21 / 100: avg data time: 1.50e-01, avg batch time: 0.9763, average train loss: 169.9309
[11/23 10:29:19 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3104, average loss: 123.6120
[11/23 10:29:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.34	
[11/23 10:29:19 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/23 10:30:57 visual_prompt]: 	Training 100/553. train loss: 72.6496,	0.8360 s / batch. (data: 1.05e-02). ETA=10:07:17, max mem: 20.9 GB 
[11/23 10:32:34 visual_prompt]: 	Training 200/553. train loss: 85.5240,	0.8120 s / batch. (data: 2.92e-04). ETA=9:48:31, max mem: 20.9 GB 
[11/23 10:34:08 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8309 s / batch. (data: 3.07e-04). ETA=10:00:48, max mem: 20.9 GB 
[11/23 10:35:46 visual_prompt]: 	Training 400/553. train loss: 131.4046,	0.8580 s / batch. (data: 2.21e-02). ETA=10:18:58, max mem: 20.9 GB 
[11/23 10:37:22 visual_prompt]: 	Training 500/553. train loss: 29.0696,	0.8360 s / batch. (data: 3.18e-04). ETA=10:01:44, max mem: 20.9 GB 
[11/23 10:38:14 visual_prompt]: Epoch 22 / 100: avg data time: 1.41e-01, avg batch time: 0.9675, average train loss: 160.6525
[11/23 10:39:09 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3104, average loss: 23.9632
[11/23 10:39:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.20	
[11/23 10:39:09 visual_prompt]: Best epoch 22: best metric: -23.963
[11/23 10:39:09 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/23 10:40:51 visual_prompt]: 	Training 100/553. train loss: 80.6201,	0.8450 s / batch. (data: 8.98e-03). ETA=10:06:04, max mem: 20.9 GB 
[11/23 10:42:29 visual_prompt]: 	Training 200/553. train loss: 155.8318,	0.8209 s / batch. (data: 3.15e-04). ETA=9:47:23, max mem: 20.9 GB 
[11/23 10:44:07 visual_prompt]: 	Training 300/553. train loss: 79.0297,	0.8285 s / batch. (data: 1.05e-02). ETA=9:51:29, max mem: 20.9 GB 
[11/23 10:45:41 visual_prompt]: 	Training 400/553. train loss: 192.9761,	0.8480 s / batch. (data: 7.61e-04). ETA=10:03:57, max mem: 20.9 GB 
[11/23 10:47:15 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8155 s / batch. (data: 3.26e-04). ETA=9:39:29, max mem: 20.9 GB 
[11/23 10:48:06 visual_prompt]: Epoch 23 / 100: avg data time: 1.43e-01, avg batch time: 0.9701, average train loss: 154.6760
[11/23 10:49:01 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3099, average loss: 340.3469
[11/23 10:49:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.29	
[11/23 10:49:01 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/23 10:50:39 visual_prompt]: 	Training 100/553. train loss: 2.7256,	0.8897 s / batch. (data: 6.53e-02). ETA=10:29:55, max mem: 20.9 GB 
[11/23 10:52:14 visual_prompt]: 	Training 200/553. train loss: 166.2729,	0.8520 s / batch. (data: 1.19e-02). ETA=10:01:46, max mem: 20.9 GB 
[11/23 10:53:50 visual_prompt]: 	Training 300/553. train loss: 232.3090,	0.8653 s / batch. (data: 4.81e-02). ETA=10:09:46, max mem: 20.9 GB 
[11/23 10:55:28 visual_prompt]: 	Training 400/553. train loss: 35.5179,	0.8438 s / batch. (data: 4.43e-04). ETA=9:53:13, max mem: 20.9 GB 
[11/23 10:57:06 visual_prompt]: 	Training 500/553. train loss: 125.8313,	0.8100 s / batch. (data: 3.50e-04). ETA=9:28:06, max mem: 20.9 GB 
[11/23 10:57:57 visual_prompt]: Epoch 24 / 100: avg data time: 1.43e-01, avg batch time: 0.9697, average train loss: 161.9279
[11/23 10:58:52 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3096, average loss: 42.9661
[11/23 10:58:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.48	
[11/23 10:58:52 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/23 11:00:36 visual_prompt]: 	Training 100/553. train loss: 334.7375,	0.8480 s / batch. (data: 3.05e-04). ETA=9:52:35, max mem: 20.9 GB 
[11/23 11:02:10 visual_prompt]: 	Training 200/553. train loss: 7.2485,	0.8811 s / batch. (data: 3.32e-02). ETA=10:14:16, max mem: 20.9 GB 
[11/23 11:03:45 visual_prompt]: 	Training 300/553. train loss: 403.8047,	0.9168 s / batch. (data: 8.33e-02). ETA=10:37:37, max mem: 20.9 GB 
[11/23 11:05:22 visual_prompt]: 	Training 400/553. train loss: 52.0730,	1.1196 s / batch. (data: 3.10e-01). ETA=12:56:46, max mem: 20.9 GB 
[11/23 11:07:00 visual_prompt]: 	Training 500/553. train loss: 298.8242,	1.3680 s / batch. (data: 5.29e-01). ETA=15:46:49, max mem: 20.9 GB 
[11/23 11:07:50 visual_prompt]: Epoch 25 / 100: avg data time: 1.45e-01, avg batch time: 0.9722, average train loss: 149.2566
[11/23 11:08:45 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3089, average loss: 127.0611
[11/23 11:08:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.03	
[11/23 11:08:45 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/23 11:10:26 visual_prompt]: 	Training 100/553. train loss: 256.6980,	0.8640 s / batch. (data: 7.97e-03). ETA=9:55:47, max mem: 20.9 GB 
[11/23 11:12:04 visual_prompt]: 	Training 200/553. train loss: 16.0175,	1.7610 s / batch. (data: 9.50e-01). ETA=20:11:27, max mem: 20.9 GB 
[11/23 11:13:42 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8026 s / batch. (data: 5.95e-04). ETA=9:10:48, max mem: 20.9 GB 
[11/23 11:15:18 visual_prompt]: 	Training 400/553. train loss: 180.0629,	0.8143 s / batch. (data: 3.00e-04). ETA=9:17:27, max mem: 20.9 GB 
[11/23 11:16:53 visual_prompt]: 	Training 500/553. train loss: 32.3860,	0.8161 s / batch. (data: 8.47e-03). ETA=9:17:18, max mem: 20.9 GB 
[11/23 11:17:42 visual_prompt]: Epoch 26 / 100: avg data time: 1.44e-01, avg batch time: 0.9704, average train loss: 159.3604
[11/23 11:18:37 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3099, average loss: 10.0895
[11/23 11:18:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 55.72	
[11/23 11:18:37 visual_prompt]: Best epoch 26: best metric: -10.089
[11/23 11:18:37 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/23 11:20:18 visual_prompt]: 	Training 100/553. train loss: 340.0575,	0.8476 s / batch. (data: 5.41e-03). ETA=9:36:41, max mem: 20.9 GB 
[11/23 11:21:54 visual_prompt]: 	Training 200/553. train loss: 360.8433,	1.0433 s / batch. (data: 2.09e-01). ETA=11:48:03, max mem: 20.9 GB 
[11/23 11:23:31 visual_prompt]: 	Training 300/553. train loss: 7.3428,	0.8450 s / batch. (data: 9.02e-03). ETA=9:32:06, max mem: 20.9 GB 
[11/23 11:25:08 visual_prompt]: 	Training 400/553. train loss: 24.9038,	0.8400 s / batch. (data: 7.50e-04). ETA=9:27:18, max mem: 20.9 GB 
[11/23 11:26:45 visual_prompt]: 	Training 500/553. train loss: 30.3918,	0.8326 s / batch. (data: 8.05e-04). ETA=9:20:55, max mem: 20.9 GB 
[11/23 11:27:34 visual_prompt]: Epoch 27 / 100: avg data time: 1.43e-01, avg batch time: 0.9706, average train loss: 136.9341
[11/23 11:28:29 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3112, average loss: 98.2299
[11/23 11:28:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.05	
[11/23 11:28:29 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/23 11:30:08 visual_prompt]: 	Training 100/553. train loss: 128.2742,	0.8336 s / batch. (data: 2.82e-04). ETA=9:19:28, max mem: 20.9 GB 
[11/23 11:31:44 visual_prompt]: 	Training 200/553. train loss: 186.6500,	0.8425 s / batch. (data: 3.09e-04). ETA=9:24:02, max mem: 20.9 GB 
[11/23 11:33:21 visual_prompt]: 	Training 300/553. train loss: 20.4521,	1.3012 s / batch. (data: 4.52e-01). ETA=14:28:58, max mem: 20.9 GB 
[11/23 11:34:56 visual_prompt]: 	Training 400/553. train loss: 116.6639,	0.8550 s / batch. (data: 5.89e-03). ETA=9:29:32, max mem: 20.9 GB 
[11/23 11:36:32 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8280 s / batch. (data: 2.84e-04). ETA=9:10:11, max mem: 20.9 GB 
[11/23 11:37:23 visual_prompt]: Epoch 28 / 100: avg data time: 1.39e-01, avg batch time: 0.9656, average train loss: 154.5328
[11/23 11:38:17 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3098, average loss: 47.1506
[11/23 11:38:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.10	
[11/23 11:38:17 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/23 11:40:03 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8361 s / batch. (data: 9.00e-03). ETA=9:13:25, max mem: 20.9 GB 
[11/23 11:41:39 visual_prompt]: 	Training 200/553. train loss: 33.9879,	1.5241 s / batch. (data: 6.96e-01). ETA=16:46:19, max mem: 20.9 GB 
[11/23 11:43:14 visual_prompt]: 	Training 300/553. train loss: 181.5701,	0.8360 s / batch. (data: 7.65e-04). ETA=9:10:35, max mem: 20.9 GB 
[11/23 11:44:48 visual_prompt]: 	Training 400/553. train loss: 188.7184,	1.2192 s / batch. (data: 4.02e-01). ETA=13:20:56, max mem: 20.9 GB 
[11/23 11:46:25 visual_prompt]: 	Training 500/553. train loss: 131.5021,	0.8440 s / batch. (data: 1.05e-02). ETA=9:13:01, max mem: 20.9 GB 
[11/23 11:47:15 visual_prompt]: Epoch 29 / 100: avg data time: 1.44e-01, avg batch time: 0.9717, average train loss: 165.1326
[11/23 11:48:09 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3103, average loss: 137.0830
[11/23 11:48:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.47	
[11/23 11:48:09 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/23 11:49:48 visual_prompt]: 	Training 100/553. train loss: 137.3621,	0.8389 s / batch. (data: 5.41e-03). ETA=9:07:34, max mem: 20.9 GB 
[11/23 11:51:26 visual_prompt]: 	Training 200/553. train loss: 293.8915,	0.8185 s / batch. (data: 3.05e-04). ETA=8:52:53, max mem: 20.9 GB 
[11/23 11:53:01 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.2413 s / batch. (data: 3.92e-01). ETA=13:26:05, max mem: 20.9 GB 
[11/23 11:54:39 visual_prompt]: 	Training 400/553. train loss: 48.6005,	1.0164 s / batch. (data: 2.00e-01). ETA=10:58:18, max mem: 20.9 GB 
[11/23 11:56:13 visual_prompt]: 	Training 500/553. train loss: 15.0019,	1.2680 s / batch. (data: 4.36e-01). ETA=13:39:11, max mem: 20.9 GB 
[11/23 11:57:06 visual_prompt]: Epoch 30 / 100: avg data time: 1.43e-01, avg batch time: 0.9695, average train loss: 152.4408
[11/23 11:58:01 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3100, average loss: 126.2633
[11/23 11:58:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.01	
[11/23 11:58:01 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[11/23 11:59:43 visual_prompt]: 	Training 100/553. train loss: 115.0993,	0.8401 s / batch. (data: 3.34e-03). ETA=9:00:34, max mem: 20.9 GB 
[11/23 12:01:22 visual_prompt]: 	Training 200/553. train loss: 250.0156,	0.8646 s / batch. (data: 5.41e-03). ETA=9:14:54, max mem: 20.9 GB 
[11/23 12:02:56 visual_prompt]: 	Training 300/553. train loss: 112.1726,	0.8251 s / batch. (data: 3.19e-04). ETA=8:48:13, max mem: 20.9 GB 
[11/23 12:04:32 visual_prompt]: 	Training 400/553. train loss: 204.9033,	1.1838 s / batch. (data: 3.56e-01). ETA=12:35:51, max mem: 20.9 GB 
[11/23 12:06:10 visual_prompt]: 	Training 500/553. train loss: 57.7240,	0.8295 s / batch. (data: 5.41e-03). ETA=8:48:14, max mem: 20.9 GB 
[11/23 12:07:00 visual_prompt]: Epoch 31 / 100: avg data time: 1.49e-01, avg batch time: 0.9751, average train loss: 147.3782
[11/23 12:07:56 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3076, average loss: 66.5390
[11/23 12:07:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.88	
[11/23 12:07:56 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[11/23 12:09:38 visual_prompt]: 	Training 100/553. train loss: 63.9537,	0.8545 s / batch. (data: 1.10e-02). ETA=9:02:00, max mem: 20.9 GB 
[11/23 12:11:14 visual_prompt]: 	Training 200/553. train loss: 80.4546,	0.8188 s / batch. (data: 7.71e-04). ETA=8:37:57, max mem: 20.9 GB 
[11/23 12:12:53 visual_prompt]: 	Training 300/553. train loss: 192.4452,	0.8304 s / batch. (data: 2.06e-02). ETA=8:43:54, max mem: 20.9 GB 
[11/23 12:14:30 visual_prompt]: 	Training 400/553. train loss: 4.1298,	0.8419 s / batch. (data: 3.16e-04). ETA=8:49:47, max mem: 20.9 GB 
[11/23 12:16:04 visual_prompt]: 	Training 500/553. train loss: 370.3149,	0.8362 s / batch. (data: 7.30e-03). ETA=8:44:49, max mem: 20.9 GB 
[11/23 12:16:52 visual_prompt]: Epoch 32 / 100: avg data time: 1.43e-01, avg batch time: 0.9703, average train loss: 144.6342
[11/23 12:17:47 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3096, average loss: 325.2564
[11/23 12:17:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.09	
[11/23 12:17:47 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[11/23 12:19:27 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8200 s / batch. (data: 2.96e-04). ETA=8:32:32, max mem: 20.9 GB 
[11/23 12:21:06 visual_prompt]: 	Training 200/553. train loss: 107.0186,	0.8549 s / batch. (data: 2.17e-02). ETA=8:52:57, max mem: 20.9 GB 
[11/23 12:22:44 visual_prompt]: 	Training 300/553. train loss: 104.5602,	0.8407 s / batch. (data: 7.99e-03). ETA=8:42:40, max mem: 20.9 GB 
[11/23 12:24:22 visual_prompt]: 	Training 400/553. train loss: 18.8342,	0.8509 s / batch. (data: 2.06e-02). ETA=8:47:37, max mem: 20.9 GB 
[11/23 12:25:59 visual_prompt]: 	Training 500/553. train loss: 92.1204,	0.8280 s / batch. (data: 4.79e-04). ETA=8:32:00, max mem: 20.9 GB 
[11/23 12:26:49 visual_prompt]: Epoch 33 / 100: avg data time: 1.53e-01, avg batch time: 0.9797, average train loss: 163.7719
[11/23 12:27:45 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3101, average loss: 33.5294
[11/23 12:27:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.21	
[11/23 12:27:45 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[11/23 12:29:28 visual_prompt]: 	Training 100/553. train loss: 100.4352,	0.8200 s / batch. (data: 8.00e-03). ETA=8:24:58, max mem: 20.9 GB 
[11/23 12:31:03 visual_prompt]: 	Training 200/553. train loss: 168.7859,	0.8317 s / batch. (data: 3.23e-04). ETA=8:30:49, max mem: 20.9 GB 
[11/23 12:32:39 visual_prompt]: 	Training 300/553. train loss: 56.7570,	0.8316 s / batch. (data: 5.43e-03). ETA=8:29:23, max mem: 20.9 GB 
[11/23 12:34:16 visual_prompt]: 	Training 400/553. train loss: 172.6722,	0.8362 s / batch. (data: 3.00e-04). ETA=8:30:47, max mem: 20.9 GB 
[11/23 12:35:53 visual_prompt]: 	Training 500/553. train loss: 72.8756,	1.2713 s / batch. (data: 4.40e-01). ETA=12:54:26, max mem: 20.9 GB 
[11/23 12:36:44 visual_prompt]: Epoch 34 / 100: avg data time: 1.47e-01, avg batch time: 0.9744, average train loss: 139.0871
[11/23 12:37:39 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3123, average loss: 100.6480
[11/23 12:37:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.71	
[11/23 12:37:39 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[11/23 12:39:22 visual_prompt]: 	Training 100/553. train loss: 222.7979,	0.8320 s / batch. (data: 3.02e-04). ETA=8:24:43, max mem: 20.9 GB 
[11/23 12:41:01 visual_prompt]: 	Training 200/553. train loss: 64.9297,	0.8080 s / batch. (data: 4.02e-04). ETA=8:08:48, max mem: 20.9 GB 
[11/23 12:42:36 visual_prompt]: 	Training 300/553. train loss: 56.2827,	0.8520 s / batch. (data: 7.96e-03). ETA=8:34:01, max mem: 20.9 GB 
[11/23 12:44:12 visual_prompt]: 	Training 400/553. train loss: 203.8705,	0.8060 s / batch. (data: 3.65e-04). ETA=8:04:53, max mem: 20.9 GB 
[11/23 12:45:47 visual_prompt]: 	Training 500/553. train loss: 13.9455,	0.9844 s / batch. (data: 1.54e-01). ETA=9:50:37, max mem: 20.9 GB 
[11/23 12:46:39 visual_prompt]: Epoch 35 / 100: avg data time: 1.49e-01, avg batch time: 0.9762, average train loss: 121.9376
[11/23 12:47:35 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3105, average loss: 78.2520
[11/23 12:47:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.94	
[11/23 12:47:35 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[11/23 12:49:15 visual_prompt]: 	Training 100/553. train loss: 188.7446,	0.8159 s / batch. (data: 3.16e-04). ETA=8:07:24, max mem: 20.9 GB 
[11/23 12:50:53 visual_prompt]: 	Training 200/553. train loss: 150.9630,	0.8244 s / batch. (data: 5.41e-03). ETA=8:11:07, max mem: 20.9 GB 
[11/23 12:52:31 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8245 s / batch. (data: 2.85e-04). ETA=8:09:49, max mem: 20.9 GB 
[11/23 12:54:07 visual_prompt]: 	Training 400/553. train loss: 8.6037,	0.8156 s / batch. (data: 3.09e-04). ETA=8:03:09, max mem: 20.9 GB 
[11/23 12:55:45 visual_prompt]: 	Training 500/553. train loss: 187.1887,	0.9689 s / batch. (data: 1.53e-01). ETA=9:32:23, max mem: 20.9 GB 
[11/23 12:56:33 visual_prompt]: Epoch 36 / 100: avg data time: 1.46e-01, avg batch time: 0.9728, average train loss: 147.0550
[11/23 12:57:28 visual_prompt]: Inference (val):avg data time: 2.77e-04, avg batch time: 0.3132, average loss: 404.8266
[11/23 12:57:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.33	
[11/23 12:57:28 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[11/23 12:59:09 visual_prompt]: 	Training 100/553. train loss: 3.6021,	0.8320 s / batch. (data: 3.07e-04). ETA=8:09:22, max mem: 20.9 GB 
[11/23 13:00:46 visual_prompt]: 	Training 200/553. train loss: 141.7495,	0.8350 s / batch. (data: 5.49e-03). ETA=8:09:45, max mem: 20.9 GB 
[11/23 13:02:24 visual_prompt]: 	Training 300/553. train loss: 111.5356,	1.4640 s / batch. (data: 6.42e-01). ETA=14:16:15, max mem: 20.9 GB 
[11/23 13:04:02 visual_prompt]: 	Training 400/553. train loss: 268.0103,	1.7827 s / batch. (data: 9.39e-01). ETA=17:19:41, max mem: 20.9 GB 
[11/23 13:05:35 visual_prompt]: 	Training 500/553. train loss: 109.1986,	0.8428 s / batch. (data: 1.05e-02). ETA=8:10:06, max mem: 20.9 GB 
[11/23 13:06:27 visual_prompt]: Epoch 37 / 100: avg data time: 1.48e-01, avg batch time: 0.9746, average train loss: 144.4816
[11/23 13:07:23 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3099, average loss: 276.0451
[11/23 13:07:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.37	
[11/23 13:07:23 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[11/23 13:09:03 visual_prompt]: 	Training 100/553. train loss: 46.1960,	0.8078 s / batch. (data: 3.13e-04). ETA=7:47:42, max mem: 20.9 GB 
[11/23 13:10:40 visual_prompt]: 	Training 200/553. train loss: 26.8297,	0.8333 s / batch. (data: 5.45e-03). ETA=8:01:03, max mem: 20.9 GB 
[11/23 13:12:18 visual_prompt]: 	Training 300/553. train loss: 63.9591,	0.8057 s / batch. (data: 2.83e-04). ETA=7:43:49, max mem: 20.9 GB 
[11/23 13:13:54 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8280 s / batch. (data: 7.98e-03). ETA=7:55:15, max mem: 20.9 GB 
[11/23 13:15:34 visual_prompt]: 	Training 500/553. train loss: 132.2172,	0.8267 s / batch. (data: 3.31e-04). ETA=7:53:06, max mem: 20.9 GB 
[11/23 13:16:24 visual_prompt]: Epoch 38 / 100: avg data time: 1.50e-01, avg batch time: 0.9774, average train loss: 141.5284
[11/23 13:17:19 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3088, average loss: 468.5666
[11/23 13:17:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.83	
[11/23 13:17:19 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[11/23 13:18:58 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8083 s / batch. (data: 5.47e-03). ETA=7:40:31, max mem: 20.9 GB 
[11/23 13:20:38 visual_prompt]: 	Training 200/553. train loss: 318.9404,	0.8449 s / batch. (data: 2.53e-02). ETA=8:00:00, max mem: 20.9 GB 
[11/23 13:22:18 visual_prompt]: 	Training 300/553. train loss: 309.1207,	0.8453 s / batch. (data: 5.47e-03). ETA=7:58:47, max mem: 20.9 GB 
[11/23 13:23:52 visual_prompt]: 	Training 400/553. train loss: 135.6061,	0.8562 s / batch. (data: 3.52e-04). ETA=8:03:33, max mem: 20.9 GB 
[11/23 13:25:28 visual_prompt]: 	Training 500/553. train loss: 73.8458,	1.4671 s / batch. (data: 6.61e-01). ETA=13:46:08, max mem: 20.9 GB 
[11/23 13:26:17 visual_prompt]: Epoch 39 / 100: avg data time: 1.44e-01, avg batch time: 0.9717, average train loss: 140.0020
[11/23 13:27:12 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3111, average loss: 137.8621
[11/23 13:27:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.12	
[11/23 13:27:12 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[11/23 13:28:53 visual_prompt]: 	Training 100/553. train loss: 257.4938,	0.8280 s / batch. (data: 3.13e-04). ETA=7:44:08, max mem: 20.9 GB 
[11/23 13:30:29 visual_prompt]: 	Training 200/553. train loss: 12.0638,	0.8280 s / batch. (data: 3.04e-04). ETA=7:42:45, max mem: 20.9 GB 
[11/23 13:32:07 visual_prompt]: 	Training 300/553. train loss: 318.8055,	0.8520 s / batch. (data: 2.95e-04). ETA=7:54:45, max mem: 20.9 GB 
[11/23 13:33:44 visual_prompt]: 	Training 400/553. train loss: 26.2209,	0.8455 s / batch. (data: 1.10e-02). ETA=7:49:42, max mem: 20.9 GB 
[11/23 13:35:20 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8370 s / batch. (data: 2.50e-04). ETA=7:43:37, max mem: 20.9 GB 
[11/23 13:36:12 visual_prompt]: Epoch 40 / 100: avg data time: 1.50e-01, avg batch time: 0.9769, average train loss: 100.7834
[11/23 13:37:07 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3101, average loss: 243.3437
[11/23 13:37:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.03	
[11/23 13:37:07 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[11/23 13:38:51 visual_prompt]: 	Training 100/553. train loss: 50.4379,	0.8120 s / batch. (data: 2.89e-04). ETA=7:27:41, max mem: 20.9 GB 
[11/23 13:40:29 visual_prompt]: 	Training 200/553. train loss: 207.1020,	0.8214 s / batch. (data: 3.70e-04). ETA=7:31:28, max mem: 20.9 GB 
[11/23 13:42:05 visual_prompt]: 	Training 300/553. train loss: 78.5583,	0.8427 s / batch. (data: 5.42e-03). ETA=7:41:48, max mem: 20.9 GB 
[11/23 13:43:39 visual_prompt]: 	Training 400/553. train loss: 188.9894,	0.8272 s / batch. (data: 6.31e-04). ETA=7:31:54, max mem: 20.9 GB 
[11/23 13:45:14 visual_prompt]: 	Training 500/553. train loss: 154.2863,	0.8378 s / batch. (data: 2.94e-04). ETA=7:36:17, max mem: 20.9 GB 
[11/23 13:46:02 visual_prompt]: Epoch 41 / 100: avg data time: 1.41e-01, avg batch time: 0.9673, average train loss: 131.5228
[11/23 13:46:56 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3100, average loss: 192.0727
[11/23 13:46:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.73	
[11/23 13:46:56 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[11/23 13:48:35 visual_prompt]: 	Training 100/553. train loss: 341.8537,	0.8215 s / batch. (data: 3.03e-04). ETA=7:25:20, max mem: 20.9 GB 
[11/23 13:50:11 visual_prompt]: 	Training 200/553. train loss: 142.7461,	0.8282 s / batch. (data: 1.64e-02). ETA=7:27:36, max mem: 20.9 GB 
[11/23 13:51:48 visual_prompt]: 	Training 300/553. train loss: 368.0974,	0.8498 s / batch. (data: 5.42e-03). ETA=7:37:51, max mem: 20.9 GB 
[11/23 13:53:25 visual_prompt]: 	Training 400/553. train loss: 129.9272,	0.8206 s / batch. (data: 3.28e-04). ETA=7:20:46, max mem: 20.9 GB 
[11/23 13:55:00 visual_prompt]: 	Training 500/553. train loss: 389.5491,	0.8440 s / batch. (data: 2.98e-04). ETA=7:31:54, max mem: 20.9 GB 
[11/23 13:55:52 visual_prompt]: Epoch 42 / 100: avg data time: 1.40e-01, avg batch time: 0.9677, average train loss: 140.2088
[11/23 13:56:46 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3107, average loss: 221.4530
[11/23 13:56:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.79	
[11/23 13:56:46 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[11/23 13:58:28 visual_prompt]: 	Training 100/553. train loss: 251.0161,	0.8142 s / batch. (data: 2.97e-04). ETA=7:13:51, max mem: 20.9 GB 
[11/23 14:00:03 visual_prompt]: 	Training 200/553. train loss: 186.0584,	0.8188 s / batch. (data: 3.09e-04). ETA=7:14:59, max mem: 20.9 GB 
[11/23 14:01:38 visual_prompt]: 	Training 300/553. train loss: 259.0633,	0.8360 s / batch. (data: 2.97e-04). ETA=7:22:43, max mem: 20.9 GB 
[11/23 14:03:13 visual_prompt]: 	Training 400/553. train loss: 175.8335,	0.8240 s / batch. (data: 7.95e-03). ETA=7:14:58, max mem: 20.9 GB 
[11/23 14:04:50 visual_prompt]: 	Training 500/553. train loss: 142.0669,	0.8320 s / batch. (data: 3.19e-04). ETA=7:17:50, max mem: 20.9 GB 
[11/23 14:05:43 visual_prompt]: Epoch 43 / 100: avg data time: 1.42e-01, avg batch time: 0.9694, average train loss: 131.8388
[11/23 14:06:37 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3106, average loss: 335.8352
[11/23 14:06:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.73	
[11/23 14:06:37 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[11/23 14:08:18 visual_prompt]: 	Training 100/553. train loss: 98.0789,	1.0587 s / batch. (data: 2.54e-01). ETA=9:14:24, max mem: 20.9 GB 
[11/23 14:09:56 visual_prompt]: 	Training 200/553. train loss: 27.2970,	0.8232 s / batch. (data: 3.06e-04). ETA=7:09:42, max mem: 20.9 GB 
[11/23 14:11:31 visual_prompt]: 	Training 300/553. train loss: 110.8865,	0.8089 s / batch. (data: 2.99e-04). ETA=7:00:53, max mem: 20.9 GB 
[11/23 14:13:06 visual_prompt]: 	Training 400/553. train loss: 64.9772,	0.8288 s / batch. (data: 3.10e-04). ETA=7:09:53, max mem: 20.9 GB 
[11/23 14:14:43 visual_prompt]: 	Training 500/553. train loss: 48.6969,	0.8209 s / batch. (data: 3.21e-04). ETA=7:04:26, max mem: 20.9 GB 
[11/23 14:15:34 visual_prompt]: Epoch 44 / 100: avg data time: 1.44e-01, avg batch time: 0.9710, average train loss: 119.7253
[11/23 14:16:29 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3111, average loss: 10.1329
[11/23 14:16:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.18	
[11/23 14:16:29 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[11/23 14:18:11 visual_prompt]: 	Training 100/553. train loss: 13.1764,	0.8440 s / batch. (data: 3.06e-04). ETA=7:14:11, max mem: 20.9 GB 
[11/23 14:19:44 visual_prompt]: 	Training 200/553. train loss: 51.7602,	0.8280 s / batch. (data: 3.27e-04). ETA=7:04:35, max mem: 20.9 GB 
[11/23 14:21:22 visual_prompt]: 	Training 300/553. train loss: 23.4284,	0.8360 s / batch. (data: 3.02e-04). ETA=7:07:18, max mem: 20.9 GB 
[11/23 14:22:57 visual_prompt]: 	Training 400/553. train loss: 156.7916,	0.8165 s / batch. (data: 5.45e-03). ETA=6:55:58, max mem: 20.9 GB 
[11/23 14:24:36 visual_prompt]: 	Training 500/553. train loss: 70.1288,	0.8199 s / batch. (data: 3.80e-03). ETA=6:56:21, max mem: 20.9 GB 
[11/23 14:25:26 visual_prompt]: Epoch 45 / 100: avg data time: 1.44e-01, avg batch time: 0.9703, average train loss: 123.9202
[11/23 14:26:21 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3115, average loss: 114.6815
[11/23 14:26:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.27	
[11/23 14:26:21 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[11/23 14:28:02 visual_prompt]: 	Training 100/553. train loss: 184.4694,	1.0880 s / batch. (data: 2.69e-01). ETA=9:09:42, max mem: 20.9 GB 
[11/23 14:29:40 visual_prompt]: 	Training 200/553. train loss: 13.2624,	0.8560 s / batch. (data: 3.15e-04). ETA=7:11:03, max mem: 20.9 GB 
[11/23 14:31:14 visual_prompt]: 	Training 300/553. train loss: 361.9542,	0.8398 s / batch. (data: 3.27e-04). ETA=7:01:32, max mem: 20.9 GB 
[11/23 14:32:51 visual_prompt]: 	Training 400/553. train loss: 314.4479,	0.8103 s / batch. (data: 2.92e-04). ETA=6:45:20, max mem: 20.9 GB 
[11/23 14:34:26 visual_prompt]: 	Training 500/553. train loss: 120.6618,	0.8245 s / batch. (data: 2.74e-04). ETA=6:51:04, max mem: 20.9 GB 
[11/23 14:35:18 visual_prompt]: Epoch 46 / 100: avg data time: 1.42e-01, avg batch time: 0.9702, average train loss: 123.0109
[11/23 14:36:12 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3108, average loss: 189.0893
[11/23 14:36:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.63	
[11/23 14:36:12 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[11/23 14:37:54 visual_prompt]: 	Training 100/553. train loss: 34.3043,	0.8358 s / batch. (data: 2.80e-04). ETA=6:54:36, max mem: 20.9 GB 
[11/23 14:39:27 visual_prompt]: 	Training 200/553. train loss: 115.0741,	0.9440 s / batch. (data: 1.03e-01). ETA=7:46:41, max mem: 20.9 GB 
[11/23 14:41:05 visual_prompt]: 	Training 300/553. train loss: 101.0320,	0.8230 s / batch. (data: 2.84e-04). ETA=6:45:30, max mem: 20.9 GB 
[11/23 14:42:41 visual_prompt]: 	Training 400/553. train loss: 10.7620,	0.8240 s / batch. (data: 2.97e-04). ETA=6:44:36, max mem: 20.9 GB 
[11/23 14:44:17 visual_prompt]: 	Training 500/553. train loss: 19.4447,	0.8718 s / batch. (data: 3.05e-04). ETA=7:06:38, max mem: 20.9 GB 
[11/23 14:45:09 visual_prompt]: Epoch 47 / 100: avg data time: 1.43e-01, avg batch time: 0.9695, average train loss: 123.5316
[11/23 14:46:04 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3091, average loss: 139.5126
[11/23 14:46:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.60	
[11/23 14:46:04 visual_prompt]: Stopping early.
[11/23 14:46:06 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 14:46:06 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 14:46:06 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/23 14:46:06 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 14:46:06 visual_prompt]: Training with config:
[11/23 14:46:06 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/23 14:46:06 visual_prompt]: Loading training data...
[11/23 14:46:06 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 14:46:06 visual_prompt]: Loading validation data...
[11/23 14:46:06 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 14:46:07 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 14:46:16 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 14:46:16 visual_prompt]: tuned percent:0.525
[11/23 14:46:16 visual_prompt]: Device used for model: 0
[11/23 14:46:16 visual_prompt]: Setting up Evaluator...
[11/23 14:46:16 visual_prompt]: Setting up Trainer...
[11/23 14:46:16 visual_prompt]: 	Setting up the optimizer...
[11/23 14:46:16 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 14:47:59 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8197 s / batch. (data: 3.05e-04). ETA=12:34:05, max mem: 20.9 GB 
[11/23 14:49:34 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8320 s / batch. (data: 3.09e-04). ETA=12:44:03, max mem: 20.9 GB 
[11/23 14:51:13 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8600 s / batch. (data: 1.60e-02). ETA=13:08:19, max mem: 20.9 GB 
[11/23 14:52:49 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8311 s / batch. (data: 3.32e-04). ETA=12:40:29, max mem: 20.9 GB 
[11/23 14:54:28 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8480 s / batch. (data: 8.02e-03). ETA=12:54:31, max mem: 20.9 GB 
[11/23 14:55:19 visual_prompt]: Epoch 1 / 100: avg data time: 1.49e-01, avg batch time: 0.9820, average train loss: 1.5403
[11/23 14:56:15 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3105, average loss: 1.5201
[11/23 14:56:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 14:56:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/23 14:57:55 visual_prompt]: 	Training 100/553. train loss: 25.0325,	0.8695 s / batch. (data: 5.46e-03). ETA=13:11:57, max mem: 20.9 GB 
[11/23 14:59:31 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8240 s / batch. (data: 3.09e-04). ETA=12:29:07, max mem: 20.9 GB 
[11/23 15:01:10 visual_prompt]: 	Training 300/553. train loss: 7.4490,	1.0075 s / batch. (data: 1.76e-01). ETA=15:14:17, max mem: 20.9 GB 
[11/23 15:02:45 visual_prompt]: 	Training 400/553. train loss: 29.1996,	0.8729 s / batch. (data: 9.92e-03). ETA=13:10:38, max mem: 20.9 GB 
[11/23 15:04:23 visual_prompt]: 	Training 500/553. train loss: 80.6536,	0.8195 s / batch. (data: 3.38e-04). ETA=12:20:55, max mem: 20.9 GB 
[11/23 15:05:13 visual_prompt]: Epoch 2 / 100: avg data time: 1.42e-01, avg batch time: 0.9730, average train loss: 26.5332
[11/23 15:06:08 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3113, average loss: 67.9892
[11/23 15:06:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.61	
[11/23 15:06:08 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/23 15:07:47 visual_prompt]: 	Training 100/553. train loss: 60.1347,	0.8237 s / batch. (data: 3.08e-04). ETA=12:22:38, max mem: 20.9 GB 
[11/23 15:09:25 visual_prompt]: 	Training 200/553. train loss: 29.4592,	0.8372 s / batch. (data: 5.58e-03). ETA=12:33:22, max mem: 20.9 GB 
[11/23 15:11:01 visual_prompt]: 	Training 300/553. train loss: 40.9004,	0.8240 s / batch. (data: 3.09e-04). ETA=12:20:08, max mem: 20.9 GB 
[11/23 15:12:38 visual_prompt]: 	Training 400/553. train loss: 14.7090,	0.8214 s / batch. (data: 3.13e-04). ETA=12:16:27, max mem: 20.9 GB 
[11/23 15:14:17 visual_prompt]: 	Training 500/553. train loss: 23.5847,	1.1563 s / batch. (data: 3.25e-01). ETA=17:14:45, max mem: 20.9 GB 
[11/23 15:15:07 visual_prompt]: Epoch 3 / 100: avg data time: 1.44e-01, avg batch time: 0.9737, average train loss: 43.5661
[11/23 15:16:03 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3113, average loss: 40.6659
[11/23 15:16:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.00	
[11/23 15:16:03 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/23 15:17:45 visual_prompt]: 	Training 100/553. train loss: 39.0314,	0.8491 s / batch. (data: 1.05e-02). ETA=12:37:43, max mem: 20.9 GB 
[11/23 15:19:24 visual_prompt]: 	Training 200/553. train loss: 61.0410,	0.8610 s / batch. (data: 5.41e-03). ETA=12:46:53, max mem: 20.9 GB 
[11/23 15:21:01 visual_prompt]: 	Training 300/553. train loss: 14.5468,	1.2437 s / batch. (data: 4.18e-01). ETA=18:25:40, max mem: 20.9 GB 
[11/23 15:22:33 visual_prompt]: 	Training 400/553. train loss: 25.0609,	0.9840 s / batch. (data: 1.29e-01). ETA=14:33:07, max mem: 20.9 GB 
[11/23 15:24:12 visual_prompt]: 	Training 500/553. train loss: 145.8390,	3.1000 s / batch. (data: 2.27e+00). ETA=1 day, 21:45:35, max mem: 20.9 GB 
[11/23 15:25:04 visual_prompt]: Epoch 4 / 100: avg data time: 1.48e-01, avg batch time: 0.9782, average train loss: 41.9266
[11/23 15:25:59 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3087, average loss: 32.3341
[11/23 15:25:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.60	
[11/23 15:25:59 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/23 15:27:38 visual_prompt]: 	Training 100/553. train loss: 386.8663,	0.8320 s / batch. (data: 2.90e-04). ETA=12:14:47, max mem: 20.9 GB 
[11/23 15:29:14 visual_prompt]: 	Training 200/553. train loss: 53.7453,	0.9897 s / batch. (data: 1.80e-01). ETA=14:32:23, max mem: 20.9 GB 
[11/23 15:30:52 visual_prompt]: 	Training 300/553. train loss: 214.9673,	0.8513 s / batch. (data: 5.88e-03). ETA=12:28:59, max mem: 20.9 GB 
[11/23 15:32:27 visual_prompt]: 	Training 400/553. train loss: 27.8766,	0.8295 s / batch. (data: 2.89e-04). ETA=12:08:25, max mem: 20.9 GB 
[11/23 15:34:04 visual_prompt]: 	Training 500/553. train loss: 19.4011,	0.8560 s / batch. (data: 2.78e-04). ETA=12:30:15, max mem: 20.9 GB 
[11/23 15:34:55 visual_prompt]: Epoch 5 / 100: avg data time: 1.43e-01, avg batch time: 0.9703, average train loss: 85.3709
[11/23 15:35:50 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3086, average loss: 19.8969
[11/23 15:35:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.41	
[11/23 15:35:50 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/23 15:37:32 visual_prompt]: 	Training 100/553. train loss: 88.2537,	0.8520 s / batch. (data: 3.60e-02). ETA=12:24:33, max mem: 20.9 GB 
[11/23 15:39:08 visual_prompt]: 	Training 200/553. train loss: 21.5776,	0.8176 s / batch. (data: 3.02e-04). ETA=11:53:06, max mem: 20.9 GB 
[11/23 15:40:43 visual_prompt]: 	Training 300/553. train loss: 217.9171,	0.8328 s / batch. (data: 7.96e-03). ETA=12:05:00, max mem: 20.9 GB 
[11/23 15:42:24 visual_prompt]: 	Training 400/553. train loss: 6.9724,	0.8320 s / batch. (data: 3.17e-04). ETA=12:02:57, max mem: 20.9 GB 
[11/23 15:43:59 visual_prompt]: 	Training 500/553. train loss: 1.2620,	0.8200 s / batch. (data: 3.40e-04). ETA=11:51:07, max mem: 20.9 GB 
[11/23 15:44:49 visual_prompt]: Epoch 6 / 100: avg data time: 1.47e-01, avg batch time: 0.9738, average train loss: 87.7367
[11/23 15:45:44 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3097, average loss: 26.4101
[11/23 15:45:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.30	
[11/23 15:45:44 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/23 15:47:23 visual_prompt]: 	Training 100/553. train loss: 50.2073,	0.8355 s / batch. (data: 7.95e-03). ETA=12:02:25, max mem: 20.9 GB 
[11/23 15:49:00 visual_prompt]: 	Training 200/553. train loss: 18.5600,	0.8600 s / batch. (data: 5.43e-03). ETA=12:22:11, max mem: 20.9 GB 
[11/23 15:50:40 visual_prompt]: 	Training 300/553. train loss: 4.7670,	1.5351 s / batch. (data: 7.14e-01). ETA=22:02:17, max mem: 20.9 GB 
[11/23 15:52:15 visual_prompt]: 	Training 400/553. train loss: 29.0680,	0.8280 s / batch. (data: 3.92e-04). ETA=11:51:50, max mem: 20.9 GB 
[11/23 15:53:52 visual_prompt]: 	Training 500/553. train loss: 176.5945,	0.8221 s / batch. (data: 4.30e-04). ETA=11:45:21, max mem: 20.9 GB 
[11/23 15:54:41 visual_prompt]: Epoch 7 / 100: avg data time: 1.42e-01, avg batch time: 0.9697, average train loss: 99.9768
[11/23 15:55:36 visual_prompt]: Inference (val):avg data time: 8.39e-05, avg batch time: 0.3093, average loss: 111.2492
[11/23 15:55:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.62	
[11/23 15:55:36 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/23 15:57:15 visual_prompt]: 	Training 100/553. train loss: 282.0644,	0.8465 s / batch. (data: 2.27e-02). ETA=12:04:09, max mem: 20.9 GB 
[11/23 15:58:54 visual_prompt]: 	Training 200/553. train loss: 32.4692,	0.8439 s / batch. (data: 7.50e-03). ETA=12:00:29, max mem: 20.9 GB 
[11/23 16:00:32 visual_prompt]: 	Training 300/553. train loss: 50.4068,	0.8396 s / batch. (data: 9.61e-03). ETA=11:55:27, max mem: 20.9 GB 
[11/23 16:02:09 visual_prompt]: 	Training 400/553. train loss: 234.5956,	0.8202 s / batch. (data: 3.29e-04). ETA=11:37:36, max mem: 20.9 GB 
[11/23 16:03:46 visual_prompt]: 	Training 500/553. train loss: 329.2100,	1.4310 s / batch. (data: 6.04e-01). ETA=20:14:38, max mem: 20.9 GB 
[11/23 16:04:37 visual_prompt]: Epoch 8 / 100: avg data time: 1.50e-01, avg batch time: 0.9770, average train loss: 113.6568
[11/23 16:05:32 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3104, average loss: 4.6736
[11/23 16:05:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.02	
[11/23 16:05:32 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/23 16:07:12 visual_prompt]: 	Training 100/553. train loss: 38.1540,	0.8520 s / batch. (data: 1.59e-02). ETA=12:00:59, max mem: 20.9 GB 
[11/23 16:08:46 visual_prompt]: 	Training 200/553. train loss: 2.7190,	0.8360 s / batch. (data: 2.98e-04). ETA=11:46:04, max mem: 20.9 GB 
[11/23 16:10:23 visual_prompt]: 	Training 300/553. train loss: 53.2733,	1.4560 s / batch. (data: 6.37e-01). ETA=20:27:18, max mem: 20.9 GB 
[11/23 16:12:00 visual_prompt]: 	Training 400/553. train loss: 240.5974,	0.8398 s / batch. (data: 7.83e-04). ETA=11:46:30, max mem: 20.9 GB 
[11/23 16:13:36 visual_prompt]: 	Training 500/553. train loss: 59.7817,	0.8486 s / batch. (data: 3.48e-02). ETA=11:52:29, max mem: 20.9 GB 
[11/23 16:14:25 visual_prompt]: Epoch 9 / 100: avg data time: 1.37e-01, avg batch time: 0.9642, average train loss: 133.1418
[11/23 16:15:20 visual_prompt]: Inference (val):avg data time: 1.70e-04, avg batch time: 0.3091, average loss: 184.4700
[11/23 16:15:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.80	
[11/23 16:15:20 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/23 16:17:02 visual_prompt]: 	Training 100/553. train loss: 277.8878,	0.8240 s / batch. (data: 2.87e-04). ETA=11:29:44, max mem: 20.9 GB 
[11/23 16:18:36 visual_prompt]: 	Training 200/553. train loss: 8.4880,	0.8451 s / batch. (data: 5.43e-03). ETA=11:45:58, max mem: 20.9 GB 
[11/23 16:20:11 visual_prompt]: 	Training 300/553. train loss: 49.5425,	1.2960 s / batch. (data: 4.56e-01). ETA=18:00:29, max mem: 20.9 GB 
[11/23 16:21:45 visual_prompt]: 	Training 400/553. train loss: 7.6892,	0.8477 s / batch. (data: 3.14e-04). ETA=11:45:19, max mem: 20.9 GB 
[11/23 16:23:22 visual_prompt]: 	Training 500/553. train loss: 26.2307,	0.8073 s / batch. (data: 3.48e-04). ETA=11:10:20, max mem: 20.9 GB 
[11/23 16:24:12 visual_prompt]: Epoch 10 / 100: avg data time: 1.35e-01, avg batch time: 0.9619, average train loss: 164.3878
[11/23 16:25:07 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3103, average loss: 31.7203
[11/23 16:25:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.98	
[11/23 16:25:07 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/23 16:26:51 visual_prompt]: 	Training 100/553. train loss: 9.9833,	0.8262 s / batch. (data: 1.02e-02). ETA=11:23:58, max mem: 20.9 GB 
[11/23 16:28:30 visual_prompt]: 	Training 200/553. train loss: 469.3429,	0.8405 s / batch. (data: 2.33e-02). ETA=11:34:23, max mem: 20.9 GB 
[11/23 16:30:06 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.9720 s / batch. (data: 1.14e+00). ETA=1 day, 3:05:54, max mem: 20.9 GB 
[11/23 16:31:40 visual_prompt]: 	Training 400/553. train loss: 100.8531,	0.8329 s / batch. (data: 7.82e-04). ETA=11:25:20, max mem: 20.9 GB 
[11/23 16:33:14 visual_prompt]: 	Training 500/553. train loss: 263.9169,	0.9840 s / batch. (data: 1.57e-01). ETA=13:28:01, max mem: 20.9 GB 
[11/23 16:34:06 visual_prompt]: Epoch 11 / 100: avg data time: 1.47e-01, avg batch time: 0.9734, average train loss: 146.0352
[11/23 16:35:01 visual_prompt]: Inference (val):avg data time: 1.68e-04, avg batch time: 0.3095, average loss: 132.6761
[11/23 16:35:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.21	
[11/23 16:35:01 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/23 16:36:43 visual_prompt]: 	Training 100/553. train loss: 60.8919,	0.8410 s / batch. (data: 1.05e-02). ETA=11:28:29, max mem: 20.9 GB 
[11/23 16:38:20 visual_prompt]: 	Training 200/553. train loss: 56.2458,	1.0464 s / batch. (data: 2.18e-01). ETA=14:14:51, max mem: 20.9 GB 
[11/23 16:39:57 visual_prompt]: 	Training 300/553. train loss: 37.1902,	0.8240 s / batch. (data: 1.20e-02). ETA=11:11:47, max mem: 20.9 GB 
[11/23 16:41:34 visual_prompt]: 	Training 400/553. train loss: 9.3180,	0.8586 s / batch. (data: 5.25e-04). ETA=11:38:32, max mem: 20.9 GB 
[11/23 16:43:11 visual_prompt]: 	Training 500/553. train loss: 13.2477,	0.8277 s / batch. (data: 7.72e-04). ETA=11:12:02, max mem: 20.9 GB 
[11/23 16:44:00 visual_prompt]: Epoch 12 / 100: avg data time: 1.49e-01, avg batch time: 0.9749, average train loss: 172.4881
[11/23 16:44:56 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3108, average loss: 32.2117
[11/23 16:44:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.13	
[11/23 16:44:56 visual_prompt]: Best epoch 12: best metric: -32.212
[11/23 16:44:56 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/23 16:46:38 visual_prompt]: 	Training 100/553. train loss: 126.3914,	0.8035 s / batch. (data: 3.49e-04). ETA=10:50:21, max mem: 20.9 GB 
[11/23 16:48:11 visual_prompt]: 	Training 200/553. train loss: 6.3517,	0.8309 s / batch. (data: 1.05e-02). ETA=11:11:09, max mem: 20.9 GB 
[11/23 16:49:48 visual_prompt]: 	Training 300/553. train loss: 238.2179,	1.6805 s / batch. (data: 8.76e-01). ETA=22:34:37, max mem: 20.9 GB 
[11/23 16:51:22 visual_prompt]: 	Training 400/553. train loss: 48.1541,	0.8482 s / batch. (data: 1.63e-02). ETA=11:22:15, max mem: 20.9 GB 
[11/23 16:52:59 visual_prompt]: 	Training 500/553. train loss: 359.6395,	0.8637 s / batch. (data: 2.96e-04). ETA=11:33:18, max mem: 20.9 GB 
[11/23 16:53:49 visual_prompt]: Epoch 13 / 100: avg data time: 1.37e-01, avg batch time: 0.9645, average train loss: 158.3108
[11/23 16:54:43 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3115, average loss: 64.0487
[11/23 16:54:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.74	
[11/23 16:54:43 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/23 16:56:24 visual_prompt]: 	Training 100/553. train loss: 354.7159,	0.8390 s / batch. (data: 2.99e-04). ETA=11:11:22, max mem: 20.9 GB 
[11/23 16:57:59 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8444 s / batch. (data: 3.19e-02). ETA=11:14:17, max mem: 20.9 GB 
[11/23 16:59:34 visual_prompt]: 	Training 300/553. train loss: 281.6381,	0.8199 s / batch. (data: 2.93e-04). ETA=10:53:22, max mem: 20.9 GB 
[11/23 17:01:09 visual_prompt]: 	Training 400/553. train loss: 143.7888,	0.8214 s / batch. (data: 2.98e-04). ETA=10:53:09, max mem: 20.9 GB 
[11/23 17:02:44 visual_prompt]: 	Training 500/553. train loss: 306.5684,	0.8270 s / batch. (data: 5.42e-03). ETA=10:56:12, max mem: 20.9 GB 
[11/23 17:03:33 visual_prompt]: Epoch 14 / 100: avg data time: 1.31e-01, avg batch time: 0.9569, average train loss: 151.1336
[11/23 17:04:27 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3093, average loss: 183.7384
[11/23 17:04:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.66	
[11/23 17:04:27 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/23 17:06:06 visual_prompt]: 	Training 100/553. train loss: 105.0922,	1.5085 s / batch. (data: 7.01e-01). ETA=19:53:10, max mem: 20.9 GB 
[11/23 17:07:40 visual_prompt]: 	Training 200/553. train loss: 1187.5264,	0.8240 s / batch. (data: 2.99e-04). ETA=10:50:22, max mem: 20.9 GB 
[11/23 17:09:18 visual_prompt]: 	Training 300/553. train loss: 32.1880,	0.8280 s / batch. (data: 3.06e-04). ETA=10:52:08, max mem: 20.9 GB 
[11/23 17:10:52 visual_prompt]: 	Training 400/553. train loss: 28.2467,	0.8960 s / batch. (data: 7.09e-02). ETA=11:44:12, max mem: 20.9 GB 
[11/23 17:12:29 visual_prompt]: 	Training 500/553. train loss: 68.7989,	0.8400 s / batch. (data: 7.96e-03). ETA=10:58:49, max mem: 20.9 GB 
[11/23 17:13:20 visual_prompt]: Epoch 15 / 100: avg data time: 1.37e-01, avg batch time: 0.9640, average train loss: 206.1229
[11/23 17:14:15 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3110, average loss: 118.2958
[11/23 17:14:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.74	
[11/23 17:14:15 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/23 17:15:54 visual_prompt]: 	Training 100/553. train loss: 202.4007,	0.8265 s / batch. (data: 3.03e-04). ETA=10:46:09, max mem: 20.9 GB 
[11/23 17:17:31 visual_prompt]: 	Training 200/553. train loss: 48.7037,	0.8216 s / batch. (data: 9.58e-03). ETA=10:40:55, max mem: 20.9 GB 
[11/23 17:19:07 visual_prompt]: 	Training 300/553. train loss: 37.0103,	0.8280 s / batch. (data: 3.88e-04). ETA=10:44:30, max mem: 20.9 GB 
[11/23 17:20:44 visual_prompt]: 	Training 400/553. train loss: 27.4519,	0.8427 s / batch. (data: 2.49e-02). ETA=10:54:35, max mem: 20.9 GB 
[11/23 17:22:19 visual_prompt]: 	Training 500/553. train loss: 121.7329,	1.4128 s / batch. (data: 5.89e-01). ETA=18:15:03, max mem: 20.9 GB 
[11/23 17:23:10 visual_prompt]: Epoch 16 / 100: avg data time: 1.41e-01, avg batch time: 0.9682, average train loss: 164.7299
[11/23 17:24:06 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3107, average loss: 117.7863
[11/23 17:24:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.32	
[11/23 17:24:06 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/23 17:25:45 visual_prompt]: 	Training 100/553. train loss: 58.3650,	0.8355 s / batch. (data: 1.15e-02). ETA=10:45:25, max mem: 20.9 GB 
[11/23 17:27:23 visual_prompt]: 	Training 200/553. train loss: 227.1207,	0.8368 s / batch. (data: 1.05e-02). ETA=10:45:04, max mem: 20.9 GB 
[11/23 17:28:58 visual_prompt]: 	Training 300/553. train loss: 46.0636,	0.8332 s / batch. (data: 1.49e-02). ETA=10:40:53, max mem: 20.9 GB 
[11/23 17:30:34 visual_prompt]: 	Training 400/553. train loss: 114.2639,	1.1240 s / batch. (data: 2.83e-01). ETA=14:22:42, max mem: 20.9 GB 
[11/23 17:32:10 visual_prompt]: 	Training 500/553. train loss: 87.6181,	1.5366 s / batch. (data: 7.22e-01). ETA=19:36:47, max mem: 20.9 GB 
[11/23 17:33:01 visual_prompt]: Epoch 17 / 100: avg data time: 1.42e-01, avg batch time: 0.9687, average train loss: 168.7049
[11/23 17:33:56 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3085, average loss: 22.6526
[11/23 17:33:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.86	
[11/23 17:33:56 visual_prompt]: Best epoch 17: best metric: -22.653
[11/23 17:33:56 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/23 17:35:37 visual_prompt]: 	Training 100/553. train loss: 131.4120,	0.8284 s / batch. (data: 2.88e-04). ETA=10:32:18, max mem: 20.9 GB 
[11/23 17:37:16 visual_prompt]: 	Training 200/553. train loss: 82.1683,	0.8312 s / batch. (data: 8.80e-04). ETA=10:33:05, max mem: 20.9 GB 
[11/23 17:38:52 visual_prompt]: 	Training 300/553. train loss: 208.3451,	0.8318 s / batch. (data: 3.34e-04). ETA=10:32:10, max mem: 20.9 GB 
[11/23 17:40:29 visual_prompt]: 	Training 400/553. train loss: 118.2526,	0.8385 s / batch. (data: 3.11e-04). ETA=10:35:52, max mem: 20.9 GB 
[11/23 17:42:04 visual_prompt]: 	Training 500/553. train loss: 105.4762,	0.8318 s / batch. (data: 3.11e-04). ETA=10:29:23, max mem: 20.9 GB 
[11/23 17:42:53 visual_prompt]: Epoch 18 / 100: avg data time: 1.44e-01, avg batch time: 0.9706, average train loss: 173.0700
[11/23 17:43:50 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3087, average loss: 234.8130
[11/23 17:43:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.62	
[11/23 17:43:50 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/23 17:45:31 visual_prompt]: 	Training 100/553. train loss: 102.9204,	0.8280 s / batch. (data: 3.08e-04). ETA=10:24:24, max mem: 20.9 GB 
[11/23 17:47:08 visual_prompt]: 	Training 200/553. train loss: 134.7831,	0.8234 s / batch. (data: 2.90e-04). ETA=10:19:35, max mem: 20.9 GB 
[11/23 17:48:44 visual_prompt]: 	Training 300/553. train loss: 185.0444,	0.8400 s / batch. (data: 2.94e-04). ETA=10:30:39, max mem: 20.9 GB 
[11/23 17:50:21 visual_prompt]: 	Training 400/553. train loss: 27.8588,	0.8058 s / batch. (data: 2.96e-04). ETA=10:03:38, max mem: 20.9 GB 
[11/23 17:51:54 visual_prompt]: 	Training 500/553. train loss: 32.7093,	0.8324 s / batch. (data: 5.39e-03). ETA=10:22:07, max mem: 20.9 GB 
[11/23 17:52:44 visual_prompt]: Epoch 19 / 100: avg data time: 1.38e-01, avg batch time: 0.9640, average train loss: 148.0561
[11/23 17:53:38 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.3109, average loss: 392.9309
[11/23 17:53:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.46	
[11/23 17:53:38 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/23 17:55:17 visual_prompt]: 	Training 100/553. train loss: 4.1941,	0.8654 s / batch. (data: 4.93e-02). ETA=10:44:39, max mem: 20.9 GB 
[11/23 17:56:55 visual_prompt]: 	Training 200/553. train loss: 0.8545,	0.8365 s / batch. (data: 2.95e-04). ETA=10:21:41, max mem: 20.9 GB 
[11/23 17:58:31 visual_prompt]: 	Training 300/553. train loss: 68.5057,	0.8440 s / batch. (data: 2.84e-04). ETA=10:25:52, max mem: 20.9 GB 
[11/23 18:00:06 visual_prompt]: 	Training 400/553. train loss: 805.9830,	0.8413 s / batch. (data: 5.41e-03). ETA=10:22:27, max mem: 20.9 GB 
[11/23 18:01:42 visual_prompt]: 	Training 500/553. train loss: 203.7889,	0.8400 s / batch. (data: 2.86e-04). ETA=10:20:06, max mem: 20.9 GB 
[11/23 18:02:33 visual_prompt]: Epoch 20 / 100: avg data time: 1.40e-01, avg batch time: 0.9672, average train loss: 131.3989
[11/23 18:03:28 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3087, average loss: 165.6595
[11/23 18:03:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.91	
[11/23 18:03:28 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/23 18:05:11 visual_prompt]: 	Training 100/553. train loss: 19.3996,	0.8095 s / batch. (data: 3.32e-04). ETA=9:55:31, max mem: 20.9 GB 
[11/23 18:06:46 visual_prompt]: 	Training 200/553. train loss: 379.2604,	0.8194 s / batch. (data: 5.35e-03). ETA=10:01:28, max mem: 20.9 GB 
[11/23 18:08:21 visual_prompt]: 	Training 300/553. train loss: 641.3419,	0.9480 s / batch. (data: 1.17e-01). ETA=11:34:16, max mem: 20.9 GB 
[11/23 18:09:57 visual_prompt]: 	Training 400/553. train loss: 61.0983,	0.8372 s / batch. (data: 2.81e-04). ETA=10:11:40, max mem: 20.9 GB 
[11/23 18:11:34 visual_prompt]: 	Training 500/553. train loss: 171.5970,	0.8280 s / batch. (data: 2.96e-04). ETA=10:03:38, max mem: 20.9 GB 
[11/23 18:12:23 visual_prompt]: Epoch 21 / 100: avg data time: 1.40e-01, avg batch time: 0.9670, average train loss: 151.6709
[11/23 18:13:18 visual_prompt]: Inference (val):avg data time: 5.84e-04, avg batch time: 0.3112, average loss: 111.4839
[11/23 18:13:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.89	
[11/23 18:13:18 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/23 18:14:58 visual_prompt]: 	Training 100/553. train loss: 122.2994,	0.8274 s / batch. (data: 7.57e-03). ETA=10:01:02, max mem: 20.9 GB 
[11/23 18:16:34 visual_prompt]: 	Training 200/553. train loss: 80.7615,	0.8360 s / batch. (data: 2.82e-04). ETA=10:05:54, max mem: 20.9 GB 
[11/23 18:18:08 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8320 s / batch. (data: 3.07e-04). ETA=10:01:38, max mem: 20.9 GB 
[11/23 18:19:46 visual_prompt]: 	Training 400/553. train loss: 179.3458,	0.8557 s / batch. (data: 7.69e-03). ETA=10:17:21, max mem: 20.9 GB 
[11/23 18:21:23 visual_prompt]: 	Training 500/553. train loss: 114.1174,	0.8059 s / batch. (data: 3.11e-04). ETA=9:40:02, max mem: 20.9 GB 
[11/23 18:22:15 visual_prompt]: Epoch 22 / 100: avg data time: 1.43e-01, avg batch time: 0.9702, average train loss: 152.8411
[11/23 18:23:10 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3101, average loss: 75.6044
[11/23 18:23:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.51	
[11/23 18:23:10 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/23 18:24:53 visual_prompt]: 	Training 100/553. train loss: 238.0629,	0.8140 s / batch. (data: 3.19e-04). ETA=9:43:50, max mem: 20.9 GB 
[11/23 18:26:29 visual_prompt]: 	Training 200/553. train loss: 108.5072,	0.8360 s / batch. (data: 1.60e-02). ETA=9:58:13, max mem: 20.9 GB 
[11/23 18:28:08 visual_prompt]: 	Training 300/553. train loss: 15.3152,	0.8535 s / batch. (data: 7.83e-04). ETA=10:09:16, max mem: 20.9 GB 
[11/23 18:29:42 visual_prompt]: 	Training 400/553. train loss: 370.6310,	0.8455 s / batch. (data: 2.86e-04). ETA=10:02:09, max mem: 20.9 GB 
[11/23 18:31:16 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8240 s / batch. (data: 2.86e-04). ETA=9:45:30, max mem: 20.9 GB 
[11/23 18:32:07 visual_prompt]: Epoch 23 / 100: avg data time: 1.44e-01, avg batch time: 0.9707, average train loss: 164.8058
[11/23 18:33:02 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3092, average loss: 19.6481
[11/23 18:33:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.39	
[11/23 18:33:02 visual_prompt]: Best epoch 23: best metric: -19.648
[11/23 18:33:02 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/23 18:34:39 visual_prompt]: 	Training 100/553. train loss: 220.7708,	0.8240 s / batch. (data: 7.95e-03). ETA=9:43:24, max mem: 20.9 GB 
[11/23 18:36:15 visual_prompt]: 	Training 200/553. train loss: 57.7030,	0.8497 s / batch. (data: 1.20e-02). ETA=10:00:11, max mem: 20.9 GB 
[11/23 18:37:52 visual_prompt]: 	Training 300/553. train loss: 165.2409,	0.8177 s / batch. (data: 8.45e-03). ETA=9:36:14, max mem: 20.9 GB 
[11/23 18:39:30 visual_prompt]: 	Training 400/553. train loss: 54.8669,	0.8423 s / batch. (data: 1.03e-02). ETA=9:52:09, max mem: 20.9 GB 
[11/23 18:41:09 visual_prompt]: 	Training 500/553. train loss: 233.0795,	0.8370 s / batch. (data: 5.41e-03). ETA=9:47:01, max mem: 20.9 GB 
[11/23 18:42:00 visual_prompt]: Epoch 24 / 100: avg data time: 1.46e-01, avg batch time: 0.9728, average train loss: 146.3013
[11/23 18:42:55 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.3095, average loss: 171.0182
[11/23 18:42:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.56	
[11/23 18:42:55 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/23 18:44:40 visual_prompt]: 	Training 100/553. train loss: 15.2874,	0.8351 s / batch. (data: 3.01e-04). ETA=9:43:32, max mem: 20.9 GB 
[11/23 18:46:14 visual_prompt]: 	Training 200/553. train loss: 186.4530,	0.8361 s / batch. (data: 1.42e-02). ETA=9:42:52, max mem: 20.9 GB 
[11/23 18:47:50 visual_prompt]: 	Training 300/553. train loss: 25.8697,	0.8397 s / batch. (data: 3.17e-04). ETA=9:43:58, max mem: 20.9 GB 
[11/23 18:49:27 visual_prompt]: 	Training 400/553. train loss: 33.5844,	1.1930 s / batch. (data: 3.68e-01). ETA=13:47:42, max mem: 20.9 GB 
[11/23 18:51:04 visual_prompt]: 	Training 500/553. train loss: 120.7112,	1.4435 s / batch. (data: 6.15e-01). ETA=16:39:04, max mem: 20.9 GB 
[11/23 18:51:55 visual_prompt]: Epoch 25 / 100: avg data time: 1.50e-01, avg batch time: 0.9761, average train loss: 152.1037
[11/23 18:52:50 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3102, average loss: 209.1437
[11/23 18:52:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.07	
[11/23 18:52:50 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/23 18:54:32 visual_prompt]: 	Training 100/553. train loss: 83.1610,	0.8267 s / batch. (data: 9.58e-03). ETA=9:30:05, max mem: 20.9 GB 
[11/23 18:56:10 visual_prompt]: 	Training 200/553. train loss: 598.5208,	1.5891 s / batch. (data: 7.75e-01). ETA=18:13:08, max mem: 20.9 GB 
[11/23 18:57:48 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8154 s / batch. (data: 8.06e-04). ETA=9:19:32, max mem: 20.9 GB 
[11/23 18:59:24 visual_prompt]: 	Training 400/553. train loss: 457.4002,	0.8248 s / batch. (data: 4.28e-04). ETA=9:24:39, max mem: 20.9 GB 
[11/23 19:01:00 visual_prompt]: 	Training 500/553. train loss: 34.4658,	0.8480 s / batch. (data: 8.08e-04). ETA=9:39:06, max mem: 20.9 GB 
[11/23 19:01:50 visual_prompt]: Epoch 26 / 100: avg data time: 1.49e-01, avg batch time: 0.9759, average train loss: 149.0037
[11/23 19:02:46 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3097, average loss: 44.7650
[11/23 19:02:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.99	
[11/23 19:02:46 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/23 19:04:28 visual_prompt]: 	Training 100/553. train loss: 14.7403,	0.8605 s / batch. (data: 4.68e-04). ETA=9:45:25, max mem: 20.9 GB 
[11/23 19:06:04 visual_prompt]: 	Training 200/553. train loss: 319.8436,	1.0278 s / batch. (data: 1.95e-01). ETA=11:37:33, max mem: 20.9 GB 
[11/23 19:07:41 visual_prompt]: 	Training 300/553. train loss: 199.6077,	0.8360 s / batch. (data: 7.95e-03). ETA=9:25:59, max mem: 20.9 GB 
[11/23 19:09:20 visual_prompt]: 	Training 400/553. train loss: 0.2961,	0.8590 s / batch. (data: 5.91e-03). ETA=9:40:07, max mem: 20.9 GB 
[11/23 19:10:57 visual_prompt]: 	Training 500/553. train loss: 63.2854,	0.8280 s / batch. (data: 1.20e-02). ETA=9:17:48, max mem: 20.9 GB 
[11/23 19:11:46 visual_prompt]: Epoch 27 / 100: avg data time: 1.49e-01, avg batch time: 0.9762, average train loss: 175.4549
[11/23 19:12:41 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3088, average loss: 296.2717
[11/23 19:12:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 34.15	
[11/23 19:12:41 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/23 19:14:21 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8129 s / batch. (data: 3.05e-04). ETA=9:05:34, max mem: 20.9 GB 
[11/23 19:15:59 visual_prompt]: 	Training 200/553. train loss: 147.5573,	0.8271 s / batch. (data: 3.12e-04). ETA=9:13:45, max mem: 20.9 GB 
[11/23 19:17:37 visual_prompt]: 	Training 300/553. train loss: 39.2931,	1.2597 s / batch. (data: 4.55e-01). ETA=14:01:15, max mem: 20.9 GB 
[11/23 19:19:11 visual_prompt]: 	Training 400/553. train loss: 13.6634,	0.8292 s / batch. (data: 3.11e-04). ETA=9:12:20, max mem: 20.9 GB 
[11/23 19:20:46 visual_prompt]: 	Training 500/553. train loss: 440.2202,	0.8240 s / batch. (data: 5.42e-03). ETA=9:07:33, max mem: 20.9 GB 
[11/23 19:21:36 visual_prompt]: Epoch 28 / 100: avg data time: 1.41e-01, avg batch time: 0.9678, average train loss: 158.8234
[11/23 19:22:31 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3120, average loss: 254.6606
[11/23 19:22:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.74	
[11/23 19:22:31 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/23 19:24:17 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8250 s / batch. (data: 2.91e-04). ETA=9:06:04, max mem: 20.9 GB 
[11/23 19:25:53 visual_prompt]: 	Training 200/553. train loss: 346.1683,	1.6812 s / batch. (data: 8.65e-01). ETA=18:30:01, max mem: 20.9 GB 
[11/23 19:27:26 visual_prompt]: 	Training 300/553. train loss: 34.8636,	0.8499 s / batch. (data: 6.87e-04). ETA=9:19:44, max mem: 20.9 GB 
[11/23 19:28:59 visual_prompt]: 	Training 400/553. train loss: 364.2102,	1.2079 s / batch. (data: 3.83e-01). ETA=13:13:30, max mem: 20.9 GB 
[11/23 19:30:35 visual_prompt]: 	Training 500/553. train loss: 46.9617,	0.8280 s / batch. (data: 3.15e-04). ETA=9:02:34, max mem: 20.9 GB 
[11/23 19:31:25 visual_prompt]: Epoch 29 / 100: avg data time: 1.38e-01, avg batch time: 0.9652, average train loss: 139.2024
[11/23 19:32:20 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3102, average loss: 19.5667
[11/23 19:32:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.65	
[11/23 19:32:20 visual_prompt]: Best epoch 29: best metric: -19.567
[11/23 19:32:20 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/23 19:33:58 visual_prompt]: 	Training 100/553. train loss: 154.6503,	0.8600 s / batch. (data: 7.96e-03). ETA=9:21:19, max mem: 20.9 GB 
[11/23 19:35:36 visual_prompt]: 	Training 200/553. train loss: 16.3775,	0.8250 s / batch. (data: 3.19e-03). ETA=8:57:07, max mem: 20.9 GB 
[11/23 19:37:10 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.3640 s / batch. (data: 5.31e-01). ETA=14:45:44, max mem: 20.9 GB 
[11/23 19:38:49 visual_prompt]: 	Training 400/553. train loss: 98.6266,	1.0874 s / batch. (data: 2.71e-01). ETA=11:44:18, max mem: 20.9 GB 
[11/23 19:40:25 visual_prompt]: 	Training 500/553. train loss: 48.5852,	1.3840 s / batch. (data: 5.34e-01). ETA=14:54:07, max mem: 20.9 GB 
[11/23 19:41:17 visual_prompt]: Epoch 30 / 100: avg data time: 1.44e-01, avg batch time: 0.9716, average train loss: 140.6919
[11/23 19:42:13 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3088, average loss: 85.6259
[11/23 19:42:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.40	
[11/23 19:42:13 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[11/23 19:43:54 visual_prompt]: 	Training 100/553. train loss: 122.3913,	0.8320 s / batch. (data: 3.37e-04). ETA=8:55:23, max mem: 20.9 GB 
[11/23 19:45:33 visual_prompt]: 	Training 200/553. train loss: 31.0906,	0.8412 s / batch. (data: 2.78e-04). ETA=8:59:55, max mem: 20.9 GB 
[11/23 19:47:06 visual_prompt]: 	Training 300/553. train loss: 229.8587,	0.8280 s / batch. (data: 3.21e-04). ETA=8:50:02, max mem: 20.9 GB 
[11/23 19:48:41 visual_prompt]: 	Training 400/553. train loss: 157.4476,	1.0440 s / batch. (data: 2.16e-01). ETA=11:06:35, max mem: 20.9 GB 
[11/23 19:50:17 visual_prompt]: 	Training 500/553. train loss: 97.8932,	0.8173 s / batch. (data: 3.13e-04). ETA=8:40:29, max mem: 20.9 GB 
[11/23 19:51:06 visual_prompt]: Epoch 31 / 100: avg data time: 1.37e-01, avg batch time: 0.9642, average train loss: 142.1276
[11/23 19:52:01 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3103, average loss: 81.8708
[11/23 19:52:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.45	
[11/23 19:52:01 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[11/23 19:53:42 visual_prompt]: 	Training 100/553. train loss: 165.4758,	0.8480 s / batch. (data: 7.57e-04). ETA=8:57:52, max mem: 20.9 GB 
[11/23 19:55:17 visual_prompt]: 	Training 200/553. train loss: 15.5328,	0.8300 s / batch. (data: 7.50e-04). ETA=8:45:05, max mem: 20.9 GB 
[11/23 19:56:56 visual_prompt]: 	Training 300/553. train loss: 174.6999,	0.8390 s / batch. (data: 1.05e-02). ETA=8:49:23, max mem: 20.9 GB 
[11/23 19:58:32 visual_prompt]: 	Training 400/553. train loss: 54.7532,	0.8320 s / batch. (data: 3.01e-04). ETA=8:43:33, max mem: 20.9 GB 
[11/23 20:00:05 visual_prompt]: 	Training 500/553. train loss: 37.7764,	0.8355 s / batch. (data: 2.93e-04). ETA=8:44:21, max mem: 20.9 GB 
[11/23 20:00:54 visual_prompt]: Epoch 32 / 100: avg data time: 1.36e-01, avg batch time: 0.9638, average train loss: 139.3340
[11/23 20:01:49 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3107, average loss: 19.7126
[11/23 20:01:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.32	
[11/23 20:01:49 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[11/23 20:03:29 visual_prompt]: 	Training 100/553. train loss: 4.8962,	0.8520 s / batch. (data: 3.28e-04). ETA=8:52:33, max mem: 20.9 GB 
[11/23 20:05:07 visual_prompt]: 	Training 200/553. train loss: 18.0701,	0.9720 s / batch. (data: 1.35e-01). ETA=10:05:56, max mem: 20.9 GB 
[11/23 20:06:44 visual_prompt]: 	Training 300/553. train loss: 210.4590,	0.8455 s / batch. (data: 2.45e-04). ETA=8:45:41, max mem: 20.9 GB 
[11/23 20:08:22 visual_prompt]: 	Training 400/553. train loss: 330.1152,	0.8283 s / batch. (data: 3.32e-04). ETA=8:33:37, max mem: 20.9 GB 
[11/23 20:09:58 visual_prompt]: 	Training 500/553. train loss: 507.6531,	0.8245 s / batch. (data: 1.05e-02). ETA=8:29:50, max mem: 20.9 GB 
[11/23 20:10:48 visual_prompt]: Epoch 33 / 100: avg data time: 1.45e-01, avg batch time: 0.9733, average train loss: 143.6312
[11/23 20:11:43 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3093, average loss: 127.1067
[11/23 20:11:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.03	
[11/23 20:11:43 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[11/23 20:13:25 visual_prompt]: 	Training 100/553. train loss: 19.8462,	0.8309 s / batch. (data: 3.31e-04). ETA=8:31:41, max mem: 20.9 GB 
[11/23 20:15:01 visual_prompt]: 	Training 200/553. train loss: 18.9643,	0.8496 s / batch. (data: 5.43e-03). ETA=8:41:48, max mem: 20.9 GB 
[11/23 20:16:37 visual_prompt]: 	Training 300/553. train loss: 309.5136,	0.8383 s / batch. (data: 5.41e-03). ETA=8:33:28, max mem: 20.9 GB 
[11/23 20:18:15 visual_prompt]: 	Training 400/553. train loss: 8.1546,	0.8329 s / batch. (data: 3.45e-04). ETA=8:28:47, max mem: 20.9 GB 
[11/23 20:19:53 visual_prompt]: 	Training 500/553. train loss: 112.1696,	1.4725 s / batch. (data: 6.44e-01). ETA=14:57:01, max mem: 20.9 GB 
[11/23 20:20:43 visual_prompt]: Epoch 34 / 100: avg data time: 1.50e-01, avg batch time: 0.9760, average train loss: 149.7802
[11/23 20:21:38 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3113, average loss: 111.8793
[11/23 20:21:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.49	
[11/23 20:21:38 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[11/23 20:23:21 visual_prompt]: 	Training 100/553. train loss: 103.1113,	0.8429 s / batch. (data: 1.23e-02). ETA=8:31:18, max mem: 20.9 GB 
[11/23 20:25:00 visual_prompt]: 	Training 200/553. train loss: 27.2771,	0.8501 s / batch. (data: 3.76e-04). ETA=8:34:15, max mem: 20.9 GB 
[11/23 20:26:35 visual_prompt]: 	Training 300/553. train loss: 37.5424,	0.8615 s / batch. (data: 2.57e-02). ETA=8:39:46, max mem: 20.9 GB 
[11/23 20:28:11 visual_prompt]: 	Training 400/553. train loss: 14.9664,	0.8360 s / batch. (data: 2.95e-04). ETA=8:22:58, max mem: 20.9 GB 
[11/23 20:29:47 visual_prompt]: 	Training 500/553. train loss: 55.5980,	0.9902 s / batch. (data: 1.81e-01). ETA=9:54:05, max mem: 20.9 GB 
[11/23 20:30:38 visual_prompt]: Epoch 35 / 100: avg data time: 1.49e-01, avg batch time: 0.9763, average train loss: 149.4510
[11/23 20:31:34 visual_prompt]: Inference (val):avg data time: 2.84e-04, avg batch time: 0.3104, average loss: 265.0159
[11/23 20:31:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.16	
[11/23 20:31:34 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[11/23 20:33:14 visual_prompt]: 	Training 100/553. train loss: 115.7175,	0.8139 s / batch. (data: 1.06e-02). ETA=8:06:15, max mem: 20.9 GB 
[11/23 20:34:52 visual_prompt]: 	Training 200/553. train loss: 506.0617,	0.8400 s / batch. (data: 1.20e-02). ETA=8:20:24, max mem: 20.9 GB 
[11/23 20:36:31 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8169 s / batch. (data: 5.44e-03). ETA=8:05:18, max mem: 20.9 GB 
[11/23 20:38:07 visual_prompt]: 	Training 400/553. train loss: 23.3570,	0.8342 s / batch. (data: 3.04e-04). ETA=8:14:10, max mem: 20.9 GB 
[11/23 20:39:45 visual_prompt]: 	Training 500/553. train loss: 290.1146,	0.8419 s / batch. (data: 3.08e-04). ETA=8:17:21, max mem: 20.9 GB 
[11/23 20:40:33 visual_prompt]: Epoch 36 / 100: avg data time: 1.48e-01, avg batch time: 0.9748, average train loss: 140.2568
[11/23 20:41:28 visual_prompt]: Inference (val):avg data time: 2.28e-04, avg batch time: 0.3106, average loss: 111.5739
[11/23 20:41:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.40	
[11/23 20:41:28 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[11/23 20:43:09 visual_prompt]: 	Training 100/553. train loss: 141.5817,	0.8134 s / batch. (data: 3.13e-04). ETA=7:58:25, max mem: 20.9 GB 
[11/23 20:44:47 visual_prompt]: 	Training 200/553. train loss: 168.3323,	0.8455 s / batch. (data: 3.02e-04). ETA=8:15:55, max mem: 20.9 GB 
[11/23 20:46:24 visual_prompt]: 	Training 300/553. train loss: 281.6743,	1.2520 s / batch. (data: 4.01e-01). ETA=12:12:15, max mem: 20.9 GB 
[11/23 20:48:04 visual_prompt]: 	Training 400/553. train loss: 3.9071,	1.8665 s / batch. (data: 1.04e+00). ETA=18:08:32, max mem: 20.9 GB 
[11/23 20:49:37 visual_prompt]: 	Training 500/553. train loss: 34.7462,	0.9944 s / batch. (data: 1.66e-01). ETA=9:38:15, max mem: 20.9 GB 
[11/23 20:50:30 visual_prompt]: Epoch 37 / 100: avg data time: 1.52e-01, avg batch time: 0.9784, average train loss: 140.7057
[11/23 20:51:25 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3116, average loss: 52.1396
[11/23 20:51:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.22	
[11/23 20:51:25 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[11/23 20:53:04 visual_prompt]: 	Training 100/553. train loss: 6.9585,	0.8279 s / batch. (data: 5.44e-03). ETA=7:59:21, max mem: 20.9 GB 
[11/23 20:54:42 visual_prompt]: 	Training 200/553. train loss: 107.9177,	1.1703 s / batch. (data: 3.26e-01). ETA=11:15:37, max mem: 20.9 GB 
[11/23 20:56:20 visual_prompt]: 	Training 300/553. train loss: 33.5977,	0.8240 s / batch. (data: 3.09e-04). ETA=7:54:20, max mem: 20.9 GB 
[11/23 20:57:55 visual_prompt]: 	Training 400/553. train loss: 126.3971,	0.8210 s / batch. (data: 3.11e-04). ETA=7:51:13, max mem: 20.9 GB 
[11/23 20:59:34 visual_prompt]: 	Training 500/553. train loss: 5.5677,	0.8318 s / batch. (data: 3.27e-04). ETA=7:56:03, max mem: 20.9 GB 
[11/23 21:00:24 visual_prompt]: Epoch 38 / 100: avg data time: 1.47e-01, avg batch time: 0.9737, average train loss: 140.6521
[11/23 21:01:19 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3084, average loss: 276.6873
[11/23 21:01:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.70	
[11/23 21:01:19 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[11/23 21:02:58 visual_prompt]: 	Training 100/553. train loss: 0.0002,	0.8264 s / batch. (data: 9.41e-03). ETA=7:50:52, max mem: 20.9 GB 
[11/23 21:04:39 visual_prompt]: 	Training 200/553. train loss: 413.8148,	0.8664 s / batch. (data: 3.84e-02). ETA=8:12:13, max mem: 20.9 GB 
[11/23 21:06:19 visual_prompt]: 	Training 300/553. train loss: 124.1934,	0.8440 s / batch. (data: 7.96e-03). ETA=7:58:04, max mem: 20.9 GB 
[11/23 21:07:54 visual_prompt]: 	Training 400/553. train loss: 115.5211,	1.2268 s / batch. (data: 4.21e-01). ETA=11:32:51, max mem: 20.9 GB 
[11/23 21:09:30 visual_prompt]: 	Training 500/553. train loss: 40.5285,	1.7160 s / batch. (data: 8.60e-01). ETA=16:06:16, max mem: 20.9 GB 
[11/23 21:10:19 visual_prompt]: Epoch 39 / 100: avg data time: 1.49e-01, avg batch time: 0.9764, average train loss: 108.4750
[11/23 21:11:14 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3099, average loss: 209.0526
[11/23 21:11:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.69	
[11/23 21:11:14 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[11/23 21:12:56 visual_prompt]: 	Training 100/553. train loss: 375.7561,	0.8280 s / batch. (data: 2.93e-04). ETA=7:44:07, max mem: 20.9 GB 
[11/23 21:14:32 visual_prompt]: 	Training 200/553. train loss: 3.0492,	0.8440 s / batch. (data: 7.95e-03). ETA=7:51:41, max mem: 20.9 GB 
[11/23 21:16:11 visual_prompt]: 	Training 300/553. train loss: 100.3886,	0.8321 s / batch. (data: 7.87e-04). ETA=7:43:40, max mem: 20.9 GB 
[11/23 21:17:48 visual_prompt]: 	Training 400/553. train loss: 117.8543,	0.8199 s / batch. (data: 1.05e-02). ETA=7:35:28, max mem: 20.9 GB 
[11/23 21:19:24 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8120 s / batch. (data: 3.15e-04). ETA=7:29:46, max mem: 20.9 GB 
[11/23 21:20:16 visual_prompt]: Epoch 40 / 100: avg data time: 1.54e-01, avg batch time: 0.9799, average train loss: 132.2321
[11/23 21:21:12 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3105, average loss: 34.4287
[11/23 21:21:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.58	
[11/23 21:21:12 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[11/23 21:22:57 visual_prompt]: 	Training 100/553. train loss: 74.8127,	0.8402 s / batch. (data: 3.11e-04). ETA=7:43:12, max mem: 20.9 GB 
[11/23 21:24:36 visual_prompt]: 	Training 200/553. train loss: 32.2467,	0.8094 s / batch. (data: 7.10e-04). ETA=7:24:53, max mem: 20.9 GB 
[11/23 21:26:12 visual_prompt]: 	Training 300/553. train loss: 170.9885,	0.8296 s / batch. (data: 4.06e-04). ETA=7:34:35, max mem: 20.9 GB 
[11/23 21:27:48 visual_prompt]: 	Training 400/553. train loss: 106.4628,	0.8228 s / batch. (data: 2.68e-04). ETA=7:29:30, max mem: 20.9 GB 
[11/23 21:29:23 visual_prompt]: 	Training 500/553. train loss: 99.3491,	0.8160 s / batch. (data: 3.04e-04). ETA=7:24:26, max mem: 20.9 GB 
[11/23 21:30:11 visual_prompt]: Epoch 41 / 100: avg data time: 1.47e-01, avg batch time: 0.9744, average train loss: 141.9813
[11/23 21:31:07 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3122, average loss: 86.1727
[11/23 21:31:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.03	
[11/23 21:31:07 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[11/23 21:32:46 visual_prompt]: 	Training 100/553. train loss: 261.2810,	0.8201 s / batch. (data: 2.97e-04). ETA=7:24:35, max mem: 20.9 GB 
[11/23 21:34:23 visual_prompt]: 	Training 200/553. train loss: 1197.9874,	0.8474 s / batch. (data: 3.14e-04). ETA=7:37:59, max mem: 20.9 GB 
[11/23 21:36:01 visual_prompt]: 	Training 300/553. train loss: 27.7327,	0.8440 s / batch. (data: 3.12e-04). ETA=7:34:44, max mem: 20.9 GB 
[11/23 21:37:38 visual_prompt]: 	Training 400/553. train loss: 58.5433,	0.8551 s / batch. (data: 2.98e-04). ETA=7:39:16, max mem: 20.9 GB 
[11/23 21:39:14 visual_prompt]: 	Training 500/553. train loss: 0.5195,	0.8318 s / batch. (data: 1.41e-02). ETA=7:25:22, max mem: 20.9 GB 
[11/23 21:40:06 visual_prompt]: Epoch 42 / 100: avg data time: 1.48e-01, avg batch time: 0.9751, average train loss: 173.6028
[11/23 21:41:01 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3108, average loss: 80.3345
[11/23 21:41:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.94	
[11/23 21:41:01 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[11/23 21:42:44 visual_prompt]: 	Training 100/553. train loss: 210.0464,	0.8290 s / batch. (data: 2.96e-04). ETA=7:21:46, max mem: 20.9 GB 
[11/23 21:44:20 visual_prompt]: 	Training 200/553. train loss: 132.1355,	0.8341 s / batch. (data: 6.02e-03). ETA=7:23:04, max mem: 20.9 GB 
[11/23 21:45:56 visual_prompt]: 	Training 300/553. train loss: 264.3711,	0.8440 s / batch. (data: 3.00e-04). ETA=7:26:57, max mem: 20.9 GB 
[11/23 21:47:31 visual_prompt]: 	Training 400/553. train loss: 103.4724,	0.8395 s / batch. (data: 5.41e-03). ETA=7:23:09, max mem: 20.9 GB 
[11/23 21:49:10 visual_prompt]: 	Training 500/553. train loss: 181.9322,	0.8476 s / batch. (data: 1.05e-02). ETA=7:26:02, max mem: 20.9 GB 
[11/23 21:50:01 visual_prompt]: Epoch 43 / 100: avg data time: 1.50e-01, avg batch time: 0.9764, average train loss: 111.9246
[11/23 21:50:57 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3088, average loss: 79.2636
[11/23 21:50:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.53	
[11/23 21:50:57 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[11/23 21:52:38 visual_prompt]: 	Training 100/553. train loss: 351.5696,	0.8258 s / batch. (data: 2.19e-02). ETA=7:12:27, max mem: 20.9 GB 
[11/23 21:54:16 visual_prompt]: 	Training 200/553. train loss: 40.4058,	0.8454 s / batch. (data: 2.14e-02). ETA=7:21:18, max mem: 20.9 GB 
[11/23 21:55:51 visual_prompt]: 	Training 300/553. train loss: 184.5063,	0.8353 s / batch. (data: 1.13e-02). ETA=7:14:40, max mem: 20.9 GB 
[11/23 21:57:26 visual_prompt]: 	Training 400/553. train loss: 34.6635,	0.8199 s / batch. (data: 2.98e-04). ETA=7:05:17, max mem: 20.9 GB 
[11/23 21:59:02 visual_prompt]: 	Training 500/553. train loss: 3.6811,	0.8277 s / batch. (data: 4.41e-04). ETA=7:07:57, max mem: 20.9 GB 
[11/23 21:59:52 visual_prompt]: Epoch 44 / 100: avg data time: 1.41e-01, avg batch time: 0.9678, average train loss: 126.6946
[11/23 22:00:47 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3107, average loss: 172.6289
[11/23 22:00:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 55.42	
[11/23 22:00:47 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[11/23 22:02:28 visual_prompt]: 	Training 100/553. train loss: 4.7575,	0.8145 s / batch. (data: 3.12e-04). ETA=6:59:02, max mem: 20.9 GB 
[11/23 22:04:00 visual_prompt]: 	Training 200/553. train loss: 76.7605,	0.8115 s / batch. (data: 5.45e-03). ETA=6:56:08, max mem: 20.9 GB 
[11/23 22:05:38 visual_prompt]: 	Training 300/553. train loss: 517.3029,	0.8550 s / batch. (data: 1.89e-02). ETA=7:17:00, max mem: 20.9 GB 
[11/23 22:07:11 visual_prompt]: 	Training 400/553. train loss: 17.2243,	0.8360 s / batch. (data: 3.34e-04). ETA=7:05:53, max mem: 20.9 GB 
[11/23 22:08:50 visual_prompt]: 	Training 500/553. train loss: 47.1717,	0.8207 s / batch. (data: 1.05e-02). ETA=6:56:45, max mem: 20.9 GB 
[11/23 22:09:41 visual_prompt]: Epoch 45 / 100: avg data time: 1.37e-01, avg batch time: 0.9654, average train loss: 104.8145
[11/23 22:10:36 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3095, average loss: 39.0389
[11/23 22:10:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.39	
[11/23 22:10:36 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[11/23 22:12:17 visual_prompt]: 	Training 100/553. train loss: 97.2375,	0.8593 s / batch. (data: 2.45e-02). ETA=7:14:09, max mem: 20.9 GB 
[11/23 22:13:55 visual_prompt]: 	Training 200/553. train loss: 1137.4886,	0.8599 s / batch. (data: 1.10e-02). ETA=7:13:00, max mem: 20.9 GB 
[11/23 22:15:31 visual_prompt]: 	Training 300/553. train loss: 211.1017,	0.8169 s / batch. (data: 5.45e-03). ETA=6:50:01, max mem: 20.9 GB 
[11/23 22:17:09 visual_prompt]: 	Training 400/553. train loss: 21.3189,	0.8335 s / batch. (data: 7.81e-04). ETA=6:56:57, max mem: 20.9 GB 
[11/23 22:18:43 visual_prompt]: 	Training 500/553. train loss: 243.4124,	0.8400 s / batch. (data: 3.19e-04). ETA=6:58:48, max mem: 20.9 GB 
[11/23 22:19:35 visual_prompt]: Epoch 46 / 100: avg data time: 1.48e-01, avg batch time: 0.9749, average train loss: 112.3867
[11/23 22:20:30 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.3118, average loss: 77.4649
[11/23 22:20:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.69	
[11/23 22:20:30 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[11/23 22:22:12 visual_prompt]: 	Training 100/553. train loss: 54.7013,	0.8240 s / batch. (data: 2.95e-04). ETA=6:48:44, max mem: 20.9 GB 
[11/23 22:23:45 visual_prompt]: 	Training 200/553. train loss: 237.6187,	1.2520 s / batch. (data: 4.08e-01). ETA=10:18:56, max mem: 20.9 GB 
[11/23 22:25:21 visual_prompt]: 	Training 300/553. train loss: 5.9041,	0.8520 s / batch. (data: 1.41e-02). ETA=6:59:46, max mem: 20.9 GB 
[11/23 22:26:57 visual_prompt]: 	Training 400/553. train loss: 57.4155,	0.8433 s / batch. (data: 8.29e-04). ETA=6:54:05, max mem: 20.9 GB 
[11/23 22:28:31 visual_prompt]: 	Training 500/553. train loss: 181.2313,	0.8200 s / batch. (data: 2.89e-04). ETA=6:41:17, max mem: 20.9 GB 
[11/23 22:29:22 visual_prompt]: Epoch 47 / 100: avg data time: 1.34e-01, avg batch time: 0.9608, average train loss: 102.7517
[11/23 22:30:16 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3102, average loss: 113.9753
[11/23 22:30:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.59	
[11/23 22:30:16 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[11/23 22:31:56 visual_prompt]: 	Training 100/553. train loss: 101.8445,	0.8341 s / batch. (data: 2.06e-02). ETA=6:46:03, max mem: 20.9 GB 
[11/23 22:33:32 visual_prompt]: 	Training 200/553. train loss: 83.2641,	0.8548 s / batch. (data: 1.05e-02). ETA=6:54:41, max mem: 20.9 GB 
[11/23 22:35:09 visual_prompt]: 	Training 300/553. train loss: 197.4628,	1.5880 s / batch. (data: 7.32e-01). ETA=12:47:45, max mem: 20.9 GB 
[11/23 22:36:42 visual_prompt]: 	Training 400/553. train loss: 575.2391,	0.8368 s / batch. (data: 7.96e-03). ETA=6:43:10, max mem: 20.9 GB 
[11/23 22:38:18 visual_prompt]: 	Training 500/553. train loss: 41.3837,	0.8271 s / batch. (data: 1.60e-02). ETA=6:37:07, max mem: 20.9 GB 
[11/23 22:39:08 visual_prompt]: Epoch 48 / 100: avg data time: 1.36e-01, avg batch time: 0.9627, average train loss: 124.2199
[11/23 22:40:03 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3079, average loss: 174.4985
[11/23 22:40:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.39	
[11/23 22:40:03 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[11/23 22:41:43 visual_prompt]: 	Training 100/553. train loss: 109.8831,	0.8359 s / batch. (data: 1.50e-02). ETA=6:39:12, max mem: 20.9 GB 
[11/23 22:43:18 visual_prompt]: 	Training 200/553. train loss: 28.6203,	0.8425 s / batch. (data: 2.85e-04). ETA=6:40:58, max mem: 20.9 GB 
[11/23 22:44:55 visual_prompt]: 	Training 300/553. train loss: 182.3253,	0.8320 s / batch. (data: 2.77e-04). ETA=6:34:35, max mem: 20.9 GB 
[11/23 22:46:33 visual_prompt]: 	Training 400/553. train loss: 143.3421,	0.8216 s / batch. (data: 7.87e-04). ETA=6:28:17, max mem: 20.9 GB 
[11/23 22:48:09 visual_prompt]: 	Training 500/553. train loss: 57.9728,	0.8281 s / batch. (data: 2.72e-04). ETA=6:29:58, max mem: 20.9 GB 
[11/23 22:48:59 visual_prompt]: Epoch 49 / 100: avg data time: 1.42e-01, avg batch time: 0.9690, average train loss: 106.7669
[11/23 22:49:54 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3094, average loss: 70.2803
[11/23 22:49:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.07	
[11/23 22:49:54 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[11/23 22:51:35 visual_prompt]: 	Training 100/553. train loss: 135.8259,	0.8240 s / batch. (data: 3.04e-04). ETA=6:25:56, max mem: 20.9 GB 
[11/23 22:53:12 visual_prompt]: 	Training 200/553. train loss: 353.7992,	0.8201 s / batch. (data: 3.02e-04). ETA=6:22:44, max mem: 20.9 GB 
[11/23 22:54:47 visual_prompt]: 	Training 300/553. train loss: 450.7252,	0.8440 s / batch. (data: 7.66e-04). ETA=6:32:29, max mem: 20.9 GB 
[11/23 22:56:21 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8441 s / batch. (data: 7.98e-03). ETA=6:31:07, max mem: 20.9 GB 
[11/23 22:57:58 visual_prompt]: 	Training 500/553. train loss: 85.6277,	0.8320 s / batch. (data: 3.04e-04). ETA=6:24:08, max mem: 20.9 GB 
[11/23 22:58:48 visual_prompt]: Epoch 50 / 100: avg data time: 1.39e-01, avg batch time: 0.9658, average train loss: 127.9936
[11/23 22:59:43 visual_prompt]: Inference (val):avg data time: 7.06e-05, avg batch time: 0.3088, average loss: 134.5663
[11/23 22:59:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.63	
[11/23 22:59:43 visual_prompt]: Stopping early.
[11/23 22:59:43 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 22:59:43 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 22:59:43 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/23 22:59:43 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 22:59:43 visual_prompt]: Training with config:
[11/23 22:59:43 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/23 22:59:43 visual_prompt]: Loading training data...
[11/23 22:59:43 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 22:59:43 visual_prompt]: Loading validation data...
[11/23 22:59:43 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 22:59:43 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 22:59:46 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 22:59:46 visual_prompt]: tuned percent:0.525
[11/23 22:59:46 visual_prompt]: Device used for model: 0
[11/23 22:59:46 visual_prompt]: Setting up Evaluator...
[11/23 22:59:46 visual_prompt]: Setting up Trainer...
[11/23 22:59:46 visual_prompt]: 	Setting up the optimizer...
[11/23 22:59:46 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 23:01:26 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8376 s / batch. (data: 7.95e-03). ETA=12:50:33, max mem: 20.9 GB 
[11/23 23:03:01 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8240 s / batch. (data: 2.79e-04). ETA=12:36:40, max mem: 20.9 GB 
[11/23 23:04:40 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8400 s / batch. (data: 3.14e-04). ETA=12:50:00, max mem: 20.9 GB 
[11/23 23:06:15 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8440 s / batch. (data: 7.95e-03). ETA=12:52:15, max mem: 20.9 GB 
[11/23 23:07:53 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8189 s / batch. (data: 2.92e-04). ETA=12:27:54, max mem: 20.9 GB 
[11/23 23:08:44 visual_prompt]: Epoch 1 / 100: avg data time: 1.39e-01, avg batch time: 0.9718, average train loss: 1.5403
[11/23 23:09:38 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3110, average loss: 1.5201
[11/23 23:09:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 23:09:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/23 23:11:18 visual_prompt]: 	Training 100/553. train loss: 32.7968,	0.8680 s / batch. (data: 3.37e-04). ETA=13:10:33, max mem: 20.9 GB 
[11/23 23:12:54 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8461 s / batch. (data: 3.37e-04). ETA=12:49:13, max mem: 20.9 GB 
[11/23 23:14:31 visual_prompt]: 	Training 300/553. train loss: 2.9277,	0.8480 s / batch. (data: 1.53e-02). ETA=12:49:31, max mem: 20.9 GB 
[11/23 23:16:07 visual_prompt]: 	Training 400/553. train loss: 36.1745,	0.8440 s / batch. (data: 3.03e-04). ETA=12:44:28, max mem: 20.9 GB 
[11/23 23:17:45 visual_prompt]: 	Training 500/553. train loss: 13.4565,	0.8320 s / batch. (data: 3.20e-04). ETA=12:32:12, max mem: 20.9 GB 
[11/23 23:18:34 visual_prompt]: Epoch 2 / 100: avg data time: 1.37e-01, avg batch time: 0.9688, average train loss: 22.7402
[11/23 23:19:29 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3088, average loss: 21.8746
[11/23 23:19:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.07	
[11/23 23:19:29 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/23 23:21:08 visual_prompt]: 	Training 100/553. train loss: 16.2583,	0.8356 s / batch. (data: 2.98e-04). ETA=12:33:19, max mem: 20.9 GB 
[11/23 23:22:45 visual_prompt]: 	Training 200/553. train loss: 23.2656,	0.8424 s / batch. (data: 1.03e-02). ETA=12:38:04, max mem: 20.9 GB 
[11/23 23:24:21 visual_prompt]: 	Training 300/553. train loss: 11.3496,	0.8572 s / batch. (data: 8.69e-03). ETA=12:49:56, max mem: 20.9 GB 
[11/23 23:26:00 visual_prompt]: 	Training 400/553. train loss: 189.8524,	0.8321 s / batch. (data: 3.10e-04). ETA=12:26:00, max mem: 20.9 GB 
[11/23 23:27:38 visual_prompt]: 	Training 500/553. train loss: 22.7126,	1.1000 s / batch. (data: 2.67e-01). ETA=16:24:22, max mem: 20.9 GB 
[11/23 23:28:28 visual_prompt]: Epoch 3 / 100: avg data time: 1.44e-01, avg batch time: 0.9751, average train loss: 34.7822
[11/23 23:29:24 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3105, average loss: 19.8792
[11/23 23:29:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.07	
[11/23 23:29:24 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/23 23:31:07 visual_prompt]: 	Training 100/553. train loss: 6.3957,	0.8255 s / batch. (data: 1.05e-02). ETA=12:16:39, max mem: 20.9 GB 
[11/23 23:32:44 visual_prompt]: 	Training 200/553. train loss: 52.6492,	0.8303 s / batch. (data: 3.09e-04). ETA=12:19:34, max mem: 20.9 GB 
[11/23 23:34:22 visual_prompt]: 	Training 300/553. train loss: 10.5361,	1.1512 s / batch. (data: 3.01e-01). ETA=17:03:27, max mem: 20.9 GB 
[11/23 23:35:55 visual_prompt]: 	Training 400/553. train loss: 52.3675,	0.8357 s / batch. (data: 1.05e-02). ETA=12:21:34, max mem: 20.9 GB 
[11/23 23:37:34 visual_prompt]: 	Training 500/553. train loss: 40.5577,	3.3369 s / batch. (data: 2.51e+00). ETA=2 days, 1:15:27, max mem: 20.9 GB 
[11/23 23:38:26 visual_prompt]: Epoch 4 / 100: avg data time: 1.52e-01, avg batch time: 0.9804, average train loss: 63.2746
[11/23 23:39:22 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3105, average loss: 170.5102
[11/23 23:39:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.57	
[11/23 23:39:22 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/23 23:41:02 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8329 s / batch. (data: 2.86e-04). ETA=12:15:32, max mem: 20.9 GB 
[11/23 23:42:39 visual_prompt]: 	Training 200/553. train loss: 3.0704,	1.1760 s / batch. (data: 3.45e-01). ETA=17:16:37, max mem: 20.9 GB 
[11/23 23:44:17 visual_prompt]: 	Training 300/553. train loss: 18.1324,	0.8480 s / batch. (data: 4.86e-04). ETA=12:26:02, max mem: 20.9 GB 
[11/23 23:45:54 visual_prompt]: 	Training 400/553. train loss: 79.8581,	0.8476 s / batch. (data: 3.12e-04). ETA=12:24:19, max mem: 20.9 GB 
[11/23 23:47:32 visual_prompt]: 	Training 500/553. train loss: 23.4526,	0.8394 s / batch. (data: 3.03e-04). ETA=12:15:43, max mem: 20.9 GB 
[11/23 23:48:23 visual_prompt]: Epoch 5 / 100: avg data time: 1.52e-01, avg batch time: 0.9794, average train loss: 79.2135
[11/23 23:49:19 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3106, average loss: 143.7809
[11/23 23:49:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.37	
[11/23 23:49:19 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/23 23:51:02 visual_prompt]: 	Training 100/553. train loss: 6.2837,	0.8231 s / batch. (data: 4.71e-04). ETA=11:59:17, max mem: 20.9 GB 
[11/23 23:52:38 visual_prompt]: 	Training 200/553. train loss: 118.3204,	0.8612 s / batch. (data: 1.56e-02). ETA=12:31:10, max mem: 20.9 GB 
[11/23 23:54:13 visual_prompt]: 	Training 300/553. train loss: 54.3363,	0.8231 s / batch. (data: 3.09e-04). ETA=11:56:36, max mem: 20.9 GB 
[11/23 23:55:54 visual_prompt]: 	Training 400/553. train loss: 99.6455,	0.8213 s / batch. (data: 3.24e-04). ETA=11:53:38, max mem: 20.9 GB 
[11/23 23:57:30 visual_prompt]: 	Training 500/553. train loss: 30.0783,	0.8328 s / batch. (data: 3.50e-04). ETA=12:02:14, max mem: 20.9 GB 
[11/23 23:58:20 visual_prompt]: Epoch 6 / 100: avg data time: 1.49e-01, avg batch time: 0.9784, average train loss: 66.1335
[11/23 23:59:15 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3099, average loss: 99.1012
[11/23 23:59:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.41	
[11/23 23:59:15 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/24 00:00:55 visual_prompt]: 	Training 100/553. train loss: 0.2706,	0.8400 s / batch. (data: 3.08e-04). ETA=12:06:20, max mem: 20.9 GB 
[11/24 00:02:32 visual_prompt]: 	Training 200/553. train loss: 25.0724,	0.8355 s / batch. (data: 1.15e-02). ETA=12:01:05, max mem: 20.9 GB 
[11/24 00:04:13 visual_prompt]: 	Training 300/553. train loss: 10.4053,	1.7130 s / batch. (data: 8.95e-01). ETA=1 day, 0:35:31, max mem: 20.9 GB 
[11/24 00:05:49 visual_prompt]: 	Training 400/553. train loss: 14.4586,	1.7024 s / batch. (data: 8.82e-01). ETA=1 day, 0:23:35, max mem: 20.9 GB 
[11/24 00:07:25 visual_prompt]: 	Training 500/553. train loss: 24.6965,	0.8361 s / batch. (data: 3.05e-04). ETA=11:57:22, max mem: 20.9 GB 
[11/24 00:08:14 visual_prompt]: Epoch 7 / 100: avg data time: 1.47e-01, avg batch time: 0.9743, average train loss: 73.2909
[11/24 00:09:10 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3100, average loss: 76.7275
[11/24 00:09:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.65	
[11/24 00:09:10 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/24 00:10:48 visual_prompt]: 	Training 100/553. train loss: 15.1555,	0.8243 s / batch. (data: 3.10e-04). ETA=11:45:08, max mem: 20.9 GB 
[11/24 00:12:27 visual_prompt]: 	Training 200/553. train loss: 95.7314,	0.9227 s / batch. (data: 1.00e-01). ETA=13:07:51, max mem: 20.9 GB 
[11/24 00:14:05 visual_prompt]: 	Training 300/553. train loss: 156.5140,	0.8240 s / batch. (data: 3.06e-04). ETA=11:42:10, max mem: 20.9 GB 
[11/24 00:15:42 visual_prompt]: 	Training 400/553. train loss: 166.6465,	0.8587 s / batch. (data: 3.08e-02). ETA=12:10:20, max mem: 20.9 GB 
[11/24 00:17:19 visual_prompt]: 	Training 500/553. train loss: 285.2204,	1.2146 s / batch. (data: 3.85e-01). ETA=17:10:57, max mem: 20.9 GB 
[11/24 00:18:11 visual_prompt]: Epoch 8 / 100: avg data time: 1.52e-01, avg batch time: 0.9780, average train loss: 99.5197
[11/24 00:19:06 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3104, average loss: 35.6332
[11/24 00:19:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.89	
[11/24 00:19:06 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/24 00:20:47 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8252 s / batch. (data: 1.07e-02). ETA=11:38:22, max mem: 20.9 GB 
[11/24 00:22:23 visual_prompt]: 	Training 200/553. train loss: 43.8888,	0.8192 s / batch. (data: 3.11e-04). ETA=11:31:53, max mem: 20.9 GB 
[11/24 00:24:01 visual_prompt]: 	Training 300/553. train loss: 137.8746,	1.5866 s / batch. (data: 7.75e-01). ETA=22:17:24, max mem: 20.9 GB 
[11/24 00:25:39 visual_prompt]: 	Training 400/553. train loss: 35.5964,	0.8125 s / batch. (data: 5.41e-03). ETA=11:23:34, max mem: 20.9 GB 
[11/24 00:27:17 visual_prompt]: 	Training 500/553. train loss: 182.5312,	1.0834 s / batch. (data: 2.37e-01). ETA=15:09:35, max mem: 20.9 GB 
[11/24 00:28:07 visual_prompt]: Epoch 9 / 100: avg data time: 1.52e-01, avg batch time: 0.9786, average train loss: 78.0807
[11/24 00:29:03 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3101, average loss: 328.8175
[11/24 00:29:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.86	
[11/24 00:29:03 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/24 00:30:47 visual_prompt]: 	Training 100/553. train loss: 26.8022,	0.8373 s / batch. (data: 3.11e-04). ETA=11:40:49, max mem: 20.9 GB 
[11/24 00:32:22 visual_prompt]: 	Training 200/553. train loss: 121.5240,	0.8113 s / batch. (data: 5.42e-03). ETA=11:17:45, max mem: 20.9 GB 
[11/24 00:33:59 visual_prompt]: 	Training 300/553. train loss: 160.5820,	1.9756 s / batch. (data: 1.17e+00). ETA=1 day, 3:27:05, max mem: 20.9 GB 
[11/24 00:35:34 visual_prompt]: 	Training 400/553. train loss: 168.6008,	0.8160 s / batch. (data: 3.39e-04). ETA=11:18:57, max mem: 20.9 GB 
[11/24 00:37:10 visual_prompt]: 	Training 500/553. train loss: 579.5065,	0.8480 s / batch. (data: 7.95e-03). ETA=11:44:09, max mem: 20.9 GB 
[11/24 00:38:00 visual_prompt]: Epoch 10 / 100: avg data time: 1.45e-01, avg batch time: 0.9713, average train loss: 138.4034
[11/24 00:38:55 visual_prompt]: Inference (val):avg data time: 3.77e-04, avg batch time: 0.3098, average loss: 7.5994
[11/24 00:38:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.49	
[11/24 00:38:55 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/24 00:40:38 visual_prompt]: 	Training 100/553. train loss: 88.7464,	0.8233 s / batch. (data: 1.59e-02). ETA=11:21:30, max mem: 20.9 GB 
[11/24 00:42:16 visual_prompt]: 	Training 200/553. train loss: 85.2204,	0.8268 s / batch. (data: 1.05e-02). ETA=11:23:06, max mem: 20.9 GB 
[11/24 00:43:52 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.9236 s / batch. (data: 1.12e+00). ETA=1 day, 2:26:00, max mem: 20.9 GB 
[11/24 00:45:28 visual_prompt]: 	Training 400/553. train loss: 99.9953,	0.8139 s / batch. (data: 5.42e-03). ETA=11:09:42, max mem: 20.9 GB 
[11/24 00:47:04 visual_prompt]: 	Training 500/553. train loss: 108.4333,	0.8280 s / batch. (data: 3.04e-04). ETA=11:19:54, max mem: 20.9 GB 
[11/24 00:47:54 visual_prompt]: Epoch 11 / 100: avg data time: 1.48e-01, avg batch time: 0.9743, average train loss: 111.4261
[11/24 00:48:49 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3092, average loss: 323.4079
[11/24 00:48:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.19	
[11/24 00:48:49 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/24 00:50:32 visual_prompt]: 	Training 100/553. train loss: 40.6332,	0.8920 s / batch. (data: 6.66e-02). ETA=12:10:12, max mem: 20.9 GB 
[11/24 00:52:10 visual_prompt]: 	Training 200/553. train loss: 21.3151,	0.8656 s / batch. (data: 2.57e-02). ETA=11:47:09, max mem: 20.9 GB 
[11/24 00:53:45 visual_prompt]: 	Training 300/553. train loss: 58.6921,	0.8403 s / batch. (data: 7.96e-03). ETA=11:25:05, max mem: 20.9 GB 
[11/24 00:55:23 visual_prompt]: 	Training 400/553. train loss: 200.1610,	0.8320 s / batch. (data: 3.11e-04). ETA=11:16:56, max mem: 20.9 GB 
[11/24 00:57:00 visual_prompt]: 	Training 500/553. train loss: 466.1448,	0.8400 s / batch. (data: 3.06e-04). ETA=11:22:01, max mem: 20.9 GB 
[11/24 00:57:50 visual_prompt]: Epoch 12 / 100: avg data time: 1.51e-01, avg batch time: 0.9772, average train loss: 115.6275
[11/24 00:58:45 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3101, average loss: 121.8249
[11/24 00:58:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.01	
[11/24 00:58:45 visual_prompt]: Best epoch 12: best metric: -121.825
[11/24 00:58:45 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/24 01:00:28 visual_prompt]: 	Training 100/553. train loss: 104.7513,	0.8360 s / batch. (data: 7.98e-03). ETA=11:16:41, max mem: 20.9 GB 
[11/24 01:02:02 visual_prompt]: 	Training 200/553. train loss: 6.9765,	0.8440 s / batch. (data: 3.13e-04). ETA=11:21:43, max mem: 20.9 GB 
[11/24 01:03:40 visual_prompt]: 	Training 300/553. train loss: 98.9173,	1.6829 s / batch. (data: 8.44e-01). ETA=22:36:31, max mem: 20.9 GB 
[11/24 01:05:16 visual_prompt]: 	Training 400/553. train loss: 208.4926,	0.8240 s / batch. (data: 5.45e-03). ETA=11:02:48, max mem: 20.9 GB 
[11/24 01:06:54 visual_prompt]: 	Training 500/553. train loss: 157.6126,	0.8676 s / batch. (data: 2.48e-04). ETA=11:36:27, max mem: 20.9 GB 
[11/24 01:07:45 visual_prompt]: Epoch 13 / 100: avg data time: 1.49e-01, avg batch time: 0.9753, average train loss: 87.1781
[11/24 01:08:40 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3085, average loss: 192.0014
[11/24 01:08:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.44	
[11/24 01:08:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/24 01:10:22 visual_prompt]: 	Training 100/553. train loss: 31.8856,	0.8596 s / batch. (data: 2.20e-02). ETA=11:27:52, max mem: 20.9 GB 
[11/24 01:11:59 visual_prompt]: 	Training 200/553. train loss: 20.6185,	0.8472 s / batch. (data: 1.48e-02). ETA=11:16:28, max mem: 20.9 GB 
[11/24 01:13:36 visual_prompt]: 	Training 300/553. train loss: 50.9220,	0.8160 s / batch. (data: 3.12e-04). ETA=10:50:14, max mem: 20.9 GB 
[11/24 01:15:13 visual_prompt]: 	Training 400/553. train loss: 113.5434,	0.8352 s / batch. (data: 2.96e-04). ETA=11:04:08, max mem: 20.9 GB 
[11/24 01:16:50 visual_prompt]: 	Training 500/553. train loss: 16.4016,	0.8453 s / batch. (data: 7.95e-03). ETA=11:10:44, max mem: 20.9 GB 
[11/24 01:17:39 visual_prompt]: Epoch 14 / 100: avg data time: 1.48e-01, avg batch time: 0.9749, average train loss: 95.4231
[11/24 01:18:35 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3095, average loss: 35.9889
[11/24 01:18:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.24	
[11/24 01:18:35 visual_prompt]: Best epoch 14: best metric: -35.989
[11/24 01:18:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/24 01:20:16 visual_prompt]: 	Training 100/553. train loss: 77.4675,	0.9576 s / batch. (data: 1.37e-01). ETA=12:37:24, max mem: 20.9 GB 
[11/24 01:21:52 visual_prompt]: 	Training 200/553. train loss: 24.2004,	0.8320 s / batch. (data: 2.93e-04). ETA=10:56:41, max mem: 20.9 GB 
[11/24 01:23:30 visual_prompt]: 	Training 300/553. train loss: 144.2614,	0.8207 s / batch. (data: 8.56e-03). ETA=10:46:22, max mem: 20.9 GB 
[11/24 01:25:05 visual_prompt]: 	Training 400/553. train loss: 97.0569,	1.1560 s / batch. (data: 3.27e-01). ETA=15:08:36, max mem: 20.9 GB 
[11/24 01:26:43 visual_prompt]: 	Training 500/553. train loss: 26.6353,	0.8756 s / batch. (data: 2.76e-02). ETA=11:26:43, max mem: 20.9 GB 
[11/24 01:27:34 visual_prompt]: Epoch 15 / 100: avg data time: 1.48e-01, avg batch time: 0.9761, average train loss: 137.0193
[11/24 01:28:30 visual_prompt]: Inference (val):avg data time: 1.76e-04, avg batch time: 0.3098, average loss: 318.4160
[11/24 01:28:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.17	
[11/24 01:28:30 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/24 01:30:10 visual_prompt]: 	Training 100/553. train loss: 188.9208,	0.8320 s / batch. (data: 3.03e-04). ETA=10:50:25, max mem: 20.9 GB 
[11/24 01:31:47 visual_prompt]: 	Training 200/553. train loss: 44.1214,	0.8148 s / batch. (data: 3.05e-04). ETA=10:35:38, max mem: 20.9 GB 
[11/24 01:33:25 visual_prompt]: 	Training 300/553. train loss: 83.9578,	0.8403 s / batch. (data: 2.06e-02). ETA=10:54:06, max mem: 20.9 GB 
[11/24 01:35:02 visual_prompt]: 	Training 400/553. train loss: 95.3845,	0.8228 s / batch. (data: 5.41e-03). ETA=10:39:04, max mem: 20.9 GB 
[11/24 01:36:39 visual_prompt]: 	Training 500/553. train loss: 26.6261,	1.6370 s / batch. (data: 8.13e-01). ETA=21:08:48, max mem: 20.9 GB 
[11/24 01:37:30 visual_prompt]: Epoch 16 / 100: avg data time: 1.50e-01, avg batch time: 0.9767, average train loss: 90.8547
[11/24 01:38:26 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3103, average loss: 12.5311
[11/24 01:38:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[11/24 01:38:26 visual_prompt]: Best epoch 16: best metric: -12.531
[11/24 01:38:26 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/24 01:40:06 visual_prompt]: 	Training 100/553. train loss: 123.2160,	0.8200 s / batch. (data: 3.33e-04). ETA=10:33:27, max mem: 20.9 GB 
[11/24 01:41:44 visual_prompt]: 	Training 200/553. train loss: 49.1753,	0.8403 s / batch. (data: 5.20e-03). ETA=10:47:44, max mem: 20.9 GB 
[11/24 01:43:20 visual_prompt]: 	Training 300/553. train loss: 124.8512,	0.8760 s / batch. (data: 2.73e-02). ETA=11:13:49, max mem: 20.9 GB 
[11/24 01:44:57 visual_prompt]: 	Training 400/553. train loss: 30.3349,	1.0120 s / batch. (data: 1.61e-01). ETA=12:56:44, max mem: 20.9 GB 
[11/24 01:46:34 visual_prompt]: 	Training 500/553. train loss: 288.6173,	1.6240 s / batch. (data: 8.14e-01). ETA=20:43:46, max mem: 20.9 GB 
[11/24 01:47:26 visual_prompt]: Epoch 17 / 100: avg data time: 1.49e-01, avg batch time: 0.9767, average train loss: 94.9273
[11/24 01:48:21 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3098, average loss: 44.3459
[11/24 01:48:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.74	
[11/24 01:48:21 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/24 01:50:02 visual_prompt]: 	Training 100/553. train loss: 103.0587,	0.8204 s / batch. (data: 8.88e-03). ETA=10:26:14, max mem: 20.9 GB 
[11/24 01:51:42 visual_prompt]: 	Training 200/553. train loss: 12.4207,	0.8680 s / batch. (data: 3.02e-04). ETA=11:01:06, max mem: 20.9 GB 
[11/24 01:53:19 visual_prompt]: 	Training 300/553. train loss: 114.4238,	0.8515 s / batch. (data: 8.18e-03). ETA=10:47:08, max mem: 20.9 GB 
[11/24 01:54:56 visual_prompt]: 	Training 400/553. train loss: 77.0176,	0.8284 s / batch. (data: 1.57e-02). ETA=10:28:12, max mem: 20.9 GB 
[11/24 01:56:31 visual_prompt]: 	Training 500/553. train loss: 48.2409,	0.8320 s / batch. (data: 5.42e-03). ETA=10:29:31, max mem: 20.9 GB 
[11/24 01:57:20 visual_prompt]: Epoch 18 / 100: avg data time: 1.47e-01, avg batch time: 0.9741, average train loss: 95.3656
[11/24 01:58:14 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3089, average loss: 165.1938
[11/24 01:58:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.67	
[11/24 01:58:14 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/24 01:59:55 visual_prompt]: 	Training 100/553. train loss: 37.5920,	0.8359 s / batch. (data: 2.12e-02). ETA=10:30:22, max mem: 20.9 GB 
[11/24 02:01:34 visual_prompt]: 	Training 200/553. train loss: 78.9635,	0.8444 s / batch. (data: 5.86e-03). ETA=10:35:22, max mem: 20.9 GB 
[11/24 02:03:12 visual_prompt]: 	Training 300/553. train loss: 444.9015,	0.8375 s / batch. (data: 3.24e-04). ETA=10:28:44, max mem: 20.9 GB 
[11/24 02:04:51 visual_prompt]: 	Training 400/553. train loss: 80.1770,	0.8430 s / batch. (data: 1.09e-02). ETA=10:31:30, max mem: 20.9 GB 
[11/24 02:06:25 visual_prompt]: 	Training 500/553. train loss: 267.2652,	0.8160 s / batch. (data: 2.92e-04). ETA=10:09:54, max mem: 20.9 GB 
[11/24 02:07:16 visual_prompt]: Epoch 19 / 100: avg data time: 1.53e-01, avg batch time: 0.9791, average train loss: 111.7685
[11/24 02:08:11 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3092, average loss: 261.0260
[11/24 02:08:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.96	
[11/24 02:08:11 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/24 02:09:51 visual_prompt]: 	Training 100/553. train loss: 41.4846,	0.8160 s / batch. (data: 3.10e-04). ETA=10:07:49, max mem: 20.9 GB 
[11/24 02:11:29 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8149 s / batch. (data: 3.13e-04). ETA=10:05:38, max mem: 20.9 GB 
[11/24 02:13:06 visual_prompt]: 	Training 300/553. train loss: 32.8508,	0.8146 s / batch. (data: 3.25e-04). ETA=10:04:02, max mem: 20.9 GB 
[11/24 02:14:42 visual_prompt]: 	Training 400/553. train loss: 91.8494,	0.8263 s / batch. (data: 1.05e-02). ETA=10:11:23, max mem: 20.9 GB 
[11/24 02:16:19 visual_prompt]: 	Training 500/553. train loss: 37.3914,	0.8559 s / batch. (data: 3.08e-04). ETA=10:31:52, max mem: 20.9 GB 
[11/24 02:17:11 visual_prompt]: Epoch 20 / 100: avg data time: 1.50e-01, avg batch time: 0.9760, average train loss: 83.9298
[11/24 02:18:05 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3090, average loss: 56.4203
[11/24 02:18:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.07	
[11/24 02:18:05 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/24 02:19:48 visual_prompt]: 	Training 100/553. train loss: 44.5488,	0.8120 s / batch. (data: 3.19e-04). ETA=9:57:21, max mem: 20.9 GB 
[11/24 02:21:24 visual_prompt]: 	Training 200/553. train loss: 0.0001,	0.8401 s / batch. (data: 2.41e-04). ETA=10:16:35, max mem: 20.9 GB 
[11/24 02:23:00 visual_prompt]: 	Training 300/553. train loss: 287.9181,	0.9748 s / batch. (data: 1.47e-01). ETA=11:53:53, max mem: 20.9 GB 
[11/24 02:24:35 visual_prompt]: 	Training 400/553. train loss: 398.9373,	0.8320 s / batch. (data: 5.41e-03). ETA=10:07:55, max mem: 20.9 GB 
[11/24 02:26:14 visual_prompt]: 	Training 500/553. train loss: 99.6390,	0.8245 s / batch. (data: 3.11e-04). ETA=10:01:01, max mem: 20.9 GB 
[11/24 02:27:03 visual_prompt]: Epoch 21 / 100: avg data time: 1.45e-01, avg batch time: 0.9725, average train loss: 98.6770
[11/24 02:27:58 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3080, average loss: 27.0083
[11/24 02:27:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.06	
[11/24 02:27:58 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/24 02:29:38 visual_prompt]: 	Training 100/553. train loss: 215.8323,	0.8516 s / batch. (data: 1.05e-02). ETA=10:18:37, max mem: 20.9 GB 
[11/24 02:31:15 visual_prompt]: 	Training 200/553. train loss: 27.9844,	0.8463 s / batch. (data: 5.42e-03). ETA=10:13:22, max mem: 20.9 GB 
[11/24 02:32:49 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8014 s / batch. (data: 3.05e-04). ETA=9:39:28, max mem: 20.9 GB 
[11/24 02:34:28 visual_prompt]: 	Training 400/553. train loss: 2.6714,	0.8532 s / batch. (data: 2.57e-02). ETA=10:15:34, max mem: 20.9 GB 
[11/24 02:36:05 visual_prompt]: 	Training 500/553. train loss: 100.9906,	0.8103 s / batch. (data: 3.03e-04). ETA=9:43:13, max mem: 20.9 GB 
[11/24 02:36:56 visual_prompt]: Epoch 22 / 100: avg data time: 1.45e-01, avg batch time: 0.9717, average train loss: 81.1880
[11/24 02:37:51 visual_prompt]: Inference (val):avg data time: 4.03e-04, avg batch time: 0.3094, average loss: 61.1178
[11/24 02:37:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.34	
[11/24 02:37:51 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/24 02:39:33 visual_prompt]: 	Training 100/553. train loss: 140.7388,	0.9760 s / batch. (data: 1.40e-01). ETA=11:40:01, max mem: 20.9 GB 
[11/24 02:41:11 visual_prompt]: 	Training 200/553. train loss: 43.6858,	0.8442 s / batch. (data: 5.43e-03). ETA=10:04:04, max mem: 20.9 GB 
[11/24 02:42:49 visual_prompt]: 	Training 300/553. train loss: 9.4103,	0.8437 s / batch. (data: 3.03e-04). ETA=10:02:19, max mem: 20.9 GB 
[11/24 02:44:25 visual_prompt]: 	Training 400/553. train loss: 2.1598,	0.8480 s / batch. (data: 8.18e-04). ETA=10:03:58, max mem: 20.9 GB 
[11/24 02:45:59 visual_prompt]: 	Training 500/553. train loss: 317.9267,	0.8230 s / batch. (data: 5.40e-03). ETA=9:44:48, max mem: 20.9 GB 
[11/24 02:46:50 visual_prompt]: Epoch 23 / 100: avg data time: 1.48e-01, avg batch time: 0.9747, average train loss: 82.0441
[11/24 02:47:45 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3096, average loss: 39.6244
[11/24 02:47:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.19	
[11/24 02:47:45 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/24 02:49:24 visual_prompt]: 	Training 100/553. train loss: 20.0959,	0.8610 s / batch. (data: 3.72e-02). ETA=10:09:37, max mem: 20.9 GB 
[11/24 02:51:00 visual_prompt]: 	Training 200/553. train loss: 130.3906,	0.8440 s / batch. (data: 1.20e-02). ETA=9:56:08, max mem: 20.9 GB 
[11/24 02:52:38 visual_prompt]: 	Training 300/553. train loss: 8.9930,	0.8640 s / batch. (data: 3.15e-02). ETA=10:08:52, max mem: 20.9 GB 
[11/24 02:54:15 visual_prompt]: 	Training 400/553. train loss: 39.7635,	0.8260 s / batch. (data: 2.25e-04). ETA=9:40:39, max mem: 20.9 GB 
[11/24 02:55:53 visual_prompt]: 	Training 500/553. train loss: 108.8881,	0.8414 s / batch. (data: 5.47e-03). ETA=9:50:05, max mem: 20.9 GB 
[11/24 02:56:44 visual_prompt]: Epoch 24 / 100: avg data time: 1.47e-01, avg batch time: 0.9741, average train loss: 81.1871
[11/24 02:57:39 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3106, average loss: 28.5256
[11/24 02:57:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 64.54	
[11/24 02:57:39 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/24 02:59:23 visual_prompt]: 	Training 100/553. train loss: 91.4839,	0.8424 s / batch. (data: 7.93e-03). ETA=9:48:40, max mem: 20.9 GB 
[11/24 03:00:57 visual_prompt]: 	Training 200/553. train loss: 77.1709,	0.8350 s / batch. (data: 3.78e-04). ETA=9:42:07, max mem: 20.9 GB 
[11/24 03:02:33 visual_prompt]: 	Training 300/553. train loss: 43.7938,	0.8240 s / batch. (data: 7.47e-03). ETA=9:33:03, max mem: 20.9 GB 
[11/24 03:04:09 visual_prompt]: 	Training 400/553. train loss: 102.3129,	1.1560 s / batch. (data: 3.12e-01). ETA=13:22:02, max mem: 20.9 GB 
[11/24 03:05:46 visual_prompt]: 	Training 500/553. train loss: 86.9998,	1.3636 s / batch. (data: 5.46e-01). ETA=15:43:47, max mem: 20.9 GB 
[11/24 03:06:37 visual_prompt]: Epoch 25 / 100: avg data time: 1.45e-01, avg batch time: 0.9716, average train loss: 77.0688
[11/24 03:07:32 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3130, average loss: 23.5282
[11/24 03:07:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 67.86	
[11/24 03:07:32 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/24 03:09:12 visual_prompt]: 	Training 100/553. train loss: 27.9279,	0.8307 s / batch. (data: 5.39e-03). ETA=9:32:52, max mem: 20.9 GB 
[11/24 03:10:50 visual_prompt]: 	Training 200/553. train loss: 114.5106,	1.6120 s / batch. (data: 7.76e-01). ETA=18:28:54, max mem: 20.9 GB 
[11/24 03:12:27 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8439 s / batch. (data: 7.85e-04). ETA=9:39:07, max mem: 20.9 GB 
[11/24 03:14:02 visual_prompt]: 	Training 400/553. train loss: 5.7127,	0.8360 s / batch. (data: 2.93e-04). ETA=9:32:18, max mem: 20.9 GB 
[11/24 03:15:37 visual_prompt]: 	Training 500/553. train loss: 82.9827,	0.8466 s / batch. (data: 2.26e-02). ETA=9:38:10, max mem: 20.9 GB 
[11/24 03:16:27 visual_prompt]: Epoch 26 / 100: avg data time: 1.41e-01, avg batch time: 0.9679, average train loss: 83.6908
[11/24 03:17:22 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3099, average loss: 96.7490
[11/24 03:17:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.30	
[11/24 03:17:22 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/24 03:19:03 visual_prompt]: 	Training 100/553. train loss: 25.1682,	0.8239 s / batch. (data: 3.14e-04). ETA=9:20:34, max mem: 20.9 GB 
[11/24 03:20:40 visual_prompt]: 	Training 200/553. train loss: 37.1963,	0.9392 s / batch. (data: 1.13e-01). ETA=10:37:28, max mem: 20.9 GB 
[11/24 03:22:17 visual_prompt]: 	Training 300/553. train loss: 144.5471,	0.8572 s / batch. (data: 2.13e-02). ETA=9:40:21, max mem: 20.9 GB 
[11/24 03:23:55 visual_prompt]: 	Training 400/553. train loss: 49.3821,	0.8585 s / batch. (data: 5.93e-03). ETA=9:39:46, max mem: 20.9 GB 
[11/24 03:25:33 visual_prompt]: 	Training 500/553. train loss: 55.3535,	0.8386 s / batch. (data: 3.29e-04). ETA=9:24:59, max mem: 20.9 GB 
[11/24 03:26:22 visual_prompt]: Epoch 27 / 100: avg data time: 1.51e-01, avg batch time: 0.9765, average train loss: 103.2925
[11/24 03:27:17 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3103, average loss: 34.6598
[11/24 03:27:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 69.30	
[11/24 03:27:17 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/24 03:28:57 visual_prompt]: 	Training 100/553. train loss: 13.8401,	0.8520 s / batch. (data: 1.60e-02). ETA=9:31:48, max mem: 20.9 GB 
[11/24 03:30:34 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8622 s / batch. (data: 3.10e-04). ETA=9:37:13, max mem: 20.9 GB 
[11/24 03:32:12 visual_prompt]: 	Training 300/553. train loss: 211.2960,	1.4037 s / batch. (data: 5.88e-01). ETA=15:37:24, max mem: 20.9 GB 
[11/24 03:33:48 visual_prompt]: 	Training 400/553. train loss: 51.1757,	0.8140 s / batch. (data: 3.02e-04). ETA=9:02:16, max mem: 20.9 GB 
[11/24 03:35:23 visual_prompt]: 	Training 500/553. train loss: 0.0495,	0.8358 s / batch. (data: 5.42e-03). ETA=9:15:22, max mem: 20.9 GB 
[11/24 03:36:15 visual_prompt]: Epoch 28 / 100: avg data time: 1.46e-01, avg batch time: 0.9719, average train loss: 84.6978
[11/24 03:37:10 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3106, average loss: 36.3148
[11/24 03:37:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 68.10	
[11/24 03:37:10 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/24 03:38:57 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8073 s / batch. (data: 2.50e-04). ETA=8:54:21, max mem: 20.9 GB 
[11/24 03:40:32 visual_prompt]: 	Training 200/553. train loss: 3.2430,	1.7440 s / batch. (data: 9.13e-01). ETA=19:11:30, max mem: 20.9 GB 
[11/24 03:42:07 visual_prompt]: 	Training 300/553. train loss: 189.5812,	0.8320 s / batch. (data: 8.95e-04). ETA=9:07:58, max mem: 20.9 GB 
[11/24 03:43:40 visual_prompt]: 	Training 400/553. train loss: 149.5882,	1.4085 s / batch. (data: 5.66e-01). ETA=15:25:18, max mem: 20.9 GB 
[11/24 03:45:17 visual_prompt]: 	Training 500/553. train loss: 33.2032,	0.8486 s / batch. (data: 1.56e-02). ETA=9:16:03, max mem: 20.9 GB 
[11/24 03:46:08 visual_prompt]: Epoch 29 / 100: avg data time: 1.45e-01, avg batch time: 0.9720, average train loss: 90.5373
[11/24 03:47:02 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3113, average loss: 19.8268
[11/24 03:47:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 67.25	
[11/24 03:47:02 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/24 03:48:41 visual_prompt]: 	Training 100/553. train loss: 47.6176,	0.8345 s / batch. (data: 2.98e-04). ETA=9:04:43, max mem: 20.9 GB 
[11/24 03:50:19 visual_prompt]: 	Training 200/553. train loss: 65.9663,	0.8195 s / batch. (data: 3.24e-04). ETA=8:53:32, max mem: 20.9 GB 
[11/24 03:51:55 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8160 s / batch. (data: 3.16e-04). ETA=8:49:53, max mem: 20.9 GB 
[11/24 03:53:34 visual_prompt]: 	Training 400/553. train loss: 375.7125,	0.8480 s / batch. (data: 3.99e-04). ETA=9:09:15, max mem: 20.9 GB 
[11/24 03:55:09 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.4760 s / batch. (data: 6.55e-01). ETA=15:53:34, max mem: 20.9 GB 
[11/24 03:56:01 visual_prompt]: Epoch 30 / 100: avg data time: 1.46e-01, avg batch time: 0.9733, average train loss: 86.8718
[11/24 03:56:56 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3085, average loss: 26.1372
[11/24 03:56:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 68.86	
[11/24 03:56:56 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[11/24 03:58:38 visual_prompt]: 	Training 100/553. train loss: 79.1386,	0.8080 s / batch. (data: 3.07e-04). ETA=8:39:56, max mem: 20.9 GB 
[11/24 04:00:17 visual_prompt]: 	Training 200/553. train loss: 163.1584,	0.8160 s / batch. (data: 5.42e-03). ETA=8:43:44, max mem: 20.9 GB 
[11/24 04:01:52 visual_prompt]: 	Training 300/553. train loss: 41.7450,	0.8280 s / batch. (data: 3.73e-04). ETA=8:50:03, max mem: 20.9 GB 
[11/24 04:03:28 visual_prompt]: 	Training 400/553. train loss: 0.0001,	1.0828 s / batch. (data: 2.50e-01). ETA=11:31:23, max mem: 20.9 GB 
[11/24 04:05:04 visual_prompt]: 	Training 500/553. train loss: 55.1165,	0.8457 s / batch. (data: 1.05e-02). ETA=8:58:32, max mem: 20.9 GB 
[11/24 04:05:54 visual_prompt]: Epoch 31 / 100: avg data time: 1.46e-01, avg batch time: 0.9722, average train loss: 85.4424
[11/24 04:06:49 visual_prompt]: Inference (val):avg data time: 2.22e-04, avg batch time: 0.3108, average loss: 91.3646
[11/24 04:06:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.24	
[11/24 04:06:49 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[11/24 04:08:31 visual_prompt]: 	Training 100/553. train loss: 45.1156,	0.8760 s / batch. (data: 7.85e-04). ETA=9:15:36, max mem: 20.9 GB 
[11/24 04:10:07 visual_prompt]: 	Training 200/553. train loss: 67.7762,	0.8225 s / batch. (data: 7.14e-04). ETA=8:40:17, max mem: 20.9 GB 
[11/24 04:11:47 visual_prompt]: 	Training 300/553. train loss: 129.0591,	0.8166 s / batch. (data: 3.13e-04). ETA=8:35:15, max mem: 20.9 GB 
[11/24 04:13:24 visual_prompt]: 	Training 400/553. train loss: 68.5516,	0.8600 s / batch. (data: 7.96e-03). ETA=9:01:09, max mem: 20.9 GB 
[11/24 04:14:58 visual_prompt]: 	Training 500/553. train loss: 40.9556,	0.8360 s / batch. (data: 3.04e-04). ETA=8:44:41, max mem: 20.9 GB 
[11/24 04:15:47 visual_prompt]: Epoch 32 / 100: avg data time: 1.47e-01, avg batch time: 0.9730, average train loss: 70.2997
[11/24 04:16:42 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3103, average loss: 17.1912
[11/24 04:16:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 70.16	
[11/24 04:16:42 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[11/24 04:18:22 visual_prompt]: 	Training 100/553. train loss: 31.4517,	1.0013 s / batch. (data: 1.84e-01). ETA=10:25:51, max mem: 20.9 GB 
[11/24 04:20:00 visual_prompt]: 	Training 200/553. train loss: 305.1340,	1.4540 s / batch. (data: 6.22e-01). ETA=15:06:26, max mem: 20.9 GB 
[11/24 04:21:36 visual_prompt]: 	Training 300/553. train loss: 6.9267,	0.8360 s / batch. (data: 1.20e-02). ETA=8:39:46, max mem: 20.9 GB 
[11/24 04:23:13 visual_prompt]: 	Training 400/553. train loss: 30.0530,	0.8266 s / batch. (data: 5.41e-03). ETA=8:32:31, max mem: 20.9 GB 
[11/24 04:24:49 visual_prompt]: 	Training 500/553. train loss: 43.6602,	0.8400 s / batch. (data: 3.11e-04). ETA=8:39:26, max mem: 20.9 GB 
[11/24 04:25:39 visual_prompt]: Epoch 33 / 100: avg data time: 1.43e-01, avg batch time: 0.9695, average train loss: 92.9202
[11/24 04:26:34 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3120, average loss: 27.9399
[11/24 04:26:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 67.38	
[11/24 04:26:34 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[11/24 04:28:15 visual_prompt]: 	Training 100/553. train loss: 25.6993,	0.8640 s / batch. (data: 4.61e-02). ETA=8:52:04, max mem: 20.9 GB 
[11/24 04:29:51 visual_prompt]: 	Training 200/553. train loss: 53.0974,	0.8200 s / batch. (data: 3.11e-04). ETA=8:23:36, max mem: 20.9 GB 
[11/24 04:31:26 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8222 s / batch. (data: 3.40e-04). ETA=8:23:35, max mem: 20.9 GB 
[11/24 04:33:03 visual_prompt]: 	Training 400/553. train loss: 113.9986,	0.8238 s / batch. (data: 2.83e-04). ETA=8:23:12, max mem: 20.9 GB 
[11/24 04:34:40 visual_prompt]: 	Training 500/553. train loss: 12.1594,	1.3765 s / batch. (data: 5.64e-01). ETA=13:58:31, max mem: 20.9 GB 
[11/24 04:35:30 visual_prompt]: Epoch 34 / 100: avg data time: 1.42e-01, avg batch time: 0.9692, average train loss: 65.4234
[11/24 04:36:25 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3107, average loss: 40.9116
[11/24 04:36:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 67.02	
[11/24 04:36:25 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[11/24 04:38:07 visual_prompt]: 	Training 100/553. train loss: 262.2329,	0.8200 s / batch. (data: 3.42e-04). ETA=8:17:26, max mem: 20.9 GB 
[11/24 04:39:45 visual_prompt]: 	Training 200/553. train loss: 113.3687,	0.8222 s / batch. (data: 5.43e-03). ETA=8:17:25, max mem: 20.9 GB 
[11/24 04:41:20 visual_prompt]: 	Training 300/553. train loss: 57.4261,	0.8104 s / batch. (data: 3.05e-04). ETA=8:08:54, max mem: 20.9 GB 
[11/24 04:42:55 visual_prompt]: 	Training 400/553. train loss: 136.0901,	0.8480 s / batch. (data: 1.22e-02). ETA=8:30:11, max mem: 20.9 GB 
[11/24 04:44:33 visual_prompt]: 	Training 500/553. train loss: 66.5506,	1.1291 s / batch. (data: 2.92e-01). ETA=11:17:26, max mem: 20.9 GB 
[11/24 04:45:24 visual_prompt]: Epoch 35 / 100: avg data time: 1.47e-01, avg batch time: 0.9736, average train loss: 97.3900
[11/24 04:46:19 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3109, average loss: 19.0127
[11/24 04:46:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 66.08	
[11/24 04:46:19 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[11/24 04:47:59 visual_prompt]: 	Training 100/553. train loss: 235.7105,	0.8200 s / batch. (data: 7.95e-03). ETA=8:09:53, max mem: 20.9 GB 
[11/24 04:49:36 visual_prompt]: 	Training 200/553. train loss: 331.8435,	0.8336 s / batch. (data: 2.90e-04). ETA=8:16:37, max mem: 20.9 GB 
[11/24 04:51:15 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8228 s / batch. (data: 7.94e-03). ETA=8:08:48, max mem: 20.9 GB 
[11/24 04:52:51 visual_prompt]: 	Training 400/553. train loss: 48.5739,	0.8320 s / batch. (data: 3.08e-04). ETA=8:12:52, max mem: 20.9 GB 
[11/24 04:54:28 visual_prompt]: 	Training 500/553. train loss: 54.7954,	1.1241 s / batch. (data: 3.20e-01). ETA=11:04:05, max mem: 20.9 GB 
[11/24 04:55:16 visual_prompt]: Epoch 36 / 100: avg data time: 1.45e-01, avg batch time: 0.9715, average train loss: 85.6177
[11/24 04:56:12 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3093, average loss: 24.8498
[11/24 04:56:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 66.00	
[11/24 04:56:12 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[11/24 04:57:53 visual_prompt]: 	Training 100/553. train loss: 132.9640,	0.8501 s / batch. (data: 2.61e-02). ETA=8:20:01, max mem: 20.9 GB 
[11/24 04:59:28 visual_prompt]: 	Training 200/553. train loss: 27.2551,	0.8520 s / batch. (data: 3.22e-04). ETA=8:19:43, max mem: 20.9 GB 
[11/24 05:01:06 visual_prompt]: 	Training 300/553. train loss: 226.2066,	1.2284 s / batch. (data: 3.87e-01). ETA=11:58:26, max mem: 20.9 GB 
[11/24 05:02:44 visual_prompt]: 	Training 400/553. train loss: 116.5245,	1.6559 s / batch. (data: 8.31e-01). ETA=16:05:44, max mem: 20.9 GB 
[11/24 05:04:17 visual_prompt]: 	Training 500/553. train loss: 65.7784,	0.8280 s / batch. (data: 7.96e-03). ETA=8:01:30, max mem: 20.9 GB 
[11/24 05:05:09 visual_prompt]: Epoch 37 / 100: avg data time: 1.45e-01, avg batch time: 0.9716, average train loss: 57.7109
[11/24 05:06:04 visual_prompt]: Inference (val):avg data time: 1.71e-04, avg batch time: 0.3095, average loss: 64.5826
[11/24 05:06:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 70.45	
[11/24 05:06:04 visual_prompt]: Stopping early.
[11/24 05:06:04 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 05:06:04 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 05:06:04 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/24 05:06:04 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 05:06:04 visual_prompt]: Training with config:
[11/24 05:06:04 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/24 05:06:04 visual_prompt]: Loading training data...
[11/24 05:06:04 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 05:06:04 visual_prompt]: Loading validation data...
[11/24 05:06:04 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 05:06:04 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 05:06:07 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 05:06:07 visual_prompt]: tuned percent:0.525
[11/24 05:06:07 visual_prompt]: Device used for model: 0
[11/24 05:06:07 visual_prompt]: Setting up Evaluator...
[11/24 05:06:07 visual_prompt]: Setting up Trainer...
[11/24 05:06:07 visual_prompt]: 	Setting up the optimizer...
[11/24 05:06:07 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 05:07:47 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8376 s / batch. (data: 1.42e-02). ETA=12:50:35, max mem: 20.9 GB 
[11/24 05:09:23 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8200 s / batch. (data: 2.98e-04). ETA=12:33:01, max mem: 20.9 GB 
[11/24 05:11:02 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.8332 s / batch. (data: 1.00e+00). ETA=1 day, 4:00:23, max mem: 20.9 GB 
[11/24 05:12:37 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8562 s / batch. (data: 1.21e-02). ETA=13:03:24, max mem: 20.9 GB 
[11/24 05:14:16 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8341 s / batch. (data: 1.38e-02). ETA=12:41:50, max mem: 20.9 GB 
[11/24 05:15:08 visual_prompt]: Epoch 1 / 100: avg data time: 1.44e-01, avg batch time: 0.9773, average train loss: 1.5403
[11/24 05:16:03 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3108, average loss: 1.5201
[11/24 05:16:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 05:16:03 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/24 05:17:43 visual_prompt]: 	Training 100/553. train loss: 12.5197,	0.8686 s / batch. (data: 2.95e-04). ETA=13:11:03, max mem: 20.9 GB 
[11/24 05:19:20 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8320 s / batch. (data: 3.52e-04). ETA=12:36:22, max mem: 20.9 GB 
[11/24 05:20:58 visual_prompt]: 	Training 300/553. train loss: 11.5898,	0.9584 s / batch. (data: 1.16e-01). ETA=14:29:39, max mem: 20.9 GB 
[11/24 05:22:35 visual_prompt]: 	Training 400/553. train loss: 19.1761,	0.8450 s / batch. (data: 5.41e-03). ETA=12:45:24, max mem: 20.9 GB 
[11/24 05:24:13 visual_prompt]: 	Training 500/553. train loss: 3.7540,	0.8440 s / batch. (data: 3.11e-04). ETA=12:43:03, max mem: 20.9 GB 
[11/24 05:25:03 visual_prompt]: Epoch 2 / 100: avg data time: 1.42e-01, avg batch time: 0.9755, average train loss: 8.2145
[11/24 05:25:58 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3105, average loss: 5.2982
[11/24 05:25:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.54	
[11/24 05:25:58 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/24 05:27:38 visual_prompt]: 	Training 100/553. train loss: 43.9622,	0.8287 s / batch. (data: 3.18e-04). ETA=12:27:05, max mem: 20.9 GB 
[11/24 05:29:16 visual_prompt]: 	Training 200/553. train loss: 15.4211,	1.8882 s / batch. (data: 1.07e+00). ETA=1 day, 4:19:09, max mem: 20.9 GB 
[11/24 05:30:51 visual_prompt]: 	Training 300/553. train loss: 16.5867,	0.8414 s / batch. (data: 3.03e-04). ETA=12:35:48, max mem: 20.9 GB 
[11/24 05:32:28 visual_prompt]: 	Training 400/553. train loss: 14.2569,	0.8602 s / batch. (data: 1.22e-02). ETA=12:51:13, max mem: 20.9 GB 
[11/24 05:34:06 visual_prompt]: 	Training 500/553. train loss: 12.0256,	1.0881 s / batch. (data: 2.25e-01). ETA=16:13:43, max mem: 20.9 GB 
[11/24 05:34:55 visual_prompt]: Epoch 3 / 100: avg data time: 1.38e-01, avg batch time: 0.9709, average train loss: 19.3897
[11/24 05:35:50 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.3106, average loss: 39.4502
[11/24 05:35:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.94	
[11/24 05:35:50 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/24 05:37:31 visual_prompt]: 	Training 100/553. train loss: 53.0640,	0.8360 s / batch. (data: 3.14e-04). ETA=12:26:01, max mem: 20.9 GB 
[11/24 05:39:08 visual_prompt]: 	Training 200/553. train loss: 50.8620,	0.8200 s / batch. (data: 3.12e-04). ETA=12:10:23, max mem: 20.9 GB 
[11/24 05:40:46 visual_prompt]: 	Training 300/553. train loss: 58.1711,	1.2530 s / batch. (data: 4.48e-01). ETA=18:33:54, max mem: 20.9 GB 
[11/24 05:42:18 visual_prompt]: 	Training 400/553. train loss: 24.9885,	1.5240 s / batch. (data: 6.88e-01). ETA=22:32:20, max mem: 20.9 GB 
[11/24 05:43:57 visual_prompt]: 	Training 500/553. train loss: 0.0108,	3.3765 s / batch. (data: 2.55e+00). ETA=2 days, 1:50:28, max mem: 20.9 GB 
[11/24 05:44:49 visual_prompt]: Epoch 4 / 100: avg data time: 1.44e-01, avg batch time: 0.9751, average train loss: 24.8907
[11/24 05:45:45 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3098, average loss: 20.4354
[11/24 05:45:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.85	
[11/24 05:45:45 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/24 05:47:24 visual_prompt]: 	Training 100/553. train loss: 32.3181,	0.8238 s / batch. (data: 7.95e-03). ETA=12:07:31, max mem: 20.9 GB 
[11/24 05:49:01 visual_prompt]: 	Training 200/553. train loss: 23.3870,	1.2166 s / batch. (data: 3.69e-01). ETA=17:52:22, max mem: 20.9 GB 
[11/24 05:50:39 visual_prompt]: 	Training 300/553. train loss: 34.9429,	0.8534 s / batch. (data: 7.96e-03). ETA=12:30:47, max mem: 20.9 GB 
[11/24 05:52:16 visual_prompt]: 	Training 400/553. train loss: 5.3576,	0.8396 s / batch. (data: 7.96e-03). ETA=12:17:15, max mem: 20.9 GB 
[11/24 05:53:53 visual_prompt]: 	Training 500/553. train loss: 38.4655,	0.8400 s / batch. (data: 3.10e-04). ETA=12:16:13, max mem: 20.9 GB 
[11/24 05:54:45 visual_prompt]: Epoch 5 / 100: avg data time: 1.46e-01, avg batch time: 0.9768, average train loss: 35.4545
[11/24 05:55:40 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3109, average loss: 52.2161
[11/24 05:55:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.09	
[11/24 05:55:40 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/24 05:57:23 visual_prompt]: 	Training 100/553. train loss: 22.3957,	0.8411 s / batch. (data: 3.07e-04). ETA=12:15:04, max mem: 20.9 GB 
[11/24 05:58:58 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8246 s / batch. (data: 4.57e-04). ETA=11:59:13, max mem: 20.9 GB 
[11/24 06:00:34 visual_prompt]: 	Training 300/553. train loss: 100.9606,	0.8320 s / batch. (data: 5.41e-03). ETA=12:04:19, max mem: 20.9 GB 
[11/24 06:02:15 visual_prompt]: 	Training 400/553. train loss: 36.3876,	0.8240 s / batch. (data: 3.78e-04). ETA=11:55:58, max mem: 20.9 GB 
[11/24 06:03:50 visual_prompt]: 	Training 500/553. train loss: 7.5757,	0.8475 s / batch. (data: 3.41e-04). ETA=12:14:58, max mem: 20.9 GB 
[11/24 06:04:40 visual_prompt]: Epoch 6 / 100: avg data time: 1.46e-01, avg batch time: 0.9759, average train loss: 46.4969
[11/24 06:05:35 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3103, average loss: 2.9002
[11/24 06:05:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 42.29	
[11/24 06:05:35 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/24 06:07:14 visual_prompt]: 	Training 100/553. train loss: 79.7656,	0.8560 s / batch. (data: 3.59e-04). ETA=12:20:10, max mem: 20.9 GB 
[11/24 06:08:51 visual_prompt]: 	Training 200/553. train loss: 23.5082,	0.8090 s / batch. (data: 3.13e-04). ETA=11:38:11, max mem: 20.9 GB 
[11/24 06:10:31 visual_prompt]: 	Training 300/553. train loss: 89.3683,	1.5586 s / batch. (data: 7.26e-01). ETA=22:22:31, max mem: 20.9 GB 
[11/24 06:12:08 visual_prompt]: 	Training 400/553. train loss: 9.0612,	1.9200 s / batch. (data: 1.07e+00). ETA=1 day, 3:30:36, max mem: 20.9 GB 
[11/24 06:13:44 visual_prompt]: 	Training 500/553. train loss: 2.2699,	0.8589 s / batch. (data: 3.06e-04). ETA=12:16:59, max mem: 20.9 GB 
[11/24 06:14:34 visual_prompt]: Epoch 7 / 100: avg data time: 1.44e-01, avg batch time: 0.9738, average train loss: 53.5550
[11/24 06:15:29 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3102, average loss: 85.8453
[11/24 06:15:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.41	
[11/24 06:15:29 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/24 06:17:09 visual_prompt]: 	Training 100/553. train loss: 54.0753,	0.8480 s / batch. (data: 5.45e-03). ETA=12:05:26, max mem: 20.9 GB 
[11/24 06:18:47 visual_prompt]: 	Training 200/553. train loss: 8.1587,	1.1515 s / batch. (data: 3.07e-01). ETA=16:23:10, max mem: 20.9 GB 
[11/24 06:20:24 visual_prompt]: 	Training 300/553. train loss: 10.8678,	0.8234 s / batch. (data: 2.59e-04). ETA=11:41:39, max mem: 20.9 GB 
[11/24 06:22:02 visual_prompt]: 	Training 400/553. train loss: 60.3140,	0.8227 s / batch. (data: 3.24e-04). ETA=11:39:39, max mem: 20.9 GB 
[11/24 06:23:39 visual_prompt]: 	Training 500/553. train loss: 200.6376,	1.1982 s / batch. (data: 3.57e-01). ETA=16:57:04, max mem: 20.9 GB 
[11/24 06:24:30 visual_prompt]: Epoch 8 / 100: avg data time: 1.50e-01, avg batch time: 0.9778, average train loss: 58.2043
[11/24 06:25:25 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3098, average loss: 165.8387
[11/24 06:25:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.33	
[11/24 06:25:25 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/24 06:27:06 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8160 s / batch. (data: 3.02e-04). ETA=11:30:33, max mem: 20.9 GB 
[11/24 06:28:42 visual_prompt]: 	Training 200/553. train loss: 10.1231,	0.8296 s / batch. (data: 5.43e-03). ETA=11:40:42, max mem: 20.9 GB 
[11/24 06:30:19 visual_prompt]: 	Training 300/553. train loss: 65.8969,	1.3880 s / batch. (data: 5.68e-01). ETA=19:29:58, max mem: 20.9 GB 
[11/24 06:31:58 visual_prompt]: 	Training 400/553. train loss: 66.5996,	0.8410 s / batch. (data: 1.05e-02). ETA=11:47:30, max mem: 20.9 GB 
[11/24 06:33:35 visual_prompt]: 	Training 500/553. train loss: 21.8308,	0.8334 s / batch. (data: 2.39e-02). ETA=11:39:45, max mem: 20.9 GB 
[11/24 06:34:25 visual_prompt]: Epoch 9 / 100: avg data time: 1.47e-01, avg batch time: 0.9748, average train loss: 71.2444
[11/24 06:35:20 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3104, average loss: 110.4002
[11/24 06:35:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.95	
[11/24 06:35:20 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/24 06:37:03 visual_prompt]: 	Training 100/553. train loss: 133.3066,	0.8160 s / batch. (data: 7.97e-03). ETA=11:22:59, max mem: 20.9 GB 
[11/24 06:38:39 visual_prompt]: 	Training 200/553. train loss: 84.7284,	0.8456 s / batch. (data: 2.06e-02). ETA=11:46:24, max mem: 20.9 GB 
[11/24 06:40:15 visual_prompt]: 	Training 300/553. train loss: 576.9364,	1.3360 s / batch. (data: 5.09e-01). ETA=18:33:50, max mem: 20.9 GB 
[11/24 06:41:49 visual_prompt]: 	Training 400/553. train loss: 51.9027,	0.8378 s / batch. (data: 3.40e-04). ETA=11:37:05, max mem: 20.9 GB 
[11/24 06:43:28 visual_prompt]: 	Training 500/553. train loss: 23.9710,	0.8371 s / batch. (data: 3.11e-04). ETA=11:35:07, max mem: 20.9 GB 
[11/24 06:44:18 visual_prompt]: Epoch 10 / 100: avg data time: 1.45e-01, avg batch time: 0.9733, average train loss: 87.3272
[11/24 06:45:14 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3098, average loss: 112.9292
[11/24 06:45:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.07	
[11/24 06:45:14 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/24 06:46:57 visual_prompt]: 	Training 100/553. train loss: 114.9638,	0.8781 s / batch. (data: 2.60e-02). ETA=12:06:54, max mem: 20.9 GB 
[11/24 06:48:35 visual_prompt]: 	Training 200/553. train loss: 88.0898,	0.8399 s / batch. (data: 3.51e-04). ETA=11:33:55, max mem: 20.9 GB 
[11/24 06:50:10 visual_prompt]: 	Training 300/553. train loss: 14.5853,	1.8793 s / batch. (data: 1.06e+00). ETA=1 day, 1:49:29, max mem: 20.9 GB 
[11/24 06:51:45 visual_prompt]: 	Training 400/553. train loss: 141.8358,	0.8181 s / batch. (data: 3.12e-04). ETA=11:13:08, max mem: 20.9 GB 
[11/24 06:53:20 visual_prompt]: 	Training 500/553. train loss: 142.3403,	0.8581 s / batch. (data: 1.00e-02). ETA=11:44:36, max mem: 20.9 GB 
[11/24 06:54:10 visual_prompt]: Epoch 11 / 100: avg data time: 1.41e-01, avg batch time: 0.9698, average train loss: 88.3990
[11/24 06:55:06 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3101, average loss: 112.8124
[11/24 06:55:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.28	
[11/24 06:55:06 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/24 06:56:48 visual_prompt]: 	Training 100/553. train loss: 19.0796,	0.8312 s / batch. (data: 3.29e-04). ETA=11:20:23, max mem: 20.9 GB 
[11/24 06:58:26 visual_prompt]: 	Training 200/553. train loss: 83.0412,	0.8410 s / batch. (data: 1.55e-02). ETA=11:27:05, max mem: 20.9 GB 
[11/24 07:00:01 visual_prompt]: 	Training 300/553. train loss: 119.7655,	0.8320 s / batch. (data: 3.04e-04). ETA=11:18:19, max mem: 20.9 GB 
[11/24 07:01:38 visual_prompt]: 	Training 400/553. train loss: 22.7943,	0.8178 s / batch. (data: 3.25e-04). ETA=11:05:24, max mem: 20.9 GB 
[11/24 07:03:15 visual_prompt]: 	Training 500/553. train loss: 38.2310,	0.8307 s / batch. (data: 7.84e-04). ETA=11:14:31, max mem: 20.9 GB 
[11/24 07:04:05 visual_prompt]: Epoch 12 / 100: avg data time: 1.47e-01, avg batch time: 0.9749, average train loss: 85.2541
[11/24 07:05:00 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3093, average loss: 152.8936
[11/24 07:05:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.11	
[11/24 07:05:00 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/24 07:06:42 visual_prompt]: 	Training 100/553. train loss: 23.9103,	0.8564 s / batch. (data: 5.43e-03). ETA=11:33:10, max mem: 20.9 GB 
[11/24 07:08:17 visual_prompt]: 	Training 200/553. train loss: 49.2319,	0.8600 s / batch. (data: 3.20e-04). ETA=11:34:39, max mem: 20.9 GB 
[11/24 07:09:54 visual_prompt]: 	Training 300/553. train loss: 28.9799,	1.6680 s / batch. (data: 8.44e-01). ETA=22:24:30, max mem: 20.9 GB 
[11/24 07:11:30 visual_prompt]: 	Training 400/553. train loss: 96.5915,	0.8374 s / batch. (data: 3.06e-04). ETA=11:13:37, max mem: 20.9 GB 
[11/24 07:13:08 visual_prompt]: 	Training 500/553. train loss: 92.0966,	0.8245 s / batch. (data: 2.90e-04). ETA=11:01:50, max mem: 20.9 GB 
[11/24 07:13:58 visual_prompt]: Epoch 13 / 100: avg data time: 1.45e-01, avg batch time: 0.9729, average train loss: 92.7903
[11/24 07:14:54 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3113, average loss: 21.4387
[11/24 07:14:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.53	
[11/24 07:14:54 visual_prompt]: Best epoch 13: best metric: -21.439
[11/24 07:14:54 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/24 07:16:36 visual_prompt]: 	Training 100/553. train loss: 13.0888,	0.8400 s / batch. (data: 7.94e-03). ETA=11:12:08, max mem: 20.9 GB 
[11/24 07:18:12 visual_prompt]: 	Training 200/553. train loss: 48.8425,	0.9236 s / batch. (data: 1.05e-01). ETA=12:17:32, max mem: 20.9 GB 
[11/24 07:19:49 visual_prompt]: 	Training 300/553. train loss: 38.0073,	0.8360 s / batch. (data: 3.15e-04). ETA=11:06:10, max mem: 20.9 GB 
[11/24 07:21:25 visual_prompt]: 	Training 400/553. train loss: 27.0583,	0.8216 s / batch. (data: 3.28e-04). ETA=10:53:18, max mem: 20.9 GB 
[11/24 07:23:03 visual_prompt]: 	Training 500/553. train loss: 136.6361,	0.8301 s / batch. (data: 2.83e-04). ETA=10:58:40, max mem: 20.9 GB 
[11/24 07:23:52 visual_prompt]: Epoch 14 / 100: avg data time: 1.45e-01, avg batch time: 0.9729, average train loss: 85.7605
[11/24 07:24:47 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3109, average loss: 75.6512
[11/24 07:24:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.40	
[11/24 07:24:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/24 07:26:28 visual_prompt]: 	Training 100/553. train loss: 33.4414,	0.8261 s / batch. (data: 3.04e-04). ETA=10:53:27, max mem: 20.9 GB 
[11/24 07:28:04 visual_prompt]: 	Training 200/553. train loss: 190.2530,	0.8404 s / batch. (data: 3.14e-04). ETA=11:03:17, max mem: 20.9 GB 
[11/24 07:29:43 visual_prompt]: 	Training 300/553. train loss: 39.7126,	0.8272 s / batch. (data: 8.36e-04). ETA=10:51:34, max mem: 20.9 GB 
[11/24 07:31:17 visual_prompt]: 	Training 400/553. train loss: 36.3495,	0.8400 s / batch. (data: 3.07e-04). ETA=11:00:12, max mem: 20.9 GB 
[11/24 07:32:55 visual_prompt]: 	Training 500/553. train loss: 174.3483,	0.8317 s / batch. (data: 2.98e-04). ETA=10:52:18, max mem: 20.9 GB 
[11/24 07:33:46 visual_prompt]: Epoch 15 / 100: avg data time: 1.47e-01, avg batch time: 0.9745, average train loss: 81.3469
[11/24 07:34:42 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3102, average loss: 42.3722
[11/24 07:34:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.24	
[11/24 07:34:42 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/24 07:36:21 visual_prompt]: 	Training 100/553. train loss: 59.0136,	0.8680 s / batch. (data: 1.20e-02). ETA=11:18:33, max mem: 20.9 GB 
[11/24 07:37:57 visual_prompt]: 	Training 200/553. train loss: 124.4367,	0.8212 s / batch. (data: 2.98e-04). ETA=10:40:38, max mem: 20.9 GB 
[11/24 07:39:35 visual_prompt]: 	Training 300/553. train loss: 10.7524,	0.8360 s / batch. (data: 2.91e-04). ETA=10:50:45, max mem: 20.9 GB 
[11/24 07:41:12 visual_prompt]: 	Training 400/553. train loss: 123.1823,	0.8254 s / batch. (data: 7.92e-04). ETA=10:41:06, max mem: 20.9 GB 
[11/24 07:42:48 visual_prompt]: 	Training 500/553. train loss: 230.5610,	1.1564 s / batch. (data: 3.38e-01). ETA=14:56:20, max mem: 20.9 GB 
[11/24 07:43:40 visual_prompt]: Epoch 16 / 100: avg data time: 1.45e-01, avg batch time: 0.9730, average train loss: 92.4808
[11/24 07:44:35 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3094, average loss: 6.6274
[11/24 07:44:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.14	
[11/24 07:44:35 visual_prompt]: Best epoch 16: best metric: -6.627
[11/24 07:44:35 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/24 07:46:16 visual_prompt]: 	Training 100/553. train loss: 2.6529,	0.8611 s / batch. (data: 2.82e-04). ETA=11:05:11, max mem: 20.9 GB 
[11/24 07:47:54 visual_prompt]: 	Training 200/553. train loss: 152.5670,	0.8209 s / batch. (data: 3.28e-04). ETA=10:32:49, max mem: 20.9 GB 
[11/24 07:49:31 visual_prompt]: 	Training 300/553. train loss: 253.1240,	0.8460 s / batch. (data: 5.41e-03). ETA=10:50:46, max mem: 20.9 GB 
[11/24 07:51:07 visual_prompt]: 	Training 400/553. train loss: 36.0315,	1.1520 s / batch. (data: 3.12e-01). ETA=14:44:10, max mem: 20.9 GB 
[11/24 07:52:43 visual_prompt]: 	Training 500/553. train loss: 14.2437,	1.5555 s / batch. (data: 7.38e-01). ETA=19:51:19, max mem: 20.9 GB 
[11/24 07:53:35 visual_prompt]: Epoch 17 / 100: avg data time: 1.47e-01, avg batch time: 0.9751, average train loss: 97.3880
[11/24 07:54:30 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3090, average loss: 34.5270
[11/24 07:54:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 37.29	
[11/24 07:54:30 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/24 07:56:11 visual_prompt]: 	Training 100/553. train loss: 21.5394,	0.8237 s / batch. (data: 7.94e-03). ETA=10:28:46, max mem: 20.9 GB 
[11/24 07:57:50 visual_prompt]: 	Training 200/553. train loss: 102.2191,	0.8576 s / batch. (data: 7.90e-04). ETA=10:53:12, max mem: 20.9 GB 
[11/24 07:59:27 visual_prompt]: 	Training 300/553. train loss: 108.1322,	0.8069 s / batch. (data: 2.98e-04). ETA=10:13:15, max mem: 20.9 GB 
[11/24 08:01:04 visual_prompt]: 	Training 400/553. train loss: 52.3275,	0.8320 s / batch. (data: 2.79e-04). ETA=10:30:56, max mem: 20.9 GB 
[11/24 08:02:39 visual_prompt]: 	Training 500/553. train loss: 47.7289,	0.8953 s / batch. (data: 6.32e-02). ETA=11:17:27, max mem: 20.9 GB 
[11/24 08:03:28 visual_prompt]: Epoch 18 / 100: avg data time: 1.46e-01, avg batch time: 0.9737, average train loss: 86.3528
[11/24 08:04:23 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3093, average loss: 2.3279
[11/24 08:04:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 40.65	rocauc: 37.02	
[11/24 08:04:23 visual_prompt]: Best epoch 18: best metric: -2.328
[11/24 08:04:23 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/24 08:06:03 visual_prompt]: 	Training 100/553. train loss: 69.7756,	1.1422 s / batch. (data: 2.99e-01). ETA=14:21:20, max mem: 20.9 GB 
[11/24 08:07:41 visual_prompt]: 	Training 200/553. train loss: 22.1144,	0.8176 s / batch. (data: 3.02e-04). ETA=10:15:11, max mem: 20.9 GB 
[11/24 08:09:18 visual_prompt]: 	Training 300/553. train loss: 408.8005,	0.8640 s / batch. (data: 3.31e-04). ETA=10:48:39, max mem: 20.9 GB 
[11/24 08:10:55 visual_prompt]: 	Training 400/553. train loss: 72.7405,	0.8441 s / batch. (data: 1.14e-03). ETA=10:32:19, max mem: 20.9 GB 
[11/24 08:12:28 visual_prompt]: 	Training 500/553. train loss: 109.6554,	0.8200 s / batch. (data: 2.99e-04). ETA=10:12:53, max mem: 20.9 GB 
[11/24 08:13:19 visual_prompt]: Epoch 19 / 100: avg data time: 1.40e-01, avg batch time: 0.9685, average train loss: 89.7050
[11/24 08:14:14 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3114, average loss: 100.0859
[11/24 08:14:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.88	
[11/24 08:14:14 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/24 08:15:53 visual_prompt]: 	Training 100/553. train loss: 79.4471,	0.8440 s / batch. (data: 3.24e-04). ETA=10:28:40, max mem: 20.9 GB 
[11/24 08:17:30 visual_prompt]: 	Training 200/553. train loss: 157.0300,	0.8349 s / batch. (data: 3.24e-04). ETA=10:20:30, max mem: 20.9 GB 
[11/24 08:19:07 visual_prompt]: 	Training 300/553. train loss: 8.1106,	0.8198 s / batch. (data: 3.73e-04). ETA=10:07:54, max mem: 20.9 GB 
[11/24 08:20:44 visual_prompt]: 	Training 400/553. train loss: 2.5973,	0.8760 s / batch. (data: 1.19e-02). ETA=10:48:07, max mem: 20.9 GB 
[11/24 08:22:20 visual_prompt]: 	Training 500/553. train loss: 169.9998,	0.8213 s / batch. (data: 1.05e-02). ETA=10:06:16, max mem: 20.9 GB 
[11/24 08:23:11 visual_prompt]: Epoch 20 / 100: avg data time: 1.44e-01, avg batch time: 0.9718, average train loss: 91.2562
[11/24 08:24:07 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3095, average loss: 90.9191
[11/24 08:24:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.28	
[11/24 08:24:07 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/24 08:25:50 visual_prompt]: 	Training 100/553. train loss: 4.2993,	0.8400 s / batch. (data: 3.23e-04). ETA=10:17:58, max mem: 20.9 GB 
[11/24 08:27:26 visual_prompt]: 	Training 200/553. train loss: 36.2410,	0.8204 s / batch. (data: 3.05e-04). ETA=10:02:12, max mem: 20.9 GB 
[11/24 08:29:02 visual_prompt]: 	Training 300/553. train loss: 135.9170,	0.8144 s / batch. (data: 3.28e-04). ETA=9:56:25, max mem: 20.9 GB 
[11/24 08:30:38 visual_prompt]: 	Training 400/553. train loss: 9.0030,	0.8359 s / batch. (data: 2.90e-04). ETA=10:10:46, max mem: 20.9 GB 
[11/24 08:32:17 visual_prompt]: 	Training 500/553. train loss: 113.5523,	0.8349 s / batch. (data: 9.27e-03). ETA=10:08:36, max mem: 20.9 GB 
[11/24 08:33:06 visual_prompt]: Epoch 21 / 100: avg data time: 1.48e-01, avg batch time: 0.9754, average train loss: 95.3262
[11/24 08:34:02 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3095, average loss: 4.7616
[11/24 08:34:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 52.42	
[11/24 08:34:02 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/24 08:35:42 visual_prompt]: 	Training 100/553. train loss: 50.1048,	0.8127 s / batch. (data: 2.40e-04). ETA=9:50:23, max mem: 20.9 GB 
[11/24 08:37:19 visual_prompt]: 	Training 200/553. train loss: 62.6233,	0.8446 s / batch. (data: 3.00e-04). ETA=10:12:07, max mem: 20.9 GB 
[11/24 08:38:53 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8280 s / batch. (data: 5.42e-03). ETA=9:58:43, max mem: 20.9 GB 
[11/24 08:40:31 visual_prompt]: 	Training 400/553. train loss: 54.8575,	0.8526 s / batch. (data: 7.95e-03). ETA=10:15:05, max mem: 20.9 GB 
[11/24 08:42:08 visual_prompt]: 	Training 500/553. train loss: 48.8533,	0.8189 s / batch. (data: 3.08e-04). ETA=9:49:24, max mem: 20.9 GB 
[11/24 08:42:59 visual_prompt]: Epoch 22 / 100: avg data time: 1.43e-01, avg batch time: 0.9714, average train loss: 75.0675
[11/24 08:43:54 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3097, average loss: 47.6330
[11/24 08:43:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.47	
[11/24 08:43:54 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/24 08:45:36 visual_prompt]: 	Training 100/553. train loss: 15.8041,	0.8267 s / batch. (data: 3.28e-04). ETA=9:52:54, max mem: 20.9 GB 
[11/24 08:47:13 visual_prompt]: 	Training 200/553. train loss: 60.4298,	0.8189 s / batch. (data: 3.02e-04). ETA=9:45:59, max mem: 20.9 GB 
[11/24 08:48:50 visual_prompt]: 	Training 300/553. train loss: 9.5525,	0.8284 s / batch. (data: 3.04e-04). ETA=9:51:23, max mem: 20.9 GB 
[11/24 08:50:24 visual_prompt]: 	Training 400/553. train loss: 76.4443,	0.8643 s / batch. (data: 8.51e-04). ETA=10:15:33, max mem: 20.9 GB 
[11/24 08:52:00 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8436 s / batch. (data: 2.05e-02). ETA=9:59:27, max mem: 20.9 GB 
[11/24 08:52:50 visual_prompt]: Epoch 23 / 100: avg data time: 1.42e-01, avg batch time: 0.9687, average train loss: 81.9495
[11/24 08:53:45 visual_prompt]: Inference (val):avg data time: 2.20e-04, avg batch time: 0.3103, average loss: 119.5339
[11/24 08:53:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.64	
[11/24 08:53:45 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/24 08:55:23 visual_prompt]: 	Training 100/553. train loss: 16.3050,	0.8173 s / batch. (data: 3.19e-04). ETA=9:38:40, max mem: 20.9 GB 
[11/24 08:56:59 visual_prompt]: 	Training 200/553. train loss: 61.7671,	0.8160 s / batch. (data: 2.96e-04). ETA=9:36:23, max mem: 20.9 GB 
[11/24 08:58:35 visual_prompt]: 	Training 300/553. train loss: 61.0474,	0.8760 s / batch. (data: 2.85e-02). ETA=10:17:18, max mem: 20.9 GB 
[11/24 09:00:12 visual_prompt]: 	Training 400/553. train loss: 97.8063,	0.8236 s / batch. (data: 2.82e-04). ETA=9:39:00, max mem: 20.9 GB 
[11/24 09:01:49 visual_prompt]: 	Training 500/553. train loss: 311.2027,	0.8255 s / batch. (data: 1.05e-02). ETA=9:38:57, max mem: 20.9 GB 
[11/24 09:02:40 visual_prompt]: Epoch 24 / 100: avg data time: 1.40e-01, avg batch time: 0.9671, average train loss: 82.1351
[11/24 09:03:35 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3109, average loss: 112.9347
[11/24 09:03:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.77	
[11/24 09:03:35 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/24 09:05:18 visual_prompt]: 	Training 100/553. train loss: 107.3279,	0.8401 s / batch. (data: 7.96e-03). ETA=9:47:02, max mem: 20.9 GB 
[11/24 09:06:51 visual_prompt]: 	Training 200/553. train loss: 86.5670,	0.8371 s / batch. (data: 3.43e-04). ETA=9:43:33, max mem: 20.9 GB 
[11/24 09:08:27 visual_prompt]: 	Training 300/553. train loss: 123.3767,	0.8480 s / batch. (data: 3.66e-04). ETA=9:49:45, max mem: 20.9 GB 
[11/24 09:10:03 visual_prompt]: 	Training 400/553. train loss: 207.6805,	1.1960 s / batch. (data: 3.60e-01). ETA=13:49:46, max mem: 20.9 GB 
[11/24 09:11:40 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.3360 s / batch. (data: 5.19e-01). ETA=15:24:39, max mem: 20.9 GB 
[11/24 09:12:31 visual_prompt]: Epoch 25 / 100: avg data time: 1.40e-01, avg batch time: 0.9692, average train loss: 88.6766
[11/24 09:13:26 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3104, average loss: 65.0240
[11/24 09:13:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.04	
[11/24 09:13:26 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/24 09:15:07 visual_prompt]: 	Training 100/553. train loss: 30.0034,	0.8295 s / batch. (data: 3.19e-04). ETA=9:32:00, max mem: 20.9 GB 
[11/24 09:16:45 visual_prompt]: 	Training 200/553. train loss: 67.1940,	1.6600 s / batch. (data: 8.45e-01). ETA=19:01:57, max mem: 20.9 GB 
[11/24 09:18:23 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8220 s / batch. (data: 3.23e-04). ETA=9:24:05, max mem: 20.9 GB 
[11/24 09:19:58 visual_prompt]: 	Training 400/553. train loss: 92.3177,	0.8160 s / batch. (data: 3.00e-04). ETA=9:18:37, max mem: 20.9 GB 
[11/24 09:21:34 visual_prompt]: 	Training 500/553. train loss: 38.7327,	0.8200 s / batch. (data: 3.11e-04). ETA=9:19:59, max mem: 20.9 GB 
[11/24 09:22:25 visual_prompt]: Epoch 26 / 100: avg data time: 1.45e-01, avg batch time: 0.9735, average train loss: 69.7019
[11/24 09:23:20 visual_prompt]: Inference (val):avg data time: 2.03e-04, avg batch time: 0.3113, average loss: 14.4951
[11/24 09:23:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.32	
[11/24 09:23:20 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[11/24 09:25:02 visual_prompt]: 	Training 100/553. train loss: 219.0955,	0.8274 s / batch. (data: 5.39e-03). ETA=9:22:54, max mem: 20.9 GB 
[11/24 09:26:38 visual_prompt]: 	Training 200/553. train loss: 145.9541,	0.8440 s / batch. (data: 7.95e-03). ETA=9:32:50, max mem: 20.9 GB 
[11/24 09:28:15 visual_prompt]: 	Training 300/553. train loss: 24.4080,	0.8360 s / batch. (data: 3.02e-04). ETA=9:25:59, max mem: 20.9 GB 
[11/24 09:29:52 visual_prompt]: 	Training 400/553. train loss: 44.4365,	0.8349 s / batch. (data: 1.10e-02). ETA=9:23:50, max mem: 20.9 GB 
[11/24 09:31:29 visual_prompt]: 	Training 500/553. train loss: 161.3302,	0.8360 s / batch. (data: 8.18e-04). ETA=9:23:11, max mem: 20.9 GB 
[11/24 09:32:18 visual_prompt]: Epoch 27 / 100: avg data time: 1.44e-01, avg batch time: 0.9726, average train loss: 74.8556
[11/24 09:33:13 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3109, average loss: 34.8650
[11/24 09:33:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.19	
[11/24 09:33:13 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[11/24 09:34:52 visual_prompt]: 	Training 100/553. train loss: 86.5541,	0.8375 s / batch. (data: 1.05e-02). ETA=9:22:03, max mem: 20.9 GB 
[11/24 09:36:29 visual_prompt]: 	Training 200/553. train loss: 22.8274,	0.8327 s / batch. (data: 2.99e-04). ETA=9:17:27, max mem: 20.9 GB 
[11/24 09:38:06 visual_prompt]: 	Training 300/553. train loss: 59.2656,	1.3952 s / batch. (data: 5.63e-01). ETA=15:31:45, max mem: 20.9 GB 
[11/24 09:39:42 visual_prompt]: 	Training 400/553. train loss: 150.4455,	0.8228 s / batch. (data: 4.32e-04). ETA=9:08:05, max mem: 20.9 GB 
[11/24 09:41:18 visual_prompt]: 	Training 500/553. train loss: 147.9897,	0.8312 s / batch. (data: 3.30e-04). ETA=9:12:18, max mem: 20.9 GB 
[11/24 09:42:08 visual_prompt]: Epoch 28 / 100: avg data time: 1.39e-01, avg batch time: 0.9666, average train loss: 69.9625
[11/24 09:43:03 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3112, average loss: 70.9363
[11/24 09:43:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.18	
[11/24 09:43:03 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[11/24 09:44:50 visual_prompt]: 	Training 100/553. train loss: 76.6778,	0.8680 s / batch. (data: 2.92e-04). ETA=9:34:33, max mem: 20.9 GB 
[11/24 09:46:26 visual_prompt]: 	Training 200/553. train loss: 29.0995,	1.6255 s / batch. (data: 7.98e-01). ETA=17:53:16, max mem: 20.9 GB 
[11/24 09:48:00 visual_prompt]: 	Training 300/553. train loss: 55.0209,	0.8240 s / batch. (data: 2.91e-04). ETA=9:02:41, max mem: 20.9 GB 
[11/24 09:49:34 visual_prompt]: 	Training 400/553. train loss: 17.1176,	0.8736 s / batch. (data: 4.07e-02). ETA=9:33:51, max mem: 20.9 GB 
[11/24 09:51:11 visual_prompt]: 	Training 500/553. train loss: 68.7080,	0.8548 s / batch. (data: 1.56e-02). ETA=9:20:05, max mem: 20.9 GB 
[11/24 09:52:02 visual_prompt]: Epoch 29 / 100: avg data time: 1.45e-01, avg batch time: 0.9742, average train loss: 77.5595
[11/24 09:52:57 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3110, average loss: 139.9693
[11/24 09:52:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.88	
[11/24 09:52:57 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[11/24 09:54:37 visual_prompt]: 	Training 100/553. train loss: 95.7120,	0.8400 s / batch. (data: 2.87e-04). ETA=9:08:16, max mem: 20.9 GB 
[11/24 09:56:14 visual_prompt]: 	Training 200/553. train loss: 103.8336,	0.8145 s / batch. (data: 3.81e-04). ETA=8:50:17, max mem: 20.9 GB 
[11/24 09:57:49 visual_prompt]: 	Training 300/553. train loss: 66.8459,	0.8240 s / batch. (data: 3.09e-04). ETA=8:55:05, max mem: 20.9 GB 
[11/24 09:59:27 visual_prompt]: 	Training 400/553. train loss: 17.2390,	1.0920 s / batch. (data: 2.52e-01). ETA=11:47:18, max mem: 20.9 GB 
[11/24 10:01:02 visual_prompt]: 	Training 500/553. train loss: 25.4649,	1.2762 s / batch. (data: 4.59e-01). ETA=13:44:30, max mem: 20.9 GB 
[11/24 10:01:55 visual_prompt]: Epoch 30 / 100: avg data time: 1.43e-01, avg batch time: 0.9715, average train loss: 77.3183
[11/24 10:02:50 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3103, average loss: 59.4566
[11/24 10:02:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.43	
[11/24 10:02:50 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[11/24 10:04:32 visual_prompt]: 	Training 100/553. train loss: 11.5690,	0.8276 s / batch. (data: 1.20e-02). ETA=8:52:35, max mem: 20.9 GB 
[11/24 10:06:11 visual_prompt]: 	Training 200/553. train loss: 33.0901,	0.8279 s / batch. (data: 3.24e-04). ETA=8:51:20, max mem: 20.9 GB 
[11/24 10:07:45 visual_prompt]: 	Training 300/553. train loss: 22.6195,	0.8514 s / batch. (data: 1.15e-02). ETA=9:05:03, max mem: 20.9 GB 
[11/24 10:09:20 visual_prompt]: 	Training 400/553. train loss: 42.0631,	1.4342 s / batch. (data: 5.99e-01). ETA=15:15:45, max mem: 20.9 GB 
[11/24 10:10:56 visual_prompt]: 	Training 500/553. train loss: 14.5550,	0.8277 s / batch. (data: 2.82e-04). ETA=8:47:05, max mem: 20.9 GB 
[11/24 10:11:46 visual_prompt]: Epoch 31 / 100: avg data time: 1.42e-01, avg batch time: 0.9703, average train loss: 73.1653
[11/24 10:12:42 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3098, average loss: 39.5140
[11/24 10:12:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.60	
[11/24 10:12:42 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[11/24 10:14:24 visual_prompt]: 	Training 100/553. train loss: 94.3087,	0.8575 s / batch. (data: 1.05e-02). ETA=9:03:54, max mem: 20.9 GB 
[11/24 10:16:00 visual_prompt]: 	Training 200/553. train loss: 99.7104,	0.8522 s / batch. (data: 3.22e-02). ETA=8:59:08, max mem: 20.9 GB 
[11/24 10:17:39 visual_prompt]: 	Training 300/553. train loss: 35.2079,	1.0040 s / batch. (data: 1.61e-01). ETA=10:33:28, max mem: 20.9 GB 
[11/24 10:19:17 visual_prompt]: 	Training 400/553. train loss: 178.5225,	0.8368 s / batch. (data: 3.03e-04). ETA=8:46:34, max mem: 20.9 GB 
[11/24 10:20:52 visual_prompt]: 	Training 500/553. train loss: 31.5324,	0.8320 s / batch. (data: 3.04e-04). ETA=8:42:10, max mem: 20.9 GB 
[11/24 10:21:41 visual_prompt]: Epoch 32 / 100: avg data time: 1.46e-01, avg batch time: 0.9743, average train loss: 80.8004
[11/24 10:22:36 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3110, average loss: 143.6310
[11/24 10:22:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.18	
[11/24 10:22:36 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[11/24 10:24:15 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8185 s / batch. (data: 3.30e-04). ETA=8:31:37, max mem: 20.9 GB 
[11/24 10:25:54 visual_prompt]: 	Training 200/553. train loss: 96.3196,	1.1403 s / batch. (data: 2.91e-01). ETA=11:50:51, max mem: 20.9 GB 
[11/24 10:27:30 visual_prompt]: 	Training 300/553. train loss: 72.6527,	0.8280 s / batch. (data: 3.05e-04). ETA=8:34:47, max mem: 20.9 GB 
[11/24 10:29:07 visual_prompt]: 	Training 400/553. train loss: 83.9825,	0.8560 s / batch. (data: 1.20e-02). ETA=8:50:46, max mem: 20.9 GB 
[11/24 10:30:43 visual_prompt]: 	Training 500/553. train loss: 125.3319,	0.8257 s / batch. (data: 3.12e-04). ETA=8:30:36, max mem: 20.9 GB 
[11/24 10:31:33 visual_prompt]: Epoch 33 / 100: avg data time: 1.43e-01, avg batch time: 0.9709, average train loss: 79.4853
[11/24 10:32:28 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3098, average loss: 10.7333
[11/24 10:32:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.54	
[11/24 10:32:28 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[11/24 10:34:08 visual_prompt]: 	Training 100/553. train loss: 30.8330,	0.8789 s / batch. (data: 4.43e-02). ETA=9:01:16, max mem: 20.9 GB 
[11/24 10:35:43 visual_prompt]: 	Training 200/553. train loss: 10.0184,	0.8190 s / batch. (data: 3.24e-04). ETA=8:23:00, max mem: 20.9 GB 
[11/24 10:37:19 visual_prompt]: 	Training 300/553. train loss: 218.5066,	0.8370 s / batch. (data: 3.25e-04). ETA=8:32:41, max mem: 20.9 GB 
[11/24 10:38:57 visual_prompt]: 	Training 400/553. train loss: 36.7482,	0.8202 s / batch. (data: 2.83e-04). ETA=8:21:00, max mem: 20.9 GB 
[11/24 10:40:34 visual_prompt]: 	Training 500/553. train loss: 12.9240,	1.3680 s / batch. (data: 5.36e-01). ETA=13:53:21, max mem: 20.9 GB 
[11/24 10:41:24 visual_prompt]: Epoch 34 / 100: avg data time: 1.41e-01, avg batch time: 0.9691, average train loss: 77.0760
[11/24 10:42:19 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3111, average loss: 128.6205
[11/24 10:42:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.07	
[11/24 10:42:19 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[11/24 10:44:02 visual_prompt]: 	Training 100/553. train loss: 85.9022,	0.8365 s / batch. (data: 5.42e-03). ETA=8:27:27, max mem: 20.9 GB 
[11/24 10:45:40 visual_prompt]: 	Training 200/553. train loss: 1.6506,	0.8400 s / batch. (data: 3.15e-04). ETA=8:28:08, max mem: 20.9 GB 
[11/24 10:47:15 visual_prompt]: 	Training 300/553. train loss: 9.8607,	0.8360 s / batch. (data: 3.45e-04). ETA=8:24:20, max mem: 20.9 GB 
[11/24 10:48:50 visual_prompt]: 	Training 400/553. train loss: 9.2928,	0.8574 s / batch. (data: 1.52e-02). ETA=8:35:51, max mem: 20.9 GB 
[11/24 10:50:27 visual_prompt]: 	Training 500/553. train loss: 38.0204,	0.9739 s / batch. (data: 1.58e-01). ETA=9:44:17, max mem: 20.9 GB 
[11/24 10:51:18 visual_prompt]: Epoch 35 / 100: avg data time: 1.45e-01, avg batch time: 0.9736, average train loss: 76.9145
[11/24 10:52:13 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3088, average loss: 226.2540
[11/24 10:52:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.64	
[11/24 10:52:13 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[11/24 10:53:53 visual_prompt]: 	Training 100/553. train loss: 204.2154,	0.8280 s / batch. (data: 3.40e-04). ETA=8:14:38, max mem: 20.9 GB 
[11/24 10:55:30 visual_prompt]: 	Training 200/553. train loss: 130.6249,	0.8280 s / batch. (data: 3.37e-04). ETA=8:13:16, max mem: 20.9 GB 
[11/24 10:57:08 visual_prompt]: 	Training 300/553. train loss: 73.9381,	0.8372 s / batch. (data: 1.05e-02). ETA=8:17:21, max mem: 20.9 GB 
[11/24 10:58:44 visual_prompt]: 	Training 400/553. train loss: 78.8268,	0.8417 s / batch. (data: 2.40e-02). ETA=8:18:36, max mem: 20.9 GB 
[11/24 11:00:21 visual_prompt]: 	Training 500/553. train loss: 132.2474,	1.1958 s / batch. (data: 3.64e-01). ETA=11:46:24, max mem: 20.9 GB 
[11/24 11:01:09 visual_prompt]: Epoch 36 / 100: avg data time: 1.42e-01, avg batch time: 0.9693, average train loss: 84.0513
[11/24 11:02:05 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3100, average loss: 107.7796
[11/24 11:02:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.74	
[11/24 11:02:05 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[11/24 11:03:45 visual_prompt]: 	Training 100/553. train loss: 194.6217,	0.8356 s / batch. (data: 3.26e-04). ETA=8:11:31, max mem: 20.9 GB 
[11/24 11:05:20 visual_prompt]: 	Training 200/553. train loss: 76.6887,	0.8320 s / batch. (data: 3.09e-04). ETA=8:07:59, max mem: 20.9 GB 
[11/24 11:06:57 visual_prompt]: 	Training 300/553. train loss: 47.6810,	1.1606 s / batch. (data: 3.30e-01). ETA=11:18:49, max mem: 20.9 GB 
[11/24 11:08:35 visual_prompt]: 	Training 400/553. train loss: 23.0807,	1.4307 s / batch. (data: 6.26e-01). ETA=13:54:22, max mem: 20.9 GB 
[11/24 11:10:07 visual_prompt]: 	Training 500/553. train loss: 54.7606,	0.8320 s / batch. (data: 7.96e-03). ETA=8:03:50, max mem: 20.9 GB 
[11/24 11:10:59 visual_prompt]: Epoch 37 / 100: avg data time: 1.37e-01, avg batch time: 0.9661, average train loss: 63.5501
[11/24 11:11:54 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3106, average loss: 28.5747
[11/24 11:11:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.05	
[11/24 11:11:54 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[11/24 11:13:32 visual_prompt]: 	Training 100/553. train loss: 72.7354,	0.8160 s / batch. (data: 3.23e-04). ETA=7:52:26, max mem: 20.9 GB 
[11/24 11:15:09 visual_prompt]: 	Training 200/553. train loss: 29.6928,	0.8535 s / batch. (data: 5.43e-03). ETA=8:12:45, max mem: 20.9 GB 
[11/24 11:16:46 visual_prompt]: 	Training 300/553. train loss: 144.9714,	0.8412 s / batch. (data: 1.05e-02). ETA=8:04:12, max mem: 20.9 GB 
[11/24 11:18:20 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8199 s / batch. (data: 4.54e-04). ETA=7:50:37, max mem: 20.9 GB 
[11/24 11:19:58 visual_prompt]: 	Training 500/553. train loss: 5.5101,	0.8400 s / batch. (data: 2.96e-04). ETA=8:00:44, max mem: 20.9 GB 
[11/24 11:20:47 visual_prompt]: Epoch 38 / 100: avg data time: 1.36e-01, avg batch time: 0.9649, average train loss: 68.1992
[11/24 11:21:42 visual_prompt]: Inference (val):avg data time: 2.77e-04, avg batch time: 0.3105, average loss: 51.7917
[11/24 11:21:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.04	
[11/24 11:21:42 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[11/24 11:23:20 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8310 s / batch. (data: 6.91e-03). ETA=7:53:28, max mem: 20.9 GB 
[11/24 11:25:03 visual_prompt]: 	Training 200/553. train loss: 141.9730,	0.8333 s / batch. (data: 7.23e-04). ETA=7:53:23, max mem: 20.9 GB 
[11/24 11:26:50 visual_prompt]: 	Training 300/553. train loss: 44.2764,	0.8355 s / batch. (data: 4.28e-04). ETA=7:53:15, max mem: 20.9 GB 
[11/24 11:28:32 visual_prompt]: 	Training 400/553. train loss: 30.7900,	0.8397 s / batch. (data: 3.31e-04). ETA=7:54:14, max mem: 20.9 GB 
[11/24 11:30:15 visual_prompt]: 	Training 500/553. train loss: 35.6469,	1.8769 s / batch. (data: 1.06e+00). ETA=17:36:52, max mem: 20.9 GB 
[11/24 11:31:06 visual_prompt]: Epoch 39 / 100: avg data time: 1.92e-01, avg batch time: 1.0197, average train loss: 62.3315
[11/24 11:32:05 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3117, average loss: 91.6431
[11/24 11:32:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.29	
[11/24 11:32:05 visual_prompt]: Stopping early.
[11/24 11:32:05 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 11:32:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 11:32:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/24 11:32:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 11:32:05 visual_prompt]: Training with config:
[11/24 11:32:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/24 11:32:05 visual_prompt]: Loading training data...
[11/24 11:32:05 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 11:32:05 visual_prompt]: Loading validation data...
[11/24 11:32:05 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 11:32:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 11:32:11 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 11:32:11 visual_prompt]: tuned percent:0.525
[11/24 11:32:11 visual_prompt]: Device used for model: 0
[11/24 11:32:11 visual_prompt]: Setting up Evaluator...
[11/24 11:32:11 visual_prompt]: Setting up Trainer...
[11/24 11:32:11 visual_prompt]: 	Setting up the optimizer...
[11/24 11:32:11 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 11:33:58 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8383 s / batch. (data: 2.23e-02). ETA=12:51:16, max mem: 20.9 GB 
[11/24 11:35:39 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8408 s / batch. (data: 1.09e-02). ETA=12:52:08, max mem: 20.9 GB 
[11/24 11:37:24 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.0796 s / batch. (data: 2.38e-01). ETA=16:29:38, max mem: 20.9 GB 
[11/24 11:39:04 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8360 s / batch. (data: 5.40e-03). ETA=12:44:56, max mem: 20.9 GB 
[11/24 11:40:47 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8320 s / batch. (data: 3.31e-04). ETA=12:39:54, max mem: 20.9 GB 
[11/24 11:41:40 visual_prompt]: Epoch 1 / 100: avg data time: 1.98e-01, avg batch time: 1.0292, average train loss: 1.5403
[11/24 11:42:38 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3107, average loss: 1.5201
[11/24 11:42:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 11:42:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/24 11:44:23 visual_prompt]: 	Training 100/553. train loss: 11.4707,	0.9320 s / batch. (data: 1.13e-01). ETA=14:08:50, max mem: 20.9 GB 
[11/24 11:46:04 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8280 s / batch. (data: 3.38e-04). ETA=12:32:44, max mem: 20.9 GB 
[11/24 11:47:47 visual_prompt]: 	Training 300/553. train loss: 5.9929,	0.9837 s / batch. (data: 1.63e-01). ETA=14:52:41, max mem: 20.9 GB 
[11/24 11:49:26 visual_prompt]: 	Training 400/553. train loss: 7.5676,	0.8409 s / batch. (data: 2.97e-04). ETA=12:41:38, max mem: 20.9 GB 
[11/24 11:51:08 visual_prompt]: 	Training 500/553. train loss: 1.9307,	0.8365 s / batch. (data: 3.02e-04). ETA=12:36:16, max mem: 20.9 GB 
[11/24 11:52:01 visual_prompt]: Epoch 2 / 100: avg data time: 1.85e-01, avg batch time: 1.0175, average train loss: 9.8064
[11/24 11:53:01 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3090, average loss: 21.4495
[11/24 11:53:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.40	
[11/24 11:53:01 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/24 11:54:47 visual_prompt]: 	Training 100/553. train loss: 15.6780,	0.8320 s / batch. (data: 4.83e-04). ETA=12:30:03, max mem: 20.9 GB 
[11/24 11:56:31 visual_prompt]: 	Training 200/553. train loss: 6.2633,	0.8327 s / batch. (data: 8.63e-03). ETA=12:29:20, max mem: 20.9 GB 
[11/24 11:58:13 visual_prompt]: 	Training 300/553. train loss: 8.7254,	0.8562 s / batch. (data: 2.34e-02). ETA=12:49:04, max mem: 20.9 GB 
[11/24 11:59:57 visual_prompt]: 	Training 400/553. train loss: 15.1111,	0.8400 s / batch. (data: 7.95e-03). ETA=12:33:07, max mem: 20.9 GB 
[11/24 12:01:41 visual_prompt]: 	Training 500/553. train loss: 71.0609,	1.3680 s / batch. (data: 5.36e-01). ETA=20:24:14, max mem: 20.9 GB 
[11/24 12:02:34 visual_prompt]: Epoch 3 / 100: avg data time: 2.03e-01, avg batch time: 1.0353, average train loss: 14.2301
[11/24 12:03:32 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3095, average loss: 21.8537
[11/24 12:03:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.46	
[11/24 12:03:32 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/24 12:05:18 visual_prompt]: 	Training 100/553. train loss: 9.9626,	0.8411 s / batch. (data: 3.33e-04). ETA=12:30:33, max mem: 20.9 GB 
[11/24 12:07:00 visual_prompt]: 	Training 200/553. train loss: 59.3786,	0.8251 s / batch. (data: 7.95e-03). ETA=12:14:54, max mem: 20.9 GB 
[11/24 12:08:41 visual_prompt]: 	Training 300/553. train loss: 4.1254,	1.5440 s / batch. (data: 7.13e-01). ETA=22:52:38, max mem: 20.9 GB 
[11/24 12:10:19 visual_prompt]: 	Training 400/553. train loss: 6.7726,	1.3200 s / batch. (data: 4.73e-01). ETA=19:31:15, max mem: 20.9 GB 
[11/24 12:12:02 visual_prompt]: 	Training 500/553. train loss: 49.7587,	3.6256 s / batch. (data: 2.80e+00). ETA=2 days, 5:31:08, max mem: 20.9 GB 
[11/24 12:12:55 visual_prompt]: Epoch 4 / 100: avg data time: 1.89e-01, avg batch time: 1.0194, average train loss: 26.8838
[11/24 12:13:53 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3105, average loss: 25.4921
[11/24 12:13:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.11	
[11/24 12:13:53 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/24 12:15:38 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8269 s / batch. (data: 3.10e-04). ETA=12:10:15, max mem: 20.9 GB 
[11/24 12:17:21 visual_prompt]: 	Training 200/553. train loss: 14.0414,	1.3300 s / batch. (data: 5.16e-01). ETA=19:32:23, max mem: 20.9 GB 
[11/24 12:19:05 visual_prompt]: 	Training 300/553. train loss: 54.3054,	0.8272 s / batch. (data: 5.50e-03). ETA=12:07:46, max mem: 20.9 GB 
[11/24 12:20:48 visual_prompt]: 	Training 400/553. train loss: 31.3902,	0.8487 s / batch. (data: 1.64e-02). ETA=12:25:18, max mem: 20.9 GB 
[11/24 12:22:31 visual_prompt]: 	Training 500/553. train loss: 138.4451,	0.8440 s / batch. (data: 3.20e-04). ETA=12:19:43, max mem: 20.9 GB 
[11/24 12:23:26 visual_prompt]: Epoch 5 / 100: avg data time: 2.05e-01, avg batch time: 1.0346, average train loss: 29.4678
[11/24 12:24:25 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3080, average loss: 44.2434
[11/24 12:24:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.93	
[11/24 12:24:25 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/24 12:26:13 visual_prompt]: 	Training 100/553. train loss: 152.0571,	0.8313 s / batch. (data: 3.33e-04). ETA=12:06:31, max mem: 20.9 GB 
[11/24 12:27:53 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8193 s / batch. (data: 1.55e-02). ETA=11:54:38, max mem: 20.9 GB 
[11/24 12:29:32 visual_prompt]: 	Training 300/553. train loss: 257.1109,	0.8200 s / batch. (data: 3.00e-04). ETA=11:53:52, max mem: 20.9 GB 
[11/24 12:31:16 visual_prompt]: 	Training 400/553. train loss: 63.9711,	0.8399 s / batch. (data: 1.21e-02). ETA=12:09:49, max mem: 20.9 GB 
[11/24 12:32:54 visual_prompt]: 	Training 500/553. train loss: 22.8794,	0.8243 s / batch. (data: 3.47e-04). ETA=11:54:52, max mem: 20.9 GB 
[11/24 12:33:46 visual_prompt]: Epoch 6 / 100: avg data time: 1.86e-01, avg batch time: 1.0158, average train loss: 43.9060
[11/24 12:34:44 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3108, average loss: 43.3593
[11/24 12:34:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.47	
[11/24 12:34:44 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/24 12:36:26 visual_prompt]: 	Training 100/553. train loss: 202.3694,	0.8312 s / batch. (data: 7.97e-03). ETA=11:58:42, max mem: 20.9 GB 
[11/24 12:38:06 visual_prompt]: 	Training 200/553. train loss: 34.1242,	0.9265 s / batch. (data: 9.73e-02). ETA=13:19:37, max mem: 20.9 GB 
[11/24 12:39:50 visual_prompt]: 	Training 300/553. train loss: 39.2158,	1.9452 s / batch. (data: 1.13e+00). ETA=1 day, 3:55:32, max mem: 20.9 GB 
[11/24 12:41:30 visual_prompt]: 	Training 400/553. train loss: 74.3282,	1.7071 s / batch. (data: 8.86e-01). ETA=1 day, 0:27:36, max mem: 20.9 GB 
[11/24 12:43:07 visual_prompt]: 	Training 500/553. train loss: 79.8908,	0.8571 s / batch. (data: 4.46e-02). ETA=12:15:24, max mem: 20.9 GB 
[11/24 12:43:59 visual_prompt]: Epoch 7 / 100: avg data time: 1.75e-01, avg batch time: 1.0044, average train loss: 55.4997
[11/24 12:44:56 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3094, average loss: 23.6999
[11/24 12:44:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.52	
[11/24 12:44:56 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/24 12:46:38 visual_prompt]: 	Training 100/553. train loss: 2.0181,	0.8216 s / batch. (data: 3.37e-04). ETA=11:42:53, max mem: 20.9 GB 
[11/24 12:48:18 visual_prompt]: 	Training 200/553. train loss: 61.1637,	0.8506 s / batch. (data: 2.61e-02). ETA=12:06:16, max mem: 20.9 GB 
[11/24 12:50:03 visual_prompt]: 	Training 300/553. train loss: 145.5445,	0.8335 s / batch. (data: 8.34e-04). ETA=11:50:18, max mem: 20.9 GB 
[11/24 12:51:46 visual_prompt]: 	Training 400/553. train loss: 41.8939,	0.8172 s / batch. (data: 3.20e-04). ETA=11:35:01, max mem: 20.9 GB 
[11/24 12:53:29 visual_prompt]: 	Training 500/553. train loss: 21.5642,	1.4273 s / batch. (data: 6.06e-01). ETA=20:11:33, max mem: 20.9 GB 
[11/24 12:54:23 visual_prompt]: Epoch 8 / 100: avg data time: 1.97e-01, avg batch time: 1.0244, average train loss: 56.5643
[11/24 12:55:22 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3089, average loss: 57.6910
[11/24 12:55:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.04	
[11/24 12:55:22 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/24 12:57:08 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8520 s / batch. (data: 7.95e-03). ETA=12:01:01, max mem: 20.9 GB 
[11/24 12:58:50 visual_prompt]: 	Training 200/553. train loss: 785.9756,	0.8226 s / batch. (data: 1.00e-02). ETA=11:34:46, max mem: 20.9 GB 
[11/24 13:00:34 visual_prompt]: 	Training 300/553. train loss: 56.0445,	1.7175 s / batch. (data: 8.96e-01). ETA=1 day, 0:07:44, max mem: 20.9 GB 
[11/24 13:02:17 visual_prompt]: 	Training 400/553. train loss: 135.3336,	0.8440 s / batch. (data: 3.10e-04). ETA=11:50:01, max mem: 20.9 GB 
[11/24 13:04:01 visual_prompt]: 	Training 500/553. train loss: 26.5560,	0.9887 s / batch. (data: 1.74e-01). ETA=13:50:05, max mem: 20.9 GB 
[11/24 13:04:54 visual_prompt]: Epoch 9 / 100: avg data time: 2.07e-01, avg batch time: 1.0337, average train loss: 68.2584
[11/24 13:05:52 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3097, average loss: 19.6656
[11/24 13:05:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.07	
[11/24 13:05:52 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/24 13:07:41 visual_prompt]: 	Training 100/553. train loss: 115.1190,	0.8292 s / batch. (data: 7.96e-03). ETA=11:34:05, max mem: 20.9 GB 
[11/24 13:09:21 visual_prompt]: 	Training 200/553. train loss: 64.3213,	0.8309 s / batch. (data: 1.07e-02). ETA=11:34:07, max mem: 20.9 GB 
[11/24 13:10:59 visual_prompt]: 	Training 300/553. train loss: 6.0597,	1.1005 s / batch. (data: 2.64e-01). ETA=15:17:32, max mem: 20.9 GB 
[11/24 13:12:36 visual_prompt]: 	Training 400/553. train loss: 90.0477,	0.8440 s / batch. (data: 3.33e-04). ETA=11:42:13, max mem: 20.9 GB 
[11/24 13:14:17 visual_prompt]: 	Training 500/553. train loss: 8.8133,	0.8348 s / batch. (data: 3.19e-04). ETA=11:33:13, max mem: 20.9 GB 
[11/24 13:15:08 visual_prompt]: Epoch 10 / 100: avg data time: 1.78e-01, avg batch time: 1.0050, average train loss: 76.3979
[11/24 13:16:04 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3114, average loss: 44.8438
[11/24 13:16:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.15	
[11/24 13:16:04 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/24 13:17:49 visual_prompt]: 	Training 100/553. train loss: 196.5261,	0.8491 s / batch. (data: 2.10e-02). ETA=11:42:53, max mem: 20.9 GB 
[11/24 13:19:30 visual_prompt]: 	Training 200/553. train loss: 278.2780,	0.8360 s / batch. (data: 3.23e-04). ETA=11:30:40, max mem: 20.9 GB 
[11/24 13:21:08 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.1126 s / batch. (data: 1.30e+00). ETA=1 day, 5:01:48, max mem: 20.9 GB 
[11/24 13:22:45 visual_prompt]: 	Training 400/553. train loss: 36.4821,	0.8497 s / batch. (data: 1.56e-02). ETA=11:39:09, max mem: 20.9 GB 
[11/24 13:24:23 visual_prompt]: 	Training 500/553. train loss: 162.8521,	0.8360 s / batch. (data: 7.96e-03). ETA=11:26:29, max mem: 20.9 GB 
[11/24 13:25:13 visual_prompt]: Epoch 11 / 100: avg data time: 1.65e-01, avg batch time: 0.9928, average train loss: 83.0005
[11/24 13:26:10 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3134, average loss: 18.7106
[11/24 13:26:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.66	
[11/24 13:26:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/24 13:27:54 visual_prompt]: 	Training 100/553. train loss: 126.5586,	0.9720 s / batch. (data: 1.40e-01). ETA=13:15:41, max mem: 20.9 GB 
[11/24 13:29:34 visual_prompt]: 	Training 200/553. train loss: 152.5565,	0.9224 s / batch. (data: 1.03e-01). ETA=12:33:31, max mem: 20.9 GB 
[11/24 13:31:11 visual_prompt]: 	Training 300/553. train loss: 21.7700,	0.8204 s / batch. (data: 1.20e-02). ETA=11:08:52, max mem: 20.9 GB 
[11/24 13:32:51 visual_prompt]: 	Training 400/553. train loss: 93.6778,	0.8357 s / batch. (data: 3.21e-04). ETA=11:19:54, max mem: 20.9 GB 
[11/24 13:34:29 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8315 s / batch. (data: 2.20e-02). ETA=11:15:07, max mem: 20.9 GB 
[11/24 13:35:20 visual_prompt]: Epoch 12 / 100: avg data time: 1.68e-01, avg batch time: 0.9951, average train loss: 86.6268
[11/24 13:36:16 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3100, average loss: 228.3232
[11/24 13:36:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.58	
[11/24 13:36:16 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/24 13:38:01 visual_prompt]: 	Training 100/553. train loss: 23.4021,	0.8143 s / batch. (data: 5.44e-03). ETA=10:59:06, max mem: 20.9 GB 
[11/24 13:39:36 visual_prompt]: 	Training 200/553. train loss: 55.1706,	0.8155 s / batch. (data: 3.10e-04). ETA=10:58:40, max mem: 20.9 GB 
[11/24 13:41:16 visual_prompt]: 	Training 300/553. train loss: 16.4816,	1.9027 s / batch. (data: 1.07e+00). ETA=1 day, 1:33:39, max mem: 20.9 GB 
[11/24 13:42:53 visual_prompt]: 	Training 400/553. train loss: 7.3962,	0.8177 s / batch. (data: 3.42e-04). ETA=10:57:46, max mem: 20.9 GB 
[11/24 13:44:34 visual_prompt]: 	Training 500/553. train loss: 32.3706,	0.8307 s / batch. (data: 2.94e-04). ETA=11:06:47, max mem: 20.9 GB 
[11/24 13:45:25 visual_prompt]: Epoch 13 / 100: avg data time: 1.65e-01, avg batch time: 0.9926, average train loss: 89.5001
[11/24 13:46:22 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3102, average loss: 41.0821
[11/24 13:46:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.43	
[11/24 13:46:22 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/24 13:48:06 visual_prompt]: 	Training 100/553. train loss: 79.9684,	0.8227 s / batch. (data: 2.94e-04). ETA=10:58:20, max mem: 20.9 GB 
[11/24 13:49:45 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8340 s / batch. (data: 2.89e-04). ETA=11:06:00, max mem: 20.9 GB 
[11/24 13:51:23 visual_prompt]: 	Training 300/553. train loss: 81.4320,	0.8880 s / batch. (data: 6.00e-02). ETA=11:47:35, max mem: 20.9 GB 
[11/24 13:53:05 visual_prompt]: 	Training 400/553. train loss: 23.2426,	0.8596 s / batch. (data: 2.36e-02). ETA=11:23:34, max mem: 20.9 GB 
[11/24 13:54:49 visual_prompt]: 	Training 500/553. train loss: 128.8445,	0.8400 s / batch. (data: 3.28e-04). ETA=11:06:33, max mem: 20.9 GB 
[11/24 13:55:42 visual_prompt]: Epoch 14 / 100: avg data time: 1.86e-01, avg batch time: 1.0130, average train loss: 94.1733
[11/24 13:56:41 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.3097, average loss: 72.9262
[11/24 13:56:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.32	
[11/24 13:56:41 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/24 13:58:27 visual_prompt]: 	Training 100/553. train loss: 107.7245,	0.9360 s / batch. (data: 1.07e-01). ETA=12:20:18, max mem: 20.9 GB 
[11/24 14:00:09 visual_prompt]: 	Training 200/553. train loss: 338.1583,	0.8353 s / batch. (data: 3.17e-04). ETA=10:59:16, max mem: 20.9 GB 
[11/24 14:01:54 visual_prompt]: 	Training 300/553. train loss: 10.7235,	0.8260 s / batch. (data: 3.26e-04). ETA=10:50:35, max mem: 20.9 GB 
[11/24 14:03:34 visual_prompt]: 	Training 400/553. train loss: 204.6113,	1.2204 s / batch. (data: 3.96e-01). ETA=15:59:10, max mem: 20.9 GB 
[11/24 14:05:17 visual_prompt]: 	Training 500/553. train loss: 120.6582,	0.8480 s / batch. (data: 2.61e-04). ETA=11:05:06, max mem: 20.9 GB 
[11/24 14:06:12 visual_prompt]: Epoch 15 / 100: avg data time: 2.05e-01, avg batch time: 1.0321, average train loss: 93.8931
[11/24 14:07:10 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3093, average loss: 5.1922
[11/24 14:07:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.96	
[11/24 14:07:10 visual_prompt]: Best epoch 15: best metric: -5.192
[11/24 14:07:10 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/24 14:08:55 visual_prompt]: 	Training 100/553. train loss: 40.4202,	0.8384 s / batch. (data: 3.19e-04). ETA=10:55:26, max mem: 20.9 GB 
[11/24 14:10:39 visual_prompt]: 	Training 200/553. train loss: 140.7945,	0.8160 s / batch. (data: 3.07e-04). ETA=10:36:33, max mem: 20.9 GB 
[11/24 14:12:20 visual_prompt]: 	Training 300/553. train loss: 13.6437,	0.8214 s / batch. (data: 2.88e-04). ETA=10:39:23, max mem: 20.9 GB 
[11/24 14:13:59 visual_prompt]: 	Training 400/553. train loss: 64.7440,	0.8340 s / batch. (data: 8.13e-04). ETA=10:47:48, max mem: 20.9 GB 
[11/24 14:15:37 visual_prompt]: 	Training 500/553. train loss: 203.5595,	1.0000 s / batch. (data: 1.67e-01). ETA=12:55:04, max mem: 20.9 GB 
[11/24 14:16:29 visual_prompt]: Epoch 16 / 100: avg data time: 1.83e-01, avg batch time: 1.0106, average train loss: 78.6311
[11/24 14:17:26 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3095, average loss: 4.1551
[11/24 14:17:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.29	
[11/24 14:17:26 visual_prompt]: Best epoch 16: best metric: -4.155
[11/24 14:17:26 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/24 14:19:08 visual_prompt]: 	Training 100/553. train loss: 3.5588,	0.8423 s / batch. (data: 7.94e-03). ETA=10:50:41, max mem: 20.9 GB 
[11/24 14:20:48 visual_prompt]: 	Training 200/553. train loss: 182.7799,	0.8150 s / batch. (data: 2.92e-04). ETA=10:28:17, max mem: 20.9 GB 
[11/24 14:22:26 visual_prompt]: 	Training 300/553. train loss: 111.7145,	0.8200 s / batch. (data: 3.12e-04). ETA=10:30:45, max mem: 20.9 GB 
[11/24 14:24:04 visual_prompt]: 	Training 400/553. train loss: 320.7078,	1.1328 s / batch. (data: 3.17e-01). ETA=14:29:25, max mem: 20.9 GB 
[11/24 14:25:43 visual_prompt]: 	Training 500/553. train loss: 124.6979,	1.6880 s / batch. (data: 8.62e-01). ETA=21:32:45, max mem: 20.9 GB 
[11/24 14:26:35 visual_prompt]: Epoch 17 / 100: avg data time: 1.66e-01, avg batch time: 0.9930, average train loss: 80.3591
[11/24 14:27:32 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3097, average loss: 219.9071
[11/24 14:27:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.28	
[11/24 14:27:32 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/24 14:29:15 visual_prompt]: 	Training 100/553. train loss: 65.2004,	0.8400 s / batch. (data: 7.57e-04). ETA=10:41:11, max mem: 20.9 GB 
[11/24 14:30:56 visual_prompt]: 	Training 200/553. train loss: 30.3554,	0.8466 s / batch. (data: 7.54e-04). ETA=10:44:49, max mem: 20.9 GB 
[11/24 14:32:35 visual_prompt]: 	Training 300/553. train loss: 118.4125,	0.8440 s / batch. (data: 2.95e-04). ETA=10:41:24, max mem: 20.9 GB 
[11/24 14:34:13 visual_prompt]: 	Training 400/553. train loss: 23.7052,	0.8501 s / batch. (data: 1.20e-02). ETA=10:44:37, max mem: 20.9 GB 
[11/24 14:35:52 visual_prompt]: 	Training 500/553. train loss: 84.4155,	0.8208 s / batch. (data: 5.46e-03). ETA=10:21:02, max mem: 20.9 GB 
[11/24 14:36:42 visual_prompt]: Epoch 18 / 100: avg data time: 1.67e-01, avg batch time: 0.9949, average train loss: 81.1541
[11/24 14:37:39 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3108, average loss: 55.5910
[11/24 14:37:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.31	
[11/24 14:37:39 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/24 14:39:21 visual_prompt]: 	Training 100/553. train loss: 33.5354,	1.0880 s / batch. (data: 2.58e-01). ETA=13:40:27, max mem: 20.9 GB 
[11/24 14:41:01 visual_prompt]: 	Training 200/553. train loss: 48.6493,	0.8271 s / batch. (data: 3.13e-04). ETA=10:22:18, max mem: 20.9 GB 
[11/24 14:42:39 visual_prompt]: 	Training 300/553. train loss: 153.9044,	0.8400 s / batch. (data: 3.19e-04). ETA=10:30:36, max mem: 20.9 GB 
[11/24 14:44:19 visual_prompt]: 	Training 400/553. train loss: 50.0509,	0.8192 s / batch. (data: 7.99e-04). ETA=10:13:39, max mem: 20.9 GB 
[11/24 14:45:55 visual_prompt]: 	Training 500/553. train loss: 2.3170,	0.8554 s / batch. (data: 5.44e-03). ETA=10:39:21, max mem: 20.9 GB 
[11/24 14:46:46 visual_prompt]: Epoch 19 / 100: avg data time: 1.61e-01, avg batch time: 0.9897, average train loss: 72.0063
[11/24 14:47:42 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3105, average loss: 17.9899
[11/24 14:47:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.09	
[11/24 14:47:42 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/24 14:49:24 visual_prompt]: 	Training 100/553. train loss: 5.2882,	0.8560 s / batch. (data: 3.16e-04). ETA=10:37:36, max mem: 20.9 GB 
[11/24 14:51:03 visual_prompt]: 	Training 200/553. train loss: 18.6954,	0.8440 s / batch. (data: 3.27e-04). ETA=10:27:16, max mem: 20.9 GB 
[11/24 14:52:43 visual_prompt]: 	Training 300/553. train loss: 22.6528,	0.8201 s / batch. (data: 3.34e-04). ETA=10:08:08, max mem: 20.9 GB 
[11/24 14:54:25 visual_prompt]: 	Training 400/553. train loss: 22.2894,	0.8383 s / batch. (data: 7.95e-03). ETA=10:20:12, max mem: 20.9 GB 
[11/24 14:56:08 visual_prompt]: 	Training 500/553. train loss: 41.9790,	0.8473 s / batch. (data: 1.19e-02). ETA=10:25:27, max mem: 20.9 GB 
[11/24 14:57:03 visual_prompt]: Epoch 20 / 100: avg data time: 1.86e-01, avg batch time: 1.0135, average train loss: 79.3548
[11/24 14:58:02 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3102, average loss: 51.9980
[11/24 14:58:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.62	
[11/24 14:58:02 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/24 14:59:51 visual_prompt]: 	Training 100/553. train loss: 77.1954,	0.8171 s / batch. (data: 3.06e-04). ETA=10:01:07, max mem: 20.9 GB 
[11/24 15:01:33 visual_prompt]: 	Training 200/553. train loss: 90.5409,	0.8169 s / batch. (data: 3.54e-04). ETA=9:59:35, max mem: 20.9 GB 
[11/24 15:03:14 visual_prompt]: 	Training 300/553. train loss: 120.1923,	1.0324 s / batch. (data: 2.04e-01). ETA=12:36:01, max mem: 20.9 GB 
[11/24 15:04:56 visual_prompt]: 	Training 400/553. train loss: 23.0740,	0.8216 s / batch. (data: 3.48e-04). ETA=10:00:17, max mem: 20.9 GB 
[11/24 15:06:42 visual_prompt]: 	Training 500/553. train loss: 62.0444,	0.8487 s / batch. (data: 8.65e-03). ETA=10:18:41, max mem: 20.9 GB 
[11/24 15:07:36 visual_prompt]: Epoch 21 / 100: avg data time: 2.11e-01, avg batch time: 1.0380, average train loss: 80.6375
[11/24 15:08:33 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3112, average loss: 5.7173
[11/24 15:08:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.95	
[11/24 15:08:33 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/24 15:10:21 visual_prompt]: 	Training 100/553. train loss: 45.5380,	0.8200 s / batch. (data: 3.20e-04). ETA=9:55:41, max mem: 20.9 GB 
[11/24 15:12:06 visual_prompt]: 	Training 200/553. train loss: 14.3998,	0.8400 s / batch. (data: 7.94e-03). ETA=10:08:47, max mem: 20.9 GB 
[11/24 15:13:47 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8280 s / batch. (data: 7.95e-03). ETA=9:58:43, max mem: 20.9 GB 
[11/24 15:15:33 visual_prompt]: 	Training 400/553. train loss: 41.4337,	0.8358 s / batch. (data: 7.05e-04). ETA=10:03:00, max mem: 20.9 GB 
[11/24 15:17:15 visual_prompt]: 	Training 500/553. train loss: 41.4893,	0.8080 s / batch. (data: 3.22e-04). ETA=9:41:34, max mem: 20.9 GB 
[11/24 15:18:11 visual_prompt]: Epoch 22 / 100: avg data time: 2.16e-01, avg batch time: 1.0448, average train loss: 82.7818
[11/24 15:19:11 visual_prompt]: Inference (val):avg data time: 4.44e-05, avg batch time: 0.3112, average loss: 30.7563
[11/24 15:19:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.44	
[11/24 15:19:11 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/24 15:20:56 visual_prompt]: 	Training 100/553. train loss: 123.4828,	0.8200 s / batch. (data: 5.43e-03). ETA=9:48:09, max mem: 20.9 GB 
[11/24 15:22:33 visual_prompt]: 	Training 200/553. train loss: 15.2011,	0.8359 s / batch. (data: 3.05e-04). ETA=9:58:09, max mem: 20.9 GB 
[11/24 15:24:11 visual_prompt]: 	Training 300/553. train loss: 234.7354,	0.8138 s / batch. (data: 2.94e-04). ETA=9:40:57, max mem: 20.9 GB 
[11/24 15:25:52 visual_prompt]: 	Training 400/553. train loss: 102.2313,	0.8201 s / batch. (data: 3.18e-04). ETA=9:44:06, max mem: 20.9 GB 
[11/24 15:27:34 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8200 s / batch. (data: 3.86e-04). ETA=9:42:38, max mem: 20.9 GB 
[11/24 15:28:27 visual_prompt]: Epoch 23 / 100: avg data time: 1.78e-01, avg batch time: 1.0049, average train loss: 75.8306
[11/24 15:29:24 visual_prompt]: Inference (val):avg data time: 4.20e-04, avg batch time: 0.3094, average loss: 146.7328
[11/24 15:29:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.00	
[11/24 15:29:24 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/24 15:31:10 visual_prompt]: 	Training 100/553. train loss: 80.0707,	0.8202 s / batch. (data: 1.18e-02). ETA=9:40:44, max mem: 20.9 GB 
[11/24 15:32:52 visual_prompt]: 	Training 200/553. train loss: 151.4542,	0.8338 s / batch. (data: 1.73e-02). ETA=9:48:56, max mem: 20.9 GB 
[11/24 15:34:29 visual_prompt]: 	Training 300/553. train loss: 10.0722,	0.8580 s / batch. (data: 3.64e-04). ETA=10:04:35, max mem: 20.9 GB 
[11/24 15:36:05 visual_prompt]: 	Training 400/553. train loss: 18.1664,	0.8456 s / batch. (data: 2.81e-02). ETA=9:54:28, max mem: 20.9 GB 
[11/24 15:37:42 visual_prompt]: 	Training 500/553. train loss: 34.1217,	0.8524 s / batch. (data: 1.02e-03). ETA=9:57:50, max mem: 20.9 GB 
[11/24 15:38:32 visual_prompt]: Epoch 24 / 100: avg data time: 1.64e-01, avg batch time: 0.9912, average train loss: 79.6634
[11/24 15:39:29 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3118, average loss: 23.4213
[11/24 15:39:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.63	
[11/24 15:39:29 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/24 15:41:21 visual_prompt]: 	Training 100/553. train loss: 141.2382,	0.8578 s / batch. (data: 1.06e-02). ETA=9:59:24, max mem: 20.9 GB 
[11/24 15:43:01 visual_prompt]: 	Training 200/553. train loss: 37.6976,	0.9555 s / batch. (data: 1.28e-01). ETA=11:06:07, max mem: 20.9 GB 
[11/24 15:44:45 visual_prompt]: 	Training 300/553. train loss: 216.4294,	1.0473 s / batch. (data: 2.38e-01). ETA=12:08:20, max mem: 20.9 GB 
[11/24 15:46:30 visual_prompt]: 	Training 400/553. train loss: 105.7682,	1.3440 s / batch. (data: 5.16e-01). ETA=15:32:28, max mem: 20.9 GB 
[11/24 15:48:14 visual_prompt]: 	Training 500/553. train loss: 40.7476,	1.7089 s / batch. (data: 8.99e-01). ETA=19:42:45, max mem: 20.9 GB 
[11/24 15:49:08 visual_prompt]: Epoch 25 / 100: avg data time: 2.19e-01, avg batch time: 1.0462, average train loss: 73.2479
[11/24 15:50:08 visual_prompt]: Inference (val):avg data time: 4.80e-05, avg batch time: 0.3075, average loss: 121.4401
[11/24 15:50:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.64	
[11/24 15:50:08 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/24 15:51:49 visual_prompt]: 	Training 100/553. train loss: 232.7358,	0.8288 s / batch. (data: 3.08e-04). ETA=9:31:33, max mem: 20.9 GB 
[11/24 15:53:35 visual_prompt]: 	Training 200/553. train loss: 22.3119,	2.0153 s / batch. (data: 1.21e+00). ETA=23:06:22, max mem: 20.9 GB 
[11/24 15:55:21 visual_prompt]: 	Training 300/553. train loss: 224.7032,	0.8743 s / batch. (data: 8.64e-04). ETA=9:59:59, max mem: 20.9 GB 
[11/24 15:56:57 visual_prompt]: 	Training 400/553. train loss: 11.7838,	0.8574 s / batch. (data: 2.14e-02). ETA=9:46:57, max mem: 20.9 GB 
[11/24 15:58:31 visual_prompt]: 	Training 500/553. train loss: 139.0402,	0.8312 s / batch. (data: 1.05e-02). ETA=9:27:40, max mem: 20.9 GB 
[11/24 15:59:21 visual_prompt]: Epoch 26 / 100: avg data time: 1.71e-01, avg batch time: 0.9988, average train loss: 90.0629
[11/24 16:00:15 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3108, average loss: 45.1047
[11/24 16:00:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.26	
[11/24 16:00:15 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[11/24 16:01:55 visual_prompt]: 	Training 100/553. train loss: 22.0259,	0.8437 s / batch. (data: 2.86e-04). ETA=9:34:01, max mem: 20.9 GB 
[11/24 16:03:29 visual_prompt]: 	Training 200/553. train loss: 166.5758,	0.8480 s / batch. (data: 1.19e-02). ETA=9:35:32, max mem: 20.9 GB 
[11/24 16:05:04 visual_prompt]: 	Training 300/553. train loss: 47.4541,	0.8473 s / batch. (data: 5.41e-03). ETA=9:33:37, max mem: 20.9 GB 
[11/24 16:06:39 visual_prompt]: 	Training 400/553. train loss: 40.6689,	0.8306 s / batch. (data: 5.40e-03). ETA=9:20:58, max mem: 20.9 GB 
[11/24 16:08:16 visual_prompt]: 	Training 500/553. train loss: 119.7567,	0.8410 s / batch. (data: 1.60e-02). ETA=9:26:34, max mem: 20.9 GB 
[11/24 16:09:04 visual_prompt]: Epoch 27 / 100: avg data time: 1.27e-01, avg batch time: 0.9566, average train loss: 77.0611
[11/24 16:09:58 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3106, average loss: 40.9811
[11/24 16:09:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.89	
[11/24 16:09:58 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[11/24 16:11:36 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8266 s / batch. (data: 5.51e-03). ETA=9:14:45, max mem: 20.9 GB 
[11/24 16:13:13 visual_prompt]: 	Training 200/553. train loss: 54.0814,	0.8321 s / batch. (data: 6.55e-03). ETA=9:17:02, max mem: 20.9 GB 
[11/24 16:14:50 visual_prompt]: 	Training 300/553. train loss: 81.7058,	1.4800 s / batch. (data: 6.55e-01). ETA=16:28:21, max mem: 20.9 GB 
[11/24 16:16:28 visual_prompt]: 	Training 400/553. train loss: 65.5140,	0.8275 s / batch. (data: 3.08e-04). ETA=9:11:14, max mem: 20.9 GB 
[11/24 16:18:12 visual_prompt]: 	Training 500/553. train loss: 182.4296,	0.8356 s / batch. (data: 5.45e-03). ETA=9:15:12, max mem: 20.9 GB 
[11/24 16:19:07 visual_prompt]: Epoch 28 / 100: avg data time: 1.64e-01, avg batch time: 0.9920, average train loss: 82.3342
[11/24 16:20:03 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3093, average loss: 18.1313
[11/24 16:20:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.82	
[11/24 16:20:03 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[11/24 16:21:50 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8316 s / batch. (data: 6.93e-04). ETA=9:10:29, max mem: 20.9 GB 
[11/24 16:23:26 visual_prompt]: 	Training 200/553. train loss: 130.6215,	1.6437 s / batch. (data: 8.31e-01). ETA=18:05:17, max mem: 20.9 GB 
[11/24 16:25:00 visual_prompt]: 	Training 300/553. train loss: 62.4456,	0.8432 s / batch. (data: 5.91e-03). ETA=9:15:20, max mem: 20.9 GB 
[11/24 16:26:37 visual_prompt]: 	Training 400/553. train loss: 62.6526,	0.8200 s / batch. (data: 3.60e-04). ETA=8:58:41, max mem: 20.9 GB 
[11/24 16:28:22 visual_prompt]: 	Training 500/553. train loss: 33.2094,	0.8322 s / batch. (data: 3.59e-04). ETA=9:05:17, max mem: 20.9 GB 
[11/24 16:29:17 visual_prompt]: Epoch 29 / 100: avg data time: 1.74e-01, avg batch time: 1.0015, average train loss: 68.3521
[11/24 16:30:14 visual_prompt]: Inference (val):avg data time: 2.05e-04, avg batch time: 0.3111, average loss: 34.2258
[11/24 16:30:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.92	
[11/24 16:30:14 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[11/24 16:31:52 visual_prompt]: 	Training 100/553. train loss: 201.8579,	0.8160 s / batch. (data: 2.95e-04). ETA=8:52:37, max mem: 20.9 GB 
[11/24 16:33:38 visual_prompt]: 	Training 200/553. train loss: 57.9761,	0.8259 s / batch. (data: 3.63e-04). ETA=8:57:42, max mem: 20.9 GB 
[11/24 16:35:22 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.5851 s / batch. (data: 7.57e-01). ETA=17:09:19, max mem: 20.9 GB 
[11/24 16:37:00 visual_prompt]: 	Training 400/553. train loss: 193.9628,	1.0579 s / batch. (data: 2.45e-01). ETA=11:25:11, max mem: 20.9 GB 
[11/24 16:38:36 visual_prompt]: 	Training 500/553. train loss: 112.9856,	1.3351 s / batch. (data: 5.01e-01). ETA=14:22:30, max mem: 20.9 GB 
[11/24 16:39:28 visual_prompt]: Epoch 30 / 100: avg data time: 1.74e-01, avg batch time: 1.0015, average train loss: 75.3280
[11/24 16:40:23 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3107, average loss: 84.2754
[11/24 16:40:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.72	
[11/24 16:40:23 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[11/24 16:42:04 visual_prompt]: 	Training 100/553. train loss: 85.2338,	0.8084 s / batch. (data: 5.43e-03). ETA=8:40:12, max mem: 20.9 GB 
[11/24 16:43:42 visual_prompt]: 	Training 200/553. train loss: 79.2875,	0.8501 s / batch. (data: 2.96e-02). ETA=9:05:38, max mem: 20.9 GB 
[11/24 16:45:15 visual_prompt]: 	Training 300/553. train loss: 85.9727,	0.8360 s / batch. (data: 3.05e-04). ETA=8:55:11, max mem: 20.9 GB 
[11/24 16:46:50 visual_prompt]: 	Training 400/553. train loss: 94.8413,	1.0199 s / batch. (data: 1.79e-01). ETA=10:51:14, max mem: 20.9 GB 
[11/24 16:48:26 visual_prompt]: 	Training 500/553. train loss: 3.8826,	0.8369 s / batch. (data: 2.04e-02). ETA=8:52:57, max mem: 20.9 GB 
[11/24 16:49:16 visual_prompt]: Epoch 31 / 100: avg data time: 1.35e-01, avg batch time: 0.9631, average train loss: 77.4775
[11/24 16:50:10 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3102, average loss: 164.9876
[11/24 16:50:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.85	
[11/24 16:50:10 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[11/24 16:51:50 visual_prompt]: 	Training 100/553. train loss: 33.6230,	0.8162 s / batch. (data: 5.51e-03). ETA=8:37:41, max mem: 20.9 GB 
[11/24 16:53:25 visual_prompt]: 	Training 200/553. train loss: 584.8047,	0.8441 s / batch. (data: 5.45e-03). ETA=8:54:01, max mem: 20.9 GB 
[11/24 16:55:05 visual_prompt]: 	Training 300/553. train loss: 94.3624,	0.8496 s / batch. (data: 1.56e-02). ETA=8:56:01, max mem: 20.9 GB 
[11/24 16:56:43 visual_prompt]: 	Training 400/553. train loss: 93.1784,	0.8400 s / batch. (data: 7.96e-03). ETA=8:48:36, max mem: 20.9 GB 
[11/24 16:58:18 visual_prompt]: 	Training 500/553. train loss: 66.1400,	0.8224 s / batch. (data: 8.53e-03). ETA=8:36:09, max mem: 20.9 GB 
[11/24 16:59:06 visual_prompt]: Epoch 32 / 100: avg data time: 1.41e-01, avg batch time: 0.9684, average train loss: 83.4985
[11/24 17:00:00 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.3101, average loss: 234.1134
[11/24 17:00:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.03	
[11/24 17:00:00 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[11/24 17:01:38 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8400 s / batch. (data: 3.04e-04). ETA=8:45:02, max mem: 20.9 GB 
[11/24 17:03:15 visual_prompt]: 	Training 200/553. train loss: 216.9189,	0.8465 s / batch. (data: 2.97e-04). ETA=8:47:41, max mem: 20.9 GB 
[11/24 17:04:49 visual_prompt]: 	Training 300/553. train loss: 64.6718,	0.8560 s / batch. (data: 2.88e-04). ETA=8:52:12, max mem: 20.9 GB 
[11/24 17:06:27 visual_prompt]: 	Training 400/553. train loss: 8.0882,	0.8249 s / batch. (data: 3.01e-04). ETA=8:31:29, max mem: 20.9 GB 
[11/24 17:08:03 visual_prompt]: 	Training 500/553. train loss: 4.8989,	0.8240 s / batch. (data: 3.23e-04). ETA=8:29:33, max mem: 20.9 GB 
[11/24 17:08:53 visual_prompt]: Epoch 33 / 100: avg data time: 1.34e-01, avg batch time: 0.9634, average train loss: 67.1823
[11/24 17:09:48 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3110, average loss: 66.9563
[11/24 17:09:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.69	
[11/24 17:09:48 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[11/24 17:11:30 visual_prompt]: 	Training 100/553. train loss: 55.6792,	0.8190 s / batch. (data: 1.05e-02). ETA=8:24:21, max mem: 20.9 GB 
[11/24 17:13:05 visual_prompt]: 	Training 200/553. train loss: 100.1291,	0.8240 s / batch. (data: 1.19e-02). ETA=8:26:05, max mem: 20.9 GB 
[11/24 17:14:41 visual_prompt]: 	Training 300/553. train loss: 10.0560,	0.8242 s / batch. (data: 3.58e-04). ETA=8:24:49, max mem: 20.9 GB 
[11/24 17:16:18 visual_prompt]: 	Training 400/553. train loss: 19.5371,	0.8440 s / batch. (data: 2.73e-04). ETA=8:35:33, max mem: 20.9 GB 
[11/24 17:17:55 visual_prompt]: 	Training 500/553. train loss: 9.3318,	1.3760 s / batch. (data: 5.35e-01). ETA=13:58:15, max mem: 20.9 GB 
[11/24 17:18:45 visual_prompt]: Epoch 34 / 100: avg data time: 1.42e-01, avg batch time: 0.9711, average train loss: 75.3529
[11/24 17:19:40 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3114, average loss: 29.7108
[11/24 17:19:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.47	
[11/24 17:19:40 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[11/24 17:21:22 visual_prompt]: 	Training 100/553. train loss: 29.5550,	0.8239 s / batch. (data: 5.60e-03). ETA=8:19:47, max mem: 20.9 GB 
[11/24 17:22:59 visual_prompt]: 	Training 200/553. train loss: 86.2786,	0.8206 s / batch. (data: 3.10e-04). ETA=8:16:26, max mem: 20.9 GB 
[11/24 17:24:35 visual_prompt]: 	Training 300/553. train loss: 75.0879,	0.8247 s / batch. (data: 3.33e-04). ETA=8:17:32, max mem: 20.9 GB 
[11/24 17:26:10 visual_prompt]: 	Training 400/553. train loss: 96.4444,	0.8121 s / batch. (data: 5.42e-03). ETA=8:08:34, max mem: 20.9 GB 
[11/24 17:27:45 visual_prompt]: 	Training 500/553. train loss: 102.2573,	0.9397 s / batch. (data: 1.11e-01). ETA=9:23:48, max mem: 20.9 GB 
[11/24 17:28:37 visual_prompt]: Epoch 35 / 100: avg data time: 1.43e-01, avg batch time: 0.9709, average train loss: 72.9441
[11/24 17:29:32 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3099, average loss: 154.1847
[11/24 17:29:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.02	
[11/24 17:29:32 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[11/24 17:31:12 visual_prompt]: 	Training 100/553. train loss: 229.5056,	0.8640 s / batch. (data: 7.95e-03). ETA=8:36:10, max mem: 20.9 GB 
[11/24 17:32:49 visual_prompt]: 	Training 200/553. train loss: 52.7191,	0.8268 s / batch. (data: 7.96e-03). ETA=8:12:34, max mem: 20.9 GB 
[11/24 17:34:28 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8065 s / batch. (data: 2.92e-04). ETA=7:59:08, max mem: 20.9 GB 
[11/24 17:36:04 visual_prompt]: 	Training 400/553. train loss: 8.9776,	0.8360 s / batch. (data: 3.75e-04). ETA=8:15:15, max mem: 20.9 GB 
[11/24 17:37:42 visual_prompt]: 	Training 500/553. train loss: 9.7776,	0.9565 s / batch. (data: 1.31e-01). ETA=9:25:04, max mem: 20.9 GB 
[11/24 17:38:30 visual_prompt]: Epoch 36 / 100: avg data time: 1.44e-01, avg batch time: 0.9716, average train loss: 79.3547
[11/24 17:39:25 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3098, average loss: 2.7531
[11/24 17:39:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.56	rocauc: 51.42	
[11/24 17:39:25 visual_prompt]: Best epoch 36: best metric: -2.753
[11/24 17:39:25 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[11/24 17:41:05 visual_prompt]: 	Training 100/553. train loss: 2.4550,	0.8803 s / batch. (data: 2.43e-02). ETA=8:37:46, max mem: 20.9 GB 
[11/24 17:42:41 visual_prompt]: 	Training 200/553. train loss: 2.7339,	0.8524 s / batch. (data: 2.93e-04). ETA=8:19:59, max mem: 20.9 GB 
[11/24 17:44:17 visual_prompt]: 	Training 300/553. train loss: 5.9133,	1.1696 s / batch. (data: 3.42e-01). ETA=11:24:04, max mem: 20.9 GB 
[11/24 17:45:55 visual_prompt]: 	Training 400/553. train loss: 22.4601,	1.8960 s / batch. (data: 1.06e+00). ETA=18:25:44, max mem: 20.9 GB 
[11/24 17:47:28 visual_prompt]: 	Training 500/553. train loss: 3.0500,	1.0995 s / batch. (data: 2.63e-01). ETA=10:39:24, max mem: 20.9 GB 
[11/24 17:48:20 visual_prompt]: Epoch 37 / 100: avg data time: 1.40e-01, avg batch time: 0.9675, average train loss: 75.5481
[11/24 17:49:14 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3112, average loss: 55.5520
[11/24 17:49:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.53	
[11/24 17:49:14 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[11/24 17:50:52 visual_prompt]: 	Training 100/553. train loss: 3.5900,	0.8520 s / batch. (data: 2.50e-02). ETA=8:13:18, max mem: 20.9 GB 
[11/24 17:52:29 visual_prompt]: 	Training 200/553. train loss: 70.7483,	1.2559 s / batch. (data: 4.39e-01). ETA=12:05:04, max mem: 20.9 GB 
[11/24 17:54:06 visual_prompt]: 	Training 300/553. train loss: 19.4833,	0.8208 s / batch. (data: 1.04e-02). ETA=7:52:28, max mem: 20.9 GB 
[11/24 17:55:40 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8274 s / batch. (data: 4.38e-04). ETA=7:54:54, max mem: 20.9 GB 
[11/24 17:57:18 visual_prompt]: 	Training 500/553. train loss: 130.8910,	0.8293 s / batch. (data: 1.05e-02). ETA=7:54:36, max mem: 20.9 GB 
[11/24 17:58:06 visual_prompt]: Epoch 38 / 100: avg data time: 1.32e-01, avg batch time: 0.9618, average train loss: 64.8333
[11/24 17:59:01 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3098, average loss: 91.3237
[11/24 17:59:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.00	
[11/24 17:59:01 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[11/24 18:00:40 visual_prompt]: 	Training 100/553. train loss: 112.4012,	0.8240 s / batch. (data: 2.88e-04). ETA=7:49:28, max mem: 20.9 GB 
[11/24 18:02:21 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8198 s / batch. (data: 2.53e-04). ETA=7:45:42, max mem: 20.9 GB 
[11/24 18:04:01 visual_prompt]: 	Training 300/553. train loss: 64.3278,	0.8310 s / batch. (data: 2.94e-04). ETA=7:50:43, max mem: 20.9 GB 
[11/24 18:05:35 visual_prompt]: 	Training 400/553. train loss: 135.3309,	0.8249 s / batch. (data: 1.40e-03). ETA=7:45:51, max mem: 20.9 GB 
[11/24 18:07:13 visual_prompt]: 	Training 500/553. train loss: 7.7295,	1.5228 s / batch. (data: 6.88e-01). ETA=14:17:29, max mem: 20.9 GB 
[11/24 18:08:03 visual_prompt]: Epoch 39 / 100: avg data time: 1.50e-01, avg batch time: 0.9790, average train loss: 61.6084
[11/24 18:08:58 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3099, average loss: 42.6650
[11/24 18:08:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.31	
[11/24 18:08:58 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[11/24 18:10:40 visual_prompt]: 	Training 100/553. train loss: 63.2694,	0.8165 s / batch. (data: 2.89e-04). ETA=7:37:40, max mem: 20.9 GB 
[11/24 18:12:15 visual_prompt]: 	Training 200/553. train loss: 65.0934,	0.8275 s / batch. (data: 3.04e-04). ETA=7:42:28, max mem: 20.9 GB 
[11/24 18:13:53 visual_prompt]: 	Training 300/553. train loss: 29.2789,	0.8328 s / batch. (data: 5.88e-03). ETA=7:44:04, max mem: 20.9 GB 
[11/24 18:15:30 visual_prompt]: 	Training 400/553. train loss: 25.7974,	0.8362 s / batch. (data: 2.99e-04). ETA=7:44:33, max mem: 20.9 GB 
[11/24 18:17:06 visual_prompt]: 	Training 500/553. train loss: 54.5019,	0.8338 s / batch. (data: 1.59e-02). ETA=7:41:50, max mem: 20.9 GB 
[11/24 18:17:59 visual_prompt]: Epoch 40 / 100: avg data time: 1.48e-01, avg batch time: 0.9775, average train loss: 62.1924
[11/24 18:18:54 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3103, average loss: 38.1881
[11/24 18:18:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.34	
[11/24 18:18:54 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[11/24 18:20:38 visual_prompt]: 	Training 100/553. train loss: 21.5577,	0.8240 s / batch. (data: 8.03e-03). ETA=7:34:19, max mem: 20.9 GB 
[11/24 18:22:16 visual_prompt]: 	Training 200/553. train loss: 70.5236,	0.8400 s / batch. (data: 7.83e-04). ETA=7:41:43, max mem: 20.9 GB 
[11/24 18:23:51 visual_prompt]: 	Training 300/553. train loss: 87.8134,	0.8283 s / batch. (data: 5.42e-03). ETA=7:33:53, max mem: 20.9 GB 
[11/24 18:25:26 visual_prompt]: 	Training 400/553. train loss: 35.0689,	0.8373 s / batch. (data: 2.97e-04). ETA=7:37:26, max mem: 20.9 GB 
[11/24 18:27:00 visual_prompt]: 	Training 500/553. train loss: 11.2892,	0.8255 s / batch. (data: 2.67e-04). ETA=7:29:35, max mem: 20.9 GB 
[11/24 18:27:48 visual_prompt]: Epoch 41 / 100: avg data time: 1.37e-01, avg batch time: 0.9654, average train loss: 69.5526
[11/24 18:28:42 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3121, average loss: 61.6427
[11/24 18:28:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.02	
[11/24 18:28:42 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[11/24 18:30:19 visual_prompt]: 	Training 100/553. train loss: 53.7447,	0.8247 s / batch. (data: 1.55e-02). ETA=7:27:03, max mem: 20.9 GB 
[11/24 18:31:55 visual_prompt]: 	Training 200/553. train loss: 100.2336,	0.8587 s / batch. (data: 2.26e-02). ETA=7:44:04, max mem: 20.9 GB 
[11/24 18:33:29 visual_prompt]: 	Training 300/553. train loss: 77.9023,	0.8480 s / batch. (data: 2.88e-04). ETA=7:36:53, max mem: 20.9 GB 
[11/24 18:35:05 visual_prompt]: 	Training 400/553. train loss: 73.5132,	0.8158 s / batch. (data: 2.88e-04). ETA=7:18:10, max mem: 20.9 GB 
[11/24 18:36:39 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8039 s / batch. (data: 4.28e-04). ETA=7:10:28, max mem: 20.9 GB 
[11/24 18:37:29 visual_prompt]: Epoch 42 / 100: avg data time: 1.25e-01, avg batch time: 0.9534, average train loss: 63.3192
[11/24 18:38:23 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3095, average loss: 107.2634
[11/24 18:38:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.22	
[11/24 18:38:23 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[11/24 18:40:04 visual_prompt]: 	Training 100/553. train loss: 24.1858,	0.8242 s / batch. (data: 7.96e-03). ETA=7:19:13, max mem: 20.9 GB 
[11/24 18:41:38 visual_prompt]: 	Training 200/553. train loss: 55.2119,	0.8325 s / batch. (data: 3.19e-04). ETA=7:22:13, max mem: 20.9 GB 
[11/24 18:43:11 visual_prompt]: 	Training 300/553. train loss: 117.6169,	0.8396 s / batch. (data: 7.96e-03). ETA=7:24:38, max mem: 20.9 GB 
[11/24 18:44:44 visual_prompt]: 	Training 400/553. train loss: 16.0340,	0.8400 s / batch. (data: 3.25e-04). ETA=7:23:26, max mem: 20.9 GB 
[11/24 18:46:21 visual_prompt]: 	Training 500/553. train loss: 111.6719,	0.8321 s / batch. (data: 5.42e-03). ETA=7:17:52, max mem: 20.9 GB 
[11/24 18:47:12 visual_prompt]: Epoch 43 / 100: avg data time: 1.27e-01, avg batch time: 0.9562, average train loss: 55.8341
[11/24 18:48:06 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3087, average loss: 21.4282
[11/24 18:48:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.69	
[11/24 18:48:06 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[11/24 18:49:45 visual_prompt]: 	Training 100/553. train loss: 34.7514,	0.9783 s / batch. (data: 1.71e-01). ETA=8:32:19, max mem: 20.9 GB 
[11/24 18:51:22 visual_prompt]: 	Training 200/553. train loss: 6.3591,	0.8340 s / batch. (data: 6.94e-03). ETA=7:15:21, max mem: 20.9 GB 
[11/24 18:52:55 visual_prompt]: 	Training 300/553. train loss: 30.0497,	0.8243 s / batch. (data: 3.49e-04). ETA=7:08:56, max mem: 20.9 GB 
[11/24 18:54:29 visual_prompt]: 	Training 400/553. train loss: 16.9997,	0.8383 s / batch. (data: 1.20e-02). ETA=7:14:47, max mem: 20.9 GB 
[11/24 18:56:04 visual_prompt]: 	Training 500/553. train loss: 52.5224,	0.8520 s / batch. (data: 7.97e-03). ETA=7:20:30, max mem: 20.9 GB 
[11/24 18:56:54 visual_prompt]: Epoch 44 / 100: avg data time: 1.25e-01, avg batch time: 0.9540, average train loss: 64.4375
[11/24 18:57:48 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3103, average loss: 77.4245
[11/24 18:57:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.14	
[11/24 18:57:48 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[11/24 18:59:28 visual_prompt]: 	Training 100/553. train loss: 105.2987,	0.8176 s / batch. (data: 8.67e-03). ETA=7:00:39, max mem: 20.9 GB 
[11/24 19:01:00 visual_prompt]: 	Training 200/553. train loss: 50.7398,	0.8352 s / batch. (data: 5.41e-03). ETA=7:08:16, max mem: 20.9 GB 
[11/24 19:02:36 visual_prompt]: 	Training 300/553. train loss: 96.7920,	0.8114 s / batch. (data: 3.25e-04). ETA=6:54:43, max mem: 20.9 GB 
[11/24 19:04:10 visual_prompt]: 	Training 400/553. train loss: 26.5873,	0.8449 s / batch. (data: 1.99e-02). ETA=7:10:26, max mem: 20.9 GB 
[11/24 19:05:47 visual_prompt]: 	Training 500/553. train loss: 0.0645,	0.8204 s / batch. (data: 3.18e-04). ETA=6:56:35, max mem: 20.9 GB 
[11/24 19:06:36 visual_prompt]: Epoch 45 / 100: avg data time: 1.25e-01, avg batch time: 0.9545, average train loss: 59.2357
[11/24 19:07:30 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3096, average loss: 1.0666
[11/24 19:07:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.47	
[11/24 19:07:30 visual_prompt]: Best epoch 45: best metric: -1.067
[11/24 19:07:30 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[11/24 19:09:10 visual_prompt]: 	Training 100/553. train loss: 80.2868,	1.3092 s / batch. (data: 4.50e-01). ETA=11:01:29, max mem: 20.9 GB 
[11/24 19:10:46 visual_prompt]: 	Training 200/553. train loss: 23.6908,	0.8598 s / batch. (data: 2.99e-04). ETA=7:13:00, max mem: 20.9 GB 
[11/24 19:12:20 visual_prompt]: 	Training 300/553. train loss: 64.1164,	0.8267 s / batch. (data: 7.96e-03). ETA=6:54:55, max mem: 20.9 GB 
[11/24 19:13:55 visual_prompt]: 	Training 400/553. train loss: 27.6399,	0.8443 s / batch. (data: 5.40e-03). ETA=7:02:20, max mem: 20.9 GB 
[11/24 19:15:28 visual_prompt]: 	Training 500/553. train loss: 203.7790,	0.8240 s / batch. (data: 2.99e-04). ETA=6:50:49, max mem: 20.9 GB 
[11/24 19:16:19 visual_prompt]: Epoch 46 / 100: avg data time: 1.29e-01, avg batch time: 0.9567, average train loss: 58.7898
[11/24 19:17:13 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3101, average loss: 58.1680
[11/24 19:17:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.47	
[11/24 19:17:13 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[11/24 19:18:54 visual_prompt]: 	Training 100/553. train loss: 102.3112,	0.8310 s / batch. (data: 1.04e-02). ETA=6:52:11, max mem: 20.9 GB 
[11/24 19:20:25 visual_prompt]: 	Training 200/553. train loss: 39.0308,	1.3164 s / batch. (data: 4.86e-01). ETA=10:50:46, max mem: 20.9 GB 
[11/24 19:22:00 visual_prompt]: 	Training 300/553. train loss: 28.9458,	0.8280 s / batch. (data: 2.97e-04). ETA=6:47:58, max mem: 20.9 GB 
[11/24 19:23:36 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8417 s / batch. (data: 1.05e-02). ETA=6:53:18, max mem: 20.9 GB 
[11/24 19:25:10 visual_prompt]: 	Training 500/553. train loss: 123.2391,	0.8194 s / batch. (data: 2.95e-04). ETA=6:41:00, max mem: 20.9 GB 
[11/24 19:26:01 visual_prompt]: Epoch 47 / 100: avg data time: 1.25e-01, avg batch time: 0.9539, average train loss: 56.4288
[11/24 19:26:55 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3105, average loss: 65.2392
[11/24 19:26:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.92	
[11/24 19:26:55 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[11/24 19:28:34 visual_prompt]: 	Training 100/553. train loss: 27.8295,	0.8377 s / batch. (data: 2.91e-04). ETA=6:47:49, max mem: 20.9 GB 
[11/24 19:30:10 visual_prompt]: 	Training 200/553. train loss: 6.9854,	0.8492 s / batch. (data: 3.29e-02). ETA=6:51:59, max mem: 20.9 GB 
[11/24 19:31:46 visual_prompt]: 	Training 300/553. train loss: 101.3854,	1.6680 s / batch. (data: 8.43e-01). ETA=13:26:25, max mem: 20.9 GB 
[11/24 19:33:18 visual_prompt]: 	Training 400/553. train loss: 37.9694,	0.8322 s / batch. (data: 3.24e-04). ETA=6:40:59, max mem: 20.9 GB 
[11/24 19:34:54 visual_prompt]: 	Training 500/553. train loss: 114.5325,	0.8259 s / batch. (data: 1.21e-02). ETA=6:36:32, max mem: 20.9 GB 
[11/24 19:35:43 visual_prompt]: Epoch 48 / 100: avg data time: 1.25e-01, avg batch time: 0.9541, average train loss: 55.6506
[11/24 19:36:36 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3102, average loss: 94.5673
[11/24 19:36:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.57	
[11/24 19:36:36 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[11/24 19:38:15 visual_prompt]: 	Training 100/553. train loss: 48.2335,	0.8225 s / batch. (data: 8.09e-04). ETA=6:32:49, max mem: 20.9 GB 
[11/24 19:39:49 visual_prompt]: 	Training 200/553. train loss: 8.1025,	0.8559 s / batch. (data: 2.07e-02). ETA=6:47:20, max mem: 20.9 GB 
[11/24 19:41:24 visual_prompt]: 	Training 300/553. train loss: 48.0971,	0.8520 s / batch. (data: 2.93e-04). ETA=6:44:04, max mem: 20.9 GB 
[11/24 19:43:02 visual_prompt]: 	Training 400/553. train loss: 18.1919,	0.8519 s / batch. (data: 5.86e-03). ETA=6:42:37, max mem: 20.9 GB 
[11/24 19:44:38 visual_prompt]: 	Training 500/553. train loss: 20.7298,	0.8208 s / batch. (data: 8.08e-04). ETA=6:26:33, max mem: 20.9 GB 
[11/24 19:45:29 visual_prompt]: Epoch 49 / 100: avg data time: 1.33e-01, avg batch time: 0.9619, average train loss: 51.4502
[11/24 19:46:23 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3097, average loss: 7.3284
[11/24 19:46:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.43	
[11/24 19:46:23 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[11/24 19:48:03 visual_prompt]: 	Training 100/553. train loss: 19.3923,	0.8357 s / batch. (data: 9.22e-03). ETA=6:31:24, max mem: 20.9 GB 
[11/24 19:49:38 visual_prompt]: 	Training 200/553. train loss: 46.2160,	0.8528 s / batch. (data: 1.27e-02). ETA=6:38:00, max mem: 20.9 GB 
[11/24 19:51:12 visual_prompt]: 	Training 300/553. train loss: 103.4884,	0.8190 s / batch. (data: 7.11e-04). ETA=6:20:51, max mem: 20.9 GB 
[11/24 19:52:46 visual_prompt]: 	Training 400/553. train loss: 9.5271,	0.8640 s / batch. (data: 2.99e-04). ETA=6:40:21, max mem: 20.9 GB 
[11/24 19:54:21 visual_prompt]: 	Training 500/553. train loss: 27.6025,	0.8441 s / batch. (data: 2.91e-04). ETA=6:29:44, max mem: 20.9 GB 
[11/24 19:55:11 visual_prompt]: Epoch 50 / 100: avg data time: 1.25e-01, avg batch time: 0.9543, average train loss: 48.2504
[11/24 19:56:05 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3109, average loss: 77.0873
[11/24 19:56:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.27	
[11/24 19:56:05 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[11/24 19:57:44 visual_prompt]: 	Training 100/553. train loss: 23.4318,	1.0800 s / batch. (data: 2.27e-01). ETA=8:15:53, max mem: 20.9 GB 
[11/24 19:59:19 visual_prompt]: 	Training 200/553. train loss: 20.1287,	0.8489 s / batch. (data: 1.55e-02). ETA=6:28:23, max mem: 20.9 GB 
[11/24 20:00:55 visual_prompt]: 	Training 300/553. train loss: 22.4726,	0.8424 s / batch. (data: 7.96e-03). ETA=6:23:58, max mem: 20.9 GB 
[11/24 20:02:30 visual_prompt]: 	Training 400/553. train loss: 32.5742,	1.3118 s / batch. (data: 4.89e-01). ETA=9:55:45, max mem: 20.9 GB 
[11/24 20:04:05 visual_prompt]: 	Training 500/553. train loss: 2.4241,	0.8476 s / batch. (data: 2.55e-04). ETA=6:23:33, max mem: 20.9 GB 
[11/24 20:04:53 visual_prompt]: Epoch 51 / 100: avg data time: 1.25e-01, avg batch time: 0.9552, average train loss: 43.6193
[11/24 20:05:47 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3102, average loss: 26.9971
[11/24 20:05:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.01	
[11/24 20:05:47 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[11/24 20:07:30 visual_prompt]: 	Training 100/553. train loss: 113.3093,	0.8481 s / batch. (data: 7.84e-04). ETA=6:21:34, max mem: 20.9 GB 
[11/24 20:09:04 visual_prompt]: 	Training 200/553. train loss: 1.6175,	0.8508 s / batch. (data: 6.74e-03). ETA=6:21:23, max mem: 20.9 GB 
[11/24 20:10:39 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8357 s / batch. (data: 2.84e-04). ETA=6:13:13, max mem: 20.9 GB 
[11/24 20:12:16 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8281 s / batch. (data: 2.42e-04). ETA=6:08:27, max mem: 20.9 GB 
[11/24 20:13:47 visual_prompt]: 	Training 500/553. train loss: 30.4923,	0.8349 s / batch. (data: 1.60e-02). ETA=6:10:04, max mem: 20.9 GB 
[11/24 20:14:35 visual_prompt]: Epoch 52 / 100: avg data time: 1.26e-01, avg batch time: 0.9550, average train loss: 49.8677
[11/24 20:15:29 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3120, average loss: 289.2762
[11/24 20:15:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.61	
[11/24 20:15:29 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[11/24 20:17:08 visual_prompt]: 	Training 100/553. train loss: 97.3026,	0.8266 s / batch. (data: 2.88e-04). ETA=6:04:19, max mem: 20.9 GB 
[11/24 20:18:43 visual_prompt]: 	Training 200/553. train loss: 114.2686,	0.8440 s / batch. (data: 7.55e-04). ETA=6:10:35, max mem: 20.9 GB 
[11/24 20:20:18 visual_prompt]: 	Training 300/553. train loss: 64.3623,	0.8197 s / batch. (data: 7.53e-04). ETA=5:58:32, max mem: 20.9 GB 
[11/24 20:21:54 visual_prompt]: 	Training 400/553. train loss: 14.2451,	0.8280 s / batch. (data: 2.93e-04). ETA=6:00:47, max mem: 20.9 GB 
[11/24 20:23:28 visual_prompt]: 	Training 500/553. train loss: 49.2907,	0.8210 s / batch. (data: 2.83e-04). ETA=5:56:22, max mem: 20.9 GB 
[11/24 20:24:19 visual_prompt]: Epoch 53 / 100: avg data time: 1.28e-01, avg batch time: 0.9574, average train loss: 45.0775
[11/24 20:25:13 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3102, average loss: 70.0438
[11/24 20:25:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.39	
[11/24 20:25:13 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[11/24 20:26:53 visual_prompt]: 	Training 100/553. train loss: 3.6098,	0.8440 s / batch. (data: 2.99e-04). ETA=6:04:12, max mem: 20.9 GB 
[11/24 20:28:29 visual_prompt]: 	Training 200/553. train loss: 99.3682,	0.8205 s / batch. (data: 2.91e-04). ETA=5:52:40, max mem: 20.9 GB 
[11/24 20:30:04 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8120 s / batch. (data: 2.92e-04). ETA=5:47:41, max mem: 20.9 GB 
[11/24 20:31:39 visual_prompt]: 	Training 400/553. train loss: 204.8930,	0.8588 s / batch. (data: 6.74e-03). ETA=6:06:17, max mem: 20.9 GB 
[11/24 20:33:15 visual_prompt]: 	Training 500/553. train loss: 23.8333,	0.8258 s / batch. (data: 7.71e-04). ETA=5:50:50, max mem: 20.9 GB 
[11/24 20:34:05 visual_prompt]: Epoch 54 / 100: avg data time: 1.34e-01, avg batch time: 0.9626, average train loss: 49.6782
[11/24 20:34:59 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3108, average loss: 161.1635
[11/24 20:34:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.30	
[11/24 20:34:59 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[11/24 20:36:37 visual_prompt]: 	Training 100/553. train loss: 3.1165,	0.8860 s / batch. (data: 5.85e-02). ETA=6:14:08, max mem: 20.9 GB 
[11/24 20:38:12 visual_prompt]: 	Training 200/553. train loss: 58.2722,	0.8345 s / batch. (data: 5.44e-03). ETA=5:51:00, max mem: 20.9 GB 
[11/24 20:39:46 visual_prompt]: 	Training 300/553. train loss: 69.6328,	0.8280 s / batch. (data: 3.08e-04). ETA=5:46:54, max mem: 20.9 GB 
[11/24 20:41:21 visual_prompt]: 	Training 400/553. train loss: 46.1825,	0.8110 s / batch. (data: 3.13e-04). ETA=5:38:25, max mem: 20.9 GB 
[11/24 20:42:56 visual_prompt]: 	Training 500/553. train loss: 38.0969,	0.8200 s / batch. (data: 2.92e-04). ETA=5:40:49, max mem: 20.9 GB 
[11/24 20:43:46 visual_prompt]: Epoch 55 / 100: avg data time: 1.23e-01, avg batch time: 0.9529, average train loss: 44.1909
[11/24 20:44:41 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3091, average loss: 23.5711
[11/24 20:44:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.92	
[11/24 20:44:41 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[11/24 20:46:21 visual_prompt]: 	Training 100/553. train loss: 30.3800,	0.8190 s / batch. (data: 2.95e-04). ETA=5:38:19, max mem: 20.9 GB 
[11/24 20:47:56 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8292 s / batch. (data: 2.50e-04). ETA=5:41:08, max mem: 20.9 GB 
[11/24 20:49:33 visual_prompt]: 	Training 300/553. train loss: 8.7538,	0.8343 s / batch. (data: 7.46e-04). ETA=5:41:50, max mem: 20.9 GB 
[11/24 20:51:09 visual_prompt]: 	Training 400/553. train loss: 65.1911,	0.8532 s / batch. (data: 1.55e-02). ETA=5:48:10, max mem: 20.9 GB 
[11/24 20:52:45 visual_prompt]: 	Training 500/553. train loss: 43.9096,	2.0800 s / batch. (data: 1.26e+00). ETA=14:05:21, max mem: 20.9 GB 
[11/24 20:53:33 visual_prompt]: Epoch 56 / 100: avg data time: 1.32e-01, avg batch time: 0.9625, average train loss: 35.5305
[11/24 20:54:27 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3110, average loss: 11.8125
[11/24 20:54:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.58	
[11/24 20:54:27 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[11/24 20:56:09 visual_prompt]: 	Training 100/553. train loss: 25.2461,	0.8516 s / batch. (data: 8.69e-04). ETA=5:43:56, max mem: 20.9 GB 
[11/24 20:57:43 visual_prompt]: 	Training 200/553. train loss: 53.7476,	0.8458 s / batch. (data: 5.89e-03). ETA=5:40:10, max mem: 20.9 GB 
[11/24 20:59:17 visual_prompt]: 	Training 300/553. train loss: 718.4894,	0.8739 s / batch. (data: 1.55e-02). ETA=5:50:02, max mem: 20.9 GB 
[11/24 21:00:51 visual_prompt]: 	Training 400/553. train loss: 99.0629,	0.8131 s / batch. (data: 2.78e-04). ETA=5:24:19, max mem: 20.9 GB 
[11/24 21:02:24 visual_prompt]: 	Training 500/553. train loss: 34.1063,	0.8309 s / batch. (data: 2.52e-04). ETA=5:30:02, max mem: 20.9 GB 
[11/24 21:03:15 visual_prompt]: Epoch 57 / 100: avg data time: 1.26e-01, avg batch time: 0.9552, average train loss: 42.7073
[11/24 21:04:10 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3091, average loss: 34.4639
[11/24 21:04:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.28	
[11/24 21:04:10 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[11/24 21:05:49 visual_prompt]: 	Training 100/553. train loss: 19.3993,	1.0320 s / batch. (data: 1.78e-01). ETA=6:47:16, max mem: 20.9 GB 
[11/24 21:07:25 visual_prompt]: 	Training 200/553. train loss: 6.9843,	0.8480 s / batch. (data: 7.95e-03). ETA=5:33:15, max mem: 20.9 GB 
[11/24 21:09:04 visual_prompt]: 	Training 300/553. train loss: 7.0246,	0.8283 s / batch. (data: 5.40e-03). ETA=5:24:07, max mem: 20.9 GB 
[11/24 21:10:39 visual_prompt]: 	Training 400/553. train loss: 31.5897,	0.8400 s / batch. (data: 7.89e-04). ETA=5:27:18, max mem: 20.9 GB 
[11/24 21:12:14 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8200 s / batch. (data: 8.63e-04). ETA=5:18:08, max mem: 20.9 GB 
[11/24 21:13:03 visual_prompt]: Epoch 58 / 100: avg data time: 1.34e-01, avg batch time: 0.9646, average train loss: 37.0176
[11/24 21:13:58 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3108, average loss: 6.4688
[11/24 21:13:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.49	
[11/24 21:13:58 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[11/24 21:15:39 visual_prompt]: 	Training 100/553. train loss: 117.1536,	0.8151 s / batch. (data: 2.50e-04). ETA=5:14:09, max mem: 20.9 GB 
[11/24 21:17:15 visual_prompt]: 	Training 200/553. train loss: 103.0690,	0.8395 s / batch. (data: 3.31e-04). ETA=5:22:11, max mem: 20.9 GB 
[11/24 21:18:50 visual_prompt]: 	Training 300/553. train loss: 68.1451,	0.8239 s / batch. (data: 5.39e-03). ETA=5:14:47, max mem: 20.9 GB 
[11/24 21:20:25 visual_prompt]: 	Training 400/553. train loss: 17.8920,	0.8155 s / batch. (data: 3.15e-04). ETA=5:10:14, max mem: 20.9 GB 
[11/24 21:22:03 visual_prompt]: 	Training 500/553. train loss: 49.7379,	0.8310 s / batch. (data: 7.55e-04). ETA=5:14:44, max mem: 20.9 GB 
[11/24 21:22:51 visual_prompt]: Epoch 59 / 100: avg data time: 1.33e-01, avg batch time: 0.9641, average train loss: 41.9175
[11/24 21:23:45 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3101, average loss: 22.8153
[11/24 21:23:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.15	
[11/24 21:23:45 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[11/24 21:25:26 visual_prompt]: 	Training 100/553. train loss: 13.0365,	0.8400 s / batch. (data: 2.96e-04). ETA=5:16:00, max mem: 20.9 GB 
[11/24 21:27:02 visual_prompt]: 	Training 200/553. train loss: 14.9202,	0.8500 s / batch. (data: 9.99e-03). ETA=5:18:23, max mem: 20.9 GB 
[11/24 21:28:35 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.9320 s / batch. (data: 1.16e-01). ETA=5:47:31, max mem: 20.9 GB 
[11/24 21:30:13 visual_prompt]: 	Training 400/553. train loss: 25.1963,	1.0636 s / batch. (data: 2.03e-01). ETA=6:34:48, max mem: 20.9 GB 
[11/24 21:31:49 visual_prompt]: 	Training 500/553. train loss: 27.4358,	0.8395 s / batch. (data: 3.01e-04). ETA=5:10:13, max mem: 20.9 GB 
[11/24 21:32:40 visual_prompt]: Epoch 60 / 100: avg data time: 1.35e-01, avg batch time: 0.9655, average train loss: 36.9652
[11/24 21:33:34 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3103, average loss: 61.2489
[11/24 21:33:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.20	
[11/24 21:33:34 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[11/24 21:35:15 visual_prompt]: 	Training 100/553. train loss: 16.1770,	0.8257 s / batch. (data: 3.17e-04). ETA=5:03:01, max mem: 20.9 GB 
[11/24 21:36:52 visual_prompt]: 	Training 200/553. train loss: 13.1755,	1.7000 s / batch. (data: 8.68e-01). ETA=10:21:03, max mem: 20.9 GB 
[11/24 21:38:28 visual_prompt]: 	Training 300/553. train loss: 22.3110,	1.3053 s / batch. (data: 4.99e-01). ETA=7:54:42, max mem: 20.9 GB 
[11/24 21:40:01 visual_prompt]: 	Training 400/553. train loss: 9.8325,	0.8320 s / batch. (data: 5.70e-04). ETA=5:01:09, max mem: 20.9 GB 
[11/24 21:41:38 visual_prompt]: 	Training 500/553. train loss: 47.2574,	2.4073 s / batch. (data: 1.60e+00). ETA=14:27:26, max mem: 20.9 GB 
[11/24 21:42:26 visual_prompt]: Epoch 61 / 100: avg data time: 1.31e-01, avg batch time: 0.9619, average train loss: 31.4750
[11/24 21:43:21 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3104, average loss: 10.8324
[11/24 21:43:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.75	
[11/24 21:43:21 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[11/24 21:45:01 visual_prompt]: 	Training 100/553. train loss: 6.1677,	0.8440 s / batch. (data: 3.05e-04). ETA=5:01:57, max mem: 20.9 GB 
[11/24 21:46:37 visual_prompt]: 	Training 200/553. train loss: 87.6317,	0.8324 s / batch. (data: 2.84e-04). ETA=4:56:25, max mem: 20.9 GB 
[11/24 21:48:11 visual_prompt]: 	Training 300/553. train loss: 35.6326,	0.8484 s / batch. (data: 8.37e-03). ETA=5:00:43, max mem: 20.9 GB 
[11/24 21:49:47 visual_prompt]: 	Training 400/553. train loss: 1.8927,	0.8211 s / batch. (data: 2.80e-04). ETA=4:49:40, max mem: 20.9 GB 
[11/24 21:51:22 visual_prompt]: 	Training 500/553. train loss: 44.4709,	0.8240 s / batch. (data: 3.00e-04). ETA=4:49:19, max mem: 20.9 GB 
[11/24 21:52:14 visual_prompt]: Epoch 62 / 100: avg data time: 1.33e-01, avg batch time: 0.9632, average train loss: 34.4736
[11/24 21:53:08 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3099, average loss: 37.4871
[11/24 21:53:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.07	
[11/24 21:53:08 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[11/24 21:54:51 visual_prompt]: 	Training 100/553. train loss: 11.6051,	0.8335 s / batch. (data: 2.77e-04). ETA=4:50:31, max mem: 20.9 GB 
[11/24 21:56:30 visual_prompt]: 	Training 200/553. train loss: 23.2807,	0.8239 s / batch. (data: 3.80e-04). ETA=4:45:48, max mem: 20.9 GB 
[11/24 21:58:05 visual_prompt]: 	Training 300/553. train loss: 17.6828,	0.8557 s / batch. (data: 1.16e-02). ETA=4:55:24, max mem: 20.9 GB 
[11/24 21:59:37 visual_prompt]: 	Training 400/553. train loss: 5.7767,	0.8177 s / batch. (data: 2.82e-04). ETA=4:40:57, max mem: 20.9 GB 
[11/24 22:01:11 visual_prompt]: 	Training 500/553. train loss: 19.2352,	0.8400 s / batch. (data: 2.92e-04). ETA=4:47:12, max mem: 20.9 GB 
[11/24 22:02:00 visual_prompt]: Epoch 63 / 100: avg data time: 1.30e-01, avg batch time: 0.9616, average train loss: 30.2110
[11/24 22:02:55 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3106, average loss: 11.3012
[11/24 22:02:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.90	
[11/24 22:02:55 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[11/24 22:04:37 visual_prompt]: 	Training 100/553. train loss: 34.9543,	0.8403 s / batch. (data: 1.10e-02). ETA=4:45:09, max mem: 20.9 GB 
[11/24 22:06:14 visual_prompt]: 	Training 200/553. train loss: 13.7656,	0.8179 s / batch. (data: 2.98e-04). ETA=4:36:10, max mem: 20.9 GB 
[11/24 22:07:47 visual_prompt]: 	Training 300/553. train loss: 339.8531,	0.8360 s / batch. (data: 2.83e-04). ETA=4:40:54, max mem: 20.9 GB 
[11/24 22:09:23 visual_prompt]: 	Training 400/553. train loss: 15.6826,	0.8480 s / batch. (data: 1.20e-02). ETA=4:43:32, max mem: 20.9 GB 
[11/24 22:10:59 visual_prompt]: 	Training 500/553. train loss: 1.9167,	0.8679 s / batch. (data: 1.20e-02). ETA=4:48:44, max mem: 20.9 GB 
[11/24 22:11:48 visual_prompt]: Epoch 64 / 100: avg data time: 1.33e-01, avg batch time: 0.9648, average train loss: 33.4037
[11/24 22:12:43 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3108, average loss: 21.2460
[11/24 22:12:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.50	
[11/24 22:12:43 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[11/24 22:14:27 visual_prompt]: 	Training 100/553. train loss: 17.4793,	0.8426 s / batch. (data: 1.56e-02). ETA=4:38:10, max mem: 20.9 GB 
[11/24 22:16:04 visual_prompt]: 	Training 200/553. train loss: 8.7240,	1.1744 s / batch. (data: 3.44e-01). ETA=6:25:46, max mem: 20.9 GB 
[11/24 22:17:37 visual_prompt]: 	Training 300/553. train loss: 23.7983,	0.9200 s / batch. (data: 9.11e-02). ETA=5:00:39, max mem: 20.9 GB 
[11/24 22:19:13 visual_prompt]: 	Training 400/553. train loss: 3.0781,	0.8520 s / batch. (data: 7.88e-04). ETA=4:37:00, max mem: 20.9 GB 
[11/24 22:20:48 visual_prompt]: 	Training 500/553. train loss: 0.3122,	0.8440 s / batch. (data: 7.95e-03). ETA=4:33:00, max mem: 20.9 GB 
[11/24 22:21:36 visual_prompt]: Epoch 65 / 100: avg data time: 1.33e-01, avg batch time: 0.9638, average train loss: 28.8003
[11/24 22:22:31 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3104, average loss: 39.8656
[11/24 22:22:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.74	
[11/24 22:22:31 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[11/24 22:24:10 visual_prompt]: 	Training 100/553. train loss: 17.0419,	0.8208 s / batch. (data: 2.81e-04). ETA=4:23:24, max mem: 20.9 GB 
[11/24 22:25:46 visual_prompt]: 	Training 200/553. train loss: 10.1598,	1.4042 s / batch. (data: 5.70e-01). ETA=7:28:17, max mem: 20.9 GB 
[11/24 22:27:24 visual_prompt]: 	Training 300/553. train loss: 11.5037,	0.8440 s / batch. (data: 3.08e-04). ETA=4:28:02, max mem: 20.9 GB 
[11/24 22:28:58 visual_prompt]: 	Training 400/553. train loss: 10.3658,	0.8407 s / batch. (data: 1.09e-02). ETA=4:25:35, max mem: 20.9 GB 
[11/24 22:30:33 visual_prompt]: 	Training 500/553. train loss: 13.5958,	0.8359 s / batch. (data: 3.20e-04). ETA=4:22:41, max mem: 20.9 GB 
[11/24 22:31:25 visual_prompt]: Epoch 66 / 100: avg data time: 1.34e-01, avg batch time: 0.9659, average train loss: 27.3825
[11/24 22:32:20 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3098, average loss: 69.7469
[11/24 22:32:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.67	
[11/24 22:32:20 visual_prompt]: Stopping early.
[11/24 22:32:20 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 22:32:20 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 22:32:20 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/24 22:32:20 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 22:32:20 visual_prompt]: Training with config:
[11/24 22:32:20 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/24 22:32:20 visual_prompt]: Loading training data...
[11/24 22:32:20 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 22:32:20 visual_prompt]: Loading validation data...
[11/24 22:32:20 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 22:32:20 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 22:32:22 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 22:32:22 visual_prompt]: tuned percent:0.525
[11/24 22:32:22 visual_prompt]: Device used for model: 0
[11/24 22:32:22 visual_prompt]: Setting up Evaluator...
[11/24 22:32:22 visual_prompt]: Setting up Trainer...
[11/24 22:32:22 visual_prompt]: 	Setting up the optimizer...
[11/24 22:32:22 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 22:34:02 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8310 s / batch. (data: 2.82e-04). ETA=12:44:33, max mem: 20.9 GB 
[11/24 22:35:37 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8360 s / batch. (data: 2.78e-04). ETA=12:47:42, max mem: 20.9 GB 
[11/24 22:37:15 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8245 s / batch. (data: 5.45e-03). ETA=12:35:45, max mem: 20.9 GB 
[11/24 22:38:49 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8350 s / batch. (data: 2.93e-04). ETA=12:44:03, max mem: 20.9 GB 
[11/24 22:40:27 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8544 s / batch. (data: 1.22e-02). ETA=13:00:22, max mem: 20.9 GB 
[11/24 22:41:18 visual_prompt]: Epoch 1 / 100: avg data time: 1.34e-01, avg batch time: 0.9682, average train loss: 1.5403
[11/24 22:42:13 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3104, average loss: 1.5201
[11/24 22:42:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 22:42:13 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/24 22:43:52 visual_prompt]: 	Training 100/553. train loss: 9.3182,	0.8544 s / batch. (data: 3.77e-02). ETA=12:58:10, max mem: 20.9 GB 
[11/24 22:45:28 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8880 s / batch. (data: 5.55e-02). ETA=13:27:18, max mem: 20.9 GB 
[11/24 22:47:06 visual_prompt]: 	Training 300/553. train loss: 5.1931,	1.1080 s / batch. (data: 2.68e-01). ETA=16:45:26, max mem: 20.9 GB 
[11/24 22:48:41 visual_prompt]: 	Training 400/553. train loss: 0.6944,	0.8576 s / batch. (data: 2.06e-02). ETA=12:56:47, max mem: 20.9 GB 
[11/24 22:50:18 visual_prompt]: 	Training 500/553. train loss: 0.5637,	0.8360 s / batch. (data: 2.99e-04). ETA=12:35:51, max mem: 20.9 GB 
[11/24 22:51:08 visual_prompt]: Epoch 2 / 100: avg data time: 1.34e-01, avg batch time: 0.9674, average train loss: 9.8902
[11/24 22:52:03 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3098, average loss: 11.6990
[11/24 22:52:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.64	
[11/24 22:52:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/24 22:53:41 visual_prompt]: 	Training 100/553. train loss: 20.6916,	0.8400 s / batch. (data: 1.19e-02). ETA=12:37:18, max mem: 20.9 GB 
[11/24 22:55:18 visual_prompt]: 	Training 200/553. train loss: 7.1333,	1.2292 s / batch. (data: 4.01e-01). ETA=18:26:07, max mem: 20.9 GB 
[11/24 22:56:53 visual_prompt]: 	Training 300/553. train loss: 9.6720,	0.8884 s / batch. (data: 2.85e-02). ETA=13:18:00, max mem: 20.9 GB 
[11/24 22:58:30 visual_prompt]: 	Training 400/553. train loss: 15.5484,	0.8184 s / batch. (data: 3.17e-04). ETA=12:13:43, max mem: 20.9 GB 
[11/24 23:00:07 visual_prompt]: 	Training 500/553. train loss: 4.5820,	1.1860 s / batch. (data: 3.63e-01). ETA=17:41:18, max mem: 20.9 GB 
[11/24 23:00:56 visual_prompt]: Epoch 3 / 100: avg data time: 1.32e-01, avg batch time: 0.9640, average train loss: 12.6822
[11/24 23:01:50 visual_prompt]: Inference (val):avg data time: 2.10e-04, avg batch time: 0.3115, average loss: 9.8754
[11/24 23:01:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.28	
[11/24 23:01:50 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/24 23:03:32 visual_prompt]: 	Training 100/553. train loss: 14.6318,	0.8300 s / batch. (data: 5.47e-03). ETA=12:20:39, max mem: 20.9 GB 
[11/24 23:05:17 visual_prompt]: 	Training 200/553. train loss: 14.6047,	0.8358 s / batch. (data: 7.88e-03). ETA=12:24:28, max mem: 20.9 GB 
[11/24 23:07:03 visual_prompt]: 	Training 300/553. train loss: 12.7162,	1.5192 s / batch. (data: 6.87e-01). ETA=22:30:35, max mem: 20.9 GB 
[11/24 23:08:40 visual_prompt]: 	Training 400/553. train loss: 4.5224,	1.4685 s / batch. (data: 6.34e-01). ETA=21:43:04, max mem: 20.9 GB 
[11/24 23:10:27 visual_prompt]: 	Training 500/553. train loss: 0.0000,	3.7050 s / batch. (data: 2.88e+00). ETA=2 days, 6:41:29, max mem: 20.9 GB 
[11/24 23:11:22 visual_prompt]: Epoch 4 / 100: avg data time: 2.01e-01, avg batch time: 1.0343, average train loss: 17.3637
[11/24 23:12:17 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3096, average loss: 93.1347
[11/24 23:12:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.46	
[11/24 23:12:17 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/24 23:13:55 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8840 s / batch. (data: 5.40e-03). ETA=13:00:43, max mem: 20.9 GB 
[11/24 23:15:30 visual_prompt]: 	Training 200/553. train loss: 6.3354,	1.0275 s / batch. (data: 2.11e-01). ETA=15:05:43, max mem: 20.9 GB 
[11/24 23:17:07 visual_prompt]: 	Training 300/553. train loss: 36.2018,	0.8395 s / batch. (data: 2.76e-04). ETA=12:18:33, max mem: 20.9 GB 
[11/24 23:18:42 visual_prompt]: 	Training 400/553. train loss: 41.4358,	0.8615 s / batch. (data: 2.93e-04). ETA=12:36:30, max mem: 20.9 GB 
[11/24 23:20:17 visual_prompt]: 	Training 500/553. train loss: 32.2609,	0.8230 s / batch. (data: 2.94e-04). ETA=12:01:19, max mem: 20.9 GB 
[11/24 23:21:07 visual_prompt]: Epoch 5 / 100: avg data time: 1.28e-01, avg batch time: 0.9595, average train loss: 27.6257
[11/24 23:22:02 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3089, average loss: 89.6345
[11/24 23:22:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.05	
[11/24 23:22:02 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/24 23:23:42 visual_prompt]: 	Training 100/553. train loss: 9.4687,	0.8400 s / batch. (data: 8.05e-04). ETA=12:14:05, max mem: 20.9 GB 
[11/24 23:25:17 visual_prompt]: 	Training 200/553. train loss: 8.8481,	0.8600 s / batch. (data: 3.07e-04). ETA=12:30:07, max mem: 20.9 GB 
[11/24 23:26:57 visual_prompt]: 	Training 300/553. train loss: 5.5799,	0.8403 s / batch. (data: 3.45e-04). ETA=12:11:31, max mem: 20.9 GB 
[11/24 23:28:48 visual_prompt]: 	Training 400/553. train loss: 35.0367,	0.8359 s / batch. (data: 1.20e-02). ETA=12:06:20, max mem: 20.9 GB 
[11/24 23:30:28 visual_prompt]: 	Training 500/553. train loss: 27.2777,	0.8973 s / batch. (data: 5.63e-02). ETA=12:58:10, max mem: 20.9 GB 
[11/24 23:31:20 visual_prompt]: Epoch 6 / 100: avg data time: 1.80e-01, avg batch time: 1.0100, average train loss: 34.5054
[11/24 23:32:21 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3102, average loss: 11.0577
[11/24 23:32:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.58	
[11/24 23:32:21 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/24 23:34:01 visual_prompt]: 	Training 100/553. train loss: 0.0337,	0.8360 s / batch. (data: 3.51e-04). ETA=12:02:52, max mem: 20.9 GB 
[11/24 23:35:38 visual_prompt]: 	Training 200/553. train loss: 22.7581,	0.8384 s / batch. (data: 1.19e-02). ETA=12:03:32, max mem: 20.9 GB 
[11/24 23:37:19 visual_prompt]: 	Training 300/553. train loss: 3.9200,	1.7360 s / batch. (data: 9.02e-01). ETA=1 day, 0:55:20, max mem: 20.9 GB 
[11/24 23:38:57 visual_prompt]: 	Training 400/553. train loss: 17.9251,	1.8250 s / batch. (data: 9.88e-01). ETA=1 day, 2:08:54, max mem: 20.9 GB 
[11/24 23:40:32 visual_prompt]: 	Training 500/553. train loss: 135.0038,	0.8239 s / batch. (data: 5.47e-03). ETA=11:46:53, max mem: 20.9 GB 
[11/24 23:41:22 visual_prompt]: Epoch 7 / 100: avg data time: 1.49e-01, avg batch time: 0.9787, average train loss: 43.3462
[11/24 23:42:18 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3096, average loss: 30.7992
[11/24 23:42:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.91	
[11/24 23:42:18 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/24 23:43:58 visual_prompt]: 	Training 100/553. train loss: 92.5026,	0.8230 s / batch. (data: 3.16e-04). ETA=11:44:01, max mem: 20.9 GB 
[11/24 23:45:37 visual_prompt]: 	Training 200/553. train loss: 133.4921,	0.8440 s / batch. (data: 3.17e-04). ETA=12:00:37, max mem: 20.9 GB 
[11/24 23:47:14 visual_prompt]: 	Training 300/553. train loss: 234.0344,	0.8200 s / batch. (data: 3.11e-04). ETA=11:38:46, max mem: 20.9 GB 
[11/24 23:48:52 visual_prompt]: 	Training 400/553. train loss: 94.3837,	0.8878 s / batch. (data: 5.83e-02). ETA=12:35:03, max mem: 20.9 GB 
[11/24 23:50:30 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.4760 s / batch. (data: 6.49e-01). ETA=20:52:52, max mem: 20.9 GB 
[11/24 23:51:21 visual_prompt]: Epoch 8 / 100: avg data time: 1.52e-01, avg batch time: 0.9819, average train loss: 73.7924
[11/24 23:52:17 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3101, average loss: 203.1180
[11/24 23:52:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.60	
[11/24 23:52:17 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/24 23:53:58 visual_prompt]: 	Training 100/553. train loss: 89.9425,	0.8280 s / batch. (data: 3.10e-04). ETA=11:40:42, max mem: 20.9 GB 
[11/24 23:55:35 visual_prompt]: 	Training 200/553. train loss: 24.2411,	0.8180 s / batch. (data: 2.91e-04). ETA=11:30:54, max mem: 20.9 GB 
[11/24 23:57:16 visual_prompt]: 	Training 300/553. train loss: 93.5788,	1.9918 s / batch. (data: 1.19e+00). ETA=1 day, 3:58:54, max mem: 20.9 GB 
[11/24 23:59:03 visual_prompt]: 	Training 400/553. train loss: 12.0334,	0.8588 s / batch. (data: 7.96e-03). ETA=12:02:30, max mem: 20.9 GB 
[11/25 00:00:47 visual_prompt]: 	Training 500/553. train loss: 3.0766,	0.9080 s / batch. (data: 5.31e-02). ETA=12:42:19, max mem: 20.9 GB 
[11/25 00:01:38 visual_prompt]: Epoch 9 / 100: avg data time: 1.84e-01, avg batch time: 1.0144, average train loss: 48.2814
[11/25 00:02:34 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3098, average loss: 42.4104
[11/25 00:02:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.11	
[11/25 00:02:34 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/25 00:04:18 visual_prompt]: 	Training 100/553. train loss: 184.7925,	0.8211 s / batch. (data: 7.34e-04). ETA=11:27:18, max mem: 20.9 GB 
[11/25 00:05:54 visual_prompt]: 	Training 200/553. train loss: 3.7531,	0.8178 s / batch. (data: 3.35e-04). ETA=11:23:12, max mem: 20.9 GB 
[11/25 00:07:31 visual_prompt]: 	Training 300/553. train loss: 19.7834,	2.7983 s / batch. (data: 1.97e+00). ETA=1 day, 14:53:01, max mem: 20.9 GB 
[11/25 00:09:07 visual_prompt]: 	Training 400/553. train loss: 71.5181,	0.8381 s / batch. (data: 8.59e-03). ETA=11:37:17, max mem: 20.9 GB 
[11/25 00:10:48 visual_prompt]: 	Training 500/553. train loss: 93.8140,	0.9413 s / batch. (data: 1.34e-01). ETA=13:01:39, max mem: 20.9 GB 
[11/25 00:11:44 visual_prompt]: Epoch 10 / 100: avg data time: 1.68e-01, avg batch time: 0.9954, average train loss: 82.9872
[11/25 00:12:45 visual_prompt]: Inference (val):avg data time: 9.32e-05, avg batch time: 0.3090, average loss: 48.1733
[11/25 00:12:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.14	
[11/25 00:12:45 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/25 00:14:34 visual_prompt]: 	Training 100/553. train loss: 151.3405,	0.8431 s / batch. (data: 5.53e-03). ETA=11:37:58, max mem: 20.9 GB 
[11/25 00:16:22 visual_prompt]: 	Training 200/553. train loss: 99.0040,	0.8361 s / batch. (data: 3.16e-04). ETA=11:30:47, max mem: 20.9 GB 
[11/25 00:18:07 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.2653 s / batch. (data: 1.44e+00). ETA=1 day, 7:07:46, max mem: 20.9 GB 
[11/25 00:19:42 visual_prompt]: 	Training 400/553. train loss: 181.2127,	0.8680 s / batch. (data: 6.81e-04). ETA=11:54:13, max mem: 20.9 GB 
[11/25 00:21:18 visual_prompt]: 	Training 500/553. train loss: 98.6965,	0.8289 s / batch. (data: 8.59e-03). ETA=11:20:38, max mem: 20.9 GB 
[11/25 00:22:09 visual_prompt]: Epoch 11 / 100: avg data time: 1.92e-01, avg batch time: 1.0189, average train loss: 72.5064
[11/25 00:23:05 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3100, average loss: 107.5733
[11/25 00:23:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.46	
[11/25 00:23:05 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/25 00:24:47 visual_prompt]: 	Training 100/553. train loss: 54.0880,	0.8091 s / batch. (data: 2.94e-04). ETA=11:02:19, max mem: 20.9 GB 
[11/25 00:26:25 visual_prompt]: 	Training 200/553. train loss: 48.0289,	0.8209 s / batch. (data: 3.23e-04). ETA=11:10:37, max mem: 20.9 GB 
[11/25 00:28:01 visual_prompt]: 	Training 300/553. train loss: 37.7816,	0.8400 s / batch. (data: 2.94e-04). ETA=11:24:51, max mem: 20.9 GB 
[11/25 00:29:39 visual_prompt]: 	Training 400/553. train loss: 92.9019,	0.8400 s / batch. (data: 3.24e-04). ETA=11:23:24, max mem: 20.9 GB 
[11/25 00:31:17 visual_prompt]: 	Training 500/553. train loss: 41.5009,	0.8400 s / batch. (data: 2.05e-02). ETA=11:22:01, max mem: 20.9 GB 
[11/25 00:32:06 visual_prompt]: Epoch 12 / 100: avg data time: 1.52e-01, avg batch time: 0.9798, average train loss: 69.8166
[11/25 00:33:02 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3094, average loss: 98.0493
[11/25 00:33:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.43	
[11/25 00:33:02 visual_prompt]: Best epoch 12: best metric: -98.049
[11/25 00:33:02 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/25 00:34:45 visual_prompt]: 	Training 100/553. train loss: 73.2958,	0.8271 s / batch. (data: 5.42e-03). ETA=11:09:28, max mem: 20.9 GB 
[11/25 00:36:28 visual_prompt]: 	Training 200/553. train loss: 69.2323,	0.8216 s / batch. (data: 3.21e-04). ETA=11:03:38, max mem: 20.9 GB 
[11/25 00:38:14 visual_prompt]: 	Training 300/553. train loss: 30.0433,	2.0219 s / batch. (data: 1.18e+00). ETA=1 day, 3:09:45, max mem: 20.9 GB 
[11/25 00:39:58 visual_prompt]: 	Training 400/553. train loss: 241.2708,	0.8412 s / batch. (data: 1.06e-02). ETA=11:16:39, max mem: 20.9 GB 
[11/25 00:41:40 visual_prompt]: 	Training 500/553. train loss: 34.8105,	0.8385 s / batch. (data: 2.92e-04). ETA=11:13:05, max mem: 20.9 GB 
[11/25 00:42:31 visual_prompt]: Epoch 13 / 100: avg data time: 2.01e-01, avg batch time: 1.0296, average train loss: 97.3977
[11/25 00:43:28 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3110, average loss: 54.5825
[11/25 00:43:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.08	
[11/25 00:43:28 visual_prompt]: Best epoch 13: best metric: -54.582
[11/25 00:43:28 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/25 00:45:10 visual_prompt]: 	Training 100/553. train loss: 58.1163,	0.8293 s / batch. (data: 2.80e-04). ETA=11:03:33, max mem: 20.9 GB 
[11/25 00:46:47 visual_prompt]: 	Training 200/553. train loss: 0.0389,	0.8480 s / batch. (data: 1.20e-02). ETA=11:17:07, max mem: 20.9 GB 
[11/25 00:48:24 visual_prompt]: 	Training 300/553. train loss: 34.0453,	0.8400 s / batch. (data: 3.24e-04). ETA=11:09:21, max mem: 20.9 GB 
[11/25 00:50:00 visual_prompt]: 	Training 400/553. train loss: 20.2472,	0.8320 s / batch. (data: 2.98e-04). ETA=11:01:34, max mem: 20.9 GB 
[11/25 00:51:37 visual_prompt]: 	Training 500/553. train loss: 164.4967,	0.8110 s / batch. (data: 3.05e-04). ETA=10:43:31, max mem: 20.9 GB 
[11/25 00:52:27 visual_prompt]: Epoch 14 / 100: avg data time: 1.46e-01, avg batch time: 0.9753, average train loss: 61.9254
[11/25 00:53:23 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3097, average loss: 27.4170
[11/25 00:53:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.04	
[11/25 00:53:23 visual_prompt]: Best epoch 14: best metric: -27.417
[11/25 00:53:23 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/25 00:55:04 visual_prompt]: 	Training 100/553. train loss: 110.0935,	0.8275 s / batch. (data: 3.51e-04). ETA=10:54:29, max mem: 20.9 GB 
[11/25 00:56:40 visual_prompt]: 	Training 200/553. train loss: 372.1039,	0.8188 s / batch. (data: 2.96e-04). ETA=10:46:17, max mem: 20.9 GB 
[11/25 00:58:20 visual_prompt]: 	Training 300/553. train loss: 5.5879,	0.8309 s / batch. (data: 7.76e-04). ETA=10:54:27, max mem: 20.9 GB 
[11/25 00:59:55 visual_prompt]: 	Training 400/553. train loss: 0.3316,	1.0480 s / batch. (data: 2.08e-01). ETA=13:43:41, max mem: 20.9 GB 
[11/25 01:01:34 visual_prompt]: 	Training 500/553. train loss: 112.7657,	0.8366 s / batch. (data: 8.50e-03). ETA=10:56:07, max mem: 20.9 GB 
[11/25 01:02:25 visual_prompt]: Epoch 15 / 100: avg data time: 1.50e-01, avg batch time: 0.9797, average train loss: 87.3216
[11/25 01:03:20 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3092, average loss: 210.5192
[11/25 01:03:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.00	
[11/25 01:03:20 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/25 01:05:00 visual_prompt]: 	Training 100/553. train loss: 16.6388,	0.8286 s / batch. (data: 2.89e-04). ETA=10:47:45, max mem: 20.9 GB 
[11/25 01:06:38 visual_prompt]: 	Training 200/553. train loss: 101.8530,	0.8399 s / batch. (data: 1.55e-02). ETA=10:55:13, max mem: 20.9 GB 
[11/25 01:08:15 visual_prompt]: 	Training 300/553. train loss: 149.5668,	0.8168 s / batch. (data: 2.79e-04). ETA=10:35:50, max mem: 20.9 GB 
[11/25 01:09:53 visual_prompt]: 	Training 400/553. train loss: 25.5012,	0.8551 s / batch. (data: 1.11e-02). ETA=11:04:09, max mem: 20.9 GB 
[11/25 01:11:29 visual_prompt]: 	Training 500/553. train loss: 156.1897,	1.0906 s / batch. (data: 2.60e-01). ETA=14:05:19, max mem: 20.9 GB 
[11/25 01:12:21 visual_prompt]: Epoch 16 / 100: avg data time: 1.50e-01, avg batch time: 0.9771, average train loss: 82.4612
[11/25 01:13:16 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3090, average loss: 25.8207
[11/25 01:13:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.73	
[11/25 01:13:16 visual_prompt]: Best epoch 16: best metric: -25.821
[11/25 01:13:16 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/25 01:14:57 visual_prompt]: 	Training 100/553. train loss: 79.3474,	0.8360 s / batch. (data: 2.90e-04). ETA=10:45:50, max mem: 20.9 GB 
[11/25 01:16:36 visual_prompt]: 	Training 200/553. train loss: 313.7220,	0.8200 s / batch. (data: 2.96e-04). ETA=10:32:05, max mem: 20.9 GB 
[11/25 01:18:13 visual_prompt]: 	Training 300/553. train loss: 178.4385,	0.8456 s / batch. (data: 1.55e-02). ETA=10:50:25, max mem: 20.9 GB 
[11/25 01:19:49 visual_prompt]: 	Training 400/553. train loss: 55.0876,	1.0850 s / batch. (data: 2.69e-01). ETA=13:52:47, max mem: 20.9 GB 
[11/25 01:21:26 visual_prompt]: 	Training 500/553. train loss: 61.1381,	1.4730 s / batch. (data: 6.63e-01). ETA=18:48:07, max mem: 20.9 GB 
[11/25 01:22:18 visual_prompt]: Epoch 17 / 100: avg data time: 1.51e-01, avg batch time: 0.9797, average train loss: 96.4472
[11/25 01:23:14 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3114, average loss: 52.4195
[11/25 01:23:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.80	
[11/25 01:23:14 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/25 01:24:55 visual_prompt]: 	Training 100/553. train loss: 179.4604,	0.8357 s / batch. (data: 5.40e-03). ETA=10:37:53, max mem: 20.9 GB 
[11/25 01:26:35 visual_prompt]: 	Training 200/553. train loss: 6.3386,	0.8612 s / batch. (data: 2.52e-02). ETA=10:55:54, max mem: 20.9 GB 
[11/25 01:28:12 visual_prompt]: 	Training 300/553. train loss: 5.8222,	0.8599 s / batch. (data: 1.05e-02). ETA=10:53:32, max mem: 20.9 GB 
[11/25 01:29:49 visual_prompt]: 	Training 400/553. train loss: 27.2760,	0.8252 s / batch. (data: 9.24e-03). ETA=10:25:44, max mem: 20.9 GB 
[11/25 01:31:26 visual_prompt]: 	Training 500/553. train loss: 103.9776,	0.8320 s / batch. (data: 3.14e-04). ETA=10:29:32, max mem: 20.9 GB 
[11/25 01:32:16 visual_prompt]: Epoch 18 / 100: avg data time: 1.51e-01, avg batch time: 0.9790, average train loss: 108.9397
[11/25 01:33:11 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3107, average loss: 174.6840
[11/25 01:33:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.30	
[11/25 01:33:11 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/25 01:34:52 visual_prompt]: 	Training 100/553. train loss: 62.0611,	1.1855 s / batch. (data: 3.82e-01). ETA=14:54:00, max mem: 20.9 GB 
[11/25 01:36:30 visual_prompt]: 	Training 200/553. train loss: 12.6486,	0.8638 s / batch. (data: 2.94e-04). ETA=10:49:56, max mem: 20.9 GB 
[11/25 01:38:07 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8025 s / batch. (data: 2.87e-04). ETA=10:02:27, max mem: 20.9 GB 
[11/25 01:39:46 visual_prompt]: 	Training 400/553. train loss: 51.9433,	0.8201 s / batch. (data: 2.71e-04). ETA=10:14:18, max mem: 20.9 GB 
[11/25 01:41:19 visual_prompt]: 	Training 500/553. train loss: 25.7980,	0.8160 s / batch. (data: 3.19e-04). ETA=10:09:53, max mem: 20.9 GB 
[11/25 01:42:10 visual_prompt]: Epoch 19 / 100: avg data time: 1.45e-01, avg batch time: 0.9744, average train loss: 68.3429
[11/25 01:43:05 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3119, average loss: 5.8235
[11/25 01:43:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.74	
[11/25 01:43:05 visual_prompt]: Best epoch 19: best metric: -5.824
[11/25 01:43:05 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/25 01:44:45 visual_prompt]: 	Training 100/553. train loss: 9.4218,	0.8413 s / batch. (data: 3.76e-03). ETA=10:26:39, max mem: 20.9 GB 
[11/25 01:46:24 visual_prompt]: 	Training 200/553. train loss: 10.9273,	0.8600 s / batch. (data: 2.79e-02). ETA=10:39:08, max mem: 20.9 GB 
[11/25 01:48:02 visual_prompt]: 	Training 300/553. train loss: 76.0858,	0.8728 s / batch. (data: 1.10e-02). ETA=10:47:11, max mem: 20.9 GB 
[11/25 01:49:39 visual_prompt]: 	Training 400/553. train loss: 187.2953,	0.8280 s / batch. (data: 1.20e-02). ETA=10:12:37, max mem: 20.9 GB 
[11/25 01:51:15 visual_prompt]: 	Training 500/553. train loss: 45.7235,	0.8369 s / batch. (data: 4.81e-03). ETA=10:17:46, max mem: 20.9 GB 
[11/25 01:52:07 visual_prompt]: Epoch 20 / 100: avg data time: 1.51e-01, avg batch time: 0.9794, average train loss: 91.9706
[11/25 01:53:03 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3102, average loss: 98.4013
[11/25 01:53:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.25	
[11/25 01:53:03 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/25 01:54:47 visual_prompt]: 	Training 100/553. train loss: 251.7324,	0.8718 s / batch. (data: 5.01e-02). ETA=10:41:20, max mem: 20.9 GB 
[11/25 01:56:23 visual_prompt]: 	Training 200/553. train loss: 142.9680,	0.8480 s / batch. (data: 2.86e-04). ETA=10:22:26, max mem: 20.9 GB 
[11/25 01:58:00 visual_prompt]: 	Training 300/553. train loss: 511.1374,	1.1480 s / batch. (data: 3.23e-01). ETA=14:00:43, max mem: 20.9 GB 
[11/25 01:59:36 visual_prompt]: 	Training 400/553. train loss: 197.3434,	0.8355 s / batch. (data: 5.43e-03). ETA=10:10:29, max mem: 20.9 GB 
[11/25 02:01:15 visual_prompt]: 	Training 500/553. train loss: 40.3862,	0.8520 s / batch. (data: 3.00e-04). ETA=10:21:06, max mem: 20.9 GB 
[11/25 02:02:05 visual_prompt]: Epoch 21 / 100: avg data time: 1.52e-01, avg batch time: 0.9807, average train loss: 107.1739
[11/25 02:03:01 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3097, average loss: 11.7291
[11/25 02:03:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.31	
[11/25 02:03:01 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/25 02:04:41 visual_prompt]: 	Training 100/553. train loss: 129.5088,	0.8357 s / batch. (data: 2.86e-04). ETA=10:07:07, max mem: 20.9 GB 
[11/25 02:06:18 visual_prompt]: 	Training 200/553. train loss: 8.2347,	0.8401 s / batch. (data: 3.02e-04). ETA=10:08:54, max mem: 20.9 GB 
[11/25 02:07:54 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8240 s / batch. (data: 7.94e-03). ETA=9:55:49, max mem: 20.9 GB 
[11/25 02:09:32 visual_prompt]: 	Training 400/553. train loss: 119.1904,	0.8760 s / batch. (data: 1.20e-02). ETA=10:31:59, max mem: 20.9 GB 
[11/25 02:11:10 visual_prompt]: 	Training 500/553. train loss: 41.2609,	0.8228 s / batch. (data: 2.99e-04). ETA=9:52:15, max mem: 20.9 GB 
[11/25 02:12:02 visual_prompt]: Epoch 22 / 100: avg data time: 1.49e-01, avg batch time: 0.9780, average train loss: 77.9296
[11/25 02:12:57 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3097, average loss: 18.1174
[11/25 02:12:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.81	
[11/25 02:12:57 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/25 02:14:40 visual_prompt]: 	Training 100/553. train loss: 18.7215,	0.8172 s / batch. (data: 3.32e-04). ETA=9:46:08, max mem: 20.9 GB 
[11/25 02:16:18 visual_prompt]: 	Training 200/553. train loss: 2.1599,	0.8417 s / batch. (data: 5.42e-03). ETA=10:02:16, max mem: 20.9 GB 
[11/25 02:17:57 visual_prompt]: 	Training 300/553. train loss: 92.5511,	0.8405 s / batch. (data: 5.42e-03). ETA=9:59:59, max mem: 20.9 GB 
[11/25 02:19:33 visual_prompt]: 	Training 400/553. train loss: 17.3159,	0.8480 s / batch. (data: 7.90e-04). ETA=10:04:00, max mem: 20.9 GB 
[11/25 02:21:08 visual_prompt]: 	Training 500/553. train loss: 85.7274,	0.8555 s / batch. (data: 1.05e-02). ETA=10:07:55, max mem: 20.9 GB 
[11/25 02:21:59 visual_prompt]: Epoch 23 / 100: avg data time: 1.51e-01, avg batch time: 0.9798, average train loss: 68.7983
[11/25 02:22:56 visual_prompt]: Inference (val):avg data time: 6.06e-05, avg batch time: 0.3223, average loss: 93.9192
[11/25 02:22:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.86	
[11/25 02:22:56 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/25 02:24:34 visual_prompt]: 	Training 100/553. train loss: 121.3363,	0.8317 s / batch. (data: 5.43e-03). ETA=9:48:51, max mem: 20.9 GB 
[11/25 02:26:10 visual_prompt]: 	Training 200/553. train loss: 30.4765,	0.8512 s / batch. (data: 1.11e-02). ETA=10:01:14, max mem: 20.9 GB 
[11/25 02:27:49 visual_prompt]: 	Training 300/553. train loss: 30.0802,	1.0782 s / batch. (data: 2.37e-01). ETA=12:39:47, max mem: 20.9 GB 
[11/25 02:29:27 visual_prompt]: 	Training 400/553. train loss: 24.1946,	0.8464 s / batch. (data: 1.04e-02). ETA=9:55:03, max mem: 20.9 GB 
[11/25 02:31:06 visual_prompt]: 	Training 500/553. train loss: 155.4979,	0.8608 s / batch. (data: 1.07e-02). ETA=10:03:44, max mem: 20.9 GB 
[11/25 02:31:57 visual_prompt]: Epoch 24 / 100: avg data time: 1.50e-01, avg batch time: 0.9786, average train loss: 77.4350
[11/25 02:32:53 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3083, average loss: 59.1853
[11/25 02:32:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.16	
[11/25 02:32:53 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/25 02:34:36 visual_prompt]: 	Training 100/553. train loss: 197.0322,	0.8277 s / batch. (data: 3.00e-04). ETA=9:38:25, max mem: 20.9 GB 
[11/25 02:36:11 visual_prompt]: 	Training 200/553. train loss: 35.0548,	0.8100 s / batch. (data: 3.25e-04). ETA=9:24:39, max mem: 20.9 GB 
[11/25 02:37:47 visual_prompt]: 	Training 300/553. train loss: 140.3271,	1.0640 s / batch. (data: 2.49e-01). ETA=12:19:58, max mem: 20.9 GB 
[11/25 02:39:25 visual_prompt]: 	Training 400/553. train loss: 81.6939,	1.2800 s / batch. (data: 4.32e-01). ETA=14:48:04, max mem: 20.9 GB 
[11/25 02:41:03 visual_prompt]: 	Training 500/553. train loss: 61.1020,	1.3745 s / batch. (data: 5.65e-01). ETA=15:51:18, max mem: 20.9 GB 
[11/25 02:41:54 visual_prompt]: Epoch 25 / 100: avg data time: 1.50e-01, avg batch time: 0.9784, average train loss: 73.0670
[11/25 02:42:49 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3115, average loss: 142.0155
[11/25 02:42:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.36	
[11/25 02:42:49 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/25 02:44:31 visual_prompt]: 	Training 100/553. train loss: 29.4657,	0.8319 s / batch. (data: 1.19e-02). ETA=9:33:38, max mem: 20.9 GB 
[11/25 02:46:10 visual_prompt]: 	Training 200/553. train loss: 385.2049,	1.6146 s / batch. (data: 7.88e-01). ETA=18:30:40, max mem: 20.9 GB 
[11/25 02:47:49 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8280 s / batch. (data: 1.05e-02). ETA=9:28:12, max mem: 20.9 GB 
[11/25 02:49:25 visual_prompt]: 	Training 400/553. train loss: 54.7317,	0.8160 s / batch. (data: 3.02e-04). ETA=9:18:36, max mem: 20.9 GB 
[11/25 02:51:01 visual_prompt]: 	Training 500/553. train loss: 57.9050,	0.8316 s / batch. (data: 2.93e-04). ETA=9:27:55, max mem: 20.9 GB 
[11/25 02:51:52 visual_prompt]: Epoch 26 / 100: avg data time: 1.51e-01, avg batch time: 0.9808, average train loss: 73.4555
[11/25 02:52:47 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3107, average loss: 80.7753
[11/25 02:52:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.97	
[11/25 02:52:47 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[11/25 02:54:29 visual_prompt]: 	Training 100/553. train loss: 146.1667,	0.8540 s / batch. (data: 3.04e-04). ETA=9:41:02, max mem: 20.9 GB 
[11/25 02:56:06 visual_prompt]: 	Training 200/553. train loss: 171.7426,	0.9796 s / batch. (data: 1.49e-01). ETA=11:04:51, max mem: 20.9 GB 
[11/25 02:57:44 visual_prompt]: 	Training 300/553. train loss: 71.7340,	0.8067 s / batch. (data: 3.20e-04). ETA=9:06:11, max mem: 20.9 GB 
[11/25 02:59:22 visual_prompt]: 	Training 400/553. train loss: 35.2200,	0.8520 s / batch. (data: 7.14e-04). ETA=9:35:24, max mem: 20.9 GB 
[11/25 03:01:00 visual_prompt]: 	Training 500/553. train loss: 3.3718,	0.8440 s / batch. (data: 7.91e-04). ETA=9:28:34, max mem: 20.9 GB 
[11/25 03:01:49 visual_prompt]: Epoch 27 / 100: avg data time: 1.51e-01, avg batch time: 0.9803, average train loss: 64.3172
[11/25 03:02:45 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3100, average loss: 175.3402
[11/25 03:02:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.95	
[11/25 03:02:45 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[11/25 03:04:25 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8110 s / batch. (data: 3.24e-04). ETA=9:04:19, max mem: 20.9 GB 
[11/25 03:06:03 visual_prompt]: 	Training 200/553. train loss: 155.6715,	0.8342 s / batch. (data: 3.14e-04). ETA=9:18:28, max mem: 20.9 GB 
[11/25 03:07:41 visual_prompt]: 	Training 300/553. train loss: 14.1574,	1.2360 s / batch. (data: 4.08e-01). ETA=13:45:25, max mem: 20.9 GB 
[11/25 03:09:17 visual_prompt]: 	Training 400/553. train loss: 171.3097,	0.8480 s / batch. (data: 7.98e-03). ETA=9:24:53, max mem: 20.9 GB 
[11/25 03:10:54 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8244 s / batch. (data: 7.95e-03). ETA=9:07:48, max mem: 20.9 GB 
[11/25 03:11:45 visual_prompt]: Epoch 28 / 100: avg data time: 1.48e-01, avg batch time: 0.9769, average train loss: 73.8999
[11/25 03:12:41 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3087, average loss: 56.4914
[11/25 03:12:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.33	
[11/25 03:12:41 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[11/25 03:14:28 visual_prompt]: 	Training 100/553. train loss: 34.7881,	0.8520 s / batch. (data: 7.98e-03). ETA=9:23:58, max mem: 20.9 GB 
[11/25 03:16:04 visual_prompt]: 	Training 200/553. train loss: 0.0895,	1.6840 s / batch. (data: 8.40e-01). ETA=18:31:53, max mem: 20.9 GB 
[11/25 03:17:40 visual_prompt]: 	Training 300/553. train loss: 121.7846,	0.8360 s / batch. (data: 2.90e-04). ETA=9:10:35, max mem: 20.9 GB 
[11/25 03:19:15 visual_prompt]: 	Training 400/553. train loss: 119.9498,	1.1462 s / batch. (data: 3.41e-01). ETA=12:32:58, max mem: 20.9 GB 
[11/25 03:20:52 visual_prompt]: 	Training 500/553. train loss: 61.3509,	0.8280 s / batch. (data: 4.15e-04). ETA=9:02:33, max mem: 20.9 GB 
[11/25 03:21:43 visual_prompt]: Epoch 29 / 100: avg data time: 1.51e-01, avg batch time: 0.9796, average train loss: 84.4679
[11/25 03:22:39 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3095, average loss: 26.3829
[11/25 03:22:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.01	
[11/25 03:22:39 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[11/25 03:24:19 visual_prompt]: 	Training 100/553. train loss: 59.7197,	0.8253 s / batch. (data: 3.11e-04). ETA=8:58:42, max mem: 20.9 GB 
[11/25 03:25:57 visual_prompt]: 	Training 200/553. train loss: 133.5192,	0.8148 s / batch. (data: 3.03e-04). ETA=8:50:28, max mem: 20.9 GB 
[11/25 03:27:32 visual_prompt]: 	Training 300/553. train loss: 78.8985,	1.1259 s / batch. (data: 2.97e-01). ETA=12:11:09, max mem: 20.9 GB 
[11/25 03:29:11 visual_prompt]: 	Training 400/553. train loss: 3.7242,	0.9002 s / batch. (data: 8.14e-02). ETA=9:43:06, max mem: 20.9 GB 
[11/25 03:30:48 visual_prompt]: 	Training 500/553. train loss: 44.4254,	1.3791 s / batch. (data: 5.48e-01). ETA=14:50:56, max mem: 20.9 GB 
[11/25 03:31:41 visual_prompt]: Epoch 30 / 100: avg data time: 1.51e-01, avg batch time: 0.9800, average train loss: 76.9443
[11/25 03:32:37 visual_prompt]: Inference (val):avg data time: 1.82e-04, avg batch time: 0.3144, average loss: 15.7663
[11/25 03:32:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.12	
[11/25 03:32:37 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[11/25 03:34:19 visual_prompt]: 	Training 100/553. train loss: 12.9275,	0.8377 s / batch. (data: 3.24e-04). ETA=8:59:05, max mem: 20.9 GB 
[11/25 03:35:58 visual_prompt]: 	Training 200/553. train loss: 239.9660,	0.8320 s / batch. (data: 5.46e-03). ETA=8:53:59, max mem: 20.9 GB 
[11/25 03:37:33 visual_prompt]: 	Training 300/553. train loss: 74.7414,	0.8400 s / batch. (data: 3.08e-04). ETA=8:57:44, max mem: 20.9 GB 
[11/25 03:39:10 visual_prompt]: 	Training 400/553. train loss: 25.2471,	1.1454 s / batch. (data: 3.20e-01). ETA=12:11:21, max mem: 20.9 GB 
[11/25 03:40:48 visual_prompt]: 	Training 500/553. train loss: 30.2537,	0.8677 s / batch. (data: 5.42e-03). ETA=9:12:33, max mem: 20.9 GB 
[11/25 03:41:38 visual_prompt]: Epoch 31 / 100: avg data time: 1.49e-01, avg batch time: 0.9781, average train loss: 63.6474
[11/25 03:42:33 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3098, average loss: 18.8359
[11/25 03:42:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.38	
[11/25 03:42:33 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[11/25 03:44:16 visual_prompt]: 	Training 100/553. train loss: 15.2904,	0.8632 s / batch. (data: 2.88e-04). ETA=9:07:29, max mem: 20.9 GB 
[11/25 03:45:52 visual_prompt]: 	Training 200/553. train loss: 58.0532,	0.8350 s / batch. (data: 3.05e-04). ETA=8:48:13, max mem: 20.9 GB 
[11/25 03:47:33 visual_prompt]: 	Training 300/553. train loss: 40.2767,	0.8141 s / batch. (data: 2.93e-04). ETA=8:33:38, max mem: 20.9 GB 
[11/25 03:49:11 visual_prompt]: 	Training 400/553. train loss: 32.6062,	0.8473 s / batch. (data: 1.84e-02). ETA=8:53:12, max mem: 20.9 GB 
[11/25 03:50:46 visual_prompt]: 	Training 500/553. train loss: 2.6893,	0.8310 s / batch. (data: 2.79e-04). ETA=8:41:33, max mem: 20.9 GB 
[11/25 03:51:36 visual_prompt]: Epoch 32 / 100: avg data time: 1.52e-01, avg batch time: 0.9807, average train loss: 68.8120
[11/25 03:52:31 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3100, average loss: 18.6799
[11/25 03:52:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.72	
[11/25 03:52:31 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[11/25 03:54:11 visual_prompt]: 	Training 100/553. train loss: 0.1960,	0.8166 s / batch. (data: 2.90e-04). ETA=8:30:24, max mem: 20.9 GB 
[11/25 03:55:50 visual_prompt]: 	Training 200/553. train loss: 67.9366,	1.1207 s / batch. (data: 3.07e-01). ETA=11:38:37, max mem: 20.9 GB 
[11/25 03:57:26 visual_prompt]: 	Training 300/553. train loss: 51.7071,	0.8387 s / batch. (data: 3.30e-04). ETA=8:41:25, max mem: 20.9 GB 
[11/25 03:59:05 visual_prompt]: 	Training 400/553. train loss: 24.1078,	0.8341 s / batch. (data: 3.00e-04). ETA=8:37:10, max mem: 20.9 GB 
[11/25 04:00:42 visual_prompt]: 	Training 500/553. train loss: 8.8907,	0.8441 s / batch. (data: 5.43e-03). ETA=8:41:58, max mem: 20.9 GB 
[11/25 04:01:32 visual_prompt]: Epoch 33 / 100: avg data time: 1.48e-01, avg batch time: 0.9770, average train loss: 60.6009
[11/25 04:02:27 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3113, average loss: 30.7730
[11/25 04:02:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.16	
[11/25 04:02:27 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[11/25 04:04:09 visual_prompt]: 	Training 100/553. train loss: 75.7294,	0.8440 s / batch. (data: 3.17e-04). ETA=8:39:45, max mem: 20.9 GB 
[11/25 04:05:45 visual_prompt]: 	Training 200/553. train loss: 66.3294,	0.8324 s / batch. (data: 3.25e-04). ETA=8:31:15, max mem: 20.9 GB 
[11/25 04:07:21 visual_prompt]: 	Training 300/553. train loss: 104.9058,	0.8160 s / batch. (data: 3.26e-04). ETA=8:19:47, max mem: 20.9 GB 
[11/25 04:09:00 visual_prompt]: 	Training 400/553. train loss: 131.9211,	0.8353 s / batch. (data: 2.90e-04). ETA=8:30:13, max mem: 20.9 GB 
[11/25 04:10:38 visual_prompt]: 	Training 500/553. train loss: 32.8797,	1.3960 s / batch. (data: 5.41e-01). ETA=14:10:23, max mem: 20.9 GB 
[11/25 04:11:28 visual_prompt]: Epoch 34 / 100: avg data time: 1.49e-01, avg batch time: 0.9785, average train loss: 70.1690
[11/25 04:12:24 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3109, average loss: 99.1878
[11/25 04:12:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.86	
[11/25 04:12:24 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[11/25 04:14:07 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8080 s / batch. (data: 3.22e-04). ETA=8:10:10, max mem: 20.9 GB 
[11/25 04:15:46 visual_prompt]: 	Training 200/553. train loss: 13.2344,	0.8527 s / batch. (data: 1.05e-02). ETA=8:35:49, max mem: 20.9 GB 
[11/25 04:17:21 visual_prompt]: 	Training 300/553. train loss: 16.2176,	0.8677 s / batch. (data: 2.78e-02). ETA=8:43:27, max mem: 20.9 GB 
[11/25 04:18:58 visual_prompt]: 	Training 400/553. train loss: 85.7057,	0.8109 s / batch. (data: 5.43e-03). ETA=8:07:51, max mem: 20.9 GB 
[11/25 04:20:34 visual_prompt]: 	Training 500/553. train loss: 59.5886,	0.8318 s / batch. (data: 2.88e-04). ETA=8:19:01, max mem: 20.9 GB 
[11/25 04:21:26 visual_prompt]: Epoch 35 / 100: avg data time: 1.52e-01, avg batch time: 0.9793, average train loss: 69.4832
[11/25 04:22:21 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3106, average loss: 222.5221
[11/25 04:22:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.66	
[11/25 04:22:21 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[11/25 04:24:02 visual_prompt]: 	Training 100/553. train loss: 14.2917,	0.8440 s / batch. (data: 3.20e-04). ETA=8:24:13, max mem: 20.9 GB 
[11/25 04:25:41 visual_prompt]: 	Training 200/553. train loss: 213.9611,	0.8343 s / batch. (data: 2.87e-04). ETA=8:17:03, max mem: 20.9 GB 
[11/25 04:27:20 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8192 s / batch. (data: 5.38e-03). ETA=8:06:42, max mem: 20.9 GB 
[11/25 04:28:56 visual_prompt]: 	Training 400/553. train loss: 34.5564,	0.8342 s / batch. (data: 3.18e-04). ETA=8:14:11, max mem: 20.9 GB 
[11/25 04:30:34 visual_prompt]: 	Training 500/553. train loss: 32.5290,	1.2720 s / batch. (data: 4.50e-01). ETA=12:31:25, max mem: 20.9 GB 
[11/25 04:31:23 visual_prompt]: Epoch 36 / 100: avg data time: 1.48e-01, avg batch time: 0.9783, average train loss: 66.5310
[11/25 04:32:18 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.3116, average loss: 187.9097
[11/25 04:32:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.82	
[11/25 04:32:18 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[11/25 04:33:59 visual_prompt]: 	Training 100/553. train loss: 40.9709,	0.8240 s / batch. (data: 3.26e-04). ETA=8:04:40, max mem: 20.9 GB 
[11/25 04:35:37 visual_prompt]: 	Training 200/553. train loss: 21.7606,	0.8258 s / batch. (data: 3.17e-04). ETA=8:04:21, max mem: 20.9 GB 
[11/25 04:37:15 visual_prompt]: 	Training 300/553. train loss: 232.1864,	1.2514 s / batch. (data: 4.23e-01). ETA=12:11:54, max mem: 20.9 GB 
[11/25 04:38:55 visual_prompt]: 	Training 400/553. train loss: 20.2850,	1.8880 s / batch. (data: 1.07e+00). ETA=18:21:05, max mem: 20.9 GB 
[11/25 04:40:28 visual_prompt]: 	Training 500/553. train loss: 67.3315,	0.9748 s / batch. (data: 1.50e-01). ETA=9:26:51, max mem: 20.9 GB 
[11/25 04:41:21 visual_prompt]: Epoch 37 / 100: avg data time: 1.52e-01, avg batch time: 0.9820, average train loss: 64.9782
[11/25 04:42:17 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3113, average loss: 151.3353
[11/25 04:42:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.82	
[11/25 04:42:17 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[11/25 04:43:57 visual_prompt]: 	Training 100/553. train loss: 80.2315,	1.0390 s / batch. (data: 1.98e-01). ETA=10:01:35, max mem: 20.9 GB 
[11/25 04:45:35 visual_prompt]: 	Training 200/553. train loss: 64.3392,	1.2440 s / batch. (data: 4.05e-01). ETA=11:58:10, max mem: 20.9 GB 
[11/25 04:47:14 visual_prompt]: 	Training 300/553. train loss: 35.4241,	0.8118 s / batch. (data: 3.01e-04). ETA=7:47:20, max mem: 20.9 GB 
[11/25 04:48:49 visual_prompt]: 	Training 400/553. train loss: 55.5618,	0.8280 s / batch. (data: 2.98e-04). ETA=7:55:15, max mem: 20.9 GB 
[11/25 04:50:29 visual_prompt]: 	Training 500/553. train loss: 100.8810,	0.8560 s / batch. (data: 2.94e-04). ETA=8:09:54, max mem: 20.9 GB 
[11/25 04:51:18 visual_prompt]: Epoch 38 / 100: avg data time: 1.50e-01, avg batch time: 0.9786, average train loss: 76.1756
[11/25 04:52:14 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3112, average loss: 0.7080
[11/25 04:52:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.61	
[11/25 04:52:14 visual_prompt]: Best epoch 38: best metric: -0.708
[11/25 04:52:14 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[11/25 04:53:54 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8355 s / batch. (data: 7.97e-03). ETA=7:56:00, max mem: 20.9 GB 
[11/25 04:55:36 visual_prompt]: 	Training 200/553. train loss: 257.0297,	0.8480 s / batch. (data: 2.94e-04). ETA=8:01:44, max mem: 20.9 GB 
[11/25 04:57:15 visual_prompt]: 	Training 300/553. train loss: 128.9853,	0.8275 s / batch. (data: 3.19e-04). ETA=7:48:43, max mem: 20.9 GB 
[11/25 04:58:51 visual_prompt]: 	Training 400/553. train loss: 10.1288,	0.8875 s / batch. (data: 5.99e-02). ETA=8:21:13, max mem: 20.9 GB 
[11/25 05:00:29 visual_prompt]: 	Training 500/553. train loss: 70.0008,	1.6995 s / batch. (data: 8.86e-01). ETA=15:57:00, max mem: 20.9 GB 
[11/25 05:01:18 visual_prompt]: Epoch 39 / 100: avg data time: 1.53e-01, avg batch time: 0.9830, average train loss: 62.4884
[11/25 05:02:12 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3109, average loss: 80.0580
[11/25 05:02:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.21	
[11/25 05:02:12 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[11/25 05:03:55 visual_prompt]: 	Training 100/553. train loss: 84.9651,	0.8151 s / batch. (data: 3.09e-04). ETA=7:36:53, max mem: 20.9 GB 
[11/25 05:05:31 visual_prompt]: 	Training 200/553. train loss: 46.7133,	0.8320 s / batch. (data: 3.05e-04). ETA=7:44:59, max mem: 20.9 GB 
[11/25 05:07:10 visual_prompt]: 	Training 300/553. train loss: 10.3414,	0.8581 s / batch. (data: 3.41e-04). ETA=7:58:08, max mem: 20.9 GB 
[11/25 05:08:48 visual_prompt]: 	Training 400/553. train loss: 82.0898,	0.8199 s / batch. (data: 7.85e-04). ETA=7:35:29, max mem: 20.9 GB 
[11/25 05:10:24 visual_prompt]: 	Training 500/553. train loss: 97.8887,	0.8320 s / batch. (data: 3.01e-04). ETA=7:40:49, max mem: 20.9 GB 
[11/25 05:11:17 visual_prompt]: Epoch 40 / 100: avg data time: 1.56e-01, avg batch time: 0.9842, average train loss: 64.2023
[11/25 05:12:12 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3114, average loss: 40.3789
[11/25 05:12:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.03	
[11/25 05:12:12 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[11/25 05:13:58 visual_prompt]: 	Training 100/553. train loss: 16.3361,	0.8280 s / batch. (data: 2.98e-04). ETA=7:36:29, max mem: 20.9 GB 
[11/25 05:15:37 visual_prompt]: 	Training 200/553. train loss: 4.5002,	0.8480 s / batch. (data: 7.90e-04). ETA=7:46:06, max mem: 20.9 GB 
[11/25 05:17:14 visual_prompt]: 	Training 300/553. train loss: 37.4190,	0.8339 s / batch. (data: 8.02e-03). ETA=7:36:58, max mem: 20.9 GB 
[11/25 05:18:50 visual_prompt]: 	Training 400/553. train loss: 63.4580,	0.8290 s / batch. (data: 7.42e-03). ETA=7:32:53, max mem: 20.9 GB 
[11/25 05:20:25 visual_prompt]: 	Training 500/553. train loss: 1.0989,	0.8245 s / batch. (data: 5.44e-03). ETA=7:29:04, max mem: 20.9 GB 
[11/25 05:21:14 visual_prompt]: Epoch 41 / 100: avg data time: 1.49e-01, avg batch time: 0.9793, average train loss: 67.2143
[11/25 05:22:09 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3110, average loss: 434.8864
[11/25 05:22:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.06	
[11/25 05:22:09 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[11/25 05:23:48 visual_prompt]: 	Training 100/553. train loss: 57.4771,	0.8313 s / batch. (data: 7.96e-03). ETA=7:30:40, max mem: 20.9 GB 
[11/25 05:25:24 visual_prompt]: 	Training 200/553. train loss: 5.7742,	0.8569 s / batch. (data: 6.02e-03). ETA=7:43:07, max mem: 20.9 GB 
[11/25 05:27:00 visual_prompt]: 	Training 300/553. train loss: 12.9850,	0.8212 s / batch. (data: 2.88e-04). ETA=7:22:26, max mem: 20.9 GB 
[11/25 05:28:36 visual_prompt]: 	Training 400/553. train loss: 11.0050,	0.8304 s / batch. (data: 3.16e-04). ETA=7:26:01, max mem: 20.9 GB 
[11/25 05:30:11 visual_prompt]: 	Training 500/553. train loss: 24.0132,	0.8437 s / batch. (data: 5.38e-03). ETA=7:31:44, max mem: 20.9 GB 
[11/25 05:31:01 visual_prompt]: Epoch 42 / 100: avg data time: 1.32e-01, avg batch time: 0.9618, average train loss: 50.8760
[11/25 05:31:56 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3098, average loss: 65.7702
[11/25 05:31:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.96	
[11/25 05:31:56 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[11/25 05:33:39 visual_prompt]: 	Training 100/553. train loss: 5.4537,	0.8372 s / batch. (data: 1.55e-02). ETA=7:26:09, max mem: 20.9 GB 
[11/25 05:35:16 visual_prompt]: 	Training 200/553. train loss: 30.4806,	0.8223 s / batch. (data: 3.26e-04). ETA=7:16:50, max mem: 20.9 GB 
[11/25 05:36:52 visual_prompt]: 	Training 300/553. train loss: 13.4713,	0.8178 s / batch. (data: 3.09e-04). ETA=7:13:05, max mem: 20.9 GB 
[11/25 05:38:27 visual_prompt]: 	Training 400/553. train loss: 116.0019,	0.8449 s / batch. (data: 1.05e-02). ETA=7:26:01, max mem: 20.9 GB 
[11/25 05:40:07 visual_prompt]: 	Training 500/553. train loss: 5.1182,	0.8480 s / batch. (data: 5.42e-03). ETA=7:26:14, max mem: 20.9 GB 
[11/25 05:40:59 visual_prompt]: Epoch 43 / 100: avg data time: 1.52e-01, avg batch time: 0.9809, average train loss: 61.3016
[11/25 05:41:55 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3106, average loss: 37.1228
[11/25 05:41:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.51	
[11/25 05:41:55 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[11/25 05:43:36 visual_prompt]: 	Training 100/553. train loss: 17.0520,	0.8455 s / batch. (data: 1.05e-02). ETA=7:22:46, max mem: 20.9 GB 
[11/25 05:45:16 visual_prompt]: 	Training 200/553. train loss: 41.8210,	0.8240 s / batch. (data: 2.83e-04). ETA=7:10:08, max mem: 20.9 GB 
[11/25 05:46:51 visual_prompt]: 	Training 300/553. train loss: 33.0272,	0.8246 s / batch. (data: 1.76e-03). ETA=7:09:04, max mem: 20.9 GB 
[11/25 05:48:28 visual_prompt]: 	Training 400/553. train loss: 18.8421,	0.8360 s / batch. (data: 5.44e-03). ETA=7:13:36, max mem: 20.9 GB 
[11/25 05:50:06 visual_prompt]: 	Training 500/553. train loss: 4.9568,	0.8176 s / batch. (data: 2.97e-04). ETA=7:02:43, max mem: 20.9 GB 
[11/25 05:50:57 visual_prompt]: Epoch 44 / 100: avg data time: 1.50e-01, avg batch time: 0.9803, average train loss: 59.3901
[11/25 05:51:52 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3105, average loss: 35.7947
[11/25 05:51:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.83	
[11/25 05:51:52 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[11/25 05:53:35 visual_prompt]: 	Training 100/553. train loss: 55.2928,	0.8320 s / batch. (data: 8.02e-03). ETA=7:08:02, max mem: 20.9 GB 
[11/25 05:55:08 visual_prompt]: 	Training 200/553. train loss: 7.9507,	0.9375 s / batch. (data: 1.02e-01). ETA=8:00:44, max mem: 20.9 GB 
[11/25 05:56:47 visual_prompt]: 	Training 300/553. train loss: 97.6082,	0.8393 s / batch. (data: 1.00e-02). ETA=7:09:00, max mem: 20.9 GB 
[11/25 05:58:22 visual_prompt]: 	Training 400/553. train loss: 46.9183,	0.8359 s / batch. (data: 1.20e-02). ETA=7:05:51, max mem: 20.9 GB 
[11/25 06:00:03 visual_prompt]: 	Training 500/553. train loss: 2.4295,	0.8400 s / batch. (data: 3.32e-04). ETA=7:06:32, max mem: 20.9 GB 
[11/25 06:00:53 visual_prompt]: Epoch 45 / 100: avg data time: 1.47e-01, avg batch time: 0.9786, average train loss: 41.2939
[11/25 06:01:49 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3109, average loss: 51.0713
[11/25 06:01:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.49	
[11/25 06:01:49 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[11/25 06:03:31 visual_prompt]: 	Training 100/553. train loss: 108.2759,	1.2513 s / batch. (data: 4.34e-01). ETA=10:32:12, max mem: 20.9 GB 
[11/25 06:05:10 visual_prompt]: 	Training 200/553. train loss: 6.5789,	0.8421 s / batch. (data: 3.06e-04). ETA=7:04:02, max mem: 20.9 GB 
[11/25 06:06:46 visual_prompt]: 	Training 300/553. train loss: 10.2063,	0.8275 s / batch. (data: 4.69e-04). ETA=6:55:19, max mem: 20.9 GB 
[11/25 06:08:24 visual_prompt]: 	Training 400/553. train loss: 48.9788,	0.8449 s / batch. (data: 7.59e-04). ETA=7:02:41, max mem: 20.9 GB 
[11/25 06:09:57 visual_prompt]: 	Training 500/553. train loss: 79.6497,	0.8281 s / batch. (data: 2.82e-04). ETA=6:52:51, max mem: 20.9 GB 
[11/25 06:10:51 visual_prompt]: Epoch 46 / 100: avg data time: 1.50e-01, avg batch time: 0.9797, average train loss: 55.6199
[11/25 06:11:46 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3080, average loss: 56.1374
[11/25 06:11:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.27	
[11/25 06:11:46 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[11/25 06:13:29 visual_prompt]: 	Training 100/553. train loss: 14.5243,	0.8400 s / batch. (data: 3.07e-04). ETA=6:56:39, max mem: 20.9 GB 
[11/25 06:15:03 visual_prompt]: 	Training 200/553. train loss: 41.7565,	1.2840 s / batch. (data: 4.45e-01). ETA=10:34:45, max mem: 20.9 GB 
[11/25 06:16:41 visual_prompt]: 	Training 300/553. train loss: 50.3816,	0.8200 s / batch. (data: 2.76e-04). ETA=6:44:01, max mem: 20.9 GB 
[11/25 06:18:18 visual_prompt]: 	Training 400/553. train loss: 16.0877,	0.8285 s / batch. (data: 7.17e-04). ETA=6:46:48, max mem: 20.9 GB 
[11/25 06:19:54 visual_prompt]: 	Training 500/553. train loss: 21.9337,	0.8237 s / batch. (data: 1.05e-02). ETA=6:43:04, max mem: 20.9 GB 
[11/25 06:20:47 visual_prompt]: Epoch 47 / 100: avg data time: 1.48e-01, avg batch time: 0.9773, average train loss: 74.1330
[11/25 06:21:42 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3097, average loss: 14.5108
[11/25 06:21:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.96	
[11/25 06:21:42 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[11/25 06:23:23 visual_prompt]: 	Training 100/553. train loss: 70.8380,	0.8154 s / batch. (data: 3.12e-04). ETA=6:36:57, max mem: 20.9 GB 
[11/25 06:25:01 visual_prompt]: 	Training 200/553. train loss: 3.5125,	0.8242 s / batch. (data: 7.56e-03). ETA=6:39:52, max mem: 20.9 GB 
[11/25 06:26:40 visual_prompt]: 	Training 300/553. train loss: 87.6523,	1.4556 s / batch. (data: 6.41e-01). ETA=11:43:45, max mem: 20.9 GB 
[11/25 06:28:14 visual_prompt]: 	Training 400/553. train loss: 48.0421,	0.8667 s / batch. (data: 3.23e-04). ETA=6:57:34, max mem: 20.9 GB 
[11/25 06:29:52 visual_prompt]: 	Training 500/553. train loss: 19.7940,	0.8411 s / batch. (data: 5.42e-03). ETA=6:43:51, max mem: 20.9 GB 
[11/25 06:30:42 visual_prompt]: Epoch 48 / 100: avg data time: 1.48e-01, avg batch time: 0.9764, average train loss: 60.5188
[11/25 06:31:38 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3105, average loss: 57.0932
[11/25 06:31:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.17	
[11/25 06:31:38 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[11/25 06:33:18 visual_prompt]: 	Training 100/553. train loss: 91.9706,	0.8254 s / batch. (data: 3.26e-04). ETA=6:34:13, max mem: 20.9 GB 
[11/25 06:34:55 visual_prompt]: 	Training 200/553. train loss: 91.7134,	0.8158 s / batch. (data: 2.86e-04). ETA=6:28:15, max mem: 20.9 GB 
[11/25 06:36:33 visual_prompt]: 	Training 300/553. train loss: 12.2309,	0.8252 s / batch. (data: 3.37e-04). ETA=6:31:23, max mem: 20.9 GB 
[11/25 06:38:12 visual_prompt]: 	Training 400/553. train loss: 2.3768,	0.8267 s / batch. (data: 9.33e-03). ETA=6:30:42, max mem: 20.9 GB 
[11/25 06:39:49 visual_prompt]: 	Training 500/553. train loss: 8.9641,	0.8510 s / batch. (data: 7.15e-04). ETA=6:40:45, max mem: 20.9 GB 
[11/25 06:40:41 visual_prompt]: Epoch 49 / 100: avg data time: 1.52e-01, avg batch time: 0.9818, average train loss: 44.0392
[11/25 06:41:36 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3101, average loss: 24.5478
[11/25 06:41:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.05	
[11/25 06:41:36 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[11/25 06:43:19 visual_prompt]: 	Training 100/553. train loss: 139.8577,	0.8400 s / batch. (data: 8.09e-04). ETA=6:33:26, max mem: 20.9 GB 
[11/25 06:44:57 visual_prompt]: 	Training 200/553. train loss: 227.0549,	0.8120 s / batch. (data: 2.72e-04). ETA=6:18:59, max mem: 20.9 GB 
[11/25 06:46:33 visual_prompt]: 	Training 300/553. train loss: 31.9506,	0.8280 s / batch. (data: 3.09e-04). ETA=6:25:03, max mem: 20.9 GB 
[11/25 06:48:09 visual_prompt]: 	Training 400/553. train loss: 15.5226,	0.8388 s / batch. (data: 3.18e-04). ETA=6:28:40, max mem: 20.9 GB 
[11/25 06:49:47 visual_prompt]: 	Training 500/553. train loss: 82.9442,	0.8456 s / batch. (data: 2.20e-02). ETA=6:30:24, max mem: 20.9 GB 
[11/25 06:50:37 visual_prompt]: Epoch 50 / 100: avg data time: 1.48e-01, avg batch time: 0.9774, average train loss: 47.5957
[11/25 06:51:33 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3101, average loss: 36.8941
[11/25 06:51:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.32	
[11/25 06:51:33 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[11/25 06:53:14 visual_prompt]: 	Training 100/553. train loss: 63.9178,	1.0560 s / batch. (data: 2.44e-01). ETA=8:04:52, max mem: 20.9 GB 
[11/25 06:54:52 visual_prompt]: 	Training 200/553. train loss: 154.4764,	0.8469 s / batch. (data: 3.08e-04). ETA=6:27:28, max mem: 20.9 GB 
[11/25 06:56:30 visual_prompt]: 	Training 300/553. train loss: 17.4423,	1.3886 s / batch. (data: 5.49e-01). ETA=10:32:59, max mem: 20.9 GB 
[11/25 06:58:07 visual_prompt]: 	Training 400/553. train loss: 223.1041,	1.3241 s / batch. (data: 4.86e-01). ETA=10:01:22, max mem: 20.9 GB 
[11/25 06:59:44 visual_prompt]: 	Training 500/553. train loss: 73.4741,	0.8293 s / batch. (data: 7.80e-04). ETA=6:15:16, max mem: 20.9 GB 
[11/25 07:00:33 visual_prompt]: Epoch 51 / 100: avg data time: 1.47e-01, avg batch time: 0.9778, average train loss: 41.7550
[11/25 07:01:29 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3094, average loss: 28.3620
[11/25 07:01:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.72	
[11/25 07:01:29 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[11/25 07:03:12 visual_prompt]: 	Training 100/553. train loss: 2.7499,	0.8485 s / batch. (data: 5.93e-03). ETA=6:21:48, max mem: 20.9 GB 
[11/25 07:04:48 visual_prompt]: 	Training 200/553. train loss: 58.9792,	0.8320 s / batch. (data: 3.19e-04). ETA=6:12:58, max mem: 20.9 GB 
[11/25 07:06:26 visual_prompt]: 	Training 300/553. train loss: 136.8985,	0.8241 s / batch. (data: 3.09e-04). ETA=6:08:03, max mem: 20.9 GB 
[11/25 07:08:05 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8200 s / batch. (data: 3.00e-04). ETA=6:04:51, max mem: 20.9 GB 
[11/25 07:09:38 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8376 s / batch. (data: 9.55e-03). ETA=6:11:17, max mem: 20.9 GB 
[11/25 07:10:28 visual_prompt]: Epoch 52 / 100: avg data time: 1.44e-01, avg batch time: 0.9743, average train loss: 42.6473
[11/25 07:11:23 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3108, average loss: 151.0794
[11/25 07:11:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.23	
[11/25 07:11:23 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[11/25 07:13:02 visual_prompt]: 	Training 100/553. train loss: 120.2621,	0.8251 s / batch. (data: 2.99e-04). ETA=6:03:40, max mem: 20.9 GB 
[11/25 07:14:38 visual_prompt]: 	Training 200/553. train loss: 60.7727,	0.8493 s / batch. (data: 2.13e-02). ETA=6:12:55, max mem: 20.9 GB 
[11/25 07:16:14 visual_prompt]: 	Training 300/553. train loss: 6.7630,	0.8485 s / batch. (data: 7.12e-04). ETA=6:11:08, max mem: 20.9 GB 
[11/25 07:17:51 visual_prompt]: 	Training 400/553. train loss: 51.5285,	0.8480 s / batch. (data: 7.95e-03). ETA=6:09:30, max mem: 20.9 GB 
[11/25 07:19:26 visual_prompt]: 	Training 500/553. train loss: 53.9246,	0.8221 s / batch. (data: 1.10e-02). ETA=5:56:51, max mem: 20.9 GB 
[11/25 07:20:17 visual_prompt]: Epoch 53 / 100: avg data time: 1.35e-01, avg batch time: 0.9657, average train loss: 38.3206
[11/25 07:21:12 visual_prompt]: Inference (val):avg data time: 2.04e-04, avg batch time: 0.3108, average loss: 9.8799
[11/25 07:21:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.63	
[11/25 07:21:12 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[11/25 07:22:55 visual_prompt]: 	Training 100/553. train loss: 23.6152,	0.8305 s / batch. (data: 3.14e-04). ETA=5:58:22, max mem: 20.9 GB 
[11/25 07:24:31 visual_prompt]: 	Training 200/553. train loss: 54.6479,	0.8320 s / batch. (data: 2.98e-04). ETA=5:57:38, max mem: 20.9 GB 
[11/25 07:26:06 visual_prompt]: 	Training 300/553. train loss: 4.9590,	0.8335 s / batch. (data: 3.23e-04). ETA=5:56:52, max mem: 20.9 GB 
[11/25 07:27:41 visual_prompt]: 	Training 400/553. train loss: 145.9541,	0.8380 s / batch. (data: 2.25e-02). ETA=5:57:26, max mem: 20.9 GB 
[11/25 07:29:17 visual_prompt]: 	Training 500/553. train loss: 40.1548,	0.8280 s / batch. (data: 3.30e-04). ETA=5:51:46, max mem: 20.9 GB 
[11/25 07:30:07 visual_prompt]: Epoch 54 / 100: avg data time: 1.39e-01, avg batch time: 0.9684, average train loss: 39.6460
[11/25 07:31:02 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3105, average loss: 24.6680
[11/25 07:31:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.68	
[11/25 07:31:02 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[11/25 07:32:41 visual_prompt]: 	Training 100/553. train loss: 29.5495,	0.8307 s / batch. (data: 3.25e-04). ETA=5:50:47, max mem: 20.9 GB 
[11/25 07:34:16 visual_prompt]: 	Training 200/553. train loss: 8.5054,	0.8200 s / batch. (data: 3.08e-04). ETA=5:44:55, max mem: 20.9 GB 
[11/25 07:35:52 visual_prompt]: 	Training 300/553. train loss: 55.7648,	0.8232 s / batch. (data: 2.85e-04). ETA=5:44:53, max mem: 20.9 GB 
[11/25 07:37:27 visual_prompt]: 	Training 400/553. train loss: 34.0442,	1.0999 s / batch. (data: 2.61e-01). ETA=7:39:00, max mem: 20.9 GB 
[11/25 07:39:02 visual_prompt]: 	Training 500/553. train loss: 2.2429,	0.8381 s / batch. (data: 1.19e-02). ETA=5:48:19, max mem: 20.9 GB 
[11/25 07:39:53 visual_prompt]: Epoch 55 / 100: avg data time: 1.31e-01, avg batch time: 0.9614, average train loss: 37.3406
[11/25 07:40:48 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3096, average loss: 72.3726
[11/25 07:40:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.51	
[11/25 07:40:48 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[11/25 07:42:29 visual_prompt]: 	Training 100/553. train loss: 30.2240,	0.8324 s / batch. (data: 7.97e-03). ETA=5:43:52, max mem: 20.9 GB 
[11/25 07:44:04 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8360 s / batch. (data: 3.10e-04). ETA=5:43:56, max mem: 20.9 GB 
[11/25 07:45:41 visual_prompt]: 	Training 300/553. train loss: 14.4014,	0.8464 s / batch. (data: 3.01e-04). ETA=5:46:49, max mem: 20.9 GB 
[11/25 07:47:18 visual_prompt]: 	Training 400/553. train loss: 55.4842,	0.8513 s / batch. (data: 1.10e-02). ETA=5:47:25, max mem: 20.9 GB 
[11/25 07:48:53 visual_prompt]: 	Training 500/553. train loss: 5.5186,	1.9546 s / batch. (data: 1.13e+00). ETA=13:14:22, max mem: 20.9 GB 
[11/25 07:49:42 visual_prompt]: Epoch 56 / 100: avg data time: 1.36e-01, avg batch time: 0.9652, average train loss: 43.4949
[11/25 07:50:37 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3096, average loss: 36.1415
[11/25 07:50:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.49	
[11/25 07:50:37 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[11/25 07:52:20 visual_prompt]: 	Training 100/553. train loss: 11.7745,	0.8309 s / batch. (data: 7.50e-04). ETA=5:35:33, max mem: 20.9 GB 
[11/25 07:53:55 visual_prompt]: 	Training 200/553. train loss: 20.4587,	0.8366 s / batch. (data: 1.07e-02). ETA=5:36:27, max mem: 20.9 GB 
[11/25 07:55:29 visual_prompt]: 	Training 300/553. train loss: 52.7563,	0.8485 s / batch. (data: 3.86e-04). ETA=5:39:50, max mem: 20.9 GB 
[11/25 07:57:04 visual_prompt]: 	Training 400/553. train loss: 107.6301,	0.8450 s / batch. (data: 2.99e-04). ETA=5:37:01, max mem: 20.9 GB 
[11/25 07:58:37 visual_prompt]: 	Training 500/553. train loss: 26.3934,	0.8389 s / batch. (data: 3.24e-04). ETA=5:33:11, max mem: 20.9 GB 
[11/25 07:59:28 visual_prompt]: Epoch 57 / 100: avg data time: 1.30e-01, avg batch time: 0.9608, average train loss: 33.1353
[11/25 08:00:22 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3111, average loss: 52.2059
[11/25 08:00:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.20	
[11/25 08:00:22 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[11/25 08:02:02 visual_prompt]: 	Training 100/553. train loss: 8.4880,	1.0822 s / batch. (data: 2.59e-01). ETA=7:07:05, max mem: 20.9 GB 
[11/25 08:03:38 visual_prompt]: 	Training 200/553. train loss: 121.7783,	0.8227 s / batch. (data: 2.90e-04). ETA=5:23:18, max mem: 20.9 GB 
[11/25 08:05:17 visual_prompt]: 	Training 300/553. train loss: 33.8112,	0.8566 s / batch. (data: 1.59e-02). ETA=5:35:11, max mem: 20.9 GB 
[11/25 08:06:52 visual_prompt]: 	Training 400/553. train loss: 18.9799,	1.3534 s / batch. (data: 5.34e-01). ETA=8:47:20, max mem: 20.9 GB 
[11/25 08:08:26 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8160 s / batch. (data: 2.86e-04). ETA=5:16:35, max mem: 20.9 GB 
[11/25 08:09:16 visual_prompt]: Epoch 58 / 100: avg data time: 1.34e-01, avg batch time: 0.9649, average train loss: 31.1715
[11/25 08:10:11 visual_prompt]: Inference (val):avg data time: 4.04e-04, avg batch time: 0.3097, average loss: 139.2448
[11/25 08:10:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.89	
[11/25 08:10:11 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[11/25 08:11:52 visual_prompt]: 	Training 100/553. train loss: 32.4072,	0.8381 s / batch. (data: 1.05e-02). ETA=5:23:02, max mem: 20.9 GB 
[11/25 08:13:29 visual_prompt]: 	Training 200/553. train loss: 0.0502,	0.8319 s / batch. (data: 5.42e-03). ETA=5:19:16, max mem: 20.9 GB 
[11/25 08:15:04 visual_prompt]: 	Training 300/553. train loss: 124.5973,	0.8386 s / batch. (data: 3.02e-04). ETA=5:20:26, max mem: 20.9 GB 
[11/25 08:16:39 visual_prompt]: 	Training 400/553. train loss: 20.7312,	0.8277 s / batch. (data: 3.37e-04). ETA=5:14:53, max mem: 20.9 GB 
[11/25 08:18:18 visual_prompt]: 	Training 500/553. train loss: 31.7928,	0.8331 s / batch. (data: 8.12e-04). ETA=5:15:32, max mem: 20.9 GB 
[11/25 08:19:06 visual_prompt]: Epoch 59 / 100: avg data time: 1.37e-01, avg batch time: 0.9676, average train loss: 33.6481
[11/25 08:20:01 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3097, average loss: 35.2008
[11/25 08:20:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.83	
[11/25 08:20:01 visual_prompt]: Stopping early.
[11/25 08:20:01 visual_prompt]: Rank of current process: 0. World size: 1
[11/25 08:20:01 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/25 08:20:01 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/25 08:20:01 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/25 08:20:01 visual_prompt]: Training with config:
[11/25 08:20:01 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/25 08:20:01 visual_prompt]: Loading training data...
[11/25 08:20:01 visual_prompt]: Constructing mammo-cbis dataset train...
[11/25 08:20:01 visual_prompt]: Loading validation data...
[11/25 08:20:01 visual_prompt]: Constructing mammo-cbis dataset val...
[11/25 08:20:01 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/25 08:20:04 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/25 08:20:04 visual_prompt]: tuned percent:0.525
[11/25 08:20:04 visual_prompt]: Device used for model: 0
[11/25 08:20:04 visual_prompt]: Setting up Evaluator...
[11/25 08:20:04 visual_prompt]: Setting up Trainer...
[11/25 08:20:04 visual_prompt]: 	Setting up the optimizer...
[11/25 08:20:04 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/25 08:21:44 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8172 s / batch. (data: 2.84e-04). ETA=12:31:47, max mem: 20.9 GB 
[11/25 08:23:18 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8370 s / batch. (data: 2.85e-04). ETA=12:48:37, max mem: 20.9 GB 
[11/25 08:24:57 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8529 s / batch. (data: 5.46e-03). ETA=13:01:47, max mem: 20.9 GB 
[11/25 08:26:31 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8178 s / batch. (data: 2.92e-04). ETA=12:28:14, max mem: 20.9 GB 
[11/25 08:28:08 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8265 s / batch. (data: 7.98e-03). ETA=12:34:54, max mem: 20.9 GB 
[11/25 08:28:59 visual_prompt]: Epoch 1 / 100: avg data time: 1.34e-01, avg batch time: 0.9674, average train loss: 1.5403
[11/25 08:29:53 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3093, average loss: 1.5201
[11/25 08:29:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/25 08:29:53 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/25 08:31:32 visual_prompt]: 	Training 100/553. train loss: 7.2424,	0.8342 s / batch. (data: 4.40e-04). ETA=12:39:44, max mem: 20.9 GB 
[11/25 08:33:07 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.1719 s / batch. (data: 3.32e-01). ETA=17:45:22, max mem: 20.9 GB 
[11/25 08:34:47 visual_prompt]: 	Training 300/553. train loss: 4.3327,	1.2893 s / batch. (data: 4.60e-01). ETA=19:30:00, max mem: 20.9 GB 
[11/25 08:36:47 visual_prompt]: 	Training 400/553. train loss: 1.6245,	0.8534 s / batch. (data: 3.43e-04). ETA=12:52:57, max mem: 20.9 GB 
[11/25 08:38:31 visual_prompt]: 	Training 500/553. train loss: 1.6753,	0.8265 s / batch. (data: 3.21e-04). ETA=12:27:13, max mem: 20.9 GB 
[11/25 08:39:20 visual_prompt]: Epoch 2 / 100: avg data time: 1.94e-01, avg batch time: 1.0260, average train loss: 12.4687
[11/25 08:40:16 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3098, average loss: 17.9150
[11/25 08:40:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.10	
[11/25 08:40:16 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/25 08:41:56 visual_prompt]: 	Training 100/553. train loss: 28.1278,	0.8388 s / batch. (data: 2.99e-04). ETA=12:36:12, max mem: 20.9 GB 
[11/25 08:43:33 visual_prompt]: 	Training 200/553. train loss: 10.4513,	0.8280 s / batch. (data: 3.15e-04). ETA=12:25:07, max mem: 20.9 GB 
[11/25 08:45:08 visual_prompt]: 	Training 300/553. train loss: 2.1308,	0.8241 s / batch. (data: 2.87e-04). ETA=12:20:11, max mem: 20.9 GB 
[11/25 08:46:46 visual_prompt]: 	Training 400/553. train loss: 64.6666,	0.9209 s / batch. (data: 8.89e-02). ETA=13:45:40, max mem: 20.9 GB 
[11/25 08:48:23 visual_prompt]: 	Training 500/553. train loss: 6.3802,	1.0603 s / batch. (data: 2.25e-01). ETA=15:48:52, max mem: 20.9 GB 
[11/25 08:49:13 visual_prompt]: Epoch 3 / 100: avg data time: 1.38e-01, avg batch time: 0.9710, average train loss: 13.6450
[11/25 08:50:07 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3102, average loss: 12.2957
[11/25 08:50:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.88	
[11/25 08:50:07 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/25 08:51:48 visual_prompt]: 	Training 100/553. train loss: 14.1667,	0.8541 s / batch. (data: 1.40e-02). ETA=12:42:07, max mem: 20.9 GB 
[11/25 08:53:24 visual_prompt]: 	Training 200/553. train loss: 13.1459,	0.8363 s / batch. (data: 3.20e-04). ETA=12:24:50, max mem: 20.9 GB 
[11/25 08:55:00 visual_prompt]: 	Training 300/553. train loss: 5.1815,	1.3978 s / batch. (data: 5.65e-01). ETA=20:42:38, max mem: 20.9 GB 
[11/25 08:56:32 visual_prompt]: 	Training 400/553. train loss: 0.4137,	1.2166 s / batch. (data: 3.82e-01). ETA=17:59:30, max mem: 20.9 GB 
[11/25 08:58:10 visual_prompt]: 	Training 500/553. train loss: 0.0000,	3.3040 s / batch. (data: 2.47e+00). ETA=2 days, 0:46:17, max mem: 20.9 GB 
[11/25 08:59:01 visual_prompt]: Epoch 4 / 100: avg data time: 1.32e-01, avg batch time: 0.9658, average train loss: 18.2450
[11/25 08:59:56 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3109, average loss: 5.3036
[11/25 08:59:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[11/25 08:59:56 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/25 09:01:34 visual_prompt]: 	Training 100/553. train loss: 83.4474,	0.8320 s / batch. (data: 7.96e-03). ETA=12:14:43, max mem: 20.9 GB 
[11/25 09:03:11 visual_prompt]: 	Training 200/553. train loss: 0.9112,	1.1480 s / batch. (data: 3.29e-01). ETA=16:51:54, max mem: 20.9 GB 
[11/25 09:04:48 visual_prompt]: 	Training 300/553. train loss: 73.5406,	0.8400 s / batch. (data: 1.61e-02). ETA=12:19:02, max mem: 20.9 GB 
[11/25 09:06:23 visual_prompt]: 	Training 400/553. train loss: 2.6459,	0.8328 s / batch. (data: 5.43e-03). ETA=12:11:17, max mem: 20.9 GB 
[11/25 09:07:59 visual_prompt]: 	Training 500/553. train loss: 22.0819,	0.8332 s / batch. (data: 1.20e-02). ETA=12:10:16, max mem: 20.9 GB 
[11/25 09:08:50 visual_prompt]: Epoch 5 / 100: avg data time: 1.34e-01, avg batch time: 0.9659, average train loss: 25.7894
[11/25 09:09:45 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3098, average loss: 3.6946
[11/25 09:09:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.25	
[11/25 09:09:45 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/25 09:11:25 visual_prompt]: 	Training 100/553. train loss: 24.2818,	0.8180 s / batch. (data: 1.04e-02). ETA=11:54:53, max mem: 20.9 GB 
[11/25 09:13:01 visual_prompt]: 	Training 200/553. train loss: 99.6738,	0.8603 s / batch. (data: 2.42e-02). ETA=12:30:22, max mem: 20.9 GB 
[11/25 09:14:35 visual_prompt]: 	Training 300/553. train loss: 8.5172,	0.8325 s / batch. (data: 3.23e-04). ETA=12:04:45, max mem: 20.9 GB 
[11/25 09:16:15 visual_prompt]: 	Training 400/553. train loss: 18.0969,	0.8427 s / batch. (data: 1.08e-02). ETA=12:12:12, max mem: 20.9 GB 
[11/25 09:17:49 visual_prompt]: 	Training 500/553. train loss: 11.3561,	0.8363 s / batch. (data: 5.48e-03). ETA=12:05:14, max mem: 20.9 GB 
[11/25 09:18:39 visual_prompt]: Epoch 6 / 100: avg data time: 1.33e-01, avg batch time: 0.9655, average train loss: 23.5509
[11/25 09:19:33 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3090, average loss: 6.1940
[11/25 09:19:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.42	
[11/25 09:19:33 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/25 09:21:11 visual_prompt]: 	Training 100/553. train loss: 71.1854,	0.8291 s / batch. (data: 3.13e-04). ETA=11:56:54, max mem: 20.9 GB 
[11/25 09:22:48 visual_prompt]: 	Training 200/553. train loss: 5.1444,	0.8320 s / batch. (data: 2.99e-04). ETA=11:58:02, max mem: 20.9 GB 
[11/25 09:24:26 visual_prompt]: 	Training 300/553. train loss: 18.4046,	1.0480 s / batch. (data: 2.01e-01). ETA=15:02:42, max mem: 20.9 GB 
[11/25 09:26:03 visual_prompt]: 	Training 400/553. train loss: 13.8397,	1.8186 s / batch. (data: 9.93e-01). ETA=1 day, 2:03:26, max mem: 20.9 GB 
[11/25 09:27:38 visual_prompt]: 	Training 500/553. train loss: 68.1425,	0.8400 s / batch. (data: 3.23e-04). ETA=12:00:44, max mem: 20.9 GB 
[11/25 09:28:27 visual_prompt]: Epoch 7 / 100: avg data time: 1.32e-01, avg batch time: 0.9642, average train loss: 24.9745
[11/25 09:29:21 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3086, average loss: 6.3741
[11/25 09:29:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.37	
[11/25 09:29:21 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/25 09:31:00 visual_prompt]: 	Training 100/553. train loss: 69.9379,	0.8120 s / batch. (data: 4.67e-04). ETA=11:34:37, max mem: 20.9 GB 
[11/25 09:32:37 visual_prompt]: 	Training 200/553. train loss: 120.0211,	0.8272 s / batch. (data: 5.42e-03). ETA=11:46:14, max mem: 20.9 GB 
[11/25 09:34:13 visual_prompt]: 	Training 300/553. train loss: 14.7993,	0.8490 s / batch. (data: 1.05e-02). ETA=12:03:29, max mem: 20.9 GB 
[11/25 09:35:48 visual_prompt]: 	Training 400/553. train loss: 60.3261,	0.8539 s / batch. (data: 5.42e-03). ETA=12:06:15, max mem: 20.9 GB 
[11/25 09:37:25 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.2520 s / batch. (data: 4.11e-01). ETA=17:42:43, max mem: 20.9 GB 
[11/25 09:38:15 visual_prompt]: Epoch 8 / 100: avg data time: 1.35e-01, avg batch time: 0.9653, average train loss: 35.4590
[11/25 09:39:10 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3089, average loss: 29.6084
[11/25 09:39:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.89	
[11/25 09:39:10 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/25 09:40:50 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8162 s / batch. (data: 5.43e-03). ETA=11:30:45, max mem: 20.9 GB 
[11/25 09:42:25 visual_prompt]: 	Training 200/553. train loss: 17.5484,	0.8406 s / batch. (data: 1.60e-02). ETA=11:49:57, max mem: 20.9 GB 
[11/25 09:44:02 visual_prompt]: 	Training 300/553. train loss: 11.5687,	1.4747 s / batch. (data: 6.42e-01). ETA=20:43:06, max mem: 20.9 GB 
[11/25 09:45:38 visual_prompt]: 	Training 400/553. train loss: 20.6134,	0.8507 s / batch. (data: 1.47e-02). ETA=11:55:39, max mem: 20.9 GB 
[11/25 09:47:15 visual_prompt]: 	Training 500/553. train loss: 41.5786,	0.8611 s / batch. (data: 2.11e-02). ETA=12:02:58, max mem: 20.9 GB 
[11/25 09:48:05 visual_prompt]: Epoch 9 / 100: avg data time: 1.34e-01, avg batch time: 0.9666, average train loss: 28.5791
[11/25 09:48:59 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3117, average loss: 23.8281
[11/25 09:49:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.98	
[11/25 09:49:00 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/25 09:50:42 visual_prompt]: 	Training 100/553. train loss: 82.2706,	0.8310 s / batch. (data: 1.78e-02). ETA=11:35:37, max mem: 20.9 GB 
[11/25 09:52:16 visual_prompt]: 	Training 200/553. train loss: 1.1150,	0.8320 s / batch. (data: 2.96e-04). ETA=11:35:00, max mem: 20.9 GB 
[11/25 09:53:52 visual_prompt]: 	Training 300/553. train loss: 30.2138,	0.8625 s / batch. (data: 1.19e-02). ETA=11:59:04, max mem: 20.9 GB 
[11/25 09:55:25 visual_prompt]: 	Training 400/553. train loss: 40.2294,	0.8273 s / batch. (data: 3.09e-04). ETA=11:28:23, max mem: 20.9 GB 
[11/25 09:57:03 visual_prompt]: 	Training 500/553. train loss: 7.8690,	0.8440 s / batch. (data: 2.52e-04). ETA=11:40:50, max mem: 20.9 GB 
[11/25 09:57:53 visual_prompt]: Epoch 10 / 100: avg data time: 1.34e-01, avg batch time: 0.9638, average train loss: 33.4559
[11/25 09:58:47 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3094, average loss: 1.7496
[11/25 09:58:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.90	
[11/25 09:58:47 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/25 10:00:29 visual_prompt]: 	Training 100/553. train loss: 52.0340,	0.8480 s / batch. (data: 4.40e-04). ETA=11:41:58, max mem: 20.9 GB 
[11/25 10:02:06 visual_prompt]: 	Training 200/553. train loss: 52.9471,	0.8197 s / batch. (data: 2.92e-04). ETA=11:17:11, max mem: 20.9 GB 
[11/25 10:03:41 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.9960 s / batch. (data: 1.15e+00). ETA=1 day, 3:25:43, max mem: 20.9 GB 
[11/25 10:05:16 visual_prompt]: 	Training 400/553. train loss: 18.9793,	0.8292 s / batch. (data: 5.37e-03). ETA=11:22:17, max mem: 20.9 GB 
[11/25 10:06:51 visual_prompt]: 	Training 500/553. train loss: 18.9895,	0.8440 s / batch. (data: 1.19e-02). ETA=11:33:03, max mem: 20.9 GB 
[11/25 10:07:41 visual_prompt]: Epoch 11 / 100: avg data time: 1.35e-01, avg batch time: 0.9655, average train loss: 33.8645
[11/25 10:08:35 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3095, average loss: 56.5795
[11/25 10:08:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.93	
[11/25 10:08:35 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/25 10:10:16 visual_prompt]: 	Training 100/553. train loss: 4.4777,	0.8440 s / batch. (data: 7.94e-03). ETA=11:30:54, max mem: 20.9 GB 
[11/25 10:11:53 visual_prompt]: 	Training 200/553. train loss: 12.2439,	0.8480 s / batch. (data: 7.36e-04). ETA=11:32:46, max mem: 20.9 GB 
[11/25 10:13:28 visual_prompt]: 	Training 300/553. train loss: 6.2091,	0.8240 s / batch. (data: 2.90e-04). ETA=11:11:47, max mem: 20.9 GB 
[11/25 10:15:04 visual_prompt]: 	Training 400/553. train loss: 36.6586,	0.8440 s / batch. (data: 3.30e-04). ETA=11:26:40, max mem: 20.9 GB 
[11/25 10:16:41 visual_prompt]: 	Training 500/553. train loss: 194.9420,	0.8360 s / batch. (data: 2.98e-04). ETA=11:18:48, max mem: 20.9 GB 
[11/25 10:17:30 visual_prompt]: Epoch 12 / 100: avg data time: 1.37e-01, avg batch time: 0.9669, average train loss: 38.2417
[11/25 10:18:25 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3102, average loss: 78.2413
[11/25 10:18:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.06	
[11/25 10:18:25 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/25 10:20:06 visual_prompt]: 	Training 100/553. train loss: 25.1437,	0.8174 s / batch. (data: 1.19e-02). ETA=11:01:37, max mem: 20.9 GB 
[11/25 10:21:40 visual_prompt]: 	Training 200/553. train loss: 12.9302,	0.8477 s / batch. (data: 1.16e-02). ETA=11:24:42, max mem: 20.9 GB 
[11/25 10:23:16 visual_prompt]: 	Training 300/553. train loss: 32.4434,	1.4171 s / batch. (data: 6.10e-01). ETA=19:02:15, max mem: 20.9 GB 
[11/25 10:24:51 visual_prompt]: 	Training 400/553. train loss: 112.1674,	0.8395 s / batch. (data: 1.05e-02). ETA=11:15:18, max mem: 20.9 GB 
[11/25 10:26:28 visual_prompt]: 	Training 500/553. train loss: 73.3396,	0.8195 s / batch. (data: 2.95e-04). ETA=10:57:51, max mem: 20.9 GB 
[11/25 10:27:18 visual_prompt]: Epoch 13 / 100: avg data time: 1.34e-01, avg batch time: 0.9636, average train loss: 42.1476
[11/25 10:28:12 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3102, average loss: 14.5400
[11/25 10:28:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 61.97	
[11/25 10:28:12 visual_prompt]: Best epoch 13: best metric: -14.540
[11/25 10:28:12 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/25 10:29:53 visual_prompt]: 	Training 100/553. train loss: 32.7932,	0.8440 s / batch. (data: 2.89e-04). ETA=11:15:19, max mem: 20.9 GB 
[11/25 10:31:28 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8405 s / batch. (data: 1.05e-02). ETA=11:11:10, max mem: 20.9 GB 
[11/25 10:33:03 visual_prompt]: 	Training 300/553. train loss: 16.0893,	0.8374 s / batch. (data: 3.22e-04). ETA=11:07:17, max mem: 20.9 GB 
[11/25 10:34:38 visual_prompt]: 	Training 400/553. train loss: 9.4391,	0.8440 s / batch. (data: 2.69e-04). ETA=11:11:06, max mem: 20.9 GB 
[11/25 10:36:13 visual_prompt]: 	Training 500/553. train loss: 11.7443,	0.8572 s / batch. (data: 1.55e-02). ETA=11:20:14, max mem: 20.9 GB 
[11/25 10:37:03 visual_prompt]: Epoch 14 / 100: avg data time: 1.26e-01, avg batch time: 0.9592, average train loss: 26.6456
[11/25 10:37:57 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3094, average loss: 7.4371
[11/25 10:37:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.78	
[11/25 10:37:57 visual_prompt]: Best epoch 14: best metric: -7.437
[11/25 10:37:57 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/25 10:39:37 visual_prompt]: 	Training 100/553. train loss: 49.4517,	0.8454 s / batch. (data: 7.66e-03). ETA=11:08:40, max mem: 20.9 GB 
[11/25 10:41:10 visual_prompt]: 	Training 200/553. train loss: 73.2785,	0.8427 s / batch. (data: 6.64e-03). ETA=11:05:08, max mem: 20.9 GB 
[11/25 10:42:47 visual_prompt]: 	Training 300/553. train loss: 45.8299,	0.8279 s / batch. (data: 3.34e-04). ETA=10:52:06, max mem: 20.9 GB 
[11/25 10:44:20 visual_prompt]: 	Training 400/553. train loss: 3.9994,	0.9200 s / batch. (data: 5.37e-02). ETA=12:03:05, max mem: 20.9 GB 
[11/25 10:45:58 visual_prompt]: 	Training 500/553. train loss: 12.0135,	0.8312 s / batch. (data: 5.58e-03). ETA=10:51:54, max mem: 20.9 GB 
[11/25 10:46:48 visual_prompt]: Epoch 15 / 100: avg data time: 1.30e-01, avg batch time: 0.9605, average train loss: 42.6542
[11/25 10:47:43 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3094, average loss: 119.1599
[11/25 10:47:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.93	
[11/25 10:47:43 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/25 10:49:22 visual_prompt]: 	Training 100/553. train loss: 28.5121,	0.8350 s / batch. (data: 2.91e-04). ETA=10:52:47, max mem: 20.9 GB 
[11/25 10:50:57 visual_prompt]: 	Training 200/553. train loss: 4.4193,	0.8179 s / batch. (data: 3.02e-04). ETA=10:37:59, max mem: 20.9 GB 
[11/25 10:52:33 visual_prompt]: 	Training 300/553. train loss: 18.1549,	0.8558 s / batch. (data: 6.50e-04). ETA=11:06:08, max mem: 20.9 GB 
[11/25 10:54:08 visual_prompt]: 	Training 400/553. train loss: 8.0040,	0.8319 s / batch. (data: 1.56e-02). ETA=10:46:08, max mem: 20.9 GB 
[11/25 10:55:42 visual_prompt]: 	Training 500/553. train loss: 2.9585,	1.2879 s / batch. (data: 4.60e-01). ETA=16:38:14, max mem: 20.9 GB 
[11/25 10:56:33 visual_prompt]: Epoch 16 / 100: avg data time: 1.28e-01, avg batch time: 0.9588, average train loss: 32.4045
[11/25 10:57:27 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3116, average loss: 11.1458
[11/25 10:57:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.12	
[11/25 10:57:27 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/25 10:59:06 visual_prompt]: 	Training 100/553. train loss: 0.0228,	0.8432 s / batch. (data: 1.05e-02). ETA=10:51:22, max mem: 20.9 GB 
[11/25 11:00:43 visual_prompt]: 	Training 200/553. train loss: 133.8049,	0.8357 s / batch. (data: 5.34e-03). ETA=10:44:10, max mem: 20.9 GB 
[11/25 11:02:18 visual_prompt]: 	Training 300/553. train loss: 80.5348,	0.8480 s / batch. (data: 2.58e-04). ETA=10:52:19, max mem: 20.9 GB 
[11/25 11:03:54 visual_prompt]: 	Training 400/553. train loss: 0.2826,	1.0920 s / batch. (data: 2.54e-01). ETA=13:58:09, max mem: 20.9 GB 
[11/25 11:05:30 visual_prompt]: 	Training 500/553. train loss: 58.0921,	1.4932 s / batch. (data: 6.83e-01). ETA=19:03:33, max mem: 20.9 GB 
[11/25 11:06:21 visual_prompt]: Epoch 17 / 100: avg data time: 1.34e-01, avg batch time: 0.9651, average train loss: 34.7385
[11/25 11:07:16 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3113, average loss: 13.7672
[11/25 11:07:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 65.09	
[11/25 11:07:16 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/25 11:08:55 visual_prompt]: 	Training 100/553. train loss: 1.1119,	0.8281 s / batch. (data: 3.03e-04). ETA=10:32:05, max mem: 20.9 GB 
[11/25 11:10:34 visual_prompt]: 	Training 200/553. train loss: 7.7370,	0.8215 s / batch. (data: 2.47e-04). ETA=10:25:43, max mem: 20.9 GB 
[11/25 11:12:10 visual_prompt]: 	Training 300/553. train loss: 15.1899,	0.8200 s / batch. (data: 3.06e-04). ETA=10:23:09, max mem: 20.9 GB 
[11/25 11:13:46 visual_prompt]: 	Training 400/553. train loss: 19.8836,	0.8360 s / batch. (data: 4.75e-04). ETA=10:33:55, max mem: 20.9 GB 
[11/25 11:15:21 visual_prompt]: 	Training 500/553. train loss: 7.3998,	0.8402 s / batch. (data: 3.43e-04). ETA=10:35:42, max mem: 20.9 GB 
[11/25 11:16:11 visual_prompt]: Epoch 18 / 100: avg data time: 1.36e-01, avg batch time: 0.9665, average train loss: 38.3788
[11/25 11:17:05 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3094, average loss: 55.3526
[11/25 11:17:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.85	
[11/25 11:17:05 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/25 11:18:45 visual_prompt]: 	Training 100/553. train loss: 26.4222,	1.0070 s / batch. (data: 1.89e-01). ETA=12:39:21, max mem: 20.9 GB 
[11/25 11:20:21 visual_prompt]: 	Training 200/553. train loss: 3.7860,	0.8320 s / batch. (data: 5.41e-03). ETA=10:26:01, max mem: 20.9 GB 
[11/25 11:21:58 visual_prompt]: 	Training 300/553. train loss: 22.6057,	0.8213 s / batch. (data: 3.17e-04). ETA=10:16:36, max mem: 20.9 GB 
[11/25 11:23:35 visual_prompt]: 	Training 400/553. train loss: 12.2948,	0.8320 s / batch. (data: 7.65e-04). ETA=10:23:15, max mem: 20.9 GB 
[11/25 11:25:07 visual_prompt]: 	Training 500/553. train loss: 7.6475,	0.8503 s / batch. (data: 1.03e-02). ETA=10:35:32, max mem: 20.9 GB 
[11/25 11:25:57 visual_prompt]: Epoch 19 / 100: avg data time: 1.30e-01, avg batch time: 0.9619, average train loss: 27.2900
[11/25 11:26:52 visual_prompt]: Inference (val):avg data time: 3.21e-04, avg batch time: 0.3105, average loss: 93.1748
[11/25 11:26:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.80	
[11/25 11:26:52 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/25 11:28:30 visual_prompt]: 	Training 100/553. train loss: 39.9212,	0.8435 s / batch. (data: 2.29e-02). ETA=10:28:20, max mem: 20.9 GB 
[11/25 11:30:07 visual_prompt]: 	Training 200/553. train loss: 2.5359,	0.9015 s / batch. (data: 6.98e-02). ETA=11:10:02, max mem: 20.9 GB 
[11/25 11:31:43 visual_prompt]: 	Training 300/553. train loss: 85.2379,	0.8214 s / batch. (data: 2.98e-04). ETA=10:09:08, max mem: 20.9 GB 
[11/25 11:33:20 visual_prompt]: 	Training 400/553. train loss: 90.9195,	0.8299 s / batch. (data: 1.20e-02). ETA=10:14:00, max mem: 20.9 GB 
[11/25 11:34:55 visual_prompt]: 	Training 500/553. train loss: 29.1014,	0.8398 s / batch. (data: 1.05e-02). ETA=10:19:58, max mem: 20.9 GB 
[11/25 11:35:46 visual_prompt]: Epoch 20 / 100: avg data time: 1.35e-01, avg batch time: 0.9662, average train loss: 36.6560
[11/25 11:36:41 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3093, average loss: 14.7902
[11/25 11:36:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.40	
[11/25 11:36:41 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/25 11:38:22 visual_prompt]: 	Training 100/553. train loss: 19.7591,	0.8478 s / batch. (data: 2.46e-02). ETA=10:23:41, max mem: 20.9 GB 
[11/25 11:39:57 visual_prompt]: 	Training 200/553. train loss: 0.0096,	0.8175 s / batch. (data: 2.92e-04). ETA=10:00:01, max mem: 20.9 GB 
[11/25 11:41:32 visual_prompt]: 	Training 300/553. train loss: 136.8840,	1.0160 s / batch. (data: 1.67e-01). ETA=12:24:02, max mem: 20.9 GB 
[11/25 11:43:06 visual_prompt]: 	Training 400/553. train loss: 25.3831,	0.8394 s / batch. (data: 2.91e-04). ETA=10:13:20, max mem: 20.9 GB 
[11/25 11:44:44 visual_prompt]: 	Training 500/553. train loss: 31.8285,	0.8835 s / batch. (data: 1.55e-02). ETA=10:44:05, max mem: 20.9 GB 
[11/25 11:45:33 visual_prompt]: Epoch 21 / 100: avg data time: 1.30e-01, avg batch time: 0.9613, average train loss: 28.5649
[11/25 11:46:27 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3107, average loss: 14.4644
[11/25 11:46:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 67.94	
[11/25 11:46:27 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/25 11:48:05 visual_prompt]: 	Training 100/553. train loss: 26.2604,	0.8240 s / batch. (data: 2.94e-04). ETA=9:58:36, max mem: 20.9 GB 
[11/25 11:49:40 visual_prompt]: 	Training 200/553. train loss: 38.4190,	0.8320 s / batch. (data: 3.37e-04). ETA=10:02:59, max mem: 20.9 GB 
[11/25 11:51:13 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8397 s / batch. (data: 3.79e-04). ETA=10:07:10, max mem: 20.9 GB 
[11/25 11:52:49 visual_prompt]: 	Training 400/553. train loss: 8.0782,	0.8324 s / batch. (data: 2.91e-04). ETA=10:00:30, max mem: 20.9 GB 
[11/25 11:54:24 visual_prompt]: 	Training 500/553. train loss: 24.0354,	0.8240 s / batch. (data: 3.18e-04). ETA=9:53:06, max mem: 20.9 GB 
[11/25 11:55:15 visual_prompt]: Epoch 22 / 100: avg data time: 1.24e-01, avg batch time: 0.9550, average train loss: 31.6379
[11/25 11:56:09 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3125, average loss: 22.8661
[11/25 11:56:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.05	
[11/25 11:56:09 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/25 11:57:49 visual_prompt]: 	Training 100/553. train loss: 69.6765,	0.8160 s / batch. (data: 2.83e-04). ETA=9:45:15, max mem: 20.9 GB 
[11/25 11:59:24 visual_prompt]: 	Training 200/553. train loss: 26.6767,	0.8280 s / batch. (data: 2.96e-04). ETA=9:52:30, max mem: 20.9 GB 
[11/25 12:01:00 visual_prompt]: 	Training 300/553. train loss: 4.3607,	0.8600 s / batch. (data: 1.60e-02). ETA=10:13:56, max mem: 20.9 GB 
[11/25 12:02:33 visual_prompt]: 	Training 400/553. train loss: 16.4394,	0.8196 s / batch. (data: 1.06e-02). ETA=9:43:44, max mem: 20.9 GB 
[11/25 12:04:06 visual_prompt]: 	Training 500/553. train loss: 0.2407,	0.8520 s / batch. (data: 2.93e-04). ETA=10:05:23, max mem: 20.9 GB 
[11/25 12:04:56 visual_prompt]: Epoch 23 / 100: avg data time: 1.22e-01, avg batch time: 0.9530, average train loss: 26.8034
[11/25 12:05:50 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3104, average loss: 13.1462
[11/25 12:05:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.32	
[11/25 12:05:50 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/25 12:07:26 visual_prompt]: 	Training 100/553. train loss: 6.5169,	0.8273 s / batch. (data: 2.92e-04). ETA=9:45:43, max mem: 20.9 GB 
[11/25 12:09:00 visual_prompt]: 	Training 200/553. train loss: 11.6133,	0.8445 s / batch. (data: 1.20e-02). ETA=9:56:31, max mem: 20.9 GB 
[11/25 12:10:36 visual_prompt]: 	Training 300/553. train loss: 7.3411,	0.8187 s / batch. (data: 2.75e-04). ETA=9:36:55, max mem: 20.9 GB 
[11/25 12:12:11 visual_prompt]: 	Training 400/553. train loss: 23.0182,	0.8293 s / batch. (data: 7.95e-03). ETA=9:43:00, max mem: 20.9 GB 
[11/25 12:13:48 visual_prompt]: 	Training 500/553. train loss: 8.7479,	0.8556 s / batch. (data: 3.12e-04). ETA=10:00:02, max mem: 20.9 GB 
[11/25 12:14:38 visual_prompt]: Epoch 24 / 100: avg data time: 1.24e-01, avg batch time: 0.9545, average train loss: 27.4048
[11/25 12:15:32 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3096, average loss: 12.1995
[11/25 12:15:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 65.38	
[11/25 12:15:32 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/25 12:17:13 visual_prompt]: 	Training 100/553. train loss: 59.5628,	0.8256 s / batch. (data: 4.17e-04). ETA=9:36:56, max mem: 20.9 GB 
[11/25 12:18:45 visual_prompt]: 	Training 200/553. train loss: 14.7123,	1.0120 s / batch. (data: 1.78e-01). ETA=11:45:29, max mem: 20.9 GB 
[11/25 12:20:20 visual_prompt]: 	Training 300/553. train loss: 24.4865,	0.8473 s / batch. (data: 3.53e-04). ETA=9:49:15, max mem: 20.9 GB 
[11/25 12:21:54 visual_prompt]: 	Training 400/553. train loss: 0.6782,	1.0400 s / batch. (data: 2.00e-01). ETA=12:01:32, max mem: 20.9 GB 
[11/25 12:23:29 visual_prompt]: 	Training 500/553. train loss: 9.5160,	1.2562 s / batch. (data: 4.21e-01). ETA=14:29:29, max mem: 20.9 GB 
[11/25 12:24:19 visual_prompt]: Epoch 25 / 100: avg data time: 1.23e-01, avg batch time: 0.9544, average train loss: 27.0481
[11/25 12:25:13 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3124, average loss: 73.2002
[11/25 12:25:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.01	
[11/25 12:25:13 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/25 12:26:52 visual_prompt]: 	Training 100/553. train loss: 6.2010,	0.8280 s / batch. (data: 2.86e-04). ETA=9:30:58, max mem: 20.9 GB 
[11/25 12:28:27 visual_prompt]: 	Training 200/553. train loss: 49.9791,	1.4704 s / batch. (data: 6.41e-01). ETA=16:51:32, max mem: 20.9 GB 
[11/25 12:30:04 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8578 s / batch. (data: 3.37e-02). ETA=9:48:41, max mem: 20.9 GB 
[11/25 12:31:37 visual_prompt]: 	Training 400/553. train loss: 3.7007,	0.8307 s / batch. (data: 1.20e-02). ETA=9:28:40, max mem: 20.9 GB 
[11/25 12:33:11 visual_prompt]: 	Training 500/553. train loss: 2.9932,	0.8433 s / batch. (data: 1.13e-02). ETA=9:35:56, max mem: 20.9 GB 
[11/25 12:34:00 visual_prompt]: Epoch 26 / 100: avg data time: 1.21e-01, avg batch time: 0.9523, average train loss: 27.0992
[11/25 12:34:54 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3110, average loss: 12.7327
[11/25 12:34:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 70.10	
[11/25 12:34:54 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[11/25 12:36:33 visual_prompt]: 	Training 100/553. train loss: 9.6838,	0.8320 s / batch. (data: 2.90e-04). ETA=9:26:03, max mem: 20.9 GB 
[11/25 12:38:07 visual_prompt]: 	Training 200/553. train loss: 61.5466,	0.8266 s / batch. (data: 1.19e-02). ETA=9:21:00, max mem: 20.9 GB 
[11/25 12:39:43 visual_prompt]: 	Training 300/553. train loss: 41.0477,	0.8080 s / batch. (data: 3.51e-04). ETA=9:07:02, max mem: 20.9 GB 
[11/25 12:41:19 visual_prompt]: 	Training 400/553. train loss: 66.3537,	0.8111 s / batch. (data: 3.17e-04). ETA=9:07:46, max mem: 20.9 GB 
[11/25 12:42:54 visual_prompt]: 	Training 500/553. train loss: 35.7847,	0.8262 s / batch. (data: 2.91e-04). ETA=9:16:37, max mem: 20.9 GB 
[11/25 12:43:42 visual_prompt]: Epoch 27 / 100: avg data time: 1.24e-01, avg batch time: 0.9549, average train loss: 27.1342
[11/25 12:44:36 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3113, average loss: 86.1610
[11/25 12:44:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 68.93	
[11/25 12:44:36 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[11/25 12:46:31 visual_prompt]: 	Training 100/553. train loss: 76.4337,	0.8107 s / batch. (data: 3.20e-04). ETA=9:04:05, max mem: 20.9 GB 
[11/25 12:48:12 visual_prompt]: 	Training 200/553. train loss: 29.6996,	0.8480 s / batch. (data: 3.24e-04). ETA=9:27:43, max mem: 20.9 GB 
[11/25 12:49:51 visual_prompt]: 	Training 300/553. train loss: 10.3368,	1.4046 s / batch. (data: 5.65e-01). ETA=15:38:00, max mem: 20.9 GB 
[11/25 12:51:29 visual_prompt]: 	Training 400/553. train loss: 18.3533,	0.8376 s / batch. (data: 5.42e-03). ETA=9:17:59, max mem: 20.9 GB 
[11/25 12:53:07 visual_prompt]: 	Training 500/553. train loss: 108.3515,	0.8291 s / batch. (data: 1.19e-02). ETA=9:10:54, max mem: 20.9 GB 
[11/25 12:54:00 visual_prompt]: Epoch 28 / 100: avg data time: 1.91e-01, avg batch time: 1.0199, average train loss: 27.1525
[11/25 12:54:58 visual_prompt]: Inference (val):avg data time: 1.68e-04, avg batch time: 0.3096, average loss: 49.8572
[11/25 12:54:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 69.52	
[11/25 12:54:58 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[11/25 12:56:45 visual_prompt]: 	Training 100/553. train loss: 23.9725,	0.8368 s / batch. (data: 5.42e-03). ETA=9:13:54, max mem: 20.9 GB 
[11/25 12:58:22 visual_prompt]: 	Training 200/553. train loss: 3.4627,	1.7724 s / batch. (data: 9.47e-01). ETA=19:30:15, max mem: 20.9 GB 
[11/25 12:59:58 visual_prompt]: 	Training 300/553. train loss: 2.1530,	0.8431 s / batch. (data: 8.05e-04). ETA=9:15:14, max mem: 20.9 GB 
[11/25 13:01:32 visual_prompt]: 	Training 400/553. train loss: 68.9398,	1.3956 s / batch. (data: 5.90e-01). ETA=15:16:50, max mem: 20.9 GB 
[11/25 13:03:10 visual_prompt]: 	Training 500/553. train loss: 11.5358,	0.8281 s / batch. (data: 3.41e-04). ETA=9:02:38, max mem: 20.9 GB 
[11/25 13:04:00 visual_prompt]: Epoch 29 / 100: avg data time: 1.51e-01, avg batch time: 0.9815, average train loss: 33.5493
[11/25 13:04:56 visual_prompt]: Inference (val):avg data time: 2.21e-04, avg batch time: 0.3112, average loss: 43.7048
[11/25 13:04:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.65	
[11/25 13:04:56 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[11/25 13:06:37 visual_prompt]: 	Training 100/553. train loss: 17.2615,	0.8480 s / batch. (data: 5.45e-03). ETA=9:13:30, max mem: 20.9 GB 
[11/25 13:08:15 visual_prompt]: 	Training 200/553. train loss: 0.0042,	0.8320 s / batch. (data: 3.08e-04). ETA=9:01:40, max mem: 20.9 GB 
[11/25 13:09:50 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.1440 s / batch. (data: 3.00e-01). ETA=12:22:53, max mem: 20.9 GB 
[11/25 13:11:29 visual_prompt]: 	Training 400/553. train loss: 9.7025,	1.3470 s / batch. (data: 5.10e-01). ETA=14:32:27, max mem: 20.9 GB 
[11/25 13:13:07 visual_prompt]: 	Training 500/553. train loss: 17.4095,	1.1360 s / batch. (data: 3.09e-01). ETA=12:13:55, max mem: 20.9 GB 
[11/25 13:14:01 visual_prompt]: Epoch 30 / 100: avg data time: 1.53e-01, avg batch time: 0.9843, average train loss: 20.2606
[11/25 13:14:57 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3092, average loss: 9.6827
[11/25 13:14:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 69.78	
[11/25 13:14:57 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[11/25 13:16:40 visual_prompt]: 	Training 100/553. train loss: 0.8287,	0.8242 s / batch. (data: 1.20e-02). ETA=8:50:24, max mem: 20.9 GB 
[11/25 13:18:19 visual_prompt]: 	Training 200/553. train loss: 5.9134,	0.8400 s / batch. (data: 3.21e-04). ETA=8:59:08, max mem: 20.9 GB 
[11/25 13:19:53 visual_prompt]: 	Training 300/553. train loss: 25.1979,	0.8320 s / batch. (data: 1.20e-02). ETA=8:52:37, max mem: 20.9 GB 
[11/25 13:21:30 visual_prompt]: 	Training 400/553. train loss: 15.6313,	1.2052 s / batch. (data: 3.69e-01). ETA=12:49:31, max mem: 20.9 GB 
[11/25 13:23:08 visual_prompt]: 	Training 500/553. train loss: 9.3144,	0.8278 s / batch. (data: 2.64e-04). ETA=8:47:12, max mem: 20.9 GB 
[11/25 13:23:57 visual_prompt]: Epoch 31 / 100: avg data time: 1.46e-01, avg batch time: 0.9778, average train loss: 27.2723
[11/25 13:24:53 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3109, average loss: 9.3283
[11/25 13:24:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 70.34	
[11/25 13:24:53 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[11/25 13:26:35 visual_prompt]: 	Training 100/553. train loss: 0.0084,	0.8454 s / batch. (data: 1.42e-02). ETA=8:56:12, max mem: 20.9 GB 
[11/25 13:28:12 visual_prompt]: 	Training 200/553. train loss: 2.5889,	0.8528 s / batch. (data: 1.09e-02). ETA=8:59:29, max mem: 20.9 GB 
[11/25 13:29:51 visual_prompt]: 	Training 300/553. train loss: 21.6072,	0.8435 s / batch. (data: 1.16e-02). ETA=8:52:13, max mem: 20.9 GB 
[11/25 13:31:30 visual_prompt]: 	Training 400/553. train loss: 57.6236,	0.8315 s / batch. (data: 2.99e-04). ETA=8:43:13, max mem: 20.9 GB 
[11/25 13:33:05 visual_prompt]: 	Training 500/553. train loss: 11.1369,	0.8320 s / batch. (data: 3.04e-04). ETA=8:42:11, max mem: 20.9 GB 
[11/25 13:33:54 visual_prompt]: Epoch 32 / 100: avg data time: 1.45e-01, avg batch time: 0.9769, average train loss: 20.3501
[11/25 13:34:49 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3114, average loss: 23.9036
[11/25 13:34:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 70.46	
[11/25 13:34:49 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[11/25 13:36:29 visual_prompt]: 	Training 100/553. train loss: 92.1395,	0.8297 s / batch. (data: 1.20e-02). ETA=8:38:38, max mem: 20.9 GB 
[11/25 13:38:08 visual_prompt]: 	Training 200/553. train loss: 27.5268,	1.1500 s / batch. (data: 3.33e-01). ETA=11:56:55, max mem: 20.9 GB 
[11/25 13:39:44 visual_prompt]: 	Training 300/553. train loss: 9.8688,	0.8664 s / batch. (data: 2.84e-04). ETA=8:58:38, max mem: 20.9 GB 
[11/25 13:41:22 visual_prompt]: 	Training 400/553. train loss: 1.8637,	0.8314 s / batch. (data: 5.42e-03). ETA=8:35:31, max mem: 20.9 GB 
[11/25 13:42:59 visual_prompt]: 	Training 500/553. train loss: 20.8694,	0.8231 s / batch. (data: 3.23e-04). ETA=8:28:59, max mem: 20.9 GB 
[11/25 13:43:50 visual_prompt]: Epoch 33 / 100: avg data time: 1.48e-01, avg batch time: 0.9780, average train loss: 33.1611
[11/25 13:44:45 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3103, average loss: 10.2849
[11/25 13:44:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 70.90	
[11/25 13:44:45 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[11/25 13:46:27 visual_prompt]: 	Training 100/553. train loss: 16.4824,	0.8269 s / batch. (data: 3.46e-04). ETA=8:29:15, max mem: 20.9 GB 
[11/25 13:48:02 visual_prompt]: 	Training 200/553. train loss: 57.6707,	0.8238 s / batch. (data: 1.20e-02). ETA=8:25:56, max mem: 20.9 GB 
[11/25 13:49:39 visual_prompt]: 	Training 300/553. train loss: 1.1536,	0.8714 s / batch. (data: 3.31e-04). ETA=8:53:44, max mem: 20.9 GB 
[11/25 13:51:17 visual_prompt]: 	Training 400/553. train loss: 7.4392,	0.8293 s / batch. (data: 3.01e-04). ETA=8:26:35, max mem: 20.9 GB 
[11/25 13:52:54 visual_prompt]: 	Training 500/553. train loss: 8.9543,	1.4220 s / batch. (data: 5.90e-01). ETA=14:26:13, max mem: 20.9 GB 
[11/25 13:53:44 visual_prompt]: Epoch 34 / 100: avg data time: 1.42e-01, avg batch time: 0.9732, average train loss: 27.6327
[11/25 13:54:39 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3113, average loss: 16.1818
[11/25 13:54:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 71.22	
[11/25 13:54:39 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[11/25 13:56:22 visual_prompt]: 	Training 100/553. train loss: 42.2535,	0.8480 s / batch. (data: 3.46e-04). ETA=8:34:25, max mem: 20.9 GB 
[11/25 13:58:00 visual_prompt]: 	Training 200/553. train loss: 33.6156,	0.8404 s / batch. (data: 3.17e-04). ETA=8:28:26, max mem: 20.9 GB 
[11/25 13:59:36 visual_prompt]: 	Training 300/553. train loss: 25.7727,	0.8210 s / batch. (data: 3.18e-04). ETA=8:15:17, max mem: 20.9 GB 
[11/25 14:01:12 visual_prompt]: 	Training 400/553. train loss: 29.7589,	0.8200 s / batch. (data: 3.29e-04). ETA=8:13:19, max mem: 20.9 GB 
[11/25 14:02:48 visual_prompt]: 	Training 500/553. train loss: 11.0623,	1.0943 s / batch. (data: 2.62e-01). ETA=10:56:32, max mem: 20.9 GB 
[11/25 14:03:39 visual_prompt]: Epoch 35 / 100: avg data time: 1.45e-01, avg batch time: 0.9759, average train loss: 22.2768
[11/25 14:04:34 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3105, average loss: 47.3933
[11/25 14:04:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 70.17	
[11/25 14:04:34 visual_prompt]: Stopping early.
[11/25 14:04:35 visual_prompt]: Rank of current process: 0. World size: 1
[11/25 14:04:35 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/25 14:04:35 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/25 14:04:35 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/25 14:04:35 visual_prompt]: Training with config:
[11/25 14:04:35 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/25 14:04:35 visual_prompt]: Loading training data...
[11/25 14:04:35 visual_prompt]: Constructing mammo-cbis dataset train...
[11/25 14:04:35 visual_prompt]: Loading validation data...
[11/25 14:04:35 visual_prompt]: Constructing mammo-cbis dataset val...
[11/25 14:04:35 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/25 14:04:40 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/25 14:04:40 visual_prompt]: tuned percent:0.525
[11/25 14:04:40 visual_prompt]: Device used for model: 0
[11/25 14:04:40 visual_prompt]: Setting up Evaluator...
[11/25 14:04:40 visual_prompt]: Setting up Trainer...
[11/25 14:04:40 visual_prompt]: 	Setting up the optimizer...
[11/25 14:04:40 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/25 14:06:21 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8266 s / batch. (data: 1.05e-02). ETA=12:40:25, max mem: 20.9 GB 
[11/25 14:07:56 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8440 s / batch. (data: 1.20e-02). ETA=12:55:05, max mem: 20.9 GB 
[11/25 14:09:36 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.5474 s / batch. (data: 6.86e-01). ETA=23:38:25, max mem: 20.9 GB 
[11/25 14:11:11 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8320 s / batch. (data: 3.10e-04). ETA=12:41:14, max mem: 20.9 GB 
[11/25 14:12:49 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8475 s / batch. (data: 7.20e-04). ETA=12:54:01, max mem: 20.9 GB 
[11/25 14:13:41 visual_prompt]: Epoch 1 / 100: avg data time: 1.43e-01, avg batch time: 0.9769, average train loss: 1.5403
[11/25 14:14:36 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3096, average loss: 1.5201
[11/25 14:14:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/25 14:14:36 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/25 14:16:18 visual_prompt]: 	Training 100/553. train loss: 2.9190,	0.8339 s / batch. (data: 3.28e-04). ETA=12:39:31, max mem: 20.9 GB 
[11/25 14:17:54 visual_prompt]: 	Training 200/553. train loss: 0.0037,	1.1952 s / batch. (data: 3.67e-01). ETA=18:06:33, max mem: 20.9 GB 
[11/25 14:19:32 visual_prompt]: 	Training 300/553. train loss: 7.8096,	0.8973 s / batch. (data: 7.27e-02). ETA=13:34:16, max mem: 20.9 GB 
[11/25 14:21:08 visual_prompt]: 	Training 400/553. train loss: 0.6231,	0.8504 s / batch. (data: 3.03e-04). ETA=12:50:16, max mem: 20.9 GB 
[11/25 14:22:46 visual_prompt]: 	Training 500/553. train loss: 3.2929,	0.8207 s / batch. (data: 3.13e-04). ETA=12:22:00, max mem: 20.9 GB 
[11/25 14:23:35 visual_prompt]: Epoch 2 / 100: avg data time: 1.40e-01, avg batch time: 0.9742, average train loss: 3.4209
[11/25 14:24:30 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3121, average loss: 19.1750
[11/25 14:24:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.67	
[11/25 14:24:30 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/25 14:26:10 visual_prompt]: 	Training 100/553. train loss: 0.7196,	0.8360 s / batch. (data: 1.20e-02). ETA=12:33:42, max mem: 20.9 GB 
[11/25 14:27:48 visual_prompt]: 	Training 200/553. train loss: 1.8945,	1.0363 s / batch. (data: 2.08e-01). ETA=15:32:34, max mem: 20.9 GB 
[11/25 14:29:24 visual_prompt]: 	Training 300/553. train loss: 4.1691,	0.8423 s / batch. (data: 3.13e-04). ETA=12:36:32, max mem: 20.9 GB 
[11/25 14:31:02 visual_prompt]: 	Training 400/553. train loss: 55.2133,	0.8321 s / batch. (data: 3.08e-04). ETA=12:25:59, max mem: 20.9 GB 
[11/25 14:32:40 visual_prompt]: 	Training 500/553. train loss: 7.2534,	1.2394 s / batch. (data: 4.21e-01). ETA=18:29:08, max mem: 20.9 GB 
[11/25 14:33:30 visual_prompt]: Epoch 3 / 100: avg data time: 1.43e-01, avg batch time: 0.9762, average train loss: 6.8817
[11/25 14:34:26 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3094, average loss: 4.1820
[11/25 14:34:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.68	
[11/25 14:34:26 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/25 14:36:08 visual_prompt]: 	Training 100/553. train loss: 20.7751,	0.8260 s / batch. (data: 2.44e-04). ETA=12:17:02, max mem: 20.9 GB 
[11/25 14:37:46 visual_prompt]: 	Training 200/553. train loss: 2.4443,	0.8284 s / batch. (data: 5.44e-03). ETA=12:17:51, max mem: 20.9 GB 
[11/25 14:39:23 visual_prompt]: 	Training 300/553. train loss: 2.6300,	1.2156 s / batch. (data: 3.86e-01). ETA=18:00:41, max mem: 20.9 GB 
[11/25 14:40:55 visual_prompt]: 	Training 400/553. train loss: 5.1956,	1.3393 s / batch. (data: 5.14e-01). ETA=19:48:25, max mem: 20.9 GB 
[11/25 14:42:34 visual_prompt]: 	Training 500/553. train loss: 49.1465,	3.3321 s / batch. (data: 2.51e+00). ETA=2 days, 1:11:13, max mem: 20.9 GB 
[11/25 14:43:26 visual_prompt]: Epoch 4 / 100: avg data time: 1.43e-01, avg batch time: 0.9770, average train loss: 9.3442
[11/25 14:44:21 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3112, average loss: 2.7230
[11/25 14:44:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.14	
[11/25 14:44:21 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/25 14:46:01 visual_prompt]: 	Training 100/553. train loss: 0.3351,	0.8709 s / batch. (data: 1.49e-02). ETA=12:49:08, max mem: 20.9 GB 
[11/25 14:47:37 visual_prompt]: 	Training 200/553. train loss: 12.2037,	1.1255 s / batch. (data: 2.80e-01). ETA=16:32:07, max mem: 20.9 GB 
[11/25 14:49:15 visual_prompt]: 	Training 300/553. train loss: 47.6977,	0.8240 s / batch. (data: 4.23e-04). ETA=12:04:56, max mem: 20.9 GB 
[11/25 14:50:51 visual_prompt]: 	Training 400/553. train loss: 16.4583,	0.8429 s / batch. (data: 1.04e-02). ETA=12:20:11, max mem: 20.9 GB 
[11/25 14:52:29 visual_prompt]: 	Training 500/553. train loss: 4.7859,	0.8471 s / batch. (data: 3.06e-04). ETA=12:22:28, max mem: 20.9 GB 
[11/25 14:53:20 visual_prompt]: Epoch 5 / 100: avg data time: 1.40e-01, avg batch time: 0.9743, average train loss: 13.9010
[11/25 14:54:16 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3095, average loss: 92.5483
[11/25 14:54:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.76	
[11/25 14:54:16 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/25 14:55:59 visual_prompt]: 	Training 100/553. train loss: 1.8484,	0.8280 s / batch. (data: 2.93e-04). ETA=12:03:36, max mem: 20.9 GB 
[11/25 14:57:35 visual_prompt]: 	Training 200/553. train loss: 11.7367,	0.8400 s / batch. (data: 5.43e-03). ETA=12:12:41, max mem: 20.9 GB 
[11/25 14:59:11 visual_prompt]: 	Training 300/553. train loss: 3.2738,	0.8376 s / batch. (data: 3.15e-04). ETA=12:09:11, max mem: 20.9 GB 
[11/25 15:00:53 visual_prompt]: 	Training 400/553. train loss: 8.4288,	0.8614 s / batch. (data: 4.82e-03). ETA=12:28:27, max mem: 20.9 GB 
[11/25 15:02:29 visual_prompt]: 	Training 500/553. train loss: 2.4603,	0.8360 s / batch. (data: 3.50e-04). ETA=12:05:01, max mem: 20.9 GB 
[11/25 15:03:19 visual_prompt]: Epoch 6 / 100: avg data time: 1.49e-01, avg batch time: 0.9819, average train loss: 21.8691
[11/25 15:04:14 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.3108, average loss: 2.3365
[11/25 15:04:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.85	
[11/25 15:04:14 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/25 15:05:54 visual_prompt]: 	Training 100/553. train loss: 13.2510,	0.8520 s / batch. (data: 3.35e-04). ETA=12:16:45, max mem: 20.9 GB 
[11/25 15:07:31 visual_prompt]: 	Training 200/553. train loss: 23.3658,	0.8440 s / batch. (data: 3.28e-04). ETA=12:08:24, max mem: 20.9 GB 
[11/25 15:09:12 visual_prompt]: 	Training 300/553. train loss: 12.4615,	1.7143 s / batch. (data: 8.81e-01). ETA=1 day, 0:36:38, max mem: 20.9 GB 
[11/25 15:10:50 visual_prompt]: 	Training 400/553. train loss: 9.6258,	1.9080 s / batch. (data: 1.08e+00). ETA=1 day, 3:20:17, max mem: 20.9 GB 
[11/25 15:12:26 visual_prompt]: 	Training 500/553. train loss: 4.9146,	0.8404 s / batch. (data: 1.20e-02). ETA=12:01:05, max mem: 20.9 GB 
[11/25 15:13:15 visual_prompt]: Epoch 7 / 100: avg data time: 1.44e-01, avg batch time: 0.9777, average train loss: 20.0644
[11/25 15:14:11 visual_prompt]: Inference (val):avg data time: 2.16e-04, avg batch time: 0.3122, average loss: 8.4642
[11/25 15:14:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.60	
[11/25 15:14:11 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/25 15:15:50 visual_prompt]: 	Training 100/553. train loss: 9.8224,	0.8598 s / batch. (data: 1.05e-02). ETA=12:15:33, max mem: 20.9 GB 
[11/25 15:17:29 visual_prompt]: 	Training 200/553. train loss: 39.4100,	0.8360 s / batch. (data: 3.30e-04). ETA=11:53:47, max mem: 20.9 GB 
[11/25 15:19:06 visual_prompt]: 	Training 300/553. train loss: 13.7431,	0.8257 s / batch. (data: 2.96e-04). ETA=11:43:37, max mem: 20.9 GB 
[11/25 15:20:43 visual_prompt]: 	Training 400/553. train loss: 51.2663,	0.8269 s / batch. (data: 1.05e-02). ETA=11:43:16, max mem: 20.9 GB 
[11/25 15:22:20 visual_prompt]: 	Training 500/553. train loss: 139.0039,	1.3386 s / batch. (data: 5.07e-01). ETA=18:56:12, max mem: 20.9 GB 
[11/25 15:23:11 visual_prompt]: Epoch 8 / 100: avg data time: 1.44e-01, avg batch time: 0.9765, average train loss: 25.8392
[11/25 15:24:06 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3092, average loss: 7.0291
[11/25 15:24:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.43	
[11/25 15:24:06 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/25 15:25:46 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8600 s / batch. (data: 3.02e-04). ETA=12:07:44, max mem: 20.9 GB 
[11/25 15:27:23 visual_prompt]: 	Training 200/553. train loss: 13.8168,	0.8440 s / batch. (data: 3.14e-04). ETA=11:52:51, max mem: 20.9 GB 
[11/25 15:29:01 visual_prompt]: 	Training 300/553. train loss: 3.5134,	1.6997 s / batch. (data: 8.68e-01). ETA=23:52:42, max mem: 20.9 GB 
[11/25 15:30:39 visual_prompt]: 	Training 400/553. train loss: 10.2903,	0.8360 s / batch. (data: 8.13e-04). ETA=11:43:18, max mem: 20.9 GB 
[11/25 15:32:16 visual_prompt]: 	Training 500/553. train loss: 21.3491,	0.8221 s / batch. (data: 3.37e-04). ETA=11:30:12, max mem: 20.9 GB 
[11/25 15:33:06 visual_prompt]: Epoch 9 / 100: avg data time: 1.44e-01, avg batch time: 0.9769, average train loss: 23.0162
[11/25 15:34:02 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3099, average loss: 42.8388
[11/25 15:34:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.46	
[11/25 15:34:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/25 15:35:45 visual_prompt]: 	Training 100/553. train loss: 12.5214,	0.8166 s / batch. (data: 3.06e-04). ETA=11:23:29, max mem: 20.9 GB 
[11/25 15:37:21 visual_prompt]: 	Training 200/553. train loss: 6.6046,	0.8280 s / batch. (data: 3.24e-04). ETA=11:31:41, max mem: 20.9 GB 
[11/25 15:38:58 visual_prompt]: 	Training 300/553. train loss: 6.6750,	2.0432 s / batch. (data: 1.22e+00). ETA=1 day, 4:23:26, max mem: 20.9 GB 
[11/25 15:40:33 visual_prompt]: 	Training 400/553. train loss: 5.2920,	0.8906 s / batch. (data: 5.43e-03). ETA=12:21:01, max mem: 20.9 GB 
[11/25 15:42:12 visual_prompt]: 	Training 500/553. train loss: 19.9925,	0.8320 s / batch. (data: 3.20e-04). ETA=11:30:53, max mem: 20.9 GB 
[11/25 15:43:03 visual_prompt]: Epoch 10 / 100: avg data time: 1.45e-01, avg batch time: 0.9781, average train loss: 30.4111
[11/25 15:43:58 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3086, average loss: 0.9898
[11/25 15:43:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.75	
[11/25 15:43:58 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/25 15:45:41 visual_prompt]: 	Training 100/553. train loss: 15.8778,	0.8440 s / batch. (data: 2.88e-04). ETA=11:38:41, max mem: 20.9 GB 
[11/25 15:47:20 visual_prompt]: 	Training 200/553. train loss: 24.1415,	0.8393 s / batch. (data: 2.98e-04). ETA=11:33:22, max mem: 20.9 GB 
[11/25 15:48:57 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.0113 s / batch. (data: 1.17e+00). ETA=1 day, 3:38:19, max mem: 20.9 GB 
[11/25 15:50:33 visual_prompt]: 	Training 400/553. train loss: 43.0872,	0.8402 s / batch. (data: 5.92e-03). ETA=11:31:20, max mem: 20.9 GB 
[11/25 15:52:09 visual_prompt]: 	Training 500/553. train loss: 45.0347,	0.8170 s / batch. (data: 2.67e-04). ETA=11:10:55, max mem: 20.9 GB 
[11/25 15:52:59 visual_prompt]: Epoch 11 / 100: avg data time: 1.45e-01, avg batch time: 0.9772, average train loss: 34.2581
[11/25 15:53:54 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3089, average loss: 21.7759
[11/25 15:53:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.63	
[11/25 15:53:54 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/25 15:55:36 visual_prompt]: 	Training 100/553. train loss: 39.2219,	0.8247 s / batch. (data: 2.50e-04). ETA=11:15:08, max mem: 20.9 GB 
[11/25 15:57:14 visual_prompt]: 	Training 200/553. train loss: 7.6297,	0.8400 s / batch. (data: 3.13e-04). ETA=11:26:14, max mem: 20.9 GB 
[11/25 15:58:49 visual_prompt]: 	Training 300/553. train loss: 12.9869,	0.8386 s / batch. (data: 1.56e-02). ETA=11:23:43, max mem: 20.9 GB 
[11/25 16:00:26 visual_prompt]: 	Training 400/553. train loss: 20.0534,	0.8230 s / batch. (data: 3.27e-04). ETA=11:09:37, max mem: 20.9 GB 
[11/25 16:02:03 visual_prompt]: 	Training 500/553. train loss: 7.4051,	0.8634 s / batch. (data: 8.34e-04). ETA=11:41:04, max mem: 20.9 GB 
[11/25 16:02:53 visual_prompt]: Epoch 12 / 100: avg data time: 1.43e-01, avg batch time: 0.9749, average train loss: 34.2184
[11/25 16:03:49 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3106, average loss: 33.8616
[11/25 16:03:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.16	
[11/25 16:03:49 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/25 16:05:32 visual_prompt]: 	Training 100/553. train loss: 18.5837,	0.8480 s / batch. (data: 3.42e-04). ETA=11:26:22, max mem: 20.9 GB 
[11/25 16:07:06 visual_prompt]: 	Training 200/553. train loss: 119.0935,	0.8283 s / batch. (data: 3.22e-04). ETA=11:09:04, max mem: 20.9 GB 
[11/25 16:08:45 visual_prompt]: 	Training 300/553. train loss: 21.8950,	1.6888 s / batch. (data: 8.53e-01). ETA=22:41:18, max mem: 20.9 GB 
[11/25 16:10:21 visual_prompt]: 	Training 400/553. train loss: 13.5954,	0.8292 s / batch. (data: 9.15e-03). ETA=11:07:00, max mem: 20.9 GB 
[11/25 16:11:59 visual_prompt]: 	Training 500/553. train loss: 20.9357,	0.8771 s / batch. (data: 5.43e-03). ETA=11:44:04, max mem: 20.9 GB 
[11/25 16:12:49 visual_prompt]: Epoch 13 / 100: avg data time: 1.45e-01, avg batch time: 0.9773, average train loss: 36.6054
[11/25 16:13:45 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3110, average loss: 12.1257
[11/25 16:13:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.44	
[11/25 16:13:45 visual_prompt]: Best epoch 13: best metric: -12.126
[11/25 16:13:45 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/25 16:15:27 visual_prompt]: 	Training 100/553. train loss: 1.8615,	0.8321 s / batch. (data: 1.57e-02). ETA=11:05:48, max mem: 20.9 GB 
[11/25 16:17:04 visual_prompt]: 	Training 200/553. train loss: 6.4021,	0.8880 s / batch. (data: 5.78e-02). ETA=11:49:05, max mem: 20.9 GB 
[11/25 16:18:42 visual_prompt]: 	Training 300/553. train loss: 68.5546,	0.8253 s / batch. (data: 9.25e-03). ETA=10:57:37, max mem: 20.9 GB 
[11/25 16:20:17 visual_prompt]: 	Training 400/553. train loss: 5.1330,	0.8297 s / batch. (data: 5.44e-03). ETA=10:59:46, max mem: 20.9 GB 
[11/25 16:21:54 visual_prompt]: 	Training 500/553. train loss: 9.8359,	0.8190 s / batch. (data: 3.02e-04). ETA=10:49:52, max mem: 20.9 GB 
[11/25 16:22:45 visual_prompt]: Epoch 14 / 100: avg data time: 1.44e-01, avg batch time: 0.9764, average train loss: 32.7403
[11/25 16:23:40 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3116, average loss: 76.2857
[11/25 16:23:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.90	
[11/25 16:23:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/25 16:25:22 visual_prompt]: 	Training 100/553. train loss: 58.4551,	0.8282 s / batch. (data: 1.56e-02). ETA=10:55:06, max mem: 20.9 GB 
[11/25 16:26:57 visual_prompt]: 	Training 200/553. train loss: 188.4184,	0.8360 s / batch. (data: 2.91e-04). ETA=10:59:51, max mem: 20.9 GB 
[11/25 16:28:36 visual_prompt]: 	Training 300/553. train loss: 24.2248,	0.8451 s / batch. (data: 7.01e-04). ETA=11:05:37, max mem: 20.9 GB 
[11/25 16:30:11 visual_prompt]: 	Training 400/553. train loss: 5.2649,	1.1640 s / batch. (data: 2.92e-01). ETA=15:14:50, max mem: 20.9 GB 
[11/25 16:31:48 visual_prompt]: 	Training 500/553. train loss: 15.9856,	0.8255 s / batch. (data: 3.34e-04). ETA=10:47:26, max mem: 20.9 GB 
[11/25 16:32:39 visual_prompt]: Epoch 15 / 100: avg data time: 1.43e-01, avg batch time: 0.9746, average train loss: 36.2378
[11/25 16:33:34 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3123, average loss: 21.1190
[11/25 16:33:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.29	
[11/25 16:33:34 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/25 16:35:14 visual_prompt]: 	Training 100/553. train loss: 136.7389,	0.8357 s / batch. (data: 3.82e-04). ETA=10:53:17, max mem: 20.9 GB 
[11/25 16:36:51 visual_prompt]: 	Training 200/553. train loss: 59.3486,	0.8440 s / batch. (data: 2.93e-04). ETA=10:58:22, max mem: 20.9 GB 
[11/25 16:38:28 visual_prompt]: 	Training 300/553. train loss: 30.4218,	0.8552 s / batch. (data: 1.05e-02). ETA=11:05:44, max mem: 20.9 GB 
[11/25 16:40:05 visual_prompt]: 	Training 400/553. train loss: 17.3393,	0.8272 s / batch. (data: 5.43e-03). ETA=10:42:33, max mem: 20.9 GB 
[11/25 16:41:41 visual_prompt]: 	Training 500/553. train loss: 11.4081,	0.8467 s / batch. (data: 1.05e-02). ETA=10:56:15, max mem: 20.9 GB 
[11/25 16:42:32 visual_prompt]: Epoch 16 / 100: avg data time: 1.39e-01, avg batch time: 0.9717, average train loss: 32.5930
[11/25 16:43:27 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3118, average loss: 44.3229
[11/25 16:43:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.94	
[11/25 16:43:27 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/25 16:45:08 visual_prompt]: 	Training 100/553. train loss: 55.8716,	0.8320 s / batch. (data: 2.91e-04). ETA=10:42:45, max mem: 20.9 GB 
[11/25 16:46:46 visual_prompt]: 	Training 200/553. train loss: 54.9315,	0.8321 s / batch. (data: 2.83e-04). ETA=10:41:25, max mem: 20.9 GB 
[11/25 16:48:23 visual_prompt]: 	Training 300/553. train loss: 15.8055,	0.8488 s / batch. (data: 2.40e-02). ETA=10:52:51, max mem: 20.9 GB 
[11/25 16:50:00 visual_prompt]: 	Training 400/553. train loss: 2.8833,	1.0400 s / batch. (data: 1.86e-01). ETA=13:18:14, max mem: 20.9 GB 
[11/25 16:51:37 visual_prompt]: 	Training 500/553. train loss: 5.9224,	1.3881 s / batch. (data: 5.69e-01). ETA=17:43:04, max mem: 20.9 GB 
[11/25 16:52:28 visual_prompt]: Epoch 17 / 100: avg data time: 1.46e-01, avg batch time: 0.9783, average train loss: 31.9066
[11/25 16:53:26 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3098, average loss: 22.5387
[11/25 16:53:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.52	
[11/25 16:53:26 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/25 16:55:12 visual_prompt]: 	Training 100/553. train loss: 33.3468,	0.8165 s / batch. (data: 6.96e-04). ETA=10:23:16, max mem: 20.9 GB 
[11/25 16:57:07 visual_prompt]: 	Training 200/553. train loss: 15.3985,	0.8316 s / batch. (data: 5.90e-03). ETA=10:33:22, max mem: 20.9 GB 
[11/25 16:58:46 visual_prompt]: 	Training 300/553. train loss: 36.3336,	0.8670 s / batch. (data: 3.10e-04). ETA=10:58:56, max mem: 20.9 GB 
[11/25 17:00:25 visual_prompt]: 	Training 400/553. train loss: 26.4739,	0.8473 s / batch. (data: 1.19e-02). ETA=10:42:29, max mem: 20.9 GB 
[11/25 17:02:02 visual_prompt]: 	Training 500/553. train loss: 26.0536,	1.0160 s / batch. (data: 1.81e-01). ETA=12:48:44, max mem: 20.9 GB 
[11/25 17:03:02 visual_prompt]: Epoch 18 / 100: avg data time: 2.11e-01, avg batch time: 1.0417, average train loss: 32.3624
[11/25 17:03:58 visual_prompt]: Inference (val):avg data time: 6.07e-04, avg batch time: 0.3083, average loss: 28.3415
[11/25 17:03:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.76	
[11/25 17:03:58 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/25 17:05:39 visual_prompt]: 	Training 100/553. train loss: 73.0952,	0.8222 s / batch. (data: 3.24e-04). ETA=10:20:03, max mem: 20.9 GB 
[11/25 17:07:16 visual_prompt]: 	Training 200/553. train loss: 18.0639,	0.8211 s / batch. (data: 2.96e-04). ETA=10:17:50, max mem: 20.9 GB 
[11/25 17:08:54 visual_prompt]: 	Training 300/553. train loss: 0.0051,	0.8587 s / batch. (data: 3.50e-02). ETA=10:44:41, max mem: 20.9 GB 
[11/25 17:10:33 visual_prompt]: 	Training 400/553. train loss: 8.9777,	0.8344 s / batch. (data: 2.99e-04). ETA=10:25:02, max mem: 20.9 GB 
[11/25 17:12:06 visual_prompt]: 	Training 500/553. train loss: 29.4289,	0.8239 s / batch. (data: 5.42e-03). ETA=10:15:48, max mem: 20.9 GB 
[11/25 17:12:57 visual_prompt]: Epoch 19 / 100: avg data time: 1.42e-01, avg batch time: 0.9741, average train loss: 37.7777
[11/25 17:13:52 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3114, average loss: 163.5188
[11/25 17:13:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.06	
[11/25 17:13:52 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/25 17:15:31 visual_prompt]: 	Training 100/553. train loss: 11.4015,	0.8480 s / batch. (data: 7.95e-03). ETA=10:31:38, max mem: 20.9 GB 
[11/25 17:17:11 visual_prompt]: 	Training 200/553. train loss: 34.3164,	0.8078 s / batch. (data: 3.31e-04). ETA=10:00:21, max mem: 20.9 GB 
[11/25 17:18:48 visual_prompt]: 	Training 300/553. train loss: 34.0807,	0.8440 s / batch. (data: 2.98e-04). ETA=10:25:51, max mem: 20.9 GB 
[11/25 17:20:26 visual_prompt]: 	Training 400/553. train loss: 1.3187,	0.8286 s / batch. (data: 3.39e-04). ETA=10:13:03, max mem: 20.9 GB 
[11/25 17:22:02 visual_prompt]: 	Training 500/553. train loss: 16.2321,	0.8360 s / batch. (data: 3.07e-04). ETA=10:17:07, max mem: 20.9 GB 
[11/25 17:22:54 visual_prompt]: Epoch 20 / 100: avg data time: 1.47e-01, avg batch time: 0.9792, average train loss: 33.0191
[11/25 17:23:49 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3099, average loss: 21.6157
[11/25 17:23:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.95	
[11/25 17:23:49 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/25 17:25:34 visual_prompt]: 	Training 100/553. train loss: 13.6234,	0.8154 s / batch. (data: 2.68e-04). ETA=9:59:51, max mem: 20.9 GB 
[11/25 17:27:10 visual_prompt]: 	Training 200/553. train loss: 70.9600,	0.8208 s / batch. (data: 5.44e-03). ETA=10:02:28, max mem: 20.9 GB 
[11/25 17:28:46 visual_prompt]: 	Training 300/553. train loss: 131.5904,	1.0255 s / batch. (data: 2.01e-01). ETA=12:31:00, max mem: 20.9 GB 
[11/25 17:30:22 visual_prompt]: 	Training 400/553. train loss: 4.7542,	0.8202 s / batch. (data: 3.04e-04). ETA=9:59:17, max mem: 20.9 GB 
[11/25 17:32:01 visual_prompt]: 	Training 500/553. train loss: 5.1656,	0.8318 s / batch. (data: 5.42e-03). ETA=10:06:22, max mem: 20.9 GB 
[11/25 17:32:51 visual_prompt]: Epoch 21 / 100: avg data time: 1.47e-01, avg batch time: 0.9786, average train loss: 32.3908
[11/25 17:33:46 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3088, average loss: 59.2093
[11/25 17:33:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.51	
[11/25 17:33:46 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/25 17:35:26 visual_prompt]: 	Training 100/553. train loss: 74.4270,	0.8520 s / batch. (data: 3.06e-04). ETA=10:18:55, max mem: 20.9 GB 
[11/25 17:37:03 visual_prompt]: 	Training 200/553. train loss: 1.8805,	0.8381 s / batch. (data: 1.05e-02). ETA=10:07:26, max mem: 20.9 GB 
[11/25 17:38:38 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8292 s / batch. (data: 3.17e-04). ETA=9:59:38, max mem: 20.9 GB 
[11/25 17:40:17 visual_prompt]: 	Training 400/553. train loss: 60.2997,	0.8359 s / batch. (data: 3.05e-04). ETA=10:03:04, max mem: 20.9 GB 
[11/25 17:41:55 visual_prompt]: 	Training 500/553. train loss: 14.4501,	0.8552 s / batch. (data: 3.21e-04). ETA=10:15:33, max mem: 20.9 GB 
[11/25 17:42:46 visual_prompt]: Epoch 22 / 100: avg data time: 1.45e-01, avg batch time: 0.9776, average train loss: 31.3885
[11/25 17:43:42 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3129, average loss: 37.5095
[11/25 17:43:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.10	
[11/25 17:43:42 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/25 17:45:25 visual_prompt]: 	Training 100/553. train loss: 14.7998,	0.8294 s / batch. (data: 5.44e-03). ETA=9:54:51, max mem: 20.9 GB 
[11/25 17:47:03 visual_prompt]: 	Training 200/553. train loss: 26.9610,	0.8508 s / batch. (data: 1.56e-02). ETA=10:08:46, max mem: 20.9 GB 
[11/25 17:48:42 visual_prompt]: 	Training 300/553. train loss: 38.7181,	0.8594 s / batch. (data: 5.84e-03). ETA=10:13:31, max mem: 20.9 GB 
[11/25 17:50:17 visual_prompt]: 	Training 400/553. train loss: 20.7494,	0.8440 s / batch. (data: 7.62e-04). ETA=10:01:08, max mem: 20.9 GB 
[11/25 17:51:53 visual_prompt]: 	Training 500/553. train loss: 2.8258,	0.8238 s / batch. (data: 2.85e-04). ETA=9:45:24, max mem: 20.9 GB 
[11/25 17:52:44 visual_prompt]: Epoch 23 / 100: avg data time: 1.48e-01, avg batch time: 0.9794, average train loss: 30.7184
[11/25 17:53:39 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3097, average loss: 31.7203
[11/25 17:53:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.50	
[11/25 17:53:39 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/25 17:55:18 visual_prompt]: 	Training 100/553. train loss: 21.1932,	0.8440 s / batch. (data: 3.01e-04). ETA=9:57:33, max mem: 20.9 GB 
[11/25 17:56:55 visual_prompt]: 	Training 200/553. train loss: 4.5434,	0.8320 s / batch. (data: 3.00e-04). ETA=9:47:41, max mem: 20.9 GB 
[11/25 17:58:33 visual_prompt]: 	Training 300/553. train loss: 5.0614,	0.8667 s / batch. (data: 3.14e-02). ETA=10:10:45, max mem: 20.9 GB 
[11/25 18:00:10 visual_prompt]: 	Training 400/553. train loss: 27.5653,	0.8669 s / batch. (data: 1.05e-02). ETA=10:09:25, max mem: 20.9 GB 
[11/25 18:01:50 visual_prompt]: 	Training 500/553. train loss: 37.7972,	0.8369 s / batch. (data: 1.05e-02). ETA=9:46:57, max mem: 20.9 GB 
[11/25 18:02:41 visual_prompt]: Epoch 24 / 100: avg data time: 1.47e-01, avg batch time: 0.9795, average train loss: 29.8137
[11/25 18:03:36 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3111, average loss: 7.4391
[11/25 18:03:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.25	
[11/25 18:03:36 visual_prompt]: Best epoch 24: best metric: -7.439
[11/25 18:03:36 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/25 18:05:21 visual_prompt]: 	Training 100/553. train loss: 53.5473,	0.8290 s / batch. (data: 3.08e-04). ETA=9:39:18, max mem: 20.9 GB 
[11/25 18:06:55 visual_prompt]: 	Training 200/553. train loss: 11.9055,	0.8443 s / batch. (data: 1.05e-02). ETA=9:48:35, max mem: 20.9 GB 
[11/25 18:08:33 visual_prompt]: 	Training 300/553. train loss: 5.0566,	0.8629 s / batch. (data: 3.29e-04). ETA=10:00:08, max mem: 20.9 GB 
[11/25 18:10:10 visual_prompt]: 	Training 400/553. train loss: 9.4415,	1.2073 s / batch. (data: 3.72e-01). ETA=13:57:36, max mem: 20.9 GB 
[11/25 18:11:48 visual_prompt]: 	Training 500/553. train loss: 11.4700,	1.2607 s / batch. (data: 4.31e-01). ETA=14:32:33, max mem: 20.9 GB 
[11/25 18:12:40 visual_prompt]: Epoch 25 / 100: avg data time: 1.51e-01, avg batch time: 0.9831, average train loss: 32.5727
[11/25 18:13:36 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3105, average loss: 24.6369
[11/25 18:13:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.47	
[11/25 18:13:36 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/25 18:15:18 visual_prompt]: 	Training 100/553. train loss: 2.8801,	0.8516 s / batch. (data: 5.56e-03). ETA=9:47:16, max mem: 20.9 GB 
[11/25 18:16:57 visual_prompt]: 	Training 200/553. train loss: 21.5737,	1.7960 s / batch. (data: 9.90e-01). ETA=20:35:31, max mem: 20.9 GB 
[11/25 18:18:37 visual_prompt]: 	Training 300/553. train loss: 17.6660,	0.8480 s / batch. (data: 2.73e-04). ETA=9:41:57, max mem: 20.9 GB 
[11/25 18:20:14 visual_prompt]: 	Training 400/553. train loss: 17.7804,	0.8395 s / batch. (data: 7.40e-03). ETA=9:34:41, max mem: 20.9 GB 
[11/25 18:21:49 visual_prompt]: 	Training 500/553. train loss: 55.5703,	0.8361 s / batch. (data: 7.97e-03). ETA=9:31:00, max mem: 20.9 GB 
[11/25 18:22:40 visual_prompt]: Epoch 26 / 100: avg data time: 1.52e-01, avg batch time: 0.9845, average train loss: 29.8481
[11/25 18:23:36 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3093, average loss: 8.4448
[11/25 18:23:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.15	
[11/25 18:23:36 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[11/25 18:25:17 visual_prompt]: 	Training 100/553. train loss: 73.5949,	0.8565 s / batch. (data: 2.44e-02). ETA=9:42:42, max mem: 20.9 GB 
[11/25 18:26:53 visual_prompt]: 	Training 200/553. train loss: 44.7822,	0.9610 s / batch. (data: 1.34e-01). ETA=10:52:12, max mem: 20.9 GB 
[11/25 18:28:29 visual_prompt]: 	Training 300/553. train loss: 13.6333,	0.8552 s / batch. (data: 2.11e-02). ETA=9:39:00, max mem: 20.9 GB 
[11/25 18:30:08 visual_prompt]: 	Training 400/553. train loss: 92.7445,	0.8328 s / batch. (data: 7.23e-04). ETA=9:22:28, max mem: 20.9 GB 
[11/25 18:31:44 visual_prompt]: 	Training 500/553. train loss: 17.2910,	0.8594 s / batch. (data: 1.45e-02). ETA=9:38:57, max mem: 20.9 GB 
[11/25 18:32:33 visual_prompt]: Epoch 27 / 100: avg data time: 1.40e-01, avg batch time: 0.9720, average train loss: 31.4765
[11/25 18:33:29 visual_prompt]: Inference (val):avg data time: 1.28e-04, avg batch time: 0.3108, average loss: 15.9482
[11/25 18:33:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.11	
[11/25 18:33:29 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[11/25 18:35:09 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8370 s / batch. (data: 3.18e-04). ETA=9:21:45, max mem: 20.9 GB 
[11/25 18:36:47 visual_prompt]: 	Training 200/553. train loss: 32.3063,	0.8379 s / batch. (data: 1.60e-02). ETA=9:20:55, max mem: 20.9 GB 
[11/25 18:38:26 visual_prompt]: 	Training 300/553. train loss: 14.7498,	1.5160 s / batch. (data: 6.99e-01). ETA=16:52:25, max mem: 20.9 GB 
[11/25 18:40:02 visual_prompt]: 	Training 400/553. train loss: 76.6420,	0.8600 s / batch. (data: 1.20e-02). ETA=9:32:53, max mem: 20.9 GB 
[11/25 18:41:37 visual_prompt]: 	Training 500/553. train loss: 21.1955,	0.8680 s / batch. (data: 3.15e-04). ETA=9:36:45, max mem: 20.9 GB 
[11/25 18:42:29 visual_prompt]: Epoch 28 / 100: avg data time: 1.44e-01, avg batch time: 0.9766, average train loss: 31.9558
[11/25 18:43:25 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3115, average loss: 46.2851
[11/25 18:43:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.27	
[11/25 18:43:25 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[11/25 18:45:13 visual_prompt]: 	Training 100/553. train loss: 59.1458,	0.8312 s / batch. (data: 2.86e-04). ETA=9:10:10, max mem: 20.9 GB 
[11/25 18:46:49 visual_prompt]: 	Training 200/553. train loss: 37.0747,	1.8074 s / batch. (data: 9.56e-01). ETA=19:53:23, max mem: 20.9 GB 
[11/25 18:48:25 visual_prompt]: 	Training 300/553. train loss: 23.9400,	0.8431 s / batch. (data: 1.16e-02). ETA=9:15:15, max mem: 20.9 GB 
[11/25 18:49:58 visual_prompt]: 	Training 400/553. train loss: 9.1341,	0.8437 s / batch. (data: 5.47e-03). ETA=9:14:16, max mem: 20.9 GB 
[11/25 18:51:36 visual_prompt]: 	Training 500/553. train loss: 5.2619,	0.8371 s / batch. (data: 3.24e-04). ETA=9:08:32, max mem: 20.9 GB 
[11/25 18:52:27 visual_prompt]: Epoch 29 / 100: avg data time: 1.48e-01, avg batch time: 0.9799, average train loss: 37.2370
[11/25 18:53:22 visual_prompt]: Inference (val):avg data time: 1.76e-04, avg batch time: 0.3095, average loss: 36.1800
[11/25 18:53:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.64	
[11/25 18:53:22 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[11/25 18:55:02 visual_prompt]: 	Training 100/553. train loss: 57.3054,	0.8231 s / batch. (data: 1.05e-02). ETA=8:57:14, max mem: 20.9 GB 
[11/25 18:56:40 visual_prompt]: 	Training 200/553. train loss: 20.0120,	0.8280 s / batch. (data: 4.86e-04). ETA=8:59:02, max mem: 20.9 GB 
[11/25 18:58:18 visual_prompt]: 	Training 300/553. train loss: 58.9110,	1.8745 s / batch. (data: 1.04e+00). ETA=20:17:17, max mem: 20.9 GB 
[11/25 18:59:56 visual_prompt]: 	Training 400/553. train loss: 32.9440,	1.0800 s / batch. (data: 2.37e-01). ETA=11:39:33, max mem: 20.9 GB 
[11/25 19:01:32 visual_prompt]: 	Training 500/553. train loss: 113.4729,	1.4240 s / batch. (data: 5.94e-01). ETA=15:19:57, max mem: 20.9 GB 
[11/25 19:02:25 visual_prompt]: Epoch 30 / 100: avg data time: 1.49e-01, avg batch time: 0.9808, average train loss: 28.7434
[11/25 19:03:20 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3094, average loss: 22.9904
[11/25 19:03:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.86	
[11/25 19:03:20 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[11/25 19:05:02 visual_prompt]: 	Training 100/553. train loss: 23.2558,	0.8400 s / batch. (data: 3.09e-04). ETA=9:00:32, max mem: 20.9 GB 
[11/25 19:06:42 visual_prompt]: 	Training 200/553. train loss: 109.4730,	0.8120 s / batch. (data: 2.99e-04). ETA=8:41:11, max mem: 20.9 GB 
[11/25 19:08:17 visual_prompt]: 	Training 300/553. train loss: 45.3426,	0.8280 s / batch. (data: 3.16e-04). ETA=8:50:03, max mem: 20.9 GB 
[11/25 19:09:53 visual_prompt]: 	Training 400/553. train loss: 52.4664,	0.8515 s / batch. (data: 5.41e-03). ETA=9:03:41, max mem: 20.9 GB 
[11/25 19:11:31 visual_prompt]: 	Training 500/553. train loss: 31.6548,	0.8437 s / batch. (data: 4.08e-03). ETA=8:57:18, max mem: 20.9 GB 
[11/25 19:12:21 visual_prompt]: Epoch 31 / 100: avg data time: 1.45e-01, avg batch time: 0.9777, average train loss: 26.0752
[11/25 19:13:16 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3111, average loss: 31.8197
[11/25 19:13:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.21	
[11/25 19:13:16 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[11/25 19:14:59 visual_prompt]: 	Training 100/553. train loss: 25.7262,	0.8360 s / batch. (data: 3.02e-04). ETA=8:50:17, max mem: 20.9 GB 
[11/25 19:16:36 visual_prompt]: 	Training 200/553. train loss: 15.7171,	0.8366 s / batch. (data: 8.18e-04). ETA=8:49:14, max mem: 20.9 GB 
[11/25 19:18:16 visual_prompt]: 	Training 300/553. train loss: 92.0406,	0.8238 s / batch. (data: 2.73e-04). ETA=8:39:45, max mem: 20.9 GB 
[11/25 19:19:54 visual_prompt]: 	Training 400/553. train loss: 13.8397,	0.8185 s / batch. (data: 3.45e-04). ETA=8:35:03, max mem: 20.9 GB 
[11/25 19:21:29 visual_prompt]: 	Training 500/553. train loss: 23.7470,	0.8307 s / batch. (data: 5.41e-03). ETA=8:41:21, max mem: 20.9 GB 
[11/25 19:22:18 visual_prompt]: Epoch 32 / 100: avg data time: 1.48e-01, avg batch time: 0.9799, average train loss: 30.6038
[11/25 19:23:14 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3095, average loss: 15.6869
[11/25 19:23:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.55	
[11/25 19:23:14 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[11/25 19:24:53 visual_prompt]: 	Training 100/553. train loss: 30.3722,	1.4107 s / batch. (data: 5.79e-01). ETA=14:41:48, max mem: 20.9 GB 
[11/25 19:26:34 visual_prompt]: 	Training 200/553. train loss: 36.4146,	1.3277 s / batch. (data: 5.20e-01). ETA=13:47:41, max mem: 20.9 GB 
[11/25 19:28:12 visual_prompt]: 	Training 300/553. train loss: 26.4358,	0.8309 s / batch. (data: 5.42e-03). ETA=8:36:34, max mem: 20.9 GB 
[11/25 19:29:52 visual_prompt]: 	Training 400/553. train loss: 7.8397,	0.8400 s / batch. (data: 4.08e-04). ETA=8:40:50, max mem: 20.9 GB 
[11/25 19:31:30 visual_prompt]: 	Training 500/553. train loss: 13.7290,	0.8320 s / batch. (data: 3.21e-04). ETA=8:34:29, max mem: 20.9 GB 
[11/25 19:32:21 visual_prompt]: Epoch 33 / 100: avg data time: 1.59e-01, avg batch time: 0.9901, average train loss: 29.5350
[11/25 19:33:18 visual_prompt]: Inference (val):avg data time: 1.64e-04, avg batch time: 0.3133, average loss: 87.5976
[11/25 19:33:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.27	
[11/25 19:33:18 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[11/25 19:35:01 visual_prompt]: 	Training 100/553. train loss: 15.7223,	0.8177 s / batch. (data: 3.57e-04). ETA=8:23:33, max mem: 20.9 GB 
[11/25 19:36:36 visual_prompt]: 	Training 200/553. train loss: 19.8765,	0.8320 s / batch. (data: 3.19e-04). ETA=8:31:01, max mem: 20.9 GB 
[11/25 19:38:12 visual_prompt]: 	Training 300/553. train loss: 59.4217,	0.8206 s / batch. (data: 7.96e-03). ETA=8:22:39, max mem: 20.9 GB 
[11/25 19:39:51 visual_prompt]: 	Training 400/553. train loss: 37.4711,	0.8466 s / batch. (data: 3.08e-04). ETA=8:37:10, max mem: 20.9 GB 
[11/25 19:41:30 visual_prompt]: 	Training 500/553. train loss: 44.6877,	1.4380 s / batch. (data: 6.33e-01). ETA=14:36:02, max mem: 20.9 GB 
[11/25 19:42:20 visual_prompt]: Epoch 34 / 100: avg data time: 1.48e-01, avg batch time: 0.9800, average train loss: 30.3251
[11/25 19:43:15 visual_prompt]: Inference (val):avg data time: 2.79e-04, avg batch time: 0.3113, average loss: 19.5082
[11/25 19:43:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.82	
[11/25 19:43:15 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[11/25 19:44:59 visual_prompt]: 	Training 100/553. train loss: 28.2258,	0.8178 s / batch. (data: 4.75e-04). ETA=8:16:05, max mem: 20.9 GB 
[11/25 19:46:37 visual_prompt]: 	Training 200/553. train loss: 40.1643,	0.8440 s / batch. (data: 3.41e-04). ETA=8:30:34, max mem: 20.9 GB 
[11/25 19:48:13 visual_prompt]: 	Training 300/553. train loss: 20.8998,	0.8206 s / batch. (data: 3.15e-04). ETA=8:15:04, max mem: 20.9 GB 
[11/25 19:49:48 visual_prompt]: 	Training 400/553. train loss: 16.4035,	0.8201 s / batch. (data: 2.98e-04). ETA=8:13:24, max mem: 20.9 GB 
[11/25 19:51:25 visual_prompt]: 	Training 500/553. train loss: 92.4864,	1.0732 s / batch. (data: 2.47e-01). ETA=10:43:52, max mem: 20.9 GB 
[11/25 19:52:17 visual_prompt]: Epoch 35 / 100: avg data time: 1.47e-01, avg batch time: 0.9788, average train loss: 32.1592
[11/25 19:53:12 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3103, average loss: 29.1534
[11/25 19:53:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.99	
[11/25 19:53:12 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[11/25 19:54:52 visual_prompt]: 	Training 100/553. train loss: 5.5278,	0.8284 s / batch. (data: 1.20e-02). ETA=8:14:53, max mem: 20.9 GB 
[11/25 19:56:31 visual_prompt]: 	Training 200/553. train loss: 61.8225,	0.8395 s / batch. (data: 1.56e-02). ETA=8:20:08, max mem: 20.9 GB 
[11/25 19:58:09 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8394 s / batch. (data: 7.40e-03). ETA=8:18:42, max mem: 20.9 GB 
[11/25 19:59:46 visual_prompt]: 	Training 400/553. train loss: 26.8228,	0.8399 s / batch. (data: 3.14e-04). ETA=8:17:35, max mem: 20.9 GB 
[11/25 20:01:23 visual_prompt]: 	Training 500/553. train loss: 8.0681,	0.9135 s / batch. (data: 5.65e-02). ETA=8:59:38, max mem: 20.9 GB 
[11/25 20:02:12 visual_prompt]: Epoch 36 / 100: avg data time: 1.43e-01, avg batch time: 0.9755, average train loss: 27.7423
[11/25 20:03:07 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3111, average loss: 88.0162
[11/25 20:03:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.70	
[11/25 20:03:07 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[11/25 20:04:48 visual_prompt]: 	Training 100/553. train loss: 7.0561,	0.8175 s / batch. (data: 3.32e-04). ETA=8:00:52, max mem: 20.9 GB 
[11/25 20:06:25 visual_prompt]: 	Training 200/553. train loss: 14.3619,	0.8360 s / batch. (data: 3.18e-04). ETA=8:10:20, max mem: 20.9 GB 
[11/25 20:08:02 visual_prompt]: 	Training 300/553. train loss: 26.8646,	1.3281 s / batch. (data: 4.95e-01). ETA=12:56:44, max mem: 20.9 GB 
[11/25 20:09:42 visual_prompt]: 	Training 400/553. train loss: 46.8616,	1.8938 s / batch. (data: 1.07e+00). ETA=18:24:26, max mem: 20.9 GB 
[11/25 20:11:15 visual_prompt]: 	Training 500/553. train loss: 29.4414,	0.9584 s / batch. (data: 1.21e-01). ETA=9:17:19, max mem: 20.9 GB 
[11/25 20:12:08 visual_prompt]: Epoch 37 / 100: avg data time: 1.45e-01, avg batch time: 0.9778, average train loss: 24.0381
[11/25 20:13:03 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3103, average loss: 10.0697
[11/25 20:13:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.56	
[11/25 20:13:03 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[11/25 20:14:43 visual_prompt]: 	Training 100/553. train loss: 12.8534,	0.8432 s / batch. (data: 5.44e-03). ETA=8:08:10, max mem: 20.9 GB 
[11/25 20:16:22 visual_prompt]: 	Training 200/553. train loss: 10.3631,	1.3982 s / batch. (data: 5.78e-01). ETA=13:27:12, max mem: 20.9 GB 
[11/25 20:18:01 visual_prompt]: 	Training 300/553. train loss: 28.5636,	0.8127 s / batch. (data: 2.98e-04). ETA=7:47:49, max mem: 20.9 GB 
[11/25 20:19:35 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8360 s / batch. (data: 3.13e-04). ETA=7:59:51, max mem: 20.9 GB 
[11/25 20:21:15 visual_prompt]: 	Training 500/553. train loss: 4.4356,	0.8336 s / batch. (data: 3.34e-04). ETA=7:57:04, max mem: 20.9 GB 
[11/25 20:22:04 visual_prompt]: Epoch 38 / 100: avg data time: 1.46e-01, avg batch time: 0.9781, average train loss: 25.6685
[11/25 20:23:00 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3110, average loss: 5.1620
[11/25 20:23:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 53.38	
[11/25 20:23:00 visual_prompt]: Best epoch 38: best metric: -5.162
[11/25 20:23:00 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[11/25 20:24:39 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8480 s / batch. (data: 3.11e-04). ETA=8:03:09, max mem: 20.9 GB 
[11/25 20:26:20 visual_prompt]: 	Training 200/553. train loss: 40.6512,	0.8240 s / batch. (data: 3.06e-04). ETA=7:48:08, max mem: 20.9 GB 
[11/25 20:28:00 visual_prompt]: 	Training 300/553. train loss: 23.4809,	0.8280 s / batch. (data: 2.78e-04). ETA=7:49:01, max mem: 20.9 GB 
[11/25 20:29:35 visual_prompt]: 	Training 400/553. train loss: 0.3322,	0.8270 s / batch. (data: 3.27e-04). ETA=7:47:05, max mem: 20.9 GB 
[11/25 20:31:13 visual_prompt]: 	Training 500/553. train loss: 21.3553,	1.6247 s / batch. (data: 7.81e-01). ETA=15:14:50, max mem: 20.9 GB 
[11/25 20:32:01 visual_prompt]: Epoch 39 / 100: avg data time: 1.46e-01, avg batch time: 0.9789, average train loss: 22.3797
[11/25 20:32:56 visual_prompt]: Inference (val):avg data time: 1.65e-04, avg batch time: 0.3103, average loss: 9.7386
[11/25 20:32:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.07	
[11/25 20:32:56 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[11/25 20:34:38 visual_prompt]: 	Training 100/553. train loss: 39.0284,	0.8264 s / batch. (data: 2.98e-04). ETA=7:43:15, max mem: 20.9 GB 
[11/25 20:36:15 visual_prompt]: 	Training 200/553. train loss: 44.7136,	0.8470 s / batch. (data: 1.05e-02). ETA=7:53:24, max mem: 20.9 GB 
[11/25 20:37:53 visual_prompt]: 	Training 300/553. train loss: 1.4765,	0.8563 s / batch. (data: 1.60e-02). ETA=7:57:09, max mem: 20.9 GB 
[11/25 20:39:31 visual_prompt]: 	Training 400/553. train loss: 12.7666,	0.8334 s / batch. (data: 3.02e-04). ETA=7:43:00, max mem: 20.9 GB 
[11/25 20:41:07 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8182 s / batch. (data: 3.01e-04). ETA=7:33:12, max mem: 20.9 GB 
[11/25 20:42:00 visual_prompt]: Epoch 40 / 100: avg data time: 1.50e-01, avg batch time: 0.9822, average train loss: 26.6628
[11/25 20:42:56 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3104, average loss: 21.7155
[11/25 20:42:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 39.32	
[11/25 20:42:56 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[11/25 20:44:43 visual_prompt]: 	Training 100/553. train loss: 26.5522,	0.8399 s / batch. (data: 5.25e-04). ETA=7:43:03, max mem: 20.9 GB 
[11/25 20:46:29 visual_prompt]: 	Training 200/553. train loss: 25.0975,	0.8343 s / batch. (data: 1.60e-02). ETA=7:38:33, max mem: 20.9 GB 
[11/25 20:48:06 visual_prompt]: 	Training 300/553. train loss: 8.7110,	0.8389 s / batch. (data: 3.16e-04). ETA=7:39:41, max mem: 20.9 GB 
[11/25 20:49:43 visual_prompt]: 	Training 400/553. train loss: 24.0848,	0.8196 s / batch. (data: 2.99e-04). ETA=7:27:47, max mem: 20.9 GB 
[11/25 20:51:20 visual_prompt]: 	Training 500/553. train loss: 2.9224,	0.8498 s / batch. (data: 1.12e-03). ETA=7:42:52, max mem: 20.9 GB 
[11/25 20:52:09 visual_prompt]: Epoch 41 / 100: avg data time: 1.68e-01, avg batch time: 0.9997, average train loss: 26.3409
[11/25 20:53:05 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3092, average loss: 32.4843
[11/25 20:53:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.24	
[11/25 20:53:05 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[11/25 20:54:45 visual_prompt]: 	Training 100/553. train loss: 15.6477,	0.8328 s / batch. (data: 5.42e-03). ETA=7:31:28, max mem: 20.9 GB 
[11/25 20:56:22 visual_prompt]: 	Training 200/553. train loss: 18.8049,	0.8323 s / batch. (data: 5.44e-03). ETA=7:29:47, max mem: 20.9 GB 
[11/25 20:58:01 visual_prompt]: 	Training 300/553. train loss: 6.5058,	0.8440 s / batch. (data: 3.10e-04). ETA=7:34:44, max mem: 20.9 GB 
[11/25 20:59:38 visual_prompt]: 	Training 400/553. train loss: 17.2064,	0.8598 s / batch. (data: 1.98e-02). ETA=7:41:48, max mem: 20.9 GB 
[11/25 21:01:14 visual_prompt]: 	Training 500/553. train loss: 91.9209,	0.8320 s / batch. (data: 7.95e-03). ETA=7:25:29, max mem: 20.9 GB 
[11/25 21:02:06 visual_prompt]: Epoch 42 / 100: avg data time: 1.46e-01, avg batch time: 0.9785, average train loss: 24.4209
[11/25 21:03:02 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3107, average loss: 36.9176
[11/25 21:03:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.04	
[11/25 21:03:02 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[11/25 21:04:45 visual_prompt]: 	Training 100/553. train loss: 7.3888,	0.8246 s / batch. (data: 8.58e-03). ETA=7:19:25, max mem: 20.9 GB 
[11/25 21:06:21 visual_prompt]: 	Training 200/553. train loss: 115.5747,	0.8251 s / batch. (data: 3.29e-04). ETA=7:18:19, max mem: 20.9 GB 
[11/25 21:07:57 visual_prompt]: 	Training 300/553. train loss: 42.1443,	0.8374 s / batch. (data: 7.95e-03). ETA=7:23:26, max mem: 20.9 GB 
[11/25 21:09:32 visual_prompt]: 	Training 400/553. train loss: 59.4686,	0.8480 s / batch. (data: 7.95e-03). ETA=7:27:39, max mem: 20.9 GB 
[11/25 21:11:11 visual_prompt]: 	Training 500/553. train loss: 38.3393,	0.8320 s / batch. (data: 3.26e-04). ETA=7:17:49, max mem: 20.9 GB 
[11/25 21:12:04 visual_prompt]: Epoch 43 / 100: avg data time: 1.48e-01, avg batch time: 0.9803, average train loss: 26.1458
[11/25 21:13:00 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3100, average loss: 18.0466
[11/25 21:13:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.67	
[11/25 21:13:00 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[11/25 21:14:40 visual_prompt]: 	Training 100/553. train loss: 41.6996,	1.3181 s / batch. (data: 4.89e-01). ETA=11:30:15, max mem: 20.9 GB 
[11/25 21:16:19 visual_prompt]: 	Training 200/553. train loss: 24.9968,	0.8227 s / batch. (data: 3.00e-04). ETA=7:09:26, max mem: 20.9 GB 
[11/25 21:17:54 visual_prompt]: 	Training 300/553. train loss: 40.6746,	0.8240 s / batch. (data: 3.18e-04). ETA=7:08:44, max mem: 20.9 GB 
[11/25 21:19:30 visual_prompt]: 	Training 400/553. train loss: 50.0597,	0.8699 s / batch. (data: 1.56e-02). ETA=7:31:12, max mem: 20.9 GB 
[11/25 21:21:07 visual_prompt]: 	Training 500/553. train loss: 11.9130,	0.8212 s / batch. (data: 3.24e-04). ETA=7:04:34, max mem: 20.9 GB 
[11/25 21:21:59 visual_prompt]: Epoch 44 / 100: avg data time: 1.42e-01, avg batch time: 0.9748, average train loss: 26.5415
[11/25 21:22:54 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3099, average loss: 106.2100
[11/25 21:22:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.68	
[11/25 21:22:54 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[11/25 21:24:37 visual_prompt]: 	Training 100/553. train loss: 18.1458,	0.8554 s / batch. (data: 2.02e-02). ETA=7:20:05, max mem: 20.9 GB 
[11/25 21:26:11 visual_prompt]: 	Training 200/553. train loss: 2.2040,	1.2249 s / batch. (data: 3.99e-01). ETA=10:28:08, max mem: 20.9 GB 
[11/25 21:27:51 visual_prompt]: 	Training 300/553. train loss: 35.3088,	0.8341 s / batch. (data: 8.24e-04). ETA=7:06:20, max mem: 20.9 GB 
[11/25 21:29:25 visual_prompt]: 	Training 400/553. train loss: 11.2677,	0.8382 s / batch. (data: 1.20e-02). ETA=7:07:01, max mem: 20.9 GB 
[11/25 21:31:05 visual_prompt]: 	Training 500/553. train loss: 35.0762,	0.8322 s / batch. (data: 5.44e-03). ETA=7:02:35, max mem: 20.9 GB 
[11/25 21:31:56 visual_prompt]: Epoch 45 / 100: avg data time: 1.46e-01, avg batch time: 0.9787, average train loss: 21.2374
[11/25 21:32:51 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3115, average loss: 8.9792
[11/25 21:32:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.90	
[11/25 21:32:51 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[11/25 21:34:32 visual_prompt]: 	Training 100/553. train loss: 0.5297,	1.1732 s / batch. (data: 3.26e-01). ETA=9:52:47, max mem: 20.9 GB 
[11/25 21:36:11 visual_prompt]: 	Training 200/553. train loss: 2.9668,	0.8375 s / batch. (data: 7.79e-04). ETA=7:01:43, max mem: 20.9 GB 
[11/25 21:37:47 visual_prompt]: 	Training 300/553. train loss: 76.9602,	0.8520 s / batch. (data: 5.47e-03). ETA=7:07:37, max mem: 20.9 GB 
[11/25 21:39:25 visual_prompt]: 	Training 400/553. train loss: 27.3665,	0.8468 s / batch. (data: 7.76e-04). ETA=7:03:36, max mem: 20.9 GB 
[11/25 21:40:58 visual_prompt]: 	Training 500/553. train loss: 0.6555,	0.8285 s / batch. (data: 2.86e-04). ETA=6:53:03, max mem: 20.9 GB 
[11/25 21:41:51 visual_prompt]: Epoch 46 / 100: avg data time: 1.44e-01, avg batch time: 0.9759, average train loss: 24.1686
[11/25 21:42:45 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3112, average loss: 13.5975
[11/25 21:42:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.89	
[11/25 21:42:45 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[11/25 21:44:27 visual_prompt]: 	Training 100/553. train loss: 10.5285,	0.8400 s / batch. (data: 4.10e-04). ETA=6:56:38, max mem: 20.9 GB 
[11/25 21:46:02 visual_prompt]: 	Training 200/553. train loss: 78.0075,	1.2420 s / batch. (data: 4.28e-01). ETA=10:13:58, max mem: 20.9 GB 
[11/25 21:47:40 visual_prompt]: 	Training 300/553. train loss: 5.8748,	0.8179 s / batch. (data: 3.35e-04). ETA=6:42:58, max mem: 20.9 GB 
[11/25 21:49:17 visual_prompt]: 	Training 400/553. train loss: 41.5618,	0.8148 s / batch. (data: 3.11e-04). ETA=6:40:05, max mem: 20.9 GB 
[11/25 21:50:53 visual_prompt]: 	Training 500/553. train loss: 12.8149,	0.8476 s / batch. (data: 3.29e-04). ETA=6:54:45, max mem: 20.9 GB 
[11/25 21:51:46 visual_prompt]: Epoch 47 / 100: avg data time: 1.44e-01, avg batch time: 0.9769, average train loss: 22.0154
[11/25 21:52:41 visual_prompt]: Inference (val):avg data time: 4.83e-05, avg batch time: 0.3092, average loss: 19.2693
[11/25 21:52:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.64	
[11/25 21:52:41 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[11/25 21:54:23 visual_prompt]: 	Training 100/553. train loss: 67.7261,	0.8496 s / batch. (data: 2.06e-02). ETA=6:53:37, max mem: 20.9 GB 
[11/25 21:56:01 visual_prompt]: 	Training 200/553. train loss: 3.7868,	0.8244 s / batch. (data: 3.32e-04). ETA=6:39:56, max mem: 20.9 GB 
[11/25 21:57:40 visual_prompt]: 	Training 300/553. train loss: 0.6299,	1.5837 s / batch. (data: 7.52e-01). ETA=12:45:42, max mem: 20.9 GB 
[11/25 21:59:13 visual_prompt]: 	Training 400/553. train loss: 0.0083,	0.8280 s / batch. (data: 3.18e-04). ETA=6:38:56, max mem: 20.9 GB 
[11/25 22:00:50 visual_prompt]: 	Training 500/553. train loss: 20.4112,	0.8260 s / batch. (data: 5.46e-03). ETA=6:36:35, max mem: 20.9 GB 
[11/25 22:01:40 visual_prompt]: Epoch 48 / 100: avg data time: 1.41e-01, avg batch time: 0.9743, average train loss: 20.5710
[11/25 22:02:35 visual_prompt]: Inference (val):avg data time: 4.71e-04, avg batch time: 0.3106, average loss: 63.7910
[11/25 22:02:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.25	
[11/25 22:02:35 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[11/25 22:04:16 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8207 s / batch. (data: 3.07e-04). ETA=6:31:58, max mem: 20.9 GB 
[11/25 22:05:52 visual_prompt]: 	Training 200/553. train loss: 20.9715,	0.8280 s / batch. (data: 2.95e-04). ETA=6:34:04, max mem: 20.9 GB 
[11/25 22:07:30 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8362 s / batch. (data: 3.29e-04). ETA=6:36:34, max mem: 20.9 GB 
[11/25 22:09:08 visual_prompt]: 	Training 400/553. train loss: 2.9353,	0.8531 s / batch. (data: 1.30e-02). ETA=6:43:09, max mem: 20.9 GB 
[11/25 22:10:45 visual_prompt]: 	Training 500/553. train loss: 14.3700,	0.8174 s / batch. (data: 2.64e-04). ETA=6:24:57, max mem: 20.9 GB 
[11/25 22:11:37 visual_prompt]: Epoch 49 / 100: avg data time: 1.46e-01, avg batch time: 0.9788, average train loss: 21.5894
[11/25 22:12:32 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3095, average loss: 18.1432
[11/25 22:12:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.55	
[11/25 22:12:32 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[11/25 22:14:15 visual_prompt]: 	Training 100/553. train loss: 13.4888,	0.8278 s / batch. (data: 2.87e-04). ETA=6:27:44, max mem: 20.9 GB 
[11/25 22:15:53 visual_prompt]: 	Training 200/553. train loss: 18.3344,	0.8311 s / batch. (data: 3.28e-04). ETA=6:27:52, max mem: 20.9 GB 
[11/25 22:17:29 visual_prompt]: 	Training 300/553. train loss: 32.4627,	0.8239 s / batch. (data: 3.19e-04). ETA=6:23:09, max mem: 20.9 GB 
[11/25 22:19:04 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8280 s / batch. (data: 7.95e-03). ETA=6:23:41, max mem: 20.9 GB 
[11/25 22:20:41 visual_prompt]: 	Training 500/553. train loss: 30.3753,	0.8520 s / batch. (data: 3.14e-04). ETA=6:33:23, max mem: 20.9 GB 
[11/25 22:21:32 visual_prompt]: Epoch 50 / 100: avg data time: 1.44e-01, avg batch time: 0.9762, average train loss: 20.2678
[11/25 22:22:27 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3109, average loss: 19.8499
[11/25 22:22:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.79	
[11/25 22:22:27 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[11/25 22:24:08 visual_prompt]: 	Training 100/553. train loss: 18.9532,	1.0145 s / batch. (data: 1.64e-01). ETA=7:45:50, max mem: 20.9 GB 
[11/25 22:25:45 visual_prompt]: 	Training 200/553. train loss: 28.8640,	0.8203 s / batch. (data: 3.32e-04). ETA=6:15:17, max mem: 20.9 GB 
[11/25 22:27:23 visual_prompt]: 	Training 300/553. train loss: 2.2775,	0.8280 s / batch. (data: 3.25e-04). ETA=6:17:26, max mem: 20.9 GB 
[11/25 22:29:02 visual_prompt]: 	Training 400/553. train loss: 2.9341,	1.2972 s / batch. (data: 4.73e-01). ETA=9:49:07, max mem: 20.9 GB 
[11/25 22:30:38 visual_prompt]: 	Training 500/553. train loss: 17.1177,	0.8199 s / batch. (data: 2.68e-04). ETA=6:11:00, max mem: 20.9 GB 
[11/25 22:31:27 visual_prompt]: Epoch 51 / 100: avg data time: 1.44e-01, avg batch time: 0.9768, average train loss: 19.0705
[11/25 22:32:22 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3093, average loss: 5.8342
[11/25 22:32:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.24	
[11/25 22:32:22 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[11/25 22:34:07 visual_prompt]: 	Training 100/553. train loss: 12.4204,	0.8361 s / batch. (data: 5.93e-03). ETA=6:16:11, max mem: 20.9 GB 
[11/25 22:35:43 visual_prompt]: 	Training 200/553. train loss: 8.5228,	0.8480 s / batch. (data: 2.93e-04). ETA=6:20:08, max mem: 20.9 GB 
[11/25 22:37:20 visual_prompt]: 	Training 300/553. train loss: 41.9723,	0.8276 s / batch. (data: 3.14e-04). ETA=6:09:36, max mem: 20.9 GB 
[11/25 22:38:58 visual_prompt]: 	Training 400/553. train loss: 88.8701,	0.8520 s / batch. (data: 7.95e-03). ETA=6:19:05, max mem: 20.9 GB 
[11/25 22:40:30 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8366 s / batch. (data: 5.49e-03). ETA=6:10:50, max mem: 20.9 GB 
[11/25 22:41:20 visual_prompt]: Epoch 52 / 100: avg data time: 1.38e-01, avg batch time: 0.9714, average train loss: 21.3370
[11/25 22:42:15 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3091, average loss: 3.1040
[11/25 22:42:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.22	
[11/25 22:42:15 visual_prompt]: Best epoch 52: best metric: -3.104
[11/25 22:42:15 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[11/25 22:43:56 visual_prompt]: 	Training 100/553. train loss: 34.9381,	0.8234 s / batch. (data: 2.73e-04). ETA=6:02:52, max mem: 20.9 GB 
[11/25 22:45:34 visual_prompt]: 	Training 200/553. train loss: 59.1264,	0.8534 s / batch. (data: 7.70e-04). ETA=6:14:42, max mem: 20.9 GB 
[11/25 22:47:10 visual_prompt]: 	Training 300/553. train loss: 30.7643,	0.8520 s / batch. (data: 7.73e-04). ETA=6:12:39, max mem: 20.9 GB 
[11/25 22:48:50 visual_prompt]: 	Training 400/553. train loss: 6.2285,	0.8633 s / batch. (data: 2.13e-02). ETA=6:16:10, max mem: 20.9 GB 
[11/25 22:50:26 visual_prompt]: 	Training 500/553. train loss: 28.4080,	0.8197 s / batch. (data: 3.02e-04). ETA=5:55:47, max mem: 20.9 GB 
[11/25 22:51:18 visual_prompt]: Epoch 53 / 100: avg data time: 1.49e-01, avg batch time: 0.9818, average train loss: 19.9654
[11/25 22:52:13 visual_prompt]: Inference (val):avg data time: 1.34e-04, avg batch time: 0.3108, average loss: 10.2193
[11/25 22:52:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.50	
[11/25 22:52:13 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[11/25 22:53:56 visual_prompt]: 	Training 100/553. train loss: 0.3219,	0.8280 s / batch. (data: 3.06e-04). ETA=5:57:17, max mem: 20.9 GB 
[11/25 22:55:33 visual_prompt]: 	Training 200/553. train loss: 6.3546,	0.8556 s / batch. (data: 5.43e-03). ETA=6:07:46, max mem: 20.9 GB 
[11/25 22:57:09 visual_prompt]: 	Training 300/553. train loss: 41.7299,	0.8196 s / batch. (data: 3.23e-04). ETA=5:50:55, max mem: 20.9 GB 
[11/25 22:58:45 visual_prompt]: 	Training 400/553. train loss: 34.6027,	0.8356 s / batch. (data: 1.05e-02). ETA=5:56:24, max mem: 20.9 GB 
[11/25 23:00:24 visual_prompt]: 	Training 500/553. train loss: 15.0122,	0.8326 s / batch. (data: 3.30e-04). ETA=5:53:44, max mem: 20.9 GB 
[11/25 23:01:14 visual_prompt]: Epoch 54 / 100: avg data time: 1.45e-01, avg batch time: 0.9788, average train loss: 20.5318
[11/25 23:02:10 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3120, average loss: 55.9242
[11/25 23:02:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.39	
[11/25 23:02:10 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[11/25 23:03:51 visual_prompt]: 	Training 100/553. train loss: 1.7116,	0.8280 s / batch. (data: 5.44e-03). ETA=5:49:39, max mem: 20.9 GB 
[11/25 23:05:27 visual_prompt]: 	Training 200/553. train loss: 6.7196,	0.8328 s / batch. (data: 1.06e-02). ETA=5:50:18, max mem: 20.9 GB 
[11/25 23:07:05 visual_prompt]: 	Training 300/553. train loss: 17.1665,	0.8610 s / batch. (data: 2.39e-02). ETA=6:00:44, max mem: 20.9 GB 
[11/25 23:08:42 visual_prompt]: 	Training 400/553. train loss: 3.7559,	0.8638 s / batch. (data: 3.09e-04). ETA=6:00:27, max mem: 20.9 GB 
[11/25 23:10:19 visual_prompt]: 	Training 500/553. train loss: 7.0962,	1.1773 s / batch. (data: 3.48e-01). ETA=8:09:19, max mem: 20.9 GB 
[11/25 23:11:10 visual_prompt]: Epoch 55 / 100: avg data time: 1.44e-01, avg batch time: 0.9767, average train loss: 18.2025
[11/25 23:12:05 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3092, average loss: 2.4929
[11/25 23:12:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.29	
[11/25 23:12:05 visual_prompt]: Best epoch 55: best metric: -2.493
[11/25 23:12:05 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[11/25 23:13:47 visual_prompt]: 	Training 100/553. train loss: 0.9013,	0.8451 s / batch. (data: 1.56e-02). ETA=5:49:05, max mem: 20.9 GB 
[11/25 23:15:23 visual_prompt]: 	Training 200/553. train loss: 2.3837,	0.8554 s / batch. (data: 2.26e-02). ETA=5:51:56, max mem: 20.9 GB 
[11/25 23:17:01 visual_prompt]: 	Training 300/553. train loss: 17.0712,	0.8290 s / batch. (data: 3.29e-04). ETA=5:39:40, max mem: 20.9 GB 
[11/25 23:18:39 visual_prompt]: 	Training 400/553. train loss: 7.0236,	0.8480 s / batch. (data: 3.18e-04). ETA=5:46:02, max mem: 20.9 GB 
[11/25 23:20:16 visual_prompt]: 	Training 500/553. train loss: 19.5637,	2.0555 s / batch. (data: 1.23e+00). ETA=13:55:23, max mem: 20.9 GB 
[11/25 23:21:05 visual_prompt]: Epoch 56 / 100: avg data time: 1.43e-01, avg batch time: 0.9762, average train loss: 15.3884
[11/25 23:22:01 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3106, average loss: 25.3055
[11/25 23:22:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.47	
[11/25 23:22:01 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[11/25 23:23:45 visual_prompt]: 	Training 100/553. train loss: 15.1756,	0.8252 s / batch. (data: 3.18e-04). ETA=5:33:17, max mem: 20.9 GB 
[11/25 23:25:21 visual_prompt]: 	Training 200/553. train loss: 45.6182,	0.8240 s / batch. (data: 3.42e-04). ETA=5:31:24, max mem: 20.9 GB 
[11/25 23:26:58 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8320 s / batch. (data: 7.95e-03). ETA=5:33:14, max mem: 20.9 GB 
[11/25 23:28:34 visual_prompt]: 	Training 400/553. train loss: 7.1038,	0.8331 s / batch. (data: 6.27e-04). ETA=5:32:18, max mem: 20.9 GB 
[11/25 23:30:09 visual_prompt]: 	Training 500/553. train loss: 67.1635,	0.8362 s / batch. (data: 5.45e-03). ETA=5:32:07, max mem: 20.9 GB 
[11/25 23:31:00 visual_prompt]: Epoch 57 / 100: avg data time: 1.42e-01, avg batch time: 0.9756, average train loss: 15.5547
[11/25 23:31:56 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3100, average loss: 5.7535
[11/25 23:31:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.17	
[11/25 23:31:56 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[11/25 23:33:36 visual_prompt]: 	Training 100/553. train loss: 7.4500,	0.8245 s / batch. (data: 3.09e-04). ETA=5:25:23, max mem: 20.9 GB 
[11/25 23:35:13 visual_prompt]: 	Training 200/553. train loss: 6.8628,	0.8557 s / batch. (data: 1.17e-02). ETA=5:36:16, max mem: 20.9 GB 
[11/25 23:36:52 visual_prompt]: 	Training 300/553. train loss: 24.6648,	0.8191 s / batch. (data: 3.17e-04). ETA=5:20:31, max mem: 20.9 GB 
[11/25 23:38:29 visual_prompt]: 	Training 400/553. train loss: 8.5359,	0.8436 s / batch. (data: 3.12e-04). ETA=5:28:41, max mem: 20.9 GB 
[11/25 23:40:05 visual_prompt]: 	Training 500/553. train loss: 185.6109,	0.8292 s / batch. (data: 3.22e-04). ETA=5:21:43, max mem: 20.9 GB 
[11/25 23:40:55 visual_prompt]: Epoch 58 / 100: avg data time: 1.41e-01, avg batch time: 0.9742, average train loss: 17.6176
[11/25 23:41:50 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3096, average loss: 7.0374
[11/25 23:41:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.23	
[11/25 23:41:50 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[11/25 23:43:33 visual_prompt]: 	Training 100/553. train loss: 1.1424,	0.8355 s / batch. (data: 7.96e-04). ETA=5:22:01, max mem: 20.9 GB 
[11/25 23:45:11 visual_prompt]: 	Training 200/553. train loss: 5.8374,	0.8483 s / batch. (data: 5.43e-03). ETA=5:25:33, max mem: 20.9 GB 
[11/25 23:46:47 visual_prompt]: 	Training 300/553. train loss: 27.1948,	0.8397 s / batch. (data: 3.30e-04). ETA=5:20:51, max mem: 20.9 GB 
[11/25 23:48:23 visual_prompt]: 	Training 400/553. train loss: 11.9470,	0.8334 s / batch. (data: 3.17e-04). ETA=5:17:03, max mem: 20.9 GB 
[11/25 23:50:02 visual_prompt]: 	Training 500/553. train loss: 6.1322,	0.8427 s / batch. (data: 8.41e-04). ETA=5:19:12, max mem: 20.9 GB 
[11/25 23:50:52 visual_prompt]: Epoch 59 / 100: avg data time: 1.46e-01, avg batch time: 0.9797, average train loss: 17.8184
[11/25 23:51:47 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3113, average loss: 3.6967
[11/25 23:51:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.48	
[11/25 23:51:47 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[11/25 23:53:29 visual_prompt]: 	Training 100/553. train loss: 2.9260,	0.8221 s / batch. (data: 5.55e-03). ETA=5:09:16, max mem: 20.9 GB 
[11/25 23:55:06 visual_prompt]: 	Training 200/553. train loss: 42.8757,	0.8314 s / batch. (data: 7.30e-03). ETA=5:11:23, max mem: 20.9 GB 
[11/25 23:56:42 visual_prompt]: 	Training 300/553. train loss: 38.8731,	2.2961 s / batch. (data: 1.47e+00). ETA=14:16:10, max mem: 20.9 GB 
[11/25 23:58:20 visual_prompt]: 	Training 400/553. train loss: 0.8077,	1.0080 s / batch. (data: 1.52e-01). ETA=6:14:11, max mem: 20.9 GB 
[11/25 23:59:58 visual_prompt]: 	Training 500/553. train loss: 3.3147,	0.8483 s / batch. (data: 3.62e-04). ETA=5:13:28, max mem: 20.9 GB 
[11/26 00:00:49 visual_prompt]: Epoch 60 / 100: avg data time: 1.47e-01, avg batch time: 0.9796, average train loss: 15.8165
[11/26 00:01:44 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3096, average loss: 15.3824
[11/26 00:01:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.05	
[11/26 00:01:44 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[11/26 00:03:26 visual_prompt]: 	Training 100/553. train loss: 29.5074,	0.8351 s / batch. (data: 1.20e-02). ETA=5:06:29, max mem: 20.9 GB 
[11/26 00:05:04 visual_prompt]: 	Training 200/553. train loss: 20.0876,	0.8476 s / batch. (data: 9.24e-03). ETA=5:09:39, max mem: 20.9 GB 
[11/26 00:06:42 visual_prompt]: 	Training 300/553. train loss: 8.4036,	0.8440 s / batch. (data: 5.43e-03). ETA=5:06:55, max mem: 20.9 GB 
[11/26 00:08:17 visual_prompt]: 	Training 400/553. train loss: 16.5174,	0.8427 s / batch. (data: 5.41e-03). ETA=5:05:04, max mem: 20.9 GB 
[11/26 00:09:54 visual_prompt]: 	Training 500/553. train loss: 34.2611,	1.9835 s / batch. (data: 1.15e+00). ETA=11:54:44, max mem: 20.9 GB 
[11/26 00:10:44 visual_prompt]: Epoch 61 / 100: avg data time: 1.42e-01, avg batch time: 0.9747, average train loss: 15.8167
[11/26 00:11:39 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3102, average loss: 10.9515
[11/26 00:11:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.38	
[11/26 00:11:39 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[11/26 00:13:20 visual_prompt]: 	Training 100/553. train loss: 22.4552,	0.8440 s / batch. (data: 3.04e-04). ETA=5:01:58, max mem: 20.9 GB 
[11/26 00:14:58 visual_prompt]: 	Training 200/553. train loss: 10.1873,	0.8335 s / batch. (data: 1.11e-02). ETA=4:56:48, max mem: 20.9 GB 
[11/26 00:16:33 visual_prompt]: 	Training 300/553. train loss: 27.1875,	0.8962 s / batch. (data: 3.62e-02). ETA=5:17:39, max mem: 20.9 GB 
[11/26 00:18:11 visual_prompt]: 	Training 400/553. train loss: 9.0329,	0.8243 s / batch. (data: 2.97e-04). ETA=4:50:47, max mem: 20.9 GB 
[11/26 00:19:45 visual_prompt]: 	Training 500/553. train loss: 14.9380,	0.8360 s / batch. (data: 3.14e-04). ETA=4:53:32, max mem: 20.9 GB 
[11/26 00:20:39 visual_prompt]: Epoch 62 / 100: avg data time: 1.43e-01, avg batch time: 0.9760, average train loss: 13.4324
[11/26 00:21:35 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3096, average loss: 13.0417
[11/26 00:21:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.03	
[11/26 00:21:35 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[11/26 00:23:19 visual_prompt]: 	Training 100/553. train loss: 2.1797,	0.8271 s / batch. (data: 3.20e-04). ETA=4:48:18, max mem: 20.9 GB 
[11/26 00:25:00 visual_prompt]: 	Training 200/553. train loss: 5.8354,	0.8320 s / batch. (data: 2.88e-04). ETA=4:48:37, max mem: 20.9 GB 
[11/26 00:26:35 visual_prompt]: 	Training 300/553. train loss: 2.0536,	0.8400 s / batch. (data: 3.62e-04). ETA=4:49:59, max mem: 20.9 GB 
[11/26 00:28:08 visual_prompt]: 	Training 400/553. train loss: 11.0377,	0.8251 s / batch. (data: 7.95e-03). ETA=4:43:28, max mem: 20.9 GB 
[11/26 00:29:43 visual_prompt]: 	Training 500/553. train loss: 7.3771,	0.8466 s / batch. (data: 1.88e-02). ETA=4:49:27, max mem: 20.9 GB 
[11/26 00:30:32 visual_prompt]: Epoch 63 / 100: avg data time: 1.38e-01, avg batch time: 0.9719, average train loss: 12.5359
[11/26 00:31:28 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3106, average loss: 14.8224
[11/26 00:31:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.12	
[11/26 00:31:28 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[11/26 00:33:11 visual_prompt]: 	Training 100/553. train loss: 4.9937,	0.8520 s / batch. (data: 7.71e-04). ETA=4:49:07, max mem: 20.9 GB 
[11/26 00:34:50 visual_prompt]: 	Training 200/553. train loss: 66.3867,	0.8124 s / batch. (data: 3.15e-04). ETA=4:34:20, max mem: 20.9 GB 
[11/26 00:36:24 visual_prompt]: 	Training 300/553. train loss: 8.0966,	0.8320 s / batch. (data: 7.97e-03). ETA=4:39:33, max mem: 20.9 GB 
[11/26 00:38:00 visual_prompt]: 	Training 400/553. train loss: 9.2367,	0.8920 s / batch. (data: 5.96e-02). ETA=4:58:14, max mem: 20.9 GB 
[11/26 00:39:38 visual_prompt]: 	Training 500/553. train loss: 31.7402,	0.8258 s / batch. (data: 7.56e-03). ETA=4:34:43, max mem: 20.9 GB 
[11/26 00:40:28 visual_prompt]: Epoch 64 / 100: avg data time: 1.44e-01, avg batch time: 0.9777, average train loss: 12.2981
[11/26 00:41:24 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3096, average loss: 4.9562
[11/26 00:41:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.95	
[11/26 00:41:24 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[11/26 00:43:08 visual_prompt]: 	Training 100/553. train loss: 8.1335,	1.1200 s / batch. (data: 2.72e-01). ETA=6:09:44, max mem: 20.9 GB 
[11/26 00:44:46 visual_prompt]: 	Training 200/553. train loss: 6.1695,	0.8287 s / batch. (data: 3.30e-04). ETA=4:32:11, max mem: 20.9 GB 
[11/26 00:46:21 visual_prompt]: 	Training 300/553. train loss: 17.0906,	0.8257 s / batch. (data: 2.90e-04). ETA=4:29:50, max mem: 20.9 GB 
[11/26 00:47:58 visual_prompt]: 	Training 400/553. train loss: 9.3688,	0.8248 s / batch. (data: 2.54e-04). ETA=4:28:10, max mem: 20.9 GB 
[11/26 00:49:34 visual_prompt]: 	Training 500/553. train loss: 17.0593,	0.8249 s / batch. (data: 5.41e-03). ETA=4:26:48, max mem: 20.9 GB 
[11/26 00:50:23 visual_prompt]: Epoch 65 / 100: avg data time: 1.41e-01, avg batch time: 0.9751, average train loss: 11.4889
[11/26 00:51:19 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3104, average loss: 7.8149
[11/26 00:51:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.17	
[11/26 00:51:19 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[11/26 00:52:59 visual_prompt]: 	Training 100/553. train loss: 6.1508,	0.8177 s / batch. (data: 3.01e-04). ETA=4:22:25, max mem: 20.9 GB 
[11/26 00:54:36 visual_prompt]: 	Training 200/553. train loss: 1.5804,	1.5136 s / batch. (data: 6.88e-01). ETA=8:03:13, max mem: 20.9 GB 
[11/26 00:56:16 visual_prompt]: 	Training 300/553. train loss: 5.7337,	0.8393 s / batch. (data: 5.41e-03). ETA=4:26:33, max mem: 20.9 GB 
[11/26 00:57:51 visual_prompt]: 	Training 400/553. train loss: 7.5557,	0.8331 s / batch. (data: 7.32e-04). ETA=4:23:11, max mem: 20.9 GB 
[11/26 00:59:27 visual_prompt]: 	Training 500/553. train loss: 17.6168,	0.8412 s / batch. (data: 5.41e-03). ETA=4:24:19, max mem: 20.9 GB 
[11/26 01:00:19 visual_prompt]: Epoch 66 / 100: avg data time: 1.43e-01, avg batch time: 0.9765, average train loss: 10.6113
[11/26 01:01:14 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3111, average loss: 6.4178
[11/26 01:01:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.53	
[11/26 01:01:14 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[11/26 01:02:57 visual_prompt]: 	Training 100/553. train loss: 12.9475,	0.8600 s / batch. (data: 2.80e-02). ETA=4:28:03, max mem: 20.9 GB 
[11/26 01:04:34 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8595 s / batch. (data: 1.05e-02). ETA=4:26:29, max mem: 20.9 GB 
[11/26 01:06:08 visual_prompt]: 	Training 300/553. train loss: 14.8793,	0.8360 s / batch. (data: 3.12e-04). ETA=4:17:47, max mem: 20.9 GB 
[11/26 01:07:45 visual_prompt]: 	Training 400/553. train loss: 13.0800,	0.8600 s / batch. (data: 1.20e-02). ETA=4:23:45, max mem: 20.9 GB 
[11/26 01:09:23 visual_prompt]: 	Training 500/553. train loss: 14.7161,	1.4829 s / batch. (data: 6.66e-01). ETA=7:32:20, max mem: 20.9 GB 
[11/26 01:10:15 visual_prompt]: Epoch 67 / 100: avg data time: 1.44e-01, avg batch time: 0.9776, average train loss: 9.5718
[11/26 01:11:10 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3096, average loss: 7.2660
[11/26 01:11:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 37.47	
[11/26 01:11:10 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[11/26 01:12:51 visual_prompt]: 	Training 100/553. train loss: 0.9755,	0.8165 s / batch. (data: 2.90e-04). ETA=4:06:57, max mem: 20.9 GB 
[11/26 01:14:30 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.0279 s / batch. (data: 1.83e-01). ETA=5:09:12, max mem: 20.9 GB 
[11/26 01:16:05 visual_prompt]: 	Training 300/553. train loss: 6.9514,	0.8337 s / batch. (data: 1.05e-02). ETA=4:09:23, max mem: 20.9 GB 
[11/26 01:17:41 visual_prompt]: 	Training 400/553. train loss: 15.7196,	0.8423 s / batch. (data: 6.22e-04). ETA=4:10:33, max mem: 20.9 GB 
[11/26 01:19:19 visual_prompt]: 	Training 500/553. train loss: 13.9565,	0.8605 s / batch. (data: 8.46e-03). ETA=4:14:32, max mem: 20.9 GB 
[11/26 01:20:10 visual_prompt]: Epoch 68 / 100: avg data time: 1.42e-01, avg batch time: 0.9763, average train loss: 9.5590
[11/26 01:21:05 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3100, average loss: 2.5293
[11/26 01:21:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.56	
[11/26 01:21:05 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[11/26 01:22:45 visual_prompt]: 	Training 100/553. train loss: 10.2932,	0.8480 s / batch. (data: 2.95e-04). ETA=4:08:41, max mem: 20.9 GB 
[11/26 01:24:22 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8320 s / batch. (data: 3.02e-04). ETA=4:02:36, max mem: 20.9 GB 
[11/26 01:25:59 visual_prompt]: 	Training 300/553. train loss: 17.7103,	0.8475 s / batch. (data: 1.05e-02). ETA=4:05:43, max mem: 20.9 GB 
[11/26 01:27:35 visual_prompt]: 	Training 400/553. train loss: 21.1518,	0.8366 s / batch. (data: 3.07e-04). ETA=4:01:09, max mem: 20.9 GB 
[11/26 01:29:11 visual_prompt]: 	Training 500/553. train loss: 9.3876,	0.8254 s / batch. (data: 3.03e-04). ETA=3:56:33, max mem: 20.9 GB 
[11/26 01:30:02 visual_prompt]: Epoch 69 / 100: avg data time: 1.38e-01, avg batch time: 0.9713, average train loss: 9.4999
[11/26 01:30:58 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3124, average loss: 5.6805
[11/26 01:30:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.36	
[11/26 01:30:58 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[11/26 01:32:37 visual_prompt]: 	Training 100/553. train loss: 8.5869,	0.8720 s / batch. (data: 7.96e-03). ETA=4:07:41, max mem: 20.9 GB 
[11/26 01:34:15 visual_prompt]: 	Training 200/553. train loss: 2.5892,	0.9162 s / batch. (data: 9.74e-02). ETA=4:18:43, max mem: 20.9 GB 
[11/26 01:35:51 visual_prompt]: 	Training 300/553. train loss: 36.9494,	0.8416 s / batch. (data: 1.20e-02). ETA=3:56:15, max mem: 20.9 GB 
[11/26 01:37:29 visual_prompt]: 	Training 400/553. train loss: 3.9782,	0.8763 s / batch. (data: 2.36e-02). ETA=4:04:32, max mem: 20.9 GB 
[11/26 01:39:04 visual_prompt]: 	Training 500/553. train loss: 12.8219,	0.8360 s / batch. (data: 5.42e-03). ETA=3:51:53, max mem: 20.9 GB 
[11/26 01:39:55 visual_prompt]: Epoch 70 / 100: avg data time: 1.39e-01, avg batch time: 0.9723, average train loss: 8.2492
[11/26 01:40:51 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3091, average loss: 10.5187
[11/26 01:40:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.50	
[11/26 01:40:51 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[11/26 01:42:32 visual_prompt]: 	Training 100/553. train loss: 11.1027,	0.8568 s / batch. (data: 5.44e-03). ETA=3:55:28, max mem: 20.9 GB 
[11/26 01:44:10 visual_prompt]: 	Training 200/553. train loss: 3.1218,	0.9768 s / batch. (data: 1.31e-01). ETA=4:26:50, max mem: 20.9 GB 
[11/26 01:45:49 visual_prompt]: 	Training 300/553. train loss: 3.3976,	0.8443 s / batch. (data: 9.39e-03). ETA=3:49:14, max mem: 20.9 GB 
[11/26 01:47:24 visual_prompt]: 	Training 400/553. train loss: 19.4440,	0.8520 s / batch. (data: 1.06e-02). ETA=3:49:53, max mem: 20.9 GB 
[11/26 01:49:01 visual_prompt]: 	Training 500/553. train loss: 4.8120,	0.9200 s / batch. (data: 6.71e-02). ETA=4:06:43, max mem: 20.9 GB 
[11/26 01:49:52 visual_prompt]: Epoch 71 / 100: avg data time: 1.43e-01, avg batch time: 0.9773, average train loss: 7.4976
[11/26 01:50:47 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3107, average loss: 6.8668
[11/26 01:50:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.44	
[11/26 01:50:47 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[11/26 01:52:31 visual_prompt]: 	Training 100/553. train loss: 6.5432,	1.0640 s / batch. (data: 2.38e-01). ETA=4:42:37, max mem: 20.9 GB 
[11/26 01:54:06 visual_prompt]: 	Training 200/553. train loss: 6.2367,	0.8409 s / batch. (data: 3.08e-04). ETA=3:41:57, max mem: 20.9 GB 
[11/26 01:55:47 visual_prompt]: 	Training 300/553. train loss: 0.8036,	0.8279 s / batch. (data: 3.22e-04). ETA=3:37:09, max mem: 20.9 GB 
[11/26 01:57:25 visual_prompt]: 	Training 400/553. train loss: 3.9885,	0.8188 s / batch. (data: 3.50e-04). ETA=3:33:23, max mem: 20.9 GB 
[11/26 01:59:08 visual_prompt]: 	Training 500/553. train loss: 3.0142,	0.8458 s / batch. (data: 3.17e-04). ETA=3:39:00, max mem: 20.9 GB 
[11/26 02:00:06 visual_prompt]: Epoch 72 / 100: avg data time: 1.77e-01, avg batch time: 1.0104, average train loss: 7.0124
[11/26 02:01:02 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3106, average loss: 1.4174
[11/26 02:01:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.01	
[11/26 02:01:02 visual_prompt]: Best epoch 72: best metric: -1.417
[11/26 02:01:02 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[11/26 02:02:45 visual_prompt]: 	Training 100/553. train loss: 3.6359,	0.8440 s / batch. (data: 7.95e-03). ETA=3:36:24, max mem: 20.9 GB 
[11/26 02:04:33 visual_prompt]: 	Training 200/553. train loss: 3.5473,	0.8240 s / batch. (data: 2.91e-04). ETA=3:29:54, max mem: 20.9 GB 
[11/26 02:06:07 visual_prompt]: 	Training 300/553. train loss: 5.9879,	0.8440 s / batch. (data: 3.09e-04). ETA=3:33:35, max mem: 20.9 GB 
[11/26 02:07:55 visual_prompt]: 	Training 400/553. train loss: 11.5023,	0.8510 s / batch. (data: 3.37e-04). ETA=3:33:56, max mem: 20.9 GB 
[11/26 02:09:32 visual_prompt]: 	Training 500/553. train loss: 10.1057,	0.8314 s / batch. (data: 5.43e-03). ETA=3:27:37, max mem: 20.9 GB 
[11/26 02:10:23 visual_prompt]: Epoch 73 / 100: avg data time: 1.82e-01, avg batch time: 1.0141, average train loss: 7.5758
[11/26 02:11:19 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3108, average loss: 2.7755
[11/26 02:11:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.55	
[11/26 02:11:19 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[11/26 02:13:05 visual_prompt]: 	Training 100/553. train loss: 5.8129,	1.4880 s / batch. (data: 6.37e-01). ETA=6:07:48, max mem: 20.9 GB 
[11/26 02:14:43 visual_prompt]: 	Training 200/553. train loss: 0.6464,	0.8474 s / batch. (data: 7.10e-04). ETA=3:28:03, max mem: 20.9 GB 
[11/26 02:16:20 visual_prompt]: 	Training 300/553. train loss: 1.5511,	0.8360 s / batch. (data: 3.55e-04). ETA=3:23:51, max mem: 20.9 GB 
[11/26 02:17:55 visual_prompt]: 	Training 400/553. train loss: 16.4838,	0.8465 s / batch. (data: 8.87e-03). ETA=3:25:00, max mem: 20.9 GB 
[11/26 02:19:31 visual_prompt]: 	Training 500/553. train loss: 3.7205,	1.5533 s / batch. (data: 7.29e-01). ETA=6:13:35, max mem: 20.9 GB 
[11/26 02:20:21 visual_prompt]: Epoch 74 / 100: avg data time: 1.47e-01, avg batch time: 0.9807, average train loss: 6.5521
[11/26 02:21:17 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3095, average loss: 5.0643
[11/26 02:21:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.67	
[11/26 02:21:17 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[11/26 02:22:59 visual_prompt]: 	Training 100/553. train loss: 5.9551,	0.8400 s / batch. (data: 3.20e-04). ETA=3:19:53, max mem: 20.9 GB 
[11/26 02:24:37 visual_prompt]: 	Training 200/553. train loss: 8.2169,	0.8240 s / batch. (data: 3.10e-04). ETA=3:14:42, max mem: 20.9 GB 
[11/26 02:26:14 visual_prompt]: 	Training 300/553. train loss: 1.0150,	0.8786 s / batch. (data: 2.95e-02). ETA=3:26:09, max mem: 20.9 GB 
[11/26 02:27:53 visual_prompt]: 	Training 400/553. train loss: 16.9524,	1.8832 s / batch. (data: 1.05e+00). ETA=7:18:42, max mem: 20.9 GB 
[11/26 02:29:29 visual_prompt]: 	Training 500/553. train loss: 6.7834,	0.8360 s / batch. (data: 2.87e-04). ETA=3:13:22, max mem: 20.9 GB 
[11/26 02:30:20 visual_prompt]: Epoch 75 / 100: avg data time: 1.48e-01, avg batch time: 0.9815, average train loss: 5.1183
[11/26 02:31:15 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3110, average loss: 1.4458
[11/26 02:31:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.30	
[11/26 02:31:15 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[11/26 02:32:59 visual_prompt]: 	Training 100/553. train loss: 13.9413,	0.8438 s / batch. (data: 8.23e-04). ETA=3:13:00, max mem: 20.9 GB 
[11/26 02:34:36 visual_prompt]: 	Training 200/553. train loss: 4.9527,	0.8176 s / batch. (data: 3.33e-04). ETA=3:05:40, max mem: 20.9 GB 
[11/26 02:36:12 visual_prompt]: 	Training 300/553. train loss: 0.9832,	0.8311 s / batch. (data: 3.05e-04). ETA=3:07:20, max mem: 20.9 GB 
[11/26 02:37:48 visual_prompt]: 	Training 400/553. train loss: 7.1801,	0.8541 s / batch. (data: 3.01e-02). ETA=3:11:05, max mem: 20.9 GB 
[11/26 02:39:24 visual_prompt]: 	Training 500/553. train loss: 4.4765,	0.8361 s / batch. (data: 7.71e-04). ETA=3:05:41, max mem: 20.9 GB 
[11/26 02:40:16 visual_prompt]: Epoch 76 / 100: avg data time: 1.45e-01, avg batch time: 0.9779, average train loss: 5.7713
[11/26 02:41:12 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3101, average loss: 4.5829
[11/26 02:41:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.87	
[11/26 02:41:12 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[11/26 02:42:53 visual_prompt]: 	Training 100/553. train loss: 3.0491,	1.1826 s / batch. (data: 3.54e-01). ETA=4:19:37, max mem: 20.9 GB 
[11/26 02:44:32 visual_prompt]: 	Training 200/553. train loss: 5.3014,	0.8400 s / batch. (data: 3.32e-04). ETA=3:03:00, max mem: 20.9 GB 
[11/26 02:46:07 visual_prompt]: 	Training 300/553. train loss: 2.1961,	0.8294 s / batch. (data: 3.24e-04). ETA=2:59:19, max mem: 20.9 GB 
[11/26 02:47:45 visual_prompt]: 	Training 400/553. train loss: 2.6444,	0.8200 s / batch. (data: 3.17e-04). ETA=2:55:54, max mem: 20.9 GB 
[11/26 02:49:21 visual_prompt]: 	Training 500/553. train loss: 1.8237,	0.8533 s / batch. (data: 5.40e-03). ETA=3:01:38, max mem: 20.9 GB 
[11/26 02:50:11 visual_prompt]: Epoch 77 / 100: avg data time: 1.42e-01, avg batch time: 0.9759, average train loss: 4.3626
[11/26 02:51:07 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3103, average loss: 0.7304
[11/26 02:51:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.36	
[11/26 02:51:07 visual_prompt]: Best epoch 77: best metric: -0.730
[11/26 02:51:07 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[11/26 02:52:47 visual_prompt]: 	Training 100/553. train loss: 2.0175,	0.8280 s / batch. (data: 3.00e-04). ETA=2:54:08, max mem: 20.9 GB 
[11/26 02:54:24 visual_prompt]: 	Training 200/553. train loss: 16.9373,	0.8605 s / batch. (data: 1.05e-02). ETA=2:59:32, max mem: 20.9 GB 
[11/26 02:56:01 visual_prompt]: 	Training 300/553. train loss: 3.0199,	0.8438 s / batch. (data: 1.92e-02). ETA=2:54:39, max mem: 20.9 GB 
[11/26 02:57:39 visual_prompt]: 	Training 400/553. train loss: 3.3504,	0.8267 s / batch. (data: 7.96e-03). ETA=2:49:44, max mem: 20.9 GB 
[11/26 02:59:15 visual_prompt]: 	Training 500/553. train loss: 1.7894,	0.8325 s / batch. (data: 3.24e-04). ETA=2:49:32, max mem: 20.9 GB 
[11/26 03:00:07 visual_prompt]: Epoch 78 / 100: avg data time: 1.42e-01, avg batch time: 0.9755, average train loss: 4.6980
[11/26 03:01:02 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3103, average loss: 1.2920
[11/26 03:01:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.01	
[11/26 03:01:02 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[11/26 03:02:45 visual_prompt]: 	Training 100/553. train loss: 0.5852,	0.8175 s / batch. (data: 3.09e-04). ETA=2:44:23, max mem: 20.9 GB 
[11/26 03:04:21 visual_prompt]: 	Training 200/553. train loss: 6.9634,	0.8376 s / batch. (data: 1.05e-02). ETA=2:47:02, max mem: 20.9 GB 
[11/26 03:05:55 visual_prompt]: 	Training 300/553. train loss: 0.8293,	1.0816 s / batch. (data: 2.64e-01). ETA=3:33:54, max mem: 20.9 GB 
[11/26 03:07:36 visual_prompt]: 	Training 400/553. train loss: 1.0083,	0.8240 s / batch. (data: 3.13e-04). ETA=2:41:35, max mem: 20.9 GB 
[11/26 03:09:14 visual_prompt]: 	Training 500/553. train loss: 10.3924,	0.8625 s / batch. (data: 7.72e-04). ETA=2:47:42, max mem: 20.9 GB 
[11/26 03:10:03 visual_prompt]: Epoch 79 / 100: avg data time: 1.43e-01, avg batch time: 0.9779, average train loss: 4.3958
[11/26 03:10:59 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3117, average loss: 7.9201
[11/26 03:10:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.35	
[11/26 03:10:59 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[11/26 03:12:39 visual_prompt]: 	Training 100/553. train loss: 0.8313,	0.8440 s / batch. (data: 3.46e-04). ETA=2:41:56, max mem: 20.9 GB 
[11/26 03:14:16 visual_prompt]: 	Training 200/553. train loss: 0.8927,	0.8509 s / batch. (data: 1.05e-02). ETA=2:41:51, max mem: 20.9 GB 
[11/26 03:15:53 visual_prompt]: 	Training 300/553. train loss: 2.7040,	1.3160 s / batch. (data: 4.60e-01). ETA=4:08:07, max mem: 20.9 GB 
[11/26 03:17:32 visual_prompt]: 	Training 400/553. train loss: 14.2382,	1.3320 s / batch. (data: 5.10e-01). ETA=4:08:55, max mem: 20.9 GB 
[11/26 03:19:07 visual_prompt]: 	Training 500/553. train loss: 11.6514,	1.0647 s / batch. (data: 2.39e-01). ETA=3:17:11, max mem: 20.9 GB 
[11/26 03:19:58 visual_prompt]: Epoch 80 / 100: avg data time: 1.41e-01, avg batch time: 0.9749, average train loss: 3.7876
[11/26 03:20:54 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3094, average loss: 4.9617
[11/26 03:20:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.33	
[11/26 03:20:54 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[11/26 03:22:36 visual_prompt]: 	Training 100/553. train loss: 1.7479,	0.8399 s / batch. (data: 3.12e-04). ETA=2:33:25, max mem: 20.9 GB 
[11/26 03:24:16 visual_prompt]: 	Training 200/553. train loss: 10.2824,	0.8754 s / batch. (data: 1.56e-02). ETA=2:38:26, max mem: 20.9 GB 
[11/26 03:25:51 visual_prompt]: 	Training 300/553. train loss: 2.4862,	0.8480 s / batch. (data: 3.24e-04). ETA=2:32:04, max mem: 20.9 GB 
[11/26 03:27:28 visual_prompt]: 	Training 400/553. train loss: 4.9085,	1.5447 s / batch. (data: 7.18e-01). ETA=4:34:26, max mem: 20.9 GB 
[11/26 03:29:03 visual_prompt]: 	Training 500/553. train loss: 1.2232,	1.9243 s / batch. (data: 1.09e+00). ETA=5:38:41, max mem: 20.9 GB 
[11/26 03:29:55 visual_prompt]: Epoch 81 / 100: avg data time: 1.45e-01, avg batch time: 0.9777, average train loss: 3.2534
[11/26 03:30:51 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3104, average loss: 1.6483
[11/26 03:30:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.38	
[11/26 03:30:51 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[11/26 03:32:32 visual_prompt]: 	Training 100/553. train loss: 0.8718,	0.8308 s / batch. (data: 3.26e-04). ETA=2:24:05, max mem: 20.9 GB 
[11/26 03:34:11 visual_prompt]: 	Training 200/553. train loss: 1.0874,	0.8399 s / batch. (data: 3.24e-04). ETA=2:24:17, max mem: 20.9 GB 
[11/26 03:35:47 visual_prompt]: 	Training 300/553. train loss: 0.8942,	1.8360 s / batch. (data: 1.02e+00). ETA=5:12:19, max mem: 20.9 GB 
[11/26 03:37:22 visual_prompt]: 	Training 400/553. train loss: 2.0164,	1.9044 s / batch. (data: 1.08e+00). ETA=5:20:47, max mem: 20.9 GB 
[11/26 03:39:00 visual_prompt]: 	Training 500/553. train loss: 0.6835,	0.8351 s / batch. (data: 5.42e-03). ETA=2:19:16, max mem: 20.9 GB 
[11/26 03:39:50 visual_prompt]: Epoch 82 / 100: avg data time: 1.42e-01, avg batch time: 0.9759, average train loss: 2.6596
[11/26 03:40:46 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3083, average loss: 3.6602
[11/26 03:40:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.97	
[11/26 03:40:46 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[11/26 03:42:28 visual_prompt]: 	Training 100/553. train loss: 3.3865,	0.8639 s / batch. (data: 1.55e-03). ETA=2:21:53, max mem: 20.9 GB 
[11/26 03:44:07 visual_prompt]: 	Training 200/553. train loss: 19.2810,	0.8407 s / batch. (data: 1.95e-02). ETA=2:16:40, max mem: 20.9 GB 
[11/26 03:45:44 visual_prompt]: 	Training 300/553. train loss: 6.4987,	0.8600 s / batch. (data: 2.96e-04). ETA=2:18:22, max mem: 20.9 GB 
[11/26 03:47:22 visual_prompt]: 	Training 400/553. train loss: 3.1918,	0.8520 s / batch. (data: 2.92e-04). ETA=2:15:39, max mem: 20.9 GB 
[11/26 03:49:00 visual_prompt]: 	Training 500/553. train loss: 0.7034,	0.8468 s / batch. (data: 2.28e-02). ETA=2:13:25, max mem: 20.9 GB 
[11/26 03:49:49 visual_prompt]: Epoch 83 / 100: avg data time: 1.48e-01, avg batch time: 0.9816, average train loss: 2.7616
[11/26 03:50:45 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3088, average loss: 3.2479
[11/26 03:50:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.66	
[11/26 03:50:45 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[11/26 03:52:27 visual_prompt]: 	Training 100/553. train loss: 0.7517,	0.8249 s / batch. (data: 3.27e-04). ETA=2:07:52, max mem: 20.9 GB 
[11/26 03:54:03 visual_prompt]: 	Training 200/553. train loss: 6.3723,	1.6167 s / batch. (data: 7.75e-01). ETA=4:07:55, max mem: 20.9 GB 
[11/26 03:55:40 visual_prompt]: 	Training 300/553. train loss: 0.7850,	0.8245 s / batch. (data: 3.07e-04). ETA=2:05:04, max mem: 20.9 GB 
[11/26 03:57:19 visual_prompt]: 	Training 400/553. train loss: 0.5892,	0.8393 s / batch. (data: 2.99e-04). ETA=2:05:54, max mem: 20.9 GB 
[11/26 03:58:54 visual_prompt]: 	Training 500/553. train loss: 0.3949,	0.8669 s / batch. (data: 1.06e-02). ETA=2:08:36, max mem: 20.9 GB 
[11/26 03:59:47 visual_prompt]: Epoch 84 / 100: avg data time: 1.48e-01, avg batch time: 0.9808, average train loss: 2.4993
[11/26 04:00:43 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3109, average loss: 2.2649
[11/26 04:00:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.07	
[11/26 04:00:43 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[11/26 04:02:26 visual_prompt]: 	Training 100/553. train loss: 1.3921,	2.9444 s / batch. (data: 2.13e+00). ETA=7:09:17, max mem: 20.9 GB 
[11/26 04:04:02 visual_prompt]: 	Training 200/553. train loss: 4.1894,	1.0017 s / batch. (data: 1.60e-01). ETA=2:24:22, max mem: 20.9 GB 
[11/26 04:05:39 visual_prompt]: 	Training 300/553. train loss: 1.8140,	0.8366 s / batch. (data: 3.23e-04). ETA=1:59:11, max mem: 20.9 GB 
[11/26 04:07:16 visual_prompt]: 	Training 400/553. train loss: 3.1759,	0.8287 s / batch. (data: 5.45e-03). ETA=1:56:40, max mem: 20.9 GB 
[11/26 04:08:54 visual_prompt]: 	Training 500/553. train loss: 2.5262,	0.8322 s / batch. (data: 2.88e-04). ETA=1:55:46, max mem: 20.9 GB 
[11/26 04:09:44 visual_prompt]: Epoch 85 / 100: avg data time: 1.46e-01, avg batch time: 0.9787, average train loss: 1.9503
[11/26 04:10:39 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3073, average loss: 2.5191
[11/26 04:10:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.71	
[11/26 04:10:39 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[11/26 04:12:23 visual_prompt]: 	Training 100/553. train loss: 0.4905,	2.9187 s / batch. (data: 2.10e+00). ETA=6:38:38, max mem: 20.9 GB 
[11/26 04:13:58 visual_prompt]: 	Training 200/553. train loss: 0.5548,	0.8360 s / batch. (data: 3.35e-04). ETA=1:52:47, max mem: 20.9 GB 
[11/26 04:15:34 visual_prompt]: 	Training 300/553. train loss: 3.0682,	0.8756 s / batch. (data: 3.39e-04). ETA=1:56:40, max mem: 20.9 GB 
[11/26 04:17:13 visual_prompt]: 	Training 400/553. train loss: 1.0020,	0.8633 s / batch. (data: 1.53e-02). ETA=1:53:35, max mem: 20.9 GB 
[11/26 04:18:51 visual_prompt]: 	Training 500/553. train loss: 2.6574,	0.8640 s / batch. (data: 1.20e-02). ETA=1:52:14, max mem: 20.9 GB 
[11/26 04:19:43 visual_prompt]: Epoch 86 / 100: avg data time: 1.48e-01, avg batch time: 0.9822, average train loss: 1.7409
[11/26 04:20:38 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3096, average loss: 0.8590
[11/26 04:20:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 54.43	
[11/26 04:20:38 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[11/26 04:22:21 visual_prompt]: 	Training 100/553. train loss: 0.8736,	0.8373 s / batch. (data: 3.19e-04). ETA=1:46:38, max mem: 20.9 GB 
[11/26 04:24:00 visual_prompt]: 	Training 200/553. train loss: 3.4571,	0.8511 s / batch. (data: 1.05e-02). ETA=1:46:59, max mem: 20.9 GB 
[11/26 04:25:38 visual_prompt]: 	Training 300/553. train loss: 1.0294,	1.5461 s / batch. (data: 7.14e-01). ETA=3:11:45, max mem: 20.9 GB 
[11/26 04:27:13 visual_prompt]: 	Training 400/553. train loss: 0.6380,	0.8500 s / batch. (data: 2.87e-04). ETA=1:44:00, max mem: 20.9 GB 
[11/26 04:28:50 visual_prompt]: 	Training 500/553. train loss: 1.9471,	0.8433 s / batch. (data: 1.20e-02). ETA=1:41:47, max mem: 20.9 GB 
[11/26 04:29:41 visual_prompt]: Epoch 87 / 100: avg data time: 1.46e-01, avg batch time: 0.9806, average train loss: 1.5781
[11/26 04:30:36 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3095, average loss: 1.7448
[11/26 04:30:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.57	
[11/26 04:30:36 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[11/26 04:32:17 visual_prompt]: 	Training 100/553. train loss: 0.0008,	0.8360 s / batch. (data: 3.15e-04). ETA=1:38:46, max mem: 20.9 GB 
[11/26 04:33:54 visual_prompt]: 	Training 200/553. train loss: 1.7567,	0.8244 s / batch. (data: 5.47e-03). ETA=1:36:01, max mem: 20.9 GB 
[11/26 04:35:34 visual_prompt]: 	Training 300/553. train loss: 0.4015,	0.8360 s / batch. (data: 3.14e-04). ETA=1:35:59, max mem: 20.9 GB 
[11/26 04:37:14 visual_prompt]: 	Training 400/553. train loss: 1.1784,	2.2543 s / batch. (data: 1.42e+00). ETA=4:15:04, max mem: 20.9 GB 
[11/26 04:38:48 visual_prompt]: 	Training 500/553. train loss: 3.3776,	0.8480 s / batch. (data: 3.15e-04). ETA=1:34:32, max mem: 20.9 GB 
[11/26 04:39:39 visual_prompt]: Epoch 88 / 100: avg data time: 1.48e-01, avg batch time: 0.9814, average train loss: 1.7065
[11/26 04:40:34 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3089, average loss: 0.7878
[11/26 04:40:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.60	
[11/26 04:40:34 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[11/26 04:42:16 visual_prompt]: 	Training 100/553. train loss: 1.7130,	0.8462 s / batch. (data: 3.28e-04). ETA=1:32:10, max mem: 20.9 GB 
[11/26 04:43:52 visual_prompt]: 	Training 200/553. train loss: 0.5703,	0.8480 s / batch. (data: 3.00e-04). ETA=1:30:57, max mem: 20.9 GB 
[11/26 04:45:31 visual_prompt]: 	Training 300/553. train loss: 1.2648,	0.8360 s / batch. (data: 3.16e-04). ETA=1:28:17, max mem: 20.9 GB 
[11/26 04:47:09 visual_prompt]: 	Training 400/553. train loss: 0.1427,	0.8714 s / batch. (data: 1.56e-02). ETA=1:30:33, max mem: 20.9 GB 
[11/26 04:48:45 visual_prompt]: 	Training 500/553. train loss: 2.9255,	0.8320 s / batch. (data: 2.73e-04). ETA=1:25:05, max mem: 20.9 GB 
[11/26 04:49:35 visual_prompt]: Epoch 89 / 100: avg data time: 1.44e-01, avg batch time: 0.9783, average train loss: 1.1049
[11/26 04:50:31 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3114, average loss: 0.6922
[11/26 04:50:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.77	
[11/26 04:50:31 visual_prompt]: Best epoch 89: best metric: -0.692
[11/26 04:50:31 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[11/26 04:52:14 visual_prompt]: 	Training 100/553. train loss: 0.7318,	0.8480 s / batch. (data: 2.92e-04). ETA=1:24:33, max mem: 20.9 GB 
[11/26 04:53:49 visual_prompt]: 	Training 200/553. train loss: 0.9195,	1.4843 s / batch. (data: 6.63e-01). ETA=2:25:32, max mem: 20.9 GB 
[11/26 04:55:26 visual_prompt]: 	Training 300/553. train loss: 2.4238,	0.8481 s / batch. (data: 2.70e-04). ETA=1:21:44, max mem: 20.9 GB 
[11/26 04:57:04 visual_prompt]: 	Training 400/553. train loss: 0.5415,	1.3955 s / batch. (data: 5.79e-01). ETA=2:12:10, max mem: 20.9 GB 
[11/26 04:58:42 visual_prompt]: 	Training 500/553. train loss: 0.6259,	0.8183 s / batch. (data: 2.58e-04). ETA=1:16:08, max mem: 20.9 GB 
[11/26 04:59:31 visual_prompt]: Epoch 90 / 100: avg data time: 1.44e-01, avg batch time: 0.9777, average train loss: 1.1953
[11/26 05:00:27 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3108, average loss: 0.7164
[11/26 05:00:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.27	
[11/26 05:00:27 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[11/26 05:02:10 visual_prompt]: 	Training 100/553. train loss: 0.7038,	0.8311 s / batch. (data: 5.43e-03). ETA=1:15:13, max mem: 20.9 GB 
[11/26 05:03:49 visual_prompt]: 	Training 200/553. train loss: 1.1274,	0.8405 s / batch. (data: 1.05e-02). ETA=1:14:40, max mem: 20.9 GB 
[11/26 05:05:27 visual_prompt]: 	Training 300/553. train loss: 0.7324,	0.8400 s / batch. (data: 2.93e-04). ETA=1:13:13, max mem: 20.9 GB 
[11/26 05:07:07 visual_prompt]: 	Training 400/553. train loss: 0.8049,	1.5724 s / batch. (data: 7.56e-01). ETA=2:14:26, max mem: 20.9 GB 
[11/26 05:08:41 visual_prompt]: 	Training 500/553. train loss: 0.6277,	0.8442 s / batch. (data: 2.06e-02). ETA=1:10:46, max mem: 20.9 GB 
[11/26 05:09:31 visual_prompt]: Epoch 91 / 100: avg data time: 1.48e-01, avg batch time: 0.9834, average train loss: 1.0164
[11/26 05:10:26 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3110, average loss: 0.7110
[11/26 05:10:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.51	
[11/26 05:10:26 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[11/26 05:12:10 visual_prompt]: 	Training 100/553. train loss: 0.6237,	1.8240 s / batch. (data: 9.99e-01). ETA=2:28:15, max mem: 20.9 GB 
[11/26 05:13:46 visual_prompt]: 	Training 200/553. train loss: 0.5295,	0.8399 s / batch. (data: 3.00e-04). ETA=1:06:52, max mem: 20.9 GB 
[11/26 05:15:22 visual_prompt]: 	Training 300/553. train loss: 0.6465,	0.8505 s / batch. (data: 1.05e-02). ETA=1:06:17, max mem: 20.9 GB 
[11/26 05:17:02 visual_prompt]: 	Training 400/553. train loss: 0.7094,	0.8522 s / batch. (data: 2.41e-02). ETA=1:05:00, max mem: 20.9 GB 
[11/26 05:18:37 visual_prompt]: 	Training 500/553. train loss: 0.9332,	0.8322 s / batch. (data: 1.05e-02). ETA=1:02:05, max mem: 20.9 GB 
[11/26 05:19:28 visual_prompt]: Epoch 92 / 100: avg data time: 1.45e-01, avg batch time: 0.9787, average train loss: 0.9226
[11/26 05:20:23 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3109, average loss: 1.0049
[11/26 05:20:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.61	
[11/26 05:20:23 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[11/26 05:22:04 visual_prompt]: 	Training 100/553. train loss: 0.9730,	0.8633 s / batch. (data: 1.55e-02). ETA=1:02:13, max mem: 20.9 GB 
[11/26 05:23:42 visual_prompt]: 	Training 200/553. train loss: 0.7291,	0.9801 s / batch. (data: 1.48e-01). ETA=1:08:59, max mem: 20.9 GB 
[11/26 05:25:18 visual_prompt]: 	Training 300/553. train loss: 0.5906,	0.8305 s / batch. (data: 1.28e-02). ETA=0:57:04, max mem: 20.9 GB 
[11/26 05:26:56 visual_prompt]: 	Training 400/553. train loss: 0.9301,	0.8295 s / batch. (data: 5.43e-03). ETA=0:55:37, max mem: 20.9 GB 
[11/26 05:28:36 visual_prompt]: 	Training 500/553. train loss: 0.6078,	0.8317 s / batch. (data: 2.92e-04). ETA=0:54:23, max mem: 20.9 GB 
[11/26 05:29:26 visual_prompt]: Epoch 93 / 100: avg data time: 1.47e-01, avg batch time: 0.9809, average train loss: 0.9613
[11/26 05:30:21 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3098, average loss: 0.6954
[11/26 05:30:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.57	
[11/26 05:30:21 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[11/26 05:32:03 visual_prompt]: 	Training 100/553. train loss: 0.8118,	0.9976 s / batch. (data: 1.53e-01). ETA=1:02:41, max mem: 20.9 GB 
[11/26 05:33:39 visual_prompt]: 	Training 200/553. train loss: 0.7761,	0.8360 s / batch. (data: 3.20e-04). ETA=0:51:08, max mem: 20.9 GB 
[11/26 05:35:19 visual_prompt]: 	Training 300/553. train loss: 0.7036,	0.8177 s / batch. (data: 3.12e-04). ETA=0:48:39, max mem: 20.9 GB 
[11/26 05:36:55 visual_prompt]: 	Training 400/553. train loss: 0.7895,	0.8720 s / batch. (data: 3.43e-02). ETA=0:50:26, max mem: 20.9 GB 
[11/26 05:38:31 visual_prompt]: 	Training 500/553. train loss: 0.8331,	0.8400 s / batch. (data: 3.06e-04). ETA=0:47:11, max mem: 20.9 GB 
[11/26 05:39:24 visual_prompt]: Epoch 94 / 100: avg data time: 1.48e-01, avg batch time: 0.9810, average train loss: 0.8435
[11/26 05:40:19 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3102, average loss: 0.7070
[11/26 05:40:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.32	
[11/26 05:40:19 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[11/26 05:42:00 visual_prompt]: 	Training 100/553. train loss: 0.6114,	0.8520 s / batch. (data: 3.05e-04). ETA=0:45:41, max mem: 20.9 GB 
[11/26 05:43:40 visual_prompt]: 	Training 200/553. train loss: 0.4697,	0.8478 s / batch. (data: 1.84e-02). ETA=0:44:03, max mem: 20.9 GB 
[11/26 05:45:16 visual_prompt]: 	Training 300/553. train loss: 0.8146,	1.7080 s / batch. (data: 8.71e-01). ETA=1:25:54, max mem: 20.9 GB 
[11/26 05:46:53 visual_prompt]: 	Training 400/553. train loss: 1.1747,	1.6420 s / batch. (data: 8.08e-01). ETA=1:19:51, max mem: 20.9 GB 
[11/26 05:48:30 visual_prompt]: 	Training 500/553. train loss: 0.2609,	0.8333 s / batch. (data: 7.95e-03). ETA=0:39:08, max mem: 20.9 GB 
[11/26 05:49:21 visual_prompt]: Epoch 95 / 100: avg data time: 1.47e-01, avg batch time: 0.9805, average train loss: 0.7454
[11/26 05:50:17 visual_prompt]: Inference (val):avg data time: 3.59e-04, avg batch time: 0.3109, average loss: 0.7121
[11/26 05:50:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.12	
[11/26 05:50:17 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[11/26 05:52:01 visual_prompt]: 	Training 100/553. train loss: 0.6479,	0.8240 s / batch. (data: 3.09e-04). ETA=0:36:35, max mem: 20.9 GB 
[11/26 05:53:40 visual_prompt]: 	Training 200/553. train loss: 0.7923,	0.8564 s / batch. (data: 1.09e-02). ETA=0:36:36, max mem: 20.9 GB 
[11/26 05:55:15 visual_prompt]: 	Training 300/553. train loss: 0.7058,	0.8495 s / batch. (data: 1.05e-02). ETA=0:34:54, max mem: 20.9 GB 
[11/26 05:56:53 visual_prompt]: 	Training 400/553. train loss: 0.8523,	0.8374 s / batch. (data: 5.43e-03). ETA=0:33:00, max mem: 20.9 GB 
[11/26 05:58:30 visual_prompt]: 	Training 500/553. train loss: 0.8478,	0.8600 s / batch. (data: 3.40e-04). ETA=0:32:27, max mem: 20.9 GB 
[11/26 05:59:19 visual_prompt]: Epoch 96 / 100: avg data time: 1.46e-01, avg batch time: 0.9806, average train loss: 0.7558
[11/26 06:00:15 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3106, average loss: 0.7252
[11/26 06:00:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.07	
[11/26 06:00:15 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[11/26 06:01:55 visual_prompt]: 	Training 100/553. train loss: 0.7071,	0.8361 s / batch. (data: 1.64e-02). ETA=0:29:25, max mem: 20.9 GB 
[11/26 06:03:35 visual_prompt]: 	Training 200/553. train loss: 0.5797,	0.8205 s / batch. (data: 3.08e-04). ETA=0:27:30, max mem: 20.9 GB 
[11/26 06:05:12 visual_prompt]: 	Training 300/553. train loss: 0.6748,	0.8354 s / batch. (data: 3.12e-04). ETA=0:26:37, max mem: 20.9 GB 
[11/26 06:06:50 visual_prompt]: 	Training 400/553. train loss: 0.5209,	0.8764 s / batch. (data: 6.96e-04). ETA=0:26:28, max mem: 20.9 GB 
[11/26 06:08:25 visual_prompt]: 	Training 500/553. train loss: 0.9771,	0.8349 s / batch. (data: 7.56e-04). ETA=0:23:49, max mem: 20.9 GB 
[11/26 06:09:18 visual_prompt]: Epoch 97 / 100: avg data time: 1.48e-01, avg batch time: 0.9821, average train loss: 0.7416
[11/26 06:10:13 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3098, average loss: 0.7607
[11/26 06:10:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.37	
[11/26 06:10:13 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[11/26 06:11:57 visual_prompt]: 	Training 100/553. train loss: 0.7290,	0.8312 s / batch. (data: 7.90e-04). ETA=0:21:35, max mem: 20.9 GB 
[11/26 06:13:33 visual_prompt]: 	Training 200/553. train loss: 0.6622,	0.8704 s / batch. (data: 7.95e-03). ETA=0:21:09, max mem: 20.9 GB 
[11/26 06:15:11 visual_prompt]: 	Training 300/553. train loss: 0.6998,	2.2000 s / batch. (data: 1.36e+00). ETA=0:49:49, max mem: 20.9 GB 
[11/26 06:16:48 visual_prompt]: 	Training 400/553. train loss: 0.6972,	1.4651 s / batch. (data: 6.40e-01). ETA=0:30:44, max mem: 20.9 GB 
[11/26 06:18:25 visual_prompt]: 	Training 500/553. train loss: 0.6701,	0.8222 s / batch. (data: 2.94e-04). ETA=0:15:52, max mem: 20.9 GB 
[11/26 06:19:16 visual_prompt]: Epoch 98 / 100: avg data time: 1.47e-01, avg batch time: 0.9812, average train loss: 0.7065
[11/26 06:20:12 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.3096, average loss: 0.6931
[11/26 06:20:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.29	
[11/26 06:20:12 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[11/26 06:21:53 visual_prompt]: 	Training 100/553. train loss: 0.6800,	0.8450 s / batch. (data: 8.89e-03). ETA=0:14:10, max mem: 20.9 GB 
[11/26 06:23:32 visual_prompt]: 	Training 200/553. train loss: 0.7118,	0.8427 s / batch. (data: 1.05e-02). ETA=0:12:43, max mem: 20.9 GB 
[11/26 06:25:11 visual_prompt]: 	Training 300/553. train loss: 0.6977,	1.3773 s / batch. (data: 5.61e-01). ETA=0:18:30, max mem: 20.9 GB 
[11/26 06:26:46 visual_prompt]: 	Training 400/553. train loss: 0.5337,	0.8276 s / batch. (data: 9.27e-03). ETA=0:09:44, max mem: 20.9 GB 
[11/26 06:28:22 visual_prompt]: 	Training 500/553. train loss: 0.6920,	0.8480 s / batch. (data: 1.20e-02). ETA=0:08:33, max mem: 20.9 GB 
[11/26 06:29:14 visual_prompt]: Epoch 99 / 100: avg data time: 1.46e-01, avg batch time: 0.9799, average train loss: 0.6978
[11/26 06:30:09 visual_prompt]: Inference (val):avg data time: 4.09e-04, avg batch time: 0.3103, average loss: 0.7010
[11/26 06:30:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.53	
[11/26 06:30:09 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[11/26 06:31:54 visual_prompt]: 	Training 100/553. train loss: 0.7001,	0.8400 s / batch. (data: 3.41e-04). ETA=0:06:20, max mem: 20.9 GB 
[11/26 06:33:29 visual_prompt]: 	Training 200/553. train loss: 0.6907,	0.8317 s / batch. (data: 3.35e-04). ETA=0:04:53, max mem: 20.9 GB 
[11/26 06:35:08 visual_prompt]: 	Training 300/553. train loss: 0.6684,	0.8440 s / batch. (data: 1.60e-02). ETA=0:03:33, max mem: 20.9 GB 
[11/26 06:36:46 visual_prompt]: 	Training 400/553. train loss: 0.8026,	0.8400 s / batch. (data: 2.96e-04). ETA=0:02:08, max mem: 20.9 GB 
[11/26 06:38:23 visual_prompt]: 	Training 500/553. train loss: 0.6228,	0.8440 s / batch. (data: 3.16e-04). ETA=0:00:44, max mem: 20.9 GB 
[11/26 06:39:12 visual_prompt]: Epoch 100 / 100: avg data time: 1.49e-01, avg batch time: 0.9822, average train loss: 0.6925
[11/26 06:40:08 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3102, average loss: 0.6868
[11/26 06:40:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.97	
[11/26 06:40:08 visual_prompt]: Best epoch 100: best metric: -0.687
[11/26 06:40:08 visual_prompt]: Rank of current process: 0. World size: 1
[11/26 06:40:08 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/26 06:40:08 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/26 06:40:08 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/26 06:40:08 visual_prompt]: Training with config:
[11/26 06:40:08 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/26 06:40:08 visual_prompt]: Loading training data...
[11/26 06:40:08 visual_prompt]: Constructing mammo-cbis dataset train...
[11/26 06:40:08 visual_prompt]: Loading validation data...
[11/26 06:40:08 visual_prompt]: Constructing mammo-cbis dataset val...
[11/26 06:40:09 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/26 06:40:16 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/26 06:40:16 visual_prompt]: tuned percent:0.525
[11/26 06:40:16 visual_prompt]: Device used for model: 0
[11/26 06:40:16 visual_prompt]: Setting up Evaluator...
[11/26 06:40:16 visual_prompt]: Setting up Trainer...
[11/26 06:40:16 visual_prompt]: 	Setting up the optimizer...
[11/26 06:40:16 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/26 06:41:58 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8303 s / batch. (data: 2.94e-04). ETA=12:43:50, max mem: 20.9 GB 
[11/26 06:43:33 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8280 s / batch. (data: 7.95e-03). ETA=12:40:22, max mem: 20.9 GB 
[11/26 06:45:13 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8360 s / batch. (data: 3.43e-04). ETA=12:46:20, max mem: 20.9 GB 
[11/26 06:46:50 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8280 s / batch. (data: 3.24e-04). ETA=12:37:39, max mem: 20.9 GB 
[11/26 06:48:29 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8360 s / batch. (data: 3.09e-04). ETA=12:43:33, max mem: 20.9 GB 
[11/26 06:49:20 visual_prompt]: Epoch 1 / 100: avg data time: 1.49e-01, avg batch time: 0.9831, average train loss: 1.5403
[11/26 06:50:15 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3095, average loss: 1.5201
[11/26 06:50:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/26 06:50:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/26 06:51:56 visual_prompt]: 	Training 100/553. train loss: 3.8541,	0.8337 s / batch. (data: 5.44e-03). ETA=12:39:17, max mem: 20.9 GB 
[11/26 06:53:33 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8398 s / batch. (data: 7.75e-03). ETA=12:43:27, max mem: 20.9 GB 
[11/26 06:55:12 visual_prompt]: 	Training 300/553. train loss: 2.6410,	0.8360 s / batch. (data: 2.92e-04). ETA=12:38:38, max mem: 20.9 GB 
[11/26 06:56:49 visual_prompt]: 	Training 400/553. train loss: 1.6751,	0.8213 s / batch. (data: 2.98e-04). ETA=12:23:52, max mem: 20.9 GB 
[11/26 06:58:27 visual_prompt]: 	Training 500/553. train loss: 0.5674,	0.8269 s / batch. (data: 2.89e-04). ETA=12:27:35, max mem: 20.9 GB 
[11/26 06:59:17 visual_prompt]: Epoch 2 / 100: avg data time: 1.46e-01, avg batch time: 0.9791, average train loss: 3.2359
[11/26 07:00:13 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3107, average loss: 10.4632
[11/26 07:00:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.18	
[11/26 07:00:13 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/26 07:01:54 visual_prompt]: 	Training 100/553. train loss: 24.3151,	0.8231 s / batch. (data: 3.20e-04). ETA=12:22:04, max mem: 20.9 GB 
[11/26 07:03:32 visual_prompt]: 	Training 200/553. train loss: 4.6712,	0.8368 s / batch. (data: 8.74e-03). ETA=12:33:03, max mem: 20.9 GB 
[11/26 07:05:09 visual_prompt]: 	Training 300/553. train loss: 2.6836,	0.8809 s / batch. (data: 2.09e-02). ETA=13:11:16, max mem: 20.9 GB 
[11/26 07:06:47 visual_prompt]: 	Training 400/553. train loss: 6.9202,	0.8273 s / batch. (data: 5.44e-03). ETA=12:21:42, max mem: 20.9 GB 
[11/26 07:08:26 visual_prompt]: 	Training 500/553. train loss: 2.7821,	1.2070 s / batch. (data: 3.76e-01). ETA=18:00:10, max mem: 20.9 GB 
[11/26 07:09:16 visual_prompt]: Epoch 3 / 100: avg data time: 1.49e-01, avg batch time: 0.9828, average train loss: 6.4887
[11/26 07:10:12 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3107, average loss: 7.2072
[11/26 07:10:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.13	
[11/26 07:10:12 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/26 07:11:56 visual_prompt]: 	Training 100/553. train loss: 21.5205,	0.8520 s / batch. (data: 3.05e-04). ETA=12:40:16, max mem: 20.9 GB 
[11/26 07:13:33 visual_prompt]: 	Training 200/553. train loss: 17.5341,	0.8253 s / batch. (data: 3.03e-04). ETA=12:15:06, max mem: 20.9 GB 
[11/26 07:15:10 visual_prompt]: 	Training 300/553. train loss: 1.5394,	1.0762 s / batch. (data: 2.39e-01). ETA=15:56:43, max mem: 20.9 GB 
[11/26 07:16:44 visual_prompt]: 	Training 400/553. train loss: 17.8045,	1.1907 s / batch. (data: 3.52e-01). ETA=17:36:35, max mem: 20.9 GB 
[11/26 07:18:23 visual_prompt]: 	Training 500/553. train loss: 15.0734,	3.2397 s / batch. (data: 2.42e+00). ETA=1 day, 23:49:20, max mem: 20.9 GB 
[11/26 07:19:15 visual_prompt]: Epoch 4 / 100: avg data time: 1.47e-01, avg batch time: 0.9818, average train loss: 8.8992
[11/26 07:20:11 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3102, average loss: 7.4956
[11/26 07:20:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.73	
[11/26 07:20:11 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/26 07:21:51 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8440 s / batch. (data: 2.96e-04). ETA=12:25:20, max mem: 20.9 GB 
[11/26 07:23:29 visual_prompt]: 	Training 200/553. train loss: 26.0106,	1.2726 s / batch. (data: 4.37e-01). ETA=18:41:42, max mem: 20.9 GB 
[11/26 07:25:08 visual_prompt]: 	Training 300/553. train loss: 27.0224,	0.8596 s / batch. (data: 1.56e-02). ETA=12:36:15, max mem: 20.9 GB 
[11/26 07:26:45 visual_prompt]: 	Training 400/553. train loss: 5.8865,	0.8449 s / batch. (data: 8.87e-03). ETA=12:21:56, max mem: 20.9 GB 
[11/26 07:28:22 visual_prompt]: 	Training 500/553. train loss: 20.3104,	0.8290 s / batch. (data: 3.02e-04). ETA=12:06:33, max mem: 20.9 GB 
[11/26 07:29:14 visual_prompt]: Epoch 5 / 100: avg data time: 1.49e-01, avg batch time: 0.9819, average train loss: 13.5538
[11/26 07:30:10 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3096, average loss: 19.2477
[11/26 07:30:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.87	
[11/26 07:30:10 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/26 07:31:53 visual_prompt]: 	Training 100/553. train loss: 18.3519,	0.8400 s / batch. (data: 3.10e-04). ETA=12:14:05, max mem: 20.9 GB 
[11/26 07:33:29 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8354 s / batch. (data: 2.90e-04). ETA=12:08:41, max mem: 20.9 GB 
[11/26 07:35:06 visual_prompt]: 	Training 300/553. train loss: 10.2809,	0.8288 s / batch. (data: 5.47e-03). ETA=12:01:34, max mem: 20.9 GB 
[11/26 07:36:46 visual_prompt]: 	Training 400/553. train loss: 44.9343,	0.8371 s / batch. (data: 3.19e-04). ETA=12:07:23, max mem: 20.9 GB 
[11/26 07:38:23 visual_prompt]: 	Training 500/553. train loss: 30.3506,	0.8577 s / batch. (data: 3.08e-02). ETA=12:23:50, max mem: 20.9 GB 
[11/26 07:39:13 visual_prompt]: Epoch 6 / 100: avg data time: 1.51e-01, avg batch time: 0.9832, average train loss: 16.0221
[11/26 07:40:09 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3124, average loss: 6.9090
[11/26 07:40:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.85	
[11/26 07:40:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/26 07:41:49 visual_prompt]: 	Training 100/553. train loss: 21.6591,	0.8307 s / batch. (data: 3.33e-04). ETA=11:58:16, max mem: 20.9 GB 
[11/26 07:43:26 visual_prompt]: 	Training 200/553. train loss: 10.9828,	0.8560 s / batch. (data: 5.45e-03). ETA=12:18:44, max mem: 20.9 GB 
[11/26 07:45:07 visual_prompt]: 	Training 300/553. train loss: 16.7040,	1.7148 s / batch. (data: 8.74e-01). ETA=1 day, 0:37:03, max mem: 20.9 GB 
[11/26 07:46:45 visual_prompt]: 	Training 400/553. train loss: 3.1928,	1.9534 s / batch. (data: 1.13e+00). ETA=1 day, 3:59:20, max mem: 20.9 GB 
[11/26 07:48:21 visual_prompt]: 	Training 500/553. train loss: 8.6548,	0.8769 s / batch. (data: 2.08e-02). ETA=12:32:22, max mem: 20.9 GB 
[11/26 07:49:11 visual_prompt]: Epoch 7 / 100: avg data time: 1.47e-01, avg batch time: 0.9792, average train loss: 20.5133
[11/26 07:50:06 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3104, average loss: 20.0773
[11/26 07:50:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.11	
[11/26 07:50:06 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/26 07:51:45 visual_prompt]: 	Training 100/553. train loss: 65.4587,	0.8282 s / batch. (data: 1.05e-02). ETA=11:48:31, max mem: 20.9 GB 
[11/26 07:53:24 visual_prompt]: 	Training 200/553. train loss: 5.9290,	0.8522 s / batch. (data: 3.34e-04). ETA=12:07:39, max mem: 20.9 GB 
[11/26 07:55:01 visual_prompt]: 	Training 300/553. train loss: 20.1993,	0.8218 s / batch. (data: 3.37e-04). ETA=11:40:16, max mem: 20.9 GB 
[11/26 07:56:39 visual_prompt]: 	Training 400/553. train loss: 4.9946,	0.8360 s / batch. (data: 7.96e-03). ETA=11:50:59, max mem: 20.9 GB 
[11/26 07:58:17 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.3840 s / batch. (data: 5.32e-01). ETA=19:34:46, max mem: 20.9 GB 
[11/26 07:59:09 visual_prompt]: Epoch 8 / 100: avg data time: 1.48e-01, avg batch time: 0.9806, average train loss: 21.8628
[11/26 08:00:04 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3103, average loss: 4.8054
[11/26 08:00:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.08	
[11/26 08:00:04 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/26 08:01:45 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8280 s / batch. (data: 4.50e-04). ETA=11:40:40, max mem: 20.9 GB 
[11/26 08:03:22 visual_prompt]: 	Training 200/553. train loss: 19.8405,	0.8598 s / batch. (data: 3.82e-04). ETA=12:06:10, max mem: 20.9 GB 
[11/26 08:05:00 visual_prompt]: 	Training 300/553. train loss: 5.5630,	1.4920 s / batch. (data: 6.40e-01). ETA=20:57:39, max mem: 20.9 GB 
[11/26 08:06:40 visual_prompt]: 	Training 400/553. train loss: 23.2621,	0.8560 s / batch. (data: 2.80e-02). ETA=12:00:07, max mem: 20.9 GB 
[11/26 08:08:18 visual_prompt]: 	Training 500/553. train loss: 11.8249,	0.9387 s / batch. (data: 1.17e-01). ETA=13:08:05, max mem: 20.9 GB 
[11/26 08:09:07 visual_prompt]: Epoch 9 / 100: avg data time: 1.50e-01, avg batch time: 0.9821, average train loss: 22.3752
[11/26 08:10:03 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3104, average loss: 28.7980
[11/26 08:10:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.92	
[11/26 08:10:03 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/26 08:11:48 visual_prompt]: 	Training 100/553. train loss: 51.8149,	0.8246 s / batch. (data: 7.96e-03). ETA=11:30:14, max mem: 20.9 GB 
[11/26 08:13:24 visual_prompt]: 	Training 200/553. train loss: 40.6482,	0.8290 s / batch. (data: 5.43e-03). ETA=11:32:33, max mem: 20.9 GB 
[11/26 08:15:01 visual_prompt]: 	Training 300/553. train loss: 216.0547,	0.8560 s / batch. (data: 7.95e-03). ETA=11:53:40, max mem: 20.9 GB 
[11/26 08:16:35 visual_prompt]: 	Training 400/553. train loss: 9.2732,	0.8221 s / batch. (data: 3.31e-04). ETA=11:24:03, max mem: 20.9 GB 
[11/26 08:18:15 visual_prompt]: 	Training 500/553. train loss: 3.4918,	0.8400 s / batch. (data: 2.04e-02). ETA=11:37:29, max mem: 20.9 GB 
[11/26 08:19:06 visual_prompt]: Epoch 10 / 100: avg data time: 1.51e-01, avg batch time: 0.9810, average train loss: 33.7567
[11/26 08:20:02 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3086, average loss: 27.9118
[11/26 08:20:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 49.09	
[11/26 08:20:02 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/26 08:21:45 visual_prompt]: 	Training 100/553. train loss: 25.8767,	0.8215 s / batch. (data: 5.42e-03). ETA=11:20:06, max mem: 20.9 GB 
[11/26 08:23:24 visual_prompt]: 	Training 200/553. train loss: 43.7321,	0.8234 s / batch. (data: 5.44e-03). ETA=11:20:15, max mem: 20.9 GB 
[11/26 08:25:00 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.0439 s / batch. (data: 1.20e+00). ETA=1 day, 4:05:10, max mem: 20.9 GB 
[11/26 08:26:36 visual_prompt]: 	Training 400/553. train loss: 3.9212,	0.8640 s / batch. (data: 7.96e-03). ETA=11:50:54, max mem: 20.9 GB 
[11/26 08:28:12 visual_prompt]: 	Training 500/553. train loss: 7.5434,	0.8265 s / batch. (data: 2.49e-03). ETA=11:18:39, max mem: 20.9 GB 
[11/26 08:29:02 visual_prompt]: Epoch 11 / 100: avg data time: 1.45e-01, avg batch time: 0.9776, average train loss: 28.0970
[11/26 08:29:58 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3119, average loss: 22.0926
[11/26 08:29:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.95	
[11/26 08:29:58 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/26 08:31:41 visual_prompt]: 	Training 100/553. train loss: 13.9978,	0.8190 s / batch. (data: 3.29e-04). ETA=11:10:29, max mem: 20.9 GB 
[11/26 08:33:19 visual_prompt]: 	Training 200/553. train loss: 86.0450,	0.8351 s / batch. (data: 3.09e-04). ETA=11:22:16, max mem: 20.9 GB 
[11/26 08:34:55 visual_prompt]: 	Training 300/553. train loss: 195.4582,	0.8640 s / batch. (data: 2.79e-02). ETA=11:44:23, max mem: 20.9 GB 
[11/26 08:36:33 visual_prompt]: 	Training 400/553. train loss: 32.3267,	0.8607 s / batch. (data: 3.36e-04). ETA=11:40:19, max mem: 20.9 GB 
[11/26 08:38:10 visual_prompt]: 	Training 500/553. train loss: 104.1752,	0.8474 s / batch. (data: 2.27e-04). ETA=11:28:05, max mem: 20.9 GB 
[11/26 08:39:00 visual_prompt]: Epoch 12 / 100: avg data time: 1.49e-01, avg batch time: 0.9807, average train loss: 34.8851
[11/26 08:39:56 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3116, average loss: 8.3360
[11/26 08:39:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.81	
[11/26 08:39:56 visual_prompt]: Best epoch 12: best metric: -8.336
[11/26 08:39:56 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/26 08:41:39 visual_prompt]: 	Training 100/553. train loss: 12.3251,	0.8231 s / batch. (data: 3.13e-04). ETA=11:06:10, max mem: 20.9 GB 
[11/26 08:43:13 visual_prompt]: 	Training 200/553. train loss: 2.0794,	0.8366 s / batch. (data: 2.69e-04). ETA=11:15:45, max mem: 20.9 GB 
[11/26 08:44:52 visual_prompt]: 	Training 300/553. train loss: 61.6420,	1.7840 s / batch. (data: 9.46e-01). ETA=23:58:01, max mem: 20.9 GB 
[11/26 08:46:29 visual_prompt]: 	Training 400/553. train loss: 211.3320,	0.8162 s / batch. (data: 3.36e-04). ETA=10:56:30, max mem: 20.9 GB 
[11/26 08:48:08 visual_prompt]: 	Training 500/553. train loss: 41.9996,	0.8440 s / batch. (data: 3.12e-04). ETA=11:17:29, max mem: 20.9 GB 
[11/26 08:48:59 visual_prompt]: Epoch 13 / 100: avg data time: 1.51e-01, avg batch time: 0.9815, average train loss: 33.2361
[11/26 08:49:55 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.3117, average loss: 23.4402
[11/26 08:49:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.31	
[11/26 08:49:55 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/26 08:51:38 visual_prompt]: 	Training 100/553. train loss: 58.2859,	0.8313 s / batch. (data: 9.16e-03). ETA=11:05:13, max mem: 20.9 GB 
[11/26 08:53:15 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.0280 s / batch. (data: 1.81e-01). ETA=13:40:51, max mem: 20.9 GB 
[11/26 08:54:52 visual_prompt]: 	Training 300/553. train loss: 8.4469,	0.8531 s / batch. (data: 2.07e-02). ETA=11:19:46, max mem: 20.9 GB 
[11/26 08:56:29 visual_prompt]: 	Training 400/553. train loss: 24.6987,	0.8179 s / batch. (data: 5.42e-03). ETA=10:50:22, max mem: 20.9 GB 
[11/26 08:58:07 visual_prompt]: 	Training 500/553. train loss: 10.9003,	0.8415 s / batch. (data: 1.07e-02). ETA=11:07:42, max mem: 20.9 GB 
[11/26 08:58:56 visual_prompt]: Epoch 14 / 100: avg data time: 1.47e-01, avg batch time: 0.9786, average train loss: 27.9338
[11/26 08:59:52 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3106, average loss: 1.4110
[11/26 08:59:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.01	
[11/26 08:59:52 visual_prompt]: Best epoch 14: best metric: -1.411
[11/26 08:59:52 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/26 09:01:33 visual_prompt]: 	Training 100/553. train loss: 33.2693,	0.8527 s / batch. (data: 3.40e-04). ETA=11:14:29, max mem: 20.9 GB 
[11/26 09:03:09 visual_prompt]: 	Training 200/553. train loss: 175.5086,	0.8520 s / batch. (data: 3.21e-04). ETA=11:12:30, max mem: 20.9 GB 
[11/26 09:04:49 visual_prompt]: 	Training 300/553. train loss: 14.0868,	0.8739 s / batch. (data: 1.56e-02). ETA=11:28:17, max mem: 20.9 GB 
[11/26 09:06:24 visual_prompt]: 	Training 400/553. train loss: 43.9856,	1.0043 s / batch. (data: 1.55e-01). ETA=13:09:22, max mem: 20.9 GB 
[11/26 09:08:02 visual_prompt]: 	Training 500/553. train loss: 22.7769,	0.9272 s / batch. (data: 1.06e-01). ETA=12:07:11, max mem: 20.9 GB 
[11/26 09:08:53 visual_prompt]: Epoch 15 / 100: avg data time: 1.48e-01, avg batch time: 0.9796, average train loss: 30.8940
[11/26 09:09:49 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3117, average loss: 51.1618
[11/26 09:09:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.25	
[11/26 09:09:49 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/26 09:11:29 visual_prompt]: 	Training 100/553. train loss: 29.5155,	0.8205 s / batch. (data: 2.83e-04). ETA=10:41:25, max mem: 20.9 GB 
[11/26 09:13:06 visual_prompt]: 	Training 200/553. train loss: 37.1712,	0.8351 s / batch. (data: 1.05e-02). ETA=10:51:25, max mem: 20.9 GB 
[11/26 09:14:44 visual_prompt]: 	Training 300/553. train loss: 95.8599,	0.8500 s / batch. (data: 5.45e-03). ETA=11:01:37, max mem: 20.9 GB 
[11/26 09:16:21 visual_prompt]: 	Training 400/553. train loss: 46.6106,	0.8360 s / batch. (data: 7.71e-04). ETA=10:49:22, max mem: 20.9 GB 
[11/26 09:17:58 visual_prompt]: 	Training 500/553. train loss: 5.6730,	0.8560 s / batch. (data: 3.13e-04). ETA=11:03:28, max mem: 20.9 GB 
[11/26 09:18:49 visual_prompt]: Epoch 16 / 100: avg data time: 1.45e-01, avg batch time: 0.9771, average train loss: 30.0371
[11/26 09:19:45 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3111, average loss: 16.7110
[11/26 09:19:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.45	
[11/26 09:19:45 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/26 09:21:25 visual_prompt]: 	Training 100/553. train loss: 0.9629,	0.8480 s / batch. (data: 1.19e-02). ETA=10:55:05, max mem: 20.9 GB 
[11/26 09:23:03 visual_prompt]: 	Training 200/553. train loss: 1.0105,	0.8210 s / batch. (data: 2.60e-04). ETA=10:32:53, max mem: 20.9 GB 
[11/26 09:24:40 visual_prompt]: 	Training 300/553. train loss: 31.9760,	0.8523 s / batch. (data: 3.33e-02). ETA=10:55:33, max mem: 20.9 GB 
[11/26 09:26:17 visual_prompt]: 	Training 400/553. train loss: 60.9852,	1.0617 s / batch. (data: 2.45e-01). ETA=13:34:55, max mem: 20.9 GB 
[11/26 09:27:54 visual_prompt]: 	Training 500/553. train loss: 10.4717,	1.3720 s / batch. (data: 5.46e-01). ETA=17:30:45, max mem: 20.9 GB 
[11/26 09:28:46 visual_prompt]: Epoch 17 / 100: avg data time: 1.47e-01, avg batch time: 0.9783, average train loss: 32.3750
[11/26 09:29:41 visual_prompt]: Inference (val):avg data time: 3.03e-04, avg batch time: 0.3100, average loss: 26.2978
[11/26 09:29:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.64	
[11/26 09:29:41 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/26 09:31:23 visual_prompt]: 	Training 100/553. train loss: 37.6888,	0.8368 s / batch. (data: 2.46e-02). ETA=10:38:46, max mem: 20.9 GB 
[11/26 09:33:03 visual_prompt]: 	Training 200/553. train loss: 47.4924,	0.8556 s / batch. (data: 2.63e-03). ETA=10:51:37, max mem: 20.9 GB 
[11/26 09:34:40 visual_prompt]: 	Training 300/553. train loss: 42.6714,	0.8354 s / batch. (data: 3.09e-04). ETA=10:34:51, max mem: 20.9 GB 
[11/26 09:36:18 visual_prompt]: 	Training 400/553. train loss: 5.9467,	0.8288 s / batch. (data: 1.10e-02). ETA=10:28:27, max mem: 20.9 GB 
[11/26 09:37:54 visual_prompt]: 	Training 500/553. train loss: 25.6926,	0.8234 s / batch. (data: 2.96e-04). ETA=10:23:00, max mem: 20.9 GB 
[11/26 09:38:44 visual_prompt]: Epoch 18 / 100: avg data time: 1.52e-01, avg batch time: 0.9816, average train loss: 33.0730
[11/26 09:39:40 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3097, average loss: 18.0114
[11/26 09:39:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.93	
[11/26 09:39:40 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/26 09:41:21 visual_prompt]: 	Training 100/553. train loss: 6.7463,	1.3602 s / batch. (data: 5.43e-01). ETA=17:05:44, max mem: 20.9 GB 
[11/26 09:43:00 visual_prompt]: 	Training 200/553. train loss: 18.9321,	0.8399 s / batch. (data: 7.95e-03). ETA=10:31:59, max mem: 20.9 GB 
[11/26 09:44:38 visual_prompt]: 	Training 300/553. train loss: 18.4357,	0.8284 s / batch. (data: 7.92e-03). ETA=10:21:55, max mem: 20.9 GB 
[11/26 09:46:17 visual_prompt]: 	Training 400/553. train loss: 20.8769,	0.8162 s / batch. (data: 3.48e-04). ETA=10:11:22, max mem: 20.9 GB 
[11/26 09:47:50 visual_prompt]: 	Training 500/553. train loss: 38.7176,	0.8696 s / batch. (data: 2.96e-02). ETA=10:49:57, max mem: 20.9 GB 
[11/26 09:48:41 visual_prompt]: Epoch 19 / 100: avg data time: 1.48e-01, avg batch time: 0.9787, average train loss: 33.2714
[11/26 09:49:37 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3114, average loss: 45.4579
[11/26 09:49:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.63	
[11/26 09:49:37 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/26 09:51:17 visual_prompt]: 	Training 100/553. train loss: 12.9582,	0.8315 s / batch. (data: 7.94e-03). ETA=10:19:22, max mem: 20.9 GB 
[11/26 09:52:55 visual_prompt]: 	Training 200/553. train loss: 39.6115,	0.8400 s / batch. (data: 3.17e-04). ETA=10:24:18, max mem: 20.9 GB 
[11/26 09:54:33 visual_prompt]: 	Training 300/553. train loss: 121.8446,	0.8382 s / batch. (data: 6.72e-04). ETA=10:21:35, max mem: 20.9 GB 
[11/26 09:56:10 visual_prompt]: 	Training 400/553. train loss: 47.2035,	0.8360 s / batch. (data: 7.95e-03). ETA=10:18:31, max mem: 20.9 GB 
[11/26 09:57:46 visual_prompt]: 	Training 500/553. train loss: 39.6276,	0.8321 s / batch. (data: 3.17e-04). ETA=10:14:17, max mem: 20.9 GB 
[11/26 09:58:39 visual_prompt]: Epoch 20 / 100: avg data time: 1.50e-01, avg batch time: 0.9805, average train loss: 35.4635
[11/26 09:59:35 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3094, average loss: 42.7701
[11/26 09:59:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.73	
[11/26 09:59:35 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/26 10:01:19 visual_prompt]: 	Training 100/553. train loss: 47.9980,	0.8340 s / batch. (data: 3.27e-04). ETA=10:13:33, max mem: 20.9 GB 
[11/26 10:02:57 visual_prompt]: 	Training 200/553. train loss: 94.1435,	0.8240 s / batch. (data: 2.97e-04). ETA=10:04:48, max mem: 20.9 GB 
[11/26 10:04:34 visual_prompt]: 	Training 300/553. train loss: 185.8542,	0.8440 s / batch. (data: 1.59e-02). ETA=10:18:04, max mem: 20.9 GB 
[11/26 10:06:12 visual_prompt]: 	Training 400/553. train loss: 1.6639,	0.8181 s / batch. (data: 3.03e-04). ETA=9:57:43, max mem: 20.9 GB 
[11/26 10:07:58 visual_prompt]: 	Training 500/553. train loss: 20.4397,	0.8316 s / batch. (data: 2.89e-04). ETA=10:06:15, max mem: 20.9 GB 
[11/26 10:08:50 visual_prompt]: Epoch 21 / 100: avg data time: 1.73e-01, avg batch time: 1.0029, average train loss: 36.2581
[11/26 10:09:52 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3110, average loss: 52.4051
[11/26 10:09:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.97	
[11/26 10:09:52 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/26 10:11:35 visual_prompt]: 	Training 100/553. train loss: 27.0023,	0.8253 s / batch. (data: 2.87e-04). ETA=9:59:30, max mem: 20.9 GB 
[11/26 10:13:14 visual_prompt]: 	Training 200/553. train loss: 39.1766,	0.8590 s / batch. (data: 2.29e-02). ETA=10:22:37, max mem: 20.9 GB 
[11/26 10:14:51 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8440 s / batch. (data: 2.97e-04). ETA=10:10:18, max mem: 20.9 GB 
[11/26 10:16:31 visual_prompt]: 	Training 400/553. train loss: 20.2337,	0.8280 s / batch. (data: 3.00e-04). ETA=9:57:20, max mem: 20.9 GB 
[11/26 10:18:10 visual_prompt]: 	Training 500/553. train loss: 19.9813,	0.8338 s / batch. (data: 3.08e-04). ETA=10:00:08, max mem: 20.9 GB 
[11/26 10:19:03 visual_prompt]: Epoch 22 / 100: avg data time: 1.66e-01, avg batch time: 0.9966, average train loss: 32.4458
[11/26 10:19:58 visual_prompt]: Inference (val):avg data time: 1.90e-04, avg batch time: 0.3094, average loss: 15.4074
[11/26 10:19:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.64	
[11/26 10:19:58 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/26 10:21:42 visual_prompt]: 	Training 100/553. train loss: 26.0183,	0.8434 s / batch. (data: 7.95e-03). ETA=10:04:56, max mem: 20.9 GB 
[11/26 10:23:20 visual_prompt]: 	Training 200/553. train loss: 13.7293,	0.8440 s / batch. (data: 7.95e-03). ETA=10:03:55, max mem: 20.9 GB 
[11/26 10:25:00 visual_prompt]: 	Training 300/553. train loss: 21.7702,	0.8364 s / batch. (data: 3.55e-04). ETA=9:57:06, max mem: 20.9 GB 
[11/26 10:26:35 visual_prompt]: 	Training 400/553. train loss: 6.2527,	0.8491 s / batch. (data: 5.43e-03). ETA=10:04:43, max mem: 20.9 GB 
[11/26 10:28:11 visual_prompt]: 	Training 500/553. train loss: 15.2459,	0.8280 s / batch. (data: 5.43e-03). ETA=9:48:19, max mem: 20.9 GB 
[11/26 10:29:02 visual_prompt]: Epoch 23 / 100: avg data time: 1.52e-01, avg batch time: 0.9830, average train loss: 34.6301
[11/26 10:29:58 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3100, average loss: 54.3078
[11/26 10:29:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.92	
[11/26 10:29:58 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/26 10:31:36 visual_prompt]: 	Training 100/553. train loss: 3.8505,	0.8400 s / batch. (data: 3.39e-04). ETA=9:54:43, max mem: 20.9 GB 
[11/26 10:33:14 visual_prompt]: 	Training 200/553. train loss: 44.8869,	0.8463 s / batch. (data: 1.05e-02). ETA=9:57:47, max mem: 20.9 GB 
[11/26 10:34:52 visual_prompt]: 	Training 300/553. train loss: 32.5267,	0.8429 s / batch. (data: 8.59e-03). ETA=9:53:57, max mem: 20.9 GB 
[11/26 10:36:29 visual_prompt]: 	Training 400/553. train loss: 2.1398,	0.8480 s / batch. (data: 3.18e-04). ETA=9:56:08, max mem: 20.9 GB 
[11/26 10:38:08 visual_prompt]: 	Training 500/553. train loss: 1.0928,	0.8522 s / batch. (data: 8.13e-03). ETA=9:57:39, max mem: 20.9 GB 
[11/26 10:38:59 visual_prompt]: Epoch 24 / 100: avg data time: 1.47e-01, avg batch time: 0.9783, average train loss: 31.5102
[11/26 10:39:54 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3095, average loss: 17.1132
[11/26 10:39:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.29	
[11/26 10:39:54 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/26 10:41:39 visual_prompt]: 	Training 100/553. train loss: 16.7239,	0.8435 s / batch. (data: 2.35e-02). ETA=9:49:26, max mem: 20.9 GB 
[11/26 10:43:13 visual_prompt]: 	Training 200/553. train loss: 9.5156,	0.8520 s / batch. (data: 3.29e-04). ETA=9:53:56, max mem: 20.9 GB 
[11/26 10:44:51 visual_prompt]: 	Training 300/553. train loss: 21.6935,	0.8289 s / batch. (data: 3.11e-04). ETA=9:36:29, max mem: 20.9 GB 
[11/26 10:46:28 visual_prompt]: 	Training 400/553. train loss: 3.4372,	1.2160 s / batch. (data: 3.67e-01). ETA=14:03:38, max mem: 20.9 GB 
[11/26 10:48:06 visual_prompt]: 	Training 500/553. train loss: 18.2334,	1.2782 s / batch. (data: 4.58e-01). ETA=14:44:39, max mem: 20.9 GB 
[11/26 10:48:57 visual_prompt]: Epoch 25 / 100: avg data time: 1.51e-01, avg batch time: 0.9817, average train loss: 34.4249
[11/26 10:49:53 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3107, average loss: 63.0027
[11/26 10:49:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.89	
[11/26 10:49:53 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/26 10:51:34 visual_prompt]: 	Training 100/553. train loss: 72.3546,	0.8320 s / batch. (data: 3.02e-04). ETA=9:33:43, max mem: 20.9 GB 
[11/26 10:53:13 visual_prompt]: 	Training 200/553. train loss: 101.9321,	1.4494 s / batch. (data: 6.33e-01). ETA=16:37:02, max mem: 20.9 GB 
[11/26 10:54:52 visual_prompt]: 	Training 300/553. train loss: 27.4071,	0.8360 s / batch. (data: 3.27e-04). ETA=9:33:42, max mem: 20.9 GB 
[11/26 10:56:29 visual_prompt]: 	Training 400/553. train loss: 30.2635,	0.8062 s / batch. (data: 3.11e-04). ETA=9:11:54, max mem: 20.9 GB 
[11/26 10:58:05 visual_prompt]: 	Training 500/553. train loss: 83.6241,	0.8352 s / batch. (data: 3.30e-04). ETA=9:30:23, max mem: 20.9 GB 
[11/26 10:58:56 visual_prompt]: Epoch 26 / 100: avg data time: 1.52e-01, avg batch time: 0.9829, average train loss: 31.8418
[11/26 10:59:52 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3109, average loss: 21.1550
[11/26 10:59:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.35	
[11/26 10:59:52 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[11/26 11:01:36 visual_prompt]: 	Training 100/553. train loss: 0.4042,	0.8194 s / batch. (data: 4.46e-04). ETA=9:17:28, max mem: 20.9 GB 
[11/26 11:03:13 visual_prompt]: 	Training 200/553. train loss: 3.8618,	0.9561 s / batch. (data: 1.33e-01). ETA=10:48:54, max mem: 20.9 GB 
[11/26 11:04:49 visual_prompt]: 	Training 300/553. train loss: 61.9464,	0.8360 s / batch. (data: 7.96e-03). ETA=9:26:00, max mem: 20.9 GB 
[11/26 11:06:28 visual_prompt]: 	Training 400/553. train loss: 5.1087,	0.8467 s / batch. (data: 7.84e-04). ETA=9:31:48, max mem: 20.9 GB 
[11/26 11:08:06 visual_prompt]: 	Training 500/553. train loss: 4.4242,	0.8400 s / batch. (data: 3.14e-04). ETA=9:25:55, max mem: 20.9 GB 
[11/26 11:08:55 visual_prompt]: Epoch 27 / 100: avg data time: 1.51e-01, avg batch time: 0.9815, average train loss: 28.6694
[11/26 11:09:51 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3105, average loss: 20.8540
[11/26 11:09:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[11/26 11:09:51 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[11/26 11:11:31 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8119 s / batch. (data: 7.86e-03). ETA=9:04:53, max mem: 20.9 GB 
[11/26 11:13:09 visual_prompt]: 	Training 200/553. train loss: 2.3331,	0.8288 s / batch. (data: 9.61e-03). ETA=9:14:52, max mem: 20.9 GB 
[11/26 11:14:48 visual_prompt]: 	Training 300/553. train loss: 10.2766,	1.5724 s / batch. (data: 7.53e-01). ETA=17:30:05, max mem: 20.9 GB 
[11/26 11:16:25 visual_prompt]: 	Training 400/553. train loss: 74.0288,	0.8480 s / batch. (data: 2.11e-02). ETA=9:24:51, max mem: 20.9 GB 
[11/26 11:18:01 visual_prompt]: 	Training 500/553. train loss: 41.2319,	0.8520 s / batch. (data: 2.97e-04). ETA=9:26:09, max mem: 20.9 GB 
[11/26 11:18:53 visual_prompt]: Epoch 28 / 100: avg data time: 1.49e-01, avg batch time: 0.9810, average train loss: 27.4799
[11/26 11:19:49 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3114, average loss: 42.9842
[11/26 11:19:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.68	
[11/26 11:19:49 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[11/26 11:21:37 visual_prompt]: 	Training 100/553. train loss: 0.1680,	0.8263 s / batch. (data: 3.10e-04). ETA=9:06:56, max mem: 20.9 GB 
[11/26 11:23:13 visual_prompt]: 	Training 200/553. train loss: 58.2281,	1.6760 s / batch. (data: 8.53e-01). ETA=18:26:36, max mem: 20.9 GB 
[11/26 11:24:50 visual_prompt]: 	Training 300/553. train loss: 37.3546,	0.8251 s / batch. (data: 3.27e-04). ETA=9:03:23, max mem: 20.9 GB 
[11/26 11:26:24 visual_prompt]: 	Training 400/553. train loss: 41.1207,	1.2840 s / batch. (data: 4.60e-01). ETA=14:03:29, max mem: 20.9 GB 
[11/26 11:28:01 visual_prompt]: 	Training 500/553. train loss: 12.3421,	0.8199 s / batch. (data: 3.08e-04). ETA=8:57:14, max mem: 20.9 GB 
[11/26 11:28:52 visual_prompt]: Epoch 29 / 100: avg data time: 1.49e-01, avg batch time: 0.9817, average train loss: 26.5714
[11/26 11:29:48 visual_prompt]: Inference (val):avg data time: 1.53e-04, avg batch time: 0.3098, average loss: 48.8234
[11/26 11:29:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.66	
[11/26 11:29:48 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[11/26 11:31:28 visual_prompt]: 	Training 100/553. train loss: 20.2464,	0.8539 s / batch. (data: 2.18e-02). ETA=9:17:20, max mem: 20.9 GB 
[11/26 11:33:06 visual_prompt]: 	Training 200/553. train loss: 11.1758,	0.8200 s / batch. (data: 3.27e-04). ETA=8:53:50, max mem: 20.9 GB 
[11/26 11:34:42 visual_prompt]: 	Training 300/553. train loss: 9.4657,	0.8348 s / batch. (data: 3.24e-04). ETA=9:02:07, max mem: 20.9 GB 
[11/26 11:36:21 visual_prompt]: 	Training 400/553. train loss: 10.0146,	1.0520 s / batch. (data: 2.12e-01). ETA=11:21:23, max mem: 20.9 GB 
[11/26 11:37:57 visual_prompt]: 	Training 500/553. train loss: 17.4269,	1.4204 s / batch. (data: 5.92e-01). ETA=15:17:38, max mem: 20.9 GB 
[11/26 11:38:50 visual_prompt]: Epoch 30 / 100: avg data time: 1.49e-01, avg batch time: 0.9802, average train loss: 26.5943
[11/26 11:39:45 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3101, average loss: 28.5451
[11/26 11:39:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.21	
[11/26 11:39:45 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[11/26 11:41:28 visual_prompt]: 	Training 100/553. train loss: 17.1579,	0.8400 s / batch. (data: 3.04e-04). ETA=9:00:31, max mem: 20.9 GB 
[11/26 11:43:08 visual_prompt]: 	Training 200/553. train loss: 26.0682,	0.8399 s / batch. (data: 7.95e-03). ETA=8:59:03, max mem: 20.9 GB 
[11/26 11:44:43 visual_prompt]: 	Training 300/553. train loss: 37.3984,	0.8440 s / batch. (data: 3.28e-04). ETA=9:00:17, max mem: 20.9 GB 
[11/26 11:46:19 visual_prompt]: 	Training 400/553. train loss: 6.7291,	0.8802 s / batch. (data: 5.05e-02). ETA=9:22:00, max mem: 20.9 GB 
[11/26 11:47:57 visual_prompt]: 	Training 500/553. train loss: 1.2326,	0.8519 s / batch. (data: 1.90e-02). ETA=9:02:29, max mem: 20.9 GB 
[11/26 11:48:48 visual_prompt]: Epoch 31 / 100: avg data time: 1.48e-01, avg batch time: 0.9804, average train loss: 29.0759
[11/26 11:49:44 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3103, average loss: 20.5089
[11/26 11:49:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.16	
[11/26 11:49:44 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[11/26 11:51:27 visual_prompt]: 	Training 100/553. train loss: 11.9108,	0.8560 s / batch. (data: 9.79e-04). ETA=9:02:57, max mem: 20.9 GB 
[11/26 11:53:05 visual_prompt]: 	Training 200/553. train loss: 70.7574,	0.8358 s / batch. (data: 1.18e-02). ETA=8:48:45, max mem: 20.9 GB 
[11/26 11:54:45 visual_prompt]: 	Training 300/553. train loss: 65.8313,	0.8277 s / batch. (data: 5.40e-03). ETA=8:42:15, max mem: 20.9 GB 
[11/26 11:56:23 visual_prompt]: 	Training 400/553. train loss: 4.8384,	0.8469 s / batch. (data: 3.46e-04). ETA=8:52:55, max mem: 20.9 GB 
[11/26 11:57:59 visual_prompt]: 	Training 500/553. train loss: 11.8337,	0.8440 s / batch. (data: 7.97e-03). ETA=8:49:42, max mem: 20.9 GB 
[11/26 11:58:48 visual_prompt]: Epoch 32 / 100: avg data time: 1.52e-01, avg batch time: 0.9837, average train loss: 29.1044
[11/26 11:59:43 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3097, average loss: 59.6293
[11/26 11:59:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.32	
[11/26 11:59:43 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[11/26 12:01:25 visual_prompt]: 	Training 100/553. train loss: 29.7008,	0.8200 s / batch. (data: 2.92e-04). ETA=8:32:33, max mem: 20.9 GB 
[11/26 12:03:04 visual_prompt]: 	Training 200/553. train loss: 10.7101,	1.3759 s / batch. (data: 5.32e-01). ETA=14:17:43, max mem: 20.9 GB 
[11/26 12:04:40 visual_prompt]: 	Training 300/553. train loss: 14.8973,	0.8466 s / batch. (data: 5.45e-03). ETA=8:46:19, max mem: 20.9 GB 
[11/26 12:06:19 visual_prompt]: 	Training 400/553. train loss: 29.4231,	0.8290 s / batch. (data: 8.99e-03). ETA=8:34:03, max mem: 20.9 GB 
[11/26 12:07:56 visual_prompt]: 	Training 500/553. train loss: 3.8533,	0.8289 s / batch. (data: 3.29e-04). ETA=8:32:36, max mem: 20.9 GB 
[11/26 12:08:46 visual_prompt]: Epoch 33 / 100: avg data time: 1.49e-01, avg batch time: 0.9817, average train loss: 30.7971
[11/26 12:09:42 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3091, average loss: 12.2733
[11/26 12:09:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.84	
[11/26 12:09:42 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[11/26 12:11:24 visual_prompt]: 	Training 100/553. train loss: 14.1192,	0.8477 s / batch. (data: 1.05e-02). ETA=8:42:03, max mem: 20.9 GB 
[11/26 12:13:00 visual_prompt]: 	Training 200/553. train loss: 21.7265,	0.8520 s / batch. (data: 1.20e-02). ETA=8:43:16, max mem: 20.9 GB 
[11/26 12:14:36 visual_prompt]: 	Training 300/553. train loss: 30.0338,	0.8600 s / batch. (data: 3.30e-04). ETA=8:46:45, max mem: 20.9 GB 
[11/26 12:16:16 visual_prompt]: 	Training 400/553. train loss: 22.0481,	0.8280 s / batch. (data: 2.97e-04). ETA=8:25:46, max mem: 20.9 GB 
[11/26 12:17:53 visual_prompt]: 	Training 500/553. train loss: 1.1315,	1.2769 s / batch. (data: 4.25e-01). ETA=12:57:50, max mem: 20.9 GB 
[11/26 12:18:43 visual_prompt]: Epoch 34 / 100: avg data time: 1.46e-01, avg batch time: 0.9786, average train loss: 27.4343
[11/26 12:19:39 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3126, average loss: 11.8197
[11/26 12:19:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.98	
[11/26 12:19:39 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[11/26 12:21:23 visual_prompt]: 	Training 100/553. train loss: 14.2905,	0.8356 s / batch. (data: 2.78e-04). ETA=8:26:54, max mem: 20.9 GB 
[11/26 12:23:00 visual_prompt]: 	Training 200/553. train loss: 29.9897,	0.8190 s / batch. (data: 3.52e-04). ETA=8:15:27, max mem: 20.9 GB 
[11/26 12:24:38 visual_prompt]: 	Training 300/553. train loss: 3.0011,	0.8330 s / batch. (data: 2.93e-04). ETA=8:22:31, max mem: 20.9 GB 
[11/26 12:26:14 visual_prompt]: 	Training 400/553. train loss: 0.7721,	0.8392 s / batch. (data: 3.12e-04). ETA=8:24:52, max mem: 20.9 GB 
[11/26 12:27:50 visual_prompt]: 	Training 500/553. train loss: 12.0348,	1.0171 s / batch. (data: 1.99e-01). ETA=10:10:12, max mem: 20.9 GB 
[11/26 12:28:42 visual_prompt]: Epoch 35 / 100: avg data time: 1.49e-01, avg batch time: 0.9807, average train loss: 30.2824
[11/26 12:29:38 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3092, average loss: 96.0170
[11/26 12:29:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.43	
[11/26 12:29:38 visual_prompt]: Stopping early.
[11/26 12:29:38 visual_prompt]: Rank of current process: 0. World size: 1
[11/26 12:29:38 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/26 12:29:38 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/26 12:29:38 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/26 12:29:38 visual_prompt]: Training with config:
[11/26 12:29:38 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/26 12:29:38 visual_prompt]: Loading training data...
[11/26 12:29:38 visual_prompt]: Constructing mammo-cbis dataset train...
[11/26 12:29:38 visual_prompt]: Loading validation data...
[11/26 12:29:38 visual_prompt]: Constructing mammo-cbis dataset val...
[11/26 12:29:38 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/26 12:29:46 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/26 12:29:46 visual_prompt]: tuned percent:0.525
[11/26 12:29:46 visual_prompt]: Device used for model: 0
[11/26 12:29:46 visual_prompt]: Setting up Evaluator...
[11/26 12:29:46 visual_prompt]: Setting up Trainer...
[11/26 12:29:46 visual_prompt]: 	Setting up the optimizer...
[11/26 12:29:46 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/26 12:31:27 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8600 s / batch. (data: 2.90e-04). ETA=13:11:12, max mem: 20.9 GB 
[11/26 12:33:04 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8564 s / batch. (data: 1.55e-02). ETA=13:06:30, max mem: 20.9 GB 
[11/26 12:34:44 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8427 s / batch. (data: 1.05e-02). ETA=12:52:27, max mem: 20.9 GB 
[11/26 12:36:20 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8440 s / batch. (data: 3.02e-04). ETA=12:52:14, max mem: 20.9 GB 
[11/26 12:38:00 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8183 s / batch. (data: 2.97e-04). ETA=12:27:23, max mem: 20.9 GB 
[11/26 12:38:51 visual_prompt]: Epoch 1 / 100: avg data time: 1.51e-01, avg batch time: 0.9847, average train loss: 1.5403
[11/26 12:39:47 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3101, average loss: 1.5201
[11/26 12:39:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/26 12:39:47 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/26 12:41:27 visual_prompt]: 	Training 100/553. train loss: 10.4643,	0.8795 s / batch. (data: 3.55e-02). ETA=13:21:04, max mem: 20.9 GB 
[11/26 12:43:04 visual_prompt]: 	Training 200/553. train loss: 0.0010,	0.8400 s / batch. (data: 3.26e-04). ETA=12:43:39, max mem: 20.9 GB 
[11/26 12:44:44 visual_prompt]: 	Training 300/553. train loss: 5.4646,	1.0904 s / batch. (data: 2.48e-01). ETA=16:29:27, max mem: 20.9 GB 
[11/26 12:46:20 visual_prompt]: 	Training 400/553. train loss: 5.3334,	0.8600 s / batch. (data: 7.96e-03). ETA=12:58:58, max mem: 20.9 GB 
[11/26 12:48:00 visual_prompt]: 	Training 500/553. train loss: 9.4859,	0.8533 s / batch. (data: 3.00e-04). ETA=12:51:30, max mem: 20.9 GB 
[11/26 12:48:50 visual_prompt]: Epoch 2 / 100: avg data time: 1.49e-01, avg batch time: 0.9827, average train loss: 4.9669
[11/26 12:49:46 visual_prompt]: Inference (val):avg data time: 4.16e-04, avg batch time: 0.3088, average loss: 17.1788
[11/26 12:49:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.88	
[11/26 12:49:46 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/26 12:51:27 visual_prompt]: 	Training 100/553. train loss: 5.1646,	0.8439 s / batch. (data: 5.64e-03). ETA=12:40:49, max mem: 20.9 GB 
[11/26 12:53:06 visual_prompt]: 	Training 200/553. train loss: 2.0174,	0.8185 s / batch. (data: 2.93e-04). ETA=12:16:32, max mem: 20.9 GB 
[11/26 12:54:43 visual_prompt]: 	Training 300/553. train loss: 3.7236,	0.8409 s / batch. (data: 5.52e-03). ETA=12:35:19, max mem: 20.9 GB 
[11/26 12:56:21 visual_prompt]: 	Training 400/553. train loss: 2.2168,	0.8280 s / batch. (data: 3.14e-04). ETA=12:22:20, max mem: 20.9 GB 
[11/26 12:58:00 visual_prompt]: 	Training 500/553. train loss: 6.8552,	1.1364 s / batch. (data: 3.10e-01). ETA=16:56:59, max mem: 20.9 GB 
[11/26 12:58:49 visual_prompt]: Epoch 3 / 100: avg data time: 1.49e-01, avg batch time: 0.9821, average train loss: 5.8513
[11/26 12:59:45 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3110, average loss: 14.7225
[11/26 12:59:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.89	
[11/26 12:59:45 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/26 13:01:28 visual_prompt]: 	Training 100/553. train loss: 0.7517,	0.8411 s / batch. (data: 1.05e-02). ETA=12:30:34, max mem: 20.9 GB 
[11/26 13:03:06 visual_prompt]: 	Training 200/553. train loss: 6.1720,	0.8240 s / batch. (data: 3.08e-04). ETA=12:13:55, max mem: 20.9 GB 
[11/26 13:04:43 visual_prompt]: 	Training 300/553. train loss: 2.6758,	1.2040 s / batch. (data: 3.86e-01). ETA=17:50:21, max mem: 20.9 GB 
[11/26 13:06:18 visual_prompt]: 	Training 400/553. train loss: 13.2228,	1.3613 s / batch. (data: 5.06e-01). ETA=20:07:54, max mem: 20.9 GB 
[11/26 13:07:57 visual_prompt]: 	Training 500/553. train loss: 0.6358,	3.3360 s / batch. (data: 2.49e+00). ETA=2 days, 1:14:39, max mem: 20.9 GB 
[11/26 13:08:49 visual_prompt]: Epoch 4 / 100: avg data time: 1.50e-01, avg batch time: 0.9828, average train loss: 10.6225
[11/26 13:09:46 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3104, average loss: 5.6601
[11/26 13:09:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.18	
[11/26 13:09:46 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/26 13:11:26 visual_prompt]: 	Training 100/553. train loss: 28.5359,	0.8449 s / batch. (data: 5.43e-03). ETA=12:26:08, max mem: 20.9 GB 
[11/26 13:13:04 visual_prompt]: 	Training 200/553. train loss: 2.2963,	0.9838 s / batch. (data: 1.63e-01). ETA=14:27:08, max mem: 20.9 GB 
[11/26 13:14:43 visual_prompt]: 	Training 300/553. train loss: 5.5521,	0.8194 s / batch. (data: 3.00e-04). ETA=12:00:55, max mem: 20.9 GB 
[11/26 13:16:19 visual_prompt]: 	Training 400/553. train loss: 3.1498,	0.8323 s / batch. (data: 3.04e-04). ETA=12:10:53, max mem: 20.9 GB 
[11/26 13:17:57 visual_prompt]: 	Training 500/553. train loss: 14.7823,	0.8480 s / batch. (data: 7.92e-03). ETA=12:23:13, max mem: 20.9 GB 
[11/26 13:18:49 visual_prompt]: Epoch 5 / 100: avg data time: 1.48e-01, avg batch time: 0.9819, average train loss: 11.0127
[11/26 13:19:44 visual_prompt]: Inference (val):avg data time: 2.31e-04, avg batch time: 0.3112, average loss: 19.4735
[11/26 13:19:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.91	
[11/26 13:19:44 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/26 13:21:27 visual_prompt]: 	Training 100/553. train loss: 5.0224,	0.8276 s / batch. (data: 3.62e-04). ETA=12:03:14, max mem: 20.9 GB 
[11/26 13:23:05 visual_prompt]: 	Training 200/553. train loss: 19.3753,	0.8582 s / batch. (data: 3.12e-04). ETA=12:28:33, max mem: 20.9 GB 
[11/26 13:24:40 visual_prompt]: 	Training 300/553. train loss: 3.0734,	0.8415 s / batch. (data: 3.18e-04). ETA=12:12:34, max mem: 20.9 GB 
[11/26 13:26:22 visual_prompt]: 	Training 400/553. train loss: 9.9597,	0.8359 s / batch. (data: 1.06e-02). ETA=12:06:19, max mem: 20.9 GB 
[11/26 13:27:59 visual_prompt]: 	Training 500/553. train loss: 16.4667,	0.8309 s / batch. (data: 5.44e-03). ETA=12:00:34, max mem: 20.9 GB 
[11/26 13:28:49 visual_prompt]: Epoch 6 / 100: avg data time: 1.50e-01, avg batch time: 0.9842, average train loss: 10.0695
[11/26 13:29:45 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3124, average loss: 8.6839
[11/26 13:29:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.73	
[11/26 13:29:45 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/26 13:31:25 visual_prompt]: 	Training 100/553. train loss: 8.0542,	0.8437 s / batch. (data: 2.36e-02). ETA=12:09:30, max mem: 20.9 GB 
[11/26 13:33:03 visual_prompt]: 	Training 200/553. train loss: 2.1701,	0.8597 s / batch. (data: 2.76e-02). ETA=12:21:56, max mem: 20.9 GB 
[11/26 13:34:44 visual_prompt]: 	Training 300/553. train loss: 27.3659,	1.9106 s / batch. (data: 1.09e+00). ETA=1 day, 3:25:42, max mem: 20.9 GB 
[11/26 13:36:21 visual_prompt]: 	Training 400/553. train loss: 0.6175,	1.5307 s / batch. (data: 7.00e-01). ETA=21:55:57, max mem: 20.9 GB 
[11/26 13:37:57 visual_prompt]: 	Training 500/553. train loss: 5.7167,	0.8440 s / batch. (data: 3.45e-04). ETA=12:04:10, max mem: 20.9 GB 
[11/26 13:38:47 visual_prompt]: Epoch 7 / 100: avg data time: 1.48e-01, avg batch time: 0.9809, average train loss: 12.7131
[11/26 13:39:43 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3094, average loss: 3.9841
[11/26 13:39:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.10	
[11/26 13:39:43 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/26 13:41:24 visual_prompt]: 	Training 100/553. train loss: 7.3276,	0.8692 s / batch. (data: 5.44e-03). ETA=12:23:34, max mem: 20.9 GB 
[11/26 13:43:03 visual_prompt]: 	Training 200/553. train loss: 51.3404,	0.8418 s / batch. (data: 4.84e-04). ETA=11:58:45, max mem: 20.9 GB 
[11/26 13:44:41 visual_prompt]: 	Training 300/553. train loss: 0.8880,	0.8242 s / batch. (data: 5.44e-03). ETA=11:42:18, max mem: 20.9 GB 
[11/26 13:46:19 visual_prompt]: 	Training 400/553. train loss: 19.9931,	0.8508 s / batch. (data: 5.46e-03). ETA=12:03:33, max mem: 20.9 GB 
[11/26 13:47:57 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.4277 s / batch. (data: 5.70e-01). ETA=20:11:52, max mem: 20.9 GB 
[11/26 13:48:49 visual_prompt]: Epoch 8 / 100: avg data time: 1.54e-01, avg batch time: 0.9862, average train loss: 25.0387
[11/26 13:49:44 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3092, average loss: 11.9663
[11/26 13:49:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.23	
[11/26 13:49:44 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/26 13:51:25 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8267 s / batch. (data: 3.01e-04). ETA=11:39:34, max mem: 20.9 GB 
[11/26 13:53:02 visual_prompt]: 	Training 200/553. train loss: 18.9466,	0.8250 s / batch. (data: 3.11e-04). ETA=11:36:48, max mem: 20.9 GB 
[11/26 13:54:40 visual_prompt]: 	Training 300/553. train loss: 1.4170,	1.5440 s / batch. (data: 7.00e-01). ETA=21:41:30, max mem: 20.9 GB 
[11/26 13:56:19 visual_prompt]: 	Training 400/553. train loss: 2.0662,	0.8445 s / batch. (data: 7.93e-04). ETA=11:50:25, max mem: 20.9 GB 
[11/26 13:57:57 visual_prompt]: 	Training 500/553. train loss: 12.9637,	0.9166 s / batch. (data: 7.76e-02). ETA=12:49:32, max mem: 20.9 GB 
[11/26 13:58:47 visual_prompt]: Epoch 9 / 100: avg data time: 1.49e-01, avg batch time: 0.9812, average train loss: 21.0203
[11/26 13:59:43 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3096, average loss: 80.6455
[11/26 13:59:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.99	
[11/26 13:59:43 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/26 14:01:27 visual_prompt]: 	Training 100/553. train loss: 54.4525,	0.8320 s / batch. (data: 3.12e-04). ETA=11:36:25, max mem: 20.9 GB 
[11/26 14:03:03 visual_prompt]: 	Training 200/553. train loss: 18.4336,	0.8389 s / batch. (data: 1.05e-02). ETA=11:40:49, max mem: 20.9 GB 
[11/26 14:04:39 visual_prompt]: 	Training 300/553. train loss: 29.5771,	1.0923 s / batch. (data: 2.65e-01). ETA=15:10:41, max mem: 20.9 GB 
[11/26 14:06:16 visual_prompt]: 	Training 400/553. train loss: 51.3899,	0.8567 s / batch. (data: 2.57e-02). ETA=11:52:49, max mem: 20.9 GB 
[11/26 14:07:55 visual_prompt]: 	Training 500/553. train loss: 3.2221,	0.8596 s / batch. (data: 1.16e-02). ETA=11:53:48, max mem: 20.9 GB 
[11/26 14:08:46 visual_prompt]: Epoch 10 / 100: avg data time: 1.51e-01, avg batch time: 0.9823, average train loss: 23.4597
[11/26 14:09:42 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3098, average loss: 35.3130
[11/26 14:09:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.92	
[11/26 14:09:42 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/26 14:11:26 visual_prompt]: 	Training 100/553. train loss: 49.0889,	0.8406 s / batch. (data: 2.46e-02). ETA=11:35:52, max mem: 20.9 GB 
[11/26 14:13:06 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8400 s / batch. (data: 1.19e-02). ETA=11:33:58, max mem: 20.9 GB 
[11/26 14:14:43 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.8108 s / batch. (data: 9.86e-01). ETA=1 day, 0:53:02, max mem: 20.9 GB 
[11/26 14:16:18 visual_prompt]: 	Training 400/553. train loss: 47.5715,	0.8260 s / batch. (data: 3.05e-04). ETA=11:19:38, max mem: 20.9 GB 
[11/26 14:17:55 visual_prompt]: 	Training 500/553. train loss: 90.2490,	0.8231 s / batch. (data: 1.13e-02). ETA=11:15:55, max mem: 20.9 GB 
[11/26 14:18:45 visual_prompt]: Epoch 11 / 100: avg data time: 1.52e-01, avg batch time: 0.9817, average train loss: 40.8844
[11/26 14:19:41 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3103, average loss: 19.9013
[11/26 14:19:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.81	
[11/26 14:19:41 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/26 14:21:24 visual_prompt]: 	Training 100/553. train loss: 33.3353,	0.8282 s / batch. (data: 3.52e-04). ETA=11:18:00, max mem: 20.9 GB 
[11/26 14:23:03 visual_prompt]: 	Training 200/553. train loss: 8.5810,	0.8384 s / batch. (data: 2.84e-04). ETA=11:24:53, max mem: 20.9 GB 
[11/26 14:24:39 visual_prompt]: 	Training 300/553. train loss: 37.9235,	0.8320 s / batch. (data: 3.12e-04). ETA=11:18:18, max mem: 20.9 GB 
[11/26 14:26:17 visual_prompt]: 	Training 400/553. train loss: 5.5246,	0.8318 s / batch. (data: 1.30e-03). ETA=11:16:43, max mem: 20.9 GB 
[11/26 14:27:54 visual_prompt]: 	Training 500/553. train loss: 361.9593,	0.8376 s / batch. (data: 7.96e-04). ETA=11:20:06, max mem: 20.9 GB 
[11/26 14:28:44 visual_prompt]: Epoch 12 / 100: avg data time: 1.51e-01, avg batch time: 0.9821, average train loss: 33.7319
[11/26 14:29:40 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3115, average loss: 6.8668
[11/26 14:29:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.22	
[11/26 14:29:40 visual_prompt]: Best epoch 12: best metric: -6.867
[11/26 14:29:40 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/26 14:31:23 visual_prompt]: 	Training 100/553. train loss: 23.7033,	0.8219 s / batch. (data: 2.93e-04). ETA=11:05:14, max mem: 20.9 GB 
[11/26 14:32:57 visual_prompt]: 	Training 200/553. train loss: 24.2083,	0.8460 s / batch. (data: 1.05e-02). ETA=11:23:19, max mem: 20.9 GB 
[11/26 14:34:36 visual_prompt]: 	Training 300/553. train loss: 11.9505,	1.6479 s / batch. (data: 8.00e-01). ETA=22:08:19, max mem: 20.9 GB 
[11/26 14:36:12 visual_prompt]: 	Training 400/553. train loss: 68.1326,	0.8245 s / batch. (data: 3.37e-04). ETA=11:03:15, max mem: 20.9 GB 
[11/26 14:37:51 visual_prompt]: 	Training 500/553. train loss: 43.9326,	0.8320 s / batch. (data: 4.21e-04). ETA=11:07:50, max mem: 20.9 GB 
[11/26 14:38:41 visual_prompt]: Epoch 13 / 100: avg data time: 1.48e-01, avg batch time: 0.9788, average train loss: 31.1751
[11/26 14:39:37 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3093, average loss: 9.2663
[11/26 14:39:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.03	
[11/26 14:39:37 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/26 14:41:20 visual_prompt]: 	Training 100/553. train loss: 9.2130,	0.8374 s / batch. (data: 1.05e-02). ETA=11:10:02, max mem: 20.9 GB 
[11/26 14:42:57 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.0240 s / batch. (data: 1.81e-01). ETA=13:37:39, max mem: 20.9 GB 
[11/26 14:44:35 visual_prompt]: 	Training 300/553. train loss: 19.4283,	0.8323 s / batch. (data: 3.26e-04). ETA=11:03:12, max mem: 20.9 GB 
[11/26 14:46:12 visual_prompt]: 	Training 400/553. train loss: 13.2197,	0.8360 s / batch. (data: 3.00e-04). ETA=11:04:45, max mem: 20.9 GB 
[11/26 14:47:49 visual_prompt]: 	Training 500/553. train loss: 6.1606,	0.8555 s / batch. (data: 3.10e-02). ETA=11:18:50, max mem: 20.9 GB 
[11/26 14:48:39 visual_prompt]: Epoch 14 / 100: avg data time: 1.49e-01, avg batch time: 0.9802, average train loss: 30.6143
[11/26 14:49:35 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3096, average loss: 9.9501
[11/26 14:49:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.47	
[11/26 14:49:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/26 14:51:16 visual_prompt]: 	Training 100/553. train loss: 16.6267,	0.8462 s / batch. (data: 1.01e-02). ETA=11:09:21, max mem: 20.9 GB 
[11/26 14:52:52 visual_prompt]: 	Training 200/553. train loss: 84.0703,	0.8440 s / batch. (data: 4.18e-04). ETA=11:06:08, max mem: 20.9 GB 
[11/26 14:54:32 visual_prompt]: 	Training 300/553. train loss: 69.5441,	0.8559 s / batch. (data: 8.08e-04). ETA=11:14:09, max mem: 20.9 GB 
[11/26 14:56:07 visual_prompt]: 	Training 400/553. train loss: 11.5677,	1.0479 s / batch. (data: 1.95e-01). ETA=13:43:36, max mem: 20.9 GB 
[11/26 14:57:46 visual_prompt]: 	Training 500/553. train loss: 1.6814,	0.8238 s / batch. (data: 4.17e-04). ETA=10:46:04, max mem: 20.9 GB 
[11/26 14:58:37 visual_prompt]: Epoch 15 / 100: avg data time: 1.49e-01, avg batch time: 0.9809, average train loss: 28.5416
[11/26 14:59:33 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3094, average loss: 5.6738
[11/26 14:59:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.66	
[11/26 14:59:33 visual_prompt]: Best epoch 15: best metric: -5.674
[11/26 14:59:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/26 15:01:14 visual_prompt]: 	Training 100/553. train loss: 2.0761,	0.8624 s / batch. (data: 4.22e-04). ETA=11:14:12, max mem: 20.9 GB 
[11/26 15:02:52 visual_prompt]: 	Training 200/553. train loss: 74.7176,	0.8190 s / batch. (data: 3.39e-04). ETA=10:38:54, max mem: 20.9 GB 
[11/26 15:04:30 visual_prompt]: 	Training 300/553. train loss: 70.4559,	0.8253 s / batch. (data: 3.40e-04). ETA=10:42:23, max mem: 20.9 GB 
[11/26 15:06:08 visual_prompt]: 	Training 400/553. train loss: 32.9066,	0.8186 s / batch. (data: 3.00e-04). ETA=10:35:48, max mem: 20.9 GB 
[11/26 15:07:45 visual_prompt]: 	Training 500/553. train loss: 10.4075,	1.2459 s / batch. (data: 4.29e-01). ETA=16:05:40, max mem: 20.9 GB 
[11/26 15:08:37 visual_prompt]: Epoch 16 / 100: avg data time: 1.52e-01, avg batch time: 0.9828, average train loss: 35.6242
[11/26 15:09:32 visual_prompt]: Inference (val):avg data time: 3.73e-04, avg batch time: 0.3117, average loss: 27.4504
[11/26 15:09:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.72	
[11/26 15:09:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/26 15:11:13 visual_prompt]: 	Training 100/553. train loss: 10.0498,	0.8520 s / batch. (data: 7.98e-03). ETA=10:58:11, max mem: 20.9 GB 
[11/26 15:12:52 visual_prompt]: 	Training 200/553. train loss: 2.1062,	0.8313 s / batch. (data: 1.04e-02). ETA=10:40:50, max mem: 20.9 GB 
[11/26 15:14:28 visual_prompt]: 	Training 300/553. train loss: 47.0998,	0.8320 s / batch. (data: 7.95e-03). ETA=10:39:58, max mem: 20.9 GB 
[11/26 15:16:06 visual_prompt]: 	Training 400/553. train loss: 48.3595,	1.1613 s / batch. (data: 3.40e-01). ETA=14:51:19, max mem: 20.9 GB 
[11/26 15:17:43 visual_prompt]: 	Training 500/553. train loss: 1.4778,	1.0080 s / batch. (data: 1.77e-01). ETA=12:51:59, max mem: 20.9 GB 
[11/26 15:18:35 visual_prompt]: Epoch 17 / 100: avg data time: 1.50e-01, avg batch time: 0.9814, average train loss: 27.8454
[11/26 15:19:31 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3110, average loss: 27.0778
[11/26 15:19:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.98	
[11/26 15:19:31 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/26 15:21:12 visual_prompt]: 	Training 100/553. train loss: 38.1946,	0.8282 s / batch. (data: 3.41e-04). ETA=10:32:09, max mem: 20.9 GB 
[11/26 15:22:55 visual_prompt]: 	Training 200/553. train loss: 16.9148,	0.8360 s / batch. (data: 3.14e-04). ETA=10:36:45, max mem: 20.9 GB 
[11/26 15:24:36 visual_prompt]: 	Training 300/553. train loss: 4.3457,	0.8246 s / batch. (data: 9.54e-03). ETA=10:26:40, max mem: 20.9 GB 
[11/26 15:26:16 visual_prompt]: 	Training 400/553. train loss: 22.5713,	0.8329 s / batch. (data: 5.43e-03). ETA=10:31:35, max mem: 20.9 GB 
[11/26 15:27:56 visual_prompt]: 	Training 500/553. train loss: 4.5779,	0.8219 s / batch. (data: 3.15e-04). ETA=10:21:52, max mem: 20.9 GB 
[11/26 15:28:47 visual_prompt]: Epoch 18 / 100: avg data time: 1.76e-01, avg batch time: 1.0052, average train loss: 34.1536
[11/26 15:29:43 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3109, average loss: 8.4810
[11/26 15:29:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.20	
[11/26 15:29:43 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/26 15:31:24 visual_prompt]: 	Training 100/553. train loss: 9.2575,	0.8285 s / batch. (data: 3.14e-04). ETA=10:24:46, max mem: 20.9 GB 
[11/26 15:33:02 visual_prompt]: 	Training 200/553. train loss: 21.5479,	0.8275 s / batch. (data: 5.43e-03). ETA=10:22:39, max mem: 20.9 GB 
[11/26 15:34:41 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8428 s / batch. (data: 3.29e-04). ETA=10:32:45, max mem: 20.9 GB 
[11/26 15:36:18 visual_prompt]: 	Training 400/553. train loss: 9.2315,	0.8321 s / batch. (data: 2.88e-04). ETA=10:23:21, max mem: 20.9 GB 
[11/26 15:37:52 visual_prompt]: 	Training 500/553. train loss: 0.2463,	0.8224 s / batch. (data: 1.87e-03). ETA=10:14:41, max mem: 20.9 GB 
[11/26 15:38:44 visual_prompt]: Epoch 19 / 100: avg data time: 1.45e-01, avg batch time: 0.9773, average train loss: 26.0521
[11/26 15:39:39 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3108, average loss: 82.6336
[11/26 15:39:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.47	
[11/26 15:39:39 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/26 15:41:19 visual_prompt]: 	Training 100/553. train loss: 8.7580,	0.8346 s / batch. (data: 7.95e-03). ETA=10:21:41, max mem: 20.9 GB 
[11/26 15:42:58 visual_prompt]: 	Training 200/553. train loss: 1.7225,	0.8360 s / batch. (data: 1.20e-02). ETA=10:21:20, max mem: 20.9 GB 
[11/26 15:44:36 visual_prompt]: 	Training 300/553. train loss: 5.0633,	0.8331 s / batch. (data: 3.20e-04). ETA=10:17:46, max mem: 20.9 GB 
[11/26 15:46:13 visual_prompt]: 	Training 400/553. train loss: 18.6988,	0.8154 s / batch. (data: 3.46e-04). ETA=10:03:17, max mem: 20.9 GB 
[11/26 15:47:50 visual_prompt]: 	Training 500/553. train loss: 0.4188,	0.8183 s / batch. (data: 2.96e-04). ETA=10:04:03, max mem: 20.9 GB 
[11/26 15:48:42 visual_prompt]: Epoch 20 / 100: avg data time: 1.49e-01, avg batch time: 0.9811, average train loss: 28.4255
[11/26 15:49:38 visual_prompt]: Inference (val):avg data time: 1.29e-04, avg batch time: 0.3094, average loss: 20.9842
[11/26 15:49:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.62	
[11/26 15:49:38 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/26 15:51:21 visual_prompt]: 	Training 100/553. train loss: 31.0035,	0.8280 s / batch. (data: 3.99e-03). ETA=10:09:07, max mem: 20.9 GB 
[11/26 15:52:58 visual_prompt]: 	Training 200/553. train loss: 56.9735,	0.8391 s / batch. (data: 9.61e-03). ETA=10:15:55, max mem: 20.9 GB 
[11/26 15:54:34 visual_prompt]: 	Training 300/553. train loss: 62.3923,	0.8320 s / batch. (data: 3.06e-04). ETA=10:09:19, max mem: 20.9 GB 
[11/26 15:56:13 visual_prompt]: 	Training 400/553. train loss: 42.8589,	0.8320 s / batch. (data: 2.95e-04). ETA=10:07:55, max mem: 20.9 GB 
[11/26 15:57:51 visual_prompt]: 	Training 500/553. train loss: 15.1508,	0.8531 s / batch. (data: 2.16e-02). ETA=10:21:54, max mem: 20.9 GB 
[11/26 15:58:41 visual_prompt]: Epoch 21 / 100: avg data time: 1.51e-01, avg batch time: 0.9831, average train loss: 22.8141
[11/26 15:59:37 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3096, average loss: 1.3783
[11/26 15:59:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.36	
[11/26 15:59:37 visual_prompt]: Best epoch 21: best metric: -1.378
[11/26 15:59:37 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/26 16:01:19 visual_prompt]: 	Training 100/553. train loss: 19.0938,	0.8323 s / batch. (data: 3.02e-04). ETA=10:04:39, max mem: 20.9 GB 
[11/26 16:02:56 visual_prompt]: 	Training 200/553. train loss: 4.2181,	0.8360 s / batch. (data: 3.05e-04). ETA=10:05:55, max mem: 20.9 GB 
[11/26 16:04:32 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8674 s / batch. (data: 5.43e-03). ETA=10:27:11, max mem: 20.9 GB 
[11/26 16:06:10 visual_prompt]: 	Training 400/553. train loss: 35.0689,	0.8329 s / batch. (data: 2.99e-04). ETA=10:00:53, max mem: 20.9 GB 
[11/26 16:07:48 visual_prompt]: 	Training 500/553. train loss: 61.7992,	0.8280 s / batch. (data: 5.45e-03). ETA=9:56:00, max mem: 20.9 GB 
[11/26 16:08:40 visual_prompt]: Epoch 22 / 100: avg data time: 1.49e-01, avg batch time: 0.9811, average train loss: 28.6100
[11/26 16:09:35 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3097, average loss: 40.7791
[11/26 16:09:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.47	
[11/26 16:09:35 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/26 16:11:18 visual_prompt]: 	Training 100/553. train loss: 17.7667,	0.8283 s / batch. (data: 3.06e-04). ETA=9:54:03, max mem: 20.9 GB 
[11/26 16:12:56 visual_prompt]: 	Training 200/553. train loss: 75.8020,	0.8242 s / batch. (data: 1.05e-02). ETA=9:49:47, max mem: 20.9 GB 
[11/26 16:14:35 visual_prompt]: 	Training 300/553. train loss: 12.9878,	0.8211 s / batch. (data: 2.95e-04). ETA=9:46:09, max mem: 20.9 GB 
[11/26 16:16:11 visual_prompt]: 	Training 400/553. train loss: 5.6556,	0.8403 s / batch. (data: 7.24e-04). ETA=9:58:30, max mem: 20.9 GB 
[11/26 16:17:47 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8200 s / batch. (data: 2.88e-04). ETA=9:42:39, max mem: 20.9 GB 
[11/26 16:18:37 visual_prompt]: Epoch 23 / 100: avg data time: 1.46e-01, avg batch time: 0.9788, average train loss: 26.6718
[11/26 16:19:33 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3081, average loss: 2.5826
[11/26 16:19:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.97	
[11/26 16:19:33 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/26 16:21:11 visual_prompt]: 	Training 100/553. train loss: 99.1741,	0.8126 s / batch. (data: 3.40e-04). ETA=9:35:18, max mem: 20.9 GB 
[11/26 16:22:49 visual_prompt]: 	Training 200/553. train loss: 30.3081,	0.9004 s / batch. (data: 2.83e-02). ETA=10:35:58, max mem: 20.9 GB 
[11/26 16:24:28 visual_prompt]: 	Training 300/553. train loss: 11.2261,	0.9800 s / batch. (data: 1.46e-01). ETA=11:30:35, max mem: 20.9 GB 
[11/26 16:26:06 visual_prompt]: 	Training 400/553. train loss: 1.6055,	0.8455 s / batch. (data: 9.16e-03). ETA=9:54:22, max mem: 20.9 GB 
[11/26 16:27:45 visual_prompt]: 	Training 500/553. train loss: 91.2325,	0.8245 s / batch. (data: 5.50e-03). ETA=9:38:15, max mem: 20.9 GB 
[11/26 16:28:37 visual_prompt]: Epoch 24 / 100: avg data time: 1.52e-01, avg batch time: 0.9842, average train loss: 29.2039
[11/26 16:29:33 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3116, average loss: 2.2943
[11/26 16:29:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.61	
[11/26 16:29:33 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/26 16:31:17 visual_prompt]: 	Training 100/553. train loss: 11.0729,	0.8397 s / batch. (data: 3.15e-04). ETA=9:46:46, max mem: 20.9 GB 
[11/26 16:32:52 visual_prompt]: 	Training 200/553. train loss: 41.8671,	0.8560 s / batch. (data: 3.25e-04). ETA=9:56:43, max mem: 20.9 GB 
[11/26 16:34:29 visual_prompt]: 	Training 300/553. train loss: 31.9652,	0.9846 s / batch. (data: 1.34e-01). ETA=11:24:43, max mem: 20.9 GB 
[11/26 16:36:06 visual_prompt]: 	Training 400/553. train loss: 5.9064,	1.2470 s / batch. (data: 4.07e-01). ETA=14:25:10, max mem: 20.9 GB 
[11/26 16:37:45 visual_prompt]: 	Training 500/553. train loss: 37.1354,	1.4120 s / batch. (data: 5.80e-01). ETA=16:17:15, max mem: 20.9 GB 
[11/26 16:38:36 visual_prompt]: Epoch 25 / 100: avg data time: 1.52e-01, avg batch time: 0.9826, average train loss: 39.3230
[11/26 16:39:31 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3107, average loss: 95.5960
[11/26 16:39:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.02	
[11/26 16:39:31 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/26 16:41:13 visual_prompt]: 	Training 100/553. train loss: 20.8582,	0.8384 s / batch. (data: 7.95e-03). ETA=9:38:09, max mem: 20.9 GB 
[11/26 16:42:52 visual_prompt]: 	Training 200/553. train loss: 25.1038,	1.7245 s / batch. (data: 9.01e-01). ETA=19:46:17, max mem: 20.9 GB 
[11/26 16:44:31 visual_prompt]: 	Training 300/553. train loss: 26.8676,	0.8203 s / batch. (data: 3.14e-04). ETA=9:22:55, max mem: 20.9 GB 
[11/26 16:46:07 visual_prompt]: 	Training 400/553. train loss: 48.3365,	0.8160 s / batch. (data: 2.93e-04). ETA=9:18:37, max mem: 20.9 GB 
[11/26 16:47:43 visual_prompt]: 	Training 500/553. train loss: 8.3600,	0.8373 s / batch. (data: 7.96e-03). ETA=9:31:49, max mem: 20.9 GB 
[11/26 16:48:34 visual_prompt]: Epoch 26 / 100: avg data time: 1.49e-01, avg batch time: 0.9807, average train loss: 25.6008
[11/26 16:49:30 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3096, average loss: 31.2810
[11/26 16:49:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.85	
[11/26 16:49:30 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[11/26 16:51:12 visual_prompt]: 	Training 100/553. train loss: 1.9237,	0.8280 s / batch. (data: 2.98e-04). ETA=9:23:20, max mem: 20.9 GB 
[11/26 16:52:49 visual_prompt]: 	Training 200/553. train loss: 16.5352,	1.0200 s / batch. (data: 1.93e-01). ETA=11:32:16, max mem: 20.9 GB 
[11/26 16:54:28 visual_prompt]: 	Training 300/553. train loss: 26.6875,	0.8440 s / batch. (data: 3.19e-04). ETA=9:31:24, max mem: 20.9 GB 
[11/26 16:56:06 visual_prompt]: 	Training 400/553. train loss: 22.0815,	0.8550 s / batch. (data: 7.79e-04). ETA=9:37:27, max mem: 20.9 GB 
[11/26 16:57:45 visual_prompt]: 	Training 500/553. train loss: 22.2407,	0.8673 s / batch. (data: 8.05e-04). ETA=9:44:17, max mem: 20.9 GB 
[11/26 16:58:33 visual_prompt]: Epoch 27 / 100: avg data time: 1.51e-01, avg batch time: 0.9831, average train loss: 29.5681
[11/26 16:59:29 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3088, average loss: 9.5758
[11/26 16:59:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.81	
[11/26 16:59:29 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[11/26 17:01:09 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8218 s / batch. (data: 3.35e-04). ETA=9:11:31, max mem: 20.9 GB 
[11/26 17:02:47 visual_prompt]: 	Training 200/553. train loss: 6.5409,	0.8480 s / batch. (data: 2.67e-04). ETA=9:27:44, max mem: 20.9 GB 
[11/26 17:04:26 visual_prompt]: 	Training 300/553. train loss: 50.0483,	1.3754 s / batch. (data: 5.44e-01). ETA=15:18:32, max mem: 20.9 GB 
[11/26 17:06:02 visual_prompt]: 	Training 400/553. train loss: 83.2198,	0.8278 s / batch. (data: 3.02e-04). ETA=9:11:25, max mem: 20.9 GB 
[11/26 17:07:38 visual_prompt]: 	Training 500/553. train loss: 151.5318,	0.8433 s / batch. (data: 2.77e-04). ETA=9:20:23, max mem: 20.9 GB 
[11/26 17:08:29 visual_prompt]: Epoch 28 / 100: avg data time: 1.46e-01, avg batch time: 0.9770, average train loss: 33.4609
[11/26 17:09:25 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3093, average loss: 80.3306
[11/26 17:09:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.43	
[11/26 17:09:25 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[11/26 17:11:13 visual_prompt]: 	Training 100/553. train loss: 1.9684,	0.8320 s / batch. (data: 7.95e-03). ETA=9:10:43, max mem: 20.9 GB 
[11/26 17:12:50 visual_prompt]: 	Training 200/553. train loss: 0.9400,	1.7051 s / batch. (data: 8.83e-01). ETA=18:45:47, max mem: 20.9 GB 
[11/26 17:14:26 visual_prompt]: 	Training 300/553. train loss: 7.0530,	0.8383 s / batch. (data: 7.17e-04). ETA=9:12:06, max mem: 20.9 GB 
[11/26 17:16:00 visual_prompt]: 	Training 400/553. train loss: 34.1399,	1.1064 s / batch. (data: 2.91e-01). ETA=12:06:51, max mem: 20.9 GB 
[11/26 17:17:38 visual_prompt]: 	Training 500/553. train loss: 18.4471,	0.8281 s / batch. (data: 5.45e-03). ETA=9:02:35, max mem: 20.9 GB 
[11/26 17:18:29 visual_prompt]: Epoch 29 / 100: avg data time: 1.50e-01, avg batch time: 0.9826, average train loss: 24.1689
[11/26 17:19:24 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3100, average loss: 9.8496
[11/26 17:19:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.99	
[11/26 17:19:24 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[11/26 17:21:05 visual_prompt]: 	Training 100/553. train loss: 60.4559,	0.8520 s / batch. (data: 2.69e-04). ETA=9:16:08, max mem: 20.9 GB 
[11/26 17:22:44 visual_prompt]: 	Training 200/553. train loss: 74.9746,	0.8267 s / batch. (data: 3.30e-04). ETA=8:58:12, max mem: 20.9 GB 
[11/26 17:24:19 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.0002 s / batch. (data: 1.81e-01). ETA=10:49:29, max mem: 20.9 GB 
[11/26 17:25:58 visual_prompt]: 	Training 400/553. train loss: 12.4831,	0.9356 s / batch. (data: 9.70e-02). ETA=10:06:01, max mem: 20.9 GB 
[11/26 17:27:35 visual_prompt]: 	Training 500/553. train loss: 11.8023,	1.3111 s / batch. (data: 4.66e-01). ETA=14:07:02, max mem: 20.9 GB 
[11/26 17:28:27 visual_prompt]: Epoch 30 / 100: avg data time: 1.51e-01, avg batch time: 0.9813, average train loss: 32.8756
[11/26 17:29:23 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3098, average loss: 43.5503
[11/26 17:29:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.66	
[11/26 17:29:23 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[11/26 17:31:06 visual_prompt]: 	Training 100/553. train loss: 11.9113,	0.8373 s / batch. (data: 2.07e-02). ETA=8:58:46, max mem: 20.9 GB 
[11/26 17:32:45 visual_prompt]: 	Training 200/553. train loss: 36.5928,	0.8560 s / batch. (data: 3.60e-04). ETA=9:09:22, max mem: 20.9 GB 
[11/26 17:34:20 visual_prompt]: 	Training 300/553. train loss: 67.0444,	0.8520 s / batch. (data: 5.44e-03). ETA=9:05:24, max mem: 20.9 GB 
[11/26 17:35:57 visual_prompt]: 	Training 400/553. train loss: 103.4922,	0.8862 s / batch. (data: 6.52e-02). ETA=9:25:50, max mem: 20.9 GB 
[11/26 17:37:35 visual_prompt]: 	Training 500/553. train loss: 14.1946,	0.8527 s / batch. (data: 2.07e-02). ETA=9:03:03, max mem: 20.9 GB 
[11/26 17:38:25 visual_prompt]: Epoch 31 / 100: avg data time: 1.48e-01, avg batch time: 0.9796, average train loss: 28.0165
[11/26 17:39:21 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3113, average loss: 30.4209
[11/26 17:39:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.14	
[11/26 17:39:21 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[11/26 17:41:04 visual_prompt]: 	Training 100/553. train loss: 2.3228,	0.8389 s / batch. (data: 5.95e-03). ETA=8:52:07, max mem: 20.9 GB 
[11/26 17:42:41 visual_prompt]: 	Training 200/553. train loss: 34.4871,	0.8295 s / batch. (data: 5.43e-03). ETA=8:44:43, max mem: 20.9 GB 
[11/26 17:44:22 visual_prompt]: 	Training 300/553. train loss: 5.7643,	0.8234 s / batch. (data: 2.99e-04). ETA=8:39:32, max mem: 20.9 GB 
[11/26 17:46:00 visual_prompt]: 	Training 400/553. train loss: 13.2650,	0.8399 s / batch. (data: 3.14e-04). ETA=8:48:30, max mem: 20.9 GB 
[11/26 17:47:35 visual_prompt]: 	Training 500/553. train loss: 23.5068,	0.8245 s / batch. (data: 3.18e-04). ETA=8:37:26, max mem: 20.9 GB 
[11/26 17:48:25 visual_prompt]: Epoch 32 / 100: avg data time: 1.51e-01, avg batch time: 0.9827, average train loss: 25.0306
[11/26 17:49:21 visual_prompt]: Inference (val):avg data time: 1.57e-04, avg batch time: 0.3104, average loss: 22.8979
[11/26 17:49:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.84	
[11/26 17:49:21 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[11/26 17:51:01 visual_prompt]: 	Training 100/553. train loss: 0.0000,	1.1520 s / batch. (data: 3.20e-01). ETA=12:00:04, max mem: 20.9 GB 
[11/26 17:52:41 visual_prompt]: 	Training 200/553. train loss: 6.4431,	0.8529 s / batch. (data: 3.32e-02). ETA=8:51:41, max mem: 20.9 GB 
[11/26 17:54:18 visual_prompt]: 	Training 300/553. train loss: 6.4296,	0.8327 s / batch. (data: 7.95e-03). ETA=8:37:41, max mem: 20.9 GB 
[11/26 17:55:57 visual_prompt]: 	Training 400/553. train loss: 9.9908,	0.8367 s / batch. (data: 7.95e-03). ETA=8:38:49, max mem: 20.9 GB 
[11/26 17:57:34 visual_prompt]: 	Training 500/553. train loss: 21.1172,	0.8217 s / batch. (data: 3.00e-04). ETA=8:28:06, max mem: 20.9 GB 
[11/26 17:58:24 visual_prompt]: Epoch 33 / 100: avg data time: 1.50e-01, avg batch time: 0.9822, average train loss: 22.6111
[11/26 17:59:21 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3099, average loss: 99.1680
[11/26 17:59:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.55	
[11/26 17:59:21 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[11/26 18:01:06 visual_prompt]: 	Training 100/553. train loss: 1.5003,	0.8320 s / batch. (data: 4.15e-04). ETA=8:32:21, max mem: 20.9 GB 
[11/26 18:02:44 visual_prompt]: 	Training 200/553. train loss: 5.6217,	0.8481 s / batch. (data: 1.20e-02). ETA=8:40:52, max mem: 20.9 GB 
[11/26 18:04:22 visual_prompt]: 	Training 300/553. train loss: 25.5805,	0.8407 s / batch. (data: 3.83e-04). ETA=8:34:55, max mem: 20.9 GB 
[11/26 18:06:04 visual_prompt]: 	Training 400/553. train loss: 4.7217,	0.8400 s / batch. (data: 2.94e-04). ETA=8:33:06, max mem: 20.9 GB 
[11/26 18:07:45 visual_prompt]: 	Training 500/553. train loss: 1.9305,	1.4943 s / batch. (data: 6.59e-01). ETA=15:10:19, max mem: 20.9 GB 
[11/26 18:08:36 visual_prompt]: Epoch 34 / 100: avg data time: 1.71e-01, avg batch time: 1.0033, average train loss: 21.0316
[11/26 18:09:33 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3105, average loss: 14.2374
[11/26 18:09:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.20	
[11/26 18:09:33 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[11/26 18:11:18 visual_prompt]: 	Training 100/553. train loss: 14.9943,	0.8473 s / batch. (data: 1.12e-02). ETA=8:33:59, max mem: 20.9 GB 
[11/26 18:13:01 visual_prompt]: 	Training 200/553. train loss: 24.9822,	0.8440 s / batch. (data: 3.25e-04). ETA=8:30:36, max mem: 20.9 GB 
[11/26 18:14:39 visual_prompt]: 	Training 300/553. train loss: 18.4030,	0.8560 s / batch. (data: 1.20e-02). ETA=8:36:24, max mem: 20.9 GB 
[11/26 18:16:17 visual_prompt]: 	Training 400/553. train loss: 50.2889,	0.8966 s / batch. (data: 6.53e-02). ETA=8:59:25, max mem: 20.9 GB 
[11/26 18:17:58 visual_prompt]: 	Training 500/553. train loss: 4.6468,	1.0399 s / batch. (data: 2.10e-01). ETA=10:23:53, max mem: 20.9 GB 
[11/26 18:18:50 visual_prompt]: Epoch 35 / 100: avg data time: 1.75e-01, avg batch time: 1.0075, average train loss: 20.0561
[11/26 18:19:48 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3099, average loss: 4.8598
[11/26 18:19:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.05	
[11/26 18:19:48 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[11/26 18:21:32 visual_prompt]: 	Training 100/553. train loss: 38.1920,	0.8374 s / batch. (data: 3.43e-04). ETA=8:20:17, max mem: 20.9 GB 
[11/26 18:23:13 visual_prompt]: 	Training 200/553. train loss: 44.7937,	0.8200 s / batch. (data: 3.31e-04). ETA=8:08:30, max mem: 20.9 GB 
[11/26 18:24:55 visual_prompt]: 	Training 300/553. train loss: 12.5179,	0.8240 s / batch. (data: 3.04e-04). ETA=8:09:32, max mem: 20.9 GB 
[11/26 18:26:35 visual_prompt]: 	Training 400/553. train loss: 16.2229,	0.8442 s / batch. (data: 2.93e-04). ETA=8:20:08, max mem: 20.9 GB 
[11/26 18:28:15 visual_prompt]: 	Training 500/553. train loss: 8.0568,	1.0889 s / batch. (data: 2.69e-01). ETA=10:43:14, max mem: 20.9 GB 
[11/26 18:29:05 visual_prompt]: Epoch 36 / 100: avg data time: 1.75e-01, avg batch time: 1.0062, average train loss: 22.4611
[11/26 18:30:02 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3091, average loss: 71.6065
[11/26 18:30:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.06	
[11/26 18:30:02 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[11/26 18:31:47 visual_prompt]: 	Training 100/553. train loss: 11.0589,	0.8525 s / batch. (data: 1.05e-02). ETA=8:21:26, max mem: 20.9 GB 
[11/26 18:33:27 visual_prompt]: 	Training 200/553. train loss: 17.2239,	0.8274 s / batch. (data: 1.05e-02). ETA=8:05:18, max mem: 20.9 GB 
[11/26 18:35:07 visual_prompt]: 	Training 300/553. train loss: 0.3063,	1.3174 s / batch. (data: 4.92e-01). ETA=12:50:30, max mem: 20.9 GB 
[11/26 18:36:50 visual_prompt]: 	Training 400/553. train loss: 24.4959,	1.6708 s / batch. (data: 8.54e-01). ETA=16:14:23, max mem: 20.9 GB 
[11/26 18:38:27 visual_prompt]: 	Training 500/553. train loss: 12.7340,	1.1207 s / batch. (data: 2.74e-01). ETA=10:51:41, max mem: 20.9 GB 
[11/26 18:39:21 visual_prompt]: Epoch 37 / 100: avg data time: 1.78e-01, avg batch time: 1.0095, average train loss: 22.0481
[11/26 18:40:18 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3098, average loss: 2.0686
[11/26 18:40:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.92	
[11/26 18:40:18 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[11/26 18:42:00 visual_prompt]: 	Training 100/553. train loss: 1.5612,	0.8667 s / batch. (data: 1.05e-02). ETA=8:21:49, max mem: 20.9 GB 
[11/26 18:43:41 visual_prompt]: 	Training 200/553. train loss: 12.0778,	1.0880 s / batch. (data: 2.53e-01). ETA=10:28:08, max mem: 20.9 GB 
[11/26 18:45:23 visual_prompt]: 	Training 300/553. train loss: 26.0816,	0.8440 s / batch. (data: 3.09e-04). ETA=8:05:50, max mem: 20.9 GB 
[11/26 18:47:01 visual_prompt]: 	Training 400/553. train loss: 24.8220,	0.8431 s / batch. (data: 1.19e-02). ETA=8:03:54, max mem: 20.9 GB 
[11/26 18:48:43 visual_prompt]: 	Training 500/553. train loss: 16.6523,	0.8325 s / batch. (data: 2.91e-04). ETA=7:56:26, max mem: 20.9 GB 
[11/26 18:49:34 visual_prompt]: Epoch 38 / 100: avg data time: 1.73e-01, avg batch time: 1.0049, average train loss: 24.5533
[11/26 18:50:31 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3105, average loss: 14.5569
[11/26 18:50:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.09	
[11/26 18:50:31 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[11/26 18:52:14 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8235 s / batch. (data: 3.85e-03). ETA=7:49:11, max mem: 20.9 GB 
[11/26 18:53:57 visual_prompt]: 	Training 200/553. train loss: 34.2023,	0.8200 s / batch. (data: 3.04e-04). ETA=7:45:50, max mem: 20.9 GB 
[11/26 18:55:41 visual_prompt]: 	Training 300/553. train loss: 65.7863,	0.8719 s / batch. (data: 8.28e-04). ETA=8:13:52, max mem: 20.9 GB 
[11/26 18:57:18 visual_prompt]: 	Training 400/553. train loss: 6.2619,	0.8850 s / batch. (data: 5.44e-02). ETA=8:19:49, max mem: 20.9 GB 
[11/26 18:58:59 visual_prompt]: 	Training 500/553. train loss: 12.1819,	1.7015 s / batch. (data: 8.72e-01). ETA=15:58:07, max mem: 20.9 GB 
[11/26 18:59:49 visual_prompt]: Epoch 39 / 100: avg data time: 1.76e-01, avg batch time: 1.0093, average train loss: 22.5044
[11/26 19:00:47 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3106, average loss: 16.7945
[11/26 19:00:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.91	
[11/26 19:00:47 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[11/26 19:02:31 visual_prompt]: 	Training 100/553. train loss: 17.2350,	0.8438 s / batch. (data: 5.43e-03). ETA=7:53:00, max mem: 20.9 GB 
[11/26 19:04:11 visual_prompt]: 	Training 200/553. train loss: 19.9245,	0.8222 s / batch. (data: 5.45e-03). ETA=7:39:29, max mem: 20.9 GB 
[11/26 19:05:52 visual_prompt]: 	Training 300/553. train loss: 35.8514,	0.8360 s / batch. (data: 3.27e-04). ETA=7:45:49, max mem: 20.9 GB 
[11/26 19:07:33 visual_prompt]: 	Training 400/553. train loss: 8.9490,	0.8190 s / batch. (data: 2.93e-04). ETA=7:35:01, max mem: 20.9 GB 
[11/26 19:09:12 visual_prompt]: 	Training 500/553. train loss: 25.1054,	0.8240 s / batch. (data: 2.99e-04). ETA=7:36:24, max mem: 20.9 GB 
[11/26 19:10:05 visual_prompt]: Epoch 40 / 100: avg data time: 1.78e-01, avg batch time: 1.0099, average train loss: 20.1436
[11/26 19:11:03 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3094, average loss: 16.0810
[11/26 19:11:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.86	
[11/26 19:11:03 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[11/26 19:12:52 visual_prompt]: 	Training 100/553. train loss: 0.6623,	0.8270 s / batch. (data: 3.10e-04). ETA=7:35:56, max mem: 20.9 GB 
[11/26 19:14:34 visual_prompt]: 	Training 200/553. train loss: 7.6635,	0.8477 s / batch. (data: 3.80e-04). ETA=7:45:56, max mem: 20.9 GB 
[11/26 19:16:13 visual_prompt]: 	Training 300/553. train loss: 33.7016,	0.8205 s / batch. (data: 3.18e-04). ETA=7:29:38, max mem: 20.9 GB 
[11/26 19:17:53 visual_prompt]: 	Training 400/553. train loss: 22.9489,	0.8400 s / batch. (data: 7.40e-04). ETA=7:38:56, max mem: 20.9 GB 
[11/26 19:19:31 visual_prompt]: 	Training 500/553. train loss: 2.8368,	0.8640 s / batch. (data: 7.16e-04). ETA=7:50:35, max mem: 20.9 GB 
[11/26 19:20:20 visual_prompt]: Epoch 41 / 100: avg data time: 1.77e-01, avg batch time: 1.0078, average train loss: 22.9542
[11/26 19:21:17 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3092, average loss: 16.2562
[11/26 19:21:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.14	
[11/26 19:21:17 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[11/26 19:22:58 visual_prompt]: 	Training 100/553. train loss: 3.2281,	0.8519 s / batch. (data: 2.43e-02). ETA=7:41:50, max mem: 20.9 GB 
[11/26 19:24:35 visual_prompt]: 	Training 200/553. train loss: 105.2772,	0.8433 s / batch. (data: 1.05e-02). ETA=7:35:45, max mem: 20.9 GB 
[11/26 19:26:13 visual_prompt]: 	Training 300/553. train loss: 15.7625,	0.8194 s / batch. (data: 3.66e-04). ETA=7:21:29, max mem: 20.9 GB 
[11/26 19:27:50 visual_prompt]: 	Training 400/553. train loss: 7.3702,	0.8286 s / batch. (data: 3.20e-04). ETA=7:25:01, max mem: 20.9 GB 
[11/26 19:29:26 visual_prompt]: 	Training 500/553. train loss: 47.2387,	0.8282 s / batch. (data: 3.05e-04). ETA=7:23:26, max mem: 20.9 GB 
[11/26 19:30:18 visual_prompt]: Epoch 42 / 100: avg data time: 1.46e-01, avg batch time: 0.9780, average train loss: 23.9258
[11/26 19:31:14 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3116, average loss: 24.8458
[11/26 19:31:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.97	
[11/26 19:31:14 visual_prompt]: Stopping early.
[11/26 19:31:14 visual_prompt]: Rank of current process: 0. World size: 1
[11/26 19:31:14 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/26 19:31:14 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/26 19:31:14 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/26 19:31:14 visual_prompt]: Training with config:
[11/26 19:31:14 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/26 19:31:14 visual_prompt]: Loading training data...
[11/26 19:31:14 visual_prompt]: Constructing mammo-cbis dataset train...
[11/26 19:31:14 visual_prompt]: Loading validation data...
[11/26 19:31:14 visual_prompt]: Constructing mammo-cbis dataset val...
[11/26 19:31:14 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/26 19:31:17 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/26 19:31:17 visual_prompt]: tuned percent:0.525
[11/26 19:31:17 visual_prompt]: Device used for model: 0
[11/26 19:31:17 visual_prompt]: Setting up Evaluator...
[11/26 19:31:17 visual_prompt]: Setting up Trainer...
[11/26 19:31:17 visual_prompt]: 	Setting up the optimizer...
[11/26 19:31:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/26 19:32:58 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8322 s / batch. (data: 3.05e-04). ETA=12:45:36, max mem: 20.9 GB 
[11/26 19:34:33 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8360 s / batch. (data: 6.89e-03). ETA=12:47:43, max mem: 20.9 GB 
[11/26 19:36:13 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.0640 s / batch. (data: 2.11e-01). ETA=16:15:19, max mem: 20.9 GB 
[11/26 19:37:49 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8476 s / batch. (data: 2.77e-02). ETA=12:55:35, max mem: 20.9 GB 
[11/26 19:39:28 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8235 s / batch. (data: 3.63e-04). ETA=12:32:10, max mem: 20.9 GB 
[11/26 19:40:20 visual_prompt]: Epoch 1 / 100: avg data time: 1.47e-01, avg batch time: 0.9811, average train loss: 1.5403
[11/26 19:41:15 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3113, average loss: 1.5201
[11/26 19:41:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/26 19:41:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/26 19:42:57 visual_prompt]: 	Training 100/553. train loss: 2.4547,	0.8177 s / batch. (data: 3.20e-04). ETA=12:24:44, max mem: 20.9 GB 
[11/26 19:44:33 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8191 s / batch. (data: 3.40e-04). ETA=12:24:37, max mem: 20.9 GB 
[11/26 19:46:13 visual_prompt]: 	Training 300/553. train loss: 7.6994,	1.0006 s / batch. (data: 1.80e-01). ETA=15:07:58, max mem: 20.9 GB 
[11/26 19:47:49 visual_prompt]: 	Training 400/553. train loss: 0.7945,	0.8720 s / batch. (data: 3.49e-04). ETA=13:09:53, max mem: 20.9 GB 
[11/26 19:49:28 visual_prompt]: 	Training 500/553. train loss: 0.8968,	0.8461 s / batch. (data: 3.30e-04). ETA=12:44:58, max mem: 20.9 GB 
[11/26 19:50:18 visual_prompt]: Epoch 2 / 100: avg data time: 1.47e-01, avg batch time: 0.9821, average train loss: 4.2426
[11/26 19:51:14 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3103, average loss: 0.7099
[11/26 19:51:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.19	rocauc: 59.89	
[11/26 19:51:14 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/26 19:52:54 visual_prompt]: 	Training 100/553. train loss: 6.4990,	0.8480 s / batch. (data: 7.95e-03). ETA=12:44:31, max mem: 20.9 GB 
[11/26 19:54:33 visual_prompt]: 	Training 200/553. train loss: 4.0049,	0.8362 s / batch. (data: 5.44e-03). ETA=12:32:28, max mem: 20.9 GB 
[11/26 19:56:10 visual_prompt]: 	Training 300/553. train loss: 0.8738,	0.8560 s / batch. (data: 3.07e-04). ETA=12:48:52, max mem: 20.9 GB 
[11/26 19:57:47 visual_prompt]: 	Training 400/553. train loss: 8.9774,	0.8641 s / batch. (data: 1.39e-02). ETA=12:54:43, max mem: 20.9 GB 
[11/26 19:59:25 visual_prompt]: 	Training 500/553. train loss: 7.3340,	1.1421 s / batch. (data: 3.26e-01). ETA=17:02:02, max mem: 20.9 GB 
[11/26 20:00:15 visual_prompt]: Epoch 3 / 100: avg data time: 1.45e-01, avg batch time: 0.9788, average train loss: 7.6226
[11/26 20:01:11 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3102, average loss: 5.2789
[11/26 20:01:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.01	
[11/26 20:01:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/26 20:02:54 visual_prompt]: 	Training 100/553. train loss: 7.9321,	0.8287 s / batch. (data: 2.98e-04). ETA=12:19:28, max mem: 20.9 GB 
[11/26 20:04:32 visual_prompt]: 	Training 200/553. train loss: 12.9808,	0.8200 s / batch. (data: 3.51e-04). ETA=12:10:20, max mem: 20.9 GB 
[11/26 20:06:10 visual_prompt]: 	Training 300/553. train loss: 8.3782,	1.2808 s / batch. (data: 4.59e-01). ETA=18:58:40, max mem: 20.9 GB 
[11/26 20:07:43 visual_prompt]: 	Training 400/553. train loss: 0.5039,	0.8480 s / batch. (data: 3.22e-04). ETA=12:32:28, max mem: 20.9 GB 
[11/26 20:09:23 visual_prompt]: 	Training 500/553. train loss: 31.1022,	3.4872 s / batch. (data: 2.67e+00). ETA=2 days, 3:28:32, max mem: 20.9 GB 
[11/26 20:10:15 visual_prompt]: Epoch 4 / 100: avg data time: 1.50e-01, avg batch time: 0.9835, average train loss: 8.4598
[11/26 20:11:10 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3094, average loss: 31.7809
[11/26 20:11:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.11	
[11/26 20:11:10 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/26 20:12:51 visual_prompt]: 	Training 100/553. train loss: 0.0982,	0.8558 s / batch. (data: 7.77e-03). ETA=12:35:45, max mem: 20.9 GB 
[11/26 20:14:28 visual_prompt]: 	Training 200/553. train loss: 5.2413,	1.1310 s / batch. (data: 3.13e-01). ETA=16:36:57, max mem: 20.9 GB 
[11/26 20:16:07 visual_prompt]: 	Training 300/553. train loss: 3.9706,	0.8423 s / batch. (data: 3.11e-04). ETA=12:21:03, max mem: 20.9 GB 
[11/26 20:17:44 visual_prompt]: 	Training 400/553. train loss: 0.8584,	0.8252 s / batch. (data: 3.24e-04). ETA=12:04:37, max mem: 20.9 GB 
[11/26 20:19:22 visual_prompt]: 	Training 500/553. train loss: 8.9242,	0.8560 s / batch. (data: 3.09e-04). ETA=12:30:16, max mem: 20.9 GB 
[11/26 20:20:14 visual_prompt]: Epoch 5 / 100: avg data time: 1.49e-01, avg batch time: 0.9833, average train loss: 10.1281
[11/26 20:21:10 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3091, average loss: 22.4373
[11/26 20:21:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.64	
[11/26 20:21:10 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/26 20:22:53 visual_prompt]: 	Training 100/553. train loss: 3.7443,	0.8319 s / batch. (data: 5.37e-03). ETA=12:06:58, max mem: 20.9 GB 
[11/26 20:24:30 visual_prompt]: 	Training 200/553. train loss: 5.5833,	0.8320 s / batch. (data: 2.98e-04). ETA=12:05:43, max mem: 20.9 GB 
[11/26 20:26:05 visual_prompt]: 	Training 300/553. train loss: 2.3348,	0.8369 s / batch. (data: 2.78e-04). ETA=12:08:35, max mem: 20.9 GB 
[11/26 20:27:47 visual_prompt]: 	Training 400/553. train loss: 26.8347,	0.8279 s / batch. (data: 3.37e-04). ETA=11:59:23, max mem: 20.9 GB 
[11/26 20:29:22 visual_prompt]: 	Training 500/553. train loss: 1.0403,	0.8284 s / batch. (data: 3.76e-04). ETA=11:58:27, max mem: 20.9 GB 
[11/26 20:30:13 visual_prompt]: Epoch 6 / 100: avg data time: 1.48e-01, avg batch time: 0.9820, average train loss: 11.6939
[11/26 20:31:09 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3107, average loss: 5.0968
[11/26 20:31:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.36	
[11/26 20:31:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/26 20:32:49 visual_prompt]: 	Training 100/553. train loss: 10.6278,	0.8284 s / batch. (data: 3.14e-04). ETA=11:56:18, max mem: 20.9 GB 
[11/26 20:34:26 visual_prompt]: 	Training 200/553. train loss: 8.8718,	1.0044 s / batch. (data: 1.56e-01). ETA=14:26:48, max mem: 20.9 GB 
[11/26 20:36:07 visual_prompt]: 	Training 300/553. train loss: 16.6753,	1.7029 s / batch. (data: 8.75e-01). ETA=1 day, 0:26:48, max mem: 20.9 GB 
[11/26 20:37:45 visual_prompt]: 	Training 400/553. train loss: 9.0358,	1.8120 s / batch. (data: 9.84e-01). ETA=1 day, 1:57:45, max mem: 20.9 GB 
[11/26 20:39:20 visual_prompt]: 	Training 500/553. train loss: 48.6467,	0.8223 s / batch. (data: 3.12e-04). ETA=11:45:35, max mem: 20.9 GB 
[11/26 20:40:10 visual_prompt]: Epoch 7 / 100: avg data time: 1.43e-01, avg batch time: 0.9783, average train loss: 14.1636
[11/26 20:41:05 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3121, average loss: 17.4630
[11/26 20:41:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.64	
[11/26 20:41:05 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/26 20:42:45 visual_prompt]: 	Training 100/553. train loss: 50.7500,	0.8318 s / batch. (data: 3.04e-04). ETA=11:51:36, max mem: 20.9 GB 
[11/26 20:44:23 visual_prompt]: 	Training 200/553. train loss: 3.8612,	0.8280 s / batch. (data: 3.07e-04). ETA=11:46:57, max mem: 20.9 GB 
[11/26 20:46:02 visual_prompt]: 	Training 300/553. train loss: 33.2820,	0.8560 s / batch. (data: 3.05e-04). ETA=12:09:26, max mem: 20.9 GB 
[11/26 20:47:40 visual_prompt]: 	Training 400/553. train loss: 1.3699,	0.8373 s / batch. (data: 3.20e-04). ETA=11:52:04, max mem: 20.9 GB 
[11/26 20:49:18 visual_prompt]: 	Training 500/553. train loss: 0.3061,	1.2920 s / batch. (data: 4.61e-01). ETA=18:16:40, max mem: 20.9 GB 
[11/26 20:50:09 visual_prompt]: Epoch 8 / 100: avg data time: 1.50e-01, avg batch time: 0.9833, average train loss: 14.8445
[11/26 20:51:05 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3109, average loss: 2.0214
[11/26 20:51:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 62.54	
[11/26 20:51:05 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/26 20:52:46 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8280 s / batch. (data: 2.88e-04). ETA=11:40:41, max mem: 20.9 GB 
[11/26 20:54:22 visual_prompt]: 	Training 200/553. train loss: 3.6899,	0.8471 s / batch. (data: 5.43e-03). ETA=11:55:26, max mem: 20.9 GB 
[11/26 20:56:00 visual_prompt]: 	Training 300/553. train loss: 13.8040,	1.5960 s / batch. (data: 7.41e-01). ETA=22:25:17, max mem: 20.9 GB 
[11/26 20:57:40 visual_prompt]: 	Training 400/553. train loss: 12.5228,	0.8499 s / batch. (data: 7.57e-04). ETA=11:54:59, max mem: 20.9 GB 
[11/26 20:59:18 visual_prompt]: 	Training 500/553. train loss: 8.6952,	0.8403 s / batch. (data: 3.43e-04). ETA=11:45:33, max mem: 20.9 GB 
[11/26 21:00:08 visual_prompt]: Epoch 9 / 100: avg data time: 1.48e-01, avg batch time: 0.9815, average train loss: 14.6695

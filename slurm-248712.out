/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/23 00:59:52 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 00:59:54 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 00:59:54 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/23 00:59:54 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 00:59:54 visual_prompt]: Training with config:
[11/23 00:59:54 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/23 00:59:54 visual_prompt]: Loading training data...
[11/23 00:59:54 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 00:59:54 visual_prompt]: Loading validation data...
[11/23 00:59:54 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 00:59:54 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 01:00:03 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 01:00:03 visual_prompt]: tuned percent:0.525
[11/23 01:00:04 visual_prompt]: Device used for model: 0
[11/23 01:00:04 visual_prompt]: Setting up Evaluator...
[11/23 01:00:04 visual_prompt]: Setting up Trainer...
[11/23 01:00:04 visual_prompt]: 	Setting up the optimizer...
[11/23 01:00:04 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 01:01:49 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8283 s / batch. (data: 4.84e-04). ETA=12:42:03, max mem: 20.9 GB 
[11/23 01:03:27 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8360 s / batch. (data: 1.03e-02). ETA=12:47:41, max mem: 20.9 GB 
[11/23 01:05:07 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3200 s / batch. (data: 4.93e-01). ETA=20:09:59, max mem: 20.9 GB 
[11/23 01:06:42 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8357 s / batch. (data: 5.41e-03). ETA=12:44:38, max mem: 20.9 GB 
[11/23 01:08:21 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8281 s / batch. (data: 5.39e-03). ETA=12:36:18, max mem: 20.9 GB 
[11/23 01:09:12 visual_prompt]: Epoch 1 / 100: avg data time: 1.60e-01, avg batch time: 0.9913, average train loss: 1.5403
[11/23 01:10:06 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3115, average loss: 1.5201
[11/23 01:10:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 01:10:06 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/23 01:11:46 visual_prompt]: 	Training 100/553. train loss: 11.0660,	1.0625 s / batch. (data: 2.42e-01). ETA=16:07:40, max mem: 20.9 GB 
[11/23 01:13:21 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.9680 s / batch. (data: 1.37e-01). ETA=14:40:01, max mem: 20.9 GB 
[11/23 01:15:00 visual_prompt]: 	Training 300/553. train loss: 16.4932,	0.9888 s / batch. (data: 1.70e-01). ETA=14:57:17, max mem: 20.9 GB 
[11/23 01:16:35 visual_prompt]: 	Training 400/553. train loss: 6.3848,	0.8312 s / batch. (data: 2.98e-04). ETA=12:32:54, max mem: 20.9 GB 
[11/23 01:18:13 visual_prompt]: 	Training 500/553. train loss: 1.8677,	0.8368 s / batch. (data: 3.21e-04). ETA=12:36:35, max mem: 20.9 GB 
[11/23 01:19:02 visual_prompt]: Epoch 2 / 100: avg data time: 1.36e-01, avg batch time: 0.9680, average train loss: 14.2649
[11/23 01:19:56 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3113, average loss: 67.7520
[11/23 01:19:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.35	
[11/23 01:19:56 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/23 01:21:35 visual_prompt]: 	Training 100/553. train loss: 36.6844,	0.8450 s / batch. (data: 1.08e-02). ETA=12:41:50, max mem: 20.9 GB 
[11/23 01:23:12 visual_prompt]: 	Training 200/553. train loss: 39.5236,	0.8348 s / batch. (data: 2.93e-04). ETA=12:31:12, max mem: 20.9 GB 
[11/23 01:24:47 visual_prompt]: 	Training 300/553. train loss: 87.4285,	0.8440 s / batch. (data: 3.04e-04). ETA=12:38:05, max mem: 20.9 GB 
[11/23 01:26:23 visual_prompt]: 	Training 400/553. train loss: 14.2160,	0.8271 s / batch. (data: 3.11e-04). ETA=12:21:34, max mem: 20.9 GB 
[11/23 01:28:01 visual_prompt]: 	Training 500/553. train loss: 37.0757,	1.1080 s / batch. (data: 2.78e-01). ETA=16:31:31, max mem: 20.9 GB 
[11/23 01:28:50 visual_prompt]: Epoch 3 / 100: avg data time: 1.35e-01, avg batch time: 0.9646, average train loss: 39.4319
[11/23 01:29:44 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3084, average loss: 28.8792
[11/23 01:29:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.40	
[11/23 01:29:44 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/23 01:31:27 visual_prompt]: 	Training 100/553. train loss: 151.6634,	0.8734 s / batch. (data: 2.54e-02). ETA=12:59:23, max mem: 20.9 GB 
[11/23 01:33:07 visual_prompt]: 	Training 200/553. train loss: 0.4510,	0.8262 s / batch. (data: 3.11e-04). ETA=12:15:53, max mem: 20.9 GB 
[11/23 01:34:45 visual_prompt]: 	Training 300/553. train loss: 7.5754,	1.2880 s / batch. (data: 4.54e-01). ETA=19:05:01, max mem: 20.9 GB 
[11/23 01:36:18 visual_prompt]: 	Training 400/553. train loss: 67.7845,	1.2680 s / batch. (data: 4.30e-01). ETA=18:45:10, max mem: 20.9 GB 
[11/23 01:37:56 visual_prompt]: 	Training 500/553. train loss: 0.0000,	2.9759 s / batch. (data: 2.17e+00). ETA=1 day, 19:55:39, max mem: 20.9 GB 
[11/23 01:38:49 visual_prompt]: Epoch 4 / 100: avg data time: 1.56e-01, avg batch time: 0.9841, average train loss: 53.2946
[11/23 01:39:44 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3112, average loss: 34.9306
[11/23 01:39:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.06	
[11/23 01:39:44 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/23 01:41:23 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8280 s / batch. (data: 2.89e-04). ETA=12:11:14, max mem: 20.9 GB 
[11/23 01:43:00 visual_prompt]: 	Training 200/553. train loss: 35.7910,	1.1909 s / batch. (data: 3.61e-01). ETA=17:29:45, max mem: 20.9 GB 
[11/23 01:44:37 visual_prompt]: 	Training 300/553. train loss: 13.4560,	0.8471 s / batch. (data: 3.27e-04). ETA=12:25:18, max mem: 20.9 GB 
[11/23 01:46:14 visual_prompt]: 	Training 400/553. train loss: 74.0575,	0.8314 s / batch. (data: 5.42e-03). ETA=12:10:05, max mem: 20.9 GB 
[11/23 01:47:51 visual_prompt]: 	Training 500/553. train loss: 51.1401,	0.8414 s / batch. (data: 9.33e-03). ETA=12:17:25, max mem: 20.9 GB 
[11/23 01:48:42 visual_prompt]: Epoch 5 / 100: avg data time: 1.45e-01, avg batch time: 0.9730, average train loss: 70.3916
[11/23 01:49:36 visual_prompt]: Inference (val):avg data time: 2.77e-04, avg batch time: 0.3115, average loss: 43.8182
[11/23 01:49:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.23	
[11/23 01:49:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/23 01:51:18 visual_prompt]: 	Training 100/553. train loss: 363.2985,	0.8453 s / batch. (data: 1.60e-02). ETA=12:18:42, max mem: 20.9 GB 
[11/23 01:52:53 visual_prompt]: 	Training 200/553. train loss: 621.9333,	0.8432 s / batch. (data: 1.12e-02). ETA=12:15:28, max mem: 20.9 GB 
[11/23 01:54:27 visual_prompt]: 	Training 300/553. train loss: 82.2972,	0.8720 s / batch. (data: 7.97e-03). ETA=12:39:09, max mem: 20.9 GB 
[11/23 01:56:07 visual_prompt]: 	Training 400/553. train loss: 33.9150,	0.8163 s / batch. (data: 5.40e-03). ETA=11:49:17, max mem: 20.9 GB 
[11/23 01:57:42 visual_prompt]: 	Training 500/553. train loss: 96.2137,	0.8199 s / batch. (data: 3.40e-04). ETA=11:51:02, max mem: 20.9 GB 
[11/23 01:58:31 visual_prompt]: Epoch 6 / 100: avg data time: 1.40e-01, avg batch time: 0.9670, average train loss: 103.8233
[11/23 01:59:26 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3103, average loss: 91.2159
[11/23 01:59:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.52	
[11/23 01:59:26 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/23 02:01:04 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8017 s / batch. (data: 3.08e-04). ETA=11:33:14, max mem: 20.9 GB 
[11/23 02:02:40 visual_prompt]: 	Training 200/553. train loss: 64.2342,	0.8290 s / batch. (data: 7.95e-03). ETA=11:55:25, max mem: 20.9 GB 
[11/23 02:04:18 visual_prompt]: 	Training 300/553. train loss: 5.0139,	1.4510 s / batch. (data: 6.43e-01). ETA=20:49:52, max mem: 20.9 GB 
[11/23 02:05:55 visual_prompt]: 	Training 400/553. train loss: 52.8119,	1.5314 s / batch. (data: 7.18e-01). ETA=21:56:34, max mem: 20.9 GB 
[11/23 02:07:31 visual_prompt]: 	Training 500/553. train loss: 68.9438,	0.8252 s / batch. (data: 5.41e-03). ETA=11:48:04, max mem: 20.9 GB 
[11/23 02:08:20 visual_prompt]: Epoch 7 / 100: avg data time: 1.39e-01, avg batch time: 0.9660, average train loss: 96.4033
[11/23 02:09:16 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3095, average loss: 174.5468
[11/23 02:09:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.70	
[11/23 02:09:16 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/23 02:10:54 visual_prompt]: 	Training 100/553. train loss: 53.3281,	0.8160 s / batch. (data: 3.60e-04). ETA=11:38:06, max mem: 20.9 GB 
[11/23 02:12:33 visual_prompt]: 	Training 200/553. train loss: 583.0183,	0.8236 s / batch. (data: 5.44e-03). ETA=11:43:11, max mem: 20.9 GB 
[11/23 02:14:09 visual_prompt]: 	Training 300/553. train loss: 62.8790,	0.8097 s / batch. (data: 2.91e-04). ETA=11:29:57, max mem: 20.9 GB 
[11/23 02:15:46 visual_prompt]: 	Training 400/553. train loss: 97.0186,	0.8560 s / batch. (data: 3.30e-04). ETA=12:08:00, max mem: 20.9 GB 
[11/23 02:17:22 visual_prompt]: 	Training 500/553. train loss: 524.1109,	1.1880 s / batch. (data: 3.43e-01). ETA=16:48:23, max mem: 20.9 GB 
[11/23 02:18:13 visual_prompt]: Epoch 8 / 100: avg data time: 1.44e-01, avg batch time: 0.9707, average train loss: 112.1865
[11/23 02:19:07 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3101, average loss: 16.1529
[11/23 02:19:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.73	
[11/23 02:19:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/23 02:20:47 visual_prompt]: 	Training 100/553. train loss: 0.3753,	0.8180 s / batch. (data: 3.07e-04). ETA=11:32:12, max mem: 20.9 GB 
[11/23 02:22:22 visual_prompt]: 	Training 200/553. train loss: 171.0539,	0.8760 s / batch. (data: 1.42e-02). ETA=12:19:52, max mem: 20.9 GB 
[11/23 02:23:59 visual_prompt]: 	Training 300/553. train loss: 55.7412,	1.5533 s / batch. (data: 7.16e-01). ETA=21:49:19, max mem: 20.9 GB 
[11/23 02:25:36 visual_prompt]: 	Training 400/553. train loss: 35.6600,	0.8204 s / batch. (data: 7.76e-04). ETA=11:30:08, max mem: 20.9 GB 
[11/23 02:27:12 visual_prompt]: 	Training 500/553. train loss: 203.6946,	0.8239 s / batch. (data: 1.05e-02). ETA=11:31:47, max mem: 20.9 GB 
[11/23 02:28:02 visual_prompt]: Epoch 9 / 100: avg data time: 1.39e-01, avg batch time: 0.9661, average train loss: 156.6385
[11/23 02:28:57 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3100, average loss: 34.7765
[11/23 02:28:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.50	
[11/23 02:28:57 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/23 02:30:39 visual_prompt]: 	Training 100/553. train loss: 35.8805,	0.8153 s / batch. (data: 3.02e-04). ETA=11:22:24, max mem: 20.9 GB 
[11/23 02:32:13 visual_prompt]: 	Training 200/553. train loss: 283.7655,	0.8280 s / batch. (data: 3.05e-04). ETA=11:31:41, max mem: 20.9 GB 
[11/23 02:33:49 visual_prompt]: 	Training 300/553. train loss: 90.8150,	0.8406 s / batch. (data: 2.84e-04). ETA=11:40:49, max mem: 20.9 GB 
[11/23 02:35:23 visual_prompt]: 	Training 400/553. train loss: 170.0476,	0.8203 s / batch. (data: 3.14e-04). ETA=11:22:32, max mem: 20.9 GB 
[11/23 02:37:00 visual_prompt]: 	Training 500/553. train loss: 16.0459,	0.8625 s / batch. (data: 2.24e-02). ETA=11:56:10, max mem: 20.9 GB 
[11/23 02:37:50 visual_prompt]: Epoch 10 / 100: avg data time: 1.38e-01, avg batch time: 0.9647, average train loss: 156.2801
[11/23 02:38:45 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3123, average loss: 7.4814
[11/23 02:38:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 57.60	
[11/23 02:38:45 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/23 02:40:27 visual_prompt]: 	Training 100/553. train loss: 436.6214,	0.8185 s / batch. (data: 5.40e-03). ETA=11:17:32, max mem: 20.9 GB 
[11/23 02:42:04 visual_prompt]: 	Training 200/553. train loss: 283.7959,	0.8403 s / batch. (data: 5.41e-03). ETA=11:34:12, max mem: 20.9 GB 
[11/23 02:43:39 visual_prompt]: 	Training 300/553. train loss: 7.6936,	2.0778 s / batch. (data: 1.24e+00). ETA=1 day, 4:33:06, max mem: 20.9 GB 
[11/23 02:45:14 visual_prompt]: 	Training 400/553. train loss: 61.5182,	0.8234 s / batch. (data: 5.39e-03). ETA=11:17:32, max mem: 20.9 GB 
[11/23 02:46:48 visual_prompt]: 	Training 500/553. train loss: 224.7789,	0.8630 s / batch. (data: 2.29e-02). ETA=11:48:38, max mem: 20.9 GB 
[11/23 02:47:38 visual_prompt]: Epoch 11 / 100: avg data time: 1.38e-01, avg batch time: 0.9644, average train loss: 171.8130
[11/23 02:48:33 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3100, average loss: 178.9753
[11/23 02:48:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.97	
[11/23 02:48:33 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/23 02:50:14 visual_prompt]: 	Training 100/553. train loss: 185.6118,	0.8185 s / batch. (data: 3.01e-04). ETA=11:10:01, max mem: 20.9 GB 
[11/23 02:51:51 visual_prompt]: 	Training 200/553. train loss: 41.3509,	1.8374 s / batch. (data: 1.01e+00). ETA=1 day, 1:01:02, max mem: 20.9 GB 
[11/23 02:53:25 visual_prompt]: 	Training 300/553. train loss: 61.1303,	0.8229 s / batch. (data: 3.02e-04). ETA=11:10:51, max mem: 20.9 GB 
[11/23 02:55:02 visual_prompt]: 	Training 400/553. train loss: 41.7388,	0.8440 s / batch. (data: 2.75e-04). ETA=11:26:41, max mem: 20.9 GB 
[11/23 02:56:37 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8520 s / batch. (data: 7.94e-04). ETA=11:31:46, max mem: 20.9 GB 
[11/23 02:57:27 visual_prompt]: Epoch 12 / 100: avg data time: 1.39e-01, avg batch time: 0.9653, average train loss: 194.8883
[11/23 02:58:21 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3100, average loss: 327.9139
[11/23 02:58:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.36	
[11/23 02:58:21 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/23 03:00:03 visual_prompt]: 	Training 100/553. train loss: 219.3569,	0.8553 s / batch. (data: 7.97e-03). ETA=11:32:18, max mem: 20.9 GB 
[11/23 03:01:35 visual_prompt]: 	Training 200/553. train loss: 397.0056,	0.8262 s / batch. (data: 1.05e-02). ETA=11:07:21, max mem: 20.9 GB 
[11/23 03:03:12 visual_prompt]: 	Training 300/553. train loss: 206.8777,	1.5963 s / batch. (data: 7.89e-01). ETA=21:26:41, max mem: 20.9 GB 
[11/23 03:04:47 visual_prompt]: 	Training 400/553. train loss: 107.5091,	0.8428 s / batch. (data: 7.96e-03). ETA=11:17:55, max mem: 20.9 GB 
[11/23 03:06:24 visual_prompt]: 	Training 500/553. train loss: 72.6317,	0.8250 s / batch. (data: 2.92e-04). ETA=11:02:16, max mem: 20.9 GB 
[11/23 03:07:14 visual_prompt]: Epoch 13 / 100: avg data time: 1.38e-01, avg batch time: 0.9638, average train loss: 154.9497
[11/23 03:08:09 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3093, average loss: 150.0563
[11/23 03:08:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.86	
[11/23 03:08:09 visual_prompt]: Best epoch 13: best metric: -150.056
[11/23 03:08:09 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/23 03:09:50 visual_prompt]: 	Training 100/553. train loss: 67.0006,	0.8320 s / batch. (data: 3.97e-04). ETA=11:05:43, max mem: 20.9 GB 
[11/23 03:11:26 visual_prompt]: 	Training 200/553. train loss: 235.3682,	0.8702 s / batch. (data: 5.04e-02). ETA=11:34:51, max mem: 20.9 GB 
[11/23 03:13:02 visual_prompt]: 	Training 300/553. train loss: 105.3834,	0.8274 s / batch. (data: 3.04e-04). ETA=10:59:20, max mem: 20.9 GB 
[11/23 03:14:37 visual_prompt]: 	Training 400/553. train loss: 222.6599,	0.8144 s / batch. (data: 7.95e-03). ETA=10:47:34, max mem: 20.9 GB 
[11/23 03:16:13 visual_prompt]: 	Training 500/553. train loss: 442.3081,	0.8160 s / batch. (data: 2.92e-04). ETA=10:47:31, max mem: 20.9 GB 
[11/23 03:17:03 visual_prompt]: Epoch 14 / 100: avg data time: 1.39e-01, avg batch time: 0.9656, average train loss: 190.7561
[11/23 03:17:58 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3107, average loss: 204.7610
[11/23 03:17:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.46	
[11/23 03:17:58 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/23 03:19:37 visual_prompt]: 	Training 100/553. train loss: 86.5963,	0.8280 s / batch. (data: 1.20e-02). ETA=10:54:54, max mem: 20.9 GB 
[11/23 03:21:11 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8440 s / batch. (data: 3.13e-04). ETA=11:06:11, max mem: 20.9 GB 
[11/23 03:22:50 visual_prompt]: 	Training 300/553. train loss: 107.6311,	0.8480 s / batch. (data: 3.08e-04). ETA=11:07:53, max mem: 20.9 GB 
[11/23 03:24:23 visual_prompt]: 	Training 400/553. train loss: 29.8066,	1.0569 s / batch. (data: 2.46e-01). ETA=13:50:40, max mem: 20.9 GB 
[11/23 03:26:00 visual_prompt]: 	Training 500/553. train loss: 232.2905,	0.8430 s / batch. (data: 1.72e-02). ETA=11:01:07, max mem: 20.9 GB 
[11/23 03:26:51 visual_prompt]: Epoch 15 / 100: avg data time: 1.37e-01, avg batch time: 0.9638, average train loss: 186.5540
[11/23 03:27:45 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3101, average loss: 120.4946
[11/23 03:27:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.99	
[11/23 03:27:45 visual_prompt]: Best epoch 15: best metric: -120.495
[11/23 03:27:45 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/23 03:29:25 visual_prompt]: 	Training 100/553. train loss: 43.8713,	0.8320 s / batch. (data: 7.95e-03). ETA=10:50:24, max mem: 20.9 GB 
[11/23 03:31:00 visual_prompt]: 	Training 200/553. train loss: 317.8667,	0.8313 s / batch. (data: 4.16e-04). ETA=10:48:29, max mem: 20.9 GB 
[11/23 03:32:37 visual_prompt]: 	Training 300/553. train loss: 10.5811,	0.8400 s / batch. (data: 8.51e-03). ETA=10:53:50, max mem: 20.9 GB 
[11/23 03:34:13 visual_prompt]: 	Training 400/553. train loss: 195.0448,	0.8492 s / batch. (data: 1.10e-02). ETA=10:59:39, max mem: 20.9 GB 
[11/23 03:35:48 visual_prompt]: 	Training 500/553. train loss: 188.4037,	1.0202 s / batch. (data: 2.03e-01). ETA=13:10:45, max mem: 20.9 GB 
[11/23 03:36:38 visual_prompt]: Epoch 16 / 100: avg data time: 1.37e-01, avg batch time: 0.9639, average train loss: 190.2302
[11/23 03:37:33 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3090, average loss: 2.1358
[11/23 03:37:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 52.24	
[11/23 03:37:33 visual_prompt]: Best epoch 16: best metric: -2.136
[11/23 03:37:33 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/23 03:39:12 visual_prompt]: 	Training 100/553. train loss: 243.7455,	0.8362 s / batch. (data: 1.59e-02). ETA=10:46:00, max mem: 20.9 GB 
[11/23 03:40:49 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8332 s / batch. (data: 1.55e-02). ETA=10:42:18, max mem: 20.9 GB 
[11/23 03:42:25 visual_prompt]: 	Training 300/553. train loss: 380.4587,	0.8320 s / batch. (data: 7.96e-03). ETA=10:39:57, max mem: 20.9 GB 
[11/23 03:44:01 visual_prompt]: 	Training 400/553. train loss: 251.5679,	1.0081 s / batch. (data: 2.01e-01). ETA=12:53:44, max mem: 20.9 GB 
[11/23 03:45:36 visual_prompt]: 	Training 500/553. train loss: 26.9485,	1.5014 s / batch. (data: 6.60e-01). ETA=19:09:52, max mem: 20.9 GB 
[11/23 03:46:27 visual_prompt]: Epoch 17 / 100: avg data time: 1.39e-01, avg batch time: 0.9661, average train loss: 160.8307
[11/23 03:47:22 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3098, average loss: 67.9313
[11/23 03:47:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.46	
[11/23 03:47:22 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/23 03:49:02 visual_prompt]: 	Training 100/553. train loss: 337.4742,	0.8320 s / batch. (data: 1.20e-02). ETA=10:35:06, max mem: 20.9 GB 
[11/23 03:50:40 visual_prompt]: 	Training 200/553. train loss: 31.8491,	0.8679 s / batch. (data: 1.09e-02). ETA=11:00:59, max mem: 20.9 GB 
[11/23 03:52:16 visual_prompt]: 	Training 300/553. train loss: 227.8670,	0.8400 s / batch. (data: 2.77e-04). ETA=10:38:23, max mem: 20.9 GB 
[11/23 03:53:51 visual_prompt]: 	Training 400/553. train loss: 127.6485,	0.8506 s / batch. (data: 2.87e-04). ETA=10:45:00, max mem: 20.9 GB 
[11/23 03:55:26 visual_prompt]: 	Training 500/553. train loss: 38.8039,	0.8467 s / batch. (data: 1.47e-02). ETA=10:40:40, max mem: 20.9 GB 
[11/23 03:56:15 visual_prompt]: Epoch 18 / 100: avg data time: 1.38e-01, avg batch time: 0.9644, average train loss: 172.3706
[11/23 03:57:10 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3091, average loss: 23.0419
[11/23 03:57:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.30	
[11/23 03:57:10 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/23 03:58:50 visual_prompt]: 	Training 100/553. train loss: 30.3725,	0.9437 s / batch. (data: 1.21e-01). ETA=11:51:40, max mem: 20.9 GB 
[11/23 04:00:26 visual_prompt]: 	Training 200/553. train loss: 76.1996,	0.8400 s / batch. (data: 3.14e-04). ETA=10:32:02, max mem: 20.9 GB 
[11/23 04:02:02 visual_prompt]: 	Training 300/553. train loss: 526.4801,	0.8402 s / batch. (data: 3.33e-04). ETA=10:30:47, max mem: 20.9 GB 
[11/23 04:03:39 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8243 s / batch. (data: 2.93e-04). ETA=10:17:27, max mem: 20.9 GB 
[11/23 04:05:11 visual_prompt]: 	Training 500/553. train loss: 9.3277,	0.8489 s / batch. (data: 1.56e-02). ETA=10:34:29, max mem: 20.9 GB 
[11/23 04:06:01 visual_prompt]: Epoch 19 / 100: avg data time: 1.33e-01, avg batch time: 0.9604, average train loss: 195.7430
[11/23 04:06:56 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3091, average loss: 76.6227
[11/23 04:06:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.08	
[11/23 04:06:56 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/23 04:08:34 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8223 s / batch. (data: 3.19e-04). ETA=10:12:29, max mem: 20.9 GB 
[11/23 04:10:11 visual_prompt]: 	Training 200/553. train loss: 80.9129,	0.8185 s / batch. (data: 3.20e-04). ETA=10:08:20, max mem: 20.9 GB 
[11/23 04:11:48 visual_prompt]: 	Training 300/553. train loss: 117.1381,	0.8146 s / batch. (data: 3.58e-04). ETA=10:04:03, max mem: 20.9 GB 
[11/23 04:13:23 visual_prompt]: 	Training 400/553. train loss: 17.3723,	0.8320 s / batch. (data: 2.88e-04). ETA=10:15:32, max mem: 20.9 GB 
[11/23 04:14:59 visual_prompt]: 	Training 500/553. train loss: 100.9234,	0.8280 s / batch. (data: 3.02e-04). ETA=10:11:14, max mem: 20.9 GB 
[11/23 04:15:50 visual_prompt]: Epoch 20 / 100: avg data time: 1.39e-01, avg batch time: 0.9662, average train loss: 160.8579
[11/23 04:16:45 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3113, average loss: 58.2357
[11/23 04:16:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.21	
[11/23 04:16:45 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/23 04:18:28 visual_prompt]: 	Training 100/553. train loss: 113.3547,	0.8370 s / batch. (data: 5.87e-03). ETA=10:15:46, max mem: 20.9 GB 
[11/23 04:20:02 visual_prompt]: 	Training 200/553. train loss: 94.7154,	0.8474 s / batch. (data: 1.60e-02). ETA=10:21:58, max mem: 20.9 GB 
[11/23 04:21:37 visual_prompt]: 	Training 300/553. train loss: 420.3132,	0.9520 s / batch. (data: 1.19e-01). ETA=11:37:10, max mem: 20.9 GB 
[11/23 04:23:13 visual_prompt]: 	Training 400/553. train loss: 258.5422,	0.8358 s / batch. (data: 2.95e-04). ETA=10:10:42, max mem: 20.9 GB 
[11/23 04:24:50 visual_prompt]: 	Training 500/553. train loss: 24.6469,	0.8160 s / batch. (data: 3.05e-04). ETA=9:54:51, max mem: 20.9 GB 
[11/23 04:25:40 visual_prompt]: Epoch 21 / 100: avg data time: 1.40e-01, avg batch time: 0.9662, average train loss: 152.4561
[11/23 04:26:34 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3099, average loss: 8.9101
[11/23 04:26:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.52	
[11/23 04:26:34 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/23 04:28:13 visual_prompt]: 	Training 100/553. train loss: 76.3283,	0.8199 s / batch. (data: 5.41e-03). ETA=9:55:35, max mem: 20.9 GB 
[11/23 04:29:49 visual_prompt]: 	Training 200/553. train loss: 141.7162,	0.8280 s / batch. (data: 3.04e-04). ETA=10:00:07, max mem: 20.9 GB 
[11/23 04:31:23 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8352 s / batch. (data: 5.41e-03). ETA=10:03:55, max mem: 20.9 GB 
[11/23 04:33:00 visual_prompt]: 	Training 400/553. train loss: 18.4423,	0.8183 s / batch. (data: 4.52e-04). ETA=9:50:20, max mem: 20.9 GB 
[11/23 04:34:37 visual_prompt]: 	Training 500/553. train loss: 34.7550,	0.8172 s / batch. (data: 3.08e-04). ETA=9:48:12, max mem: 20.9 GB 
[11/23 04:35:28 visual_prompt]: Epoch 22 / 100: avg data time: 1.39e-01, avg batch time: 0.9650, average train loss: 170.8602
[11/23 04:36:22 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3095, average loss: 246.9585
[11/23 04:36:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.10	
[11/23 04:36:22 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/23 04:38:03 visual_prompt]: 	Training 100/553. train loss: 186.1240,	0.8288 s / batch. (data: 1.06e-02). ETA=9:54:27, max mem: 20.9 GB 
[11/23 04:39:40 visual_prompt]: 	Training 200/553. train loss: 272.6903,	0.8440 s / batch. (data: 7.96e-03). ETA=10:03:55, max mem: 20.9 GB 
[11/23 04:41:18 visual_prompt]: 	Training 300/553. train loss: 47.7774,	0.8322 s / batch. (data: 3.19e-04). ETA=9:54:04, max mem: 20.9 GB 
[11/23 04:42:52 visual_prompt]: 	Training 400/553. train loss: 123.1810,	0.8418 s / batch. (data: 5.46e-03). ETA=9:59:31, max mem: 20.9 GB 
[11/23 04:44:26 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8040 s / batch. (data: 3.41e-04). ETA=9:31:16, max mem: 20.9 GB 
[11/23 04:45:16 visual_prompt]: Epoch 23 / 100: avg data time: 1.38e-01, avg batch time: 0.9647, average train loss: 160.6898
[11/23 04:46:10 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.3114, average loss: 47.8060
[11/23 04:46:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.12	
[11/23 04:46:11 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/23 04:47:47 visual_prompt]: 	Training 100/553. train loss: 14.8031,	0.8240 s / batch. (data: 3.08e-04). ETA=9:43:23, max mem: 20.9 GB 
[11/23 04:49:23 visual_prompt]: 	Training 200/553. train loss: 150.9551,	0.8214 s / batch. (data: 2.86e-04). ETA=9:40:10, max mem: 20.9 GB 
[11/23 04:50:59 visual_prompt]: 	Training 300/553. train loss: 100.0307,	0.8772 s / batch. (data: 3.87e-02). ETA=10:18:07, max mem: 20.9 GB 
[11/23 04:52:36 visual_prompt]: 	Training 400/553. train loss: 40.2626,	0.8398 s / batch. (data: 3.00e-04). ETA=9:50:22, max mem: 20.9 GB 
[11/23 04:54:14 visual_prompt]: 	Training 500/553. train loss: 169.1389,	0.8155 s / batch. (data: 3.23e-04). ETA=9:31:57, max mem: 20.9 GB 
[11/23 04:55:04 visual_prompt]: Epoch 24 / 100: avg data time: 1.38e-01, avg batch time: 0.9650, average train loss: 156.1355
[11/23 04:55:59 visual_prompt]: Inference (val):avg data time: 2.58e-04, avg batch time: 0.3127, average loss: 125.1586
[11/23 04:55:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.33	
[11/23 04:55:59 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/23 04:57:41 visual_prompt]: 	Training 100/553. train loss: 52.7218,	0.8280 s / batch. (data: 3.60e-04). ETA=9:38:37, max mem: 20.9 GB 
[11/23 04:59:14 visual_prompt]: 	Training 200/553. train loss: 223.0438,	0.8561 s / batch. (data: 2.10e-02). ETA=9:56:49, max mem: 20.9 GB 
[11/23 05:00:50 visual_prompt]: 	Training 300/553. train loss: 104.7876,	0.8486 s / batch. (data: 2.06e-02). ETA=9:50:08, max mem: 20.9 GB 
[11/23 05:02:25 visual_prompt]: 	Training 400/553. train loss: 683.1397,	0.8358 s / batch. (data: 3.37e-04). ETA=9:39:53, max mem: 20.9 GB 
[11/23 05:04:01 visual_prompt]: 	Training 500/553. train loss: 55.3867,	1.0479 s / batch. (data: 2.10e-01). ETA=12:05:15, max mem: 20.9 GB 
[11/23 05:04:52 visual_prompt]: Epoch 25 / 100: avg data time: 1.36e-01, avg batch time: 0.9633, average train loss: 164.1294
[11/23 05:05:46 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3097, average loss: 7.3253
[11/23 05:05:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.97	
[11/23 05:05:46 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/23 05:07:26 visual_prompt]: 	Training 100/553. train loss: 129.7458,	0.8169 s / batch. (data: 2.86e-04). ETA=9:23:18, max mem: 20.9 GB 
[11/23 05:09:03 visual_prompt]: 	Training 200/553. train loss: 1.3798,	1.5436 s / batch. (data: 7.27e-01). ETA=17:41:51, max mem: 20.9 GB 
[11/23 05:10:41 visual_prompt]: 	Training 300/553. train loss: 7.9059,	0.8493 s / batch. (data: 1.09e-02). ETA=9:42:48, max mem: 20.9 GB 
[11/23 05:12:15 visual_prompt]: 	Training 400/553. train loss: 55.3619,	0.8357 s / batch. (data: 3.25e-04). ETA=9:32:05, max mem: 20.9 GB 
[11/23 05:13:50 visual_prompt]: 	Training 500/553. train loss: 29.2409,	0.8405 s / batch. (data: 2.91e-04). ETA=9:33:58, max mem: 20.9 GB 
[11/23 05:14:40 visual_prompt]: Epoch 26 / 100: avg data time: 1.37e-01, avg batch time: 0.9639, average train loss: 145.6865
[11/23 05:15:34 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3104, average loss: 113.1753
[11/23 05:15:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.51	
[11/23 05:15:34 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/23 05:17:15 visual_prompt]: 	Training 100/553. train loss: 7.8318,	0.8306 s / batch. (data: 3.73e-04). ETA=9:25:07, max mem: 20.9 GB 
[11/23 05:18:50 visual_prompt]: 	Training 200/553. train loss: 111.5258,	1.1822 s / batch. (data: 3.57e-01). ETA=13:22:20, max mem: 20.9 GB 
[11/23 05:20:26 visual_prompt]: 	Training 300/553. train loss: 89.1500,	0.8245 s / batch. (data: 7.96e-03). ETA=9:18:11, max mem: 20.9 GB 
[11/23 05:22:03 visual_prompt]: 	Training 400/553. train loss: 120.9062,	0.8202 s / batch. (data: 2.98e-04). ETA=9:13:55, max mem: 20.9 GB 
[11/23 05:23:39 visual_prompt]: 	Training 500/553. train loss: 298.4228,	0.8364 s / batch. (data: 5.41e-03). ETA=9:23:30, max mem: 20.9 GB 
[11/23 05:24:27 visual_prompt]: Epoch 27 / 100: avg data time: 1.37e-01, avg batch time: 0.9640, average train loss: 150.7523
[11/23 05:25:22 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3107, average loss: 162.6716
[11/23 05:25:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.07	
[11/23 05:25:22 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/23 05:27:01 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8078 s / batch. (data: 3.58e-04). ETA=9:02:10, max mem: 20.9 GB 
[11/23 05:28:37 visual_prompt]: 	Training 200/553. train loss: 48.0571,	0.8320 s / batch. (data: 1.20e-02). ETA=9:17:00, max mem: 20.9 GB 
[11/23 05:30:14 visual_prompt]: 	Training 300/553. train loss: 1.7146,	0.8595 s / batch. (data: 1.32e-02). ETA=9:33:59, max mem: 20.9 GB 
[11/23 05:31:49 visual_prompt]: 	Training 400/553. train loss: 7.2133,	0.8310 s / batch. (data: 2.95e-04). ETA=9:13:35, max mem: 20.9 GB 
[11/23 05:33:23 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8197 s / batch. (data: 5.42e-03). ETA=9:04:39, max mem: 20.9 GB 
[11/23 05:34:15 visual_prompt]: Epoch 28 / 100: avg data time: 1.37e-01, avg batch time: 0.9638, average train loss: 158.9682
[11/23 05:35:09 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3117, average loss: 344.8593
[11/23 05:35:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.29	
[11/23 05:35:10 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/23 05:36:55 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8116 s / batch. (data: 3.05e-04). ETA=8:57:13, max mem: 20.9 GB 
[11/23 05:38:30 visual_prompt]: 	Training 200/553. train loss: 63.6507,	1.5899 s / batch. (data: 7.82e-01). ETA=17:29:45, max mem: 20.9 GB 
[11/23 05:40:03 visual_prompt]: 	Training 300/553. train loss: 61.6685,	0.8134 s / batch. (data: 3.02e-04). ETA=8:55:43, max mem: 20.9 GB 
[11/23 05:41:35 visual_prompt]: 	Training 400/553. train loss: 166.5122,	1.2133 s / batch. (data: 3.54e-01). ETA=13:17:04, max mem: 20.9 GB 
[11/23 05:43:11 visual_prompt]: 	Training 500/553. train loss: 145.6787,	0.8321 s / batch. (data: 1.20e-02). ETA=9:05:13, max mem: 20.9 GB 
[11/23 05:44:01 visual_prompt]: Epoch 29 / 100: avg data time: 1.35e-01, avg batch time: 0.9618, average train loss: 138.1748
[11/23 05:44:56 visual_prompt]: Inference (val):avg data time: 3.35e-04, avg batch time: 0.3099, average loss: 183.6311
[11/23 05:44:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.50	
[11/23 05:44:56 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/23 05:46:35 visual_prompt]: 	Training 100/553. train loss: 19.7214,	0.8389 s / batch. (data: 2.82e-04). ETA=9:07:32, max mem: 20.9 GB 
[11/23 05:48:12 visual_prompt]: 	Training 200/553. train loss: 509.7855,	0.8820 s / batch. (data: 5.41e-03). ETA=9:34:14, max mem: 20.9 GB 
[11/23 05:49:46 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8560 s / batch. (data: 7.96e-03). ETA=9:15:52, max mem: 20.9 GB 
[11/23 05:51:23 visual_prompt]: 	Training 400/553. train loss: 28.2085,	1.0739 s / batch. (data: 2.40e-01). ETA=11:35:35, max mem: 20.9 GB 
[11/23 05:52:58 visual_prompt]: 	Training 500/553. train loss: 231.7779,	1.3057 s / batch. (data: 4.77e-01). ETA=14:03:32, max mem: 20.9 GB 
[11/23 05:53:50 visual_prompt]: Epoch 30 / 100: avg data time: 1.39e-01, avg batch time: 0.9649, average train loss: 124.2348
[11/23 05:54:45 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3102, average loss: 68.2561
[11/23 05:54:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.27	
[11/23 05:54:45 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[11/23 05:56:26 visual_prompt]: 	Training 100/553. train loss: 158.6663,	0.8440 s / batch. (data: 3.04e-04). ETA=9:03:08, max mem: 20.9 GB 
[11/23 05:58:04 visual_prompt]: 	Training 200/553. train loss: 74.8282,	0.8235 s / batch. (data: 2.71e-04). ETA=8:48:32, max mem: 20.9 GB 
[11/23 05:59:37 visual_prompt]: 	Training 300/553. train loss: 216.2123,	0.8234 s / batch. (data: 2.82e-04). ETA=8:47:07, max mem: 20.9 GB 
[11/23 06:01:12 visual_prompt]: 	Training 400/553. train loss: 41.8518,	1.0444 s / batch. (data: 2.27e-01). ETA=11:06:50, max mem: 20.9 GB 
[11/23 06:02:47 visual_prompt]: 	Training 500/553. train loss: 106.7536,	0.8120 s / batch. (data: 3.04e-04). ETA=8:37:06, max mem: 20.9 GB 
[11/23 06:03:36 visual_prompt]: Epoch 31 / 100: avg data time: 1.34e-01, avg batch time: 0.9611, average train loss: 147.2976
[11/23 06:04:31 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3102, average loss: 26.1778
[11/23 06:04:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.19	
[11/23 06:04:31 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[11/23 06:06:12 visual_prompt]: 	Training 100/553. train loss: 12.1126,	0.8400 s / batch. (data: 3.07e-04). ETA=8:52:48, max mem: 20.9 GB 
[11/23 06:07:47 visual_prompt]: 	Training 200/553. train loss: 213.4811,	0.8231 s / batch. (data: 7.57e-04). ETA=8:40:40, max mem: 20.9 GB 
[11/23 06:09:26 visual_prompt]: 	Training 300/553. train loss: 59.8548,	0.8320 s / batch. (data: 3.06e-04). ETA=8:44:56, max mem: 20.9 GB 
[11/23 06:11:02 visual_prompt]: 	Training 400/553. train loss: 17.2170,	0.8280 s / batch. (data: 2.97e-04). ETA=8:41:02, max mem: 20.9 GB 
[11/23 06:12:34 visual_prompt]: 	Training 500/553. train loss: 19.9192,	0.8292 s / batch. (data: 5.40e-03). ETA=8:40:25, max mem: 20.9 GB 
[11/23 06:13:22 visual_prompt]: Epoch 32 / 100: avg data time: 1.35e-01, avg batch time: 0.9601, average train loss: 150.7823
[11/23 06:14:16 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3091, average loss: 6.9319
[11/23 06:14:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.54	
[11/23 06:14:16 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[11/23 06:15:54 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8160 s / batch. (data: 2.95e-04). ETA=8:30:03, max mem: 20.9 GB 
[11/23 06:17:30 visual_prompt]: 	Training 200/553. train loss: 62.5337,	0.9015 s / batch. (data: 9.45e-02). ETA=9:21:59, max mem: 20.9 GB 
[11/23 06:19:05 visual_prompt]: 	Training 300/553. train loss: 8.7674,	0.8188 s / batch. (data: 2.74e-04). ETA=8:29:04, max mem: 20.9 GB 
[11/23 06:20:40 visual_prompt]: 	Training 400/553. train loss: 132.0480,	0.8239 s / batch. (data: 1.05e-02). ETA=8:30:52, max mem: 20.9 GB 
[11/23 06:22:15 visual_prompt]: 	Training 500/553. train loss: 26.6511,	0.8478 s / batch. (data: 1.05e-02). ETA=8:44:16, max mem: 20.9 GB 
[11/23 06:23:04 visual_prompt]: Epoch 33 / 100: avg data time: 1.27e-01, avg batch time: 0.9534, average train loss: 159.6651
[11/23 06:23:58 visual_prompt]: Inference (val):avg data time: 4.97e-04, avg batch time: 0.3116, average loss: 174.1229
[11/23 06:23:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.58	
[11/23 06:23:58 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[11/23 06:25:38 visual_prompt]: 	Training 100/553. train loss: 52.6122,	0.8160 s / batch. (data: 3.28e-04). ETA=8:22:31, max mem: 20.9 GB 
[11/23 06:27:10 visual_prompt]: 	Training 200/553. train loss: 68.4719,	0.8242 s / batch. (data: 1.23e-02). ETA=8:26:11, max mem: 20.9 GB 
[11/23 06:28:46 visual_prompt]: 	Training 300/553. train loss: 172.0752,	0.8107 s / batch. (data: 3.23e-04). ETA=8:16:34, max mem: 20.9 GB 
[11/23 06:30:22 visual_prompt]: 	Training 400/553. train loss: 610.7329,	0.8400 s / batch. (data: 2.80e-04). ETA=8:33:07, max mem: 20.9 GB 
[11/23 06:31:57 visual_prompt]: 	Training 500/553. train loss: 380.0856,	1.4240 s / batch. (data: 5.59e-01). ETA=14:27:27, max mem: 20.9 GB 
[11/23 06:32:47 visual_prompt]: Epoch 34 / 100: avg data time: 1.29e-01, avg batch time: 0.9563, average train loss: 164.0386
[11/23 06:33:41 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3112, average loss: 78.3900
[11/23 06:33:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 55.08	
[11/23 06:33:41 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[11/23 06:35:23 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8099 s / batch. (data: 8.33e-03). ETA=8:11:17, max mem: 20.9 GB 
[11/23 06:37:00 visual_prompt]: 	Training 200/553. train loss: 61.8306,	0.8200 s / batch. (data: 3.26e-04). ETA=8:16:03, max mem: 20.9 GB 
[11/23 06:38:34 visual_prompt]: 	Training 300/553. train loss: 18.9457,	0.8345 s / batch. (data: 2.90e-04). ETA=8:23:26, max mem: 20.9 GB 
[11/23 06:40:09 visual_prompt]: 	Training 400/553. train loss: 100.3453,	0.8320 s / batch. (data: 3.14e-04). ETA=8:20:33, max mem: 20.9 GB 
[11/23 06:41:44 visual_prompt]: 	Training 500/553. train loss: 194.8438,	0.8522 s / batch. (data: 8.18e-03). ETA=8:31:17, max mem: 20.9 GB 
[11/23 06:42:35 visual_prompt]: Epoch 35 / 100: avg data time: 1.39e-01, avg batch time: 0.9656, average train loss: 160.5242
[11/23 06:43:30 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.3113, average loss: 12.3208
[11/23 06:43:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.55	
[11/23 06:43:30 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[11/23 06:45:09 visual_prompt]: 	Training 100/553. train loss: 15.6176,	0.8274 s / batch. (data: 3.09e-04). ETA=8:14:19, max mem: 20.9 GB 
[11/23 06:46:46 visual_prompt]: 	Training 200/553. train loss: 491.0982,	0.8246 s / batch. (data: 3.32e-04). ETA=8:11:16, max mem: 20.9 GB 
[11/23 06:48:23 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8522 s / batch. (data: 2.42e-02). ETA=8:26:17, max mem: 20.9 GB 
[11/23 06:49:59 visual_prompt]: 	Training 400/553. train loss: 8.7644,	0.8205 s / batch. (data: 2.79e-04). ETA=8:06:03, max mem: 20.9 GB 
[11/23 06:51:35 visual_prompt]: 	Training 500/553. train loss: 37.5856,	0.8723 s / batch. (data: 5.38e-02). ETA=8:35:18, max mem: 20.9 GB 
[11/23 06:52:22 visual_prompt]: Epoch 36 / 100: avg data time: 1.36e-01, avg batch time: 0.9628, average train loss: 152.7814
[11/23 06:53:17 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3111, average loss: 30.2378
[11/23 06:53:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.42	
[11/23 06:53:17 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[11/23 06:54:57 visual_prompt]: 	Training 100/553. train loss: 75.8718,	0.8189 s / batch. (data: 2.94e-04). ETA=8:01:41, max mem: 20.9 GB 
[11/23 06:56:32 visual_prompt]: 	Training 200/553. train loss: 103.2251,	0.8370 s / batch. (data: 2.78e-04). ETA=8:10:54, max mem: 20.9 GB 
[11/23 06:58:08 visual_prompt]: 	Training 300/553. train loss: 166.5088,	1.1331 s / batch. (data: 3.16e-01). ETA=11:02:43, max mem: 20.9 GB 
[11/23 06:59:47 visual_prompt]: 	Training 400/553. train loss: 6.2100,	1.8117 s / batch. (data: 9.85e-01). ETA=17:36:33, max mem: 20.9 GB 
[11/23 07:01:19 visual_prompt]: 	Training 500/553. train loss: 125.8673,	0.8356 s / batch. (data: 6.47e-03). ETA=8:05:54, max mem: 20.9 GB 
[11/23 07:02:11 visual_prompt]: Epoch 37 / 100: avg data time: 1.38e-01, avg batch time: 0.9651, average train loss: 124.8053
[11/23 07:03:05 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3097, average loss: 122.9183
[11/23 07:03:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.28	
[11/23 07:03:05 visual_prompt]: Stopping early.
[11/23 07:03:05 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 07:03:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 07:03:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/23 07:03:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 07:03:05 visual_prompt]: Training with config:
[11/23 07:03:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/23 07:03:05 visual_prompt]: Loading training data...
[11/23 07:03:05 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 07:03:05 visual_prompt]: Loading validation data...
[11/23 07:03:05 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 07:03:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 07:03:10 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 07:03:10 visual_prompt]: tuned percent:0.525
[11/23 07:03:10 visual_prompt]: Device used for model: 0
[11/23 07:03:10 visual_prompt]: Setting up Evaluator...
[11/23 07:03:10 visual_prompt]: Setting up Trainer...
[11/23 07:03:10 visual_prompt]: 	Setting up the optimizer...
[11/23 07:03:10 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 07:04:50 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8359 s / batch. (data: 4.60e-04). ETA=12:49:03, max mem: 20.9 GB 
[11/23 07:06:24 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8760 s / batch. (data: 2.86e-04). ETA=13:24:28, max mem: 20.9 GB 
[11/23 07:08:02 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8550 s / batch. (data: 3.69e-02). ETA=13:03:42, max mem: 20.9 GB 
[11/23 07:09:37 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8254 s / batch. (data: 3.05e-04). ETA=12:35:14, max mem: 20.9 GB 
[11/23 07:11:15 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8720 s / batch. (data: 1.19e-02). ETA=13:16:28, max mem: 20.9 GB 
[11/23 07:12:06 visual_prompt]: Epoch 1 / 100: avg data time: 1.34e-01, avg batch time: 0.9680, average train loss: 1.5403
[11/23 07:13:00 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3116, average loss: 1.5201
[11/23 07:13:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 07:13:00 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/23 07:14:39 visual_prompt]: 	Training 100/553. train loss: 10.6743,	0.8360 s / batch. (data: 3.24e-04). ETA=12:41:24, max mem: 20.9 GB 
[11/23 07:16:15 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8272 s / batch. (data: 1.12e-02). ETA=12:32:03, max mem: 20.9 GB 
[11/23 07:17:53 visual_prompt]: 	Training 300/553. train loss: 0.8119,	1.0164 s / batch. (data: 1.79e-01). ETA=15:22:22, max mem: 20.9 GB 
[11/23 07:19:28 visual_prompt]: 	Training 400/553. train loss: 11.9164,	0.8440 s / batch. (data: 7.97e-03). ETA=12:44:28, max mem: 20.9 GB 
[11/23 07:21:05 visual_prompt]: 	Training 500/553. train loss: 8.2818,	0.8200 s / batch. (data: 2.98e-04). ETA=12:21:23, max mem: 20.9 GB 
[11/23 07:21:55 visual_prompt]: Epoch 2 / 100: avg data time: 1.34e-01, avg batch time: 0.9662, average train loss: 15.2291
[11/23 07:22:49 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3093, average loss: 15.8872
[11/23 07:22:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.27	
[11/23 07:22:49 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/23 07:24:28 visual_prompt]: 	Training 100/553. train loss: 37.7481,	0.8203 s / batch. (data: 5.41e-03). ETA=12:19:31, max mem: 20.9 GB 
[11/23 07:26:04 visual_prompt]: 	Training 200/553. train loss: 27.9482,	1.0840 s / batch. (data: 2.41e-01). ETA=16:15:31, max mem: 20.9 GB 
[11/23 07:27:40 visual_prompt]: 	Training 300/553. train loss: 10.3654,	0.8507 s / batch. (data: 2.95e-04). ETA=12:44:08, max mem: 20.9 GB 
[11/23 07:29:16 visual_prompt]: 	Training 400/553. train loss: 96.2913,	0.8327 s / batch. (data: 7.96e-03). ETA=12:26:33, max mem: 20.9 GB 
[11/23 07:30:53 visual_prompt]: 	Training 500/553. train loss: 67.0479,	1.0800 s / batch. (data: 2.54e-01). ETA=16:06:30, max mem: 20.9 GB 
[11/23 07:31:42 visual_prompt]: Epoch 3 / 100: avg data time: 1.33e-01, avg batch time: 0.9638, average train loss: 40.2478
[11/23 07:32:37 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3103, average loss: 14.2773
[11/23 07:32:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.73	
[11/23 07:32:37 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/23 07:34:18 visual_prompt]: 	Training 100/553. train loss: 16.8942,	0.8600 s / batch. (data: 3.05e-04). ETA=12:47:25, max mem: 20.9 GB 
[11/23 07:35:54 visual_prompt]: 	Training 200/553. train loss: 4.3071,	0.8400 s / batch. (data: 1.19e-02). ETA=12:28:09, max mem: 20.9 GB 
[11/23 07:37:31 visual_prompt]: 	Training 300/553. train loss: 86.3309,	1.1198 s / batch. (data: 3.01e-01). ETA=16:35:33, max mem: 20.9 GB 
[11/23 07:39:03 visual_prompt]: 	Training 400/553. train loss: 79.9603,	0.9092 s / batch. (data: 9.56e-02). ETA=13:26:45, max mem: 20.9 GB 
[11/23 07:40:41 visual_prompt]: 	Training 500/553. train loss: 0.0007,	3.3659 s / batch. (data: 2.55e+00). ETA=2 days, 1:41:07, max mem: 20.9 GB 
[11/23 07:41:32 visual_prompt]: Epoch 4 / 100: avg data time: 1.39e-01, avg batch time: 0.9677, average train loss: 63.0865
[11/23 07:42:27 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3106, average loss: 14.3394
[11/23 07:42:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.73	
[11/23 07:42:27 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/23 07:44:04 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8360 s / batch. (data: 3.08e-04). ETA=12:18:18, max mem: 20.9 GB 
[11/23 07:45:41 visual_prompt]: 	Training 200/553. train loss: 38.8275,	1.0153 s / batch. (data: 1.92e-01). ETA=14:54:58, max mem: 20.9 GB 
[11/23 07:47:18 visual_prompt]: 	Training 300/553. train loss: 151.9556,	0.8240 s / batch. (data: 5.37e-03). ETA=12:04:57, max mem: 20.9 GB 
[11/23 07:48:53 visual_prompt]: 	Training 400/553. train loss: 228.9831,	0.8320 s / batch. (data: 2.81e-04). ETA=12:10:35, max mem: 20.9 GB 
[11/23 07:50:30 visual_prompt]: 	Training 500/553. train loss: 162.6530,	0.8236 s / batch. (data: 7.96e-03). ETA=12:01:50, max mem: 20.9 GB 
[11/23 07:51:21 visual_prompt]: Epoch 5 / 100: avg data time: 1.36e-01, avg batch time: 0.9655, average train loss: 59.6810
[11/23 07:52:15 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3104, average loss: 53.9044
[11/23 07:52:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.30	
[11/23 07:52:15 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/23 07:53:57 visual_prompt]: 	Training 100/553. train loss: 39.3588,	0.8400 s / batch. (data: 7.68e-04). ETA=12:14:06, max mem: 20.9 GB 
[11/23 07:55:32 visual_prompt]: 	Training 200/553. train loss: 64.8807,	0.8320 s / batch. (data: 2.81e-04). ETA=12:05:43, max mem: 20.9 GB 
[11/23 07:57:07 visual_prompt]: 	Training 300/553. train loss: 19.3553,	0.8280 s / batch. (data: 3.62e-04). ETA=12:00:48, max mem: 20.9 GB 
[11/23 07:58:46 visual_prompt]: 	Training 400/553. train loss: 41.5402,	0.8278 s / batch. (data: 5.42e-03). ETA=11:59:19, max mem: 20.9 GB 
[11/23 08:00:21 visual_prompt]: 	Training 500/553. train loss: 67.3078,	0.8112 s / batch. (data: 3.15e-04). ETA=11:43:30, max mem: 20.9 GB 
[11/23 08:01:11 visual_prompt]: Epoch 6 / 100: avg data time: 1.39e-01, avg batch time: 0.9681, average train loss: 76.6691
[11/23 08:02:05 visual_prompt]: Inference (val):avg data time: 3.14e-04, avg batch time: 0.3110, average loss: 44.6269
[11/23 08:02:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.56	
[11/23 08:02:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/23 08:03:43 visual_prompt]: 	Training 100/553. train loss: 801.0915,	0.8322 s / batch. (data: 8.11e-03). ETA=11:59:36, max mem: 20.9 GB 
[11/23 08:05:20 visual_prompt]: 	Training 200/553. train loss: 8.7732,	0.8243 s / batch. (data: 1.47e-03). ETA=11:51:21, max mem: 20.9 GB 
[11/23 08:06:59 visual_prompt]: 	Training 300/553. train loss: 77.1621,	1.7385 s / batch. (data: 9.26e-01). ETA=1 day, 0:57:27, max mem: 20.9 GB 
[11/23 08:08:35 visual_prompt]: 	Training 400/553. train loss: 87.9645,	1.7440 s / batch. (data: 9.25e-01). ETA=1 day, 0:59:18, max mem: 20.9 GB 
[11/23 08:10:10 visual_prompt]: 	Training 500/553. train loss: 22.3223,	0.8598 s / batch. (data: 7.81e-03). ETA=12:17:46, max mem: 20.9 GB 
[11/23 08:10:58 visual_prompt]: Epoch 7 / 100: avg data time: 1.36e-01, avg batch time: 0.9632, average train loss: 118.3119
[11/23 08:11:53 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3103, average loss: 200.2819
[11/23 08:11:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.85	
[11/23 08:11:53 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/23 08:13:30 visual_prompt]: 	Training 100/553. train loss: 38.9741,	0.8318 s / batch. (data: 3.09e-04). ETA=11:51:36, max mem: 20.9 GB 
[11/23 08:15:08 visual_prompt]: 	Training 200/553. train loss: 10.0451,	0.8238 s / batch. (data: 3.38e-04). ETA=11:43:22, max mem: 20.9 GB 
[11/23 08:16:44 visual_prompt]: 	Training 300/553. train loss: 94.3741,	0.8314 s / batch. (data: 2.87e-04). ETA=11:48:28, max mem: 20.9 GB 
[11/23 08:18:20 visual_prompt]: 	Training 400/553. train loss: 1.2774,	0.8280 s / batch. (data: 3.02e-04). ETA=11:44:11, max mem: 20.9 GB 
[11/23 08:19:56 visual_prompt]: 	Training 500/553. train loss: 130.4558,	1.3377 s / batch. (data: 5.12e-01). ETA=18:55:28, max mem: 20.9 GB 
[11/23 08:20:46 visual_prompt]: Epoch 8 / 100: avg data time: 1.36e-01, avg batch time: 0.9638, average train loss: 105.7661
[11/23 08:21:40 visual_prompt]: Inference (val):avg data time: 3.70e-04, avg batch time: 0.3112, average loss: 36.9941
[11/23 08:21:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.93	
[11/23 08:21:40 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/23 08:23:18 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8323 s / batch. (data: 7.41e-04). ETA=11:44:18, max mem: 20.9 GB 
[11/23 08:24:54 visual_prompt]: 	Training 200/553. train loss: 0.3531,	0.8472 s / batch. (data: 1.11e-02). ETA=11:55:32, max mem: 20.9 GB 
[11/23 08:26:30 visual_prompt]: 	Training 300/553. train loss: 52.0310,	1.3080 s / batch. (data: 4.94e-01). ETA=18:22:32, max mem: 20.9 GB 
[11/23 08:28:07 visual_prompt]: 	Training 400/553. train loss: 45.0630,	0.8360 s / batch. (data: 1.20e-02). ETA=11:43:18, max mem: 20.9 GB 
[11/23 08:29:44 visual_prompt]: 	Training 500/553. train loss: 277.6619,	0.8510 s / batch. (data: 1.10e-02). ETA=11:54:30, max mem: 20.9 GB 
[11/23 08:30:33 visual_prompt]: Epoch 9 / 100: avg data time: 1.38e-01, avg batch time: 0.9644, average train loss: 126.2693
[11/23 08:31:28 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.3125, average loss: 152.5892
[11/23 08:31:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.18	
[11/23 08:31:28 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/23 08:33:09 visual_prompt]: 	Training 100/553. train loss: 187.3826,	0.8200 s / batch. (data: 4.54e-04). ETA=11:26:21, max mem: 20.9 GB 
[11/23 08:34:44 visual_prompt]: 	Training 200/553. train loss: 37.7419,	0.8080 s / batch. (data: 3.13e-04). ETA=11:14:58, max mem: 20.9 GB 
[11/23 08:36:19 visual_prompt]: 	Training 300/553. train loss: 90.5083,	0.8266 s / batch. (data: 3.11e-04). ETA=11:29:10, max mem: 20.9 GB 
[11/23 08:37:53 visual_prompt]: 	Training 400/553. train loss: 265.5869,	0.8280 s / batch. (data: 2.88e-04). ETA=11:28:56, max mem: 20.9 GB 
[11/23 08:39:30 visual_prompt]: 	Training 500/553. train loss: 52.3765,	0.8399 s / batch. (data: 1.05e-02). ETA=11:37:27, max mem: 20.9 GB 
[11/23 08:40:21 visual_prompt]: Epoch 10 / 100: avg data time: 1.37e-01, avg batch time: 0.9642, average train loss: 146.8459
[11/23 08:41:16 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3100, average loss: 202.4706
[11/23 08:41:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.96	
[11/23 08:41:16 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/23 08:42:57 visual_prompt]: 	Training 100/553. train loss: 170.1281,	0.8349 s / batch. (data: 3.01e-04). ETA=11:31:10, max mem: 20.9 GB 
[11/23 08:44:34 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8176 s / batch. (data: 5.40e-03). ETA=11:15:30, max mem: 20.9 GB 
[11/23 08:46:10 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.0155 s / batch. (data: 1.16e+00). ETA=1 day, 3:41:47, max mem: 20.9 GB 
[11/23 08:47:44 visual_prompt]: 	Training 400/553. train loss: 144.2417,	0.8201 s / batch. (data: 2.87e-04). ETA=11:14:48, max mem: 20.9 GB 
[11/23 08:49:19 visual_prompt]: 	Training 500/553. train loss: 227.5852,	0.8320 s / batch. (data: 2.77e-04). ETA=11:23:14, max mem: 20.9 GB 
[11/23 08:50:08 visual_prompt]: Epoch 11 / 100: avg data time: 1.36e-01, avg batch time: 0.9631, average train loss: 167.5750
[11/23 08:51:03 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3099, average loss: 24.0446
[11/23 08:51:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.40	
[11/23 08:51:03 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/23 08:52:44 visual_prompt]: 	Training 100/553. train loss: 120.9846,	0.8224 s / batch. (data: 2.87e-04). ETA=11:13:13, max mem: 20.9 GB 
[11/23 08:54:21 visual_prompt]: 	Training 200/553. train loss: 51.3779,	1.1440 s / batch. (data: 3.22e-01). ETA=15:34:34, max mem: 20.9 GB 
[11/23 08:55:55 visual_prompt]: 	Training 300/553. train loss: 18.5274,	0.8640 s / batch. (data: 7.95e-03). ETA=11:44:23, max mem: 20.9 GB 
[11/23 08:57:32 visual_prompt]: 	Training 400/553. train loss: 151.8916,	0.8280 s / batch. (data: 3.12e-04). ETA=11:13:39, max mem: 20.9 GB 
[11/23 08:59:07 visual_prompt]: 	Training 500/553. train loss: 212.2830,	0.8394 s / batch. (data: 5.41e-03). ETA=11:21:34, max mem: 20.9 GB 
[11/23 08:59:57 visual_prompt]: Epoch 12 / 100: avg data time: 1.37e-01, avg batch time: 0.9653, average train loss: 174.7716
[11/23 09:00:51 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3115, average loss: 324.7250
[11/23 09:00:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.69	
[11/23 09:00:51 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/23 09:02:33 visual_prompt]: 	Training 100/553. train loss: 46.6780,	0.8428 s / batch. (data: 7.99e-03). ETA=11:22:08, max mem: 20.9 GB 
[11/23 09:04:06 visual_prompt]: 	Training 200/553. train loss: 108.3145,	0.8105 s / batch. (data: 2.99e-04). ETA=10:54:38, max mem: 20.9 GB 
[11/23 09:05:43 visual_prompt]: 	Training 300/553. train loss: 56.2402,	1.5774 s / batch. (data: 7.48e-01). ETA=21:11:31, max mem: 20.9 GB 
[11/23 09:07:17 visual_prompt]: 	Training 400/553. train loss: 401.1780,	0.8328 s / batch. (data: 3.50e-04). ETA=11:09:54, max mem: 20.9 GB 
[11/23 09:08:54 visual_prompt]: 	Training 500/553. train loss: 278.3896,	0.8426 s / batch. (data: 2.69e-04). ETA=11:16:24, max mem: 20.9 GB 
[11/23 09:09:45 visual_prompt]: Epoch 13 / 100: avg data time: 1.38e-01, avg batch time: 0.9639, average train loss: 174.0920
[11/23 09:10:39 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3109, average loss: 44.2031
[11/23 09:10:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 50.94	
[11/23 09:10:39 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/23 09:12:20 visual_prompt]: 	Training 100/553. train loss: 1101.5603,	0.8292 s / batch. (data: 2.91e-04). ETA=11:03:31, max mem: 20.9 GB 
[11/23 09:13:55 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8252 s / batch. (data: 1.29e-02). ETA=10:58:56, max mem: 20.9 GB 
[11/23 09:15:32 visual_prompt]: 	Training 300/553. train loss: 54.3812,	0.8160 s / batch. (data: 2.83e-04). ETA=10:50:15, max mem: 20.9 GB 
[11/23 09:17:07 visual_prompt]: 	Training 400/553. train loss: 266.3464,	0.8360 s / batch. (data: 3.16e-04). ETA=11:04:45, max mem: 20.9 GB 
[11/23 09:18:43 visual_prompt]: 	Training 500/553. train loss: 163.6314,	0.8150 s / batch. (data: 3.02e-04). ETA=10:46:42, max mem: 20.9 GB 
[11/23 09:19:33 visual_prompt]: Epoch 14 / 100: avg data time: 1.39e-01, avg batch time: 0.9654, average train loss: 158.8360
[11/23 09:20:28 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3096, average loss: 98.8413
[11/23 09:20:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.20	
[11/23 09:20:28 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/23 09:22:07 visual_prompt]: 	Training 100/553. train loss: 106.0826,	0.8440 s / batch. (data: 3.07e-04). ETA=11:07:35, max mem: 20.9 GB 
[11/23 09:23:42 visual_prompt]: 	Training 200/553. train loss: 797.1801,	0.8247 s / batch. (data: 2.95e-04). ETA=10:50:55, max mem: 20.9 GB 
[11/23 09:25:20 visual_prompt]: 	Training 300/553. train loss: 390.2479,	0.8745 s / batch. (data: 1.03e-02). ETA=11:28:48, max mem: 20.9 GB 
[11/23 09:26:54 visual_prompt]: 	Training 400/553. train loss: 66.4679,	1.0240 s / batch. (data: 1.89e-01). ETA=13:24:48, max mem: 20.9 GB 
[11/23 09:28:30 visual_prompt]: 	Training 500/553. train loss: 56.4750,	0.8400 s / batch. (data: 2.97e-04). ETA=10:58:49, max mem: 20.9 GB 
[11/23 09:29:22 visual_prompt]: Epoch 15 / 100: avg data time: 1.38e-01, avg batch time: 0.9653, average train loss: 165.2661
[11/23 09:30:16 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3098, average loss: 518.4259
[11/23 09:30:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.15	
[11/23 09:30:16 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/23 09:31:55 visual_prompt]: 	Training 100/553. train loss: 103.7124,	0.8360 s / batch. (data: 3.00e-04). ETA=10:53:33, max mem: 20.9 GB 
[11/23 09:33:31 visual_prompt]: 	Training 200/553. train loss: 263.4484,	0.8521 s / batch. (data: 8.94e-03). ETA=11:04:41, max mem: 20.9 GB 
[11/23 09:35:08 visual_prompt]: 	Training 300/553. train loss: 192.0815,	0.8283 s / batch. (data: 1.01e-03). ETA=10:44:44, max mem: 20.9 GB 
[11/23 09:36:43 visual_prompt]: 	Training 400/553. train loss: 154.5278,	0.8498 s / batch. (data: 2.76e-04). ETA=11:00:02, max mem: 20.9 GB 
[11/23 09:38:18 visual_prompt]: 	Training 500/553. train loss: 77.0727,	1.0193 s / batch. (data: 2.01e-01). ETA=13:10:01, max mem: 20.9 GB 
[11/23 09:39:09 visual_prompt]: Epoch 16 / 100: avg data time: 1.37e-01, avg batch time: 0.9633, average train loss: 160.8979
[11/23 09:40:04 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3099, average loss: 42.0164
[11/23 09:40:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 51.17	
[11/23 09:40:04 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/23 09:41:43 visual_prompt]: 	Training 100/553. train loss: 53.9767,	0.8560 s / batch. (data: 7.97e-03). ETA=11:01:17, max mem: 20.9 GB 
[11/23 09:43:20 visual_prompt]: 	Training 200/553. train loss: 253.7905,	0.8360 s / batch. (data: 7.96e-03). ETA=10:44:26, max mem: 20.9 GB 
[11/23 09:44:56 visual_prompt]: 	Training 300/553. train loss: 176.7908,	0.8551 s / batch. (data: 2.96e-04). ETA=10:57:43, max mem: 20.9 GB 
[11/23 09:46:31 visual_prompt]: 	Training 400/553. train loss: 53.5881,	0.8242 s / batch. (data: 1.66e-02). ETA=10:32:36, max mem: 20.9 GB 
[11/23 09:48:06 visual_prompt]: 	Training 500/553. train loss: 101.5773,	1.0971 s / batch. (data: 2.74e-01). ETA=14:00:15, max mem: 20.9 GB 
[11/23 09:48:58 visual_prompt]: Epoch 17 / 100: avg data time: 1.38e-01, avg batch time: 0.9657, average train loss: 177.7189
[11/23 09:49:53 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3111, average loss: 180.0793
[11/23 09:49:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.05	
[11/23 09:49:53 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/23 09:51:33 visual_prompt]: 	Training 100/553. train loss: 42.3787,	0.8400 s / batch. (data: 2.93e-04). ETA=10:41:10, max mem: 20.9 GB 
[11/23 09:53:11 visual_prompt]: 	Training 200/553. train loss: 41.7151,	0.8360 s / batch. (data: 2.93e-04). ETA=10:36:44, max mem: 20.9 GB 
[11/23 09:54:47 visual_prompt]: 	Training 300/553. train loss: 76.5457,	0.8157 s / batch. (data: 3.09e-04). ETA=10:19:55, max mem: 20.9 GB 
[11/23 09:56:22 visual_prompt]: 	Training 400/553. train loss: 110.9185,	0.8106 s / batch. (data: 2.90e-04). ETA=10:14:40, max mem: 20.9 GB 
[11/23 09:57:57 visual_prompt]: 	Training 500/553. train loss: 114.4731,	0.8108 s / batch. (data: 2.75e-04). ETA=10:13:29, max mem: 20.9 GB 
[11/23 09:58:46 visual_prompt]: Epoch 18 / 100: avg data time: 1.37e-01, avg batch time: 0.9647, average train loss: 154.6931
[11/23 09:59:41 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3091, average loss: 218.2068
[11/23 09:59:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.01	
[11/23 09:59:41 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/23 10:01:22 visual_prompt]: 	Training 100/553. train loss: 100.4358,	0.8178 s / batch. (data: 3.82e-04). ETA=10:16:42, max mem: 20.9 GB 
[11/23 10:02:59 visual_prompt]: 	Training 200/553. train loss: 326.8077,	0.8345 s / batch. (data: 2.97e-04). ETA=10:27:54, max mem: 20.9 GB 
[11/23 10:04:37 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8289 s / batch. (data: 5.08e-04). ETA=10:22:18, max mem: 20.9 GB 
[11/23 10:06:16 visual_prompt]: 	Training 400/553. train loss: 211.2019,	0.8161 s / batch. (data: 8.17e-04). ETA=10:11:22, max mem: 20.9 GB 
[11/23 10:07:48 visual_prompt]: 	Training 500/553. train loss: 568.7255,	0.8237 s / batch. (data: 3.23e-04). ETA=10:15:38, max mem: 20.9 GB 
[11/23 10:08:39 visual_prompt]: Epoch 19 / 100: avg data time: 1.46e-01, avg batch time: 0.9729, average train loss: 181.0746
[11/23 10:09:34 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3104, average loss: 410.1010
[11/23 10:09:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 36.76	
[11/23 10:09:34 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/23 10:11:12 visual_prompt]: 	Training 100/553. train loss: 10.1556,	0.8734 s / batch. (data: 2.14e-02). ETA=10:50:33, max mem: 20.9 GB 
[11/23 10:12:50 visual_prompt]: 	Training 200/553. train loss: 81.5022,	0.8815 s / batch. (data: 2.54e-02). ETA=10:55:06, max mem: 20.9 GB 
[11/23 10:14:26 visual_prompt]: 	Training 300/553. train loss: 175.6327,	0.8247 s / batch. (data: 5.40e-03). ETA=10:11:33, max mem: 20.9 GB 
[11/23 10:16:02 visual_prompt]: 	Training 400/553. train loss: 416.4282,	0.8118 s / batch. (data: 2.97e-04). ETA=10:00:39, max mem: 20.9 GB 
[11/23 10:17:37 visual_prompt]: 	Training 500/553. train loss: 294.9010,	0.8480 s / batch. (data: 2.97e-04). ETA=10:25:59, max mem: 20.9 GB 
[11/23 10:18:29 visual_prompt]: Epoch 20 / 100: avg data time: 1.40e-01, avg batch time: 0.9663, average train loss: 159.3108
[11/23 10:19:24 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3124, average loss: 33.0233
[11/23 10:19:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.73	
[11/23 10:19:24 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/23 10:21:05 visual_prompt]: 	Training 100/553. train loss: 51.4833,	0.8240 s / batch. (data: 3.01e-04). ETA=10:06:11, max mem: 20.9 GB 
[11/23 10:22:41 visual_prompt]: 	Training 200/553. train loss: 244.3890,	0.8336 s / batch. (data: 5.39e-03). ETA=10:11:51, max mem: 20.9 GB 
[11/23 10:24:18 visual_prompt]: 	Training 300/553. train loss: 32.4769,	0.9379 s / batch. (data: 1.31e-01). ETA=11:26:51, max mem: 20.9 GB 
[11/23 10:25:54 visual_prompt]: 	Training 400/553. train loss: 14.3152,	0.8360 s / batch. (data: 3.15e-04). ETA=10:10:51, max mem: 20.9 GB 
[11/23 10:27:34 visual_prompt]: 	Training 500/553. train loss: 38.1298,	0.8505 s / batch. (data: 2.23e-02). ETA=10:20:00, max mem: 20.9 GB 
[11/23 10:28:24 visual_prompt]: Epoch 21 / 100: avg data time: 1.50e-01, avg batch time: 0.9763, average train loss: 169.9309
[11/23 10:29:19 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3104, average loss: 123.6120
[11/23 10:29:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.34	
[11/23 10:29:19 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/23 10:30:57 visual_prompt]: 	Training 100/553. train loss: 72.6496,	0.8360 s / batch. (data: 1.05e-02). ETA=10:07:17, max mem: 20.9 GB 
[11/23 10:32:34 visual_prompt]: 	Training 200/553. train loss: 85.5240,	0.8120 s / batch. (data: 2.92e-04). ETA=9:48:31, max mem: 20.9 GB 
[11/23 10:34:08 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8309 s / batch. (data: 3.07e-04). ETA=10:00:48, max mem: 20.9 GB 
[11/23 10:35:46 visual_prompt]: 	Training 400/553. train loss: 131.4046,	0.8580 s / batch. (data: 2.21e-02). ETA=10:18:58, max mem: 20.9 GB 
[11/23 10:37:22 visual_prompt]: 	Training 500/553. train loss: 29.0696,	0.8360 s / batch. (data: 3.18e-04). ETA=10:01:44, max mem: 20.9 GB 
[11/23 10:38:14 visual_prompt]: Epoch 22 / 100: avg data time: 1.41e-01, avg batch time: 0.9675, average train loss: 160.6525
[11/23 10:39:09 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3104, average loss: 23.9632
[11/23 10:39:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.20	
[11/23 10:39:09 visual_prompt]: Best epoch 22: best metric: -23.963
[11/23 10:39:09 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/23 10:40:51 visual_prompt]: 	Training 100/553. train loss: 80.6201,	0.8450 s / batch. (data: 8.98e-03). ETA=10:06:04, max mem: 20.9 GB 
[11/23 10:42:29 visual_prompt]: 	Training 200/553. train loss: 155.8318,	0.8209 s / batch. (data: 3.15e-04). ETA=9:47:23, max mem: 20.9 GB 
[11/23 10:44:07 visual_prompt]: 	Training 300/553. train loss: 79.0297,	0.8285 s / batch. (data: 1.05e-02). ETA=9:51:29, max mem: 20.9 GB 
[11/23 10:45:41 visual_prompt]: 	Training 400/553. train loss: 192.9761,	0.8480 s / batch. (data: 7.61e-04). ETA=10:03:57, max mem: 20.9 GB 
[11/23 10:47:15 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8155 s / batch. (data: 3.26e-04). ETA=9:39:29, max mem: 20.9 GB 
[11/23 10:48:06 visual_prompt]: Epoch 23 / 100: avg data time: 1.43e-01, avg batch time: 0.9701, average train loss: 154.6760
[11/23 10:49:01 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3099, average loss: 340.3469
[11/23 10:49:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.29	
[11/23 10:49:01 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/23 10:50:39 visual_prompt]: 	Training 100/553. train loss: 2.7256,	0.8897 s / batch. (data: 6.53e-02). ETA=10:29:55, max mem: 20.9 GB 
[11/23 10:52:14 visual_prompt]: 	Training 200/553. train loss: 166.2729,	0.8520 s / batch. (data: 1.19e-02). ETA=10:01:46, max mem: 20.9 GB 
[11/23 10:53:50 visual_prompt]: 	Training 300/553. train loss: 232.3090,	0.8653 s / batch. (data: 4.81e-02). ETA=10:09:46, max mem: 20.9 GB 
[11/23 10:55:28 visual_prompt]: 	Training 400/553. train loss: 35.5179,	0.8438 s / batch. (data: 4.43e-04). ETA=9:53:13, max mem: 20.9 GB 
[11/23 10:57:06 visual_prompt]: 	Training 500/553. train loss: 125.8313,	0.8100 s / batch. (data: 3.50e-04). ETA=9:28:06, max mem: 20.9 GB 
[11/23 10:57:57 visual_prompt]: Epoch 24 / 100: avg data time: 1.43e-01, avg batch time: 0.9697, average train loss: 161.9279
[11/23 10:58:52 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3096, average loss: 42.9661
[11/23 10:58:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.48	
[11/23 10:58:52 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/23 11:00:36 visual_prompt]: 	Training 100/553. train loss: 334.7375,	0.8480 s / batch. (data: 3.05e-04). ETA=9:52:35, max mem: 20.9 GB 
[11/23 11:02:10 visual_prompt]: 	Training 200/553. train loss: 7.2485,	0.8811 s / batch. (data: 3.32e-02). ETA=10:14:16, max mem: 20.9 GB 
[11/23 11:03:45 visual_prompt]: 	Training 300/553. train loss: 403.8047,	0.9168 s / batch. (data: 8.33e-02). ETA=10:37:37, max mem: 20.9 GB 
[11/23 11:05:22 visual_prompt]: 	Training 400/553. train loss: 52.0730,	1.1196 s / batch. (data: 3.10e-01). ETA=12:56:46, max mem: 20.9 GB 
[11/23 11:07:00 visual_prompt]: 	Training 500/553. train loss: 298.8242,	1.3680 s / batch. (data: 5.29e-01). ETA=15:46:49, max mem: 20.9 GB 
[11/23 11:07:50 visual_prompt]: Epoch 25 / 100: avg data time: 1.45e-01, avg batch time: 0.9722, average train loss: 149.2566
[11/23 11:08:45 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3089, average loss: 127.0611
[11/23 11:08:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.03	
[11/23 11:08:45 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/23 11:10:26 visual_prompt]: 	Training 100/553. train loss: 256.6980,	0.8640 s / batch. (data: 7.97e-03). ETA=9:55:47, max mem: 20.9 GB 
[11/23 11:12:04 visual_prompt]: 	Training 200/553. train loss: 16.0175,	1.7610 s / batch. (data: 9.50e-01). ETA=20:11:27, max mem: 20.9 GB 
[11/23 11:13:42 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8026 s / batch. (data: 5.95e-04). ETA=9:10:48, max mem: 20.9 GB 
[11/23 11:15:18 visual_prompt]: 	Training 400/553. train loss: 180.0629,	0.8143 s / batch. (data: 3.00e-04). ETA=9:17:27, max mem: 20.9 GB 
[11/23 11:16:53 visual_prompt]: 	Training 500/553. train loss: 32.3860,	0.8161 s / batch. (data: 8.47e-03). ETA=9:17:18, max mem: 20.9 GB 
[11/23 11:17:42 visual_prompt]: Epoch 26 / 100: avg data time: 1.44e-01, avg batch time: 0.9704, average train loss: 159.3604
[11/23 11:18:37 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3099, average loss: 10.0895
[11/23 11:18:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 55.72	
[11/23 11:18:37 visual_prompt]: Best epoch 26: best metric: -10.089
[11/23 11:18:37 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/23 11:20:18 visual_prompt]: 	Training 100/553. train loss: 340.0575,	0.8476 s / batch. (data: 5.41e-03). ETA=9:36:41, max mem: 20.9 GB 
[11/23 11:21:54 visual_prompt]: 	Training 200/553. train loss: 360.8433,	1.0433 s / batch. (data: 2.09e-01). ETA=11:48:03, max mem: 20.9 GB 
[11/23 11:23:31 visual_prompt]: 	Training 300/553. train loss: 7.3428,	0.8450 s / batch. (data: 9.02e-03). ETA=9:32:06, max mem: 20.9 GB 
[11/23 11:25:08 visual_prompt]: 	Training 400/553. train loss: 24.9038,	0.8400 s / batch. (data: 7.50e-04). ETA=9:27:18, max mem: 20.9 GB 
[11/23 11:26:45 visual_prompt]: 	Training 500/553. train loss: 30.3918,	0.8326 s / batch. (data: 8.05e-04). ETA=9:20:55, max mem: 20.9 GB 
[11/23 11:27:34 visual_prompt]: Epoch 27 / 100: avg data time: 1.43e-01, avg batch time: 0.9706, average train loss: 136.9341
[11/23 11:28:29 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3112, average loss: 98.2299
[11/23 11:28:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.05	
[11/23 11:28:29 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/23 11:30:08 visual_prompt]: 	Training 100/553. train loss: 128.2742,	0.8336 s / batch. (data: 2.82e-04). ETA=9:19:28, max mem: 20.9 GB 
[11/23 11:31:44 visual_prompt]: 	Training 200/553. train loss: 186.6500,	0.8425 s / batch. (data: 3.09e-04). ETA=9:24:02, max mem: 20.9 GB 
[11/23 11:33:21 visual_prompt]: 	Training 300/553. train loss: 20.4521,	1.3012 s / batch. (data: 4.52e-01). ETA=14:28:58, max mem: 20.9 GB 
[11/23 11:34:56 visual_prompt]: 	Training 400/553. train loss: 116.6639,	0.8550 s / batch. (data: 5.89e-03). ETA=9:29:32, max mem: 20.9 GB 
[11/23 11:36:32 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8280 s / batch. (data: 2.84e-04). ETA=9:10:11, max mem: 20.9 GB 
[11/23 11:37:23 visual_prompt]: Epoch 28 / 100: avg data time: 1.39e-01, avg batch time: 0.9656, average train loss: 154.5328
[11/23 11:38:17 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3098, average loss: 47.1506
[11/23 11:38:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.10	
[11/23 11:38:17 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/23 11:40:03 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8361 s / batch. (data: 9.00e-03). ETA=9:13:25, max mem: 20.9 GB 
[11/23 11:41:39 visual_prompt]: 	Training 200/553. train loss: 33.9879,	1.5241 s / batch. (data: 6.96e-01). ETA=16:46:19, max mem: 20.9 GB 
[11/23 11:43:14 visual_prompt]: 	Training 300/553. train loss: 181.5701,	0.8360 s / batch. (data: 7.65e-04). ETA=9:10:35, max mem: 20.9 GB 
[11/23 11:44:48 visual_prompt]: 	Training 400/553. train loss: 188.7184,	1.2192 s / batch. (data: 4.02e-01). ETA=13:20:56, max mem: 20.9 GB 
[11/23 11:46:25 visual_prompt]: 	Training 500/553. train loss: 131.5021,	0.8440 s / batch. (data: 1.05e-02). ETA=9:13:01, max mem: 20.9 GB 
[11/23 11:47:15 visual_prompt]: Epoch 29 / 100: avg data time: 1.44e-01, avg batch time: 0.9717, average train loss: 165.1326
[11/23 11:48:09 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3103, average loss: 137.0830
[11/23 11:48:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.47	
[11/23 11:48:09 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/23 11:49:48 visual_prompt]: 	Training 100/553. train loss: 137.3621,	0.8389 s / batch. (data: 5.41e-03). ETA=9:07:34, max mem: 20.9 GB 
[11/23 11:51:26 visual_prompt]: 	Training 200/553. train loss: 293.8915,	0.8185 s / batch. (data: 3.05e-04). ETA=8:52:53, max mem: 20.9 GB 
[11/23 11:53:01 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.2413 s / batch. (data: 3.92e-01). ETA=13:26:05, max mem: 20.9 GB 
[11/23 11:54:39 visual_prompt]: 	Training 400/553. train loss: 48.6005,	1.0164 s / batch. (data: 2.00e-01). ETA=10:58:18, max mem: 20.9 GB 
[11/23 11:56:13 visual_prompt]: 	Training 500/553. train loss: 15.0019,	1.2680 s / batch. (data: 4.36e-01). ETA=13:39:11, max mem: 20.9 GB 
[11/23 11:57:06 visual_prompt]: Epoch 30 / 100: avg data time: 1.43e-01, avg batch time: 0.9695, average train loss: 152.4408
[11/23 11:58:01 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3100, average loss: 126.2633
[11/23 11:58:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.01	
[11/23 11:58:01 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[11/23 11:59:43 visual_prompt]: 	Training 100/553. train loss: 115.0993,	0.8401 s / batch. (data: 3.34e-03). ETA=9:00:34, max mem: 20.9 GB 
[11/23 12:01:22 visual_prompt]: 	Training 200/553. train loss: 250.0156,	0.8646 s / batch. (data: 5.41e-03). ETA=9:14:54, max mem: 20.9 GB 
[11/23 12:02:56 visual_prompt]: 	Training 300/553. train loss: 112.1726,	0.8251 s / batch. (data: 3.19e-04). ETA=8:48:13, max mem: 20.9 GB 
[11/23 12:04:32 visual_prompt]: 	Training 400/553. train loss: 204.9033,	1.1838 s / batch. (data: 3.56e-01). ETA=12:35:51, max mem: 20.9 GB 
[11/23 12:06:10 visual_prompt]: 	Training 500/553. train loss: 57.7240,	0.8295 s / batch. (data: 5.41e-03). ETA=8:48:14, max mem: 20.9 GB 
[11/23 12:07:00 visual_prompt]: Epoch 31 / 100: avg data time: 1.49e-01, avg batch time: 0.9751, average train loss: 147.3782
[11/23 12:07:56 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3076, average loss: 66.5390
[11/23 12:07:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.88	
[11/23 12:07:56 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[11/23 12:09:38 visual_prompt]: 	Training 100/553. train loss: 63.9537,	0.8545 s / batch. (data: 1.10e-02). ETA=9:02:00, max mem: 20.9 GB 
[11/23 12:11:14 visual_prompt]: 	Training 200/553. train loss: 80.4546,	0.8188 s / batch. (data: 7.71e-04). ETA=8:37:57, max mem: 20.9 GB 
[11/23 12:12:53 visual_prompt]: 	Training 300/553. train loss: 192.4452,	0.8304 s / batch. (data: 2.06e-02). ETA=8:43:54, max mem: 20.9 GB 
[11/23 12:14:30 visual_prompt]: 	Training 400/553. train loss: 4.1298,	0.8419 s / batch. (data: 3.16e-04). ETA=8:49:47, max mem: 20.9 GB 
[11/23 12:16:04 visual_prompt]: 	Training 500/553. train loss: 370.3149,	0.8362 s / batch. (data: 7.30e-03). ETA=8:44:49, max mem: 20.9 GB 
[11/23 12:16:52 visual_prompt]: Epoch 32 / 100: avg data time: 1.43e-01, avg batch time: 0.9703, average train loss: 144.6342
[11/23 12:17:47 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3096, average loss: 325.2564
[11/23 12:17:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.09	
[11/23 12:17:47 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[11/23 12:19:27 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8200 s / batch. (data: 2.96e-04). ETA=8:32:32, max mem: 20.9 GB 
[11/23 12:21:06 visual_prompt]: 	Training 200/553. train loss: 107.0186,	0.8549 s / batch. (data: 2.17e-02). ETA=8:52:57, max mem: 20.9 GB 
[11/23 12:22:44 visual_prompt]: 	Training 300/553. train loss: 104.5602,	0.8407 s / batch. (data: 7.99e-03). ETA=8:42:40, max mem: 20.9 GB 
[11/23 12:24:22 visual_prompt]: 	Training 400/553. train loss: 18.8342,	0.8509 s / batch. (data: 2.06e-02). ETA=8:47:37, max mem: 20.9 GB 
[11/23 12:25:59 visual_prompt]: 	Training 500/553. train loss: 92.1204,	0.8280 s / batch. (data: 4.79e-04). ETA=8:32:00, max mem: 20.9 GB 
[11/23 12:26:49 visual_prompt]: Epoch 33 / 100: avg data time: 1.53e-01, avg batch time: 0.9797, average train loss: 163.7719
[11/23 12:27:45 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3101, average loss: 33.5294
[11/23 12:27:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.21	
[11/23 12:27:45 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[11/23 12:29:28 visual_prompt]: 	Training 100/553. train loss: 100.4352,	0.8200 s / batch. (data: 8.00e-03). ETA=8:24:58, max mem: 20.9 GB 
[11/23 12:31:03 visual_prompt]: 	Training 200/553. train loss: 168.7859,	0.8317 s / batch. (data: 3.23e-04). ETA=8:30:49, max mem: 20.9 GB 
[11/23 12:32:39 visual_prompt]: 	Training 300/553. train loss: 56.7570,	0.8316 s / batch. (data: 5.43e-03). ETA=8:29:23, max mem: 20.9 GB 
[11/23 12:34:16 visual_prompt]: 	Training 400/553. train loss: 172.6722,	0.8362 s / batch. (data: 3.00e-04). ETA=8:30:47, max mem: 20.9 GB 
[11/23 12:35:53 visual_prompt]: 	Training 500/553. train loss: 72.8756,	1.2713 s / batch. (data: 4.40e-01). ETA=12:54:26, max mem: 20.9 GB 
[11/23 12:36:44 visual_prompt]: Epoch 34 / 100: avg data time: 1.47e-01, avg batch time: 0.9744, average train loss: 139.0871
[11/23 12:37:39 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3123, average loss: 100.6480
[11/23 12:37:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.71	
[11/23 12:37:39 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[11/23 12:39:22 visual_prompt]: 	Training 100/553. train loss: 222.7979,	0.8320 s / batch. (data: 3.02e-04). ETA=8:24:43, max mem: 20.9 GB 
[11/23 12:41:01 visual_prompt]: 	Training 200/553. train loss: 64.9297,	0.8080 s / batch. (data: 4.02e-04). ETA=8:08:48, max mem: 20.9 GB 
[11/23 12:42:36 visual_prompt]: 	Training 300/553. train loss: 56.2827,	0.8520 s / batch. (data: 7.96e-03). ETA=8:34:01, max mem: 20.9 GB 
[11/23 12:44:12 visual_prompt]: 	Training 400/553. train loss: 203.8705,	0.8060 s / batch. (data: 3.65e-04). ETA=8:04:53, max mem: 20.9 GB 
[11/23 12:45:47 visual_prompt]: 	Training 500/553. train loss: 13.9455,	0.9844 s / batch. (data: 1.54e-01). ETA=9:50:37, max mem: 20.9 GB 
[11/23 12:46:39 visual_prompt]: Epoch 35 / 100: avg data time: 1.49e-01, avg batch time: 0.9762, average train loss: 121.9376
[11/23 12:47:35 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3105, average loss: 78.2520
[11/23 12:47:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.94	
[11/23 12:47:35 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[11/23 12:49:15 visual_prompt]: 	Training 100/553. train loss: 188.7446,	0.8159 s / batch. (data: 3.16e-04). ETA=8:07:24, max mem: 20.9 GB 
[11/23 12:50:53 visual_prompt]: 	Training 200/553. train loss: 150.9630,	0.8244 s / batch. (data: 5.41e-03). ETA=8:11:07, max mem: 20.9 GB 
[11/23 12:52:31 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8245 s / batch. (data: 2.85e-04). ETA=8:09:49, max mem: 20.9 GB 
[11/23 12:54:07 visual_prompt]: 	Training 400/553. train loss: 8.6037,	0.8156 s / batch. (data: 3.09e-04). ETA=8:03:09, max mem: 20.9 GB 
[11/23 12:55:45 visual_prompt]: 	Training 500/553. train loss: 187.1887,	0.9689 s / batch. (data: 1.53e-01). ETA=9:32:23, max mem: 20.9 GB 
[11/23 12:56:33 visual_prompt]: Epoch 36 / 100: avg data time: 1.46e-01, avg batch time: 0.9728, average train loss: 147.0550
[11/23 12:57:28 visual_prompt]: Inference (val):avg data time: 2.77e-04, avg batch time: 0.3132, average loss: 404.8266
[11/23 12:57:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.33	
[11/23 12:57:28 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[11/23 12:59:09 visual_prompt]: 	Training 100/553. train loss: 3.6021,	0.8320 s / batch. (data: 3.07e-04). ETA=8:09:22, max mem: 20.9 GB 
[11/23 13:00:46 visual_prompt]: 	Training 200/553. train loss: 141.7495,	0.8350 s / batch. (data: 5.49e-03). ETA=8:09:45, max mem: 20.9 GB 
[11/23 13:02:24 visual_prompt]: 	Training 300/553. train loss: 111.5356,	1.4640 s / batch. (data: 6.42e-01). ETA=14:16:15, max mem: 20.9 GB 
[11/23 13:04:02 visual_prompt]: 	Training 400/553. train loss: 268.0103,	1.7827 s / batch. (data: 9.39e-01). ETA=17:19:41, max mem: 20.9 GB 
[11/23 13:05:35 visual_prompt]: 	Training 500/553. train loss: 109.1986,	0.8428 s / batch. (data: 1.05e-02). ETA=8:10:06, max mem: 20.9 GB 
[11/23 13:06:27 visual_prompt]: Epoch 37 / 100: avg data time: 1.48e-01, avg batch time: 0.9746, average train loss: 144.4816
[11/23 13:07:23 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3099, average loss: 276.0451
[11/23 13:07:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.37	
[11/23 13:07:23 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[11/23 13:09:03 visual_prompt]: 	Training 100/553. train loss: 46.1960,	0.8078 s / batch. (data: 3.13e-04). ETA=7:47:42, max mem: 20.9 GB 
[11/23 13:10:40 visual_prompt]: 	Training 200/553. train loss: 26.8297,	0.8333 s / batch. (data: 5.45e-03). ETA=8:01:03, max mem: 20.9 GB 
[11/23 13:12:18 visual_prompt]: 	Training 300/553. train loss: 63.9591,	0.8057 s / batch. (data: 2.83e-04). ETA=7:43:49, max mem: 20.9 GB 
[11/23 13:13:54 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8280 s / batch. (data: 7.98e-03). ETA=7:55:15, max mem: 20.9 GB 
[11/23 13:15:34 visual_prompt]: 	Training 500/553. train loss: 132.2172,	0.8267 s / batch. (data: 3.31e-04). ETA=7:53:06, max mem: 20.9 GB 
[11/23 13:16:24 visual_prompt]: Epoch 38 / 100: avg data time: 1.50e-01, avg batch time: 0.9774, average train loss: 141.5284
[11/23 13:17:19 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3088, average loss: 468.5666
[11/23 13:17:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.83	
[11/23 13:17:19 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[11/23 13:18:58 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8083 s / batch. (data: 5.47e-03). ETA=7:40:31, max mem: 20.9 GB 
[11/23 13:20:38 visual_prompt]: 	Training 200/553. train loss: 318.9404,	0.8449 s / batch. (data: 2.53e-02). ETA=8:00:00, max mem: 20.9 GB 
[11/23 13:22:18 visual_prompt]: 	Training 300/553. train loss: 309.1207,	0.8453 s / batch. (data: 5.47e-03). ETA=7:58:47, max mem: 20.9 GB 
[11/23 13:23:52 visual_prompt]: 	Training 400/553. train loss: 135.6061,	0.8562 s / batch. (data: 3.52e-04). ETA=8:03:33, max mem: 20.9 GB 
[11/23 13:25:28 visual_prompt]: 	Training 500/553. train loss: 73.8458,	1.4671 s / batch. (data: 6.61e-01). ETA=13:46:08, max mem: 20.9 GB 
[11/23 13:26:17 visual_prompt]: Epoch 39 / 100: avg data time: 1.44e-01, avg batch time: 0.9717, average train loss: 140.0020
[11/23 13:27:12 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3111, average loss: 137.8621
[11/23 13:27:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.12	
[11/23 13:27:12 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[11/23 13:28:53 visual_prompt]: 	Training 100/553. train loss: 257.4938,	0.8280 s / batch. (data: 3.13e-04). ETA=7:44:08, max mem: 20.9 GB 
[11/23 13:30:29 visual_prompt]: 	Training 200/553. train loss: 12.0638,	0.8280 s / batch. (data: 3.04e-04). ETA=7:42:45, max mem: 20.9 GB 
[11/23 13:32:07 visual_prompt]: 	Training 300/553. train loss: 318.8055,	0.8520 s / batch. (data: 2.95e-04). ETA=7:54:45, max mem: 20.9 GB 
[11/23 13:33:44 visual_prompt]: 	Training 400/553. train loss: 26.2209,	0.8455 s / batch. (data: 1.10e-02). ETA=7:49:42, max mem: 20.9 GB 
[11/23 13:35:20 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8370 s / batch. (data: 2.50e-04). ETA=7:43:37, max mem: 20.9 GB 
[11/23 13:36:12 visual_prompt]: Epoch 40 / 100: avg data time: 1.50e-01, avg batch time: 0.9769, average train loss: 100.7834
[11/23 13:37:07 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3101, average loss: 243.3437
[11/23 13:37:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.03	
[11/23 13:37:07 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[11/23 13:38:51 visual_prompt]: 	Training 100/553. train loss: 50.4379,	0.8120 s / batch. (data: 2.89e-04). ETA=7:27:41, max mem: 20.9 GB 
[11/23 13:40:29 visual_prompt]: 	Training 200/553. train loss: 207.1020,	0.8214 s / batch. (data: 3.70e-04). ETA=7:31:28, max mem: 20.9 GB 
[11/23 13:42:05 visual_prompt]: 	Training 300/553. train loss: 78.5583,	0.8427 s / batch. (data: 5.42e-03). ETA=7:41:48, max mem: 20.9 GB 
[11/23 13:43:39 visual_prompt]: 	Training 400/553. train loss: 188.9894,	0.8272 s / batch. (data: 6.31e-04). ETA=7:31:54, max mem: 20.9 GB 
[11/23 13:45:14 visual_prompt]: 	Training 500/553. train loss: 154.2863,	0.8378 s / batch. (data: 2.94e-04). ETA=7:36:17, max mem: 20.9 GB 
[11/23 13:46:02 visual_prompt]: Epoch 41 / 100: avg data time: 1.41e-01, avg batch time: 0.9673, average train loss: 131.5228
[11/23 13:46:56 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3100, average loss: 192.0727
[11/23 13:46:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.73	
[11/23 13:46:56 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[11/23 13:48:35 visual_prompt]: 	Training 100/553. train loss: 341.8537,	0.8215 s / batch. (data: 3.03e-04). ETA=7:25:20, max mem: 20.9 GB 
[11/23 13:50:11 visual_prompt]: 	Training 200/553. train loss: 142.7461,	0.8282 s / batch. (data: 1.64e-02). ETA=7:27:36, max mem: 20.9 GB 
[11/23 13:51:48 visual_prompt]: 	Training 300/553. train loss: 368.0974,	0.8498 s / batch. (data: 5.42e-03). ETA=7:37:51, max mem: 20.9 GB 
[11/23 13:53:25 visual_prompt]: 	Training 400/553. train loss: 129.9272,	0.8206 s / batch. (data: 3.28e-04). ETA=7:20:46, max mem: 20.9 GB 
[11/23 13:55:00 visual_prompt]: 	Training 500/553. train loss: 389.5491,	0.8440 s / batch. (data: 2.98e-04). ETA=7:31:54, max mem: 20.9 GB 
[11/23 13:55:52 visual_prompt]: Epoch 42 / 100: avg data time: 1.40e-01, avg batch time: 0.9677, average train loss: 140.2088
[11/23 13:56:46 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3107, average loss: 221.4530
[11/23 13:56:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.79	
[11/23 13:56:46 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[11/23 13:58:28 visual_prompt]: 	Training 100/553. train loss: 251.0161,	0.8142 s / batch. (data: 2.97e-04). ETA=7:13:51, max mem: 20.9 GB 
[11/23 14:00:03 visual_prompt]: 	Training 200/553. train loss: 186.0584,	0.8188 s / batch. (data: 3.09e-04). ETA=7:14:59, max mem: 20.9 GB 
[11/23 14:01:38 visual_prompt]: 	Training 300/553. train loss: 259.0633,	0.8360 s / batch. (data: 2.97e-04). ETA=7:22:43, max mem: 20.9 GB 
[11/23 14:03:13 visual_prompt]: 	Training 400/553. train loss: 175.8335,	0.8240 s / batch. (data: 7.95e-03). ETA=7:14:58, max mem: 20.9 GB 
[11/23 14:04:50 visual_prompt]: 	Training 500/553. train loss: 142.0669,	0.8320 s / batch. (data: 3.19e-04). ETA=7:17:50, max mem: 20.9 GB 
[11/23 14:05:43 visual_prompt]: Epoch 43 / 100: avg data time: 1.42e-01, avg batch time: 0.9694, average train loss: 131.8388
[11/23 14:06:37 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3106, average loss: 335.8352
[11/23 14:06:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.73	
[11/23 14:06:37 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[11/23 14:08:18 visual_prompt]: 	Training 100/553. train loss: 98.0789,	1.0587 s / batch. (data: 2.54e-01). ETA=9:14:24, max mem: 20.9 GB 
[11/23 14:09:56 visual_prompt]: 	Training 200/553. train loss: 27.2970,	0.8232 s / batch. (data: 3.06e-04). ETA=7:09:42, max mem: 20.9 GB 
[11/23 14:11:31 visual_prompt]: 	Training 300/553. train loss: 110.8865,	0.8089 s / batch. (data: 2.99e-04). ETA=7:00:53, max mem: 20.9 GB 
[11/23 14:13:06 visual_prompt]: 	Training 400/553. train loss: 64.9772,	0.8288 s / batch. (data: 3.10e-04). ETA=7:09:53, max mem: 20.9 GB 
[11/23 14:14:43 visual_prompt]: 	Training 500/553. train loss: 48.6969,	0.8209 s / batch. (data: 3.21e-04). ETA=7:04:26, max mem: 20.9 GB 
[11/23 14:15:34 visual_prompt]: Epoch 44 / 100: avg data time: 1.44e-01, avg batch time: 0.9710, average train loss: 119.7253
[11/23 14:16:29 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3111, average loss: 10.1329
[11/23 14:16:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.18	
[11/23 14:16:29 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[11/23 14:18:11 visual_prompt]: 	Training 100/553. train loss: 13.1764,	0.8440 s / batch. (data: 3.06e-04). ETA=7:14:11, max mem: 20.9 GB 
[11/23 14:19:44 visual_prompt]: 	Training 200/553. train loss: 51.7602,	0.8280 s / batch. (data: 3.27e-04). ETA=7:04:35, max mem: 20.9 GB 
[11/23 14:21:22 visual_prompt]: 	Training 300/553. train loss: 23.4284,	0.8360 s / batch. (data: 3.02e-04). ETA=7:07:18, max mem: 20.9 GB 
[11/23 14:22:57 visual_prompt]: 	Training 400/553. train loss: 156.7916,	0.8165 s / batch. (data: 5.45e-03). ETA=6:55:58, max mem: 20.9 GB 
[11/23 14:24:36 visual_prompt]: 	Training 500/553. train loss: 70.1288,	0.8199 s / batch. (data: 3.80e-03). ETA=6:56:21, max mem: 20.9 GB 
[11/23 14:25:26 visual_prompt]: Epoch 45 / 100: avg data time: 1.44e-01, avg batch time: 0.9703, average train loss: 123.9202
[11/23 14:26:21 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3115, average loss: 114.6815
[11/23 14:26:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.27	
[11/23 14:26:21 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[11/23 14:28:02 visual_prompt]: 	Training 100/553. train loss: 184.4694,	1.0880 s / batch. (data: 2.69e-01). ETA=9:09:42, max mem: 20.9 GB 
[11/23 14:29:40 visual_prompt]: 	Training 200/553. train loss: 13.2624,	0.8560 s / batch. (data: 3.15e-04). ETA=7:11:03, max mem: 20.9 GB 
[11/23 14:31:14 visual_prompt]: 	Training 300/553. train loss: 361.9542,	0.8398 s / batch. (data: 3.27e-04). ETA=7:01:32, max mem: 20.9 GB 
[11/23 14:32:51 visual_prompt]: 	Training 400/553. train loss: 314.4479,	0.8103 s / batch. (data: 2.92e-04). ETA=6:45:20, max mem: 20.9 GB 
[11/23 14:34:26 visual_prompt]: 	Training 500/553. train loss: 120.6618,	0.8245 s / batch. (data: 2.74e-04). ETA=6:51:04, max mem: 20.9 GB 
[11/23 14:35:18 visual_prompt]: Epoch 46 / 100: avg data time: 1.42e-01, avg batch time: 0.9702, average train loss: 123.0109
[11/23 14:36:12 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3108, average loss: 189.0893
[11/23 14:36:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.63	
[11/23 14:36:12 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[11/23 14:37:54 visual_prompt]: 	Training 100/553. train loss: 34.3043,	0.8358 s / batch. (data: 2.80e-04). ETA=6:54:36, max mem: 20.9 GB 
[11/23 14:39:27 visual_prompt]: 	Training 200/553. train loss: 115.0741,	0.9440 s / batch. (data: 1.03e-01). ETA=7:46:41, max mem: 20.9 GB 
[11/23 14:41:05 visual_prompt]: 	Training 300/553. train loss: 101.0320,	0.8230 s / batch. (data: 2.84e-04). ETA=6:45:30, max mem: 20.9 GB 
[11/23 14:42:41 visual_prompt]: 	Training 400/553. train loss: 10.7620,	0.8240 s / batch. (data: 2.97e-04). ETA=6:44:36, max mem: 20.9 GB 
[11/23 14:44:17 visual_prompt]: 	Training 500/553. train loss: 19.4447,	0.8718 s / batch. (data: 3.05e-04). ETA=7:06:38, max mem: 20.9 GB 
[11/23 14:45:09 visual_prompt]: Epoch 47 / 100: avg data time: 1.43e-01, avg batch time: 0.9695, average train loss: 123.5316
[11/23 14:46:04 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3091, average loss: 139.5126
[11/23 14:46:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.60	
[11/23 14:46:04 visual_prompt]: Stopping early.
[11/23 14:46:06 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 14:46:06 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 14:46:06 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/23 14:46:06 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 14:46:06 visual_prompt]: Training with config:
[11/23 14:46:06 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/23 14:46:06 visual_prompt]: Loading training data...
[11/23 14:46:06 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 14:46:06 visual_prompt]: Loading validation data...
[11/23 14:46:06 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 14:46:07 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 14:46:16 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 14:46:16 visual_prompt]: tuned percent:0.525
[11/23 14:46:16 visual_prompt]: Device used for model: 0
[11/23 14:46:16 visual_prompt]: Setting up Evaluator...
[11/23 14:46:16 visual_prompt]: Setting up Trainer...
[11/23 14:46:16 visual_prompt]: 	Setting up the optimizer...
[11/23 14:46:16 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 14:47:59 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8197 s / batch. (data: 3.05e-04). ETA=12:34:05, max mem: 20.9 GB 
[11/23 14:49:34 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8320 s / batch. (data: 3.09e-04). ETA=12:44:03, max mem: 20.9 GB 
[11/23 14:51:13 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8600 s / batch. (data: 1.60e-02). ETA=13:08:19, max mem: 20.9 GB 
[11/23 14:52:49 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8311 s / batch. (data: 3.32e-04). ETA=12:40:29, max mem: 20.9 GB 
[11/23 14:54:28 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8480 s / batch. (data: 8.02e-03). ETA=12:54:31, max mem: 20.9 GB 
[11/23 14:55:19 visual_prompt]: Epoch 1 / 100: avg data time: 1.49e-01, avg batch time: 0.9820, average train loss: 1.5403
[11/23 14:56:15 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3105, average loss: 1.5201
[11/23 14:56:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 14:56:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/23 14:57:55 visual_prompt]: 	Training 100/553. train loss: 25.0325,	0.8695 s / batch. (data: 5.46e-03). ETA=13:11:57, max mem: 20.9 GB 
[11/23 14:59:31 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8240 s / batch. (data: 3.09e-04). ETA=12:29:07, max mem: 20.9 GB 
[11/23 15:01:10 visual_prompt]: 	Training 300/553. train loss: 7.4490,	1.0075 s / batch. (data: 1.76e-01). ETA=15:14:17, max mem: 20.9 GB 
[11/23 15:02:45 visual_prompt]: 	Training 400/553. train loss: 29.1996,	0.8729 s / batch. (data: 9.92e-03). ETA=13:10:38, max mem: 20.9 GB 
[11/23 15:04:23 visual_prompt]: 	Training 500/553. train loss: 80.6536,	0.8195 s / batch. (data: 3.38e-04). ETA=12:20:55, max mem: 20.9 GB 
[11/23 15:05:13 visual_prompt]: Epoch 2 / 100: avg data time: 1.42e-01, avg batch time: 0.9730, average train loss: 26.5332
[11/23 15:06:08 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3113, average loss: 67.9892
[11/23 15:06:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.61	
[11/23 15:06:08 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/23 15:07:47 visual_prompt]: 	Training 100/553. train loss: 60.1347,	0.8237 s / batch. (data: 3.08e-04). ETA=12:22:38, max mem: 20.9 GB 
[11/23 15:09:25 visual_prompt]: 	Training 200/553. train loss: 29.4592,	0.8372 s / batch. (data: 5.58e-03). ETA=12:33:22, max mem: 20.9 GB 
[11/23 15:11:01 visual_prompt]: 	Training 300/553. train loss: 40.9004,	0.8240 s / batch. (data: 3.09e-04). ETA=12:20:08, max mem: 20.9 GB 
[11/23 15:12:38 visual_prompt]: 	Training 400/553. train loss: 14.7090,	0.8214 s / batch. (data: 3.13e-04). ETA=12:16:27, max mem: 20.9 GB 
[11/23 15:14:17 visual_prompt]: 	Training 500/553. train loss: 23.5847,	1.1563 s / batch. (data: 3.25e-01). ETA=17:14:45, max mem: 20.9 GB 
[11/23 15:15:07 visual_prompt]: Epoch 3 / 100: avg data time: 1.44e-01, avg batch time: 0.9737, average train loss: 43.5661
[11/23 15:16:03 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3113, average loss: 40.6659
[11/23 15:16:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.00	
[11/23 15:16:03 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/23 15:17:45 visual_prompt]: 	Training 100/553. train loss: 39.0314,	0.8491 s / batch. (data: 1.05e-02). ETA=12:37:43, max mem: 20.9 GB 
[11/23 15:19:24 visual_prompt]: 	Training 200/553. train loss: 61.0410,	0.8610 s / batch. (data: 5.41e-03). ETA=12:46:53, max mem: 20.9 GB 
[11/23 15:21:01 visual_prompt]: 	Training 300/553. train loss: 14.5468,	1.2437 s / batch. (data: 4.18e-01). ETA=18:25:40, max mem: 20.9 GB 
[11/23 15:22:33 visual_prompt]: 	Training 400/553. train loss: 25.0609,	0.9840 s / batch. (data: 1.29e-01). ETA=14:33:07, max mem: 20.9 GB 
[11/23 15:24:12 visual_prompt]: 	Training 500/553. train loss: 145.8390,	3.1000 s / batch. (data: 2.27e+00). ETA=1 day, 21:45:35, max mem: 20.9 GB 
[11/23 15:25:04 visual_prompt]: Epoch 4 / 100: avg data time: 1.48e-01, avg batch time: 0.9782, average train loss: 41.9266
[11/23 15:25:59 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3087, average loss: 32.3341
[11/23 15:25:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.60	
[11/23 15:25:59 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/23 15:27:38 visual_prompt]: 	Training 100/553. train loss: 386.8663,	0.8320 s / batch. (data: 2.90e-04). ETA=12:14:47, max mem: 20.9 GB 
[11/23 15:29:14 visual_prompt]: 	Training 200/553. train loss: 53.7453,	0.9897 s / batch. (data: 1.80e-01). ETA=14:32:23, max mem: 20.9 GB 
[11/23 15:30:52 visual_prompt]: 	Training 300/553. train loss: 214.9673,	0.8513 s / batch. (data: 5.88e-03). ETA=12:28:59, max mem: 20.9 GB 
[11/23 15:32:27 visual_prompt]: 	Training 400/553. train loss: 27.8766,	0.8295 s / batch. (data: 2.89e-04). ETA=12:08:25, max mem: 20.9 GB 
[11/23 15:34:04 visual_prompt]: 	Training 500/553. train loss: 19.4011,	0.8560 s / batch. (data: 2.78e-04). ETA=12:30:15, max mem: 20.9 GB 
[11/23 15:34:55 visual_prompt]: Epoch 5 / 100: avg data time: 1.43e-01, avg batch time: 0.9703, average train loss: 85.3709
[11/23 15:35:50 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3086, average loss: 19.8969
[11/23 15:35:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.41	
[11/23 15:35:50 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/23 15:37:32 visual_prompt]: 	Training 100/553. train loss: 88.2537,	0.8520 s / batch. (data: 3.60e-02). ETA=12:24:33, max mem: 20.9 GB 
[11/23 15:39:08 visual_prompt]: 	Training 200/553. train loss: 21.5776,	0.8176 s / batch. (data: 3.02e-04). ETA=11:53:06, max mem: 20.9 GB 
[11/23 15:40:43 visual_prompt]: 	Training 300/553. train loss: 217.9171,	0.8328 s / batch. (data: 7.96e-03). ETA=12:05:00, max mem: 20.9 GB 
[11/23 15:42:24 visual_prompt]: 	Training 400/553. train loss: 6.9724,	0.8320 s / batch. (data: 3.17e-04). ETA=12:02:57, max mem: 20.9 GB 
[11/23 15:43:59 visual_prompt]: 	Training 500/553. train loss: 1.2620,	0.8200 s / batch. (data: 3.40e-04). ETA=11:51:07, max mem: 20.9 GB 
[11/23 15:44:49 visual_prompt]: Epoch 6 / 100: avg data time: 1.47e-01, avg batch time: 0.9738, average train loss: 87.7367
[11/23 15:45:44 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3097, average loss: 26.4101
[11/23 15:45:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.30	
[11/23 15:45:44 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/23 15:47:23 visual_prompt]: 	Training 100/553. train loss: 50.2073,	0.8355 s / batch. (data: 7.95e-03). ETA=12:02:25, max mem: 20.9 GB 
[11/23 15:49:00 visual_prompt]: 	Training 200/553. train loss: 18.5600,	0.8600 s / batch. (data: 5.43e-03). ETA=12:22:11, max mem: 20.9 GB 
[11/23 15:50:40 visual_prompt]: 	Training 300/553. train loss: 4.7670,	1.5351 s / batch. (data: 7.14e-01). ETA=22:02:17, max mem: 20.9 GB 
[11/23 15:52:15 visual_prompt]: 	Training 400/553. train loss: 29.0680,	0.8280 s / batch. (data: 3.92e-04). ETA=11:51:50, max mem: 20.9 GB 
[11/23 15:53:52 visual_prompt]: 	Training 500/553. train loss: 176.5945,	0.8221 s / batch. (data: 4.30e-04). ETA=11:45:21, max mem: 20.9 GB 
[11/23 15:54:41 visual_prompt]: Epoch 7 / 100: avg data time: 1.42e-01, avg batch time: 0.9697, average train loss: 99.9768
[11/23 15:55:36 visual_prompt]: Inference (val):avg data time: 8.39e-05, avg batch time: 0.3093, average loss: 111.2492
[11/23 15:55:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.62	
[11/23 15:55:36 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/23 15:57:15 visual_prompt]: 	Training 100/553. train loss: 282.0644,	0.8465 s / batch. (data: 2.27e-02). ETA=12:04:09, max mem: 20.9 GB 
[11/23 15:58:54 visual_prompt]: 	Training 200/553. train loss: 32.4692,	0.8439 s / batch. (data: 7.50e-03). ETA=12:00:29, max mem: 20.9 GB 
[11/23 16:00:32 visual_prompt]: 	Training 300/553. train loss: 50.4068,	0.8396 s / batch. (data: 9.61e-03). ETA=11:55:27, max mem: 20.9 GB 
[11/23 16:02:09 visual_prompt]: 	Training 400/553. train loss: 234.5956,	0.8202 s / batch. (data: 3.29e-04). ETA=11:37:36, max mem: 20.9 GB 
[11/23 16:03:46 visual_prompt]: 	Training 500/553. train loss: 329.2100,	1.4310 s / batch. (data: 6.04e-01). ETA=20:14:38, max mem: 20.9 GB 
[11/23 16:04:37 visual_prompt]: Epoch 8 / 100: avg data time: 1.50e-01, avg batch time: 0.9770, average train loss: 113.6568
[11/23 16:05:32 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3104, average loss: 4.6736
[11/23 16:05:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.02	
[11/23 16:05:32 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/23 16:07:12 visual_prompt]: 	Training 100/553. train loss: 38.1540,	0.8520 s / batch. (data: 1.59e-02). ETA=12:00:59, max mem: 20.9 GB 
[11/23 16:08:46 visual_prompt]: 	Training 200/553. train loss: 2.7190,	0.8360 s / batch. (data: 2.98e-04). ETA=11:46:04, max mem: 20.9 GB 
[11/23 16:10:23 visual_prompt]: 	Training 300/553. train loss: 53.2733,	1.4560 s / batch. (data: 6.37e-01). ETA=20:27:18, max mem: 20.9 GB 
[11/23 16:12:00 visual_prompt]: 	Training 400/553. train loss: 240.5974,	0.8398 s / batch. (data: 7.83e-04). ETA=11:46:30, max mem: 20.9 GB 
[11/23 16:13:36 visual_prompt]: 	Training 500/553. train loss: 59.7817,	0.8486 s / batch. (data: 3.48e-02). ETA=11:52:29, max mem: 20.9 GB 
[11/23 16:14:25 visual_prompt]: Epoch 9 / 100: avg data time: 1.37e-01, avg batch time: 0.9642, average train loss: 133.1418
[11/23 16:15:20 visual_prompt]: Inference (val):avg data time: 1.70e-04, avg batch time: 0.3091, average loss: 184.4700
[11/23 16:15:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.80	
[11/23 16:15:20 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/23 16:17:02 visual_prompt]: 	Training 100/553. train loss: 277.8878,	0.8240 s / batch. (data: 2.87e-04). ETA=11:29:44, max mem: 20.9 GB 
[11/23 16:18:36 visual_prompt]: 	Training 200/553. train loss: 8.4880,	0.8451 s / batch. (data: 5.43e-03). ETA=11:45:58, max mem: 20.9 GB 
[11/23 16:20:11 visual_prompt]: 	Training 300/553. train loss: 49.5425,	1.2960 s / batch. (data: 4.56e-01). ETA=18:00:29, max mem: 20.9 GB 
[11/23 16:21:45 visual_prompt]: 	Training 400/553. train loss: 7.6892,	0.8477 s / batch. (data: 3.14e-04). ETA=11:45:19, max mem: 20.9 GB 
[11/23 16:23:22 visual_prompt]: 	Training 500/553. train loss: 26.2307,	0.8073 s / batch. (data: 3.48e-04). ETA=11:10:20, max mem: 20.9 GB 
[11/23 16:24:12 visual_prompt]: Epoch 10 / 100: avg data time: 1.35e-01, avg batch time: 0.9619, average train loss: 164.3878
[11/23 16:25:07 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3103, average loss: 31.7203
[11/23 16:25:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.98	
[11/23 16:25:07 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/23 16:26:51 visual_prompt]: 	Training 100/553. train loss: 9.9833,	0.8262 s / batch. (data: 1.02e-02). ETA=11:23:58, max mem: 20.9 GB 
[11/23 16:28:30 visual_prompt]: 	Training 200/553. train loss: 469.3429,	0.8405 s / batch. (data: 2.33e-02). ETA=11:34:23, max mem: 20.9 GB 
[11/23 16:30:06 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.9720 s / batch. (data: 1.14e+00). ETA=1 day, 3:05:54, max mem: 20.9 GB 
[11/23 16:31:40 visual_prompt]: 	Training 400/553. train loss: 100.8531,	0.8329 s / batch. (data: 7.82e-04). ETA=11:25:20, max mem: 20.9 GB 
[11/23 16:33:14 visual_prompt]: 	Training 500/553. train loss: 263.9169,	0.9840 s / batch. (data: 1.57e-01). ETA=13:28:01, max mem: 20.9 GB 
[11/23 16:34:06 visual_prompt]: Epoch 11 / 100: avg data time: 1.47e-01, avg batch time: 0.9734, average train loss: 146.0352
[11/23 16:35:01 visual_prompt]: Inference (val):avg data time: 1.68e-04, avg batch time: 0.3095, average loss: 132.6761
[11/23 16:35:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.21	
[11/23 16:35:01 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/23 16:36:43 visual_prompt]: 	Training 100/553. train loss: 60.8919,	0.8410 s / batch. (data: 1.05e-02). ETA=11:28:29, max mem: 20.9 GB 
[11/23 16:38:20 visual_prompt]: 	Training 200/553. train loss: 56.2458,	1.0464 s / batch. (data: 2.18e-01). ETA=14:14:51, max mem: 20.9 GB 
[11/23 16:39:57 visual_prompt]: 	Training 300/553. train loss: 37.1902,	0.8240 s / batch. (data: 1.20e-02). ETA=11:11:47, max mem: 20.9 GB 
[11/23 16:41:34 visual_prompt]: 	Training 400/553. train loss: 9.3180,	0.8586 s / batch. (data: 5.25e-04). ETA=11:38:32, max mem: 20.9 GB 
[11/23 16:43:11 visual_prompt]: 	Training 500/553. train loss: 13.2477,	0.8277 s / batch. (data: 7.72e-04). ETA=11:12:02, max mem: 20.9 GB 
[11/23 16:44:00 visual_prompt]: Epoch 12 / 100: avg data time: 1.49e-01, avg batch time: 0.9749, average train loss: 172.4881
[11/23 16:44:56 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3108, average loss: 32.2117
[11/23 16:44:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.13	
[11/23 16:44:56 visual_prompt]: Best epoch 12: best metric: -32.212
[11/23 16:44:56 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/23 16:46:38 visual_prompt]: 	Training 100/553. train loss: 126.3914,	0.8035 s / batch. (data: 3.49e-04). ETA=10:50:21, max mem: 20.9 GB 
[11/23 16:48:11 visual_prompt]: 	Training 200/553. train loss: 6.3517,	0.8309 s / batch. (data: 1.05e-02). ETA=11:11:09, max mem: 20.9 GB 
[11/23 16:49:48 visual_prompt]: 	Training 300/553. train loss: 238.2179,	1.6805 s / batch. (data: 8.76e-01). ETA=22:34:37, max mem: 20.9 GB 
[11/23 16:51:22 visual_prompt]: 	Training 400/553. train loss: 48.1541,	0.8482 s / batch. (data: 1.63e-02). ETA=11:22:15, max mem: 20.9 GB 
[11/23 16:52:59 visual_prompt]: 	Training 500/553. train loss: 359.6395,	0.8637 s / batch. (data: 2.96e-04). ETA=11:33:18, max mem: 20.9 GB 
[11/23 16:53:49 visual_prompt]: Epoch 13 / 100: avg data time: 1.37e-01, avg batch time: 0.9645, average train loss: 158.3108
[11/23 16:54:43 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3115, average loss: 64.0487
[11/23 16:54:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.74	
[11/23 16:54:43 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/23 16:56:24 visual_prompt]: 	Training 100/553. train loss: 354.7159,	0.8390 s / batch. (data: 2.99e-04). ETA=11:11:22, max mem: 20.9 GB 
[11/23 16:57:59 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8444 s / batch. (data: 3.19e-02). ETA=11:14:17, max mem: 20.9 GB 
[11/23 16:59:34 visual_prompt]: 	Training 300/553. train loss: 281.6381,	0.8199 s / batch. (data: 2.93e-04). ETA=10:53:22, max mem: 20.9 GB 
[11/23 17:01:09 visual_prompt]: 	Training 400/553. train loss: 143.7888,	0.8214 s / batch. (data: 2.98e-04). ETA=10:53:09, max mem: 20.9 GB 
[11/23 17:02:44 visual_prompt]: 	Training 500/553. train loss: 306.5684,	0.8270 s / batch. (data: 5.42e-03). ETA=10:56:12, max mem: 20.9 GB 
[11/23 17:03:33 visual_prompt]: Epoch 14 / 100: avg data time: 1.31e-01, avg batch time: 0.9569, average train loss: 151.1336
[11/23 17:04:27 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3093, average loss: 183.7384
[11/23 17:04:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.66	
[11/23 17:04:27 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/23 17:06:06 visual_prompt]: 	Training 100/553. train loss: 105.0922,	1.5085 s / batch. (data: 7.01e-01). ETA=19:53:10, max mem: 20.9 GB 
[11/23 17:07:40 visual_prompt]: 	Training 200/553. train loss: 1187.5264,	0.8240 s / batch. (data: 2.99e-04). ETA=10:50:22, max mem: 20.9 GB 
[11/23 17:09:18 visual_prompt]: 	Training 300/553. train loss: 32.1880,	0.8280 s / batch. (data: 3.06e-04). ETA=10:52:08, max mem: 20.9 GB 
[11/23 17:10:52 visual_prompt]: 	Training 400/553. train loss: 28.2467,	0.8960 s / batch. (data: 7.09e-02). ETA=11:44:12, max mem: 20.9 GB 
[11/23 17:12:29 visual_prompt]: 	Training 500/553. train loss: 68.7989,	0.8400 s / batch. (data: 7.96e-03). ETA=10:58:49, max mem: 20.9 GB 
[11/23 17:13:20 visual_prompt]: Epoch 15 / 100: avg data time: 1.37e-01, avg batch time: 0.9640, average train loss: 206.1229
[11/23 17:14:15 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3110, average loss: 118.2958
[11/23 17:14:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.74	
[11/23 17:14:15 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/23 17:15:54 visual_prompt]: 	Training 100/553. train loss: 202.4007,	0.8265 s / batch. (data: 3.03e-04). ETA=10:46:09, max mem: 20.9 GB 
[11/23 17:17:31 visual_prompt]: 	Training 200/553. train loss: 48.7037,	0.8216 s / batch. (data: 9.58e-03). ETA=10:40:55, max mem: 20.9 GB 
[11/23 17:19:07 visual_prompt]: 	Training 300/553. train loss: 37.0103,	0.8280 s / batch. (data: 3.88e-04). ETA=10:44:30, max mem: 20.9 GB 
[11/23 17:20:44 visual_prompt]: 	Training 400/553. train loss: 27.4519,	0.8427 s / batch. (data: 2.49e-02). ETA=10:54:35, max mem: 20.9 GB 
[11/23 17:22:19 visual_prompt]: 	Training 500/553. train loss: 121.7329,	1.4128 s / batch. (data: 5.89e-01). ETA=18:15:03, max mem: 20.9 GB 
[11/23 17:23:10 visual_prompt]: Epoch 16 / 100: avg data time: 1.41e-01, avg batch time: 0.9682, average train loss: 164.7299
[11/23 17:24:06 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3107, average loss: 117.7863
[11/23 17:24:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.32	
[11/23 17:24:06 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/23 17:25:45 visual_prompt]: 	Training 100/553. train loss: 58.3650,	0.8355 s / batch. (data: 1.15e-02). ETA=10:45:25, max mem: 20.9 GB 
[11/23 17:27:23 visual_prompt]: 	Training 200/553. train loss: 227.1207,	0.8368 s / batch. (data: 1.05e-02). ETA=10:45:04, max mem: 20.9 GB 
[11/23 17:28:58 visual_prompt]: 	Training 300/553. train loss: 46.0636,	0.8332 s / batch. (data: 1.49e-02). ETA=10:40:53, max mem: 20.9 GB 
[11/23 17:30:34 visual_prompt]: 	Training 400/553. train loss: 114.2639,	1.1240 s / batch. (data: 2.83e-01). ETA=14:22:42, max mem: 20.9 GB 
[11/23 17:32:10 visual_prompt]: 	Training 500/553. train loss: 87.6181,	1.5366 s / batch. (data: 7.22e-01). ETA=19:36:47, max mem: 20.9 GB 
[11/23 17:33:01 visual_prompt]: Epoch 17 / 100: avg data time: 1.42e-01, avg batch time: 0.9687, average train loss: 168.7049
[11/23 17:33:56 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3085, average loss: 22.6526
[11/23 17:33:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.86	
[11/23 17:33:56 visual_prompt]: Best epoch 17: best metric: -22.653
[11/23 17:33:56 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/23 17:35:37 visual_prompt]: 	Training 100/553. train loss: 131.4120,	0.8284 s / batch. (data: 2.88e-04). ETA=10:32:18, max mem: 20.9 GB 
[11/23 17:37:16 visual_prompt]: 	Training 200/553. train loss: 82.1683,	0.8312 s / batch. (data: 8.80e-04). ETA=10:33:05, max mem: 20.9 GB 
[11/23 17:38:52 visual_prompt]: 	Training 300/553. train loss: 208.3451,	0.8318 s / batch. (data: 3.34e-04). ETA=10:32:10, max mem: 20.9 GB 
[11/23 17:40:29 visual_prompt]: 	Training 400/553. train loss: 118.2526,	0.8385 s / batch. (data: 3.11e-04). ETA=10:35:52, max mem: 20.9 GB 
[11/23 17:42:04 visual_prompt]: 	Training 500/553. train loss: 105.4762,	0.8318 s / batch. (data: 3.11e-04). ETA=10:29:23, max mem: 20.9 GB 
[11/23 17:42:53 visual_prompt]: Epoch 18 / 100: avg data time: 1.44e-01, avg batch time: 0.9706, average train loss: 173.0700
[11/23 17:43:50 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3087, average loss: 234.8130
[11/23 17:43:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.62	
[11/23 17:43:50 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/23 17:45:31 visual_prompt]: 	Training 100/553. train loss: 102.9204,	0.8280 s / batch. (data: 3.08e-04). ETA=10:24:24, max mem: 20.9 GB 
[11/23 17:47:08 visual_prompt]: 	Training 200/553. train loss: 134.7831,	0.8234 s / batch. (data: 2.90e-04). ETA=10:19:35, max mem: 20.9 GB 
[11/23 17:48:44 visual_prompt]: 	Training 300/553. train loss: 185.0444,	0.8400 s / batch. (data: 2.94e-04). ETA=10:30:39, max mem: 20.9 GB 
[11/23 17:50:21 visual_prompt]: 	Training 400/553. train loss: 27.8588,	0.8058 s / batch. (data: 2.96e-04). ETA=10:03:38, max mem: 20.9 GB 
[11/23 17:51:54 visual_prompt]: 	Training 500/553. train loss: 32.7093,	0.8324 s / batch. (data: 5.39e-03). ETA=10:22:07, max mem: 20.9 GB 
[11/23 17:52:44 visual_prompt]: Epoch 19 / 100: avg data time: 1.38e-01, avg batch time: 0.9640, average train loss: 148.0561
[11/23 17:53:38 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.3109, average loss: 392.9309
[11/23 17:53:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.46	
[11/23 17:53:38 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/23 17:55:17 visual_prompt]: 	Training 100/553. train loss: 4.1941,	0.8654 s / batch. (data: 4.93e-02). ETA=10:44:39, max mem: 20.9 GB 
[11/23 17:56:55 visual_prompt]: 	Training 200/553. train loss: 0.8545,	0.8365 s / batch. (data: 2.95e-04). ETA=10:21:41, max mem: 20.9 GB 
[11/23 17:58:31 visual_prompt]: 	Training 300/553. train loss: 68.5057,	0.8440 s / batch. (data: 2.84e-04). ETA=10:25:52, max mem: 20.9 GB 
[11/23 18:00:06 visual_prompt]: 	Training 400/553. train loss: 805.9830,	0.8413 s / batch. (data: 5.41e-03). ETA=10:22:27, max mem: 20.9 GB 
[11/23 18:01:42 visual_prompt]: 	Training 500/553. train loss: 203.7889,	0.8400 s / batch. (data: 2.86e-04). ETA=10:20:06, max mem: 20.9 GB 
[11/23 18:02:33 visual_prompt]: Epoch 20 / 100: avg data time: 1.40e-01, avg batch time: 0.9672, average train loss: 131.3989
[11/23 18:03:28 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3087, average loss: 165.6595
[11/23 18:03:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.91	
[11/23 18:03:28 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/23 18:05:11 visual_prompt]: 	Training 100/553. train loss: 19.3996,	0.8095 s / batch. (data: 3.32e-04). ETA=9:55:31, max mem: 20.9 GB 
[11/23 18:06:46 visual_prompt]: 	Training 200/553. train loss: 379.2604,	0.8194 s / batch. (data: 5.35e-03). ETA=10:01:28, max mem: 20.9 GB 
[11/23 18:08:21 visual_prompt]: 	Training 300/553. train loss: 641.3419,	0.9480 s / batch. (data: 1.17e-01). ETA=11:34:16, max mem: 20.9 GB 
[11/23 18:09:57 visual_prompt]: 	Training 400/553. train loss: 61.0983,	0.8372 s / batch. (data: 2.81e-04). ETA=10:11:40, max mem: 20.9 GB 
[11/23 18:11:34 visual_prompt]: 	Training 500/553. train loss: 171.5970,	0.8280 s / batch. (data: 2.96e-04). ETA=10:03:38, max mem: 20.9 GB 
[11/23 18:12:23 visual_prompt]: Epoch 21 / 100: avg data time: 1.40e-01, avg batch time: 0.9670, average train loss: 151.6709
[11/23 18:13:18 visual_prompt]: Inference (val):avg data time: 5.84e-04, avg batch time: 0.3112, average loss: 111.4839
[11/23 18:13:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.89	
[11/23 18:13:18 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/23 18:14:58 visual_prompt]: 	Training 100/553. train loss: 122.2994,	0.8274 s / batch. (data: 7.57e-03). ETA=10:01:02, max mem: 20.9 GB 
[11/23 18:16:34 visual_prompt]: 	Training 200/553. train loss: 80.7615,	0.8360 s / batch. (data: 2.82e-04). ETA=10:05:54, max mem: 20.9 GB 
[11/23 18:18:08 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8320 s / batch. (data: 3.07e-04). ETA=10:01:38, max mem: 20.9 GB 
[11/23 18:19:46 visual_prompt]: 	Training 400/553. train loss: 179.3458,	0.8557 s / batch. (data: 7.69e-03). ETA=10:17:21, max mem: 20.9 GB 
[11/23 18:21:23 visual_prompt]: 	Training 500/553. train loss: 114.1174,	0.8059 s / batch. (data: 3.11e-04). ETA=9:40:02, max mem: 20.9 GB 
[11/23 18:22:15 visual_prompt]: Epoch 22 / 100: avg data time: 1.43e-01, avg batch time: 0.9702, average train loss: 152.8411
[11/23 18:23:10 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3101, average loss: 75.6044
[11/23 18:23:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.51	
[11/23 18:23:10 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/23 18:24:53 visual_prompt]: 	Training 100/553. train loss: 238.0629,	0.8140 s / batch. (data: 3.19e-04). ETA=9:43:50, max mem: 20.9 GB 
[11/23 18:26:29 visual_prompt]: 	Training 200/553. train loss: 108.5072,	0.8360 s / batch. (data: 1.60e-02). ETA=9:58:13, max mem: 20.9 GB 
[11/23 18:28:08 visual_prompt]: 	Training 300/553. train loss: 15.3152,	0.8535 s / batch. (data: 7.83e-04). ETA=10:09:16, max mem: 20.9 GB 
[11/23 18:29:42 visual_prompt]: 	Training 400/553. train loss: 370.6310,	0.8455 s / batch. (data: 2.86e-04). ETA=10:02:09, max mem: 20.9 GB 
[11/23 18:31:16 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8240 s / batch. (data: 2.86e-04). ETA=9:45:30, max mem: 20.9 GB 
[11/23 18:32:07 visual_prompt]: Epoch 23 / 100: avg data time: 1.44e-01, avg batch time: 0.9707, average train loss: 164.8058
[11/23 18:33:02 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3092, average loss: 19.6481
[11/23 18:33:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.39	
[11/23 18:33:02 visual_prompt]: Best epoch 23: best metric: -19.648
[11/23 18:33:02 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/23 18:34:39 visual_prompt]: 	Training 100/553. train loss: 220.7708,	0.8240 s / batch. (data: 7.95e-03). ETA=9:43:24, max mem: 20.9 GB 
[11/23 18:36:15 visual_prompt]: 	Training 200/553. train loss: 57.7030,	0.8497 s / batch. (data: 1.20e-02). ETA=10:00:11, max mem: 20.9 GB 
[11/23 18:37:52 visual_prompt]: 	Training 300/553. train loss: 165.2409,	0.8177 s / batch. (data: 8.45e-03). ETA=9:36:14, max mem: 20.9 GB 
[11/23 18:39:30 visual_prompt]: 	Training 400/553. train loss: 54.8669,	0.8423 s / batch. (data: 1.03e-02). ETA=9:52:09, max mem: 20.9 GB 
[11/23 18:41:09 visual_prompt]: 	Training 500/553. train loss: 233.0795,	0.8370 s / batch. (data: 5.41e-03). ETA=9:47:01, max mem: 20.9 GB 
[11/23 18:42:00 visual_prompt]: Epoch 24 / 100: avg data time: 1.46e-01, avg batch time: 0.9728, average train loss: 146.3013
[11/23 18:42:55 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.3095, average loss: 171.0182
[11/23 18:42:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.56	
[11/23 18:42:55 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/23 18:44:40 visual_prompt]: 	Training 100/553. train loss: 15.2874,	0.8351 s / batch. (data: 3.01e-04). ETA=9:43:32, max mem: 20.9 GB 
[11/23 18:46:14 visual_prompt]: 	Training 200/553. train loss: 186.4530,	0.8361 s / batch. (data: 1.42e-02). ETA=9:42:52, max mem: 20.9 GB 
[11/23 18:47:50 visual_prompt]: 	Training 300/553. train loss: 25.8697,	0.8397 s / batch. (data: 3.17e-04). ETA=9:43:58, max mem: 20.9 GB 
[11/23 18:49:27 visual_prompt]: 	Training 400/553. train loss: 33.5844,	1.1930 s / batch. (data: 3.68e-01). ETA=13:47:42, max mem: 20.9 GB 
[11/23 18:51:04 visual_prompt]: 	Training 500/553. train loss: 120.7112,	1.4435 s / batch. (data: 6.15e-01). ETA=16:39:04, max mem: 20.9 GB 
[11/23 18:51:55 visual_prompt]: Epoch 25 / 100: avg data time: 1.50e-01, avg batch time: 0.9761, average train loss: 152.1037
[11/23 18:52:50 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3102, average loss: 209.1437
[11/23 18:52:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.07	
[11/23 18:52:50 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/23 18:54:32 visual_prompt]: 	Training 100/553. train loss: 83.1610,	0.8267 s / batch. (data: 9.58e-03). ETA=9:30:05, max mem: 20.9 GB 
[11/23 18:56:10 visual_prompt]: 	Training 200/553. train loss: 598.5208,	1.5891 s / batch. (data: 7.75e-01). ETA=18:13:08, max mem: 20.9 GB 
[11/23 18:57:48 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8154 s / batch. (data: 8.06e-04). ETA=9:19:32, max mem: 20.9 GB 
[11/23 18:59:24 visual_prompt]: 	Training 400/553. train loss: 457.4002,	0.8248 s / batch. (data: 4.28e-04). ETA=9:24:39, max mem: 20.9 GB 
[11/23 19:01:00 visual_prompt]: 	Training 500/553. train loss: 34.4658,	0.8480 s / batch. (data: 8.08e-04). ETA=9:39:06, max mem: 20.9 GB 
[11/23 19:01:50 visual_prompt]: Epoch 26 / 100: avg data time: 1.49e-01, avg batch time: 0.9759, average train loss: 149.0037
[11/23 19:02:46 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3097, average loss: 44.7650
[11/23 19:02:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.99	
[11/23 19:02:46 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/23 19:04:28 visual_prompt]: 	Training 100/553. train loss: 14.7403,	0.8605 s / batch. (data: 4.68e-04). ETA=9:45:25, max mem: 20.9 GB 
[11/23 19:06:04 visual_prompt]: 	Training 200/553. train loss: 319.8436,	1.0278 s / batch. (data: 1.95e-01). ETA=11:37:33, max mem: 20.9 GB 
[11/23 19:07:41 visual_prompt]: 	Training 300/553. train loss: 199.6077,	0.8360 s / batch. (data: 7.95e-03). ETA=9:25:59, max mem: 20.9 GB 
[11/23 19:09:20 visual_prompt]: 	Training 400/553. train loss: 0.2961,	0.8590 s / batch. (data: 5.91e-03). ETA=9:40:07, max mem: 20.9 GB 
[11/23 19:10:57 visual_prompt]: 	Training 500/553. train loss: 63.2854,	0.8280 s / batch. (data: 1.20e-02). ETA=9:17:48, max mem: 20.9 GB 
[11/23 19:11:46 visual_prompt]: Epoch 27 / 100: avg data time: 1.49e-01, avg batch time: 0.9762, average train loss: 175.4549
[11/23 19:12:41 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3088, average loss: 296.2717
[11/23 19:12:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 34.15	
[11/23 19:12:41 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/23 19:14:21 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8129 s / batch. (data: 3.05e-04). ETA=9:05:34, max mem: 20.9 GB 
[11/23 19:15:59 visual_prompt]: 	Training 200/553. train loss: 147.5573,	0.8271 s / batch. (data: 3.12e-04). ETA=9:13:45, max mem: 20.9 GB 
[11/23 19:17:37 visual_prompt]: 	Training 300/553. train loss: 39.2931,	1.2597 s / batch. (data: 4.55e-01). ETA=14:01:15, max mem: 20.9 GB 
[11/23 19:19:11 visual_prompt]: 	Training 400/553. train loss: 13.6634,	0.8292 s / batch. (data: 3.11e-04). ETA=9:12:20, max mem: 20.9 GB 
[11/23 19:20:46 visual_prompt]: 	Training 500/553. train loss: 440.2202,	0.8240 s / batch. (data: 5.42e-03). ETA=9:07:33, max mem: 20.9 GB 
[11/23 19:21:36 visual_prompt]: Epoch 28 / 100: avg data time: 1.41e-01, avg batch time: 0.9678, average train loss: 158.8234
[11/23 19:22:31 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3120, average loss: 254.6606
[11/23 19:22:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.74	
[11/23 19:22:31 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/23 19:24:17 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8250 s / batch. (data: 2.91e-04). ETA=9:06:04, max mem: 20.9 GB 
[11/23 19:25:53 visual_prompt]: 	Training 200/553. train loss: 346.1683,	1.6812 s / batch. (data: 8.65e-01). ETA=18:30:01, max mem: 20.9 GB 
[11/23 19:27:26 visual_prompt]: 	Training 300/553. train loss: 34.8636,	0.8499 s / batch. (data: 6.87e-04). ETA=9:19:44, max mem: 20.9 GB 
[11/23 19:28:59 visual_prompt]: 	Training 400/553. train loss: 364.2102,	1.2079 s / batch. (data: 3.83e-01). ETA=13:13:30, max mem: 20.9 GB 
[11/23 19:30:35 visual_prompt]: 	Training 500/553. train loss: 46.9617,	0.8280 s / batch. (data: 3.15e-04). ETA=9:02:34, max mem: 20.9 GB 
[11/23 19:31:25 visual_prompt]: Epoch 29 / 100: avg data time: 1.38e-01, avg batch time: 0.9652, average train loss: 139.2024
[11/23 19:32:20 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3102, average loss: 19.5667
[11/23 19:32:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.65	
[11/23 19:32:20 visual_prompt]: Best epoch 29: best metric: -19.567
[11/23 19:32:20 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/23 19:33:58 visual_prompt]: 	Training 100/553. train loss: 154.6503,	0.8600 s / batch. (data: 7.96e-03). ETA=9:21:19, max mem: 20.9 GB 
[11/23 19:35:36 visual_prompt]: 	Training 200/553. train loss: 16.3775,	0.8250 s / batch. (data: 3.19e-03). ETA=8:57:07, max mem: 20.9 GB 
[11/23 19:37:10 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.3640 s / batch. (data: 5.31e-01). ETA=14:45:44, max mem: 20.9 GB 
[11/23 19:38:49 visual_prompt]: 	Training 400/553. train loss: 98.6266,	1.0874 s / batch. (data: 2.71e-01). ETA=11:44:18, max mem: 20.9 GB 
[11/23 19:40:25 visual_prompt]: 	Training 500/553. train loss: 48.5852,	1.3840 s / batch. (data: 5.34e-01). ETA=14:54:07, max mem: 20.9 GB 
[11/23 19:41:17 visual_prompt]: Epoch 30 / 100: avg data time: 1.44e-01, avg batch time: 0.9716, average train loss: 140.6919
[11/23 19:42:13 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3088, average loss: 85.6259
[11/23 19:42:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.40	
[11/23 19:42:13 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[11/23 19:43:54 visual_prompt]: 	Training 100/553. train loss: 122.3913,	0.8320 s / batch. (data: 3.37e-04). ETA=8:55:23, max mem: 20.9 GB 
[11/23 19:45:33 visual_prompt]: 	Training 200/553. train loss: 31.0906,	0.8412 s / batch. (data: 2.78e-04). ETA=8:59:55, max mem: 20.9 GB 
[11/23 19:47:06 visual_prompt]: 	Training 300/553. train loss: 229.8587,	0.8280 s / batch. (data: 3.21e-04). ETA=8:50:02, max mem: 20.9 GB 
[11/23 19:48:41 visual_prompt]: 	Training 400/553. train loss: 157.4476,	1.0440 s / batch. (data: 2.16e-01). ETA=11:06:35, max mem: 20.9 GB 
[11/23 19:50:17 visual_prompt]: 	Training 500/553. train loss: 97.8932,	0.8173 s / batch. (data: 3.13e-04). ETA=8:40:29, max mem: 20.9 GB 
[11/23 19:51:06 visual_prompt]: Epoch 31 / 100: avg data time: 1.37e-01, avg batch time: 0.9642, average train loss: 142.1276
[11/23 19:52:01 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3103, average loss: 81.8708
[11/23 19:52:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.45	
[11/23 19:52:01 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[11/23 19:53:42 visual_prompt]: 	Training 100/553. train loss: 165.4758,	0.8480 s / batch. (data: 7.57e-04). ETA=8:57:52, max mem: 20.9 GB 
[11/23 19:55:17 visual_prompt]: 	Training 200/553. train loss: 15.5328,	0.8300 s / batch. (data: 7.50e-04). ETA=8:45:05, max mem: 20.9 GB 
[11/23 19:56:56 visual_prompt]: 	Training 300/553. train loss: 174.6999,	0.8390 s / batch. (data: 1.05e-02). ETA=8:49:23, max mem: 20.9 GB 
[11/23 19:58:32 visual_prompt]: 	Training 400/553. train loss: 54.7532,	0.8320 s / batch. (data: 3.01e-04). ETA=8:43:33, max mem: 20.9 GB 
[11/23 20:00:05 visual_prompt]: 	Training 500/553. train loss: 37.7764,	0.8355 s / batch. (data: 2.93e-04). ETA=8:44:21, max mem: 20.9 GB 
[11/23 20:00:54 visual_prompt]: Epoch 32 / 100: avg data time: 1.36e-01, avg batch time: 0.9638, average train loss: 139.3340
[11/23 20:01:49 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3107, average loss: 19.7126
[11/23 20:01:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.32	
[11/23 20:01:49 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[11/23 20:03:29 visual_prompt]: 	Training 100/553. train loss: 4.8962,	0.8520 s / batch. (data: 3.28e-04). ETA=8:52:33, max mem: 20.9 GB 
[11/23 20:05:07 visual_prompt]: 	Training 200/553. train loss: 18.0701,	0.9720 s / batch. (data: 1.35e-01). ETA=10:05:56, max mem: 20.9 GB 
[11/23 20:06:44 visual_prompt]: 	Training 300/553. train loss: 210.4590,	0.8455 s / batch. (data: 2.45e-04). ETA=8:45:41, max mem: 20.9 GB 
[11/23 20:08:22 visual_prompt]: 	Training 400/553. train loss: 330.1152,	0.8283 s / batch. (data: 3.32e-04). ETA=8:33:37, max mem: 20.9 GB 
[11/23 20:09:58 visual_prompt]: 	Training 500/553. train loss: 507.6531,	0.8245 s / batch. (data: 1.05e-02). ETA=8:29:50, max mem: 20.9 GB 
[11/23 20:10:48 visual_prompt]: Epoch 33 / 100: avg data time: 1.45e-01, avg batch time: 0.9733, average train loss: 143.6312
[11/23 20:11:43 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3093, average loss: 127.1067
[11/23 20:11:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.03	
[11/23 20:11:43 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[11/23 20:13:25 visual_prompt]: 	Training 100/553. train loss: 19.8462,	0.8309 s / batch. (data: 3.31e-04). ETA=8:31:41, max mem: 20.9 GB 
[11/23 20:15:01 visual_prompt]: 	Training 200/553. train loss: 18.9643,	0.8496 s / batch. (data: 5.43e-03). ETA=8:41:48, max mem: 20.9 GB 
[11/23 20:16:37 visual_prompt]: 	Training 300/553. train loss: 309.5136,	0.8383 s / batch. (data: 5.41e-03). ETA=8:33:28, max mem: 20.9 GB 
[11/23 20:18:15 visual_prompt]: 	Training 400/553. train loss: 8.1546,	0.8329 s / batch. (data: 3.45e-04). ETA=8:28:47, max mem: 20.9 GB 
[11/23 20:19:53 visual_prompt]: 	Training 500/553. train loss: 112.1696,	1.4725 s / batch. (data: 6.44e-01). ETA=14:57:01, max mem: 20.9 GB 
[11/23 20:20:43 visual_prompt]: Epoch 34 / 100: avg data time: 1.50e-01, avg batch time: 0.9760, average train loss: 149.7802
[11/23 20:21:38 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3113, average loss: 111.8793
[11/23 20:21:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.49	
[11/23 20:21:38 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[11/23 20:23:21 visual_prompt]: 	Training 100/553. train loss: 103.1113,	0.8429 s / batch. (data: 1.23e-02). ETA=8:31:18, max mem: 20.9 GB 
[11/23 20:25:00 visual_prompt]: 	Training 200/553. train loss: 27.2771,	0.8501 s / batch. (data: 3.76e-04). ETA=8:34:15, max mem: 20.9 GB 
[11/23 20:26:35 visual_prompt]: 	Training 300/553. train loss: 37.5424,	0.8615 s / batch. (data: 2.57e-02). ETA=8:39:46, max mem: 20.9 GB 
[11/23 20:28:11 visual_prompt]: 	Training 400/553. train loss: 14.9664,	0.8360 s / batch. (data: 2.95e-04). ETA=8:22:58, max mem: 20.9 GB 
[11/23 20:29:47 visual_prompt]: 	Training 500/553. train loss: 55.5980,	0.9902 s / batch. (data: 1.81e-01). ETA=9:54:05, max mem: 20.9 GB 
[11/23 20:30:38 visual_prompt]: Epoch 35 / 100: avg data time: 1.49e-01, avg batch time: 0.9763, average train loss: 149.4510
[11/23 20:31:34 visual_prompt]: Inference (val):avg data time: 2.84e-04, avg batch time: 0.3104, average loss: 265.0159
[11/23 20:31:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.16	
[11/23 20:31:34 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[11/23 20:33:14 visual_prompt]: 	Training 100/553. train loss: 115.7175,	0.8139 s / batch. (data: 1.06e-02). ETA=8:06:15, max mem: 20.9 GB 
[11/23 20:34:52 visual_prompt]: 	Training 200/553. train loss: 506.0617,	0.8400 s / batch. (data: 1.20e-02). ETA=8:20:24, max mem: 20.9 GB 
[11/23 20:36:31 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8169 s / batch. (data: 5.44e-03). ETA=8:05:18, max mem: 20.9 GB 
[11/23 20:38:07 visual_prompt]: 	Training 400/553. train loss: 23.3570,	0.8342 s / batch. (data: 3.04e-04). ETA=8:14:10, max mem: 20.9 GB 
[11/23 20:39:45 visual_prompt]: 	Training 500/553. train loss: 290.1146,	0.8419 s / batch. (data: 3.08e-04). ETA=8:17:21, max mem: 20.9 GB 
[11/23 20:40:33 visual_prompt]: Epoch 36 / 100: avg data time: 1.48e-01, avg batch time: 0.9748, average train loss: 140.2568
[11/23 20:41:28 visual_prompt]: Inference (val):avg data time: 2.28e-04, avg batch time: 0.3106, average loss: 111.5739
[11/23 20:41:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.40	
[11/23 20:41:28 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[11/23 20:43:09 visual_prompt]: 	Training 100/553. train loss: 141.5817,	0.8134 s / batch. (data: 3.13e-04). ETA=7:58:25, max mem: 20.9 GB 
[11/23 20:44:47 visual_prompt]: 	Training 200/553. train loss: 168.3323,	0.8455 s / batch. (data: 3.02e-04). ETA=8:15:55, max mem: 20.9 GB 
[11/23 20:46:24 visual_prompt]: 	Training 300/553. train loss: 281.6743,	1.2520 s / batch. (data: 4.01e-01). ETA=12:12:15, max mem: 20.9 GB 
[11/23 20:48:04 visual_prompt]: 	Training 400/553. train loss: 3.9071,	1.8665 s / batch. (data: 1.04e+00). ETA=18:08:32, max mem: 20.9 GB 
[11/23 20:49:37 visual_prompt]: 	Training 500/553. train loss: 34.7462,	0.9944 s / batch. (data: 1.66e-01). ETA=9:38:15, max mem: 20.9 GB 
[11/23 20:50:30 visual_prompt]: Epoch 37 / 100: avg data time: 1.52e-01, avg batch time: 0.9784, average train loss: 140.7057
[11/23 20:51:25 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3116, average loss: 52.1396
[11/23 20:51:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.22	
[11/23 20:51:25 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[11/23 20:53:04 visual_prompt]: 	Training 100/553. train loss: 6.9585,	0.8279 s / batch. (data: 5.44e-03). ETA=7:59:21, max mem: 20.9 GB 
[11/23 20:54:42 visual_prompt]: 	Training 200/553. train loss: 107.9177,	1.1703 s / batch. (data: 3.26e-01). ETA=11:15:37, max mem: 20.9 GB 
[11/23 20:56:20 visual_prompt]: 	Training 300/553. train loss: 33.5977,	0.8240 s / batch. (data: 3.09e-04). ETA=7:54:20, max mem: 20.9 GB 
[11/23 20:57:55 visual_prompt]: 	Training 400/553. train loss: 126.3971,	0.8210 s / batch. (data: 3.11e-04). ETA=7:51:13, max mem: 20.9 GB 
[11/23 20:59:34 visual_prompt]: 	Training 500/553. train loss: 5.5677,	0.8318 s / batch. (data: 3.27e-04). ETA=7:56:03, max mem: 20.9 GB 
[11/23 21:00:24 visual_prompt]: Epoch 38 / 100: avg data time: 1.47e-01, avg batch time: 0.9737, average train loss: 140.6521
[11/23 21:01:19 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3084, average loss: 276.6873
[11/23 21:01:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.70	
[11/23 21:01:19 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[11/23 21:02:58 visual_prompt]: 	Training 100/553. train loss: 0.0002,	0.8264 s / batch. (data: 9.41e-03). ETA=7:50:52, max mem: 20.9 GB 
[11/23 21:04:39 visual_prompt]: 	Training 200/553. train loss: 413.8148,	0.8664 s / batch. (data: 3.84e-02). ETA=8:12:13, max mem: 20.9 GB 
[11/23 21:06:19 visual_prompt]: 	Training 300/553. train loss: 124.1934,	0.8440 s / batch. (data: 7.96e-03). ETA=7:58:04, max mem: 20.9 GB 
[11/23 21:07:54 visual_prompt]: 	Training 400/553. train loss: 115.5211,	1.2268 s / batch. (data: 4.21e-01). ETA=11:32:51, max mem: 20.9 GB 
[11/23 21:09:30 visual_prompt]: 	Training 500/553. train loss: 40.5285,	1.7160 s / batch. (data: 8.60e-01). ETA=16:06:16, max mem: 20.9 GB 
[11/23 21:10:19 visual_prompt]: Epoch 39 / 100: avg data time: 1.49e-01, avg batch time: 0.9764, average train loss: 108.4750
[11/23 21:11:14 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3099, average loss: 209.0526
[11/23 21:11:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.69	
[11/23 21:11:14 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[11/23 21:12:56 visual_prompt]: 	Training 100/553. train loss: 375.7561,	0.8280 s / batch. (data: 2.93e-04). ETA=7:44:07, max mem: 20.9 GB 
[11/23 21:14:32 visual_prompt]: 	Training 200/553. train loss: 3.0492,	0.8440 s / batch. (data: 7.95e-03). ETA=7:51:41, max mem: 20.9 GB 
[11/23 21:16:11 visual_prompt]: 	Training 300/553. train loss: 100.3886,	0.8321 s / batch. (data: 7.87e-04). ETA=7:43:40, max mem: 20.9 GB 
[11/23 21:17:48 visual_prompt]: 	Training 400/553. train loss: 117.8543,	0.8199 s / batch. (data: 1.05e-02). ETA=7:35:28, max mem: 20.9 GB 
[11/23 21:19:24 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8120 s / batch. (data: 3.15e-04). ETA=7:29:46, max mem: 20.9 GB 
[11/23 21:20:16 visual_prompt]: Epoch 40 / 100: avg data time: 1.54e-01, avg batch time: 0.9799, average train loss: 132.2321
[11/23 21:21:12 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3105, average loss: 34.4287
[11/23 21:21:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.58	
[11/23 21:21:12 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[11/23 21:22:57 visual_prompt]: 	Training 100/553. train loss: 74.8127,	0.8402 s / batch. (data: 3.11e-04). ETA=7:43:12, max mem: 20.9 GB 
[11/23 21:24:36 visual_prompt]: 	Training 200/553. train loss: 32.2467,	0.8094 s / batch. (data: 7.10e-04). ETA=7:24:53, max mem: 20.9 GB 
[11/23 21:26:12 visual_prompt]: 	Training 300/553. train loss: 170.9885,	0.8296 s / batch. (data: 4.06e-04). ETA=7:34:35, max mem: 20.9 GB 
[11/23 21:27:48 visual_prompt]: 	Training 400/553. train loss: 106.4628,	0.8228 s / batch. (data: 2.68e-04). ETA=7:29:30, max mem: 20.9 GB 
[11/23 21:29:23 visual_prompt]: 	Training 500/553. train loss: 99.3491,	0.8160 s / batch. (data: 3.04e-04). ETA=7:24:26, max mem: 20.9 GB 
[11/23 21:30:11 visual_prompt]: Epoch 41 / 100: avg data time: 1.47e-01, avg batch time: 0.9744, average train loss: 141.9813
[11/23 21:31:07 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3122, average loss: 86.1727
[11/23 21:31:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.03	
[11/23 21:31:07 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[11/23 21:32:46 visual_prompt]: 	Training 100/553. train loss: 261.2810,	0.8201 s / batch. (data: 2.97e-04). ETA=7:24:35, max mem: 20.9 GB 
[11/23 21:34:23 visual_prompt]: 	Training 200/553. train loss: 1197.9874,	0.8474 s / batch. (data: 3.14e-04). ETA=7:37:59, max mem: 20.9 GB 
[11/23 21:36:01 visual_prompt]: 	Training 300/553. train loss: 27.7327,	0.8440 s / batch. (data: 3.12e-04). ETA=7:34:44, max mem: 20.9 GB 
[11/23 21:37:38 visual_prompt]: 	Training 400/553. train loss: 58.5433,	0.8551 s / batch. (data: 2.98e-04). ETA=7:39:16, max mem: 20.9 GB 
[11/23 21:39:14 visual_prompt]: 	Training 500/553. train loss: 0.5195,	0.8318 s / batch. (data: 1.41e-02). ETA=7:25:22, max mem: 20.9 GB 
[11/23 21:40:06 visual_prompt]: Epoch 42 / 100: avg data time: 1.48e-01, avg batch time: 0.9751, average train loss: 173.6028
[11/23 21:41:01 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3108, average loss: 80.3345
[11/23 21:41:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.94	
[11/23 21:41:01 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[11/23 21:42:44 visual_prompt]: 	Training 100/553. train loss: 210.0464,	0.8290 s / batch. (data: 2.96e-04). ETA=7:21:46, max mem: 20.9 GB 
[11/23 21:44:20 visual_prompt]: 	Training 200/553. train loss: 132.1355,	0.8341 s / batch. (data: 6.02e-03). ETA=7:23:04, max mem: 20.9 GB 
[11/23 21:45:56 visual_prompt]: 	Training 300/553. train loss: 264.3711,	0.8440 s / batch. (data: 3.00e-04). ETA=7:26:57, max mem: 20.9 GB 
[11/23 21:47:31 visual_prompt]: 	Training 400/553. train loss: 103.4724,	0.8395 s / batch. (data: 5.41e-03). ETA=7:23:09, max mem: 20.9 GB 
[11/23 21:49:10 visual_prompt]: 	Training 500/553. train loss: 181.9322,	0.8476 s / batch. (data: 1.05e-02). ETA=7:26:02, max mem: 20.9 GB 
[11/23 21:50:01 visual_prompt]: Epoch 43 / 100: avg data time: 1.50e-01, avg batch time: 0.9764, average train loss: 111.9246
[11/23 21:50:57 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3088, average loss: 79.2636
[11/23 21:50:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.53	
[11/23 21:50:57 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[11/23 21:52:38 visual_prompt]: 	Training 100/553. train loss: 351.5696,	0.8258 s / batch. (data: 2.19e-02). ETA=7:12:27, max mem: 20.9 GB 
[11/23 21:54:16 visual_prompt]: 	Training 200/553. train loss: 40.4058,	0.8454 s / batch. (data: 2.14e-02). ETA=7:21:18, max mem: 20.9 GB 
[11/23 21:55:51 visual_prompt]: 	Training 300/553. train loss: 184.5063,	0.8353 s / batch. (data: 1.13e-02). ETA=7:14:40, max mem: 20.9 GB 
[11/23 21:57:26 visual_prompt]: 	Training 400/553. train loss: 34.6635,	0.8199 s / batch. (data: 2.98e-04). ETA=7:05:17, max mem: 20.9 GB 
[11/23 21:59:02 visual_prompt]: 	Training 500/553. train loss: 3.6811,	0.8277 s / batch. (data: 4.41e-04). ETA=7:07:57, max mem: 20.9 GB 
[11/23 21:59:52 visual_prompt]: Epoch 44 / 100: avg data time: 1.41e-01, avg batch time: 0.9678, average train loss: 126.6946
[11/23 22:00:47 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3107, average loss: 172.6289
[11/23 22:00:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 55.42	
[11/23 22:00:47 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[11/23 22:02:28 visual_prompt]: 	Training 100/553. train loss: 4.7575,	0.8145 s / batch. (data: 3.12e-04). ETA=6:59:02, max mem: 20.9 GB 
[11/23 22:04:00 visual_prompt]: 	Training 200/553. train loss: 76.7605,	0.8115 s / batch. (data: 5.45e-03). ETA=6:56:08, max mem: 20.9 GB 
[11/23 22:05:38 visual_prompt]: 	Training 300/553. train loss: 517.3029,	0.8550 s / batch. (data: 1.89e-02). ETA=7:17:00, max mem: 20.9 GB 
[11/23 22:07:11 visual_prompt]: 	Training 400/553. train loss: 17.2243,	0.8360 s / batch. (data: 3.34e-04). ETA=7:05:53, max mem: 20.9 GB 
[11/23 22:08:50 visual_prompt]: 	Training 500/553. train loss: 47.1717,	0.8207 s / batch. (data: 1.05e-02). ETA=6:56:45, max mem: 20.9 GB 
[11/23 22:09:41 visual_prompt]: Epoch 45 / 100: avg data time: 1.37e-01, avg batch time: 0.9654, average train loss: 104.8145
[11/23 22:10:36 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3095, average loss: 39.0389
[11/23 22:10:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.39	
[11/23 22:10:36 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[11/23 22:12:17 visual_prompt]: 	Training 100/553. train loss: 97.2375,	0.8593 s / batch. (data: 2.45e-02). ETA=7:14:09, max mem: 20.9 GB 
[11/23 22:13:55 visual_prompt]: 	Training 200/553. train loss: 1137.4886,	0.8599 s / batch. (data: 1.10e-02). ETA=7:13:00, max mem: 20.9 GB 
[11/23 22:15:31 visual_prompt]: 	Training 300/553. train loss: 211.1017,	0.8169 s / batch. (data: 5.45e-03). ETA=6:50:01, max mem: 20.9 GB 
[11/23 22:17:09 visual_prompt]: 	Training 400/553. train loss: 21.3189,	0.8335 s / batch. (data: 7.81e-04). ETA=6:56:57, max mem: 20.9 GB 
[11/23 22:18:43 visual_prompt]: 	Training 500/553. train loss: 243.4124,	0.8400 s / batch. (data: 3.19e-04). ETA=6:58:48, max mem: 20.9 GB 
[11/23 22:19:35 visual_prompt]: Epoch 46 / 100: avg data time: 1.48e-01, avg batch time: 0.9749, average train loss: 112.3867
[11/23 22:20:30 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.3118, average loss: 77.4649
[11/23 22:20:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.69	
[11/23 22:20:30 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[11/23 22:22:12 visual_prompt]: 	Training 100/553. train loss: 54.7013,	0.8240 s / batch. (data: 2.95e-04). ETA=6:48:44, max mem: 20.9 GB 
[11/23 22:23:45 visual_prompt]: 	Training 200/553. train loss: 237.6187,	1.2520 s / batch. (data: 4.08e-01). ETA=10:18:56, max mem: 20.9 GB 
[11/23 22:25:21 visual_prompt]: 	Training 300/553. train loss: 5.9041,	0.8520 s / batch. (data: 1.41e-02). ETA=6:59:46, max mem: 20.9 GB 
[11/23 22:26:57 visual_prompt]: 	Training 400/553. train loss: 57.4155,	0.8433 s / batch. (data: 8.29e-04). ETA=6:54:05, max mem: 20.9 GB 
[11/23 22:28:31 visual_prompt]: 	Training 500/553. train loss: 181.2313,	0.8200 s / batch. (data: 2.89e-04). ETA=6:41:17, max mem: 20.9 GB 
[11/23 22:29:22 visual_prompt]: Epoch 47 / 100: avg data time: 1.34e-01, avg batch time: 0.9608, average train loss: 102.7517
[11/23 22:30:16 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3102, average loss: 113.9753
[11/23 22:30:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.59	
[11/23 22:30:16 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[11/23 22:31:56 visual_prompt]: 	Training 100/553. train loss: 101.8445,	0.8341 s / batch. (data: 2.06e-02). ETA=6:46:03, max mem: 20.9 GB 
[11/23 22:33:32 visual_prompt]: 	Training 200/553. train loss: 83.2641,	0.8548 s / batch. (data: 1.05e-02). ETA=6:54:41, max mem: 20.9 GB 
[11/23 22:35:09 visual_prompt]: 	Training 300/553. train loss: 197.4628,	1.5880 s / batch. (data: 7.32e-01). ETA=12:47:45, max mem: 20.9 GB 
[11/23 22:36:42 visual_prompt]: 	Training 400/553. train loss: 575.2391,	0.8368 s / batch. (data: 7.96e-03). ETA=6:43:10, max mem: 20.9 GB 
[11/23 22:38:18 visual_prompt]: 	Training 500/553. train loss: 41.3837,	0.8271 s / batch. (data: 1.60e-02). ETA=6:37:07, max mem: 20.9 GB 
[11/23 22:39:08 visual_prompt]: Epoch 48 / 100: avg data time: 1.36e-01, avg batch time: 0.9627, average train loss: 124.2199
[11/23 22:40:03 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3079, average loss: 174.4985
[11/23 22:40:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.39	
[11/23 22:40:03 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[11/23 22:41:43 visual_prompt]: 	Training 100/553. train loss: 109.8831,	0.8359 s / batch. (data: 1.50e-02). ETA=6:39:12, max mem: 20.9 GB 
[11/23 22:43:18 visual_prompt]: 	Training 200/553. train loss: 28.6203,	0.8425 s / batch. (data: 2.85e-04). ETA=6:40:58, max mem: 20.9 GB 
[11/23 22:44:55 visual_prompt]: 	Training 300/553. train loss: 182.3253,	0.8320 s / batch. (data: 2.77e-04). ETA=6:34:35, max mem: 20.9 GB 
[11/23 22:46:33 visual_prompt]: 	Training 400/553. train loss: 143.3421,	0.8216 s / batch. (data: 7.87e-04). ETA=6:28:17, max mem: 20.9 GB 
[11/23 22:48:09 visual_prompt]: 	Training 500/553. train loss: 57.9728,	0.8281 s / batch. (data: 2.72e-04). ETA=6:29:58, max mem: 20.9 GB 
[11/23 22:48:59 visual_prompt]: Epoch 49 / 100: avg data time: 1.42e-01, avg batch time: 0.9690, average train loss: 106.7669
[11/23 22:49:54 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3094, average loss: 70.2803
[11/23 22:49:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.07	
[11/23 22:49:54 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[11/23 22:51:35 visual_prompt]: 	Training 100/553. train loss: 135.8259,	0.8240 s / batch. (data: 3.04e-04). ETA=6:25:56, max mem: 20.9 GB 
[11/23 22:53:12 visual_prompt]: 	Training 200/553. train loss: 353.7992,	0.8201 s / batch. (data: 3.02e-04). ETA=6:22:44, max mem: 20.9 GB 
[11/23 22:54:47 visual_prompt]: 	Training 300/553. train loss: 450.7252,	0.8440 s / batch. (data: 7.66e-04). ETA=6:32:29, max mem: 20.9 GB 
[11/23 22:56:21 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8441 s / batch. (data: 7.98e-03). ETA=6:31:07, max mem: 20.9 GB 
[11/23 22:57:58 visual_prompt]: 	Training 500/553. train loss: 85.6277,	0.8320 s / batch. (data: 3.04e-04). ETA=6:24:08, max mem: 20.9 GB 
[11/23 22:58:48 visual_prompt]: Epoch 50 / 100: avg data time: 1.39e-01, avg batch time: 0.9658, average train loss: 127.9936
[11/23 22:59:43 visual_prompt]: Inference (val):avg data time: 7.06e-05, avg batch time: 0.3088, average loss: 134.5663
[11/23 22:59:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.63	
[11/23 22:59:43 visual_prompt]: Stopping early.
[11/23 22:59:43 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 22:59:43 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 22:59:43 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/23 22:59:43 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 22:59:43 visual_prompt]: Training with config:
[11/23 22:59:43 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/23 22:59:43 visual_prompt]: Loading training data...
[11/23 22:59:43 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 22:59:43 visual_prompt]: Loading validation data...
[11/23 22:59:43 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 22:59:43 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 22:59:46 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 22:59:46 visual_prompt]: tuned percent:0.525
[11/23 22:59:46 visual_prompt]: Device used for model: 0
[11/23 22:59:46 visual_prompt]: Setting up Evaluator...
[11/23 22:59:46 visual_prompt]: Setting up Trainer...
[11/23 22:59:46 visual_prompt]: 	Setting up the optimizer...
[11/23 22:59:46 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 23:01:26 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8376 s / batch. (data: 7.95e-03). ETA=12:50:33, max mem: 20.9 GB 
[11/23 23:03:01 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8240 s / batch. (data: 2.79e-04). ETA=12:36:40, max mem: 20.9 GB 
[11/23 23:04:40 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8400 s / batch. (data: 3.14e-04). ETA=12:50:00, max mem: 20.9 GB 
[11/23 23:06:15 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8440 s / batch. (data: 7.95e-03). ETA=12:52:15, max mem: 20.9 GB 
[11/23 23:07:53 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8189 s / batch. (data: 2.92e-04). ETA=12:27:54, max mem: 20.9 GB 
[11/23 23:08:44 visual_prompt]: Epoch 1 / 100: avg data time: 1.39e-01, avg batch time: 0.9718, average train loss: 1.5403
[11/23 23:09:38 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3110, average loss: 1.5201
[11/23 23:09:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 23:09:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/23 23:11:18 visual_prompt]: 	Training 100/553. train loss: 32.7968,	0.8680 s / batch. (data: 3.37e-04). ETA=13:10:33, max mem: 20.9 GB 
[11/23 23:12:54 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8461 s / batch. (data: 3.37e-04). ETA=12:49:13, max mem: 20.9 GB 
[11/23 23:14:31 visual_prompt]: 	Training 300/553. train loss: 2.9277,	0.8480 s / batch. (data: 1.53e-02). ETA=12:49:31, max mem: 20.9 GB 
[11/23 23:16:07 visual_prompt]: 	Training 400/553. train loss: 36.1745,	0.8440 s / batch. (data: 3.03e-04). ETA=12:44:28, max mem: 20.9 GB 
[11/23 23:17:45 visual_prompt]: 	Training 500/553. train loss: 13.4565,	0.8320 s / batch. (data: 3.20e-04). ETA=12:32:12, max mem: 20.9 GB 
[11/23 23:18:34 visual_prompt]: Epoch 2 / 100: avg data time: 1.37e-01, avg batch time: 0.9688, average train loss: 22.7402
[11/23 23:19:29 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3088, average loss: 21.8746
[11/23 23:19:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.07	
[11/23 23:19:29 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/23 23:21:08 visual_prompt]: 	Training 100/553. train loss: 16.2583,	0.8356 s / batch. (data: 2.98e-04). ETA=12:33:19, max mem: 20.9 GB 
[11/23 23:22:45 visual_prompt]: 	Training 200/553. train loss: 23.2656,	0.8424 s / batch. (data: 1.03e-02). ETA=12:38:04, max mem: 20.9 GB 
[11/23 23:24:21 visual_prompt]: 	Training 300/553. train loss: 11.3496,	0.8572 s / batch. (data: 8.69e-03). ETA=12:49:56, max mem: 20.9 GB 
[11/23 23:26:00 visual_prompt]: 	Training 400/553. train loss: 189.8524,	0.8321 s / batch. (data: 3.10e-04). ETA=12:26:00, max mem: 20.9 GB 
[11/23 23:27:38 visual_prompt]: 	Training 500/553. train loss: 22.7126,	1.1000 s / batch. (data: 2.67e-01). ETA=16:24:22, max mem: 20.9 GB 
[11/23 23:28:28 visual_prompt]: Epoch 3 / 100: avg data time: 1.44e-01, avg batch time: 0.9751, average train loss: 34.7822
[11/23 23:29:24 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3105, average loss: 19.8792
[11/23 23:29:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.07	
[11/23 23:29:24 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/23 23:31:07 visual_prompt]: 	Training 100/553. train loss: 6.3957,	0.8255 s / batch. (data: 1.05e-02). ETA=12:16:39, max mem: 20.9 GB 
[11/23 23:32:44 visual_prompt]: 	Training 200/553. train loss: 52.6492,	0.8303 s / batch. (data: 3.09e-04). ETA=12:19:34, max mem: 20.9 GB 
[11/23 23:34:22 visual_prompt]: 	Training 300/553. train loss: 10.5361,	1.1512 s / batch. (data: 3.01e-01). ETA=17:03:27, max mem: 20.9 GB 
[11/23 23:35:55 visual_prompt]: 	Training 400/553. train loss: 52.3675,	0.8357 s / batch. (data: 1.05e-02). ETA=12:21:34, max mem: 20.9 GB 
[11/23 23:37:34 visual_prompt]: 	Training 500/553. train loss: 40.5577,	3.3369 s / batch. (data: 2.51e+00). ETA=2 days, 1:15:27, max mem: 20.9 GB 
[11/23 23:38:26 visual_prompt]: Epoch 4 / 100: avg data time: 1.52e-01, avg batch time: 0.9804, average train loss: 63.2746
[11/23 23:39:22 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3105, average loss: 170.5102
[11/23 23:39:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.57	
[11/23 23:39:22 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/23 23:41:02 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8329 s / batch. (data: 2.86e-04). ETA=12:15:32, max mem: 20.9 GB 
[11/23 23:42:39 visual_prompt]: 	Training 200/553. train loss: 3.0704,	1.1760 s / batch. (data: 3.45e-01). ETA=17:16:37, max mem: 20.9 GB 
[11/23 23:44:17 visual_prompt]: 	Training 300/553. train loss: 18.1324,	0.8480 s / batch. (data: 4.86e-04). ETA=12:26:02, max mem: 20.9 GB 
[11/23 23:45:54 visual_prompt]: 	Training 400/553. train loss: 79.8581,	0.8476 s / batch. (data: 3.12e-04). ETA=12:24:19, max mem: 20.9 GB 
[11/23 23:47:32 visual_prompt]: 	Training 500/553. train loss: 23.4526,	0.8394 s / batch. (data: 3.03e-04). ETA=12:15:43, max mem: 20.9 GB 
[11/23 23:48:23 visual_prompt]: Epoch 5 / 100: avg data time: 1.52e-01, avg batch time: 0.9794, average train loss: 79.2135
[11/23 23:49:19 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3106, average loss: 143.7809
[11/23 23:49:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.37	
[11/23 23:49:19 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/23 23:51:02 visual_prompt]: 	Training 100/553. train loss: 6.2837,	0.8231 s / batch. (data: 4.71e-04). ETA=11:59:17, max mem: 20.9 GB 
[11/23 23:52:38 visual_prompt]: 	Training 200/553. train loss: 118.3204,	0.8612 s / batch. (data: 1.56e-02). ETA=12:31:10, max mem: 20.9 GB 
[11/23 23:54:13 visual_prompt]: 	Training 300/553. train loss: 54.3363,	0.8231 s / batch. (data: 3.09e-04). ETA=11:56:36, max mem: 20.9 GB 
[11/23 23:55:54 visual_prompt]: 	Training 400/553. train loss: 99.6455,	0.8213 s / batch. (data: 3.24e-04). ETA=11:53:38, max mem: 20.9 GB 
[11/23 23:57:30 visual_prompt]: 	Training 500/553. train loss: 30.0783,	0.8328 s / batch. (data: 3.50e-04). ETA=12:02:14, max mem: 20.9 GB 
[11/23 23:58:20 visual_prompt]: Epoch 6 / 100: avg data time: 1.49e-01, avg batch time: 0.9784, average train loss: 66.1335
[11/23 23:59:15 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3099, average loss: 99.1012
[11/23 23:59:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.41	
[11/23 23:59:15 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/24 00:00:55 visual_prompt]: 	Training 100/553. train loss: 0.2706,	0.8400 s / batch. (data: 3.08e-04). ETA=12:06:20, max mem: 20.9 GB 
[11/24 00:02:32 visual_prompt]: 	Training 200/553. train loss: 25.0724,	0.8355 s / batch. (data: 1.15e-02). ETA=12:01:05, max mem: 20.9 GB 
[11/24 00:04:13 visual_prompt]: 	Training 300/553. train loss: 10.4053,	1.7130 s / batch. (data: 8.95e-01). ETA=1 day, 0:35:31, max mem: 20.9 GB 
[11/24 00:05:49 visual_prompt]: 	Training 400/553. train loss: 14.4586,	1.7024 s / batch. (data: 8.82e-01). ETA=1 day, 0:23:35, max mem: 20.9 GB 
[11/24 00:07:25 visual_prompt]: 	Training 500/553. train loss: 24.6965,	0.8361 s / batch. (data: 3.05e-04). ETA=11:57:22, max mem: 20.9 GB 
[11/24 00:08:14 visual_prompt]: Epoch 7 / 100: avg data time: 1.47e-01, avg batch time: 0.9743, average train loss: 73.2909
[11/24 00:09:10 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3100, average loss: 76.7275
[11/24 00:09:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.65	
[11/24 00:09:10 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/24 00:10:48 visual_prompt]: 	Training 100/553. train loss: 15.1555,	0.8243 s / batch. (data: 3.10e-04). ETA=11:45:08, max mem: 20.9 GB 
[11/24 00:12:27 visual_prompt]: 	Training 200/553. train loss: 95.7314,	0.9227 s / batch. (data: 1.00e-01). ETA=13:07:51, max mem: 20.9 GB 
[11/24 00:14:05 visual_prompt]: 	Training 300/553. train loss: 156.5140,	0.8240 s / batch. (data: 3.06e-04). ETA=11:42:10, max mem: 20.9 GB 
[11/24 00:15:42 visual_prompt]: 	Training 400/553. train loss: 166.6465,	0.8587 s / batch. (data: 3.08e-02). ETA=12:10:20, max mem: 20.9 GB 
[11/24 00:17:19 visual_prompt]: 	Training 500/553. train loss: 285.2204,	1.2146 s / batch. (data: 3.85e-01). ETA=17:10:57, max mem: 20.9 GB 
[11/24 00:18:11 visual_prompt]: Epoch 8 / 100: avg data time: 1.52e-01, avg batch time: 0.9780, average train loss: 99.5197
[11/24 00:19:06 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3104, average loss: 35.6332
[11/24 00:19:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.89	
[11/24 00:19:06 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/24 00:20:47 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8252 s / batch. (data: 1.07e-02). ETA=11:38:22, max mem: 20.9 GB 
[11/24 00:22:23 visual_prompt]: 	Training 200/553. train loss: 43.8888,	0.8192 s / batch. (data: 3.11e-04). ETA=11:31:53, max mem: 20.9 GB 
[11/24 00:24:01 visual_prompt]: 	Training 300/553. train loss: 137.8746,	1.5866 s / batch. (data: 7.75e-01). ETA=22:17:24, max mem: 20.9 GB 
[11/24 00:25:39 visual_prompt]: 	Training 400/553. train loss: 35.5964,	0.8125 s / batch. (data: 5.41e-03). ETA=11:23:34, max mem: 20.9 GB 
[11/24 00:27:17 visual_prompt]: 	Training 500/553. train loss: 182.5312,	1.0834 s / batch. (data: 2.37e-01). ETA=15:09:35, max mem: 20.9 GB 
[11/24 00:28:07 visual_prompt]: Epoch 9 / 100: avg data time: 1.52e-01, avg batch time: 0.9786, average train loss: 78.0807
[11/24 00:29:03 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3101, average loss: 328.8175
[11/24 00:29:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.86	
[11/24 00:29:03 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/24 00:30:47 visual_prompt]: 	Training 100/553. train loss: 26.8022,	0.8373 s / batch. (data: 3.11e-04). ETA=11:40:49, max mem: 20.9 GB 
[11/24 00:32:22 visual_prompt]: 	Training 200/553. train loss: 121.5240,	0.8113 s / batch. (data: 5.42e-03). ETA=11:17:45, max mem: 20.9 GB 
[11/24 00:33:59 visual_prompt]: 	Training 300/553. train loss: 160.5820,	1.9756 s / batch. (data: 1.17e+00). ETA=1 day, 3:27:05, max mem: 20.9 GB 
[11/24 00:35:34 visual_prompt]: 	Training 400/553. train loss: 168.6008,	0.8160 s / batch. (data: 3.39e-04). ETA=11:18:57, max mem: 20.9 GB 
[11/24 00:37:10 visual_prompt]: 	Training 500/553. train loss: 579.5065,	0.8480 s / batch. (data: 7.95e-03). ETA=11:44:09, max mem: 20.9 GB 
[11/24 00:38:00 visual_prompt]: Epoch 10 / 100: avg data time: 1.45e-01, avg batch time: 0.9713, average train loss: 138.4034
[11/24 00:38:55 visual_prompt]: Inference (val):avg data time: 3.77e-04, avg batch time: 0.3098, average loss: 7.5994
[11/24 00:38:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.49	
[11/24 00:38:55 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/24 00:40:38 visual_prompt]: 	Training 100/553. train loss: 88.7464,	0.8233 s / batch. (data: 1.59e-02). ETA=11:21:30, max mem: 20.9 GB 
[11/24 00:42:16 visual_prompt]: 	Training 200/553. train loss: 85.2204,	0.8268 s / batch. (data: 1.05e-02). ETA=11:23:06, max mem: 20.9 GB 
[11/24 00:43:52 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.9236 s / batch. (data: 1.12e+00). ETA=1 day, 2:26:00, max mem: 20.9 GB 
[11/24 00:45:28 visual_prompt]: 	Training 400/553. train loss: 99.9953,	0.8139 s / batch. (data: 5.42e-03). ETA=11:09:42, max mem: 20.9 GB 
[11/24 00:47:04 visual_prompt]: 	Training 500/553. train loss: 108.4333,	0.8280 s / batch. (data: 3.04e-04). ETA=11:19:54, max mem: 20.9 GB 
[11/24 00:47:54 visual_prompt]: Epoch 11 / 100: avg data time: 1.48e-01, avg batch time: 0.9743, average train loss: 111.4261
[11/24 00:48:49 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3092, average loss: 323.4079
[11/24 00:48:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.19	
[11/24 00:48:49 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/24 00:50:32 visual_prompt]: 	Training 100/553. train loss: 40.6332,	0.8920 s / batch. (data: 6.66e-02). ETA=12:10:12, max mem: 20.9 GB 
[11/24 00:52:10 visual_prompt]: 	Training 200/553. train loss: 21.3151,	0.8656 s / batch. (data: 2.57e-02). ETA=11:47:09, max mem: 20.9 GB 
[11/24 00:53:45 visual_prompt]: 	Training 300/553. train loss: 58.6921,	0.8403 s / batch. (data: 7.96e-03). ETA=11:25:05, max mem: 20.9 GB 
[11/24 00:55:23 visual_prompt]: 	Training 400/553. train loss: 200.1610,	0.8320 s / batch. (data: 3.11e-04). ETA=11:16:56, max mem: 20.9 GB 
[11/24 00:57:00 visual_prompt]: 	Training 500/553. train loss: 466.1448,	0.8400 s / batch. (data: 3.06e-04). ETA=11:22:01, max mem: 20.9 GB 
[11/24 00:57:50 visual_prompt]: Epoch 12 / 100: avg data time: 1.51e-01, avg batch time: 0.9772, average train loss: 115.6275
[11/24 00:58:45 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3101, average loss: 121.8249
[11/24 00:58:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.01	
[11/24 00:58:45 visual_prompt]: Best epoch 12: best metric: -121.825
[11/24 00:58:45 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/24 01:00:28 visual_prompt]: 	Training 100/553. train loss: 104.7513,	0.8360 s / batch. (data: 7.98e-03). ETA=11:16:41, max mem: 20.9 GB 
[11/24 01:02:02 visual_prompt]: 	Training 200/553. train loss: 6.9765,	0.8440 s / batch. (data: 3.13e-04). ETA=11:21:43, max mem: 20.9 GB 
[11/24 01:03:40 visual_prompt]: 	Training 300/553. train loss: 98.9173,	1.6829 s / batch. (data: 8.44e-01). ETA=22:36:31, max mem: 20.9 GB 
[11/24 01:05:16 visual_prompt]: 	Training 400/553. train loss: 208.4926,	0.8240 s / batch. (data: 5.45e-03). ETA=11:02:48, max mem: 20.9 GB 
[11/24 01:06:54 visual_prompt]: 	Training 500/553. train loss: 157.6126,	0.8676 s / batch. (data: 2.48e-04). ETA=11:36:27, max mem: 20.9 GB 
[11/24 01:07:45 visual_prompt]: Epoch 13 / 100: avg data time: 1.49e-01, avg batch time: 0.9753, average train loss: 87.1781
[11/24 01:08:40 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3085, average loss: 192.0014
[11/24 01:08:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.44	
[11/24 01:08:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/24 01:10:22 visual_prompt]: 	Training 100/553. train loss: 31.8856,	0.8596 s / batch. (data: 2.20e-02). ETA=11:27:52, max mem: 20.9 GB 
[11/24 01:11:59 visual_prompt]: 	Training 200/553. train loss: 20.6185,	0.8472 s / batch. (data: 1.48e-02). ETA=11:16:28, max mem: 20.9 GB 
[11/24 01:13:36 visual_prompt]: 	Training 300/553. train loss: 50.9220,	0.8160 s / batch. (data: 3.12e-04). ETA=10:50:14, max mem: 20.9 GB 
[11/24 01:15:13 visual_prompt]: 	Training 400/553. train loss: 113.5434,	0.8352 s / batch. (data: 2.96e-04). ETA=11:04:08, max mem: 20.9 GB 
[11/24 01:16:50 visual_prompt]: 	Training 500/553. train loss: 16.4016,	0.8453 s / batch. (data: 7.95e-03). ETA=11:10:44, max mem: 20.9 GB 
[11/24 01:17:39 visual_prompt]: Epoch 14 / 100: avg data time: 1.48e-01, avg batch time: 0.9749, average train loss: 95.4231
[11/24 01:18:35 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3095, average loss: 35.9889
[11/24 01:18:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.24	
[11/24 01:18:35 visual_prompt]: Best epoch 14: best metric: -35.989
[11/24 01:18:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/24 01:20:16 visual_prompt]: 	Training 100/553. train loss: 77.4675,	0.9576 s / batch. (data: 1.37e-01). ETA=12:37:24, max mem: 20.9 GB 
[11/24 01:21:52 visual_prompt]: 	Training 200/553. train loss: 24.2004,	0.8320 s / batch. (data: 2.93e-04). ETA=10:56:41, max mem: 20.9 GB 
[11/24 01:23:30 visual_prompt]: 	Training 300/553. train loss: 144.2614,	0.8207 s / batch. (data: 8.56e-03). ETA=10:46:22, max mem: 20.9 GB 
[11/24 01:25:05 visual_prompt]: 	Training 400/553. train loss: 97.0569,	1.1560 s / batch. (data: 3.27e-01). ETA=15:08:36, max mem: 20.9 GB 
[11/24 01:26:43 visual_prompt]: 	Training 500/553. train loss: 26.6353,	0.8756 s / batch. (data: 2.76e-02). ETA=11:26:43, max mem: 20.9 GB 
[11/24 01:27:34 visual_prompt]: Epoch 15 / 100: avg data time: 1.48e-01, avg batch time: 0.9761, average train loss: 137.0193
[11/24 01:28:30 visual_prompt]: Inference (val):avg data time: 1.76e-04, avg batch time: 0.3098, average loss: 318.4160
[11/24 01:28:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.17	
[11/24 01:28:30 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/24 01:30:10 visual_prompt]: 	Training 100/553. train loss: 188.9208,	0.8320 s / batch. (data: 3.03e-04). ETA=10:50:25, max mem: 20.9 GB 
[11/24 01:31:47 visual_prompt]: 	Training 200/553. train loss: 44.1214,	0.8148 s / batch. (data: 3.05e-04). ETA=10:35:38, max mem: 20.9 GB 
[11/24 01:33:25 visual_prompt]: 	Training 300/553. train loss: 83.9578,	0.8403 s / batch. (data: 2.06e-02). ETA=10:54:06, max mem: 20.9 GB 
[11/24 01:35:02 visual_prompt]: 	Training 400/553. train loss: 95.3845,	0.8228 s / batch. (data: 5.41e-03). ETA=10:39:04, max mem: 20.9 GB 
[11/24 01:36:39 visual_prompt]: 	Training 500/553. train loss: 26.6261,	1.6370 s / batch. (data: 8.13e-01). ETA=21:08:48, max mem: 20.9 GB 
[11/24 01:37:30 visual_prompt]: Epoch 16 / 100: avg data time: 1.50e-01, avg batch time: 0.9767, average train loss: 90.8547
[11/24 01:38:26 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3103, average loss: 12.5311
[11/24 01:38:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[11/24 01:38:26 visual_prompt]: Best epoch 16: best metric: -12.531
[11/24 01:38:26 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/24 01:40:06 visual_prompt]: 	Training 100/553. train loss: 123.2160,	0.8200 s / batch. (data: 3.33e-04). ETA=10:33:27, max mem: 20.9 GB 
[11/24 01:41:44 visual_prompt]: 	Training 200/553. train loss: 49.1753,	0.8403 s / batch. (data: 5.20e-03). ETA=10:47:44, max mem: 20.9 GB 
[11/24 01:43:20 visual_prompt]: 	Training 300/553. train loss: 124.8512,	0.8760 s / batch. (data: 2.73e-02). ETA=11:13:49, max mem: 20.9 GB 
[11/24 01:44:57 visual_prompt]: 	Training 400/553. train loss: 30.3349,	1.0120 s / batch. (data: 1.61e-01). ETA=12:56:44, max mem: 20.9 GB 
[11/24 01:46:34 visual_prompt]: 	Training 500/553. train loss: 288.6173,	1.6240 s / batch. (data: 8.14e-01). ETA=20:43:46, max mem: 20.9 GB 
[11/24 01:47:26 visual_prompt]: Epoch 17 / 100: avg data time: 1.49e-01, avg batch time: 0.9767, average train loss: 94.9273
[11/24 01:48:21 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3098, average loss: 44.3459
[11/24 01:48:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.74	
[11/24 01:48:21 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/24 01:50:02 visual_prompt]: 	Training 100/553. train loss: 103.0587,	0.8204 s / batch. (data: 8.88e-03). ETA=10:26:14, max mem: 20.9 GB 
[11/24 01:51:42 visual_prompt]: 	Training 200/553. train loss: 12.4207,	0.8680 s / batch. (data: 3.02e-04). ETA=11:01:06, max mem: 20.9 GB 
[11/24 01:53:19 visual_prompt]: 	Training 300/553. train loss: 114.4238,	0.8515 s / batch. (data: 8.18e-03). ETA=10:47:08, max mem: 20.9 GB 
[11/24 01:54:56 visual_prompt]: 	Training 400/553. train loss: 77.0176,	0.8284 s / batch. (data: 1.57e-02). ETA=10:28:12, max mem: 20.9 GB 
[11/24 01:56:31 visual_prompt]: 	Training 500/553. train loss: 48.2409,	0.8320 s / batch. (data: 5.42e-03). ETA=10:29:31, max mem: 20.9 GB 
[11/24 01:57:20 visual_prompt]: Epoch 18 / 100: avg data time: 1.47e-01, avg batch time: 0.9741, average train loss: 95.3656
[11/24 01:58:14 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3089, average loss: 165.1938
[11/24 01:58:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.67	
[11/24 01:58:14 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/24 01:59:55 visual_prompt]: 	Training 100/553. train loss: 37.5920,	0.8359 s / batch. (data: 2.12e-02). ETA=10:30:22, max mem: 20.9 GB 
[11/24 02:01:34 visual_prompt]: 	Training 200/553. train loss: 78.9635,	0.8444 s / batch. (data: 5.86e-03). ETA=10:35:22, max mem: 20.9 GB 
[11/24 02:03:12 visual_prompt]: 	Training 300/553. train loss: 444.9015,	0.8375 s / batch. (data: 3.24e-04). ETA=10:28:44, max mem: 20.9 GB 
[11/24 02:04:51 visual_prompt]: 	Training 400/553. train loss: 80.1770,	0.8430 s / batch. (data: 1.09e-02). ETA=10:31:30, max mem: 20.9 GB 
[11/24 02:06:25 visual_prompt]: 	Training 500/553. train loss: 267.2652,	0.8160 s / batch. (data: 2.92e-04). ETA=10:09:54, max mem: 20.9 GB 
[11/24 02:07:16 visual_prompt]: Epoch 19 / 100: avg data time: 1.53e-01, avg batch time: 0.9791, average train loss: 111.7685
[11/24 02:08:11 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3092, average loss: 261.0260
[11/24 02:08:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.96	
[11/24 02:08:11 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/24 02:09:51 visual_prompt]: 	Training 100/553. train loss: 41.4846,	0.8160 s / batch. (data: 3.10e-04). ETA=10:07:49, max mem: 20.9 GB 
[11/24 02:11:29 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8149 s / batch. (data: 3.13e-04). ETA=10:05:38, max mem: 20.9 GB 
[11/24 02:13:06 visual_prompt]: 	Training 300/553. train loss: 32.8508,	0.8146 s / batch. (data: 3.25e-04). ETA=10:04:02, max mem: 20.9 GB 
[11/24 02:14:42 visual_prompt]: 	Training 400/553. train loss: 91.8494,	0.8263 s / batch. (data: 1.05e-02). ETA=10:11:23, max mem: 20.9 GB 
[11/24 02:16:19 visual_prompt]: 	Training 500/553. train loss: 37.3914,	0.8559 s / batch. (data: 3.08e-04). ETA=10:31:52, max mem: 20.9 GB 
[11/24 02:17:11 visual_prompt]: Epoch 20 / 100: avg data time: 1.50e-01, avg batch time: 0.9760, average train loss: 83.9298
[11/24 02:18:05 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3090, average loss: 56.4203
[11/24 02:18:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.07	
[11/24 02:18:05 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/24 02:19:48 visual_prompt]: 	Training 100/553. train loss: 44.5488,	0.8120 s / batch. (data: 3.19e-04). ETA=9:57:21, max mem: 20.9 GB 
[11/24 02:21:24 visual_prompt]: 	Training 200/553. train loss: 0.0001,	0.8401 s / batch. (data: 2.41e-04). ETA=10:16:35, max mem: 20.9 GB 
[11/24 02:23:00 visual_prompt]: 	Training 300/553. train loss: 287.9181,	0.9748 s / batch. (data: 1.47e-01). ETA=11:53:53, max mem: 20.9 GB 
[11/24 02:24:35 visual_prompt]: 	Training 400/553. train loss: 398.9373,	0.8320 s / batch. (data: 5.41e-03). ETA=10:07:55, max mem: 20.9 GB 
[11/24 02:26:14 visual_prompt]: 	Training 500/553. train loss: 99.6390,	0.8245 s / batch. (data: 3.11e-04). ETA=10:01:01, max mem: 20.9 GB 
[11/24 02:27:03 visual_prompt]: Epoch 21 / 100: avg data time: 1.45e-01, avg batch time: 0.9725, average train loss: 98.6770
[11/24 02:27:58 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3080, average loss: 27.0083
[11/24 02:27:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.06	
[11/24 02:27:58 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/24 02:29:38 visual_prompt]: 	Training 100/553. train loss: 215.8323,	0.8516 s / batch. (data: 1.05e-02). ETA=10:18:37, max mem: 20.9 GB 
[11/24 02:31:15 visual_prompt]: 	Training 200/553. train loss: 27.9844,	0.8463 s / batch. (data: 5.42e-03). ETA=10:13:22, max mem: 20.9 GB 
[11/24 02:32:49 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8014 s / batch. (data: 3.05e-04). ETA=9:39:28, max mem: 20.9 GB 
[11/24 02:34:28 visual_prompt]: 	Training 400/553. train loss: 2.6714,	0.8532 s / batch. (data: 2.57e-02). ETA=10:15:34, max mem: 20.9 GB 
[11/24 02:36:05 visual_prompt]: 	Training 500/553. train loss: 100.9906,	0.8103 s / batch. (data: 3.03e-04). ETA=9:43:13, max mem: 20.9 GB 
[11/24 02:36:56 visual_prompt]: Epoch 22 / 100: avg data time: 1.45e-01, avg batch time: 0.9717, average train loss: 81.1880
[11/24 02:37:51 visual_prompt]: Inference (val):avg data time: 4.03e-04, avg batch time: 0.3094, average loss: 61.1178
[11/24 02:37:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.34	
[11/24 02:37:51 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/24 02:39:33 visual_prompt]: 	Training 100/553. train loss: 140.7388,	0.9760 s / batch. (data: 1.40e-01). ETA=11:40:01, max mem: 20.9 GB 
[11/24 02:41:11 visual_prompt]: 	Training 200/553. train loss: 43.6858,	0.8442 s / batch. (data: 5.43e-03). ETA=10:04:04, max mem: 20.9 GB 
[11/24 02:42:49 visual_prompt]: 	Training 300/553. train loss: 9.4103,	0.8437 s / batch. (data: 3.03e-04). ETA=10:02:19, max mem: 20.9 GB 
[11/24 02:44:25 visual_prompt]: 	Training 400/553. train loss: 2.1598,	0.8480 s / batch. (data: 8.18e-04). ETA=10:03:58, max mem: 20.9 GB 
[11/24 02:45:59 visual_prompt]: 	Training 500/553. train loss: 317.9267,	0.8230 s / batch. (data: 5.40e-03). ETA=9:44:48, max mem: 20.9 GB 
[11/24 02:46:50 visual_prompt]: Epoch 23 / 100: avg data time: 1.48e-01, avg batch time: 0.9747, average train loss: 82.0441
[11/24 02:47:45 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3096, average loss: 39.6244
[11/24 02:47:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.19	
[11/24 02:47:45 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/24 02:49:24 visual_prompt]: 	Training 100/553. train loss: 20.0959,	0.8610 s / batch. (data: 3.72e-02). ETA=10:09:37, max mem: 20.9 GB 
[11/24 02:51:00 visual_prompt]: 	Training 200/553. train loss: 130.3906,	0.8440 s / batch. (data: 1.20e-02). ETA=9:56:08, max mem: 20.9 GB 
[11/24 02:52:38 visual_prompt]: 	Training 300/553. train loss: 8.9930,	0.8640 s / batch. (data: 3.15e-02). ETA=10:08:52, max mem: 20.9 GB 
[11/24 02:54:15 visual_prompt]: 	Training 400/553. train loss: 39.7635,	0.8260 s / batch. (data: 2.25e-04). ETA=9:40:39, max mem: 20.9 GB 
[11/24 02:55:53 visual_prompt]: 	Training 500/553. train loss: 108.8881,	0.8414 s / batch. (data: 5.47e-03). ETA=9:50:05, max mem: 20.9 GB 
[11/24 02:56:44 visual_prompt]: Epoch 24 / 100: avg data time: 1.47e-01, avg batch time: 0.9741, average train loss: 81.1871
[11/24 02:57:39 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3106, average loss: 28.5256
[11/24 02:57:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 64.54	
[11/24 02:57:39 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/24 02:59:23 visual_prompt]: 	Training 100/553. train loss: 91.4839,	0.8424 s / batch. (data: 7.93e-03). ETA=9:48:40, max mem: 20.9 GB 
[11/24 03:00:57 visual_prompt]: 	Training 200/553. train loss: 77.1709,	0.8350 s / batch. (data: 3.78e-04). ETA=9:42:07, max mem: 20.9 GB 
[11/24 03:02:33 visual_prompt]: 	Training 300/553. train loss: 43.7938,	0.8240 s / batch. (data: 7.47e-03). ETA=9:33:03, max mem: 20.9 GB 
[11/24 03:04:09 visual_prompt]: 	Training 400/553. train loss: 102.3129,	1.1560 s / batch. (data: 3.12e-01). ETA=13:22:02, max mem: 20.9 GB 
[11/24 03:05:46 visual_prompt]: 	Training 500/553. train loss: 86.9998,	1.3636 s / batch. (data: 5.46e-01). ETA=15:43:47, max mem: 20.9 GB 
[11/24 03:06:37 visual_prompt]: Epoch 25 / 100: avg data time: 1.45e-01, avg batch time: 0.9716, average train loss: 77.0688
[11/24 03:07:32 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3130, average loss: 23.5282
[11/24 03:07:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 67.86	
[11/24 03:07:32 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/24 03:09:12 visual_prompt]: 	Training 100/553. train loss: 27.9279,	0.8307 s / batch. (data: 5.39e-03). ETA=9:32:52, max mem: 20.9 GB 
[11/24 03:10:50 visual_prompt]: 	Training 200/553. train loss: 114.5106,	1.6120 s / batch. (data: 7.76e-01). ETA=18:28:54, max mem: 20.9 GB 
[11/24 03:12:27 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8439 s / batch. (data: 7.85e-04). ETA=9:39:07, max mem: 20.9 GB 
[11/24 03:14:02 visual_prompt]: 	Training 400/553. train loss: 5.7127,	0.8360 s / batch. (data: 2.93e-04). ETA=9:32:18, max mem: 20.9 GB 
[11/24 03:15:37 visual_prompt]: 	Training 500/553. train loss: 82.9827,	0.8466 s / batch. (data: 2.26e-02). ETA=9:38:10, max mem: 20.9 GB 
[11/24 03:16:27 visual_prompt]: Epoch 26 / 100: avg data time: 1.41e-01, avg batch time: 0.9679, average train loss: 83.6908
[11/24 03:17:22 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3099, average loss: 96.7490
[11/24 03:17:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.30	
[11/24 03:17:22 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/24 03:19:03 visual_prompt]: 	Training 100/553. train loss: 25.1682,	0.8239 s / batch. (data: 3.14e-04). ETA=9:20:34, max mem: 20.9 GB 
[11/24 03:20:40 visual_prompt]: 	Training 200/553. train loss: 37.1963,	0.9392 s / batch. (data: 1.13e-01). ETA=10:37:28, max mem: 20.9 GB 
[11/24 03:22:17 visual_prompt]: 	Training 300/553. train loss: 144.5471,	0.8572 s / batch. (data: 2.13e-02). ETA=9:40:21, max mem: 20.9 GB 
[11/24 03:23:55 visual_prompt]: 	Training 400/553. train loss: 49.3821,	0.8585 s / batch. (data: 5.93e-03). ETA=9:39:46, max mem: 20.9 GB 
[11/24 03:25:33 visual_prompt]: 	Training 500/553. train loss: 55.3535,	0.8386 s / batch. (data: 3.29e-04). ETA=9:24:59, max mem: 20.9 GB 
[11/24 03:26:22 visual_prompt]: Epoch 27 / 100: avg data time: 1.51e-01, avg batch time: 0.9765, average train loss: 103.2925
[11/24 03:27:17 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3103, average loss: 34.6598
[11/24 03:27:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 69.30	
[11/24 03:27:17 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/24 03:28:57 visual_prompt]: 	Training 100/553. train loss: 13.8401,	0.8520 s / batch. (data: 1.60e-02). ETA=9:31:48, max mem: 20.9 GB 
[11/24 03:30:34 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8622 s / batch. (data: 3.10e-04). ETA=9:37:13, max mem: 20.9 GB 
[11/24 03:32:12 visual_prompt]: 	Training 300/553. train loss: 211.2960,	1.4037 s / batch. (data: 5.88e-01). ETA=15:37:24, max mem: 20.9 GB 
[11/24 03:33:48 visual_prompt]: 	Training 400/553. train loss: 51.1757,	0.8140 s / batch. (data: 3.02e-04). ETA=9:02:16, max mem: 20.9 GB 
[11/24 03:35:23 visual_prompt]: 	Training 500/553. train loss: 0.0495,	0.8358 s / batch. (data: 5.42e-03). ETA=9:15:22, max mem: 20.9 GB 
[11/24 03:36:15 visual_prompt]: Epoch 28 / 100: avg data time: 1.46e-01, avg batch time: 0.9719, average train loss: 84.6978
[11/24 03:37:10 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3106, average loss: 36.3148
[11/24 03:37:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 68.10	
[11/24 03:37:10 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/24 03:38:57 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8073 s / batch. (data: 2.50e-04). ETA=8:54:21, max mem: 20.9 GB 
[11/24 03:40:32 visual_prompt]: 	Training 200/553. train loss: 3.2430,	1.7440 s / batch. (data: 9.13e-01). ETA=19:11:30, max mem: 20.9 GB 
[11/24 03:42:07 visual_prompt]: 	Training 300/553. train loss: 189.5812,	0.8320 s / batch. (data: 8.95e-04). ETA=9:07:58, max mem: 20.9 GB 
[11/24 03:43:40 visual_prompt]: 	Training 400/553. train loss: 149.5882,	1.4085 s / batch. (data: 5.66e-01). ETA=15:25:18, max mem: 20.9 GB 
[11/24 03:45:17 visual_prompt]: 	Training 500/553. train loss: 33.2032,	0.8486 s / batch. (data: 1.56e-02). ETA=9:16:03, max mem: 20.9 GB 
[11/24 03:46:08 visual_prompt]: Epoch 29 / 100: avg data time: 1.45e-01, avg batch time: 0.9720, average train loss: 90.5373
[11/24 03:47:02 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3113, average loss: 19.8268
[11/24 03:47:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 67.25	
[11/24 03:47:02 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/24 03:48:41 visual_prompt]: 	Training 100/553. train loss: 47.6176,	0.8345 s / batch. (data: 2.98e-04). ETA=9:04:43, max mem: 20.9 GB 
[11/24 03:50:19 visual_prompt]: 	Training 200/553. train loss: 65.9663,	0.8195 s / batch. (data: 3.24e-04). ETA=8:53:32, max mem: 20.9 GB 
[11/24 03:51:55 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8160 s / batch. (data: 3.16e-04). ETA=8:49:53, max mem: 20.9 GB 
[11/24 03:53:34 visual_prompt]: 	Training 400/553. train loss: 375.7125,	0.8480 s / batch. (data: 3.99e-04). ETA=9:09:15, max mem: 20.9 GB 
[11/24 03:55:09 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.4760 s / batch. (data: 6.55e-01). ETA=15:53:34, max mem: 20.9 GB 
[11/24 03:56:01 visual_prompt]: Epoch 30 / 100: avg data time: 1.46e-01, avg batch time: 0.9733, average train loss: 86.8718
[11/24 03:56:56 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3085, average loss: 26.1372
[11/24 03:56:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 68.86	
[11/24 03:56:56 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[11/24 03:58:38 visual_prompt]: 	Training 100/553. train loss: 79.1386,	0.8080 s / batch. (data: 3.07e-04). ETA=8:39:56, max mem: 20.9 GB 
[11/24 04:00:17 visual_prompt]: 	Training 200/553. train loss: 163.1584,	0.8160 s / batch. (data: 5.42e-03). ETA=8:43:44, max mem: 20.9 GB 
[11/24 04:01:52 visual_prompt]: 	Training 300/553. train loss: 41.7450,	0.8280 s / batch. (data: 3.73e-04). ETA=8:50:03, max mem: 20.9 GB 
[11/24 04:03:28 visual_prompt]: 	Training 400/553. train loss: 0.0001,	1.0828 s / batch. (data: 2.50e-01). ETA=11:31:23, max mem: 20.9 GB 
[11/24 04:05:04 visual_prompt]: 	Training 500/553. train loss: 55.1165,	0.8457 s / batch. (data: 1.05e-02). ETA=8:58:32, max mem: 20.9 GB 
[11/24 04:05:54 visual_prompt]: Epoch 31 / 100: avg data time: 1.46e-01, avg batch time: 0.9722, average train loss: 85.4424
[11/24 04:06:49 visual_prompt]: Inference (val):avg data time: 2.22e-04, avg batch time: 0.3108, average loss: 91.3646
[11/24 04:06:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.24	
[11/24 04:06:49 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[11/24 04:08:31 visual_prompt]: 	Training 100/553. train loss: 45.1156,	0.8760 s / batch. (data: 7.85e-04). ETA=9:15:36, max mem: 20.9 GB 
[11/24 04:10:07 visual_prompt]: 	Training 200/553. train loss: 67.7762,	0.8225 s / batch. (data: 7.14e-04). ETA=8:40:17, max mem: 20.9 GB 
[11/24 04:11:47 visual_prompt]: 	Training 300/553. train loss: 129.0591,	0.8166 s / batch. (data: 3.13e-04). ETA=8:35:15, max mem: 20.9 GB 
[11/24 04:13:24 visual_prompt]: 	Training 400/553. train loss: 68.5516,	0.8600 s / batch. (data: 7.96e-03). ETA=9:01:09, max mem: 20.9 GB 
[11/24 04:14:58 visual_prompt]: 	Training 500/553. train loss: 40.9556,	0.8360 s / batch. (data: 3.04e-04). ETA=8:44:41, max mem: 20.9 GB 
[11/24 04:15:47 visual_prompt]: Epoch 32 / 100: avg data time: 1.47e-01, avg batch time: 0.9730, average train loss: 70.2997
[11/24 04:16:42 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3103, average loss: 17.1912
[11/24 04:16:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 70.16	
[11/24 04:16:42 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[11/24 04:18:22 visual_prompt]: 	Training 100/553. train loss: 31.4517,	1.0013 s / batch. (data: 1.84e-01). ETA=10:25:51, max mem: 20.9 GB 
[11/24 04:20:00 visual_prompt]: 	Training 200/553. train loss: 305.1340,	1.4540 s / batch. (data: 6.22e-01). ETA=15:06:26, max mem: 20.9 GB 
[11/24 04:21:36 visual_prompt]: 	Training 300/553. train loss: 6.9267,	0.8360 s / batch. (data: 1.20e-02). ETA=8:39:46, max mem: 20.9 GB 
[11/24 04:23:13 visual_prompt]: 	Training 400/553. train loss: 30.0530,	0.8266 s / batch. (data: 5.41e-03). ETA=8:32:31, max mem: 20.9 GB 
[11/24 04:24:49 visual_prompt]: 	Training 500/553. train loss: 43.6602,	0.8400 s / batch. (data: 3.11e-04). ETA=8:39:26, max mem: 20.9 GB 
[11/24 04:25:39 visual_prompt]: Epoch 33 / 100: avg data time: 1.43e-01, avg batch time: 0.9695, average train loss: 92.9202
[11/24 04:26:34 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3120, average loss: 27.9399
[11/24 04:26:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 67.38	
[11/24 04:26:34 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[11/24 04:28:15 visual_prompt]: 	Training 100/553. train loss: 25.6993,	0.8640 s / batch. (data: 4.61e-02). ETA=8:52:04, max mem: 20.9 GB 
[11/24 04:29:51 visual_prompt]: 	Training 200/553. train loss: 53.0974,	0.8200 s / batch. (data: 3.11e-04). ETA=8:23:36, max mem: 20.9 GB 
[11/24 04:31:26 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8222 s / batch. (data: 3.40e-04). ETA=8:23:35, max mem: 20.9 GB 
[11/24 04:33:03 visual_prompt]: 	Training 400/553. train loss: 113.9986,	0.8238 s / batch. (data: 2.83e-04). ETA=8:23:12, max mem: 20.9 GB 
[11/24 04:34:40 visual_prompt]: 	Training 500/553. train loss: 12.1594,	1.3765 s / batch. (data: 5.64e-01). ETA=13:58:31, max mem: 20.9 GB 
[11/24 04:35:30 visual_prompt]: Epoch 34 / 100: avg data time: 1.42e-01, avg batch time: 0.9692, average train loss: 65.4234
[11/24 04:36:25 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3107, average loss: 40.9116
[11/24 04:36:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 67.02	
[11/24 04:36:25 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[11/24 04:38:07 visual_prompt]: 	Training 100/553. train loss: 262.2329,	0.8200 s / batch. (data: 3.42e-04). ETA=8:17:26, max mem: 20.9 GB 
[11/24 04:39:45 visual_prompt]: 	Training 200/553. train loss: 113.3687,	0.8222 s / batch. (data: 5.43e-03). ETA=8:17:25, max mem: 20.9 GB 
[11/24 04:41:20 visual_prompt]: 	Training 300/553. train loss: 57.4261,	0.8104 s / batch. (data: 3.05e-04). ETA=8:08:54, max mem: 20.9 GB 
[11/24 04:42:55 visual_prompt]: 	Training 400/553. train loss: 136.0901,	0.8480 s / batch. (data: 1.22e-02). ETA=8:30:11, max mem: 20.9 GB 
[11/24 04:44:33 visual_prompt]: 	Training 500/553. train loss: 66.5506,	1.1291 s / batch. (data: 2.92e-01). ETA=11:17:26, max mem: 20.9 GB 
[11/24 04:45:24 visual_prompt]: Epoch 35 / 100: avg data time: 1.47e-01, avg batch time: 0.9736, average train loss: 97.3900
[11/24 04:46:19 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3109, average loss: 19.0127
[11/24 04:46:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 66.08	
[11/24 04:46:19 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[11/24 04:47:59 visual_prompt]: 	Training 100/553. train loss: 235.7105,	0.8200 s / batch. (data: 7.95e-03). ETA=8:09:53, max mem: 20.9 GB 
[11/24 04:49:36 visual_prompt]: 	Training 200/553. train loss: 331.8435,	0.8336 s / batch. (data: 2.90e-04). ETA=8:16:37, max mem: 20.9 GB 
[11/24 04:51:15 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8228 s / batch. (data: 7.94e-03). ETA=8:08:48, max mem: 20.9 GB 
[11/24 04:52:51 visual_prompt]: 	Training 400/553. train loss: 48.5739,	0.8320 s / batch. (data: 3.08e-04). ETA=8:12:52, max mem: 20.9 GB 
[11/24 04:54:28 visual_prompt]: 	Training 500/553. train loss: 54.7954,	1.1241 s / batch. (data: 3.20e-01). ETA=11:04:05, max mem: 20.9 GB 
[11/24 04:55:16 visual_prompt]: Epoch 36 / 100: avg data time: 1.45e-01, avg batch time: 0.9715, average train loss: 85.6177
[11/24 04:56:12 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3093, average loss: 24.8498
[11/24 04:56:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 66.00	
[11/24 04:56:12 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[11/24 04:57:53 visual_prompt]: 	Training 100/553. train loss: 132.9640,	0.8501 s / batch. (data: 2.61e-02). ETA=8:20:01, max mem: 20.9 GB 
[11/24 04:59:28 visual_prompt]: 	Training 200/553. train loss: 27.2551,	0.8520 s / batch. (data: 3.22e-04). ETA=8:19:43, max mem: 20.9 GB 
[11/24 05:01:06 visual_prompt]: 	Training 300/553. train loss: 226.2066,	1.2284 s / batch. (data: 3.87e-01). ETA=11:58:26, max mem: 20.9 GB 
[11/24 05:02:44 visual_prompt]: 	Training 400/553. train loss: 116.5245,	1.6559 s / batch. (data: 8.31e-01). ETA=16:05:44, max mem: 20.9 GB 
[11/24 05:04:17 visual_prompt]: 	Training 500/553. train loss: 65.7784,	0.8280 s / batch. (data: 7.96e-03). ETA=8:01:30, max mem: 20.9 GB 
[11/24 05:05:09 visual_prompt]: Epoch 37 / 100: avg data time: 1.45e-01, avg batch time: 0.9716, average train loss: 57.7109
[11/24 05:06:04 visual_prompt]: Inference (val):avg data time: 1.71e-04, avg batch time: 0.3095, average loss: 64.5826
[11/24 05:06:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 70.45	
[11/24 05:06:04 visual_prompt]: Stopping early.
[11/24 05:06:04 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 05:06:04 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 05:06:04 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/24 05:06:04 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 05:06:04 visual_prompt]: Training with config:
[11/24 05:06:04 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/24 05:06:04 visual_prompt]: Loading training data...
[11/24 05:06:04 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 05:06:04 visual_prompt]: Loading validation data...
[11/24 05:06:04 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 05:06:04 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 05:06:07 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 05:06:07 visual_prompt]: tuned percent:0.525
[11/24 05:06:07 visual_prompt]: Device used for model: 0
[11/24 05:06:07 visual_prompt]: Setting up Evaluator...
[11/24 05:06:07 visual_prompt]: Setting up Trainer...
[11/24 05:06:07 visual_prompt]: 	Setting up the optimizer...
[11/24 05:06:07 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 05:07:47 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8376 s / batch. (data: 1.42e-02). ETA=12:50:35, max mem: 20.9 GB 
[11/24 05:09:23 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8200 s / batch. (data: 2.98e-04). ETA=12:33:01, max mem: 20.9 GB 
[11/24 05:11:02 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.8332 s / batch. (data: 1.00e+00). ETA=1 day, 4:00:23, max mem: 20.9 GB 
[11/24 05:12:37 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8562 s / batch. (data: 1.21e-02). ETA=13:03:24, max mem: 20.9 GB 
[11/24 05:14:16 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8341 s / batch. (data: 1.38e-02). ETA=12:41:50, max mem: 20.9 GB 
[11/24 05:15:08 visual_prompt]: Epoch 1 / 100: avg data time: 1.44e-01, avg batch time: 0.9773, average train loss: 1.5403
[11/24 05:16:03 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3108, average loss: 1.5201
[11/24 05:16:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 05:16:03 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/24 05:17:43 visual_prompt]: 	Training 100/553. train loss: 12.5197,	0.8686 s / batch. (data: 2.95e-04). ETA=13:11:03, max mem: 20.9 GB 
[11/24 05:19:20 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8320 s / batch. (data: 3.52e-04). ETA=12:36:22, max mem: 20.9 GB 
[11/24 05:20:58 visual_prompt]: 	Training 300/553. train loss: 11.5898,	0.9584 s / batch. (data: 1.16e-01). ETA=14:29:39, max mem: 20.9 GB 
[11/24 05:22:35 visual_prompt]: 	Training 400/553. train loss: 19.1761,	0.8450 s / batch. (data: 5.41e-03). ETA=12:45:24, max mem: 20.9 GB 
[11/24 05:24:13 visual_prompt]: 	Training 500/553. train loss: 3.7540,	0.8440 s / batch. (data: 3.11e-04). ETA=12:43:03, max mem: 20.9 GB 
[11/24 05:25:03 visual_prompt]: Epoch 2 / 100: avg data time: 1.42e-01, avg batch time: 0.9755, average train loss: 8.2145
[11/24 05:25:58 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3105, average loss: 5.2982
[11/24 05:25:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.54	
[11/24 05:25:58 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/24 05:27:38 visual_prompt]: 	Training 100/553. train loss: 43.9622,	0.8287 s / batch. (data: 3.18e-04). ETA=12:27:05, max mem: 20.9 GB 
[11/24 05:29:16 visual_prompt]: 	Training 200/553. train loss: 15.4211,	1.8882 s / batch. (data: 1.07e+00). ETA=1 day, 4:19:09, max mem: 20.9 GB 
[11/24 05:30:51 visual_prompt]: 	Training 300/553. train loss: 16.5867,	0.8414 s / batch. (data: 3.03e-04). ETA=12:35:48, max mem: 20.9 GB 
[11/24 05:32:28 visual_prompt]: 	Training 400/553. train loss: 14.2569,	0.8602 s / batch. (data: 1.22e-02). ETA=12:51:13, max mem: 20.9 GB 
[11/24 05:34:06 visual_prompt]: 	Training 500/553. train loss: 12.0256,	1.0881 s / batch. (data: 2.25e-01). ETA=16:13:43, max mem: 20.9 GB 
[11/24 05:34:55 visual_prompt]: Epoch 3 / 100: avg data time: 1.38e-01, avg batch time: 0.9709, average train loss: 19.3897
[11/24 05:35:50 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.3106, average loss: 39.4502
[11/24 05:35:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.94	
[11/24 05:35:50 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/24 05:37:31 visual_prompt]: 	Training 100/553. train loss: 53.0640,	0.8360 s / batch. (data: 3.14e-04). ETA=12:26:01, max mem: 20.9 GB 
[11/24 05:39:08 visual_prompt]: 	Training 200/553. train loss: 50.8620,	0.8200 s / batch. (data: 3.12e-04). ETA=12:10:23, max mem: 20.9 GB 
[11/24 05:40:46 visual_prompt]: 	Training 300/553. train loss: 58.1711,	1.2530 s / batch. (data: 4.48e-01). ETA=18:33:54, max mem: 20.9 GB 
[11/24 05:42:18 visual_prompt]: 	Training 400/553. train loss: 24.9885,	1.5240 s / batch. (data: 6.88e-01). ETA=22:32:20, max mem: 20.9 GB 
[11/24 05:43:57 visual_prompt]: 	Training 500/553. train loss: 0.0108,	3.3765 s / batch. (data: 2.55e+00). ETA=2 days, 1:50:28, max mem: 20.9 GB 
[11/24 05:44:49 visual_prompt]: Epoch 4 / 100: avg data time: 1.44e-01, avg batch time: 0.9751, average train loss: 24.8907
[11/24 05:45:45 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3098, average loss: 20.4354
[11/24 05:45:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.85	
[11/24 05:45:45 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/24 05:47:24 visual_prompt]: 	Training 100/553. train loss: 32.3181,	0.8238 s / batch. (data: 7.95e-03). ETA=12:07:31, max mem: 20.9 GB 
[11/24 05:49:01 visual_prompt]: 	Training 200/553. train loss: 23.3870,	1.2166 s / batch. (data: 3.69e-01). ETA=17:52:22, max mem: 20.9 GB 
[11/24 05:50:39 visual_prompt]: 	Training 300/553. train loss: 34.9429,	0.8534 s / batch. (data: 7.96e-03). ETA=12:30:47, max mem: 20.9 GB 
[11/24 05:52:16 visual_prompt]: 	Training 400/553. train loss: 5.3576,	0.8396 s / batch. (data: 7.96e-03). ETA=12:17:15, max mem: 20.9 GB 
[11/24 05:53:53 visual_prompt]: 	Training 500/553. train loss: 38.4655,	0.8400 s / batch. (data: 3.10e-04). ETA=12:16:13, max mem: 20.9 GB 
[11/24 05:54:45 visual_prompt]: Epoch 5 / 100: avg data time: 1.46e-01, avg batch time: 0.9768, average train loss: 35.4545
[11/24 05:55:40 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3109, average loss: 52.2161
[11/24 05:55:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.09	
[11/24 05:55:40 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/24 05:57:23 visual_prompt]: 	Training 100/553. train loss: 22.3957,	0.8411 s / batch. (data: 3.07e-04). ETA=12:15:04, max mem: 20.9 GB 
[11/24 05:58:58 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8246 s / batch. (data: 4.57e-04). ETA=11:59:13, max mem: 20.9 GB 
[11/24 06:00:34 visual_prompt]: 	Training 300/553. train loss: 100.9606,	0.8320 s / batch. (data: 5.41e-03). ETA=12:04:19, max mem: 20.9 GB 
[11/24 06:02:15 visual_prompt]: 	Training 400/553. train loss: 36.3876,	0.8240 s / batch. (data: 3.78e-04). ETA=11:55:58, max mem: 20.9 GB 
[11/24 06:03:50 visual_prompt]: 	Training 500/553. train loss: 7.5757,	0.8475 s / batch. (data: 3.41e-04). ETA=12:14:58, max mem: 20.9 GB 
[11/24 06:04:40 visual_prompt]: Epoch 6 / 100: avg data time: 1.46e-01, avg batch time: 0.9759, average train loss: 46.4969
[11/24 06:05:35 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3103, average loss: 2.9002
[11/24 06:05:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 42.29	
[11/24 06:05:35 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/24 06:07:14 visual_prompt]: 	Training 100/553. train loss: 79.7656,	0.8560 s / batch. (data: 3.59e-04). ETA=12:20:10, max mem: 20.9 GB 
[11/24 06:08:51 visual_prompt]: 	Training 200/553. train loss: 23.5082,	0.8090 s / batch. (data: 3.13e-04). ETA=11:38:11, max mem: 20.9 GB 
[11/24 06:10:31 visual_prompt]: 	Training 300/553. train loss: 89.3683,	1.5586 s / batch. (data: 7.26e-01). ETA=22:22:31, max mem: 20.9 GB 
[11/24 06:12:08 visual_prompt]: 	Training 400/553. train loss: 9.0612,	1.9200 s / batch. (data: 1.07e+00). ETA=1 day, 3:30:36, max mem: 20.9 GB 
[11/24 06:13:44 visual_prompt]: 	Training 500/553. train loss: 2.2699,	0.8589 s / batch. (data: 3.06e-04). ETA=12:16:59, max mem: 20.9 GB 
[11/24 06:14:34 visual_prompt]: Epoch 7 / 100: avg data time: 1.44e-01, avg batch time: 0.9738, average train loss: 53.5550
[11/24 06:15:29 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3102, average loss: 85.8453
[11/24 06:15:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.41	
[11/24 06:15:29 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/24 06:17:09 visual_prompt]: 	Training 100/553. train loss: 54.0753,	0.8480 s / batch. (data: 5.45e-03). ETA=12:05:26, max mem: 20.9 GB 
[11/24 06:18:47 visual_prompt]: 	Training 200/553. train loss: 8.1587,	1.1515 s / batch. (data: 3.07e-01). ETA=16:23:10, max mem: 20.9 GB 
[11/24 06:20:24 visual_prompt]: 	Training 300/553. train loss: 10.8678,	0.8234 s / batch. (data: 2.59e-04). ETA=11:41:39, max mem: 20.9 GB 
[11/24 06:22:02 visual_prompt]: 	Training 400/553. train loss: 60.3140,	0.8227 s / batch. (data: 3.24e-04). ETA=11:39:39, max mem: 20.9 GB 
[11/24 06:23:39 visual_prompt]: 	Training 500/553. train loss: 200.6376,	1.1982 s / batch. (data: 3.57e-01). ETA=16:57:04, max mem: 20.9 GB 
[11/24 06:24:30 visual_prompt]: Epoch 8 / 100: avg data time: 1.50e-01, avg batch time: 0.9778, average train loss: 58.2043
[11/24 06:25:25 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3098, average loss: 165.8387
[11/24 06:25:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.33	
[11/24 06:25:25 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/24 06:27:06 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8160 s / batch. (data: 3.02e-04). ETA=11:30:33, max mem: 20.9 GB 
[11/24 06:28:42 visual_prompt]: 	Training 200/553. train loss: 10.1231,	0.8296 s / batch. (data: 5.43e-03). ETA=11:40:42, max mem: 20.9 GB 
[11/24 06:30:19 visual_prompt]: 	Training 300/553. train loss: 65.8969,	1.3880 s / batch. (data: 5.68e-01). ETA=19:29:58, max mem: 20.9 GB 
[11/24 06:31:58 visual_prompt]: 	Training 400/553. train loss: 66.5996,	0.8410 s / batch. (data: 1.05e-02). ETA=11:47:30, max mem: 20.9 GB 
[11/24 06:33:35 visual_prompt]: 	Training 500/553. train loss: 21.8308,	0.8334 s / batch. (data: 2.39e-02). ETA=11:39:45, max mem: 20.9 GB 
[11/24 06:34:25 visual_prompt]: Epoch 9 / 100: avg data time: 1.47e-01, avg batch time: 0.9748, average train loss: 71.2444
[11/24 06:35:20 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3104, average loss: 110.4002
[11/24 06:35:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.95	
[11/24 06:35:20 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/24 06:37:03 visual_prompt]: 	Training 100/553. train loss: 133.3066,	0.8160 s / batch. (data: 7.97e-03). ETA=11:22:59, max mem: 20.9 GB 
[11/24 06:38:39 visual_prompt]: 	Training 200/553. train loss: 84.7284,	0.8456 s / batch. (data: 2.06e-02). ETA=11:46:24, max mem: 20.9 GB 
[11/24 06:40:15 visual_prompt]: 	Training 300/553. train loss: 576.9364,	1.3360 s / batch. (data: 5.09e-01). ETA=18:33:50, max mem: 20.9 GB 
[11/24 06:41:49 visual_prompt]: 	Training 400/553. train loss: 51.9027,	0.8378 s / batch. (data: 3.40e-04). ETA=11:37:05, max mem: 20.9 GB 
[11/24 06:43:28 visual_prompt]: 	Training 500/553. train loss: 23.9710,	0.8371 s / batch. (data: 3.11e-04). ETA=11:35:07, max mem: 20.9 GB 
[11/24 06:44:18 visual_prompt]: Epoch 10 / 100: avg data time: 1.45e-01, avg batch time: 0.9733, average train loss: 87.3272
[11/24 06:45:14 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3098, average loss: 112.9292
[11/24 06:45:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.07	
[11/24 06:45:14 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/24 06:46:57 visual_prompt]: 	Training 100/553. train loss: 114.9638,	0.8781 s / batch. (data: 2.60e-02). ETA=12:06:54, max mem: 20.9 GB 
[11/24 06:48:35 visual_prompt]: 	Training 200/553. train loss: 88.0898,	0.8399 s / batch. (data: 3.51e-04). ETA=11:33:55, max mem: 20.9 GB 
[11/24 06:50:10 visual_prompt]: 	Training 300/553. train loss: 14.5853,	1.8793 s / batch. (data: 1.06e+00). ETA=1 day, 1:49:29, max mem: 20.9 GB 
[11/24 06:51:45 visual_prompt]: 	Training 400/553. train loss: 141.8358,	0.8181 s / batch. (data: 3.12e-04). ETA=11:13:08, max mem: 20.9 GB 
[11/24 06:53:20 visual_prompt]: 	Training 500/553. train loss: 142.3403,	0.8581 s / batch. (data: 1.00e-02). ETA=11:44:36, max mem: 20.9 GB 
[11/24 06:54:10 visual_prompt]: Epoch 11 / 100: avg data time: 1.41e-01, avg batch time: 0.9698, average train loss: 88.3990
[11/24 06:55:06 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3101, average loss: 112.8124
[11/24 06:55:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.28	
[11/24 06:55:06 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/24 06:56:48 visual_prompt]: 	Training 100/553. train loss: 19.0796,	0.8312 s / batch. (data: 3.29e-04). ETA=11:20:23, max mem: 20.9 GB 
[11/24 06:58:26 visual_prompt]: 	Training 200/553. train loss: 83.0412,	0.8410 s / batch. (data: 1.55e-02). ETA=11:27:05, max mem: 20.9 GB 
[11/24 07:00:01 visual_prompt]: 	Training 300/553. train loss: 119.7655,	0.8320 s / batch. (data: 3.04e-04). ETA=11:18:19, max mem: 20.9 GB 
[11/24 07:01:38 visual_prompt]: 	Training 400/553. train loss: 22.7943,	0.8178 s / batch. (data: 3.25e-04). ETA=11:05:24, max mem: 20.9 GB 
[11/24 07:03:15 visual_prompt]: 	Training 500/553. train loss: 38.2310,	0.8307 s / batch. (data: 7.84e-04). ETA=11:14:31, max mem: 20.9 GB 
[11/24 07:04:05 visual_prompt]: Epoch 12 / 100: avg data time: 1.47e-01, avg batch time: 0.9749, average train loss: 85.2541
[11/24 07:05:00 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3093, average loss: 152.8936
[11/24 07:05:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.11	
[11/24 07:05:00 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/24 07:06:42 visual_prompt]: 	Training 100/553. train loss: 23.9103,	0.8564 s / batch. (data: 5.43e-03). ETA=11:33:10, max mem: 20.9 GB 
[11/24 07:08:17 visual_prompt]: 	Training 200/553. train loss: 49.2319,	0.8600 s / batch. (data: 3.20e-04). ETA=11:34:39, max mem: 20.9 GB 
[11/24 07:09:54 visual_prompt]: 	Training 300/553. train loss: 28.9799,	1.6680 s / batch. (data: 8.44e-01). ETA=22:24:30, max mem: 20.9 GB 
[11/24 07:11:30 visual_prompt]: 	Training 400/553. train loss: 96.5915,	0.8374 s / batch. (data: 3.06e-04). ETA=11:13:37, max mem: 20.9 GB 
[11/24 07:13:08 visual_prompt]: 	Training 500/553. train loss: 92.0966,	0.8245 s / batch. (data: 2.90e-04). ETA=11:01:50, max mem: 20.9 GB 
[11/24 07:13:58 visual_prompt]: Epoch 13 / 100: avg data time: 1.45e-01, avg batch time: 0.9729, average train loss: 92.7903
[11/24 07:14:54 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3113, average loss: 21.4387
[11/24 07:14:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.53	
[11/24 07:14:54 visual_prompt]: Best epoch 13: best metric: -21.439
[11/24 07:14:54 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/24 07:16:36 visual_prompt]: 	Training 100/553. train loss: 13.0888,	0.8400 s / batch. (data: 7.94e-03). ETA=11:12:08, max mem: 20.9 GB 
[11/24 07:18:12 visual_prompt]: 	Training 200/553. train loss: 48.8425,	0.9236 s / batch. (data: 1.05e-01). ETA=12:17:32, max mem: 20.9 GB 
[11/24 07:19:49 visual_prompt]: 	Training 300/553. train loss: 38.0073,	0.8360 s / batch. (data: 3.15e-04). ETA=11:06:10, max mem: 20.9 GB 
[11/24 07:21:25 visual_prompt]: 	Training 400/553. train loss: 27.0583,	0.8216 s / batch. (data: 3.28e-04). ETA=10:53:18, max mem: 20.9 GB 
[11/24 07:23:03 visual_prompt]: 	Training 500/553. train loss: 136.6361,	0.8301 s / batch. (data: 2.83e-04). ETA=10:58:40, max mem: 20.9 GB 
[11/24 07:23:52 visual_prompt]: Epoch 14 / 100: avg data time: 1.45e-01, avg batch time: 0.9729, average train loss: 85.7605
[11/24 07:24:47 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3109, average loss: 75.6512
[11/24 07:24:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.40	
[11/24 07:24:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/24 07:26:28 visual_prompt]: 	Training 100/553. train loss: 33.4414,	0.8261 s / batch. (data: 3.04e-04). ETA=10:53:27, max mem: 20.9 GB 
[11/24 07:28:04 visual_prompt]: 	Training 200/553. train loss: 190.2530,	0.8404 s / batch. (data: 3.14e-04). ETA=11:03:17, max mem: 20.9 GB 
[11/24 07:29:43 visual_prompt]: 	Training 300/553. train loss: 39.7126,	0.8272 s / batch. (data: 8.36e-04). ETA=10:51:34, max mem: 20.9 GB 
[11/24 07:31:17 visual_prompt]: 	Training 400/553. train loss: 36.3495,	0.8400 s / batch. (data: 3.07e-04). ETA=11:00:12, max mem: 20.9 GB 
[11/24 07:32:55 visual_prompt]: 	Training 500/553. train loss: 174.3483,	0.8317 s / batch. (data: 2.98e-04). ETA=10:52:18, max mem: 20.9 GB 
[11/24 07:33:46 visual_prompt]: Epoch 15 / 100: avg data time: 1.47e-01, avg batch time: 0.9745, average train loss: 81.3469
[11/24 07:34:42 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3102, average loss: 42.3722
[11/24 07:34:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.24	
[11/24 07:34:42 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/24 07:36:21 visual_prompt]: 	Training 100/553. train loss: 59.0136,	0.8680 s / batch. (data: 1.20e-02). ETA=11:18:33, max mem: 20.9 GB 
[11/24 07:37:57 visual_prompt]: 	Training 200/553. train loss: 124.4367,	0.8212 s / batch. (data: 2.98e-04). ETA=10:40:38, max mem: 20.9 GB 
[11/24 07:39:35 visual_prompt]: 	Training 300/553. train loss: 10.7524,	0.8360 s / batch. (data: 2.91e-04). ETA=10:50:45, max mem: 20.9 GB 
[11/24 07:41:12 visual_prompt]: 	Training 400/553. train loss: 123.1823,	0.8254 s / batch. (data: 7.92e-04). ETA=10:41:06, max mem: 20.9 GB 
[11/24 07:42:48 visual_prompt]: 	Training 500/553. train loss: 230.5610,	1.1564 s / batch. (data: 3.38e-01). ETA=14:56:20, max mem: 20.9 GB 
[11/24 07:43:40 visual_prompt]: Epoch 16 / 100: avg data time: 1.45e-01, avg batch time: 0.9730, average train loss: 92.4808
[11/24 07:44:35 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3094, average loss: 6.6274
[11/24 07:44:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.14	
[11/24 07:44:35 visual_prompt]: Best epoch 16: best metric: -6.627
[11/24 07:44:35 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/24 07:46:16 visual_prompt]: 	Training 100/553. train loss: 2.6529,	0.8611 s / batch. (data: 2.82e-04). ETA=11:05:11, max mem: 20.9 GB 
[11/24 07:47:54 visual_prompt]: 	Training 200/553. train loss: 152.5670,	0.8209 s / batch. (data: 3.28e-04). ETA=10:32:49, max mem: 20.9 GB 
[11/24 07:49:31 visual_prompt]: 	Training 300/553. train loss: 253.1240,	0.8460 s / batch. (data: 5.41e-03). ETA=10:50:46, max mem: 20.9 GB 
[11/24 07:51:07 visual_prompt]: 	Training 400/553. train loss: 36.0315,	1.1520 s / batch. (data: 3.12e-01). ETA=14:44:10, max mem: 20.9 GB 
[11/24 07:52:43 visual_prompt]: 	Training 500/553. train loss: 14.2437,	1.5555 s / batch. (data: 7.38e-01). ETA=19:51:19, max mem: 20.9 GB 
[11/24 07:53:35 visual_prompt]: Epoch 17 / 100: avg data time: 1.47e-01, avg batch time: 0.9751, average train loss: 97.3880
[11/24 07:54:30 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3090, average loss: 34.5270
[11/24 07:54:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 37.29	
[11/24 07:54:30 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/24 07:56:11 visual_prompt]: 	Training 100/553. train loss: 21.5394,	0.8237 s / batch. (data: 7.94e-03). ETA=10:28:46, max mem: 20.9 GB 
[11/24 07:57:50 visual_prompt]: 	Training 200/553. train loss: 102.2191,	0.8576 s / batch. (data: 7.90e-04). ETA=10:53:12, max mem: 20.9 GB 
[11/24 07:59:27 visual_prompt]: 	Training 300/553. train loss: 108.1322,	0.8069 s / batch. (data: 2.98e-04). ETA=10:13:15, max mem: 20.9 GB 
[11/24 08:01:04 visual_prompt]: 	Training 400/553. train loss: 52.3275,	0.8320 s / batch. (data: 2.79e-04). ETA=10:30:56, max mem: 20.9 GB 
[11/24 08:02:39 visual_prompt]: 	Training 500/553. train loss: 47.7289,	0.8953 s / batch. (data: 6.32e-02). ETA=11:17:27, max mem: 20.9 GB 
[11/24 08:03:28 visual_prompt]: Epoch 18 / 100: avg data time: 1.46e-01, avg batch time: 0.9737, average train loss: 86.3528
[11/24 08:04:23 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3093, average loss: 2.3279
[11/24 08:04:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 40.65	rocauc: 37.02	
[11/24 08:04:23 visual_prompt]: Best epoch 18: best metric: -2.328
[11/24 08:04:23 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/24 08:06:03 visual_prompt]: 	Training 100/553. train loss: 69.7756,	1.1422 s / batch. (data: 2.99e-01). ETA=14:21:20, max mem: 20.9 GB 
[11/24 08:07:41 visual_prompt]: 	Training 200/553. train loss: 22.1144,	0.8176 s / batch. (data: 3.02e-04). ETA=10:15:11, max mem: 20.9 GB 
[11/24 08:09:18 visual_prompt]: 	Training 300/553. train loss: 408.8005,	0.8640 s / batch. (data: 3.31e-04). ETA=10:48:39, max mem: 20.9 GB 
[11/24 08:10:55 visual_prompt]: 	Training 400/553. train loss: 72.7405,	0.8441 s / batch. (data: 1.14e-03). ETA=10:32:19, max mem: 20.9 GB 
[11/24 08:12:28 visual_prompt]: 	Training 500/553. train loss: 109.6554,	0.8200 s / batch. (data: 2.99e-04). ETA=10:12:53, max mem: 20.9 GB 
[11/24 08:13:19 visual_prompt]: Epoch 19 / 100: avg data time: 1.40e-01, avg batch time: 0.9685, average train loss: 89.7050
[11/24 08:14:14 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3114, average loss: 100.0859
[11/24 08:14:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.88	
[11/24 08:14:14 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/24 08:15:53 visual_prompt]: 	Training 100/553. train loss: 79.4471,	0.8440 s / batch. (data: 3.24e-04). ETA=10:28:40, max mem: 20.9 GB 
[11/24 08:17:30 visual_prompt]: 	Training 200/553. train loss: 157.0300,	0.8349 s / batch. (data: 3.24e-04). ETA=10:20:30, max mem: 20.9 GB 
[11/24 08:19:07 visual_prompt]: 	Training 300/553. train loss: 8.1106,	0.8198 s / batch. (data: 3.73e-04). ETA=10:07:54, max mem: 20.9 GB 
[11/24 08:20:44 visual_prompt]: 	Training 400/553. train loss: 2.5973,	0.8760 s / batch. (data: 1.19e-02). ETA=10:48:07, max mem: 20.9 GB 
[11/24 08:22:20 visual_prompt]: 	Training 500/553. train loss: 169.9998,	0.8213 s / batch. (data: 1.05e-02). ETA=10:06:16, max mem: 20.9 GB 
[11/24 08:23:11 visual_prompt]: Epoch 20 / 100: avg data time: 1.44e-01, avg batch time: 0.9718, average train loss: 91.2562
[11/24 08:24:07 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3095, average loss: 90.9191
[11/24 08:24:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.28	
[11/24 08:24:07 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/24 08:25:50 visual_prompt]: 	Training 100/553. train loss: 4.2993,	0.8400 s / batch. (data: 3.23e-04). ETA=10:17:58, max mem: 20.9 GB 
[11/24 08:27:26 visual_prompt]: 	Training 200/553. train loss: 36.2410,	0.8204 s / batch. (data: 3.05e-04). ETA=10:02:12, max mem: 20.9 GB 
[11/24 08:29:02 visual_prompt]: 	Training 300/553. train loss: 135.9170,	0.8144 s / batch. (data: 3.28e-04). ETA=9:56:25, max mem: 20.9 GB 
[11/24 08:30:38 visual_prompt]: 	Training 400/553. train loss: 9.0030,	0.8359 s / batch. (data: 2.90e-04). ETA=10:10:46, max mem: 20.9 GB 
[11/24 08:32:17 visual_prompt]: 	Training 500/553. train loss: 113.5523,	0.8349 s / batch. (data: 9.27e-03). ETA=10:08:36, max mem: 20.9 GB 
[11/24 08:33:06 visual_prompt]: Epoch 21 / 100: avg data time: 1.48e-01, avg batch time: 0.9754, average train loss: 95.3262
[11/24 08:34:02 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3095, average loss: 4.7616
[11/24 08:34:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 52.42	
[11/24 08:34:02 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/24 08:35:42 visual_prompt]: 	Training 100/553. train loss: 50.1048,	0.8127 s / batch. (data: 2.40e-04). ETA=9:50:23, max mem: 20.9 GB 
[11/24 08:37:19 visual_prompt]: 	Training 200/553. train loss: 62.6233,	0.8446 s / batch. (data: 3.00e-04). ETA=10:12:07, max mem: 20.9 GB 
[11/24 08:38:53 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8280 s / batch. (data: 5.42e-03). ETA=9:58:43, max mem: 20.9 GB 
[11/24 08:40:31 visual_prompt]: 	Training 400/553. train loss: 54.8575,	0.8526 s / batch. (data: 7.95e-03). ETA=10:15:05, max mem: 20.9 GB 
[11/24 08:42:08 visual_prompt]: 	Training 500/553. train loss: 48.8533,	0.8189 s / batch. (data: 3.08e-04). ETA=9:49:24, max mem: 20.9 GB 
[11/24 08:42:59 visual_prompt]: Epoch 22 / 100: avg data time: 1.43e-01, avg batch time: 0.9714, average train loss: 75.0675
[11/24 08:43:54 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3097, average loss: 47.6330
[11/24 08:43:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.47	
[11/24 08:43:54 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/24 08:45:36 visual_prompt]: 	Training 100/553. train loss: 15.8041,	0.8267 s / batch. (data: 3.28e-04). ETA=9:52:54, max mem: 20.9 GB 
[11/24 08:47:13 visual_prompt]: 	Training 200/553. train loss: 60.4298,	0.8189 s / batch. (data: 3.02e-04). ETA=9:45:59, max mem: 20.9 GB 
[11/24 08:48:50 visual_prompt]: 	Training 300/553. train loss: 9.5525,	0.8284 s / batch. (data: 3.04e-04). ETA=9:51:23, max mem: 20.9 GB 
[11/24 08:50:24 visual_prompt]: 	Training 400/553. train loss: 76.4443,	0.8643 s / batch. (data: 8.51e-04). ETA=10:15:33, max mem: 20.9 GB 
[11/24 08:52:00 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8436 s / batch. (data: 2.05e-02). ETA=9:59:27, max mem: 20.9 GB 
[11/24 08:52:50 visual_prompt]: Epoch 23 / 100: avg data time: 1.42e-01, avg batch time: 0.9687, average train loss: 81.9495
[11/24 08:53:45 visual_prompt]: Inference (val):avg data time: 2.20e-04, avg batch time: 0.3103, average loss: 119.5339
[11/24 08:53:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.64	
[11/24 08:53:45 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/24 08:55:23 visual_prompt]: 	Training 100/553. train loss: 16.3050,	0.8173 s / batch. (data: 3.19e-04). ETA=9:38:40, max mem: 20.9 GB 
[11/24 08:56:59 visual_prompt]: 	Training 200/553. train loss: 61.7671,	0.8160 s / batch. (data: 2.96e-04). ETA=9:36:23, max mem: 20.9 GB 
[11/24 08:58:35 visual_prompt]: 	Training 300/553. train loss: 61.0474,	0.8760 s / batch. (data: 2.85e-02). ETA=10:17:18, max mem: 20.9 GB 
[11/24 09:00:12 visual_prompt]: 	Training 400/553. train loss: 97.8063,	0.8236 s / batch. (data: 2.82e-04). ETA=9:39:00, max mem: 20.9 GB 
[11/24 09:01:49 visual_prompt]: 	Training 500/553. train loss: 311.2027,	0.8255 s / batch. (data: 1.05e-02). ETA=9:38:57, max mem: 20.9 GB 
[11/24 09:02:40 visual_prompt]: Epoch 24 / 100: avg data time: 1.40e-01, avg batch time: 0.9671, average train loss: 82.1351
[11/24 09:03:35 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3109, average loss: 112.9347
[11/24 09:03:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.77	
[11/24 09:03:35 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/24 09:05:18 visual_prompt]: 	Training 100/553. train loss: 107.3279,	0.8401 s / batch. (data: 7.96e-03). ETA=9:47:02, max mem: 20.9 GB 
[11/24 09:06:51 visual_prompt]: 	Training 200/553. train loss: 86.5670,	0.8371 s / batch. (data: 3.43e-04). ETA=9:43:33, max mem: 20.9 GB 
[11/24 09:08:27 visual_prompt]: 	Training 300/553. train loss: 123.3767,	0.8480 s / batch. (data: 3.66e-04). ETA=9:49:45, max mem: 20.9 GB 
[11/24 09:10:03 visual_prompt]: 	Training 400/553. train loss: 207.6805,	1.1960 s / batch. (data: 3.60e-01). ETA=13:49:46, max mem: 20.9 GB 
[11/24 09:11:40 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.3360 s / batch. (data: 5.19e-01). ETA=15:24:39, max mem: 20.9 GB 
[11/24 09:12:31 visual_prompt]: Epoch 25 / 100: avg data time: 1.40e-01, avg batch time: 0.9692, average train loss: 88.6766
[11/24 09:13:26 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3104, average loss: 65.0240
[11/24 09:13:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.04	
[11/24 09:13:26 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/24 09:15:07 visual_prompt]: 	Training 100/553. train loss: 30.0034,	0.8295 s / batch. (data: 3.19e-04). ETA=9:32:00, max mem: 20.9 GB 
[11/24 09:16:45 visual_prompt]: 	Training 200/553. train loss: 67.1940,	1.6600 s / batch. (data: 8.45e-01). ETA=19:01:57, max mem: 20.9 GB 
[11/24 09:18:23 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8220 s / batch. (data: 3.23e-04). ETA=9:24:05, max mem: 20.9 GB 
[11/24 09:19:58 visual_prompt]: 	Training 400/553. train loss: 92.3177,	0.8160 s / batch. (data: 3.00e-04). ETA=9:18:37, max mem: 20.9 GB 
[11/24 09:21:34 visual_prompt]: 	Training 500/553. train loss: 38.7327,	0.8200 s / batch. (data: 3.11e-04). ETA=9:19:59, max mem: 20.9 GB 
[11/24 09:22:25 visual_prompt]: Epoch 26 / 100: avg data time: 1.45e-01, avg batch time: 0.9735, average train loss: 69.7019
[11/24 09:23:20 visual_prompt]: Inference (val):avg data time: 2.03e-04, avg batch time: 0.3113, average loss: 14.4951
[11/24 09:23:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.32	
[11/24 09:23:20 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[11/24 09:25:02 visual_prompt]: 	Training 100/553. train loss: 219.0955,	0.8274 s / batch. (data: 5.39e-03). ETA=9:22:54, max mem: 20.9 GB 
[11/24 09:26:38 visual_prompt]: 	Training 200/553. train loss: 145.9541,	0.8440 s / batch. (data: 7.95e-03). ETA=9:32:50, max mem: 20.9 GB 
[11/24 09:28:15 visual_prompt]: 	Training 300/553. train loss: 24.4080,	0.8360 s / batch. (data: 3.02e-04). ETA=9:25:59, max mem: 20.9 GB 
[11/24 09:29:52 visual_prompt]: 	Training 400/553. train loss: 44.4365,	0.8349 s / batch. (data: 1.10e-02). ETA=9:23:50, max mem: 20.9 GB 
[11/24 09:31:29 visual_prompt]: 	Training 500/553. train loss: 161.3302,	0.8360 s / batch. (data: 8.18e-04). ETA=9:23:11, max mem: 20.9 GB 
[11/24 09:32:18 visual_prompt]: Epoch 27 / 100: avg data time: 1.44e-01, avg batch time: 0.9726, average train loss: 74.8556
[11/24 09:33:13 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3109, average loss: 34.8650
[11/24 09:33:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.19	
[11/24 09:33:13 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[11/24 09:34:52 visual_prompt]: 	Training 100/553. train loss: 86.5541,	0.8375 s / batch. (data: 1.05e-02). ETA=9:22:03, max mem: 20.9 GB 
[11/24 09:36:29 visual_prompt]: 	Training 200/553. train loss: 22.8274,	0.8327 s / batch. (data: 2.99e-04). ETA=9:17:27, max mem: 20.9 GB 
[11/24 09:38:06 visual_prompt]: 	Training 300/553. train loss: 59.2656,	1.3952 s / batch. (data: 5.63e-01). ETA=15:31:45, max mem: 20.9 GB 
[11/24 09:39:42 visual_prompt]: 	Training 400/553. train loss: 150.4455,	0.8228 s / batch. (data: 4.32e-04). ETA=9:08:05, max mem: 20.9 GB 
[11/24 09:41:18 visual_prompt]: 	Training 500/553. train loss: 147.9897,	0.8312 s / batch. (data: 3.30e-04). ETA=9:12:18, max mem: 20.9 GB 
[11/24 09:42:08 visual_prompt]: Epoch 28 / 100: avg data time: 1.39e-01, avg batch time: 0.9666, average train loss: 69.9625
[11/24 09:43:03 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3112, average loss: 70.9363
[11/24 09:43:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.18	
[11/24 09:43:03 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[11/24 09:44:50 visual_prompt]: 	Training 100/553. train loss: 76.6778,	0.8680 s / batch. (data: 2.92e-04). ETA=9:34:33, max mem: 20.9 GB 
[11/24 09:46:26 visual_prompt]: 	Training 200/553. train loss: 29.0995,	1.6255 s / batch. (data: 7.98e-01). ETA=17:53:16, max mem: 20.9 GB 
[11/24 09:48:00 visual_prompt]: 	Training 300/553. train loss: 55.0209,	0.8240 s / batch. (data: 2.91e-04). ETA=9:02:41, max mem: 20.9 GB 
[11/24 09:49:34 visual_prompt]: 	Training 400/553. train loss: 17.1176,	0.8736 s / batch. (data: 4.07e-02). ETA=9:33:51, max mem: 20.9 GB 
[11/24 09:51:11 visual_prompt]: 	Training 500/553. train loss: 68.7080,	0.8548 s / batch. (data: 1.56e-02). ETA=9:20:05, max mem: 20.9 GB 
[11/24 09:52:02 visual_prompt]: Epoch 29 / 100: avg data time: 1.45e-01, avg batch time: 0.9742, average train loss: 77.5595
[11/24 09:52:57 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3110, average loss: 139.9693
[11/24 09:52:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.88	
[11/24 09:52:57 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[11/24 09:54:37 visual_prompt]: 	Training 100/553. train loss: 95.7120,	0.8400 s / batch. (data: 2.87e-04). ETA=9:08:16, max mem: 20.9 GB 
[11/24 09:56:14 visual_prompt]: 	Training 200/553. train loss: 103.8336,	0.8145 s / batch. (data: 3.81e-04). ETA=8:50:17, max mem: 20.9 GB 
[11/24 09:57:49 visual_prompt]: 	Training 300/553. train loss: 66.8459,	0.8240 s / batch. (data: 3.09e-04). ETA=8:55:05, max mem: 20.9 GB 
[11/24 09:59:27 visual_prompt]: 	Training 400/553. train loss: 17.2390,	1.0920 s / batch. (data: 2.52e-01). ETA=11:47:18, max mem: 20.9 GB 
[11/24 10:01:02 visual_prompt]: 	Training 500/553. train loss: 25.4649,	1.2762 s / batch. (data: 4.59e-01). ETA=13:44:30, max mem: 20.9 GB 
[11/24 10:01:55 visual_prompt]: Epoch 30 / 100: avg data time: 1.43e-01, avg batch time: 0.9715, average train loss: 77.3183
[11/24 10:02:50 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3103, average loss: 59.4566
[11/24 10:02:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.43	
[11/24 10:02:50 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[11/24 10:04:32 visual_prompt]: 	Training 100/553. train loss: 11.5690,	0.8276 s / batch. (data: 1.20e-02). ETA=8:52:35, max mem: 20.9 GB 
[11/24 10:06:11 visual_prompt]: 	Training 200/553. train loss: 33.0901,	0.8279 s / batch. (data: 3.24e-04). ETA=8:51:20, max mem: 20.9 GB 
[11/24 10:07:45 visual_prompt]: 	Training 300/553. train loss: 22.6195,	0.8514 s / batch. (data: 1.15e-02). ETA=9:05:03, max mem: 20.9 GB 
[11/24 10:09:20 visual_prompt]: 	Training 400/553. train loss: 42.0631,	1.4342 s / batch. (data: 5.99e-01). ETA=15:15:45, max mem: 20.9 GB 
[11/24 10:10:56 visual_prompt]: 	Training 500/553. train loss: 14.5550,	0.8277 s / batch. (data: 2.82e-04). ETA=8:47:05, max mem: 20.9 GB 
[11/24 10:11:46 visual_prompt]: Epoch 31 / 100: avg data time: 1.42e-01, avg batch time: 0.9703, average train loss: 73.1653
[11/24 10:12:42 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3098, average loss: 39.5140
[11/24 10:12:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.60	
[11/24 10:12:42 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[11/24 10:14:24 visual_prompt]: 	Training 100/553. train loss: 94.3087,	0.8575 s / batch. (data: 1.05e-02). ETA=9:03:54, max mem: 20.9 GB 
[11/24 10:16:00 visual_prompt]: 	Training 200/553. train loss: 99.7104,	0.8522 s / batch. (data: 3.22e-02). ETA=8:59:08, max mem: 20.9 GB 
[11/24 10:17:39 visual_prompt]: 	Training 300/553. train loss: 35.2079,	1.0040 s / batch. (data: 1.61e-01). ETA=10:33:28, max mem: 20.9 GB 
[11/24 10:19:17 visual_prompt]: 	Training 400/553. train loss: 178.5225,	0.8368 s / batch. (data: 3.03e-04). ETA=8:46:34, max mem: 20.9 GB 
[11/24 10:20:52 visual_prompt]: 	Training 500/553. train loss: 31.5324,	0.8320 s / batch. (data: 3.04e-04). ETA=8:42:10, max mem: 20.9 GB 
[11/24 10:21:41 visual_prompt]: Epoch 32 / 100: avg data time: 1.46e-01, avg batch time: 0.9743, average train loss: 80.8004
[11/24 10:22:36 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3110, average loss: 143.6310
[11/24 10:22:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.18	
[11/24 10:22:36 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[11/24 10:24:15 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8185 s / batch. (data: 3.30e-04). ETA=8:31:37, max mem: 20.9 GB 
[11/24 10:25:54 visual_prompt]: 	Training 200/553. train loss: 96.3196,	1.1403 s / batch. (data: 2.91e-01). ETA=11:50:51, max mem: 20.9 GB 
[11/24 10:27:30 visual_prompt]: 	Training 300/553. train loss: 72.6527,	0.8280 s / batch. (data: 3.05e-04). ETA=8:34:47, max mem: 20.9 GB 
[11/24 10:29:07 visual_prompt]: 	Training 400/553. train loss: 83.9825,	0.8560 s / batch. (data: 1.20e-02). ETA=8:50:46, max mem: 20.9 GB 
[11/24 10:30:43 visual_prompt]: 	Training 500/553. train loss: 125.3319,	0.8257 s / batch. (data: 3.12e-04). ETA=8:30:36, max mem: 20.9 GB 
[11/24 10:31:33 visual_prompt]: Epoch 33 / 100: avg data time: 1.43e-01, avg batch time: 0.9709, average train loss: 79.4853
[11/24 10:32:28 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3098, average loss: 10.7333
[11/24 10:32:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.54	
[11/24 10:32:28 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[11/24 10:34:08 visual_prompt]: 	Training 100/553. train loss: 30.8330,	0.8789 s / batch. (data: 4.43e-02). ETA=9:01:16, max mem: 20.9 GB 
[11/24 10:35:43 visual_prompt]: 	Training 200/553. train loss: 10.0184,	0.8190 s / batch. (data: 3.24e-04). ETA=8:23:00, max mem: 20.9 GB 
[11/24 10:37:19 visual_prompt]: 	Training 300/553. train loss: 218.5066,	0.8370 s / batch. (data: 3.25e-04). ETA=8:32:41, max mem: 20.9 GB 
[11/24 10:38:57 visual_prompt]: 	Training 400/553. train loss: 36.7482,	0.8202 s / batch. (data: 2.83e-04). ETA=8:21:00, max mem: 20.9 GB 
[11/24 10:40:34 visual_prompt]: 	Training 500/553. train loss: 12.9240,	1.3680 s / batch. (data: 5.36e-01). ETA=13:53:21, max mem: 20.9 GB 
[11/24 10:41:24 visual_prompt]: Epoch 34 / 100: avg data time: 1.41e-01, avg batch time: 0.9691, average train loss: 77.0760
[11/24 10:42:19 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3111, average loss: 128.6205
[11/24 10:42:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.07	
[11/24 10:42:19 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[11/24 10:44:02 visual_prompt]: 	Training 100/553. train loss: 85.9022,	0.8365 s / batch. (data: 5.42e-03). ETA=8:27:27, max mem: 20.9 GB 
[11/24 10:45:40 visual_prompt]: 	Training 200/553. train loss: 1.6506,	0.8400 s / batch. (data: 3.15e-04). ETA=8:28:08, max mem: 20.9 GB 
[11/24 10:47:15 visual_prompt]: 	Training 300/553. train loss: 9.8607,	0.8360 s / batch. (data: 3.45e-04). ETA=8:24:20, max mem: 20.9 GB 
[11/24 10:48:50 visual_prompt]: 	Training 400/553. train loss: 9.2928,	0.8574 s / batch. (data: 1.52e-02). ETA=8:35:51, max mem: 20.9 GB 
[11/24 10:50:27 visual_prompt]: 	Training 500/553. train loss: 38.0204,	0.9739 s / batch. (data: 1.58e-01). ETA=9:44:17, max mem: 20.9 GB 
[11/24 10:51:18 visual_prompt]: Epoch 35 / 100: avg data time: 1.45e-01, avg batch time: 0.9736, average train loss: 76.9145
[11/24 10:52:13 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3088, average loss: 226.2540
[11/24 10:52:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.64	
[11/24 10:52:13 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[11/24 10:53:53 visual_prompt]: 	Training 100/553. train loss: 204.2154,	0.8280 s / batch. (data: 3.40e-04). ETA=8:14:38, max mem: 20.9 GB 
[11/24 10:55:30 visual_prompt]: 	Training 200/553. train loss: 130.6249,	0.8280 s / batch. (data: 3.37e-04). ETA=8:13:16, max mem: 20.9 GB 
[11/24 10:57:08 visual_prompt]: 	Training 300/553. train loss: 73.9381,	0.8372 s / batch. (data: 1.05e-02). ETA=8:17:21, max mem: 20.9 GB 
[11/24 10:58:44 visual_prompt]: 	Training 400/553. train loss: 78.8268,	0.8417 s / batch. (data: 2.40e-02). ETA=8:18:36, max mem: 20.9 GB 
[11/24 11:00:21 visual_prompt]: 	Training 500/553. train loss: 132.2474,	1.1958 s / batch. (data: 3.64e-01). ETA=11:46:24, max mem: 20.9 GB 
[11/24 11:01:09 visual_prompt]: Epoch 36 / 100: avg data time: 1.42e-01, avg batch time: 0.9693, average train loss: 84.0513
[11/24 11:02:05 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3100, average loss: 107.7796
[11/24 11:02:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.74	
[11/24 11:02:05 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[11/24 11:03:45 visual_prompt]: 	Training 100/553. train loss: 194.6217,	0.8356 s / batch. (data: 3.26e-04). ETA=8:11:31, max mem: 20.9 GB 
[11/24 11:05:20 visual_prompt]: 	Training 200/553. train loss: 76.6887,	0.8320 s / batch. (data: 3.09e-04). ETA=8:07:59, max mem: 20.9 GB 
[11/24 11:06:57 visual_prompt]: 	Training 300/553. train loss: 47.6810,	1.1606 s / batch. (data: 3.30e-01). ETA=11:18:49, max mem: 20.9 GB 
[11/24 11:08:35 visual_prompt]: 	Training 400/553. train loss: 23.0807,	1.4307 s / batch. (data: 6.26e-01). ETA=13:54:22, max mem: 20.9 GB 
[11/24 11:10:07 visual_prompt]: 	Training 500/553. train loss: 54.7606,	0.8320 s / batch. (data: 7.96e-03). ETA=8:03:50, max mem: 20.9 GB 
[11/24 11:10:59 visual_prompt]: Epoch 37 / 100: avg data time: 1.37e-01, avg batch time: 0.9661, average train loss: 63.5501
[11/24 11:11:54 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3106, average loss: 28.5747
[11/24 11:11:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.05	
[11/24 11:11:54 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[11/24 11:13:32 visual_prompt]: 	Training 100/553. train loss: 72.7354,	0.8160 s / batch. (data: 3.23e-04). ETA=7:52:26, max mem: 20.9 GB 
[11/24 11:15:09 visual_prompt]: 	Training 200/553. train loss: 29.6928,	0.8535 s / batch. (data: 5.43e-03). ETA=8:12:45, max mem: 20.9 GB 
[11/24 11:16:46 visual_prompt]: 	Training 300/553. train loss: 144.9714,	0.8412 s / batch. (data: 1.05e-02). ETA=8:04:12, max mem: 20.9 GB 
[11/24 11:18:20 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8199 s / batch. (data: 4.54e-04). ETA=7:50:37, max mem: 20.9 GB 
[11/24 11:19:58 visual_prompt]: 	Training 500/553. train loss: 5.5101,	0.8400 s / batch. (data: 2.96e-04). ETA=8:00:44, max mem: 20.9 GB 
[11/24 11:20:47 visual_prompt]: Epoch 38 / 100: avg data time: 1.36e-01, avg batch time: 0.9649, average train loss: 68.1992
[11/24 11:21:42 visual_prompt]: Inference (val):avg data time: 2.77e-04, avg batch time: 0.3105, average loss: 51.7917
[11/24 11:21:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.04	
[11/24 11:21:42 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[11/24 11:23:20 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8310 s / batch. (data: 6.91e-03). ETA=7:53:28, max mem: 20.9 GB 
[11/24 11:25:03 visual_prompt]: 	Training 200/553. train loss: 141.9730,	0.8333 s / batch. (data: 7.23e-04). ETA=7:53:23, max mem: 20.9 GB 
[11/24 11:26:50 visual_prompt]: 	Training 300/553. train loss: 44.2764,	0.8355 s / batch. (data: 4.28e-04). ETA=7:53:15, max mem: 20.9 GB 
[11/24 11:28:32 visual_prompt]: 	Training 400/553. train loss: 30.7900,	0.8397 s / batch. (data: 3.31e-04). ETA=7:54:14, max mem: 20.9 GB 
[11/24 11:30:15 visual_prompt]: 	Training 500/553. train loss: 35.6469,	1.8769 s / batch. (data: 1.06e+00). ETA=17:36:52, max mem: 20.9 GB 
[11/24 11:31:06 visual_prompt]: Epoch 39 / 100: avg data time: 1.92e-01, avg batch time: 1.0197, average train loss: 62.3315
[11/24 11:32:05 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3117, average loss: 91.6431
[11/24 11:32:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.29	
[11/24 11:32:05 visual_prompt]: Stopping early.
[11/24 11:32:05 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 11:32:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 11:32:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/24 11:32:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 11:32:05 visual_prompt]: Training with config:
[11/24 11:32:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/24 11:32:05 visual_prompt]: Loading training data...
[11/24 11:32:05 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 11:32:05 visual_prompt]: Loading validation data...
[11/24 11:32:05 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 11:32:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 11:32:11 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 11:32:11 visual_prompt]: tuned percent:0.525
[11/24 11:32:11 visual_prompt]: Device used for model: 0
[11/24 11:32:11 visual_prompt]: Setting up Evaluator...
[11/24 11:32:11 visual_prompt]: Setting up Trainer...
[11/24 11:32:11 visual_prompt]: 	Setting up the optimizer...
[11/24 11:32:11 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 11:33:58 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8383 s / batch. (data: 2.23e-02). ETA=12:51:16, max mem: 20.9 GB 
[11/24 11:35:39 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8408 s / batch. (data: 1.09e-02). ETA=12:52:08, max mem: 20.9 GB 
[11/24 11:37:24 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.0796 s / batch. (data: 2.38e-01). ETA=16:29:38, max mem: 20.9 GB 
[11/24 11:39:04 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8360 s / batch. (data: 5.40e-03). ETA=12:44:56, max mem: 20.9 GB 
[11/24 11:40:47 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8320 s / batch. (data: 3.31e-04). ETA=12:39:54, max mem: 20.9 GB 
[11/24 11:41:40 visual_prompt]: Epoch 1 / 100: avg data time: 1.98e-01, avg batch time: 1.0292, average train loss: 1.5403
[11/24 11:42:38 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3107, average loss: 1.5201
[11/24 11:42:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 11:42:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/24 11:44:23 visual_prompt]: 	Training 100/553. train loss: 11.4707,	0.9320 s / batch. (data: 1.13e-01). ETA=14:08:50, max mem: 20.9 GB 
[11/24 11:46:04 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8280 s / batch. (data: 3.38e-04). ETA=12:32:44, max mem: 20.9 GB 
[11/24 11:47:47 visual_prompt]: 	Training 300/553. train loss: 5.9929,	0.9837 s / batch. (data: 1.63e-01). ETA=14:52:41, max mem: 20.9 GB 
[11/24 11:49:26 visual_prompt]: 	Training 400/553. train loss: 7.5676,	0.8409 s / batch. (data: 2.97e-04). ETA=12:41:38, max mem: 20.9 GB 
[11/24 11:51:08 visual_prompt]: 	Training 500/553. train loss: 1.9307,	0.8365 s / batch. (data: 3.02e-04). ETA=12:36:16, max mem: 20.9 GB 
[11/24 11:52:01 visual_prompt]: Epoch 2 / 100: avg data time: 1.85e-01, avg batch time: 1.0175, average train loss: 9.8064
[11/24 11:53:01 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3090, average loss: 21.4495
[11/24 11:53:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.40	
[11/24 11:53:01 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/24 11:54:47 visual_prompt]: 	Training 100/553. train loss: 15.6780,	0.8320 s / batch. (data: 4.83e-04). ETA=12:30:03, max mem: 20.9 GB 
[11/24 11:56:31 visual_prompt]: 	Training 200/553. train loss: 6.2633,	0.8327 s / batch. (data: 8.63e-03). ETA=12:29:20, max mem: 20.9 GB 
[11/24 11:58:13 visual_prompt]: 	Training 300/553. train loss: 8.7254,	0.8562 s / batch. (data: 2.34e-02). ETA=12:49:04, max mem: 20.9 GB 
[11/24 11:59:57 visual_prompt]: 	Training 400/553. train loss: 15.1111,	0.8400 s / batch. (data: 7.95e-03). ETA=12:33:07, max mem: 20.9 GB 
[11/24 12:01:41 visual_prompt]: 	Training 500/553. train loss: 71.0609,	1.3680 s / batch. (data: 5.36e-01). ETA=20:24:14, max mem: 20.9 GB 
[11/24 12:02:34 visual_prompt]: Epoch 3 / 100: avg data time: 2.03e-01, avg batch time: 1.0353, average train loss: 14.2301
[11/24 12:03:32 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3095, average loss: 21.8537
[11/24 12:03:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.46	
[11/24 12:03:32 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/24 12:05:18 visual_prompt]: 	Training 100/553. train loss: 9.9626,	0.8411 s / batch. (data: 3.33e-04). ETA=12:30:33, max mem: 20.9 GB 
[11/24 12:07:00 visual_prompt]: 	Training 200/553. train loss: 59.3786,	0.8251 s / batch. (data: 7.95e-03). ETA=12:14:54, max mem: 20.9 GB 
[11/24 12:08:41 visual_prompt]: 	Training 300/553. train loss: 4.1254,	1.5440 s / batch. (data: 7.13e-01). ETA=22:52:38, max mem: 20.9 GB 
[11/24 12:10:19 visual_prompt]: 	Training 400/553. train loss: 6.7726,	1.3200 s / batch. (data: 4.73e-01). ETA=19:31:15, max mem: 20.9 GB 
[11/24 12:12:02 visual_prompt]: 	Training 500/553. train loss: 49.7587,	3.6256 s / batch. (data: 2.80e+00). ETA=2 days, 5:31:08, max mem: 20.9 GB 
[11/24 12:12:55 visual_prompt]: Epoch 4 / 100: avg data time: 1.89e-01, avg batch time: 1.0194, average train loss: 26.8838
[11/24 12:13:53 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3105, average loss: 25.4921
[11/24 12:13:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.11	
[11/24 12:13:53 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/24 12:15:38 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8269 s / batch. (data: 3.10e-04). ETA=12:10:15, max mem: 20.9 GB 
[11/24 12:17:21 visual_prompt]: 	Training 200/553. train loss: 14.0414,	1.3300 s / batch. (data: 5.16e-01). ETA=19:32:23, max mem: 20.9 GB 
[11/24 12:19:05 visual_prompt]: 	Training 300/553. train loss: 54.3054,	0.8272 s / batch. (data: 5.50e-03). ETA=12:07:46, max mem: 20.9 GB 
[11/24 12:20:48 visual_prompt]: 	Training 400/553. train loss: 31.3902,	0.8487 s / batch. (data: 1.64e-02). ETA=12:25:18, max mem: 20.9 GB 
[11/24 12:22:31 visual_prompt]: 	Training 500/553. train loss: 138.4451,	0.8440 s / batch. (data: 3.20e-04). ETA=12:19:43, max mem: 20.9 GB 
[11/24 12:23:26 visual_prompt]: Epoch 5 / 100: avg data time: 2.05e-01, avg batch time: 1.0346, average train loss: 29.4678
[11/24 12:24:25 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3080, average loss: 44.2434
[11/24 12:24:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.93	
[11/24 12:24:25 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/24 12:26:13 visual_prompt]: 	Training 100/553. train loss: 152.0571,	0.8313 s / batch. (data: 3.33e-04). ETA=12:06:31, max mem: 20.9 GB 
[11/24 12:27:53 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8193 s / batch. (data: 1.55e-02). ETA=11:54:38, max mem: 20.9 GB 
[11/24 12:29:32 visual_prompt]: 	Training 300/553. train loss: 257.1109,	0.8200 s / batch. (data: 3.00e-04). ETA=11:53:52, max mem: 20.9 GB 
[11/24 12:31:16 visual_prompt]: 	Training 400/553. train loss: 63.9711,	0.8399 s / batch. (data: 1.21e-02). ETA=12:09:49, max mem: 20.9 GB 
[11/24 12:32:54 visual_prompt]: 	Training 500/553. train loss: 22.8794,	0.8243 s / batch. (data: 3.47e-04). ETA=11:54:52, max mem: 20.9 GB 
[11/24 12:33:46 visual_prompt]: Epoch 6 / 100: avg data time: 1.86e-01, avg batch time: 1.0158, average train loss: 43.9060
[11/24 12:34:44 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3108, average loss: 43.3593
[11/24 12:34:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.47	
[11/24 12:34:44 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/24 12:36:26 visual_prompt]: 	Training 100/553. train loss: 202.3694,	0.8312 s / batch. (data: 7.97e-03). ETA=11:58:42, max mem: 20.9 GB 
[11/24 12:38:06 visual_prompt]: 	Training 200/553. train loss: 34.1242,	0.9265 s / batch. (data: 9.73e-02). ETA=13:19:37, max mem: 20.9 GB 
[11/24 12:39:50 visual_prompt]: 	Training 300/553. train loss: 39.2158,	1.9452 s / batch. (data: 1.13e+00). ETA=1 day, 3:55:32, max mem: 20.9 GB 
[11/24 12:41:30 visual_prompt]: 	Training 400/553. train loss: 74.3282,	1.7071 s / batch. (data: 8.86e-01). ETA=1 day, 0:27:36, max mem: 20.9 GB 
[11/24 12:43:07 visual_prompt]: 	Training 500/553. train loss: 79.8908,	0.8571 s / batch. (data: 4.46e-02). ETA=12:15:24, max mem: 20.9 GB 
[11/24 12:43:59 visual_prompt]: Epoch 7 / 100: avg data time: 1.75e-01, avg batch time: 1.0044, average train loss: 55.4997
[11/24 12:44:56 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3094, average loss: 23.6999
[11/24 12:44:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.52	
[11/24 12:44:56 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/24 12:46:38 visual_prompt]: 	Training 100/553. train loss: 2.0181,	0.8216 s / batch. (data: 3.37e-04). ETA=11:42:53, max mem: 20.9 GB 
[11/24 12:48:18 visual_prompt]: 	Training 200/553. train loss: 61.1637,	0.8506 s / batch. (data: 2.61e-02). ETA=12:06:16, max mem: 20.9 GB 
[11/24 12:50:03 visual_prompt]: 	Training 300/553. train loss: 145.5445,	0.8335 s / batch. (data: 8.34e-04). ETA=11:50:18, max mem: 20.9 GB 
[11/24 12:51:46 visual_prompt]: 	Training 400/553. train loss: 41.8939,	0.8172 s / batch. (data: 3.20e-04). ETA=11:35:01, max mem: 20.9 GB 
[11/24 12:53:29 visual_prompt]: 	Training 500/553. train loss: 21.5642,	1.4273 s / batch. (data: 6.06e-01). ETA=20:11:33, max mem: 20.9 GB 
[11/24 12:54:23 visual_prompt]: Epoch 8 / 100: avg data time: 1.97e-01, avg batch time: 1.0244, average train loss: 56.5643
[11/24 12:55:22 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3089, average loss: 57.6910
[11/24 12:55:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.04	
[11/24 12:55:22 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/24 12:57:08 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8520 s / batch. (data: 7.95e-03). ETA=12:01:01, max mem: 20.9 GB 
[11/24 12:58:50 visual_prompt]: 	Training 200/553. train loss: 785.9756,	0.8226 s / batch. (data: 1.00e-02). ETA=11:34:46, max mem: 20.9 GB 
[11/24 13:00:34 visual_prompt]: 	Training 300/553. train loss: 56.0445,	1.7175 s / batch. (data: 8.96e-01). ETA=1 day, 0:07:44, max mem: 20.9 GB 
[11/24 13:02:17 visual_prompt]: 	Training 400/553. train loss: 135.3336,	0.8440 s / batch. (data: 3.10e-04). ETA=11:50:01, max mem: 20.9 GB 
[11/24 13:04:01 visual_prompt]: 	Training 500/553. train loss: 26.5560,	0.9887 s / batch. (data: 1.74e-01). ETA=13:50:05, max mem: 20.9 GB 
[11/24 13:04:54 visual_prompt]: Epoch 9 / 100: avg data time: 2.07e-01, avg batch time: 1.0337, average train loss: 68.2584
[11/24 13:05:52 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3097, average loss: 19.6656
[11/24 13:05:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.07	
[11/24 13:05:52 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/24 13:07:41 visual_prompt]: 	Training 100/553. train loss: 115.1190,	0.8292 s / batch. (data: 7.96e-03). ETA=11:34:05, max mem: 20.9 GB 
[11/24 13:09:21 visual_prompt]: 	Training 200/553. train loss: 64.3213,	0.8309 s / batch. (data: 1.07e-02). ETA=11:34:07, max mem: 20.9 GB 
[11/24 13:10:59 visual_prompt]: 	Training 300/553. train loss: 6.0597,	1.1005 s / batch. (data: 2.64e-01). ETA=15:17:32, max mem: 20.9 GB 
[11/24 13:12:36 visual_prompt]: 	Training 400/553. train loss: 90.0477,	0.8440 s / batch. (data: 3.33e-04). ETA=11:42:13, max mem: 20.9 GB 
[11/24 13:14:17 visual_prompt]: 	Training 500/553. train loss: 8.8133,	0.8348 s / batch. (data: 3.19e-04). ETA=11:33:13, max mem: 20.9 GB 
[11/24 13:15:08 visual_prompt]: Epoch 10 / 100: avg data time: 1.78e-01, avg batch time: 1.0050, average train loss: 76.3979
[11/24 13:16:04 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3114, average loss: 44.8438
[11/24 13:16:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.15	
[11/24 13:16:04 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/24 13:17:49 visual_prompt]: 	Training 100/553. train loss: 196.5261,	0.8491 s / batch. (data: 2.10e-02). ETA=11:42:53, max mem: 20.9 GB 
[11/24 13:19:30 visual_prompt]: 	Training 200/553. train loss: 278.2780,	0.8360 s / batch. (data: 3.23e-04). ETA=11:30:40, max mem: 20.9 GB 
[11/24 13:21:08 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.1126 s / batch. (data: 1.30e+00). ETA=1 day, 5:01:48, max mem: 20.9 GB 
[11/24 13:22:45 visual_prompt]: 	Training 400/553. train loss: 36.4821,	0.8497 s / batch. (data: 1.56e-02). ETA=11:39:09, max mem: 20.9 GB 
[11/24 13:24:23 visual_prompt]: 	Training 500/553. train loss: 162.8521,	0.8360 s / batch. (data: 7.96e-03). ETA=11:26:29, max mem: 20.9 GB 
[11/24 13:25:13 visual_prompt]: Epoch 11 / 100: avg data time: 1.65e-01, avg batch time: 0.9928, average train loss: 83.0005
[11/24 13:26:10 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3134, average loss: 18.7106
[11/24 13:26:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.66	
[11/24 13:26:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/24 13:27:54 visual_prompt]: 	Training 100/553. train loss: 126.5586,	0.9720 s / batch. (data: 1.40e-01). ETA=13:15:41, max mem: 20.9 GB 
[11/24 13:29:34 visual_prompt]: 	Training 200/553. train loss: 152.5565,	0.9224 s / batch. (data: 1.03e-01). ETA=12:33:31, max mem: 20.9 GB 
[11/24 13:31:11 visual_prompt]: 	Training 300/553. train loss: 21.7700,	0.8204 s / batch. (data: 1.20e-02). ETA=11:08:52, max mem: 20.9 GB 
[11/24 13:32:51 visual_prompt]: 	Training 400/553. train loss: 93.6778,	0.8357 s / batch. (data: 3.21e-04). ETA=11:19:54, max mem: 20.9 GB 
[11/24 13:34:29 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8315 s / batch. (data: 2.20e-02). ETA=11:15:07, max mem: 20.9 GB 
[11/24 13:35:20 visual_prompt]: Epoch 12 / 100: avg data time: 1.68e-01, avg batch time: 0.9951, average train loss: 86.6268
[11/24 13:36:16 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3100, average loss: 228.3232
[11/24 13:36:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.58	
[11/24 13:36:16 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/24 13:38:01 visual_prompt]: 	Training 100/553. train loss: 23.4021,	0.8143 s / batch. (data: 5.44e-03). ETA=10:59:06, max mem: 20.9 GB 
[11/24 13:39:36 visual_prompt]: 	Training 200/553. train loss: 55.1706,	0.8155 s / batch. (data: 3.10e-04). ETA=10:58:40, max mem: 20.9 GB 
[11/24 13:41:16 visual_prompt]: 	Training 300/553. train loss: 16.4816,	1.9027 s / batch. (data: 1.07e+00). ETA=1 day, 1:33:39, max mem: 20.9 GB 
[11/24 13:42:53 visual_prompt]: 	Training 400/553. train loss: 7.3962,	0.8177 s / batch. (data: 3.42e-04). ETA=10:57:46, max mem: 20.9 GB 
[11/24 13:44:34 visual_prompt]: 	Training 500/553. train loss: 32.3706,	0.8307 s / batch. (data: 2.94e-04). ETA=11:06:47, max mem: 20.9 GB 
[11/24 13:45:25 visual_prompt]: Epoch 13 / 100: avg data time: 1.65e-01, avg batch time: 0.9926, average train loss: 89.5001
[11/24 13:46:22 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3102, average loss: 41.0821
[11/24 13:46:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.43	
[11/24 13:46:22 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/24 13:48:06 visual_prompt]: 	Training 100/553. train loss: 79.9684,	0.8227 s / batch. (data: 2.94e-04). ETA=10:58:20, max mem: 20.9 GB 
[11/24 13:49:45 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8340 s / batch. (data: 2.89e-04). ETA=11:06:00, max mem: 20.9 GB 
[11/24 13:51:23 visual_prompt]: 	Training 300/553. train loss: 81.4320,	0.8880 s / batch. (data: 6.00e-02). ETA=11:47:35, max mem: 20.9 GB 
[11/24 13:53:05 visual_prompt]: 	Training 400/553. train loss: 23.2426,	0.8596 s / batch. (data: 2.36e-02). ETA=11:23:34, max mem: 20.9 GB 
[11/24 13:54:49 visual_prompt]: 	Training 500/553. train loss: 128.8445,	0.8400 s / batch. (data: 3.28e-04). ETA=11:06:33, max mem: 20.9 GB 
[11/24 13:55:42 visual_prompt]: Epoch 14 / 100: avg data time: 1.86e-01, avg batch time: 1.0130, average train loss: 94.1733
[11/24 13:56:41 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.3097, average loss: 72.9262
[11/24 13:56:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.32	
[11/24 13:56:41 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/24 13:58:27 visual_prompt]: 	Training 100/553. train loss: 107.7245,	0.9360 s / batch. (data: 1.07e-01). ETA=12:20:18, max mem: 20.9 GB 
[11/24 14:00:09 visual_prompt]: 	Training 200/553. train loss: 338.1583,	0.8353 s / batch. (data: 3.17e-04). ETA=10:59:16, max mem: 20.9 GB 
[11/24 14:01:54 visual_prompt]: 	Training 300/553. train loss: 10.7235,	0.8260 s / batch. (data: 3.26e-04). ETA=10:50:35, max mem: 20.9 GB 
[11/24 14:03:34 visual_prompt]: 	Training 400/553. train loss: 204.6113,	1.2204 s / batch. (data: 3.96e-01). ETA=15:59:10, max mem: 20.9 GB 
[11/24 14:05:17 visual_prompt]: 	Training 500/553. train loss: 120.6582,	0.8480 s / batch. (data: 2.61e-04). ETA=11:05:06, max mem: 20.9 GB 
[11/24 14:06:12 visual_prompt]: Epoch 15 / 100: avg data time: 2.05e-01, avg batch time: 1.0321, average train loss: 93.8931
[11/24 14:07:10 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3093, average loss: 5.1922
[11/24 14:07:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.96	
[11/24 14:07:10 visual_prompt]: Best epoch 15: best metric: -5.192
[11/24 14:07:10 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/24 14:08:55 visual_prompt]: 	Training 100/553. train loss: 40.4202,	0.8384 s / batch. (data: 3.19e-04). ETA=10:55:26, max mem: 20.9 GB 
[11/24 14:10:39 visual_prompt]: 	Training 200/553. train loss: 140.7945,	0.8160 s / batch. (data: 3.07e-04). ETA=10:36:33, max mem: 20.9 GB 
[11/24 14:12:20 visual_prompt]: 	Training 300/553. train loss: 13.6437,	0.8214 s / batch. (data: 2.88e-04). ETA=10:39:23, max mem: 20.9 GB 
[11/24 14:13:59 visual_prompt]: 	Training 400/553. train loss: 64.7440,	0.8340 s / batch. (data: 8.13e-04). ETA=10:47:48, max mem: 20.9 GB 
[11/24 14:15:37 visual_prompt]: 	Training 500/553. train loss: 203.5595,	1.0000 s / batch. (data: 1.67e-01). ETA=12:55:04, max mem: 20.9 GB 
[11/24 14:16:29 visual_prompt]: Epoch 16 / 100: avg data time: 1.83e-01, avg batch time: 1.0106, average train loss: 78.6311
[11/24 14:17:26 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3095, average loss: 4.1551
[11/24 14:17:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.29	
[11/24 14:17:26 visual_prompt]: Best epoch 16: best metric: -4.155
[11/24 14:17:26 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/24 14:19:08 visual_prompt]: 	Training 100/553. train loss: 3.5588,	0.8423 s / batch. (data: 7.94e-03). ETA=10:50:41, max mem: 20.9 GB 
[11/24 14:20:48 visual_prompt]: 	Training 200/553. train loss: 182.7799,	0.8150 s / batch. (data: 2.92e-04). ETA=10:28:17, max mem: 20.9 GB 
[11/24 14:22:26 visual_prompt]: 	Training 300/553. train loss: 111.7145,	0.8200 s / batch. (data: 3.12e-04). ETA=10:30:45, max mem: 20.9 GB 
[11/24 14:24:04 visual_prompt]: 	Training 400/553. train loss: 320.7078,	1.1328 s / batch. (data: 3.17e-01). ETA=14:29:25, max mem: 20.9 GB 
[11/24 14:25:43 visual_prompt]: 	Training 500/553. train loss: 124.6979,	1.6880 s / batch. (data: 8.62e-01). ETA=21:32:45, max mem: 20.9 GB 
[11/24 14:26:35 visual_prompt]: Epoch 17 / 100: avg data time: 1.66e-01, avg batch time: 0.9930, average train loss: 80.3591
[11/24 14:27:32 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3097, average loss: 219.9071
[11/24 14:27:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.28	
[11/24 14:27:32 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/24 14:29:15 visual_prompt]: 	Training 100/553. train loss: 65.2004,	0.8400 s / batch. (data: 7.57e-04). ETA=10:41:11, max mem: 20.9 GB 
[11/24 14:30:56 visual_prompt]: 	Training 200/553. train loss: 30.3554,	0.8466 s / batch. (data: 7.54e-04). ETA=10:44:49, max mem: 20.9 GB 
[11/24 14:32:35 visual_prompt]: 	Training 300/553. train loss: 118.4125,	0.8440 s / batch. (data: 2.95e-04). ETA=10:41:24, max mem: 20.9 GB 
[11/24 14:34:13 visual_prompt]: 	Training 400/553. train loss: 23.7052,	0.8501 s / batch. (data: 1.20e-02). ETA=10:44:37, max mem: 20.9 GB 
[11/24 14:35:52 visual_prompt]: 	Training 500/553. train loss: 84.4155,	0.8208 s / batch. (data: 5.46e-03). ETA=10:21:02, max mem: 20.9 GB 
[11/24 14:36:42 visual_prompt]: Epoch 18 / 100: avg data time: 1.67e-01, avg batch time: 0.9949, average train loss: 81.1541
[11/24 14:37:39 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3108, average loss: 55.5910
[11/24 14:37:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.31	
[11/24 14:37:39 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/24 14:39:21 visual_prompt]: 	Training 100/553. train loss: 33.5354,	1.0880 s / batch. (data: 2.58e-01). ETA=13:40:27, max mem: 20.9 GB 
[11/24 14:41:01 visual_prompt]: 	Training 200/553. train loss: 48.6493,	0.8271 s / batch. (data: 3.13e-04). ETA=10:22:18, max mem: 20.9 GB 
[11/24 14:42:39 visual_prompt]: 	Training 300/553. train loss: 153.9044,	0.8400 s / batch. (data: 3.19e-04). ETA=10:30:36, max mem: 20.9 GB 
[11/24 14:44:19 visual_prompt]: 	Training 400/553. train loss: 50.0509,	0.8192 s / batch. (data: 7.99e-04). ETA=10:13:39, max mem: 20.9 GB 
[11/24 14:45:55 visual_prompt]: 	Training 500/553. train loss: 2.3170,	0.8554 s / batch. (data: 5.44e-03). ETA=10:39:21, max mem: 20.9 GB 
[11/24 14:46:46 visual_prompt]: Epoch 19 / 100: avg data time: 1.61e-01, avg batch time: 0.9897, average train loss: 72.0063
[11/24 14:47:42 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3105, average loss: 17.9899
[11/24 14:47:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.09	
[11/24 14:47:42 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/24 14:49:24 visual_prompt]: 	Training 100/553. train loss: 5.2882,	0.8560 s / batch. (data: 3.16e-04). ETA=10:37:36, max mem: 20.9 GB 
[11/24 14:51:03 visual_prompt]: 	Training 200/553. train loss: 18.6954,	0.8440 s / batch. (data: 3.27e-04). ETA=10:27:16, max mem: 20.9 GB 
[11/24 14:52:43 visual_prompt]: 	Training 300/553. train loss: 22.6528,	0.8201 s / batch. (data: 3.34e-04). ETA=10:08:08, max mem: 20.9 GB 
[11/24 14:54:25 visual_prompt]: 	Training 400/553. train loss: 22.2894,	0.8383 s / batch. (data: 7.95e-03). ETA=10:20:12, max mem: 20.9 GB 
[11/24 14:56:08 visual_prompt]: 	Training 500/553. train loss: 41.9790,	0.8473 s / batch. (data: 1.19e-02). ETA=10:25:27, max mem: 20.9 GB 
[11/24 14:57:03 visual_prompt]: Epoch 20 / 100: avg data time: 1.86e-01, avg batch time: 1.0135, average train loss: 79.3548
[11/24 14:58:02 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3102, average loss: 51.9980
[11/24 14:58:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.62	
[11/24 14:58:02 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/24 14:59:51 visual_prompt]: 	Training 100/553. train loss: 77.1954,	0.8171 s / batch. (data: 3.06e-04). ETA=10:01:07, max mem: 20.9 GB 
[11/24 15:01:33 visual_prompt]: 	Training 200/553. train loss: 90.5409,	0.8169 s / batch. (data: 3.54e-04). ETA=9:59:35, max mem: 20.9 GB 
[11/24 15:03:14 visual_prompt]: 	Training 300/553. train loss: 120.1923,	1.0324 s / batch. (data: 2.04e-01). ETA=12:36:01, max mem: 20.9 GB 
[11/24 15:04:56 visual_prompt]: 	Training 400/553. train loss: 23.0740,	0.8216 s / batch. (data: 3.48e-04). ETA=10:00:17, max mem: 20.9 GB 
[11/24 15:06:42 visual_prompt]: 	Training 500/553. train loss: 62.0444,	0.8487 s / batch. (data: 8.65e-03). ETA=10:18:41, max mem: 20.9 GB 
[11/24 15:07:36 visual_prompt]: Epoch 21 / 100: avg data time: 2.11e-01, avg batch time: 1.0380, average train loss: 80.6375
[11/24 15:08:33 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3112, average loss: 5.7173
[11/24 15:08:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.95	
[11/24 15:08:33 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/24 15:10:21 visual_prompt]: 	Training 100/553. train loss: 45.5380,	0.8200 s / batch. (data: 3.20e-04). ETA=9:55:41, max mem: 20.9 GB 
[11/24 15:12:06 visual_prompt]: 	Training 200/553. train loss: 14.3998,	0.8400 s / batch. (data: 7.94e-03). ETA=10:08:47, max mem: 20.9 GB 
[11/24 15:13:47 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8280 s / batch. (data: 7.95e-03). ETA=9:58:43, max mem: 20.9 GB 
[11/24 15:15:33 visual_prompt]: 	Training 400/553. train loss: 41.4337,	0.8358 s / batch. (data: 7.05e-04). ETA=10:03:00, max mem: 20.9 GB 
[11/24 15:17:15 visual_prompt]: 	Training 500/553. train loss: 41.4893,	0.8080 s / batch. (data: 3.22e-04). ETA=9:41:34, max mem: 20.9 GB 
[11/24 15:18:11 visual_prompt]: Epoch 22 / 100: avg data time: 2.16e-01, avg batch time: 1.0448, average train loss: 82.7818
[11/24 15:19:11 visual_prompt]: Inference (val):avg data time: 4.44e-05, avg batch time: 0.3112, average loss: 30.7563
[11/24 15:19:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.44	
[11/24 15:19:11 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/24 15:20:56 visual_prompt]: 	Training 100/553. train loss: 123.4828,	0.8200 s / batch. (data: 5.43e-03). ETA=9:48:09, max mem: 20.9 GB 
[11/24 15:22:33 visual_prompt]: 	Training 200/553. train loss: 15.2011,	0.8359 s / batch. (data: 3.05e-04). ETA=9:58:09, max mem: 20.9 GB 
[11/24 15:24:11 visual_prompt]: 	Training 300/553. train loss: 234.7354,	0.8138 s / batch. (data: 2.94e-04). ETA=9:40:57, max mem: 20.9 GB 
[11/24 15:25:52 visual_prompt]: 	Training 400/553. train loss: 102.2313,	0.8201 s / batch. (data: 3.18e-04). ETA=9:44:06, max mem: 20.9 GB 
[11/24 15:27:34 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8200 s / batch. (data: 3.86e-04). ETA=9:42:38, max mem: 20.9 GB 
[11/24 15:28:27 visual_prompt]: Epoch 23 / 100: avg data time: 1.78e-01, avg batch time: 1.0049, average train loss: 75.8306
[11/24 15:29:24 visual_prompt]: Inference (val):avg data time: 4.20e-04, avg batch time: 0.3094, average loss: 146.7328
[11/24 15:29:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.00	
[11/24 15:29:24 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/24 15:31:10 visual_prompt]: 	Training 100/553. train loss: 80.0707,	0.8202 s / batch. (data: 1.18e-02). ETA=9:40:44, max mem: 20.9 GB 
[11/24 15:32:52 visual_prompt]: 	Training 200/553. train loss: 151.4542,	0.8338 s / batch. (data: 1.73e-02). ETA=9:48:56, max mem: 20.9 GB 
[11/24 15:34:29 visual_prompt]: 	Training 300/553. train loss: 10.0722,	0.8580 s / batch. (data: 3.64e-04). ETA=10:04:35, max mem: 20.9 GB 
[11/24 15:36:05 visual_prompt]: 	Training 400/553. train loss: 18.1664,	0.8456 s / batch. (data: 2.81e-02). ETA=9:54:28, max mem: 20.9 GB 
[11/24 15:37:42 visual_prompt]: 	Training 500/553. train loss: 34.1217,	0.8524 s / batch. (data: 1.02e-03). ETA=9:57:50, max mem: 20.9 GB 
[11/24 15:38:32 visual_prompt]: Epoch 24 / 100: avg data time: 1.64e-01, avg batch time: 0.9912, average train loss: 79.6634
[11/24 15:39:29 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3118, average loss: 23.4213
[11/24 15:39:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.63	
[11/24 15:39:29 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/24 15:41:21 visual_prompt]: 	Training 100/553. train loss: 141.2382,	0.8578 s / batch. (data: 1.06e-02). ETA=9:59:24, max mem: 20.9 GB 
[11/24 15:43:01 visual_prompt]: 	Training 200/553. train loss: 37.6976,	0.9555 s / batch. (data: 1.28e-01). ETA=11:06:07, max mem: 20.9 GB 
[11/24 15:44:45 visual_prompt]: 	Training 300/553. train loss: 216.4294,	1.0473 s / batch. (data: 2.38e-01). ETA=12:08:20, max mem: 20.9 GB 
[11/24 15:46:30 visual_prompt]: 	Training 400/553. train loss: 105.7682,	1.3440 s / batch. (data: 5.16e-01). ETA=15:32:28, max mem: 20.9 GB 
[11/24 15:48:14 visual_prompt]: 	Training 500/553. train loss: 40.7476,	1.7089 s / batch. (data: 8.99e-01). ETA=19:42:45, max mem: 20.9 GB 
[11/24 15:49:08 visual_prompt]: Epoch 25 / 100: avg data time: 2.19e-01, avg batch time: 1.0462, average train loss: 73.2479
[11/24 15:50:08 visual_prompt]: Inference (val):avg data time: 4.80e-05, avg batch time: 0.3075, average loss: 121.4401
[11/24 15:50:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.64	
[11/24 15:50:08 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/24 15:51:49 visual_prompt]: 	Training 100/553. train loss: 232.7358,	0.8288 s / batch. (data: 3.08e-04). ETA=9:31:33, max mem: 20.9 GB 
[11/24 15:53:35 visual_prompt]: 	Training 200/553. train loss: 22.3119,	2.0153 s / batch. (data: 1.21e+00). ETA=23:06:22, max mem: 20.9 GB 
[11/24 15:55:21 visual_prompt]: 	Training 300/553. train loss: 224.7032,	0.8743 s / batch. (data: 8.64e-04). ETA=9:59:59, max mem: 20.9 GB 
[11/24 15:56:57 visual_prompt]: 	Training 400/553. train loss: 11.7838,	0.8574 s / batch. (data: 2.14e-02). ETA=9:46:57, max mem: 20.9 GB 
[11/24 15:58:31 visual_prompt]: 	Training 500/553. train loss: 139.0402,	0.8312 s / batch. (data: 1.05e-02). ETA=9:27:40, max mem: 20.9 GB 
[11/24 15:59:21 visual_prompt]: Epoch 26 / 100: avg data time: 1.71e-01, avg batch time: 0.9988, average train loss: 90.0629
[11/24 16:00:15 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3108, average loss: 45.1047
[11/24 16:00:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.26	
[11/24 16:00:15 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[11/24 16:01:55 visual_prompt]: 	Training 100/553. train loss: 22.0259,	0.8437 s / batch. (data: 2.86e-04). ETA=9:34:01, max mem: 20.9 GB 
[11/24 16:03:29 visual_prompt]: 	Training 200/553. train loss: 166.5758,	0.8480 s / batch. (data: 1.19e-02). ETA=9:35:32, max mem: 20.9 GB 
[11/24 16:05:04 visual_prompt]: 	Training 300/553. train loss: 47.4541,	0.8473 s / batch. (data: 5.41e-03). ETA=9:33:37, max mem: 20.9 GB 
[11/24 16:06:39 visual_prompt]: 	Training 400/553. train loss: 40.6689,	0.8306 s / batch. (data: 5.40e-03). ETA=9:20:58, max mem: 20.9 GB 
[11/24 16:08:16 visual_prompt]: 	Training 500/553. train loss: 119.7567,	0.8410 s / batch. (data: 1.60e-02). ETA=9:26:34, max mem: 20.9 GB 
[11/24 16:09:04 visual_prompt]: Epoch 27 / 100: avg data time: 1.27e-01, avg batch time: 0.9566, average train loss: 77.0611
[11/24 16:09:58 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3106, average loss: 40.9811
[11/24 16:09:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.89	
[11/24 16:09:58 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[11/24 16:11:36 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8266 s / batch. (data: 5.51e-03). ETA=9:14:45, max mem: 20.9 GB 
[11/24 16:13:13 visual_prompt]: 	Training 200/553. train loss: 54.0814,	0.8321 s / batch. (data: 6.55e-03). ETA=9:17:02, max mem: 20.9 GB 
[11/24 16:14:50 visual_prompt]: 	Training 300/553. train loss: 81.7058,	1.4800 s / batch. (data: 6.55e-01). ETA=16:28:21, max mem: 20.9 GB 
[11/24 16:16:28 visual_prompt]: 	Training 400/553. train loss: 65.5140,	0.8275 s / batch. (data: 3.08e-04). ETA=9:11:14, max mem: 20.9 GB 
[11/24 16:18:12 visual_prompt]: 	Training 500/553. train loss: 182.4296,	0.8356 s / batch. (data: 5.45e-03). ETA=9:15:12, max mem: 20.9 GB 
[11/24 16:19:07 visual_prompt]: Epoch 28 / 100: avg data time: 1.64e-01, avg batch time: 0.9920, average train loss: 82.3342
[11/24 16:20:03 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3093, average loss: 18.1313
[11/24 16:20:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.82	
[11/24 16:20:03 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[11/24 16:21:50 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8316 s / batch. (data: 6.93e-04). ETA=9:10:29, max mem: 20.9 GB 
[11/24 16:23:26 visual_prompt]: 	Training 200/553. train loss: 130.6215,	1.6437 s / batch. (data: 8.31e-01). ETA=18:05:17, max mem: 20.9 GB 
[11/24 16:25:00 visual_prompt]: 	Training 300/553. train loss: 62.4456,	0.8432 s / batch. (data: 5.91e-03). ETA=9:15:20, max mem: 20.9 GB 
[11/24 16:26:37 visual_prompt]: 	Training 400/553. train loss: 62.6526,	0.8200 s / batch. (data: 3.60e-04). ETA=8:58:41, max mem: 20.9 GB 
[11/24 16:28:22 visual_prompt]: 	Training 500/553. train loss: 33.2094,	0.8322 s / batch. (data: 3.59e-04). ETA=9:05:17, max mem: 20.9 GB 
[11/24 16:29:17 visual_prompt]: Epoch 29 / 100: avg data time: 1.74e-01, avg batch time: 1.0015, average train loss: 68.3521
[11/24 16:30:14 visual_prompt]: Inference (val):avg data time: 2.05e-04, avg batch time: 0.3111, average loss: 34.2258
[11/24 16:30:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.92	
[11/24 16:30:14 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[11/24 16:31:52 visual_prompt]: 	Training 100/553. train loss: 201.8579,	0.8160 s / batch. (data: 2.95e-04). ETA=8:52:37, max mem: 20.9 GB 
[11/24 16:33:38 visual_prompt]: 	Training 200/553. train loss: 57.9761,	0.8259 s / batch. (data: 3.63e-04). ETA=8:57:42, max mem: 20.9 GB 
[11/24 16:35:22 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.5851 s / batch. (data: 7.57e-01). ETA=17:09:19, max mem: 20.9 GB 
[11/24 16:37:00 visual_prompt]: 	Training 400/553. train loss: 193.9628,	1.0579 s / batch. (data: 2.45e-01). ETA=11:25:11, max mem: 20.9 GB 
[11/24 16:38:36 visual_prompt]: 	Training 500/553. train loss: 112.9856,	1.3351 s / batch. (data: 5.01e-01). ETA=14:22:30, max mem: 20.9 GB 
[11/24 16:39:28 visual_prompt]: Epoch 30 / 100: avg data time: 1.74e-01, avg batch time: 1.0015, average train loss: 75.3280
[11/24 16:40:23 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3107, average loss: 84.2754
[11/24 16:40:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.72	
[11/24 16:40:23 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[11/24 16:42:04 visual_prompt]: 	Training 100/553. train loss: 85.2338,	0.8084 s / batch. (data: 5.43e-03). ETA=8:40:12, max mem: 20.9 GB 
[11/24 16:43:42 visual_prompt]: 	Training 200/553. train loss: 79.2875,	0.8501 s / batch. (data: 2.96e-02). ETA=9:05:38, max mem: 20.9 GB 
[11/24 16:45:15 visual_prompt]: 	Training 300/553. train loss: 85.9727,	0.8360 s / batch. (data: 3.05e-04). ETA=8:55:11, max mem: 20.9 GB 
[11/24 16:46:50 visual_prompt]: 	Training 400/553. train loss: 94.8413,	1.0199 s / batch. (data: 1.79e-01). ETA=10:51:14, max mem: 20.9 GB 
[11/24 16:48:26 visual_prompt]: 	Training 500/553. train loss: 3.8826,	0.8369 s / batch. (data: 2.04e-02). ETA=8:52:57, max mem: 20.9 GB 
[11/24 16:49:16 visual_prompt]: Epoch 31 / 100: avg data time: 1.35e-01, avg batch time: 0.9631, average train loss: 77.4775
[11/24 16:50:10 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3102, average loss: 164.9876
[11/24 16:50:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.85	
[11/24 16:50:10 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[11/24 16:51:50 visual_prompt]: 	Training 100/553. train loss: 33.6230,	0.8162 s / batch. (data: 5.51e-03). ETA=8:37:41, max mem: 20.9 GB 
[11/24 16:53:25 visual_prompt]: 	Training 200/553. train loss: 584.8047,	0.8441 s / batch. (data: 5.45e-03). ETA=8:54:01, max mem: 20.9 GB 
[11/24 16:55:05 visual_prompt]: 	Training 300/553. train loss: 94.3624,	0.8496 s / batch. (data: 1.56e-02). ETA=8:56:01, max mem: 20.9 GB 
[11/24 16:56:43 visual_prompt]: 	Training 400/553. train loss: 93.1784,	0.8400 s / batch. (data: 7.96e-03). ETA=8:48:36, max mem: 20.9 GB 
[11/24 16:58:18 visual_prompt]: 	Training 500/553. train loss: 66.1400,	0.8224 s / batch. (data: 8.53e-03). ETA=8:36:09, max mem: 20.9 GB 
[11/24 16:59:06 visual_prompt]: Epoch 32 / 100: avg data time: 1.41e-01, avg batch time: 0.9684, average train loss: 83.4985
[11/24 17:00:00 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.3101, average loss: 234.1134
[11/24 17:00:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.03	
[11/24 17:00:00 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[11/24 17:01:38 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8400 s / batch. (data: 3.04e-04). ETA=8:45:02, max mem: 20.9 GB 
[11/24 17:03:15 visual_prompt]: 	Training 200/553. train loss: 216.9189,	0.8465 s / batch. (data: 2.97e-04). ETA=8:47:41, max mem: 20.9 GB 
[11/24 17:04:49 visual_prompt]: 	Training 300/553. train loss: 64.6718,	0.8560 s / batch. (data: 2.88e-04). ETA=8:52:12, max mem: 20.9 GB 
[11/24 17:06:27 visual_prompt]: 	Training 400/553. train loss: 8.0882,	0.8249 s / batch. (data: 3.01e-04). ETA=8:31:29, max mem: 20.9 GB 
[11/24 17:08:03 visual_prompt]: 	Training 500/553. train loss: 4.8989,	0.8240 s / batch. (data: 3.23e-04). ETA=8:29:33, max mem: 20.9 GB 
[11/24 17:08:53 visual_prompt]: Epoch 33 / 100: avg data time: 1.34e-01, avg batch time: 0.9634, average train loss: 67.1823
[11/24 17:09:48 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3110, average loss: 66.9563
[11/24 17:09:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.69	
[11/24 17:09:48 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[11/24 17:11:30 visual_prompt]: 	Training 100/553. train loss: 55.6792,	0.8190 s / batch. (data: 1.05e-02). ETA=8:24:21, max mem: 20.9 GB 
[11/24 17:13:05 visual_prompt]: 	Training 200/553. train loss: 100.1291,	0.8240 s / batch. (data: 1.19e-02). ETA=8:26:05, max mem: 20.9 GB 
[11/24 17:14:41 visual_prompt]: 	Training 300/553. train loss: 10.0560,	0.8242 s / batch. (data: 3.58e-04). ETA=8:24:49, max mem: 20.9 GB 
[11/24 17:16:18 visual_prompt]: 	Training 400/553. train loss: 19.5371,	0.8440 s / batch. (data: 2.73e-04). ETA=8:35:33, max mem: 20.9 GB 
[11/24 17:17:55 visual_prompt]: 	Training 500/553. train loss: 9.3318,	1.3760 s / batch. (data: 5.35e-01). ETA=13:58:15, max mem: 20.9 GB 
[11/24 17:18:45 visual_prompt]: Epoch 34 / 100: avg data time: 1.42e-01, avg batch time: 0.9711, average train loss: 75.3529
[11/24 17:19:40 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3114, average loss: 29.7108
[11/24 17:19:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.47	
[11/24 17:19:40 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[11/24 17:21:22 visual_prompt]: 	Training 100/553. train loss: 29.5550,	0.8239 s / batch. (data: 5.60e-03). ETA=8:19:47, max mem: 20.9 GB 
[11/24 17:22:59 visual_prompt]: 	Training 200/553. train loss: 86.2786,	0.8206 s / batch. (data: 3.10e-04). ETA=8:16:26, max mem: 20.9 GB 
[11/24 17:24:35 visual_prompt]: 	Training 300/553. train loss: 75.0879,	0.8247 s / batch. (data: 3.33e-04). ETA=8:17:32, max mem: 20.9 GB 
[11/24 17:26:10 visual_prompt]: 	Training 400/553. train loss: 96.4444,	0.8121 s / batch. (data: 5.42e-03). ETA=8:08:34, max mem: 20.9 GB 
[11/24 17:27:45 visual_prompt]: 	Training 500/553. train loss: 102.2573,	0.9397 s / batch. (data: 1.11e-01). ETA=9:23:48, max mem: 20.9 GB 
[11/24 17:28:37 visual_prompt]: Epoch 35 / 100: avg data time: 1.43e-01, avg batch time: 0.9709, average train loss: 72.9441
[11/24 17:29:32 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3099, average loss: 154.1847
[11/24 17:29:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.02	
[11/24 17:29:32 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[11/24 17:31:12 visual_prompt]: 	Training 100/553. train loss: 229.5056,	0.8640 s / batch. (data: 7.95e-03). ETA=8:36:10, max mem: 20.9 GB 
[11/24 17:32:49 visual_prompt]: 	Training 200/553. train loss: 52.7191,	0.8268 s / batch. (data: 7.96e-03). ETA=8:12:34, max mem: 20.9 GB 
[11/24 17:34:28 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8065 s / batch. (data: 2.92e-04). ETA=7:59:08, max mem: 20.9 GB 
[11/24 17:36:04 visual_prompt]: 	Training 400/553. train loss: 8.9776,	0.8360 s / batch. (data: 3.75e-04). ETA=8:15:15, max mem: 20.9 GB 
[11/24 17:37:42 visual_prompt]: 	Training 500/553. train loss: 9.7776,	0.9565 s / batch. (data: 1.31e-01). ETA=9:25:04, max mem: 20.9 GB 
[11/24 17:38:30 visual_prompt]: Epoch 36 / 100: avg data time: 1.44e-01, avg batch time: 0.9716, average train loss: 79.3547
[11/24 17:39:25 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3098, average loss: 2.7531
[11/24 17:39:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.56	rocauc: 51.42	
[11/24 17:39:25 visual_prompt]: Best epoch 36: best metric: -2.753
[11/24 17:39:25 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[11/24 17:41:05 visual_prompt]: 	Training 100/553. train loss: 2.4550,	0.8803 s / batch. (data: 2.43e-02). ETA=8:37:46, max mem: 20.9 GB 
[11/24 17:42:41 visual_prompt]: 	Training 200/553. train loss: 2.7339,	0.8524 s / batch. (data: 2.93e-04). ETA=8:19:59, max mem: 20.9 GB 
[11/24 17:44:17 visual_prompt]: 	Training 300/553. train loss: 5.9133,	1.1696 s / batch. (data: 3.42e-01). ETA=11:24:04, max mem: 20.9 GB 
[11/24 17:45:55 visual_prompt]: 	Training 400/553. train loss: 22.4601,	1.8960 s / batch. (data: 1.06e+00). ETA=18:25:44, max mem: 20.9 GB 
[11/24 17:47:28 visual_prompt]: 	Training 500/553. train loss: 3.0500,	1.0995 s / batch. (data: 2.63e-01). ETA=10:39:24, max mem: 20.9 GB 
[11/24 17:48:20 visual_prompt]: Epoch 37 / 100: avg data time: 1.40e-01, avg batch time: 0.9675, average train loss: 75.5481
[11/24 17:49:14 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3112, average loss: 55.5520
[11/24 17:49:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.53	
[11/24 17:49:14 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[11/24 17:50:52 visual_prompt]: 	Training 100/553. train loss: 3.5900,	0.8520 s / batch. (data: 2.50e-02). ETA=8:13:18, max mem: 20.9 GB 
[11/24 17:52:29 visual_prompt]: 	Training 200/553. train loss: 70.7483,	1.2559 s / batch. (data: 4.39e-01). ETA=12:05:04, max mem: 20.9 GB 
[11/24 17:54:06 visual_prompt]: 	Training 300/553. train loss: 19.4833,	0.8208 s / batch. (data: 1.04e-02). ETA=7:52:28, max mem: 20.9 GB 
[11/24 17:55:40 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8274 s / batch. (data: 4.38e-04). ETA=7:54:54, max mem: 20.9 GB 
[11/24 17:57:18 visual_prompt]: 	Training 500/553. train loss: 130.8910,	0.8293 s / batch. (data: 1.05e-02). ETA=7:54:36, max mem: 20.9 GB 
[11/24 17:58:06 visual_prompt]: Epoch 38 / 100: avg data time: 1.32e-01, avg batch time: 0.9618, average train loss: 64.8333
[11/24 17:59:01 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3098, average loss: 91.3237
[11/24 17:59:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.00	
[11/24 17:59:01 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[11/24 18:00:40 visual_prompt]: 	Training 100/553. train loss: 112.4012,	0.8240 s / batch. (data: 2.88e-04). ETA=7:49:28, max mem: 20.9 GB 
[11/24 18:02:21 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8198 s / batch. (data: 2.53e-04). ETA=7:45:42, max mem: 20.9 GB 
[11/24 18:04:01 visual_prompt]: 	Training 300/553. train loss: 64.3278,	0.8310 s / batch. (data: 2.94e-04). ETA=7:50:43, max mem: 20.9 GB 
[11/24 18:05:35 visual_prompt]: 	Training 400/553. train loss: 135.3309,	0.8249 s / batch. (data: 1.40e-03). ETA=7:45:51, max mem: 20.9 GB 
[11/24 18:07:13 visual_prompt]: 	Training 500/553. train loss: 7.7295,	1.5228 s / batch. (data: 6.88e-01). ETA=14:17:29, max mem: 20.9 GB 
[11/24 18:08:03 visual_prompt]: Epoch 39 / 100: avg data time: 1.50e-01, avg batch time: 0.9790, average train loss: 61.6084
[11/24 18:08:58 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3099, average loss: 42.6650
[11/24 18:08:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.31	
[11/24 18:08:58 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[11/24 18:10:40 visual_prompt]: 	Training 100/553. train loss: 63.2694,	0.8165 s / batch. (data: 2.89e-04). ETA=7:37:40, max mem: 20.9 GB 
[11/24 18:12:15 visual_prompt]: 	Training 200/553. train loss: 65.0934,	0.8275 s / batch. (data: 3.04e-04). ETA=7:42:28, max mem: 20.9 GB 
[11/24 18:13:53 visual_prompt]: 	Training 300/553. train loss: 29.2789,	0.8328 s / batch. (data: 5.88e-03). ETA=7:44:04, max mem: 20.9 GB 
[11/24 18:15:30 visual_prompt]: 	Training 400/553. train loss: 25.7974,	0.8362 s / batch. (data: 2.99e-04). ETA=7:44:33, max mem: 20.9 GB 
[11/24 18:17:06 visual_prompt]: 	Training 500/553. train loss: 54.5019,	0.8338 s / batch. (data: 1.59e-02). ETA=7:41:50, max mem: 20.9 GB 
[11/24 18:17:59 visual_prompt]: Epoch 40 / 100: avg data time: 1.48e-01, avg batch time: 0.9775, average train loss: 62.1924
[11/24 18:18:54 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3103, average loss: 38.1881
[11/24 18:18:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.34	
[11/24 18:18:54 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[11/24 18:20:38 visual_prompt]: 	Training 100/553. train loss: 21.5577,	0.8240 s / batch. (data: 8.03e-03). ETA=7:34:19, max mem: 20.9 GB 
[11/24 18:22:16 visual_prompt]: 	Training 200/553. train loss: 70.5236,	0.8400 s / batch. (data: 7.83e-04). ETA=7:41:43, max mem: 20.9 GB 
[11/24 18:23:51 visual_prompt]: 	Training 300/553. train loss: 87.8134,	0.8283 s / batch. (data: 5.42e-03). ETA=7:33:53, max mem: 20.9 GB 
[11/24 18:25:26 visual_prompt]: 	Training 400/553. train loss: 35.0689,	0.8373 s / batch. (data: 2.97e-04). ETA=7:37:26, max mem: 20.9 GB 
[11/24 18:27:00 visual_prompt]: 	Training 500/553. train loss: 11.2892,	0.8255 s / batch. (data: 2.67e-04). ETA=7:29:35, max mem: 20.9 GB 
[11/24 18:27:48 visual_prompt]: Epoch 41 / 100: avg data time: 1.37e-01, avg batch time: 0.9654, average train loss: 69.5526
[11/24 18:28:42 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3121, average loss: 61.6427
[11/24 18:28:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.02	
[11/24 18:28:42 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[11/24 18:30:19 visual_prompt]: 	Training 100/553. train loss: 53.7447,	0.8247 s / batch. (data: 1.55e-02). ETA=7:27:03, max mem: 20.9 GB 
[11/24 18:31:55 visual_prompt]: 	Training 200/553. train loss: 100.2336,	0.8587 s / batch. (data: 2.26e-02). ETA=7:44:04, max mem: 20.9 GB 
[11/24 18:33:29 visual_prompt]: 	Training 300/553. train loss: 77.9023,	0.8480 s / batch. (data: 2.88e-04). ETA=7:36:53, max mem: 20.9 GB 
[11/24 18:35:05 visual_prompt]: 	Training 400/553. train loss: 73.5132,	0.8158 s / batch. (data: 2.88e-04). ETA=7:18:10, max mem: 20.9 GB 
[11/24 18:36:39 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8039 s / batch. (data: 4.28e-04). ETA=7:10:28, max mem: 20.9 GB 
[11/24 18:37:29 visual_prompt]: Epoch 42 / 100: avg data time: 1.25e-01, avg batch time: 0.9534, average train loss: 63.3192
[11/24 18:38:23 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3095, average loss: 107.2634
[11/24 18:38:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.22	
[11/24 18:38:23 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[11/24 18:40:04 visual_prompt]: 	Training 100/553. train loss: 24.1858,	0.8242 s / batch. (data: 7.96e-03). ETA=7:19:13, max mem: 20.9 GB 
[11/24 18:41:38 visual_prompt]: 	Training 200/553. train loss: 55.2119,	0.8325 s / batch. (data: 3.19e-04). ETA=7:22:13, max mem: 20.9 GB 
[11/24 18:43:11 visual_prompt]: 	Training 300/553. train loss: 117.6169,	0.8396 s / batch. (data: 7.96e-03). ETA=7:24:38, max mem: 20.9 GB 
[11/24 18:44:44 visual_prompt]: 	Training 400/553. train loss: 16.0340,	0.8400 s / batch. (data: 3.25e-04). ETA=7:23:26, max mem: 20.9 GB 
[11/24 18:46:21 visual_prompt]: 	Training 500/553. train loss: 111.6719,	0.8321 s / batch. (data: 5.42e-03). ETA=7:17:52, max mem: 20.9 GB 
[11/24 18:47:12 visual_prompt]: Epoch 43 / 100: avg data time: 1.27e-01, avg batch time: 0.9562, average train loss: 55.8341
[11/24 18:48:06 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3087, average loss: 21.4282
[11/24 18:48:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.69	
[11/24 18:48:06 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[11/24 18:49:45 visual_prompt]: 	Training 100/553. train loss: 34.7514,	0.9783 s / batch. (data: 1.71e-01). ETA=8:32:19, max mem: 20.9 GB 
[11/24 18:51:22 visual_prompt]: 	Training 200/553. train loss: 6.3591,	0.8340 s / batch. (data: 6.94e-03). ETA=7:15:21, max mem: 20.9 GB 
[11/24 18:52:55 visual_prompt]: 	Training 300/553. train loss: 30.0497,	0.8243 s / batch. (data: 3.49e-04). ETA=7:08:56, max mem: 20.9 GB 
[11/24 18:54:29 visual_prompt]: 	Training 400/553. train loss: 16.9997,	0.8383 s / batch. (data: 1.20e-02). ETA=7:14:47, max mem: 20.9 GB 
[11/24 18:56:04 visual_prompt]: 	Training 500/553. train loss: 52.5224,	0.8520 s / batch. (data: 7.97e-03). ETA=7:20:30, max mem: 20.9 GB 
[11/24 18:56:54 visual_prompt]: Epoch 44 / 100: avg data time: 1.25e-01, avg batch time: 0.9540, average train loss: 64.4375
[11/24 18:57:48 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3103, average loss: 77.4245
[11/24 18:57:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.14	
[11/24 18:57:48 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[11/24 18:59:28 visual_prompt]: 	Training 100/553. train loss: 105.2987,	0.8176 s / batch. (data: 8.67e-03). ETA=7:00:39, max mem: 20.9 GB 
[11/24 19:01:00 visual_prompt]: 	Training 200/553. train loss: 50.7398,	0.8352 s / batch. (data: 5.41e-03). ETA=7:08:16, max mem: 20.9 GB 
[11/24 19:02:36 visual_prompt]: 	Training 300/553. train loss: 96.7920,	0.8114 s / batch. (data: 3.25e-04). ETA=6:54:43, max mem: 20.9 GB 
[11/24 19:04:10 visual_prompt]: 	Training 400/553. train loss: 26.5873,	0.8449 s / batch. (data: 1.99e-02). ETA=7:10:26, max mem: 20.9 GB 
[11/24 19:05:47 visual_prompt]: 	Training 500/553. train loss: 0.0645,	0.8204 s / batch. (data: 3.18e-04). ETA=6:56:35, max mem: 20.9 GB 
[11/24 19:06:36 visual_prompt]: Epoch 45 / 100: avg data time: 1.25e-01, avg batch time: 0.9545, average train loss: 59.2357
[11/24 19:07:30 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3096, average loss: 1.0666
[11/24 19:07:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.47	
[11/24 19:07:30 visual_prompt]: Best epoch 45: best metric: -1.067
[11/24 19:07:30 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[11/24 19:09:10 visual_prompt]: 	Training 100/553. train loss: 80.2868,	1.3092 s / batch. (data: 4.50e-01). ETA=11:01:29, max mem: 20.9 GB 
[11/24 19:10:46 visual_prompt]: 	Training 200/553. train loss: 23.6908,	0.8598 s / batch. (data: 2.99e-04). ETA=7:13:00, max mem: 20.9 GB 
[11/24 19:12:20 visual_prompt]: 	Training 300/553. train loss: 64.1164,	0.8267 s / batch. (data: 7.96e-03). ETA=6:54:55, max mem: 20.9 GB 
[11/24 19:13:55 visual_prompt]: 	Training 400/553. train loss: 27.6399,	0.8443 s / batch. (data: 5.40e-03). ETA=7:02:20, max mem: 20.9 GB 
[11/24 19:15:28 visual_prompt]: 	Training 500/553. train loss: 203.7790,	0.8240 s / batch. (data: 2.99e-04). ETA=6:50:49, max mem: 20.9 GB 
[11/24 19:16:19 visual_prompt]: Epoch 46 / 100: avg data time: 1.29e-01, avg batch time: 0.9567, average train loss: 58.7898
[11/24 19:17:13 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3101, average loss: 58.1680
[11/24 19:17:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.47	
[11/24 19:17:13 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[11/24 19:18:54 visual_prompt]: 	Training 100/553. train loss: 102.3112,	0.8310 s / batch. (data: 1.04e-02). ETA=6:52:11, max mem: 20.9 GB 
[11/24 19:20:25 visual_prompt]: 	Training 200/553. train loss: 39.0308,	1.3164 s / batch. (data: 4.86e-01). ETA=10:50:46, max mem: 20.9 GB 
[11/24 19:22:00 visual_prompt]: 	Training 300/553. train loss: 28.9458,	0.8280 s / batch. (data: 2.97e-04). ETA=6:47:58, max mem: 20.9 GB 
[11/24 19:23:36 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8417 s / batch. (data: 1.05e-02). ETA=6:53:18, max mem: 20.9 GB 
[11/24 19:25:10 visual_prompt]: 	Training 500/553. train loss: 123.2391,	0.8194 s / batch. (data: 2.95e-04). ETA=6:41:00, max mem: 20.9 GB 
[11/24 19:26:01 visual_prompt]: Epoch 47 / 100: avg data time: 1.25e-01, avg batch time: 0.9539, average train loss: 56.4288
[11/24 19:26:55 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3105, average loss: 65.2392
[11/24 19:26:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.92	
[11/24 19:26:55 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[11/24 19:28:34 visual_prompt]: 	Training 100/553. train loss: 27.8295,	0.8377 s / batch. (data: 2.91e-04). ETA=6:47:49, max mem: 20.9 GB 
[11/24 19:30:10 visual_prompt]: 	Training 200/553. train loss: 6.9854,	0.8492 s / batch. (data: 3.29e-02). ETA=6:51:59, max mem: 20.9 GB 
[11/24 19:31:46 visual_prompt]: 	Training 300/553. train loss: 101.3854,	1.6680 s / batch. (data: 8.43e-01). ETA=13:26:25, max mem: 20.9 GB 
[11/24 19:33:18 visual_prompt]: 	Training 400/553. train loss: 37.9694,	0.8322 s / batch. (data: 3.24e-04). ETA=6:40:59, max mem: 20.9 GB 
[11/24 19:34:54 visual_prompt]: 	Training 500/553. train loss: 114.5325,	0.8259 s / batch. (data: 1.21e-02). ETA=6:36:32, max mem: 20.9 GB 
[11/24 19:35:43 visual_prompt]: Epoch 48 / 100: avg data time: 1.25e-01, avg batch time: 0.9541, average train loss: 55.6506
[11/24 19:36:36 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3102, average loss: 94.5673
[11/24 19:36:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.57	
[11/24 19:36:36 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[11/24 19:38:15 visual_prompt]: 	Training 100/553. train loss: 48.2335,	0.8225 s / batch. (data: 8.09e-04). ETA=6:32:49, max mem: 20.9 GB 
[11/24 19:39:49 visual_prompt]: 	Training 200/553. train loss: 8.1025,	0.8559 s / batch. (data: 2.07e-02). ETA=6:47:20, max mem: 20.9 GB 
[11/24 19:41:24 visual_prompt]: 	Training 300/553. train loss: 48.0971,	0.8520 s / batch. (data: 2.93e-04). ETA=6:44:04, max mem: 20.9 GB 
[11/24 19:43:02 visual_prompt]: 	Training 400/553. train loss: 18.1919,	0.8519 s / batch. (data: 5.86e-03). ETA=6:42:37, max mem: 20.9 GB 
[11/24 19:44:38 visual_prompt]: 	Training 500/553. train loss: 20.7298,	0.8208 s / batch. (data: 8.08e-04). ETA=6:26:33, max mem: 20.9 GB 
[11/24 19:45:29 visual_prompt]: Epoch 49 / 100: avg data time: 1.33e-01, avg batch time: 0.9619, average train loss: 51.4502
[11/24 19:46:23 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3097, average loss: 7.3284
[11/24 19:46:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.43	
[11/24 19:46:23 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[11/24 19:48:03 visual_prompt]: 	Training 100/553. train loss: 19.3923,	0.8357 s / batch. (data: 9.22e-03). ETA=6:31:24, max mem: 20.9 GB 
[11/24 19:49:38 visual_prompt]: 	Training 200/553. train loss: 46.2160,	0.8528 s / batch. (data: 1.27e-02). ETA=6:38:00, max mem: 20.9 GB 
[11/24 19:51:12 visual_prompt]: 	Training 300/553. train loss: 103.4884,	0.8190 s / batch. (data: 7.11e-04). ETA=6:20:51, max mem: 20.9 GB 
[11/24 19:52:46 visual_prompt]: 	Training 400/553. train loss: 9.5271,	0.8640 s / batch. (data: 2.99e-04). ETA=6:40:21, max mem: 20.9 GB 
[11/24 19:54:21 visual_prompt]: 	Training 500/553. train loss: 27.6025,	0.8441 s / batch. (data: 2.91e-04). ETA=6:29:44, max mem: 20.9 GB 
[11/24 19:55:11 visual_prompt]: Epoch 50 / 100: avg data time: 1.25e-01, avg batch time: 0.9543, average train loss: 48.2504
[11/24 19:56:05 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3109, average loss: 77.0873
[11/24 19:56:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.27	
[11/24 19:56:05 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[11/24 19:57:44 visual_prompt]: 	Training 100/553. train loss: 23.4318,	1.0800 s / batch. (data: 2.27e-01). ETA=8:15:53, max mem: 20.9 GB 
[11/24 19:59:19 visual_prompt]: 	Training 200/553. train loss: 20.1287,	0.8489 s / batch. (data: 1.55e-02). ETA=6:28:23, max mem: 20.9 GB 
[11/24 20:00:55 visual_prompt]: 	Training 300/553. train loss: 22.4726,	0.8424 s / batch. (data: 7.96e-03). ETA=6:23:58, max mem: 20.9 GB 
[11/24 20:02:30 visual_prompt]: 	Training 400/553. train loss: 32.5742,	1.3118 s / batch. (data: 4.89e-01). ETA=9:55:45, max mem: 20.9 GB 
[11/24 20:04:05 visual_prompt]: 	Training 500/553. train loss: 2.4241,	0.8476 s / batch. (data: 2.55e-04). ETA=6:23:33, max mem: 20.9 GB 
[11/24 20:04:53 visual_prompt]: Epoch 51 / 100: avg data time: 1.25e-01, avg batch time: 0.9552, average train loss: 43.6193
[11/24 20:05:47 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3102, average loss: 26.9971
[11/24 20:05:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.01	
[11/24 20:05:47 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[11/24 20:07:30 visual_prompt]: 	Training 100/553. train loss: 113.3093,	0.8481 s / batch. (data: 7.84e-04). ETA=6:21:34, max mem: 20.9 GB 
[11/24 20:09:04 visual_prompt]: 	Training 200/553. train loss: 1.6175,	0.8508 s / batch. (data: 6.74e-03). ETA=6:21:23, max mem: 20.9 GB 
[11/24 20:10:39 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8357 s / batch. (data: 2.84e-04). ETA=6:13:13, max mem: 20.9 GB 
[11/24 20:12:16 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8281 s / batch. (data: 2.42e-04). ETA=6:08:27, max mem: 20.9 GB 
[11/24 20:13:47 visual_prompt]: 	Training 500/553. train loss: 30.4923,	0.8349 s / batch. (data: 1.60e-02). ETA=6:10:04, max mem: 20.9 GB 
[11/24 20:14:35 visual_prompt]: Epoch 52 / 100: avg data time: 1.26e-01, avg batch time: 0.9550, average train loss: 49.8677
[11/24 20:15:29 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3120, average loss: 289.2762
[11/24 20:15:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.61	
[11/24 20:15:29 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[11/24 20:17:08 visual_prompt]: 	Training 100/553. train loss: 97.3026,	0.8266 s / batch. (data: 2.88e-04). ETA=6:04:19, max mem: 20.9 GB 
[11/24 20:18:43 visual_prompt]: 	Training 200/553. train loss: 114.2686,	0.8440 s / batch. (data: 7.55e-04). ETA=6:10:35, max mem: 20.9 GB 
[11/24 20:20:18 visual_prompt]: 	Training 300/553. train loss: 64.3623,	0.8197 s / batch. (data: 7.53e-04). ETA=5:58:32, max mem: 20.9 GB 
[11/24 20:21:54 visual_prompt]: 	Training 400/553. train loss: 14.2451,	0.8280 s / batch. (data: 2.93e-04). ETA=6:00:47, max mem: 20.9 GB 
[11/24 20:23:28 visual_prompt]: 	Training 500/553. train loss: 49.2907,	0.8210 s / batch. (data: 2.83e-04). ETA=5:56:22, max mem: 20.9 GB 
[11/24 20:24:19 visual_prompt]: Epoch 53 / 100: avg data time: 1.28e-01, avg batch time: 0.9574, average train loss: 45.0775
[11/24 20:25:13 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3102, average loss: 70.0438
[11/24 20:25:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.39	
[11/24 20:25:13 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[11/24 20:26:53 visual_prompt]: 	Training 100/553. train loss: 3.6098,	0.8440 s / batch. (data: 2.99e-04). ETA=6:04:12, max mem: 20.9 GB 
[11/24 20:28:29 visual_prompt]: 	Training 200/553. train loss: 99.3682,	0.8205 s / batch. (data: 2.91e-04). ETA=5:52:40, max mem: 20.9 GB 
[11/24 20:30:04 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8120 s / batch. (data: 2.92e-04). ETA=5:47:41, max mem: 20.9 GB 
[11/24 20:31:39 visual_prompt]: 	Training 400/553. train loss: 204.8930,	0.8588 s / batch. (data: 6.74e-03). ETA=6:06:17, max mem: 20.9 GB 
[11/24 20:33:15 visual_prompt]: 	Training 500/553. train loss: 23.8333,	0.8258 s / batch. (data: 7.71e-04). ETA=5:50:50, max mem: 20.9 GB 
[11/24 20:34:05 visual_prompt]: Epoch 54 / 100: avg data time: 1.34e-01, avg batch time: 0.9626, average train loss: 49.6782
[11/24 20:34:59 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3108, average loss: 161.1635
[11/24 20:34:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.30	
[11/24 20:34:59 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[11/24 20:36:37 visual_prompt]: 	Training 100/553. train loss: 3.1165,	0.8860 s / batch. (data: 5.85e-02). ETA=6:14:08, max mem: 20.9 GB 
[11/24 20:38:12 visual_prompt]: 	Training 200/553. train loss: 58.2722,	0.8345 s / batch. (data: 5.44e-03). ETA=5:51:00, max mem: 20.9 GB 
[11/24 20:39:46 visual_prompt]: 	Training 300/553. train loss: 69.6328,	0.8280 s / batch. (data: 3.08e-04). ETA=5:46:54, max mem: 20.9 GB 
[11/24 20:41:21 visual_prompt]: 	Training 400/553. train loss: 46.1825,	0.8110 s / batch. (data: 3.13e-04). ETA=5:38:25, max mem: 20.9 GB 
[11/24 20:42:56 visual_prompt]: 	Training 500/553. train loss: 38.0969,	0.8200 s / batch. (data: 2.92e-04). ETA=5:40:49, max mem: 20.9 GB 
[11/24 20:43:46 visual_prompt]: Epoch 55 / 100: avg data time: 1.23e-01, avg batch time: 0.9529, average train loss: 44.1909
[11/24 20:44:41 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3091, average loss: 23.5711
[11/24 20:44:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.92	
[11/24 20:44:41 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[11/24 20:46:21 visual_prompt]: 	Training 100/553. train loss: 30.3800,	0.8190 s / batch. (data: 2.95e-04). ETA=5:38:19, max mem: 20.9 GB 
[11/24 20:47:56 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8292 s / batch. (data: 2.50e-04). ETA=5:41:08, max mem: 20.9 GB 
[11/24 20:49:33 visual_prompt]: 	Training 300/553. train loss: 8.7538,	0.8343 s / batch. (data: 7.46e-04). ETA=5:41:50, max mem: 20.9 GB 
[11/24 20:51:09 visual_prompt]: 	Training 400/553. train loss: 65.1911,	0.8532 s / batch. (data: 1.55e-02). ETA=5:48:10, max mem: 20.9 GB 
[11/24 20:52:45 visual_prompt]: 	Training 500/553. train loss: 43.9096,	2.0800 s / batch. (data: 1.26e+00). ETA=14:05:21, max mem: 20.9 GB 
[11/24 20:53:33 visual_prompt]: Epoch 56 / 100: avg data time: 1.32e-01, avg batch time: 0.9625, average train loss: 35.5305
[11/24 20:54:27 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3110, average loss: 11.8125
[11/24 20:54:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.58	
[11/24 20:54:27 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[11/24 20:56:09 visual_prompt]: 	Training 100/553. train loss: 25.2461,	0.8516 s / batch. (data: 8.69e-04). ETA=5:43:56, max mem: 20.9 GB 
[11/24 20:57:43 visual_prompt]: 	Training 200/553. train loss: 53.7476,	0.8458 s / batch. (data: 5.89e-03). ETA=5:40:10, max mem: 20.9 GB 
[11/24 20:59:17 visual_prompt]: 	Training 300/553. train loss: 718.4894,	0.8739 s / batch. (data: 1.55e-02). ETA=5:50:02, max mem: 20.9 GB 
[11/24 21:00:51 visual_prompt]: 	Training 400/553. train loss: 99.0629,	0.8131 s / batch. (data: 2.78e-04). ETA=5:24:19, max mem: 20.9 GB 
[11/24 21:02:24 visual_prompt]: 	Training 500/553. train loss: 34.1063,	0.8309 s / batch. (data: 2.52e-04). ETA=5:30:02, max mem: 20.9 GB 
[11/24 21:03:15 visual_prompt]: Epoch 57 / 100: avg data time: 1.26e-01, avg batch time: 0.9552, average train loss: 42.7073
[11/24 21:04:10 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3091, average loss: 34.4639
[11/24 21:04:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.28	
[11/24 21:04:10 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[11/24 21:05:49 visual_prompt]: 	Training 100/553. train loss: 19.3993,	1.0320 s / batch. (data: 1.78e-01). ETA=6:47:16, max mem: 20.9 GB 
[11/24 21:07:25 visual_prompt]: 	Training 200/553. train loss: 6.9843,	0.8480 s / batch. (data: 7.95e-03). ETA=5:33:15, max mem: 20.9 GB 
[11/24 21:09:04 visual_prompt]: 	Training 300/553. train loss: 7.0246,	0.8283 s / batch. (data: 5.40e-03). ETA=5:24:07, max mem: 20.9 GB 
[11/24 21:10:39 visual_prompt]: 	Training 400/553. train loss: 31.5897,	0.8400 s / batch. (data: 7.89e-04). ETA=5:27:18, max mem: 20.9 GB 
[11/24 21:12:14 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8200 s / batch. (data: 8.63e-04). ETA=5:18:08, max mem: 20.9 GB 
[11/24 21:13:03 visual_prompt]: Epoch 58 / 100: avg data time: 1.34e-01, avg batch time: 0.9646, average train loss: 37.0176
[11/24 21:13:58 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3108, average loss: 6.4688
[11/24 21:13:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.49	
[11/24 21:13:58 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[11/24 21:15:39 visual_prompt]: 	Training 100/553. train loss: 117.1536,	0.8151 s / batch. (data: 2.50e-04). ETA=5:14:09, max mem: 20.9 GB 
[11/24 21:17:15 visual_prompt]: 	Training 200/553. train loss: 103.0690,	0.8395 s / batch. (data: 3.31e-04). ETA=5:22:11, max mem: 20.9 GB 
[11/24 21:18:50 visual_prompt]: 	Training 300/553. train loss: 68.1451,	0.8239 s / batch. (data: 5.39e-03). ETA=5:14:47, max mem: 20.9 GB 
[11/24 21:20:25 visual_prompt]: 	Training 400/553. train loss: 17.8920,	0.8155 s / batch. (data: 3.15e-04). ETA=5:10:14, max mem: 20.9 GB 
[11/24 21:22:03 visual_prompt]: 	Training 500/553. train loss: 49.7379,	0.8310 s / batch. (data: 7.55e-04). ETA=5:14:44, max mem: 20.9 GB 
[11/24 21:22:51 visual_prompt]: Epoch 59 / 100: avg data time: 1.33e-01, avg batch time: 0.9641, average train loss: 41.9175
[11/24 21:23:45 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3101, average loss: 22.8153
[11/24 21:23:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.15	
[11/24 21:23:45 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[11/24 21:25:26 visual_prompt]: 	Training 100/553. train loss: 13.0365,	0.8400 s / batch. (data: 2.96e-04). ETA=5:16:00, max mem: 20.9 GB 
[11/24 21:27:02 visual_prompt]: 	Training 200/553. train loss: 14.9202,	0.8500 s / batch. (data: 9.99e-03). ETA=5:18:23, max mem: 20.9 GB 
[11/24 21:28:35 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.9320 s / batch. (data: 1.16e-01). ETA=5:47:31, max mem: 20.9 GB 
[11/24 21:30:13 visual_prompt]: 	Training 400/553. train loss: 25.1963,	1.0636 s / batch. (data: 2.03e-01). ETA=6:34:48, max mem: 20.9 GB 
[11/24 21:31:49 visual_prompt]: 	Training 500/553. train loss: 27.4358,	0.8395 s / batch. (data: 3.01e-04). ETA=5:10:13, max mem: 20.9 GB 
[11/24 21:32:40 visual_prompt]: Epoch 60 / 100: avg data time: 1.35e-01, avg batch time: 0.9655, average train loss: 36.9652
[11/24 21:33:34 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3103, average loss: 61.2489
[11/24 21:33:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.20	
[11/24 21:33:34 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[11/24 21:35:15 visual_prompt]: 	Training 100/553. train loss: 16.1770,	0.8257 s / batch. (data: 3.17e-04). ETA=5:03:01, max mem: 20.9 GB 
[11/24 21:36:52 visual_prompt]: 	Training 200/553. train loss: 13.1755,	1.7000 s / batch. (data: 8.68e-01). ETA=10:21:03, max mem: 20.9 GB 
[11/24 21:38:28 visual_prompt]: 	Training 300/553. train loss: 22.3110,	1.3053 s / batch. (data: 4.99e-01). ETA=7:54:42, max mem: 20.9 GB 
[11/24 21:40:01 visual_prompt]: 	Training 400/553. train loss: 9.8325,	0.8320 s / batch. (data: 5.70e-04). ETA=5:01:09, max mem: 20.9 GB 
[11/24 21:41:38 visual_prompt]: 	Training 500/553. train loss: 47.2574,	2.4073 s / batch. (data: 1.60e+00). ETA=14:27:26, max mem: 20.9 GB 
[11/24 21:42:26 visual_prompt]: Epoch 61 / 100: avg data time: 1.31e-01, avg batch time: 0.9619, average train loss: 31.4750
[11/24 21:43:21 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3104, average loss: 10.8324
[11/24 21:43:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.75	
[11/24 21:43:21 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[11/24 21:45:01 visual_prompt]: 	Training 100/553. train loss: 6.1677,	0.8440 s / batch. (data: 3.05e-04). ETA=5:01:57, max mem: 20.9 GB 
[11/24 21:46:37 visual_prompt]: 	Training 200/553. train loss: 87.6317,	0.8324 s / batch. (data: 2.84e-04). ETA=4:56:25, max mem: 20.9 GB 
[11/24 21:48:11 visual_prompt]: 	Training 300/553. train loss: 35.6326,	0.8484 s / batch. (data: 8.37e-03). ETA=5:00:43, max mem: 20.9 GB 
[11/24 21:49:47 visual_prompt]: 	Training 400/553. train loss: 1.8927,	0.8211 s / batch. (data: 2.80e-04). ETA=4:49:40, max mem: 20.9 GB 
[11/24 21:51:22 visual_prompt]: 	Training 500/553. train loss: 44.4709,	0.8240 s / batch. (data: 3.00e-04). ETA=4:49:19, max mem: 20.9 GB 
[11/24 21:52:14 visual_prompt]: Epoch 62 / 100: avg data time: 1.33e-01, avg batch time: 0.9632, average train loss: 34.4736
[11/24 21:53:08 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3099, average loss: 37.4871
[11/24 21:53:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.07	
[11/24 21:53:08 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[11/24 21:54:51 visual_prompt]: 	Training 100/553. train loss: 11.6051,	0.8335 s / batch. (data: 2.77e-04). ETA=4:50:31, max mem: 20.9 GB 
[11/24 21:56:30 visual_prompt]: 	Training 200/553. train loss: 23.2807,	0.8239 s / batch. (data: 3.80e-04). ETA=4:45:48, max mem: 20.9 GB 
[11/24 21:58:05 visual_prompt]: 	Training 300/553. train loss: 17.6828,	0.8557 s / batch. (data: 1.16e-02). ETA=4:55:24, max mem: 20.9 GB 
[11/24 21:59:37 visual_prompt]: 	Training 400/553. train loss: 5.7767,	0.8177 s / batch. (data: 2.82e-04). ETA=4:40:57, max mem: 20.9 GB 
[11/24 22:01:11 visual_prompt]: 	Training 500/553. train loss: 19.2352,	0.8400 s / batch. (data: 2.92e-04). ETA=4:47:12, max mem: 20.9 GB 
[11/24 22:02:00 visual_prompt]: Epoch 63 / 100: avg data time: 1.30e-01, avg batch time: 0.9616, average train loss: 30.2110
[11/24 22:02:55 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3106, average loss: 11.3012
[11/24 22:02:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.90	
[11/24 22:02:55 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[11/24 22:04:37 visual_prompt]: 	Training 100/553. train loss: 34.9543,	0.8403 s / batch. (data: 1.10e-02). ETA=4:45:09, max mem: 20.9 GB 
[11/24 22:06:14 visual_prompt]: 	Training 200/553. train loss: 13.7656,	0.8179 s / batch. (data: 2.98e-04). ETA=4:36:10, max mem: 20.9 GB 
[11/24 22:07:47 visual_prompt]: 	Training 300/553. train loss: 339.8531,	0.8360 s / batch. (data: 2.83e-04). ETA=4:40:54, max mem: 20.9 GB 
[11/24 22:09:23 visual_prompt]: 	Training 400/553. train loss: 15.6826,	0.8480 s / batch. (data: 1.20e-02). ETA=4:43:32, max mem: 20.9 GB 
[11/24 22:10:59 visual_prompt]: 	Training 500/553. train loss: 1.9167,	0.8679 s / batch. (data: 1.20e-02). ETA=4:48:44, max mem: 20.9 GB 
[11/24 22:11:48 visual_prompt]: Epoch 64 / 100: avg data time: 1.33e-01, avg batch time: 0.9648, average train loss: 33.4037
[11/24 22:12:43 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3108, average loss: 21.2460
[11/24 22:12:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.50	
[11/24 22:12:43 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[11/24 22:14:27 visual_prompt]: 	Training 100/553. train loss: 17.4793,	0.8426 s / batch. (data: 1.56e-02). ETA=4:38:10, max mem: 20.9 GB 
[11/24 22:16:04 visual_prompt]: 	Training 200/553. train loss: 8.7240,	1.1744 s / batch. (data: 3.44e-01). ETA=6:25:46, max mem: 20.9 GB 
[11/24 22:17:37 visual_prompt]: 	Training 300/553. train loss: 23.7983,	0.9200 s / batch. (data: 9.11e-02). ETA=5:00:39, max mem: 20.9 GB 
[11/24 22:19:13 visual_prompt]: 	Training 400/553. train loss: 3.0781,	0.8520 s / batch. (data: 7.88e-04). ETA=4:37:00, max mem: 20.9 GB 
[11/24 22:20:48 visual_prompt]: 	Training 500/553. train loss: 0.3122,	0.8440 s / batch. (data: 7.95e-03). ETA=4:33:00, max mem: 20.9 GB 
[11/24 22:21:36 visual_prompt]: Epoch 65 / 100: avg data time: 1.33e-01, avg batch time: 0.9638, average train loss: 28.8003
[11/24 22:22:31 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3104, average loss: 39.8656
[11/24 22:22:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.74	
[11/24 22:22:31 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[11/24 22:24:10 visual_prompt]: 	Training 100/553. train loss: 17.0419,	0.8208 s / batch. (data: 2.81e-04). ETA=4:23:24, max mem: 20.9 GB 
[11/24 22:25:46 visual_prompt]: 	Training 200/553. train loss: 10.1598,	1.4042 s / batch. (data: 5.70e-01). ETA=7:28:17, max mem: 20.9 GB 
[11/24 22:27:24 visual_prompt]: 	Training 300/553. train loss: 11.5037,	0.8440 s / batch. (data: 3.08e-04). ETA=4:28:02, max mem: 20.9 GB 
[11/24 22:28:58 visual_prompt]: 	Training 400/553. train loss: 10.3658,	0.8407 s / batch. (data: 1.09e-02). ETA=4:25:35, max mem: 20.9 GB 
[11/24 22:30:33 visual_prompt]: 	Training 500/553. train loss: 13.5958,	0.8359 s / batch. (data: 3.20e-04). ETA=4:22:41, max mem: 20.9 GB 
[11/24 22:31:25 visual_prompt]: Epoch 66 / 100: avg data time: 1.34e-01, avg batch time: 0.9659, average train loss: 27.3825
[11/24 22:32:20 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3098, average loss: 69.7469
[11/24 22:32:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.67	
[11/24 22:32:20 visual_prompt]: Stopping early.
[11/24 22:32:20 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 22:32:20 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 22:32:20 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/24 22:32:20 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 22:32:20 visual_prompt]: Training with config:
[11/24 22:32:20 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/24 22:32:20 visual_prompt]: Loading training data...
[11/24 22:32:20 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 22:32:20 visual_prompt]: Loading validation data...
[11/24 22:32:20 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 22:32:20 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 22:32:22 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 22:32:22 visual_prompt]: tuned percent:0.525
[11/24 22:32:22 visual_prompt]: Device used for model: 0
[11/24 22:32:22 visual_prompt]: Setting up Evaluator...
[11/24 22:32:22 visual_prompt]: Setting up Trainer...
[11/24 22:32:22 visual_prompt]: 	Setting up the optimizer...
[11/24 22:32:22 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 22:34:02 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8310 s / batch. (data: 2.82e-04). ETA=12:44:33, max mem: 20.9 GB 
[11/24 22:35:37 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8360 s / batch. (data: 2.78e-04). ETA=12:47:42, max mem: 20.9 GB 
[11/24 22:37:15 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8245 s / batch. (data: 5.45e-03). ETA=12:35:45, max mem: 20.9 GB 
[11/24 22:38:49 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8350 s / batch. (data: 2.93e-04). ETA=12:44:03, max mem: 20.9 GB 
[11/24 22:40:27 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8544 s / batch. (data: 1.22e-02). ETA=13:00:22, max mem: 20.9 GB 
[11/24 22:41:18 visual_prompt]: Epoch 1 / 100: avg data time: 1.34e-01, avg batch time: 0.9682, average train loss: 1.5403
[11/24 22:42:13 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3104, average loss: 1.5201
[11/24 22:42:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 22:42:13 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/24 22:43:52 visual_prompt]: 	Training 100/553. train loss: 9.3182,	0.8544 s / batch. (data: 3.77e-02). ETA=12:58:10, max mem: 20.9 GB 
[11/24 22:45:28 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8880 s / batch. (data: 5.55e-02). ETA=13:27:18, max mem: 20.9 GB 
[11/24 22:47:06 visual_prompt]: 	Training 300/553. train loss: 5.1931,	1.1080 s / batch. (data: 2.68e-01). ETA=16:45:26, max mem: 20.9 GB 
[11/24 22:48:41 visual_prompt]: 	Training 400/553. train loss: 0.6944,	0.8576 s / batch. (data: 2.06e-02). ETA=12:56:47, max mem: 20.9 GB 
[11/24 22:50:18 visual_prompt]: 	Training 500/553. train loss: 0.5637,	0.8360 s / batch. (data: 2.99e-04). ETA=12:35:51, max mem: 20.9 GB 
[11/24 22:51:08 visual_prompt]: Epoch 2 / 100: avg data time: 1.34e-01, avg batch time: 0.9674, average train loss: 9.8902
[11/24 22:52:03 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3098, average loss: 11.6990
[11/24 22:52:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.64	
[11/24 22:52:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/24 22:53:41 visual_prompt]: 	Training 100/553. train loss: 20.6916,	0.8400 s / batch. (data: 1.19e-02). ETA=12:37:18, max mem: 20.9 GB 
[11/24 22:55:18 visual_prompt]: 	Training 200/553. train loss: 7.1333,	1.2292 s / batch. (data: 4.01e-01). ETA=18:26:07, max mem: 20.9 GB 
[11/24 22:56:53 visual_prompt]: 	Training 300/553. train loss: 9.6720,	0.8884 s / batch. (data: 2.85e-02). ETA=13:18:00, max mem: 20.9 GB 
[11/24 22:58:30 visual_prompt]: 	Training 400/553. train loss: 15.5484,	0.8184 s / batch. (data: 3.17e-04). ETA=12:13:43, max mem: 20.9 GB 
[11/24 23:00:07 visual_prompt]: 	Training 500/553. train loss: 4.5820,	1.1860 s / batch. (data: 3.63e-01). ETA=17:41:18, max mem: 20.9 GB 
[11/24 23:00:56 visual_prompt]: Epoch 3 / 100: avg data time: 1.32e-01, avg batch time: 0.9640, average train loss: 12.6822
[11/24 23:01:50 visual_prompt]: Inference (val):avg data time: 2.10e-04, avg batch time: 0.3115, average loss: 9.8754
[11/24 23:01:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.28	
[11/24 23:01:50 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/24 23:03:32 visual_prompt]: 	Training 100/553. train loss: 14.6318,	0.8300 s / batch. (data: 5.47e-03). ETA=12:20:39, max mem: 20.9 GB 
[11/24 23:05:17 visual_prompt]: 	Training 200/553. train loss: 14.6047,	0.8358 s / batch. (data: 7.88e-03). ETA=12:24:28, max mem: 20.9 GB 
[11/24 23:07:03 visual_prompt]: 	Training 300/553. train loss: 12.7162,	1.5192 s / batch. (data: 6.87e-01). ETA=22:30:35, max mem: 20.9 GB 
[11/24 23:08:40 visual_prompt]: 	Training 400/553. train loss: 4.5224,	1.4685 s / batch. (data: 6.34e-01). ETA=21:43:04, max mem: 20.9 GB 
[11/24 23:10:27 visual_prompt]: 	Training 500/553. train loss: 0.0000,	3.7050 s / batch. (data: 2.88e+00). ETA=2 days, 6:41:29, max mem: 20.9 GB 
[11/24 23:11:22 visual_prompt]: Epoch 4 / 100: avg data time: 2.01e-01, avg batch time: 1.0343, average train loss: 17.3637
[11/24 23:12:17 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3096, average loss: 93.1347
[11/24 23:12:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.46	
[11/24 23:12:17 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/24 23:13:55 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8840 s / batch. (data: 5.40e-03). ETA=13:00:43, max mem: 20.9 GB 
[11/24 23:15:30 visual_prompt]: 	Training 200/553. train loss: 6.3354,	1.0275 s / batch. (data: 2.11e-01). ETA=15:05:43, max mem: 20.9 GB 
[11/24 23:17:07 visual_prompt]: 	Training 300/553. train loss: 36.2018,	0.8395 s / batch. (data: 2.76e-04). ETA=12:18:33, max mem: 20.9 GB 
[11/24 23:18:42 visual_prompt]: 	Training 400/553. train loss: 41.4358,	0.8615 s / batch. (data: 2.93e-04). ETA=12:36:30, max mem: 20.9 GB 
[11/24 23:20:17 visual_prompt]: 	Training 500/553. train loss: 32.2609,	0.8230 s / batch. (data: 2.94e-04). ETA=12:01:19, max mem: 20.9 GB 
[11/24 23:21:07 visual_prompt]: Epoch 5 / 100: avg data time: 1.28e-01, avg batch time: 0.9595, average train loss: 27.6257
[11/24 23:22:02 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3089, average loss: 89.6345
[11/24 23:22:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.05	
[11/24 23:22:02 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/24 23:23:42 visual_prompt]: 	Training 100/553. train loss: 9.4687,	0.8400 s / batch. (data: 8.05e-04). ETA=12:14:05, max mem: 20.9 GB 
[11/24 23:25:17 visual_prompt]: 	Training 200/553. train loss: 8.8481,	0.8600 s / batch. (data: 3.07e-04). ETA=12:30:07, max mem: 20.9 GB 
[11/24 23:26:57 visual_prompt]: 	Training 300/553. train loss: 5.5799,	0.8403 s / batch. (data: 3.45e-04). ETA=12:11:31, max mem: 20.9 GB 
[11/24 23:28:48 visual_prompt]: 	Training 400/553. train loss: 35.0367,	0.8359 s / batch. (data: 1.20e-02). ETA=12:06:20, max mem: 20.9 GB 
[11/24 23:30:28 visual_prompt]: 	Training 500/553. train loss: 27.2777,	0.8973 s / batch. (data: 5.63e-02). ETA=12:58:10, max mem: 20.9 GB 
[11/24 23:31:20 visual_prompt]: Epoch 6 / 100: avg data time: 1.80e-01, avg batch time: 1.0100, average train loss: 34.5054
[11/24 23:32:21 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3102, average loss: 11.0577
[11/24 23:32:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.58	
[11/24 23:32:21 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/24 23:34:01 visual_prompt]: 	Training 100/553. train loss: 0.0337,	0.8360 s / batch. (data: 3.51e-04). ETA=12:02:52, max mem: 20.9 GB 
[11/24 23:35:38 visual_prompt]: 	Training 200/553. train loss: 22.7581,	0.8384 s / batch. (data: 1.19e-02). ETA=12:03:32, max mem: 20.9 GB 
[11/24 23:37:19 visual_prompt]: 	Training 300/553. train loss: 3.9200,	1.7360 s / batch. (data: 9.02e-01). ETA=1 day, 0:55:20, max mem: 20.9 GB 
[11/24 23:38:57 visual_prompt]: 	Training 400/553. train loss: 17.9251,	1.8250 s / batch. (data: 9.88e-01). ETA=1 day, 2:08:54, max mem: 20.9 GB 
[11/24 23:40:32 visual_prompt]: 	Training 500/553. train loss: 135.0038,	0.8239 s / batch. (data: 5.47e-03). ETA=11:46:53, max mem: 20.9 GB 
[11/24 23:41:22 visual_prompt]: Epoch 7 / 100: avg data time: 1.49e-01, avg batch time: 0.9787, average train loss: 43.3462
[11/24 23:42:18 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3096, average loss: 30.7992
[11/24 23:42:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.91	
[11/24 23:42:18 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/24 23:43:58 visual_prompt]: 	Training 100/553. train loss: 92.5026,	0.8230 s / batch. (data: 3.16e-04). ETA=11:44:01, max mem: 20.9 GB 
[11/24 23:45:37 visual_prompt]: 	Training 200/553. train loss: 133.4921,	0.8440 s / batch. (data: 3.17e-04). ETA=12:00:37, max mem: 20.9 GB 
[11/24 23:47:14 visual_prompt]: 	Training 300/553. train loss: 234.0344,	0.8200 s / batch. (data: 3.11e-04). ETA=11:38:46, max mem: 20.9 GB 
[11/24 23:48:52 visual_prompt]: 	Training 400/553. train loss: 94.3837,	0.8878 s / batch. (data: 5.83e-02). ETA=12:35:03, max mem: 20.9 GB 
[11/24 23:50:30 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.4760 s / batch. (data: 6.49e-01). ETA=20:52:52, max mem: 20.9 GB 
[11/24 23:51:21 visual_prompt]: Epoch 8 / 100: avg data time: 1.52e-01, avg batch time: 0.9819, average train loss: 73.7924
[11/24 23:52:17 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3101, average loss: 203.1180
[11/24 23:52:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.60	
[11/24 23:52:17 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/24 23:53:58 visual_prompt]: 	Training 100/553. train loss: 89.9425,	0.8280 s / batch. (data: 3.10e-04). ETA=11:40:42, max mem: 20.9 GB 
[11/24 23:55:35 visual_prompt]: 	Training 200/553. train loss: 24.2411,	0.8180 s / batch. (data: 2.91e-04). ETA=11:30:54, max mem: 20.9 GB 
[11/24 23:57:16 visual_prompt]: 	Training 300/553. train loss: 93.5788,	1.9918 s / batch. (data: 1.19e+00). ETA=1 day, 3:58:54, max mem: 20.9 GB 
[11/24 23:59:03 visual_prompt]: 	Training 400/553. train loss: 12.0334,	0.8588 s / batch. (data: 7.96e-03). ETA=12:02:30, max mem: 20.9 GB 
[11/25 00:00:47 visual_prompt]: 	Training 500/553. train loss: 3.0766,	0.9080 s / batch. (data: 5.31e-02). ETA=12:42:19, max mem: 20.9 GB 
[11/25 00:01:38 visual_prompt]: Epoch 9 / 100: avg data time: 1.84e-01, avg batch time: 1.0144, average train loss: 48.2814
[11/25 00:02:34 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3098, average loss: 42.4104
[11/25 00:02:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.11	
[11/25 00:02:34 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/25 00:04:18 visual_prompt]: 	Training 100/553. train loss: 184.7925,	0.8211 s / batch. (data: 7.34e-04). ETA=11:27:18, max mem: 20.9 GB 
[11/25 00:05:54 visual_prompt]: 	Training 200/553. train loss: 3.7531,	0.8178 s / batch. (data: 3.35e-04). ETA=11:23:12, max mem: 20.9 GB 
[11/25 00:07:31 visual_prompt]: 	Training 300/553. train loss: 19.7834,	2.7983 s / batch. (data: 1.97e+00). ETA=1 day, 14:53:01, max mem: 20.9 GB 
[11/25 00:09:07 visual_prompt]: 	Training 400/553. train loss: 71.5181,	0.8381 s / batch. (data: 8.59e-03). ETA=11:37:17, max mem: 20.9 GB 
[11/25 00:10:48 visual_prompt]: 	Training 500/553. train loss: 93.8140,	0.9413 s / batch. (data: 1.34e-01). ETA=13:01:39, max mem: 20.9 GB 
[11/25 00:11:44 visual_prompt]: Epoch 10 / 100: avg data time: 1.68e-01, avg batch time: 0.9954, average train loss: 82.9872
[11/25 00:12:45 visual_prompt]: Inference (val):avg data time: 9.32e-05, avg batch time: 0.3090, average loss: 48.1733
[11/25 00:12:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.14	
[11/25 00:12:45 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/25 00:14:34 visual_prompt]: 	Training 100/553. train loss: 151.3405,	0.8431 s / batch. (data: 5.53e-03). ETA=11:37:58, max mem: 20.9 GB 
[11/25 00:16:22 visual_prompt]: 	Training 200/553. train loss: 99.0040,	0.8361 s / batch. (data: 3.16e-04). ETA=11:30:47, max mem: 20.9 GB 
[11/25 00:18:07 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.2653 s / batch. (data: 1.44e+00). ETA=1 day, 7:07:46, max mem: 20.9 GB 
[11/25 00:19:42 visual_prompt]: 	Training 400/553. train loss: 181.2127,	0.8680 s / batch. (data: 6.81e-04). ETA=11:54:13, max mem: 20.9 GB 
[11/25 00:21:18 visual_prompt]: 	Training 500/553. train loss: 98.6965,	0.8289 s / batch. (data: 8.59e-03). ETA=11:20:38, max mem: 20.9 GB 
[11/25 00:22:09 visual_prompt]: Epoch 11 / 100: avg data time: 1.92e-01, avg batch time: 1.0189, average train loss: 72.5064
[11/25 00:23:05 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3100, average loss: 107.5733
[11/25 00:23:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.46	
[11/25 00:23:05 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/25 00:24:47 visual_prompt]: 	Training 100/553. train loss: 54.0880,	0.8091 s / batch. (data: 2.94e-04). ETA=11:02:19, max mem: 20.9 GB 
[11/25 00:26:25 visual_prompt]: 	Training 200/553. train loss: 48.0289,	0.8209 s / batch. (data: 3.23e-04). ETA=11:10:37, max mem: 20.9 GB 
[11/25 00:28:01 visual_prompt]: 	Training 300/553. train loss: 37.7816,	0.8400 s / batch. (data: 2.94e-04). ETA=11:24:51, max mem: 20.9 GB 
[11/25 00:29:39 visual_prompt]: 	Training 400/553. train loss: 92.9019,	0.8400 s / batch. (data: 3.24e-04). ETA=11:23:24, max mem: 20.9 GB 
[11/25 00:31:17 visual_prompt]: 	Training 500/553. train loss: 41.5009,	0.8400 s / batch. (data: 2.05e-02). ETA=11:22:01, max mem: 20.9 GB 
[11/25 00:32:06 visual_prompt]: Epoch 12 / 100: avg data time: 1.52e-01, avg batch time: 0.9798, average train loss: 69.8166
[11/25 00:33:02 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3094, average loss: 98.0493
[11/25 00:33:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.43	
[11/25 00:33:02 visual_prompt]: Best epoch 12: best metric: -98.049
[11/25 00:33:02 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/25 00:34:45 visual_prompt]: 	Training 100/553. train loss: 73.2958,	0.8271 s / batch. (data: 5.42e-03). ETA=11:09:28, max mem: 20.9 GB 
[11/25 00:36:28 visual_prompt]: 	Training 200/553. train loss: 69.2323,	0.8216 s / batch. (data: 3.21e-04). ETA=11:03:38, max mem: 20.9 GB 
[11/25 00:38:14 visual_prompt]: 	Training 300/553. train loss: 30.0433,	2.0219 s / batch. (data: 1.18e+00). ETA=1 day, 3:09:45, max mem: 20.9 GB 
[11/25 00:39:58 visual_prompt]: 	Training 400/553. train loss: 241.2708,	0.8412 s / batch. (data: 1.06e-02). ETA=11:16:39, max mem: 20.9 GB 
[11/25 00:41:40 visual_prompt]: 	Training 500/553. train loss: 34.8105,	0.8385 s / batch. (data: 2.92e-04). ETA=11:13:05, max mem: 20.9 GB 
[11/25 00:42:31 visual_prompt]: Epoch 13 / 100: avg data time: 2.01e-01, avg batch time: 1.0296, average train loss: 97.3977
[11/25 00:43:28 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3110, average loss: 54.5825
[11/25 00:43:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.08	
[11/25 00:43:28 visual_prompt]: Best epoch 13: best metric: -54.582
[11/25 00:43:28 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/25 00:45:10 visual_prompt]: 	Training 100/553. train loss: 58.1163,	0.8293 s / batch. (data: 2.80e-04). ETA=11:03:33, max mem: 20.9 GB 
[11/25 00:46:47 visual_prompt]: 	Training 200/553. train loss: 0.0389,	0.8480 s / batch. (data: 1.20e-02). ETA=11:17:07, max mem: 20.9 GB 
[11/25 00:48:24 visual_prompt]: 	Training 300/553. train loss: 34.0453,	0.8400 s / batch. (data: 3.24e-04). ETA=11:09:21, max mem: 20.9 GB 
[11/25 00:50:00 visual_prompt]: 	Training 400/553. train loss: 20.2472,	0.8320 s / batch. (data: 2.98e-04). ETA=11:01:34, max mem: 20.9 GB 
[11/25 00:51:37 visual_prompt]: 	Training 500/553. train loss: 164.4967,	0.8110 s / batch. (data: 3.05e-04). ETA=10:43:31, max mem: 20.9 GB 
[11/25 00:52:27 visual_prompt]: Epoch 14 / 100: avg data time: 1.46e-01, avg batch time: 0.9753, average train loss: 61.9254
[11/25 00:53:23 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3097, average loss: 27.4170
[11/25 00:53:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.04	
[11/25 00:53:23 visual_prompt]: Best epoch 14: best metric: -27.417
[11/25 00:53:23 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/25 00:55:04 visual_prompt]: 	Training 100/553. train loss: 110.0935,	0.8275 s / batch. (data: 3.51e-04). ETA=10:54:29, max mem: 20.9 GB 
[11/25 00:56:40 visual_prompt]: 	Training 200/553. train loss: 372.1039,	0.8188 s / batch. (data: 2.96e-04). ETA=10:46:17, max mem: 20.9 GB 
[11/25 00:58:20 visual_prompt]: 	Training 300/553. train loss: 5.5879,	0.8309 s / batch. (data: 7.76e-04). ETA=10:54:27, max mem: 20.9 GB 
[11/25 00:59:55 visual_prompt]: 	Training 400/553. train loss: 0.3316,	1.0480 s / batch. (data: 2.08e-01). ETA=13:43:41, max mem: 20.9 GB 
[11/25 01:01:34 visual_prompt]: 	Training 500/553. train loss: 112.7657,	0.8366 s / batch. (data: 8.50e-03). ETA=10:56:07, max mem: 20.9 GB 
[11/25 01:02:25 visual_prompt]: Epoch 15 / 100: avg data time: 1.50e-01, avg batch time: 0.9797, average train loss: 87.3216
[11/25 01:03:20 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3092, average loss: 210.5192
[11/25 01:03:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.00	
[11/25 01:03:20 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/25 01:05:00 visual_prompt]: 	Training 100/553. train loss: 16.6388,	0.8286 s / batch. (data: 2.89e-04). ETA=10:47:45, max mem: 20.9 GB 
[11/25 01:06:38 visual_prompt]: 	Training 200/553. train loss: 101.8530,	0.8399 s / batch. (data: 1.55e-02). ETA=10:55:13, max mem: 20.9 GB 
[11/25 01:08:15 visual_prompt]: 	Training 300/553. train loss: 149.5668,	0.8168 s / batch. (data: 2.79e-04). ETA=10:35:50, max mem: 20.9 GB 
[11/25 01:09:53 visual_prompt]: 	Training 400/553. train loss: 25.5012,	0.8551 s / batch. (data: 1.11e-02). ETA=11:04:09, max mem: 20.9 GB 
[11/25 01:11:29 visual_prompt]: 	Training 500/553. train loss: 156.1897,	1.0906 s / batch. (data: 2.60e-01). ETA=14:05:19, max mem: 20.9 GB 
[11/25 01:12:21 visual_prompt]: Epoch 16 / 100: avg data time: 1.50e-01, avg batch time: 0.9771, average train loss: 82.4612
[11/25 01:13:16 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3090, average loss: 25.8207
[11/25 01:13:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.73	
[11/25 01:13:16 visual_prompt]: Best epoch 16: best metric: -25.821
[11/25 01:13:16 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/25 01:14:57 visual_prompt]: 	Training 100/553. train loss: 79.3474,	0.8360 s / batch. (data: 2.90e-04). ETA=10:45:50, max mem: 20.9 GB 
[11/25 01:16:36 visual_prompt]: 	Training 200/553. train loss: 313.7220,	0.8200 s / batch. (data: 2.96e-04). ETA=10:32:05, max mem: 20.9 GB 
[11/25 01:18:13 visual_prompt]: 	Training 300/553. train loss: 178.4385,	0.8456 s / batch. (data: 1.55e-02). ETA=10:50:25, max mem: 20.9 GB 
[11/25 01:19:49 visual_prompt]: 	Training 400/553. train loss: 55.0876,	1.0850 s / batch. (data: 2.69e-01). ETA=13:52:47, max mem: 20.9 GB 
[11/25 01:21:26 visual_prompt]: 	Training 500/553. train loss: 61.1381,	1.4730 s / batch. (data: 6.63e-01). ETA=18:48:07, max mem: 20.9 GB 
[11/25 01:22:18 visual_prompt]: Epoch 17 / 100: avg data time: 1.51e-01, avg batch time: 0.9797, average train loss: 96.4472
[11/25 01:23:14 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3114, average loss: 52.4195
[11/25 01:23:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.80	
[11/25 01:23:14 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/25 01:24:55 visual_prompt]: 	Training 100/553. train loss: 179.4604,	0.8357 s / batch. (data: 5.40e-03). ETA=10:37:53, max mem: 20.9 GB 
[11/25 01:26:35 visual_prompt]: 	Training 200/553. train loss: 6.3386,	0.8612 s / batch. (data: 2.52e-02). ETA=10:55:54, max mem: 20.9 GB 
[11/25 01:28:12 visual_prompt]: 	Training 300/553. train loss: 5.8222,	0.8599 s / batch. (data: 1.05e-02). ETA=10:53:32, max mem: 20.9 GB 
[11/25 01:29:49 visual_prompt]: 	Training 400/553. train loss: 27.2760,	0.8252 s / batch. (data: 9.24e-03). ETA=10:25:44, max mem: 20.9 GB 
[11/25 01:31:26 visual_prompt]: 	Training 500/553. train loss: 103.9776,	0.8320 s / batch. (data: 3.14e-04). ETA=10:29:32, max mem: 20.9 GB 
[11/25 01:32:16 visual_prompt]: Epoch 18 / 100: avg data time: 1.51e-01, avg batch time: 0.9790, average train loss: 108.9397
[11/25 01:33:11 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3107, average loss: 174.6840
[11/25 01:33:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.30	
[11/25 01:33:11 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/25 01:34:52 visual_prompt]: 	Training 100/553. train loss: 62.0611,	1.1855 s / batch. (data: 3.82e-01). ETA=14:54:00, max mem: 20.9 GB 
[11/25 01:36:30 visual_prompt]: 	Training 200/553. train loss: 12.6486,	0.8638 s / batch. (data: 2.94e-04). ETA=10:49:56, max mem: 20.9 GB 
[11/25 01:38:07 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8025 s / batch. (data: 2.87e-04). ETA=10:02:27, max mem: 20.9 GB 
[11/25 01:39:46 visual_prompt]: 	Training 400/553. train loss: 51.9433,	0.8201 s / batch. (data: 2.71e-04). ETA=10:14:18, max mem: 20.9 GB 
[11/25 01:41:19 visual_prompt]: 	Training 500/553. train loss: 25.7980,	0.8160 s / batch. (data: 3.19e-04). ETA=10:09:53, max mem: 20.9 GB 
[11/25 01:42:10 visual_prompt]: Epoch 19 / 100: avg data time: 1.45e-01, avg batch time: 0.9744, average train loss: 68.3429
[11/25 01:43:05 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3119, average loss: 5.8235
[11/25 01:43:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.74	
[11/25 01:43:05 visual_prompt]: Best epoch 19: best metric: -5.824
[11/25 01:43:05 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/25 01:44:45 visual_prompt]: 	Training 100/553. train loss: 9.4218,	0.8413 s / batch. (data: 3.76e-03). ETA=10:26:39, max mem: 20.9 GB 
[11/25 01:46:24 visual_prompt]: 	Training 200/553. train loss: 10.9273,	0.8600 s / batch. (data: 2.79e-02). ETA=10:39:08, max mem: 20.9 GB 
[11/25 01:48:02 visual_prompt]: 	Training 300/553. train loss: 76.0858,	0.8728 s / batch. (data: 1.10e-02). ETA=10:47:11, max mem: 20.9 GB 
[11/25 01:49:39 visual_prompt]: 	Training 400/553. train loss: 187.2953,	0.8280 s / batch. (data: 1.20e-02). ETA=10:12:37, max mem: 20.9 GB 
[11/25 01:51:15 visual_prompt]: 	Training 500/553. train loss: 45.7235,	0.8369 s / batch. (data: 4.81e-03). ETA=10:17:46, max mem: 20.9 GB 
[11/25 01:52:07 visual_prompt]: Epoch 20 / 100: avg data time: 1.51e-01, avg batch time: 0.9794, average train loss: 91.9706
[11/25 01:53:03 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3102, average loss: 98.4013
[11/25 01:53:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.25	
[11/25 01:53:03 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/25 01:54:47 visual_prompt]: 	Training 100/553. train loss: 251.7324,	0.8718 s / batch. (data: 5.01e-02). ETA=10:41:20, max mem: 20.9 GB 
[11/25 01:56:23 visual_prompt]: 	Training 200/553. train loss: 142.9680,	0.8480 s / batch. (data: 2.86e-04). ETA=10:22:26, max mem: 20.9 GB 
[11/25 01:58:00 visual_prompt]: 	Training 300/553. train loss: 511.1374,	1.1480 s / batch. (data: 3.23e-01). ETA=14:00:43, max mem: 20.9 GB 
[11/25 01:59:36 visual_prompt]: 	Training 400/553. train loss: 197.3434,	0.8355 s / batch. (data: 5.43e-03). ETA=10:10:29, max mem: 20.9 GB 
[11/25 02:01:15 visual_prompt]: 	Training 500/553. train loss: 40.3862,	0.8520 s / batch. (data: 3.00e-04). ETA=10:21:06, max mem: 20.9 GB 
[11/25 02:02:05 visual_prompt]: Epoch 21 / 100: avg data time: 1.52e-01, avg batch time: 0.9807, average train loss: 107.1739
[11/25 02:03:01 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3097, average loss: 11.7291
[11/25 02:03:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.31	
[11/25 02:03:01 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/25 02:04:41 visual_prompt]: 	Training 100/553. train loss: 129.5088,	0.8357 s / batch. (data: 2.86e-04). ETA=10:07:07, max mem: 20.9 GB 
[11/25 02:06:18 visual_prompt]: 	Training 200/553. train loss: 8.2347,	0.8401 s / batch. (data: 3.02e-04). ETA=10:08:54, max mem: 20.9 GB 
[11/25 02:07:54 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8240 s / batch. (data: 7.94e-03). ETA=9:55:49, max mem: 20.9 GB 
[11/25 02:09:32 visual_prompt]: 	Training 400/553. train loss: 119.1904,	0.8760 s / batch. (data: 1.20e-02). ETA=10:31:59, max mem: 20.9 GB 
[11/25 02:11:10 visual_prompt]: 	Training 500/553. train loss: 41.2609,	0.8228 s / batch. (data: 2.99e-04). ETA=9:52:15, max mem: 20.9 GB 
[11/25 02:12:02 visual_prompt]: Epoch 22 / 100: avg data time: 1.49e-01, avg batch time: 0.9780, average train loss: 77.9296
[11/25 02:12:57 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3097, average loss: 18.1174
[11/25 02:12:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.81	
[11/25 02:12:57 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/25 02:14:40 visual_prompt]: 	Training 100/553. train loss: 18.7215,	0.8172 s / batch. (data: 3.32e-04). ETA=9:46:08, max mem: 20.9 GB 
[11/25 02:16:18 visual_prompt]: 	Training 200/553. train loss: 2.1599,	0.8417 s / batch. (data: 5.42e-03). ETA=10:02:16, max mem: 20.9 GB 
[11/25 02:17:57 visual_prompt]: 	Training 300/553. train loss: 92.5511,	0.8405 s / batch. (data: 5.42e-03). ETA=9:59:59, max mem: 20.9 GB 
[11/25 02:19:33 visual_prompt]: 	Training 400/553. train loss: 17.3159,	0.8480 s / batch. (data: 7.90e-04). ETA=10:04:00, max mem: 20.9 GB 
[11/25 02:21:08 visual_prompt]: 	Training 500/553. train loss: 85.7274,	0.8555 s / batch. (data: 1.05e-02). ETA=10:07:55, max mem: 20.9 GB 
[11/25 02:21:59 visual_prompt]: Epoch 23 / 100: avg data time: 1.51e-01, avg batch time: 0.9798, average train loss: 68.7983
[11/25 02:22:56 visual_prompt]: Inference (val):avg data time: 6.06e-05, avg batch time: 0.3223, average loss: 93.9192
[11/25 02:22:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.86	
[11/25 02:22:56 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/25 02:24:34 visual_prompt]: 	Training 100/553. train loss: 121.3363,	0.8317 s / batch. (data: 5.43e-03). ETA=9:48:51, max mem: 20.9 GB 
[11/25 02:26:10 visual_prompt]: 	Training 200/553. train loss: 30.4765,	0.8512 s / batch. (data: 1.11e-02). ETA=10:01:14, max mem: 20.9 GB 
[11/25 02:27:49 visual_prompt]: 	Training 300/553. train loss: 30.0802,	1.0782 s / batch. (data: 2.37e-01). ETA=12:39:47, max mem: 20.9 GB 
[11/25 02:29:27 visual_prompt]: 	Training 400/553. train loss: 24.1946,	0.8464 s / batch. (data: 1.04e-02). ETA=9:55:03, max mem: 20.9 GB 
[11/25 02:31:06 visual_prompt]: 	Training 500/553. train loss: 155.4979,	0.8608 s / batch. (data: 1.07e-02). ETA=10:03:44, max mem: 20.9 GB 
[11/25 02:31:57 visual_prompt]: Epoch 24 / 100: avg data time: 1.50e-01, avg batch time: 0.9786, average train loss: 77.4350
[11/25 02:32:53 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3083, average loss: 59.1853
[11/25 02:32:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.16	
[11/25 02:32:53 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/25 02:34:36 visual_prompt]: 	Training 100/553. train loss: 197.0322,	0.8277 s / batch. (data: 3.00e-04). ETA=9:38:25, max mem: 20.9 GB 
[11/25 02:36:11 visual_prompt]: 	Training 200/553. train loss: 35.0548,	0.8100 s / batch. (data: 3.25e-04). ETA=9:24:39, max mem: 20.9 GB 
[11/25 02:37:47 visual_prompt]: 	Training 300/553. train loss: 140.3271,	1.0640 s / batch. (data: 2.49e-01). ETA=12:19:58, max mem: 20.9 GB 
[11/25 02:39:25 visual_prompt]: 	Training 400/553. train loss: 81.6939,	1.2800 s / batch. (data: 4.32e-01). ETA=14:48:04, max mem: 20.9 GB 
[11/25 02:41:03 visual_prompt]: 	Training 500/553. train loss: 61.1020,	1.3745 s / batch. (data: 5.65e-01). ETA=15:51:18, max mem: 20.9 GB 
[11/25 02:41:54 visual_prompt]: Epoch 25 / 100: avg data time: 1.50e-01, avg batch time: 0.9784, average train loss: 73.0670
[11/25 02:42:49 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3115, average loss: 142.0155
[11/25 02:42:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.36	
[11/25 02:42:49 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/25 02:44:31 visual_prompt]: 	Training 100/553. train loss: 29.4657,	0.8319 s / batch. (data: 1.19e-02). ETA=9:33:38, max mem: 20.9 GB 
[11/25 02:46:10 visual_prompt]: 	Training 200/553. train loss: 385.2049,	1.6146 s / batch. (data: 7.88e-01). ETA=18:30:40, max mem: 20.9 GB 
[11/25 02:47:49 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8280 s / batch. (data: 1.05e-02). ETA=9:28:12, max mem: 20.9 GB 
[11/25 02:49:25 visual_prompt]: 	Training 400/553. train loss: 54.7317,	0.8160 s / batch. (data: 3.02e-04). ETA=9:18:36, max mem: 20.9 GB 
[11/25 02:51:01 visual_prompt]: 	Training 500/553. train loss: 57.9050,	0.8316 s / batch. (data: 2.93e-04). ETA=9:27:55, max mem: 20.9 GB 
[11/25 02:51:52 visual_prompt]: Epoch 26 / 100: avg data time: 1.51e-01, avg batch time: 0.9808, average train loss: 73.4555
[11/25 02:52:47 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3107, average loss: 80.7753
[11/25 02:52:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.97	
[11/25 02:52:47 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[11/25 02:54:29 visual_prompt]: 	Training 100/553. train loss: 146.1667,	0.8540 s / batch. (data: 3.04e-04). ETA=9:41:02, max mem: 20.9 GB 
[11/25 02:56:06 visual_prompt]: 	Training 200/553. train loss: 171.7426,	0.9796 s / batch. (data: 1.49e-01). ETA=11:04:51, max mem: 20.9 GB 
[11/25 02:57:44 visual_prompt]: 	Training 300/553. train loss: 71.7340,	0.8067 s / batch. (data: 3.20e-04). ETA=9:06:11, max mem: 20.9 GB 
[11/25 02:59:22 visual_prompt]: 	Training 400/553. train loss: 35.2200,	0.8520 s / batch. (data: 7.14e-04). ETA=9:35:24, max mem: 20.9 GB 
[11/25 03:01:00 visual_prompt]: 	Training 500/553. train loss: 3.3718,	0.8440 s / batch. (data: 7.91e-04). ETA=9:28:34, max mem: 20.9 GB 
[11/25 03:01:49 visual_prompt]: Epoch 27 / 100: avg data time: 1.51e-01, avg batch time: 0.9803, average train loss: 64.3172
[11/25 03:02:45 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3100, average loss: 175.3402
[11/25 03:02:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.95	
[11/25 03:02:45 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[11/25 03:04:25 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8110 s / batch. (data: 3.24e-04). ETA=9:04:19, max mem: 20.9 GB 
[11/25 03:06:03 visual_prompt]: 	Training 200/553. train loss: 155.6715,	0.8342 s / batch. (data: 3.14e-04). ETA=9:18:28, max mem: 20.9 GB 
[11/25 03:07:41 visual_prompt]: 	Training 300/553. train loss: 14.1574,	1.2360 s / batch. (data: 4.08e-01). ETA=13:45:25, max mem: 20.9 GB 
[11/25 03:09:17 visual_prompt]: 	Training 400/553. train loss: 171.3097,	0.8480 s / batch. (data: 7.98e-03). ETA=9:24:53, max mem: 20.9 GB 
[11/25 03:10:54 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8244 s / batch. (data: 7.95e-03). ETA=9:07:48, max mem: 20.9 GB 
[11/25 03:11:45 visual_prompt]: Epoch 28 / 100: avg data time: 1.48e-01, avg batch time: 0.9769, average train loss: 73.8999
[11/25 03:12:41 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3087, average loss: 56.4914
[11/25 03:12:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.33	
[11/25 03:12:41 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[11/25 03:14:28 visual_prompt]: 	Training 100/553. train loss: 34.7881,	0.8520 s / batch. (data: 7.98e-03). ETA=9:23:58, max mem: 20.9 GB 
[11/25 03:16:04 visual_prompt]: 	Training 200/553. train loss: 0.0895,	1.6840 s / batch. (data: 8.40e-01). ETA=18:31:53, max mem: 20.9 GB 
[11/25 03:17:40 visual_prompt]: 	Training 300/553. train loss: 121.7846,	0.8360 s / batch. (data: 2.90e-04). ETA=9:10:35, max mem: 20.9 GB 
[11/25 03:19:15 visual_prompt]: 	Training 400/553. train loss: 119.9498,	1.1462 s / batch. (data: 3.41e-01). ETA=12:32:58, max mem: 20.9 GB 
[11/25 03:20:52 visual_prompt]: 	Training 500/553. train loss: 61.3509,	0.8280 s / batch. (data: 4.15e-04). ETA=9:02:33, max mem: 20.9 GB 
[11/25 03:21:43 visual_prompt]: Epoch 29 / 100: avg data time: 1.51e-01, avg batch time: 0.9796, average train loss: 84.4679
[11/25 03:22:39 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3095, average loss: 26.3829
[11/25 03:22:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.01	
[11/25 03:22:39 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[11/25 03:24:19 visual_prompt]: 	Training 100/553. train loss: 59.7197,	0.8253 s / batch. (data: 3.11e-04). ETA=8:58:42, max mem: 20.9 GB 
[11/25 03:25:57 visual_prompt]: 	Training 200/553. train loss: 133.5192,	0.8148 s / batch. (data: 3.03e-04). ETA=8:50:28, max mem: 20.9 GB 
[11/25 03:27:32 visual_prompt]: 	Training 300/553. train loss: 78.8985,	1.1259 s / batch. (data: 2.97e-01). ETA=12:11:09, max mem: 20.9 GB 
[11/25 03:29:11 visual_prompt]: 	Training 400/553. train loss: 3.7242,	0.9002 s / batch. (data: 8.14e-02). ETA=9:43:06, max mem: 20.9 GB 
[11/25 03:30:48 visual_prompt]: 	Training 500/553. train loss: 44.4254,	1.3791 s / batch. (data: 5.48e-01). ETA=14:50:56, max mem: 20.9 GB 
[11/25 03:31:41 visual_prompt]: Epoch 30 / 100: avg data time: 1.51e-01, avg batch time: 0.9800, average train loss: 76.9443
[11/25 03:32:37 visual_prompt]: Inference (val):avg data time: 1.82e-04, avg batch time: 0.3144, average loss: 15.7663
[11/25 03:32:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.12	
[11/25 03:32:37 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[11/25 03:34:19 visual_prompt]: 	Training 100/553. train loss: 12.9275,	0.8377 s / batch. (data: 3.24e-04). ETA=8:59:05, max mem: 20.9 GB 
[11/25 03:35:58 visual_prompt]: 	Training 200/553. train loss: 239.9660,	0.8320 s / batch. (data: 5.46e-03). ETA=8:53:59, max mem: 20.9 GB 
[11/25 03:37:33 visual_prompt]: 	Training 300/553. train loss: 74.7414,	0.8400 s / batch. (data: 3.08e-04). ETA=8:57:44, max mem: 20.9 GB 
[11/25 03:39:10 visual_prompt]: 	Training 400/553. train loss: 25.2471,	1.1454 s / batch. (data: 3.20e-01). ETA=12:11:21, max mem: 20.9 GB 
[11/25 03:40:48 visual_prompt]: 	Training 500/553. train loss: 30.2537,	0.8677 s / batch. (data: 5.42e-03). ETA=9:12:33, max mem: 20.9 GB 
[11/25 03:41:38 visual_prompt]: Epoch 31 / 100: avg data time: 1.49e-01, avg batch time: 0.9781, average train loss: 63.6474
[11/25 03:42:33 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3098, average loss: 18.8359
[11/25 03:42:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.38	
[11/25 03:42:33 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[11/25 03:44:16 visual_prompt]: 	Training 100/553. train loss: 15.2904,	0.8632 s / batch. (data: 2.88e-04). ETA=9:07:29, max mem: 20.9 GB 
[11/25 03:45:52 visual_prompt]: 	Training 200/553. train loss: 58.0532,	0.8350 s / batch. (data: 3.05e-04). ETA=8:48:13, max mem: 20.9 GB 
[11/25 03:47:33 visual_prompt]: 	Training 300/553. train loss: 40.2767,	0.8141 s / batch. (data: 2.93e-04). ETA=8:33:38, max mem: 20.9 GB 
[11/25 03:49:11 visual_prompt]: 	Training 400/553. train loss: 32.6062,	0.8473 s / batch. (data: 1.84e-02). ETA=8:53:12, max mem: 20.9 GB 
[11/25 03:50:46 visual_prompt]: 	Training 500/553. train loss: 2.6893,	0.8310 s / batch. (data: 2.79e-04). ETA=8:41:33, max mem: 20.9 GB 
[11/25 03:51:36 visual_prompt]: Epoch 32 / 100: avg data time: 1.52e-01, avg batch time: 0.9807, average train loss: 68.8120
[11/25 03:52:31 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3100, average loss: 18.6799
[11/25 03:52:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.72	
[11/25 03:52:31 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[11/25 03:54:11 visual_prompt]: 	Training 100/553. train loss: 0.1960,	0.8166 s / batch. (data: 2.90e-04). ETA=8:30:24, max mem: 20.9 GB 
[11/25 03:55:50 visual_prompt]: 	Training 200/553. train loss: 67.9366,	1.1207 s / batch. (data: 3.07e-01). ETA=11:38:37, max mem: 20.9 GB 
[11/25 03:57:26 visual_prompt]: 	Training 300/553. train loss: 51.7071,	0.8387 s / batch. (data: 3.30e-04). ETA=8:41:25, max mem: 20.9 GB 
[11/25 03:59:05 visual_prompt]: 	Training 400/553. train loss: 24.1078,	0.8341 s / batch. (data: 3.00e-04). ETA=8:37:10, max mem: 20.9 GB 
[11/25 04:00:42 visual_prompt]: 	Training 500/553. train loss: 8.8907,	0.8441 s / batch. (data: 5.43e-03). ETA=8:41:58, max mem: 20.9 GB 
[11/25 04:01:32 visual_prompt]: Epoch 33 / 100: avg data time: 1.48e-01, avg batch time: 0.9770, average train loss: 60.6009
[11/25 04:02:27 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3113, average loss: 30.7730
[11/25 04:02:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.16	
[11/25 04:02:27 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[11/25 04:04:09 visual_prompt]: 	Training 100/553. train loss: 75.7294,	0.8440 s / batch. (data: 3.17e-04). ETA=8:39:45, max mem: 20.9 GB 
[11/25 04:05:45 visual_prompt]: 	Training 200/553. train loss: 66.3294,	0.8324 s / batch. (data: 3.25e-04). ETA=8:31:15, max mem: 20.9 GB 
[11/25 04:07:21 visual_prompt]: 	Training 300/553. train loss: 104.9058,	0.8160 s / batch. (data: 3.26e-04). ETA=8:19:47, max mem: 20.9 GB 
[11/25 04:09:00 visual_prompt]: 	Training 400/553. train loss: 131.9211,	0.8353 s / batch. (data: 2.90e-04). ETA=8:30:13, max mem: 20.9 GB 
[11/25 04:10:38 visual_prompt]: 	Training 500/553. train loss: 32.8797,	1.3960 s / batch. (data: 5.41e-01). ETA=14:10:23, max mem: 20.9 GB 
[11/25 04:11:28 visual_prompt]: Epoch 34 / 100: avg data time: 1.49e-01, avg batch time: 0.9785, average train loss: 70.1690
[11/25 04:12:24 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3109, average loss: 99.1878
[11/25 04:12:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.86	
[11/25 04:12:24 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[11/25 04:14:07 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8080 s / batch. (data: 3.22e-04). ETA=8:10:10, max mem: 20.9 GB 
[11/25 04:15:46 visual_prompt]: 	Training 200/553. train loss: 13.2344,	0.8527 s / batch. (data: 1.05e-02). ETA=8:35:49, max mem: 20.9 GB 
[11/25 04:17:21 visual_prompt]: 	Training 300/553. train loss: 16.2176,	0.8677 s / batch. (data: 2.78e-02). ETA=8:43:27, max mem: 20.9 GB 
[11/25 04:18:58 visual_prompt]: 	Training 400/553. train loss: 85.7057,	0.8109 s / batch. (data: 5.43e-03). ETA=8:07:51, max mem: 20.9 GB 
[11/25 04:20:34 visual_prompt]: 	Training 500/553. train loss: 59.5886,	0.8318 s / batch. (data: 2.88e-04). ETA=8:19:01, max mem: 20.9 GB 
[11/25 04:21:26 visual_prompt]: Epoch 35 / 100: avg data time: 1.52e-01, avg batch time: 0.9793, average train loss: 69.4832
[11/25 04:22:21 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3106, average loss: 222.5221
[11/25 04:22:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.66	
[11/25 04:22:21 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[11/25 04:24:02 visual_prompt]: 	Training 100/553. train loss: 14.2917,	0.8440 s / batch. (data: 3.20e-04). ETA=8:24:13, max mem: 20.9 GB 
[11/25 04:25:41 visual_prompt]: 	Training 200/553. train loss: 213.9611,	0.8343 s / batch. (data: 2.87e-04). ETA=8:17:03, max mem: 20.9 GB 
[11/25 04:27:20 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8192 s / batch. (data: 5.38e-03). ETA=8:06:42, max mem: 20.9 GB 
[11/25 04:28:56 visual_prompt]: 	Training 400/553. train loss: 34.5564,	0.8342 s / batch. (data: 3.18e-04). ETA=8:14:11, max mem: 20.9 GB 
[11/25 04:30:34 visual_prompt]: 	Training 500/553. train loss: 32.5290,	1.2720 s / batch. (data: 4.50e-01). ETA=12:31:25, max mem: 20.9 GB 
[11/25 04:31:23 visual_prompt]: Epoch 36 / 100: avg data time: 1.48e-01, avg batch time: 0.9783, average train loss: 66.5310
[11/25 04:32:18 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.3116, average loss: 187.9097
[11/25 04:32:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.82	
[11/25 04:32:18 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[11/25 04:33:59 visual_prompt]: 	Training 100/553. train loss: 40.9709,	0.8240 s / batch. (data: 3.26e-04). ETA=8:04:40, max mem: 20.9 GB 
[11/25 04:35:37 visual_prompt]: 	Training 200/553. train loss: 21.7606,	0.8258 s / batch. (data: 3.17e-04). ETA=8:04:21, max mem: 20.9 GB 
[11/25 04:37:15 visual_prompt]: 	Training 300/553. train loss: 232.1864,	1.2514 s / batch. (data: 4.23e-01). ETA=12:11:54, max mem: 20.9 GB 
[11/25 04:38:55 visual_prompt]: 	Training 400/553. train loss: 20.2850,	1.8880 s / batch. (data: 1.07e+00). ETA=18:21:05, max mem: 20.9 GB 
[11/25 04:40:28 visual_prompt]: 	Training 500/553. train loss: 67.3315,	0.9748 s / batch. (data: 1.50e-01). ETA=9:26:51, max mem: 20.9 GB 
[11/25 04:41:21 visual_prompt]: Epoch 37 / 100: avg data time: 1.52e-01, avg batch time: 0.9820, average train loss: 64.9782
[11/25 04:42:17 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3113, average loss: 151.3353
[11/25 04:42:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.82	
[11/25 04:42:17 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[11/25 04:43:57 visual_prompt]: 	Training 100/553. train loss: 80.2315,	1.0390 s / batch. (data: 1.98e-01). ETA=10:01:35, max mem: 20.9 GB 
[11/25 04:45:35 visual_prompt]: 	Training 200/553. train loss: 64.3392,	1.2440 s / batch. (data: 4.05e-01). ETA=11:58:10, max mem: 20.9 GB 
[11/25 04:47:14 visual_prompt]: 	Training 300/553. train loss: 35.4241,	0.8118 s / batch. (data: 3.01e-04). ETA=7:47:20, max mem: 20.9 GB 
[11/25 04:48:49 visual_prompt]: 	Training 400/553. train loss: 55.5618,	0.8280 s / batch. (data: 2.98e-04). ETA=7:55:15, max mem: 20.9 GB 
[11/25 04:50:29 visual_prompt]: 	Training 500/553. train loss: 100.8810,	0.8560 s / batch. (data: 2.94e-04). ETA=8:09:54, max mem: 20.9 GB 
[11/25 04:51:18 visual_prompt]: Epoch 38 / 100: avg data time: 1.50e-01, avg batch time: 0.9786, average train loss: 76.1756
[11/25 04:52:14 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3112, average loss: 0.7080
[11/25 04:52:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.61	
[11/25 04:52:14 visual_prompt]: Best epoch 38: best metric: -0.708
[11/25 04:52:14 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[11/25 04:53:54 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8355 s / batch. (data: 7.97e-03). ETA=7:56:00, max mem: 20.9 GB 
[11/25 04:55:36 visual_prompt]: 	Training 200/553. train loss: 257.0297,	0.8480 s / batch. (data: 2.94e-04). ETA=8:01:44, max mem: 20.9 GB 
[11/25 04:57:15 visual_prompt]: 	Training 300/553. train loss: 128.9853,	0.8275 s / batch. (data: 3.19e-04). ETA=7:48:43, max mem: 20.9 GB 
[11/25 04:58:51 visual_prompt]: 	Training 400/553. train loss: 10.1288,	0.8875 s / batch. (data: 5.99e-02). ETA=8:21:13, max mem: 20.9 GB 
[11/25 05:00:29 visual_prompt]: 	Training 500/553. train loss: 70.0008,	1.6995 s / batch. (data: 8.86e-01). ETA=15:57:00, max mem: 20.9 GB 
[11/25 05:01:18 visual_prompt]: Epoch 39 / 100: avg data time: 1.53e-01, avg batch time: 0.9830, average train loss: 62.4884
[11/25 05:02:12 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3109, average loss: 80.0580
[11/25 05:02:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.21	
[11/25 05:02:12 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[11/25 05:03:55 visual_prompt]: 	Training 100/553. train loss: 84.9651,	0.8151 s / batch. (data: 3.09e-04). ETA=7:36:53, max mem: 20.9 GB 
[11/25 05:05:31 visual_prompt]: 	Training 200/553. train loss: 46.7133,	0.8320 s / batch. (data: 3.05e-04). ETA=7:44:59, max mem: 20.9 GB 
[11/25 05:07:10 visual_prompt]: 	Training 300/553. train loss: 10.3414,	0.8581 s / batch. (data: 3.41e-04). ETA=7:58:08, max mem: 20.9 GB 
[11/25 05:08:48 visual_prompt]: 	Training 400/553. train loss: 82.0898,	0.8199 s / batch. (data: 7.85e-04). ETA=7:35:29, max mem: 20.9 GB 
[11/25 05:10:24 visual_prompt]: 	Training 500/553. train loss: 97.8887,	0.8320 s / batch. (data: 3.01e-04). ETA=7:40:49, max mem: 20.9 GB 
[11/25 05:11:17 visual_prompt]: Epoch 40 / 100: avg data time: 1.56e-01, avg batch time: 0.9842, average train loss: 64.2023
[11/25 05:12:12 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3114, average loss: 40.3789
[11/25 05:12:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.03	
[11/25 05:12:12 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[11/25 05:13:58 visual_prompt]: 	Training 100/553. train loss: 16.3361,	0.8280 s / batch. (data: 2.98e-04). ETA=7:36:29, max mem: 20.9 GB 
[11/25 05:15:37 visual_prompt]: 	Training 200/553. train loss: 4.5002,	0.8480 s / batch. (data: 7.90e-04). ETA=7:46:06, max mem: 20.9 GB 
[11/25 05:17:14 visual_prompt]: 	Training 300/553. train loss: 37.4190,	0.8339 s / batch. (data: 8.02e-03). ETA=7:36:58, max mem: 20.9 GB 
[11/25 05:18:50 visual_prompt]: 	Training 400/553. train loss: 63.4580,	0.8290 s / batch. (data: 7.42e-03). ETA=7:32:53, max mem: 20.9 GB 
[11/25 05:20:25 visual_prompt]: 	Training 500/553. train loss: 1.0989,	0.8245 s / batch. (data: 5.44e-03). ETA=7:29:04, max mem: 20.9 GB 
[11/25 05:21:14 visual_prompt]: Epoch 41 / 100: avg data time: 1.49e-01, avg batch time: 0.9793, average train loss: 67.2143
[11/25 05:22:09 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3110, average loss: 434.8864
[11/25 05:22:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.06	
[11/25 05:22:09 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[11/25 05:23:48 visual_prompt]: 	Training 100/553. train loss: 57.4771,	0.8313 s / batch. (data: 7.96e-03). ETA=7:30:40, max mem: 20.9 GB 
[11/25 05:25:24 visual_prompt]: 	Training 200/553. train loss: 5.7742,	0.8569 s / batch. (data: 6.02e-03). ETA=7:43:07, max mem: 20.9 GB 
[11/25 05:27:00 visual_prompt]: 	Training 300/553. train loss: 12.9850,	0.8212 s / batch. (data: 2.88e-04). ETA=7:22:26, max mem: 20.9 GB 
[11/25 05:28:36 visual_prompt]: 	Training 400/553. train loss: 11.0050,	0.8304 s / batch. (data: 3.16e-04). ETA=7:26:01, max mem: 20.9 GB 
[11/25 05:30:11 visual_prompt]: 	Training 500/553. train loss: 24.0132,	0.8437 s / batch. (data: 5.38e-03). ETA=7:31:44, max mem: 20.9 GB 
[11/25 05:31:01 visual_prompt]: Epoch 42 / 100: avg data time: 1.32e-01, avg batch time: 0.9618, average train loss: 50.8760
[11/25 05:31:56 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3098, average loss: 65.7702
[11/25 05:31:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.96	
[11/25 05:31:56 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[11/25 05:33:39 visual_prompt]: 	Training 100/553. train loss: 5.4537,	0.8372 s / batch. (data: 1.55e-02). ETA=7:26:09, max mem: 20.9 GB 
[11/25 05:35:16 visual_prompt]: 	Training 200/553. train loss: 30.4806,	0.8223 s / batch. (data: 3.26e-04). ETA=7:16:50, max mem: 20.9 GB 
[11/25 05:36:52 visual_prompt]: 	Training 300/553. train loss: 13.4713,	0.8178 s / batch. (data: 3.09e-04). ETA=7:13:05, max mem: 20.9 GB 
[11/25 05:38:27 visual_prompt]: 	Training 400/553. train loss: 116.0019,	0.8449 s / batch. (data: 1.05e-02). ETA=7:26:01, max mem: 20.9 GB 
[11/25 05:40:07 visual_prompt]: 	Training 500/553. train loss: 5.1182,	0.8480 s / batch. (data: 5.42e-03). ETA=7:26:14, max mem: 20.9 GB 
[11/25 05:40:59 visual_prompt]: Epoch 43 / 100: avg data time: 1.52e-01, avg batch time: 0.9809, average train loss: 61.3016
[11/25 05:41:55 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3106, average loss: 37.1228
[11/25 05:41:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.51	
[11/25 05:41:55 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[11/25 05:43:36 visual_prompt]: 	Training 100/553. train loss: 17.0520,	0.8455 s / batch. (data: 1.05e-02). ETA=7:22:46, max mem: 20.9 GB 
[11/25 05:45:16 visual_prompt]: 	Training 200/553. train loss: 41.8210,	0.8240 s / batch. (data: 2.83e-04). ETA=7:10:08, max mem: 20.9 GB 
[11/25 05:46:51 visual_prompt]: 	Training 300/553. train loss: 33.0272,	0.8246 s / batch. (data: 1.76e-03). ETA=7:09:04, max mem: 20.9 GB 
[11/25 05:48:28 visual_prompt]: 	Training 400/553. train loss: 18.8421,	0.8360 s / batch. (data: 5.44e-03). ETA=7:13:36, max mem: 20.9 GB 
[11/25 05:50:06 visual_prompt]: 	Training 500/553. train loss: 4.9568,	0.8176 s / batch. (data: 2.97e-04). ETA=7:02:43, max mem: 20.9 GB 
[11/25 05:50:57 visual_prompt]: Epoch 44 / 100: avg data time: 1.50e-01, avg batch time: 0.9803, average train loss: 59.3901
[11/25 05:51:52 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3105, average loss: 35.7947
[11/25 05:51:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.83	
[11/25 05:51:52 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[11/25 05:53:35 visual_prompt]: 	Training 100/553. train loss: 55.2928,	0.8320 s / batch. (data: 8.02e-03). ETA=7:08:02, max mem: 20.9 GB 
[11/25 05:55:08 visual_prompt]: 	Training 200/553. train loss: 7.9507,	0.9375 s / batch. (data: 1.02e-01). ETA=8:00:44, max mem: 20.9 GB 
[11/25 05:56:47 visual_prompt]: 	Training 300/553. train loss: 97.6082,	0.8393 s / batch. (data: 1.00e-02). ETA=7:09:00, max mem: 20.9 GB 
[11/25 05:58:22 visual_prompt]: 	Training 400/553. train loss: 46.9183,	0.8359 s / batch. (data: 1.20e-02). ETA=7:05:51, max mem: 20.9 GB 
[11/25 06:00:03 visual_prompt]: 	Training 500/553. train loss: 2.4295,	0.8400 s / batch. (data: 3.32e-04). ETA=7:06:32, max mem: 20.9 GB 
[11/25 06:00:53 visual_prompt]: Epoch 45 / 100: avg data time: 1.47e-01, avg batch time: 0.9786, average train loss: 41.2939
[11/25 06:01:49 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3109, average loss: 51.0713
[11/25 06:01:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.49	
[11/25 06:01:49 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[11/25 06:03:31 visual_prompt]: 	Training 100/553. train loss: 108.2759,	1.2513 s / batch. (data: 4.34e-01). ETA=10:32:12, max mem: 20.9 GB 
[11/25 06:05:10 visual_prompt]: 	Training 200/553. train loss: 6.5789,	0.8421 s / batch. (data: 3.06e-04). ETA=7:04:02, max mem: 20.9 GB 
[11/25 06:06:46 visual_prompt]: 	Training 300/553. train loss: 10.2063,	0.8275 s / batch. (data: 4.69e-04). ETA=6:55:19, max mem: 20.9 GB 
[11/25 06:08:24 visual_prompt]: 	Training 400/553. train loss: 48.9788,	0.8449 s / batch. (data: 7.59e-04). ETA=7:02:41, max mem: 20.9 GB 
[11/25 06:09:57 visual_prompt]: 	Training 500/553. train loss: 79.6497,	0.8281 s / batch. (data: 2.82e-04). ETA=6:52:51, max mem: 20.9 GB 
[11/25 06:10:51 visual_prompt]: Epoch 46 / 100: avg data time: 1.50e-01, avg batch time: 0.9797, average train loss: 55.6199
[11/25 06:11:46 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3080, average loss: 56.1374
[11/25 06:11:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.27	
[11/25 06:11:46 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[11/25 06:13:29 visual_prompt]: 	Training 100/553. train loss: 14.5243,	0.8400 s / batch. (data: 3.07e-04). ETA=6:56:39, max mem: 20.9 GB 
[11/25 06:15:03 visual_prompt]: 	Training 200/553. train loss: 41.7565,	1.2840 s / batch. (data: 4.45e-01). ETA=10:34:45, max mem: 20.9 GB 
[11/25 06:16:41 visual_prompt]: 	Training 300/553. train loss: 50.3816,	0.8200 s / batch. (data: 2.76e-04). ETA=6:44:01, max mem: 20.9 GB 
[11/25 06:18:18 visual_prompt]: 	Training 400/553. train loss: 16.0877,	0.8285 s / batch. (data: 7.17e-04). ETA=6:46:48, max mem: 20.9 GB 
[11/25 06:19:54 visual_prompt]: 	Training 500/553. train loss: 21.9337,	0.8237 s / batch. (data: 1.05e-02). ETA=6:43:04, max mem: 20.9 GB 
[11/25 06:20:47 visual_prompt]: Epoch 47 / 100: avg data time: 1.48e-01, avg batch time: 0.9773, average train loss: 74.1330
[11/25 06:21:42 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3097, average loss: 14.5108
[11/25 06:21:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.96	
[11/25 06:21:42 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[11/25 06:23:23 visual_prompt]: 	Training 100/553. train loss: 70.8380,	0.8154 s / batch. (data: 3.12e-04). ETA=6:36:57, max mem: 20.9 GB 
[11/25 06:25:01 visual_prompt]: 	Training 200/553. train loss: 3.5125,	0.8242 s / batch. (data: 7.56e-03). ETA=6:39:52, max mem: 20.9 GB 
[11/25 06:26:40 visual_prompt]: 	Training 300/553. train loss: 87.6523,	1.4556 s / batch. (data: 6.41e-01). ETA=11:43:45, max mem: 20.9 GB 
[11/25 06:28:14 visual_prompt]: 	Training 400/553. train loss: 48.0421,	0.8667 s / batch. (data: 3.23e-04). ETA=6:57:34, max mem: 20.9 GB 
[11/25 06:29:52 visual_prompt]: 	Training 500/553. train loss: 19.7940,	0.8411 s / batch. (data: 5.42e-03). ETA=6:43:51, max mem: 20.9 GB 
[11/25 06:30:42 visual_prompt]: Epoch 48 / 100: avg data time: 1.48e-01, avg batch time: 0.9764, average train loss: 60.5188
[11/25 06:31:38 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3105, average loss: 57.0932
[11/25 06:31:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.17	
[11/25 06:31:38 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[11/25 06:33:18 visual_prompt]: 	Training 100/553. train loss: 91.9706,	0.8254 s / batch. (data: 3.26e-04). ETA=6:34:13, max mem: 20.9 GB 
[11/25 06:34:55 visual_prompt]: 	Training 200/553. train loss: 91.7134,	0.8158 s / batch. (data: 2.86e-04). ETA=6:28:15, max mem: 20.9 GB 
[11/25 06:36:33 visual_prompt]: 	Training 300/553. train loss: 12.2309,	0.8252 s / batch. (data: 3.37e-04). ETA=6:31:23, max mem: 20.9 GB 
[11/25 06:38:12 visual_prompt]: 	Training 400/553. train loss: 2.3768,	0.8267 s / batch. (data: 9.33e-03). ETA=6:30:42, max mem: 20.9 GB 
[11/25 06:39:49 visual_prompt]: 	Training 500/553. train loss: 8.9641,	0.8510 s / batch. (data: 7.15e-04). ETA=6:40:45, max mem: 20.9 GB 
[11/25 06:40:41 visual_prompt]: Epoch 49 / 100: avg data time: 1.52e-01, avg batch time: 0.9818, average train loss: 44.0392
[11/25 06:41:36 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3101, average loss: 24.5478
[11/25 06:41:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.05	
[11/25 06:41:36 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[11/25 06:43:19 visual_prompt]: 	Training 100/553. train loss: 139.8577,	0.8400 s / batch. (data: 8.09e-04). ETA=6:33:26, max mem: 20.9 GB 
[11/25 06:44:57 visual_prompt]: 	Training 200/553. train loss: 227.0549,	0.8120 s / batch. (data: 2.72e-04). ETA=6:18:59, max mem: 20.9 GB 
[11/25 06:46:33 visual_prompt]: 	Training 300/553. train loss: 31.9506,	0.8280 s / batch. (data: 3.09e-04). ETA=6:25:03, max mem: 20.9 GB 
[11/25 06:48:09 visual_prompt]: 	Training 400/553. train loss: 15.5226,	0.8388 s / batch. (data: 3.18e-04). ETA=6:28:40, max mem: 20.9 GB 
[11/25 06:49:47 visual_prompt]: 	Training 500/553. train loss: 82.9442,	0.8456 s / batch. (data: 2.20e-02). ETA=6:30:24, max mem: 20.9 GB 
[11/25 06:50:37 visual_prompt]: Epoch 50 / 100: avg data time: 1.48e-01, avg batch time: 0.9774, average train loss: 47.5957
[11/25 06:51:33 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3101, average loss: 36.8941
[11/25 06:51:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.32	
[11/25 06:51:33 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[11/25 06:53:14 visual_prompt]: 	Training 100/553. train loss: 63.9178,	1.0560 s / batch. (data: 2.44e-01). ETA=8:04:52, max mem: 20.9 GB 
[11/25 06:54:52 visual_prompt]: 	Training 200/553. train loss: 154.4764,	0.8469 s / batch. (data: 3.08e-04). ETA=6:27:28, max mem: 20.9 GB 
[11/25 06:56:30 visual_prompt]: 	Training 300/553. train loss: 17.4423,	1.3886 s / batch. (data: 5.49e-01). ETA=10:32:59, max mem: 20.9 GB 
[11/25 06:58:07 visual_prompt]: 	Training 400/553. train loss: 223.1041,	1.3241 s / batch. (data: 4.86e-01). ETA=10:01:22, max mem: 20.9 GB 
[11/25 06:59:44 visual_prompt]: 	Training 500/553. train loss: 73.4741,	0.8293 s / batch. (data: 7.80e-04). ETA=6:15:16, max mem: 20.9 GB 
[11/25 07:00:33 visual_prompt]: Epoch 51 / 100: avg data time: 1.47e-01, avg batch time: 0.9778, average train loss: 41.7550
[11/25 07:01:29 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3094, average loss: 28.3620
[11/25 07:01:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.72	
[11/25 07:01:29 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[11/25 07:03:12 visual_prompt]: 	Training 100/553. train loss: 2.7499,	0.8485 s / batch. (data: 5.93e-03). ETA=6:21:48, max mem: 20.9 GB 
[11/25 07:04:48 visual_prompt]: 	Training 200/553. train loss: 58.9792,	0.8320 s / batch. (data: 3.19e-04). ETA=6:12:58, max mem: 20.9 GB 
[11/25 07:06:26 visual_prompt]: 	Training 300/553. train loss: 136.8985,	0.8241 s / batch. (data: 3.09e-04). ETA=6:08:03, max mem: 20.9 GB 
[11/25 07:08:05 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8200 s / batch. (data: 3.00e-04). ETA=6:04:51, max mem: 20.9 GB 
[11/25 07:09:38 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8376 s / batch. (data: 9.55e-03). ETA=6:11:17, max mem: 20.9 GB 
[11/25 07:10:28 visual_prompt]: Epoch 52 / 100: avg data time: 1.44e-01, avg batch time: 0.9743, average train loss: 42.6473
[11/25 07:11:23 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3108, average loss: 151.0794
[11/25 07:11:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.23	
[11/25 07:11:23 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[11/25 07:13:02 visual_prompt]: 	Training 100/553. train loss: 120.2621,	0.8251 s / batch. (data: 2.99e-04). ETA=6:03:40, max mem: 20.9 GB 
[11/25 07:14:38 visual_prompt]: 	Training 200/553. train loss: 60.7727,	0.8493 s / batch. (data: 2.13e-02). ETA=6:12:55, max mem: 20.9 GB 
[11/25 07:16:14 visual_prompt]: 	Training 300/553. train loss: 6.7630,	0.8485 s / batch. (data: 7.12e-04). ETA=6:11:08, max mem: 20.9 GB 
[11/25 07:17:51 visual_prompt]: 	Training 400/553. train loss: 51.5285,	0.8480 s / batch. (data: 7.95e-03). ETA=6:09:30, max mem: 20.9 GB 
[11/25 07:19:26 visual_prompt]: 	Training 500/553. train loss: 53.9246,	0.8221 s / batch. (data: 1.10e-02). ETA=5:56:51, max mem: 20.9 GB 
[11/25 07:20:17 visual_prompt]: Epoch 53 / 100: avg data time: 1.35e-01, avg batch time: 0.9657, average train loss: 38.3206
[11/25 07:21:12 visual_prompt]: Inference (val):avg data time: 2.04e-04, avg batch time: 0.3108, average loss: 9.8799
[11/25 07:21:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.63	
[11/25 07:21:12 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[11/25 07:22:55 visual_prompt]: 	Training 100/553. train loss: 23.6152,	0.8305 s / batch. (data: 3.14e-04). ETA=5:58:22, max mem: 20.9 GB 
[11/25 07:24:31 visual_prompt]: 	Training 200/553. train loss: 54.6479,	0.8320 s / batch. (data: 2.98e-04). ETA=5:57:38, max mem: 20.9 GB 
[11/25 07:26:06 visual_prompt]: 	Training 300/553. train loss: 4.9590,	0.8335 s / batch. (data: 3.23e-04). ETA=5:56:52, max mem: 20.9 GB 
[11/25 07:27:41 visual_prompt]: 	Training 400/553. train loss: 145.9541,	0.8380 s / batch. (data: 2.25e-02). ETA=5:57:26, max mem: 20.9 GB 
[11/25 07:29:17 visual_prompt]: 	Training 500/553. train loss: 40.1548,	0.8280 s / batch. (data: 3.30e-04). ETA=5:51:46, max mem: 20.9 GB 
[11/25 07:30:07 visual_prompt]: Epoch 54 / 100: avg data time: 1.39e-01, avg batch time: 0.9684, average train loss: 39.6460
[11/25 07:31:02 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3105, average loss: 24.6680
[11/25 07:31:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.68	
[11/25 07:31:02 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[11/25 07:32:41 visual_prompt]: 	Training 100/553. train loss: 29.5495,	0.8307 s / batch. (data: 3.25e-04). ETA=5:50:47, max mem: 20.9 GB 
[11/25 07:34:16 visual_prompt]: 	Training 200/553. train loss: 8.5054,	0.8200 s / batch. (data: 3.08e-04). ETA=5:44:55, max mem: 20.9 GB 
[11/25 07:35:52 visual_prompt]: 	Training 300/553. train loss: 55.7648,	0.8232 s / batch. (data: 2.85e-04). ETA=5:44:53, max mem: 20.9 GB 
[11/25 07:37:27 visual_prompt]: 	Training 400/553. train loss: 34.0442,	1.0999 s / batch. (data: 2.61e-01). ETA=7:39:00, max mem: 20.9 GB 
[11/25 07:39:02 visual_prompt]: 	Training 500/553. train loss: 2.2429,	0.8381 s / batch. (data: 1.19e-02). ETA=5:48:19, max mem: 20.9 GB 
[11/25 07:39:53 visual_prompt]: Epoch 55 / 100: avg data time: 1.31e-01, avg batch time: 0.9614, average train loss: 37.3406
[11/25 07:40:48 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3096, average loss: 72.3726
[11/25 07:40:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.51	
[11/25 07:40:48 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[11/25 07:42:29 visual_prompt]: 	Training 100/553. train loss: 30.2240,	0.8324 s / batch. (data: 7.97e-03). ETA=5:43:52, max mem: 20.9 GB 
[11/25 07:44:04 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8360 s / batch. (data: 3.10e-04). ETA=5:43:56, max mem: 20.9 GB 
[11/25 07:45:41 visual_prompt]: 	Training 300/553. train loss: 14.4014,	0.8464 s / batch. (data: 3.01e-04). ETA=5:46:49, max mem: 20.9 GB 
[11/25 07:47:18 visual_prompt]: 	Training 400/553. train loss: 55.4842,	0.8513 s / batch. (data: 1.10e-02). ETA=5:47:25, max mem: 20.9 GB 
[11/25 07:48:53 visual_prompt]: 	Training 500/553. train loss: 5.5186,	1.9546 s / batch. (data: 1.13e+00). ETA=13:14:22, max mem: 20.9 GB 
[11/25 07:49:42 visual_prompt]: Epoch 56 / 100: avg data time: 1.36e-01, avg batch time: 0.9652, average train loss: 43.4949
[11/25 07:50:37 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3096, average loss: 36.1415
[11/25 07:50:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.49	
[11/25 07:50:37 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[11/25 07:52:20 visual_prompt]: 	Training 100/553. train loss: 11.7745,	0.8309 s / batch. (data: 7.50e-04). ETA=5:35:33, max mem: 20.9 GB 
[11/25 07:53:55 visual_prompt]: 	Training 200/553. train loss: 20.4587,	0.8366 s / batch. (data: 1.07e-02). ETA=5:36:27, max mem: 20.9 GB 
[11/25 07:55:29 visual_prompt]: 	Training 300/553. train loss: 52.7563,	0.8485 s / batch. (data: 3.86e-04). ETA=5:39:50, max mem: 20.9 GB 
[11/25 07:57:04 visual_prompt]: 	Training 400/553. train loss: 107.6301,	0.8450 s / batch. (data: 2.99e-04). ETA=5:37:01, max mem: 20.9 GB 
[11/25 07:58:37 visual_prompt]: 	Training 500/553. train loss: 26.3934,	0.8389 s / batch. (data: 3.24e-04). ETA=5:33:11, max mem: 20.9 GB 
[11/25 07:59:28 visual_prompt]: Epoch 57 / 100: avg data time: 1.30e-01, avg batch time: 0.9608, average train loss: 33.1353
[11/25 08:00:22 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3111, average loss: 52.2059
[11/25 08:00:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.20	
[11/25 08:00:22 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[11/25 08:02:02 visual_prompt]: 	Training 100/553. train loss: 8.4880,	1.0822 s / batch. (data: 2.59e-01). ETA=7:07:05, max mem: 20.9 GB 
[11/25 08:03:38 visual_prompt]: 	Training 200/553. train loss: 121.7783,	0.8227 s / batch. (data: 2.90e-04). ETA=5:23:18, max mem: 20.9 GB 
[11/25 08:05:17 visual_prompt]: 	Training 300/553. train loss: 33.8112,	0.8566 s / batch. (data: 1.59e-02). ETA=5:35:11, max mem: 20.9 GB 
[11/25 08:06:52 visual_prompt]: 	Training 400/553. train loss: 18.9799,	1.3534 s / batch. (data: 5.34e-01). ETA=8:47:20, max mem: 20.9 GB 
[11/25 08:08:26 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8160 s / batch. (data: 2.86e-04). ETA=5:16:35, max mem: 20.9 GB 
[11/25 08:09:16 visual_prompt]: Epoch 58 / 100: avg data time: 1.34e-01, avg batch time: 0.9649, average train loss: 31.1715
[11/25 08:10:11 visual_prompt]: Inference (val):avg data time: 4.04e-04, avg batch time: 0.3097, average loss: 139.2448
[11/25 08:10:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.89	
[11/25 08:10:11 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[11/25 08:11:52 visual_prompt]: 	Training 100/553. train loss: 32.4072,	0.8381 s / batch. (data: 1.05e-02). ETA=5:23:02, max mem: 20.9 GB 
[11/25 08:13:29 visual_prompt]: 	Training 200/553. train loss: 0.0502,	0.8319 s / batch. (data: 5.42e-03). ETA=5:19:16, max mem: 20.9 GB 
[11/25 08:15:04 visual_prompt]: 	Training 300/553. train loss: 124.5973,	0.8386 s / batch. (data: 3.02e-04). ETA=5:20:26, max mem: 20.9 GB 
[11/25 08:16:39 visual_prompt]: 	Training 400/553. train loss: 20.7312,	0.8277 s / batch. (data: 3.37e-04). ETA=5:14:53, max mem: 20.9 GB 
[11/25 08:18:18 visual_prompt]: 	Training 500/553. train loss: 31.7928,	0.8331 s / batch. (data: 8.12e-04). ETA=5:15:32, max mem: 20.9 GB 
[11/25 08:19:06 visual_prompt]: Epoch 59 / 100: avg data time: 1.37e-01, avg batch time: 0.9676, average train loss: 33.6481
[11/25 08:20:01 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3097, average loss: 35.2008
[11/25 08:20:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.83	
[11/25 08:20:01 visual_prompt]: Stopping early.
[11/25 08:20:01 visual_prompt]: Rank of current process: 0. World size: 1
[11/25 08:20:01 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/25 08:20:01 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/25 08:20:01 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/25 08:20:01 visual_prompt]: Training with config:
[11/25 08:20:01 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/25 08:20:01 visual_prompt]: Loading training data...
[11/25 08:20:01 visual_prompt]: Constructing mammo-cbis dataset train...
[11/25 08:20:01 visual_prompt]: Loading validation data...
[11/25 08:20:01 visual_prompt]: Constructing mammo-cbis dataset val...
[11/25 08:20:01 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/25 08:20:04 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/25 08:20:04 visual_prompt]: tuned percent:0.525
[11/25 08:20:04 visual_prompt]: Device used for model: 0
[11/25 08:20:04 visual_prompt]: Setting up Evaluator...
[11/25 08:20:04 visual_prompt]: Setting up Trainer...
[11/25 08:20:04 visual_prompt]: 	Setting up the optimizer...
[11/25 08:20:04 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/25 08:21:44 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8172 s / batch. (data: 2.84e-04). ETA=12:31:47, max mem: 20.9 GB 
[11/25 08:23:18 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8370 s / batch. (data: 2.85e-04). ETA=12:48:37, max mem: 20.9 GB 
[11/25 08:24:57 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8529 s / batch. (data: 5.46e-03). ETA=13:01:47, max mem: 20.9 GB 
[11/25 08:26:31 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8178 s / batch. (data: 2.92e-04). ETA=12:28:14, max mem: 20.9 GB 
[11/25 08:28:08 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8265 s / batch. (data: 7.98e-03). ETA=12:34:54, max mem: 20.9 GB 
[11/25 08:28:59 visual_prompt]: Epoch 1 / 100: avg data time: 1.34e-01, avg batch time: 0.9674, average train loss: 1.5403
[11/25 08:29:53 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3093, average loss: 1.5201
[11/25 08:29:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/25 08:29:53 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/25 08:31:32 visual_prompt]: 	Training 100/553. train loss: 7.2424,	0.8342 s / batch. (data: 4.40e-04). ETA=12:39:44, max mem: 20.9 GB 
[11/25 08:33:07 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.1719 s / batch. (data: 3.32e-01). ETA=17:45:22, max mem: 20.9 GB 
[11/25 08:34:47 visual_prompt]: 	Training 300/553. train loss: 4.3327,	1.2893 s / batch. (data: 4.60e-01). ETA=19:30:00, max mem: 20.9 GB 
[11/25 08:36:47 visual_prompt]: 	Training 400/553. train loss: 1.6245,	0.8534 s / batch. (data: 3.43e-04). ETA=12:52:57, max mem: 20.9 GB 
[11/25 08:38:31 visual_prompt]: 	Training 500/553. train loss: 1.6753,	0.8265 s / batch. (data: 3.21e-04). ETA=12:27:13, max mem: 20.9 GB 
[11/25 08:39:20 visual_prompt]: Epoch 2 / 100: avg data time: 1.94e-01, avg batch time: 1.0260, average train loss: 12.4687
[11/25 08:40:16 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3098, average loss: 17.9150
[11/25 08:40:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.10	
[11/25 08:40:16 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/25 08:41:56 visual_prompt]: 	Training 100/553. train loss: 28.1278,	0.8388 s / batch. (data: 2.99e-04). ETA=12:36:12, max mem: 20.9 GB 
[11/25 08:43:33 visual_prompt]: 	Training 200/553. train loss: 10.4513,	0.8280 s / batch. (data: 3.15e-04). ETA=12:25:07, max mem: 20.9 GB 
[11/25 08:45:08 visual_prompt]: 	Training 300/553. train loss: 2.1308,	0.8241 s / batch. (data: 2.87e-04). ETA=12:20:11, max mem: 20.9 GB 
[11/25 08:46:46 visual_prompt]: 	Training 400/553. train loss: 64.6666,	0.9209 s / batch. (data: 8.89e-02). ETA=13:45:40, max mem: 20.9 GB 
[11/25 08:48:23 visual_prompt]: 	Training 500/553. train loss: 6.3802,	1.0603 s / batch. (data: 2.25e-01). ETA=15:48:52, max mem: 20.9 GB 
[11/25 08:49:13 visual_prompt]: Epoch 3 / 100: avg data time: 1.38e-01, avg batch time: 0.9710, average train loss: 13.6450
[11/25 08:50:07 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3102, average loss: 12.2957
[11/25 08:50:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.88	
[11/25 08:50:07 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/25 08:51:48 visual_prompt]: 	Training 100/553. train loss: 14.1667,	0.8541 s / batch. (data: 1.40e-02). ETA=12:42:07, max mem: 20.9 GB 
[11/25 08:53:24 visual_prompt]: 	Training 200/553. train loss: 13.1459,	0.8363 s / batch. (data: 3.20e-04). ETA=12:24:50, max mem: 20.9 GB 
[11/25 08:55:00 visual_prompt]: 	Training 300/553. train loss: 5.1815,	1.3978 s / batch. (data: 5.65e-01). ETA=20:42:38, max mem: 20.9 GB 
[11/25 08:56:32 visual_prompt]: 	Training 400/553. train loss: 0.4137,	1.2166 s / batch. (data: 3.82e-01). ETA=17:59:30, max mem: 20.9 GB 
[11/25 08:58:10 visual_prompt]: 	Training 500/553. train loss: 0.0000,	3.3040 s / batch. (data: 2.47e+00). ETA=2 days, 0:46:17, max mem: 20.9 GB 
[11/25 08:59:01 visual_prompt]: Epoch 4 / 100: avg data time: 1.32e-01, avg batch time: 0.9658, average train loss: 18.2450
[11/25 08:59:56 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3109, average loss: 5.3036
[11/25 08:59:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[11/25 08:59:56 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/25 09:01:34 visual_prompt]: 	Training 100/553. train loss: 83.4474,	0.8320 s / batch. (data: 7.96e-03). ETA=12:14:43, max mem: 20.9 GB 
[11/25 09:03:11 visual_prompt]: 	Training 200/553. train loss: 0.9112,	1.1480 s / batch. (data: 3.29e-01). ETA=16:51:54, max mem: 20.9 GB 
[11/25 09:04:48 visual_prompt]: 	Training 300/553. train loss: 73.5406,	0.8400 s / batch. (data: 1.61e-02). ETA=12:19:02, max mem: 20.9 GB 
[11/25 09:06:23 visual_prompt]: 	Training 400/553. train loss: 2.6459,	0.8328 s / batch. (data: 5.43e-03). ETA=12:11:17, max mem: 20.9 GB 
[11/25 09:07:59 visual_prompt]: 	Training 500/553. train loss: 22.0819,	0.8332 s / batch. (data: 1.20e-02). ETA=12:10:16, max mem: 20.9 GB 
[11/25 09:08:50 visual_prompt]: Epoch 5 / 100: avg data time: 1.34e-01, avg batch time: 0.9659, average train loss: 25.7894
[11/25 09:09:45 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3098, average loss: 3.6946
[11/25 09:09:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.25	
[11/25 09:09:45 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/25 09:11:25 visual_prompt]: 	Training 100/553. train loss: 24.2818,	0.8180 s / batch. (data: 1.04e-02). ETA=11:54:53, max mem: 20.9 GB 
[11/25 09:13:01 visual_prompt]: 	Training 200/553. train loss: 99.6738,	0.8603 s / batch. (data: 2.42e-02). ETA=12:30:22, max mem: 20.9 GB 
[11/25 09:14:35 visual_prompt]: 	Training 300/553. train loss: 8.5172,	0.8325 s / batch. (data: 3.23e-04). ETA=12:04:45, max mem: 20.9 GB 
[11/25 09:16:15 visual_prompt]: 	Training 400/553. train loss: 18.0969,	0.8427 s / batch. (data: 1.08e-02). ETA=12:12:12, max mem: 20.9 GB 
[11/25 09:17:49 visual_prompt]: 	Training 500/553. train loss: 11.3561,	0.8363 s / batch. (data: 5.48e-03). ETA=12:05:14, max mem: 20.9 GB 
[11/25 09:18:39 visual_prompt]: Epoch 6 / 100: avg data time: 1.33e-01, avg batch time: 0.9655, average train loss: 23.5509
[11/25 09:19:33 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3090, average loss: 6.1940
[11/25 09:19:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.42	
[11/25 09:19:33 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/25 09:21:11 visual_prompt]: 	Training 100/553. train loss: 71.1854,	0.8291 s / batch. (data: 3.13e-04). ETA=11:56:54, max mem: 20.9 GB 
[11/25 09:22:48 visual_prompt]: 	Training 200/553. train loss: 5.1444,	0.8320 s / batch. (data: 2.99e-04). ETA=11:58:02, max mem: 20.9 GB 
[11/25 09:24:26 visual_prompt]: 	Training 300/553. train loss: 18.4046,	1.0480 s / batch. (data: 2.01e-01). ETA=15:02:42, max mem: 20.9 GB 
[11/25 09:26:03 visual_prompt]: 	Training 400/553. train loss: 13.8397,	1.8186 s / batch. (data: 9.93e-01). ETA=1 day, 2:03:26, max mem: 20.9 GB 
[11/25 09:27:38 visual_prompt]: 	Training 500/553. train loss: 68.1425,	0.8400 s / batch. (data: 3.23e-04). ETA=12:00:44, max mem: 20.9 GB 
[11/25 09:28:27 visual_prompt]: Epoch 7 / 100: avg data time: 1.32e-01, avg batch time: 0.9642, average train loss: 24.9745
[11/25 09:29:21 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3086, average loss: 6.3741
[11/25 09:29:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.37	
[11/25 09:29:21 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/25 09:31:00 visual_prompt]: 	Training 100/553. train loss: 69.9379,	0.8120 s / batch. (data: 4.67e-04). ETA=11:34:37, max mem: 20.9 GB 
[11/25 09:32:37 visual_prompt]: 	Training 200/553. train loss: 120.0211,	0.8272 s / batch. (data: 5.42e-03). ETA=11:46:14, max mem: 20.9 GB 
[11/25 09:34:13 visual_prompt]: 	Training 300/553. train loss: 14.7993,	0.8490 s / batch. (data: 1.05e-02). ETA=12:03:29, max mem: 20.9 GB 
[11/25 09:35:48 visual_prompt]: 	Training 400/553. train loss: 60.3261,	0.8539 s / batch. (data: 5.42e-03). ETA=12:06:15, max mem: 20.9 GB 
[11/25 09:37:25 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.2520 s / batch. (data: 4.11e-01). ETA=17:42:43, max mem: 20.9 GB 
[11/25 09:38:15 visual_prompt]: Epoch 8 / 100: avg data time: 1.35e-01, avg batch time: 0.9653, average train loss: 35.4590
[11/25 09:39:10 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3089, average loss: 29.6084
[11/25 09:39:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.89	
[11/25 09:39:10 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/25 09:40:50 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8162 s / batch. (data: 5.43e-03). ETA=11:30:45, max mem: 20.9 GB 
[11/25 09:42:25 visual_prompt]: 	Training 200/553. train loss: 17.5484,	0.8406 s / batch. (data: 1.60e-02). ETA=11:49:57, max mem: 20.9 GB 
[11/25 09:44:02 visual_prompt]: 	Training 300/553. train loss: 11.5687,	1.4747 s / batch. (data: 6.42e-01). ETA=20:43:06, max mem: 20.9 GB 
[11/25 09:45:38 visual_prompt]: 	Training 400/553. train loss: 20.6134,	0.8507 s / batch. (data: 1.47e-02). ETA=11:55:39, max mem: 20.9 GB 
[11/25 09:47:15 visual_prompt]: 	Training 500/553. train loss: 41.5786,	0.8611 s / batch. (data: 2.11e-02). ETA=12:02:58, max mem: 20.9 GB 
[11/25 09:48:05 visual_prompt]: Epoch 9 / 100: avg data time: 1.34e-01, avg batch time: 0.9666, average train loss: 28.5791
[11/25 09:48:59 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3117, average loss: 23.8281
[11/25 09:49:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.98	
[11/25 09:49:00 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/25 09:50:42 visual_prompt]: 	Training 100/553. train loss: 82.2706,	0.8310 s / batch. (data: 1.78e-02). ETA=11:35:37, max mem: 20.9 GB 
[11/25 09:52:16 visual_prompt]: 	Training 200/553. train loss: 1.1150,	0.8320 s / batch. (data: 2.96e-04). ETA=11:35:00, max mem: 20.9 GB 
[11/25 09:53:52 visual_prompt]: 	Training 300/553. train loss: 30.2138,	0.8625 s / batch. (data: 1.19e-02). ETA=11:59:04, max mem: 20.9 GB 
[11/25 09:55:25 visual_prompt]: 	Training 400/553. train loss: 40.2294,	0.8273 s / batch. (data: 3.09e-04). ETA=11:28:23, max mem: 20.9 GB 
[11/25 09:57:03 visual_prompt]: 	Training 500/553. train loss: 7.8690,	0.8440 s / batch. (data: 2.52e-04). ETA=11:40:50, max mem: 20.9 GB 
[11/25 09:57:53 visual_prompt]: Epoch 10 / 100: avg data time: 1.34e-01, avg batch time: 0.9638, average train loss: 33.4559
[11/25 09:58:47 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3094, average loss: 1.7496
[11/25 09:58:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.90	
[11/25 09:58:47 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/25 10:00:29 visual_prompt]: 	Training 100/553. train loss: 52.0340,	0.8480 s / batch. (data: 4.40e-04). ETA=11:41:58, max mem: 20.9 GB 
[11/25 10:02:06 visual_prompt]: 	Training 200/553. train loss: 52.9471,	0.8197 s / batch. (data: 2.92e-04). ETA=11:17:11, max mem: 20.9 GB 
[11/25 10:03:41 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.9960 s / batch. (data: 1.15e+00). ETA=1 day, 3:25:43, max mem: 20.9 GB 
[11/25 10:05:16 visual_prompt]: 	Training 400/553. train loss: 18.9793,	0.8292 s / batch. (data: 5.37e-03). ETA=11:22:17, max mem: 20.9 GB 
[11/25 10:06:51 visual_prompt]: 	Training 500/553. train loss: 18.9895,	0.8440 s / batch. (data: 1.19e-02). ETA=11:33:03, max mem: 20.9 GB 
[11/25 10:07:41 visual_prompt]: Epoch 11 / 100: avg data time: 1.35e-01, avg batch time: 0.9655, average train loss: 33.8645
[11/25 10:08:35 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3095, average loss: 56.5795
[11/25 10:08:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.93	
[11/25 10:08:35 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/25 10:10:16 visual_prompt]: 	Training 100/553. train loss: 4.4777,	0.8440 s / batch. (data: 7.94e-03). ETA=11:30:54, max mem: 20.9 GB 
[11/25 10:11:53 visual_prompt]: 	Training 200/553. train loss: 12.2439,	0.8480 s / batch. (data: 7.36e-04). ETA=11:32:46, max mem: 20.9 GB 
[11/25 10:13:28 visual_prompt]: 	Training 300/553. train loss: 6.2091,	0.8240 s / batch. (data: 2.90e-04). ETA=11:11:47, max mem: 20.9 GB 
[11/25 10:15:04 visual_prompt]: 	Training 400/553. train loss: 36.6586,	0.8440 s / batch. (data: 3.30e-04). ETA=11:26:40, max mem: 20.9 GB 
[11/25 10:16:41 visual_prompt]: 	Training 500/553. train loss: 194.9420,	0.8360 s / batch. (data: 2.98e-04). ETA=11:18:48, max mem: 20.9 GB 
[11/25 10:17:30 visual_prompt]: Epoch 12 / 100: avg data time: 1.37e-01, avg batch time: 0.9669, average train loss: 38.2417
[11/25 10:18:25 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3102, average loss: 78.2413
[11/25 10:18:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.06	
[11/25 10:18:25 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/25 10:20:06 visual_prompt]: 	Training 100/553. train loss: 25.1437,	0.8174 s / batch. (data: 1.19e-02). ETA=11:01:37, max mem: 20.9 GB 
[11/25 10:21:40 visual_prompt]: 	Training 200/553. train loss: 12.9302,	0.8477 s / batch. (data: 1.16e-02). ETA=11:24:42, max mem: 20.9 GB 
[11/25 10:23:16 visual_prompt]: 	Training 300/553. train loss: 32.4434,	1.4171 s / batch. (data: 6.10e-01). ETA=19:02:15, max mem: 20.9 GB 
[11/25 10:24:51 visual_prompt]: 	Training 400/553. train loss: 112.1674,	0.8395 s / batch. (data: 1.05e-02). ETA=11:15:18, max mem: 20.9 GB 
[11/25 10:26:28 visual_prompt]: 	Training 500/553. train loss: 73.3396,	0.8195 s / batch. (data: 2.95e-04). ETA=10:57:51, max mem: 20.9 GB 
[11/25 10:27:18 visual_prompt]: Epoch 13 / 100: avg data time: 1.34e-01, avg batch time: 0.9636, average train loss: 42.1476
[11/25 10:28:12 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3102, average loss: 14.5400
[11/25 10:28:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 61.97	
[11/25 10:28:12 visual_prompt]: Best epoch 13: best metric: -14.540
[11/25 10:28:12 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/25 10:29:53 visual_prompt]: 	Training 100/553. train loss: 32.7932,	0.8440 s / batch. (data: 2.89e-04). ETA=11:15:19, max mem: 20.9 GB 
[11/25 10:31:28 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8405 s / batch. (data: 1.05e-02). ETA=11:11:10, max mem: 20.9 GB 
[11/25 10:33:03 visual_prompt]: 	Training 300/553. train loss: 16.0893,	0.8374 s / batch. (data: 3.22e-04). ETA=11:07:17, max mem: 20.9 GB 
[11/25 10:34:38 visual_prompt]: 	Training 400/553. train loss: 9.4391,	0.8440 s / batch. (data: 2.69e-04). ETA=11:11:06, max mem: 20.9 GB 
[11/25 10:36:13 visual_prompt]: 	Training 500/553. train loss: 11.7443,	0.8572 s / batch. (data: 1.55e-02). ETA=11:20:14, max mem: 20.9 GB 
[11/25 10:37:03 visual_prompt]: Epoch 14 / 100: avg data time: 1.26e-01, avg batch time: 0.9592, average train loss: 26.6456
[11/25 10:37:57 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3094, average loss: 7.4371
[11/25 10:37:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.78	
[11/25 10:37:57 visual_prompt]: Best epoch 14: best metric: -7.437
[11/25 10:37:57 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/25 10:39:37 visual_prompt]: 	Training 100/553. train loss: 49.4517,	0.8454 s / batch. (data: 7.66e-03). ETA=11:08:40, max mem: 20.9 GB 
[11/25 10:41:10 visual_prompt]: 	Training 200/553. train loss: 73.2785,	0.8427 s / batch. (data: 6.64e-03). ETA=11:05:08, max mem: 20.9 GB 
[11/25 10:42:47 visual_prompt]: 	Training 300/553. train loss: 45.8299,	0.8279 s / batch. (data: 3.34e-04). ETA=10:52:06, max mem: 20.9 GB 
[11/25 10:44:20 visual_prompt]: 	Training 400/553. train loss: 3.9994,	0.9200 s / batch. (data: 5.37e-02). ETA=12:03:05, max mem: 20.9 GB 
[11/25 10:45:58 visual_prompt]: 	Training 500/553. train loss: 12.0135,	0.8312 s / batch. (data: 5.58e-03). ETA=10:51:54, max mem: 20.9 GB 
[11/25 10:46:48 visual_prompt]: Epoch 15 / 100: avg data time: 1.30e-01, avg batch time: 0.9605, average train loss: 42.6542
[11/25 10:47:43 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3094, average loss: 119.1599
[11/25 10:47:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.93	
[11/25 10:47:43 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/25 10:49:22 visual_prompt]: 	Training 100/553. train loss: 28.5121,	0.8350 s / batch. (data: 2.91e-04). ETA=10:52:47, max mem: 20.9 GB 
[11/25 10:50:57 visual_prompt]: 	Training 200/553. train loss: 4.4193,	0.8179 s / batch. (data: 3.02e-04). ETA=10:37:59, max mem: 20.9 GB 
[11/25 10:52:33 visual_prompt]: 	Training 300/553. train loss: 18.1549,	0.8558 s / batch. (data: 6.50e-04). ETA=11:06:08, max mem: 20.9 GB 
[11/25 10:54:08 visual_prompt]: 	Training 400/553. train loss: 8.0040,	0.8319 s / batch. (data: 1.56e-02). ETA=10:46:08, max mem: 20.9 GB 
[11/25 10:55:42 visual_prompt]: 	Training 500/553. train loss: 2.9585,	1.2879 s / batch. (data: 4.60e-01). ETA=16:38:14, max mem: 20.9 GB 
[11/25 10:56:33 visual_prompt]: Epoch 16 / 100: avg data time: 1.28e-01, avg batch time: 0.9588, average train loss: 32.4045
[11/25 10:57:27 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3116, average loss: 11.1458
[11/25 10:57:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.12	
[11/25 10:57:27 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/25 10:59:06 visual_prompt]: 	Training 100/553. train loss: 0.0228,	0.8432 s / batch. (data: 1.05e-02). ETA=10:51:22, max mem: 20.9 GB 
[11/25 11:00:43 visual_prompt]: 	Training 200/553. train loss: 133.8049,	0.8357 s / batch. (data: 5.34e-03). ETA=10:44:10, max mem: 20.9 GB 
[11/25 11:02:18 visual_prompt]: 	Training 300/553. train loss: 80.5348,	0.8480 s / batch. (data: 2.58e-04). ETA=10:52:19, max mem: 20.9 GB 
[11/25 11:03:54 visual_prompt]: 	Training 400/553. train loss: 0.2826,	1.0920 s / batch. (data: 2.54e-01). ETA=13:58:09, max mem: 20.9 GB 
[11/25 11:05:30 visual_prompt]: 	Training 500/553. train loss: 58.0921,	1.4932 s / batch. (data: 6.83e-01). ETA=19:03:33, max mem: 20.9 GB 
[11/25 11:06:21 visual_prompt]: Epoch 17 / 100: avg data time: 1.34e-01, avg batch time: 0.9651, average train loss: 34.7385
[11/25 11:07:16 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3113, average loss: 13.7672
[11/25 11:07:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 65.09	
[11/25 11:07:16 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/25 11:08:55 visual_prompt]: 	Training 100/553. train loss: 1.1119,	0.8281 s / batch. (data: 3.03e-04). ETA=10:32:05, max mem: 20.9 GB 
[11/25 11:10:34 visual_prompt]: 	Training 200/553. train loss: 7.7370,	0.8215 s / batch. (data: 2.47e-04). ETA=10:25:43, max mem: 20.9 GB 
[11/25 11:12:10 visual_prompt]: 	Training 300/553. train loss: 15.1899,	0.8200 s / batch. (data: 3.06e-04). ETA=10:23:09, max mem: 20.9 GB 
[11/25 11:13:46 visual_prompt]: 	Training 400/553. train loss: 19.8836,	0.8360 s / batch. (data: 4.75e-04). ETA=10:33:55, max mem: 20.9 GB 
[11/25 11:15:21 visual_prompt]: 	Training 500/553. train loss: 7.3998,	0.8402 s / batch. (data: 3.43e-04). ETA=10:35:42, max mem: 20.9 GB 
[11/25 11:16:11 visual_prompt]: Epoch 18 / 100: avg data time: 1.36e-01, avg batch time: 0.9665, average train loss: 38.3788
[11/25 11:17:05 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3094, average loss: 55.3526
[11/25 11:17:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.85	
[11/25 11:17:05 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/25 11:18:45 visual_prompt]: 	Training 100/553. train loss: 26.4222,	1.0070 s / batch. (data: 1.89e-01). ETA=12:39:21, max mem: 20.9 GB 
[11/25 11:20:21 visual_prompt]: 	Training 200/553. train loss: 3.7860,	0.8320 s / batch. (data: 5.41e-03). ETA=10:26:01, max mem: 20.9 GB 
[11/25 11:21:58 visual_prompt]: 	Training 300/553. train loss: 22.6057,	0.8213 s / batch. (data: 3.17e-04). ETA=10:16:36, max mem: 20.9 GB 
[11/25 11:23:35 visual_prompt]: 	Training 400/553. train loss: 12.2948,	0.8320 s / batch. (data: 7.65e-04). ETA=10:23:15, max mem: 20.9 GB 
[11/25 11:25:07 visual_prompt]: 	Training 500/553. train loss: 7.6475,	0.8503 s / batch. (data: 1.03e-02). ETA=10:35:32, max mem: 20.9 GB 
[11/25 11:25:57 visual_prompt]: Epoch 19 / 100: avg data time: 1.30e-01, avg batch time: 0.9619, average train loss: 27.2900
[11/25 11:26:52 visual_prompt]: Inference (val):avg data time: 3.21e-04, avg batch time: 0.3105, average loss: 93.1748
[11/25 11:26:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.80	
[11/25 11:26:52 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/25 11:28:30 visual_prompt]: 	Training 100/553. train loss: 39.9212,	0.8435 s / batch. (data: 2.29e-02). ETA=10:28:20, max mem: 20.9 GB 
[11/25 11:30:07 visual_prompt]: 	Training 200/553. train loss: 2.5359,	0.9015 s / batch. (data: 6.98e-02). ETA=11:10:02, max mem: 20.9 GB 
[11/25 11:31:43 visual_prompt]: 	Training 300/553. train loss: 85.2379,	0.8214 s / batch. (data: 2.98e-04). ETA=10:09:08, max mem: 20.9 GB 
[11/25 11:33:20 visual_prompt]: 	Training 400/553. train loss: 90.9195,	0.8299 s / batch. (data: 1.20e-02). ETA=10:14:00, max mem: 20.9 GB 
[11/25 11:34:55 visual_prompt]: 	Training 500/553. train loss: 29.1014,	0.8398 s / batch. (data: 1.05e-02). ETA=10:19:58, max mem: 20.9 GB 
[11/25 11:35:46 visual_prompt]: Epoch 20 / 100: avg data time: 1.35e-01, avg batch time: 0.9662, average train loss: 36.6560
[11/25 11:36:41 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3093, average loss: 14.7902
[11/25 11:36:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.40	
[11/25 11:36:41 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/25 11:38:22 visual_prompt]: 	Training 100/553. train loss: 19.7591,	0.8478 s / batch. (data: 2.46e-02). ETA=10:23:41, max mem: 20.9 GB 
[11/25 11:39:57 visual_prompt]: 	Training 200/553. train loss: 0.0096,	0.8175 s / batch. (data: 2.92e-04). ETA=10:00:01, max mem: 20.9 GB 
[11/25 11:41:32 visual_prompt]: 	Training 300/553. train loss: 136.8840,	1.0160 s / batch. (data: 1.67e-01). ETA=12:24:02, max mem: 20.9 GB 
[11/25 11:43:06 visual_prompt]: 	Training 400/553. train loss: 25.3831,	0.8394 s / batch. (data: 2.91e-04). ETA=10:13:20, max mem: 20.9 GB 
[11/25 11:44:44 visual_prompt]: 	Training 500/553. train loss: 31.8285,	0.8835 s / batch. (data: 1.55e-02). ETA=10:44:05, max mem: 20.9 GB 
[11/25 11:45:33 visual_prompt]: Epoch 21 / 100: avg data time: 1.30e-01, avg batch time: 0.9613, average train loss: 28.5649
[11/25 11:46:27 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3107, average loss: 14.4644
[11/25 11:46:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 67.94	
[11/25 11:46:27 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/25 11:48:05 visual_prompt]: 	Training 100/553. train loss: 26.2604,	0.8240 s / batch. (data: 2.94e-04). ETA=9:58:36, max mem: 20.9 GB 
[11/25 11:49:40 visual_prompt]: 	Training 200/553. train loss: 38.4190,	0.8320 s / batch. (data: 3.37e-04). ETA=10:02:59, max mem: 20.9 GB 
[11/25 11:51:13 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8397 s / batch. (data: 3.79e-04). ETA=10:07:10, max mem: 20.9 GB 
[11/25 11:52:49 visual_prompt]: 	Training 400/553. train loss: 8.0782,	0.8324 s / batch. (data: 2.91e-04). ETA=10:00:30, max mem: 20.9 GB 
[11/25 11:54:24 visual_prompt]: 	Training 500/553. train loss: 24.0354,	0.8240 s / batch. (data: 3.18e-04). ETA=9:53:06, max mem: 20.9 GB 
[11/25 11:55:15 visual_prompt]: Epoch 22 / 100: avg data time: 1.24e-01, avg batch time: 0.9550, average train loss: 31.6379
[11/25 11:56:09 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3125, average loss: 22.8661
[11/25 11:56:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.05	
[11/25 11:56:09 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/25 11:57:49 visual_prompt]: 	Training 100/553. train loss: 69.6765,	0.8160 s / batch. (data: 2.83e-04). ETA=9:45:15, max mem: 20.9 GB 
[11/25 11:59:24 visual_prompt]: 	Training 200/553. train loss: 26.6767,	0.8280 s / batch. (data: 2.96e-04). ETA=9:52:30, max mem: 20.9 GB 
[11/25 12:01:00 visual_prompt]: 	Training 300/553. train loss: 4.3607,	0.8600 s / batch. (data: 1.60e-02). ETA=10:13:56, max mem: 20.9 GB 
[11/25 12:02:33 visual_prompt]: 	Training 400/553. train loss: 16.4394,	0.8196 s / batch. (data: 1.06e-02). ETA=9:43:44, max mem: 20.9 GB 
[11/25 12:04:06 visual_prompt]: 	Training 500/553. train loss: 0.2407,	0.8520 s / batch. (data: 2.93e-04). ETA=10:05:23, max mem: 20.9 GB 
[11/25 12:04:56 visual_prompt]: Epoch 23 / 100: avg data time: 1.22e-01, avg batch time: 0.9530, average train loss: 26.8034
[11/25 12:05:50 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3104, average loss: 13.1462
[11/25 12:05:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.32	
[11/25 12:05:50 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/25 12:07:26 visual_prompt]: 	Training 100/553. train loss: 6.5169,	0.8273 s / batch. (data: 2.92e-04). ETA=9:45:43, max mem: 20.9 GB 
[11/25 12:09:00 visual_prompt]: 	Training 200/553. train loss: 11.6133,	0.8445 s / batch. (data: 1.20e-02). ETA=9:56:31, max mem: 20.9 GB 
[11/25 12:10:36 visual_prompt]: 	Training 300/553. train loss: 7.3411,	0.8187 s / batch. (data: 2.75e-04). ETA=9:36:55, max mem: 20.9 GB 
[11/25 12:12:11 visual_prompt]: 	Training 400/553. train loss: 23.0182,	0.8293 s / batch. (data: 7.95e-03). ETA=9:43:00, max mem: 20.9 GB 
[11/25 12:13:48 visual_prompt]: 	Training 500/553. train loss: 8.7479,	0.8556 s / batch. (data: 3.12e-04). ETA=10:00:02, max mem: 20.9 GB 
[11/25 12:14:38 visual_prompt]: Epoch 24 / 100: avg data time: 1.24e-01, avg batch time: 0.9545, average train loss: 27.4048
[11/25 12:15:32 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3096, average loss: 12.1995
[11/25 12:15:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 65.38	
[11/25 12:15:32 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/25 12:17:13 visual_prompt]: 	Training 100/553. train loss: 59.5628,	0.8256 s / batch. (data: 4.17e-04). ETA=9:36:56, max mem: 20.9 GB 
[11/25 12:18:45 visual_prompt]: 	Training 200/553. train loss: 14.7123,	1.0120 s / batch. (data: 1.78e-01). ETA=11:45:29, max mem: 20.9 GB 
[11/25 12:20:20 visual_prompt]: 	Training 300/553. train loss: 24.4865,	0.8473 s / batch. (data: 3.53e-04). ETA=9:49:15, max mem: 20.9 GB 
[11/25 12:21:54 visual_prompt]: 	Training 400/553. train loss: 0.6782,	1.0400 s / batch. (data: 2.00e-01). ETA=12:01:32, max mem: 20.9 GB 
[11/25 12:23:29 visual_prompt]: 	Training 500/553. train loss: 9.5160,	1.2562 s / batch. (data: 4.21e-01). ETA=14:29:29, max mem: 20.9 GB 
[11/25 12:24:19 visual_prompt]: Epoch 25 / 100: avg data time: 1.23e-01, avg batch time: 0.9544, average train loss: 27.0481
[11/25 12:25:13 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3124, average loss: 73.2002
[11/25 12:25:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.01	
[11/25 12:25:13 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/25 12:26:52 visual_prompt]: 	Training 100/553. train loss: 6.2010,	0.8280 s / batch. (data: 2.86e-04). ETA=9:30:58, max mem: 20.9 GB 
[11/25 12:28:27 visual_prompt]: 	Training 200/553. train loss: 49.9791,	1.4704 s / batch. (data: 6.41e-01). ETA=16:51:32, max mem: 20.9 GB 
[11/25 12:30:04 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8578 s / batch. (data: 3.37e-02). ETA=9:48:41, max mem: 20.9 GB 
[11/25 12:31:37 visual_prompt]: 	Training 400/553. train loss: 3.7007,	0.8307 s / batch. (data: 1.20e-02). ETA=9:28:40, max mem: 20.9 GB 
[11/25 12:33:11 visual_prompt]: 	Training 500/553. train loss: 2.9932,	0.8433 s / batch. (data: 1.13e-02). ETA=9:35:56, max mem: 20.9 GB 
[11/25 12:34:00 visual_prompt]: Epoch 26 / 100: avg data time: 1.21e-01, avg batch time: 0.9523, average train loss: 27.0992
[11/25 12:34:54 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3110, average loss: 12.7327
[11/25 12:34:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 70.10	
[11/25 12:34:54 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[11/25 12:36:33 visual_prompt]: 	Training 100/553. train loss: 9.6838,	0.8320 s / batch. (data: 2.90e-04). ETA=9:26:03, max mem: 20.9 GB 
[11/25 12:38:07 visual_prompt]: 	Training 200/553. train loss: 61.5466,	0.8266 s / batch. (data: 1.19e-02). ETA=9:21:00, max mem: 20.9 GB 
[11/25 12:39:43 visual_prompt]: 	Training 300/553. train loss: 41.0477,	0.8080 s / batch. (data: 3.51e-04). ETA=9:07:02, max mem: 20.9 GB 
[11/25 12:41:19 visual_prompt]: 	Training 400/553. train loss: 66.3537,	0.8111 s / batch. (data: 3.17e-04). ETA=9:07:46, max mem: 20.9 GB 
[11/25 12:42:54 visual_prompt]: 	Training 500/553. train loss: 35.7847,	0.8262 s / batch. (data: 2.91e-04). ETA=9:16:37, max mem: 20.9 GB 
[11/25 12:43:42 visual_prompt]: Epoch 27 / 100: avg data time: 1.24e-01, avg batch time: 0.9549, average train loss: 27.1342
[11/25 12:44:36 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3113, average loss: 86.1610
[11/25 12:44:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 68.93	
[11/25 12:44:36 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[11/25 12:46:31 visual_prompt]: 	Training 100/553. train loss: 76.4337,	0.8107 s / batch. (data: 3.20e-04). ETA=9:04:05, max mem: 20.9 GB 
[11/25 12:48:12 visual_prompt]: 	Training 200/553. train loss: 29.6996,	0.8480 s / batch. (data: 3.24e-04). ETA=9:27:43, max mem: 20.9 GB 
[11/25 12:49:51 visual_prompt]: 	Training 300/553. train loss: 10.3368,	1.4046 s / batch. (data: 5.65e-01). ETA=15:38:00, max mem: 20.9 GB 
[11/25 12:51:29 visual_prompt]: 	Training 400/553. train loss: 18.3533,	0.8376 s / batch. (data: 5.42e-03). ETA=9:17:59, max mem: 20.9 GB 
[11/25 12:53:07 visual_prompt]: 	Training 500/553. train loss: 108.3515,	0.8291 s / batch. (data: 1.19e-02). ETA=9:10:54, max mem: 20.9 GB 
[11/25 12:54:00 visual_prompt]: Epoch 28 / 100: avg data time: 1.91e-01, avg batch time: 1.0199, average train loss: 27.1525
[11/25 12:54:58 visual_prompt]: Inference (val):avg data time: 1.68e-04, avg batch time: 0.3096, average loss: 49.8572
[11/25 12:54:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 69.52	
[11/25 12:54:58 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[11/25 12:56:45 visual_prompt]: 	Training 100/553. train loss: 23.9725,	0.8368 s / batch. (data: 5.42e-03). ETA=9:13:54, max mem: 20.9 GB 
[11/25 12:58:22 visual_prompt]: 	Training 200/553. train loss: 3.4627,	1.7724 s / batch. (data: 9.47e-01). ETA=19:30:15, max mem: 20.9 GB 
[11/25 12:59:58 visual_prompt]: 	Training 300/553. train loss: 2.1530,	0.8431 s / batch. (data: 8.05e-04). ETA=9:15:14, max mem: 20.9 GB 
[11/25 13:01:32 visual_prompt]: 	Training 400/553. train loss: 68.9398,	1.3956 s / batch. (data: 5.90e-01). ETA=15:16:50, max mem: 20.9 GB 
[11/25 13:03:10 visual_prompt]: 	Training 500/553. train loss: 11.5358,	0.8281 s / batch. (data: 3.41e-04). ETA=9:02:38, max mem: 20.9 GB 
[11/25 13:04:00 visual_prompt]: Epoch 29 / 100: avg data time: 1.51e-01, avg batch time: 0.9815, average train loss: 33.5493
[11/25 13:04:56 visual_prompt]: Inference (val):avg data time: 2.21e-04, avg batch time: 0.3112, average loss: 43.7048
[11/25 13:04:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.65	
[11/25 13:04:56 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[11/25 13:06:37 visual_prompt]: 	Training 100/553. train loss: 17.2615,	0.8480 s / batch. (data: 5.45e-03). ETA=9:13:30, max mem: 20.9 GB 
[11/25 13:08:15 visual_prompt]: 	Training 200/553. train loss: 0.0042,	0.8320 s / batch. (data: 3.08e-04). ETA=9:01:40, max mem: 20.9 GB 
[11/25 13:09:50 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.1440 s / batch. (data: 3.00e-01). ETA=12:22:53, max mem: 20.9 GB 
[11/25 13:11:29 visual_prompt]: 	Training 400/553. train loss: 9.7025,	1.3470 s / batch. (data: 5.10e-01). ETA=14:32:27, max mem: 20.9 GB 
[11/25 13:13:07 visual_prompt]: 	Training 500/553. train loss: 17.4095,	1.1360 s / batch. (data: 3.09e-01). ETA=12:13:55, max mem: 20.9 GB 
[11/25 13:14:01 visual_prompt]: Epoch 30 / 100: avg data time: 1.53e-01, avg batch time: 0.9843, average train loss: 20.2606
[11/25 13:14:57 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3092, average loss: 9.6827
[11/25 13:14:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 69.78	
[11/25 13:14:57 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[11/25 13:16:40 visual_prompt]: 	Training 100/553. train loss: 0.8287,	0.8242 s / batch. (data: 1.20e-02). ETA=8:50:24, max mem: 20.9 GB 
[11/25 13:18:19 visual_prompt]: 	Training 200/553. train loss: 5.9134,	0.8400 s / batch. (data: 3.21e-04). ETA=8:59:08, max mem: 20.9 GB 
[11/25 13:19:53 visual_prompt]: 	Training 300/553. train loss: 25.1979,	0.8320 s / batch. (data: 1.20e-02). ETA=8:52:37, max mem: 20.9 GB 
[11/25 13:21:30 visual_prompt]: 	Training 400/553. train loss: 15.6313,	1.2052 s / batch. (data: 3.69e-01). ETA=12:49:31, max mem: 20.9 GB 
[11/25 13:23:08 visual_prompt]: 	Training 500/553. train loss: 9.3144,	0.8278 s / batch. (data: 2.64e-04). ETA=8:47:12, max mem: 20.9 GB 
[11/25 13:23:57 visual_prompt]: Epoch 31 / 100: avg data time: 1.46e-01, avg batch time: 0.9778, average train loss: 27.2723
[11/25 13:24:53 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3109, average loss: 9.3283
[11/25 13:24:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 70.34	
[11/25 13:24:53 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[11/25 13:26:35 visual_prompt]: 	Training 100/553. train loss: 0.0084,	0.8454 s / batch. (data: 1.42e-02). ETA=8:56:12, max mem: 20.9 GB 
[11/25 13:28:12 visual_prompt]: 	Training 200/553. train loss: 2.5889,	0.8528 s / batch. (data: 1.09e-02). ETA=8:59:29, max mem: 20.9 GB 
[11/25 13:29:51 visual_prompt]: 	Training 300/553. train loss: 21.6072,	0.8435 s / batch. (data: 1.16e-02). ETA=8:52:13, max mem: 20.9 GB 
[11/25 13:31:30 visual_prompt]: 	Training 400/553. train loss: 57.6236,	0.8315 s / batch. (data: 2.99e-04). ETA=8:43:13, max mem: 20.9 GB 
[11/25 13:33:05 visual_prompt]: 	Training 500/553. train loss: 11.1369,	0.8320 s / batch. (data: 3.04e-04). ETA=8:42:11, max mem: 20.9 GB 
[11/25 13:33:54 visual_prompt]: Epoch 32 / 100: avg data time: 1.45e-01, avg batch time: 0.9769, average train loss: 20.3501
[11/25 13:34:49 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3114, average loss: 23.9036
[11/25 13:34:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 70.46	
[11/25 13:34:49 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[11/25 13:36:29 visual_prompt]: 	Training 100/553. train loss: 92.1395,	0.8297 s / batch. (data: 1.20e-02). ETA=8:38:38, max mem: 20.9 GB 
[11/25 13:38:08 visual_prompt]: 	Training 200/553. train loss: 27.5268,	1.1500 s / batch. (data: 3.33e-01). ETA=11:56:55, max mem: 20.9 GB 
[11/25 13:39:44 visual_prompt]: 	Training 300/553. train loss: 9.8688,	0.8664 s / batch. (data: 2.84e-04). ETA=8:58:38, max mem: 20.9 GB 
[11/25 13:41:22 visual_prompt]: 	Training 400/553. train loss: 1.8637,	0.8314 s / batch. (data: 5.42e-03). ETA=8:35:31, max mem: 20.9 GB 
[11/25 13:42:59 visual_prompt]: 	Training 500/553. train loss: 20.8694,	0.8231 s / batch. (data: 3.23e-04). ETA=8:28:59, max mem: 20.9 GB 
[11/25 13:43:50 visual_prompt]: Epoch 33 / 100: avg data time: 1.48e-01, avg batch time: 0.9780, average train loss: 33.1611
[11/25 13:44:45 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3103, average loss: 10.2849
[11/25 13:44:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 70.90	
[11/25 13:44:45 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[11/25 13:46:27 visual_prompt]: 	Training 100/553. train loss: 16.4824,	0.8269 s / batch. (data: 3.46e-04). ETA=8:29:15, max mem: 20.9 GB 
[11/25 13:48:02 visual_prompt]: 	Training 200/553. train loss: 57.6707,	0.8238 s / batch. (data: 1.20e-02). ETA=8:25:56, max mem: 20.9 GB 
[11/25 13:49:39 visual_prompt]: 	Training 300/553. train loss: 1.1536,	0.8714 s / batch. (data: 3.31e-04). ETA=8:53:44, max mem: 20.9 GB 
[11/25 13:51:17 visual_prompt]: 	Training 400/553. train loss: 7.4392,	0.8293 s / batch. (data: 3.01e-04). ETA=8:26:35, max mem: 20.9 GB 
[11/25 13:52:54 visual_prompt]: 	Training 500/553. train loss: 8.9543,	1.4220 s / batch. (data: 5.90e-01). ETA=14:26:13, max mem: 20.9 GB 
[11/25 13:53:44 visual_prompt]: Epoch 34 / 100: avg data time: 1.42e-01, avg batch time: 0.9732, average train loss: 27.6327
[11/25 13:54:39 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3113, average loss: 16.1818
[11/25 13:54:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 71.22	
[11/25 13:54:39 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[11/25 13:56:22 visual_prompt]: 	Training 100/553. train loss: 42.2535,	0.8480 s / batch. (data: 3.46e-04). ETA=8:34:25, max mem: 20.9 GB 
[11/25 13:58:00 visual_prompt]: 	Training 200/553. train loss: 33.6156,	0.8404 s / batch. (data: 3.17e-04). ETA=8:28:26, max mem: 20.9 GB 
[11/25 13:59:36 visual_prompt]: 	Training 300/553. train loss: 25.7727,	0.8210 s / batch. (data: 3.18e-04). ETA=8:15:17, max mem: 20.9 GB 
[11/25 14:01:12 visual_prompt]: 	Training 400/553. train loss: 29.7589,	0.8200 s / batch. (data: 3.29e-04). ETA=8:13:19, max mem: 20.9 GB 
[11/25 14:02:48 visual_prompt]: 	Training 500/553. train loss: 11.0623,	1.0943 s / batch. (data: 2.62e-01). ETA=10:56:32, max mem: 20.9 GB 
[11/25 14:03:39 visual_prompt]: Epoch 35 / 100: avg data time: 1.45e-01, avg batch time: 0.9759, average train loss: 22.2768
[11/25 14:04:34 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3105, average loss: 47.3933
[11/25 14:04:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 70.17	
[11/25 14:04:34 visual_prompt]: Stopping early.
[11/25 14:04:35 visual_prompt]: Rank of current process: 0. World size: 1
[11/25 14:04:35 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/25 14:04:35 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/25 14:04:35 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/25 14:04:35 visual_prompt]: Training with config:
[11/25 14:04:35 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/25 14:04:35 visual_prompt]: Loading training data...
[11/25 14:04:35 visual_prompt]: Constructing mammo-cbis dataset train...
[11/25 14:04:35 visual_prompt]: Loading validation data...
[11/25 14:04:35 visual_prompt]: Constructing mammo-cbis dataset val...
[11/25 14:04:35 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/25 14:04:40 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/25 14:04:40 visual_prompt]: tuned percent:0.525
[11/25 14:04:40 visual_prompt]: Device used for model: 0
[11/25 14:04:40 visual_prompt]: Setting up Evaluator...
[11/25 14:04:40 visual_prompt]: Setting up Trainer...
[11/25 14:04:40 visual_prompt]: 	Setting up the optimizer...
[11/25 14:04:40 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/25 14:06:21 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8266 s / batch. (data: 1.05e-02). ETA=12:40:25, max mem: 20.9 GB 
[11/25 14:07:56 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8440 s / batch. (data: 1.20e-02). ETA=12:55:05, max mem: 20.9 GB 
[11/25 14:09:36 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.5474 s / batch. (data: 6.86e-01). ETA=23:38:25, max mem: 20.9 GB 
[11/25 14:11:11 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8320 s / batch. (data: 3.10e-04). ETA=12:41:14, max mem: 20.9 GB 
[11/25 14:12:49 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8475 s / batch. (data: 7.20e-04). ETA=12:54:01, max mem: 20.9 GB 
[11/25 14:13:41 visual_prompt]: Epoch 1 / 100: avg data time: 1.43e-01, avg batch time: 0.9769, average train loss: 1.5403
[11/25 14:14:36 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3096, average loss: 1.5201
[11/25 14:14:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/25 14:14:36 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/25 14:16:18 visual_prompt]: 	Training 100/553. train loss: 2.9190,	0.8339 s / batch. (data: 3.28e-04). ETA=12:39:31, max mem: 20.9 GB 
[11/25 14:17:54 visual_prompt]: 	Training 200/553. train loss: 0.0037,	1.1952 s / batch. (data: 3.67e-01). ETA=18:06:33, max mem: 20.9 GB 
[11/25 14:19:32 visual_prompt]: 	Training 300/553. train loss: 7.8096,	0.8973 s / batch. (data: 7.27e-02). ETA=13:34:16, max mem: 20.9 GB 
[11/25 14:21:08 visual_prompt]: 	Training 400/553. train loss: 0.6231,	0.8504 s / batch. (data: 3.03e-04). ETA=12:50:16, max mem: 20.9 GB 
[11/25 14:22:46 visual_prompt]: 	Training 500/553. train loss: 3.2929,	0.8207 s / batch. (data: 3.13e-04). ETA=12:22:00, max mem: 20.9 GB 
[11/25 14:23:35 visual_prompt]: Epoch 2 / 100: avg data time: 1.40e-01, avg batch time: 0.9742, average train loss: 3.4209
[11/25 14:24:30 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3121, average loss: 19.1750
[11/25 14:24:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.67	
[11/25 14:24:30 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/25 14:26:10 visual_prompt]: 	Training 100/553. train loss: 0.7196,	0.8360 s / batch. (data: 1.20e-02). ETA=12:33:42, max mem: 20.9 GB 
[11/25 14:27:48 visual_prompt]: 	Training 200/553. train loss: 1.8945,	1.0363 s / batch. (data: 2.08e-01). ETA=15:32:34, max mem: 20.9 GB 
[11/25 14:29:24 visual_prompt]: 	Training 300/553. train loss: 4.1691,	0.8423 s / batch. (data: 3.13e-04). ETA=12:36:32, max mem: 20.9 GB 
[11/25 14:31:02 visual_prompt]: 	Training 400/553. train loss: 55.2133,	0.8321 s / batch. (data: 3.08e-04). ETA=12:25:59, max mem: 20.9 GB 
[11/25 14:32:40 visual_prompt]: 	Training 500/553. train loss: 7.2534,	1.2394 s / batch. (data: 4.21e-01). ETA=18:29:08, max mem: 20.9 GB 
[11/25 14:33:30 visual_prompt]: Epoch 3 / 100: avg data time: 1.43e-01, avg batch time: 0.9762, average train loss: 6.8817
[11/25 14:34:26 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3094, average loss: 4.1820
[11/25 14:34:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.68	
[11/25 14:34:26 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/25 14:36:08 visual_prompt]: 	Training 100/553. train loss: 20.7751,	0.8260 s / batch. (data: 2.44e-04). ETA=12:17:02, max mem: 20.9 GB 
[11/25 14:37:46 visual_prompt]: 	Training 200/553. train loss: 2.4443,	0.8284 s / batch. (data: 5.44e-03). ETA=12:17:51, max mem: 20.9 GB 
[11/25 14:39:23 visual_prompt]: 	Training 300/553. train loss: 2.6300,	1.2156 s / batch. (data: 3.86e-01). ETA=18:00:41, max mem: 20.9 GB 
[11/25 14:40:55 visual_prompt]: 	Training 400/553. train loss: 5.1956,	1.3393 s / batch. (data: 5.14e-01). ETA=19:48:25, max mem: 20.9 GB 
[11/25 14:42:34 visual_prompt]: 	Training 500/553. train loss: 49.1465,	3.3321 s / batch. (data: 2.51e+00). ETA=2 days, 1:11:13, max mem: 20.9 GB 
[11/25 14:43:26 visual_prompt]: Epoch 4 / 100: avg data time: 1.43e-01, avg batch time: 0.9770, average train loss: 9.3442
[11/25 14:44:21 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3112, average loss: 2.7230
[11/25 14:44:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.14	
[11/25 14:44:21 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/25 14:46:01 visual_prompt]: 	Training 100/553. train loss: 0.3351,	0.8709 s / batch. (data: 1.49e-02). ETA=12:49:08, max mem: 20.9 GB 
[11/25 14:47:37 visual_prompt]: 	Training 200/553. train loss: 12.2037,	1.1255 s / batch. (data: 2.80e-01). ETA=16:32:07, max mem: 20.9 GB 
[11/25 14:49:15 visual_prompt]: 	Training 300/553. train loss: 47.6977,	0.8240 s / batch. (data: 4.23e-04). ETA=12:04:56, max mem: 20.9 GB 
[11/25 14:50:51 visual_prompt]: 	Training 400/553. train loss: 16.4583,	0.8429 s / batch. (data: 1.04e-02). ETA=12:20:11, max mem: 20.9 GB 
[11/25 14:52:29 visual_prompt]: 	Training 500/553. train loss: 4.7859,	0.8471 s / batch. (data: 3.06e-04). ETA=12:22:28, max mem: 20.9 GB 
[11/25 14:53:20 visual_prompt]: Epoch 5 / 100: avg data time: 1.40e-01, avg batch time: 0.9743, average train loss: 13.9010
[11/25 14:54:16 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3095, average loss: 92.5483
[11/25 14:54:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.76	
[11/25 14:54:16 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/25 14:55:59 visual_prompt]: 	Training 100/553. train loss: 1.8484,	0.8280 s / batch. (data: 2.93e-04). ETA=12:03:36, max mem: 20.9 GB 
[11/25 14:57:35 visual_prompt]: 	Training 200/553. train loss: 11.7367,	0.8400 s / batch. (data: 5.43e-03). ETA=12:12:41, max mem: 20.9 GB 
[11/25 14:59:11 visual_prompt]: 	Training 300/553. train loss: 3.2738,	0.8376 s / batch. (data: 3.15e-04). ETA=12:09:11, max mem: 20.9 GB 
[11/25 15:00:53 visual_prompt]: 	Training 400/553. train loss: 8.4288,	0.8614 s / batch. (data: 4.82e-03). ETA=12:28:27, max mem: 20.9 GB 
[11/25 15:02:29 visual_prompt]: 	Training 500/553. train loss: 2.4603,	0.8360 s / batch. (data: 3.50e-04). ETA=12:05:01, max mem: 20.9 GB 
[11/25 15:03:19 visual_prompt]: Epoch 6 / 100: avg data time: 1.49e-01, avg batch time: 0.9819, average train loss: 21.8691
[11/25 15:04:14 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.3108, average loss: 2.3365
[11/25 15:04:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.85	
[11/25 15:04:14 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/25 15:05:54 visual_prompt]: 	Training 100/553. train loss: 13.2510,	0.8520 s / batch. (data: 3.35e-04). ETA=12:16:45, max mem: 20.9 GB 
[11/25 15:07:31 visual_prompt]: 	Training 200/553. train loss: 23.3658,	0.8440 s / batch. (data: 3.28e-04). ETA=12:08:24, max mem: 20.9 GB 
[11/25 15:09:12 visual_prompt]: 	Training 300/553. train loss: 12.4615,	1.7143 s / batch. (data: 8.81e-01). ETA=1 day, 0:36:38, max mem: 20.9 GB 
[11/25 15:10:50 visual_prompt]: 	Training 400/553. train loss: 9.6258,	1.9080 s / batch. (data: 1.08e+00). ETA=1 day, 3:20:17, max mem: 20.9 GB 
[11/25 15:12:26 visual_prompt]: 	Training 500/553. train loss: 4.9146,	0.8404 s / batch. (data: 1.20e-02). ETA=12:01:05, max mem: 20.9 GB 
[11/25 15:13:15 visual_prompt]: Epoch 7 / 100: avg data time: 1.44e-01, avg batch time: 0.9777, average train loss: 20.0644
[11/25 15:14:11 visual_prompt]: Inference (val):avg data time: 2.16e-04, avg batch time: 0.3122, average loss: 8.4642
[11/25 15:14:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.60	
[11/25 15:14:11 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/25 15:15:50 visual_prompt]: 	Training 100/553. train loss: 9.8224,	0.8598 s / batch. (data: 1.05e-02). ETA=12:15:33, max mem: 20.9 GB 
[11/25 15:17:29 visual_prompt]: 	Training 200/553. train loss: 39.4100,	0.8360 s / batch. (data: 3.30e-04). ETA=11:53:47, max mem: 20.9 GB 
[11/25 15:19:06 visual_prompt]: 	Training 300/553. train loss: 13.7431,	0.8257 s / batch. (data: 2.96e-04). ETA=11:43:37, max mem: 20.9 GB 
[11/25 15:20:43 visual_prompt]: 	Training 400/553. train loss: 51.2663,	0.8269 s / batch. (data: 1.05e-02). ETA=11:43:16, max mem: 20.9 GB 
[11/25 15:22:20 visual_prompt]: 	Training 500/553. train loss: 139.0039,	1.3386 s / batch. (data: 5.07e-01). ETA=18:56:12, max mem: 20.9 GB 
[11/25 15:23:11 visual_prompt]: Epoch 8 / 100: avg data time: 1.44e-01, avg batch time: 0.9765, average train loss: 25.8392
[11/25 15:24:06 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3092, average loss: 7.0291
[11/25 15:24:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.43	
[11/25 15:24:06 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/25 15:25:46 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8600 s / batch. (data: 3.02e-04). ETA=12:07:44, max mem: 20.9 GB 
[11/25 15:27:23 visual_prompt]: 	Training 200/553. train loss: 13.8168,	0.8440 s / batch. (data: 3.14e-04). ETA=11:52:51, max mem: 20.9 GB 
[11/25 15:29:01 visual_prompt]: 	Training 300/553. train loss: 3.5134,	1.6997 s / batch. (data: 8.68e-01). ETA=23:52:42, max mem: 20.9 GB 
[11/25 15:30:39 visual_prompt]: 	Training 400/553. train loss: 10.2903,	0.8360 s / batch. (data: 8.13e-04). ETA=11:43:18, max mem: 20.9 GB 
[11/25 15:32:16 visual_prompt]: 	Training 500/553. train loss: 21.3491,	0.8221 s / batch. (data: 3.37e-04). ETA=11:30:12, max mem: 20.9 GB 
[11/25 15:33:06 visual_prompt]: Epoch 9 / 100: avg data time: 1.44e-01, avg batch time: 0.9769, average train loss: 23.0162
[11/25 15:34:02 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3099, average loss: 42.8388
[11/25 15:34:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.46	
[11/25 15:34:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/25 15:35:45 visual_prompt]: 	Training 100/553. train loss: 12.5214,	0.8166 s / batch. (data: 3.06e-04). ETA=11:23:29, max mem: 20.9 GB 
[11/25 15:37:21 visual_prompt]: 	Training 200/553. train loss: 6.6046,	0.8280 s / batch. (data: 3.24e-04). ETA=11:31:41, max mem: 20.9 GB 
[11/25 15:38:58 visual_prompt]: 	Training 300/553. train loss: 6.6750,	2.0432 s / batch. (data: 1.22e+00). ETA=1 day, 4:23:26, max mem: 20.9 GB 
[11/25 15:40:33 visual_prompt]: 	Training 400/553. train loss: 5.2920,	0.8906 s / batch. (data: 5.43e-03). ETA=12:21:01, max mem: 20.9 GB 
[11/25 15:42:12 visual_prompt]: 	Training 500/553. train loss: 19.9925,	0.8320 s / batch. (data: 3.20e-04). ETA=11:30:53, max mem: 20.9 GB 
[11/25 15:43:03 visual_prompt]: Epoch 10 / 100: avg data time: 1.45e-01, avg batch time: 0.9781, average train loss: 30.4111
[11/25 15:43:58 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3086, average loss: 0.9898
[11/25 15:43:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.75	
[11/25 15:43:58 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/25 15:45:41 visual_prompt]: 	Training 100/553. train loss: 15.8778,	0.8440 s / batch. (data: 2.88e-04). ETA=11:38:41, max mem: 20.9 GB 
[11/25 15:47:20 visual_prompt]: 	Training 200/553. train loss: 24.1415,	0.8393 s / batch. (data: 2.98e-04). ETA=11:33:22, max mem: 20.9 GB 
[11/25 15:48:57 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.0113 s / batch. (data: 1.17e+00). ETA=1 day, 3:38:19, max mem: 20.9 GB 
[11/25 15:50:33 visual_prompt]: 	Training 400/553. train loss: 43.0872,	0.8402 s / batch. (data: 5.92e-03). ETA=11:31:20, max mem: 20.9 GB 
[11/25 15:52:09 visual_prompt]: 	Training 500/553. train loss: 45.0347,	0.8170 s / batch. (data: 2.67e-04). ETA=11:10:55, max mem: 20.9 GB 
[11/25 15:52:59 visual_prompt]: Epoch 11 / 100: avg data time: 1.45e-01, avg batch time: 0.9772, average train loss: 34.2581
[11/25 15:53:54 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3089, average loss: 21.7759
[11/25 15:53:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.63	
[11/25 15:53:54 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/25 15:55:36 visual_prompt]: 	Training 100/553. train loss: 39.2219,	0.8247 s / batch. (data: 2.50e-04). ETA=11:15:08, max mem: 20.9 GB 
[11/25 15:57:14 visual_prompt]: 	Training 200/553. train loss: 7.6297,	0.8400 s / batch. (data: 3.13e-04). ETA=11:26:14, max mem: 20.9 GB 
[11/25 15:58:49 visual_prompt]: 	Training 300/553. train loss: 12.9869,	0.8386 s / batch. (data: 1.56e-02). ETA=11:23:43, max mem: 20.9 GB 
[11/25 16:00:26 visual_prompt]: 	Training 400/553. train loss: 20.0534,	0.8230 s / batch. (data: 3.27e-04). ETA=11:09:37, max mem: 20.9 GB 
[11/25 16:02:03 visual_prompt]: 	Training 500/553. train loss: 7.4051,	0.8634 s / batch. (data: 8.34e-04). ETA=11:41:04, max mem: 20.9 GB 
[11/25 16:02:53 visual_prompt]: Epoch 12 / 100: avg data time: 1.43e-01, avg batch time: 0.9749, average train loss: 34.2184
[11/25 16:03:49 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3106, average loss: 33.8616
[11/25 16:03:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.16	
[11/25 16:03:49 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/25 16:05:32 visual_prompt]: 	Training 100/553. train loss: 18.5837,	0.8480 s / batch. (data: 3.42e-04). ETA=11:26:22, max mem: 20.9 GB 
[11/25 16:07:06 visual_prompt]: 	Training 200/553. train loss: 119.0935,	0.8283 s / batch. (data: 3.22e-04). ETA=11:09:04, max mem: 20.9 GB 
[11/25 16:08:45 visual_prompt]: 	Training 300/553. train loss: 21.8950,	1.6888 s / batch. (data: 8.53e-01). ETA=22:41:18, max mem: 20.9 GB 
[11/25 16:10:21 visual_prompt]: 	Training 400/553. train loss: 13.5954,	0.8292 s / batch. (data: 9.15e-03). ETA=11:07:00, max mem: 20.9 GB 
[11/25 16:11:59 visual_prompt]: 	Training 500/553. train loss: 20.9357,	0.8771 s / batch. (data: 5.43e-03). ETA=11:44:04, max mem: 20.9 GB 
[11/25 16:12:49 visual_prompt]: Epoch 13 / 100: avg data time: 1.45e-01, avg batch time: 0.9773, average train loss: 36.6054
[11/25 16:13:45 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3110, average loss: 12.1257
[11/25 16:13:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.44	
[11/25 16:13:45 visual_prompt]: Best epoch 13: best metric: -12.126
[11/25 16:13:45 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/25 16:15:27 visual_prompt]: 	Training 100/553. train loss: 1.8615,	0.8321 s / batch. (data: 1.57e-02). ETA=11:05:48, max mem: 20.9 GB 
[11/25 16:17:04 visual_prompt]: 	Training 200/553. train loss: 6.4021,	0.8880 s / batch. (data: 5.78e-02). ETA=11:49:05, max mem: 20.9 GB 
[11/25 16:18:42 visual_prompt]: 	Training 300/553. train loss: 68.5546,	0.8253 s / batch. (data: 9.25e-03). ETA=10:57:37, max mem: 20.9 GB 
[11/25 16:20:17 visual_prompt]: 	Training 400/553. train loss: 5.1330,	0.8297 s / batch. (data: 5.44e-03). ETA=10:59:46, max mem: 20.9 GB 
[11/25 16:21:54 visual_prompt]: 	Training 500/553. train loss: 9.8359,	0.8190 s / batch. (data: 3.02e-04). ETA=10:49:52, max mem: 20.9 GB 
[11/25 16:22:45 visual_prompt]: Epoch 14 / 100: avg data time: 1.44e-01, avg batch time: 0.9764, average train loss: 32.7403
[11/25 16:23:40 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3116, average loss: 76.2857
[11/25 16:23:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.90	
[11/25 16:23:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/25 16:25:22 visual_prompt]: 	Training 100/553. train loss: 58.4551,	0.8282 s / batch. (data: 1.56e-02). ETA=10:55:06, max mem: 20.9 GB 
[11/25 16:26:57 visual_prompt]: 	Training 200/553. train loss: 188.4184,	0.8360 s / batch. (data: 2.91e-04). ETA=10:59:51, max mem: 20.9 GB 
[11/25 16:28:36 visual_prompt]: 	Training 300/553. train loss: 24.2248,	0.8451 s / batch. (data: 7.01e-04). ETA=11:05:37, max mem: 20.9 GB 
[11/25 16:30:11 visual_prompt]: 	Training 400/553. train loss: 5.2649,	1.1640 s / batch. (data: 2.92e-01). ETA=15:14:50, max mem: 20.9 GB 
[11/25 16:31:48 visual_prompt]: 	Training 500/553. train loss: 15.9856,	0.8255 s / batch. (data: 3.34e-04). ETA=10:47:26, max mem: 20.9 GB 
[11/25 16:32:39 visual_prompt]: Epoch 15 / 100: avg data time: 1.43e-01, avg batch time: 0.9746, average train loss: 36.2378
[11/25 16:33:34 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3123, average loss: 21.1190
[11/25 16:33:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.29	
[11/25 16:33:34 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/25 16:35:14 visual_prompt]: 	Training 100/553. train loss: 136.7389,	0.8357 s / batch. (data: 3.82e-04). ETA=10:53:17, max mem: 20.9 GB 
[11/25 16:36:51 visual_prompt]: 	Training 200/553. train loss: 59.3486,	0.8440 s / batch. (data: 2.93e-04). ETA=10:58:22, max mem: 20.9 GB 
[11/25 16:38:28 visual_prompt]: 	Training 300/553. train loss: 30.4218,	0.8552 s / batch. (data: 1.05e-02). ETA=11:05:44, max mem: 20.9 GB 
[11/25 16:40:05 visual_prompt]: 	Training 400/553. train loss: 17.3393,	0.8272 s / batch. (data: 5.43e-03). ETA=10:42:33, max mem: 20.9 GB 
[11/25 16:41:41 visual_prompt]: 	Training 500/553. train loss: 11.4081,	0.8467 s / batch. (data: 1.05e-02). ETA=10:56:15, max mem: 20.9 GB 
[11/25 16:42:32 visual_prompt]: Epoch 16 / 100: avg data time: 1.39e-01, avg batch time: 0.9717, average train loss: 32.5930
[11/25 16:43:27 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3118, average loss: 44.3229
[11/25 16:43:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.94	
[11/25 16:43:27 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/25 16:45:08 visual_prompt]: 	Training 100/553. train loss: 55.8716,	0.8320 s / batch. (data: 2.91e-04). ETA=10:42:45, max mem: 20.9 GB 
[11/25 16:46:46 visual_prompt]: 	Training 200/553. train loss: 54.9315,	0.8321 s / batch. (data: 2.83e-04). ETA=10:41:25, max mem: 20.9 GB 
[11/25 16:48:23 visual_prompt]: 	Training 300/553. train loss: 15.8055,	0.8488 s / batch. (data: 2.40e-02). ETA=10:52:51, max mem: 20.9 GB 
[11/25 16:50:00 visual_prompt]: 	Training 400/553. train loss: 2.8833,	1.0400 s / batch. (data: 1.86e-01). ETA=13:18:14, max mem: 20.9 GB 
[11/25 16:51:37 visual_prompt]: 	Training 500/553. train loss: 5.9224,	1.3881 s / batch. (data: 5.69e-01). ETA=17:43:04, max mem: 20.9 GB 
[11/25 16:52:28 visual_prompt]: Epoch 17 / 100: avg data time: 1.46e-01, avg batch time: 0.9783, average train loss: 31.9066
[11/25 16:53:26 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3098, average loss: 22.5387
[11/25 16:53:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.52	
[11/25 16:53:26 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/25 16:55:12 visual_prompt]: 	Training 100/553. train loss: 33.3468,	0.8165 s / batch. (data: 6.96e-04). ETA=10:23:16, max mem: 20.9 GB 
[11/25 16:57:07 visual_prompt]: 	Training 200/553. train loss: 15.3985,	0.8316 s / batch. (data: 5.90e-03). ETA=10:33:22, max mem: 20.9 GB 
[11/25 16:58:46 visual_prompt]: 	Training 300/553. train loss: 36.3336,	0.8670 s / batch. (data: 3.10e-04). ETA=10:58:56, max mem: 20.9 GB 
[11/25 17:00:25 visual_prompt]: 	Training 400/553. train loss: 26.4739,	0.8473 s / batch. (data: 1.19e-02). ETA=10:42:29, max mem: 20.9 GB 
[11/25 17:02:02 visual_prompt]: 	Training 500/553. train loss: 26.0536,	1.0160 s / batch. (data: 1.81e-01). ETA=12:48:44, max mem: 20.9 GB 
[11/25 17:03:02 visual_prompt]: Epoch 18 / 100: avg data time: 2.11e-01, avg batch time: 1.0417, average train loss: 32.3624
[11/25 17:03:58 visual_prompt]: Inference (val):avg data time: 6.07e-04, avg batch time: 0.3083, average loss: 28.3415
[11/25 17:03:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.76	
[11/25 17:03:58 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/25 17:05:39 visual_prompt]: 	Training 100/553. train loss: 73.0952,	0.8222 s / batch. (data: 3.24e-04). ETA=10:20:03, max mem: 20.9 GB 
[11/25 17:07:16 visual_prompt]: 	Training 200/553. train loss: 18.0639,	0.8211 s / batch. (data: 2.96e-04). ETA=10:17:50, max mem: 20.9 GB 
[11/25 17:08:54 visual_prompt]: 	Training 300/553. train loss: 0.0051,	0.8587 s / batch. (data: 3.50e-02). ETA=10:44:41, max mem: 20.9 GB 
[11/25 17:10:33 visual_prompt]: 	Training 400/553. train loss: 8.9777,	0.8344 s / batch. (data: 2.99e-04). ETA=10:25:02, max mem: 20.9 GB 
[11/25 17:12:06 visual_prompt]: 	Training 500/553. train loss: 29.4289,	0.8239 s / batch. (data: 5.42e-03). ETA=10:15:48, max mem: 20.9 GB 
[11/25 17:12:57 visual_prompt]: Epoch 19 / 100: avg data time: 1.42e-01, avg batch time: 0.9741, average train loss: 37.7777
[11/25 17:13:52 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3114, average loss: 163.5188
[11/25 17:13:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.06	
[11/25 17:13:52 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/25 17:15:31 visual_prompt]: 	Training 100/553. train loss: 11.4015,	0.8480 s / batch. (data: 7.95e-03). ETA=10:31:38, max mem: 20.9 GB 
[11/25 17:17:11 visual_prompt]: 	Training 200/553. train loss: 34.3164,	0.8078 s / batch. (data: 3.31e-04). ETA=10:00:21, max mem: 20.9 GB 
[11/25 17:18:48 visual_prompt]: 	Training 300/553. train loss: 34.0807,	0.8440 s / batch. (data: 2.98e-04). ETA=10:25:51, max mem: 20.9 GB 
[11/25 17:20:26 visual_prompt]: 	Training 400/553. train loss: 1.3187,	0.8286 s / batch. (data: 3.39e-04). ETA=10:13:03, max mem: 20.9 GB 
[11/25 17:22:02 visual_prompt]: 	Training 500/553. train loss: 16.2321,	0.8360 s / batch. (data: 3.07e-04). ETA=10:17:07, max mem: 20.9 GB 
[11/25 17:22:54 visual_prompt]: Epoch 20 / 100: avg data time: 1.47e-01, avg batch time: 0.9792, average train loss: 33.0191
[11/25 17:23:49 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3099, average loss: 21.6157
[11/25 17:23:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.95	
[11/25 17:23:49 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/25 17:25:34 visual_prompt]: 	Training 100/553. train loss: 13.6234,	0.8154 s / batch. (data: 2.68e-04). ETA=9:59:51, max mem: 20.9 GB 
[11/25 17:27:10 visual_prompt]: 	Training 200/553. train loss: 70.9600,	0.8208 s / batch. (data: 5.44e-03). ETA=10:02:28, max mem: 20.9 GB 
[11/25 17:28:46 visual_prompt]: 	Training 300/553. train loss: 131.5904,	1.0255 s / batch. (data: 2.01e-01). ETA=12:31:00, max mem: 20.9 GB 
[11/25 17:30:22 visual_prompt]: 	Training 400/553. train loss: 4.7542,	0.8202 s / batch. (data: 3.04e-04). ETA=9:59:17, max mem: 20.9 GB 
[11/25 17:32:01 visual_prompt]: 	Training 500/553. train loss: 5.1656,	0.8318 s / batch. (data: 5.42e-03). ETA=10:06:22, max mem: 20.9 GB 
[11/25 17:32:51 visual_prompt]: Epoch 21 / 100: avg data time: 1.47e-01, avg batch time: 0.9786, average train loss: 32.3908
[11/25 17:33:46 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3088, average loss: 59.2093
[11/25 17:33:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.51	
[11/25 17:33:46 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/25 17:35:26 visual_prompt]: 	Training 100/553. train loss: 74.4270,	0.8520 s / batch. (data: 3.06e-04). ETA=10:18:55, max mem: 20.9 GB 
[11/25 17:37:03 visual_prompt]: 	Training 200/553. train loss: 1.8805,	0.8381 s / batch. (data: 1.05e-02). ETA=10:07:26, max mem: 20.9 GB 
[11/25 17:38:38 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8292 s / batch. (data: 3.17e-04). ETA=9:59:38, max mem: 20.9 GB 
[11/25 17:40:17 visual_prompt]: 	Training 400/553. train loss: 60.2997,	0.8359 s / batch. (data: 3.05e-04). ETA=10:03:04, max mem: 20.9 GB 
[11/25 17:41:55 visual_prompt]: 	Training 500/553. train loss: 14.4501,	0.8552 s / batch. (data: 3.21e-04). ETA=10:15:33, max mem: 20.9 GB 
[11/25 17:42:46 visual_prompt]: Epoch 22 / 100: avg data time: 1.45e-01, avg batch time: 0.9776, average train loss: 31.3885
[11/25 17:43:42 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3129, average loss: 37.5095
[11/25 17:43:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.10	
[11/25 17:43:42 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/25 17:45:25 visual_prompt]: 	Training 100/553. train loss: 14.7998,	0.8294 s / batch. (data: 5.44e-03). ETA=9:54:51, max mem: 20.9 GB 
[11/25 17:47:03 visual_prompt]: 	Training 200/553. train loss: 26.9610,	0.8508 s / batch. (data: 1.56e-02). ETA=10:08:46, max mem: 20.9 GB 
[11/25 17:48:42 visual_prompt]: 	Training 300/553. train loss: 38.7181,	0.8594 s / batch. (data: 5.84e-03). ETA=10:13:31, max mem: 20.9 GB 
[11/25 17:50:17 visual_prompt]: 	Training 400/553. train loss: 20.7494,	0.8440 s / batch. (data: 7.62e-04). ETA=10:01:08, max mem: 20.9 GB 
[11/25 17:51:53 visual_prompt]: 	Training 500/553. train loss: 2.8258,	0.8238 s / batch. (data: 2.85e-04). ETA=9:45:24, max mem: 20.9 GB 
[11/25 17:52:44 visual_prompt]: Epoch 23 / 100: avg data time: 1.48e-01, avg batch time: 0.9794, average train loss: 30.7184
[11/25 17:53:39 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3097, average loss: 31.7203
[11/25 17:53:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.50	
[11/25 17:53:39 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/25 17:55:18 visual_prompt]: 	Training 100/553. train loss: 21.1932,	0.8440 s / batch. (data: 3.01e-04). ETA=9:57:33, max mem: 20.9 GB 
[11/25 17:56:55 visual_prompt]: 	Training 200/553. train loss: 4.5434,	0.8320 s / batch. (data: 3.00e-04). ETA=9:47:41, max mem: 20.9 GB 
[11/25 17:58:33 visual_prompt]: 	Training 300/553. train loss: 5.0614,	0.8667 s / batch. (data: 3.14e-02). ETA=10:10:45, max mem: 20.9 GB 
[11/25 18:00:10 visual_prompt]: 	Training 400/553. train loss: 27.5653,	0.8669 s / batch. (data: 1.05e-02). ETA=10:09:25, max mem: 20.9 GB 
[11/25 18:01:50 visual_prompt]: 	Training 500/553. train loss: 37.7972,	0.8369 s / batch. (data: 1.05e-02). ETA=9:46:57, max mem: 20.9 GB 
[11/25 18:02:41 visual_prompt]: Epoch 24 / 100: avg data time: 1.47e-01, avg batch time: 0.9795, average train loss: 29.8137
[11/25 18:03:36 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3111, average loss: 7.4391
[11/25 18:03:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.25	
[11/25 18:03:36 visual_prompt]: Best epoch 24: best metric: -7.439
[11/25 18:03:36 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/25 18:05:21 visual_prompt]: 	Training 100/553. train loss: 53.5473,	0.8290 s / batch. (data: 3.08e-04). ETA=9:39:18, max mem: 20.9 GB 
[11/25 18:06:55 visual_prompt]: 	Training 200/553. train loss: 11.9055,	0.8443 s / batch. (data: 1.05e-02). ETA=9:48:35, max mem: 20.9 GB 
[11/25 18:08:33 visual_prompt]: 	Training 300/553. train loss: 5.0566,	0.8629 s / batch. (data: 3.29e-04). ETA=10:00:08, max mem: 20.9 GB 
[11/25 18:10:10 visual_prompt]: 	Training 400/553. train loss: 9.4415,	1.2073 s / batch. (data: 3.72e-01). ETA=13:57:36, max mem: 20.9 GB 
[11/25 18:11:48 visual_prompt]: 	Training 500/553. train loss: 11.4700,	1.2607 s / batch. (data: 4.31e-01). ETA=14:32:33, max mem: 20.9 GB 
[11/25 18:12:40 visual_prompt]: Epoch 25 / 100: avg data time: 1.51e-01, avg batch time: 0.9831, average train loss: 32.5727
[11/25 18:13:36 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3105, average loss: 24.6369
[11/25 18:13:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.47	
[11/25 18:13:36 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/25 18:15:18 visual_prompt]: 	Training 100/553. train loss: 2.8801,	0.8516 s / batch. (data: 5.56e-03). ETA=9:47:16, max mem: 20.9 GB 
[11/25 18:16:57 visual_prompt]: 	Training 200/553. train loss: 21.5737,	1.7960 s / batch. (data: 9.90e-01). ETA=20:35:31, max mem: 20.9 GB 
[11/25 18:18:37 visual_prompt]: 	Training 300/553. train loss: 17.6660,	0.8480 s / batch. (data: 2.73e-04). ETA=9:41:57, max mem: 20.9 GB 
[11/25 18:20:14 visual_prompt]: 	Training 400/553. train loss: 17.7804,	0.8395 s / batch. (data: 7.40e-03). ETA=9:34:41, max mem: 20.9 GB 
[11/25 18:21:49 visual_prompt]: 	Training 500/553. train loss: 55.5703,	0.8361 s / batch. (data: 7.97e-03). ETA=9:31:00, max mem: 20.9 GB 
[11/25 18:22:40 visual_prompt]: Epoch 26 / 100: avg data time: 1.52e-01, avg batch time: 0.9845, average train loss: 29.8481
[11/25 18:23:36 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3093, average loss: 8.4448
[11/25 18:23:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.15	
[11/25 18:23:36 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[11/25 18:25:17 visual_prompt]: 	Training 100/553. train loss: 73.5949,	0.8565 s / batch. (data: 2.44e-02). ETA=9:42:42, max mem: 20.9 GB 
[11/25 18:26:53 visual_prompt]: 	Training 200/553. train loss: 44.7822,	0.9610 s / batch. (data: 1.34e-01). ETA=10:52:12, max mem: 20.9 GB 
[11/25 18:28:29 visual_prompt]: 	Training 300/553. train loss: 13.6333,	0.8552 s / batch. (data: 2.11e-02). ETA=9:39:00, max mem: 20.9 GB 
[11/25 18:30:08 visual_prompt]: 	Training 400/553. train loss: 92.7445,	0.8328 s / batch. (data: 7.23e-04). ETA=9:22:28, max mem: 20.9 GB 
[11/25 18:31:44 visual_prompt]: 	Training 500/553. train loss: 17.2910,	0.8594 s / batch. (data: 1.45e-02). ETA=9:38:57, max mem: 20.9 GB 
[11/25 18:32:33 visual_prompt]: Epoch 27 / 100: avg data time: 1.40e-01, avg batch time: 0.9720, average train loss: 31.4765
[11/25 18:33:29 visual_prompt]: Inference (val):avg data time: 1.28e-04, avg batch time: 0.3108, average loss: 15.9482
[11/25 18:33:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.11	
[11/25 18:33:29 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[11/25 18:35:09 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8370 s / batch. (data: 3.18e-04). ETA=9:21:45, max mem: 20.9 GB 
[11/25 18:36:47 visual_prompt]: 	Training 200/553. train loss: 32.3063,	0.8379 s / batch. (data: 1.60e-02). ETA=9:20:55, max mem: 20.9 GB 
[11/25 18:38:26 visual_prompt]: 	Training 300/553. train loss: 14.7498,	1.5160 s / batch. (data: 6.99e-01). ETA=16:52:25, max mem: 20.9 GB 
[11/25 18:40:02 visual_prompt]: 	Training 400/553. train loss: 76.6420,	0.8600 s / batch. (data: 1.20e-02). ETA=9:32:53, max mem: 20.9 GB 
[11/25 18:41:37 visual_prompt]: 	Training 500/553. train loss: 21.1955,	0.8680 s / batch. (data: 3.15e-04). ETA=9:36:45, max mem: 20.9 GB 
[11/25 18:42:29 visual_prompt]: Epoch 28 / 100: avg data time: 1.44e-01, avg batch time: 0.9766, average train loss: 31.9558
[11/25 18:43:25 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3115, average loss: 46.2851
[11/25 18:43:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.27	
[11/25 18:43:25 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[11/25 18:45:13 visual_prompt]: 	Training 100/553. train loss: 59.1458,	0.8312 s / batch. (data: 2.86e-04). ETA=9:10:10, max mem: 20.9 GB 
[11/25 18:46:49 visual_prompt]: 	Training 200/553. train loss: 37.0747,	1.8074 s / batch. (data: 9.56e-01). ETA=19:53:23, max mem: 20.9 GB 
[11/25 18:48:25 visual_prompt]: 	Training 300/553. train loss: 23.9400,	0.8431 s / batch. (data: 1.16e-02). ETA=9:15:15, max mem: 20.9 GB 
[11/25 18:49:58 visual_prompt]: 	Training 400/553. train loss: 9.1341,	0.8437 s / batch. (data: 5.47e-03). ETA=9:14:16, max mem: 20.9 GB 
[11/25 18:51:36 visual_prompt]: 	Training 500/553. train loss: 5.2619,	0.8371 s / batch. (data: 3.24e-04). ETA=9:08:32, max mem: 20.9 GB 
[11/25 18:52:27 visual_prompt]: Epoch 29 / 100: avg data time: 1.48e-01, avg batch time: 0.9799, average train loss: 37.2370
[11/25 18:53:22 visual_prompt]: Inference (val):avg data time: 1.76e-04, avg batch time: 0.3095, average loss: 36.1800
[11/25 18:53:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.64	
[11/25 18:53:22 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[11/25 18:55:02 visual_prompt]: 	Training 100/553. train loss: 57.3054,	0.8231 s / batch. (data: 1.05e-02). ETA=8:57:14, max mem: 20.9 GB 
[11/25 18:56:40 visual_prompt]: 	Training 200/553. train loss: 20.0120,	0.8280 s / batch. (data: 4.86e-04). ETA=8:59:02, max mem: 20.9 GB 
[11/25 18:58:18 visual_prompt]: 	Training 300/553. train loss: 58.9110,	1.8745 s / batch. (data: 1.04e+00). ETA=20:17:17, max mem: 20.9 GB 
[11/25 18:59:56 visual_prompt]: 	Training 400/553. train loss: 32.9440,	1.0800 s / batch. (data: 2.37e-01). ETA=11:39:33, max mem: 20.9 GB 
[11/25 19:01:32 visual_prompt]: 	Training 500/553. train loss: 113.4729,	1.4240 s / batch. (data: 5.94e-01). ETA=15:19:57, max mem: 20.9 GB 
[11/25 19:02:25 visual_prompt]: Epoch 30 / 100: avg data time: 1.49e-01, avg batch time: 0.9808, average train loss: 28.7434
[11/25 19:03:20 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3094, average loss: 22.9904
[11/25 19:03:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.86	
[11/25 19:03:20 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[11/25 19:05:02 visual_prompt]: 	Training 100/553. train loss: 23.2558,	0.8400 s / batch. (data: 3.09e-04). ETA=9:00:32, max mem: 20.9 GB 
[11/25 19:06:42 visual_prompt]: 	Training 200/553. train loss: 109.4730,	0.8120 s / batch. (data: 2.99e-04). ETA=8:41:11, max mem: 20.9 GB 
[11/25 19:08:17 visual_prompt]: 	Training 300/553. train loss: 45.3426,	0.8280 s / batch. (data: 3.16e-04). ETA=8:50:03, max mem: 20.9 GB 
[11/25 19:09:53 visual_prompt]: 	Training 400/553. train loss: 52.4664,	0.8515 s / batch. (data: 5.41e-03). ETA=9:03:41, max mem: 20.9 GB 
[11/25 19:11:31 visual_prompt]: 	Training 500/553. train loss: 31.6548,	0.8437 s / batch. (data: 4.08e-03). ETA=8:57:18, max mem: 20.9 GB 
[11/25 19:12:21 visual_prompt]: Epoch 31 / 100: avg data time: 1.45e-01, avg batch time: 0.9777, average train loss: 26.0752
[11/25 19:13:16 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3111, average loss: 31.8197
[11/25 19:13:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.21	
[11/25 19:13:16 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[11/25 19:14:59 visual_prompt]: 	Training 100/553. train loss: 25.7262,	0.8360 s / batch. (data: 3.02e-04). ETA=8:50:17, max mem: 20.9 GB 
[11/25 19:16:36 visual_prompt]: 	Training 200/553. train loss: 15.7171,	0.8366 s / batch. (data: 8.18e-04). ETA=8:49:14, max mem: 20.9 GB 
[11/25 19:18:16 visual_prompt]: 	Training 300/553. train loss: 92.0406,	0.8238 s / batch. (data: 2.73e-04). ETA=8:39:45, max mem: 20.9 GB 
[11/25 19:19:54 visual_prompt]: 	Training 400/553. train loss: 13.8397,	0.8185 s / batch. (data: 3.45e-04). ETA=8:35:03, max mem: 20.9 GB 
[11/25 19:21:29 visual_prompt]: 	Training 500/553. train loss: 23.7470,	0.8307 s / batch. (data: 5.41e-03). ETA=8:41:21, max mem: 20.9 GB 
[11/25 19:22:18 visual_prompt]: Epoch 32 / 100: avg data time: 1.48e-01, avg batch time: 0.9799, average train loss: 30.6038
[11/25 19:23:14 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3095, average loss: 15.6869
[11/25 19:23:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.55	
[11/25 19:23:14 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[11/25 19:24:53 visual_prompt]: 	Training 100/553. train loss: 30.3722,	1.4107 s / batch. (data: 5.79e-01). ETA=14:41:48, max mem: 20.9 GB 
[11/25 19:26:34 visual_prompt]: 	Training 200/553. train loss: 36.4146,	1.3277 s / batch. (data: 5.20e-01). ETA=13:47:41, max mem: 20.9 GB 
[11/25 19:28:12 visual_prompt]: 	Training 300/553. train loss: 26.4358,	0.8309 s / batch. (data: 5.42e-03). ETA=8:36:34, max mem: 20.9 GB 
[11/25 19:29:52 visual_prompt]: 	Training 400/553. train loss: 7.8397,	0.8400 s / batch. (data: 4.08e-04). ETA=8:40:50, max mem: 20.9 GB 
[11/25 19:31:30 visual_prompt]: 	Training 500/553. train loss: 13.7290,	0.8320 s / batch. (data: 3.21e-04). ETA=8:34:29, max mem: 20.9 GB 
[11/25 19:32:21 visual_prompt]: Epoch 33 / 100: avg data time: 1.59e-01, avg batch time: 0.9901, average train loss: 29.5350
[11/25 19:33:18 visual_prompt]: Inference (val):avg data time: 1.64e-04, avg batch time: 0.3133, average loss: 87.5976
[11/25 19:33:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.27	
[11/25 19:33:18 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[11/25 19:35:01 visual_prompt]: 	Training 100/553. train loss: 15.7223,	0.8177 s / batch. (data: 3.57e-04). ETA=8:23:33, max mem: 20.9 GB 
[11/25 19:36:36 visual_prompt]: 	Training 200/553. train loss: 19.8765,	0.8320 s / batch. (data: 3.19e-04). ETA=8:31:01, max mem: 20.9 GB 
[11/25 19:38:12 visual_prompt]: 	Training 300/553. train loss: 59.4217,	0.8206 s / batch. (data: 7.96e-03). ETA=8:22:39, max mem: 20.9 GB 
[11/25 19:39:51 visual_prompt]: 	Training 400/553. train loss: 37.4711,	0.8466 s / batch. (data: 3.08e-04). ETA=8:37:10, max mem: 20.9 GB 
[11/25 19:41:30 visual_prompt]: 	Training 500/553. train loss: 44.6877,	1.4380 s / batch. (data: 6.33e-01). ETA=14:36:02, max mem: 20.9 GB 
[11/25 19:42:20 visual_prompt]: Epoch 34 / 100: avg data time: 1.48e-01, avg batch time: 0.9800, average train loss: 30.3251
[11/25 19:43:15 visual_prompt]: Inference (val):avg data time: 2.79e-04, avg batch time: 0.3113, average loss: 19.5082
[11/25 19:43:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.82	
[11/25 19:43:15 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[11/25 19:44:59 visual_prompt]: 	Training 100/553. train loss: 28.2258,	0.8178 s / batch. (data: 4.75e-04). ETA=8:16:05, max mem: 20.9 GB 
[11/25 19:46:37 visual_prompt]: 	Training 200/553. train loss: 40.1643,	0.8440 s / batch. (data: 3.41e-04). ETA=8:30:34, max mem: 20.9 GB 
[11/25 19:48:13 visual_prompt]: 	Training 300/553. train loss: 20.8998,	0.8206 s / batch. (data: 3.15e-04). ETA=8:15:04, max mem: 20.9 GB 
[11/25 19:49:48 visual_prompt]: 	Training 400/553. train loss: 16.4035,	0.8201 s / batch. (data: 2.98e-04). ETA=8:13:24, max mem: 20.9 GB 
[11/25 19:51:25 visual_prompt]: 	Training 500/553. train loss: 92.4864,	1.0732 s / batch. (data: 2.47e-01). ETA=10:43:52, max mem: 20.9 GB 
[11/25 19:52:17 visual_prompt]: Epoch 35 / 100: avg data time: 1.47e-01, avg batch time: 0.9788, average train loss: 32.1592
[11/25 19:53:12 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3103, average loss: 29.1534
[11/25 19:53:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.99	
[11/25 19:53:12 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[11/25 19:54:52 visual_prompt]: 	Training 100/553. train loss: 5.5278,	0.8284 s / batch. (data: 1.20e-02). ETA=8:14:53, max mem: 20.9 GB 
[11/25 19:56:31 visual_prompt]: 	Training 200/553. train loss: 61.8225,	0.8395 s / batch. (data: 1.56e-02). ETA=8:20:08, max mem: 20.9 GB 
[11/25 19:58:09 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8394 s / batch. (data: 7.40e-03). ETA=8:18:42, max mem: 20.9 GB 
[11/25 19:59:46 visual_prompt]: 	Training 400/553. train loss: 26.8228,	0.8399 s / batch. (data: 3.14e-04). ETA=8:17:35, max mem: 20.9 GB 
[11/25 20:01:23 visual_prompt]: 	Training 500/553. train loss: 8.0681,	0.9135 s / batch. (data: 5.65e-02). ETA=8:59:38, max mem: 20.9 GB 
[11/25 20:02:12 visual_prompt]: Epoch 36 / 100: avg data time: 1.43e-01, avg batch time: 0.9755, average train loss: 27.7423
[11/25 20:03:07 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3111, average loss: 88.0162
[11/25 20:03:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.70	
[11/25 20:03:07 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[11/25 20:04:48 visual_prompt]: 	Training 100/553. train loss: 7.0561,	0.8175 s / batch. (data: 3.32e-04). ETA=8:00:52, max mem: 20.9 GB 
[11/25 20:06:25 visual_prompt]: 	Training 200/553. train loss: 14.3619,	0.8360 s / batch. (data: 3.18e-04). ETA=8:10:20, max mem: 20.9 GB 
[11/25 20:08:02 visual_prompt]: 	Training 300/553. train loss: 26.8646,	1.3281 s / batch. (data: 4.95e-01). ETA=12:56:44, max mem: 20.9 GB 
[11/25 20:09:42 visual_prompt]: 	Training 400/553. train loss: 46.8616,	1.8938 s / batch. (data: 1.07e+00). ETA=18:24:26, max mem: 20.9 GB 
[11/25 20:11:15 visual_prompt]: 	Training 500/553. train loss: 29.4414,	0.9584 s / batch. (data: 1.21e-01). ETA=9:17:19, max mem: 20.9 GB 
[11/25 20:12:08 visual_prompt]: Epoch 37 / 100: avg data time: 1.45e-01, avg batch time: 0.9778, average train loss: 24.0381
[11/25 20:13:03 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3103, average loss: 10.0697
[11/25 20:13:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.56	
[11/25 20:13:03 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[11/25 20:14:43 visual_prompt]: 	Training 100/553. train loss: 12.8534,	0.8432 s / batch. (data: 5.44e-03). ETA=8:08:10, max mem: 20.9 GB 
[11/25 20:16:22 visual_prompt]: 	Training 200/553. train loss: 10.3631,	1.3982 s / batch. (data: 5.78e-01). ETA=13:27:12, max mem: 20.9 GB 
[11/25 20:18:01 visual_prompt]: 	Training 300/553. train loss: 28.5636,	0.8127 s / batch. (data: 2.98e-04). ETA=7:47:49, max mem: 20.9 GB 
[11/25 20:19:35 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8360 s / batch. (data: 3.13e-04). ETA=7:59:51, max mem: 20.9 GB 
[11/25 20:21:15 visual_prompt]: 	Training 500/553. train loss: 4.4356,	0.8336 s / batch. (data: 3.34e-04). ETA=7:57:04, max mem: 20.9 GB 
[11/25 20:22:04 visual_prompt]: Epoch 38 / 100: avg data time: 1.46e-01, avg batch time: 0.9781, average train loss: 25.6685
[11/25 20:23:00 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3110, average loss: 5.1620
[11/25 20:23:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 53.38	
[11/25 20:23:00 visual_prompt]: Best epoch 38: best metric: -5.162
[11/25 20:23:00 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[11/25 20:24:39 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8480 s / batch. (data: 3.11e-04). ETA=8:03:09, max mem: 20.9 GB 
[11/25 20:26:20 visual_prompt]: 	Training 200/553. train loss: 40.6512,	0.8240 s / batch. (data: 3.06e-04). ETA=7:48:08, max mem: 20.9 GB 
[11/25 20:28:00 visual_prompt]: 	Training 300/553. train loss: 23.4809,	0.8280 s / batch. (data: 2.78e-04). ETA=7:49:01, max mem: 20.9 GB 
[11/25 20:29:35 visual_prompt]: 	Training 400/553. train loss: 0.3322,	0.8270 s / batch. (data: 3.27e-04). ETA=7:47:05, max mem: 20.9 GB 
[11/25 20:31:13 visual_prompt]: 	Training 500/553. train loss: 21.3553,	1.6247 s / batch. (data: 7.81e-01). ETA=15:14:50, max mem: 20.9 GB 
[11/25 20:32:01 visual_prompt]: Epoch 39 / 100: avg data time: 1.46e-01, avg batch time: 0.9789, average train loss: 22.3797
[11/25 20:32:56 visual_prompt]: Inference (val):avg data time: 1.65e-04, avg batch time: 0.3103, average loss: 9.7386
[11/25 20:32:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.07	
[11/25 20:32:56 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[11/25 20:34:38 visual_prompt]: 	Training 100/553. train loss: 39.0284,	0.8264 s / batch. (data: 2.98e-04). ETA=7:43:15, max mem: 20.9 GB 
[11/25 20:36:15 visual_prompt]: 	Training 200/553. train loss: 44.7136,	0.8470 s / batch. (data: 1.05e-02). ETA=7:53:24, max mem: 20.9 GB 
[11/25 20:37:53 visual_prompt]: 	Training 300/553. train loss: 1.4765,	0.8563 s / batch. (data: 1.60e-02). ETA=7:57:09, max mem: 20.9 GB 
[11/25 20:39:31 visual_prompt]: 	Training 400/553. train loss: 12.7666,	0.8334 s / batch. (data: 3.02e-04). ETA=7:43:00, max mem: 20.9 GB 
[11/25 20:41:07 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8182 s / batch. (data: 3.01e-04). ETA=7:33:12, max mem: 20.9 GB 
[11/25 20:42:00 visual_prompt]: Epoch 40 / 100: avg data time: 1.50e-01, avg batch time: 0.9822, average train loss: 26.6628
[11/25 20:42:56 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3104, average loss: 21.7155
[11/25 20:42:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 39.32	
[11/25 20:42:56 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[11/25 20:44:43 visual_prompt]: 	Training 100/553. train loss: 26.5522,	0.8399 s / batch. (data: 5.25e-04). ETA=7:43:03, max mem: 20.9 GB 
[11/25 20:46:29 visual_prompt]: 	Training 200/553. train loss: 25.0975,	0.8343 s / batch. (data: 1.60e-02). ETA=7:38:33, max mem: 20.9 GB 
[11/25 20:48:06 visual_prompt]: 	Training 300/553. train loss: 8.7110,	0.8389 s / batch. (data: 3.16e-04). ETA=7:39:41, max mem: 20.9 GB 
[11/25 20:49:43 visual_prompt]: 	Training 400/553. train loss: 24.0848,	0.8196 s / batch. (data: 2.99e-04). ETA=7:27:47, max mem: 20.9 GB 
[11/25 20:51:20 visual_prompt]: 	Training 500/553. train loss: 2.9224,	0.8498 s / batch. (data: 1.12e-03). ETA=7:42:52, max mem: 20.9 GB 
[11/25 20:52:09 visual_prompt]: Epoch 41 / 100: avg data time: 1.68e-01, avg batch time: 0.9997, average train loss: 26.3409
[11/25 20:53:05 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3092, average loss: 32.4843
[11/25 20:53:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.24	
[11/25 20:53:05 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[11/25 20:54:45 visual_prompt]: 	Training 100/553. train loss: 15.6477,	0.8328 s / batch. (data: 5.42e-03). ETA=7:31:28, max mem: 20.9 GB 
[11/25 20:56:22 visual_prompt]: 	Training 200/553. train loss: 18.8049,	0.8323 s / batch. (data: 5.44e-03). ETA=7:29:47, max mem: 20.9 GB 
[11/25 20:58:01 visual_prompt]: 	Training 300/553. train loss: 6.5058,	0.8440 s / batch. (data: 3.10e-04). ETA=7:34:44, max mem: 20.9 GB 
[11/25 20:59:38 visual_prompt]: 	Training 400/553. train loss: 17.2064,	0.8598 s / batch. (data: 1.98e-02). ETA=7:41:48, max mem: 20.9 GB 
[11/25 21:01:14 visual_prompt]: 	Training 500/553. train loss: 91.9209,	0.8320 s / batch. (data: 7.95e-03). ETA=7:25:29, max mem: 20.9 GB 
[11/25 21:02:06 visual_prompt]: Epoch 42 / 100: avg data time: 1.46e-01, avg batch time: 0.9785, average train loss: 24.4209
[11/25 21:03:02 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3107, average loss: 36.9176
[11/25 21:03:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.04	
[11/25 21:03:02 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[11/25 21:04:45 visual_prompt]: 	Training 100/553. train loss: 7.3888,	0.8246 s / batch. (data: 8.58e-03). ETA=7:19:25, max mem: 20.9 GB 
[11/25 21:06:21 visual_prompt]: 	Training 200/553. train loss: 115.5747,	0.8251 s / batch. (data: 3.29e-04). ETA=7:18:19, max mem: 20.9 GB 
[11/25 21:07:57 visual_prompt]: 	Training 300/553. train loss: 42.1443,	0.8374 s / batch. (data: 7.95e-03). ETA=7:23:26, max mem: 20.9 GB 
[11/25 21:09:32 visual_prompt]: 	Training 400/553. train loss: 59.4686,	0.8480 s / batch. (data: 7.95e-03). ETA=7:27:39, max mem: 20.9 GB 
[11/25 21:11:11 visual_prompt]: 	Training 500/553. train loss: 38.3393,	0.8320 s / batch. (data: 3.26e-04). ETA=7:17:49, max mem: 20.9 GB 
[11/25 21:12:04 visual_prompt]: Epoch 43 / 100: avg data time: 1.48e-01, avg batch time: 0.9803, average train loss: 26.1458
[11/25 21:13:00 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3100, average loss: 18.0466
[11/25 21:13:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.67	
[11/25 21:13:00 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[11/25 21:14:40 visual_prompt]: 	Training 100/553. train loss: 41.6996,	1.3181 s / batch. (data: 4.89e-01). ETA=11:30:15, max mem: 20.9 GB 
[11/25 21:16:19 visual_prompt]: 	Training 200/553. train loss: 24.9968,	0.8227 s / batch. (data: 3.00e-04). ETA=7:09:26, max mem: 20.9 GB 
[11/25 21:17:54 visual_prompt]: 	Training 300/553. train loss: 40.6746,	0.8240 s / batch. (data: 3.18e-04). ETA=7:08:44, max mem: 20.9 GB 
[11/25 21:19:30 visual_prompt]: 	Training 400/553. train loss: 50.0597,	0.8699 s / batch. (data: 1.56e-02). ETA=7:31:12, max mem: 20.9 GB 
[11/25 21:21:07 visual_prompt]: 	Training 500/553. train loss: 11.9130,	0.8212 s / batch. (data: 3.24e-04). ETA=7:04:34, max mem: 20.9 GB 
[11/25 21:21:59 visual_prompt]: Epoch 44 / 100: avg data time: 1.42e-01, avg batch time: 0.9748, average train loss: 26.5415
[11/25 21:22:54 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3099, average loss: 106.2100
[11/25 21:22:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.68	
[11/25 21:22:54 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[11/25 21:24:37 visual_prompt]: 	Training 100/553. train loss: 18.1458,	0.8554 s / batch. (data: 2.02e-02). ETA=7:20:05, max mem: 20.9 GB 
[11/25 21:26:11 visual_prompt]: 	Training 200/553. train loss: 2.2040,	1.2249 s / batch. (data: 3.99e-01). ETA=10:28:08, max mem: 20.9 GB 
[11/25 21:27:51 visual_prompt]: 	Training 300/553. train loss: 35.3088,	0.8341 s / batch. (data: 8.24e-04). ETA=7:06:20, max mem: 20.9 GB 
[11/25 21:29:25 visual_prompt]: 	Training 400/553. train loss: 11.2677,	0.8382 s / batch. (data: 1.20e-02). ETA=7:07:01, max mem: 20.9 GB 
[11/25 21:31:05 visual_prompt]: 	Training 500/553. train loss: 35.0762,	0.8322 s / batch. (data: 5.44e-03). ETA=7:02:35, max mem: 20.9 GB 
[11/25 21:31:56 visual_prompt]: Epoch 45 / 100: avg data time: 1.46e-01, avg batch time: 0.9787, average train loss: 21.2374
[11/25 21:32:51 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3115, average loss: 8.9792
[11/25 21:32:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.90	
[11/25 21:32:51 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[11/25 21:34:32 visual_prompt]: 	Training 100/553. train loss: 0.5297,	1.1732 s / batch. (data: 3.26e-01). ETA=9:52:47, max mem: 20.9 GB 
[11/25 21:36:11 visual_prompt]: 	Training 200/553. train loss: 2.9668,	0.8375 s / batch. (data: 7.79e-04). ETA=7:01:43, max mem: 20.9 GB 
[11/25 21:37:47 visual_prompt]: 	Training 300/553. train loss: 76.9602,	0.8520 s / batch. (data: 5.47e-03). ETA=7:07:37, max mem: 20.9 GB 
[11/25 21:39:25 visual_prompt]: 	Training 400/553. train loss: 27.3665,	0.8468 s / batch. (data: 7.76e-04). ETA=7:03:36, max mem: 20.9 GB 
[11/25 21:40:58 visual_prompt]: 	Training 500/553. train loss: 0.6555,	0.8285 s / batch. (data: 2.86e-04). ETA=6:53:03, max mem: 20.9 GB 
[11/25 21:41:51 visual_prompt]: Epoch 46 / 100: avg data time: 1.44e-01, avg batch time: 0.9759, average train loss: 24.1686
[11/25 21:42:45 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3112, average loss: 13.5975
[11/25 21:42:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.89	
[11/25 21:42:45 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[11/25 21:44:27 visual_prompt]: 	Training 100/553. train loss: 10.5285,	0.8400 s / batch. (data: 4.10e-04). ETA=6:56:38, max mem: 20.9 GB 
[11/25 21:46:02 visual_prompt]: 	Training 200/553. train loss: 78.0075,	1.2420 s / batch. (data: 4.28e-01). ETA=10:13:58, max mem: 20.9 GB 
[11/25 21:47:40 visual_prompt]: 	Training 300/553. train loss: 5.8748,	0.8179 s / batch. (data: 3.35e-04). ETA=6:42:58, max mem: 20.9 GB 
[11/25 21:49:17 visual_prompt]: 	Training 400/553. train loss: 41.5618,	0.8148 s / batch. (data: 3.11e-04). ETA=6:40:05, max mem: 20.9 GB 
[11/25 21:50:53 visual_prompt]: 	Training 500/553. train loss: 12.8149,	0.8476 s / batch. (data: 3.29e-04). ETA=6:54:45, max mem: 20.9 GB 
[11/25 21:51:46 visual_prompt]: Epoch 47 / 100: avg data time: 1.44e-01, avg batch time: 0.9769, average train loss: 22.0154
[11/25 21:52:41 visual_prompt]: Inference (val):avg data time: 4.83e-05, avg batch time: 0.3092, average loss: 19.2693
[11/25 21:52:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.64	
[11/25 21:52:41 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[11/25 21:54:23 visual_prompt]: 	Training 100/553. train loss: 67.7261,	0.8496 s / batch. (data: 2.06e-02). ETA=6:53:37, max mem: 20.9 GB 
[11/25 21:56:01 visual_prompt]: 	Training 200/553. train loss: 3.7868,	0.8244 s / batch. (data: 3.32e-04). ETA=6:39:56, max mem: 20.9 GB 
[11/25 21:57:40 visual_prompt]: 	Training 300/553. train loss: 0.6299,	1.5837 s / batch. (data: 7.52e-01). ETA=12:45:42, max mem: 20.9 GB 
[11/25 21:59:13 visual_prompt]: 	Training 400/553. train loss: 0.0083,	0.8280 s / batch. (data: 3.18e-04). ETA=6:38:56, max mem: 20.9 GB 
[11/25 22:00:50 visual_prompt]: 	Training 500/553. train loss: 20.4112,	0.8260 s / batch. (data: 5.46e-03). ETA=6:36:35, max mem: 20.9 GB 
[11/25 22:01:40 visual_prompt]: Epoch 48 / 100: avg data time: 1.41e-01, avg batch time: 0.9743, average train loss: 20.5710
[11/25 22:02:35 visual_prompt]: Inference (val):avg data time: 4.71e-04, avg batch time: 0.3106, average loss: 63.7910
[11/25 22:02:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.25	
[11/25 22:02:35 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[11/25 22:04:16 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8207 s / batch. (data: 3.07e-04). ETA=6:31:58, max mem: 20.9 GB 
[11/25 22:05:52 visual_prompt]: 	Training 200/553. train loss: 20.9715,	0.8280 s / batch. (data: 2.95e-04). ETA=6:34:04, max mem: 20.9 GB 
[11/25 22:07:30 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8362 s / batch. (data: 3.29e-04). ETA=6:36:34, max mem: 20.9 GB 
[11/25 22:09:08 visual_prompt]: 	Training 400/553. train loss: 2.9353,	0.8531 s / batch. (data: 1.30e-02). ETA=6:43:09, max mem: 20.9 GB 
[11/25 22:10:45 visual_prompt]: 	Training 500/553. train loss: 14.3700,	0.8174 s / batch. (data: 2.64e-04). ETA=6:24:57, max mem: 20.9 GB 
[11/25 22:11:37 visual_prompt]: Epoch 49 / 100: avg data time: 1.46e-01, avg batch time: 0.9788, average train loss: 21.5894
[11/25 22:12:32 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3095, average loss: 18.1432
[11/25 22:12:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.55	
[11/25 22:12:32 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[11/25 22:14:15 visual_prompt]: 	Training 100/553. train loss: 13.4888,	0.8278 s / batch. (data: 2.87e-04). ETA=6:27:44, max mem: 20.9 GB 
[11/25 22:15:53 visual_prompt]: 	Training 200/553. train loss: 18.3344,	0.8311 s / batch. (data: 3.28e-04). ETA=6:27:52, max mem: 20.9 GB 
[11/25 22:17:29 visual_prompt]: 	Training 300/553. train loss: 32.4627,	0.8239 s / batch. (data: 3.19e-04). ETA=6:23:09, max mem: 20.9 GB 
[11/25 22:19:04 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8280 s / batch. (data: 7.95e-03). ETA=6:23:41, max mem: 20.9 GB 
[11/25 22:20:41 visual_prompt]: 	Training 500/553. train loss: 30.3753,	0.8520 s / batch. (data: 3.14e-04). ETA=6:33:23, max mem: 20.9 GB 
[11/25 22:21:32 visual_prompt]: Epoch 50 / 100: avg data time: 1.44e-01, avg batch time: 0.9762, average train loss: 20.2678
[11/25 22:22:27 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3109, average loss: 19.8499
[11/25 22:22:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.79	
[11/25 22:22:27 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[11/25 22:24:08 visual_prompt]: 	Training 100/553. train loss: 18.9532,	1.0145 s / batch. (data: 1.64e-01). ETA=7:45:50, max mem: 20.9 GB 
[11/25 22:25:45 visual_prompt]: 	Training 200/553. train loss: 28.8640,	0.8203 s / batch. (data: 3.32e-04). ETA=6:15:17, max mem: 20.9 GB 
[11/25 22:27:23 visual_prompt]: 	Training 300/553. train loss: 2.2775,	0.8280 s / batch. (data: 3.25e-04). ETA=6:17:26, max mem: 20.9 GB 
[11/25 22:29:02 visual_prompt]: 	Training 400/553. train loss: 2.9341,	1.2972 s / batch. (data: 4.73e-01). ETA=9:49:07, max mem: 20.9 GB 
[11/25 22:30:38 visual_prompt]: 	Training 500/553. train loss: 17.1177,	0.8199 s / batch. (data: 2.68e-04). ETA=6:11:00, max mem: 20.9 GB 
[11/25 22:31:27 visual_prompt]: Epoch 51 / 100: avg data time: 1.44e-01, avg batch time: 0.9768, average train loss: 19.0705
[11/25 22:32:22 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3093, average loss: 5.8342
[11/25 22:32:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.24	
[11/25 22:32:22 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[11/25 22:34:07 visual_prompt]: 	Training 100/553. train loss: 12.4204,	0.8361 s / batch. (data: 5.93e-03). ETA=6:16:11, max mem: 20.9 GB 
[11/25 22:35:43 visual_prompt]: 	Training 200/553. train loss: 8.5228,	0.8480 s / batch. (data: 2.93e-04). ETA=6:20:08, max mem: 20.9 GB 
[11/25 22:37:20 visual_prompt]: 	Training 300/553. train loss: 41.9723,	0.8276 s / batch. (data: 3.14e-04). ETA=6:09:36, max mem: 20.9 GB 
[11/25 22:38:58 visual_prompt]: 	Training 400/553. train loss: 88.8701,	0.8520 s / batch. (data: 7.95e-03). ETA=6:19:05, max mem: 20.9 GB 
[11/25 22:40:30 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8366 s / batch. (data: 5.49e-03). ETA=6:10:50, max mem: 20.9 GB 
[11/25 22:41:20 visual_prompt]: Epoch 52 / 100: avg data time: 1.38e-01, avg batch time: 0.9714, average train loss: 21.3370
[11/25 22:42:15 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3091, average loss: 3.1040
[11/25 22:42:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.22	
[11/25 22:42:15 visual_prompt]: Best epoch 52: best metric: -3.104
[11/25 22:42:15 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[11/25 22:43:56 visual_prompt]: 	Training 100/553. train loss: 34.9381,	0.8234 s / batch. (data: 2.73e-04). ETA=6:02:52, max mem: 20.9 GB 
[11/25 22:45:34 visual_prompt]: 	Training 200/553. train loss: 59.1264,	0.8534 s / batch. (data: 7.70e-04). ETA=6:14:42, max mem: 20.9 GB 
[11/25 22:47:10 visual_prompt]: 	Training 300/553. train loss: 30.7643,	0.8520 s / batch. (data: 7.73e-04). ETA=6:12:39, max mem: 20.9 GB 
[11/25 22:48:50 visual_prompt]: 	Training 400/553. train loss: 6.2285,	0.8633 s / batch. (data: 2.13e-02). ETA=6:16:10, max mem: 20.9 GB 
[11/25 22:50:26 visual_prompt]: 	Training 500/553. train loss: 28.4080,	0.8197 s / batch. (data: 3.02e-04). ETA=5:55:47, max mem: 20.9 GB 
[11/25 22:51:18 visual_prompt]: Epoch 53 / 100: avg data time: 1.49e-01, avg batch time: 0.9818, average train loss: 19.9654
[11/25 22:52:13 visual_prompt]: Inference (val):avg data time: 1.34e-04, avg batch time: 0.3108, average loss: 10.2193
[11/25 22:52:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.50	
[11/25 22:52:13 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[11/25 22:53:56 visual_prompt]: 	Training 100/553. train loss: 0.3219,	0.8280 s / batch. (data: 3.06e-04). ETA=5:57:17, max mem: 20.9 GB 
[11/25 22:55:33 visual_prompt]: 	Training 200/553. train loss: 6.3546,	0.8556 s / batch. (data: 5.43e-03). ETA=6:07:46, max mem: 20.9 GB 
[11/25 22:57:09 visual_prompt]: 	Training 300/553. train loss: 41.7299,	0.8196 s / batch. (data: 3.23e-04). ETA=5:50:55, max mem: 20.9 GB 
[11/25 22:58:45 visual_prompt]: 	Training 400/553. train loss: 34.6027,	0.8356 s / batch. (data: 1.05e-02). ETA=5:56:24, max mem: 20.9 GB 
[11/25 23:00:24 visual_prompt]: 	Training 500/553. train loss: 15.0122,	0.8326 s / batch. (data: 3.30e-04). ETA=5:53:44, max mem: 20.9 GB 
[11/25 23:01:14 visual_prompt]: Epoch 54 / 100: avg data time: 1.45e-01, avg batch time: 0.9788, average train loss: 20.5318
[11/25 23:02:10 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3120, average loss: 55.9242
[11/25 23:02:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.39	
[11/25 23:02:10 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[11/25 23:03:51 visual_prompt]: 	Training 100/553. train loss: 1.7116,	0.8280 s / batch. (data: 5.44e-03). ETA=5:49:39, max mem: 20.9 GB 
[11/25 23:05:27 visual_prompt]: 	Training 200/553. train loss: 6.7196,	0.8328 s / batch. (data: 1.06e-02). ETA=5:50:18, max mem: 20.9 GB 
[11/25 23:07:05 visual_prompt]: 	Training 300/553. train loss: 17.1665,	0.8610 s / batch. (data: 2.39e-02). ETA=6:00:44, max mem: 20.9 GB 
[11/25 23:08:42 visual_prompt]: 	Training 400/553. train loss: 3.7559,	0.8638 s / batch. (data: 3.09e-04). ETA=6:00:27, max mem: 20.9 GB 
[11/25 23:10:19 visual_prompt]: 	Training 500/553. train loss: 7.0962,	1.1773 s / batch. (data: 3.48e-01). ETA=8:09:19, max mem: 20.9 GB 
[11/25 23:11:10 visual_prompt]: Epoch 55 / 100: avg data time: 1.44e-01, avg batch time: 0.9767, average train loss: 18.2025
[11/25 23:12:05 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3092, average loss: 2.4929
[11/25 23:12:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.29	
[11/25 23:12:05 visual_prompt]: Best epoch 55: best metric: -2.493
[11/25 23:12:05 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[11/25 23:13:47 visual_prompt]: 	Training 100/553. train loss: 0.9013,	0.8451 s / batch. (data: 1.56e-02). ETA=5:49:05, max mem: 20.9 GB 
[11/25 23:15:23 visual_prompt]: 	Training 200/553. train loss: 2.3837,	0.8554 s / batch. (data: 2.26e-02). ETA=5:51:56, max mem: 20.9 GB 
[11/25 23:17:01 visual_prompt]: 	Training 300/553. train loss: 17.0712,	0.8290 s / batch. (data: 3.29e-04). ETA=5:39:40, max mem: 20.9 GB 
[11/25 23:18:39 visual_prompt]: 	Training 400/553. train loss: 7.0236,	0.8480 s / batch. (data: 3.18e-04). ETA=5:46:02, max mem: 20.9 GB 
[11/25 23:20:16 visual_prompt]: 	Training 500/553. train loss: 19.5637,	2.0555 s / batch. (data: 1.23e+00). ETA=13:55:23, max mem: 20.9 GB 
[11/25 23:21:05 visual_prompt]: Epoch 56 / 100: avg data time: 1.43e-01, avg batch time: 0.9762, average train loss: 15.3884
[11/25 23:22:01 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3106, average loss: 25.3055
[11/25 23:22:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.47	
[11/25 23:22:01 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[11/25 23:23:45 visual_prompt]: 	Training 100/553. train loss: 15.1756,	0.8252 s / batch. (data: 3.18e-04). ETA=5:33:17, max mem: 20.9 GB 
[11/25 23:25:21 visual_prompt]: 	Training 200/553. train loss: 45.6182,	0.8240 s / batch. (data: 3.42e-04). ETA=5:31:24, max mem: 20.9 GB 
[11/25 23:26:58 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8320 s / batch. (data: 7.95e-03). ETA=5:33:14, max mem: 20.9 GB 
[11/25 23:28:34 visual_prompt]: 	Training 400/553. train loss: 7.1038,	0.8331 s / batch. (data: 6.27e-04). ETA=5:32:18, max mem: 20.9 GB 
[11/25 23:30:09 visual_prompt]: 	Training 500/553. train loss: 67.1635,	0.8362 s / batch. (data: 5.45e-03). ETA=5:32:07, max mem: 20.9 GB 
[11/25 23:31:00 visual_prompt]: Epoch 57 / 100: avg data time: 1.42e-01, avg batch time: 0.9756, average train loss: 15.5547
[11/25 23:31:56 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3100, average loss: 5.7535
[11/25 23:31:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.17	
[11/25 23:31:56 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[11/25 23:33:36 visual_prompt]: 	Training 100/553. train loss: 7.4500,	0.8245 s / batch. (data: 3.09e-04). ETA=5:25:23, max mem: 20.9 GB 
[11/25 23:35:13 visual_prompt]: 	Training 200/553. train loss: 6.8628,	0.8557 s / batch. (data: 1.17e-02). ETA=5:36:16, max mem: 20.9 GB 
[11/25 23:36:52 visual_prompt]: 	Training 300/553. train loss: 24.6648,	0.8191 s / batch. (data: 3.17e-04). ETA=5:20:31, max mem: 20.9 GB 
[11/25 23:38:29 visual_prompt]: 	Training 400/553. train loss: 8.5359,	0.8436 s / batch. (data: 3.12e-04). ETA=5:28:41, max mem: 20.9 GB 
[11/25 23:40:05 visual_prompt]: 	Training 500/553. train loss: 185.6109,	0.8292 s / batch. (data: 3.22e-04). ETA=5:21:43, max mem: 20.9 GB 
[11/25 23:40:55 visual_prompt]: Epoch 58 / 100: avg data time: 1.41e-01, avg batch time: 0.9742, average train loss: 17.6176
[11/25 23:41:50 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3096, average loss: 7.0374
[11/25 23:41:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.23	
[11/25 23:41:50 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[11/25 23:43:33 visual_prompt]: 	Training 100/553. train loss: 1.1424,	0.8355 s / batch. (data: 7.96e-04). ETA=5:22:01, max mem: 20.9 GB 
[11/25 23:45:11 visual_prompt]: 	Training 200/553. train loss: 5.8374,	0.8483 s / batch. (data: 5.43e-03). ETA=5:25:33, max mem: 20.9 GB 
[11/25 23:46:47 visual_prompt]: 	Training 300/553. train loss: 27.1948,	0.8397 s / batch. (data: 3.30e-04). ETA=5:20:51, max mem: 20.9 GB 
[11/25 23:48:23 visual_prompt]: 	Training 400/553. train loss: 11.9470,	0.8334 s / batch. (data: 3.17e-04). ETA=5:17:03, max mem: 20.9 GB 
[11/25 23:50:02 visual_prompt]: 	Training 500/553. train loss: 6.1322,	0.8427 s / batch. (data: 8.41e-04). ETA=5:19:12, max mem: 20.9 GB 
[11/25 23:50:52 visual_prompt]: Epoch 59 / 100: avg data time: 1.46e-01, avg batch time: 0.9797, average train loss: 17.8184
[11/25 23:51:47 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3113, average loss: 3.6967
[11/25 23:51:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.48	
[11/25 23:51:47 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[11/25 23:53:29 visual_prompt]: 	Training 100/553. train loss: 2.9260,	0.8221 s / batch. (data: 5.55e-03). ETA=5:09:16, max mem: 20.9 GB 
[11/25 23:55:06 visual_prompt]: 	Training 200/553. train loss: 42.8757,	0.8314 s / batch. (data: 7.30e-03). ETA=5:11:23, max mem: 20.9 GB 
[11/25 23:56:42 visual_prompt]: 	Training 300/553. train loss: 38.8731,	2.2961 s / batch. (data: 1.47e+00). ETA=14:16:10, max mem: 20.9 GB 
[11/25 23:58:20 visual_prompt]: 	Training 400/553. train loss: 0.8077,	1.0080 s / batch. (data: 1.52e-01). ETA=6:14:11, max mem: 20.9 GB 
[11/25 23:59:58 visual_prompt]: 	Training 500/553. train loss: 3.3147,	0.8483 s / batch. (data: 3.62e-04). ETA=5:13:28, max mem: 20.9 GB 
[11/26 00:00:49 visual_prompt]: Epoch 60 / 100: avg data time: 1.47e-01, avg batch time: 0.9796, average train loss: 15.8165
[11/26 00:01:44 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3096, average loss: 15.3824
[11/26 00:01:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.05	
[11/26 00:01:44 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[11/26 00:03:26 visual_prompt]: 	Training 100/553. train loss: 29.5074,	0.8351 s / batch. (data: 1.20e-02). ETA=5:06:29, max mem: 20.9 GB 
[11/26 00:05:04 visual_prompt]: 	Training 200/553. train loss: 20.0876,	0.8476 s / batch. (data: 9.24e-03). ETA=5:09:39, max mem: 20.9 GB 
[11/26 00:06:42 visual_prompt]: 	Training 300/553. train loss: 8.4036,	0.8440 s / batch. (data: 5.43e-03). ETA=5:06:55, max mem: 20.9 GB 
[11/26 00:08:17 visual_prompt]: 	Training 400/553. train loss: 16.5174,	0.8427 s / batch. (data: 5.41e-03). ETA=5:05:04, max mem: 20.9 GB 
[11/26 00:09:54 visual_prompt]: 	Training 500/553. train loss: 34.2611,	1.9835 s / batch. (data: 1.15e+00). ETA=11:54:44, max mem: 20.9 GB 
[11/26 00:10:44 visual_prompt]: Epoch 61 / 100: avg data time: 1.42e-01, avg batch time: 0.9747, average train loss: 15.8167
[11/26 00:11:39 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3102, average loss: 10.9515
[11/26 00:11:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.38	
[11/26 00:11:39 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[11/26 00:13:20 visual_prompt]: 	Training 100/553. train loss: 22.4552,	0.8440 s / batch. (data: 3.04e-04). ETA=5:01:58, max mem: 20.9 GB 
[11/26 00:14:58 visual_prompt]: 	Training 200/553. train loss: 10.1873,	0.8335 s / batch. (data: 1.11e-02). ETA=4:56:48, max mem: 20.9 GB 
[11/26 00:16:33 visual_prompt]: 	Training 300/553. train loss: 27.1875,	0.8962 s / batch. (data: 3.62e-02). ETA=5:17:39, max mem: 20.9 GB 
[11/26 00:18:11 visual_prompt]: 	Training 400/553. train loss: 9.0329,	0.8243 s / batch. (data: 2.97e-04). ETA=4:50:47, max mem: 20.9 GB 
[11/26 00:19:45 visual_prompt]: 	Training 500/553. train loss: 14.9380,	0.8360 s / batch. (data: 3.14e-04). ETA=4:53:32, max mem: 20.9 GB 
[11/26 00:20:39 visual_prompt]: Epoch 62 / 100: avg data time: 1.43e-01, avg batch time: 0.9760, average train loss: 13.4324
[11/26 00:21:35 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3096, average loss: 13.0417
[11/26 00:21:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.03	
[11/26 00:21:35 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[11/26 00:23:19 visual_prompt]: 	Training 100/553. train loss: 2.1797,	0.8271 s / batch. (data: 3.20e-04). ETA=4:48:18, max mem: 20.9 GB 
[11/26 00:25:00 visual_prompt]: 	Training 200/553. train loss: 5.8354,	0.8320 s / batch. (data: 2.88e-04). ETA=4:48:37, max mem: 20.9 GB 
[11/26 00:26:35 visual_prompt]: 	Training 300/553. train loss: 2.0536,	0.8400 s / batch. (data: 3.62e-04). ETA=4:49:59, max mem: 20.9 GB 
[11/26 00:28:08 visual_prompt]: 	Training 400/553. train loss: 11.0377,	0.8251 s / batch. (data: 7.95e-03). ETA=4:43:28, max mem: 20.9 GB 
[11/26 00:29:43 visual_prompt]: 	Training 500/553. train loss: 7.3771,	0.8466 s / batch. (data: 1.88e-02). ETA=4:49:27, max mem: 20.9 GB 
[11/26 00:30:32 visual_prompt]: Epoch 63 / 100: avg data time: 1.38e-01, avg batch time: 0.9719, average train loss: 12.5359
[11/26 00:31:28 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3106, average loss: 14.8224
[11/26 00:31:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.12	
[11/26 00:31:28 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[11/26 00:33:11 visual_prompt]: 	Training 100/553. train loss: 4.9937,	0.8520 s / batch. (data: 7.71e-04). ETA=4:49:07, max mem: 20.9 GB 
[11/26 00:34:50 visual_prompt]: 	Training 200/553. train loss: 66.3867,	0.8124 s / batch. (data: 3.15e-04). ETA=4:34:20, max mem: 20.9 GB 
[11/26 00:36:24 visual_prompt]: 	Training 300/553. train loss: 8.0966,	0.8320 s / batch. (data: 7.97e-03). ETA=4:39:33, max mem: 20.9 GB 
[11/26 00:38:00 visual_prompt]: 	Training 400/553. train loss: 9.2367,	0.8920 s / batch. (data: 5.96e-02). ETA=4:58:14, max mem: 20.9 GB 
[11/26 00:39:38 visual_prompt]: 	Training 500/553. train loss: 31.7402,	0.8258 s / batch. (data: 7.56e-03). ETA=4:34:43, max mem: 20.9 GB 
[11/26 00:40:28 visual_prompt]: Epoch 64 / 100: avg data time: 1.44e-01, avg batch time: 0.9777, average train loss: 12.2981
[11/26 00:41:24 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3096, average loss: 4.9562
[11/26 00:41:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.95	
[11/26 00:41:24 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[11/26 00:43:08 visual_prompt]: 	Training 100/553. train loss: 8.1335,	1.1200 s / batch. (data: 2.72e-01). ETA=6:09:44, max mem: 20.9 GB 
[11/26 00:44:46 visual_prompt]: 	Training 200/553. train loss: 6.1695,	0.8287 s / batch. (data: 3.30e-04). ETA=4:32:11, max mem: 20.9 GB 
[11/26 00:46:21 visual_prompt]: 	Training 300/553. train loss: 17.0906,	0.8257 s / batch. (data: 2.90e-04). ETA=4:29:50, max mem: 20.9 GB 
[11/26 00:47:58 visual_prompt]: 	Training 400/553. train loss: 9.3688,	0.8248 s / batch. (data: 2.54e-04). ETA=4:28:10, max mem: 20.9 GB 
[11/26 00:49:34 visual_prompt]: 	Training 500/553. train loss: 17.0593,	0.8249 s / batch. (data: 5.41e-03). ETA=4:26:48, max mem: 20.9 GB 
[11/26 00:50:23 visual_prompt]: Epoch 65 / 100: avg data time: 1.41e-01, avg batch time: 0.9751, average train loss: 11.4889
[11/26 00:51:19 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3104, average loss: 7.8149
[11/26 00:51:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.17	
[11/26 00:51:19 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[11/26 00:52:59 visual_prompt]: 	Training 100/553. train loss: 6.1508,	0.8177 s / batch. (data: 3.01e-04). ETA=4:22:25, max mem: 20.9 GB 
[11/26 00:54:36 visual_prompt]: 	Training 200/553. train loss: 1.5804,	1.5136 s / batch. (data: 6.88e-01). ETA=8:03:13, max mem: 20.9 GB 
[11/26 00:56:16 visual_prompt]: 	Training 300/553. train loss: 5.7337,	0.8393 s / batch. (data: 5.41e-03). ETA=4:26:33, max mem: 20.9 GB 
[11/26 00:57:51 visual_prompt]: 	Training 400/553. train loss: 7.5557,	0.8331 s / batch. (data: 7.32e-04). ETA=4:23:11, max mem: 20.9 GB 
[11/26 00:59:27 visual_prompt]: 	Training 500/553. train loss: 17.6168,	0.8412 s / batch. (data: 5.41e-03). ETA=4:24:19, max mem: 20.9 GB 
[11/26 01:00:19 visual_prompt]: Epoch 66 / 100: avg data time: 1.43e-01, avg batch time: 0.9765, average train loss: 10.6113
[11/26 01:01:14 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3111, average loss: 6.4178
[11/26 01:01:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.53	
[11/26 01:01:14 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[11/26 01:02:57 visual_prompt]: 	Training 100/553. train loss: 12.9475,	0.8600 s / batch. (data: 2.80e-02). ETA=4:28:03, max mem: 20.9 GB 
[11/26 01:04:34 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8595 s / batch. (data: 1.05e-02). ETA=4:26:29, max mem: 20.9 GB 
[11/26 01:06:08 visual_prompt]: 	Training 300/553. train loss: 14.8793,	0.8360 s / batch. (data: 3.12e-04). ETA=4:17:47, max mem: 20.9 GB 
[11/26 01:07:45 visual_prompt]: 	Training 400/553. train loss: 13.0800,	0.8600 s / batch. (data: 1.20e-02). ETA=4:23:45, max mem: 20.9 GB 
[11/26 01:09:23 visual_prompt]: 	Training 500/553. train loss: 14.7161,	1.4829 s / batch. (data: 6.66e-01). ETA=7:32:20, max mem: 20.9 GB 
[11/26 01:10:15 visual_prompt]: Epoch 67 / 100: avg data time: 1.44e-01, avg batch time: 0.9776, average train loss: 9.5718
[11/26 01:11:10 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3096, average loss: 7.2660
[11/26 01:11:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 37.47	
[11/26 01:11:10 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[11/26 01:12:51 visual_prompt]: 	Training 100/553. train loss: 0.9755,	0.8165 s / batch. (data: 2.90e-04). ETA=4:06:57, max mem: 20.9 GB 
[11/26 01:14:30 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.0279 s / batch. (data: 1.83e-01). ETA=5:09:12, max mem: 20.9 GB 
[11/26 01:16:05 visual_prompt]: 	Training 300/553. train loss: 6.9514,	0.8337 s / batch. (data: 1.05e-02). ETA=4:09:23, max mem: 20.9 GB 
[11/26 01:17:41 visual_prompt]: 	Training 400/553. train loss: 15.7196,	0.8423 s / batch. (data: 6.22e-04). ETA=4:10:33, max mem: 20.9 GB 
[11/26 01:19:19 visual_prompt]: 	Training 500/553. train loss: 13.9565,	0.8605 s / batch. (data: 8.46e-03). ETA=4:14:32, max mem: 20.9 GB 
[11/26 01:20:10 visual_prompt]: Epoch 68 / 100: avg data time: 1.42e-01, avg batch time: 0.9763, average train loss: 9.5590
[11/26 01:21:05 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3100, average loss: 2.5293
[11/26 01:21:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.56	
[11/26 01:21:05 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[11/26 01:22:45 visual_prompt]: 	Training 100/553. train loss: 10.2932,	0.8480 s / batch. (data: 2.95e-04). ETA=4:08:41, max mem: 20.9 GB 
[11/26 01:24:22 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8320 s / batch. (data: 3.02e-04). ETA=4:02:36, max mem: 20.9 GB 
[11/26 01:25:59 visual_prompt]: 	Training 300/553. train loss: 17.7103,	0.8475 s / batch. (data: 1.05e-02). ETA=4:05:43, max mem: 20.9 GB 
[11/26 01:27:35 visual_prompt]: 	Training 400/553. train loss: 21.1518,	0.8366 s / batch. (data: 3.07e-04). ETA=4:01:09, max mem: 20.9 GB 
[11/26 01:29:11 visual_prompt]: 	Training 500/553. train loss: 9.3876,	0.8254 s / batch. (data: 3.03e-04). ETA=3:56:33, max mem: 20.9 GB 
[11/26 01:30:02 visual_prompt]: Epoch 69 / 100: avg data time: 1.38e-01, avg batch time: 0.9713, average train loss: 9.4999
[11/26 01:30:58 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3124, average loss: 5.6805
[11/26 01:30:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.36	
[11/26 01:30:58 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[11/26 01:32:37 visual_prompt]: 	Training 100/553. train loss: 8.5869,	0.8720 s / batch. (data: 7.96e-03). ETA=4:07:41, max mem: 20.9 GB 
[11/26 01:34:15 visual_prompt]: 	Training 200/553. train loss: 2.5892,	0.9162 s / batch. (data: 9.74e-02). ETA=4:18:43, max mem: 20.9 GB 
[11/26 01:35:51 visual_prompt]: 	Training 300/553. train loss: 36.9494,	0.8416 s / batch. (data: 1.20e-02). ETA=3:56:15, max mem: 20.9 GB 
[11/26 01:37:29 visual_prompt]: 	Training 400/553. train loss: 3.9782,	0.8763 s / batch. (data: 2.36e-02). ETA=4:04:32, max mem: 20.9 GB 
[11/26 01:39:04 visual_prompt]: 	Training 500/553. train loss: 12.8219,	0.8360 s / batch. (data: 5.42e-03). ETA=3:51:53, max mem: 20.9 GB 
[11/26 01:39:55 visual_prompt]: Epoch 70 / 100: avg data time: 1.39e-01, avg batch time: 0.9723, average train loss: 8.2492
[11/26 01:40:51 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3091, average loss: 10.5187
[11/26 01:40:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.50	
[11/26 01:40:51 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[11/26 01:42:32 visual_prompt]: 	Training 100/553. train loss: 11.1027,	0.8568 s / batch. (data: 5.44e-03). ETA=3:55:28, max mem: 20.9 GB 
[11/26 01:44:10 visual_prompt]: 	Training 200/553. train loss: 3.1218,	0.9768 s / batch. (data: 1.31e-01). ETA=4:26:50, max mem: 20.9 GB 
[11/26 01:45:49 visual_prompt]: 	Training 300/553. train loss: 3.3976,	0.8443 s / batch. (data: 9.39e-03). ETA=3:49:14, max mem: 20.9 GB 
[11/26 01:47:24 visual_prompt]: 	Training 400/553. train loss: 19.4440,	0.8520 s / batch. (data: 1.06e-02). ETA=3:49:53, max mem: 20.9 GB 
[11/26 01:49:01 visual_prompt]: 	Training 500/553. train loss: 4.8120,	0.9200 s / batch. (data: 6.71e-02). ETA=4:06:43, max mem: 20.9 GB 
[11/26 01:49:52 visual_prompt]: Epoch 71 / 100: avg data time: 1.43e-01, avg batch time: 0.9773, average train loss: 7.4976
[11/26 01:50:47 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3107, average loss: 6.8668
[11/26 01:50:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.44	
[11/26 01:50:47 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[11/26 01:52:31 visual_prompt]: 	Training 100/553. train loss: 6.5432,	1.0640 s / batch. (data: 2.38e-01). ETA=4:42:37, max mem: 20.9 GB 
[11/26 01:54:06 visual_prompt]: 	Training 200/553. train loss: 6.2367,	0.8409 s / batch. (data: 3.08e-04). ETA=3:41:57, max mem: 20.9 GB 
[11/26 01:55:47 visual_prompt]: 	Training 300/553. train loss: 0.8036,	0.8279 s / batch. (data: 3.22e-04). ETA=3:37:09, max mem: 20.9 GB 
[11/26 01:57:25 visual_prompt]: 	Training 400/553. train loss: 3.9885,	0.8188 s / batch. (data: 3.50e-04). ETA=3:33:23, max mem: 20.9 GB 
[11/26 01:59:08 visual_prompt]: 	Training 500/553. train loss: 3.0142,	0.8458 s / batch. (data: 3.17e-04). ETA=3:39:00, max mem: 20.9 GB 
[11/26 02:00:06 visual_prompt]: Epoch 72 / 100: avg data time: 1.77e-01, avg batch time: 1.0104, average train loss: 7.0124
[11/26 02:01:02 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3106, average loss: 1.4174
[11/26 02:01:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.01	
[11/26 02:01:02 visual_prompt]: Best epoch 72: best metric: -1.417
[11/26 02:01:02 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[11/26 02:02:45 visual_prompt]: 	Training 100/553. train loss: 3.6359,	0.8440 s / batch. (data: 7.95e-03). ETA=3:36:24, max mem: 20.9 GB 
[11/26 02:04:33 visual_prompt]: 	Training 200/553. train loss: 3.5473,	0.8240 s / batch. (data: 2.91e-04). ETA=3:29:54, max mem: 20.9 GB 
[11/26 02:06:07 visual_prompt]: 	Training 300/553. train loss: 5.9879,	0.8440 s / batch. (data: 3.09e-04). ETA=3:33:35, max mem: 20.9 GB 
[11/26 02:07:55 visual_prompt]: 	Training 400/553. train loss: 11.5023,	0.8510 s / batch. (data: 3.37e-04). ETA=3:33:56, max mem: 20.9 GB 
[11/26 02:09:32 visual_prompt]: 	Training 500/553. train loss: 10.1057,	0.8314 s / batch. (data: 5.43e-03). ETA=3:27:37, max mem: 20.9 GB 
[11/26 02:10:23 visual_prompt]: Epoch 73 / 100: avg data time: 1.82e-01, avg batch time: 1.0141, average train loss: 7.5758
[11/26 02:11:19 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3108, average loss: 2.7755
[11/26 02:11:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.55	
[11/26 02:11:19 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[11/26 02:13:05 visual_prompt]: 	Training 100/553. train loss: 5.8129,	1.4880 s / batch. (data: 6.37e-01). ETA=6:07:48, max mem: 20.9 GB 
[11/26 02:14:43 visual_prompt]: 	Training 200/553. train loss: 0.6464,	0.8474 s / batch. (data: 7.10e-04). ETA=3:28:03, max mem: 20.9 GB 
[11/26 02:16:20 visual_prompt]: 	Training 300/553. train loss: 1.5511,	0.8360 s / batch. (data: 3.55e-04). ETA=3:23:51, max mem: 20.9 GB 
[11/26 02:17:55 visual_prompt]: 	Training 400/553. train loss: 16.4838,	0.8465 s / batch. (data: 8.87e-03). ETA=3:25:00, max mem: 20.9 GB 
[11/26 02:19:31 visual_prompt]: 	Training 500/553. train loss: 3.7205,	1.5533 s / batch. (data: 7.29e-01). ETA=6:13:35, max mem: 20.9 GB 
[11/26 02:20:21 visual_prompt]: Epoch 74 / 100: avg data time: 1.47e-01, avg batch time: 0.9807, average train loss: 6.5521
[11/26 02:21:17 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3095, average loss: 5.0643
[11/26 02:21:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.67	
[11/26 02:21:17 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[11/26 02:22:59 visual_prompt]: 	Training 100/553. train loss: 5.9551,	0.8400 s / batch. (data: 3.20e-04). ETA=3:19:53, max mem: 20.9 GB 
[11/26 02:24:37 visual_prompt]: 	Training 200/553. train loss: 8.2169,	0.8240 s / batch. (data: 3.10e-04). ETA=3:14:42, max mem: 20.9 GB 
[11/26 02:26:14 visual_prompt]: 	Training 300/553. train loss: 1.0150,	0.8786 s / batch. (data: 2.95e-02). ETA=3:26:09, max mem: 20.9 GB 
[11/26 02:27:53 visual_prompt]: 	Training 400/553. train loss: 16.9524,	1.8832 s / batch. (data: 1.05e+00). ETA=7:18:42, max mem: 20.9 GB 
[11/26 02:29:29 visual_prompt]: 	Training 500/553. train loss: 6.7834,	0.8360 s / batch. (data: 2.87e-04). ETA=3:13:22, max mem: 20.9 GB 
[11/26 02:30:20 visual_prompt]: Epoch 75 / 100: avg data time: 1.48e-01, avg batch time: 0.9815, average train loss: 5.1183
[11/26 02:31:15 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3110, average loss: 1.4458
[11/26 02:31:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.30	
[11/26 02:31:15 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[11/26 02:32:59 visual_prompt]: 	Training 100/553. train loss: 13.9413,	0.8438 s / batch. (data: 8.23e-04). ETA=3:13:00, max mem: 20.9 GB 
[11/26 02:34:36 visual_prompt]: 	Training 200/553. train loss: 4.9527,	0.8176 s / batch. (data: 3.33e-04). ETA=3:05:40, max mem: 20.9 GB 
[11/26 02:36:12 visual_prompt]: 	Training 300/553. train loss: 0.9832,	0.8311 s / batch. (data: 3.05e-04). ETA=3:07:20, max mem: 20.9 GB 
[11/26 02:37:48 visual_prompt]: 	Training 400/553. train loss: 7.1801,	0.8541 s / batch. (data: 3.01e-02). ETA=3:11:05, max mem: 20.9 GB 
[11/26 02:39:24 visual_prompt]: 	Training 500/553. train loss: 4.4765,	0.8361 s / batch. (data: 7.71e-04). ETA=3:05:41, max mem: 20.9 GB 
[11/26 02:40:16 visual_prompt]: Epoch 76 / 100: avg data time: 1.45e-01, avg batch time: 0.9779, average train loss: 5.7713
[11/26 02:41:12 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3101, average loss: 4.5829
[11/26 02:41:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.87	
[11/26 02:41:12 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[11/26 02:42:53 visual_prompt]: 	Training 100/553. train loss: 3.0491,	1.1826 s / batch. (data: 3.54e-01). ETA=4:19:37, max mem: 20.9 GB 
[11/26 02:44:32 visual_prompt]: 	Training 200/553. train loss: 5.3014,	0.8400 s / batch. (data: 3.32e-04). ETA=3:03:00, max mem: 20.9 GB 
[11/26 02:46:07 visual_prompt]: 	Training 300/553. train loss: 2.1961,	0.8294 s / batch. (data: 3.24e-04). ETA=2:59:19, max mem: 20.9 GB 
[11/26 02:47:45 visual_prompt]: 	Training 400/553. train loss: 2.6444,	0.8200 s / batch. (data: 3.17e-04). ETA=2:55:54, max mem: 20.9 GB 
[11/26 02:49:21 visual_prompt]: 	Training 500/553. train loss: 1.8237,	0.8533 s / batch. (data: 5.40e-03). ETA=3:01:38, max mem: 20.9 GB 
[11/26 02:50:11 visual_prompt]: Epoch 77 / 100: avg data time: 1.42e-01, avg batch time: 0.9759, average train loss: 4.3626
[11/26 02:51:07 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3103, average loss: 0.7304
[11/26 02:51:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.36	
[11/26 02:51:07 visual_prompt]: Best epoch 77: best metric: -0.730
[11/26 02:51:07 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[11/26 02:52:47 visual_prompt]: 	Training 100/553. train loss: 2.0175,	0.8280 s / batch. (data: 3.00e-04). ETA=2:54:08, max mem: 20.9 GB 
[11/26 02:54:24 visual_prompt]: 	Training 200/553. train loss: 16.9373,	0.8605 s / batch. (data: 1.05e-02). ETA=2:59:32, max mem: 20.9 GB 
[11/26 02:56:01 visual_prompt]: 	Training 300/553. train loss: 3.0199,	0.8438 s / batch. (data: 1.92e-02). ETA=2:54:39, max mem: 20.9 GB 
[11/26 02:57:39 visual_prompt]: 	Training 400/553. train loss: 3.3504,	0.8267 s / batch. (data: 7.96e-03). ETA=2:49:44, max mem: 20.9 GB 
[11/26 02:59:15 visual_prompt]: 	Training 500/553. train loss: 1.7894,	0.8325 s / batch. (data: 3.24e-04). ETA=2:49:32, max mem: 20.9 GB 
[11/26 03:00:07 visual_prompt]: Epoch 78 / 100: avg data time: 1.42e-01, avg batch time: 0.9755, average train loss: 4.6980
[11/26 03:01:02 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3103, average loss: 1.2920
[11/26 03:01:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.01	
[11/26 03:01:02 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[11/26 03:02:45 visual_prompt]: 	Training 100/553. train loss: 0.5852,	0.8175 s / batch. (data: 3.09e-04). ETA=2:44:23, max mem: 20.9 GB 
[11/26 03:04:21 visual_prompt]: 	Training 200/553. train loss: 6.9634,	0.8376 s / batch. (data: 1.05e-02). ETA=2:47:02, max mem: 20.9 GB 
[11/26 03:05:55 visual_prompt]: 	Training 300/553. train loss: 0.8293,	1.0816 s / batch. (data: 2.64e-01). ETA=3:33:54, max mem: 20.9 GB 
[11/26 03:07:36 visual_prompt]: 	Training 400/553. train loss: 1.0083,	0.8240 s / batch. (data: 3.13e-04). ETA=2:41:35, max mem: 20.9 GB 
[11/26 03:09:14 visual_prompt]: 	Training 500/553. train loss: 10.3924,	0.8625 s / batch. (data: 7.72e-04). ETA=2:47:42, max mem: 20.9 GB 
[11/26 03:10:03 visual_prompt]: Epoch 79 / 100: avg data time: 1.43e-01, avg batch time: 0.9779, average train loss: 4.3958
[11/26 03:10:59 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3117, average loss: 7.9201
[11/26 03:10:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.35	
[11/26 03:10:59 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[11/26 03:12:39 visual_prompt]: 	Training 100/553. train loss: 0.8313,	0.8440 s / batch. (data: 3.46e-04). ETA=2:41:56, max mem: 20.9 GB 
[11/26 03:14:16 visual_prompt]: 	Training 200/553. train loss: 0.8927,	0.8509 s / batch. (data: 1.05e-02). ETA=2:41:51, max mem: 20.9 GB 
[11/26 03:15:53 visual_prompt]: 	Training 300/553. train loss: 2.7040,	1.3160 s / batch. (data: 4.60e-01). ETA=4:08:07, max mem: 20.9 GB 
[11/26 03:17:32 visual_prompt]: 	Training 400/553. train loss: 14.2382,	1.3320 s / batch. (data: 5.10e-01). ETA=4:08:55, max mem: 20.9 GB 
[11/26 03:19:07 visual_prompt]: 	Training 500/553. train loss: 11.6514,	1.0647 s / batch. (data: 2.39e-01). ETA=3:17:11, max mem: 20.9 GB 
[11/26 03:19:58 visual_prompt]: Epoch 80 / 100: avg data time: 1.41e-01, avg batch time: 0.9749, average train loss: 3.7876
[11/26 03:20:54 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3094, average loss: 4.9617
[11/26 03:20:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.33	
[11/26 03:20:54 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[11/26 03:22:36 visual_prompt]: 	Training 100/553. train loss: 1.7479,	0.8399 s / batch. (data: 3.12e-04). ETA=2:33:25, max mem: 20.9 GB 
[11/26 03:24:16 visual_prompt]: 	Training 200/553. train loss: 10.2824,	0.8754 s / batch. (data: 1.56e-02). ETA=2:38:26, max mem: 20.9 GB 
[11/26 03:25:51 visual_prompt]: 	Training 300/553. train loss: 2.4862,	0.8480 s / batch. (data: 3.24e-04). ETA=2:32:04, max mem: 20.9 GB 
[11/26 03:27:28 visual_prompt]: 	Training 400/553. train loss: 4.9085,	1.5447 s / batch. (data: 7.18e-01). ETA=4:34:26, max mem: 20.9 GB 
[11/26 03:29:03 visual_prompt]: 	Training 500/553. train loss: 1.2232,	1.9243 s / batch. (data: 1.09e+00). ETA=5:38:41, max mem: 20.9 GB 
[11/26 03:29:55 visual_prompt]: Epoch 81 / 100: avg data time: 1.45e-01, avg batch time: 0.9777, average train loss: 3.2534
[11/26 03:30:51 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3104, average loss: 1.6483
[11/26 03:30:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.38	
[11/26 03:30:51 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[11/26 03:32:32 visual_prompt]: 	Training 100/553. train loss: 0.8718,	0.8308 s / batch. (data: 3.26e-04). ETA=2:24:05, max mem: 20.9 GB 
[11/26 03:34:11 visual_prompt]: 	Training 200/553. train loss: 1.0874,	0.8399 s / batch. (data: 3.24e-04). ETA=2:24:17, max mem: 20.9 GB 
[11/26 03:35:47 visual_prompt]: 	Training 300/553. train loss: 0.8942,	1.8360 s / batch. (data: 1.02e+00). ETA=5:12:19, max mem: 20.9 GB 
[11/26 03:37:22 visual_prompt]: 	Training 400/553. train loss: 2.0164,	1.9044 s / batch. (data: 1.08e+00). ETA=5:20:47, max mem: 20.9 GB 
[11/26 03:39:00 visual_prompt]: 	Training 500/553. train loss: 0.6835,	0.8351 s / batch. (data: 5.42e-03). ETA=2:19:16, max mem: 20.9 GB 
[11/26 03:39:50 visual_prompt]: Epoch 82 / 100: avg data time: 1.42e-01, avg batch time: 0.9759, average train loss: 2.6596
[11/26 03:40:46 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3083, average loss: 3.6602
[11/26 03:40:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.97	
[11/26 03:40:46 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[11/26 03:42:28 visual_prompt]: 	Training 100/553. train loss: 3.3865,	0.8639 s / batch. (data: 1.55e-03). ETA=2:21:53, max mem: 20.9 GB 
[11/26 03:44:07 visual_prompt]: 	Training 200/553. train loss: 19.2810,	0.8407 s / batch. (data: 1.95e-02). ETA=2:16:40, max mem: 20.9 GB 
[11/26 03:45:44 visual_prompt]: 	Training 300/553. train loss: 6.4987,	0.8600 s / batch. (data: 2.96e-04). ETA=2:18:22, max mem: 20.9 GB 
[11/26 03:47:22 visual_prompt]: 	Training 400/553. train loss: 3.1918,	0.8520 s / batch. (data: 2.92e-04). ETA=2:15:39, max mem: 20.9 GB 
[11/26 03:49:00 visual_prompt]: 	Training 500/553. train loss: 0.7034,	0.8468 s / batch. (data: 2.28e-02). ETA=2:13:25, max mem: 20.9 GB 
[11/26 03:49:49 visual_prompt]: Epoch 83 / 100: avg data time: 1.48e-01, avg batch time: 0.9816, average train loss: 2.7616
[11/26 03:50:45 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3088, average loss: 3.2479
[11/26 03:50:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.66	
[11/26 03:50:45 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[11/26 03:52:27 visual_prompt]: 	Training 100/553. train loss: 0.7517,	0.8249 s / batch. (data: 3.27e-04). ETA=2:07:52, max mem: 20.9 GB 
[11/26 03:54:03 visual_prompt]: 	Training 200/553. train loss: 6.3723,	1.6167 s / batch. (data: 7.75e-01). ETA=4:07:55, max mem: 20.9 GB 
[11/26 03:55:40 visual_prompt]: 	Training 300/553. train loss: 0.7850,	0.8245 s / batch. (data: 3.07e-04). ETA=2:05:04, max mem: 20.9 GB 
[11/26 03:57:19 visual_prompt]: 	Training 400/553. train loss: 0.5892,	0.8393 s / batch. (data: 2.99e-04). ETA=2:05:54, max mem: 20.9 GB 
[11/26 03:58:54 visual_prompt]: 	Training 500/553. train loss: 0.3949,	0.8669 s / batch. (data: 1.06e-02). ETA=2:08:36, max mem: 20.9 GB 
[11/26 03:59:47 visual_prompt]: Epoch 84 / 100: avg data time: 1.48e-01, avg batch time: 0.9808, average train loss: 2.4993
[11/26 04:00:43 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3109, average loss: 2.2649
[11/26 04:00:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.07	
[11/26 04:00:43 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[11/26 04:02:26 visual_prompt]: 	Training 100/553. train loss: 1.3921,	2.9444 s / batch. (data: 2.13e+00). ETA=7:09:17, max mem: 20.9 GB 
[11/26 04:04:02 visual_prompt]: 	Training 200/553. train loss: 4.1894,	1.0017 s / batch. (data: 1.60e-01). ETA=2:24:22, max mem: 20.9 GB 
[11/26 04:05:39 visual_prompt]: 	Training 300/553. train loss: 1.8140,	0.8366 s / batch. (data: 3.23e-04). ETA=1:59:11, max mem: 20.9 GB 
[11/26 04:07:16 visual_prompt]: 	Training 400/553. train loss: 3.1759,	0.8287 s / batch. (data: 5.45e-03). ETA=1:56:40, max mem: 20.9 GB 
[11/26 04:08:54 visual_prompt]: 	Training 500/553. train loss: 2.5262,	0.8322 s / batch. (data: 2.88e-04). ETA=1:55:46, max mem: 20.9 GB 
[11/26 04:09:44 visual_prompt]: Epoch 85 / 100: avg data time: 1.46e-01, avg batch time: 0.9787, average train loss: 1.9503
[11/26 04:10:39 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3073, average loss: 2.5191
[11/26 04:10:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.71	
[11/26 04:10:39 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[11/26 04:12:23 visual_prompt]: 	Training 100/553. train loss: 0.4905,	2.9187 s / batch. (data: 2.10e+00). ETA=6:38:38, max mem: 20.9 GB 
[11/26 04:13:58 visual_prompt]: 	Training 200/553. train loss: 0.5548,	0.8360 s / batch. (data: 3.35e-04). ETA=1:52:47, max mem: 20.9 GB 
[11/26 04:15:34 visual_prompt]: 	Training 300/553. train loss: 3.0682,	0.8756 s / batch. (data: 3.39e-04). ETA=1:56:40, max mem: 20.9 GB 
[11/26 04:17:13 visual_prompt]: 	Training 400/553. train loss: 1.0020,	0.8633 s / batch. (data: 1.53e-02). ETA=1:53:35, max mem: 20.9 GB 
[11/26 04:18:51 visual_prompt]: 	Training 500/553. train loss: 2.6574,	0.8640 s / batch. (data: 1.20e-02). ETA=1:52:14, max mem: 20.9 GB 
[11/26 04:19:43 visual_prompt]: Epoch 86 / 100: avg data time: 1.48e-01, avg batch time: 0.9822, average train loss: 1.7409
[11/26 04:20:38 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3096, average loss: 0.8590
[11/26 04:20:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 54.43	
[11/26 04:20:38 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[11/26 04:22:21 visual_prompt]: 	Training 100/553. train loss: 0.8736,	0.8373 s / batch. (data: 3.19e-04). ETA=1:46:38, max mem: 20.9 GB 
[11/26 04:24:00 visual_prompt]: 	Training 200/553. train loss: 3.4571,	0.8511 s / batch. (data: 1.05e-02). ETA=1:46:59, max mem: 20.9 GB 
[11/26 04:25:38 visual_prompt]: 	Training 300/553. train loss: 1.0294,	1.5461 s / batch. (data: 7.14e-01). ETA=3:11:45, max mem: 20.9 GB 
[11/26 04:27:13 visual_prompt]: 	Training 400/553. train loss: 0.6380,	0.8500 s / batch. (data: 2.87e-04). ETA=1:44:00, max mem: 20.9 GB 
[11/26 04:28:50 visual_prompt]: 	Training 500/553. train loss: 1.9471,	0.8433 s / batch. (data: 1.20e-02). ETA=1:41:47, max mem: 20.9 GB 
[11/26 04:29:41 visual_prompt]: Epoch 87 / 100: avg data time: 1.46e-01, avg batch time: 0.9806, average train loss: 1.5781
[11/26 04:30:36 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3095, average loss: 1.7448
[11/26 04:30:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.57	
[11/26 04:30:36 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[11/26 04:32:17 visual_prompt]: 	Training 100/553. train loss: 0.0008,	0.8360 s / batch. (data: 3.15e-04). ETA=1:38:46, max mem: 20.9 GB 
[11/26 04:33:54 visual_prompt]: 	Training 200/553. train loss: 1.7567,	0.8244 s / batch. (data: 5.47e-03). ETA=1:36:01, max mem: 20.9 GB 
[11/26 04:35:34 visual_prompt]: 	Training 300/553. train loss: 0.4015,	0.8360 s / batch. (data: 3.14e-04). ETA=1:35:59, max mem: 20.9 GB 
[11/26 04:37:14 visual_prompt]: 	Training 400/553. train loss: 1.1784,	2.2543 s / batch. (data: 1.42e+00). ETA=4:15:04, max mem: 20.9 GB 
[11/26 04:38:48 visual_prompt]: 	Training 500/553. train loss: 3.3776,	0.8480 s / batch. (data: 3.15e-04). ETA=1:34:32, max mem: 20.9 GB 
[11/26 04:39:39 visual_prompt]: Epoch 88 / 100: avg data time: 1.48e-01, avg batch time: 0.9814, average train loss: 1.7065
[11/26 04:40:34 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3089, average loss: 0.7878
[11/26 04:40:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.60	
[11/26 04:40:34 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[11/26 04:42:16 visual_prompt]: 	Training 100/553. train loss: 1.7130,	0.8462 s / batch. (data: 3.28e-04). ETA=1:32:10, max mem: 20.9 GB 
[11/26 04:43:52 visual_prompt]: 	Training 200/553. train loss: 0.5703,	0.8480 s / batch. (data: 3.00e-04). ETA=1:30:57, max mem: 20.9 GB 
[11/26 04:45:31 visual_prompt]: 	Training 300/553. train loss: 1.2648,	0.8360 s / batch. (data: 3.16e-04). ETA=1:28:17, max mem: 20.9 GB 
[11/26 04:47:09 visual_prompt]: 	Training 400/553. train loss: 0.1427,	0.8714 s / batch. (data: 1.56e-02). ETA=1:30:33, max mem: 20.9 GB 
[11/26 04:48:45 visual_prompt]: 	Training 500/553. train loss: 2.9255,	0.8320 s / batch. (data: 2.73e-04). ETA=1:25:05, max mem: 20.9 GB 
[11/26 04:49:35 visual_prompt]: Epoch 89 / 100: avg data time: 1.44e-01, avg batch time: 0.9783, average train loss: 1.1049
[11/26 04:50:31 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3114, average loss: 0.6922
[11/26 04:50:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.77	
[11/26 04:50:31 visual_prompt]: Best epoch 89: best metric: -0.692
[11/26 04:50:31 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[11/26 04:52:14 visual_prompt]: 	Training 100/553. train loss: 0.7318,	0.8480 s / batch. (data: 2.92e-04). ETA=1:24:33, max mem: 20.9 GB 
[11/26 04:53:49 visual_prompt]: 	Training 200/553. train loss: 0.9195,	1.4843 s / batch. (data: 6.63e-01). ETA=2:25:32, max mem: 20.9 GB 
[11/26 04:55:26 visual_prompt]: 	Training 300/553. train loss: 2.4238,	0.8481 s / batch. (data: 2.70e-04). ETA=1:21:44, max mem: 20.9 GB 
[11/26 04:57:04 visual_prompt]: 	Training 400/553. train loss: 0.5415,	1.3955 s / batch. (data: 5.79e-01). ETA=2:12:10, max mem: 20.9 GB 
[11/26 04:58:42 visual_prompt]: 	Training 500/553. train loss: 0.6259,	0.8183 s / batch. (data: 2.58e-04). ETA=1:16:08, max mem: 20.9 GB 
[11/26 04:59:31 visual_prompt]: Epoch 90 / 100: avg data time: 1.44e-01, avg batch time: 0.9777, average train loss: 1.1953
[11/26 05:00:27 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3108, average loss: 0.7164
[11/26 05:00:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.27	
[11/26 05:00:27 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[11/26 05:02:10 visual_prompt]: 	Training 100/553. train loss: 0.7038,	0.8311 s / batch. (data: 5.43e-03). ETA=1:15:13, max mem: 20.9 GB 
[11/26 05:03:49 visual_prompt]: 	Training 200/553. train loss: 1.1274,	0.8405 s / batch. (data: 1.05e-02). ETA=1:14:40, max mem: 20.9 GB 
[11/26 05:05:27 visual_prompt]: 	Training 300/553. train loss: 0.7324,	0.8400 s / batch. (data: 2.93e-04). ETA=1:13:13, max mem: 20.9 GB 
[11/26 05:07:07 visual_prompt]: 	Training 400/553. train loss: 0.8049,	1.5724 s / batch. (data: 7.56e-01). ETA=2:14:26, max mem: 20.9 GB 
[11/26 05:08:41 visual_prompt]: 	Training 500/553. train loss: 0.6277,	0.8442 s / batch. (data: 2.06e-02). ETA=1:10:46, max mem: 20.9 GB 
[11/26 05:09:31 visual_prompt]: Epoch 91 / 100: avg data time: 1.48e-01, avg batch time: 0.9834, average train loss: 1.0164
[11/26 05:10:26 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3110, average loss: 0.7110
[11/26 05:10:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.51	
[11/26 05:10:26 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[11/26 05:12:10 visual_prompt]: 	Training 100/553. train loss: 0.6237,	1.8240 s / batch. (data: 9.99e-01). ETA=2:28:15, max mem: 20.9 GB 
[11/26 05:13:46 visual_prompt]: 	Training 200/553. train loss: 0.5295,	0.8399 s / batch. (data: 3.00e-04). ETA=1:06:52, max mem: 20.9 GB 
[11/26 05:15:22 visual_prompt]: 	Training 300/553. train loss: 0.6465,	0.8505 s / batch. (data: 1.05e-02). ETA=1:06:17, max mem: 20.9 GB 
[11/26 05:17:02 visual_prompt]: 	Training 400/553. train loss: 0.7094,	0.8522 s / batch. (data: 2.41e-02). ETA=1:05:00, max mem: 20.9 GB 
[11/26 05:18:37 visual_prompt]: 	Training 500/553. train loss: 0.9332,	0.8322 s / batch. (data: 1.05e-02). ETA=1:02:05, max mem: 20.9 GB 
[11/26 05:19:28 visual_prompt]: Epoch 92 / 100: avg data time: 1.45e-01, avg batch time: 0.9787, average train loss: 0.9226
[11/26 05:20:23 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3109, average loss: 1.0049
[11/26 05:20:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.61	
[11/26 05:20:23 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[11/26 05:22:04 visual_prompt]: 	Training 100/553. train loss: 0.9730,	0.8633 s / batch. (data: 1.55e-02). ETA=1:02:13, max mem: 20.9 GB 
[11/26 05:23:42 visual_prompt]: 	Training 200/553. train loss: 0.7291,	0.9801 s / batch. (data: 1.48e-01). ETA=1:08:59, max mem: 20.9 GB 
[11/26 05:25:18 visual_prompt]: 	Training 300/553. train loss: 0.5906,	0.8305 s / batch. (data: 1.28e-02). ETA=0:57:04, max mem: 20.9 GB 
[11/26 05:26:56 visual_prompt]: 	Training 400/553. train loss: 0.9301,	0.8295 s / batch. (data: 5.43e-03). ETA=0:55:37, max mem: 20.9 GB 
[11/26 05:28:36 visual_prompt]: 	Training 500/553. train loss: 0.6078,	0.8317 s / batch. (data: 2.92e-04). ETA=0:54:23, max mem: 20.9 GB 
[11/26 05:29:26 visual_prompt]: Epoch 93 / 100: avg data time: 1.47e-01, avg batch time: 0.9809, average train loss: 0.9613
[11/26 05:30:21 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3098, average loss: 0.6954
[11/26 05:30:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.57	
[11/26 05:30:21 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[11/26 05:32:03 visual_prompt]: 	Training 100/553. train loss: 0.8118,	0.9976 s / batch. (data: 1.53e-01). ETA=1:02:41, max mem: 20.9 GB 
[11/26 05:33:39 visual_prompt]: 	Training 200/553. train loss: 0.7761,	0.8360 s / batch. (data: 3.20e-04). ETA=0:51:08, max mem: 20.9 GB 
[11/26 05:35:19 visual_prompt]: 	Training 300/553. train loss: 0.7036,	0.8177 s / batch. (data: 3.12e-04). ETA=0:48:39, max mem: 20.9 GB 
[11/26 05:36:55 visual_prompt]: 	Training 400/553. train loss: 0.7895,	0.8720 s / batch. (data: 3.43e-02). ETA=0:50:26, max mem: 20.9 GB 
[11/26 05:38:31 visual_prompt]: 	Training 500/553. train loss: 0.8331,	0.8400 s / batch. (data: 3.06e-04). ETA=0:47:11, max mem: 20.9 GB 
[11/26 05:39:24 visual_prompt]: Epoch 94 / 100: avg data time: 1.48e-01, avg batch time: 0.9810, average train loss: 0.8435
[11/26 05:40:19 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3102, average loss: 0.7070
[11/26 05:40:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.32	
[11/26 05:40:19 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[11/26 05:42:00 visual_prompt]: 	Training 100/553. train loss: 0.6114,	0.8520 s / batch. (data: 3.05e-04). ETA=0:45:41, max mem: 20.9 GB 
[11/26 05:43:40 visual_prompt]: 	Training 200/553. train loss: 0.4697,	0.8478 s / batch. (data: 1.84e-02). ETA=0:44:03, max mem: 20.9 GB 
[11/26 05:45:16 visual_prompt]: 	Training 300/553. train loss: 0.8146,	1.7080 s / batch. (data: 8.71e-01). ETA=1:25:54, max mem: 20.9 GB 
[11/26 05:46:53 visual_prompt]: 	Training 400/553. train loss: 1.1747,	1.6420 s / batch. (data: 8.08e-01). ETA=1:19:51, max mem: 20.9 GB 
[11/26 05:48:30 visual_prompt]: 	Training 500/553. train loss: 0.2609,	0.8333 s / batch. (data: 7.95e-03). ETA=0:39:08, max mem: 20.9 GB 
[11/26 05:49:21 visual_prompt]: Epoch 95 / 100: avg data time: 1.47e-01, avg batch time: 0.9805, average train loss: 0.7454
[11/26 05:50:17 visual_prompt]: Inference (val):avg data time: 3.59e-04, avg batch time: 0.3109, average loss: 0.7121
[11/26 05:50:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.12	
[11/26 05:50:17 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[11/26 05:52:01 visual_prompt]: 	Training 100/553. train loss: 0.6479,	0.8240 s / batch. (data: 3.09e-04). ETA=0:36:35, max mem: 20.9 GB 
[11/26 05:53:40 visual_prompt]: 	Training 200/553. train loss: 0.7923,	0.8564 s / batch. (data: 1.09e-02). ETA=0:36:36, max mem: 20.9 GB 
[11/26 05:55:15 visual_prompt]: 	Training 300/553. train loss: 0.7058,	0.8495 s / batch. (data: 1.05e-02). ETA=0:34:54, max mem: 20.9 GB 
[11/26 05:56:53 visual_prompt]: 	Training 400/553. train loss: 0.8523,	0.8374 s / batch. (data: 5.43e-03). ETA=0:33:00, max mem: 20.9 GB 
[11/26 05:58:30 visual_prompt]: 	Training 500/553. train loss: 0.8478,	0.8600 s / batch. (data: 3.40e-04). ETA=0:32:27, max mem: 20.9 GB 
[11/26 05:59:19 visual_prompt]: Epoch 96 / 100: avg data time: 1.46e-01, avg batch time: 0.9806, average train loss: 0.7558
[11/26 06:00:15 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3106, average loss: 0.7252
[11/26 06:00:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.07	
[11/26 06:00:15 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[11/26 06:01:55 visual_prompt]: 	Training 100/553. train loss: 0.7071,	0.8361 s / batch. (data: 1.64e-02). ETA=0:29:25, max mem: 20.9 GB 
[11/26 06:03:35 visual_prompt]: 	Training 200/553. train loss: 0.5797,	0.8205 s / batch. (data: 3.08e-04). ETA=0:27:30, max mem: 20.9 GB 
[11/26 06:05:12 visual_prompt]: 	Training 300/553. train loss: 0.6748,	0.8354 s / batch. (data: 3.12e-04). ETA=0:26:37, max mem: 20.9 GB 
[11/26 06:06:50 visual_prompt]: 	Training 400/553. train loss: 0.5209,	0.8764 s / batch. (data: 6.96e-04). ETA=0:26:28, max mem: 20.9 GB 
[11/26 06:08:25 visual_prompt]: 	Training 500/553. train loss: 0.9771,	0.8349 s / batch. (data: 7.56e-04). ETA=0:23:49, max mem: 20.9 GB 
[11/26 06:09:18 visual_prompt]: Epoch 97 / 100: avg data time: 1.48e-01, avg batch time: 0.9821, average train loss: 0.7416
[11/26 06:10:13 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3098, average loss: 0.7607
[11/26 06:10:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.37	
[11/26 06:10:13 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[11/26 06:11:57 visual_prompt]: 	Training 100/553. train loss: 0.7290,	0.8312 s / batch. (data: 7.90e-04). ETA=0:21:35, max mem: 20.9 GB 
[11/26 06:13:33 visual_prompt]: 	Training 200/553. train loss: 0.6622,	0.8704 s / batch. (data: 7.95e-03). ETA=0:21:09, max mem: 20.9 GB 
[11/26 06:15:11 visual_prompt]: 	Training 300/553. train loss: 0.6998,	2.2000 s / batch. (data: 1.36e+00). ETA=0:49:49, max mem: 20.9 GB 
[11/26 06:16:48 visual_prompt]: 	Training 400/553. train loss: 0.6972,	1.4651 s / batch. (data: 6.40e-01). ETA=0:30:44, max mem: 20.9 GB 
[11/26 06:18:25 visual_prompt]: 	Training 500/553. train loss: 0.6701,	0.8222 s / batch. (data: 2.94e-04). ETA=0:15:52, max mem: 20.9 GB 
[11/26 06:19:16 visual_prompt]: Epoch 98 / 100: avg data time: 1.47e-01, avg batch time: 0.9812, average train loss: 0.7065
[11/26 06:20:12 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.3096, average loss: 0.6931
[11/26 06:20:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.29	
[11/26 06:20:12 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[11/26 06:21:53 visual_prompt]: 	Training 100/553. train loss: 0.6800,	0.8450 s / batch. (data: 8.89e-03). ETA=0:14:10, max mem: 20.9 GB 
[11/26 06:23:32 visual_prompt]: 	Training 200/553. train loss: 0.7118,	0.8427 s / batch. (data: 1.05e-02). ETA=0:12:43, max mem: 20.9 GB 
[11/26 06:25:11 visual_prompt]: 	Training 300/553. train loss: 0.6977,	1.3773 s / batch. (data: 5.61e-01). ETA=0:18:30, max mem: 20.9 GB 
[11/26 06:26:46 visual_prompt]: 	Training 400/553. train loss: 0.5337,	0.8276 s / batch. (data: 9.27e-03). ETA=0:09:44, max mem: 20.9 GB 
[11/26 06:28:22 visual_prompt]: 	Training 500/553. train loss: 0.6920,	0.8480 s / batch. (data: 1.20e-02). ETA=0:08:33, max mem: 20.9 GB 
[11/26 06:29:14 visual_prompt]: Epoch 99 / 100: avg data time: 1.46e-01, avg batch time: 0.9799, average train loss: 0.6978
[11/26 06:30:09 visual_prompt]: Inference (val):avg data time: 4.09e-04, avg batch time: 0.3103, average loss: 0.7010
[11/26 06:30:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.53	
[11/26 06:30:09 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[11/26 06:31:54 visual_prompt]: 	Training 100/553. train loss: 0.7001,	0.8400 s / batch. (data: 3.41e-04). ETA=0:06:20, max mem: 20.9 GB 
[11/26 06:33:29 visual_prompt]: 	Training 200/553. train loss: 0.6907,	0.8317 s / batch. (data: 3.35e-04). ETA=0:04:53, max mem: 20.9 GB 
[11/26 06:35:08 visual_prompt]: 	Training 300/553. train loss: 0.6684,	0.8440 s / batch. (data: 1.60e-02). ETA=0:03:33, max mem: 20.9 GB 
[11/26 06:36:46 visual_prompt]: 	Training 400/553. train loss: 0.8026,	0.8400 s / batch. (data: 2.96e-04). ETA=0:02:08, max mem: 20.9 GB 
[11/26 06:38:23 visual_prompt]: 	Training 500/553. train loss: 0.6228,	0.8440 s / batch. (data: 3.16e-04). ETA=0:00:44, max mem: 20.9 GB 
[11/26 06:39:12 visual_prompt]: Epoch 100 / 100: avg data time: 1.49e-01, avg batch time: 0.9822, average train loss: 0.6925
[11/26 06:40:08 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3102, average loss: 0.6868
[11/26 06:40:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.97	
[11/26 06:40:08 visual_prompt]: Best epoch 100: best metric: -0.687
[11/26 06:40:08 visual_prompt]: Rank of current process: 0. World size: 1
[11/26 06:40:08 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/26 06:40:08 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/26 06:40:08 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/26 06:40:08 visual_prompt]: Training with config:
[11/26 06:40:08 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/26 06:40:08 visual_prompt]: Loading training data...
[11/26 06:40:08 visual_prompt]: Constructing mammo-cbis dataset train...
[11/26 06:40:08 visual_prompt]: Loading validation data...
[11/26 06:40:08 visual_prompt]: Constructing mammo-cbis dataset val...
[11/26 06:40:09 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/26 06:40:16 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/26 06:40:16 visual_prompt]: tuned percent:0.525
[11/26 06:40:16 visual_prompt]: Device used for model: 0
[11/26 06:40:16 visual_prompt]: Setting up Evaluator...
[11/26 06:40:16 visual_prompt]: Setting up Trainer...
[11/26 06:40:16 visual_prompt]: 	Setting up the optimizer...
[11/26 06:40:16 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/26 06:41:58 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8303 s / batch. (data: 2.94e-04). ETA=12:43:50, max mem: 20.9 GB 
[11/26 06:43:33 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8280 s / batch. (data: 7.95e-03). ETA=12:40:22, max mem: 20.9 GB 
[11/26 06:45:13 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8360 s / batch. (data: 3.43e-04). ETA=12:46:20, max mem: 20.9 GB 
[11/26 06:46:50 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8280 s / batch. (data: 3.24e-04). ETA=12:37:39, max mem: 20.9 GB 
[11/26 06:48:29 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8360 s / batch. (data: 3.09e-04). ETA=12:43:33, max mem: 20.9 GB 
[11/26 06:49:20 visual_prompt]: Epoch 1 / 100: avg data time: 1.49e-01, avg batch time: 0.9831, average train loss: 1.5403
[11/26 06:50:15 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3095, average loss: 1.5201
[11/26 06:50:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/26 06:50:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/26 06:51:56 visual_prompt]: 	Training 100/553. train loss: 3.8541,	0.8337 s / batch. (data: 5.44e-03). ETA=12:39:17, max mem: 20.9 GB 
[11/26 06:53:33 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8398 s / batch. (data: 7.75e-03). ETA=12:43:27, max mem: 20.9 GB 
[11/26 06:55:12 visual_prompt]: 	Training 300/553. train loss: 2.6410,	0.8360 s / batch. (data: 2.92e-04). ETA=12:38:38, max mem: 20.9 GB 
[11/26 06:56:49 visual_prompt]: 	Training 400/553. train loss: 1.6751,	0.8213 s / batch. (data: 2.98e-04). ETA=12:23:52, max mem: 20.9 GB 
[11/26 06:58:27 visual_prompt]: 	Training 500/553. train loss: 0.5674,	0.8269 s / batch. (data: 2.89e-04). ETA=12:27:35, max mem: 20.9 GB 
[11/26 06:59:17 visual_prompt]: Epoch 2 / 100: avg data time: 1.46e-01, avg batch time: 0.9791, average train loss: 3.2359
[11/26 07:00:13 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3107, average loss: 10.4632
[11/26 07:00:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.18	
[11/26 07:00:13 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/26 07:01:54 visual_prompt]: 	Training 100/553. train loss: 24.3151,	0.8231 s / batch. (data: 3.20e-04). ETA=12:22:04, max mem: 20.9 GB 
[11/26 07:03:32 visual_prompt]: 	Training 200/553. train loss: 4.6712,	0.8368 s / batch. (data: 8.74e-03). ETA=12:33:03, max mem: 20.9 GB 
[11/26 07:05:09 visual_prompt]: 	Training 300/553. train loss: 2.6836,	0.8809 s / batch. (data: 2.09e-02). ETA=13:11:16, max mem: 20.9 GB 
[11/26 07:06:47 visual_prompt]: 	Training 400/553. train loss: 6.9202,	0.8273 s / batch. (data: 5.44e-03). ETA=12:21:42, max mem: 20.9 GB 
[11/26 07:08:26 visual_prompt]: 	Training 500/553. train loss: 2.7821,	1.2070 s / batch. (data: 3.76e-01). ETA=18:00:10, max mem: 20.9 GB 
[11/26 07:09:16 visual_prompt]: Epoch 3 / 100: avg data time: 1.49e-01, avg batch time: 0.9828, average train loss: 6.4887
[11/26 07:10:12 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3107, average loss: 7.2072
[11/26 07:10:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.13	
[11/26 07:10:12 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/26 07:11:56 visual_prompt]: 	Training 100/553. train loss: 21.5205,	0.8520 s / batch. (data: 3.05e-04). ETA=12:40:16, max mem: 20.9 GB 
[11/26 07:13:33 visual_prompt]: 	Training 200/553. train loss: 17.5341,	0.8253 s / batch. (data: 3.03e-04). ETA=12:15:06, max mem: 20.9 GB 
[11/26 07:15:10 visual_prompt]: 	Training 300/553. train loss: 1.5394,	1.0762 s / batch. (data: 2.39e-01). ETA=15:56:43, max mem: 20.9 GB 
[11/26 07:16:44 visual_prompt]: 	Training 400/553. train loss: 17.8045,	1.1907 s / batch. (data: 3.52e-01). ETA=17:36:35, max mem: 20.9 GB 
[11/26 07:18:23 visual_prompt]: 	Training 500/553. train loss: 15.0734,	3.2397 s / batch. (data: 2.42e+00). ETA=1 day, 23:49:20, max mem: 20.9 GB 
[11/26 07:19:15 visual_prompt]: Epoch 4 / 100: avg data time: 1.47e-01, avg batch time: 0.9818, average train loss: 8.8992
[11/26 07:20:11 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3102, average loss: 7.4956
[11/26 07:20:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.73	
[11/26 07:20:11 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/26 07:21:51 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8440 s / batch. (data: 2.96e-04). ETA=12:25:20, max mem: 20.9 GB 
[11/26 07:23:29 visual_prompt]: 	Training 200/553. train loss: 26.0106,	1.2726 s / batch. (data: 4.37e-01). ETA=18:41:42, max mem: 20.9 GB 
[11/26 07:25:08 visual_prompt]: 	Training 300/553. train loss: 27.0224,	0.8596 s / batch. (data: 1.56e-02). ETA=12:36:15, max mem: 20.9 GB 
[11/26 07:26:45 visual_prompt]: 	Training 400/553. train loss: 5.8865,	0.8449 s / batch. (data: 8.87e-03). ETA=12:21:56, max mem: 20.9 GB 
[11/26 07:28:22 visual_prompt]: 	Training 500/553. train loss: 20.3104,	0.8290 s / batch. (data: 3.02e-04). ETA=12:06:33, max mem: 20.9 GB 
[11/26 07:29:14 visual_prompt]: Epoch 5 / 100: avg data time: 1.49e-01, avg batch time: 0.9819, average train loss: 13.5538
[11/26 07:30:10 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3096, average loss: 19.2477
[11/26 07:30:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.87	
[11/26 07:30:10 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/26 07:31:53 visual_prompt]: 	Training 100/553. train loss: 18.3519,	0.8400 s / batch. (data: 3.10e-04). ETA=12:14:05, max mem: 20.9 GB 
[11/26 07:33:29 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8354 s / batch. (data: 2.90e-04). ETA=12:08:41, max mem: 20.9 GB 
[11/26 07:35:06 visual_prompt]: 	Training 300/553. train loss: 10.2809,	0.8288 s / batch. (data: 5.47e-03). ETA=12:01:34, max mem: 20.9 GB 
[11/26 07:36:46 visual_prompt]: 	Training 400/553. train loss: 44.9343,	0.8371 s / batch. (data: 3.19e-04). ETA=12:07:23, max mem: 20.9 GB 
[11/26 07:38:23 visual_prompt]: 	Training 500/553. train loss: 30.3506,	0.8577 s / batch. (data: 3.08e-02). ETA=12:23:50, max mem: 20.9 GB 
[11/26 07:39:13 visual_prompt]: Epoch 6 / 100: avg data time: 1.51e-01, avg batch time: 0.9832, average train loss: 16.0221
[11/26 07:40:09 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3124, average loss: 6.9090
[11/26 07:40:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.85	
[11/26 07:40:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/26 07:41:49 visual_prompt]: 	Training 100/553. train loss: 21.6591,	0.8307 s / batch. (data: 3.33e-04). ETA=11:58:16, max mem: 20.9 GB 
[11/26 07:43:26 visual_prompt]: 	Training 200/553. train loss: 10.9828,	0.8560 s / batch. (data: 5.45e-03). ETA=12:18:44, max mem: 20.9 GB 
[11/26 07:45:07 visual_prompt]: 	Training 300/553. train loss: 16.7040,	1.7148 s / batch. (data: 8.74e-01). ETA=1 day, 0:37:03, max mem: 20.9 GB 
[11/26 07:46:45 visual_prompt]: 	Training 400/553. train loss: 3.1928,	1.9534 s / batch. (data: 1.13e+00). ETA=1 day, 3:59:20, max mem: 20.9 GB 
[11/26 07:48:21 visual_prompt]: 	Training 500/553. train loss: 8.6548,	0.8769 s / batch. (data: 2.08e-02). ETA=12:32:22, max mem: 20.9 GB 
[11/26 07:49:11 visual_prompt]: Epoch 7 / 100: avg data time: 1.47e-01, avg batch time: 0.9792, average train loss: 20.5133
[11/26 07:50:06 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3104, average loss: 20.0773
[11/26 07:50:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.11	
[11/26 07:50:06 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/26 07:51:45 visual_prompt]: 	Training 100/553. train loss: 65.4587,	0.8282 s / batch. (data: 1.05e-02). ETA=11:48:31, max mem: 20.9 GB 
[11/26 07:53:24 visual_prompt]: 	Training 200/553. train loss: 5.9290,	0.8522 s / batch. (data: 3.34e-04). ETA=12:07:39, max mem: 20.9 GB 
[11/26 07:55:01 visual_prompt]: 	Training 300/553. train loss: 20.1993,	0.8218 s / batch. (data: 3.37e-04). ETA=11:40:16, max mem: 20.9 GB 
[11/26 07:56:39 visual_prompt]: 	Training 400/553. train loss: 4.9946,	0.8360 s / batch. (data: 7.96e-03). ETA=11:50:59, max mem: 20.9 GB 
[11/26 07:58:17 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.3840 s / batch. (data: 5.32e-01). ETA=19:34:46, max mem: 20.9 GB 
[11/26 07:59:09 visual_prompt]: Epoch 8 / 100: avg data time: 1.48e-01, avg batch time: 0.9806, average train loss: 21.8628
[11/26 08:00:04 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3103, average loss: 4.8054
[11/26 08:00:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.08	
[11/26 08:00:04 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/26 08:01:45 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8280 s / batch. (data: 4.50e-04). ETA=11:40:40, max mem: 20.9 GB 
[11/26 08:03:22 visual_prompt]: 	Training 200/553. train loss: 19.8405,	0.8598 s / batch. (data: 3.82e-04). ETA=12:06:10, max mem: 20.9 GB 
[11/26 08:05:00 visual_prompt]: 	Training 300/553. train loss: 5.5630,	1.4920 s / batch. (data: 6.40e-01). ETA=20:57:39, max mem: 20.9 GB 
[11/26 08:06:40 visual_prompt]: 	Training 400/553. train loss: 23.2621,	0.8560 s / batch. (data: 2.80e-02). ETA=12:00:07, max mem: 20.9 GB 
[11/26 08:08:18 visual_prompt]: 	Training 500/553. train loss: 11.8249,	0.9387 s / batch. (data: 1.17e-01). ETA=13:08:05, max mem: 20.9 GB 
[11/26 08:09:07 visual_prompt]: Epoch 9 / 100: avg data time: 1.50e-01, avg batch time: 0.9821, average train loss: 22.3752
[11/26 08:10:03 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3104, average loss: 28.7980
[11/26 08:10:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.92	
[11/26 08:10:03 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/26 08:11:48 visual_prompt]: 	Training 100/553. train loss: 51.8149,	0.8246 s / batch. (data: 7.96e-03). ETA=11:30:14, max mem: 20.9 GB 
[11/26 08:13:24 visual_prompt]: 	Training 200/553. train loss: 40.6482,	0.8290 s / batch. (data: 5.43e-03). ETA=11:32:33, max mem: 20.9 GB 
[11/26 08:15:01 visual_prompt]: 	Training 300/553. train loss: 216.0547,	0.8560 s / batch. (data: 7.95e-03). ETA=11:53:40, max mem: 20.9 GB 
[11/26 08:16:35 visual_prompt]: 	Training 400/553. train loss: 9.2732,	0.8221 s / batch. (data: 3.31e-04). ETA=11:24:03, max mem: 20.9 GB 
[11/26 08:18:15 visual_prompt]: 	Training 500/553. train loss: 3.4918,	0.8400 s / batch. (data: 2.04e-02). ETA=11:37:29, max mem: 20.9 GB 
[11/26 08:19:06 visual_prompt]: Epoch 10 / 100: avg data time: 1.51e-01, avg batch time: 0.9810, average train loss: 33.7567
[11/26 08:20:02 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3086, average loss: 27.9118
[11/26 08:20:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 49.09	
[11/26 08:20:02 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/26 08:21:45 visual_prompt]: 	Training 100/553. train loss: 25.8767,	0.8215 s / batch. (data: 5.42e-03). ETA=11:20:06, max mem: 20.9 GB 
[11/26 08:23:24 visual_prompt]: 	Training 200/553. train loss: 43.7321,	0.8234 s / batch. (data: 5.44e-03). ETA=11:20:15, max mem: 20.9 GB 
[11/26 08:25:00 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.0439 s / batch. (data: 1.20e+00). ETA=1 day, 4:05:10, max mem: 20.9 GB 
[11/26 08:26:36 visual_prompt]: 	Training 400/553. train loss: 3.9212,	0.8640 s / batch. (data: 7.96e-03). ETA=11:50:54, max mem: 20.9 GB 
[11/26 08:28:12 visual_prompt]: 	Training 500/553. train loss: 7.5434,	0.8265 s / batch. (data: 2.49e-03). ETA=11:18:39, max mem: 20.9 GB 
[11/26 08:29:02 visual_prompt]: Epoch 11 / 100: avg data time: 1.45e-01, avg batch time: 0.9776, average train loss: 28.0970
[11/26 08:29:58 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3119, average loss: 22.0926
[11/26 08:29:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.95	
[11/26 08:29:58 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/26 08:31:41 visual_prompt]: 	Training 100/553. train loss: 13.9978,	0.8190 s / batch. (data: 3.29e-04). ETA=11:10:29, max mem: 20.9 GB 
[11/26 08:33:19 visual_prompt]: 	Training 200/553. train loss: 86.0450,	0.8351 s / batch. (data: 3.09e-04). ETA=11:22:16, max mem: 20.9 GB 
[11/26 08:34:55 visual_prompt]: 	Training 300/553. train loss: 195.4582,	0.8640 s / batch. (data: 2.79e-02). ETA=11:44:23, max mem: 20.9 GB 
[11/26 08:36:33 visual_prompt]: 	Training 400/553. train loss: 32.3267,	0.8607 s / batch. (data: 3.36e-04). ETA=11:40:19, max mem: 20.9 GB 
[11/26 08:38:10 visual_prompt]: 	Training 500/553. train loss: 104.1752,	0.8474 s / batch. (data: 2.27e-04). ETA=11:28:05, max mem: 20.9 GB 
[11/26 08:39:00 visual_prompt]: Epoch 12 / 100: avg data time: 1.49e-01, avg batch time: 0.9807, average train loss: 34.8851
[11/26 08:39:56 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3116, average loss: 8.3360
[11/26 08:39:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.81	
[11/26 08:39:56 visual_prompt]: Best epoch 12: best metric: -8.336
[11/26 08:39:56 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/26 08:41:39 visual_prompt]: 	Training 100/553. train loss: 12.3251,	0.8231 s / batch. (data: 3.13e-04). ETA=11:06:10, max mem: 20.9 GB 
[11/26 08:43:13 visual_prompt]: 	Training 200/553. train loss: 2.0794,	0.8366 s / batch. (data: 2.69e-04). ETA=11:15:45, max mem: 20.9 GB 
[11/26 08:44:52 visual_prompt]: 	Training 300/553. train loss: 61.6420,	1.7840 s / batch. (data: 9.46e-01). ETA=23:58:01, max mem: 20.9 GB 
[11/26 08:46:29 visual_prompt]: 	Training 400/553. train loss: 211.3320,	0.8162 s / batch. (data: 3.36e-04). ETA=10:56:30, max mem: 20.9 GB 
[11/26 08:48:08 visual_prompt]: 	Training 500/553. train loss: 41.9996,	0.8440 s / batch. (data: 3.12e-04). ETA=11:17:29, max mem: 20.9 GB 
[11/26 08:48:59 visual_prompt]: Epoch 13 / 100: avg data time: 1.51e-01, avg batch time: 0.9815, average train loss: 33.2361
[11/26 08:49:55 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.3117, average loss: 23.4402
[11/26 08:49:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.31	
[11/26 08:49:55 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/26 08:51:38 visual_prompt]: 	Training 100/553. train loss: 58.2859,	0.8313 s / batch. (data: 9.16e-03). ETA=11:05:13, max mem: 20.9 GB 
[11/26 08:53:15 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.0280 s / batch. (data: 1.81e-01). ETA=13:40:51, max mem: 20.9 GB 
[11/26 08:54:52 visual_prompt]: 	Training 300/553. train loss: 8.4469,	0.8531 s / batch. (data: 2.07e-02). ETA=11:19:46, max mem: 20.9 GB 
[11/26 08:56:29 visual_prompt]: 	Training 400/553. train loss: 24.6987,	0.8179 s / batch. (data: 5.42e-03). ETA=10:50:22, max mem: 20.9 GB 
[11/26 08:58:07 visual_prompt]: 	Training 500/553. train loss: 10.9003,	0.8415 s / batch. (data: 1.07e-02). ETA=11:07:42, max mem: 20.9 GB 
[11/26 08:58:56 visual_prompt]: Epoch 14 / 100: avg data time: 1.47e-01, avg batch time: 0.9786, average train loss: 27.9338
[11/26 08:59:52 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3106, average loss: 1.4110
[11/26 08:59:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.01	
[11/26 08:59:52 visual_prompt]: Best epoch 14: best metric: -1.411
[11/26 08:59:52 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/26 09:01:33 visual_prompt]: 	Training 100/553. train loss: 33.2693,	0.8527 s / batch. (data: 3.40e-04). ETA=11:14:29, max mem: 20.9 GB 
[11/26 09:03:09 visual_prompt]: 	Training 200/553. train loss: 175.5086,	0.8520 s / batch. (data: 3.21e-04). ETA=11:12:30, max mem: 20.9 GB 
[11/26 09:04:49 visual_prompt]: 	Training 300/553. train loss: 14.0868,	0.8739 s / batch. (data: 1.56e-02). ETA=11:28:17, max mem: 20.9 GB 
[11/26 09:06:24 visual_prompt]: 	Training 400/553. train loss: 43.9856,	1.0043 s / batch. (data: 1.55e-01). ETA=13:09:22, max mem: 20.9 GB 
[11/26 09:08:02 visual_prompt]: 	Training 500/553. train loss: 22.7769,	0.9272 s / batch. (data: 1.06e-01). ETA=12:07:11, max mem: 20.9 GB 
[11/26 09:08:53 visual_prompt]: Epoch 15 / 100: avg data time: 1.48e-01, avg batch time: 0.9796, average train loss: 30.8940
[11/26 09:09:49 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3117, average loss: 51.1618
[11/26 09:09:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.25	
[11/26 09:09:49 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/26 09:11:29 visual_prompt]: 	Training 100/553. train loss: 29.5155,	0.8205 s / batch. (data: 2.83e-04). ETA=10:41:25, max mem: 20.9 GB 
[11/26 09:13:06 visual_prompt]: 	Training 200/553. train loss: 37.1712,	0.8351 s / batch. (data: 1.05e-02). ETA=10:51:25, max mem: 20.9 GB 
[11/26 09:14:44 visual_prompt]: 	Training 300/553. train loss: 95.8599,	0.8500 s / batch. (data: 5.45e-03). ETA=11:01:37, max mem: 20.9 GB 
[11/26 09:16:21 visual_prompt]: 	Training 400/553. train loss: 46.6106,	0.8360 s / batch. (data: 7.71e-04). ETA=10:49:22, max mem: 20.9 GB 
[11/26 09:17:58 visual_prompt]: 	Training 500/553. train loss: 5.6730,	0.8560 s / batch. (data: 3.13e-04). ETA=11:03:28, max mem: 20.9 GB 
[11/26 09:18:49 visual_prompt]: Epoch 16 / 100: avg data time: 1.45e-01, avg batch time: 0.9771, average train loss: 30.0371
[11/26 09:19:45 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3111, average loss: 16.7110
[11/26 09:19:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.45	
[11/26 09:19:45 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/26 09:21:25 visual_prompt]: 	Training 100/553. train loss: 0.9629,	0.8480 s / batch. (data: 1.19e-02). ETA=10:55:05, max mem: 20.9 GB 
[11/26 09:23:03 visual_prompt]: 	Training 200/553. train loss: 1.0105,	0.8210 s / batch. (data: 2.60e-04). ETA=10:32:53, max mem: 20.9 GB 
[11/26 09:24:40 visual_prompt]: 	Training 300/553. train loss: 31.9760,	0.8523 s / batch. (data: 3.33e-02). ETA=10:55:33, max mem: 20.9 GB 
[11/26 09:26:17 visual_prompt]: 	Training 400/553. train loss: 60.9852,	1.0617 s / batch. (data: 2.45e-01). ETA=13:34:55, max mem: 20.9 GB 
[11/26 09:27:54 visual_prompt]: 	Training 500/553. train loss: 10.4717,	1.3720 s / batch. (data: 5.46e-01). ETA=17:30:45, max mem: 20.9 GB 
[11/26 09:28:46 visual_prompt]: Epoch 17 / 100: avg data time: 1.47e-01, avg batch time: 0.9783, average train loss: 32.3750
[11/26 09:29:41 visual_prompt]: Inference (val):avg data time: 3.03e-04, avg batch time: 0.3100, average loss: 26.2978
[11/26 09:29:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.64	
[11/26 09:29:41 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/26 09:31:23 visual_prompt]: 	Training 100/553. train loss: 37.6888,	0.8368 s / batch. (data: 2.46e-02). ETA=10:38:46, max mem: 20.9 GB 
[11/26 09:33:03 visual_prompt]: 	Training 200/553. train loss: 47.4924,	0.8556 s / batch. (data: 2.63e-03). ETA=10:51:37, max mem: 20.9 GB 
[11/26 09:34:40 visual_prompt]: 	Training 300/553. train loss: 42.6714,	0.8354 s / batch. (data: 3.09e-04). ETA=10:34:51, max mem: 20.9 GB 
[11/26 09:36:18 visual_prompt]: 	Training 400/553. train loss: 5.9467,	0.8288 s / batch. (data: 1.10e-02). ETA=10:28:27, max mem: 20.9 GB 
[11/26 09:37:54 visual_prompt]: 	Training 500/553. train loss: 25.6926,	0.8234 s / batch. (data: 2.96e-04). ETA=10:23:00, max mem: 20.9 GB 
[11/26 09:38:44 visual_prompt]: Epoch 18 / 100: avg data time: 1.52e-01, avg batch time: 0.9816, average train loss: 33.0730
[11/26 09:39:40 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3097, average loss: 18.0114
[11/26 09:39:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.93	
[11/26 09:39:40 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/26 09:41:21 visual_prompt]: 	Training 100/553. train loss: 6.7463,	1.3602 s / batch. (data: 5.43e-01). ETA=17:05:44, max mem: 20.9 GB 
[11/26 09:43:00 visual_prompt]: 	Training 200/553. train loss: 18.9321,	0.8399 s / batch. (data: 7.95e-03). ETA=10:31:59, max mem: 20.9 GB 
[11/26 09:44:38 visual_prompt]: 	Training 300/553. train loss: 18.4357,	0.8284 s / batch. (data: 7.92e-03). ETA=10:21:55, max mem: 20.9 GB 
[11/26 09:46:17 visual_prompt]: 	Training 400/553. train loss: 20.8769,	0.8162 s / batch. (data: 3.48e-04). ETA=10:11:22, max mem: 20.9 GB 
[11/26 09:47:50 visual_prompt]: 	Training 500/553. train loss: 38.7176,	0.8696 s / batch. (data: 2.96e-02). ETA=10:49:57, max mem: 20.9 GB 
[11/26 09:48:41 visual_prompt]: Epoch 19 / 100: avg data time: 1.48e-01, avg batch time: 0.9787, average train loss: 33.2714
[11/26 09:49:37 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3114, average loss: 45.4579
[11/26 09:49:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.63	
[11/26 09:49:37 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/26 09:51:17 visual_prompt]: 	Training 100/553. train loss: 12.9582,	0.8315 s / batch. (data: 7.94e-03). ETA=10:19:22, max mem: 20.9 GB 
[11/26 09:52:55 visual_prompt]: 	Training 200/553. train loss: 39.6115,	0.8400 s / batch. (data: 3.17e-04). ETA=10:24:18, max mem: 20.9 GB 
[11/26 09:54:33 visual_prompt]: 	Training 300/553. train loss: 121.8446,	0.8382 s / batch. (data: 6.72e-04). ETA=10:21:35, max mem: 20.9 GB 
[11/26 09:56:10 visual_prompt]: 	Training 400/553. train loss: 47.2035,	0.8360 s / batch. (data: 7.95e-03). ETA=10:18:31, max mem: 20.9 GB 
[11/26 09:57:46 visual_prompt]: 	Training 500/553. train loss: 39.6276,	0.8321 s / batch. (data: 3.17e-04). ETA=10:14:17, max mem: 20.9 GB 
[11/26 09:58:39 visual_prompt]: Epoch 20 / 100: avg data time: 1.50e-01, avg batch time: 0.9805, average train loss: 35.4635
[11/26 09:59:35 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3094, average loss: 42.7701
[11/26 09:59:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.73	
[11/26 09:59:35 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/26 10:01:19 visual_prompt]: 	Training 100/553. train loss: 47.9980,	0.8340 s / batch. (data: 3.27e-04). ETA=10:13:33, max mem: 20.9 GB 
[11/26 10:02:57 visual_prompt]: 	Training 200/553. train loss: 94.1435,	0.8240 s / batch. (data: 2.97e-04). ETA=10:04:48, max mem: 20.9 GB 
[11/26 10:04:34 visual_prompt]: 	Training 300/553. train loss: 185.8542,	0.8440 s / batch. (data: 1.59e-02). ETA=10:18:04, max mem: 20.9 GB 
[11/26 10:06:12 visual_prompt]: 	Training 400/553. train loss: 1.6639,	0.8181 s / batch. (data: 3.03e-04). ETA=9:57:43, max mem: 20.9 GB 
[11/26 10:07:58 visual_prompt]: 	Training 500/553. train loss: 20.4397,	0.8316 s / batch. (data: 2.89e-04). ETA=10:06:15, max mem: 20.9 GB 
[11/26 10:08:50 visual_prompt]: Epoch 21 / 100: avg data time: 1.73e-01, avg batch time: 1.0029, average train loss: 36.2581
[11/26 10:09:52 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3110, average loss: 52.4051
[11/26 10:09:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.97	
[11/26 10:09:52 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/26 10:11:35 visual_prompt]: 	Training 100/553. train loss: 27.0023,	0.8253 s / batch. (data: 2.87e-04). ETA=9:59:30, max mem: 20.9 GB 
[11/26 10:13:14 visual_prompt]: 	Training 200/553. train loss: 39.1766,	0.8590 s / batch. (data: 2.29e-02). ETA=10:22:37, max mem: 20.9 GB 
[11/26 10:14:51 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8440 s / batch. (data: 2.97e-04). ETA=10:10:18, max mem: 20.9 GB 
[11/26 10:16:31 visual_prompt]: 	Training 400/553. train loss: 20.2337,	0.8280 s / batch. (data: 3.00e-04). ETA=9:57:20, max mem: 20.9 GB 
[11/26 10:18:10 visual_prompt]: 	Training 500/553. train loss: 19.9813,	0.8338 s / batch. (data: 3.08e-04). ETA=10:00:08, max mem: 20.9 GB 
[11/26 10:19:03 visual_prompt]: Epoch 22 / 100: avg data time: 1.66e-01, avg batch time: 0.9966, average train loss: 32.4458
[11/26 10:19:58 visual_prompt]: Inference (val):avg data time: 1.90e-04, avg batch time: 0.3094, average loss: 15.4074
[11/26 10:19:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.64	
[11/26 10:19:58 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/26 10:21:42 visual_prompt]: 	Training 100/553. train loss: 26.0183,	0.8434 s / batch. (data: 7.95e-03). ETA=10:04:56, max mem: 20.9 GB 
[11/26 10:23:20 visual_prompt]: 	Training 200/553. train loss: 13.7293,	0.8440 s / batch. (data: 7.95e-03). ETA=10:03:55, max mem: 20.9 GB 
[11/26 10:25:00 visual_prompt]: 	Training 300/553. train loss: 21.7702,	0.8364 s / batch. (data: 3.55e-04). ETA=9:57:06, max mem: 20.9 GB 
[11/26 10:26:35 visual_prompt]: 	Training 400/553. train loss: 6.2527,	0.8491 s / batch. (data: 5.43e-03). ETA=10:04:43, max mem: 20.9 GB 
[11/26 10:28:11 visual_prompt]: 	Training 500/553. train loss: 15.2459,	0.8280 s / batch. (data: 5.43e-03). ETA=9:48:19, max mem: 20.9 GB 
[11/26 10:29:02 visual_prompt]: Epoch 23 / 100: avg data time: 1.52e-01, avg batch time: 0.9830, average train loss: 34.6301
[11/26 10:29:58 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3100, average loss: 54.3078
[11/26 10:29:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.92	
[11/26 10:29:58 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/26 10:31:36 visual_prompt]: 	Training 100/553. train loss: 3.8505,	0.8400 s / batch. (data: 3.39e-04). ETA=9:54:43, max mem: 20.9 GB 
[11/26 10:33:14 visual_prompt]: 	Training 200/553. train loss: 44.8869,	0.8463 s / batch. (data: 1.05e-02). ETA=9:57:47, max mem: 20.9 GB 
[11/26 10:34:52 visual_prompt]: 	Training 300/553. train loss: 32.5267,	0.8429 s / batch. (data: 8.59e-03). ETA=9:53:57, max mem: 20.9 GB 
[11/26 10:36:29 visual_prompt]: 	Training 400/553. train loss: 2.1398,	0.8480 s / batch. (data: 3.18e-04). ETA=9:56:08, max mem: 20.9 GB 
[11/26 10:38:08 visual_prompt]: 	Training 500/553. train loss: 1.0928,	0.8522 s / batch. (data: 8.13e-03). ETA=9:57:39, max mem: 20.9 GB 
[11/26 10:38:59 visual_prompt]: Epoch 24 / 100: avg data time: 1.47e-01, avg batch time: 0.9783, average train loss: 31.5102
[11/26 10:39:54 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3095, average loss: 17.1132
[11/26 10:39:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.29	
[11/26 10:39:54 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/26 10:41:39 visual_prompt]: 	Training 100/553. train loss: 16.7239,	0.8435 s / batch. (data: 2.35e-02). ETA=9:49:26, max mem: 20.9 GB 
[11/26 10:43:13 visual_prompt]: 	Training 200/553. train loss: 9.5156,	0.8520 s / batch. (data: 3.29e-04). ETA=9:53:56, max mem: 20.9 GB 
[11/26 10:44:51 visual_prompt]: 	Training 300/553. train loss: 21.6935,	0.8289 s / batch. (data: 3.11e-04). ETA=9:36:29, max mem: 20.9 GB 
[11/26 10:46:28 visual_prompt]: 	Training 400/553. train loss: 3.4372,	1.2160 s / batch. (data: 3.67e-01). ETA=14:03:38, max mem: 20.9 GB 
[11/26 10:48:06 visual_prompt]: 	Training 500/553. train loss: 18.2334,	1.2782 s / batch. (data: 4.58e-01). ETA=14:44:39, max mem: 20.9 GB 
[11/26 10:48:57 visual_prompt]: Epoch 25 / 100: avg data time: 1.51e-01, avg batch time: 0.9817, average train loss: 34.4249
[11/26 10:49:53 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3107, average loss: 63.0027
[11/26 10:49:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.89	
[11/26 10:49:53 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/26 10:51:34 visual_prompt]: 	Training 100/553. train loss: 72.3546,	0.8320 s / batch. (data: 3.02e-04). ETA=9:33:43, max mem: 20.9 GB 
[11/26 10:53:13 visual_prompt]: 	Training 200/553. train loss: 101.9321,	1.4494 s / batch. (data: 6.33e-01). ETA=16:37:02, max mem: 20.9 GB 
[11/26 10:54:52 visual_prompt]: 	Training 300/553. train loss: 27.4071,	0.8360 s / batch. (data: 3.27e-04). ETA=9:33:42, max mem: 20.9 GB 
[11/26 10:56:29 visual_prompt]: 	Training 400/553. train loss: 30.2635,	0.8062 s / batch. (data: 3.11e-04). ETA=9:11:54, max mem: 20.9 GB 
[11/26 10:58:05 visual_prompt]: 	Training 500/553. train loss: 83.6241,	0.8352 s / batch. (data: 3.30e-04). ETA=9:30:23, max mem: 20.9 GB 
[11/26 10:58:56 visual_prompt]: Epoch 26 / 100: avg data time: 1.52e-01, avg batch time: 0.9829, average train loss: 31.8418
[11/26 10:59:52 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3109, average loss: 21.1550
[11/26 10:59:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.35	
[11/26 10:59:52 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[11/26 11:01:36 visual_prompt]: 	Training 100/553. train loss: 0.4042,	0.8194 s / batch. (data: 4.46e-04). ETA=9:17:28, max mem: 20.9 GB 
[11/26 11:03:13 visual_prompt]: 	Training 200/553. train loss: 3.8618,	0.9561 s / batch. (data: 1.33e-01). ETA=10:48:54, max mem: 20.9 GB 
[11/26 11:04:49 visual_prompt]: 	Training 300/553. train loss: 61.9464,	0.8360 s / batch. (data: 7.96e-03). ETA=9:26:00, max mem: 20.9 GB 
[11/26 11:06:28 visual_prompt]: 	Training 400/553. train loss: 5.1087,	0.8467 s / batch. (data: 7.84e-04). ETA=9:31:48, max mem: 20.9 GB 
[11/26 11:08:06 visual_prompt]: 	Training 500/553. train loss: 4.4242,	0.8400 s / batch. (data: 3.14e-04). ETA=9:25:55, max mem: 20.9 GB 
[11/26 11:08:55 visual_prompt]: Epoch 27 / 100: avg data time: 1.51e-01, avg batch time: 0.9815, average train loss: 28.6694
[11/26 11:09:51 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3105, average loss: 20.8540
[11/26 11:09:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[11/26 11:09:51 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[11/26 11:11:31 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8119 s / batch. (data: 7.86e-03). ETA=9:04:53, max mem: 20.9 GB 
[11/26 11:13:09 visual_prompt]: 	Training 200/553. train loss: 2.3331,	0.8288 s / batch. (data: 9.61e-03). ETA=9:14:52, max mem: 20.9 GB 
[11/26 11:14:48 visual_prompt]: 	Training 300/553. train loss: 10.2766,	1.5724 s / batch. (data: 7.53e-01). ETA=17:30:05, max mem: 20.9 GB 
[11/26 11:16:25 visual_prompt]: 	Training 400/553. train loss: 74.0288,	0.8480 s / batch. (data: 2.11e-02). ETA=9:24:51, max mem: 20.9 GB 
[11/26 11:18:01 visual_prompt]: 	Training 500/553. train loss: 41.2319,	0.8520 s / batch. (data: 2.97e-04). ETA=9:26:09, max mem: 20.9 GB 
[11/26 11:18:53 visual_prompt]: Epoch 28 / 100: avg data time: 1.49e-01, avg batch time: 0.9810, average train loss: 27.4799
[11/26 11:19:49 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3114, average loss: 42.9842
[11/26 11:19:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.68	
[11/26 11:19:49 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[11/26 11:21:37 visual_prompt]: 	Training 100/553. train loss: 0.1680,	0.8263 s / batch. (data: 3.10e-04). ETA=9:06:56, max mem: 20.9 GB 
[11/26 11:23:13 visual_prompt]: 	Training 200/553. train loss: 58.2281,	1.6760 s / batch. (data: 8.53e-01). ETA=18:26:36, max mem: 20.9 GB 
[11/26 11:24:50 visual_prompt]: 	Training 300/553. train loss: 37.3546,	0.8251 s / batch. (data: 3.27e-04). ETA=9:03:23, max mem: 20.9 GB 
[11/26 11:26:24 visual_prompt]: 	Training 400/553. train loss: 41.1207,	1.2840 s / batch. (data: 4.60e-01). ETA=14:03:29, max mem: 20.9 GB 
[11/26 11:28:01 visual_prompt]: 	Training 500/553. train loss: 12.3421,	0.8199 s / batch. (data: 3.08e-04). ETA=8:57:14, max mem: 20.9 GB 
[11/26 11:28:52 visual_prompt]: Epoch 29 / 100: avg data time: 1.49e-01, avg batch time: 0.9817, average train loss: 26.5714
[11/26 11:29:48 visual_prompt]: Inference (val):avg data time: 1.53e-04, avg batch time: 0.3098, average loss: 48.8234
[11/26 11:29:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.66	
[11/26 11:29:48 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[11/26 11:31:28 visual_prompt]: 	Training 100/553. train loss: 20.2464,	0.8539 s / batch. (data: 2.18e-02). ETA=9:17:20, max mem: 20.9 GB 
[11/26 11:33:06 visual_prompt]: 	Training 200/553. train loss: 11.1758,	0.8200 s / batch. (data: 3.27e-04). ETA=8:53:50, max mem: 20.9 GB 
[11/26 11:34:42 visual_prompt]: 	Training 300/553. train loss: 9.4657,	0.8348 s / batch. (data: 3.24e-04). ETA=9:02:07, max mem: 20.9 GB 
[11/26 11:36:21 visual_prompt]: 	Training 400/553. train loss: 10.0146,	1.0520 s / batch. (data: 2.12e-01). ETA=11:21:23, max mem: 20.9 GB 
[11/26 11:37:57 visual_prompt]: 	Training 500/553. train loss: 17.4269,	1.4204 s / batch. (data: 5.92e-01). ETA=15:17:38, max mem: 20.9 GB 
[11/26 11:38:50 visual_prompt]: Epoch 30 / 100: avg data time: 1.49e-01, avg batch time: 0.9802, average train loss: 26.5943
[11/26 11:39:45 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3101, average loss: 28.5451
[11/26 11:39:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.21	
[11/26 11:39:45 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[11/26 11:41:28 visual_prompt]: 	Training 100/553. train loss: 17.1579,	0.8400 s / batch. (data: 3.04e-04). ETA=9:00:31, max mem: 20.9 GB 
[11/26 11:43:08 visual_prompt]: 	Training 200/553. train loss: 26.0682,	0.8399 s / batch. (data: 7.95e-03). ETA=8:59:03, max mem: 20.9 GB 
[11/26 11:44:43 visual_prompt]: 	Training 300/553. train loss: 37.3984,	0.8440 s / batch. (data: 3.28e-04). ETA=9:00:17, max mem: 20.9 GB 
[11/26 11:46:19 visual_prompt]: 	Training 400/553. train loss: 6.7291,	0.8802 s / batch. (data: 5.05e-02). ETA=9:22:00, max mem: 20.9 GB 
[11/26 11:47:57 visual_prompt]: 	Training 500/553. train loss: 1.2326,	0.8519 s / batch. (data: 1.90e-02). ETA=9:02:29, max mem: 20.9 GB 
[11/26 11:48:48 visual_prompt]: Epoch 31 / 100: avg data time: 1.48e-01, avg batch time: 0.9804, average train loss: 29.0759
[11/26 11:49:44 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3103, average loss: 20.5089
[11/26 11:49:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.16	
[11/26 11:49:44 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[11/26 11:51:27 visual_prompt]: 	Training 100/553. train loss: 11.9108,	0.8560 s / batch. (data: 9.79e-04). ETA=9:02:57, max mem: 20.9 GB 
[11/26 11:53:05 visual_prompt]: 	Training 200/553. train loss: 70.7574,	0.8358 s / batch. (data: 1.18e-02). ETA=8:48:45, max mem: 20.9 GB 
[11/26 11:54:45 visual_prompt]: 	Training 300/553. train loss: 65.8313,	0.8277 s / batch. (data: 5.40e-03). ETA=8:42:15, max mem: 20.9 GB 
[11/26 11:56:23 visual_prompt]: 	Training 400/553. train loss: 4.8384,	0.8469 s / batch. (data: 3.46e-04). ETA=8:52:55, max mem: 20.9 GB 
[11/26 11:57:59 visual_prompt]: 	Training 500/553. train loss: 11.8337,	0.8440 s / batch. (data: 7.97e-03). ETA=8:49:42, max mem: 20.9 GB 
[11/26 11:58:48 visual_prompt]: Epoch 32 / 100: avg data time: 1.52e-01, avg batch time: 0.9837, average train loss: 29.1044
[11/26 11:59:43 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3097, average loss: 59.6293
[11/26 11:59:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.32	
[11/26 11:59:43 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[11/26 12:01:25 visual_prompt]: 	Training 100/553. train loss: 29.7008,	0.8200 s / batch. (data: 2.92e-04). ETA=8:32:33, max mem: 20.9 GB 
[11/26 12:03:04 visual_prompt]: 	Training 200/553. train loss: 10.7101,	1.3759 s / batch. (data: 5.32e-01). ETA=14:17:43, max mem: 20.9 GB 
[11/26 12:04:40 visual_prompt]: 	Training 300/553. train loss: 14.8973,	0.8466 s / batch. (data: 5.45e-03). ETA=8:46:19, max mem: 20.9 GB 
[11/26 12:06:19 visual_prompt]: 	Training 400/553. train loss: 29.4231,	0.8290 s / batch. (data: 8.99e-03). ETA=8:34:03, max mem: 20.9 GB 
[11/26 12:07:56 visual_prompt]: 	Training 500/553. train loss: 3.8533,	0.8289 s / batch. (data: 3.29e-04). ETA=8:32:36, max mem: 20.9 GB 
[11/26 12:08:46 visual_prompt]: Epoch 33 / 100: avg data time: 1.49e-01, avg batch time: 0.9817, average train loss: 30.7971
[11/26 12:09:42 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3091, average loss: 12.2733
[11/26 12:09:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.84	
[11/26 12:09:42 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[11/26 12:11:24 visual_prompt]: 	Training 100/553. train loss: 14.1192,	0.8477 s / batch. (data: 1.05e-02). ETA=8:42:03, max mem: 20.9 GB 
[11/26 12:13:00 visual_prompt]: 	Training 200/553. train loss: 21.7265,	0.8520 s / batch. (data: 1.20e-02). ETA=8:43:16, max mem: 20.9 GB 
[11/26 12:14:36 visual_prompt]: 	Training 300/553. train loss: 30.0338,	0.8600 s / batch. (data: 3.30e-04). ETA=8:46:45, max mem: 20.9 GB 
[11/26 12:16:16 visual_prompt]: 	Training 400/553. train loss: 22.0481,	0.8280 s / batch. (data: 2.97e-04). ETA=8:25:46, max mem: 20.9 GB 
[11/26 12:17:53 visual_prompt]: 	Training 500/553. train loss: 1.1315,	1.2769 s / batch. (data: 4.25e-01). ETA=12:57:50, max mem: 20.9 GB 
[11/26 12:18:43 visual_prompt]: Epoch 34 / 100: avg data time: 1.46e-01, avg batch time: 0.9786, average train loss: 27.4343
[11/26 12:19:39 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3126, average loss: 11.8197
[11/26 12:19:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.98	
[11/26 12:19:39 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[11/26 12:21:23 visual_prompt]: 	Training 100/553. train loss: 14.2905,	0.8356 s / batch. (data: 2.78e-04). ETA=8:26:54, max mem: 20.9 GB 
[11/26 12:23:00 visual_prompt]: 	Training 200/553. train loss: 29.9897,	0.8190 s / batch. (data: 3.52e-04). ETA=8:15:27, max mem: 20.9 GB 
[11/26 12:24:38 visual_prompt]: 	Training 300/553. train loss: 3.0011,	0.8330 s / batch. (data: 2.93e-04). ETA=8:22:31, max mem: 20.9 GB 
[11/26 12:26:14 visual_prompt]: 	Training 400/553. train loss: 0.7721,	0.8392 s / batch. (data: 3.12e-04). ETA=8:24:52, max mem: 20.9 GB 
[11/26 12:27:50 visual_prompt]: 	Training 500/553. train loss: 12.0348,	1.0171 s / batch. (data: 1.99e-01). ETA=10:10:12, max mem: 20.9 GB 
[11/26 12:28:42 visual_prompt]: Epoch 35 / 100: avg data time: 1.49e-01, avg batch time: 0.9807, average train loss: 30.2824
[11/26 12:29:38 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3092, average loss: 96.0170
[11/26 12:29:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.43	
[11/26 12:29:38 visual_prompt]: Stopping early.
[11/26 12:29:38 visual_prompt]: Rank of current process: 0. World size: 1
[11/26 12:29:38 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/26 12:29:38 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/26 12:29:38 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/26 12:29:38 visual_prompt]: Training with config:
[11/26 12:29:38 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/26 12:29:38 visual_prompt]: Loading training data...
[11/26 12:29:38 visual_prompt]: Constructing mammo-cbis dataset train...
[11/26 12:29:38 visual_prompt]: Loading validation data...
[11/26 12:29:38 visual_prompt]: Constructing mammo-cbis dataset val...
[11/26 12:29:38 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/26 12:29:46 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/26 12:29:46 visual_prompt]: tuned percent:0.525
[11/26 12:29:46 visual_prompt]: Device used for model: 0
[11/26 12:29:46 visual_prompt]: Setting up Evaluator...
[11/26 12:29:46 visual_prompt]: Setting up Trainer...
[11/26 12:29:46 visual_prompt]: 	Setting up the optimizer...
[11/26 12:29:46 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/26 12:31:27 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8600 s / batch. (data: 2.90e-04). ETA=13:11:12, max mem: 20.9 GB 
[11/26 12:33:04 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8564 s / batch. (data: 1.55e-02). ETA=13:06:30, max mem: 20.9 GB 
[11/26 12:34:44 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8427 s / batch. (data: 1.05e-02). ETA=12:52:27, max mem: 20.9 GB 
[11/26 12:36:20 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8440 s / batch. (data: 3.02e-04). ETA=12:52:14, max mem: 20.9 GB 
[11/26 12:38:00 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8183 s / batch. (data: 2.97e-04). ETA=12:27:23, max mem: 20.9 GB 
[11/26 12:38:51 visual_prompt]: Epoch 1 / 100: avg data time: 1.51e-01, avg batch time: 0.9847, average train loss: 1.5403
[11/26 12:39:47 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3101, average loss: 1.5201
[11/26 12:39:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/26 12:39:47 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/26 12:41:27 visual_prompt]: 	Training 100/553. train loss: 10.4643,	0.8795 s / batch. (data: 3.55e-02). ETA=13:21:04, max mem: 20.9 GB 
[11/26 12:43:04 visual_prompt]: 	Training 200/553. train loss: 0.0010,	0.8400 s / batch. (data: 3.26e-04). ETA=12:43:39, max mem: 20.9 GB 
[11/26 12:44:44 visual_prompt]: 	Training 300/553. train loss: 5.4646,	1.0904 s / batch. (data: 2.48e-01). ETA=16:29:27, max mem: 20.9 GB 
[11/26 12:46:20 visual_prompt]: 	Training 400/553. train loss: 5.3334,	0.8600 s / batch. (data: 7.96e-03). ETA=12:58:58, max mem: 20.9 GB 
[11/26 12:48:00 visual_prompt]: 	Training 500/553. train loss: 9.4859,	0.8533 s / batch. (data: 3.00e-04). ETA=12:51:30, max mem: 20.9 GB 
[11/26 12:48:50 visual_prompt]: Epoch 2 / 100: avg data time: 1.49e-01, avg batch time: 0.9827, average train loss: 4.9669
[11/26 12:49:46 visual_prompt]: Inference (val):avg data time: 4.16e-04, avg batch time: 0.3088, average loss: 17.1788
[11/26 12:49:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.88	
[11/26 12:49:46 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/26 12:51:27 visual_prompt]: 	Training 100/553. train loss: 5.1646,	0.8439 s / batch. (data: 5.64e-03). ETA=12:40:49, max mem: 20.9 GB 
[11/26 12:53:06 visual_prompt]: 	Training 200/553. train loss: 2.0174,	0.8185 s / batch. (data: 2.93e-04). ETA=12:16:32, max mem: 20.9 GB 
[11/26 12:54:43 visual_prompt]: 	Training 300/553. train loss: 3.7236,	0.8409 s / batch. (data: 5.52e-03). ETA=12:35:19, max mem: 20.9 GB 
[11/26 12:56:21 visual_prompt]: 	Training 400/553. train loss: 2.2168,	0.8280 s / batch. (data: 3.14e-04). ETA=12:22:20, max mem: 20.9 GB 
[11/26 12:58:00 visual_prompt]: 	Training 500/553. train loss: 6.8552,	1.1364 s / batch. (data: 3.10e-01). ETA=16:56:59, max mem: 20.9 GB 
[11/26 12:58:49 visual_prompt]: Epoch 3 / 100: avg data time: 1.49e-01, avg batch time: 0.9821, average train loss: 5.8513
[11/26 12:59:45 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3110, average loss: 14.7225
[11/26 12:59:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.89	
[11/26 12:59:45 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/26 13:01:28 visual_prompt]: 	Training 100/553. train loss: 0.7517,	0.8411 s / batch. (data: 1.05e-02). ETA=12:30:34, max mem: 20.9 GB 
[11/26 13:03:06 visual_prompt]: 	Training 200/553. train loss: 6.1720,	0.8240 s / batch. (data: 3.08e-04). ETA=12:13:55, max mem: 20.9 GB 
[11/26 13:04:43 visual_prompt]: 	Training 300/553. train loss: 2.6758,	1.2040 s / batch. (data: 3.86e-01). ETA=17:50:21, max mem: 20.9 GB 
[11/26 13:06:18 visual_prompt]: 	Training 400/553. train loss: 13.2228,	1.3613 s / batch. (data: 5.06e-01). ETA=20:07:54, max mem: 20.9 GB 
[11/26 13:07:57 visual_prompt]: 	Training 500/553. train loss: 0.6358,	3.3360 s / batch. (data: 2.49e+00). ETA=2 days, 1:14:39, max mem: 20.9 GB 
[11/26 13:08:49 visual_prompt]: Epoch 4 / 100: avg data time: 1.50e-01, avg batch time: 0.9828, average train loss: 10.6225
[11/26 13:09:46 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3104, average loss: 5.6601
[11/26 13:09:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.18	
[11/26 13:09:46 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/26 13:11:26 visual_prompt]: 	Training 100/553. train loss: 28.5359,	0.8449 s / batch. (data: 5.43e-03). ETA=12:26:08, max mem: 20.9 GB 
[11/26 13:13:04 visual_prompt]: 	Training 200/553. train loss: 2.2963,	0.9838 s / batch. (data: 1.63e-01). ETA=14:27:08, max mem: 20.9 GB 
[11/26 13:14:43 visual_prompt]: 	Training 300/553. train loss: 5.5521,	0.8194 s / batch. (data: 3.00e-04). ETA=12:00:55, max mem: 20.9 GB 
[11/26 13:16:19 visual_prompt]: 	Training 400/553. train loss: 3.1498,	0.8323 s / batch. (data: 3.04e-04). ETA=12:10:53, max mem: 20.9 GB 
[11/26 13:17:57 visual_prompt]: 	Training 500/553. train loss: 14.7823,	0.8480 s / batch. (data: 7.92e-03). ETA=12:23:13, max mem: 20.9 GB 
[11/26 13:18:49 visual_prompt]: Epoch 5 / 100: avg data time: 1.48e-01, avg batch time: 0.9819, average train loss: 11.0127
[11/26 13:19:44 visual_prompt]: Inference (val):avg data time: 2.31e-04, avg batch time: 0.3112, average loss: 19.4735
[11/26 13:19:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.91	
[11/26 13:19:44 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/26 13:21:27 visual_prompt]: 	Training 100/553. train loss: 5.0224,	0.8276 s / batch. (data: 3.62e-04). ETA=12:03:14, max mem: 20.9 GB 
[11/26 13:23:05 visual_prompt]: 	Training 200/553. train loss: 19.3753,	0.8582 s / batch. (data: 3.12e-04). ETA=12:28:33, max mem: 20.9 GB 
[11/26 13:24:40 visual_prompt]: 	Training 300/553. train loss: 3.0734,	0.8415 s / batch. (data: 3.18e-04). ETA=12:12:34, max mem: 20.9 GB 
[11/26 13:26:22 visual_prompt]: 	Training 400/553. train loss: 9.9597,	0.8359 s / batch. (data: 1.06e-02). ETA=12:06:19, max mem: 20.9 GB 
[11/26 13:27:59 visual_prompt]: 	Training 500/553. train loss: 16.4667,	0.8309 s / batch. (data: 5.44e-03). ETA=12:00:34, max mem: 20.9 GB 
[11/26 13:28:49 visual_prompt]: Epoch 6 / 100: avg data time: 1.50e-01, avg batch time: 0.9842, average train loss: 10.0695
[11/26 13:29:45 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3124, average loss: 8.6839
[11/26 13:29:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.73	
[11/26 13:29:45 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/26 13:31:25 visual_prompt]: 	Training 100/553. train loss: 8.0542,	0.8437 s / batch. (data: 2.36e-02). ETA=12:09:30, max mem: 20.9 GB 
[11/26 13:33:03 visual_prompt]: 	Training 200/553. train loss: 2.1701,	0.8597 s / batch. (data: 2.76e-02). ETA=12:21:56, max mem: 20.9 GB 
[11/26 13:34:44 visual_prompt]: 	Training 300/553. train loss: 27.3659,	1.9106 s / batch. (data: 1.09e+00). ETA=1 day, 3:25:42, max mem: 20.9 GB 
[11/26 13:36:21 visual_prompt]: 	Training 400/553. train loss: 0.6175,	1.5307 s / batch. (data: 7.00e-01). ETA=21:55:57, max mem: 20.9 GB 
[11/26 13:37:57 visual_prompt]: 	Training 500/553. train loss: 5.7167,	0.8440 s / batch. (data: 3.45e-04). ETA=12:04:10, max mem: 20.9 GB 
[11/26 13:38:47 visual_prompt]: Epoch 7 / 100: avg data time: 1.48e-01, avg batch time: 0.9809, average train loss: 12.7131
[11/26 13:39:43 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3094, average loss: 3.9841
[11/26 13:39:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.10	
[11/26 13:39:43 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/26 13:41:24 visual_prompt]: 	Training 100/553. train loss: 7.3276,	0.8692 s / batch. (data: 5.44e-03). ETA=12:23:34, max mem: 20.9 GB 
[11/26 13:43:03 visual_prompt]: 	Training 200/553. train loss: 51.3404,	0.8418 s / batch. (data: 4.84e-04). ETA=11:58:45, max mem: 20.9 GB 
[11/26 13:44:41 visual_prompt]: 	Training 300/553. train loss: 0.8880,	0.8242 s / batch. (data: 5.44e-03). ETA=11:42:18, max mem: 20.9 GB 
[11/26 13:46:19 visual_prompt]: 	Training 400/553. train loss: 19.9931,	0.8508 s / batch. (data: 5.46e-03). ETA=12:03:33, max mem: 20.9 GB 
[11/26 13:47:57 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.4277 s / batch. (data: 5.70e-01). ETA=20:11:52, max mem: 20.9 GB 
[11/26 13:48:49 visual_prompt]: Epoch 8 / 100: avg data time: 1.54e-01, avg batch time: 0.9862, average train loss: 25.0387
[11/26 13:49:44 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3092, average loss: 11.9663
[11/26 13:49:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.23	
[11/26 13:49:44 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/26 13:51:25 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8267 s / batch. (data: 3.01e-04). ETA=11:39:34, max mem: 20.9 GB 
[11/26 13:53:02 visual_prompt]: 	Training 200/553. train loss: 18.9466,	0.8250 s / batch. (data: 3.11e-04). ETA=11:36:48, max mem: 20.9 GB 
[11/26 13:54:40 visual_prompt]: 	Training 300/553. train loss: 1.4170,	1.5440 s / batch. (data: 7.00e-01). ETA=21:41:30, max mem: 20.9 GB 
[11/26 13:56:19 visual_prompt]: 	Training 400/553. train loss: 2.0662,	0.8445 s / batch. (data: 7.93e-04). ETA=11:50:25, max mem: 20.9 GB 
[11/26 13:57:57 visual_prompt]: 	Training 500/553. train loss: 12.9637,	0.9166 s / batch. (data: 7.76e-02). ETA=12:49:32, max mem: 20.9 GB 
[11/26 13:58:47 visual_prompt]: Epoch 9 / 100: avg data time: 1.49e-01, avg batch time: 0.9812, average train loss: 21.0203
[11/26 13:59:43 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3096, average loss: 80.6455
[11/26 13:59:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.99	
[11/26 13:59:43 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/26 14:01:27 visual_prompt]: 	Training 100/553. train loss: 54.4525,	0.8320 s / batch. (data: 3.12e-04). ETA=11:36:25, max mem: 20.9 GB 
[11/26 14:03:03 visual_prompt]: 	Training 200/553. train loss: 18.4336,	0.8389 s / batch. (data: 1.05e-02). ETA=11:40:49, max mem: 20.9 GB 
[11/26 14:04:39 visual_prompt]: 	Training 300/553. train loss: 29.5771,	1.0923 s / batch. (data: 2.65e-01). ETA=15:10:41, max mem: 20.9 GB 
[11/26 14:06:16 visual_prompt]: 	Training 400/553. train loss: 51.3899,	0.8567 s / batch. (data: 2.57e-02). ETA=11:52:49, max mem: 20.9 GB 
[11/26 14:07:55 visual_prompt]: 	Training 500/553. train loss: 3.2221,	0.8596 s / batch. (data: 1.16e-02). ETA=11:53:48, max mem: 20.9 GB 
[11/26 14:08:46 visual_prompt]: Epoch 10 / 100: avg data time: 1.51e-01, avg batch time: 0.9823, average train loss: 23.4597
[11/26 14:09:42 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3098, average loss: 35.3130
[11/26 14:09:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.92	
[11/26 14:09:42 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/26 14:11:26 visual_prompt]: 	Training 100/553. train loss: 49.0889,	0.8406 s / batch. (data: 2.46e-02). ETA=11:35:52, max mem: 20.9 GB 
[11/26 14:13:06 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8400 s / batch. (data: 1.19e-02). ETA=11:33:58, max mem: 20.9 GB 
[11/26 14:14:43 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.8108 s / batch. (data: 9.86e-01). ETA=1 day, 0:53:02, max mem: 20.9 GB 
[11/26 14:16:18 visual_prompt]: 	Training 400/553. train loss: 47.5715,	0.8260 s / batch. (data: 3.05e-04). ETA=11:19:38, max mem: 20.9 GB 
[11/26 14:17:55 visual_prompt]: 	Training 500/553. train loss: 90.2490,	0.8231 s / batch. (data: 1.13e-02). ETA=11:15:55, max mem: 20.9 GB 
[11/26 14:18:45 visual_prompt]: Epoch 11 / 100: avg data time: 1.52e-01, avg batch time: 0.9817, average train loss: 40.8844
[11/26 14:19:41 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3103, average loss: 19.9013
[11/26 14:19:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.81	
[11/26 14:19:41 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/26 14:21:24 visual_prompt]: 	Training 100/553. train loss: 33.3353,	0.8282 s / batch. (data: 3.52e-04). ETA=11:18:00, max mem: 20.9 GB 
[11/26 14:23:03 visual_prompt]: 	Training 200/553. train loss: 8.5810,	0.8384 s / batch. (data: 2.84e-04). ETA=11:24:53, max mem: 20.9 GB 
[11/26 14:24:39 visual_prompt]: 	Training 300/553. train loss: 37.9235,	0.8320 s / batch. (data: 3.12e-04). ETA=11:18:18, max mem: 20.9 GB 
[11/26 14:26:17 visual_prompt]: 	Training 400/553. train loss: 5.5246,	0.8318 s / batch. (data: 1.30e-03). ETA=11:16:43, max mem: 20.9 GB 
[11/26 14:27:54 visual_prompt]: 	Training 500/553. train loss: 361.9593,	0.8376 s / batch. (data: 7.96e-04). ETA=11:20:06, max mem: 20.9 GB 
[11/26 14:28:44 visual_prompt]: Epoch 12 / 100: avg data time: 1.51e-01, avg batch time: 0.9821, average train loss: 33.7319
[11/26 14:29:40 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3115, average loss: 6.8668
[11/26 14:29:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.22	
[11/26 14:29:40 visual_prompt]: Best epoch 12: best metric: -6.867
[11/26 14:29:40 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/26 14:31:23 visual_prompt]: 	Training 100/553. train loss: 23.7033,	0.8219 s / batch. (data: 2.93e-04). ETA=11:05:14, max mem: 20.9 GB 
[11/26 14:32:57 visual_prompt]: 	Training 200/553. train loss: 24.2083,	0.8460 s / batch. (data: 1.05e-02). ETA=11:23:19, max mem: 20.9 GB 
[11/26 14:34:36 visual_prompt]: 	Training 300/553. train loss: 11.9505,	1.6479 s / batch. (data: 8.00e-01). ETA=22:08:19, max mem: 20.9 GB 
[11/26 14:36:12 visual_prompt]: 	Training 400/553. train loss: 68.1326,	0.8245 s / batch. (data: 3.37e-04). ETA=11:03:15, max mem: 20.9 GB 
[11/26 14:37:51 visual_prompt]: 	Training 500/553. train loss: 43.9326,	0.8320 s / batch. (data: 4.21e-04). ETA=11:07:50, max mem: 20.9 GB 
[11/26 14:38:41 visual_prompt]: Epoch 13 / 100: avg data time: 1.48e-01, avg batch time: 0.9788, average train loss: 31.1751
[11/26 14:39:37 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3093, average loss: 9.2663
[11/26 14:39:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.03	
[11/26 14:39:37 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/26 14:41:20 visual_prompt]: 	Training 100/553. train loss: 9.2130,	0.8374 s / batch. (data: 1.05e-02). ETA=11:10:02, max mem: 20.9 GB 
[11/26 14:42:57 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.0240 s / batch. (data: 1.81e-01). ETA=13:37:39, max mem: 20.9 GB 
[11/26 14:44:35 visual_prompt]: 	Training 300/553. train loss: 19.4283,	0.8323 s / batch. (data: 3.26e-04). ETA=11:03:12, max mem: 20.9 GB 
[11/26 14:46:12 visual_prompt]: 	Training 400/553. train loss: 13.2197,	0.8360 s / batch. (data: 3.00e-04). ETA=11:04:45, max mem: 20.9 GB 
[11/26 14:47:49 visual_prompt]: 	Training 500/553. train loss: 6.1606,	0.8555 s / batch. (data: 3.10e-02). ETA=11:18:50, max mem: 20.9 GB 
[11/26 14:48:39 visual_prompt]: Epoch 14 / 100: avg data time: 1.49e-01, avg batch time: 0.9802, average train loss: 30.6143
[11/26 14:49:35 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3096, average loss: 9.9501
[11/26 14:49:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.47	
[11/26 14:49:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/26 14:51:16 visual_prompt]: 	Training 100/553. train loss: 16.6267,	0.8462 s / batch. (data: 1.01e-02). ETA=11:09:21, max mem: 20.9 GB 
[11/26 14:52:52 visual_prompt]: 	Training 200/553. train loss: 84.0703,	0.8440 s / batch. (data: 4.18e-04). ETA=11:06:08, max mem: 20.9 GB 
[11/26 14:54:32 visual_prompt]: 	Training 300/553. train loss: 69.5441,	0.8559 s / batch. (data: 8.08e-04). ETA=11:14:09, max mem: 20.9 GB 
[11/26 14:56:07 visual_prompt]: 	Training 400/553. train loss: 11.5677,	1.0479 s / batch. (data: 1.95e-01). ETA=13:43:36, max mem: 20.9 GB 
[11/26 14:57:46 visual_prompt]: 	Training 500/553. train loss: 1.6814,	0.8238 s / batch. (data: 4.17e-04). ETA=10:46:04, max mem: 20.9 GB 
[11/26 14:58:37 visual_prompt]: Epoch 15 / 100: avg data time: 1.49e-01, avg batch time: 0.9809, average train loss: 28.5416
[11/26 14:59:33 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3094, average loss: 5.6738
[11/26 14:59:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.66	
[11/26 14:59:33 visual_prompt]: Best epoch 15: best metric: -5.674
[11/26 14:59:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/26 15:01:14 visual_prompt]: 	Training 100/553. train loss: 2.0761,	0.8624 s / batch. (data: 4.22e-04). ETA=11:14:12, max mem: 20.9 GB 
[11/26 15:02:52 visual_prompt]: 	Training 200/553. train loss: 74.7176,	0.8190 s / batch. (data: 3.39e-04). ETA=10:38:54, max mem: 20.9 GB 
[11/26 15:04:30 visual_prompt]: 	Training 300/553. train loss: 70.4559,	0.8253 s / batch. (data: 3.40e-04). ETA=10:42:23, max mem: 20.9 GB 
[11/26 15:06:08 visual_prompt]: 	Training 400/553. train loss: 32.9066,	0.8186 s / batch. (data: 3.00e-04). ETA=10:35:48, max mem: 20.9 GB 
[11/26 15:07:45 visual_prompt]: 	Training 500/553. train loss: 10.4075,	1.2459 s / batch. (data: 4.29e-01). ETA=16:05:40, max mem: 20.9 GB 
[11/26 15:08:37 visual_prompt]: Epoch 16 / 100: avg data time: 1.52e-01, avg batch time: 0.9828, average train loss: 35.6242
[11/26 15:09:32 visual_prompt]: Inference (val):avg data time: 3.73e-04, avg batch time: 0.3117, average loss: 27.4504
[11/26 15:09:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.72	
[11/26 15:09:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/26 15:11:13 visual_prompt]: 	Training 100/553. train loss: 10.0498,	0.8520 s / batch. (data: 7.98e-03). ETA=10:58:11, max mem: 20.9 GB 
[11/26 15:12:52 visual_prompt]: 	Training 200/553. train loss: 2.1062,	0.8313 s / batch. (data: 1.04e-02). ETA=10:40:50, max mem: 20.9 GB 
[11/26 15:14:28 visual_prompt]: 	Training 300/553. train loss: 47.0998,	0.8320 s / batch. (data: 7.95e-03). ETA=10:39:58, max mem: 20.9 GB 
[11/26 15:16:06 visual_prompt]: 	Training 400/553. train loss: 48.3595,	1.1613 s / batch. (data: 3.40e-01). ETA=14:51:19, max mem: 20.9 GB 
[11/26 15:17:43 visual_prompt]: 	Training 500/553. train loss: 1.4778,	1.0080 s / batch. (data: 1.77e-01). ETA=12:51:59, max mem: 20.9 GB 
[11/26 15:18:35 visual_prompt]: Epoch 17 / 100: avg data time: 1.50e-01, avg batch time: 0.9814, average train loss: 27.8454
[11/26 15:19:31 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3110, average loss: 27.0778
[11/26 15:19:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.98	
[11/26 15:19:31 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/26 15:21:12 visual_prompt]: 	Training 100/553. train loss: 38.1946,	0.8282 s / batch. (data: 3.41e-04). ETA=10:32:09, max mem: 20.9 GB 
[11/26 15:22:55 visual_prompt]: 	Training 200/553. train loss: 16.9148,	0.8360 s / batch. (data: 3.14e-04). ETA=10:36:45, max mem: 20.9 GB 
[11/26 15:24:36 visual_prompt]: 	Training 300/553. train loss: 4.3457,	0.8246 s / batch. (data: 9.54e-03). ETA=10:26:40, max mem: 20.9 GB 
[11/26 15:26:16 visual_prompt]: 	Training 400/553. train loss: 22.5713,	0.8329 s / batch. (data: 5.43e-03). ETA=10:31:35, max mem: 20.9 GB 
[11/26 15:27:56 visual_prompt]: 	Training 500/553. train loss: 4.5779,	0.8219 s / batch. (data: 3.15e-04). ETA=10:21:52, max mem: 20.9 GB 
[11/26 15:28:47 visual_prompt]: Epoch 18 / 100: avg data time: 1.76e-01, avg batch time: 1.0052, average train loss: 34.1536
[11/26 15:29:43 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3109, average loss: 8.4810
[11/26 15:29:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.20	
[11/26 15:29:43 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/26 15:31:24 visual_prompt]: 	Training 100/553. train loss: 9.2575,	0.8285 s / batch. (data: 3.14e-04). ETA=10:24:46, max mem: 20.9 GB 
[11/26 15:33:02 visual_prompt]: 	Training 200/553. train loss: 21.5479,	0.8275 s / batch. (data: 5.43e-03). ETA=10:22:39, max mem: 20.9 GB 
[11/26 15:34:41 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8428 s / batch. (data: 3.29e-04). ETA=10:32:45, max mem: 20.9 GB 
[11/26 15:36:18 visual_prompt]: 	Training 400/553. train loss: 9.2315,	0.8321 s / batch. (data: 2.88e-04). ETA=10:23:21, max mem: 20.9 GB 
[11/26 15:37:52 visual_prompt]: 	Training 500/553. train loss: 0.2463,	0.8224 s / batch. (data: 1.87e-03). ETA=10:14:41, max mem: 20.9 GB 
[11/26 15:38:44 visual_prompt]: Epoch 19 / 100: avg data time: 1.45e-01, avg batch time: 0.9773, average train loss: 26.0521
[11/26 15:39:39 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3108, average loss: 82.6336
[11/26 15:39:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.47	
[11/26 15:39:39 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/26 15:41:19 visual_prompt]: 	Training 100/553. train loss: 8.7580,	0.8346 s / batch. (data: 7.95e-03). ETA=10:21:41, max mem: 20.9 GB 
[11/26 15:42:58 visual_prompt]: 	Training 200/553. train loss: 1.7225,	0.8360 s / batch. (data: 1.20e-02). ETA=10:21:20, max mem: 20.9 GB 
[11/26 15:44:36 visual_prompt]: 	Training 300/553. train loss: 5.0633,	0.8331 s / batch. (data: 3.20e-04). ETA=10:17:46, max mem: 20.9 GB 
[11/26 15:46:13 visual_prompt]: 	Training 400/553. train loss: 18.6988,	0.8154 s / batch. (data: 3.46e-04). ETA=10:03:17, max mem: 20.9 GB 
[11/26 15:47:50 visual_prompt]: 	Training 500/553. train loss: 0.4188,	0.8183 s / batch. (data: 2.96e-04). ETA=10:04:03, max mem: 20.9 GB 
[11/26 15:48:42 visual_prompt]: Epoch 20 / 100: avg data time: 1.49e-01, avg batch time: 0.9811, average train loss: 28.4255
[11/26 15:49:38 visual_prompt]: Inference (val):avg data time: 1.29e-04, avg batch time: 0.3094, average loss: 20.9842
[11/26 15:49:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.62	
[11/26 15:49:38 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/26 15:51:21 visual_prompt]: 	Training 100/553. train loss: 31.0035,	0.8280 s / batch. (data: 3.99e-03). ETA=10:09:07, max mem: 20.9 GB 
[11/26 15:52:58 visual_prompt]: 	Training 200/553. train loss: 56.9735,	0.8391 s / batch. (data: 9.61e-03). ETA=10:15:55, max mem: 20.9 GB 
[11/26 15:54:34 visual_prompt]: 	Training 300/553. train loss: 62.3923,	0.8320 s / batch. (data: 3.06e-04). ETA=10:09:19, max mem: 20.9 GB 
[11/26 15:56:13 visual_prompt]: 	Training 400/553. train loss: 42.8589,	0.8320 s / batch. (data: 2.95e-04). ETA=10:07:55, max mem: 20.9 GB 
[11/26 15:57:51 visual_prompt]: 	Training 500/553. train loss: 15.1508,	0.8531 s / batch. (data: 2.16e-02). ETA=10:21:54, max mem: 20.9 GB 
[11/26 15:58:41 visual_prompt]: Epoch 21 / 100: avg data time: 1.51e-01, avg batch time: 0.9831, average train loss: 22.8141
[11/26 15:59:37 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3096, average loss: 1.3783
[11/26 15:59:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.36	
[11/26 15:59:37 visual_prompt]: Best epoch 21: best metric: -1.378
[11/26 15:59:37 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/26 16:01:19 visual_prompt]: 	Training 100/553. train loss: 19.0938,	0.8323 s / batch. (data: 3.02e-04). ETA=10:04:39, max mem: 20.9 GB 
[11/26 16:02:56 visual_prompt]: 	Training 200/553. train loss: 4.2181,	0.8360 s / batch. (data: 3.05e-04). ETA=10:05:55, max mem: 20.9 GB 
[11/26 16:04:32 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8674 s / batch. (data: 5.43e-03). ETA=10:27:11, max mem: 20.9 GB 
[11/26 16:06:10 visual_prompt]: 	Training 400/553. train loss: 35.0689,	0.8329 s / batch. (data: 2.99e-04). ETA=10:00:53, max mem: 20.9 GB 
[11/26 16:07:48 visual_prompt]: 	Training 500/553. train loss: 61.7992,	0.8280 s / batch. (data: 5.45e-03). ETA=9:56:00, max mem: 20.9 GB 
[11/26 16:08:40 visual_prompt]: Epoch 22 / 100: avg data time: 1.49e-01, avg batch time: 0.9811, average train loss: 28.6100
[11/26 16:09:35 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3097, average loss: 40.7791
[11/26 16:09:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.47	
[11/26 16:09:35 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/26 16:11:18 visual_prompt]: 	Training 100/553. train loss: 17.7667,	0.8283 s / batch. (data: 3.06e-04). ETA=9:54:03, max mem: 20.9 GB 
[11/26 16:12:56 visual_prompt]: 	Training 200/553. train loss: 75.8020,	0.8242 s / batch. (data: 1.05e-02). ETA=9:49:47, max mem: 20.9 GB 
[11/26 16:14:35 visual_prompt]: 	Training 300/553. train loss: 12.9878,	0.8211 s / batch. (data: 2.95e-04). ETA=9:46:09, max mem: 20.9 GB 
[11/26 16:16:11 visual_prompt]: 	Training 400/553. train loss: 5.6556,	0.8403 s / batch. (data: 7.24e-04). ETA=9:58:30, max mem: 20.9 GB 
[11/26 16:17:47 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8200 s / batch. (data: 2.88e-04). ETA=9:42:39, max mem: 20.9 GB 
[11/26 16:18:37 visual_prompt]: Epoch 23 / 100: avg data time: 1.46e-01, avg batch time: 0.9788, average train loss: 26.6718
[11/26 16:19:33 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3081, average loss: 2.5826
[11/26 16:19:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.97	
[11/26 16:19:33 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/26 16:21:11 visual_prompt]: 	Training 100/553. train loss: 99.1741,	0.8126 s / batch. (data: 3.40e-04). ETA=9:35:18, max mem: 20.9 GB 
[11/26 16:22:49 visual_prompt]: 	Training 200/553. train loss: 30.3081,	0.9004 s / batch. (data: 2.83e-02). ETA=10:35:58, max mem: 20.9 GB 
[11/26 16:24:28 visual_prompt]: 	Training 300/553. train loss: 11.2261,	0.9800 s / batch. (data: 1.46e-01). ETA=11:30:35, max mem: 20.9 GB 
[11/26 16:26:06 visual_prompt]: 	Training 400/553. train loss: 1.6055,	0.8455 s / batch. (data: 9.16e-03). ETA=9:54:22, max mem: 20.9 GB 
[11/26 16:27:45 visual_prompt]: 	Training 500/553. train loss: 91.2325,	0.8245 s / batch. (data: 5.50e-03). ETA=9:38:15, max mem: 20.9 GB 
[11/26 16:28:37 visual_prompt]: Epoch 24 / 100: avg data time: 1.52e-01, avg batch time: 0.9842, average train loss: 29.2039
[11/26 16:29:33 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3116, average loss: 2.2943
[11/26 16:29:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.61	
[11/26 16:29:33 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/26 16:31:17 visual_prompt]: 	Training 100/553. train loss: 11.0729,	0.8397 s / batch. (data: 3.15e-04). ETA=9:46:46, max mem: 20.9 GB 
[11/26 16:32:52 visual_prompt]: 	Training 200/553. train loss: 41.8671,	0.8560 s / batch. (data: 3.25e-04). ETA=9:56:43, max mem: 20.9 GB 
[11/26 16:34:29 visual_prompt]: 	Training 300/553. train loss: 31.9652,	0.9846 s / batch. (data: 1.34e-01). ETA=11:24:43, max mem: 20.9 GB 
[11/26 16:36:06 visual_prompt]: 	Training 400/553. train loss: 5.9064,	1.2470 s / batch. (data: 4.07e-01). ETA=14:25:10, max mem: 20.9 GB 
[11/26 16:37:45 visual_prompt]: 	Training 500/553. train loss: 37.1354,	1.4120 s / batch. (data: 5.80e-01). ETA=16:17:15, max mem: 20.9 GB 
[11/26 16:38:36 visual_prompt]: Epoch 25 / 100: avg data time: 1.52e-01, avg batch time: 0.9826, average train loss: 39.3230
[11/26 16:39:31 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3107, average loss: 95.5960
[11/26 16:39:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.02	
[11/26 16:39:31 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/26 16:41:13 visual_prompt]: 	Training 100/553. train loss: 20.8582,	0.8384 s / batch. (data: 7.95e-03). ETA=9:38:09, max mem: 20.9 GB 
[11/26 16:42:52 visual_prompt]: 	Training 200/553. train loss: 25.1038,	1.7245 s / batch. (data: 9.01e-01). ETA=19:46:17, max mem: 20.9 GB 
[11/26 16:44:31 visual_prompt]: 	Training 300/553. train loss: 26.8676,	0.8203 s / batch. (data: 3.14e-04). ETA=9:22:55, max mem: 20.9 GB 
[11/26 16:46:07 visual_prompt]: 	Training 400/553. train loss: 48.3365,	0.8160 s / batch. (data: 2.93e-04). ETA=9:18:37, max mem: 20.9 GB 
[11/26 16:47:43 visual_prompt]: 	Training 500/553. train loss: 8.3600,	0.8373 s / batch. (data: 7.96e-03). ETA=9:31:49, max mem: 20.9 GB 
[11/26 16:48:34 visual_prompt]: Epoch 26 / 100: avg data time: 1.49e-01, avg batch time: 0.9807, average train loss: 25.6008
[11/26 16:49:30 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3096, average loss: 31.2810
[11/26 16:49:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.85	
[11/26 16:49:30 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[11/26 16:51:12 visual_prompt]: 	Training 100/553. train loss: 1.9237,	0.8280 s / batch. (data: 2.98e-04). ETA=9:23:20, max mem: 20.9 GB 
[11/26 16:52:49 visual_prompt]: 	Training 200/553. train loss: 16.5352,	1.0200 s / batch. (data: 1.93e-01). ETA=11:32:16, max mem: 20.9 GB 
[11/26 16:54:28 visual_prompt]: 	Training 300/553. train loss: 26.6875,	0.8440 s / batch. (data: 3.19e-04). ETA=9:31:24, max mem: 20.9 GB 
[11/26 16:56:06 visual_prompt]: 	Training 400/553. train loss: 22.0815,	0.8550 s / batch. (data: 7.79e-04). ETA=9:37:27, max mem: 20.9 GB 
[11/26 16:57:45 visual_prompt]: 	Training 500/553. train loss: 22.2407,	0.8673 s / batch. (data: 8.05e-04). ETA=9:44:17, max mem: 20.9 GB 
[11/26 16:58:33 visual_prompt]: Epoch 27 / 100: avg data time: 1.51e-01, avg batch time: 0.9831, average train loss: 29.5681
[11/26 16:59:29 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3088, average loss: 9.5758
[11/26 16:59:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.81	
[11/26 16:59:29 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[11/26 17:01:09 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8218 s / batch. (data: 3.35e-04). ETA=9:11:31, max mem: 20.9 GB 
[11/26 17:02:47 visual_prompt]: 	Training 200/553. train loss: 6.5409,	0.8480 s / batch. (data: 2.67e-04). ETA=9:27:44, max mem: 20.9 GB 
[11/26 17:04:26 visual_prompt]: 	Training 300/553. train loss: 50.0483,	1.3754 s / batch. (data: 5.44e-01). ETA=15:18:32, max mem: 20.9 GB 
[11/26 17:06:02 visual_prompt]: 	Training 400/553. train loss: 83.2198,	0.8278 s / batch. (data: 3.02e-04). ETA=9:11:25, max mem: 20.9 GB 
[11/26 17:07:38 visual_prompt]: 	Training 500/553. train loss: 151.5318,	0.8433 s / batch. (data: 2.77e-04). ETA=9:20:23, max mem: 20.9 GB 
[11/26 17:08:29 visual_prompt]: Epoch 28 / 100: avg data time: 1.46e-01, avg batch time: 0.9770, average train loss: 33.4609
[11/26 17:09:25 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3093, average loss: 80.3306
[11/26 17:09:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.43	
[11/26 17:09:25 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[11/26 17:11:13 visual_prompt]: 	Training 100/553. train loss: 1.9684,	0.8320 s / batch. (data: 7.95e-03). ETA=9:10:43, max mem: 20.9 GB 
[11/26 17:12:50 visual_prompt]: 	Training 200/553. train loss: 0.9400,	1.7051 s / batch. (data: 8.83e-01). ETA=18:45:47, max mem: 20.9 GB 
[11/26 17:14:26 visual_prompt]: 	Training 300/553. train loss: 7.0530,	0.8383 s / batch. (data: 7.17e-04). ETA=9:12:06, max mem: 20.9 GB 
[11/26 17:16:00 visual_prompt]: 	Training 400/553. train loss: 34.1399,	1.1064 s / batch. (data: 2.91e-01). ETA=12:06:51, max mem: 20.9 GB 
[11/26 17:17:38 visual_prompt]: 	Training 500/553. train loss: 18.4471,	0.8281 s / batch. (data: 5.45e-03). ETA=9:02:35, max mem: 20.9 GB 
[11/26 17:18:29 visual_prompt]: Epoch 29 / 100: avg data time: 1.50e-01, avg batch time: 0.9826, average train loss: 24.1689
[11/26 17:19:24 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3100, average loss: 9.8496
[11/26 17:19:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.99	
[11/26 17:19:24 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[11/26 17:21:05 visual_prompt]: 	Training 100/553. train loss: 60.4559,	0.8520 s / batch. (data: 2.69e-04). ETA=9:16:08, max mem: 20.9 GB 
[11/26 17:22:44 visual_prompt]: 	Training 200/553. train loss: 74.9746,	0.8267 s / batch. (data: 3.30e-04). ETA=8:58:12, max mem: 20.9 GB 
[11/26 17:24:19 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.0002 s / batch. (data: 1.81e-01). ETA=10:49:29, max mem: 20.9 GB 
[11/26 17:25:58 visual_prompt]: 	Training 400/553. train loss: 12.4831,	0.9356 s / batch. (data: 9.70e-02). ETA=10:06:01, max mem: 20.9 GB 
[11/26 17:27:35 visual_prompt]: 	Training 500/553. train loss: 11.8023,	1.3111 s / batch. (data: 4.66e-01). ETA=14:07:02, max mem: 20.9 GB 
[11/26 17:28:27 visual_prompt]: Epoch 30 / 100: avg data time: 1.51e-01, avg batch time: 0.9813, average train loss: 32.8756
[11/26 17:29:23 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3098, average loss: 43.5503
[11/26 17:29:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.66	
[11/26 17:29:23 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[11/26 17:31:06 visual_prompt]: 	Training 100/553. train loss: 11.9113,	0.8373 s / batch. (data: 2.07e-02). ETA=8:58:46, max mem: 20.9 GB 
[11/26 17:32:45 visual_prompt]: 	Training 200/553. train loss: 36.5928,	0.8560 s / batch. (data: 3.60e-04). ETA=9:09:22, max mem: 20.9 GB 
[11/26 17:34:20 visual_prompt]: 	Training 300/553. train loss: 67.0444,	0.8520 s / batch. (data: 5.44e-03). ETA=9:05:24, max mem: 20.9 GB 
[11/26 17:35:57 visual_prompt]: 	Training 400/553. train loss: 103.4922,	0.8862 s / batch. (data: 6.52e-02). ETA=9:25:50, max mem: 20.9 GB 
[11/26 17:37:35 visual_prompt]: 	Training 500/553. train loss: 14.1946,	0.8527 s / batch. (data: 2.07e-02). ETA=9:03:03, max mem: 20.9 GB 
[11/26 17:38:25 visual_prompt]: Epoch 31 / 100: avg data time: 1.48e-01, avg batch time: 0.9796, average train loss: 28.0165
[11/26 17:39:21 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3113, average loss: 30.4209
[11/26 17:39:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.14	
[11/26 17:39:21 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[11/26 17:41:04 visual_prompt]: 	Training 100/553. train loss: 2.3228,	0.8389 s / batch. (data: 5.95e-03). ETA=8:52:07, max mem: 20.9 GB 
[11/26 17:42:41 visual_prompt]: 	Training 200/553. train loss: 34.4871,	0.8295 s / batch. (data: 5.43e-03). ETA=8:44:43, max mem: 20.9 GB 
[11/26 17:44:22 visual_prompt]: 	Training 300/553. train loss: 5.7643,	0.8234 s / batch. (data: 2.99e-04). ETA=8:39:32, max mem: 20.9 GB 
[11/26 17:46:00 visual_prompt]: 	Training 400/553. train loss: 13.2650,	0.8399 s / batch. (data: 3.14e-04). ETA=8:48:30, max mem: 20.9 GB 
[11/26 17:47:35 visual_prompt]: 	Training 500/553. train loss: 23.5068,	0.8245 s / batch. (data: 3.18e-04). ETA=8:37:26, max mem: 20.9 GB 
[11/26 17:48:25 visual_prompt]: Epoch 32 / 100: avg data time: 1.51e-01, avg batch time: 0.9827, average train loss: 25.0306
[11/26 17:49:21 visual_prompt]: Inference (val):avg data time: 1.57e-04, avg batch time: 0.3104, average loss: 22.8979
[11/26 17:49:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.84	
[11/26 17:49:21 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[11/26 17:51:01 visual_prompt]: 	Training 100/553. train loss: 0.0000,	1.1520 s / batch. (data: 3.20e-01). ETA=12:00:04, max mem: 20.9 GB 
[11/26 17:52:41 visual_prompt]: 	Training 200/553. train loss: 6.4431,	0.8529 s / batch. (data: 3.32e-02). ETA=8:51:41, max mem: 20.9 GB 
[11/26 17:54:18 visual_prompt]: 	Training 300/553. train loss: 6.4296,	0.8327 s / batch. (data: 7.95e-03). ETA=8:37:41, max mem: 20.9 GB 
[11/26 17:55:57 visual_prompt]: 	Training 400/553. train loss: 9.9908,	0.8367 s / batch. (data: 7.95e-03). ETA=8:38:49, max mem: 20.9 GB 
[11/26 17:57:34 visual_prompt]: 	Training 500/553. train loss: 21.1172,	0.8217 s / batch. (data: 3.00e-04). ETA=8:28:06, max mem: 20.9 GB 
[11/26 17:58:24 visual_prompt]: Epoch 33 / 100: avg data time: 1.50e-01, avg batch time: 0.9822, average train loss: 22.6111
[11/26 17:59:21 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3099, average loss: 99.1680
[11/26 17:59:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.55	
[11/26 17:59:21 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[11/26 18:01:06 visual_prompt]: 	Training 100/553. train loss: 1.5003,	0.8320 s / batch. (data: 4.15e-04). ETA=8:32:21, max mem: 20.9 GB 
[11/26 18:02:44 visual_prompt]: 	Training 200/553. train loss: 5.6217,	0.8481 s / batch. (data: 1.20e-02). ETA=8:40:52, max mem: 20.9 GB 
[11/26 18:04:22 visual_prompt]: 	Training 300/553. train loss: 25.5805,	0.8407 s / batch. (data: 3.83e-04). ETA=8:34:55, max mem: 20.9 GB 
[11/26 18:06:04 visual_prompt]: 	Training 400/553. train loss: 4.7217,	0.8400 s / batch. (data: 2.94e-04). ETA=8:33:06, max mem: 20.9 GB 
[11/26 18:07:45 visual_prompt]: 	Training 500/553. train loss: 1.9305,	1.4943 s / batch. (data: 6.59e-01). ETA=15:10:19, max mem: 20.9 GB 
[11/26 18:08:36 visual_prompt]: Epoch 34 / 100: avg data time: 1.71e-01, avg batch time: 1.0033, average train loss: 21.0316
[11/26 18:09:33 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3105, average loss: 14.2374
[11/26 18:09:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.20	
[11/26 18:09:33 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[11/26 18:11:18 visual_prompt]: 	Training 100/553. train loss: 14.9943,	0.8473 s / batch. (data: 1.12e-02). ETA=8:33:59, max mem: 20.9 GB 
[11/26 18:13:01 visual_prompt]: 	Training 200/553. train loss: 24.9822,	0.8440 s / batch. (data: 3.25e-04). ETA=8:30:36, max mem: 20.9 GB 
[11/26 18:14:39 visual_prompt]: 	Training 300/553. train loss: 18.4030,	0.8560 s / batch. (data: 1.20e-02). ETA=8:36:24, max mem: 20.9 GB 
[11/26 18:16:17 visual_prompt]: 	Training 400/553. train loss: 50.2889,	0.8966 s / batch. (data: 6.53e-02). ETA=8:59:25, max mem: 20.9 GB 
[11/26 18:17:58 visual_prompt]: 	Training 500/553. train loss: 4.6468,	1.0399 s / batch. (data: 2.10e-01). ETA=10:23:53, max mem: 20.9 GB 
[11/26 18:18:50 visual_prompt]: Epoch 35 / 100: avg data time: 1.75e-01, avg batch time: 1.0075, average train loss: 20.0561
[11/26 18:19:48 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3099, average loss: 4.8598
[11/26 18:19:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.05	
[11/26 18:19:48 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[11/26 18:21:32 visual_prompt]: 	Training 100/553. train loss: 38.1920,	0.8374 s / batch. (data: 3.43e-04). ETA=8:20:17, max mem: 20.9 GB 
[11/26 18:23:13 visual_prompt]: 	Training 200/553. train loss: 44.7937,	0.8200 s / batch. (data: 3.31e-04). ETA=8:08:30, max mem: 20.9 GB 
[11/26 18:24:55 visual_prompt]: 	Training 300/553. train loss: 12.5179,	0.8240 s / batch. (data: 3.04e-04). ETA=8:09:32, max mem: 20.9 GB 
[11/26 18:26:35 visual_prompt]: 	Training 400/553. train loss: 16.2229,	0.8442 s / batch. (data: 2.93e-04). ETA=8:20:08, max mem: 20.9 GB 
[11/26 18:28:15 visual_prompt]: 	Training 500/553. train loss: 8.0568,	1.0889 s / batch. (data: 2.69e-01). ETA=10:43:14, max mem: 20.9 GB 
[11/26 18:29:05 visual_prompt]: Epoch 36 / 100: avg data time: 1.75e-01, avg batch time: 1.0062, average train loss: 22.4611
[11/26 18:30:02 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3091, average loss: 71.6065
[11/26 18:30:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.06	
[11/26 18:30:02 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[11/26 18:31:47 visual_prompt]: 	Training 100/553. train loss: 11.0589,	0.8525 s / batch. (data: 1.05e-02). ETA=8:21:26, max mem: 20.9 GB 
[11/26 18:33:27 visual_prompt]: 	Training 200/553. train loss: 17.2239,	0.8274 s / batch. (data: 1.05e-02). ETA=8:05:18, max mem: 20.9 GB 
[11/26 18:35:07 visual_prompt]: 	Training 300/553. train loss: 0.3063,	1.3174 s / batch. (data: 4.92e-01). ETA=12:50:30, max mem: 20.9 GB 
[11/26 18:36:50 visual_prompt]: 	Training 400/553. train loss: 24.4959,	1.6708 s / batch. (data: 8.54e-01). ETA=16:14:23, max mem: 20.9 GB 
[11/26 18:38:27 visual_prompt]: 	Training 500/553. train loss: 12.7340,	1.1207 s / batch. (data: 2.74e-01). ETA=10:51:41, max mem: 20.9 GB 
[11/26 18:39:21 visual_prompt]: Epoch 37 / 100: avg data time: 1.78e-01, avg batch time: 1.0095, average train loss: 22.0481
[11/26 18:40:18 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3098, average loss: 2.0686
[11/26 18:40:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.92	
[11/26 18:40:18 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[11/26 18:42:00 visual_prompt]: 	Training 100/553. train loss: 1.5612,	0.8667 s / batch. (data: 1.05e-02). ETA=8:21:49, max mem: 20.9 GB 
[11/26 18:43:41 visual_prompt]: 	Training 200/553. train loss: 12.0778,	1.0880 s / batch. (data: 2.53e-01). ETA=10:28:08, max mem: 20.9 GB 
[11/26 18:45:23 visual_prompt]: 	Training 300/553. train loss: 26.0816,	0.8440 s / batch. (data: 3.09e-04). ETA=8:05:50, max mem: 20.9 GB 
[11/26 18:47:01 visual_prompt]: 	Training 400/553. train loss: 24.8220,	0.8431 s / batch. (data: 1.19e-02). ETA=8:03:54, max mem: 20.9 GB 
[11/26 18:48:43 visual_prompt]: 	Training 500/553. train loss: 16.6523,	0.8325 s / batch. (data: 2.91e-04). ETA=7:56:26, max mem: 20.9 GB 
[11/26 18:49:34 visual_prompt]: Epoch 38 / 100: avg data time: 1.73e-01, avg batch time: 1.0049, average train loss: 24.5533
[11/26 18:50:31 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3105, average loss: 14.5569
[11/26 18:50:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.09	
[11/26 18:50:31 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[11/26 18:52:14 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8235 s / batch. (data: 3.85e-03). ETA=7:49:11, max mem: 20.9 GB 
[11/26 18:53:57 visual_prompt]: 	Training 200/553. train loss: 34.2023,	0.8200 s / batch. (data: 3.04e-04). ETA=7:45:50, max mem: 20.9 GB 
[11/26 18:55:41 visual_prompt]: 	Training 300/553. train loss: 65.7863,	0.8719 s / batch. (data: 8.28e-04). ETA=8:13:52, max mem: 20.9 GB 
[11/26 18:57:18 visual_prompt]: 	Training 400/553. train loss: 6.2619,	0.8850 s / batch. (data: 5.44e-02). ETA=8:19:49, max mem: 20.9 GB 
[11/26 18:58:59 visual_prompt]: 	Training 500/553. train loss: 12.1819,	1.7015 s / batch. (data: 8.72e-01). ETA=15:58:07, max mem: 20.9 GB 
[11/26 18:59:49 visual_prompt]: Epoch 39 / 100: avg data time: 1.76e-01, avg batch time: 1.0093, average train loss: 22.5044
[11/26 19:00:47 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3106, average loss: 16.7945
[11/26 19:00:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.91	
[11/26 19:00:47 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[11/26 19:02:31 visual_prompt]: 	Training 100/553. train loss: 17.2350,	0.8438 s / batch. (data: 5.43e-03). ETA=7:53:00, max mem: 20.9 GB 
[11/26 19:04:11 visual_prompt]: 	Training 200/553. train loss: 19.9245,	0.8222 s / batch. (data: 5.45e-03). ETA=7:39:29, max mem: 20.9 GB 
[11/26 19:05:52 visual_prompt]: 	Training 300/553. train loss: 35.8514,	0.8360 s / batch. (data: 3.27e-04). ETA=7:45:49, max mem: 20.9 GB 
[11/26 19:07:33 visual_prompt]: 	Training 400/553. train loss: 8.9490,	0.8190 s / batch. (data: 2.93e-04). ETA=7:35:01, max mem: 20.9 GB 
[11/26 19:09:12 visual_prompt]: 	Training 500/553. train loss: 25.1054,	0.8240 s / batch. (data: 2.99e-04). ETA=7:36:24, max mem: 20.9 GB 
[11/26 19:10:05 visual_prompt]: Epoch 40 / 100: avg data time: 1.78e-01, avg batch time: 1.0099, average train loss: 20.1436
[11/26 19:11:03 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3094, average loss: 16.0810
[11/26 19:11:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.86	
[11/26 19:11:03 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[11/26 19:12:52 visual_prompt]: 	Training 100/553. train loss: 0.6623,	0.8270 s / batch. (data: 3.10e-04). ETA=7:35:56, max mem: 20.9 GB 
[11/26 19:14:34 visual_prompt]: 	Training 200/553. train loss: 7.6635,	0.8477 s / batch. (data: 3.80e-04). ETA=7:45:56, max mem: 20.9 GB 
[11/26 19:16:13 visual_prompt]: 	Training 300/553. train loss: 33.7016,	0.8205 s / batch. (data: 3.18e-04). ETA=7:29:38, max mem: 20.9 GB 
[11/26 19:17:53 visual_prompt]: 	Training 400/553. train loss: 22.9489,	0.8400 s / batch. (data: 7.40e-04). ETA=7:38:56, max mem: 20.9 GB 
[11/26 19:19:31 visual_prompt]: 	Training 500/553. train loss: 2.8368,	0.8640 s / batch. (data: 7.16e-04). ETA=7:50:35, max mem: 20.9 GB 
[11/26 19:20:20 visual_prompt]: Epoch 41 / 100: avg data time: 1.77e-01, avg batch time: 1.0078, average train loss: 22.9542
[11/26 19:21:17 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3092, average loss: 16.2562
[11/26 19:21:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.14	
[11/26 19:21:17 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[11/26 19:22:58 visual_prompt]: 	Training 100/553. train loss: 3.2281,	0.8519 s / batch. (data: 2.43e-02). ETA=7:41:50, max mem: 20.9 GB 
[11/26 19:24:35 visual_prompt]: 	Training 200/553. train loss: 105.2772,	0.8433 s / batch. (data: 1.05e-02). ETA=7:35:45, max mem: 20.9 GB 
[11/26 19:26:13 visual_prompt]: 	Training 300/553. train loss: 15.7625,	0.8194 s / batch. (data: 3.66e-04). ETA=7:21:29, max mem: 20.9 GB 
[11/26 19:27:50 visual_prompt]: 	Training 400/553. train loss: 7.3702,	0.8286 s / batch. (data: 3.20e-04). ETA=7:25:01, max mem: 20.9 GB 
[11/26 19:29:26 visual_prompt]: 	Training 500/553. train loss: 47.2387,	0.8282 s / batch. (data: 3.05e-04). ETA=7:23:26, max mem: 20.9 GB 
[11/26 19:30:18 visual_prompt]: Epoch 42 / 100: avg data time: 1.46e-01, avg batch time: 0.9780, average train loss: 23.9258
[11/26 19:31:14 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3116, average loss: 24.8458
[11/26 19:31:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.97	
[11/26 19:31:14 visual_prompt]: Stopping early.
[11/26 19:31:14 visual_prompt]: Rank of current process: 0. World size: 1
[11/26 19:31:14 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/26 19:31:14 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/26 19:31:14 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/26 19:31:14 visual_prompt]: Training with config:
[11/26 19:31:14 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/26 19:31:14 visual_prompt]: Loading training data...
[11/26 19:31:14 visual_prompt]: Constructing mammo-cbis dataset train...
[11/26 19:31:14 visual_prompt]: Loading validation data...
[11/26 19:31:14 visual_prompt]: Constructing mammo-cbis dataset val...
[11/26 19:31:14 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/26 19:31:17 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/26 19:31:17 visual_prompt]: tuned percent:0.525
[11/26 19:31:17 visual_prompt]: Device used for model: 0
[11/26 19:31:17 visual_prompt]: Setting up Evaluator...
[11/26 19:31:17 visual_prompt]: Setting up Trainer...
[11/26 19:31:17 visual_prompt]: 	Setting up the optimizer...
[11/26 19:31:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/26 19:32:58 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8322 s / batch. (data: 3.05e-04). ETA=12:45:36, max mem: 20.9 GB 
[11/26 19:34:33 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8360 s / batch. (data: 6.89e-03). ETA=12:47:43, max mem: 20.9 GB 
[11/26 19:36:13 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.0640 s / batch. (data: 2.11e-01). ETA=16:15:19, max mem: 20.9 GB 
[11/26 19:37:49 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8476 s / batch. (data: 2.77e-02). ETA=12:55:35, max mem: 20.9 GB 
[11/26 19:39:28 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8235 s / batch. (data: 3.63e-04). ETA=12:32:10, max mem: 20.9 GB 
[11/26 19:40:20 visual_prompt]: Epoch 1 / 100: avg data time: 1.47e-01, avg batch time: 0.9811, average train loss: 1.5403
[11/26 19:41:15 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3113, average loss: 1.5201
[11/26 19:41:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/26 19:41:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/26 19:42:57 visual_prompt]: 	Training 100/553. train loss: 2.4547,	0.8177 s / batch. (data: 3.20e-04). ETA=12:24:44, max mem: 20.9 GB 
[11/26 19:44:33 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8191 s / batch. (data: 3.40e-04). ETA=12:24:37, max mem: 20.9 GB 
[11/26 19:46:13 visual_prompt]: 	Training 300/553. train loss: 7.6994,	1.0006 s / batch. (data: 1.80e-01). ETA=15:07:58, max mem: 20.9 GB 
[11/26 19:47:49 visual_prompt]: 	Training 400/553. train loss: 0.7945,	0.8720 s / batch. (data: 3.49e-04). ETA=13:09:53, max mem: 20.9 GB 
[11/26 19:49:28 visual_prompt]: 	Training 500/553. train loss: 0.8968,	0.8461 s / batch. (data: 3.30e-04). ETA=12:44:58, max mem: 20.9 GB 
[11/26 19:50:18 visual_prompt]: Epoch 2 / 100: avg data time: 1.47e-01, avg batch time: 0.9821, average train loss: 4.2426
[11/26 19:51:14 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3103, average loss: 0.7099
[11/26 19:51:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.19	rocauc: 59.89	
[11/26 19:51:14 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/26 19:52:54 visual_prompt]: 	Training 100/553. train loss: 6.4990,	0.8480 s / batch. (data: 7.95e-03). ETA=12:44:31, max mem: 20.9 GB 
[11/26 19:54:33 visual_prompt]: 	Training 200/553. train loss: 4.0049,	0.8362 s / batch. (data: 5.44e-03). ETA=12:32:28, max mem: 20.9 GB 
[11/26 19:56:10 visual_prompt]: 	Training 300/553. train loss: 0.8738,	0.8560 s / batch. (data: 3.07e-04). ETA=12:48:52, max mem: 20.9 GB 
[11/26 19:57:47 visual_prompt]: 	Training 400/553. train loss: 8.9774,	0.8641 s / batch. (data: 1.39e-02). ETA=12:54:43, max mem: 20.9 GB 
[11/26 19:59:25 visual_prompt]: 	Training 500/553. train loss: 7.3340,	1.1421 s / batch. (data: 3.26e-01). ETA=17:02:02, max mem: 20.9 GB 
[11/26 20:00:15 visual_prompt]: Epoch 3 / 100: avg data time: 1.45e-01, avg batch time: 0.9788, average train loss: 7.6226
[11/26 20:01:11 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3102, average loss: 5.2789
[11/26 20:01:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.01	
[11/26 20:01:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/26 20:02:54 visual_prompt]: 	Training 100/553. train loss: 7.9321,	0.8287 s / batch. (data: 2.98e-04). ETA=12:19:28, max mem: 20.9 GB 
[11/26 20:04:32 visual_prompt]: 	Training 200/553. train loss: 12.9808,	0.8200 s / batch. (data: 3.51e-04). ETA=12:10:20, max mem: 20.9 GB 
[11/26 20:06:10 visual_prompt]: 	Training 300/553. train loss: 8.3782,	1.2808 s / batch. (data: 4.59e-01). ETA=18:58:40, max mem: 20.9 GB 
[11/26 20:07:43 visual_prompt]: 	Training 400/553. train loss: 0.5039,	0.8480 s / batch. (data: 3.22e-04). ETA=12:32:28, max mem: 20.9 GB 
[11/26 20:09:23 visual_prompt]: 	Training 500/553. train loss: 31.1022,	3.4872 s / batch. (data: 2.67e+00). ETA=2 days, 3:28:32, max mem: 20.9 GB 
[11/26 20:10:15 visual_prompt]: Epoch 4 / 100: avg data time: 1.50e-01, avg batch time: 0.9835, average train loss: 8.4598
[11/26 20:11:10 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3094, average loss: 31.7809
[11/26 20:11:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.11	
[11/26 20:11:10 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/26 20:12:51 visual_prompt]: 	Training 100/553. train loss: 0.0982,	0.8558 s / batch. (data: 7.77e-03). ETA=12:35:45, max mem: 20.9 GB 
[11/26 20:14:28 visual_prompt]: 	Training 200/553. train loss: 5.2413,	1.1310 s / batch. (data: 3.13e-01). ETA=16:36:57, max mem: 20.9 GB 
[11/26 20:16:07 visual_prompt]: 	Training 300/553. train loss: 3.9706,	0.8423 s / batch. (data: 3.11e-04). ETA=12:21:03, max mem: 20.9 GB 
[11/26 20:17:44 visual_prompt]: 	Training 400/553. train loss: 0.8584,	0.8252 s / batch. (data: 3.24e-04). ETA=12:04:37, max mem: 20.9 GB 
[11/26 20:19:22 visual_prompt]: 	Training 500/553. train loss: 8.9242,	0.8560 s / batch. (data: 3.09e-04). ETA=12:30:16, max mem: 20.9 GB 
[11/26 20:20:14 visual_prompt]: Epoch 5 / 100: avg data time: 1.49e-01, avg batch time: 0.9833, average train loss: 10.1281
[11/26 20:21:10 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3091, average loss: 22.4373
[11/26 20:21:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.64	
[11/26 20:21:10 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/26 20:22:53 visual_prompt]: 	Training 100/553. train loss: 3.7443,	0.8319 s / batch. (data: 5.37e-03). ETA=12:06:58, max mem: 20.9 GB 
[11/26 20:24:30 visual_prompt]: 	Training 200/553. train loss: 5.5833,	0.8320 s / batch. (data: 2.98e-04). ETA=12:05:43, max mem: 20.9 GB 
[11/26 20:26:05 visual_prompt]: 	Training 300/553. train loss: 2.3348,	0.8369 s / batch. (data: 2.78e-04). ETA=12:08:35, max mem: 20.9 GB 
[11/26 20:27:47 visual_prompt]: 	Training 400/553. train loss: 26.8347,	0.8279 s / batch. (data: 3.37e-04). ETA=11:59:23, max mem: 20.9 GB 
[11/26 20:29:22 visual_prompt]: 	Training 500/553. train loss: 1.0403,	0.8284 s / batch. (data: 3.76e-04). ETA=11:58:27, max mem: 20.9 GB 
[11/26 20:30:13 visual_prompt]: Epoch 6 / 100: avg data time: 1.48e-01, avg batch time: 0.9820, average train loss: 11.6939
[11/26 20:31:09 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3107, average loss: 5.0968
[11/26 20:31:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.36	
[11/26 20:31:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/26 20:32:49 visual_prompt]: 	Training 100/553. train loss: 10.6278,	0.8284 s / batch. (data: 3.14e-04). ETA=11:56:18, max mem: 20.9 GB 
[11/26 20:34:26 visual_prompt]: 	Training 200/553. train loss: 8.8718,	1.0044 s / batch. (data: 1.56e-01). ETA=14:26:48, max mem: 20.9 GB 
[11/26 20:36:07 visual_prompt]: 	Training 300/553. train loss: 16.6753,	1.7029 s / batch. (data: 8.75e-01). ETA=1 day, 0:26:48, max mem: 20.9 GB 
[11/26 20:37:45 visual_prompt]: 	Training 400/553. train loss: 9.0358,	1.8120 s / batch. (data: 9.84e-01). ETA=1 day, 1:57:45, max mem: 20.9 GB 
[11/26 20:39:20 visual_prompt]: 	Training 500/553. train loss: 48.6467,	0.8223 s / batch. (data: 3.12e-04). ETA=11:45:35, max mem: 20.9 GB 
[11/26 20:40:10 visual_prompt]: Epoch 7 / 100: avg data time: 1.43e-01, avg batch time: 0.9783, average train loss: 14.1636
[11/26 20:41:05 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3121, average loss: 17.4630
[11/26 20:41:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.64	
[11/26 20:41:05 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/26 20:42:45 visual_prompt]: 	Training 100/553. train loss: 50.7500,	0.8318 s / batch. (data: 3.04e-04). ETA=11:51:36, max mem: 20.9 GB 
[11/26 20:44:23 visual_prompt]: 	Training 200/553. train loss: 3.8612,	0.8280 s / batch. (data: 3.07e-04). ETA=11:46:57, max mem: 20.9 GB 
[11/26 20:46:02 visual_prompt]: 	Training 300/553. train loss: 33.2820,	0.8560 s / batch. (data: 3.05e-04). ETA=12:09:26, max mem: 20.9 GB 
[11/26 20:47:40 visual_prompt]: 	Training 400/553. train loss: 1.3699,	0.8373 s / batch. (data: 3.20e-04). ETA=11:52:04, max mem: 20.9 GB 
[11/26 20:49:18 visual_prompt]: 	Training 500/553. train loss: 0.3061,	1.2920 s / batch. (data: 4.61e-01). ETA=18:16:40, max mem: 20.9 GB 
[11/26 20:50:09 visual_prompt]: Epoch 8 / 100: avg data time: 1.50e-01, avg batch time: 0.9833, average train loss: 14.8445
[11/26 20:51:05 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3109, average loss: 2.0214
[11/26 20:51:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 62.54	
[11/26 20:51:05 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/26 20:52:46 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8280 s / batch. (data: 2.88e-04). ETA=11:40:41, max mem: 20.9 GB 
[11/26 20:54:22 visual_prompt]: 	Training 200/553. train loss: 3.6899,	0.8471 s / batch. (data: 5.43e-03). ETA=11:55:26, max mem: 20.9 GB 
[11/26 20:56:00 visual_prompt]: 	Training 300/553. train loss: 13.8040,	1.5960 s / batch. (data: 7.41e-01). ETA=22:25:17, max mem: 20.9 GB 
[11/26 20:57:40 visual_prompt]: 	Training 400/553. train loss: 12.5228,	0.8499 s / batch. (data: 7.57e-04). ETA=11:54:59, max mem: 20.9 GB 
[11/26 20:59:18 visual_prompt]: 	Training 500/553. train loss: 8.6952,	0.8403 s / batch. (data: 3.43e-04). ETA=11:45:33, max mem: 20.9 GB 
[11/26 21:00:08 visual_prompt]: Epoch 9 / 100: avg data time: 1.48e-01, avg batch time: 0.9815, average train loss: 14.6695
[11/26 21:01:03 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3116, average loss: 9.3240
[11/26 21:01:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.57	
[11/26 21:01:03 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/26 21:02:47 visual_prompt]: 	Training 100/553. train loss: 36.7253,	0.8245 s / batch. (data: 3.31e-04). ETA=11:30:10, max mem: 20.9 GB 
[11/26 21:04:22 visual_prompt]: 	Training 200/553. train loss: 19.9705,	0.8435 s / batch. (data: 5.17e-04). ETA=11:44:39, max mem: 20.9 GB 
[11/26 21:05:58 visual_prompt]: 	Training 300/553. train loss: 46.9929,	0.8398 s / batch. (data: 2.93e-04). ETA=11:40:09, max mem: 20.9 GB 
[11/26 21:07:33 visual_prompt]: 	Training 400/553. train loss: 38.0591,	0.8400 s / batch. (data: 3.32e-04). ETA=11:38:56, max mem: 20.9 GB 
[11/26 21:09:13 visual_prompt]: 	Training 500/553. train loss: 6.8261,	0.8240 s / batch. (data: 3.13e-04). ETA=11:24:14, max mem: 20.9 GB 
[11/26 21:10:04 visual_prompt]: Epoch 10 / 100: avg data time: 1.45e-01, avg batch time: 0.9783, average train loss: 22.4447
[11/26 21:11:00 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3102, average loss: 16.9751
[11/26 21:11:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.92	
[11/26 21:11:00 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/26 21:12:44 visual_prompt]: 	Training 100/553. train loss: 74.9341,	0.8400 s / batch. (data: 3.12e-04). ETA=11:35:23, max mem: 20.9 GB 
[11/26 21:14:23 visual_prompt]: 	Training 200/553. train loss: 29.2614,	0.8397 s / batch. (data: 2.99e-04). ETA=11:33:43, max mem: 20.9 GB 
[11/26 21:16:00 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.8988 s / batch. (data: 1.09e+00). ETA=1 day, 2:05:33, max mem: 20.9 GB 
[11/26 21:17:36 visual_prompt]: 	Training 400/553. train loss: 18.4664,	0.8439 s / batch. (data: 5.40e-03). ETA=11:34:23, max mem: 20.9 GB 
[11/26 21:19:12 visual_prompt]: 	Training 500/553. train loss: 7.8388,	0.8393 s / batch. (data: 5.41e-03). ETA=11:29:14, max mem: 20.9 GB 
[11/26 21:20:02 visual_prompt]: Epoch 11 / 100: avg data time: 1.48e-01, avg batch time: 0.9811, average train loss: 22.0659
[11/26 21:20:58 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3097, average loss: 2.4523
[11/26 21:20:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 69.60	
[11/26 21:20:58 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/26 21:22:42 visual_prompt]: 	Training 100/553. train loss: 3.9416,	0.8244 s / batch. (data: 3.37e-04). ETA=11:14:50, max mem: 20.9 GB 
[11/26 21:24:21 visual_prompt]: 	Training 200/553. train loss: 24.5018,	0.8329 s / batch. (data: 2.78e-04). ETA=11:20:25, max mem: 20.9 GB 
[11/26 21:25:56 visual_prompt]: 	Training 300/553. train loss: 32.0297,	0.8196 s / batch. (data: 3.17e-04). ETA=11:08:13, max mem: 20.9 GB 
[11/26 21:27:35 visual_prompt]: 	Training 400/553. train loss: 28.2493,	0.8392 s / batch. (data: 3.09e-04). ETA=11:22:45, max mem: 20.9 GB 
[11/26 21:29:13 visual_prompt]: 	Training 500/553. train loss: 82.2206,	0.8597 s / batch. (data: 7.89e-04). ETA=11:38:01, max mem: 20.9 GB 
[11/26 21:30:03 visual_prompt]: Epoch 12 / 100: avg data time: 1.52e-01, avg batch time: 0.9843, average train loss: 18.6137
[11/26 21:30:58 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3099, average loss: 22.1416
[11/26 21:30:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.73	
[11/26 21:30:58 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/26 21:32:42 visual_prompt]: 	Training 100/553. train loss: 10.2265,	0.8520 s / batch. (data: 1.20e-02). ETA=11:29:34, max mem: 20.9 GB 
[11/26 21:34:16 visual_prompt]: 	Training 200/553. train loss: 12.1518,	0.8440 s / batch. (data: 2.86e-04). ETA=11:21:44, max mem: 20.9 GB 
[11/26 21:35:55 visual_prompt]: 	Training 300/553. train loss: 6.5078,	1.5560 s / batch. (data: 7.19e-01). ETA=20:54:14, max mem: 20.9 GB 
[11/26 21:37:30 visual_prompt]: 	Training 400/553. train loss: 5.5350,	0.8412 s / batch. (data: 1.14e-02). ETA=11:16:38, max mem: 20.9 GB 
[11/26 21:39:09 visual_prompt]: 	Training 500/553. train loss: 30.8013,	0.8652 s / batch. (data: 1.19e-02). ETA=11:34:29, max mem: 20.9 GB 
[11/26 21:40:00 visual_prompt]: Epoch 13 / 100: avg data time: 1.46e-01, avg batch time: 0.9787, average train loss: 19.2707
[11/26 21:40:55 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3099, average loss: 9.5355
[11/26 21:40:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 68.06	
[11/26 21:40:55 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/26 21:42:38 visual_prompt]: 	Training 100/553. train loss: 3.6630,	0.8730 s / batch. (data: 1.30e-02). ETA=11:38:32, max mem: 20.9 GB 
[11/26 21:44:15 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.0000 s / batch. (data: 1.58e-01). ETA=13:18:30, max mem: 20.9 GB 
[11/26 21:45:53 visual_prompt]: 	Training 300/553. train loss: 13.1360,	0.8520 s / batch. (data: 3.10e-04). ETA=11:18:56, max mem: 20.9 GB 
[11/26 21:47:29 visual_prompt]: 	Training 400/553. train loss: 8.2468,	0.8305 s / batch. (data: 2.97e-04). ETA=11:00:23, max mem: 20.9 GB 
[11/26 21:49:06 visual_prompt]: 	Training 500/553. train loss: 6.0314,	0.8400 s / batch. (data: 3.15e-04). ETA=11:06:33, max mem: 20.9 GB 
[11/26 21:49:56 visual_prompt]: Epoch 14 / 100: avg data time: 1.46e-01, avg batch time: 0.9784, average train loss: 20.2889
[11/26 21:50:52 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3115, average loss: 5.4824
[11/26 21:50:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.59	
[11/26 21:50:52 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/26 21:52:33 visual_prompt]: 	Training 100/553. train loss: 2.4841,	0.8615 s / batch. (data: 1.05e-02). ETA=11:21:25, max mem: 20.9 GB 
[11/26 21:54:09 visual_prompt]: 	Training 200/553. train loss: 0.0061,	0.8360 s / batch. (data: 7.95e-03). ETA=10:59:51, max mem: 20.9 GB 
[11/26 21:55:48 visual_prompt]: 	Training 300/553. train loss: 34.1132,	0.8237 s / batch. (data: 7.71e-04). ETA=10:48:44, max mem: 20.9 GB 
[11/26 21:57:22 visual_prompt]: 	Training 400/553. train loss: 30.7254,	0.8568 s / batch. (data: 3.65e-02). ETA=11:13:27, max mem: 20.9 GB 
[11/26 21:59:00 visual_prompt]: 	Training 500/553. train loss: 8.6655,	0.8188 s / batch. (data: 3.17e-04). ETA=10:42:10, max mem: 20.9 GB 
[11/26 21:59:51 visual_prompt]: Epoch 15 / 100: avg data time: 1.42e-01, avg batch time: 0.9750, average train loss: 21.3577
[11/26 22:00:46 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3098, average loss: 4.9152
[11/26 22:00:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.58	
[11/26 22:00:46 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/26 22:02:27 visual_prompt]: 	Training 100/553. train loss: 25.2839,	0.8200 s / batch. (data: 2.98e-04). ETA=10:41:02, max mem: 20.9 GB 
[11/26 22:04:03 visual_prompt]: 	Training 200/553. train loss: 20.3574,	0.8280 s / batch. (data: 2.57e-04). ETA=10:45:55, max mem: 20.9 GB 
[11/26 22:05:41 visual_prompt]: 	Training 300/553. train loss: 11.8521,	0.8305 s / batch. (data: 9.34e-03). ETA=10:46:26, max mem: 20.9 GB 
[11/26 22:07:17 visual_prompt]: 	Training 400/553. train loss: 19.4738,	0.8320 s / batch. (data: 1.55e-02). ETA=10:46:17, max mem: 20.9 GB 
[11/26 22:08:54 visual_prompt]: 	Training 500/553. train loss: 3.9495,	1.2700 s / batch. (data: 4.45e-01). ETA=16:24:23, max mem: 20.9 GB 
[11/26 22:09:44 visual_prompt]: Epoch 16 / 100: avg data time: 1.40e-01, avg batch time: 0.9731, average train loss: 17.7663
[11/26 22:10:39 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3092, average loss: 2.0187
[11/26 22:10:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 64.72	
[11/26 22:10:39 visual_prompt]: Best epoch 16: best metric: -2.019
[11/26 22:10:39 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/26 22:12:19 visual_prompt]: 	Training 100/553. train loss: 8.9375,	0.8205 s / batch. (data: 3.19e-04). ETA=10:33:52, max mem: 20.9 GB 
[11/26 22:13:57 visual_prompt]: 	Training 200/553. train loss: 45.9730,	0.8200 s / batch. (data: 3.12e-04). ETA=10:32:05, max mem: 20.9 GB 
[11/26 22:15:34 visual_prompt]: 	Training 300/553. train loss: 22.9622,	0.8256 s / batch. (data: 3.08e-04). ETA=10:35:03, max mem: 20.9 GB 
[11/26 22:17:11 visual_prompt]: 	Training 400/553. train loss: 14.7286,	1.3760 s / batch. (data: 5.53e-01). ETA=17:36:09, max mem: 20.9 GB 
[11/26 22:18:48 visual_prompt]: 	Training 500/553. train loss: 7.3184,	1.5840 s / batch. (data: 7.51e-01). ETA=20:13:07, max mem: 20.9 GB 
[11/26 22:19:40 visual_prompt]: Epoch 17 / 100: avg data time: 1.45e-01, avg batch time: 0.9781, average train loss: 19.2786
[11/26 22:20:36 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3116, average loss: 27.4367
[11/26 22:20:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.51	
[11/26 22:20:36 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/26 22:22:18 visual_prompt]: 	Training 100/553. train loss: 5.2559,	0.8495 s / batch. (data: 1.06e-02). ETA=10:48:26, max mem: 20.9 GB 
[11/26 22:23:57 visual_prompt]: 	Training 200/553. train loss: 3.6647,	0.8343 s / batch. (data: 7.45e-04). ETA=10:35:25, max mem: 20.9 GB 
[11/26 22:25:35 visual_prompt]: 	Training 300/553. train loss: 5.1940,	0.8247 s / batch. (data: 2.99e-04). ETA=10:26:44, max mem: 20.9 GB 
[11/26 22:27:12 visual_prompt]: 	Training 400/553. train loss: 25.8491,	0.8372 s / batch. (data: 9.11e-03). ETA=10:34:49, max mem: 20.9 GB 
[11/26 22:28:48 visual_prompt]: 	Training 500/553. train loss: 14.6559,	0.8444 s / batch. (data: 3.29e-04). ETA=10:38:55, max mem: 20.9 GB 
[11/26 22:29:38 visual_prompt]: Epoch 18 / 100: avg data time: 1.48e-01, avg batch time: 0.9805, average train loss: 21.2044
[11/26 22:30:34 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3089, average loss: 31.1111
[11/26 22:30:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.99	
[11/26 22:30:34 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/26 22:32:15 visual_prompt]: 	Training 100/553. train loss: 8.5300,	0.8320 s / batch. (data: 3.25e-04). ETA=10:27:24, max mem: 20.9 GB 
[11/26 22:33:53 visual_prompt]: 	Training 200/553. train loss: 10.2427,	0.8200 s / batch. (data: 3.01e-04). ETA=10:16:59, max mem: 20.9 GB 
[11/26 22:35:31 visual_prompt]: 	Training 300/553. train loss: 0.0001,	0.8389 s / batch. (data: 5.61e-03). ETA=10:29:50, max mem: 20.9 GB 
[11/26 22:37:08 visual_prompt]: 	Training 400/553. train loss: 9.9992,	0.8529 s / batch. (data: 1.05e-02). ETA=10:38:54, max mem: 20.9 GB 
[11/26 22:38:42 visual_prompt]: 	Training 500/553. train loss: 8.1673,	0.8522 s / batch. (data: 3.12e-04). ETA=10:36:59, max mem: 20.9 GB 
[11/26 22:39:32 visual_prompt]: Epoch 19 / 100: avg data time: 1.41e-01, avg batch time: 0.9737, average train loss: 16.9489
[11/26 22:40:28 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3110, average loss: 9.2377
[11/26 22:40:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 66.11	
[11/26 22:40:28 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/26 22:42:07 visual_prompt]: 	Training 100/553. train loss: 25.5115,	0.8240 s / batch. (data: 2.76e-04). ETA=10:13:48, max mem: 20.9 GB 
[11/26 22:43:45 visual_prompt]: 	Training 200/553. train loss: 2.2696,	0.8518 s / batch. (data: 1.05e-02). ETA=10:33:06, max mem: 20.9 GB 
[11/26 22:45:22 visual_prompt]: 	Training 300/553. train loss: 13.7544,	0.8442 s / batch. (data: 7.74e-04). ETA=10:26:02, max mem: 20.9 GB 
[11/26 22:46:58 visual_prompt]: 	Training 400/553. train loss: 30.7075,	0.8297 s / batch. (data: 1.20e-02). ETA=10:13:54, max mem: 20.9 GB 
[11/26 22:48:34 visual_prompt]: 	Training 500/553. train loss: 84.3555,	0.8360 s / batch. (data: 3.04e-04). ETA=10:17:07, max mem: 20.9 GB 
[11/26 22:49:26 visual_prompt]: Epoch 20 / 100: avg data time: 1.41e-01, avg batch time: 0.9731, average train loss: 21.6830
[11/26 22:50:21 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3100, average loss: 7.5366
[11/26 22:50:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 68.91	
[11/26 22:50:21 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/26 22:52:05 visual_prompt]: 	Training 100/553. train loss: 3.1234,	0.8286 s / batch. (data: 1.05e-02). ETA=10:09:34, max mem: 20.9 GB 
[11/26 22:53:40 visual_prompt]: 	Training 200/553. train loss: 36.0146,	0.8320 s / batch. (data: 3.16e-04). ETA=10:10:43, max mem: 20.9 GB 
[11/26 22:55:17 visual_prompt]: 	Training 300/553. train loss: 81.9775,	1.1158 s / batch. (data: 2.82e-01). ETA=13:37:06, max mem: 20.9 GB 
[11/26 22:56:52 visual_prompt]: 	Training 400/553. train loss: 1.2556,	0.8270 s / batch. (data: 7.95e-03). ETA=10:04:17, max mem: 20.9 GB 
[11/26 22:58:31 visual_prompt]: 	Training 500/553. train loss: 7.6277,	0.8516 s / batch. (data: 2.92e-04). ETA=10:20:48, max mem: 20.9 GB 
[11/26 22:59:21 visual_prompt]: Epoch 21 / 100: avg data time: 1.42e-01, avg batch time: 0.9751, average train loss: 16.5011
[11/26 23:00:16 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3083, average loss: 7.3796
[11/26 23:00:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 67.41	
[11/26 23:00:16 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/26 23:01:55 visual_prompt]: 	Training 100/553. train loss: 47.6375,	0.8200 s / batch. (data: 3.04e-04). ETA=9:55:40, max mem: 20.9 GB 
[11/26 23:03:32 visual_prompt]: 	Training 200/553. train loss: 13.8386,	0.8511 s / batch. (data: 5.44e-03). ETA=10:16:52, max mem: 20.9 GB 
[11/26 23:05:07 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8467 s / batch. (data: 3.55e-04). ETA=10:12:16, max mem: 20.9 GB 
[11/26 23:06:44 visual_prompt]: 	Training 400/553. train loss: 4.3740,	0.8267 s / batch. (data: 3.17e-04). ETA=9:56:26, max mem: 20.9 GB 
[11/26 23:08:22 visual_prompt]: 	Training 500/553. train loss: 23.4144,	0.8272 s / batch. (data: 5.37e-03). ETA=9:55:22, max mem: 20.9 GB 
[11/26 23:09:14 visual_prompt]: Epoch 22 / 100: avg data time: 1.39e-01, avg batch time: 0.9725, average train loss: 15.0954
[11/26 23:10:09 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3107, average loss: 4.8684
[11/26 23:10:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 68.48	
[11/26 23:10:09 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/26 23:11:51 visual_prompt]: 	Training 100/553. train loss: 9.3038,	0.8320 s / batch. (data: 3.14e-04). ETA=9:56:45, max mem: 20.9 GB 
[11/26 23:13:28 visual_prompt]: 	Training 200/553. train loss: 66.7489,	0.8266 s / batch. (data: 3.18e-04). ETA=9:51:28, max mem: 20.9 GB 
[11/26 23:15:06 visual_prompt]: 	Training 300/553. train loss: 12.7685,	0.8459 s / batch. (data: 7.37e-04). ETA=10:03:51, max mem: 20.9 GB 
[11/26 23:16:41 visual_prompt]: 	Training 400/553. train loss: 9.6662,	0.8859 s / batch. (data: 3.54e-02). ETA=10:30:59, max mem: 20.9 GB 
[11/26 23:18:17 visual_prompt]: 	Training 500/553. train loss: 29.2016,	0.8336 s / batch. (data: 3.01e-04). ETA=9:52:19, max mem: 20.9 GB 
[11/26 23:19:08 visual_prompt]: Epoch 23 / 100: avg data time: 1.43e-01, avg batch time: 0.9756, average train loss: 19.5543
[11/26 23:20:04 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3114, average loss: 8.3050
[11/26 23:20:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 67.63	
[11/26 23:20:04 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/26 23:21:43 visual_prompt]: 	Training 100/553. train loss: 40.1461,	0.8392 s / batch. (data: 7.19e-03). ETA=9:54:11, max mem: 20.9 GB 
[11/26 23:23:19 visual_prompt]: 	Training 200/553. train loss: 23.6031,	0.8480 s / batch. (data: 2.97e-04). ETA=9:58:58, max mem: 20.9 GB 
[11/26 23:24:58 visual_prompt]: 	Training 300/553. train loss: 2.1520,	0.8597 s / batch. (data: 4.12e-02). ETA=10:05:48, max mem: 20.9 GB 
[11/26 23:26:35 visual_prompt]: 	Training 400/553. train loss: 34.9383,	0.8200 s / batch. (data: 3.25e-04). ETA=9:36:27, max mem: 20.9 GB 
[11/26 23:28:14 visual_prompt]: 	Training 500/553. train loss: 33.8811,	0.8433 s / batch. (data: 3.43e-04). ETA=9:51:28, max mem: 20.9 GB 
[11/26 23:29:06 visual_prompt]: Epoch 24 / 100: avg data time: 1.47e-01, avg batch time: 0.9796, average train loss: 22.2876
[11/26 23:30:01 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3115, average loss: 48.8690
[11/26 23:30:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.95	
[11/26 23:30:01 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/26 23:31:46 visual_prompt]: 	Training 100/553. train loss: 26.6344,	0.8728 s / batch. (data: 2.08e-02). ETA=10:09:54, max mem: 20.9 GB 
[11/26 23:33:21 visual_prompt]: 	Training 200/553. train loss: 9.9352,	1.1571 s / batch. (data: 3.33e-01). ETA=13:26:39, max mem: 20.9 GB 
[11/26 23:34:58 visual_prompt]: 	Training 300/553. train loss: 7.5309,	0.8520 s / batch. (data: 5.42e-03). ETA=9:52:32, max mem: 20.9 GB 
[11/26 23:36:35 visual_prompt]: 	Training 400/553. train loss: 1.2932,	1.0752 s / batch. (data: 2.57e-01). ETA=12:25:59, max mem: 20.9 GB 
[11/26 23:38:13 visual_prompt]: 	Training 500/553. train loss: 9.2158,	1.4107 s / batch. (data: 5.54e-01). ETA=16:16:21, max mem: 20.9 GB 
[11/26 23:39:04 visual_prompt]: Epoch 25 / 100: avg data time: 1.47e-01, avg batch time: 0.9803, average train loss: 16.1054
[11/26 23:39:59 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.3103, average loss: 20.5659
[11/26 23:39:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 69.31	
[11/26 23:39:59 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/26 23:41:40 visual_prompt]: 	Training 100/553. train loss: 25.2308,	0.8360 s / batch. (data: 3.17e-04). ETA=9:36:28, max mem: 20.9 GB 
[11/26 23:43:18 visual_prompt]: 	Training 200/553. train loss: 77.3484,	1.6704 s / batch. (data: 8.35e-01). ETA=19:09:06, max mem: 20.9 GB 
[11/26 23:44:57 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8595 s / batch. (data: 5.43e-03). ETA=9:49:49, max mem: 20.9 GB 
[11/26 23:46:34 visual_prompt]: 	Training 400/553. train loss: 18.4817,	0.8370 s / batch. (data: 1.05e-02). ETA=9:32:58, max mem: 20.9 GB 
[11/26 23:48:09 visual_prompt]: 	Training 500/553. train loss: 1.8468,	0.8360 s / batch. (data: 3.00e-04). ETA=9:30:55, max mem: 20.9 GB 
[11/26 23:49:00 visual_prompt]: Epoch 26 / 100: avg data time: 1.45e-01, avg batch time: 0.9781, average train loss: 16.2934
[11/26 23:49:55 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3100, average loss: 30.2772
[11/26 23:49:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.28	
[11/26 23:49:55 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[11/26 23:51:38 visual_prompt]: 	Training 100/553. train loss: 0.0244,	0.8167 s / batch. (data: 3.16e-04). ETA=9:15:39, max mem: 20.9 GB 
[11/26 23:53:14 visual_prompt]: 	Training 200/553. train loss: 27.9886,	1.2280 s / batch. (data: 4.01e-01). ETA=13:53:26, max mem: 20.9 GB 
[11/26 23:54:52 visual_prompt]: 	Training 300/553. train loss: 10.9421,	0.8334 s / batch. (data: 3.06e-04). ETA=9:24:13, max mem: 20.9 GB 
[11/26 23:56:30 visual_prompt]: 	Training 400/553. train loss: 11.9090,	0.8370 s / batch. (data: 8.08e-04). ETA=9:25:16, max mem: 20.9 GB 
[11/26 23:58:08 visual_prompt]: 	Training 500/553. train loss: 27.1996,	0.8668 s / batch. (data: 1.05e-02). ETA=9:43:58, max mem: 20.9 GB 
[11/26 23:58:57 visual_prompt]: Epoch 27 / 100: avg data time: 1.45e-01, avg batch time: 0.9785, average train loss: 16.6082
[11/26 23:59:52 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3113, average loss: 9.7985
[11/26 23:59:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 70.27	
[11/26 23:59:52 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[11/27 00:01:31 visual_prompt]: 	Training 100/553. train loss: 10.9440,	0.8492 s / batch. (data: 1.56e-02). ETA=9:29:57, max mem: 20.9 GB 
[11/27 00:03:09 visual_prompt]: 	Training 200/553. train loss: 19.2374,	0.8288 s / batch. (data: 2.73e-04). ETA=9:14:52, max mem: 20.9 GB 
[11/27 00:04:47 visual_prompt]: 	Training 300/553. train loss: 10.8698,	1.1040 s / batch. (data: 2.60e-01). ETA=12:17:16, max mem: 20.9 GB 
[11/27 00:06:23 visual_prompt]: 	Training 400/553. train loss: 2.6526,	0.8452 s / batch. (data: 1.60e-02). ETA=9:23:02, max mem: 20.9 GB 
[11/27 00:07:59 visual_prompt]: 	Training 500/553. train loss: 4.4598,	0.8428 s / batch. (data: 7.20e-04). ETA=9:20:02, max mem: 20.9 GB 
[11/27 00:08:50 visual_prompt]: Epoch 28 / 100: avg data time: 1.40e-01, avg batch time: 0.9725, average train loss: 18.9422
[11/27 00:09:45 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3100, average loss: 4.2045
[11/27 00:09:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 70.05	
[11/27 00:09:45 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[11/27 00:11:31 visual_prompt]: 	Training 100/553. train loss: 7.5982,	0.8584 s / batch. (data: 1.44e-02). ETA=9:28:12, max mem: 20.9 GB 
[11/27 00:13:08 visual_prompt]: 	Training 200/553. train loss: 63.5579,	1.8079 s / batch. (data: 9.97e-01). ETA=19:53:43, max mem: 20.9 GB 
[11/27 00:14:43 visual_prompt]: 	Training 300/553. train loss: 7.7657,	0.8253 s / batch. (data: 2.95e-04). ETA=9:03:32, max mem: 20.9 GB 
[11/27 00:16:16 visual_prompt]: 	Training 400/553. train loss: 9.4884,	1.2800 s / batch. (data: 4.58e-01). ETA=14:00:50, max mem: 20.9 GB 
[11/27 00:17:53 visual_prompt]: 	Training 500/553. train loss: 23.7843,	0.8343 s / batch. (data: 5.44e-03). ETA=9:06:42, max mem: 20.9 GB 
[11/27 00:18:43 visual_prompt]: Epoch 29 / 100: avg data time: 1.41e-01, avg batch time: 0.9736, average train loss: 19.7971
[11/27 00:19:39 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.3100, average loss: 5.3655
[11/27 00:19:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 69.41	
[11/27 00:19:39 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[11/27 00:21:18 visual_prompt]: 	Training 100/553. train loss: 22.5831,	0.8640 s / batch. (data: 1.59e-02). ETA=9:23:55, max mem: 20.9 GB 
[11/27 00:22:57 visual_prompt]: 	Training 200/553. train loss: 40.8704,	0.8201 s / batch. (data: 3.19e-04). ETA=8:53:55, max mem: 20.9 GB 
[11/27 00:24:32 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8300 s / batch. (data: 5.44e-03). ETA=8:59:00, max mem: 20.9 GB 
[11/27 00:26:11 visual_prompt]: 	Training 400/553. train loss: 15.9673,	0.9850 s / batch. (data: 1.65e-01). ETA=10:38:00, max mem: 20.9 GB 
[11/27 00:27:47 visual_prompt]: 	Training 500/553. train loss: 6.7287,	1.0934 s / batch. (data: 2.62e-01). ETA=11:46:25, max mem: 20.9 GB 
[11/27 00:28:40 visual_prompt]: Epoch 30 / 100: avg data time: 1.45e-01, avg batch time: 0.9783, average train loss: 13.3866
[11/27 00:29:35 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3094, average loss: 33.6132
[11/27 00:29:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 72.45	
[11/27 00:29:35 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[11/27 00:31:18 visual_prompt]: 	Training 100/553. train loss: 7.6240,	0.8322 s / batch. (data: 3.19e-04). ETA=8:55:29, max mem: 20.9 GB 
[11/27 00:32:57 visual_prompt]: 	Training 200/553. train loss: 32.3795,	0.8440 s / batch. (data: 3.31e-04). ETA=9:01:43, max mem: 20.9 GB 
[11/27 00:34:32 visual_prompt]: 	Training 300/553. train loss: 14.8131,	0.8217 s / batch. (data: 2.99e-04). ETA=8:46:02, max mem: 20.9 GB 
[11/27 00:36:09 visual_prompt]: 	Training 400/553. train loss: 18.6171,	1.2957 s / batch. (data: 4.52e-01). ETA=13:47:17, max mem: 20.9 GB 
[11/27 00:37:46 visual_prompt]: 	Training 500/553. train loss: 23.6077,	0.8353 s / batch. (data: 3.24e-04). ETA=8:51:56, max mem: 20.9 GB 
[11/27 00:38:35 visual_prompt]: Epoch 31 / 100: avg data time: 1.43e-01, avg batch time: 0.9759, average train loss: 19.2744
[11/27 00:39:31 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3105, average loss: 20.5524
[11/27 00:39:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.05	
[11/27 00:39:31 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[11/27 00:41:13 visual_prompt]: 	Training 100/553. train loss: 0.0099,	0.8627 s / batch. (data: 1.05e-02). ETA=9:07:11, max mem: 20.9 GB 
[11/27 00:42:51 visual_prompt]: 	Training 200/553. train loss: 0.2769,	0.8369 s / batch. (data: 8.28e-04). ETA=8:49:27, max mem: 20.9 GB 
[11/27 00:44:30 visual_prompt]: 	Training 300/553. train loss: 20.3937,	1.1760 s / batch. (data: 3.27e-01). ETA=12:21:59, max mem: 20.9 GB 
[11/27 00:46:09 visual_prompt]: 	Training 400/553. train loss: 13.2891,	0.8594 s / batch. (data: 1.14e-02). ETA=9:00:48, max mem: 20.9 GB 
[11/27 00:47:43 visual_prompt]: 	Training 500/553. train loss: 7.3354,	0.8218 s / batch. (data: 3.00e-04). ETA=8:35:46, max mem: 20.9 GB 
[11/27 00:48:33 visual_prompt]: Epoch 32 / 100: avg data time: 1.47e-01, avg batch time: 0.9802, average train loss: 14.4753
[11/27 00:49:28 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3111, average loss: 6.4045
[11/27 00:49:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 70.50	
[11/27 00:49:28 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[11/27 00:51:08 visual_prompt]: 	Training 100/553. train loss: 0.0000,	1.1352 s / batch. (data: 3.05e-01). ETA=11:49:32, max mem: 20.9 GB 
[11/27 00:52:47 visual_prompt]: 	Training 200/553. train loss: 5.2247,	0.8737 s / batch. (data: 5.01e-02). ETA=9:04:39, max mem: 20.9 GB 
[11/27 00:54:23 visual_prompt]: 	Training 300/553. train loss: 0.2166,	0.8520 s / batch. (data: 2.75e-04). ETA=8:49:42, max mem: 20.9 GB 
[11/27 00:56:00 visual_prompt]: 	Training 400/553. train loss: 19.4482,	0.8362 s / batch. (data: 2.92e-04). ETA=8:38:31, max mem: 20.9 GB 
[11/27 00:57:36 visual_prompt]: 	Training 500/553. train loss: 16.2498,	0.8520 s / batch. (data: 5.45e-03). ETA=8:46:52, max mem: 20.9 GB 
[11/27 00:58:26 visual_prompt]: Epoch 33 / 100: avg data time: 1.40e-01, avg batch time: 0.9728, average train loss: 16.3991
[11/27 00:59:21 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3111, average loss: 7.3089
[11/27 00:59:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 71.01	
[11/27 00:59:21 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[11/27 01:01:03 visual_prompt]: 	Training 100/553. train loss: 6.8604,	0.8403 s / batch. (data: 1.77e-02). ETA=8:37:28, max mem: 20.9 GB 
[11/27 01:02:38 visual_prompt]: 	Training 200/553. train loss: 36.1142,	0.8360 s / batch. (data: 3.16e-04). ETA=8:33:28, max mem: 20.9 GB 
[11/27 01:04:14 visual_prompt]: 	Training 300/553. train loss: 9.7900,	0.8434 s / batch. (data: 7.95e-03). ETA=8:36:36, max mem: 20.9 GB 
[11/27 01:05:51 visual_prompt]: 	Training 400/553. train loss: 5.5626,	0.8680 s / batch. (data: 7.95e-03). ETA=8:50:13, max mem: 20.9 GB 
[11/27 01:07:29 visual_prompt]: 	Training 500/553. train loss: 2.1267,	1.3703 s / batch. (data: 5.38e-01). ETA=13:54:44, max mem: 20.9 GB 
[11/27 01:08:18 visual_prompt]: Epoch 34 / 100: avg data time: 1.38e-01, avg batch time: 0.9710, average train loss: 15.2596
[11/27 01:09:14 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3108, average loss: 10.1406
[11/27 01:09:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 70.73	
[11/27 01:09:14 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[11/27 01:10:56 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8400 s / batch. (data: 3.18e-04). ETA=8:29:33, max mem: 20.9 GB 
[11/27 01:12:33 visual_prompt]: 	Training 200/553. train loss: 10.2225,	0.8440 s / batch. (data: 3.23e-04). ETA=8:30:35, max mem: 20.9 GB 
[11/27 01:14:09 visual_prompt]: 	Training 300/553. train loss: 46.3032,	0.8320 s / batch. (data: 3.11e-04). ETA=8:21:55, max mem: 20.9 GB 
[11/27 01:15:46 visual_prompt]: 	Training 400/553. train loss: 7.1608,	0.8480 s / batch. (data: 1.53e-02). ETA=8:30:11, max mem: 20.9 GB 
[11/27 01:17:22 visual_prompt]: 	Training 500/553. train loss: 9.5867,	1.0080 s / batch. (data: 1.78e-01). ETA=10:04:46, max mem: 20.9 GB 
[11/27 01:18:14 visual_prompt]: Epoch 35 / 100: avg data time: 1.45e-01, avg batch time: 0.9767, average train loss: 18.8911
[11/27 01:19:09 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3097, average loss: 52.2399
[11/27 01:19:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 70.78	
[11/27 01:19:09 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[11/27 01:20:49 visual_prompt]: 	Training 100/553. train loss: 14.2369,	0.8473 s / batch. (data: 2.38e-02). ETA=8:26:11, max mem: 20.9 GB 
[11/27 01:22:27 visual_prompt]: 	Training 200/553. train loss: 21.4093,	0.8320 s / batch. (data: 3.22e-04). ETA=8:15:39, max mem: 20.9 GB 
[11/27 01:24:07 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8400 s / batch. (data: 3.12e-04). ETA=8:19:01, max mem: 20.9 GB 
[11/27 01:25:43 visual_prompt]: 	Training 400/553. train loss: 3.3295,	1.4800 s / batch. (data: 6.42e-01). ETA=14:36:45, max mem: 20.9 GB 
[11/27 01:27:21 visual_prompt]: 	Training 500/553. train loss: 7.8547,	1.1330 s / batch. (data: 2.81e-01). ETA=11:09:20, max mem: 20.9 GB 
[11/27 01:28:10 visual_prompt]: Epoch 36 / 100: avg data time: 1.44e-01, avg batch time: 0.9776, average train loss: 13.3187
[11/27 01:29:06 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3109, average loss: 3.2688
[11/27 01:29:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 70.25	
[11/27 01:29:06 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[11/27 01:30:47 visual_prompt]: 	Training 100/553. train loss: 37.2811,	0.8174 s / batch. (data: 3.35e-04). ETA=8:00:48, max mem: 20.9 GB 
[11/27 01:32:24 visual_prompt]: 	Training 200/553. train loss: 4.0412,	0.8201 s / batch. (data: 2.97e-04). ETA=8:01:00, max mem: 20.9 GB 
[11/27 01:34:02 visual_prompt]: 	Training 300/553. train loss: 25.0179,	1.5268 s / batch. (data: 7.09e-01). ETA=14:52:59, max mem: 20.9 GB 
[11/27 01:35:41 visual_prompt]: 	Training 400/553. train loss: 13.3034,	1.9360 s / batch. (data: 1.11e+00). ETA=18:49:05, max mem: 20.9 GB 
[11/27 01:37:15 visual_prompt]: 	Training 500/553. train loss: 0.8058,	1.0776 s / batch. (data: 2.42e-01). ETA=10:26:40, max mem: 20.9 GB 
[11/27 01:38:08 visual_prompt]: Epoch 37 / 100: avg data time: 1.47e-01, avg batch time: 0.9802, average train loss: 13.7613
[11/27 01:39:02 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3099, average loss: 5.6956
[11/27 01:39:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 70.76	
[11/27 01:39:02 visual_prompt]: Stopping early.
[11/27 01:39:02 visual_prompt]: Rank of current process: 0. World size: 1
[11/27 01:39:02 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/27 01:39:02 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/27 01:39:02 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/27 01:39:02 visual_prompt]: Training with config:
[11/27 01:39:02 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr5.0_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/27 01:39:02 visual_prompt]: Loading training data...
[11/27 01:39:02 visual_prompt]: Constructing mammo-cbis dataset train...
[11/27 01:39:02 visual_prompt]: Loading validation data...
[11/27 01:39:02 visual_prompt]: Constructing mammo-cbis dataset val...
[11/27 01:39:02 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/27 01:39:05 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/27 01:39:05 visual_prompt]: tuned percent:0.525
[11/27 01:39:05 visual_prompt]: Device used for model: 0
[11/27 01:39:05 visual_prompt]: Setting up Evaluator...
[11/27 01:39:05 visual_prompt]: Setting up Trainer...
[11/27 01:39:05 visual_prompt]: 	Setting up the optimizer...
[11/27 01:39:05 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/27 01:40:45 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8240 s / batch. (data: 3.05e-04). ETA=12:38:05, max mem: 20.9 GB 
[11/27 01:42:21 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8480 s / batch. (data: 3.17e-04). ETA=12:58:43, max mem: 20.9 GB 
[11/27 01:44:01 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8440 s / batch. (data: 1.19e-02). ETA=12:53:39, max mem: 20.9 GB 
[11/27 01:45:37 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8280 s / batch. (data: 2.54e-04). ETA=12:37:38, max mem: 20.9 GB 
[11/27 01:47:17 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8426 s / batch. (data: 7.85e-04). ETA=12:49:36, max mem: 20.9 GB 
[11/27 01:48:08 visual_prompt]: Epoch 1 / 100: avg data time: 1.47e-01, avg batch time: 0.9810, average train loss: 1.5403
[11/27 01:49:03 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3089, average loss: 1.5201
[11/27 01:49:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/27 01:49:03 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/27 01:50:44 visual_prompt]: 	Training 100/553. train loss: 0.7798,	0.8280 s / batch. (data: 2.84e-04). ETA=12:34:10, max mem: 20.9 GB 
[11/27 01:52:20 visual_prompt]: 	Training 200/553. train loss: 0.0007,	0.8215 s / batch. (data: 3.06e-04). ETA=12:26:52, max mem: 20.9 GB 
[11/27 01:53:58 visual_prompt]: 	Training 300/553. train loss: 0.8430,	0.9280 s / batch. (data: 1.06e-01). ETA=14:02:06, max mem: 20.9 GB 
[11/27 01:55:34 visual_prompt]: 	Training 400/553. train loss: 0.8061,	0.8480 s / batch. (data: 7.95e-03). ETA=12:48:06, max mem: 20.9 GB 
[11/27 01:57:12 visual_prompt]: 	Training 500/553. train loss: 0.6105,	0.8297 s / batch. (data: 1.05e-02). ETA=12:30:07, max mem: 20.9 GB 
[11/27 01:58:03 visual_prompt]: Epoch 2 / 100: avg data time: 1.41e-01, avg batch time: 0.9752, average train loss: 1.5226
[11/27 01:58:58 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3101, average loss: 0.8135
[11/27 01:58:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.50	
[11/27 01:58:58 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/27 02:00:38 visual_prompt]: 	Training 100/553. train loss: 3.1812,	0.8230 s / batch. (data: 5.48e-03). ETA=12:22:01, max mem: 20.9 GB 
[11/27 02:02:16 visual_prompt]: 	Training 200/553. train loss: 2.0711,	0.8188 s / batch. (data: 3.01e-04). ETA=12:16:49, max mem: 20.9 GB 
[11/27 02:03:52 visual_prompt]: 	Training 300/553. train loss: 0.9746,	0.8332 s / batch. (data: 5.39e-03). ETA=12:28:23, max mem: 20.9 GB 
[11/27 02:05:29 visual_prompt]: 	Training 400/553. train loss: 0.0115,	0.8465 s / batch. (data: 2.87e-04). ETA=12:38:59, max mem: 20.9 GB 
[11/27 02:07:08 visual_prompt]: 	Training 500/553. train loss: 4.7185,	1.0880 s / batch. (data: 2.41e-01). ETA=16:13:40, max mem: 20.9 GB 
[11/27 02:07:57 visual_prompt]: Epoch 3 / 100: avg data time: 1.40e-01, avg batch time: 0.9741, average train loss: 2.5513
[11/27 02:08:52 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3108, average loss: 7.7699
[11/27 02:08:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.54	
[11/27 02:08:52 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/27 02:10:34 visual_prompt]: 	Training 100/553. train loss: 8.1876,	0.8354 s / batch. (data: 1.19e-02). ETA=12:25:26, max mem: 20.9 GB 
[11/27 02:12:11 visual_prompt]: 	Training 200/553. train loss: 2.3320,	0.8391 s / batch. (data: 7.95e-03). ETA=12:27:22, max mem: 20.9 GB 
[11/27 02:13:49 visual_prompt]: 	Training 300/553. train loss: 1.5272,	1.1400 s / batch. (data: 3.02e-01). ETA=16:53:30, max mem: 20.9 GB 
[11/27 02:15:21 visual_prompt]: 	Training 400/553. train loss: 2.3384,	0.9962 s / batch. (data: 1.35e-01). ETA=14:43:57, max mem: 20.9 GB 
[11/27 02:17:01 visual_prompt]: 	Training 500/553. train loss: 15.5396,	3.2112 s / batch. (data: 2.39e+00). ETA=1 day, 23:24:05, max mem: 20.9 GB 
[11/27 02:17:52 visual_prompt]: Epoch 4 / 100: avg data time: 1.42e-01, avg batch time: 0.9763, average train loss: 4.5100
[11/27 02:18:48 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3110, average loss: 4.4083
[11/27 02:18:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.16	
[11/27 02:18:48 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/27 02:20:28 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8387 s / batch. (data: 1.05e-02). ETA=12:20:41, max mem: 20.9 GB 
[11/27 02:22:04 visual_prompt]: 	Training 200/553. train loss: 6.5708,	1.0846 s / batch. (data: 2.53e-01). ETA=15:55:59, max mem: 20.9 GB 
[11/27 02:23:41 visual_prompt]: 	Training 300/553. train loss: 5.6033,	0.8320 s / batch. (data: 3.02e-04). ETA=12:11:59, max mem: 20.9 GB 
[11/27 02:25:18 visual_prompt]: 	Training 400/553. train loss: 16.1834,	0.8440 s / batch. (data: 2.88e-04). ETA=12:21:09, max mem: 20.9 GB 
[11/27 02:26:55 visual_prompt]: 	Training 500/553. train loss: 1.6618,	0.8440 s / batch. (data: 2.19e-02). ETA=12:19:45, max mem: 20.9 GB 
[11/27 02:27:46 visual_prompt]: Epoch 5 / 100: avg data time: 1.41e-01, avg batch time: 0.9742, average train loss: 5.5591
[11/27 02:28:42 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3098, average loss: 6.7012
[11/27 02:28:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.04	
[11/27 02:28:42 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/27 02:30:24 visual_prompt]: 	Training 100/553. train loss: 14.3782,	0.8440 s / batch. (data: 7.68e-04). ETA=12:17:36, max mem: 20.9 GB 
[11/27 02:32:00 visual_prompt]: 	Training 200/553. train loss: 7.5807,	0.8480 s / batch. (data: 3.40e-04). ETA=12:19:38, max mem: 20.9 GB 
[11/27 02:33:35 visual_prompt]: 	Training 300/553. train loss: 8.5515,	0.8351 s / batch. (data: 3.28e-04). ETA=12:07:02, max mem: 20.9 GB 
[11/27 02:35:17 visual_prompt]: 	Training 400/553. train loss: 6.8434,	0.8320 s / batch. (data: 3.57e-04). ETA=12:02:55, max mem: 20.9 GB 
[11/27 02:36:52 visual_prompt]: 	Training 500/553. train loss: 6.3768,	0.8520 s / batch. (data: 3.49e-04). ETA=12:18:53, max mem: 20.9 GB 
[11/27 02:37:43 visual_prompt]: Epoch 6 / 100: avg data time: 1.44e-01, avg batch time: 0.9778, average train loss: 7.4405
[11/27 02:38:38 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3099, average loss: 0.7440
[11/27 02:38:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.26	
[11/27 02:38:38 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/27 02:40:19 visual_prompt]: 	Training 100/553. train loss: 4.6580,	0.8362 s / batch. (data: 7.95e-03). ETA=12:03:05, max mem: 20.9 GB 
[11/27 02:41:56 visual_prompt]: 	Training 200/553. train loss: 6.2999,	0.8281 s / batch. (data: 3.08e-04). ETA=11:54:39, max mem: 20.9 GB 
[11/27 02:43:37 visual_prompt]: 	Training 300/553. train loss: 1.2756,	1.8372 s / batch. (data: 1.01e+00). ETA=1 day, 2:22:28, max mem: 20.9 GB 
[11/27 02:45:14 visual_prompt]: 	Training 400/553. train loss: 2.8603,	1.6547 s / batch. (data: 8.35e-01). ETA=23:42:32, max mem: 20.9 GB 
[11/27 02:46:50 visual_prompt]: 	Training 500/553. train loss: 7.7095,	0.8481 s / batch. (data: 3.18e-04). ETA=12:07:40, max mem: 20.9 GB 
[11/27 02:47:39 visual_prompt]: Epoch 7 / 100: avg data time: 1.43e-01, avg batch time: 0.9770, average train loss: 8.8655
[11/27 02:48:34 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3097, average loss: 0.8059
[11/27 02:48:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.19	
[11/27 02:48:34 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/27 02:50:14 visual_prompt]: 	Training 100/553. train loss: 5.4048,	0.8800 s / batch. (data: 3.15e-04). ETA=12:32:50, max mem: 20.9 GB 
[11/27 02:51:52 visual_prompt]: 	Training 200/553. train loss: 13.6669,	0.8373 s / batch. (data: 2.92e-04). ETA=11:54:56, max mem: 20.9 GB 
[11/27 02:53:29 visual_prompt]: 	Training 300/553. train loss: 3.8111,	0.8400 s / batch. (data: 2.89e-04). ETA=11:55:49, max mem: 20.9 GB 
[11/27 02:55:07 visual_prompt]: 	Training 400/553. train loss: 13.1761,	0.8365 s / batch. (data: 8.73e-03). ETA=11:51:27, max mem: 20.9 GB 
[11/27 02:56:43 visual_prompt]: 	Training 500/553. train loss: 58.6027,	1.3623 s / batch. (data: 5.34e-01). ETA=19:16:19, max mem: 20.9 GB 
[11/27 02:57:34 visual_prompt]: Epoch 8 / 100: avg data time: 1.41e-01, avg batch time: 0.9757, average train loss: 10.5169
[11/27 02:58:30 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3113, average loss: 7.4940
[11/27 02:58:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.82	
[11/27 02:58:30 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/27 03:00:10 visual_prompt]: 	Training 100/553. train loss: 11.2248,	0.8818 s / batch. (data: 2.58e-02). ETA=12:26:16, max mem: 20.9 GB 
[11/27 03:01:46 visual_prompt]: 	Training 200/553. train loss: 17.2262,	0.8314 s / batch. (data: 7.96e-03). ETA=11:42:09, max mem: 20.9 GB 
[11/27 03:03:24 visual_prompt]: 	Training 300/553. train loss: 1.2406,	1.4097 s / batch. (data: 5.70e-01). ETA=19:48:17, max mem: 20.9 GB 
[11/27 03:05:02 visual_prompt]: 	Training 400/553. train loss: 14.1322,	0.8264 s / batch. (data: 7.48e-04). ETA=11:35:15, max mem: 20.9 GB 
[11/27 03:06:39 visual_prompt]: 	Training 500/553. train loss: 28.9069,	0.8885 s / batch. (data: 5.56e-02). ETA=12:26:01, max mem: 20.9 GB 
[11/27 03:07:29 visual_prompt]: Epoch 9 / 100: avg data time: 1.41e-01, avg batch time: 0.9746, average train loss: 14.5256
[11/27 03:08:24 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3105, average loss: 7.8257
[11/27 03:08:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.12	
[11/27 03:08:24 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/27 03:10:08 visual_prompt]: 	Training 100/553. train loss: 14.5590,	0.8560 s / batch. (data: 3.09e-04). ETA=11:56:29, max mem: 20.9 GB 
[11/27 03:11:43 visual_prompt]: 	Training 200/553. train loss: 1.0110,	0.8517 s / batch. (data: 1.04e-02). ETA=11:51:29, max mem: 20.9 GB 
[11/27 03:13:19 visual_prompt]: 	Training 300/553. train loss: 35.9825,	1.5680 s / batch. (data: 7.41e-01). ETA=21:47:17, max mem: 20.9 GB 
[11/27 03:14:55 visual_prompt]: 	Training 400/553. train loss: 25.0067,	0.8459 s / batch. (data: 5.56e-03). ETA=11:43:48, max mem: 20.9 GB 
[11/27 03:16:33 visual_prompt]: 	Training 500/553. train loss: 3.2665,	0.8360 s / batch. (data: 3.58e-04). ETA=11:34:11, max mem: 20.9 GB 
[11/27 03:17:24 visual_prompt]: Epoch 10 / 100: avg data time: 1.42e-01, avg batch time: 0.9762, average train loss: 15.3921
[11/27 03:18:19 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3112, average loss: 8.6444
[11/27 03:18:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.01	
[11/27 03:18:19 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/27 03:20:02 visual_prompt]: 	Training 100/553. train loss: 2.8445,	0.8326 s / batch. (data: 8.51e-03). ETA=11:29:13, max mem: 20.9 GB 
[11/27 03:21:41 visual_prompt]: 	Training 200/553. train loss: 48.9576,	0.8199 s / batch. (data: 4.67e-04). ETA=11:17:24, max mem: 20.9 GB 
[11/27 03:23:18 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.6855 s / batch. (data: 8.51e-01). ETA=23:09:43, max mem: 20.9 GB 
[11/27 03:24:54 visual_prompt]: 	Training 400/553. train loss: 2.8517,	0.8480 s / batch. (data: 2.96e-04). ETA=11:37:45, max mem: 20.9 GB 
[11/27 03:26:29 visual_prompt]: 	Training 500/553. train loss: 48.7725,	0.8175 s / batch. (data: 3.85e-03). ETA=11:11:17, max mem: 20.9 GB 
[11/27 03:27:21 visual_prompt]: Epoch 11 / 100: avg data time: 1.45e-01, avg batch time: 0.9783, average train loss: 15.8937
[11/27 03:28:16 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3104, average loss: 2.6196
[11/27 03:28:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.89	
[11/27 03:28:16 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/27 03:29:59 visual_prompt]: 	Training 100/553. train loss: 16.8725,	0.8792 s / batch. (data: 5.99e-02). ETA=11:59:43, max mem: 20.9 GB 
[11/27 03:31:37 visual_prompt]: 	Training 200/553. train loss: 10.2299,	1.1240 s / batch. (data: 3.00e-01). ETA=15:18:16, max mem: 20.9 GB 
[11/27 03:33:13 visual_prompt]: 	Training 300/553. train loss: 23.4843,	0.8407 s / batch. (data: 2.92e-04). ETA=11:25:22, max mem: 20.9 GB 
[11/27 03:34:52 visual_prompt]: 	Training 400/553. train loss: 21.7349,	0.8566 s / batch. (data: 9.41e-03). ETA=11:36:58, max mem: 20.9 GB 
[11/27 03:36:29 visual_prompt]: 	Training 500/553. train loss: 6.1085,	0.8520 s / batch. (data: 3.36e-04). ETA=11:31:47, max mem: 20.9 GB 
[11/27 03:37:19 visual_prompt]: Epoch 12 / 100: avg data time: 1.47e-01, avg batch time: 0.9809, average train loss: 15.7183
[11/27 03:38:15 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3106, average loss: 9.6142
[11/27 03:38:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.14	
[11/27 03:38:15 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/27 03:39:58 visual_prompt]: 	Training 100/553. train loss: 5.4204,	0.8565 s / batch. (data: 2.04e-02). ETA=11:33:13, max mem: 20.9 GB 
[11/27 03:41:31 visual_prompt]: 	Training 200/553. train loss: 18.7901,	0.8705 s / batch. (data: 2.25e-02). ETA=11:43:07, max mem: 20.9 GB 
[11/27 03:43:10 visual_prompt]: 	Training 300/553. train loss: 4.6744,	1.6965 s / batch. (data: 8.45e-01). ETA=22:47:29, max mem: 20.9 GB 
[11/27 03:44:47 visual_prompt]: 	Training 400/553. train loss: 22.9821,	0.8477 s / batch. (data: 7.95e-03). ETA=11:21:53, max mem: 20.9 GB 
[11/27 03:46:25 visual_prompt]: 	Training 500/553. train loss: 55.6965,	0.8579 s / batch. (data: 9.73e-03). ETA=11:28:37, max mem: 20.9 GB 
[11/27 03:47:17 visual_prompt]: Epoch 13 / 100: avg data time: 1.47e-01, avg batch time: 0.9798, average train loss: 18.4516
[11/27 03:48:12 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3102, average loss: 39.4750
[11/27 03:48:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.53	
[11/27 03:48:12 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/27 03:49:55 visual_prompt]: 	Training 100/553. train loss: 7.5923,	0.8400 s / batch. (data: 3.52e-04). ETA=11:12:08, max mem: 20.9 GB 
[11/27 03:51:32 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.9800 s / batch. (data: 1.54e-01). ETA=13:02:32, max mem: 20.9 GB 
[11/27 03:53:08 visual_prompt]: 	Training 300/553. train loss: 13.0603,	0.8318 s / batch. (data: 1.08e-02). ETA=11:02:48, max mem: 20.9 GB 
[11/27 03:54:45 visual_prompt]: 	Training 400/553. train loss: 15.3959,	0.8445 s / batch. (data: 1.19e-02). ETA=11:11:33, max mem: 20.9 GB 
[11/27 03:56:22 visual_prompt]: 	Training 500/553. train loss: 9.6030,	0.8259 s / batch. (data: 2.99e-04). ETA=10:55:21, max mem: 20.9 GB 
[11/27 03:57:12 visual_prompt]: Epoch 14 / 100: avg data time: 1.43e-01, avg batch time: 0.9764, average train loss: 17.0693
[11/27 03:58:08 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3089, average loss: 30.5229
[11/27 03:58:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.87	
[11/27 03:58:08 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/27 03:59:49 visual_prompt]: 	Training 100/553. train loss: 9.4049,	0.8417 s / batch. (data: 3.17e-04). ETA=11:05:43, max mem: 20.9 GB 
[11/27 04:01:25 visual_prompt]: 	Training 200/553. train loss: 97.4315,	0.8274 s / batch. (data: 3.02e-04). ETA=10:53:04, max mem: 20.9 GB 
[11/27 04:03:05 visual_prompt]: 	Training 300/553. train loss: 53.9615,	0.8569 s / batch. (data: 3.45e-04). ETA=11:14:54, max mem: 20.9 GB 
[11/27 04:04:40 visual_prompt]: 	Training 400/553. train loss: 5.0474,	0.8628 s / batch. (data: 1.56e-02). ETA=11:18:08, max mem: 20.9 GB 
[11/27 04:06:19 visual_prompt]: 	Training 500/553. train loss: 2.5343,	0.8358 s / batch. (data: 7.95e-03). ETA=10:55:30, max mem: 20.9 GB 
[11/27 04:07:09 visual_prompt]: Epoch 15 / 100: avg data time: 1.46e-01, avg batch time: 0.9793, average train loss: 15.8687
[11/27 04:08:05 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3106, average loss: 5.2078
[11/27 04:08:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.60	
[11/27 04:08:05 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/27 04:09:44 visual_prompt]: 	Training 100/553. train loss: 2.0542,	0.8298 s / batch. (data: 3.10e-04). ETA=10:48:42, max mem: 20.9 GB 
[11/27 04:11:22 visual_prompt]: 	Training 200/553. train loss: 7.6370,	0.8493 s / batch. (data: 1.55e-02). ETA=11:02:31, max mem: 20.9 GB 
[11/27 04:13:00 visual_prompt]: 	Training 300/553. train loss: 33.3452,	0.8449 s / batch. (data: 7.98e-03). ETA=10:57:41, max mem: 20.9 GB 
[11/27 04:14:37 visual_prompt]: 	Training 400/553. train loss: 29.9088,	0.8240 s / batch. (data: 3.44e-04). ETA=10:40:02, max mem: 20.9 GB 
[11/27 04:16:13 visual_prompt]: 	Training 500/553. train loss: 51.7052,	1.4320 s / batch. (data: 5.99e-01). ETA=18:29:53, max mem: 20.9 GB 
[11/27 04:17:05 visual_prompt]: Epoch 16 / 100: avg data time: 1.43e-01, avg batch time: 0.9763, average train loss: 16.8948
[11/27 04:18:00 visual_prompt]: Inference (val):avg data time: 1.60e-04, avg batch time: 0.3103, average loss: 15.7914
[11/27 04:18:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.27	
[11/27 04:18:00 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/27 04:19:40 visual_prompt]: 	Training 100/553. train loss: 4.7427,	0.8520 s / batch. (data: 1.19e-02). ETA=10:58:10, max mem: 20.9 GB 
[11/27 04:21:19 visual_prompt]: 	Training 200/553. train loss: 38.9712,	0.8185 s / batch. (data: 3.04e-04). ETA=10:30:54, max mem: 20.9 GB 
[11/27 04:22:56 visual_prompt]: 	Training 300/553. train loss: 31.7634,	0.8329 s / batch. (data: 2.92e-04). ETA=10:40:37, max mem: 20.9 GB 
[11/27 04:24:33 visual_prompt]: 	Training 400/553. train loss: 12.3765,	1.1040 s / batch. (data: 2.77e-01). ETA=14:07:21, max mem: 20.9 GB 
[11/27 04:26:10 visual_prompt]: 	Training 500/553. train loss: 48.7165,	1.5800 s / batch. (data: 7.16e-01). ETA=20:10:03, max mem: 20.9 GB 
[11/27 04:27:02 visual_prompt]: Epoch 17 / 100: avg data time: 1.45e-01, avg batch time: 0.9795, average train loss: 15.6109
[11/27 04:27:58 visual_prompt]: Inference (val):avg data time: 1.99e-04, avg batch time: 0.3094, average loss: 55.4827
[11/27 04:27:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.53	
[11/27 04:27:58 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/27 04:29:39 visual_prompt]: 	Training 100/553. train loss: 9.5563,	0.8360 s / batch. (data: 3.20e-04). ETA=10:38:06, max mem: 20.9 GB 
[11/27 04:31:19 visual_prompt]: 	Training 200/553. train loss: 3.4535,	0.8512 s / batch. (data: 1.04e-02). ETA=10:48:17, max mem: 20.9 GB 
[11/27 04:32:56 visual_prompt]: 	Training 300/553. train loss: 1.8644,	0.8362 s / batch. (data: 3.51e-03). ETA=10:35:29, max mem: 20.9 GB 
[11/27 04:34:33 visual_prompt]: 	Training 400/553. train loss: 1.3397,	0.8248 s / batch. (data: 5.46e-03). ETA=10:25:29, max mem: 20.9 GB 
[11/27 04:36:10 visual_prompt]: 	Training 500/553. train loss: 4.7031,	0.8332 s / batch. (data: 3.03e-04). ETA=10:30:28, max mem: 20.9 GB 
[11/27 04:37:00 visual_prompt]: Epoch 18 / 100: avg data time: 1.46e-01, avg batch time: 0.9800, average train loss: 14.6306
[11/27 04:37:55 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3106, average loss: 12.1063
[11/27 04:37:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.99	
[11/27 04:37:55 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/27 04:39:36 visual_prompt]: 	Training 100/553. train loss: 19.3821,	1.1885 s / batch. (data: 3.59e-01). ETA=14:56:16, max mem: 20.9 GB 
[11/27 04:41:15 visual_prompt]: 	Training 200/553. train loss: 1.5416,	0.8695 s / batch. (data: 3.74e-02). ETA=10:54:14, max mem: 20.9 GB 
[11/27 04:42:52 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8188 s / batch. (data: 3.39e-04). ETA=10:14:42, max mem: 20.9 GB 
[11/27 04:44:31 visual_prompt]: 	Training 400/553. train loss: 3.8163,	0.8444 s / batch. (data: 7.94e-04). ETA=10:32:34, max mem: 20.9 GB 
[11/27 04:46:04 visual_prompt]: 	Training 500/553. train loss: 1.2225,	0.8355 s / batch. (data: 5.47e-03). ETA=10:24:28, max mem: 20.9 GB 
[11/27 04:46:55 visual_prompt]: Epoch 19 / 100: avg data time: 1.42e-01, avg batch time: 0.9763, average train loss: 17.8823
[11/27 04:47:51 visual_prompt]: Inference (val):avg data time: 3.70e-04, avg batch time: 0.3112, average loss: 38.2857
[11/27 04:47:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.57	
[11/27 04:47:51 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/27 04:49:31 visual_prompt]: 	Training 100/553. train loss: 8.3854,	0.8320 s / batch. (data: 3.21e-04). ETA=10:19:43, max mem: 20.9 GB 
[11/27 04:51:10 visual_prompt]: 	Training 200/553. train loss: 15.4685,	0.8320 s / batch. (data: 7.95e-03). ETA=10:18:20, max mem: 20.9 GB 
[11/27 04:52:48 visual_prompt]: 	Training 300/553. train loss: 7.9870,	0.8280 s / batch. (data: 2.96e-04). ETA=10:14:00, max mem: 20.9 GB 
[11/27 04:54:25 visual_prompt]: 	Training 400/553. train loss: 4.9703,	0.8360 s / batch. (data: 3.01e-04). ETA=10:18:32, max mem: 20.9 GB 
[11/27 04:56:02 visual_prompt]: 	Training 500/553. train loss: 7.5007,	0.8560 s / batch. (data: 2.98e-04). ETA=10:31:54, max mem: 20.9 GB 
[11/27 04:56:55 visual_prompt]: Epoch 20 / 100: avg data time: 1.49e-01, avg batch time: 0.9824, average train loss: 15.1892
[11/27 04:57:50 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.3087, average loss: 4.2038
[11/27 04:57:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.69	
[11/27 04:57:50 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[11/27 04:59:33 visual_prompt]: 	Training 100/553. train loss: 34.4833,	0.8884 s / batch. (data: 6.22e-02). ETA=10:53:35, max mem: 20.9 GB 
[11/27 05:01:10 visual_prompt]: 	Training 200/553. train loss: 46.2701,	0.8366 s / batch. (data: 2.93e-04). ETA=10:14:04, max mem: 20.9 GB 
[11/27 05:02:46 visual_prompt]: 	Training 300/553. train loss: 20.9474,	1.0880 s / batch. (data: 2.53e-01). ETA=13:16:47, max mem: 20.9 GB 
[11/27 05:04:22 visual_prompt]: 	Training 400/553. train loss: 60.0103,	0.8327 s / batch. (data: 7.95e-03). ETA=10:08:23, max mem: 20.9 GB 
[11/27 05:06:00 visual_prompt]: 	Training 500/553. train loss: 14.5543,	0.8360 s / batch. (data: 3.12e-04). ETA=10:09:27, max mem: 20.9 GB 
[11/27 05:06:50 visual_prompt]: Epoch 21 / 100: avg data time: 1.42e-01, avg batch time: 0.9760, average train loss: 16.8275
[11/27 05:07:45 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3106, average loss: 16.6812
[11/27 05:07:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.55	
[11/27 05:07:45 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[11/27 05:09:25 visual_prompt]: 	Training 100/553. train loss: 5.9140,	0.8247 s / batch. (data: 4.13e-03). ETA=9:59:05, max mem: 20.9 GB 
[11/27 05:11:03 visual_prompt]: 	Training 200/553. train loss: 5.5664,	0.8395 s / batch. (data: 1.20e-02). ETA=10:08:27, max mem: 20.9 GB 
[11/27 05:12:37 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8272 s / batch. (data: 3.10e-04). ETA=9:58:11, max mem: 20.9 GB 
[11/27 05:14:15 visual_prompt]: 	Training 400/553. train loss: 9.2249,	0.8492 s / batch. (data: 1.60e-02). ETA=10:12:40, max mem: 20.9 GB 
[11/27 05:15:53 visual_prompt]: 	Training 500/553. train loss: 17.4958,	0.8347 s / batch. (data: 3.34e-04). ETA=10:00:46, max mem: 20.9 GB 
[11/27 05:16:44 visual_prompt]: Epoch 22 / 100: avg data time: 1.42e-01, avg batch time: 0.9746, average train loss: 14.0558
[11/27 05:17:40 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3104, average loss: 25.2010
[11/27 05:17:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.91	
[11/27 05:17:40 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[11/27 05:19:23 visual_prompt]: 	Training 100/553. train loss: 2.0199,	0.8240 s / batch. (data: 3.15e-04). ETA=9:51:00, max mem: 20.9 GB 
[11/27 05:21:00 visual_prompt]: 	Training 200/553. train loss: 18.6884,	0.8308 s / batch. (data: 1.21e-02). ETA=9:54:29, max mem: 20.9 GB 
[11/27 05:22:40 visual_prompt]: 	Training 300/553. train loss: 11.0697,	0.8432 s / batch. (data: 5.44e-03). ETA=10:01:59, max mem: 20.9 GB 
[11/27 05:24:16 visual_prompt]: 	Training 400/553. train loss: 3.5827,	0.8192 s / batch. (data: 2.84e-04). ETA=9:43:26, max mem: 20.9 GB 
[11/27 05:25:51 visual_prompt]: 	Training 500/553. train loss: 3.3811,	0.8559 s / batch. (data: 2.06e-02). ETA=10:08:09, max mem: 20.9 GB 
[11/27 05:26:42 visual_prompt]: Epoch 23 / 100: avg data time: 1.48e-01, avg batch time: 0.9810, average train loss: 15.2128
[11/27 05:27:38 visual_prompt]: Inference (val):avg data time: 2.65e-04, avg batch time: 0.3116, average loss: 10.3884
[11/27 05:27:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.68	
[11/27 05:27:38 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[11/27 05:29:16 visual_prompt]: 	Training 100/553. train loss: 5.0066,	0.8249 s / batch. (data: 7.90e-03). ETA=9:44:00, max mem: 20.9 GB 
[11/27 05:30:52 visual_prompt]: 	Training 200/553. train loss: 35.4084,	0.8289 s / batch. (data: 5.39e-03). ETA=9:45:27, max mem: 20.9 GB 
[11/27 05:32:30 visual_prompt]: 	Training 300/553. train loss: 17.1040,	0.8326 s / batch. (data: 3.08e-04). ETA=9:46:43, max mem: 20.9 GB 
[11/27 05:34:07 visual_prompt]: 	Training 400/553. train loss: 8.9767,	0.8500 s / batch. (data: 3.21e-04). ETA=9:57:35, max mem: 20.9 GB 
[11/27 05:35:46 visual_prompt]: 	Training 500/553. train loss: 4.3326,	0.8560 s / batch. (data: 1.20e-02). ETA=10:00:20, max mem: 20.9 GB 
[11/27 05:36:37 visual_prompt]: Epoch 24 / 100: avg data time: 1.42e-01, avg batch time: 0.9756, average train loss: 16.5487
[11/27 05:37:33 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3106, average loss: 10.6571
[11/27 05:37:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.46	
[11/27 05:37:33 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[11/27 05:39:16 visual_prompt]: 	Training 100/553. train loss: 18.5648,	0.8312 s / batch. (data: 3.53e-04). ETA=9:40:50, max mem: 20.9 GB 
[11/27 05:40:48 visual_prompt]: 	Training 200/553. train loss: 16.5736,	0.9695 s / batch. (data: 1.44e-01). ETA=11:15:51, max mem: 20.9 GB 
[11/27 05:42:24 visual_prompt]: 	Training 300/553. train loss: 13.8536,	0.8364 s / batch. (data: 5.47e-03). ETA=9:41:43, max mem: 20.9 GB 
[11/27 05:44:01 visual_prompt]: 	Training 400/553. train loss: 37.6028,	1.1735 s / batch. (data: 3.57e-01). ETA=13:34:09, max mem: 20.9 GB 
[11/27 05:45:39 visual_prompt]: 	Training 500/553. train loss: 9.4053,	1.3135 s / batch. (data: 4.85e-01). ETA=15:09:08, max mem: 20.9 GB 
[11/27 05:46:29 visual_prompt]: Epoch 25 / 100: avg data time: 1.37e-01, avg batch time: 0.9707, average train loss: 14.4637
[11/27 05:47:25 visual_prompt]: Inference (val):avg data time: 4.40e-05, avg batch time: 0.3097, average loss: 13.4995
[11/27 05:47:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.05	
[11/27 05:47:25 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[11/27 05:49:06 visual_prompt]: 	Training 100/553. train loss: 36.1180,	0.8316 s / batch. (data: 3.05e-04). ETA=9:33:26, max mem: 20.9 GB 
[11/27 05:50:45 visual_prompt]: 	Training 200/553. train loss: 11.8235,	1.7003 s / batch. (data: 8.64e-01). ETA=19:29:40, max mem: 20.9 GB 
[11/27 05:52:24 visual_prompt]: 	Training 300/553. train loss: 0.1046,	0.8621 s / batch. (data: 6.98e-04). ETA=9:51:35, max mem: 20.9 GB 
[11/27 05:54:01 visual_prompt]: 	Training 400/553. train loss: 7.7271,	0.8400 s / batch. (data: 2.92e-04). ETA=9:35:02, max mem: 20.9 GB 
[11/27 05:55:36 visual_prompt]: 	Training 500/553. train loss: 35.4509,	0.8448 s / batch. (data: 3.20e-04). ETA=9:36:56, max mem: 20.9 GB 
[11/27 05:56:26 visual_prompt]: Epoch 26 / 100: avg data time: 1.46e-01, avg batch time: 0.9788, average train loss: 16.9598
[11/27 05:57:22 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3110, average loss: 6.9093
[11/27 05:57:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.78	
[11/27 05:57:22 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[11/27 05:59:05 visual_prompt]: 	Training 100/553. train loss: 14.0180,	0.8608 s / batch. (data: 9.76e-03). ETA=9:45:38, max mem: 20.9 GB 
[11/27 06:00:42 visual_prompt]: 	Training 200/553. train loss: 2.9926,	0.9879 s / batch. (data: 1.59e-01). ETA=11:10:31, max mem: 20.9 GB 
[11/27 06:02:19 visual_prompt]: 	Training 300/553. train loss: 3.9115,	0.8280 s / batch. (data: 3.36e-04). ETA=9:20:34, max mem: 20.9 GB 
[11/27 06:03:58 visual_prompt]: 	Training 400/553. train loss: 6.8911,	0.8199 s / batch. (data: 3.17e-04). ETA=9:13:44, max mem: 20.9 GB 
[11/27 06:05:36 visual_prompt]: 	Training 500/553. train loss: 5.6276,	0.8362 s / batch. (data: 7.93e-04). ETA=9:23:20, max mem: 20.9 GB 
[11/27 06:06:24 visual_prompt]: Epoch 27 / 100: avg data time: 1.47e-01, avg batch time: 0.9808, average train loss: 15.7101
[11/27 06:07:20 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3102, average loss: 21.7686
[11/27 06:07:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.08	
[11/27 06:07:20 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[11/27 06:09:00 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8320 s / batch. (data: 7.95e-03). ETA=9:18:23, max mem: 20.9 GB 
[11/27 06:10:38 visual_prompt]: 	Training 200/553. train loss: 5.6202,	0.8491 s / batch. (data: 1.19e-02). ETA=9:28:28, max mem: 20.9 GB 
[11/27 06:12:16 visual_prompt]: 	Training 300/553. train loss: 5.2136,	1.4679 s / batch. (data: 6.24e-01). ETA=16:20:17, max mem: 20.9 GB 
[11/27 06:13:51 visual_prompt]: 	Training 400/553. train loss: 2.7923,	0.8302 s / batch. (data: 5.41e-03). ETA=9:13:03, max mem: 20.9 GB 
[11/27 06:15:27 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8200 s / batch. (data: 3.13e-04). ETA=9:04:54, max mem: 20.9 GB 
[11/27 06:16:19 visual_prompt]: Epoch 28 / 100: avg data time: 1.40e-01, avg batch time: 0.9743, average train loss: 13.6394
[11/27 06:17:14 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3096, average loss: 7.5203
[11/27 06:17:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.00	
[11/27 06:17:14 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[11/27 06:19:01 visual_prompt]: 	Training 100/553. train loss: 22.3982,	0.8213 s / batch. (data: 3.03e-04). ETA=9:03:37, max mem: 20.9 GB 
[11/27 06:20:37 visual_prompt]: 	Training 200/553. train loss: 2.4717,	1.7120 s / batch. (data: 8.73e-01). ETA=18:50:22, max mem: 20.9 GB 
[11/27 06:22:12 visual_prompt]: 	Training 300/553. train loss: 3.4475,	0.8630 s / batch. (data: 1.09e-02). ETA=9:28:22, max mem: 20.9 GB 
[11/27 06:23:46 visual_prompt]: 	Training 400/553. train loss: 17.5801,	1.2057 s / batch. (data: 3.65e-01). ETA=13:12:02, max mem: 20.9 GB 
[11/27 06:25:24 visual_prompt]: 	Training 500/553. train loss: 6.5623,	0.8200 s / batch. (data: 3.33e-04). ETA=8:57:18, max mem: 20.9 GB 
[11/27 06:26:14 visual_prompt]: Epoch 29 / 100: avg data time: 1.42e-01, avg batch time: 0.9761, average train loss: 15.3266
[11/27 06:27:09 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3107, average loss: 16.0214
[11/27 06:27:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.26	
[11/27 06:27:09 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[11/27 06:28:49 visual_prompt]: 	Training 100/553. train loss: 25.7991,	0.8315 s / batch. (data: 5.43e-03). ETA=9:02:45, max mem: 20.9 GB 
[11/27 06:30:27 visual_prompt]: 	Training 200/553. train loss: 70.0805,	0.8476 s / batch. (data: 2.24e-02). ETA=9:11:50, max mem: 20.9 GB 
[11/27 06:32:03 visual_prompt]: 	Training 300/553. train loss: 0.0962,	1.9277 s / batch. (data: 1.10e+00). ETA=20:51:50, max mem: 20.9 GB 
[11/27 06:33:41 visual_prompt]: 	Training 400/553. train loss: 31.9951,	1.0216 s / batch. (data: 1.69e-01). ETA=11:01:44, max mem: 20.9 GB 
[11/27 06:35:16 visual_prompt]: 	Training 500/553. train loss: 27.1111,	1.2040 s / batch. (data: 3.64e-01). ETA=12:57:50, max mem: 20.9 GB 
[11/27 06:36:09 visual_prompt]: Epoch 30 / 100: avg data time: 1.42e-01, avg batch time: 0.9753, average train loss: 15.6942
[11/27 06:37:05 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3096, average loss: 13.3111
[11/27 06:37:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.03	
[11/27 06:37:05 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[11/27 06:38:48 visual_prompt]: 	Training 100/553. train loss: 11.2105,	0.8407 s / batch. (data: 5.41e-03). ETA=9:00:58, max mem: 20.9 GB 
[11/27 06:40:27 visual_prompt]: 	Training 200/553. train loss: 24.2543,	0.8680 s / batch. (data: 1.19e-02). ETA=9:17:07, max mem: 20.9 GB 
[11/27 06:42:02 visual_prompt]: 	Training 300/553. train loss: 19.0948,	0.8520 s / batch. (data: 5.79e-03). ETA=9:05:25, max mem: 20.9 GB 
[11/27 06:43:39 visual_prompt]: 	Training 400/553. train loss: 37.6698,	1.2385 s / batch. (data: 4.15e-01). ETA=13:10:46, max mem: 20.9 GB 
[11/27 06:45:17 visual_prompt]: 	Training 500/553. train loss: 2.2036,	0.8440 s / batch. (data: 3.19e-04). ETA=8:57:28, max mem: 20.9 GB 
[11/27 06:46:07 visual_prompt]: Epoch 31 / 100: avg data time: 1.47e-01, avg batch time: 0.9804, average train loss: 14.0625
[11/27 06:47:03 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3117, average loss: 1.2447
[11/27 06:47:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.04	
[11/27 06:47:03 visual_prompt]: Best epoch 31: best metric: -1.245
[11/27 06:47:03 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[11/27 06:48:45 visual_prompt]: 	Training 100/553. train loss: 6.3096,	0.8320 s / batch. (data: 3.07e-04). ETA=8:47:43, max mem: 20.9 GB 
[11/27 06:50:23 visual_prompt]: 	Training 200/553. train loss: 39.0313,	0.8187 s / batch. (data: 3.12e-04). ETA=8:37:55, max mem: 20.9 GB 
[11/27 06:52:02 visual_prompt]: 	Training 300/553. train loss: 6.9512,	0.8280 s / batch. (data: 3.29e-04). ETA=8:42:25, max mem: 20.9 GB 
[11/27 06:53:41 visual_prompt]: 	Training 400/553. train loss: 22.4263,	0.8440 s / batch. (data: 3.04e-04). ETA=8:51:06, max mem: 20.9 GB 
[11/27 06:55:16 visual_prompt]: 	Training 500/553. train loss: 3.6112,	0.8273 s / batch. (data: 7.95e-03). ETA=8:39:14, max mem: 20.9 GB 
[11/27 06:56:05 visual_prompt]: Epoch 32 / 100: avg data time: 1.46e-01, avg batch time: 0.9800, average train loss: 14.8127
[11/27 06:57:00 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3086, average loss: 42.6813
[11/27 06:57:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.59	
[11/27 06:57:00 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[11/27 06:58:40 visual_prompt]: 	Training 100/553. train loss: 0.0000,	1.2521 s / batch. (data: 4.10e-01). ETA=13:02:37, max mem: 20.9 GB 
[11/27 07:00:19 visual_prompt]: 	Training 200/553. train loss: 4.3486,	0.8196 s / batch. (data: 3.10e-04). ETA=8:30:56, max mem: 20.9 GB 
[11/27 07:01:56 visual_prompt]: 	Training 300/553. train loss: 11.4001,	0.8905 s / batch. (data: 1.56e-02). ETA=9:13:39, max mem: 20.9 GB 
[11/27 07:03:34 visual_prompt]: 	Training 400/553. train loss: 0.8651,	0.8200 s / batch. (data: 3.14e-04). ETA=8:28:26, max mem: 20.9 GB 
[11/27 07:05:10 visual_prompt]: 	Training 500/553. train loss: 3.4264,	0.8320 s / batch. (data: 3.34e-04). ETA=8:34:30, max mem: 20.9 GB 
[11/27 07:06:01 visual_prompt]: Epoch 33 / 100: avg data time: 1.43e-01, avg batch time: 0.9772, average train loss: 13.1241
[11/27 07:06:56 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3086, average loss: 7.1616
[11/27 07:06:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.67	
[11/27 07:06:56 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[11/27 07:08:39 visual_prompt]: 	Training 100/553. train loss: 11.0495,	1.1024 s / batch. (data: 2.87e-01). ETA=11:18:55, max mem: 20.9 GB 
[11/27 07:10:14 visual_prompt]: 	Training 200/553. train loss: 8.1172,	1.0615 s / batch. (data: 2.38e-01). ETA=10:51:55, max mem: 20.9 GB 
[11/27 07:11:50 visual_prompt]: 	Training 300/553. train loss: 14.3706,	0.8480 s / batch. (data: 3.50e-04). ETA=8:39:23, max mem: 20.9 GB 
[11/27 07:13:28 visual_prompt]: 	Training 400/553. train loss: 3.3792,	0.8400 s / batch. (data: 3.28e-04). ETA=8:33:06, max mem: 20.9 GB 
[11/27 07:15:05 visual_prompt]: 	Training 500/553. train loss: 4.9781,	1.1740 s / batch. (data: 3.50e-01). ETA=11:55:11, max mem: 20.9 GB 
[11/27 07:15:56 visual_prompt]: Epoch 34 / 100: avg data time: 1.41e-01, avg batch time: 0.9750, average train loss: 13.8015
[11/27 07:16:51 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3102, average loss: 20.8252
[11/27 07:16:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.94	
[11/27 07:16:51 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[11/27 07:18:33 visual_prompt]: 	Training 100/553. train loss: 7.3974,	0.8295 s / batch. (data: 3.27e-04). ETA=8:23:10, max mem: 20.9 GB 
[11/27 07:20:11 visual_prompt]: 	Training 200/553. train loss: 4.7591,	0.8200 s / batch. (data: 3.21e-04). ETA=8:16:04, max mem: 20.9 GB 
[11/27 07:21:47 visual_prompt]: 	Training 300/553. train loss: 6.4350,	0.8200 s / batch. (data: 3.20e-04). ETA=8:14:41, max mem: 20.9 GB 
[11/27 07:23:22 visual_prompt]: 	Training 400/553. train loss: 12.5995,	0.8485 s / batch. (data: 3.39e-04). ETA=8:30:27, max mem: 20.9 GB 
[11/27 07:24:58 visual_prompt]: 	Training 500/553. train loss: 3.3689,	1.0397 s / batch. (data: 1.88e-01). ETA=10:23:47, max mem: 20.9 GB 
[11/27 07:25:49 visual_prompt]: Epoch 35 / 100: avg data time: 1.39e-01, avg batch time: 0.9730, average train loss: 13.4037
[11/27 07:26:45 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3099, average loss: 7.7906
[11/27 07:26:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.24	
[11/27 07:26:45 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[11/27 07:28:25 visual_prompt]: 	Training 100/553. train loss: 9.8696,	0.8593 s / batch. (data: 1.57e-02). ETA=8:33:22, max mem: 20.9 GB 
[11/27 07:30:02 visual_prompt]: 	Training 200/553. train loss: 16.3175,	0.8234 s / batch. (data: 3.39e-04). ETA=8:10:31, max mem: 20.9 GB 
[11/27 07:31:41 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8440 s / batch. (data: 4.19e-04). ETA=8:21:23, max mem: 20.9 GB 
[11/27 07:33:17 visual_prompt]: 	Training 400/553. train loss: 84.3921,	0.8248 s / batch. (data: 3.10e-04). ETA=8:08:35, max mem: 20.9 GB 
[11/27 07:34:55 visual_prompt]: 	Training 500/553. train loss: 35.7780,	0.8354 s / batch. (data: 3.36e-04). ETA=8:13:29, max mem: 20.9 GB 
[11/27 07:35:43 visual_prompt]: Epoch 36 / 100: avg data time: 1.38e-01, avg batch time: 0.9724, average train loss: 13.2126
[11/27 07:36:38 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3100, average loss: 44.4674
[11/27 07:36:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.32	
[11/27 07:36:38 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[11/27 07:38:19 visual_prompt]: 	Training 100/553. train loss: 19.8949,	0.8649 s / batch. (data: 5.43e-03). ETA=8:28:44, max mem: 20.9 GB 
[11/27 07:39:56 visual_prompt]: 	Training 200/553. train loss: 6.0677,	0.8259 s / batch. (data: 7.94e-03). ETA=8:04:25, max mem: 20.9 GB 
[11/27 07:41:33 visual_prompt]: 	Training 300/553. train loss: 10.1501,	1.1720 s / batch. (data: 3.54e-01). ETA=11:25:26, max mem: 20.9 GB 
[11/27 07:43:12 visual_prompt]: 	Training 400/553. train loss: 29.3611,	1.6186 s / batch. (data: 8.06e-01). ETA=15:43:57, max mem: 20.9 GB 
[11/27 07:44:46 visual_prompt]: 	Training 500/553. train loss: 9.0529,	1.0772 s / batch. (data: 2.58e-01). ETA=10:26:26, max mem: 20.9 GB 
[11/27 07:45:38 visual_prompt]: Epoch 37 / 100: avg data time: 1.41e-01, avg batch time: 0.9758, average train loss: 13.5388
[11/27 07:46:33 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3099, average loss: 9.7950
[11/27 07:46:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.77	
[11/27 07:46:33 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[11/27 07:48:11 visual_prompt]: 	Training 100/553. train loss: 7.4689,	0.8277 s / batch. (data: 5.43e-03). ETA=7:59:11, max mem: 20.9 GB 
[11/27 07:49:50 visual_prompt]: 	Training 200/553. train loss: 15.4342,	1.2200 s / batch. (data: 3.64e-01). ETA=11:44:19, max mem: 20.9 GB 
[11/27 07:51:28 visual_prompt]: 	Training 300/553. train loss: 4.2719,	0.8521 s / batch. (data: 2.58e-04). ETA=8:10:29, max mem: 20.9 GB 
[11/27 07:53:03 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8199 s / batch. (data: 3.02e-04). ETA=7:50:35, max mem: 20.9 GB 
[11/27 07:54:43 visual_prompt]: 	Training 500/553. train loss: 31.6885,	0.8476 s / batch. (data: 1.05e-02). ETA=8:05:06, max mem: 20.9 GB 
[11/27 07:55:32 visual_prompt]: Epoch 38 / 100: avg data time: 1.39e-01, avg batch time: 0.9739, average train loss: 12.6907
[11/27 07:56:27 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3077, average loss: 7.2202
[11/27 07:56:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.49	
[11/27 07:56:27 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[11/27 07:58:07 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8560 s / batch. (data: 5.45e-03). ETA=8:07:43, max mem: 20.9 GB 
[11/27 07:59:48 visual_prompt]: 	Training 200/553. train loss: 47.7523,	0.8313 s / batch. (data: 2.91e-04). ETA=7:52:14, max mem: 20.9 GB 
[11/27 08:01:27 visual_prompt]: 	Training 300/553. train loss: 17.8667,	0.8280 s / batch. (data: 2.91e-04). ETA=7:49:00, max mem: 20.9 GB 
[11/27 08:03:01 visual_prompt]: 	Training 400/553. train loss: 23.3660,	0.8294 s / batch. (data: 3.02e-04). ETA=7:48:25, max mem: 20.9 GB 
[11/27 08:04:39 visual_prompt]: 	Training 500/553. train loss: 0.7639,	1.6158 s / batch. (data: 7.82e-01). ETA=15:09:51, max mem: 20.9 GB 
[11/27 08:05:27 visual_prompt]: Epoch 39 / 100: avg data time: 1.42e-01, avg batch time: 0.9761, average train loss: 11.4451
[11/27 08:06:22 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3111, average loss: 2.7492
[11/27 08:06:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.50	
[11/27 08:06:22 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[11/27 08:08:04 visual_prompt]: 	Training 100/553. train loss: 9.1090,	0.8320 s / batch. (data: 2.99e-04). ETA=7:46:23, max mem: 20.9 GB 
[11/27 08:09:41 visual_prompt]: 	Training 200/553. train loss: 22.3345,	0.8244 s / batch. (data: 5.41e-03). ETA=7:40:43, max mem: 20.9 GB 
[11/27 08:11:19 visual_prompt]: 	Training 300/553. train loss: 4.3439,	0.8185 s / batch. (data: 3.09e-04). ETA=7:36:05, max mem: 20.9 GB 
[11/27 08:12:57 visual_prompt]: 	Training 400/553. train loss: 2.9576,	0.8520 s / batch. (data: 8.03e-04). ETA=7:53:19, max mem: 20.9 GB 
[11/27 08:14:34 visual_prompt]: 	Training 500/553. train loss: 40.9235,	0.8197 s / batch. (data: 2.89e-04). ETA=7:34:00, max mem: 20.9 GB 
[11/27 08:15:26 visual_prompt]: Epoch 40 / 100: avg data time: 1.49e-01, avg batch time: 0.9832, average train loss: 12.8938
[11/27 08:16:22 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3111, average loss: 19.6490
[11/27 08:16:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.70	
[11/27 08:16:22 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[11/27 08:18:08 visual_prompt]: 	Training 100/553. train loss: 11.0489,	0.8408 s / batch. (data: 2.93e-04). ETA=7:43:33, max mem: 20.9 GB 
[11/27 08:19:47 visual_prompt]: 	Training 200/553. train loss: 6.3381,	0.8395 s / batch. (data: 8.83e-04). ETA=7:41:28, max mem: 20.9 GB 
[11/27 08:21:23 visual_prompt]: 	Training 300/553. train loss: 20.3824,	0.8360 s / batch. (data: 2.97e-04). ETA=7:38:07, max mem: 20.9 GB 
[11/27 08:23:00 visual_prompt]: 	Training 400/553. train loss: 3.8152,	0.8268 s / batch. (data: 7.95e-03). ETA=7:31:41, max mem: 20.9 GB 
[11/27 08:24:35 visual_prompt]: 	Training 500/553. train loss: 7.7854,	0.8600 s / batch. (data: 7.98e-04). ETA=7:48:24, max mem: 20.9 GB 
[11/27 08:25:23 visual_prompt]: Epoch 41 / 100: avg data time: 1.44e-01, avg batch time: 0.9779, average train loss: 10.7878
[11/27 08:26:19 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3119, average loss: 9.9622
[11/27 08:26:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.04	
[11/27 08:26:19 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[11/27 08:27:58 visual_prompt]: 	Training 100/553. train loss: 14.1490,	0.8520 s / batch. (data: 1.20e-02). ETA=7:41:53, max mem: 20.9 GB 
[11/27 08:29:36 visual_prompt]: 	Training 200/553. train loss: 4.1314,	0.8702 s / batch. (data: 3.79e-02). ETA=7:50:16, max mem: 20.9 GB 
[11/27 08:31:14 visual_prompt]: 	Training 300/553. train loss: 84.9100,	0.8320 s / batch. (data: 2.65e-04). ETA=7:28:16, max mem: 20.9 GB 
[11/27 08:32:52 visual_prompt]: 	Training 400/553. train loss: 26.1052,	0.8503 s / batch. (data: 1.05e-02). ETA=7:36:43, max mem: 20.9 GB 
[11/27 08:34:28 visual_prompt]: 	Training 500/553. train loss: 40.4194,	0.8364 s / batch. (data: 1.20e-02). ETA=7:27:50, max mem: 20.9 GB 
[11/27 08:35:20 visual_prompt]: Epoch 42 / 100: avg data time: 1.45e-01, avg batch time: 0.9790, average train loss: 12.7187
[11/27 08:36:16 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3115, average loss: 12.5974
[11/27 08:36:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.95	
[11/27 08:36:16 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[11/27 08:37:59 visual_prompt]: 	Training 100/553. train loss: 6.3088,	0.8348 s / batch. (data: 1.09e-02). ETA=7:24:50, max mem: 20.9 GB 
[11/27 08:39:36 visual_prompt]: 	Training 200/553. train loss: 9.3037,	0.8309 s / batch. (data: 3.02e-04). ETA=7:21:24, max mem: 20.9 GB 
[11/27 08:41:11 visual_prompt]: 	Training 300/553. train loss: 23.7119,	0.8440 s / batch. (data: 7.96e-03). ETA=7:26:57, max mem: 20.9 GB 
[11/27 08:42:47 visual_prompt]: 	Training 400/553. train loss: 15.8980,	0.8446 s / batch. (data: 5.42e-03). ETA=7:25:52, max mem: 20.9 GB 
[11/27 08:44:25 visual_prompt]: 	Training 500/553. train loss: 24.1402,	0.8440 s / batch. (data: 7.95e-03). ETA=7:24:08, max mem: 20.9 GB 
[11/27 08:45:18 visual_prompt]: Epoch 43 / 100: avg data time: 1.48e-01, avg batch time: 0.9815, average train loss: 11.7274
[11/27 08:46:14 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3095, average loss: 0.7868
[11/27 08:46:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.77	
[11/27 08:46:14 visual_prompt]: Best epoch 43: best metric: -0.787
[11/27 08:46:14 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[11/27 08:47:55 visual_prompt]: 	Training 100/553. train loss: 9.9746,	0.8589 s / batch. (data: 3.23e-02). ETA=7:29:48, max mem: 20.9 GB 
[11/27 08:49:36 visual_prompt]: 	Training 200/553. train loss: 6.3026,	0.8288 s / batch. (data: 3.94e-04). ETA=7:12:38, max mem: 20.9 GB 
[11/27 08:51:11 visual_prompt]: 	Training 300/553. train loss: 2.8780,	0.8637 s / batch. (data: 2.37e-02). ETA=7:29:26, max mem: 20.9 GB 
[11/27 08:52:48 visual_prompt]: 	Training 400/553. train loss: 2.2921,	0.8560 s / batch. (data: 1.19e-02). ETA=7:23:59, max mem: 20.9 GB 
[11/27 08:54:25 visual_prompt]: 	Training 500/553. train loss: 9.7416,	0.8213 s / batch. (data: 3.50e-04). ETA=7:04:37, max mem: 20.9 GB 
[11/27 08:55:16 visual_prompt]: Epoch 44 / 100: avg data time: 1.46e-01, avg batch time: 0.9803, average train loss: 13.5504
[11/27 08:56:12 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3100, average loss: 16.0814
[11/27 08:56:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.62	
[11/27 08:56:12 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[11/27 08:57:56 visual_prompt]: 	Training 100/553. train loss: 13.3801,	0.8438 s / batch. (data: 1.17e-02). ETA=7:14:05, max mem: 20.9 GB 
[11/27 08:59:29 visual_prompt]: 	Training 200/553. train loss: 4.4990,	0.8208 s / batch. (data: 3.51e-04). ETA=7:00:53, max mem: 20.9 GB 
[11/27 09:01:08 visual_prompt]: 	Training 300/553. train loss: 13.6453,	0.8415 s / batch. (data: 5.44e-03). ETA=7:10:07, max mem: 20.9 GB 
[11/27 09:02:43 visual_prompt]: 	Training 400/553. train loss: 1.0923,	0.9006 s / batch. (data: 4.06e-02). ETA=7:38:50, max mem: 20.9 GB 
[11/27 09:04:23 visual_prompt]: 	Training 500/553. train loss: 8.1593,	0.8299 s / batch. (data: 1.20e-02). ETA=7:01:26, max mem: 20.9 GB 
[11/27 09:05:14 visual_prompt]: Epoch 45 / 100: avg data time: 1.45e-01, avg batch time: 0.9789, average train loss: 9.7635
[11/27 09:06:10 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3111, average loss: 12.0047
[11/27 09:06:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.85	
[11/27 09:06:10 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[11/27 09:07:52 visual_prompt]: 	Training 100/553. train loss: 11.9958,	1.1720 s / batch. (data: 3.31e-01). ETA=9:52:09, max mem: 20.9 GB 
[11/27 09:09:30 visual_prompt]: 	Training 200/553. train loss: 5.7200,	0.8681 s / batch. (data: 8.16e-04). ETA=7:17:09, max mem: 20.9 GB 
[11/27 09:11:06 visual_prompt]: 	Training 300/553. train loss: 2.5024,	0.8280 s / batch. (data: 3.02e-04). ETA=6:55:35, max mem: 20.9 GB 
[11/27 09:12:45 visual_prompt]: 	Training 400/553. train loss: 34.9781,	0.8401 s / batch. (data: 3.31e-04). ETA=7:00:14, max mem: 20.9 GB 
[11/27 09:14:18 visual_prompt]: 	Training 500/553. train loss: 23.0178,	0.8560 s / batch. (data: 3.09e-04). ETA=7:06:47, max mem: 20.9 GB 
[11/27 09:15:11 visual_prompt]: Epoch 46 / 100: avg data time: 1.45e-01, avg batch time: 0.9798, average train loss: 10.1163
[11/27 09:16:07 visual_prompt]: Inference (val):avg data time: 4.14e-04, avg batch time: 0.3103, average loss: 0.8559
[11/27 09:16:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.24	
[11/27 09:16:07 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[11/27 09:17:49 visual_prompt]: 	Training 100/553. train loss: 6.0168,	0.8562 s / batch. (data: 1.64e-02). ETA=7:04:42, max mem: 20.9 GB 
[11/27 09:19:23 visual_prompt]: 	Training 200/553. train loss: 23.3625,	1.1080 s / batch. (data: 2.60e-01). ETA=9:07:44, max mem: 20.9 GB 
[11/27 09:21:00 visual_prompt]: 	Training 300/553. train loss: 8.4394,	0.8392 s / batch. (data: 3.17e-04). ETA=6:53:28, max mem: 20.9 GB 
[11/27 09:22:38 visual_prompt]: 	Training 400/553. train loss: 4.9957,	0.8590 s / batch. (data: 1.06e-02). ETA=7:01:47, max mem: 20.9 GB 
[11/27 09:24:14 visual_prompt]: 	Training 500/553. train loss: 65.7577,	0.8560 s / batch. (data: 7.93e-03). ETA=6:58:53, max mem: 20.9 GB 
[11/27 09:25:06 visual_prompt]: Epoch 47 / 100: avg data time: 1.41e-01, avg batch time: 0.9757, average train loss: 9.7706
[11/27 09:26:02 visual_prompt]: Inference (val):avg data time: 8.55e-05, avg batch time: 0.3109, average loss: 12.2605
[11/27 09:26:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.52	
[11/27 09:26:02 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[11/27 09:27:45 visual_prompt]: 	Training 100/553. train loss: 11.4915,	0.8436 s / batch. (data: 1.60e-02). ETA=6:50:40, max mem: 20.9 GB 
[11/27 09:29:23 visual_prompt]: 	Training 200/553. train loss: 2.3317,	0.8420 s / batch. (data: 1.06e-02). ETA=6:48:28, max mem: 20.9 GB 
[11/27 09:31:01 visual_prompt]: 	Training 300/553. train loss: 3.6406,	1.4927 s / batch. (data: 6.76e-01). ETA=12:01:42, max mem: 20.9 GB 
[11/27 09:32:36 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8431 s / batch. (data: 3.69e-04). ETA=6:46:13, max mem: 20.9 GB 
[11/27 09:34:13 visual_prompt]: 	Training 500/553. train loss: 1.0134,	0.8430 s / batch. (data: 5.40e-03). ETA=6:44:44, max mem: 20.9 GB 
[11/27 09:35:04 visual_prompt]: Epoch 48 / 100: avg data time: 1.47e-01, avg batch time: 0.9805, average train loss: 10.0806
[11/27 09:36:00 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3115, average loss: 8.3874
[11/27 09:36:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 54.46	
[11/27 09:36:00 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[11/27 09:37:40 visual_prompt]: 	Training 100/553. train loss: 4.2137,	0.8288 s / batch. (data: 3.34e-04). ETA=6:35:49, max mem: 20.9 GB 
[11/27 09:39:16 visual_prompt]: 	Training 200/553. train loss: 6.3268,	0.8560 s / batch. (data: 5.42e-03). ETA=6:47:23, max mem: 20.9 GB 
[11/27 09:40:53 visual_prompt]: 	Training 300/553. train loss: 5.4387,	0.8306 s / batch. (data: 5.44e-03). ETA=6:33:56, max mem: 20.9 GB 
[11/27 09:42:32 visual_prompt]: 	Training 400/553. train loss: 9.2339,	0.8320 s / batch. (data: 3.01e-04). ETA=6:33:12, max mem: 20.9 GB 
[11/27 09:44:09 visual_prompt]: 	Training 500/553. train loss: 20.2864,	0.8450 s / batch. (data: 5.91e-03). ETA=6:37:55, max mem: 20.9 GB 
[11/27 09:45:01 visual_prompt]: Epoch 49 / 100: avg data time: 1.44e-01, avg batch time: 0.9789, average train loss: 10.2483
[11/27 09:45:57 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3113, average loss: 31.1753
[11/27 09:45:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.42	
[11/27 09:45:57 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[11/27 09:47:39 visual_prompt]: 	Training 100/553. train loss: 13.0194,	0.8312 s / batch. (data: 7.78e-04). ETA=6:29:19, max mem: 20.9 GB 
[11/27 09:49:17 visual_prompt]: 	Training 200/553. train loss: 6.3970,	0.8466 s / batch. (data: 2.73e-04). ETA=6:35:07, max mem: 20.9 GB 
[11/27 09:50:58 visual_prompt]: 	Training 300/553. train loss: 4.3012,	0.8421 s / batch. (data: 8.05e-04). ETA=6:31:36, max mem: 20.9 GB 
[11/27 09:52:33 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8320 s / batch. (data: 2.88e-04). ETA=6:25:32, max mem: 20.9 GB 
[11/27 09:54:12 visual_prompt]: 	Training 500/553. train loss: 17.3486,	0.8240 s / batch. (data: 4.54e-04). ETA=6:20:26, max mem: 20.9 GB 
[11/27 09:55:01 visual_prompt]: Epoch 50 / 100: avg data time: 1.51e-01, avg batch time: 0.9845, average train loss: 9.8647
[11/27 09:55:57 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.3101, average loss: 5.1785
[11/27 09:55:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.00	
[11/27 09:55:57 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[11/27 09:57:38 visual_prompt]: 	Training 100/553. train loss: 0.5580,	1.0197 s / batch. (data: 1.91e-01). ETA=7:48:14, max mem: 20.9 GB 
[11/27 09:59:16 visual_prompt]: 	Training 200/553. train loss: 15.9955,	0.8400 s / batch. (data: 2.66e-04). ETA=6:24:18, max mem: 20.9 GB 
[11/27 10:00:54 visual_prompt]: 	Training 300/553. train loss: 0.6149,	0.8311 s / batch. (data: 1.05e-02). ETA=6:18:49, max mem: 20.9 GB 
[11/27 10:02:32 visual_prompt]: 	Training 400/553. train loss: 0.6389,	1.3863 s / batch. (data: 5.28e-01). ETA=10:29:37, max mem: 20.9 GB 
[11/27 10:04:08 visual_prompt]: 	Training 500/553. train loss: 6.4993,	0.8526 s / batch. (data: 2.05e-02). ETA=6:25:47, max mem: 20.9 GB 
[11/27 10:04:58 visual_prompt]: Epoch 51 / 100: avg data time: 1.44e-01, avg batch time: 0.9782, average train loss: 8.9297
[11/27 10:05:54 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3111, average loss: 6.8355
[11/27 10:05:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.55	
[11/27 10:05:54 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[11/27 10:07:39 visual_prompt]: 	Training 100/553. train loss: 4.5848,	0.8204 s / batch. (data: 3.20e-04). ETA=6:09:07, max mem: 20.9 GB 
[11/27 10:09:19 visual_prompt]: 	Training 200/553. train loss: 7.6705,	0.8186 s / batch. (data: 3.03e-04). ETA=6:06:58, max mem: 20.9 GB 
[11/27 10:10:58 visual_prompt]: 	Training 300/553. train loss: 14.7017,	0.8320 s / batch. (data: 2.99e-04). ETA=6:11:34, max mem: 20.9 GB 
[11/27 10:12:36 visual_prompt]: 	Training 400/553. train loss: 0.0287,	0.8200 s / batch. (data: 2.98e-04). ETA=6:04:50, max mem: 20.9 GB 
[11/27 10:14:09 visual_prompt]: 	Training 500/553. train loss: 35.5324,	0.8407 s / batch. (data: 2.83e-04). ETA=6:12:38, max mem: 20.9 GB 
[11/27 10:15:00 visual_prompt]: Epoch 52 / 100: avg data time: 1.55e-01, avg batch time: 0.9887, average train loss: 8.8161
[11/27 10:15:59 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3100, average loss: 24.7753
[11/27 10:15:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.00	
[11/27 10:15:59 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[11/27 10:17:41 visual_prompt]: 	Training 100/553. train loss: 8.9939,	0.8243 s / batch. (data: 2.91e-04). ETA=6:03:18, max mem: 20.9 GB 
[11/27 10:19:19 visual_prompt]: 	Training 200/553. train loss: 2.5567,	0.8480 s / batch. (data: 3.12e-04). ETA=6:12:19, max mem: 20.9 GB 
[11/27 10:20:58 visual_prompt]: 	Training 300/553. train loss: 12.7372,	0.8526 s / batch. (data: 1.56e-02). ETA=6:12:55, max mem: 20.9 GB 
[11/27 10:22:38 visual_prompt]: 	Training 400/553. train loss: 12.6790,	0.8436 s / batch. (data: 1.20e-02). ETA=6:07:34, max mem: 20.9 GB 
[11/27 10:24:14 visual_prompt]: 	Training 500/553. train loss: 0.4942,	0.8280 s / batch. (data: 5.41e-03). ETA=5:59:24, max mem: 20.9 GB 
[11/27 10:25:06 visual_prompt]: Epoch 53 / 100: avg data time: 1.55e-01, avg batch time: 0.9889, average train loss: 8.1290
[11/27 10:26:01 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3113, average loss: 2.3776
[11/27 10:26:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 49.26	
[11/27 10:26:01 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[11/27 10:27:45 visual_prompt]: 	Training 100/553. train loss: 29.1194,	0.8600 s / batch. (data: 1.19e-02). ETA=6:11:05, max mem: 20.9 GB 
[11/27 10:29:23 visual_prompt]: 	Training 200/553. train loss: 3.0860,	0.8217 s / batch. (data: 3.71e-04). ETA=5:53:12, max mem: 20.9 GB 
[11/27 10:30:59 visual_prompt]: 	Training 300/553. train loss: 10.0813,	0.8474 s / batch. (data: 7.97e-03). ETA=6:02:51, max mem: 20.9 GB 
[11/27 10:32:36 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8240 s / batch. (data: 1.21e-02). ETA=5:51:27, max mem: 20.9 GB 
[11/27 10:34:13 visual_prompt]: 	Training 500/553. train loss: 23.6134,	0.8195 s / batch. (data: 3.08e-04). ETA=5:48:09, max mem: 20.9 GB 
[11/27 10:35:05 visual_prompt]: Epoch 54 / 100: avg data time: 1.48e-01, avg batch time: 0.9827, average train loss: 8.0860
[11/27 10:36:01 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3103, average loss: 8.8044
[11/27 10:36:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.25	
[11/27 10:36:01 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[11/27 10:37:42 visual_prompt]: 	Training 100/553. train loss: 3.3041,	0.8473 s / batch. (data: 3.20e-04). ETA=5:57:47, max mem: 20.9 GB 
[11/27 10:39:17 visual_prompt]: 	Training 200/553. train loss: 1.0561,	0.8256 s / batch. (data: 3.22e-04). ETA=5:47:17, max mem: 20.9 GB 
[11/27 10:40:54 visual_prompt]: 	Training 300/553. train loss: 9.7359,	0.8440 s / batch. (data: 2.95e-04). ETA=5:53:35, max mem: 20.9 GB 
[11/27 10:42:31 visual_prompt]: 	Training 400/553. train loss: 25.0819,	0.8280 s / batch. (data: 3.15e-04). ETA=5:45:31, max mem: 20.9 GB 
[11/27 10:44:08 visual_prompt]: 	Training 500/553. train loss: 23.9369,	0.8320 s / batch. (data: 1.20e-02). ETA=5:45:48, max mem: 20.9 GB 
[11/27 10:45:00 visual_prompt]: Epoch 55 / 100: avg data time: 1.40e-01, avg batch time: 0.9749, average train loss: 9.2435
[11/27 10:45:56 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3081, average loss: 25.2491
[11/27 10:45:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.33	
[11/27 10:45:56 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[11/27 10:47:38 visual_prompt]: 	Training 100/553. train loss: 2.7082,	0.8210 s / batch. (data: 3.11e-04). ETA=5:39:09, max mem: 20.9 GB 
[11/27 10:49:15 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8373 s / batch. (data: 1.20e-02). ETA=5:44:28, max mem: 20.9 GB 
[11/27 10:50:53 visual_prompt]: 	Training 300/553. train loss: 3.9432,	0.8440 s / batch. (data: 2.96e-04). ETA=5:45:49, max mem: 20.9 GB 
[11/27 10:52:31 visual_prompt]: 	Training 400/553. train loss: 0.8440,	0.8360 s / batch. (data: 7.38e-04). ETA=5:41:10, max mem: 20.9 GB 
[11/27 10:54:08 visual_prompt]: 	Training 500/553. train loss: 2.1989,	1.7932 s / batch. (data: 9.70e-01). ETA=12:08:47, max mem: 20.9 GB 
[11/27 10:54:57 visual_prompt]: Epoch 56 / 100: avg data time: 1.44e-01, avg batch time: 0.9786, average train loss: 7.8256
[11/27 10:55:52 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3097, average loss: 4.9500
[11/27 10:55:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.65	
[11/27 10:55:52 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[11/27 10:57:36 visual_prompt]: 	Training 100/553. train loss: 4.5333,	0.8172 s / batch. (data: 3.23e-04). ETA=5:30:02, max mem: 20.9 GB 
[11/27 10:59:12 visual_prompt]: 	Training 200/553. train loss: 2.5458,	0.8351 s / batch. (data: 3.31e-04). ETA=5:35:52, max mem: 20.9 GB 
[11/27 11:00:48 visual_prompt]: 	Training 300/553. train loss: 11.6346,	0.8329 s / batch. (data: 5.44e-03). ETA=5:33:36, max mem: 20.9 GB 
[11/27 11:02:24 visual_prompt]: 	Training 400/553. train loss: 10.6415,	0.8459 s / batch. (data: 1.19e-02). ETA=5:37:23, max mem: 20.9 GB 
[11/27 11:03:58 visual_prompt]: 	Training 500/553. train loss: 17.6957,	0.8320 s / batch. (data: 5.43e-03). ETA=5:30:27, max mem: 20.9 GB 
[11/27 11:04:50 visual_prompt]: Epoch 57 / 100: avg data time: 1.38e-01, avg batch time: 0.9727, average train loss: 7.8701
[11/27 11:05:46 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3094, average loss: 10.9908
[11/27 11:05:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.54	
[11/27 11:05:46 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[11/27 11:07:28 visual_prompt]: 	Training 100/553. train loss: 2.7099,	1.1720 s / batch. (data: 3.45e-01). ETA=7:42:31, max mem: 20.9 GB 
[11/27 11:09:06 visual_prompt]: 	Training 200/553. train loss: 7.6866,	0.8240 s / batch. (data: 2.98e-04). ETA=5:23:49, max mem: 20.9 GB 
[11/27 11:10:45 visual_prompt]: 	Training 300/553. train loss: 8.2824,	0.8185 s / batch. (data: 2.86e-04). ETA=5:20:18, max mem: 20.9 GB 
[11/27 11:12:22 visual_prompt]: 	Training 400/553. train loss: 8.0013,	0.8237 s / batch. (data: 3.07e-04). ETA=5:20:56, max mem: 20.9 GB 
[11/27 11:13:58 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8285 s / batch. (data: 2.98e-04). ETA=5:21:25, max mem: 20.9 GB 
[11/27 11:14:48 visual_prompt]: Epoch 58 / 100: avg data time: 1.45e-01, avg batch time: 0.9790, average train loss: 8.1003
[11/27 11:15:43 visual_prompt]: Inference (val):avg data time: 1.40e-04, avg batch time: 0.3106, average loss: 9.1299
[11/27 11:15:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.18	
[11/27 11:15:43 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[11/27 11:17:25 visual_prompt]: 	Training 100/553. train loss: 2.8699,	0.8581 s / batch. (data: 1.41e-02). ETA=5:30:43, max mem: 20.9 GB 
[11/27 11:19:03 visual_prompt]: 	Training 200/553. train loss: 2.2821,	0.8320 s / batch. (data: 3.21e-04). ETA=5:19:16, max mem: 20.9 GB 
[11/27 11:20:38 visual_prompt]: 	Training 300/553. train loss: 7.4595,	0.8800 s / batch. (data: 7.85e-04). ETA=5:36:14, max mem: 20.9 GB 
[11/27 11:22:14 visual_prompt]: 	Training 400/553. train loss: 1.5838,	0.8510 s / batch. (data: 3.53e-04). ETA=5:23:43, max mem: 20.9 GB 
[11/27 11:23:52 visual_prompt]: 	Training 500/553. train loss: 8.1765,	0.8480 s / batch. (data: 2.91e-04). ETA=5:21:11, max mem: 20.9 GB 
[11/27 11:24:42 visual_prompt]: Epoch 59 / 100: avg data time: 1.40e-01, avg batch time: 0.9746, average train loss: 6.8542
[11/27 11:25:37 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3098, average loss: 3.3368
[11/27 11:25:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.76	
[11/27 11:25:37 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[11/27 11:27:19 visual_prompt]: 	Training 100/553. train loss: 5.9352,	0.8474 s / batch. (data: 1.05e-02). ETA=5:18:49, max mem: 20.9 GB 
[11/27 11:28:56 visual_prompt]: 	Training 200/553. train loss: 19.6947,	0.8240 s / batch. (data: 3.02e-04). ETA=5:08:38, max mem: 20.9 GB 
[11/27 11:30:31 visual_prompt]: 	Training 300/553. train loss: 29.0580,	2.1401 s / batch. (data: 1.32e+00). ETA=13:18:00, max mem: 20.9 GB 
[11/27 11:32:09 visual_prompt]: 	Training 400/553. train loss: 1.0867,	0.8510 s / batch. (data: 3.27e-02). ETA=5:15:53, max mem: 20.9 GB 
[11/27 11:33:46 visual_prompt]: 	Training 500/553. train loss: 4.7463,	0.8499 s / batch. (data: 1.12e-02). ETA=5:14:03, max mem: 20.9 GB 
[11/27 11:34:38 visual_prompt]: Epoch 60 / 100: avg data time: 1.43e-01, avg batch time: 0.9774, average train loss: 7.1085
[11/27 11:35:33 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.3106, average loss: 18.0813
[11/27 11:35:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.35	
[11/27 11:35:33 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[11/27 11:37:15 visual_prompt]: 	Training 100/553. train loss: 6.3321,	0.8383 s / batch. (data: 3.27e-04). ETA=5:07:39, max mem: 20.9 GB 
[11/27 11:38:54 visual_prompt]: 	Training 200/553. train loss: 4.7226,	1.3240 s / batch. (data: 4.94e-01). ETA=8:03:41, max mem: 20.9 GB 
[11/27 11:40:32 visual_prompt]: 	Training 300/553. train loss: 2.4420,	1.2760 s / batch. (data: 4.56e-01). ETA=7:44:02, max mem: 20.9 GB 
[11/27 11:42:07 visual_prompt]: 	Training 400/553. train loss: 3.1152,	0.8319 s / batch. (data: 3.86e-04). ETA=5:01:09, max mem: 20.9 GB 
[11/27 11:43:45 visual_prompt]: 	Training 500/553. train loss: 12.1293,	2.5033 s / batch. (data: 1.67e+00). ETA=15:02:00, max mem: 20.9 GB 
[11/27 11:44:34 visual_prompt]: Epoch 61 / 100: avg data time: 1.43e-01, avg batch time: 0.9783, average train loss: 6.6655
[11/27 11:45:29 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3098, average loss: 4.2397
[11/27 11:45:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.59	
[11/27 11:45:29 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[11/27 11:47:10 visual_prompt]: 	Training 100/553. train loss: 2.5879,	0.8347 s / batch. (data: 5.44e-03). ETA=4:58:39, max mem: 20.9 GB 
[11/27 11:48:47 visual_prompt]: 	Training 200/553. train loss: 1.6795,	0.8486 s / batch. (data: 1.05e-02). ETA=5:02:11, max mem: 20.9 GB 
[11/27 11:50:24 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8279 s / batch. (data: 3.35e-04). ETA=4:53:27, max mem: 20.9 GB 
[11/27 11:52:01 visual_prompt]: 	Training 400/553. train loss: 1.7692,	0.8549 s / batch. (data: 1.08e-02). ETA=5:01:35, max mem: 20.9 GB 
[11/27 11:53:36 visual_prompt]: 	Training 500/553. train loss: 6.1887,	0.8545 s / batch. (data: 1.05e-02). ETA=5:00:00, max mem: 20.9 GB 
[11/27 11:54:29 visual_prompt]: Epoch 62 / 100: avg data time: 1.41e-01, avg batch time: 0.9751, average train loss: 5.6053
[11/27 11:55:24 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3102, average loss: 2.3602
[11/27 11:55:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.07	
[11/27 11:55:24 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[11/27 11:57:09 visual_prompt]: 	Training 100/553. train loss: 5.6347,	0.8600 s / batch. (data: 2.11e-02). ETA=4:59:47, max mem: 20.9 GB 
[11/27 11:58:49 visual_prompt]: 	Training 200/553. train loss: 2.5757,	0.8240 s / batch. (data: 3.22e-04). ETA=4:45:50, max mem: 20.9 GB 
[11/27 12:00:25 visual_prompt]: 	Training 300/553. train loss: 2.1667,	0.8447 s / batch. (data: 2.82e-04). ETA=4:51:37, max mem: 20.9 GB 
[11/27 12:01:57 visual_prompt]: 	Training 400/553. train loss: 9.1465,	0.8453 s / batch. (data: 3.29e-04). ETA=4:50:25, max mem: 20.9 GB 
[11/27 12:03:32 visual_prompt]: 	Training 500/553. train loss: 6.9035,	0.8460 s / batch. (data: 1.39e-02). ETA=4:49:14, max mem: 20.9 GB 
[11/27 12:04:22 visual_prompt]: Epoch 63 / 100: avg data time: 1.39e-01, avg batch time: 0.9725, average train loss: 6.1301
[11/27 12:05:18 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3101, average loss: 3.9931
[11/27 12:05:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.69	
[11/27 12:05:18 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[11/27 12:07:01 visual_prompt]: 	Training 100/553. train loss: 0.0087,	0.8600 s / batch. (data: 7.97e-04). ETA=4:51:50, max mem: 20.9 GB 
[11/27 12:08:39 visual_prompt]: 	Training 200/553. train loss: 2.7041,	0.8447 s / batch. (data: 6.80e-04). ETA=4:45:15, max mem: 20.9 GB 
[11/27 12:10:12 visual_prompt]: 	Training 300/553. train loss: 2.3742,	0.8343 s / batch. (data: 3.44e-04). ETA=4:40:20, max mem: 20.9 GB 
[11/27 12:11:50 visual_prompt]: 	Training 400/553. train loss: 4.6672,	0.8200 s / batch. (data: 3.29e-04). ETA=4:34:09, max mem: 20.9 GB 
[11/27 12:13:26 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8509 s / batch. (data: 5.42e-03). ETA=4:43:04, max mem: 20.9 GB 
[11/27 12:14:16 visual_prompt]: Epoch 64 / 100: avg data time: 1.39e-01, avg batch time: 0.9730, average train loss: 5.4880
[11/27 12:15:12 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3104, average loss: 2.1380
[11/27 12:15:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.45	
[11/27 12:15:12 visual_prompt]: Stopping early.
[11/27 12:15:12 visual_prompt]: Rank of current process: 0. World size: 1
[11/27 12:15:12 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/27 12:15:12 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/27 12:15:12 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/27 12:15:12 visual_prompt]: Training with config:
[11/27 12:15:12 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr5.0_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/27 12:15:12 visual_prompt]: Loading training data...
[11/27 12:15:12 visual_prompt]: Constructing mammo-cbis dataset train...
[11/27 12:15:12 visual_prompt]: Loading validation data...
[11/27 12:15:12 visual_prompt]: Constructing mammo-cbis dataset val...
[11/27 12:15:12 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/27 12:15:14 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/27 12:15:14 visual_prompt]: tuned percent:0.525
[11/27 12:15:15 visual_prompt]: Device used for model: 0
[11/27 12:15:15 visual_prompt]: Setting up Evaluator...
[11/27 12:15:15 visual_prompt]: Setting up Trainer...
[11/27 12:15:15 visual_prompt]: 	Setting up the optimizer...
[11/27 12:15:15 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/27 12:16:55 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8204 s / batch. (data: 4.16e-04). ETA=12:34:45, max mem: 20.9 GB 
[11/27 12:18:30 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8394 s / batch. (data: 3.98e-04). ETA=12:50:53, max mem: 20.9 GB 
[11/27 12:20:09 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8334 s / batch. (data: 3.08e-04). ETA=12:43:56, max mem: 20.9 GB 
[11/27 12:21:45 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8337 s / batch. (data: 5.42e-03). ETA=12:42:47, max mem: 20.9 GB 
[11/27 12:23:24 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8536 s / batch. (data: 5.59e-03). ETA=12:59:39, max mem: 20.9 GB 
[11/27 12:24:16 visual_prompt]: Epoch 1 / 100: avg data time: 1.44e-01, avg batch time: 0.9790, average train loss: 1.5403
[11/27 12:25:12 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3114, average loss: 1.5201
[11/27 12:25:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/27 12:25:12 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/27 12:26:52 visual_prompt]: 	Training 100/553. train loss: 0.9062,	1.4704 s / batch. (data: 6.40e-01). ETA=22:19:12, max mem: 20.9 GB 
[11/27 12:28:28 visual_prompt]: 	Training 200/553. train loss: 0.0004,	0.8193 s / batch. (data: 3.35e-04). ETA=12:24:48, max mem: 20.9 GB 
[11/27 12:30:07 visual_prompt]: 	Training 300/553. train loss: 2.3715,	0.8915 s / batch. (data: 7.23e-02). ETA=13:29:01, max mem: 20.9 GB 
[11/27 12:31:43 visual_prompt]: 	Training 400/553. train loss: 0.8650,	0.8296 s / batch. (data: 6.35e-03). ETA=12:31:27, max mem: 20.9 GB 
[11/27 12:33:22 visual_prompt]: 	Training 500/553. train loss: 0.7718,	0.8360 s / batch. (data: 3.33e-04). ETA=12:35:51, max mem: 20.9 GB 
[11/27 12:34:11 visual_prompt]: Epoch 2 / 100: avg data time: 1.40e-01, avg batch time: 0.9756, average train loss: 1.8079
[11/27 12:35:06 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3093, average loss: 2.7599
[11/27 12:35:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.04	
[11/27 12:35:06 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/27 12:36:46 visual_prompt]: 	Training 100/553. train loss: 0.8204,	0.8252 s / batch. (data: 2.50e-04). ETA=12:23:57, max mem: 20.9 GB 
[11/27 12:38:24 visual_prompt]: 	Training 200/553. train loss: 0.8124,	0.8458 s / batch. (data: 3.18e-04). ETA=12:41:09, max mem: 20.9 GB 
[11/27 12:39:59 visual_prompt]: 	Training 300/553. train loss: 1.1878,	0.8382 s / batch. (data: 7.95e-03). ETA=12:32:54, max mem: 20.9 GB 
[11/27 12:41:36 visual_prompt]: 	Training 400/553. train loss: 3.5394,	0.8414 s / batch. (data: 3.02e-04). ETA=12:34:21, max mem: 20.9 GB 
[11/27 12:43:14 visual_prompt]: 	Training 500/553. train loss: 0.9536,	1.0029 s / batch. (data: 1.85e-01). ETA=14:57:32, max mem: 20.9 GB 
[11/27 12:44:04 visual_prompt]: Epoch 3 / 100: avg data time: 1.37e-01, avg batch time: 0.9712, average train loss: 2.4124
[11/27 12:44:59 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3113, average loss: 3.8518
[11/27 12:44:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.31	
[11/27 12:44:59 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/27 12:46:41 visual_prompt]: 	Training 100/553. train loss: 5.8539,	0.8360 s / batch. (data: 3.39e-04). ETA=12:26:00, max mem: 20.9 GB 
[11/27 12:48:18 visual_prompt]: 	Training 200/553. train loss: 4.1352,	0.8520 s / batch. (data: 2.32e-02). ETA=12:38:52, max mem: 20.9 GB 
[11/27 12:49:56 visual_prompt]: 	Training 300/553. train loss: 1.8282,	1.1840 s / batch. (data: 3.48e-01). ETA=17:32:34, max mem: 20.9 GB 
[11/27 12:51:28 visual_prompt]: 	Training 400/553. train loss: 22.1794,	0.9710 s / batch. (data: 1.31e-01). ETA=14:21:36, max mem: 20.9 GB 
[11/27 12:53:07 visual_prompt]: 	Training 500/553. train loss: 0.6721,	3.3240 s / batch. (data: 2.48e+00). ETA=2 days, 1:04:01, max mem: 20.9 GB 
[11/27 12:53:59 visual_prompt]: Epoch 4 / 100: avg data time: 1.42e-01, avg batch time: 0.9763, average train loss: 3.9943
[11/27 12:54:54 visual_prompt]: Inference (val):avg data time: 2.90e-04, avg batch time: 0.3106, average loss: 1.0283
[11/27 12:54:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.76	
[11/27 12:54:54 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/27 12:56:35 visual_prompt]: 	Training 100/553. train loss: 15.8534,	0.8360 s / batch. (data: 3.16e-04). ETA=12:18:17, max mem: 20.9 GB 
[11/27 12:58:12 visual_prompt]: 	Training 200/553. train loss: 0.5989,	1.1628 s / batch. (data: 3.33e-01). ETA=17:04:57, max mem: 20.9 GB 
[11/27 12:59:51 visual_prompt]: 	Training 300/553. train loss: 20.0197,	0.8440 s / batch. (data: 2.73e-04). ETA=12:22:33, max mem: 20.9 GB 
[11/27 13:01:27 visual_prompt]: 	Training 400/553. train loss: 3.1582,	0.8310 s / batch. (data: 3.05e-04). ETA=12:09:43, max mem: 20.9 GB 
[11/27 13:03:04 visual_prompt]: 	Training 500/553. train loss: 6.6953,	0.8440 s / batch. (data: 3.39e-04). ETA=12:19:43, max mem: 20.9 GB 
[11/27 13:03:55 visual_prompt]: Epoch 5 / 100: avg data time: 1.42e-01, avg batch time: 0.9776, average train loss: 5.6487
[11/27 13:04:50 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.3094, average loss: 10.8251
[11/27 13:04:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.64	
[11/27 13:04:50 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/27 13:06:33 visual_prompt]: 	Training 100/553. train loss: 0.5371,	0.8432 s / batch. (data: 7.40e-04). ETA=12:16:51, max mem: 20.9 GB 
[11/27 13:08:09 visual_prompt]: 	Training 200/553. train loss: 22.6648,	0.8200 s / batch. (data: 3.19e-04). ETA=11:55:15, max mem: 20.9 GB 
[11/27 13:09:44 visual_prompt]: 	Training 300/553. train loss: 5.7618,	0.8258 s / batch. (data: 3.01e-04). ETA=11:58:54, max mem: 20.9 GB 
[11/27 13:11:25 visual_prompt]: 	Training 400/553. train loss: 2.8443,	0.8375 s / batch. (data: 3.70e-04). ETA=12:07:45, max mem: 20.9 GB 
[11/27 13:13:00 visual_prompt]: 	Training 500/553. train loss: 16.9213,	0.8639 s / batch. (data: 5.32e-04). ETA=12:29:14, max mem: 20.9 GB 
[11/27 13:13:50 visual_prompt]: Epoch 6 / 100: avg data time: 1.42e-01, avg batch time: 0.9762, average train loss: 7.5020
[11/27 13:14:46 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3102, average loss: 3.5944
[11/27 13:14:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.87	
[11/27 13:14:46 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/27 13:16:27 visual_prompt]: 	Training 100/553. train loss: 8.8278,	0.8573 s / batch. (data: 1.16e-03). ETA=12:21:20, max mem: 20.9 GB 
[11/27 13:18:03 visual_prompt]: 	Training 200/553. train loss: 3.8332,	0.8176 s / batch. (data: 3.09e-04). ETA=11:45:38, max mem: 20.9 GB 
[11/27 13:19:44 visual_prompt]: 	Training 300/553. train loss: 4.1304,	1.8483 s / batch. (data: 1.03e+00). ETA=1 day, 2:32:02, max mem: 20.9 GB 
[11/27 13:21:21 visual_prompt]: 	Training 400/553. train loss: 4.4043,	1.6327 s / batch. (data: 8.01e-01). ETA=23:23:39, max mem: 20.9 GB 
[11/27 13:22:56 visual_prompt]: 	Training 500/553. train loss: 33.8506,	0.8400 s / batch. (data: 3.21e-04). ETA=12:00:45, max mem: 20.9 GB 
[11/27 13:23:47 visual_prompt]: Epoch 7 / 100: avg data time: 1.43e-01, avg batch time: 0.9773, average train loss: 9.9244
[11/27 13:24:43 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3095, average loss: 0.8425
[11/27 13:24:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.95	
[11/27 13:24:43 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/27 13:26:22 visual_prompt]: 	Training 100/553. train loss: 7.2772,	0.8520 s / batch. (data: 7.96e-03). ETA=12:08:52, max mem: 20.9 GB 
[11/27 13:28:01 visual_prompt]: 	Training 200/553. train loss: 4.0536,	0.8546 s / batch. (data: 2.25e-02). ETA=12:09:38, max mem: 20.9 GB 
[11/27 13:29:39 visual_prompt]: 	Training 300/553. train loss: 4.9816,	0.8290 s / batch. (data: 2.88e-04). ETA=11:46:25, max mem: 20.9 GB 
[11/27 13:31:16 visual_prompt]: 	Training 400/553. train loss: 9.5666,	0.8549 s / batch. (data: 9.98e-03). ETA=12:07:05, max mem: 20.9 GB 
[11/27 13:32:53 visual_prompt]: 	Training 500/553. train loss: 32.3815,	1.4095 s / batch. (data: 5.79e-01). ETA=19:56:26, max mem: 20.9 GB 
[11/27 13:33:44 visual_prompt]: Epoch 8 / 100: avg data time: 1.44e-01, avg batch time: 0.9781, average train loss: 9.7738
[11/27 13:34:39 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3100, average loss: 8.3319
[11/27 13:34:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.51	
[11/27 13:34:39 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/27 13:36:21 visual_prompt]: 	Training 100/553. train loss: 8.8853,	0.8627 s / batch. (data: 1.55e-02). ETA=12:10:02, max mem: 20.9 GB 
[11/27 13:37:57 visual_prompt]: 	Training 200/553. train loss: 10.7656,	0.8480 s / batch. (data: 1.20e-02). ETA=11:56:12, max mem: 20.9 GB 
[11/27 13:39:35 visual_prompt]: 	Training 300/553. train loss: 3.2274,	1.4262 s / batch. (data: 6.02e-01). ETA=20:02:09, max mem: 20.9 GB 
[11/27 13:41:14 visual_prompt]: 	Training 400/553. train loss: 3.7891,	0.8570 s / batch. (data: 3.78e-02). ETA=12:00:59, max mem: 20.9 GB 
[11/27 13:42:50 visual_prompt]: 	Training 500/553. train loss: 5.6567,	0.8444 s / batch. (data: 3.01e-04). ETA=11:48:57, max mem: 20.9 GB 
[11/27 13:43:40 visual_prompt]: Epoch 9 / 100: avg data time: 1.44e-01, avg batch time: 0.9783, average train loss: 9.8596
[11/27 13:44:36 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3116, average loss: 9.2470
[11/27 13:44:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.70	
[11/27 13:44:36 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/27 13:46:20 visual_prompt]: 	Training 100/553. train loss: 79.6099,	0.8200 s / batch. (data: 3.04e-04). ETA=11:26:23, max mem: 20.9 GB 
[11/27 13:47:56 visual_prompt]: 	Training 200/553. train loss: 2.5538,	0.8350 s / batch. (data: 7.95e-03). ETA=11:37:34, max mem: 20.9 GB 
[11/27 13:49:33 visual_prompt]: 	Training 300/553. train loss: 30.6063,	1.7693 s / batch. (data: 9.50e-01). ETA=1 day, 0:35:03, max mem: 20.9 GB 
[11/27 13:51:12 visual_prompt]: 	Training 400/553. train loss: 14.8067,	2.1280 s / batch. (data: 1.31e+00). ETA=1 day, 5:30:35, max mem: 20.9 GB 
[11/27 13:52:52 visual_prompt]: 	Training 500/553. train loss: 11.6864,	0.8350 s / batch. (data: 9.67e-03). ETA=11:33:20, max mem: 20.9 GB 
[11/27 13:53:44 visual_prompt]: Epoch 10 / 100: avg data time: 1.57e-01, avg batch time: 0.9898, average train loss: 16.9430
[11/27 13:54:39 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3115, average loss: 3.1175
[11/27 13:54:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.73	
[11/27 13:54:39 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/27 13:56:22 visual_prompt]: 	Training 100/553. train loss: 8.8875,	0.8645 s / batch. (data: 3.24e-02). ETA=11:55:39, max mem: 20.9 GB 
[11/27 13:58:01 visual_prompt]: 	Training 200/553. train loss: 1.5306,	0.8360 s / batch. (data: 1.19e-02). ETA=11:30:40, max mem: 20.9 GB 
[11/27 13:59:39 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.2830 s / batch. (data: 1.46e+00). ETA=1 day, 7:22:19, max mem: 20.9 GB 
[11/27 14:01:14 visual_prompt]: 	Training 400/553. train loss: 8.2091,	0.8480 s / batch. (data: 2.50e-03). ETA=11:37:45, max mem: 20.9 GB 
[11/27 14:02:50 visual_prompt]: 	Training 500/553. train loss: 4.4132,	0.8424 s / batch. (data: 2.24e-02). ETA=11:31:43, max mem: 20.9 GB 
[11/27 14:03:40 visual_prompt]: Epoch 11 / 100: avg data time: 1.43e-01, avg batch time: 0.9783, average train loss: 17.7631
[11/27 14:04:36 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3118, average loss: 2.8473
[11/27 14:04:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.31	
[11/27 14:04:36 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/27 14:06:19 visual_prompt]: 	Training 100/553. train loss: 6.9889,	0.8395 s / batch. (data: 3.13e-04). ETA=11:27:12, max mem: 20.9 GB 
[11/27 14:07:57 visual_prompt]: 	Training 200/553. train loss: 3.7040,	0.8584 s / batch. (data: 2.24e-02). ETA=11:41:18, max mem: 20.9 GB 
[11/27 14:09:33 visual_prompt]: 	Training 300/553. train loss: 6.6281,	0.8344 s / batch. (data: 3.71e-04). ETA=11:20:15, max mem: 20.9 GB 
[11/27 14:11:11 visual_prompt]: 	Training 400/553. train loss: 1.5824,	0.8306 s / batch. (data: 1.05e-02). ETA=11:15:46, max mem: 20.9 GB 
[11/27 14:12:48 visual_prompt]: 	Training 500/553. train loss: 67.3412,	0.8090 s / batch. (data: 3.28e-04). ETA=10:56:49, max mem: 20.9 GB 
[11/27 14:13:37 visual_prompt]: Epoch 12 / 100: avg data time: 1.44e-01, avg batch time: 0.9781, average train loss: 17.0899
[11/27 14:14:33 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3114, average loss: 4.5959
[11/27 14:14:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.78	
[11/27 14:14:33 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/27 14:16:15 visual_prompt]: 	Training 100/553. train loss: 5.7661,	0.8594 s / batch. (data: 1.55e-02). ETA=11:35:37, max mem: 20.9 GB 
[11/27 14:17:49 visual_prompt]: 	Training 200/553. train loss: 3.9897,	0.8509 s / batch. (data: 3.35e-04). ETA=11:27:18, max mem: 20.9 GB 
[11/27 14:19:27 visual_prompt]: 	Training 300/553. train loss: 12.3657,	1.6753 s / batch. (data: 8.28e-01). ETA=22:30:23, max mem: 20.9 GB 
[11/27 14:21:03 visual_prompt]: 	Training 400/553. train loss: 35.4608,	0.8200 s / batch. (data: 3.35e-04). ETA=10:59:34, max mem: 20.9 GB 
[11/27 14:22:40 visual_prompt]: 	Training 500/553. train loss: 4.5832,	0.8461 s / batch. (data: 1.01e-02). ETA=11:19:12, max mem: 20.9 GB 
[11/27 14:23:30 visual_prompt]: Epoch 13 / 100: avg data time: 1.38e-01, avg batch time: 0.9718, average train loss: 15.4765
[11/27 14:24:25 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3115, average loss: 11.7818
[11/27 14:24:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.04	
[11/27 14:24:26 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/27 14:26:08 visual_prompt]: 	Training 100/553. train loss: 21.9701,	0.8400 s / batch. (data: 3.14e-04). ETA=11:12:09, max mem: 20.9 GB 
[11/27 14:27:44 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.9462 s / batch. (data: 1.17e-01). ETA=12:35:35, max mem: 20.9 GB 
[11/27 14:29:22 visual_prompt]: 	Training 300/553. train loss: 42.3929,	0.8320 s / batch. (data: 7.95e-03). ETA=11:02:57, max mem: 20.9 GB 
[11/27 14:30:59 visual_prompt]: 	Training 400/553. train loss: 13.3594,	0.8412 s / batch. (data: 3.14e-04). ETA=11:08:55, max mem: 20.9 GB 
[11/27 14:32:36 visual_prompt]: 	Training 500/553. train loss: 32.8339,	0.8619 s / batch. (data: 1.55e-02). ETA=11:23:58, max mem: 20.9 GB 
[11/27 14:33:26 visual_prompt]: Epoch 14 / 100: avg data time: 1.43e-01, avg batch time: 0.9765, average train loss: 14.9519
[11/27 14:34:21 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3103, average loss: 8.8228
[11/27 14:34:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.32	
[11/27 14:34:21 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/27 14:36:03 visual_prompt]: 	Training 100/553. train loss: 8.3991,	0.8288 s / batch. (data: 3.19e-04). ETA=10:55:35, max mem: 20.9 GB 
[11/27 14:37:38 visual_prompt]: 	Training 200/553. train loss: 28.2339,	0.8250 s / batch. (data: 3.26e-04). ETA=10:51:10, max mem: 20.9 GB 
[11/27 14:39:18 visual_prompt]: 	Training 300/553. train loss: 34.7296,	0.8382 s / batch. (data: 7.35e-04). ETA=11:00:10, max mem: 20.9 GB 
[11/27 14:40:52 visual_prompt]: 	Training 400/553. train loss: 0.6504,	0.9895 s / batch. (data: 1.70e-01). ETA=12:57:40, max mem: 20.9 GB 
[11/27 14:42:31 visual_prompt]: 	Training 500/553. train loss: 9.5385,	0.8479 s / batch. (data: 1.19e-02). ETA=11:05:02, max mem: 20.9 GB 
[11/27 14:43:22 visual_prompt]: Epoch 15 / 100: avg data time: 1.45e-01, avg batch time: 0.9775, average train loss: 18.0007
[11/27 14:44:18 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3095, average loss: 21.3152
[11/27 14:44:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.15	
[11/27 14:44:18 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/27 14:45:58 visual_prompt]: 	Training 100/553. train loss: 24.1684,	0.8298 s / batch. (data: 3.26e-04). ETA=10:48:39, max mem: 20.9 GB 
[11/27 14:47:34 visual_prompt]: 	Training 200/553. train loss: 22.2943,	0.8626 s / batch. (data: 1.55e-02). ETA=11:12:54, max mem: 20.9 GB 
[11/27 14:49:12 visual_prompt]: 	Training 300/553. train loss: 19.5646,	0.8240 s / batch. (data: 2.50e-04). ETA=10:41:26, max mem: 20.9 GB 
[11/27 14:50:48 visual_prompt]: 	Training 400/553. train loss: 10.7294,	0.8523 s / batch. (data: 1.64e-02). ETA=11:02:00, max mem: 20.9 GB 
[11/27 14:52:24 visual_prompt]: 	Training 500/553. train loss: 7.4963,	1.2466 s / batch. (data: 4.26e-01). ETA=16:06:13, max mem: 20.9 GB 
[11/27 14:53:15 visual_prompt]: Epoch 16 / 100: avg data time: 1.39e-01, avg batch time: 0.9722, average train loss: 16.8012
[11/27 14:54:11 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3110, average loss: 1.8160
[11/27 14:54:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.65	
[11/27 14:54:11 visual_prompt]: Best epoch 16: best metric: -1.816
[11/27 14:54:11 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/27 14:55:51 visual_prompt]: 	Training 100/553. train loss: 3.3493,	0.8287 s / batch. (data: 1.20e-02). ETA=10:40:09, max mem: 20.9 GB 
[11/27 14:57:29 visual_prompt]: 	Training 200/553. train loss: 6.1398,	0.8335 s / batch. (data: 2.49e-04). ETA=10:42:29, max mem: 20.9 GB 
[11/27 14:59:05 visual_prompt]: 	Training 300/553. train loss: 0.5005,	0.8519 s / batch. (data: 1.05e-02). ETA=10:55:18, max mem: 20.9 GB 
[11/27 15:00:42 visual_prompt]: 	Training 400/553. train loss: 5.0081,	1.0100 s / batch. (data: 1.86e-01). ETA=12:55:13, max mem: 20.9 GB 
[11/27 15:02:17 visual_prompt]: 	Training 500/553. train loss: 4.7998,	1.4618 s / batch. (data: 6.29e-01). ETA=18:39:32, max mem: 20.9 GB 
[11/27 15:03:09 visual_prompt]: Epoch 17 / 100: avg data time: 1.41e-01, avg batch time: 0.9737, average train loss: 15.2727
[11/27 15:04:05 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3101, average loss: 18.4460
[11/27 15:04:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.13	
[11/27 15:04:05 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/27 15:05:46 visual_prompt]: 	Training 100/553. train loss: 28.6263,	0.8400 s / batch. (data: 3.14e-04). ETA=10:41:12, max mem: 20.9 GB 
[11/27 15:07:25 visual_prompt]: 	Training 200/553. train loss: 32.5979,	0.8278 s / batch. (data: 5.41e-03). ETA=10:30:29, max mem: 20.9 GB 
[11/27 15:09:02 visual_prompt]: 	Training 300/553. train loss: 8.1456,	0.8319 s / batch. (data: 1.20e-02). ETA=10:32:14, max mem: 20.9 GB 
[11/27 15:10:38 visual_prompt]: 	Training 400/553. train loss: 0.7908,	0.8320 s / batch. (data: 5.42e-03). ETA=10:30:56, max mem: 20.9 GB 
[11/27 15:12:14 visual_prompt]: 	Training 500/553. train loss: 7.3902,	0.8326 s / batch. (data: 3.18e-04). ETA=10:30:00, max mem: 20.9 GB 
[11/27 15:13:04 visual_prompt]: Epoch 18 / 100: avg data time: 1.41e-01, avg batch time: 0.9743, average train loss: 14.4648
[11/27 15:13:59 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3095, average loss: 4.8361
[11/27 15:13:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.09	
[11/27 15:13:59 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/27 15:15:39 visual_prompt]: 	Training 100/553. train loss: 2.9938,	1.1835 s / batch. (data: 3.59e-01). ETA=14:52:26, max mem: 20.9 GB 
[11/27 15:17:16 visual_prompt]: 	Training 200/553. train loss: 9.4797,	0.8240 s / batch. (data: 2.78e-04). ETA=10:20:01, max mem: 20.9 GB 
[11/27 15:18:53 visual_prompt]: 	Training 300/553. train loss: 67.5854,	0.8360 s / batch. (data: 3.24e-04). ETA=10:27:38, max mem: 20.9 GB 
[11/27 15:20:32 visual_prompt]: 	Training 400/553. train loss: 6.8751,	0.8639 s / batch. (data: 5.91e-03). ETA=10:47:06, max mem: 20.9 GB 
[11/27 15:22:05 visual_prompt]: 	Training 500/553. train loss: 5.4041,	0.8483 s / batch. (data: 8.21e-03). ETA=10:34:02, max mem: 20.9 GB 
[11/27 15:22:55 visual_prompt]: Epoch 19 / 100: avg data time: 1.35e-01, avg batch time: 0.9696, average train loss: 14.8815
[11/27 15:23:50 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3112, average loss: 20.3297
[11/27 15:23:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.58	
[11/27 15:23:50 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/27 15:25:29 visual_prompt]: 	Training 100/553. train loss: 3.2001,	0.9560 s / batch. (data: 1.38e-01). ETA=11:52:06, max mem: 20.9 GB 
[11/27 15:27:08 visual_prompt]: 	Training 200/553. train loss: 48.3757,	0.8233 s / batch. (data: 3.32e-04). ETA=10:11:52, max mem: 20.9 GB 
[11/27 15:28:44 visual_prompt]: 	Training 300/553. train loss: 2.0789,	0.8760 s / batch. (data: 1.20e-02). ETA=10:49:36, max mem: 20.9 GB 
[11/27 15:30:21 visual_prompt]: 	Training 400/553. train loss: 1.1873,	0.8520 s / batch. (data: 5.43e-03). ETA=10:30:23, max mem: 20.9 GB 
[11/27 15:31:57 visual_prompt]: 	Training 500/553. train loss: 11.6085,	0.8629 s / batch. (data: 2.32e-02). ETA=10:37:01, max mem: 20.9 GB 
[11/27 15:32:48 visual_prompt]: Epoch 20 / 100: avg data time: 1.39e-01, avg batch time: 0.9724, average train loss: 16.6486
[11/27 15:33:43 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3107, average loss: 26.0773
[11/27 15:33:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.82	
[11/27 15:33:43 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[11/27 15:35:25 visual_prompt]: 	Training 100/553. train loss: 11.1664,	0.8304 s / batch. (data: 3.21e-04). ETA=10:10:53, max mem: 20.9 GB 
[11/27 15:36:59 visual_prompt]: 	Training 200/553. train loss: 27.7361,	0.8476 s / batch. (data: 2.35e-02). ETA=10:22:06, max mem: 20.9 GB 
[11/27 15:38:35 visual_prompt]: 	Training 300/553. train loss: 31.1452,	0.8360 s / batch. (data: 3.06e-04). ETA=10:12:13, max mem: 20.9 GB 
[11/27 15:40:12 visual_prompt]: 	Training 400/553. train loss: 5.8189,	0.8383 s / batch. (data: 8.00e-03). ETA=10:12:29, max mem: 20.9 GB 
[11/27 15:41:50 visual_prompt]: 	Training 500/553. train loss: 26.2486,	0.8440 s / batch. (data: 3.09e-04). ETA=10:15:15, max mem: 20.9 GB 
[11/27 15:42:39 visual_prompt]: Epoch 21 / 100: avg data time: 1.36e-01, avg batch time: 0.9697, average train loss: 15.1601
[11/27 15:43:35 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3098, average loss: 8.8783
[11/27 15:43:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 51.47	
[11/27 15:43:35 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[11/27 15:45:14 visual_prompt]: 	Training 100/553. train loss: 46.0157,	0.8379 s / batch. (data: 2.18e-02). ETA=10:08:39, max mem: 20.9 GB 
[11/27 15:46:51 visual_prompt]: 	Training 200/553. train loss: 3.3613,	0.8440 s / batch. (data: 2.85e-04). ETA=10:11:43, max mem: 20.9 GB 
[11/27 15:48:26 visual_prompt]: 	Training 300/553. train loss: 0.0013,	0.8278 s / batch. (data: 7.96e-03). ETA=9:58:35, max mem: 20.9 GB 
[11/27 15:50:04 visual_prompt]: 	Training 400/553. train loss: 23.6424,	0.8320 s / batch. (data: 2.95e-04). ETA=10:00:13, max mem: 20.9 GB 
[11/27 15:51:41 visual_prompt]: 	Training 500/553. train loss: 4.7286,	0.8306 s / batch. (data: 1.05e-02). ETA=9:57:52, max mem: 20.9 GB 
[11/27 15:52:33 visual_prompt]: Epoch 22 / 100: avg data time: 1.40e-01, avg batch time: 0.9737, average train loss: 13.4969
[11/27 15:53:28 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3124, average loss: 11.3088
[11/27 15:53:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.83	
[11/27 15:53:28 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[11/27 15:55:11 visual_prompt]: 	Training 100/553. train loss: 2.1520,	0.8312 s / batch. (data: 5.44e-03). ETA=9:56:11, max mem: 20.9 GB 
[11/27 15:56:48 visual_prompt]: 	Training 200/553. train loss: 30.8022,	0.8336 s / batch. (data: 3.93e-04). ETA=9:56:28, max mem: 20.9 GB 
[11/27 15:58:26 visual_prompt]: 	Training 300/553. train loss: 8.1021,	0.8685 s / batch. (data: 1.59e-02). ETA=10:20:00, max mem: 20.9 GB 
[11/27 16:00:01 visual_prompt]: 	Training 400/553. train loss: 5.6357,	0.8723 s / batch. (data: 5.90e-03). ETA=10:21:18, max mem: 20.9 GB 
[11/27 16:01:36 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8240 s / batch. (data: 3.12e-04). ETA=9:45:29, max mem: 20.9 GB 
[11/27 16:02:26 visual_prompt]: Epoch 23 / 100: avg data time: 1.39e-01, avg batch time: 0.9724, average train loss: 16.6052
[11/27 16:03:21 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3103, average loss: 17.7758
[11/27 16:03:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.79	
[11/27 16:03:21 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[11/27 16:05:00 visual_prompt]: 	Training 100/553. train loss: 23.6612,	0.8440 s / batch. (data: 3.98e-04). ETA=9:57:31, max mem: 20.9 GB 
[11/27 16:06:36 visual_prompt]: 	Training 200/553. train loss: 9.9993,	0.8440 s / batch. (data: 3.20e-04). ETA=9:56:08, max mem: 20.9 GB 
[11/27 16:08:13 visual_prompt]: 	Training 300/553. train loss: 4.3296,	0.8520 s / batch. (data: 1.96e-02). ETA=10:00:22, max mem: 20.9 GB 
[11/27 16:09:51 visual_prompt]: 	Training 400/553. train loss: 7.3109,	0.8312 s / batch. (data: 1.18e-02). ETA=9:44:20, max mem: 20.9 GB 
[11/27 16:11:29 visual_prompt]: 	Training 500/553. train loss: 12.1950,	0.8210 s / batch. (data: 5.17e-04). ETA=9:35:46, max mem: 20.9 GB 
[11/27 16:12:20 visual_prompt]: Epoch 24 / 100: avg data time: 1.39e-01, avg batch time: 0.9731, average train loss: 15.6678
[11/27 16:13:15 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3109, average loss: 12.2164
[11/27 16:13:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.22	
[11/27 16:13:15 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[11/27 16:14:59 visual_prompt]: 	Training 100/553. train loss: 1.2045,	0.8656 s / batch. (data: 1.76e-02). ETA=10:04:52, max mem: 20.9 GB 
[11/27 16:16:33 visual_prompt]: 	Training 200/553. train loss: 7.4703,	0.8400 s / batch. (data: 1.19e-02). ETA=9:45:36, max mem: 20.9 GB 
[11/27 16:18:10 visual_prompt]: 	Training 300/553. train loss: 4.8980,	0.8323 s / batch. (data: 1.15e-02). ETA=9:38:48, max mem: 20.9 GB 
[11/27 16:19:46 visual_prompt]: 	Training 400/553. train loss: 1.9689,	1.0897 s / batch. (data: 2.71e-01). ETA=12:36:01, max mem: 20.9 GB 
[11/27 16:21:27 visual_prompt]: 	Training 500/553. train loss: 35.4260,	1.2991 s / batch. (data: 4.54e-01). ETA=14:59:07, max mem: 20.9 GB 
[11/27 16:22:18 visual_prompt]: Epoch 25 / 100: avg data time: 1.48e-01, avg batch time: 0.9809, average train loss: 15.9382
[11/27 16:23:13 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3102, average loss: 8.8091
[11/27 16:23:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.45	
[11/27 16:23:13 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[11/27 16:24:57 visual_prompt]: 	Training 100/553. train loss: 2.1147,	0.8440 s / batch. (data: 3.01e-04). ETA=9:41:59, max mem: 20.9 GB 
[11/27 16:26:34 visual_prompt]: 	Training 200/553. train loss: 17.9493,	0.8923 s / batch. (data: 5.18e-02). ETA=10:13:49, max mem: 20.9 GB 
[11/27 16:28:14 visual_prompt]: 	Training 300/553. train loss: 0.0001,	0.8281 s / batch. (data: 3.02e-04). ETA=9:28:15, max mem: 20.9 GB 
[11/27 16:29:49 visual_prompt]: 	Training 400/553. train loss: 16.3151,	0.8440 s / batch. (data: 7.95e-03). ETA=9:37:46, max mem: 20.9 GB 
[11/27 16:31:25 visual_prompt]: 	Training 500/553. train loss: 10.0856,	0.8554 s / batch. (data: 1.13e-02). ETA=9:44:08, max mem: 20.9 GB 
[11/27 16:32:15 visual_prompt]: Epoch 26 / 100: avg data time: 1.47e-01, avg batch time: 0.9807, average train loss: 12.4177
[11/27 16:33:11 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3110, average loss: 10.5732
[11/27 16:33:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.24	
[11/27 16:33:11 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[11/27 16:34:52 visual_prompt]: 	Training 100/553. train loss: 2.7791,	0.8409 s / batch. (data: 3.29e-04). ETA=9:32:05, max mem: 20.9 GB 
[11/27 16:36:30 visual_prompt]: 	Training 200/553. train loss: 93.5871,	0.8939 s / batch. (data: 7.92e-02). ETA=10:06:42, max mem: 20.9 GB 
[11/27 16:38:08 visual_prompt]: 	Training 300/553. train loss: 2.9835,	0.8560 s / batch. (data: 5.64e-03). ETA=9:39:33, max mem: 20.9 GB 
[11/27 16:39:45 visual_prompt]: 	Training 400/553. train loss: 11.9248,	0.8391 s / batch. (data: 3.82e-04). ETA=9:26:42, max mem: 20.9 GB 
[11/27 16:41:23 visual_prompt]: 	Training 500/553. train loss: 10.5463,	0.8480 s / batch. (data: 8.23e-04). ETA=9:31:17, max mem: 20.9 GB 
[11/27 16:42:12 visual_prompt]: Epoch 27 / 100: avg data time: 1.44e-01, avg batch time: 0.9781, average train loss: 14.5366
[11/27 16:43:07 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3105, average loss: 13.4303
[11/27 16:43:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.34	
[11/27 16:43:07 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[11/27 16:44:47 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8200 s / batch. (data: 3.05e-04). ETA=9:10:20, max mem: 20.9 GB 
[11/27 16:46:25 visual_prompt]: 	Training 200/553. train loss: 6.1713,	0.8360 s / batch. (data: 7.98e-03). ETA=9:19:43, max mem: 20.9 GB 
[11/27 16:48:02 visual_prompt]: 	Training 300/553. train loss: 3.1755,	1.4152 s / batch. (data: 5.98e-01). ETA=15:45:04, max mem: 20.9 GB 
[11/27 16:49:39 visual_prompt]: 	Training 400/553. train loss: 91.5033,	0.8289 s / batch. (data: 3.00e-04). ETA=9:12:10, max mem: 20.9 GB 
[11/27 16:51:14 visual_prompt]: 	Training 500/553. train loss: 14.5788,	0.8481 s / batch. (data: 2.54e-04). ETA=9:23:31, max mem: 20.9 GB 
[11/27 16:52:06 visual_prompt]: Epoch 28 / 100: avg data time: 1.40e-01, avg batch time: 0.9739, average train loss: 16.3554
[11/27 16:53:01 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3103, average loss: 1.2334
[11/27 16:53:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.91	
[11/27 16:53:01 visual_prompt]: Best epoch 28: best metric: -1.233
[11/27 16:53:01 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[11/27 16:54:49 visual_prompt]: 	Training 100/553. train loss: 0.0007,	0.8360 s / batch. (data: 3.22e-04). ETA=9:13:22, max mem: 20.9 GB 
[11/27 16:56:25 visual_prompt]: 	Training 200/553. train loss: 12.2392,	1.6732 s / batch. (data: 8.21e-01). ETA=18:24:45, max mem: 20.9 GB 
[11/27 16:58:00 visual_prompt]: 	Training 300/553. train loss: 7.0682,	0.8280 s / batch. (data: 3.18e-04). ETA=9:05:18, max mem: 20.9 GB 
[11/27 16:59:34 visual_prompt]: 	Training 400/553. train loss: 2.5316,	1.3024 s / batch. (data: 4.74e-01). ETA=14:15:36, max mem: 20.9 GB 
[11/27 17:01:12 visual_prompt]: 	Training 500/553. train loss: 6.7069,	0.8320 s / batch. (data: 3.52e-04). ETA=9:05:10, max mem: 20.9 GB 
[11/27 17:02:03 visual_prompt]: Epoch 29 / 100: avg data time: 1.45e-01, avg batch time: 0.9788, average train loss: 16.1975
[11/27 17:02:58 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3111, average loss: 42.7478
[11/27 17:02:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.75	
[11/27 17:02:58 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[11/27 17:04:38 visual_prompt]: 	Training 100/553. train loss: 12.7678,	0.8560 s / batch. (data: 2.96e-04). ETA=9:18:44, max mem: 20.9 GB 
[11/27 17:06:16 visual_prompt]: 	Training 200/553. train loss: 37.2850,	0.8220 s / batch. (data: 3.40e-04). ETA=8:55:10, max mem: 20.9 GB 
[11/27 17:08:00 visual_prompt]: 	Training 300/553. train loss: 30.1160,	0.8355 s / batch. (data: 5.51e-03). ETA=9:02:34, max mem: 20.9 GB 
[11/27 17:09:39 visual_prompt]: 	Training 400/553. train loss: 19.1996,	1.1280 s / batch. (data: 2.97e-01). ETA=12:10:37, max mem: 20.9 GB 
[11/27 17:11:16 visual_prompt]: 	Training 500/553. train loss: 8.1708,	1.4425 s / batch. (data: 5.96e-01). ETA=15:31:53, max mem: 20.9 GB 
[11/27 17:12:08 visual_prompt]: Epoch 30 / 100: avg data time: 1.60e-01, avg batch time: 0.9947, average train loss: 15.8050
[11/27 17:13:04 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3095, average loss: 7.5050
[11/27 17:13:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.89	
[11/27 17:13:04 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[11/27 17:14:47 visual_prompt]: 	Training 100/553. train loss: 5.6367,	0.8560 s / batch. (data: 2.94e-04). ETA=9:10:50, max mem: 20.9 GB 
[11/27 17:16:27 visual_prompt]: 	Training 200/553. train loss: 6.6392,	0.8392 s / batch. (data: 2.05e-02). ETA=8:58:38, max mem: 20.9 GB 
[11/27 17:18:02 visual_prompt]: 	Training 300/553. train loss: 41.8236,	0.8439 s / batch. (data: 5.44e-03). ETA=9:00:14, max mem: 20.9 GB 
[11/27 17:19:39 visual_prompt]: 	Training 400/553. train loss: 0.9660,	1.1416 s / batch. (data: 2.95e-01). ETA=12:08:55, max mem: 20.9 GB 
[11/27 17:21:17 visual_prompt]: 	Training 500/553. train loss: 3.8787,	0.8555 s / batch. (data: 3.16e-02). ETA=9:04:48, max mem: 20.9 GB 
[11/27 17:22:07 visual_prompt]: Epoch 31 / 100: avg data time: 1.47e-01, avg batch time: 0.9807, average train loss: 13.4559
[11/27 17:23:02 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3097, average loss: 26.2762
[11/27 17:23:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 45.88	
[11/27 17:23:02 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[11/27 17:24:45 visual_prompt]: 	Training 100/553. train loss: 0.9339,	0.8340 s / batch. (data: 5.42e-03). ETA=8:48:59, max mem: 20.9 GB 
[11/27 17:26:22 visual_prompt]: 	Training 200/553. train loss: 30.8756,	0.8388 s / batch. (data: 3.11e-04). ETA=8:50:36, max mem: 20.9 GB 
[11/27 17:28:02 visual_prompt]: 	Training 300/553. train loss: 6.8317,	0.8277 s / batch. (data: 7.97e-03). ETA=8:42:12, max mem: 20.9 GB 
[11/27 17:29:40 visual_prompt]: 	Training 400/553. train loss: 3.5445,	0.8484 s / batch. (data: 3.28e-04). ETA=8:53:54, max mem: 20.9 GB 
[11/27 17:31:14 visual_prompt]: 	Training 500/553. train loss: 9.5365,	0.8365 s / batch. (data: 2.99e-04). ETA=8:44:58, max mem: 20.9 GB 
[11/27 17:32:03 visual_prompt]: Epoch 32 / 100: avg data time: 1.44e-01, avg batch time: 0.9783, average train loss: 12.0881
[11/27 17:32:59 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3109, average loss: 9.6817
[11/27 17:32:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.85	
[11/27 17:32:59 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[11/27 17:34:39 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8514 s / batch. (data: 1.13e-02). ETA=8:52:09, max mem: 20.9 GB 
[11/27 17:36:18 visual_prompt]: 	Training 200/553. train loss: 16.7587,	1.0646 s / batch. (data: 2.32e-01). ETA=11:03:40, max mem: 20.9 GB 
[11/27 17:37:54 visual_prompt]: 	Training 300/553. train loss: 19.7131,	0.8201 s / batch. (data: 3.00e-04). ETA=8:29:52, max mem: 20.9 GB 
[11/27 17:39:33 visual_prompt]: 	Training 400/553. train loss: 7.7959,	0.8621 s / batch. (data: 3.13e-04). ETA=8:54:35, max mem: 20.9 GB 
[11/27 17:41:09 visual_prompt]: 	Training 500/553. train loss: 2.3726,	0.8299 s / batch. (data: 1.04e-02). ETA=8:33:12, max mem: 20.9 GB 
[11/27 17:41:59 visual_prompt]: Epoch 33 / 100: avg data time: 1.42e-01, avg batch time: 0.9762, average train loss: 14.4954
[11/27 17:42:54 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3098, average loss: 1.2949
[11/27 17:42:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.83	
[11/27 17:42:54 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[11/27 17:44:37 visual_prompt]: 	Training 100/553. train loss: 21.6753,	1.0680 s / batch. (data: 2.28e-01). ETA=10:57:44, max mem: 20.9 GB 
[11/27 17:46:12 visual_prompt]: 	Training 200/553. train loss: 4.5809,	0.8320 s / batch. (data: 3.35e-04). ETA=8:31:00, max mem: 20.9 GB 
[11/27 17:47:49 visual_prompt]: 	Training 300/553. train loss: 2.4369,	0.8520 s / batch. (data: 1.05e-02). ETA=8:41:50, max mem: 20.9 GB 
[11/27 17:49:27 visual_prompt]: 	Training 400/553. train loss: 0.8394,	0.8286 s / batch. (data: 5.43e-03). ETA=8:26:10, max mem: 20.9 GB 
[11/27 17:51:04 visual_prompt]: 	Training 500/553. train loss: 5.2098,	1.4431 s / batch. (data: 6.05e-01). ETA=14:39:07, max mem: 20.9 GB 
[11/27 17:51:54 visual_prompt]: Epoch 34 / 100: avg data time: 1.43e-01, avg batch time: 0.9766, average train loss: 15.0269
[11/27 17:52:50 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3111, average loss: 12.5809
[11/27 17:52:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.34	
[11/27 17:52:50 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[11/27 17:54:33 visual_prompt]: 	Training 100/553. train loss: 1.5603,	0.8421 s / batch. (data: 1.05e-02). ETA=8:30:49, max mem: 20.9 GB 
[11/27 17:56:11 visual_prompt]: 	Training 200/553. train loss: 7.1569,	0.8568 s / batch. (data: 5.41e-03). ETA=8:38:19, max mem: 20.9 GB 
[11/27 17:57:47 visual_prompt]: 	Training 300/553. train loss: 9.4010,	0.8374 s / batch. (data: 9.31e-03). ETA=8:25:11, max mem: 20.9 GB 
[11/27 17:59:23 visual_prompt]: 	Training 400/553. train loss: 3.3447,	0.8329 s / batch. (data: 3.08e-04). ETA=8:21:04, max mem: 20.9 GB 
[11/27 18:01:00 visual_prompt]: 	Training 500/553. train loss: 5.2290,	0.8247 s / batch. (data: 5.45e-03). ETA=8:14:46, max mem: 20.9 GB 
[11/27 18:01:51 visual_prompt]: Epoch 35 / 100: avg data time: 1.44e-01, avg batch time: 0.9778, average train loss: 13.1977
[11/27 18:02:47 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3120, average loss: 1.6425
[11/27 18:02:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.10	
[11/27 18:02:47 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[11/27 18:04:27 visual_prompt]: 	Training 100/553. train loss: 0.5133,	0.8440 s / batch. (data: 7.95e-03). ETA=8:24:12, max mem: 20.9 GB 
[11/27 18:06:06 visual_prompt]: 	Training 200/553. train loss: 56.9221,	0.8439 s / batch. (data: 1.59e-02). ETA=8:22:45, max mem: 20.9 GB 
[11/27 18:07:44 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8241 s / batch. (data: 3.19e-04). ETA=8:09:33, max mem: 20.9 GB 
[11/27 18:09:20 visual_prompt]: 	Training 400/553. train loss: 10.9672,	0.8560 s / batch. (data: 3.31e-04). ETA=8:27:06, max mem: 20.9 GB 
[11/27 18:10:59 visual_prompt]: 	Training 500/553. train loss: 10.8781,	1.0562 s / batch. (data: 2.39e-01). ETA=10:23:56, max mem: 20.9 GB 
[11/27 18:11:47 visual_prompt]: Epoch 36 / 100: avg data time: 1.43e-01, avg batch time: 0.9769, average train loss: 11.3525
[11/27 18:12:43 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3115, average loss: 42.5914
[11/27 18:12:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.18	
[11/27 18:12:43 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[11/27 18:14:23 visual_prompt]: 	Training 100/553. train loss: 4.1352,	0.8303 s / batch. (data: 5.42e-03). ETA=8:08:22, max mem: 20.9 GB 
[11/27 18:16:00 visual_prompt]: 	Training 200/553. train loss: 6.0755,	0.8410 s / batch. (data: 1.17e-02). ETA=8:13:15, max mem: 20.9 GB 
[11/27 18:17:37 visual_prompt]: 	Training 300/553. train loss: 57.1658,	1.4719 s / batch. (data: 6.45e-01). ETA=14:20:50, max mem: 20.9 GB 
[11/27 18:19:16 visual_prompt]: 	Training 400/553. train loss: 5.1942,	1.2280 s / batch. (data: 3.84e-01). ETA=11:56:09, max mem: 20.9 GB 
[11/27 18:20:49 visual_prompt]: 	Training 500/553. train loss: 6.9139,	0.8760 s / batch. (data: 2.41e-02). ETA=8:29:24, max mem: 20.9 GB 
[11/27 18:21:41 visual_prompt]: Epoch 37 / 100: avg data time: 1.40e-01, avg batch time: 0.9741, average train loss: 14.5513
[11/27 18:22:37 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3096, average loss: 26.6352
[11/27 18:22:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.11	
[11/27 18:22:37 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[11/27 18:24:16 visual_prompt]: 	Training 100/553. train loss: 18.7491,	0.8321 s / batch. (data: 3.11e-04). ETA=8:01:46, max mem: 20.9 GB 
[11/27 18:25:53 visual_prompt]: 	Training 200/553. train loss: 3.7498,	0.8553 s / batch. (data: 1.52e-02). ETA=8:13:45, max mem: 20.9 GB 
[11/27 18:27:32 visual_prompt]: 	Training 300/553. train loss: 22.9882,	0.8189 s / batch. (data: 2.99e-04). ETA=7:51:22, max mem: 20.9 GB 
[11/27 18:29:06 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8600 s / batch. (data: 2.00e-02). ETA=8:13:37, max mem: 20.9 GB 
[11/27 18:30:45 visual_prompt]: 	Training 500/553. train loss: 12.5993,	0.8333 s / batch. (data: 1.02e-02). ETA=7:56:53, max mem: 20.9 GB 
[11/27 18:31:34 visual_prompt]: Epoch 38 / 100: avg data time: 1.38e-01, avg batch time: 0.9715, average train loss: 13.2105
[11/27 18:32:29 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3104, average loss: 1.1450
[11/27 18:32:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.30	
[11/27 18:32:29 visual_prompt]: Best epoch 38: best metric: -1.145
[11/27 18:32:29 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[11/27 18:34:09 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8364 s / batch. (data: 6.32e-03). ETA=7:56:32, max mem: 20.9 GB 
[11/27 18:35:49 visual_prompt]: 	Training 200/553. train loss: 7.1017,	0.8437 s / batch. (data: 1.20e-02). ETA=7:59:18, max mem: 20.9 GB 
[11/27 18:37:29 visual_prompt]: 	Training 300/553. train loss: 19.4099,	0.8195 s / batch. (data: 2.93e-04). ETA=7:44:10, max mem: 20.9 GB 
[11/27 18:39:03 visual_prompt]: 	Training 400/553. train loss: 25.4852,	1.0280 s / batch. (data: 1.75e-01). ETA=9:40:35, max mem: 20.9 GB 
[11/27 18:40:39 visual_prompt]: 	Training 500/553. train loss: 7.9714,	1.5468 s / batch. (data: 7.28e-01). ETA=14:31:00, max mem: 20.9 GB 
[11/27 18:41:28 visual_prompt]: Epoch 39 / 100: avg data time: 1.40e-01, avg batch time: 0.9746, average train loss: 11.1693
[11/27 18:42:23 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3119, average loss: 11.1331
[11/27 18:42:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.75	
[11/27 18:42:23 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[11/27 18:44:05 visual_prompt]: 	Training 100/553. train loss: 12.8464,	0.8560 s / batch. (data: 2.00e-02). ETA=7:59:49, max mem: 20.9 GB 
[11/27 18:45:41 visual_prompt]: 	Training 200/553. train loss: 8.2352,	0.8284 s / batch. (data: 3.13e-04). ETA=7:43:00, max mem: 20.9 GB 
[11/27 18:47:20 visual_prompt]: 	Training 300/553. train loss: 8.4137,	0.8459 s / batch. (data: 1.39e-02). ETA=7:51:22, max mem: 20.9 GB 
[11/27 18:48:57 visual_prompt]: 	Training 400/553. train loss: 3.6512,	0.8525 s / batch. (data: 5.89e-03). ETA=7:53:36, max mem: 20.9 GB 
[11/27 18:50:32 visual_prompt]: 	Training 500/553. train loss: 17.7791,	0.8440 s / batch. (data: 3.07e-04). ETA=7:47:29, max mem: 20.9 GB 
[11/27 18:51:24 visual_prompt]: Epoch 40 / 100: avg data time: 1.43e-01, avg batch time: 0.9771, average train loss: 12.2651
[11/27 18:52:19 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3100, average loss: 7.9841
[11/27 18:52:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.35	
[11/27 18:52:19 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[11/27 18:54:04 visual_prompt]: 	Training 100/553. train loss: 4.6326,	0.8400 s / batch. (data: 5.45e-03). ETA=7:43:06, max mem: 20.9 GB 
[11/27 18:55:44 visual_prompt]: 	Training 200/553. train loss: 11.2113,	0.8433 s / batch. (data: 7.99e-04). ETA=7:43:33, max mem: 20.9 GB 
[11/27 18:57:20 visual_prompt]: 	Training 300/553. train loss: 74.7709,	0.8474 s / batch. (data: 3.04e-04). ETA=7:44:22, max mem: 20.9 GB 
[11/27 18:58:57 visual_prompt]: 	Training 400/553. train loss: 10.3165,	0.8402 s / batch. (data: 2.06e-02). ETA=7:39:02, max mem: 20.9 GB 
[11/27 19:00:32 visual_prompt]: 	Training 500/553. train loss: 5.7795,	0.8480 s / batch. (data: 7.99e-04). ETA=7:41:52, max mem: 20.9 GB 
[11/27 19:01:20 visual_prompt]: Epoch 41 / 100: avg data time: 1.44e-01, avg batch time: 0.9783, average train loss: 15.6686
[11/27 19:02:16 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3121, average loss: 1.5621
[11/27 19:02:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.75	
[11/27 19:02:16 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[11/27 19:03:56 visual_prompt]: 	Training 100/553. train loss: 40.2209,	0.8325 s / batch. (data: 8.46e-03). ETA=7:31:19, max mem: 20.9 GB 
[11/27 19:05:34 visual_prompt]: 	Training 200/553. train loss: 75.6881,	0.8209 s / batch. (data: 3.16e-04). ETA=7:23:38, max mem: 20.9 GB 
[11/27 19:07:12 visual_prompt]: 	Training 300/553. train loss: 19.6024,	0.8400 s / batch. (data: 3.97e-04). ETA=7:32:33, max mem: 20.9 GB 
[11/27 19:08:49 visual_prompt]: 	Training 400/553. train loss: 5.8587,	0.8458 s / batch. (data: 1.20e-02). ETA=7:34:16, max mem: 20.9 GB 
[11/27 19:10:25 visual_prompt]: 	Training 500/553. train loss: 104.2383,	0.8240 s / batch. (data: 3.44e-04). ETA=7:21:12, max mem: 20.9 GB 
[11/27 19:11:17 visual_prompt]: Epoch 42 / 100: avg data time: 1.45e-01, avg batch time: 0.9792, average train loss: 14.3884
[11/27 19:12:13 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3113, average loss: 36.3718
[11/27 19:12:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.19	
[11/27 19:12:13 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[11/27 19:13:57 visual_prompt]: 	Training 100/553. train loss: 4.2328,	0.8351 s / batch. (data: 1.05e-02). ETA=7:25:00, max mem: 20.9 GB 
[11/27 19:15:33 visual_prompt]: 	Training 200/553. train loss: 17.3884,	0.8288 s / batch. (data: 3.22e-04). ETA=7:20:18, max mem: 20.9 GB 
[11/27 19:17:08 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8199 s / batch. (data: 3.19e-04). ETA=7:14:11, max mem: 20.9 GB 
[11/27 19:18:44 visual_prompt]: 	Training 400/553. train loss: 3.4249,	0.8205 s / batch. (data: 2.79e-04). ETA=7:13:07, max mem: 20.9 GB 
[11/27 19:20:23 visual_prompt]: 	Training 500/553. train loss: 29.2855,	0.8366 s / batch. (data: 5.41e-03). ETA=7:20:15, max mem: 20.9 GB 
[11/27 19:21:15 visual_prompt]: Epoch 43 / 100: avg data time: 1.45e-01, avg batch time: 0.9792, average train loss: 12.4036
[11/27 19:22:11 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3103, average loss: 24.4953
[11/27 19:22:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.87	
[11/27 19:22:11 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[11/27 19:23:52 visual_prompt]: 	Training 100/553. train loss: 0.9345,	0.8396 s / batch. (data: 1.05e-02). ETA=7:19:39, max mem: 20.9 GB 
[11/27 19:25:32 visual_prompt]: 	Training 200/553. train loss: 2.4608,	0.8520 s / batch. (data: 3.04e-04). ETA=7:24:45, max mem: 20.9 GB 
[11/27 19:27:07 visual_prompt]: 	Training 300/553. train loss: 8.1179,	0.8425 s / batch. (data: 3.01e-04). ETA=7:18:23, max mem: 20.9 GB 
[11/27 19:28:43 visual_prompt]: 	Training 400/553. train loss: 0.3930,	0.8600 s / batch. (data: 2.67e-02). ETA=7:26:05, max mem: 20.9 GB 
[11/27 19:30:20 visual_prompt]: 	Training 500/553. train loss: 7.7799,	0.8440 s / batch. (data: 7.95e-03). ETA=7:16:21, max mem: 20.9 GB 
[11/27 19:31:10 visual_prompt]: Epoch 44 / 100: avg data time: 1.41e-01, avg batch time: 0.9756, average train loss: 12.4275
[11/27 19:32:05 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3108, average loss: 14.9444
[11/27 19:32:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.96	
[11/27 19:32:05 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[11/27 19:33:49 visual_prompt]: 	Training 100/553. train loss: 7.5158,	0.8360 s / batch. (data: 3.00e-04). ETA=7:10:05, max mem: 20.9 GB 
[11/27 19:35:22 visual_prompt]: 	Training 200/553. train loss: 9.6313,	0.8526 s / batch. (data: 5.43e-03). ETA=7:17:12, max mem: 20.9 GB 
[11/27 19:37:01 visual_prompt]: 	Training 300/553. train loss: 2.9258,	0.8480 s / batch. (data: 3.04e-04). ETA=7:13:26, max mem: 20.9 GB 
[11/27 19:38:35 visual_prompt]: 	Training 400/553. train loss: 4.2912,	0.8280 s / batch. (data: 3.27e-04). ETA=7:01:49, max mem: 20.9 GB 
[11/27 19:40:14 visual_prompt]: 	Training 500/553. train loss: 0.7238,	0.8287 s / batch. (data: 5.45e-03). ETA=7:00:49, max mem: 20.9 GB 
[11/27 19:41:04 visual_prompt]: Epoch 45 / 100: avg data time: 1.39e-01, avg batch time: 0.9737, average train loss: 9.5140
[11/27 19:42:00 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3090, average loss: 18.0233
[11/27 19:42:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.83	
[11/27 19:42:00 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[11/27 19:43:42 visual_prompt]: 	Training 100/553. train loss: 17.1797,	1.0499 s / batch. (data: 2.15e-01). ETA=8:50:28, max mem: 20.9 GB 
[11/27 19:45:20 visual_prompt]: 	Training 200/553. train loss: 8.9671,	0.8336 s / batch. (data: 8.47e-04). ETA=6:59:46, max mem: 20.9 GB 
[11/27 19:46:56 visual_prompt]: 	Training 300/553. train loss: 12.5220,	0.8360 s / batch. (data: 2.87e-04). ETA=6:59:36, max mem: 20.9 GB 
[11/27 19:48:33 visual_prompt]: 	Training 400/553. train loss: 1.9691,	0.8453 s / batch. (data: 1.20e-02). ETA=7:02:51, max mem: 20.9 GB 
[11/27 19:50:07 visual_prompt]: 	Training 500/553. train loss: 40.2306,	0.8403 s / batch. (data: 5.44e-03). ETA=6:58:58, max mem: 20.9 GB 
[11/27 19:50:59 visual_prompt]: Epoch 46 / 100: avg data time: 1.41e-01, avg batch time: 0.9759, average train loss: 8.7812
[11/27 19:51:55 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3102, average loss: 6.7670
[11/27 19:51:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.64	
[11/27 19:51:55 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[11/27 19:53:37 visual_prompt]: 	Training 100/553. train loss: 6.5043,	0.8395 s / batch. (data: 1.12e-02). ETA=6:56:23, max mem: 20.9 GB 
[11/27 19:55:11 visual_prompt]: 	Training 200/553. train loss: 19.0623,	1.1638 s / batch. (data: 3.37e-01). ETA=9:35:19, max mem: 20.9 GB 
[11/27 19:56:49 visual_prompt]: 	Training 300/553. train loss: 2.0360,	0.8391 s / batch. (data: 3.29e-04). ETA=6:53:26, max mem: 20.9 GB 
[11/27 19:58:27 visual_prompt]: 	Training 400/553. train loss: 17.8990,	0.8416 s / batch. (data: 1.20e-02). ETA=6:53:15, max mem: 20.9 GB 
[11/27 20:00:03 visual_prompt]: 	Training 500/553. train loss: 15.1629,	0.8600 s / batch. (data: 2.99e-04). ETA=7:00:51, max mem: 20.9 GB 
[11/27 20:00:55 visual_prompt]: Epoch 47 / 100: avg data time: 1.41e-01, avg batch time: 0.9759, average train loss: 10.7019
[11/27 20:01:50 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3093, average loss: 13.3165
[11/27 20:01:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.68	
[11/27 20:01:50 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[11/27 20:03:32 visual_prompt]: 	Training 100/553. train loss: 0.7990,	0.8563 s / batch. (data: 1.56e-02). ETA=6:56:50, max mem: 20.9 GB 
[11/27 20:05:09 visual_prompt]: 	Training 200/553. train loss: 3.6159,	0.8251 s / batch. (data: 3.01e-04). ETA=6:40:18, max mem: 20.9 GB 
[11/27 20:06:47 visual_prompt]: 	Training 300/553. train loss: 23.6609,	1.4848 s / batch. (data: 6.67e-01). ETA=11:57:52, max mem: 20.9 GB 
[11/27 20:08:21 visual_prompt]: 	Training 400/553. train loss: 16.5899,	0.8280 s / batch. (data: 3.26e-04). ETA=6:38:56, max mem: 20.9 GB 
[11/27 20:09:58 visual_prompt]: 	Training 500/553. train loss: 6.4613,	0.8360 s / batch. (data: 3.36e-04). ETA=6:41:23, max mem: 20.9 GB 
[11/27 20:10:48 visual_prompt]: Epoch 48 / 100: avg data time: 1.37e-01, avg batch time: 0.9720, average train loss: 8.3151
[11/27 20:11:43 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3111, average loss: 10.6992
[11/27 20:11:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.03	
[11/27 20:11:43 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[11/27 20:13:24 visual_prompt]: 	Training 100/553. train loss: 7.3680,	0.8265 s / batch. (data: 7.23e-04). ETA=6:34:44, max mem: 20.9 GB 
[11/27 20:14:59 visual_prompt]: 	Training 200/553. train loss: 4.8656,	0.8438 s / batch. (data: 1.56e-02). ETA=6:41:35, max mem: 20.9 GB 
[11/27 20:16:37 visual_prompt]: 	Training 300/553. train loss: 0.0002,	0.8200 s / batch. (data: 2.47e-04). ETA=6:28:54, max mem: 20.9 GB 
[11/27 20:18:15 visual_prompt]: 	Training 400/553. train loss: 1.1833,	0.8374 s / batch. (data: 7.93e-03). ETA=6:35:46, max mem: 20.9 GB 
[11/27 20:19:52 visual_prompt]: 	Training 500/553. train loss: 1.8481,	0.8321 s / batch. (data: 2.64e-04). ETA=6:31:50, max mem: 20.9 GB 
[11/27 20:20:43 visual_prompt]: Epoch 49 / 100: avg data time: 1.41e-01, avg batch time: 0.9759, average train loss: 7.7641
[11/27 20:21:38 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3108, average loss: 4.4623
[11/27 20:21:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.18	
[11/27 20:21:38 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[11/27 20:23:21 visual_prompt]: 	Training 100/553. train loss: 11.2692,	0.8192 s / batch. (data: 3.12e-04). ETA=6:23:40, max mem: 20.9 GB 
[11/27 20:24:59 visual_prompt]: 	Training 200/553. train loss: 6.6445,	0.8400 s / batch. (data: 3.04e-04). ETA=6:32:02, max mem: 20.9 GB 
[11/27 20:26:34 visual_prompt]: 	Training 300/553. train loss: 16.0209,	0.8480 s / batch. (data: 1.20e-02). ETA=6:34:21, max mem: 20.9 GB 
[11/27 20:28:10 visual_prompt]: 	Training 400/553. train loss: 44.0007,	0.8501 s / batch. (data: 1.04e-02). ETA=6:33:56, max mem: 20.9 GB 
[11/27 20:29:49 visual_prompt]: 	Training 500/553. train loss: 2.4115,	0.8538 s / batch. (data: 3.15e-04). ETA=6:34:12, max mem: 20.9 GB 
[11/27 20:30:39 visual_prompt]: Epoch 50 / 100: avg data time: 1.43e-01, avg batch time: 0.9773, average train loss: 9.6166
[11/27 20:31:34 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3112, average loss: 7.1900
[11/27 20:31:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.40	
[11/27 20:31:34 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[11/27 20:33:16 visual_prompt]: 	Training 100/553. train loss: 0.7930,	1.1929 s / batch. (data: 3.30e-01). ETA=9:07:44, max mem: 20.9 GB 
[11/27 20:34:54 visual_prompt]: 	Training 200/553. train loss: 1.1470,	0.8640 s / batch. (data: 1.20e-02). ETA=6:35:16, max mem: 20.9 GB 
[11/27 20:36:32 visual_prompt]: 	Training 300/553. train loss: 9.3558,	0.8189 s / batch. (data: 3.16e-04). ETA=6:13:17, max mem: 20.9 GB 
[11/27 20:38:10 visual_prompt]: 	Training 400/553. train loss: 2.7052,	1.3846 s / batch. (data: 5.62e-01). ETA=10:28:50, max mem: 20.9 GB 
[11/27 20:39:47 visual_prompt]: 	Training 500/553. train loss: 3.3948,	0.8560 s / batch. (data: 3.96e-04). ETA=6:27:21, max mem: 20.9 GB 
[11/27 20:40:36 visual_prompt]: Epoch 51 / 100: avg data time: 1.46e-01, avg batch time: 0.9796, average train loss: 8.6294
[11/27 20:41:32 visual_prompt]: Inference (val):avg data time: 3.82e-04, avg batch time: 0.3118, average loss: 9.7046
[11/27 20:41:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.04	
[11/27 20:41:32 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[11/27 20:43:17 visual_prompt]: 	Training 100/553. train loss: 5.6225,	0.8415 s / batch. (data: 8.20e-04). ETA=6:18:38, max mem: 20.9 GB 
[11/27 20:44:53 visual_prompt]: 	Training 200/553. train loss: 2.4858,	0.8360 s / batch. (data: 3.18e-04). ETA=6:14:45, max mem: 20.9 GB 
[11/27 20:46:32 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8286 s / batch. (data: 2.73e-04). ETA=6:10:03, max mem: 20.9 GB 
[11/27 20:48:11 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8270 s / batch. (data: 3.21e-04). ETA=6:07:59, max mem: 20.9 GB 
[11/27 20:49:44 visual_prompt]: 	Training 500/553. train loss: 4.0076,	0.8400 s / batch. (data: 3.05e-04). ETA=6:12:22, max mem: 20.9 GB 
[11/27 20:50:33 visual_prompt]: Epoch 52 / 100: avg data time: 1.45e-01, avg batch time: 0.9792, average train loss: 9.5048
[11/27 20:51:29 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3113, average loss: 33.6538
[11/27 20:51:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.28	
[11/27 20:51:29 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[11/27 20:53:10 visual_prompt]: 	Training 100/553. train loss: 1.9284,	0.8292 s / batch. (data: 7.96e-03). ETA=6:05:28, max mem: 20.9 GB 
[11/27 20:54:48 visual_prompt]: 	Training 200/553. train loss: 17.4840,	0.8752 s / batch. (data: 1.57e-02). ETA=6:24:15, max mem: 20.9 GB 
[11/27 20:56:24 visual_prompt]: 	Training 300/553. train loss: 4.5768,	0.8320 s / batch. (data: 3.03e-04). ETA=6:03:54, max mem: 20.9 GB 
[11/27 20:58:04 visual_prompt]: 	Training 400/553. train loss: 27.4502,	0.8680 s / batch. (data: 3.81e-02). ETA=6:18:12, max mem: 20.9 GB 
[11/27 20:59:39 visual_prompt]: 	Training 500/553. train loss: 15.7527,	0.8323 s / batch. (data: 3.24e-04). ETA=6:01:15, max mem: 20.9 GB 
[11/27 21:00:30 visual_prompt]: Epoch 53 / 100: avg data time: 1.44e-01, avg batch time: 0.9786, average train loss: 8.4135
[11/27 21:01:25 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.3098, average loss: 13.4247
[11/27 21:01:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.34	
[11/27 21:01:25 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[11/27 21:03:08 visual_prompt]: 	Training 100/553. train loss: 6.2089,	0.8165 s / batch. (data: 3.25e-04). ETA=5:52:20, max mem: 20.9 GB 
[11/27 21:04:45 visual_prompt]: 	Training 200/553. train loss: 0.9815,	0.8432 s / batch. (data: 3.06e-04). ETA=6:02:28, max mem: 20.9 GB 
[11/27 21:06:20 visual_prompt]: 	Training 300/553. train loss: 12.3980,	0.8440 s / batch. (data: 3.17e-04). ETA=6:01:22, max mem: 20.9 GB 
[11/27 21:07:56 visual_prompt]: 	Training 400/553. train loss: 0.0163,	0.8319 s / batch. (data: 5.28e-04). ETA=5:54:49, max mem: 20.9 GB 
[11/27 21:09:33 visual_prompt]: 	Training 500/553. train loss: 6.7592,	0.8640 s / batch. (data: 3.22e-04). ETA=6:07:03, max mem: 20.9 GB 
[11/27 21:10:24 visual_prompt]: Epoch 54 / 100: avg data time: 1.40e-01, avg batch time: 0.9745, average train loss: 7.7581
[11/27 21:11:19 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3103, average loss: 4.7546
[11/27 21:11:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.15	
[11/27 21:11:19 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[11/27 21:13:00 visual_prompt]: 	Training 100/553. train loss: 3.4486,	0.8361 s / batch. (data: 2.72e-04). ETA=5:53:05, max mem: 20.9 GB 
[11/27 21:14:35 visual_prompt]: 	Training 200/553. train loss: 9.7509,	0.8511 s / batch. (data: 2.16e-02). ETA=5:58:00, max mem: 20.9 GB 
[11/27 21:16:13 visual_prompt]: 	Training 300/553. train loss: 15.2804,	0.8680 s / batch. (data: 7.95e-03). ETA=6:03:39, max mem: 20.9 GB 
[11/27 21:17:50 visual_prompt]: 	Training 400/553. train loss: 23.5790,	1.1000 s / batch. (data: 2.51e-01). ETA=7:39:01, max mem: 20.9 GB 
[11/27 21:19:25 visual_prompt]: 	Training 500/553. train loss: 4.6534,	1.0827 s / batch. (data: 2.16e-01). ETA=7:30:00, max mem: 20.9 GB 
[11/27 21:20:17 visual_prompt]: Epoch 55 / 100: avg data time: 1.36e-01, avg batch time: 0.9721, average train loss: 7.0936
[11/27 21:21:12 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3099, average loss: 3.5381
[11/27 21:21:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.52	
[11/27 21:21:12 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[11/27 21:22:54 visual_prompt]: 	Training 100/553. train loss: 21.8858,	0.8280 s / batch. (data: 3.21e-04). ETA=5:42:01, max mem: 20.9 GB 
[11/27 21:24:30 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8444 s / batch. (data: 5.41e-03). ETA=5:47:23, max mem: 20.9 GB 
[11/27 21:26:08 visual_prompt]: 	Training 300/553. train loss: 8.2923,	0.8521 s / batch. (data: 2.89e-04). ETA=5:49:07, max mem: 20.9 GB 
[11/27 21:27:46 visual_prompt]: 	Training 400/553. train loss: 3.0326,	0.8627 s / batch. (data: 2.11e-02). ETA=5:52:02, max mem: 20.9 GB 
[11/27 21:29:22 visual_prompt]: 	Training 500/553. train loss: 3.9480,	1.7920 s / batch. (data: 9.47e-01). ETA=12:08:17, max mem: 20.9 GB 
[11/27 21:30:12 visual_prompt]: Epoch 56 / 100: avg data time: 1.40e-01, avg batch time: 0.9750, average train loss: 7.8403
[11/27 21:31:07 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3115, average loss: 5.8923
[11/27 21:31:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.80	
[11/27 21:31:07 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[11/27 21:32:51 visual_prompt]: 	Training 100/553. train loss: 13.1561,	0.8370 s / batch. (data: 7.68e-04). ETA=5:38:01, max mem: 20.9 GB 
[11/27 21:34:28 visual_prompt]: 	Training 200/553. train loss: 2.6305,	0.8434 s / batch. (data: 8.00e-04). ETA=5:39:13, max mem: 20.9 GB 
[11/27 21:36:04 visual_prompt]: 	Training 300/553. train loss: 0.2899,	0.8409 s / batch. (data: 3.14e-04). ETA=5:36:48, max mem: 20.9 GB 
[11/27 21:37:40 visual_prompt]: 	Training 400/553. train loss: 3.7319,	0.8639 s / batch. (data: 1.19e-02). ETA=5:44:35, max mem: 20.9 GB 
[11/27 21:39:14 visual_prompt]: 	Training 500/553. train loss: 9.5495,	0.8461 s / batch. (data: 1.55e-02). ETA=5:36:04, max mem: 20.9 GB 
[11/27 21:40:06 visual_prompt]: Epoch 57 / 100: avg data time: 1.40e-01, avg batch time: 0.9753, average train loss: 6.2053
[11/27 21:41:02 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3093, average loss: 10.1785
[11/27 21:41:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.08	
[11/27 21:41:02 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[11/27 21:42:43 visual_prompt]: 	Training 100/553. train loss: 0.6905,	1.1835 s / batch. (data: 3.56e-01). ETA=7:47:04, max mem: 20.9 GB 
[11/27 21:44:21 visual_prompt]: 	Training 200/553. train loss: 2.1621,	0.8424 s / batch. (data: 7.93e-03). ETA=5:31:02, max mem: 20.9 GB 
[11/27 21:46:01 visual_prompt]: 	Training 300/553. train loss: 11.4583,	0.8192 s / batch. (data: 8.34e-04). ETA=5:20:33, max mem: 20.9 GB 
[11/27 21:47:37 visual_prompt]: 	Training 400/553. train loss: 1.1007,	0.8320 s / batch. (data: 2.97e-04). ETA=5:24:11, max mem: 20.9 GB 
[11/27 21:49:13 visual_prompt]: 	Training 500/553. train loss: 9.4521,	0.8195 s / batch. (data: 3.01e-04). ETA=5:17:56, max mem: 20.9 GB 
[11/27 21:50:02 visual_prompt]: Epoch 58 / 100: avg data time: 1.43e-01, avg batch time: 0.9772, average train loss: 6.1276
[11/27 21:50:57 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.3099, average loss: 21.7529
[11/27 21:50:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.84	
[11/27 21:50:57 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[11/27 21:52:40 visual_prompt]: 	Training 100/553. train loss: 0.6619,	0.8175 s / batch. (data: 4.43e-04). ETA=5:15:04, max mem: 20.9 GB 
[11/27 21:54:17 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8329 s / batch. (data: 3.37e-04). ETA=5:19:39, max mem: 20.9 GB 
[11/27 21:55:52 visual_prompt]: 	Training 300/553. train loss: 15.4370,	0.8280 s / batch. (data: 3.00e-04). ETA=5:16:23, max mem: 20.9 GB 
[11/27 21:57:28 visual_prompt]: 	Training 400/553. train loss: 4.9001,	0.8680 s / batch. (data: 3.23e-04). ETA=5:30:12, max mem: 20.9 GB 
[11/27 21:59:07 visual_prompt]: 	Training 500/553. train loss: 6.7450,	0.8551 s / batch. (data: 1.19e-03). ETA=5:23:52, max mem: 20.9 GB 
[11/27 21:59:55 visual_prompt]: Epoch 59 / 100: avg data time: 1.38e-01, avg batch time: 0.9721, average train loss: 5.8436
[11/27 22:00:50 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3106, average loss: 10.6170
[11/27 22:00:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.77	
[11/27 22:00:50 visual_prompt]: Stopping early.
[11/27 22:00:51 visual_prompt]: Rank of current process: 0. World size: 1
[11/27 22:00:51 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/27 22:00:51 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/27 22:00:51 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/27 22:00:51 visual_prompt]: Training with config:
[11/27 22:00:51 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr5.0_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/27 22:00:51 visual_prompt]: Loading training data...
[11/27 22:00:51 visual_prompt]: Constructing mammo-cbis dataset train...
[11/27 22:00:51 visual_prompt]: Loading validation data...
[11/27 22:00:51 visual_prompt]: Constructing mammo-cbis dataset val...
[11/27 22:00:51 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/27 22:00:53 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/27 22:00:53 visual_prompt]: tuned percent:0.525
[11/27 22:00:54 visual_prompt]: Device used for model: 0
[11/27 22:00:54 visual_prompt]: Setting up Evaluator...
[11/27 22:00:54 visual_prompt]: Setting up Trainer...
[11/27 22:00:54 visual_prompt]: 	Setting up the optimizer...
[11/27 22:00:54 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/27 22:02:34 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8440 s / batch. (data: 7.96e-03). ETA=12:56:30, max mem: 20.9 GB 
[11/27 22:04:10 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8339 s / batch. (data: 1.19e-02). ETA=12:45:47, max mem: 20.9 GB 
[11/27 22:05:49 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8994 s / batch. (data: 6.69e-02). ETA=13:44:29, max mem: 20.9 GB 
[11/27 22:07:24 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8440 s / batch. (data: 7.96e-03). ETA=12:52:16, max mem: 20.9 GB 
[11/27 22:09:02 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8192 s / batch. (data: 2.93e-04). ETA=12:28:13, max mem: 20.9 GB 
[11/27 22:09:54 visual_prompt]: Epoch 1 / 100: avg data time: 1.42e-01, avg batch time: 0.9762, average train loss: 1.5403
[11/27 22:10:49 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3091, average loss: 1.5201
[11/27 22:10:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/27 22:10:49 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/27 22:12:29 visual_prompt]: 	Training 100/553. train loss: 3.5035,	0.8480 s / batch. (data: 3.62e-04). ETA=12:52:20, max mem: 20.9 GB 
[11/27 22:14:06 visual_prompt]: 	Training 200/553. train loss: 0.0003,	0.8419 s / batch. (data: 5.49e-03). ETA=12:45:22, max mem: 20.9 GB 
[11/27 22:15:44 visual_prompt]: 	Training 300/553. train loss: 0.8492,	0.9195 s / batch. (data: 8.57e-02). ETA=13:54:25, max mem: 20.9 GB 
[11/27 22:17:20 visual_prompt]: 	Training 400/553. train loss: 4.5294,	0.8401 s / batch. (data: 3.21e-04). ETA=12:40:59, max mem: 20.9 GB 
[11/27 22:18:59 visual_prompt]: 	Training 500/553. train loss: 0.4798,	0.8190 s / batch. (data: 2.88e-04). ETA=12:20:26, max mem: 20.9 GB 
[11/27 22:19:48 visual_prompt]: Epoch 2 / 100: avg data time: 1.41e-01, avg batch time: 0.9746, average train loss: 2.0718
[11/27 22:20:44 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3114, average loss: 1.9715
[11/27 22:20:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.22	
[11/27 22:20:44 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/27 22:22:24 visual_prompt]: 	Training 100/553. train loss: 0.6295,	0.8428 s / batch. (data: 2.28e-02). ETA=12:39:52, max mem: 20.9 GB 
[11/27 22:24:02 visual_prompt]: 	Training 200/553. train loss: 0.9610,	1.3360 s / batch. (data: 4.94e-01). ETA=20:02:16, max mem: 20.9 GB 
[11/27 22:25:38 visual_prompt]: 	Training 300/553. train loss: 2.4749,	0.8208 s / batch. (data: 2.62e-04). ETA=12:17:14, max mem: 20.9 GB 
[11/27 22:27:16 visual_prompt]: 	Training 400/553. train loss: 0.2250,	0.8192 s / batch. (data: 3.02e-04). ETA=12:14:30, max mem: 20.9 GB 
[11/27 22:28:54 visual_prompt]: 	Training 500/553. train loss: 1.0738,	1.0118 s / batch. (data: 1.80e-01). ETA=15:05:29, max mem: 20.9 GB 
[11/27 22:29:44 visual_prompt]: Epoch 3 / 100: avg data time: 1.42e-01, avg batch time: 0.9758, average train loss: 2.3930
[11/27 22:30:39 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.3097, average loss: 2.9331
[11/27 22:30:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.74	
[11/27 22:30:39 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/27 22:32:21 visual_prompt]: 	Training 100/553. train loss: 2.0854,	0.8765 s / batch. (data: 2.85e-02). ETA=13:02:09, max mem: 20.9 GB 
[11/27 22:33:58 visual_prompt]: 	Training 200/553. train loss: 1.2467,	0.8330 s / batch. (data: 5.43e-03). ETA=12:21:54, max mem: 20.9 GB 
[11/27 22:35:36 visual_prompt]: 	Training 300/553. train loss: 0.7352,	1.2760 s / batch. (data: 4.21e-01). ETA=18:54:23, max mem: 20.9 GB 
[11/27 22:37:09 visual_prompt]: 	Training 400/553. train loss: 3.3312,	0.8503 s / batch. (data: 3.40e-04). ETA=12:34:28, max mem: 20.9 GB 
[11/27 22:38:47 visual_prompt]: 	Training 500/553. train loss: 13.1551,	3.2401 s / batch. (data: 2.41e+00). ETA=1 day, 23:49:42, max mem: 20.9 GB 
[11/27 22:39:40 visual_prompt]: Epoch 4 / 100: avg data time: 1.43e-01, avg batch time: 0.9777, average train loss: 3.2573
[11/27 22:40:35 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3108, average loss: 3.1964
[11/27 22:40:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.71	
[11/27 22:40:35 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/27 22:42:15 visual_prompt]: 	Training 100/553. train loss: 4.0519,	0.8620 s / batch. (data: 5.46e-03). ETA=12:41:13, max mem: 20.9 GB 
[11/27 22:43:52 visual_prompt]: 	Training 200/553. train loss: 8.2089,	1.1192 s / batch. (data: 2.88e-01). ETA=16:26:31, max mem: 20.9 GB 
[11/27 22:45:30 visual_prompt]: 	Training 300/553. train loss: 2.9179,	0.8506 s / batch. (data: 1.48e-02). ETA=12:28:21, max mem: 20.9 GB 
[11/27 22:47:06 visual_prompt]: 	Training 400/553. train loss: 3.8703,	0.8516 s / batch. (data: 3.39e-04). ETA=12:27:48, max mem: 20.9 GB 
[11/27 22:48:43 visual_prompt]: 	Training 500/553. train loss: 9.9950,	0.8386 s / batch. (data: 5.44e-03). ETA=12:14:57, max mem: 20.9 GB 
[11/27 22:49:35 visual_prompt]: Epoch 5 / 100: avg data time: 1.41e-01, avg batch time: 0.9754, average train loss: 6.9270
[11/27 22:50:30 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3107, average loss: 4.2445
[11/27 22:50:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.30	
[11/27 22:50:30 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/27 22:52:13 visual_prompt]: 	Training 100/553. train loss: 4.6621,	0.8363 s / batch. (data: 7.79e-04). ETA=12:10:52, max mem: 20.9 GB 
[11/27 22:53:49 visual_prompt]: 	Training 200/553. train loss: 21.5458,	0.8480 s / batch. (data: 7.99e-03). ETA=12:19:40, max mem: 20.9 GB 
[11/27 22:55:24 visual_prompt]: 	Training 300/553. train loss: 1.0072,	0.8215 s / batch. (data: 3.15e-04). ETA=11:55:08, max mem: 20.9 GB 
[11/27 22:57:06 visual_prompt]: 	Training 400/553. train loss: 11.3812,	0.8396 s / batch. (data: 1.56e-02). ETA=12:09:32, max mem: 20.9 GB 
[11/27 22:58:41 visual_prompt]: 	Training 500/553. train loss: 8.4550,	0.8406 s / batch. (data: 3.35e-04). ETA=12:09:01, max mem: 20.9 GB 
[11/27 22:59:31 visual_prompt]: Epoch 6 / 100: avg data time: 1.44e-01, avg batch time: 0.9778, average train loss: 6.8165
[11/27 23:00:26 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3094, average loss: 2.8051
[11/27 23:00:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.58	
[11/27 23:00:26 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/27 23:02:06 visual_prompt]: 	Training 100/553. train loss: 6.3153,	0.8316 s / batch. (data: 3.60e-04). ETA=11:59:02, max mem: 20.9 GB 
[11/27 23:03:43 visual_prompt]: 	Training 200/553. train loss: 0.6921,	0.8434 s / batch. (data: 5.44e-03). ETA=12:07:50, max mem: 20.9 GB 
[11/27 23:05:24 visual_prompt]: 	Training 300/553. train loss: 3.9191,	1.6400 s / batch. (data: 7.88e-01). ETA=23:32:37, max mem: 20.9 GB 
[11/27 23:07:01 visual_prompt]: 	Training 400/553. train loss: 2.8755,	1.9876 s / batch. (data: 1.17e+00). ETA=1 day, 4:28:44, max mem: 20.9 GB 
[11/27 23:08:37 visual_prompt]: 	Training 500/553. train loss: 3.8488,	0.8429 s / batch. (data: 3.44e-04). ETA=12:03:12, max mem: 20.9 GB 
[11/27 23:09:27 visual_prompt]: Epoch 7 / 100: avg data time: 1.43e-01, avg batch time: 0.9771, average train loss: 4.8161
[11/27 23:10:23 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3111, average loss: 4.5820
[11/27 23:10:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.39	
[11/27 23:10:23 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/27 23:12:03 visual_prompt]: 	Training 100/553. train loss: 3.1925,	0.8480 s / batch. (data: 5.94e-03). ETA=12:05:26, max mem: 20.9 GB 
[11/27 23:13:41 visual_prompt]: 	Training 200/553. train loss: 21.3880,	0.8480 s / batch. (data: 3.35e-04). ETA=12:04:01, max mem: 20.9 GB 
[11/27 23:15:19 visual_prompt]: 	Training 300/553. train loss: 1.0200,	0.8600 s / batch. (data: 2.96e-04). ETA=12:12:50, max mem: 20.9 GB 
[11/27 23:16:56 visual_prompt]: 	Training 400/553. train loss: 13.3662,	0.8696 s / batch. (data: 3.53e-03). ETA=12:19:34, max mem: 20.9 GB 
[11/27 23:18:33 visual_prompt]: 	Training 500/553. train loss: 0.2706,	0.8561 s / batch. (data: 1.83e-02). ETA=12:06:38, max mem: 20.9 GB 
[11/27 23:19:26 visual_prompt]: Epoch 8 / 100: avg data time: 1.46e-01, avg batch time: 0.9812, average train loss: 8.8876
[11/27 23:20:21 visual_prompt]: Inference (val):avg data time: 2.07e-04, avg batch time: 0.3108, average loss: 1.0892
[11/27 23:20:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.81	
[11/27 23:20:21 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/27 23:22:03 visual_prompt]: 	Training 100/553. train loss: 0.0001,	0.8180 s / batch. (data: 3.26e-04). ETA=11:32:12, max mem: 20.9 GB 
[11/27 23:23:39 visual_prompt]: 	Training 200/553. train loss: 3.8978,	0.8331 s / batch. (data: 3.31e-04). ETA=11:43:36, max mem: 20.9 GB 
[11/27 23:25:17 visual_prompt]: 	Training 300/553. train loss: 3.0510,	1.4513 s / batch. (data: 6.35e-01). ETA=20:23:19, max mem: 20.9 GB 
[11/27 23:26:55 visual_prompt]: 	Training 400/553. train loss: 1.7741,	0.8394 s / batch. (data: 3.25e-04). ETA=11:46:08, max mem: 20.9 GB 
[11/27 23:28:33 visual_prompt]: 	Training 500/553. train loss: 5.9535,	0.8519 s / batch. (data: 3.71e-04). ETA=11:55:17, max mem: 20.9 GB 
[11/27 23:29:23 visual_prompt]: Epoch 9 / 100: avg data time: 1.44e-01, avg batch time: 0.9792, average train loss: 7.6257
[11/27 23:30:19 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3103, average loss: 15.7628
[11/27 23:30:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.76	
[11/27 23:30:19 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/27 23:32:02 visual_prompt]: 	Training 100/553. train loss: 19.4007,	0.8480 s / batch. (data: 3.50e-04). ETA=11:49:48, max mem: 20.9 GB 
[11/27 23:33:37 visual_prompt]: 	Training 200/553. train loss: 0.5049,	0.8463 s / batch. (data: 1.05e-02). ETA=11:47:00, max mem: 20.9 GB 
[11/27 23:35:14 visual_prompt]: 	Training 300/553. train loss: 6.9126,	2.1720 s / batch. (data: 1.33e+00). ETA=1 day, 6:10:50, max mem: 20.9 GB 
[11/27 23:36:49 visual_prompt]: 	Training 400/553. train loss: 13.4654,	0.8584 s / batch. (data: 1.04e-02). ETA=11:54:15, max mem: 20.9 GB 
[11/27 23:38:27 visual_prompt]: 	Training 500/553. train loss: 2.0865,	0.8632 s / batch. (data: 5.44e-03). ETA=11:56:49, max mem: 20.9 GB 
[11/27 23:39:17 visual_prompt]: Epoch 10 / 100: avg data time: 1.40e-01, avg batch time: 0.9740, average train loss: 10.7126
[11/27 23:40:13 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3089, average loss: 5.2410
[11/27 23:40:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.73	
[11/27 23:40:13 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/27 23:41:57 visual_prompt]: 	Training 100/553. train loss: 12.9361,	0.8346 s / batch. (data: 1.05e-02). ETA=11:30:52, max mem: 20.9 GB 
[11/27 23:43:36 visual_prompt]: 	Training 200/553. train loss: 5.3582,	0.8480 s / batch. (data: 2.95e-04). ETA=11:40:35, max mem: 20.9 GB 
[11/27 23:45:14 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.0129 s / batch. (data: 1.19e+00). ETA=1 day, 3:39:38, max mem: 20.9 GB 
[11/27 23:46:49 visual_prompt]: 	Training 400/553. train loss: 7.4933,	0.8329 s / batch. (data: 2.60e-04). ETA=11:25:20, max mem: 20.9 GB 
[11/27 23:48:25 visual_prompt]: 	Training 500/553. train loss: 0.5588,	0.8203 s / batch. (data: 3.03e-04). ETA=11:13:34, max mem: 20.9 GB 
[11/27 23:49:15 visual_prompt]: Epoch 11 / 100: avg data time: 1.45e-01, avg batch time: 0.9793, average train loss: 8.1792
[11/27 23:50:11 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3102, average loss: 11.4596
[11/27 23:50:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.25	
[11/27 23:50:11 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/27 23:51:54 visual_prompt]: 	Training 100/553. train loss: 0.3006,	0.8680 s / batch. (data: 3.95e-02). ETA=11:50:34, max mem: 20.9 GB 
[11/27 23:53:32 visual_prompt]: 	Training 200/553. train loss: 5.7857,	0.8168 s / batch. (data: 3.27e-04). ETA=11:07:16, max mem: 20.9 GB 
[11/27 23:55:08 visual_prompt]: 	Training 300/553. train loss: 13.7108,	0.8189 s / batch. (data: 3.32e-04). ETA=11:07:38, max mem: 20.9 GB 
[11/27 23:56:46 visual_prompt]: 	Training 400/553. train loss: 19.6701,	0.8322 s / batch. (data: 3.14e-04). ETA=11:17:04, max mem: 20.9 GB 
[11/27 23:58:24 visual_prompt]: 	Training 500/553. train loss: 23.8542,	0.8434 s / batch. (data: 2.97e-04). ETA=11:24:47, max mem: 20.9 GB 
[11/27 23:59:13 visual_prompt]: Epoch 12 / 100: avg data time: 1.48e-01, avg batch time: 0.9812, average train loss: 15.1743
[11/28 00:00:09 visual_prompt]: Inference (val):avg data time: 5.04e-05, avg batch time: 0.3110, average loss: 90.2101
[11/28 00:00:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.14	
[11/28 00:00:09 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/28 00:01:53 visual_prompt]: 	Training 100/553. train loss: 1.5743,	0.8565 s / batch. (data: 1.56e-02). ETA=11:33:16, max mem: 20.9 GB 
[11/28 00:03:27 visual_prompt]: 	Training 200/553. train loss: 3.3038,	0.8452 s / batch. (data: 3.01e-04). ETA=11:22:43, max mem: 20.9 GB 
[11/28 00:05:05 visual_prompt]: 	Training 300/553. train loss: 8.8213,	1.5200 s / batch. (data: 6.82e-01). ETA=20:25:13, max mem: 20.9 GB 
[11/28 00:06:41 visual_prompt]: 	Training 400/553. train loss: 14.4938,	0.8640 s / batch. (data: 1.19e-02). ETA=11:34:59, max mem: 20.9 GB 
[11/28 00:08:19 visual_prompt]: 	Training 500/553. train loss: 3.1417,	0.8455 s / batch. (data: 2.55e-02). ETA=11:18:42, max mem: 20.9 GB 
[11/28 00:09:11 visual_prompt]: Epoch 13 / 100: avg data time: 1.44e-01, avg batch time: 0.9791, average train loss: 10.2507
[11/28 00:10:08 visual_prompt]: Inference (val):avg data time: 7.57e-04, avg batch time: 0.3098, average loss: 3.3825
[11/28 00:10:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.35	
[11/28 00:10:08 visual_prompt]: Best epoch 13: best metric: -3.383
[11/28 00:10:08 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/28 00:11:51 visual_prompt]: 	Training 100/553. train loss: 1.9361,	0.8529 s / batch. (data: 2.06e-02). ETA=11:22:27, max mem: 20.9 GB 
[11/28 00:13:28 visual_prompt]: 	Training 200/553. train loss: 0.0023,	0.9280 s / batch. (data: 7.35e-02). ETA=12:21:01, max mem: 20.9 GB 
[11/28 00:15:25 visual_prompt]: 	Training 300/553. train loss: 0.9843,	0.8440 s / batch. (data: 7.95e-03). ETA=11:12:31, max mem: 20.9 GB 
[11/28 00:17:02 visual_prompt]: 	Training 400/553. train loss: 11.0351,	0.8480 s / batch. (data: 3.23e-04). ETA=11:14:17, max mem: 20.9 GB 
[11/28 00:18:39 visual_prompt]: 	Training 500/553. train loss: 21.5263,	1.1971 s / batch. (data: 3.76e-01). ETA=15:49:57, max mem: 20.9 GB 
[11/28 00:19:29 visual_prompt]: Epoch 14 / 100: avg data time: 1.81e-01, avg batch time: 1.0141, average train loss: 9.0660
[11/28 00:20:24 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3121, average loss: 5.8439
[11/28 00:20:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.18	
[11/28 00:20:24 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/28 00:22:05 visual_prompt]: 	Training 100/553. train loss: 15.9422,	0.8325 s / batch. (data: 3.22e-04). ETA=10:58:28, max mem: 20.9 GB 
[11/28 00:23:40 visual_prompt]: 	Training 200/553. train loss: 24.2661,	0.8325 s / batch. (data: 3.04e-04). ETA=10:57:05, max mem: 20.9 GB 
[11/28 00:25:19 visual_prompt]: 	Training 300/553. train loss: 1.1437,	0.8542 s / batch. (data: 7.07e-04). ETA=11:12:45, max mem: 20.9 GB 
[11/28 00:26:53 visual_prompt]: 	Training 400/553. train loss: 1.0355,	0.8283 s / batch. (data: 3.59e-04). ETA=10:51:01, max mem: 20.9 GB 
[11/28 00:28:31 visual_prompt]: 	Training 500/553. train loss: 90.8781,	0.8493 s / batch. (data: 1.60e-02). ETA=11:06:07, max mem: 20.9 GB 
[11/28 00:29:22 visual_prompt]: Epoch 15 / 100: avg data time: 1.38e-01, avg batch time: 0.9727, average train loss: 17.6134
[11/28 00:30:17 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3108, average loss: 23.9815
[11/28 00:30:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.29	
[11/28 00:30:17 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/28 00:31:57 visual_prompt]: 	Training 100/553. train loss: 25.2271,	0.8176 s / batch. (data: 3.19e-04). ETA=10:39:08, max mem: 20.9 GB 
[11/28 00:33:35 visual_prompt]: 	Training 200/553. train loss: 25.7270,	0.8600 s / batch. (data: 1.60e-02). ETA=11:10:52, max mem: 20.9 GB 
[11/28 00:35:16 visual_prompt]: 	Training 300/553. train loss: 57.3220,	0.8165 s / batch. (data: 3.07e-04). ETA=10:35:32, max mem: 20.9 GB 
[11/28 00:36:54 visual_prompt]: 	Training 400/553. train loss: 5.9560,	0.8399 s / batch. (data: 3.08e-04). ETA=10:52:22, max mem: 20.9 GB 
[11/28 00:38:29 visual_prompt]: 	Training 500/553. train loss: 8.8824,	0.8590 s / batch. (data: 4.09e-02). ETA=11:05:50, max mem: 20.9 GB 
[11/28 00:39:21 visual_prompt]: Epoch 16 / 100: avg data time: 1.50e-01, avg batch time: 0.9833, average train loss: 14.8550
[11/28 00:40:17 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3103, average loss: 13.6446
[11/28 00:40:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.17	
[11/28 00:40:17 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/28 00:41:57 visual_prompt]: 	Training 100/553. train loss: 26.1969,	0.8114 s / batch. (data: 3.09e-04). ETA=10:26:47, max mem: 20.9 GB 
[11/28 00:43:35 visual_prompt]: 	Training 200/553. train loss: 56.1493,	0.8400 s / batch. (data: 2.89e-04). ETA=10:47:31, max mem: 20.9 GB 
[11/28 00:45:11 visual_prompt]: 	Training 300/553. train loss: 1.6608,	0.8399 s / batch. (data: 3.00e-04). ETA=10:46:03, max mem: 20.9 GB 
[11/28 00:46:47 visual_prompt]: 	Training 400/553. train loss: 58.0613,	0.9902 s / batch. (data: 1.73e-01). ETA=12:40:00, max mem: 20.9 GB 
[11/28 00:48:23 visual_prompt]: 	Training 500/553. train loss: 10.7328,	1.5249 s / batch. (data: 7.04e-01). ETA=19:27:50, max mem: 20.9 GB 
[11/28 00:49:15 visual_prompt]: Epoch 17 / 100: avg data time: 1.41e-01, avg batch time: 0.9737, average train loss: 18.0756
[11/28 00:50:11 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3096, average loss: 18.8841
[11/28 00:50:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.52	
[11/28 00:50:11 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/28 00:51:52 visual_prompt]: 	Training 100/553. train loss: 27.9208,	0.8440 s / batch. (data: 7.96e-03). ETA=10:44:13, max mem: 20.9 GB 
[11/28 00:53:32 visual_prompt]: 	Training 200/553. train loss: 27.2477,	0.8499 s / batch. (data: 1.10e-02). ETA=10:47:17, max mem: 20.9 GB 
[11/28 00:55:09 visual_prompt]: 	Training 300/553. train loss: 2.7293,	0.8400 s / batch. (data: 7.96e-03). ETA=10:38:22, max mem: 20.9 GB 
[11/28 00:56:46 visual_prompt]: 	Training 400/553. train loss: 2.6912,	0.8280 s / batch. (data: 2.77e-04). ETA=10:27:52, max mem: 20.9 GB 
[11/28 00:58:23 visual_prompt]: 	Training 500/553. train loss: 39.4254,	0.8520 s / batch. (data: 7.95e-03). ETA=10:44:39, max mem: 20.9 GB 
[11/28 00:59:13 visual_prompt]: Epoch 18 / 100: avg data time: 1.47e-01, avg batch time: 0.9800, average train loss: 21.2450
[11/28 01:00:08 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3117, average loss: 20.1407
[11/28 01:00:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.29	
[11/28 01:00:08 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/28 01:01:50 visual_prompt]: 	Training 100/553. train loss: 1.8607,	1.3691 s / batch. (data: 5.53e-01). ETA=17:12:25, max mem: 20.9 GB 
[11/28 01:03:28 visual_prompt]: 	Training 200/553. train loss: 2.2904,	0.8396 s / batch. (data: 3.00e-04). ETA=10:31:43, max mem: 20.9 GB 
[11/28 01:05:06 visual_prompt]: 	Training 300/553. train loss: 7.7977,	0.8487 s / batch. (data: 2.40e-02). ETA=10:37:11, max mem: 20.9 GB 
[11/28 01:06:45 visual_prompt]: 	Training 400/553. train loss: 4.5759,	0.8399 s / batch. (data: 7.61e-04). ETA=10:29:11, max mem: 20.9 GB 
[11/28 01:08:17 visual_prompt]: 	Training 500/553. train loss: 25.7462,	0.8320 s / batch. (data: 3.19e-04). ETA=10:21:51, max mem: 20.9 GB 
[11/28 01:09:08 visual_prompt]: Epoch 19 / 100: avg data time: 1.41e-01, avg batch time: 0.9752, average train loss: 13.8323
[11/28 01:10:03 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3089, average loss: 33.4310
[11/28 01:10:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.05	
[11/28 01:10:03 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/28 01:11:42 visual_prompt]: 	Training 100/553. train loss: 5.0464,	0.8460 s / batch. (data: 7.98e-03). ETA=10:30:11, max mem: 20.9 GB 
[11/28 01:13:20 visual_prompt]: 	Training 200/553. train loss: 8.0982,	0.8191 s / batch. (data: 3.03e-04). ETA=10:08:46, max mem: 20.9 GB 
[11/28 01:14:58 visual_prompt]: 	Training 300/553. train loss: 6.0087,	0.8520 s / batch. (data: 3.11e-04). ETA=10:31:48, max mem: 20.9 GB 
[11/28 01:16:34 visual_prompt]: 	Training 400/553. train loss: 1.3213,	0.8560 s / batch. (data: 3.09e-04). ETA=10:33:20, max mem: 20.9 GB 
[11/28 01:18:10 visual_prompt]: 	Training 500/553. train loss: 14.1554,	0.8254 s / batch. (data: 5.50e-03). ETA=10:09:17, max mem: 20.9 GB 
[11/28 01:19:02 visual_prompt]: Epoch 20 / 100: avg data time: 1.42e-01, avg batch time: 0.9748, average train loss: 15.2412
[11/28 01:19:58 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3096, average loss: 41.8419
[11/28 01:19:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.45	
[11/28 01:19:58 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[11/28 01:21:40 visual_prompt]: 	Training 100/553. train loss: 1.8224,	0.8400 s / batch. (data: 3.21e-04). ETA=10:17:57, max mem: 20.9 GB 
[11/28 01:23:16 visual_prompt]: 	Training 200/553. train loss: 2.8603,	0.8183 s / batch. (data: 3.19e-04). ETA=10:00:39, max mem: 20.9 GB 
[11/28 01:24:53 visual_prompt]: 	Training 300/553. train loss: 6.2862,	0.8479 s / batch. (data: 1.19e-02). ETA=10:20:58, max mem: 20.9 GB 
[11/28 01:26:29 visual_prompt]: 	Training 400/553. train loss: 6.4808,	0.8265 s / batch. (data: 5.42e-03). ETA=10:03:54, max mem: 20.9 GB 
[11/28 01:28:06 visual_prompt]: 	Training 500/553. train loss: 13.3490,	0.8481 s / batch. (data: 2.74e-04). ETA=10:18:13, max mem: 20.9 GB 
[11/28 01:28:56 visual_prompt]: Epoch 21 / 100: avg data time: 1.39e-01, avg batch time: 0.9725, average train loss: 12.1980
[11/28 01:29:51 visual_prompt]: Inference (val):avg data time: 2.78e-04, avg batch time: 0.3116, average loss: 3.1222
[11/28 01:29:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.84	
[11/28 01:29:51 visual_prompt]: Best epoch 21: best metric: -3.122
[11/28 01:29:51 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[11/28 01:31:31 visual_prompt]: 	Training 100/553. train loss: 0.8268,	0.8320 s / batch. (data: 7.96e-03). ETA=10:04:24, max mem: 20.9 GB 
[11/28 01:33:08 visual_prompt]: 	Training 200/553. train loss: 16.9778,	0.8206 s / batch. (data: 2.72e-04). ETA=9:54:47, max mem: 20.9 GB 
[11/28 01:34:43 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8400 s / batch. (data: 3.98e-03). ETA=10:07:23, max mem: 20.9 GB 
[11/28 01:36:21 visual_prompt]: 	Training 400/553. train loss: 1.0901,	0.8214 s / batch. (data: 3.13e-04). ETA=9:52:33, max mem: 20.9 GB 
[11/28 01:37:59 visual_prompt]: 	Training 500/553. train loss: 1.1772,	0.8519 s / batch. (data: 3.47e-04). ETA=10:13:12, max mem: 20.9 GB 
[11/28 01:38:51 visual_prompt]: Epoch 22 / 100: avg data time: 1.42e-01, avg batch time: 0.9759, average train loss: 7.3076
[11/28 01:39:45 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3094, average loss: 2.9483
[11/28 01:39:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.49	
[11/28 01:39:45 visual_prompt]: Best epoch 22: best metric: -2.948
[11/28 01:39:45 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[11/28 01:41:27 visual_prompt]: 	Training 100/553. train loss: 1.9362,	0.8320 s / batch. (data: 5.47e-03). ETA=9:56:43, max mem: 20.9 GB 
[11/28 01:43:05 visual_prompt]: 	Training 200/553. train loss: 60.2175,	0.8320 s / batch. (data: 1.20e-02). ETA=9:55:21, max mem: 20.9 GB 
[11/28 01:44:43 visual_prompt]: 	Training 300/553. train loss: 0.7793,	0.8564 s / batch. (data: 6.21e-03). ETA=10:11:24, max mem: 20.9 GB 
[11/28 01:46:19 visual_prompt]: 	Training 400/553. train loss: 8.3411,	0.8360 s / batch. (data: 3.06e-04). ETA=9:55:26, max mem: 20.9 GB 
[11/28 01:47:55 visual_prompt]: 	Training 500/553. train loss: 1.2086,	0.8482 s / batch. (data: 3.14e-04). ETA=10:02:41, max mem: 20.9 GB 
[11/28 01:48:45 visual_prompt]: Epoch 23 / 100: avg data time: 1.43e-01, avg batch time: 0.9752, average train loss: 12.7887
[11/28 01:49:40 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3102, average loss: 2.1401
[11/28 01:49:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.53	
[11/28 01:49:40 visual_prompt]: Best epoch 23: best metric: -2.140
[11/28 01:49:40 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[11/28 01:51:19 visual_prompt]: 	Training 100/553. train loss: 9.5365,	0.9804 s / batch. (data: 1.41e-01). ETA=11:34:06, max mem: 20.9 GB 
[11/28 01:52:56 visual_prompt]: 	Training 200/553. train loss: 9.0446,	0.8186 s / batch. (data: 2.90e-04). ETA=9:38:13, max mem: 20.9 GB 
[11/28 01:54:33 visual_prompt]: 	Training 300/553. train loss: 2.1329,	0.8760 s / batch. (data: 4.62e-02). ETA=10:17:18, max mem: 20.9 GB 
[11/28 01:56:10 visual_prompt]: 	Training 400/553. train loss: 12.1201,	0.8377 s / batch. (data: 7.96e-03). ETA=9:48:54, max mem: 20.9 GB 
[11/28 01:57:49 visual_prompt]: 	Training 500/553. train loss: 47.7156,	0.8111 s / batch. (data: 3.51e-04). ETA=9:28:51, max mem: 20.9 GB 
[11/28 01:58:40 visual_prompt]: Epoch 24 / 100: avg data time: 1.42e-01, avg batch time: 0.9763, average train loss: 12.5920
[11/28 01:59:36 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3102, average loss: 7.1258
[11/28 01:59:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.65	
[11/28 01:59:36 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[11/28 02:01:20 visual_prompt]: 	Training 100/553. train loss: 1.2235,	0.8613 s / batch. (data: 1.55e-02). ETA=10:01:51, max mem: 20.9 GB 
[11/28 02:02:55 visual_prompt]: 	Training 200/553. train loss: 8.7227,	0.8440 s / batch. (data: 7.97e-03). ETA=9:48:22, max mem: 20.9 GB 
[11/28 02:04:32 visual_prompt]: 	Training 300/553. train loss: 2.6724,	0.8215 s / batch. (data: 3.42e-04). ETA=9:31:17, max mem: 20.9 GB 
[11/28 02:06:09 visual_prompt]: 	Training 400/553. train loss: 1.7222,	1.2560 s / batch. (data: 4.32e-01). ETA=14:31:25, max mem: 20.9 GB 
[11/28 02:07:47 visual_prompt]: 	Training 500/553. train loss: 17.6020,	1.6524 s / batch. (data: 8.22e-01). ETA=19:03:40, max mem: 20.9 GB 
[11/28 02:08:38 visual_prompt]: Epoch 25 / 100: avg data time: 1.46e-01, avg batch time: 0.9799, average train loss: 16.1932
[11/28 02:09:34 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3095, average loss: 15.1664
[11/28 02:09:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.57	
[11/28 02:09:34 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[11/28 02:11:16 visual_prompt]: 	Training 100/553. train loss: 10.9355,	0.8171 s / batch. (data: 3.15e-04). ETA=9:23:27, max mem: 20.9 GB 
[11/28 02:12:54 visual_prompt]: 	Training 200/553. train loss: 48.2566,	1.6719 s / batch. (data: 8.34e-01). ETA=19:10:07, max mem: 20.9 GB 
[11/28 02:14:33 visual_prompt]: 	Training 300/553. train loss: 0.4749,	0.8248 s / batch. (data: 2.95e-04). ETA=9:26:02, max mem: 20.9 GB 
[11/28 02:16:10 visual_prompt]: 	Training 400/553. train loss: 1.9479,	0.8255 s / batch. (data: 2.97e-04). ETA=9:25:07, max mem: 20.9 GB 
[11/28 02:17:45 visual_prompt]: 	Training 500/553. train loss: 38.6370,	0.8323 s / batch. (data: 3.14e-04). ETA=9:28:23, max mem: 20.9 GB 
[11/28 02:18:36 visual_prompt]: Epoch 26 / 100: avg data time: 1.46e-01, avg batch time: 0.9802, average train loss: 9.0953
[11/28 02:19:31 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3115, average loss: 13.0650
[11/28 02:19:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.30	
[11/28 02:19:31 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[11/28 02:21:13 visual_prompt]: 	Training 100/553. train loss: 12.2426,	0.8358 s / batch. (data: 5.43e-03). ETA=9:28:39, max mem: 20.9 GB 
[11/28 02:22:50 visual_prompt]: 	Training 200/553. train loss: 53.9378,	0.8962 s / batch. (data: 7.35e-02). ETA=10:08:14, max mem: 20.9 GB 
[11/28 02:24:27 visual_prompt]: 	Training 300/553. train loss: 9.5595,	0.8189 s / batch. (data: 3.17e-04). ETA=9:14:25, max mem: 20.9 GB 
[11/28 02:26:06 visual_prompt]: 	Training 400/553. train loss: 0.7024,	0.8439 s / batch. (data: 2.56e-02). ETA=9:29:57, max mem: 20.9 GB 
[11/28 02:27:44 visual_prompt]: 	Training 500/553. train loss: 2.0934,	0.8338 s / batch. (data: 3.04e-04). ETA=9:21:44, max mem: 20.9 GB 
[11/28 02:28:32 visual_prompt]: Epoch 27 / 100: avg data time: 1.45e-01, avg batch time: 0.9782, average train loss: 9.9040
[11/28 02:29:28 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3105, average loss: 7.5517
[11/28 02:29:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.10	
[11/28 02:29:28 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[11/28 02:31:08 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8280 s / batch. (data: 3.35e-04). ETA=9:15:43, max mem: 20.9 GB 
[11/28 02:32:46 visual_prompt]: 	Training 200/553. train loss: 24.6397,	0.8352 s / batch. (data: 1.71e-02). ETA=9:19:08, max mem: 20.9 GB 
[11/28 02:34:25 visual_prompt]: 	Training 300/553. train loss: 27.1145,	1.5041 s / batch. (data: 6.76e-01). ETA=16:44:27, max mem: 20.9 GB 
[11/28 02:36:01 visual_prompt]: 	Training 400/553. train loss: 6.8131,	0.8440 s / batch. (data: 2.74e-04). ETA=9:22:14, max mem: 20.9 GB 
[11/28 02:37:38 visual_prompt]: 	Training 500/553. train loss: 21.5959,	0.8400 s / batch. (data: 7.73e-04). ETA=9:18:10, max mem: 20.9 GB 
[11/28 02:38:28 visual_prompt]: Epoch 28 / 100: avg data time: 1.44e-01, avg batch time: 0.9778, average train loss: 11.7015
[11/28 02:39:24 visual_prompt]: Inference (val):avg data time: 9.73e-05, avg batch time: 0.3103, average loss: 6.2508
[11/28 02:39:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.10	
[11/28 02:39:24 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[11/28 02:41:12 visual_prompt]: 	Training 100/553. train loss: 15.6510,	0.8280 s / batch. (data: 2.70e-04). ETA=9:08:04, max mem: 20.9 GB 
[11/28 02:42:49 visual_prompt]: 	Training 200/553. train loss: 8.5501,	1.7611 s / batch. (data: 9.39e-01). ETA=19:22:46, max mem: 20.9 GB 
[11/28 02:44:23 visual_prompt]: 	Training 300/553. train loss: 3.5969,	0.8361 s / batch. (data: 3.04e-04). ETA=9:10:38, max mem: 20.9 GB 
[11/28 02:45:58 visual_prompt]: 	Training 400/553. train loss: 21.7936,	0.8300 s / batch. (data: 3.65e-04). ETA=9:05:16, max mem: 20.9 GB 
[11/28 02:47:36 visual_prompt]: 	Training 500/553. train loss: 11.5920,	0.8354 s / batch. (data: 5.12e-04). ETA=9:07:23, max mem: 20.9 GB 
[11/28 02:48:27 visual_prompt]: Epoch 29 / 100: avg data time: 1.48e-01, avg batch time: 0.9815, average train loss: 8.9642
[11/28 02:49:23 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3110, average loss: 2.9152
[11/28 02:49:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.47	
[11/28 02:49:23 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[11/28 02:51:03 visual_prompt]: 	Training 100/553. train loss: 3.1779,	0.8514 s / batch. (data: 1.14e-02). ETA=9:15:43, max mem: 20.9 GB 
[11/28 02:52:41 visual_prompt]: 	Training 200/553. train loss: 5.0161,	0.8323 s / batch. (data: 1.17e-02). ETA=9:01:53, max mem: 20.9 GB 
[11/28 02:54:17 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.2544 s / batch. (data: 4.17e-01). ETA=13:34:33, max mem: 20.9 GB 
[11/28 02:55:56 visual_prompt]: 	Training 400/553. train loss: 4.8886,	0.9720 s / batch. (data: 1.13e-01). ETA=10:29:35, max mem: 20.9 GB 
[11/28 02:57:32 visual_prompt]: 	Training 500/553. train loss: 2.4504,	1.3010 s / batch. (data: 4.68e-01). ETA=14:00:32, max mem: 20.9 GB 
[11/28 02:58:25 visual_prompt]: Epoch 30 / 100: avg data time: 1.46e-01, avg batch time: 0.9792, average train loss: 12.4585
[11/28 02:59:20 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3115, average loss: 1.8207
[11/28 02:59:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.72	
[11/28 02:59:20 visual_prompt]: Best epoch 30: best metric: -1.821
[11/28 02:59:20 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[11/28 03:01:03 visual_prompt]: 	Training 100/553. train loss: 9.1074,	0.8400 s / batch. (data: 3.23e-04). ETA=9:00:32, max mem: 20.9 GB 
[11/28 03:02:42 visual_prompt]: 	Training 200/553. train loss: 8.3460,	0.8604 s / batch. (data: 3.24e-02). ETA=9:12:13, max mem: 20.9 GB 
[11/28 03:04:17 visual_prompt]: 	Training 300/553. train loss: 14.9731,	0.8360 s / batch. (data: 2.96e-04). ETA=8:55:11, max mem: 20.9 GB 
[11/28 03:05:54 visual_prompt]: 	Training 400/553. train loss: 5.0719,	1.2823 s / batch. (data: 4.38e-01). ETA=13:38:44, max mem: 20.9 GB 
[11/28 03:07:31 visual_prompt]: 	Training 500/553. train loss: 2.2913,	0.8329 s / batch. (data: 3.22e-04). ETA=8:50:26, max mem: 20.9 GB 
[11/28 03:08:21 visual_prompt]: Epoch 31 / 100: avg data time: 1.44e-01, avg batch time: 0.9779, average train loss: 9.2928
[11/28 03:09:17 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3094, average loss: 3.2092
[11/28 03:09:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.53	
[11/28 03:09:17 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[11/28 03:11:00 visual_prompt]: 	Training 100/553. train loss: 0.6588,	0.8402 s / batch. (data: 2.79e-04). ETA=8:52:55, max mem: 20.9 GB 
[11/28 03:12:37 visual_prompt]: 	Training 200/553. train loss: 32.1023,	0.8850 s / batch. (data: 1.10e-02). ETA=9:19:52, max mem: 20.9 GB 
[11/28 03:14:17 visual_prompt]: 	Training 300/553. train loss: 7.0660,	0.8173 s / batch. (data: 3.13e-04). ETA=8:35:40, max mem: 20.9 GB 
[11/28 03:15:54 visual_prompt]: 	Training 400/553. train loss: 4.8223,	0.8320 s / batch. (data: 2.91e-04). ETA=8:43:33, max mem: 20.9 GB 
[11/28 03:17:29 visual_prompt]: 	Training 500/553. train loss: 10.7617,	0.8400 s / batch. (data: 3.02e-04). ETA=8:47:12, max mem: 20.9 GB 
[11/28 03:18:18 visual_prompt]: Epoch 32 / 100: avg data time: 1.44e-01, avg batch time: 0.9776, average train loss: 9.7804
[11/28 03:19:13 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3110, average loss: 3.2846
[11/28 03:19:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.36	
[11/28 03:19:13 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[11/28 03:20:53 visual_prompt]: 	Training 100/553. train loss: 34.7115,	0.8362 s / batch. (data: 1.20e-02). ETA=8:42:41, max mem: 20.9 GB 
[11/28 03:22:32 visual_prompt]: 	Training 200/553. train loss: 38.2422,	1.0240 s / batch. (data: 1.86e-01). ETA=10:38:22, max mem: 20.9 GB 
[11/28 03:24:09 visual_prompt]: 	Training 300/553. train loss: 14.4613,	0.8377 s / batch. (data: 1.64e-02). ETA=8:40:49, max mem: 20.9 GB 
[11/28 03:25:47 visual_prompt]: 	Training 400/553. train loss: 5.4463,	0.8331 s / batch. (data: 1.17e-02). ETA=8:36:33, max mem: 20.9 GB 
[11/28 03:27:24 visual_prompt]: 	Training 500/553. train loss: 7.4308,	0.8546 s / batch. (data: 5.44e-03). ETA=8:48:27, max mem: 20.9 GB 
[11/28 03:28:14 visual_prompt]: Epoch 33 / 100: avg data time: 1.45e-01, avg batch time: 0.9785, average train loss: 19.5149
[11/28 03:29:10 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3101, average loss: 24.6686
[11/28 03:29:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.60	
[11/28 03:29:10 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[11/28 03:30:53 visual_prompt]: 	Training 100/553. train loss: 1.7167,	0.8341 s / batch. (data: 3.36e-04). ETA=8:33:41, max mem: 20.9 GB 
[11/28 03:32:29 visual_prompt]: 	Training 200/553. train loss: 14.8736,	0.8438 s / batch. (data: 1.05e-02). ETA=8:38:14, max mem: 20.9 GB 
[11/28 03:34:05 visual_prompt]: 	Training 300/553. train loss: 20.2678,	0.8560 s / batch. (data: 1.19e-02). ETA=8:44:18, max mem: 20.9 GB 
[11/28 03:35:43 visual_prompt]: 	Training 400/553. train loss: 8.5092,	0.8721 s / batch. (data: 2.94e-04). ETA=8:52:41, max mem: 20.9 GB 
[11/28 03:37:20 visual_prompt]: 	Training 500/553. train loss: 3.7622,	1.3040 s / batch. (data: 4.64e-01). ETA=13:14:22, max mem: 20.9 GB 
[11/28 03:38:10 visual_prompt]: Epoch 34 / 100: avg data time: 1.42e-01, avg batch time: 0.9762, average train loss: 12.8303
[11/28 03:39:05 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3109, average loss: 1.4369
[11/28 03:39:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.19	
[11/28 03:39:05 visual_prompt]: Best epoch 34: best metric: -1.437
[11/28 03:39:05 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[11/28 03:40:48 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8301 s / batch. (data: 1.17e-02). ETA=8:23:35, max mem: 20.9 GB 
[11/28 03:42:27 visual_prompt]: 	Training 200/553. train loss: 27.4742,	0.8527 s / batch. (data: 2.46e-02). ETA=8:35:49, max mem: 20.9 GB 
[11/28 03:44:01 visual_prompt]: 	Training 300/553. train loss: 29.2407,	0.8400 s / batch. (data: 2.95e-04). ETA=8:26:47, max mem: 20.9 GB 
[11/28 03:45:37 visual_prompt]: 	Training 400/553. train loss: 15.7028,	0.8350 s / batch. (data: 7.34e-04). ETA=8:22:23, max mem: 20.9 GB 
[11/28 03:47:12 visual_prompt]: 	Training 500/553. train loss: 4.1077,	0.8607 s / batch. (data: 3.39e-04). ETA=8:36:23, max mem: 20.9 GB 
[11/28 03:48:04 visual_prompt]: Epoch 35 / 100: avg data time: 1.40e-01, avg batch time: 0.9741, average train loss: 11.6603
[11/28 03:49:00 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3135, average loss: 23.6302
[11/28 03:49:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.73	
[11/28 03:49:00 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[11/28 03:50:41 visual_prompt]: 	Training 100/553. train loss: 7.9954,	0.8560 s / batch. (data: 7.94e-03). ETA=8:31:23, max mem: 20.9 GB 
[11/28 03:52:22 visual_prompt]: 	Training 200/553. train loss: 13.3117,	0.8445 s / batch. (data: 2.44e-02). ETA=8:23:05, max mem: 20.9 GB 
[11/28 03:54:04 visual_prompt]: 	Training 300/553. train loss: 0.1916,	0.8400 s / batch. (data: 3.00e-04). ETA=8:19:01, max mem: 20.9 GB 
[11/28 03:55:42 visual_prompt]: 	Training 400/553. train loss: 0.6452,	0.8466 s / batch. (data: 3.29e-04). ETA=8:21:32, max mem: 20.9 GB 
[11/28 03:57:21 visual_prompt]: 	Training 500/553. train loss: 0.7493,	0.8950 s / batch. (data: 7.67e-02). ETA=8:48:41, max mem: 20.9 GB 
[11/28 03:58:10 visual_prompt]: Epoch 36 / 100: avg data time: 1.61e-01, avg batch time: 0.9946, average train loss: 8.5697
[11/28 03:59:07 visual_prompt]: Inference (val):avg data time: 2.07e-04, avg batch time: 0.3104, average loss: 18.4189
[11/28 03:59:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.89	
[11/28 03:59:07 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[11/28 04:00:49 visual_prompt]: 	Training 100/553. train loss: 1.4602,	0.8679 s / batch. (data: 4.76e-04). ETA=8:30:28, max mem: 20.9 GB 
[11/28 04:02:25 visual_prompt]: 	Training 200/553. train loss: 15.3461,	0.8521 s / batch. (data: 7.96e-03). ETA=8:19:46, max mem: 20.9 GB 
[11/28 04:04:02 visual_prompt]: 	Training 300/553. train loss: 15.0805,	1.3146 s / batch. (data: 4.85e-01). ETA=12:48:53, max mem: 20.9 GB 
[11/28 04:05:43 visual_prompt]: 	Training 400/553. train loss: 8.8508,	1.5840 s / batch. (data: 7.56e-01). ETA=15:23:46, max mem: 20.9 GB 
[11/28 04:07:16 visual_prompt]: 	Training 500/553. train loss: 16.4601,	0.9960 s / batch. (data: 1.49e-01). ETA=9:39:12, max mem: 20.9 GB 
[11/28 04:08:08 visual_prompt]: Epoch 37 / 100: avg data time: 1.45e-01, avg batch time: 0.9789, average train loss: 10.1057
[11/28 04:09:04 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3109, average loss: 0.8617
[11/28 04:09:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.60	
[11/28 04:09:04 visual_prompt]: Best epoch 37: best metric: -0.862
[11/28 04:09:04 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[11/28 04:10:43 visual_prompt]: 	Training 100/553. train loss: 2.4563,	0.8400 s / batch. (data: 3.13e-04). ETA=8:06:21, max mem: 20.9 GB 
[11/28 04:12:20 visual_prompt]: 	Training 200/553. train loss: 1.2625,	0.8194 s / batch. (data: 3.21e-04). ETA=7:53:04, max mem: 20.9 GB 
[11/28 04:13:59 visual_prompt]: 	Training 300/553. train loss: 0.6433,	0.8252 s / batch. (data: 3.14e-04). ETA=7:55:00, max mem: 20.9 GB 
[11/28 04:15:34 visual_prompt]: 	Training 400/553. train loss: 0.0056,	0.8474 s / batch. (data: 2.81e-02). ETA=8:06:24, max mem: 20.9 GB 
[11/28 04:17:13 visual_prompt]: 	Training 500/553. train loss: 10.5995,	0.8240 s / batch. (data: 3.29e-04). ETA=7:51:34, max mem: 20.9 GB 
[11/28 04:18:02 visual_prompt]: Epoch 38 / 100: avg data time: 1.38e-01, avg batch time: 0.9732, average train loss: 9.1150
[11/28 04:18:57 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3106, average loss: 3.2880
[11/28 04:18:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.51	
[11/28 04:18:57 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[11/28 04:20:36 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8433 s / batch. (data: 7.96e-03). ETA=8:00:30, max mem: 20.9 GB 
[11/28 04:22:16 visual_prompt]: 	Training 200/553. train loss: 6.0318,	0.8308 s / batch. (data: 3.10e-04). ETA=7:52:00, max mem: 20.9 GB 
[11/28 04:23:56 visual_prompt]: 	Training 300/553. train loss: 1.6360,	0.8543 s / batch. (data: 1.42e-02). ETA=8:03:54, max mem: 20.9 GB 
[11/28 04:25:30 visual_prompt]: 	Training 400/553. train loss: 0.5821,	0.8290 s / batch. (data: 2.94e-04). ETA=7:48:12, max mem: 20.9 GB 
[11/28 04:27:08 visual_prompt]: 	Training 500/553. train loss: 1.3739,	1.3240 s / batch. (data: 4.85e-01). ETA=12:25:32, max mem: 20.9 GB 
[11/28 04:27:57 visual_prompt]: Epoch 39 / 100: avg data time: 1.42e-01, avg batch time: 0.9761, average train loss: 5.6180
[11/28 04:28:52 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3087, average loss: 1.1674
[11/28 04:28:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.95	
[11/28 04:28:52 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[11/28 04:30:33 visual_prompt]: 	Training 100/553. train loss: 16.0991,	0.8312 s / batch. (data: 3.07e-04). ETA=7:45:57, max mem: 20.9 GB 
[11/28 04:32:09 visual_prompt]: 	Training 200/553. train loss: 6.9398,	0.8440 s / batch. (data: 5.43e-03). ETA=7:51:40, max mem: 20.9 GB 
[11/28 04:33:47 visual_prompt]: 	Training 300/553. train loss: 4.2776,	0.8274 s / batch. (data: 3.33e-04). ETA=7:41:03, max mem: 20.9 GB 
[11/28 04:35:25 visual_prompt]: 	Training 400/553. train loss: 7.2788,	0.8640 s / batch. (data: 8.96e-04). ETA=8:00:00, max mem: 20.9 GB 
[11/28 04:37:01 visual_prompt]: 	Training 500/553. train loss: 0.1957,	0.8440 s / batch. (data: 3.34e-04). ETA=7:47:28, max mem: 20.9 GB 
[11/28 04:37:53 visual_prompt]: Epoch 40 / 100: avg data time: 1.43e-01, avg batch time: 0.9779, average train loss: 8.9193
[11/28 04:38:47 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3108, average loss: 6.3768
[11/28 04:38:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.94	
[11/28 04:38:47 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[11/28 04:40:33 visual_prompt]: 	Training 100/553. train loss: 2.3122,	0.8244 s / batch. (data: 8.40e-03). ETA=7:34:32, max mem: 20.9 GB 
[11/28 04:42:12 visual_prompt]: 	Training 200/553. train loss: 1.0709,	0.8400 s / batch. (data: 2.96e-04). ETA=7:41:42, max mem: 20.9 GB 
[11/28 04:43:47 visual_prompt]: 	Training 300/553. train loss: 1.7893,	0.8199 s / batch. (data: 3.02e-04). ETA=7:29:19, max mem: 20.9 GB 
[11/28 04:45:22 visual_prompt]: 	Training 400/553. train loss: 0.7781,	0.8320 s / batch. (data: 2.88e-04). ETA=7:34:33, max mem: 20.9 GB 
[11/28 04:46:56 visual_prompt]: 	Training 500/553. train loss: 2.0934,	0.8320 s / batch. (data: 3.17e-04). ETA=7:33:09, max mem: 20.9 GB 
[11/28 04:47:45 visual_prompt]: Epoch 41 / 100: avg data time: 1.37e-01, avg batch time: 0.9715, average train loss: 9.9561
[11/28 04:48:40 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3104, average loss: 6.7181
[11/28 04:48:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.14	
[11/28 04:48:40 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[11/28 04:50:20 visual_prompt]: 	Training 100/553. train loss: 2.5553,	0.8170 s / batch. (data: 2.92e-04). ETA=7:22:53, max mem: 20.9 GB 
[11/28 04:51:58 visual_prompt]: 	Training 200/553. train loss: 2.2946,	0.8480 s / batch. (data: 3.23e-04). ETA=7:38:17, max mem: 20.9 GB 
[11/28 04:53:36 visual_prompt]: 	Training 300/553. train loss: 44.8118,	0.8194 s / batch. (data: 3.10e-04). ETA=7:21:27, max mem: 20.9 GB 
[11/28 04:55:13 visual_prompt]: 	Training 400/553. train loss: 12.1536,	0.8194 s / batch. (data: 3.29e-04). ETA=7:20:07, max mem: 20.9 GB 
[11/28 04:56:50 visual_prompt]: 	Training 500/553. train loss: 0.0021,	0.8297 s / batch. (data: 2.80e-04). ETA=7:24:15, max mem: 20.9 GB 
[11/28 04:57:42 visual_prompt]: Epoch 42 / 100: avg data time: 1.44e-01, avg batch time: 0.9794, average train loss: 6.4853
[11/28 04:58:37 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3104, average loss: 7.4987
[11/28 04:58:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.33	
[11/28 04:58:37 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[11/28 05:00:21 visual_prompt]: 	Training 100/553. train loss: 0.5445,	0.8367 s / batch. (data: 5.91e-03). ETA=7:25:52, max mem: 20.9 GB 
[11/28 05:01:57 visual_prompt]: 	Training 200/553. train loss: 0.9264,	0.8192 s / batch. (data: 3.30e-04). ETA=7:15:09, max mem: 20.9 GB 
[11/28 05:03:32 visual_prompt]: 	Training 300/553. train loss: 5.0809,	0.8399 s / batch. (data: 3.12e-04). ETA=7:24:47, max mem: 20.9 GB 
[11/28 05:05:07 visual_prompt]: 	Training 400/553. train loss: 1.0758,	0.8320 s / batch. (data: 3.01e-04). ETA=7:19:12, max mem: 20.9 GB 
[11/28 05:06:46 visual_prompt]: 	Training 500/553. train loss: 1.8287,	0.8281 s / batch. (data: 5.42e-03). ETA=7:15:46, max mem: 20.9 GB 
[11/28 05:07:38 visual_prompt]: Epoch 43 / 100: avg data time: 1.44e-01, avg batch time: 0.9772, average train loss: 3.7523
[11/28 05:08:33 visual_prompt]: Inference (val):avg data time: 3.07e-04, avg batch time: 0.3092, average loss: 1.0851
[11/28 05:08:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.34	
[11/28 05:08:33 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[11/28 05:10:14 visual_prompt]: 	Training 100/553. train loss: 1.5960,	1.5346 s / batch. (data: 6.99e-01). ETA=13:23:39, max mem: 20.9 GB 
[11/28 05:11:53 visual_prompt]: 	Training 200/553. train loss: 8.2660,	0.8329 s / batch. (data: 3.00e-04). ETA=7:14:47, max mem: 20.9 GB 
[11/28 05:13:28 visual_prompt]: 	Training 300/553. train loss: 4.5938,	0.8280 s / batch. (data: 3.12e-04). ETA=7:10:52, max mem: 20.9 GB 
[11/28 05:15:05 visual_prompt]: 	Training 400/553. train loss: 15.1157,	0.8320 s / batch. (data: 3.50e-04). ETA=7:11:32, max mem: 20.9 GB 
[11/28 05:16:43 visual_prompt]: 	Training 500/553. train loss: 3.6249,	0.8440 s / batch. (data: 3.16e-04). ETA=7:16:21, max mem: 20.9 GB 
[11/28 05:17:33 visual_prompt]: Epoch 44 / 100: avg data time: 1.43e-01, avg batch time: 0.9774, average train loss: 10.5993
[11/28 05:18:28 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.3097, average loss: 16.9967
[11/28 05:18:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.53	
[11/28 05:18:28 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[11/28 05:20:10 visual_prompt]: 	Training 100/553. train loss: 10.2564,	0.8596 s / batch. (data: 6.77e-04). ETA=7:22:12, max mem: 20.9 GB 
[11/28 05:21:43 visual_prompt]: 	Training 200/553. train loss: 6.4706,	0.8473 s / batch. (data: 2.72e-04). ETA=7:14:28, max mem: 20.9 GB 
[11/28 05:23:21 visual_prompt]: 	Training 300/553. train loss: 8.1185,	0.8519 s / batch. (data: 3.00e-04). ETA=7:15:26, max mem: 20.9 GB 
[11/28 05:24:57 visual_prompt]: 	Training 400/553. train loss: 5.1428,	0.8424 s / batch. (data: 5.41e-03). ETA=7:09:09, max mem: 20.9 GB 
[11/28 05:26:36 visual_prompt]: 	Training 500/553. train loss: 4.8661,	0.8318 s / batch. (data: 3.19e-04). ETA=7:02:22, max mem: 20.9 GB 
[11/28 05:27:27 visual_prompt]: Epoch 45 / 100: avg data time: 1.39e-01, avg batch time: 0.9739, average train loss: 5.8682
[11/28 05:28:21 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3107, average loss: 1.5019
[11/28 05:28:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.04	
[11/28 05:28:21 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[11/28 05:30:02 visual_prompt]: 	Training 100/553. train loss: 4.5672,	1.2040 s / batch. (data: 3.63e-01). ETA=10:08:19, max mem: 20.9 GB 
[11/28 05:31:41 visual_prompt]: 	Training 200/553. train loss: 11.4203,	0.8289 s / batch. (data: 3.05e-04). ETA=6:57:26, max mem: 20.9 GB 
[11/28 05:33:16 visual_prompt]: 	Training 300/553. train loss: 3.6374,	0.8276 s / batch. (data: 5.40e-03). ETA=6:55:21, max mem: 20.9 GB 
[11/28 05:34:55 visual_prompt]: 	Training 400/553. train loss: 2.0307,	0.8439 s / batch. (data: 7.96e-04). ETA=7:02:09, max mem: 20.9 GB 
[11/28 05:36:27 visual_prompt]: 	Training 500/553. train loss: 31.1081,	0.8483 s / batch. (data: 3.32e-04). ETA=7:02:55, max mem: 20.9 GB 
[11/28 05:37:21 visual_prompt]: Epoch 46 / 100: avg data time: 1.41e-01, avg batch time: 0.9762, average train loss: 5.2327
[11/28 05:38:17 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.3100, average loss: 3.8369
[11/28 05:38:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.58	
[11/28 05:38:17 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[11/28 05:39:59 visual_prompt]: 	Training 100/553. train loss: 7.1362,	0.8242 s / batch. (data: 3.45e-04). ETA=6:48:50, max mem: 20.9 GB 
[11/28 05:41:33 visual_prompt]: 	Training 200/553. train loss: 11.4090,	1.3080 s / batch. (data: 4.77e-01). ETA=10:46:37, max mem: 20.9 GB 
[11/28 05:43:10 visual_prompt]: 	Training 300/553. train loss: 4.9667,	0.8265 s / batch. (data: 5.43e-03). ETA=6:47:13, max mem: 20.9 GB 
[11/28 05:44:49 visual_prompt]: 	Training 400/553. train loss: 1.0379,	0.8307 s / batch. (data: 3.94e-04). ETA=6:47:54, max mem: 20.9 GB 
[11/28 05:46:24 visual_prompt]: 	Training 500/553. train loss: 12.5615,	0.8440 s / batch. (data: 2.95e-04). ETA=6:53:01, max mem: 20.9 GB 
[11/28 05:47:16 visual_prompt]: Epoch 47 / 100: avg data time: 1.40e-01, avg batch time: 0.9747, average train loss: 5.7454
[11/28 05:48:11 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3079, average loss: 0.6999
[11/28 05:48:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 43.50	rocauc: 56.76	
[11/28 05:48:11 visual_prompt]: Best epoch 47: best metric: -0.700
[11/28 05:48:11 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[11/28 05:49:52 visual_prompt]: 	Training 100/553. train loss: 3.1295,	0.8520 s / batch. (data: 2.93e-04). ETA=6:54:46, max mem: 20.9 GB 
[11/28 05:51:29 visual_prompt]: 	Training 200/553. train loss: 0.5968,	0.8400 s / batch. (data: 2.85e-04). ETA=6:47:32, max mem: 20.9 GB 
[11/28 05:53:07 visual_prompt]: 	Training 300/553. train loss: 5.2766,	1.6678 s / batch. (data: 8.50e-01). ETA=13:26:20, max mem: 20.9 GB 
[11/28 05:54:41 visual_prompt]: 	Training 400/553. train loss: 0.0635,	0.8479 s / batch. (data: 3.54e-04). ETA=6:48:33, max mem: 20.9 GB 
[11/28 05:56:18 visual_prompt]: 	Training 500/553. train loss: 5.1195,	0.8280 s / batch. (data: 2.90e-03). ETA=6:37:34, max mem: 20.9 GB 
[11/28 05:57:09 visual_prompt]: Epoch 48 / 100: avg data time: 1.40e-01, avg batch time: 0.9727, average train loss: 4.9506
[11/28 05:58:04 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3101, average loss: 11.4459
[11/28 05:58:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.97	
[11/28 05:58:04 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[11/28 05:59:45 visual_prompt]: 	Training 100/553. train loss: 4.8152,	0.8157 s / batch. (data: 3.23e-04). ETA=6:29:35, max mem: 20.9 GB 
[11/28 06:01:22 visual_prompt]: 	Training 200/553. train loss: 5.7341,	0.8466 s / batch. (data: 7.96e-03). ETA=6:42:55, max mem: 20.9 GB 
[11/28 06:03:00 visual_prompt]: 	Training 300/553. train loss: 7.1820,	0.8289 s / batch. (data: 3.12e-04). ETA=6:33:07, max mem: 20.9 GB 
[11/28 06:04:38 visual_prompt]: 	Training 400/553. train loss: 9.0521,	0.8200 s / batch. (data: 3.04e-04). ETA=6:27:31, max mem: 20.9 GB 
[11/28 06:06:16 visual_prompt]: 	Training 500/553. train loss: 0.6756,	0.8480 s / batch. (data: 7.97e-03). ETA=6:39:20, max mem: 20.9 GB 
[11/28 06:07:07 visual_prompt]: Epoch 49 / 100: avg data time: 1.48e-01, avg batch time: 0.9818, average train loss: 4.6919
[11/28 06:08:03 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3105, average loss: 1.8727
[11/28 06:08:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.81	
[11/28 06:08:03 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[11/28 06:09:46 visual_prompt]: 	Training 100/553. train loss: 0.7409,	0.8493 s / batch. (data: 7.74e-04). ETA=6:37:46, max mem: 20.9 GB 
[11/28 06:11:23 visual_prompt]: 	Training 200/553. train loss: 16.3499,	0.8399 s / batch. (data: 7.18e-04). ETA=6:31:58, max mem: 20.9 GB 
[11/28 06:12:59 visual_prompt]: 	Training 300/553. train loss: 3.4335,	0.8331 s / batch. (data: 1.56e-02). ETA=6:27:26, max mem: 20.9 GB 
[11/28 06:14:34 visual_prompt]: 	Training 400/553. train loss: 0.2178,	0.8401 s / batch. (data: 5.43e-03). ETA=6:29:16, max mem: 20.9 GB 
[11/28 06:16:13 visual_prompt]: 	Training 500/553. train loss: 7.9378,	0.8360 s / batch. (data: 3.09e-04). ETA=6:25:59, max mem: 20.9 GB 
[11/28 06:17:03 visual_prompt]: Epoch 50 / 100: avg data time: 1.44e-01, avg batch time: 0.9761, average train loss: 3.8544
[11/28 06:17:58 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3100, average loss: 1.1306
[11/28 06:17:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.34	
[11/28 06:17:58 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[11/28 06:19:39 visual_prompt]: 	Training 100/553. train loss: 3.7684,	1.1400 s / batch. (data: 2.98e-01). ETA=8:43:27, max mem: 20.9 GB 
[11/28 06:21:17 visual_prompt]: 	Training 200/553. train loss: 8.2651,	0.8236 s / batch. (data: 7.95e-03). ETA=6:16:47, max mem: 20.9 GB 
[11/28 06:22:55 visual_prompt]: 	Training 300/553. train loss: 0.5341,	0.8278 s / batch. (data: 1.05e-02). ETA=6:17:20, max mem: 20.9 GB 
[11/28 06:24:32 visual_prompt]: 	Training 400/553. train loss: 8.5789,	1.3960 s / batch. (data: 5.73e-01). ETA=10:34:00, max mem: 20.9 GB 
[11/28 06:26:09 visual_prompt]: 	Training 500/553. train loss: 0.8546,	0.8280 s / batch. (data: 3.43e-04). ETA=6:14:39, max mem: 20.9 GB 
[11/28 06:26:58 visual_prompt]: Epoch 51 / 100: avg data time: 1.43e-01, avg batch time: 0.9767, average train loss: 3.7384
[11/28 06:27:54 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3105, average loss: 0.6999
[11/28 06:27:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.23	
[11/28 06:27:54 visual_prompt]: Best epoch 51: best metric: -0.700
[11/28 06:27:54 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[11/28 06:29:38 visual_prompt]: 	Training 100/553. train loss: 11.8221,	0.8312 s / batch. (data: 5.48e-03). ETA=6:14:00, max mem: 20.9 GB 
[11/28 06:31:14 visual_prompt]: 	Training 200/553. train loss: 0.6621,	0.8359 s / batch. (data: 7.95e-03). ETA=6:14:42, max mem: 20.9 GB 
[11/28 06:32:51 visual_prompt]: 	Training 300/553. train loss: 0.3386,	0.8515 s / batch. (data: 5.48e-03). ETA=6:20:16, max mem: 20.9 GB 
[11/28 06:34:29 visual_prompt]: 	Training 400/553. train loss: 7.0128,	0.8391 s / batch. (data: 1.05e-02). ETA=6:13:21, max mem: 20.9 GB 
[11/28 06:36:01 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8440 s / batch. (data: 2.98e-04). ETA=6:14:07, max mem: 20.9 GB 
[11/28 06:36:50 visual_prompt]: Epoch 52 / 100: avg data time: 1.38e-01, avg batch time: 0.9704, average train loss: 8.5044
[11/28 06:37:46 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3100, average loss: 16.2109
[11/28 06:37:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.94	
[11/28 06:37:46 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[11/28 06:39:28 visual_prompt]: 	Training 100/553. train loss: 1.3408,	0.8270 s / batch. (data: 3.03e-04). ETA=6:04:27, max mem: 20.9 GB 
[11/28 06:41:05 visual_prompt]: 	Training 200/553. train loss: 0.2647,	0.8302 s / batch. (data: 3.14e-04). ETA=6:04:30, max mem: 20.9 GB 
[11/28 06:42:40 visual_prompt]: 	Training 300/553. train loss: 9.9964,	0.8332 s / batch. (data: 1.20e-02). ETA=6:04:26, max mem: 20.9 GB 
[11/28 06:44:20 visual_prompt]: 	Training 400/553. train loss: 1.2182,	0.8360 s / batch. (data: 3.31e-04). ETA=6:04:16, max mem: 20.9 GB 
[11/28 06:45:55 visual_prompt]: 	Training 500/553. train loss: 7.5149,	0.8489 s / batch. (data: 2.04e-02). ETA=6:08:30, max mem: 20.9 GB 
[11/28 06:46:46 visual_prompt]: Epoch 53 / 100: avg data time: 1.43e-01, avg batch time: 0.9772, average train loss: 5.3401
[11/28 06:47:41 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3116, average loss: 14.8889
[11/28 06:47:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.86	
[11/28 06:47:41 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[11/28 06:49:22 visual_prompt]: 	Training 100/553. train loss: 2.1315,	0.8384 s / batch. (data: 2.06e-02). ETA=6:01:46, max mem: 20.9 GB 
[11/28 06:50:59 visual_prompt]: 	Training 200/553. train loss: 10.6493,	0.8916 s / batch. (data: 2.78e-02). ETA=6:23:15, max mem: 20.9 GB 
[11/28 06:52:34 visual_prompt]: 	Training 300/553. train loss: 5.9574,	0.8371 s / batch. (data: 3.40e-04). ETA=5:58:24, max mem: 20.9 GB 
[11/28 06:54:10 visual_prompt]: 	Training 400/553. train loss: 4.9209,	0.8399 s / batch. (data: 3.32e-04). ETA=5:58:14, max mem: 20.9 GB 
[11/28 06:55:47 visual_prompt]: 	Training 500/553. train loss: 2.9053,	0.8337 s / batch. (data: 5.44e-03). ETA=5:54:12, max mem: 20.9 GB 
[11/28 06:56:38 visual_prompt]: Epoch 54 / 100: avg data time: 1.34e-01, avg batch time: 0.9699, average train loss: 5.2600
[11/28 06:57:33 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3092, average loss: 11.6650
[11/28 06:57:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.79	
[11/28 06:57:33 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[11/28 06:59:14 visual_prompt]: 	Training 100/553. train loss: 1.2028,	0.8240 s / batch. (data: 3.01e-04). ETA=5:47:58, max mem: 20.9 GB 
[11/28 07:00:50 visual_prompt]: 	Training 200/553. train loss: 16.6755,	0.8480 s / batch. (data: 3.04e-04). ETA=5:56:41, max mem: 20.9 GB 
[11/28 07:02:27 visual_prompt]: 	Training 300/553. train loss: 3.5134,	0.8487 s / batch. (data: 1.56e-02). ETA=5:55:34, max mem: 20.9 GB 
[11/28 07:04:04 visual_prompt]: 	Training 400/553. train loss: 10.3159,	1.4872 s / batch. (data: 6.53e-01). ETA=10:20:37, max mem: 20.9 GB 
[11/28 07:05:39 visual_prompt]: 	Training 500/553. train loss: 4.8559,	0.8575 s / batch. (data: 5.46e-03). ETA=5:56:23, max mem: 20.9 GB 
[11/28 07:06:31 visual_prompt]: Epoch 55 / 100: avg data time: 1.37e-01, avg batch time: 0.9716, average train loss: 5.5410
[11/28 07:07:26 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3112, average loss: 9.3833
[11/28 07:07:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.64	
[11/28 07:07:26 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[11/28 07:09:09 visual_prompt]: 	Training 100/553. train loss: 1.1103,	0.8216 s / batch. (data: 3.03e-04). ETA=5:39:23, max mem: 20.9 GB 
[11/28 07:10:44 visual_prompt]: 	Training 200/553. train loss: 0.2164,	0.8395 s / batch. (data: 3.08e-04). ETA=5:45:22, max mem: 20.9 GB 
[11/28 07:12:24 visual_prompt]: 	Training 300/553. train loss: 1.0083,	0.8360 s / batch. (data: 3.41e-04). ETA=5:42:33, max mem: 20.9 GB 
[11/28 07:14:02 visual_prompt]: 	Training 400/553. train loss: 1.0088,	0.8716 s / batch. (data: 5.90e-03). ETA=5:55:41, max mem: 20.9 GB 
[11/28 07:15:39 visual_prompt]: 	Training 500/553. train loss: 4.3350,	2.1640 s / batch. (data: 1.33e+00). ETA=14:39:28, max mem: 20.9 GB 
[11/28 07:16:28 visual_prompt]: Epoch 56 / 100: avg data time: 1.45e-01, avg batch time: 0.9785, average train loss: 3.4236
[11/28 07:17:23 visual_prompt]: Inference (val):avg data time: 5.88e-05, avg batch time: 0.3112, average loss: 4.8735
[11/28 07:17:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.04	
[11/28 07:17:23 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[11/28 07:19:07 visual_prompt]: 	Training 100/553. train loss: 0.6384,	0.8307 s / batch. (data: 2.72e-04). ETA=5:35:28, max mem: 20.9 GB 
[11/28 07:20:44 visual_prompt]: 	Training 200/553. train loss: 0.6226,	0.8390 s / batch. (data: 5.44e-03). ETA=5:37:26, max mem: 20.9 GB 
[11/28 07:22:20 visual_prompt]: 	Training 300/553. train loss: 0.0616,	0.8480 s / batch. (data: 1.99e-02). ETA=5:39:38, max mem: 20.9 GB 
[11/28 07:23:56 visual_prompt]: 	Training 400/553. train loss: 1.2673,	0.8480 s / batch. (data: 3.85e-04). ETA=5:38:14, max mem: 20.9 GB 
[11/28 07:25:31 visual_prompt]: 	Training 500/553. train loss: 1.4663,	0.8506 s / batch. (data: 1.04e-02). ETA=5:37:52, max mem: 20.9 GB 
[11/28 07:26:24 visual_prompt]: Epoch 57 / 100: avg data time: 1.42e-01, avg batch time: 0.9771, average train loss: 3.2758
[11/28 07:27:19 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3102, average loss: 6.1833
[11/28 07:27:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.85	
[11/28 07:27:19 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[11/28 07:29:01 visual_prompt]: 	Training 100/553. train loss: 1.7930,	1.0594 s / batch. (data: 2.32e-01). ETA=6:58:05, max mem: 20.9 GB 
[11/28 07:30:39 visual_prompt]: 	Training 200/553. train loss: 2.8647,	0.8367 s / batch. (data: 5.51e-03). ETA=5:28:47, max mem: 20.9 GB 
[11/28 07:32:18 visual_prompt]: 	Training 300/553. train loss: 3.5130,	0.8440 s / batch. (data: 1.20e-02). ETA=5:30:16, max mem: 20.9 GB 
[11/28 07:33:55 visual_prompt]: 	Training 400/553. train loss: 3.5694,	0.8288 s / batch. (data: 3.14e-04). ETA=5:22:56, max mem: 20.9 GB 
[11/28 07:35:31 visual_prompt]: 	Training 500/553. train loss: 4.1884,	0.8457 s / batch. (data: 3.00e-04). ETA=5:28:07, max mem: 20.9 GB 
[11/28 07:36:21 visual_prompt]: Epoch 58 / 100: avg data time: 1.45e-01, avg batch time: 0.9795, average train loss: 4.2337
[11/28 07:37:17 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3114, average loss: 1.2878
[11/28 07:37:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.34	
[11/28 07:37:17 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[11/28 07:39:00 visual_prompt]: 	Training 100/553. train loss: 1.5850,	0.8280 s / batch. (data: 4.96e-04). ETA=5:19:07, max mem: 20.9 GB 
[11/28 07:40:38 visual_prompt]: 	Training 200/553. train loss: 0.0001,	0.8309 s / batch. (data: 5.43e-03). ETA=5:18:51, max mem: 20.9 GB 
[11/28 07:42:15 visual_prompt]: 	Training 300/553. train loss: 6.9652,	0.8594 s / batch. (data: 5.61e-03). ETA=5:28:23, max mem: 20.9 GB 
[11/28 07:43:51 visual_prompt]: 	Training 400/553. train loss: 1.0044,	0.8325 s / batch. (data: 8.42e-03). ETA=5:16:42, max mem: 20.9 GB 
[11/28 07:45:30 visual_prompt]: 	Training 500/553. train loss: 4.3351,	0.8602 s / batch. (data: 1.09e-02). ETA=5:25:48, max mem: 20.9 GB 
[11/28 07:46:18 visual_prompt]: Epoch 59 / 100: avg data time: 1.44e-01, avg batch time: 0.9781, average train loss: 3.7559
[11/28 07:47:13 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3097, average loss: 2.3761
[11/28 07:47:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.99	
[11/28 07:47:13 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[11/28 07:48:53 visual_prompt]: 	Training 100/553. train loss: 2.9310,	0.8370 s / batch. (data: 1.55e-02). ETA=5:14:54, max mem: 20.9 GB 
[11/28 07:50:30 visual_prompt]: 	Training 200/553. train loss: 3.4186,	0.8360 s / batch. (data: 3.24e-04). ETA=5:13:07, max mem: 20.9 GB 
[11/28 07:52:06 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.9638 s / batch. (data: 1.12e+00). ETA=12:12:15, max mem: 20.9 GB 
[11/28 07:53:44 visual_prompt]: 	Training 400/553. train loss: 0.8414,	0.9359 s / batch. (data: 1.13e-01). ETA=5:47:24, max mem: 20.9 GB 
[11/28 07:55:22 visual_prompt]: 	Training 500/553. train loss: 0.6160,	0.8294 s / batch. (data: 3.19e-04). ETA=5:06:29, max mem: 20.9 GB 
[11/28 07:56:13 visual_prompt]: Epoch 60 / 100: avg data time: 1.44e-01, avg batch time: 0.9775, average train loss: 2.8352
[11/28 07:57:09 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3106, average loss: 8.2320
[11/28 07:57:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.87	
[11/28 07:57:09 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[11/28 07:58:50 visual_prompt]: 	Training 100/553. train loss: 2.5247,	0.8661 s / batch. (data: 4.93e-02). ETA=5:17:51, max mem: 20.9 GB 
[11/28 08:00:28 visual_prompt]: 	Training 200/553. train loss: 2.5481,	1.6803 s / batch. (data: 8.36e-01). ETA=10:13:51, max mem: 20.9 GB 
[11/28 08:02:05 visual_prompt]: 	Training 300/553. train loss: 8.1168,	1.3039 s / batch. (data: 4.66e-01). ETA=7:54:11, max mem: 20.9 GB 
[11/28 08:03:39 visual_prompt]: 	Training 400/553. train loss: 0.9167,	0.8299 s / batch. (data: 3.32e-04). ETA=5:00:24, max mem: 20.9 GB 
[11/28 08:05:18 visual_prompt]: 	Training 500/553. train loss: 2.1288,	2.6089 s / batch. (data: 1.79e+00). ETA=15:40:05, max mem: 20.9 GB 
[11/28 08:06:07 visual_prompt]: Epoch 61 / 100: avg data time: 1.40e-01, avg batch time: 0.9734, average train loss: 5.1663
[11/28 08:07:03 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3081, average loss: 4.0654
[11/28 08:07:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.58	
[11/28 08:07:03 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[11/28 08:08:44 visual_prompt]: 	Training 100/553. train loss: 0.6616,	0.8413 s / batch. (data: 3.00e-04). ETA=5:01:01, max mem: 20.9 GB 
[11/28 08:10:22 visual_prompt]: 	Training 200/553. train loss: 0.7322,	0.8389 s / batch. (data: 2.06e-02). ETA=4:58:44, max mem: 20.9 GB 
[11/28 08:11:57 visual_prompt]: 	Training 300/553. train loss: 3.3843,	0.8280 s / batch. (data: 3.40e-04). ETA=4:53:29, max mem: 20.9 GB 
[11/28 08:13:35 visual_prompt]: 	Training 400/553. train loss: 4.0434,	0.8288 s / batch. (data: 3.12e-04). ETA=4:52:23, max mem: 20.9 GB 
[11/28 08:15:10 visual_prompt]: 	Training 500/553. train loss: 2.6342,	0.8359 s / batch. (data: 7.84e-03). ETA=4:53:29, max mem: 20.9 GB 
[11/28 08:16:04 visual_prompt]: Epoch 62 / 100: avg data time: 1.44e-01, avg batch time: 0.9776, average train loss: 2.2123
[11/28 08:16:59 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.3111, average loss: 6.2874
[11/28 08:16:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.54	
[11/28 08:16:59 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[11/28 08:18:44 visual_prompt]: 	Training 100/553. train loss: 6.2589,	0.8492 s / batch. (data: 7.95e-03). ETA=4:56:00, max mem: 20.9 GB 
[11/28 08:20:24 visual_prompt]: 	Training 200/553. train loss: 5.9336,	0.8544 s / batch. (data: 2.24e-02). ETA=4:56:24, max mem: 20.9 GB 
[11/28 08:21:59 visual_prompt]: 	Training 300/553. train loss: 0.8044,	0.8280 s / batch. (data: 3.03e-04). ETA=4:45:51, max mem: 20.9 GB 
[11/28 08:23:31 visual_prompt]: 	Training 400/553. train loss: 2.8658,	0.8200 s / batch. (data: 3.10e-04). ETA=4:41:43, max mem: 20.9 GB 
[11/28 08:25:06 visual_prompt]: 	Training 500/553. train loss: 0.8137,	0.8397 s / batch. (data: 5.43e-03). ETA=4:47:04, max mem: 20.9 GB 
[11/28 08:25:55 visual_prompt]: Epoch 63 / 100: avg data time: 1.37e-01, avg batch time: 0.9700, average train loss: 2.6216
[11/28 08:26:51 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3101, average loss: 2.3884
[11/28 08:26:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.03	
[11/28 08:26:51 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[11/28 08:28:34 visual_prompt]: 	Training 100/553. train loss: 0.0482,	0.8335 s / batch. (data: 8.08e-04). ETA=4:42:50, max mem: 20.9 GB 
[11/28 08:30:13 visual_prompt]: 	Training 200/553. train loss: 1.9375,	0.8631 s / batch. (data: 2.71e-02). ETA=4:51:27, max mem: 20.9 GB 
[11/28 08:31:47 visual_prompt]: 	Training 300/553. train loss: 8.7293,	0.8540 s / batch. (data: 5.37e-03). ETA=4:46:57, max mem: 20.9 GB 
[11/28 08:33:23 visual_prompt]: 	Training 400/553. train loss: 5.6863,	0.8360 s / batch. (data: 1.22e-02). ETA=4:39:30, max mem: 20.9 GB 
[11/28 08:34:59 visual_prompt]: 	Training 500/553. train loss: 0.1400,	0.8299 s / batch. (data: 2.89e-04). ETA=4:36:04, max mem: 20.9 GB 
[11/28 08:35:50 visual_prompt]: Epoch 64 / 100: avg data time: 1.41e-01, avg batch time: 0.9744, average train loss: 4.3293
[11/28 08:36:45 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3097, average loss: 8.6779
[11/28 08:36:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.02	
[11/28 08:36:45 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[11/28 08:38:29 visual_prompt]: 	Training 100/553. train loss: 4.5883,	0.8428 s / batch. (data: 1.08e-02). ETA=4:38:13, max mem: 20.9 GB 
[11/28 08:40:07 visual_prompt]: 	Training 200/553. train loss: 0.8529,	1.2918 s / batch. (data: 4.74e-01). ETA=7:04:19, max mem: 20.9 GB 
[11/28 08:41:42 visual_prompt]: 	Training 300/553. train loss: 1.6087,	1.0846 s / batch. (data: 2.65e-01). ETA=5:54:27, max mem: 20.9 GB 
[11/28 08:43:17 visual_prompt]: 	Training 400/553. train loss: 2.3950,	0.8360 s / batch. (data: 2.90e-04). ETA=4:31:49, max mem: 20.9 GB 
[11/28 08:44:54 visual_prompt]: 	Training 500/553. train loss: 0.6915,	0.8440 s / batch. (data: 3.11e-04). ETA=4:33:00, max mem: 20.9 GB 
[11/28 08:45:43 visual_prompt]: Epoch 65 / 100: avg data time: 1.38e-01, avg batch time: 0.9723, average train loss: 2.8459
[11/28 08:46:38 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3106, average loss: 1.9032
[11/28 08:46:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.48	
[11/28 08:46:38 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[11/28 08:48:18 visual_prompt]: 	Training 100/553. train loss: 2.3882,	0.8318 s / batch. (data: 2.81e-04). ETA=4:26:55, max mem: 20.9 GB 
[11/28 08:49:53 visual_prompt]: 	Training 200/553. train loss: 1.3214,	1.4250 s / batch. (data: 6.01e-01). ETA=7:34:56, max mem: 20.9 GB 
[11/28 08:51:33 visual_prompt]: 	Training 300/553. train loss: 3.5978,	0.8283 s / batch. (data: 2.89e-04). ETA=4:23:02, max mem: 20.9 GB 
[11/28 08:53:08 visual_prompt]: 	Training 400/553. train loss: 1.0606,	0.8200 s / batch. (data: 2.93e-04). ETA=4:19:03, max mem: 20.9 GB 
[11/28 08:54:44 visual_prompt]: 	Training 500/553. train loss: 1.7128,	0.8259 s / batch. (data: 3.43e-04). ETA=4:19:31, max mem: 20.9 GB 
[11/28 08:55:36 visual_prompt]: Epoch 66 / 100: avg data time: 1.38e-01, avg batch time: 0.9723, average train loss: 2.3398
[11/28 08:56:31 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3087, average loss: 1.5432
[11/28 08:56:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.83	
[11/28 08:56:31 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[11/28 08:58:14 visual_prompt]: 	Training 100/553. train loss: 0.4442,	0.8284 s / batch. (data: 3.13e-04). ETA=4:18:12, max mem: 20.9 GB 
[11/28 08:59:52 visual_prompt]: 	Training 200/553. train loss: 0.0358,	0.8369 s / batch. (data: 1.05e-02). ETA=4:19:27, max mem: 20.9 GB 
[11/28 09:01:26 visual_prompt]: 	Training 300/553. train loss: 1.6564,	0.8560 s / batch. (data: 7.95e-03). ETA=4:23:57, max mem: 20.9 GB 
[11/28 09:02:59 visual_prompt]: 	Training 400/553. train loss: 0.8198,	0.8600 s / batch. (data: 3.07e-04). ETA=4:23:46, max mem: 20.9 GB 
[11/28 09:04:38 visual_prompt]: 	Training 500/553. train loss: 6.0533,	1.3600 s / batch. (data: 5.17e-01). ETA=6:54:50, max mem: 20.9 GB 
[11/28 09:05:30 visual_prompt]: Epoch 67 / 100: avg data time: 1.39e-01, avg batch time: 0.9728, average train loss: 2.1030
[11/28 09:06:25 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3089, average loss: 1.6451
[11/28 09:06:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.40	
[11/28 09:06:25 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[11/28 09:08:07 visual_prompt]: 	Training 100/553. train loss: 0.9233,	0.8369 s / batch. (data: 3.06e-04). ETA=4:13:09, max mem: 20.9 GB 
[11/28 09:09:46 visual_prompt]: 	Training 200/553. train loss: 2.2552,	1.0870 s / batch. (data: 2.45e-01). ETA=5:26:59, max mem: 20.9 GB 
[11/28 09:11:21 visual_prompt]: 	Training 300/553. train loss: 1.7263,	0.8482 s / batch. (data: 8.15e-04). ETA=4:13:43, max mem: 20.9 GB 
[11/28 09:12:57 visual_prompt]: 	Training 400/553. train loss: 1.9842,	0.8373 s / batch. (data: 7.95e-03). ETA=4:09:04, max mem: 20.9 GB 
[11/28 09:14:34 visual_prompt]: 	Training 500/553. train loss: 1.7006,	0.8680 s / batch. (data: 2.81e-04). ETA=4:16:46, max mem: 20.9 GB 
[11/28 09:15:25 visual_prompt]: Epoch 68 / 100: avg data time: 1.41e-01, avg batch time: 0.9757, average train loss: 1.6055
[11/28 09:16:21 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3122, average loss: 0.9274
[11/28 09:16:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.49	
[11/28 09:16:21 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[11/28 09:18:02 visual_prompt]: 	Training 100/553. train loss: 1.3756,	0.8480 s / batch. (data: 2.90e-04). ETA=4:08:41, max mem: 20.9 GB 
[11/28 09:19:40 visual_prompt]: 	Training 200/553. train loss: 1.4827,	0.8320 s / batch. (data: 3.03e-04). ETA=4:02:37, max mem: 20.9 GB 
[11/28 09:21:17 visual_prompt]: 	Training 300/553. train loss: 2.3488,	0.8200 s / batch. (data: 2.94e-04). ETA=3:57:44, max mem: 20.9 GB 
[11/28 09:22:54 visual_prompt]: 	Training 400/553. train loss: 1.7498,	0.8315 s / batch. (data: 1.07e-02). ETA=3:59:40, max mem: 20.9 GB 
[11/28 09:24:31 visual_prompt]: 	Training 500/553. train loss: 0.9750,	0.8432 s / batch. (data: 2.49e-02). ETA=4:01:39, max mem: 20.9 GB 
[11/28 09:25:22 visual_prompt]: Epoch 69 / 100: avg data time: 1.45e-01, avg batch time: 0.9794, average train loss: 1.6998
[11/28 09:26:18 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3087, average loss: 1.5392
[11/28 09:26:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.83	
[11/28 09:26:18 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[11/28 09:27:59 visual_prompt]: 	Training 100/553. train loss: 0.7165,	0.8319 s / batch. (data: 5.43e-03). ETA=3:56:17, max mem: 20.9 GB 
[11/28 09:29:35 visual_prompt]: 	Training 200/553. train loss: 3.0905,	0.8295 s / batch. (data: 1.05e-02). ETA=3:54:14, max mem: 20.9 GB 
[11/28 09:31:12 visual_prompt]: 	Training 300/553. train loss: 8.4204,	0.8516 s / batch. (data: 5.44e-03). ETA=3:59:04, max mem: 20.9 GB 
[11/28 09:32:50 visual_prompt]: 	Training 400/553. train loss: 0.7865,	1.1484 s / batch. (data: 3.12e-01). ETA=5:20:27, max mem: 20.9 GB 
[11/28 09:34:25 visual_prompt]: 	Training 500/553. train loss: 3.3952,	0.8332 s / batch. (data: 1.46e-02). ETA=3:51:06, max mem: 20.9 GB 
[11/28 09:35:17 visual_prompt]: Epoch 70 / 100: avg data time: 1.40e-01, avg batch time: 0.9735, average train loss: 1.6299
[11/28 09:36:12 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3120, average loss: 0.8393
[11/28 09:36:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.97	
[11/28 09:36:12 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[11/28 09:37:53 visual_prompt]: 	Training 100/553. train loss: 4.0746,	0.8480 s / batch. (data: 1.19e-02). ETA=3:53:03, max mem: 20.9 GB 
[11/28 09:39:30 visual_prompt]: 	Training 200/553. train loss: 1.2399,	0.8440 s / batch. (data: 3.25e-04). ETA=3:50:33, max mem: 20.9 GB 
[11/28 09:41:09 visual_prompt]: 	Training 300/553. train loss: 1.0314,	0.8351 s / batch. (data: 5.45e-03). ETA=3:46:44, max mem: 20.9 GB 
[11/28 09:42:43 visual_prompt]: 	Training 400/553. train loss: 2.2785,	0.8600 s / batch. (data: 6.74e-03). ETA=3:52:02, max mem: 20.9 GB 
[11/28 09:44:19 visual_prompt]: 	Training 500/553. train loss: 0.7954,	0.8480 s / batch. (data: 3.31e-04). ETA=3:47:23, max mem: 20.9 GB 
[11/28 09:45:10 visual_prompt]: Epoch 71 / 100: avg data time: 1.39e-01, avg batch time: 0.9736, average train loss: 1.5987
[11/28 09:46:06 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3101, average loss: 0.6879
[11/28 09:46:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.10	
[11/28 09:46:06 visual_prompt]: Best epoch 71: best metric: -0.688
[11/28 09:46:06 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[11/28 09:47:49 visual_prompt]: 	Training 100/553. train loss: 0.7532,	1.0920 s / batch. (data: 2.72e-01). ETA=4:50:02, max mem: 20.9 GB 
[11/28 09:49:25 visual_prompt]: 	Training 200/553. train loss: 1.5626,	0.8473 s / batch. (data: 3.12e-04). ETA=3:43:38, max mem: 20.9 GB 
[11/28 09:51:02 visual_prompt]: 	Training 300/553. train loss: 0.6855,	0.8442 s / batch. (data: 7.98e-03). ETA=3:41:24, max mem: 20.9 GB 
[11/28 09:52:39 visual_prompt]: 	Training 400/553. train loss: 0.5587,	1.9036 s / batch. (data: 1.06e+00). ETA=8:16:07, max mem: 20.9 GB 
[11/28 09:54:12 visual_prompt]: 	Training 500/553. train loss: 0.7960,	0.8574 s / batch. (data: 2.96e-04). ETA=3:42:01, max mem: 20.9 GB 
[11/28 09:55:04 visual_prompt]: Epoch 72 / 100: avg data time: 1.40e-01, avg batch time: 0.9732, average train loss: 1.2615
[11/28 09:55:59 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3103, average loss: 1.1497
[11/28 09:55:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.65	
[11/28 09:55:59 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[11/28 09:57:40 visual_prompt]: 	Training 100/553. train loss: 0.8194,	0.8279 s / batch. (data: 1.11e-02). ETA=3:32:17, max mem: 20.9 GB 
[11/28 09:59:19 visual_prompt]: 	Training 200/553. train loss: 5.3167,	0.8182 s / batch. (data: 2.99e-04). ETA=3:28:24, max mem: 20.9 GB 
[11/28 10:00:52 visual_prompt]: 	Training 300/553. train loss: 0.7773,	0.8359 s / batch. (data: 1.03e-02). ETA=3:31:32, max mem: 20.9 GB 
[11/28 10:02:30 visual_prompt]: 	Training 400/553. train loss: 0.5624,	0.8480 s / batch. (data: 3.07e-04). ETA=3:33:10, max mem: 20.9 GB 
[11/28 10:04:07 visual_prompt]: 	Training 500/553. train loss: 0.9740,	0.8292 s / batch. (data: 3.65e-04). ETA=3:27:05, max mem: 20.9 GB 
[11/28 10:04:58 visual_prompt]: Epoch 73 / 100: avg data time: 1.39e-01, avg batch time: 0.9728, average train loss: 2.8037
[11/28 10:05:53 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3105, average loss: 4.5696
[11/28 10:05:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.72	
[11/28 10:05:53 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[11/28 10:07:39 visual_prompt]: 	Training 100/553. train loss: 4.5404,	1.3299 s / batch. (data: 5.01e-01). ETA=5:28:43, max mem: 20.9 GB 
[11/28 10:09:16 visual_prompt]: 	Training 200/553. train loss: 0.9309,	0.8252 s / batch. (data: 2.79e-04). ETA=3:22:36, max mem: 20.9 GB 
[11/28 10:10:52 visual_prompt]: 	Training 300/553. train loss: 0.6000,	0.8196 s / batch. (data: 3.17e-04). ETA=3:19:52, max mem: 20.9 GB 
[11/28 10:12:28 visual_prompt]: 	Training 400/553. train loss: 3.3900,	0.8440 s / batch. (data: 1.20e-02). ETA=3:24:24, max mem: 20.9 GB 
[11/28 10:14:03 visual_prompt]: 	Training 500/553. train loss: 1.8797,	1.9120 s / batch. (data: 1.08e+00). ETA=7:39:51, max mem: 20.9 GB 
[11/28 10:14:53 visual_prompt]: Epoch 74 / 100: avg data time: 1.43e-01, avg batch time: 0.9771, average train loss: 2.1771
[11/28 10:15:48 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3086, average loss: 2.7001
[11/28 10:15:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.13	
[11/28 10:15:48 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[11/28 10:17:31 visual_prompt]: 	Training 100/553. train loss: 0.7296,	1.5360 s / batch. (data: 6.82e-01). ETA=6:05:31, max mem: 20.9 GB 
[11/28 10:19:08 visual_prompt]: 	Training 200/553. train loss: 0.6479,	1.4288 s / batch. (data: 6.13e-01). ETA=5:37:37, max mem: 20.9 GB 
[11/28 10:20:44 visual_prompt]: 	Training 300/553. train loss: 1.5655,	0.8280 s / batch. (data: 5.43e-03). ETA=3:14:17, max mem: 20.9 GB 
[11/28 10:22:23 visual_prompt]: 	Training 400/553. train loss: 4.6580,	1.9838 s / batch. (data: 1.17e+00). ETA=7:42:09, max mem: 20.9 GB 
[11/28 10:23:59 visual_prompt]: 	Training 500/553. train loss: 0.8275,	0.8232 s / batch. (data: 2.89e-04). ETA=3:10:24, max mem: 20.9 GB 
[11/28 10:24:50 visual_prompt]: Epoch 75 / 100: avg data time: 1.47e-01, avg batch time: 0.9795, average train loss: 1.5086
[11/28 10:25:45 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3117, average loss: 4.3480
[11/28 10:25:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.35	
[11/28 10:25:45 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[11/28 10:27:29 visual_prompt]: 	Training 100/553. train loss: 2.0288,	0.8452 s / batch. (data: 1.03e-03). ETA=3:13:20, max mem: 20.9 GB 
[11/28 10:29:05 visual_prompt]: 	Training 200/553. train loss: 1.7805,	0.8406 s / batch. (data: 1.60e-02). ETA=3:10:53, max mem: 20.9 GB 
[11/28 10:30:41 visual_prompt]: 	Training 300/553. train loss: 1.4685,	0.8483 s / batch. (data: 3.26e-04). ETA=3:11:13, max mem: 20.9 GB 
[11/28 10:32:17 visual_prompt]: 	Training 400/553. train loss: 0.6902,	0.9104 s / batch. (data: 4.87e-02). ETA=3:23:42, max mem: 20.9 GB 
[11/28 10:33:53 visual_prompt]: 	Training 500/553. train loss: 0.8605,	0.8658 s / batch. (data: 2.59e-02). ETA=3:12:17, max mem: 20.9 GB 
[11/28 10:34:44 visual_prompt]: Epoch 76 / 100: avg data time: 1.39e-01, avg batch time: 0.9735, average train loss: 1.5438
[11/28 10:35:39 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3098, average loss: 0.6868
[11/28 10:35:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 54.47	
[11/28 10:35:39 visual_prompt]: Best epoch 76: best metric: -0.687
[11/28 10:35:39 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[11/28 10:37:21 visual_prompt]: 	Training 100/553. train loss: 0.9176,	2.0760 s / batch. (data: 1.23e+00). ETA=7:35:44, max mem: 20.9 GB 
[11/28 10:39:00 visual_prompt]: 	Training 200/553. train loss: 2.4776,	0.8331 s / batch. (data: 3.23e-04). ETA=3:01:30, max mem: 20.9 GB 
[11/28 10:40:34 visual_prompt]: 	Training 300/553. train loss: 0.5595,	0.8346 s / batch. (data: 3.45e-04). ETA=3:00:26, max mem: 20.9 GB 
[11/28 10:42:13 visual_prompt]: 	Training 400/553. train loss: 0.7289,	0.8670 s / batch. (data: 1.09e-02). ETA=3:05:59, max mem: 20.9 GB 
[11/28 10:43:49 visual_prompt]: 	Training 500/553. train loss: 1.7833,	0.8476 s / batch. (data: 3.04e-04). ETA=3:00:25, max mem: 20.9 GB 
[11/28 10:44:40 visual_prompt]: Epoch 77 / 100: avg data time: 1.43e-01, avg batch time: 0.9770, average train loss: 1.6303
[11/28 10:45:35 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3098, average loss: 1.0934
[11/28 10:45:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.17	
[11/28 10:45:35 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[11/28 10:47:15 visual_prompt]: 	Training 100/553. train loss: 0.7147,	0.8360 s / batch. (data: 3.10e-04). ETA=2:55:49, max mem: 20.9 GB 
[11/28 10:48:52 visual_prompt]: 	Training 200/553. train loss: 1.9571,	0.8440 s / batch. (data: 2.95e-04). ETA=2:56:06, max mem: 20.9 GB 
[11/28 10:50:29 visual_prompt]: 	Training 300/553. train loss: 1.8316,	0.8200 s / batch. (data: 3.02e-04). ETA=2:49:43, max mem: 20.9 GB 
[11/28 10:52:15 visual_prompt]: 	Training 400/553. train loss: 3.6737,	0.8280 s / batch. (data: 3.80e-04). ETA=2:50:00, max mem: 20.9 GB 
[11/28 10:53:53 visual_prompt]: 	Training 500/553. train loss: 0.6867,	0.8396 s / batch. (data: 3.48e-04). ETA=2:50:59, max mem: 20.9 GB 
[11/28 10:54:46 visual_prompt]: Epoch 78 / 100: avg data time: 1.63e-01, avg batch time: 0.9961, average train loss: 1.0529
[11/28 10:55:44 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3087, average loss: 0.8153
[11/28 10:55:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.69	
[11/28 10:55:44 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[11/28 10:57:27 visual_prompt]: 	Training 100/553. train loss: 1.8138,	0.8347 s / batch. (data: 3.06e-04). ETA=2:47:51, max mem: 20.9 GB 
[11/28 10:59:07 visual_prompt]: 	Training 200/553. train loss: 0.7785,	0.8311 s / batch. (data: 4.38e-04). ETA=2:45:44, max mem: 20.9 GB 
[11/28 11:00:42 visual_prompt]: 	Training 300/553. train loss: 0.7412,	2.1120 s / batch. (data: 1.25e+00). ETA=6:57:41, max mem: 20.9 GB 
[11/28 11:02:24 visual_prompt]: 	Training 400/553. train loss: 0.6647,	0.8589 s / batch. (data: 2.62e-02). ETA=2:48:25, max mem: 20.9 GB 
[11/28 11:04:04 visual_prompt]: 	Training 500/553. train loss: 2.2500,	0.8437 s / batch. (data: 1.19e-02). ETA=2:44:03, max mem: 20.9 GB 
[11/28 11:04:54 visual_prompt]: Epoch 79 / 100: avg data time: 1.62e-01, avg batch time: 0.9947, average train loss: 1.2480
[11/28 11:05:50 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3114, average loss: 0.9160
[11/28 11:05:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.13	
[11/28 11:05:50 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[11/28 11:07:31 visual_prompt]: 	Training 100/553. train loss: 1.7955,	0.8543 s / batch. (data: 1.58e-02). ETA=2:43:55, max mem: 20.9 GB 
[11/28 11:09:08 visual_prompt]: 	Training 200/553. train loss: 0.8393,	0.8400 s / batch. (data: 3.41e-04). ETA=2:39:47, max mem: 20.9 GB 
[11/28 11:10:44 visual_prompt]: 	Training 300/553. train loss: 0.6311,	1.3280 s / batch. (data: 4.80e-01). ETA=4:10:23, max mem: 20.9 GB 
[11/28 11:12:25 visual_prompt]: 	Training 400/553. train loss: 1.6594,	0.8440 s / batch. (data: 8.53e-03). ETA=2:37:43, max mem: 20.9 GB 
[11/28 11:14:02 visual_prompt]: 	Training 500/553. train loss: 1.3117,	1.2484 s / batch. (data: 4.19e-01). ETA=3:51:13, max mem: 20.9 GB 
[11/28 11:14:54 visual_prompt]: Epoch 80 / 100: avg data time: 1.49e-01, avg batch time: 0.9836, average train loss: 1.0760
[11/28 11:15:50 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3108, average loss: 0.9694
[11/28 11:15:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.84	
[11/28 11:15:50 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[11/28 11:17:37 visual_prompt]: 	Training 100/553. train loss: 0.5599,	0.8384 s / batch. (data: 3.15e-04). ETA=2:33:08, max mem: 20.9 GB 
[11/28 11:19:17 visual_prompt]: 	Training 200/553. train loss: 0.3914,	0.8280 s / batch. (data: 3.39e-04). ETA=2:29:52, max mem: 20.9 GB 
[11/28 11:20:53 visual_prompt]: 	Training 300/553. train loss: 0.6530,	0.8226 s / batch. (data: 3.24e-04). ETA=2:27:31, max mem: 20.9 GB 
[11/28 11:22:31 visual_prompt]: 	Training 400/553. train loss: 0.9743,	1.2963 s / batch. (data: 4.47e-01). ETA=3:50:18, max mem: 20.9 GB 
[11/28 11:24:06 visual_prompt]: 	Training 500/553. train loss: 1.1581,	1.7991 s / batch. (data: 9.72e-01). ETA=5:16:38, max mem: 20.9 GB 
[11/28 11:24:57 visual_prompt]: Epoch 81 / 100: avg data time: 1.54e-01, avg batch time: 0.9880, average train loss: 1.0089
[11/28 11:25:52 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3093, average loss: 0.7145
[11/28 11:25:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.09	
[11/28 11:25:52 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[11/28 11:27:34 visual_prompt]: 	Training 100/553. train loss: 1.1227,	0.8373 s / batch. (data: 3.27e-04). ETA=2:25:14, max mem: 20.9 GB 
[11/28 11:29:13 visual_prompt]: 	Training 200/553. train loss: 1.4934,	0.8240 s / batch. (data: 3.33e-04). ETA=2:21:33, max mem: 20.9 GB 
[11/28 11:30:52 visual_prompt]: 	Training 300/553. train loss: 0.5479,	2.2077 s / batch. (data: 1.38e+00). ETA=6:15:33, max mem: 20.9 GB 
[11/28 11:32:30 visual_prompt]: 	Training 400/553. train loss: 3.7751,	1.8880 s / batch. (data: 1.06e+00). ETA=5:18:01, max mem: 20.9 GB 
[11/28 11:34:21 visual_prompt]: 	Training 500/553. train loss: 0.7632,	0.8144 s / batch. (data: 3.55e-04). ETA=2:15:50, max mem: 20.9 GB 
[11/28 11:35:19 visual_prompt]: Epoch 82 / 100: avg data time: 1.92e-01, avg batch time: 1.0247, average train loss: 1.0113
[11/28 11:36:30 visual_prompt]: Inference (val):avg data time: 7.53e-05, avg batch time: 0.3095, average loss: 3.1171
[11/28 11:36:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.84	
[11/28 11:36:30 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[11/28 11:38:29 visual_prompt]: 	Training 100/553. train loss: 0.2367,	0.8338 s / batch. (data: 8.88e-04). ETA=2:16:56, max mem: 20.9 GB 
[11/28 11:40:24 visual_prompt]: 	Training 200/553. train loss: 3.0343,	0.8490 s / batch. (data: 3.34e-04). ETA=2:18:01, max mem: 20.9 GB 
[11/28 11:42:14 visual_prompt]: 	Training 300/553. train loss: 0.9125,	0.8318 s / batch. (data: 5.43e-03). ETA=2:13:50, max mem: 20.9 GB 
[11/28 11:44:10 visual_prompt]: 	Training 400/553. train loss: 0.7300,	0.8504 s / batch. (data: 1.07e-02). ETA=2:15:25, max mem: 20.9 GB 
[11/28 11:46:06 visual_prompt]: 	Training 500/553. train loss: 0.6911,	0.8230 s / batch. (data: 3.34e-04). ETA=2:09:41, max mem: 20.9 GB 
[11/28 11:47:04 visual_prompt]: Epoch 83 / 100: avg data time: 3.12e-01, avg batch time: 1.1451, average train loss: 0.9261
[11/28 11:48:14 visual_prompt]: Inference (val):avg data time: 9.47e-05, avg batch time: 0.3113, average loss: 0.7916
[11/28 11:48:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.45	
[11/28 11:48:14 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[11/28 11:50:17 visual_prompt]: 	Training 100/553. train loss: 0.6994,	0.8542 s / batch. (data: 3.46e-04). ETA=2:12:25, max mem: 20.9 GB 
[11/28 11:52:12 visual_prompt]: 	Training 200/553. train loss: 1.0073,	2.1752 s / batch. (data: 1.34e+00). ETA=5:33:34, max mem: 20.9 GB 
[11/28 11:54:08 visual_prompt]: 	Training 300/553. train loss: 0.5363,	0.8491 s / batch. (data: 1.34e-02). ETA=2:08:47, max mem: 20.9 GB 
[11/28 11:56:04 visual_prompt]: 	Training 400/553. train loss: 0.5649,	0.8391 s / batch. (data: 1.12e-02). ETA=2:05:52, max mem: 20.9 GB 
[11/28 11:57:55 visual_prompt]: 	Training 500/553. train loss: 0.3171,	0.8480 s / batch. (data: 5.25e-04). ETA=2:05:47, max mem: 20.9 GB 
[11/28 11:58:57 visual_prompt]: Epoch 84 / 100: avg data time: 3.29e-01, avg batch time: 1.1632, average train loss: 0.8315
[11/28 12:00:05 visual_prompt]: Inference (val):avg data time: 7.48e-05, avg batch time: 0.3109, average loss: 0.9600
[11/28 12:00:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.11	
[11/28 12:00:05 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[11/28 12:02:07 visual_prompt]: 	Training 100/553. train loss: 0.6242,	1.8181 s / batch. (data: 9.51e-01). ETA=4:25:05, max mem: 20.9 GB 
[11/28 12:04:00 visual_prompt]: 	Training 200/553. train loss: 0.5619,	0.8507 s / batch. (data: 1.26e-03). ETA=2:02:36, max mem: 20.9 GB 
[11/28 12:05:52 visual_prompt]: 	Training 300/553. train loss: 0.5595,	0.8166 s / batch. (data: 3.23e-04). ETA=1:56:19, max mem: 20.9 GB 
[11/28 12:07:43 visual_prompt]: 	Training 400/553. train loss: 1.1902,	0.8400 s / batch. (data: 1.20e-02). ETA=1:58:16, max mem: 20.9 GB 
[11/28 12:09:38 visual_prompt]: 	Training 500/553. train loss: 0.6516,	0.8520 s / batch. (data: 8.59e-04). ETA=1:58:32, max mem: 20.9 GB 
[11/28 12:10:35 visual_prompt]: Epoch 85 / 100: avg data time: 3.06e-01, avg batch time: 1.1391, average train loss: 0.8607
[11/28 12:11:40 visual_prompt]: Inference (val):avg data time: 2.34e-04, avg batch time: 0.3106, average loss: 0.8210
[11/28 12:11:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.08	
[11/28 12:11:40 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[11/28 12:13:39 visual_prompt]: 	Training 100/553. train loss: 0.5173,	3.6882 s / batch. (data: 2.86e+00). ETA=8:23:44, max mem: 20.9 GB 
[11/28 12:15:28 visual_prompt]: 	Training 200/553. train loss: 0.5713,	0.8300 s / batch. (data: 1.20e-03). ETA=1:51:59, max mem: 20.9 GB 
[11/28 12:17:19 visual_prompt]: 	Training 300/553. train loss: 1.4493,	0.8442 s / batch. (data: 4.92e-04). ETA=1:52:29, max mem: 20.9 GB 
[11/28 12:19:13 visual_prompt]: 	Training 400/553. train loss: 0.6979,	0.8494 s / batch. (data: 1.11e-02). ETA=1:51:45, max mem: 20.9 GB 
[11/28 12:21:08 visual_prompt]: 	Training 500/553. train loss: 0.7469,	0.8520 s / batch. (data: 5.32e-03). ETA=1:50:41, max mem: 20.9 GB 
[11/28 12:22:09 visual_prompt]: Epoch 86 / 100: avg data time: 3.03e-01, avg batch time: 1.1361, average train loss: 0.8593
[11/28 12:23:17 visual_prompt]: Inference (val):avg data time: 8.31e-05, avg batch time: 0.3104, average loss: 0.6983
[11/28 12:23:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.56	
[11/28 12:23:17 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[11/28 12:25:22 visual_prompt]: 	Training 100/553. train loss: 0.8616,	0.8153 s / batch. (data: 3.28e-04). ETA=1:43:50, max mem: 20.9 GB 
[11/28 12:27:18 visual_prompt]: 	Training 200/553. train loss: 0.8722,	0.8319 s / batch. (data: 4.95e-04). ETA=1:44:34, max mem: 20.9 GB 
[11/28 12:29:14 visual_prompt]: 	Training 300/553. train loss: 0.6174,	1.7120 s / batch. (data: 8.78e-01). ETA=3:32:20, max mem: 20.9 GB 
[11/28 12:31:06 visual_prompt]: 	Training 400/553. train loss: 0.9162,	0.8438 s / batch. (data: 9.82e-04). ETA=1:43:14, max mem: 20.9 GB 
[11/28 12:33:00 visual_prompt]: 	Training 500/553. train loss: 0.7152,	0.8556 s / batch. (data: 7.60e-03). ETA=1:43:16, max mem: 20.9 GB 
[11/28 12:34:01 visual_prompt]: Epoch 87 / 100: avg data time: 3.29e-01, avg batch time: 1.1635, average train loss: 0.7880
[11/28 12:35:09 visual_prompt]: Inference (val):avg data time: 5.65e-05, avg batch time: 0.3096, average loss: 0.8593
[11/28 12:35:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.30	
[11/28 12:35:09 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[11/28 12:37:06 visual_prompt]: 	Training 100/553. train loss: 0.3169,	0.8512 s / batch. (data: 2.10e-02). ETA=1:40:34, max mem: 20.9 GB 
[11/28 12:39:00 visual_prompt]: 	Training 200/553. train loss: 0.4857,	0.8280 s / batch. (data: 3.65e-04). ETA=1:36:27, max mem: 20.9 GB 
[11/28 12:40:58 visual_prompt]: 	Training 300/553. train loss: 0.0617,	0.8320 s / batch. (data: 3.76e-04). ETA=1:35:31, max mem: 20.9 GB 
[11/28 12:42:54 visual_prompt]: 	Training 400/553. train loss: 0.8329,	2.8449 s / batch. (data: 2.02e+00). ETA=5:21:54, max mem: 20.9 GB 
[11/28 12:44:42 visual_prompt]: 	Training 500/553. train loss: 0.8370,	1.4050 s / batch. (data: 5.67e-01). ETA=2:36:38, max mem: 20.9 GB 
[11/28 12:45:40 visual_prompt]: Epoch 88 / 100: avg data time: 3.08e-01, avg batch time: 1.1410, average train loss: 0.8413
[11/28 12:46:45 visual_prompt]: Inference (val):avg data time: 5.57e-05, avg batch time: 0.3099, average loss: 0.7564
[11/28 12:46:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.88	
[11/28 12:46:45 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[11/28 12:48:43 visual_prompt]: 	Training 100/553. train loss: 0.7168,	0.8291 s / batch. (data: 5.77e-03). ETA=1:30:18, max mem: 20.9 GB 
[11/28 12:50:33 visual_prompt]: 	Training 200/553. train loss: 0.9254,	0.8352 s / batch. (data: 2.95e-03). ETA=1:29:35, max mem: 20.9 GB 
[11/28 12:52:26 visual_prompt]: 	Training 300/553. train loss: 0.7014,	0.8200 s / batch. (data: 3.33e-04). ETA=1:26:35, max mem: 20.9 GB 
[11/28 12:54:18 visual_prompt]: 	Training 400/553. train loss: 1.1130,	0.8309 s / batch. (data: 3.42e-04). ETA=1:26:21, max mem: 20.9 GB 
[11/28 12:56:09 visual_prompt]: 	Training 500/553. train loss: 0.6733,	0.8263 s / batch. (data: 8.62e-04). ETA=1:24:30, max mem: 20.9 GB 
[11/28 12:57:07 visual_prompt]: Epoch 89 / 100: avg data time: 2.91e-01, avg batch time: 1.1234, average train loss: 0.7850
[11/28 12:58:12 visual_prompt]: Inference (val):avg data time: 4.83e-05, avg batch time: 0.3097, average loss: 0.8713
[11/28 12:58:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.68	
[11/28 12:58:12 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[11/28 13:00:11 visual_prompt]: 	Training 100/553. train loss: 0.5887,	0.8320 s / batch. (data: 9.05e-04). ETA=1:22:57, max mem: 20.9 GB 
[11/28 13:02:09 visual_prompt]: 	Training 200/553. train loss: 0.6307,	1.8002 s / batch. (data: 9.72e-01). ETA=2:56:30, max mem: 20.9 GB 
[11/28 13:04:03 visual_prompt]: 	Training 300/553. train loss: 1.2918,	0.8298 s / batch. (data: 7.80e-03). ETA=1:19:58, max mem: 20.9 GB 
[11/28 13:05:59 visual_prompt]: 	Training 400/553. train loss: 0.8055,	0.8197 s / batch. (data: 5.47e-03). ETA=1:17:38, max mem: 20.9 GB 
[11/28 13:07:54 visual_prompt]: 	Training 500/553. train loss: 0.6507,	0.8560 s / batch. (data: 3.49e-04). ETA=1:19:39, max mem: 20.9 GB 
[11/28 13:08:52 visual_prompt]: Epoch 90 / 100: avg data time: 3.25e-01, avg batch time: 1.1579, average train loss: 0.7644
[11/28 13:10:01 visual_prompt]: Inference (val):avg data time: 8.30e-05, avg batch time: 0.3121, average loss: 0.9329
[11/28 13:10:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.70	
[11/28 13:10:01 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[11/28 13:12:04 visual_prompt]: 	Training 100/553. train loss: 0.7190,	0.8308 s / batch. (data: 3.31e-04). ETA=1:15:11, max mem: 20.9 GB 
[11/28 13:14:01 visual_prompt]: 	Training 200/553. train loss: 0.4803,	0.8400 s / batch. (data: 3.04e-04). ETA=1:14:37, max mem: 20.9 GB 
[11/28 13:15:58 visual_prompt]: 	Training 300/553. train loss: 0.8297,	0.8606 s / batch. (data: 1.78e-02). ETA=1:15:01, max mem: 20.9 GB 
[11/28 13:17:57 visual_prompt]: 	Training 400/553. train loss: 0.7352,	2.2151 s / batch. (data: 1.39e+00). ETA=3:09:23, max mem: 20.9 GB 
[11/28 13:19:48 visual_prompt]: 	Training 500/553. train loss: 0.9635,	0.8283 s / batch. (data: 3.14e-04). ETA=1:09:26, max mem: 20.9 GB 
[11/28 13:20:47 visual_prompt]: Epoch 91 / 100: avg data time: 3.34e-01, avg batch time: 1.1672, average train loss: 0.7431
[11/28 13:21:56 visual_prompt]: Inference (val):avg data time: 8.11e-05, avg batch time: 0.3100, average loss: 0.7229
[11/28 13:21:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 68.92	
[11/28 13:21:56 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[11/28 13:24:00 visual_prompt]: 	Training 100/553. train loss: 0.8772,	2.8199 s / batch. (data: 1.97e+00). ETA=3:49:12, max mem: 20.9 GB 
[11/28 13:25:52 visual_prompt]: 	Training 200/553. train loss: 1.1519,	0.8512 s / batch. (data: 6.38e-03). ETA=1:07:46, max mem: 20.9 GB 
[11/28 13:27:44 visual_prompt]: 	Training 300/553. train loss: 0.6088,	1.1895 s / batch. (data: 3.62e-01). ETA=1:32:43, max mem: 20.9 GB 
[11/28 13:29:40 visual_prompt]: 	Training 400/553. train loss: 0.6533,	0.8258 s / batch. (data: 1.02e-02). ETA=1:02:59, max mem: 20.9 GB 
[11/28 13:31:34 visual_prompt]: 	Training 500/553. train loss: 0.9115,	0.8360 s / batch. (data: 7.95e-03). ETA=1:02:22, max mem: 20.9 GB 
[11/28 13:32:34 visual_prompt]: Epoch 92 / 100: avg data time: 3.20e-01, avg batch time: 1.1528, average train loss: 0.7151
[11/28 13:33:44 visual_prompt]: Inference (val):avg data time: 9.26e-05, avg batch time: 0.3109, average loss: 1.3657
[11/28 13:33:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.17	
[11/28 13:33:44 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[11/28 13:35:45 visual_prompt]: 	Training 100/553. train loss: 0.4733,	0.8382 s / batch. (data: 5.96e-03). ETA=1:00:24, max mem: 20.9 GB 
[11/28 13:37:42 visual_prompt]: 	Training 200/553. train loss: 0.9180,	1.6588 s / batch. (data: 8.27e-01). ETA=1:56:46, max mem: 20.9 GB 
[11/28 13:39:39 visual_prompt]: 	Training 300/553. train loss: 0.6854,	2.5280 s / batch. (data: 1.70e+00). ETA=2:53:45, max mem: 20.9 GB 
[11/28 13:41:35 visual_prompt]: 	Training 400/553. train loss: 0.5116,	1.7059 s / batch. (data: 8.88e-01). ETA=1:54:24, max mem: 20.9 GB 
[11/28 13:43:34 visual_prompt]: 	Training 500/553. train loss: 0.9180,	0.8465 s / batch. (data: 1.13e-02). ETA=0:55:21, max mem: 20.9 GB 
[11/28 13:44:33 visual_prompt]: Epoch 93 / 100: avg data time: 3.38e-01, avg batch time: 1.1728, average train loss: 0.7061
[11/28 13:45:42 visual_prompt]: Inference (val):avg data time: 1.36e-04, avg batch time: 0.3108, average loss: 0.6318
[11/28 13:45:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.48	
[11/28 13:45:42 visual_prompt]: Best epoch 93: best metric: -0.632
[11/28 13:45:42 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[11/28 13:47:43 visual_prompt]: 	Training 100/553. train loss: 0.8262,	0.8277 s / batch. (data: 1.09e-03). ETA=0:52:01, max mem: 20.9 GB 
[11/28 13:49:36 visual_prompt]: 	Training 200/553. train loss: 1.0453,	1.4594 s / batch. (data: 6.45e-01). ETA=1:29:17, max mem: 20.9 GB 
[11/28 13:51:32 visual_prompt]: 	Training 300/553. train loss: 0.9138,	0.8274 s / batch. (data: 3.52e-04). ETA=0:49:14, max mem: 20.9 GB 
[11/28 13:53:28 visual_prompt]: 	Training 400/553. train loss: 0.8219,	1.2413 s / batch. (data: 4.10e-01). ETA=1:11:48, max mem: 20.9 GB 
[11/28 13:55:22 visual_prompt]: 	Training 500/553. train loss: 0.6527,	1.0630 s / batch. (data: 2.29e-01). ETA=0:59:43, max mem: 20.9 GB 
[11/28 13:56:23 visual_prompt]: Epoch 94 / 100: avg data time: 3.25e-01, avg batch time: 1.1593, average train loss: 0.6658
[11/28 13:57:32 visual_prompt]: Inference (val):avg data time: 4.92e-04, avg batch time: 0.3102, average loss: 0.7013
[11/28 13:57:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 69.60	
[11/28 13:57:32 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[11/28 13:59:35 visual_prompt]: 	Training 100/553. train loss: 0.7940,	0.8467 s / batch. (data: 6.11e-03). ETA=0:45:24, max mem: 20.9 GB 
[11/28 14:01:34 visual_prompt]: 	Training 200/553. train loss: 0.4286,	1.3749 s / batch. (data: 5.42e-01). ETA=1:11:27, max mem: 20.9 GB 
[11/28 14:03:28 visual_prompt]: 	Training 300/553. train loss: 0.7362,	2.3474 s / batch. (data: 1.52e+00). ETA=1:58:04, max mem: 20.9 GB 
[11/28 14:05:26 visual_prompt]: 	Training 400/553. train loss: 1.1178,	3.0752 s / batch. (data: 2.26e+00). ETA=2:29:33, max mem: 20.9 GB 
[11/28 14:07:20 visual_prompt]: 	Training 500/553. train loss: 0.5274,	0.8338 s / batch. (data: 3.19e-04). ETA=0:39:09, max mem: 20.9 GB 
[11/28 14:08:20 visual_prompt]: Epoch 95 / 100: avg data time: 3.37e-01, avg batch time: 1.1713, average train loss: 0.6509
[11/28 14:09:26 visual_prompt]: Inference (val):avg data time: 5.73e-05, avg batch time: 0.3105, average loss: 0.6308
[11/28 14:09:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 70.32	
[11/28 14:09:26 visual_prompt]: Best epoch 95: best metric: -0.631
[11/28 14:09:26 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[11/28 14:11:29 visual_prompt]: 	Training 100/553. train loss: 1.0444,	0.8240 s / batch. (data: 3.68e-04). ETA=0:36:35, max mem: 20.9 GB 
[11/28 14:13:26 visual_prompt]: 	Training 200/553. train loss: 0.5652,	0.8518 s / batch. (data: 2.29e-03). ETA=0:36:24, max mem: 20.9 GB 
[11/28 14:15:17 visual_prompt]: 	Training 300/553. train loss: 0.5882,	1.7786 s / batch. (data: 9.64e-01). ETA=1:13:04, max mem: 20.9 GB 
[11/28 14:17:10 visual_prompt]: 	Training 400/553. train loss: 0.3947,	0.8319 s / batch. (data: 7.76e-04). ETA=0:32:47, max mem: 20.9 GB 
[11/28 14:19:04 visual_prompt]: 	Training 500/553. train loss: 0.9310,	0.8329 s / batch. (data: 7.92e-03). ETA=0:31:26, max mem: 20.9 GB 
[11/28 14:20:03 visual_prompt]: Epoch 96 / 100: avg data time: 3.16e-01, avg batch time: 1.1502, average train loss: 0.6444
[11/28 14:21:12 visual_prompt]: Inference (val):avg data time: 3.68e-04, avg batch time: 0.3111, average loss: 0.6290
[11/28 14:21:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 70.18	
[11/28 14:21:12 visual_prompt]: Best epoch 96: best metric: -0.629
[11/28 14:21:12 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[11/28 14:23:11 visual_prompt]: 	Training 100/553. train loss: 0.8252,	0.8201 s / batch. (data: 3.53e-04). ETA=0:28:51, max mem: 20.9 GB 
[11/28 14:25:09 visual_prompt]: 	Training 200/553. train loss: 0.4050,	0.8400 s / batch. (data: 7.96e-03). ETA=0:28:10, max mem: 20.9 GB 
[11/28 14:27:04 visual_prompt]: 	Training 300/553. train loss: 0.7936,	0.8681 s / batch. (data: 2.92e-02). ETA=0:27:39, max mem: 20.9 GB 
[11/28 14:28:59 visual_prompt]: 	Training 400/553. train loss: 0.3982,	0.8337 s / batch. (data: 6.79e-04). ETA=0:25:10, max mem: 20.9 GB 
[11/28 14:30:52 visual_prompt]: 	Training 500/553. train loss: 1.2295,	0.8300 s / batch. (data: 1.15e-03). ETA=0:23:40, max mem: 20.9 GB 
[11/28 14:31:54 visual_prompt]: Epoch 97 / 100: avg data time: 3.26e-01, avg batch time: 1.1597, average train loss: 0.6328
[11/28 14:33:01 visual_prompt]: Inference (val):avg data time: 6.93e-05, avg batch time: 0.3108, average loss: 0.6306
[11/28 14:33:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 70.85	
[11/28 14:33:01 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[11/28 14:35:05 visual_prompt]: 	Training 100/553. train loss: 0.5270,	0.8574 s / batch. (data: 1.34e-02). ETA=0:22:16, max mem: 20.9 GB 
[11/28 14:36:57 visual_prompt]: 	Training 200/553. train loss: 0.7179,	0.8390 s / batch. (data: 1.58e-02). ETA=0:20:24, max mem: 20.9 GB 
[11/28 14:38:53 visual_prompt]: 	Training 300/553. train loss: 0.8244,	2.9311 s / batch. (data: 2.12e+00). ETA=1:06:23, max mem: 20.9 GB 
[11/28 14:40:48 visual_prompt]: 	Training 400/553. train loss: 0.7800,	2.4543 s / batch. (data: 1.63e+00). ETA=0:51:29, max mem: 20.9 GB 
[11/28 14:42:42 visual_prompt]: 	Training 500/553. train loss: 0.8072,	0.8449 s / batch. (data: 1.05e-02). ETA=0:16:19, max mem: 20.9 GB 
[11/28 14:43:43 visual_prompt]: Epoch 98 / 100: avg data time: 3.26e-01, avg batch time: 1.1589, average train loss: 0.6228
[11/28 14:44:50 visual_prompt]: Inference (val):avg data time: 2.35e-04, avg batch time: 0.3107, average loss: 0.6224
[11/28 14:44:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 70.40	
[11/28 14:44:50 visual_prompt]: Best epoch 98: best metric: -0.622
[11/28 14:44:50 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[11/28 14:46:50 visual_prompt]: 	Training 100/553. train loss: 0.7898,	0.8280 s / batch. (data: 7.65e-04). ETA=0:13:52, max mem: 20.9 GB 
[11/28 14:48:47 visual_prompt]: 	Training 200/553. train loss: 0.7750,	0.8251 s / batch. (data: 7.62e-04). ETA=0:12:27, max mem: 20.9 GB 
[11/28 14:50:46 visual_prompt]: 	Training 300/553. train loss: 1.2188,	2.2395 s / batch. (data: 1.42e+00). ETA=0:30:05, max mem: 20.9 GB 
[11/28 14:52:37 visual_prompt]: 	Training 400/553. train loss: 0.7227,	1.5762 s / batch. (data: 7.28e-01). ETA=0:18:32, max mem: 20.9 GB 
[11/28 14:54:31 visual_prompt]: 	Training 500/553. train loss: 0.6670,	0.8579 s / batch. (data: 7.61e-04). ETA=0:08:39, max mem: 20.9 GB 
[11/28 14:55:31 visual_prompt]: Epoch 99 / 100: avg data time: 3.25e-01, avg batch time: 1.1588, average train loss: 0.6092
[11/28 14:56:39 visual_prompt]: Inference (val):avg data time: 7.52e-05, avg batch time: 0.3105, average loss: 0.6209
[11/28 14:56:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 70.80	
[11/28 14:56:39 visual_prompt]: Best epoch 99: best metric: -0.621
[11/28 14:56:39 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[11/28 14:58:44 visual_prompt]: 	Training 100/553. train loss: 0.6347,	1.8056 s / batch. (data: 9.69e-01). ETA=0:13:37, max mem: 20.9 GB 
[11/28 15:00:37 visual_prompt]: 	Training 200/553. train loss: 0.6811,	1.3322 s / batch. (data: 4.79e-01). ETA=0:07:50, max mem: 20.9 GB 
[11/28 15:02:34 visual_prompt]: 	Training 300/553. train loss: 0.4746,	0.8939 s / batch. (data: 7.55e-02). ETA=0:03:46, max mem: 20.9 GB 
[11/28 15:04:29 visual_prompt]: 	Training 400/553. train loss: 0.5761,	0.8160 s / batch. (data: 3.90e-04). ETA=0:02:04, max mem: 20.9 GB 
[11/28 15:06:20 visual_prompt]: 	Training 500/553. train loss: 0.5968,	0.8349 s / batch. (data: 5.47e-03). ETA=0:00:44, max mem: 20.9 GB 
[11/28 15:07:19 visual_prompt]: Epoch 100 / 100: avg data time: 3.24e-01, avg batch time: 1.1580, average train loss: 0.6025
[11/28 15:08:26 visual_prompt]: Inference (val):avg data time: 6.75e-05, avg batch time: 0.3101, average loss: 0.6216
[11/28 15:08:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 70.97	
[11/28 15:08:27 visual_prompt]: Rank of current process: 0. World size: 1
[11/28 15:08:27 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 15:08:27 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/28 15:08:27 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/28 15:08:27 visual_prompt]: Training with config:
[11/28 15:08:27 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr5.0_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/28 15:08:27 visual_prompt]: Loading training data...
[11/28 15:08:27 visual_prompt]: Constructing mammo-cbis dataset train...
[11/28 15:08:27 visual_prompt]: Loading validation data...
[11/28 15:08:27 visual_prompt]: Constructing mammo-cbis dataset val...
[11/28 15:08:27 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/28 15:08:30 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/28 15:08:30 visual_prompt]: tuned percent:0.525
[11/28 15:08:30 visual_prompt]: Device used for model: 0
[11/28 15:08:30 visual_prompt]: Setting up Evaluator...
[11/28 15:08:30 visual_prompt]: Setting up Trainer...
[11/28 15:08:30 visual_prompt]: 	Setting up the optimizer...
[11/28 15:08:30 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/28 15:10:30 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8283 s / batch. (data: 7.95e-03). ETA=12:41:59, max mem: 20.9 GB 
[11/28 15:12:24 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8361 s / batch. (data: 5.44e-03). ETA=12:47:46, max mem: 20.9 GB 
[11/28 15:14:22 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.4882 s / batch. (data: 1.65e+00). ETA=1 day, 14:00:49, max mem: 20.9 GB 
[11/28 15:16:14 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8337 s / batch. (data: 9.01e-04). ETA=12:42:47, max mem: 20.9 GB 
[11/28 15:18:13 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8479 s / batch. (data: 6.41e-03). ETA=12:54:24, max mem: 20.9 GB 
[11/28 15:19:13 visual_prompt]: Epoch 1 / 100: avg data time: 3.28e-01, avg batch time: 1.1624, average train loss: 1.5403
[11/28 15:20:20 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3113, average loss: 1.5201
[11/28 15:20:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/28 15:20:20 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/28 15:22:20 visual_prompt]: 	Training 100/553. train loss: 3.0043,	0.9945 s / batch. (data: 1.69e-01). ETA=15:05:47, max mem: 20.9 GB 
[11/28 15:24:15 visual_prompt]: 	Training 200/553. train loss: 0.0004,	2.1080 s / batch. (data: 1.27e+00). ETA=1 day, 7:56:22, max mem: 20.9 GB 
[11/28 15:26:12 visual_prompt]: 	Training 300/553. train loss: 1.0276,	1.6763 s / batch. (data: 8.19e-01). ETA=1 day, 1:21:10, max mem: 20.9 GB 
[11/28 15:28:05 visual_prompt]: 	Training 400/553. train loss: 2.9529,	0.8270 s / batch. (data: 3.28e-04). ETA=12:29:04, max mem: 20.9 GB 
[11/28 15:30:02 visual_prompt]: 	Training 500/553. train loss: 1.0274,	0.8163 s / batch. (data: 3.15e-04). ETA=12:17:59, max mem: 20.9 GB 
[11/28 15:31:01 visual_prompt]: Epoch 2 / 100: avg data time: 3.24e-01, avg batch time: 1.1581, average train loss: 1.8527
[11/28 15:32:09 visual_prompt]: Inference (val):avg data time: 7.73e-05, avg batch time: 0.3092, average loss: 3.8356
[11/28 15:32:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.38	
[11/28 15:32:09 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/28 15:34:07 visual_prompt]: 	Training 100/553. train loss: 4.4338,	0.8331 s / batch. (data: 6.11e-03). ETA=12:31:08, max mem: 20.9 GB 
[11/28 15:36:04 visual_prompt]: 	Training 200/553. train loss: 0.7001,	0.8321 s / batch. (data: 1.12e-03). ETA=12:28:50, max mem: 20.9 GB 
[11/28 15:37:58 visual_prompt]: 	Training 300/553. train loss: 2.6151,	0.8186 s / batch. (data: 1.10e-03). ETA=12:15:19, max mem: 20.9 GB 
[11/28 15:39:53 visual_prompt]: 	Training 400/553. train loss: 1.4173,	0.8400 s / batch. (data: 3.34e-04). ETA=12:33:07, max mem: 20.9 GB 
[11/28 15:41:49 visual_prompt]: 	Training 500/553. train loss: 1.0324,	1.6782 s / batch. (data: 8.32e-01). ETA=1 day, 1:01:46, max mem: 20.9 GB 
[11/28 15:42:46 visual_prompt]: Epoch 3 / 100: avg data time: 3.19e-01, avg batch time: 1.1529, average train loss: 2.5510
[11/28 15:43:53 visual_prompt]: Inference (val):avg data time: 7.27e-05, avg batch time: 0.3124, average loss: 1.8970
[11/28 15:43:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.23	
[11/28 15:43:53 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/28 15:45:55 visual_prompt]: 	Training 100/553. train loss: 2.2802,	0.8340 s / batch. (data: 1.19e-02). ETA=12:24:11, max mem: 20.9 GB 
[11/28 15:47:51 visual_prompt]: 	Training 200/553. train loss: 2.3283,	0.8265 s / batch. (data: 1.30e-03). ETA=12:16:08, max mem: 20.9 GB 
[11/28 15:49:46 visual_prompt]: 	Training 300/553. train loss: 1.8069,	1.6558 s / batch. (data: 8.20e-01). ETA=1 day, 0:32:00, max mem: 20.9 GB 
[11/28 15:51:36 visual_prompt]: 	Training 400/553. train loss: 1.2089,	1.2399 s / batch. (data: 4.10e-01). ETA=18:20:14, max mem: 20.9 GB 
[11/28 15:53:32 visual_prompt]: 	Training 500/553. train loss: 0.0000,	4.2838 s / batch. (data: 3.45e+00). ETA=2 days, 15:14:08, max mem: 20.9 GB 
[11/28 15:54:32 visual_prompt]: Epoch 4 / 100: avg data time: 3.22e-01, avg batch time: 1.1555, average train loss: 2.9854
[11/28 15:55:38 visual_prompt]: Inference (val):avg data time: 6.26e-05, avg batch time: 0.3108, average loss: 12.4888
[11/28 15:55:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.00	
[11/28 15:55:38 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/28 15:57:36 visual_prompt]: 	Training 100/553. train loss: 0.0002,	0.8619 s / batch. (data: 7.56e-04). ETA=12:41:12, max mem: 20.9 GB 
[11/28 15:59:29 visual_prompt]: 	Training 200/553. train loss: 6.0476,	1.3375 s / batch. (data: 5.18e-01). ETA=19:38:57, max mem: 20.9 GB 
[11/28 16:01:24 visual_prompt]: 	Training 300/553. train loss: 16.7818,	0.8480 s / batch. (data: 7.95e-03). ETA=12:26:05, max mem: 20.9 GB 
[11/28 16:03:17 visual_prompt]: 	Training 400/553. train loss: 16.1333,	0.8306 s / batch. (data: 3.35e-04). ETA=12:09:23, max mem: 20.9 GB 
[11/28 16:05:13 visual_prompt]: 	Training 500/553. train loss: 4.3187,	0.8351 s / batch. (data: 1.19e-02). ETA=12:11:55, max mem: 20.9 GB 
[11/28 16:06:13 visual_prompt]: Epoch 5 / 100: avg data time: 3.14e-01, avg batch time: 1.1470, average train loss: 5.7854
[11/28 16:07:21 visual_prompt]: Inference (val):avg data time: 8.12e-05, avg batch time: 0.3099, average loss: 14.4886
[11/28 16:07:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.19	
[11/28 16:07:21 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/28 16:09:24 visual_prompt]: 	Training 100/553. train loss: 11.8735,	0.8710 s / batch. (data: 5.97e-03). ETA=12:41:10, max mem: 20.9 GB 
[11/28 16:11:19 visual_prompt]: 	Training 200/553. train loss: 8.3384,	0.8320 s / batch. (data: 3.40e-04). ETA=12:05:41, max mem: 20.9 GB 
[11/28 16:13:11 visual_prompt]: 	Training 300/553. train loss: 5.6462,	0.8419 s / batch. (data: 1.77e-02). ETA=12:12:57, max mem: 20.9 GB 
[11/28 16:15:12 visual_prompt]: 	Training 400/553. train loss: 4.8664,	0.8411 s / batch. (data: 9.05e-03). ETA=12:10:48, max mem: 20.9 GB 
[11/28 16:17:06 visual_prompt]: 	Training 500/553. train loss: 1.8894,	1.1745 s / batch. (data: 3.59e-01). ETA=16:58:35, max mem: 20.9 GB 
[11/28 16:18:05 visual_prompt]: Epoch 6 / 100: avg data time: 3.29e-01, avg batch time: 1.1637, average train loss: 6.9691
[11/28 16:19:12 visual_prompt]: Inference (val):avg data time: 7.50e-05, avg batch time: 0.3102, average loss: 13.6566
[11/28 16:19:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.65	
[11/28 16:19:12 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/28 16:21:11 visual_prompt]: 	Training 100/553. train loss: 7.1549,	0.8580 s / batch. (data: 9.85e-03). ETA=12:21:53, max mem: 20.9 GB 
[11/28 16:23:06 visual_prompt]: 	Training 200/553. train loss: 2.6272,	0.8264 s / batch. (data: 9.88e-04). ETA=11:53:12, max mem: 20.9 GB 
[11/28 16:25:07 visual_prompt]: 	Training 300/553. train loss: 2.2763,	2.6700 s / batch. (data: 1.86e+00). ETA=1 day, 14:19:53, max mem: 20.9 GB 
[11/28 16:27:02 visual_prompt]: 	Training 400/553. train loss: 0.9221,	2.7836 s / batch. (data: 1.97e+00). ETA=1 day, 15:53:05, max mem: 20.9 GB 
[11/28 16:28:53 visual_prompt]: 	Training 500/553. train loss: 0.9348,	0.8363 s / batch. (data: 3.13e-04). ETA=11:57:36, max mem: 20.9 GB 
[11/28 16:29:50 visual_prompt]: Epoch 7 / 100: avg data time: 3.20e-01, avg batch time: 1.1536, average train loss: 6.1081
[11/28 16:30:56 visual_prompt]: Inference (val):avg data time: 6.53e-05, avg batch time: 0.3101, average loss: 3.2896
[11/28 16:30:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.96	
[11/28 16:30:56 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/28 16:32:52 visual_prompt]: 	Training 100/553. train loss: 16.5645,	0.8466 s / batch. (data: 2.83e-02). ETA=12:04:14, max mem: 20.9 GB 
[11/28 16:34:47 visual_prompt]: 	Training 200/553. train loss: 1.7785,	0.8209 s / batch. (data: 1.17e-03). ETA=11:40:52, max mem: 20.9 GB 
[11/28 16:36:41 visual_prompt]: 	Training 300/553. train loss: 17.5561,	0.8432 s / batch. (data: 2.78e-03). ETA=11:58:34, max mem: 20.9 GB 
[11/28 16:38:35 visual_prompt]: 	Training 400/553. train loss: 4.2794,	1.3866 s / batch. (data: 5.53e-01). ETA=19:39:17, max mem: 20.9 GB 
[11/28 16:40:29 visual_prompt]: 	Training 500/553. train loss: 36.0258,	2.0776 s / batch. (data: 1.24e+00). ETA=1 day, 5:23:32, max mem: 20.9 GB 
[11/28 16:41:28 visual_prompt]: Epoch 8 / 100: avg data time: 3.09e-01, avg batch time: 1.1420, average train loss: 7.0746
[11/28 16:42:33 visual_prompt]: Inference (val):avg data time: 5.52e-05, avg batch time: 0.3105, average loss: 1.8158
[11/28 16:42:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.71	
[11/28 16:42:34 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/28 16:44:31 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8437 s / batch. (data: 1.02e-03). ETA=11:53:58, max mem: 20.9 GB 
[11/28 16:46:23 visual_prompt]: 	Training 200/553. train loss: 1.3922,	0.8282 s / batch. (data: 5.51e-03). ETA=11:39:31, max mem: 20.9 GB 
[11/28 16:48:18 visual_prompt]: 	Training 300/553. train loss: 9.8045,	2.4874 s / batch. (data: 1.67e+00). ETA=1 day, 10:56:44, max mem: 20.9 GB 
[11/28 16:50:12 visual_prompt]: 	Training 400/553. train loss: 16.5693,	0.8475 s / batch. (data: 2.44e-02). ETA=11:52:57, max mem: 20.9 GB 
[11/28 16:52:06 visual_prompt]: 	Training 500/553. train loss: 1.3801,	1.3452 s / batch. (data: 5.04e-01). ETA=18:49:26, max mem: 20.9 GB 
[11/28 16:53:05 visual_prompt]: Epoch 9 / 100: avg data time: 3.07e-01, avg batch time: 1.1411, average train loss: 5.9887
[11/28 16:54:12 visual_prompt]: Inference (val):avg data time: 6.77e-05, avg batch time: 0.3110, average loss: 4.5217
[11/28 16:54:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.49	
[11/28 16:54:12 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/28 16:56:14 visual_prompt]: 	Training 100/553. train loss: 20.1672,	0.8432 s / batch. (data: 6.69e-04). ETA=11:45:50, max mem: 20.9 GB 
[11/28 16:58:06 visual_prompt]: 	Training 200/553. train loss: 7.2029,	0.8491 s / batch. (data: 2.88e-02). ETA=11:49:18, max mem: 20.9 GB 
[11/28 17:00:02 visual_prompt]: 	Training 300/553. train loss: 16.4779,	4.0763 s / batch. (data: 3.24e+00). ETA=2 days, 8:38:31, max mem: 20.9 GB 
[11/28 17:01:54 visual_prompt]: 	Training 400/553. train loss: 4.8384,	1.3043 s / batch. (data: 4.72e-01). ETA=18:05:13, max mem: 20.9 GB 
[11/28 17:03:49 visual_prompt]: 	Training 500/553. train loss: 27.9044,	0.8505 s / batch. (data: 1.05e-02). ETA=11:46:13, max mem: 20.9 GB 
[11/28 17:04:49 visual_prompt]: Epoch 10 / 100: avg data time: 3.17e-01, avg batch time: 1.1507, average train loss: 9.6479
[11/28 17:05:55 visual_prompt]: Inference (val):avg data time: 6.31e-05, avg batch time: 0.3100, average loss: 12.2237
[11/28 17:05:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.87	
[11/28 17:05:55 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/28 17:07:58 visual_prompt]: 	Training 100/553. train loss: 0.4391,	0.8202 s / batch. (data: 5.27e-04). ETA=11:19:00, max mem: 20.9 GB 
[11/28 17:09:54 visual_prompt]: 	Training 200/553. train loss: 0.8890,	0.8400 s / batch. (data: 9.08e-04). ETA=11:34:00, max mem: 20.9 GB 
[11/28 17:11:46 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.6534 s / batch. (data: 1.82e+00). ETA=1 day, 12:27:43, max mem: 20.9 GB 
[11/28 17:13:36 visual_prompt]: 	Training 400/553. train loss: 2.5199,	0.8238 s / batch. (data: 8.37e-04). ETA=11:17:49, max mem: 20.9 GB 
[11/28 17:15:27 visual_prompt]: 	Training 500/553. train loss: 3.1253,	0.8174 s / batch. (data: 3.50e-04). ETA=11:11:12, max mem: 20.9 GB 
[11/28 17:16:27 visual_prompt]: Epoch 11 / 100: avg data time: 3.08e-01, avg batch time: 1.1427, average train loss: 5.9548
[11/28 17:17:35 visual_prompt]: Inference (val):avg data time: 8.00e-05, avg batch time: 0.3095, average loss: 8.7608
[11/28 17:17:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.98	
[11/28 17:17:35 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/28 17:19:36 visual_prompt]: 	Training 100/553. train loss: 4.0551,	0.8421 s / batch. (data: 1.56e-02). ETA=11:29:22, max mem: 20.9 GB 
[11/28 17:21:31 visual_prompt]: 	Training 200/553. train loss: 3.1000,	0.9659 s / batch. (data: 1.34e-01). ETA=13:09:03, max mem: 20.9 GB 
[11/28 17:23:24 visual_prompt]: 	Training 300/553. train loss: 2.7414,	0.8519 s / batch. (data: 1.07e-03). ETA=11:34:33, max mem: 20.9 GB 
[11/28 17:25:21 visual_prompt]: 	Training 400/553. train loss: 3.0529,	0.8496 s / batch. (data: 1.33e-02). ETA=11:31:14, max mem: 20.9 GB 
[11/28 17:27:16 visual_prompt]: 	Training 500/553. train loss: 39.8382,	0.8235 s / batch. (data: 1.14e-03). ETA=11:08:39, max mem: 20.9 GB 
[11/28 17:28:15 visual_prompt]: Epoch 12 / 100: avg data time: 3.22e-01, avg batch time: 1.1567, average train loss: 7.5559
[11/28 17:29:21 visual_prompt]: Inference (val):avg data time: 6.15e-05, avg batch time: 0.3104, average loss: 7.9266
[11/28 17:29:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.23	
[11/28 17:29:21 visual_prompt]: Best epoch 12: best metric: -7.927
[11/28 17:29:21 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/28 17:31:21 visual_prompt]: 	Training 100/553. train loss: 1.6612,	0.8281 s / batch. (data: 3.51e-04). ETA=11:10:17, max mem: 20.9 GB 
[11/28 17:33:14 visual_prompt]: 	Training 200/553. train loss: 1.3221,	0.8523 s / batch. (data: 8.64e-04). ETA=11:28:23, max mem: 20.9 GB 
[11/28 17:35:11 visual_prompt]: 	Training 300/553. train loss: 1.4190,	2.6562 s / batch. (data: 1.82e+00). ETA=1 day, 11:41:06, max mem: 20.9 GB 
[11/28 17:37:05 visual_prompt]: 	Training 400/553. train loss: 28.4845,	0.8433 s / batch. (data: 2.81e-04). ETA=11:18:19, max mem: 20.9 GB 
[11/28 17:39:03 visual_prompt]: 	Training 500/553. train loss: 0.2549,	0.8441 s / batch. (data: 1.19e-02). ETA=11:17:33, max mem: 20.9 GB 
[11/28 17:40:02 visual_prompt]: Epoch 13 / 100: avg data time: 3.22e-01, avg batch time: 1.1584, average train loss: 6.1947
[11/28 17:41:09 visual_prompt]: Inference (val):avg data time: 6.42e-05, avg batch time: 0.3113, average loss: 1.4006
[11/28 17:41:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.02	
[11/28 17:41:09 visual_prompt]: Best epoch 13: best metric: -1.401
[11/28 17:41:09 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/28 17:43:11 visual_prompt]: 	Training 100/553. train loss: 8.7893,	0.8441 s / batch. (data: 7.92e-03). ETA=11:15:26, max mem: 20.9 GB 
[11/28 17:45:07 visual_prompt]: 	Training 200/553. train loss: 0.0000,	2.0131 s / batch. (data: 1.19e+00). ETA=1 day, 2:47:30, max mem: 20.9 GB 
[11/28 17:47:04 visual_prompt]: 	Training 300/553. train loss: 0.4008,	1.3790 s / batch. (data: 5.54e-01). ETA=18:18:53, max mem: 20.9 GB 
[11/28 17:48:58 visual_prompt]: 	Training 400/553. train loss: 2.3632,	0.8359 s / batch. (data: 6.43e-03). ETA=11:04:42, max mem: 20.9 GB 
[11/28 17:50:55 visual_prompt]: 	Training 500/553. train loss: 16.9422,	0.8416 s / batch. (data: 1.58e-02). ETA=11:07:49, max mem: 20.9 GB 
[11/28 17:51:53 visual_prompt]: Epoch 14 / 100: avg data time: 3.29e-01, avg batch time: 1.1639, average train loss: 4.8561
[11/28 17:53:01 visual_prompt]: Inference (val):avg data time: 7.48e-05, avg batch time: 0.3107, average loss: 5.0464
[11/28 17:53:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.00	
[11/28 17:53:01 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/28 17:55:02 visual_prompt]: 	Training 100/553. train loss: 9.6415,	0.8358 s / batch. (data: 3.37e-04). ETA=11:01:07, max mem: 20.9 GB 
[11/28 17:56:58 visual_prompt]: 	Training 200/553. train loss: 0.2219,	0.8576 s / batch. (data: 7.70e-04). ETA=11:16:55, max mem: 20.9 GB 
[11/28 17:58:55 visual_prompt]: 	Training 300/553. train loss: 9.0622,	0.8558 s / batch. (data: 9.59e-04). ETA=11:14:02, max mem: 20.9 GB 
[11/28 18:00:48 visual_prompt]: 	Training 400/553. train loss: 7.6745,	0.8236 s / batch. (data: 9.81e-04). ETA=10:47:18, max mem: 20.9 GB 
[11/28 18:02:45 visual_prompt]: 	Training 500/553. train loss: 3.0960,	0.8194 s / batch. (data: 3.59e-04). ETA=10:42:39, max mem: 20.9 GB 
[11/28 18:03:46 visual_prompt]: Epoch 15 / 100: avg data time: 3.30e-01, avg batch time: 1.1652, average train loss: 8.3364
[11/28 18:04:53 visual_prompt]: Inference (val):avg data time: 1.61e-04, avg batch time: 0.3097, average loss: 7.1108
[11/28 18:04:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.17	
[11/28 18:04:53 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/28 18:06:51 visual_prompt]: 	Training 100/553. train loss: 1.6939,	0.8482 s / batch. (data: 1.07e-02). ETA=11:03:04, max mem: 20.9 GB 
[11/28 18:08:46 visual_prompt]: 	Training 200/553. train loss: 35.5122,	0.8281 s / batch. (data: 5.17e-04). ETA=10:45:57, max mem: 20.9 GB 
[11/28 18:10:42 visual_prompt]: 	Training 300/553. train loss: 23.5092,	0.8352 s / batch. (data: 2.99e-04). ETA=10:50:10, max mem: 20.9 GB 
[11/28 18:12:36 visual_prompt]: 	Training 400/553. train loss: 0.4262,	0.8396 s / batch. (data: 1.02e-02). ETA=10:52:10, max mem: 20.9 GB 
[11/28 18:14:31 visual_prompt]: 	Training 500/553. train loss: 1.3423,	1.7348 s / batch. (data: 9.08e-01). ETA=22:24:37, max mem: 20.9 GB 
[11/28 18:15:31 visual_prompt]: Epoch 16 / 100: avg data time: 3.20e-01, avg batch time: 1.1532, average train loss: 6.4424
[11/28 18:16:38 visual_prompt]: Inference (val):avg data time: 6.54e-05, avg batch time: 0.3094, average loss: 1.2462
[11/28 18:16:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 63.31	
[11/28 18:16:38 visual_prompt]: Best epoch 16: best metric: -1.246
[11/28 18:16:38 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/28 18:18:38 visual_prompt]: 	Training 100/553. train loss: 3.2804,	0.8676 s / batch. (data: 1.12e-02). ETA=11:10:12, max mem: 20.9 GB 
[11/28 18:20:35 visual_prompt]: 	Training 200/553. train loss: 15.9472,	0.8410 s / batch. (data: 6.92e-03). ETA=10:48:16, max mem: 20.9 GB 
[11/28 18:22:29 visual_prompt]: 	Training 300/553. train loss: 2.5579,	0.8372 s / batch. (data: 1.07e-03). ETA=10:43:58, max mem: 20.9 GB 
[11/28 18:24:22 visual_prompt]: 	Training 400/553. train loss: 2.8205,	1.5039 s / batch. (data: 6.79e-01). ETA=19:14:16, max mem: 20.9 GB 
[11/28 18:26:16 visual_prompt]: 	Training 500/553. train loss: 9.4810,	2.2197 s / batch. (data: 1.37e+00). ETA=1 day, 4:19:57, max mem: 20.9 GB 
[11/28 18:27:16 visual_prompt]: Epoch 17 / 100: avg data time: 3.20e-01, avg batch time: 1.1544, average train loss: 5.8217
[11/28 18:28:24 visual_prompt]: Inference (val):avg data time: 7.33e-05, avg batch time: 0.3105, average loss: 2.4304
[11/28 18:28:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 65.42	
[11/28 18:28:24 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/28 18:30:22 visual_prompt]: 	Training 100/553. train loss: 5.2362,	0.8287 s / batch. (data: 7.47e-04). ETA=10:32:34, max mem: 20.9 GB 
[11/28 18:32:20 visual_prompt]: 	Training 200/553. train loss: 10.0353,	0.8520 s / batch. (data: 3.57e-04). ETA=10:48:55, max mem: 20.9 GB 
[11/28 18:34:16 visual_prompt]: 	Training 300/553. train loss: 3.7636,	0.8342 s / batch. (data: 1.59e-02). ETA=10:33:58, max mem: 20.9 GB 
[11/28 18:36:10 visual_prompt]: 	Training 400/553. train loss: 2.8356,	0.8392 s / batch. (data: 1.19e-02). ETA=10:36:24, max mem: 20.9 GB 
[11/28 18:38:03 visual_prompt]: 	Training 500/553. train loss: 3.3602,	0.8360 s / batch. (data: 5.47e-03). ETA=10:32:33, max mem: 20.9 GB 
[11/28 18:39:01 visual_prompt]: Epoch 18 / 100: avg data time: 3.19e-01, avg batch time: 1.1522, average train loss: 4.8105
[11/28 18:40:08 visual_prompt]: Inference (val):avg data time: 6.71e-05, avg batch time: 0.3126, average loss: 3.5719
[11/28 18:40:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.67	
[11/28 18:40:08 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/28 18:42:10 visual_prompt]: 	Training 100/553. train loss: 10.9852,	0.9280 s / batch. (data: 8.76e-02). ETA=11:39:48, max mem: 20.9 GB 
[11/28 18:44:07 visual_prompt]: 	Training 200/553. train loss: 2.1276,	0.8202 s / batch. (data: 7.80e-04). ETA=10:17:08, max mem: 20.9 GB 
[11/28 18:46:00 visual_prompt]: 	Training 300/553. train loss: 16.7522,	0.9122 s / batch. (data: 9.40e-02). ETA=11:24:52, max mem: 20.9 GB 
[11/28 18:47:55 visual_prompt]: 	Training 400/553. train loss: 0.8231,	0.8459 s / batch. (data: 1.06e-02). ETA=10:33:41, max mem: 20.9 GB 
[11/28 18:49:45 visual_prompt]: 	Training 500/553. train loss: 5.8514,	0.8332 s / batch. (data: 1.09e-03). ETA=10:22:43, max mem: 20.9 GB 
[11/28 18:50:45 visual_prompt]: Epoch 19 / 100: avg data time: 3.17e-01, avg batch time: 1.1514, average train loss: 5.4547
[11/28 18:51:52 visual_prompt]: Inference (val):avg data time: 7.49e-04, avg batch time: 0.3107, average loss: 15.1166
[11/28 18:51:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.62	
[11/28 18:51:52 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/28 18:53:50 visual_prompt]: 	Training 100/553. train loss: 7.5258,	0.8199 s / batch. (data: 3.32e-04). ETA=10:10:45, max mem: 20.9 GB 
[11/28 18:55:46 visual_prompt]: 	Training 200/553. train loss: 0.0937,	0.8253 s / batch. (data: 8.30e-04). ETA=10:13:22, max mem: 20.9 GB 
[11/28 18:57:41 visual_prompt]: 	Training 300/553. train loss: 11.4970,	0.8283 s / batch. (data: 3.13e-04). ETA=10:14:11, max mem: 20.9 GB 
[11/28 18:59:36 visual_prompt]: 	Training 400/553. train loss: 7.0201,	0.8808 s / batch. (data: 1.38e-02). ETA=10:51:41, max mem: 20.9 GB 
[11/28 19:01:30 visual_prompt]: 	Training 500/553. train loss: 37.8318,	0.8193 s / batch. (data: 1.25e-03). ETA=10:04:49, max mem: 20.9 GB 
[11/28 19:02:32 visual_prompt]: Epoch 20 / 100: avg data time: 3.22e-01, avg batch time: 1.1567, average train loss: 7.4035
[11/28 19:03:39 visual_prompt]: Inference (val):avg data time: 6.86e-05, avg batch time: 0.3111, average loss: 4.7180
[11/28 19:03:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.29	
[11/28 19:03:39 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[11/28 19:05:44 visual_prompt]: 	Training 100/553. train loss: 1.1744,	0.8287 s / batch. (data: 3.51e-04). ETA=10:09:38, max mem: 20.9 GB 
[11/28 19:07:38 visual_prompt]: 	Training 200/553. train loss: 15.9846,	0.8282 s / batch. (data: 1.13e-02). ETA=10:07:52, max mem: 20.9 GB 
[11/28 19:09:32 visual_prompt]: 	Training 300/553. train loss: 3.3645,	1.6240 s / batch. (data: 7.93e-01). ETA=19:49:18, max mem: 20.9 GB 
[11/28 19:11:26 visual_prompt]: 	Training 400/553. train loss: 11.6615,	0.8243 s / batch. (data: 1.13e-03). ETA=10:02:17, max mem: 20.9 GB 
[11/28 19:13:23 visual_prompt]: 	Training 500/553. train loss: 7.7146,	0.8440 s / batch. (data: 7.86e-03). ETA=10:15:15, max mem: 20.9 GB 
[11/28 19:14:21 visual_prompt]: Epoch 21 / 100: avg data time: 3.27e-01, avg batch time: 1.1611, average train loss: 7.2807
[11/28 19:15:30 visual_prompt]: Inference (val):avg data time: 7.23e-05, avg batch time: 0.3101, average loss: 4.3996
[11/28 19:15:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.04	
[11/28 19:15:30 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[11/28 19:17:31 visual_prompt]: 	Training 100/553. train loss: 11.0785,	0.8160 s / batch. (data: 3.29e-04). ETA=9:52:48, max mem: 20.9 GB 
[11/28 19:19:29 visual_prompt]: 	Training 200/553. train loss: 3.7789,	0.8576 s / batch. (data: 8.06e-03). ETA=10:21:32, max mem: 20.9 GB 
[11/28 19:21:22 visual_prompt]: 	Training 300/553. train loss: 0.0005,	0.8357 s / batch. (data: 8.01e-03). ETA=10:04:16, max mem: 20.9 GB 
[11/28 19:23:18 visual_prompt]: 	Training 400/553. train loss: 1.1607,	0.8383 s / batch. (data: 3.33e-04). ETA=10:04:45, max mem: 20.9 GB 
[11/28 19:25:15 visual_prompt]: 	Training 500/553. train loss: 1.8572,	0.8713 s / batch. (data: 1.54e-02). ETA=10:27:10, max mem: 20.9 GB 
[11/28 19:26:16 visual_prompt]: Epoch 22 / 100: avg data time: 3.34e-01, avg batch time: 1.1692, average train loss: 5.3035
[11/28 19:27:25 visual_prompt]: Inference (val):avg data time: 2.57e-04, avg batch time: 0.3108, average loss: 5.3951
[11/28 19:27:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.83	
[11/28 19:27:25 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[11/28 19:29:28 visual_prompt]: 	Training 100/553. train loss: 2.2837,	0.8365 s / batch. (data: 3.11e-04). ETA=9:59:59, max mem: 20.9 GB 
[11/28 19:31:26 visual_prompt]: 	Training 200/553. train loss: 3.1618,	0.8350 s / batch. (data: 1.15e-02). ETA=9:57:29, max mem: 20.9 GB 
[11/28 19:33:25 visual_prompt]: 	Training 300/553. train loss: 0.9660,	0.8501 s / batch. (data: 9.83e-04). ETA=10:06:52, max mem: 20.9 GB 
[11/28 19:35:19 visual_prompt]: 	Training 400/553. train loss: 0.7535,	0.8566 s / batch. (data: 2.93e-03). ETA=10:10:05, max mem: 20.9 GB 
[11/28 19:37:15 visual_prompt]: 	Training 500/553. train loss: 17.1139,	0.8542 s / batch. (data: 1.06e-03). ETA=10:06:56, max mem: 20.9 GB 
[11/28 19:38:15 visual_prompt]: Epoch 23 / 100: avg data time: 3.40e-01, avg batch time: 1.1755, average train loss: 6.0506
[11/28 19:39:24 visual_prompt]: Inference (val):avg data time: 8.65e-05, avg batch time: 0.3106, average loss: 1.1173
[11/28 19:39:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 69.92	
[11/28 19:39:24 visual_prompt]: Best epoch 23: best metric: -1.117
[11/28 19:39:24 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[11/28 19:41:23 visual_prompt]: 	Training 100/553. train loss: 3.6433,	0.8360 s / batch. (data: 5.54e-03). ETA=9:51:53, max mem: 20.9 GB 
[11/28 19:43:20 visual_prompt]: 	Training 200/553. train loss: 2.6525,	0.8487 s / batch. (data: 1.87e-02). ETA=9:59:28, max mem: 20.9 GB 
[11/28 19:45:17 visual_prompt]: 	Training 300/553. train loss: 4.9809,	1.3353 s / batch. (data: 5.09e-01). ETA=15:40:56, max mem: 20.9 GB 
[11/28 19:47:16 visual_prompt]: 	Training 400/553. train loss: 0.7819,	0.8515 s / batch. (data: 2.24e-02). ETA=9:58:36, max mem: 20.9 GB 
[11/28 19:49:16 visual_prompt]: 	Training 500/553. train loss: 0.2565,	0.8520 s / batch. (data: 1.10e-03). ETA=9:57:31, max mem: 20.9 GB 
[11/28 19:50:18 visual_prompt]: Epoch 24 / 100: avg data time: 3.48e-01, avg batch time: 1.1829, average train loss: 4.8728
[11/28 19:51:28 visual_prompt]: Inference (val):avg data time: 8.59e-05, avg batch time: 0.3108, average loss: 8.6880
[11/28 19:51:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 70.08	
[11/28 19:51:28 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[11/28 19:53:36 visual_prompt]: 	Training 100/553. train loss: 11.0764,	0.8370 s / batch. (data: 9.01e-04). ETA=9:44:53, max mem: 20.9 GB 
[11/28 19:55:31 visual_prompt]: 	Training 200/553. train loss: 3.3035,	0.8370 s / batch. (data: 1.09e-02). ETA=9:43:28, max mem: 20.9 GB 
[11/28 19:57:27 visual_prompt]: 	Training 300/553. train loss: 4.5802,	1.6200 s / batch. (data: 7.73e-01). ETA=18:46:40, max mem: 20.9 GB 
[11/28 19:59:26 visual_prompt]: 	Training 400/553. train loss: 6.3984,	2.1866 s / batch. (data: 1.36e+00). ETA=1 day, 1:17:03, max mem: 20.9 GB 
[11/28 20:01:25 visual_prompt]: 	Training 500/553. train loss: 2.0448,	2.2142 s / batch. (data: 1.39e+00). ETA=1 day, 1:32:29, max mem: 20.9 GB 
[11/28 20:02:25 visual_prompt]: Epoch 25 / 100: avg data time: 3.51e-01, avg batch time: 1.1869, average train loss: 6.7018
[11/28 20:03:35 visual_prompt]: Inference (val):avg data time: 3.56e-04, avg batch time: 0.3130, average loss: 15.9009
[11/28 20:03:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.48	
[11/28 20:03:35 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[11/28 20:05:38 visual_prompt]: 	Training 100/553. train loss: 10.9048,	0.8470 s / batch. (data: 7.94e-04). ETA=9:44:04, max mem: 20.9 GB 
[11/28 20:07:38 visual_prompt]: 	Training 200/553. train loss: 16.0287,	2.8647 s / batch. (data: 2.03e+00). ETA=1 day, 8:50:41, max mem: 20.9 GB 
[11/28 20:09:38 visual_prompt]: 	Training 300/553. train loss: 5.3045,	0.8560 s / batch. (data: 7.78e-03). ETA=9:47:26, max mem: 20.9 GB 
[11/28 20:11:34 visual_prompt]: 	Training 400/553. train loss: 1.7387,	0.8167 s / batch. (data: 8.16e-04). ETA=9:19:07, max mem: 20.9 GB 
[11/28 20:13:31 visual_prompt]: 	Training 500/553. train loss: 5.1397,	0.8655 s / batch. (data: 3.33e-02). ETA=9:51:04, max mem: 20.9 GB 
[11/28 20:14:32 visual_prompt]: Epoch 26 / 100: avg data time: 3.51e-01, avg batch time: 1.1872, average train loss: 5.4226
[11/28 20:15:41 visual_prompt]: Inference (val):avg data time: 8.92e-05, avg batch time: 0.3117, average loss: 1.9294
[11/28 20:15:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 71.24	
[11/28 20:15:41 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[11/28 20:17:46 visual_prompt]: 	Training 100/553. train loss: 7.7995,	0.8591 s / batch. (data: 1.10e-02). ETA=9:44:31, max mem: 20.9 GB 
[11/28 20:19:43 visual_prompt]: 	Training 200/553. train loss: 17.9861,	1.9000 s / batch. (data: 1.07e+00). ETA=21:29:31, max mem: 20.9 GB 
[11/28 20:21:41 visual_prompt]: 	Training 300/553. train loss: 6.7700,	1.1286 s / batch. (data: 3.05e-01). ETA=12:44:06, max mem: 20.9 GB 
[11/28 20:23:38 visual_prompt]: 	Training 400/553. train loss: 9.3514,	0.8555 s / batch. (data: 5.53e-03). ETA=9:37:45, max mem: 20.9 GB 
[11/28 20:25:35 visual_prompt]: 	Training 500/553. train loss: 5.1421,	0.8508 s / batch. (data: 2.08e-03). ETA=9:33:10, max mem: 20.9 GB 
[11/28 20:26:34 visual_prompt]: Epoch 27 / 100: avg data time: 3.45e-01, avg batch time: 1.1804, average train loss: 5.3024
[11/28 20:27:43 visual_prompt]: Inference (val):avg data time: 9.02e-05, avg batch time: 0.3097, average loss: 2.4663
[11/28 20:27:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 72.00	
[11/28 20:27:43 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[11/28 20:29:46 visual_prompt]: 	Training 100/553. train loss: 3.7675,	0.8235 s / batch. (data: 5.49e-04). ETA=9:12:40, max mem: 20.9 GB 
[11/28 20:31:44 visual_prompt]: 	Training 200/553. train loss: 23.0866,	0.8361 s / batch. (data: 3.34e-04). ETA=9:19:45, max mem: 20.9 GB 
[11/28 20:33:42 visual_prompt]: 	Training 300/553. train loss: 22.1772,	2.2161 s / batch. (data: 1.36e+00). ETA=1 day, 0:39:56, max mem: 20.9 GB 
[11/28 20:35:39 visual_prompt]: 	Training 400/553. train loss: 19.7659,	0.8274 s / batch. (data: 3.15e-04). ETA=9:11:08, max mem: 20.9 GB 
[11/28 20:37:35 visual_prompt]: 	Training 500/553. train loss: 0.9137,	0.8480 s / batch. (data: 3.15e-04). ETA=9:23:28, max mem: 20.9 GB 
[11/28 20:38:37 visual_prompt]: Epoch 28 / 100: avg data time: 3.47e-01, avg batch time: 1.1832, average train loss: 5.5347
[11/28 20:39:46 visual_prompt]: Inference (val):avg data time: 9.43e-05, avg batch time: 0.3126, average loss: 1.6315
[11/28 20:39:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 69.93	
[11/28 20:39:46 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[11/28 20:41:57 visual_prompt]: 	Training 100/553. train loss: 0.6146,	0.8205 s / batch. (data: 7.46e-04). ETA=9:03:06, max mem: 20.9 GB 
[11/28 20:43:55 visual_prompt]: 	Training 200/553. train loss: 22.7147,	2.4803 s / batch. (data: 1.66e+00). ETA=1 day, 3:17:39, max mem: 20.9 GB 
[11/28 20:45:49 visual_prompt]: 	Training 300/553. train loss: 14.2245,	0.8321 s / batch. (data: 7.76e-04). ETA=9:08:01, max mem: 20.9 GB 
[11/28 20:47:42 visual_prompt]: 	Training 400/553. train loss: 11.3983,	1.7628 s / batch. (data: 9.39e-01). ETA=19:18:03, max mem: 20.9 GB 
[11/28 20:49:40 visual_prompt]: 	Training 500/553. train loss: 14.5313,	0.8629 s / batch. (data: 1.07e-02). ETA=9:25:26, max mem: 20.9 GB 
[11/28 20:50:40 visual_prompt]: Epoch 29 / 100: avg data time: 3.48e-01, avg batch time: 1.1826, average train loss: 6.5519
[11/28 20:51:48 visual_prompt]: Inference (val):avg data time: 7.77e-05, avg batch time: 0.3097, average loss: 5.4480
[11/28 20:51:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 71.10	
[11/28 20:51:48 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[11/28 20:53:48 visual_prompt]: 	Training 100/553. train loss: 4.4012,	0.8219 s / batch. (data: 1.04e-03). ETA=8:56:27, max mem: 20.9 GB 
[11/28 20:55:44 visual_prompt]: 	Training 200/553. train loss: 2.6239,	0.8379 s / batch. (data: 3.14e-04). ETA=9:05:30, max mem: 20.9 GB 
[11/28 20:57:37 visual_prompt]: 	Training 300/553. train loss: 0.0003,	0.9723 s / batch. (data: 1.48e-01). ETA=10:31:22, max mem: 20.9 GB 
[11/28 20:59:36 visual_prompt]: 	Training 400/553. train loss: 9.0745,	1.5200 s / batch. (data: 6.80e-01). ETA=16:24:31, max mem: 20.9 GB 
[11/28 21:01:32 visual_prompt]: 	Training 500/553. train loss: 1.3446,	2.2995 s / batch. (data: 1.46e+00). ETA=1 day, 0:45:36, max mem: 20.9 GB 
[11/28 21:02:34 visual_prompt]: Epoch 30 / 100: avg data time: 3.32e-01, avg batch time: 1.1671, average train loss: 4.8709
[11/28 21:03:41 visual_prompt]: Inference (val):avg data time: 6.94e-05, avg batch time: 0.3109, average loss: 1.7702
[11/28 21:03:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 71.46	
[11/28 21:03:41 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[11/28 21:05:43 visual_prompt]: 	Training 100/553. train loss: 3.3388,	0.8331 s / batch. (data: 5.42e-03). ETA=8:56:06, max mem: 20.9 GB 
[11/28 21:07:43 visual_prompt]: 	Training 200/553. train loss: 5.7669,	0.8488 s / batch. (data: 1.07e-02). ETA=9:04:47, max mem: 20.9 GB 
[11/28 21:09:35 visual_prompt]: 	Training 300/553. train loss: 1.8027,	0.8735 s / batch. (data: 3.78e-02). ETA=9:19:10, max mem: 20.9 GB 
[11/28 21:11:29 visual_prompt]: 	Training 400/553. train loss: 0.7635,	0.8497 s / batch. (data: 1.19e-02). ETA=9:02:31, max mem: 20.9 GB 
[11/28 21:13:24 visual_prompt]: 	Training 500/553. train loss: 4.7992,	0.8640 s / batch. (data: 6.18e-03). ETA=9:10:12, max mem: 20.9 GB 
[11/28 21:14:23 visual_prompt]: Epoch 31 / 100: avg data time: 3.26e-01, avg batch time: 1.1604, average train loss: 5.5309
[11/28 21:15:30 visual_prompt]: Inference (val):avg data time: 7.14e-05, avg batch time: 0.3090, average loss: 2.6270
[11/28 21:15:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 73.17	
[11/28 21:15:30 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[11/28 21:17:32 visual_prompt]: 	Training 100/553. train loss: 3.3297,	0.8456 s / batch. (data: 2.18e-03). ETA=8:56:20, max mem: 20.9 GB 
[11/28 21:19:29 visual_prompt]: 	Training 200/553. train loss: 0.2150,	0.8399 s / batch. (data: 1.32e-03). ETA=8:51:21, max mem: 20.9 GB 
[11/28 21:21:29 visual_prompt]: 	Training 300/553. train loss: 12.3047,	0.8144 s / batch. (data: 3.54e-04). ETA=8:33:51, max mem: 20.9 GB 
[11/28 21:23:24 visual_prompt]: 	Training 400/553. train loss: 4.1105,	0.8502 s / batch. (data: 7.79e-03). ETA=8:55:00, max mem: 20.9 GB 
[11/28 21:25:16 visual_prompt]: 	Training 500/553. train loss: 3.4288,	0.8444 s / batch. (data: 3.42e-04). ETA=8:49:56, max mem: 20.9 GB 
[11/28 21:26:13 visual_prompt]: Epoch 32 / 100: avg data time: 3.28e-01, avg batch time: 1.1630, average train loss: 4.1184
[11/28 21:27:20 visual_prompt]: Inference (val):avg data time: 6.74e-05, avg batch time: 0.3105, average loss: 2.0776
[11/28 21:27:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 68.07	
[11/28 21:27:20 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[11/28 21:29:21 visual_prompt]: 	Training 100/553. train loss: 1.1823,	0.9127 s / batch. (data: 4.85e-02). ETA=9:30:31, max mem: 20.9 GB 
[11/28 21:31:20 visual_prompt]: 	Training 200/553. train loss: 3.7484,	2.4229 s / batch. (data: 1.60e+00). ETA=1 day, 1:10:25, max mem: 20.9 GB 
[11/28 21:33:17 visual_prompt]: 	Training 300/553. train loss: 2.3543,	0.8255 s / batch. (data: 5.38e-04). ETA=8:33:15, max mem: 20.9 GB 
[11/28 21:35:13 visual_prompt]: 	Training 400/553. train loss: 1.3910,	0.8689 s / batch. (data: 2.50e-02). ETA=8:58:45, max mem: 20.9 GB 
[11/28 21:37:08 visual_prompt]: 	Training 500/553. train loss: 5.4955,	1.9890 s / batch. (data: 1.15e+00). ETA=20:29:58, max mem: 20.9 GB 
[11/28 21:38:06 visual_prompt]: Epoch 33 / 100: avg data time: 3.34e-01, avg batch time: 1.1682, average train loss: 5.7833
[11/28 21:39:13 visual_prompt]: Inference (val):avg data time: 7.73e-05, avg batch time: 0.3121, average loss: 3.3489
[11/28 21:39:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 71.64	
[11/28 21:39:13 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[11/28 21:41:15 visual_prompt]: 	Training 100/553. train loss: 4.8498,	1.4211 s / batch. (data: 5.89e-01). ETA=14:35:09, max mem: 20.9 GB 
[11/28 21:43:08 visual_prompt]: 	Training 200/553. train loss: 13.0847,	0.8480 s / batch. (data: 3.27e-04). ETA=8:40:49, max mem: 20.9 GB 
[11/28 21:45:04 visual_prompt]: 	Training 300/553. train loss: 1.1124,	0.8326 s / batch. (data: 8.48e-04). ETA=8:29:59, max mem: 20.9 GB 
[11/28 21:47:00 visual_prompt]: 	Training 400/553. train loss: 0.3108,	0.8378 s / batch. (data: 1.18e-02). ETA=8:31:44, max mem: 20.9 GB 
[11/28 21:48:54 visual_prompt]: 	Training 500/553. train loss: 2.7579,	1.9906 s / batch. (data: 1.16e+00). ETA=20:12:39, max mem: 20.9 GB 
[11/28 21:49:53 visual_prompt]: Epoch 34 / 100: avg data time: 3.23e-01, avg batch time: 1.1564, average train loss: 4.3912
[11/28 21:50:59 visual_prompt]: Inference (val):avg data time: 6.13e-05, avg batch time: 0.3101, average loss: 2.7150
[11/28 21:50:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 74.03	
[11/28 21:50:59 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[11/28 21:53:01 visual_prompt]: 	Training 100/553. train loss: 1.7249,	0.8416 s / batch. (data: 2.33e-03). ETA=8:30:32, max mem: 20.9 GB 
[11/28 21:54:58 visual_prompt]: 	Training 200/553. train loss: 6.6556,	0.8388 s / batch. (data: 1.06e-02). ETA=8:27:25, max mem: 20.9 GB 
[11/28 21:56:51 visual_prompt]: 	Training 300/553. train loss: 13.3379,	0.8302 s / batch. (data: 8.24e-04). ETA=8:20:51, max mem: 20.9 GB 
[11/28 21:58:45 visual_prompt]: 	Training 400/553. train loss: 14.0743,	1.4299 s / batch. (data: 5.96e-01). ETA=14:20:16, max mem: 20.9 GB 
[11/28 22:00:40 visual_prompt]: 	Training 500/553. train loss: 2.7219,	1.1443 s / batch. (data: 3.18e-01). ETA=11:26:32, max mem: 20.9 GB 
[11/28 22:01:41 visual_prompt]: Epoch 35 / 100: avg data time: 3.27e-01, avg batch time: 1.1600, average train loss: 5.7482
[11/28 22:02:48 visual_prompt]: Inference (val):avg data time: 6.92e-05, avg batch time: 0.3118, average loss: 6.5095
[11/28 22:02:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 71.10	
[11/28 22:02:48 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[11/28 22:04:47 visual_prompt]: 	Training 100/553. train loss: 14.5993,	0.8279 s / batch. (data: 3.49e-04). ETA=8:14:37, max mem: 20.9 GB 
[11/28 22:06:44 visual_prompt]: 	Training 200/553. train loss: 6.4547,	1.0845 s / batch. (data: 2.45e-01). ETA=10:46:05, max mem: 20.9 GB 
[11/28 22:08:42 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8580 s / batch. (data: 2.57e-02). ETA=8:29:42, max mem: 20.9 GB 
[11/28 22:10:37 visual_prompt]: 	Training 400/553. train loss: 2.0341,	0.8325 s / batch. (data: 3.16e-04). ETA=8:13:12, max mem: 20.9 GB 
[11/28 22:12:34 visual_prompt]: 	Training 500/553. train loss: 4.2121,	1.5720 s / batch. (data: 7.38e-01). ETA=15:28:38, max mem: 20.9 GB 
[11/28 22:13:30 visual_prompt]: Epoch 36 / 100: avg data time: 3.27e-01, avg batch time: 1.1614, average train loss: 5.4749
[11/28 22:14:38 visual_prompt]: Inference (val):avg data time: 8.30e-05, avg batch time: 0.3115, average loss: 11.8365
[11/28 22:14:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 70.14	
[11/28 22:14:38 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[11/28 22:16:39 visual_prompt]: 	Training 100/553. train loss: 2.6765,	0.8400 s / batch. (data: 1.20e-02). ETA=8:14:05, max mem: 20.9 GB 
[11/28 22:18:36 visual_prompt]: 	Training 200/553. train loss: 4.3420,	0.8294 s / batch. (data: 3.64e-04). ETA=8:06:29, max mem: 20.9 GB 
[11/28 22:20:39 visual_prompt]: 	Training 300/553. train loss: 2.0901,	1.9561 s / batch. (data: 1.14e+00). ETA=19:04:04, max mem: 20.9 GB 
[11/28 22:22:39 visual_prompt]: 	Training 400/553. train loss: 2.6582,	2.5186 s / batch. (data: 1.70e+00). ETA=1 day, 0:28:52, max mem: 20.9 GB 
[11/28 22:24:30 visual_prompt]: 	Training 500/553. train loss: 6.6427,	1.1907 s / batch. (data: 3.49e-01). ETA=11:32:26, max mem: 20.9 GB 
[11/28 22:25:32 visual_prompt]: Epoch 37 / 100: avg data time: 3.49e-01, avg batch time: 1.1835, average train loss: 5.3824
[11/28 22:26:41 visual_prompt]: Inference (val):avg data time: 7.77e-05, avg batch time: 0.3109, average loss: 3.0341
[11/28 22:26:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 72.71	
[11/28 22:26:41 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[11/28 22:28:41 visual_prompt]: 	Training 100/553. train loss: 0.7960,	1.3620 s / batch. (data: 5.28e-01). ETA=13:08:33, max mem: 20.9 GB 
[11/28 22:30:40 visual_prompt]: 	Training 200/553. train loss: 5.3084,	2.1705 s / batch. (data: 1.35e+00). ETA=20:53:03, max mem: 20.9 GB 
[11/28 22:32:37 visual_prompt]: 	Training 300/553. train loss: 1.5758,	0.8257 s / batch. (data: 1.09e-02). ETA=7:55:19, max mem: 20.9 GB 
[11/28 22:34:30 visual_prompt]: 	Training 400/553. train loss: 3.0380,	1.1515 s / batch. (data: 3.16e-01). ETA=11:00:56, max mem: 20.9 GB 
[11/28 22:36:28 visual_prompt]: 	Training 500/553. train loss: 0.5371,	0.8437 s / batch. (data: 6.43e-03). ETA=8:02:53, max mem: 20.9 GB 
[11/28 22:37:28 visual_prompt]: Epoch 38 / 100: avg data time: 3.35e-01, avg batch time: 1.1702, average train loss: 3.9838
[11/28 22:38:36 visual_prompt]: Inference (val):avg data time: 8.00e-05, avg batch time: 0.3115, average loss: 14.4158
[11/28 22:38:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 71.77	
[11/28 22:38:36 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[11/28 22:40:37 visual_prompt]: 	Training 100/553. train loss: 1.2073,	0.8602 s / batch. (data: 8.94e-04). ETA=8:10:06, max mem: 20.9 GB 
[11/28 22:42:38 visual_prompt]: 	Training 200/553. train loss: 3.9554,	0.8402 s / batch. (data: 7.70e-03). ETA=7:57:19, max mem: 20.9 GB 
[11/28 22:44:38 visual_prompt]: 	Training 300/553. train loss: 14.1522,	0.8524 s / batch. (data: 1.20e-02). ETA=8:02:50, max mem: 20.9 GB 
[11/28 22:46:30 visual_prompt]: 	Training 400/553. train loss: 0.3010,	0.8234 s / batch. (data: 1.23e-03). ETA=7:45:01, max mem: 20.9 GB 
[11/28 22:48:28 visual_prompt]: 	Training 500/553. train loss: 1.6181,	2.1051 s / batch. (data: 1.29e+00). ETA=19:45:24, max mem: 20.9 GB 
[11/28 22:49:26 visual_prompt]: Epoch 39 / 100: avg data time: 3.41e-01, avg batch time: 1.1762, average train loss: 4.4049
[11/28 22:50:35 visual_prompt]: Inference (val):avg data time: 2.67e-04, avg batch time: 0.3111, average loss: 1.2261
[11/28 22:50:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 71.91	
[11/28 22:50:35 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[11/28 22:52:39 visual_prompt]: 	Training 100/553. train loss: 1.7057,	0.8524 s / batch. (data: 2.08e-02). ETA=7:57:48, max mem: 20.9 GB 
[11/28 22:54:36 visual_prompt]: 	Training 200/553. train loss: 0.4424,	0.8240 s / batch. (data: 3.35e-04). ETA=7:40:31, max mem: 20.9 GB 
[11/28 22:56:35 visual_prompt]: 	Training 300/553. train loss: 9.5181,	0.8606 s / batch. (data: 2.70e-02). ETA=7:59:32, max mem: 20.9 GB 
[11/28 22:58:32 visual_prompt]: 	Training 400/553. train loss: 6.5844,	0.8568 s / batch. (data: 2.95e-03). ETA=7:56:00, max mem: 20.9 GB 
[11/28 23:00:27 visual_prompt]: 	Training 500/553. train loss: 0.1665,	0.8243 s / batch. (data: 4.24e-04). ETA=7:36:34, max mem: 20.9 GB 
[11/28 23:01:30 visual_prompt]: Epoch 40 / 100: avg data time: 3.49e-01, avg batch time: 1.1833, average train loss: 5.1532
[11/28 23:02:39 visual_prompt]: Inference (val):avg data time: 9.05e-05, avg batch time: 0.3117, average loss: 3.3709
[11/28 23:02:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 72.86	
[11/28 23:02:39 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[11/28 23:04:43 visual_prompt]: 	Training 100/553. train loss: 7.4116,	0.8600 s / batch. (data: 3.09e-04). ETA=7:54:08, max mem: 20.9 GB 
[11/28 23:06:42 visual_prompt]: 	Training 200/553. train loss: 5.8050,	0.8728 s / batch. (data: 1.10e-02). ETA=7:59:45, max mem: 20.9 GB 
[11/28 23:08:36 visual_prompt]: 	Training 300/553. train loss: 3.6423,	0.8275 s / batch. (data: 8.11e-04). ETA=7:33:28, max mem: 20.9 GB 
[11/28 23:10:33 visual_prompt]: 	Training 400/553. train loss: 1.0504,	0.8554 s / batch. (data: 6.35e-03). ETA=7:47:18, max mem: 20.9 GB 
[11/28 23:12:24 visual_prompt]: 	Training 500/553. train loss: 0.5816,	0.8308 s / batch. (data: 3.80e-04). ETA=7:32:30, max mem: 20.9 GB 
[11/28 23:13:23 visual_prompt]: Epoch 41 / 100: avg data time: 3.31e-01, avg batch time: 1.1652, average train loss: 4.9769
[11/28 23:14:32 visual_prompt]: Inference (val):avg data time: 3.42e-04, avg batch time: 0.3135, average loss: 4.7466
[11/28 23:14:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 72.88	
[11/28 23:14:32 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[11/28 23:16:33 visual_prompt]: 	Training 100/553. train loss: 1.4164,	0.8369 s / batch. (data: 3.32e-04). ETA=7:33:42, max mem: 20.9 GB 
[11/28 23:18:28 visual_prompt]: 	Training 200/553. train loss: 7.5603,	0.9118 s / batch. (data: 6.21e-02). ETA=8:12:47, max mem: 20.9 GB 
[11/28 23:20:25 visual_prompt]: 	Training 300/553. train loss: 0.8108,	0.8445 s / batch. (data: 3.47e-04). ETA=7:34:58, max mem: 20.9 GB 
[11/28 23:22:23 visual_prompt]: 	Training 400/553. train loss: 1.2233,	0.8199 s / batch. (data: 3.14e-04). ETA=7:20:22, max mem: 20.9 GB 
[11/28 23:24:19 visual_prompt]: 	Training 500/553. train loss: 0.0020,	0.8537 s / batch. (data: 1.15e-02). ETA=7:37:07, max mem: 20.9 GB 
[11/28 23:25:20 visual_prompt]: Epoch 42 / 100: avg data time: 3.36e-01, avg batch time: 1.1712, average train loss: 4.2628
[11/28 23:26:28 visual_prompt]: Inference (val):avg data time: 7.14e-05, avg batch time: 0.3112, average loss: 7.5962
[11/28 23:26:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 70.97	
[11/28 23:26:28 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[11/28 23:28:32 visual_prompt]: 	Training 100/553. train loss: 2.3619,	0.8290 s / batch. (data: 2.97e-04). ETA=7:21:47, max mem: 20.9 GB 
[11/28 23:30:28 visual_prompt]: 	Training 200/553. train loss: 2.8174,	0.8561 s / batch. (data: 1.01e-03). ETA=7:34:45, max mem: 20.9 GB 
[11/28 23:32:22 visual_prompt]: 	Training 300/553. train loss: 1.1107,	0.8596 s / batch. (data: 7.78e-03). ETA=7:35:13, max mem: 20.9 GB 
[11/28 23:34:17 visual_prompt]: 	Training 400/553. train loss: 1.0876,	0.8283 s / batch. (data: 3.10e-04). ETA=7:17:14, max mem: 20.9 GB 
[11/28 23:36:15 visual_prompt]: 	Training 500/553. train loss: 2.4176,	0.8199 s / batch. (data: 3.58e-04). ETA=7:11:28, max mem: 20.9 GB 
[11/28 23:37:19 visual_prompt]: Epoch 43 / 100: avg data time: 3.40e-01, avg batch time: 1.1755, average train loss: 3.6043
[11/28 23:38:26 visual_prompt]: Inference (val):avg data time: 2.29e-04, avg batch time: 0.3095, average loss: 2.6831
[11/28 23:38:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 73.52	
[11/28 23:38:26 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[11/28 23:40:28 visual_prompt]: 	Training 100/553. train loss: 1.3416,	1.9972 s / batch. (data: 1.17e+00). ETA=17:25:53, max mem: 20.9 GB 
[11/28 23:42:26 visual_prompt]: 	Training 200/553. train loss: 0.2602,	0.8441 s / batch. (data: 1.22e-03). ETA=7:20:37, max mem: 20.9 GB 
[11/28 23:44:17 visual_prompt]: 	Training 300/553. train loss: 1.0570,	0.8735 s / batch. (data: 2.57e-02). ETA=7:34:30, max mem: 20.9 GB 
[11/28 23:46:11 visual_prompt]: 	Training 400/553. train loss: 2.9464,	0.8342 s / batch. (data: 1.18e-03). ETA=7:12:40, max mem: 20.9 GB 
[11/28 23:48:05 visual_prompt]: 	Training 500/553. train loss: 0.0147,	0.8761 s / batch. (data: 1.19e-02). ETA=7:32:58, max mem: 20.9 GB 
[11/28 23:49:04 visual_prompt]: Epoch 44 / 100: avg data time: 3.19e-01, avg batch time: 1.1529, average train loss: 3.7207
[11/28 23:50:10 visual_prompt]: Inference (val):avg data time: 1.67e-04, avg batch time: 0.3124, average loss: 1.4913
[11/28 23:50:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 72.31	
[11/28 23:50:10 visual_prompt]: Stopping early.
[11/28 23:50:10 visual_prompt]: Rank of current process: 0. World size: 1
[11/28 23:50:10 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 23:50:10 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/28 23:50:10 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/28 23:50:10 visual_prompt]: Training with config:
[11/28 23:50:10 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr2.5_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/28 23:50:10 visual_prompt]: Loading training data...
[11/28 23:50:10 visual_prompt]: Constructing mammo-cbis dataset train...
[11/28 23:50:11 visual_prompt]: Loading validation data...
[11/28 23:50:11 visual_prompt]: Constructing mammo-cbis dataset val...
[11/28 23:50:11 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/28 23:50:14 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/28 23:50:14 visual_prompt]: tuned percent:0.525
[11/28 23:50:14 visual_prompt]: Device used for model: 0
[11/28 23:50:14 visual_prompt]: Setting up Evaluator...
[11/28 23:50:14 visual_prompt]: Setting up Trainer...
[11/28 23:50:14 visual_prompt]: 	Setting up the optimizer...
[11/28 23:50:14 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/28 23:52:13 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8388 s / batch. (data: 2.11e-02). ETA=12:51:41, max mem: 20.9 GB 
[11/28 23:54:06 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8477 s / batch. (data: 9.87e-04). ETA=12:58:25, max mem: 20.9 GB 
[11/28 23:56:05 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.5960 s / batch. (data: 1.76e+00). ETA=1 day, 15:39:39, max mem: 20.9 GB 
[11/28 23:57:57 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8311 s / batch. (data: 1.10e-03). ETA=12:40:29, max mem: 20.9 GB 
[11/28 23:59:56 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8346 s / batch. (data: 9.17e-04). ETA=12:42:15, max mem: 20.9 GB 
[11/29 00:00:57 visual_prompt]: Epoch 1 / 100: avg data time: 3.29e-01, avg batch time: 1.1633, average train loss: 1.5403
[11/29 00:02:06 visual_prompt]: Inference (val):avg data time: 8.87e-05, avg batch time: 0.3119, average loss: 1.5201
[11/29 00:02:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/29 00:02:06 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/29 00:04:05 visual_prompt]: 	Training 100/553. train loss: 1.3178,	0.8321 s / batch. (data: 7.87e-03). ETA=12:37:53, max mem: 20.9 GB 
[11/29 00:05:59 visual_prompt]: 	Training 200/553. train loss: 0.2776,	0.8354 s / batch. (data: 7.96e-03). ETA=12:39:29, max mem: 20.9 GB 
[11/29 00:07:56 visual_prompt]: 	Training 300/553. train loss: 1.1687,	1.7134 s / batch. (data: 8.87e-01). ETA=1 day, 1:54:48, max mem: 20.9 GB 
[11/29 00:09:48 visual_prompt]: 	Training 400/553. train loss: 1.4125,	0.8758 s / batch. (data: 1.25e-03). ETA=13:13:19, max mem: 20.9 GB 
[11/29 00:11:46 visual_prompt]: 	Training 500/553. train loss: 0.5618,	0.8352 s / batch. (data: 8.54e-04). ETA=12:35:06, max mem: 20.9 GB 
[11/29 00:12:44 visual_prompt]: Epoch 2 / 100: avg data time: 3.18e-01, avg batch time: 1.1520, average train loss: 1.0792
[11/29 00:13:53 visual_prompt]: Inference (val):avg data time: 2.63e-04, avg batch time: 0.3111, average loss: 2.9495
[11/29 00:13:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.70	
[11/29 00:13:53 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/29 00:15:54 visual_prompt]: 	Training 100/553. train loss: 1.1325,	0.8358 s / batch. (data: 9.54e-04). ETA=12:33:32, max mem: 20.9 GB 
[11/29 00:17:52 visual_prompt]: 	Training 200/553. train loss: 0.7032,	1.9961 s / batch. (data: 1.15e+00). ETA=1 day, 5:56:16, max mem: 20.9 GB 
[11/29 00:19:47 visual_prompt]: 	Training 300/553. train loss: 0.8751,	0.8227 s / batch. (data: 3.87e-04). ETA=12:18:57, max mem: 20.9 GB 
[11/29 00:21:45 visual_prompt]: 	Training 400/553. train loss: 0.4530,	0.8511 s / batch. (data: 7.66e-04). ETA=12:43:01, max mem: 20.9 GB 
[11/29 00:23:43 visual_prompt]: 	Training 500/553. train loss: 0.7412,	1.8129 s / batch. (data: 9.84e-01). ETA=1 day, 3:02:19, max mem: 20.9 GB 
[11/29 00:24:42 visual_prompt]: Epoch 3 / 100: avg data time: 3.39e-01, avg batch time: 1.1731, average train loss: 1.4825
[11/29 00:25:51 visual_prompt]: Inference (val):avg data time: 8.17e-05, avg batch time: 0.3114, average loss: 1.3046
[11/29 00:25:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.65	
[11/29 00:25:51 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/29 00:27:54 visual_prompt]: 	Training 100/553. train loss: 2.1249,	0.8298 s / batch. (data: 6.57e-03). ETA=12:20:29, max mem: 20.9 GB 
[11/29 00:29:50 visual_prompt]: 	Training 200/553. train loss: 3.7853,	0.8360 s / batch. (data: 3.65e-04). ETA=12:24:36, max mem: 20.9 GB 
[11/29 00:31:47 visual_prompt]: 	Training 300/553. train loss: 0.7608,	2.4199 s / batch. (data: 1.56e+00). ETA=1 day, 11:51:21, max mem: 20.9 GB 
[11/29 00:33:39 visual_prompt]: 	Training 400/553. train loss: 0.5434,	1.9981 s / batch. (data: 1.16e+00). ETA=1 day, 5:33:00, max mem: 20.9 GB 
[11/29 00:35:38 visual_prompt]: 	Training 500/553. train loss: 0.2530,	4.4967 s / batch. (data: 3.66e+00). ETA=2 days, 18:22:39, max mem: 20.9 GB 
[11/29 00:36:39 visual_prompt]: Epoch 4 / 100: avg data time: 3.37e-01, avg batch time: 1.1720, average train loss: 2.4336
[11/29 00:37:48 visual_prompt]: Inference (val):avg data time: 8.55e-05, avg batch time: 0.3121, average loss: 0.9127
[11/29 00:37:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.69	
[11/29 00:37:48 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/29 00:39:49 visual_prompt]: 	Training 100/553. train loss: 0.0051,	0.8596 s / batch. (data: 1.05e-03). ETA=12:39:10, max mem: 20.9 GB 
[11/29 00:41:46 visual_prompt]: 	Training 200/553. train loss: 1.9763,	1.9171 s / batch. (data: 1.08e+00). ETA=1 day, 4:09:53, max mem: 20.9 GB 
[11/29 00:43:46 visual_prompt]: 	Training 300/553. train loss: 1.9640,	0.8560 s / batch. (data: 2.86e-03). ETA=12:33:05, max mem: 20.9 GB 
[11/29 00:45:42 visual_prompt]: 	Training 400/553. train loss: 0.7200,	0.8401 s / batch. (data: 7.94e-03). ETA=12:17:40, max mem: 20.9 GB 
[11/29 00:47:39 visual_prompt]: 	Training 500/553. train loss: 0.5754,	0.8182 s / batch. (data: 3.33e-04). ETA=11:57:05, max mem: 20.9 GB 
[11/29 00:48:40 visual_prompt]: Epoch 5 / 100: avg data time: 3.44e-01, avg batch time: 1.1792, average train loss: 2.6534
[11/29 00:49:48 visual_prompt]: Inference (val):avg data time: 7.60e-05, avg batch time: 0.3126, average loss: 5.0650
[11/29 00:49:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.42	
[11/29 00:49:48 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/29 00:51:52 visual_prompt]: 	Training 100/553. train loss: 2.6954,	0.8696 s / batch. (data: 2.42e-02). ETA=12:39:59, max mem: 20.9 GB 
[11/29 00:53:47 visual_prompt]: 	Training 200/553. train loss: 6.5801,	0.8448 s / batch. (data: 7.57e-03). ETA=12:16:54, max mem: 20.9 GB 
[11/29 00:55:43 visual_prompt]: 	Training 300/553. train loss: 5.0656,	0.8549 s / batch. (data: 1.16e-02). ETA=12:24:17, max mem: 20.9 GB 
[11/29 00:57:44 visual_prompt]: 	Training 400/553. train loss: 1.3784,	0.8478 s / batch. (data: 7.66e-04). ETA=12:16:39, max mem: 20.9 GB 
[11/29 00:59:39 visual_prompt]: 	Training 500/553. train loss: 0.1031,	1.1692 s / batch. (data: 3.40e-01). ETA=16:54:00, max mem: 20.9 GB 
[11/29 01:00:39 visual_prompt]: Epoch 6 / 100: avg data time: 3.41e-01, avg batch time: 1.1763, average train loss: 3.5055
[11/29 01:01:46 visual_prompt]: Inference (val):avg data time: 6.95e-05, avg batch time: 0.3095, average loss: 6.6380
[11/29 01:01:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 37.33	
[11/29 01:01:46 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/29 01:03:47 visual_prompt]: 	Training 100/553. train loss: 12.1488,	0.8182 s / batch. (data: 1.27e-03). ETA=11:47:27, max mem: 20.9 GB 
[11/29 01:05:44 visual_prompt]: 	Training 200/553. train loss: 1.0307,	0.8437 s / batch. (data: 1.08e-03). ETA=12:08:06, max mem: 20.9 GB 
[11/29 01:07:46 visual_prompt]: 	Training 300/553. train loss: 3.5153,	2.3552 s / batch. (data: 1.53e+00). ETA=1 day, 9:48:41, max mem: 20.9 GB 
[11/29 01:09:43 visual_prompt]: 	Training 400/553. train loss: 2.3256,	2.7333 s / batch. (data: 1.90e+00). ETA=1 day, 15:09:50, max mem: 20.9 GB 
[11/29 01:11:38 visual_prompt]: 	Training 500/553. train loss: 3.2859,	0.8583 s / batch. (data: 2.41e-02). ETA=12:16:27, max mem: 20.9 GB 
[11/29 01:12:37 visual_prompt]: Epoch 7 / 100: avg data time: 3.42e-01, avg batch time: 1.1768, average train loss: 4.0773
[11/29 01:13:47 visual_prompt]: Inference (val):avg data time: 1.78e-04, avg batch time: 0.3128, average loss: 2.2820
[11/29 01:13:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.19	
[11/29 01:13:47 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/29 01:15:44 visual_prompt]: 	Training 100/553. train loss: 2.5110,	1.2287 s / batch. (data: 3.86e-01). ETA=17:31:09, max mem: 20.9 GB 
[11/29 01:17:41 visual_prompt]: 	Training 200/553. train loss: 3.3374,	0.8443 s / batch. (data: 3.14e-04). ETA=12:00:51, max mem: 20.9 GB 
[11/29 01:19:38 visual_prompt]: 	Training 300/553. train loss: 2.0184,	0.8609 s / batch. (data: 8.80e-04). ETA=12:13:35, max mem: 20.9 GB 
[11/29 01:21:34 visual_prompt]: 	Training 400/553. train loss: 0.7383,	1.2128 s / batch. (data: 3.80e-01). ETA=17:11:28, max mem: 20.9 GB 
[11/29 01:23:30 visual_prompt]: 	Training 500/553. train loss: 2.7546,	2.1901 s / batch. (data: 1.37e+00). ETA=1 day, 6:58:57, max mem: 20.9 GB 
[11/29 01:24:30 visual_prompt]: Epoch 8 / 100: avg data time: 3.28e-01, avg batch time: 1.1628, average train loss: 5.1077
[11/29 01:25:38 visual_prompt]: Inference (val):avg data time: 8.10e-05, avg batch time: 0.3086, average loss: 0.8597
[11/29 01:25:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.15	
[11/29 01:25:38 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/29 01:27:38 visual_prompt]: 	Training 100/553. train loss: 4.1649,	0.8770 s / batch. (data: 1.14e-02). ETA=12:22:12, max mem: 20.9 GB 
[11/29 01:29:32 visual_prompt]: 	Training 200/553. train loss: 0.6673,	0.8199 s / batch. (data: 8.75e-04). ETA=11:32:29, max mem: 20.9 GB 
[11/29 01:31:26 visual_prompt]: 	Training 300/553. train loss: 7.3893,	2.4647 s / batch. (data: 1.64e+00). ETA=1 day, 10:37:33, max mem: 20.9 GB 
[11/29 01:33:23 visual_prompt]: 	Training 400/553. train loss: 0.6317,	0.8241 s / batch. (data: 3.89e-03). ETA=11:33:18, max mem: 20.9 GB 
[11/29 01:35:19 visual_prompt]: 	Training 500/553. train loss: 3.4955,	0.8264 s / batch. (data: 5.51e-03). ETA=11:33:49, max mem: 20.9 GB 
[11/29 01:36:17 visual_prompt]: Epoch 9 / 100: avg data time: 3.22e-01, avg batch time: 1.1557, average train loss: 5.4163
[11/29 01:37:23 visual_prompt]: Inference (val):avg data time: 6.33e-05, avg batch time: 0.3094, average loss: 1.2256
[11/29 01:37:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.59	
[11/29 01:37:23 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/29 01:39:26 visual_prompt]: 	Training 100/553. train loss: 28.6720,	0.8582 s / batch. (data: 5.48e-03). ETA=11:58:20, max mem: 20.9 GB 
[11/29 01:41:19 visual_prompt]: 	Training 200/553. train loss: 5.4005,	0.8445 s / batch. (data: 1.23e-02). ETA=11:45:27, max mem: 20.9 GB 
[11/29 01:43:15 visual_prompt]: 	Training 300/553. train loss: 17.1056,	1.9410 s / batch. (data: 1.09e+00). ETA=1 day, 2:58:15, max mem: 20.9 GB 
[11/29 01:45:07 visual_prompt]: 	Training 400/553. train loss: 6.5896,	0.8221 s / batch. (data: 5.47e-03). ETA=11:24:01, max mem: 20.9 GB 
[11/29 01:47:05 visual_prompt]: 	Training 500/553. train loss: 0.4526,	0.8387 s / batch. (data: 2.40e-02). ETA=11:36:25, max mem: 20.9 GB 
[11/29 01:48:05 visual_prompt]: Epoch 10 / 100: avg data time: 3.26e-01, avg batch time: 1.1604, average train loss: 7.0842
[11/29 01:49:12 visual_prompt]: Inference (val):avg data time: 7.82e-05, avg batch time: 0.3105, average loss: 1.9893
[11/29 01:49:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.73	
[11/29 01:49:12 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/29 01:51:16 visual_prompt]: 	Training 100/553. train loss: 2.3298,	0.8214 s / batch. (data: 3.31e-04). ETA=11:19:58, max mem: 20.9 GB 
[11/29 01:53:15 visual_prompt]: 	Training 200/553. train loss: 27.7675,	0.8315 s / batch. (data: 4.69e-04). ETA=11:26:55, max mem: 20.9 GB 
[11/29 01:55:10 visual_prompt]: 	Training 300/553. train loss: 34.4139,	1.6385 s / batch. (data: 7.97e-01). ETA=22:30:55, max mem: 20.9 GB 
[11/29 01:57:05 visual_prompt]: 	Training 400/553. train loss: 3.9413,	0.8710 s / batch. (data: 1.47e-02). ETA=11:56:41, max mem: 20.9 GB 
[11/29 01:59:01 visual_prompt]: 	Training 500/553. train loss: 1.0671,	0.8726 s / batch. (data: 2.89e-02). ETA=11:56:34, max mem: 20.9 GB 
[11/29 02:00:00 visual_prompt]: Epoch 11 / 100: avg data time: 3.35e-01, avg batch time: 1.1709, average train loss: 7.7834
[11/29 02:01:09 visual_prompt]: Inference (val):avg data time: 7.77e-05, avg batch time: 0.3089, average loss: 0.8970
[11/29 02:01:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.27	
[11/29 02:01:09 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/29 02:03:13 visual_prompt]: 	Training 100/553. train loss: 8.2900,	0.9559 s / batch. (data: 1.26e-01). ETA=13:02:33, max mem: 20.9 GB 
[11/29 02:05:12 visual_prompt]: 	Training 200/553. train loss: 4.5576,	1.8224 s / batch. (data: 1.01e+00). ETA=1 day, 0:48:49, max mem: 20.9 GB 
[11/29 02:07:07 visual_prompt]: 	Training 300/553. train loss: 6.9376,	0.8323 s / batch. (data: 3.18e-04). ETA=11:18:36, max mem: 20.9 GB 
[11/29 02:09:04 visual_prompt]: 	Training 400/553. train loss: 4.4879,	0.8302 s / batch. (data: 1.04e-03). ETA=11:15:28, max mem: 20.9 GB 
[11/29 02:11:01 visual_prompt]: 	Training 500/553. train loss: 0.7148,	0.8520 s / batch. (data: 5.45e-03). ETA=11:31:45, max mem: 20.9 GB 
[11/29 02:12:00 visual_prompt]: Epoch 12 / 100: avg data time: 3.43e-01, avg batch time: 1.1772, average train loss: 7.2357
[11/29 02:13:09 visual_prompt]: Inference (val):avg data time: 7.40e-05, avg batch time: 0.3106, average loss: 3.4477
[11/29 02:13:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.88	
[11/29 02:13:09 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/29 02:15:11 visual_prompt]: 	Training 100/553. train loss: 3.1823,	0.8403 s / batch. (data: 1.11e-03). ETA=11:20:07, max mem: 20.9 GB 
[11/29 02:17:04 visual_prompt]: 	Training 200/553. train loss: 5.5554,	0.8318 s / batch. (data: 8.56e-04). ETA=11:11:54, max mem: 20.9 GB 
[11/29 02:19:01 visual_prompt]: 	Training 300/553. train loss: 0.9770,	2.5161 s / batch. (data: 1.68e+00). ETA=1 day, 9:48:10, max mem: 20.9 GB 
[11/29 02:20:56 visual_prompt]: 	Training 400/553. train loss: 24.2262,	0.8275 s / batch. (data: 9.39e-03). ETA=11:05:36, max mem: 20.9 GB 
[11/29 02:22:53 visual_prompt]: 	Training 500/553. train loss: 4.8191,	0.8480 s / batch. (data: 5.50e-03). ETA=11:20:42, max mem: 20.9 GB 
[11/29 02:23:53 visual_prompt]: Epoch 13 / 100: avg data time: 3.30e-01, avg batch time: 1.1647, average train loss: 7.4968
[11/29 02:25:00 visual_prompt]: Inference (val):avg data time: 1.80e-04, avg batch time: 0.3096, average loss: 4.0477
[11/29 02:25:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.12	
[11/29 02:25:00 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/29 02:27:02 visual_prompt]: 	Training 100/553. train loss: 3.8470,	0.8640 s / batch. (data: 3.40e-04). ETA=11:31:22, max mem: 20.9 GB 
[11/29 02:28:56 visual_prompt]: 	Training 200/553. train loss: 31.0490,	1.5200 s / batch. (data: 6.75e-01). ETA=20:13:44, max mem: 20.9 GB 
[11/29 02:30:52 visual_prompt]: 	Training 300/553. train loss: 3.5751,	1.1660 s / batch. (data: 3.19e-01). ETA=15:29:07, max mem: 20.9 GB 
[11/29 02:32:47 visual_prompt]: 	Training 400/553. train loss: 1.4584,	0.9179 s / batch. (data: 9.22e-02). ETA=12:09:52, max mem: 20.9 GB 
[11/29 02:34:45 visual_prompt]: 	Training 500/553. train loss: 5.5519,	0.8384 s / batch. (data: 1.07e-03). ETA=11:05:17, max mem: 20.9 GB 
[11/29 02:35:42 visual_prompt]: Epoch 14 / 100: avg data time: 3.27e-01, avg batch time: 1.1618, average train loss: 6.7657
[11/29 02:36:51 visual_prompt]: Inference (val):avg data time: 1.82e-04, avg batch time: 0.3121, average loss: 1.5707
[11/29 02:36:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.87	
[11/29 02:36:51 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/29 02:38:52 visual_prompt]: 	Training 100/553. train loss: 8.5192,	0.8679 s / batch. (data: 1.21e-03). ETA=11:26:30, max mem: 20.9 GB 
[11/29 02:40:48 visual_prompt]: 	Training 200/553. train loss: 7.0463,	0.8565 s / batch. (data: 1.08e-02). ETA=11:16:01, max mem: 20.9 GB 
[11/29 02:42:47 visual_prompt]: 	Training 300/553. train loss: 14.5544,	0.8480 s / batch. (data: 9.60e-04). ETA=11:07:54, max mem: 20.9 GB 
[11/29 02:44:41 visual_prompt]: 	Training 400/553. train loss: 6.5952,	0.8260 s / batch. (data: 3.57e-04). ETA=10:49:13, max mem: 20.9 GB 
[11/29 02:46:39 visual_prompt]: 	Training 500/553. train loss: 8.2057,	0.8333 s / batch. (data: 6.16e-03). ETA=10:53:33, max mem: 20.9 GB 
[11/29 02:47:39 visual_prompt]: Epoch 15 / 100: avg data time: 3.37e-01, avg batch time: 1.1711, average train loss: 8.7889
[11/29 02:48:47 visual_prompt]: Inference (val):avg data time: 7.53e-05, avg batch time: 0.3101, average loss: 2.6206
[11/29 02:48:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.29	
[11/29 02:48:47 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/29 02:50:46 visual_prompt]: 	Training 100/553. train loss: 2.7006,	0.8303 s / batch. (data: 5.44e-03). ETA=10:49:06, max mem: 20.9 GB 
[11/29 02:52:43 visual_prompt]: 	Training 200/553. train loss: 13.8312,	0.8800 s / batch. (data: 3.31e-04). ETA=11:26:28, max mem: 20.9 GB 
[11/29 02:54:40 visual_prompt]: 	Training 300/553. train loss: 1.7735,	0.8477 s / batch. (data: 9.43e-04). ETA=10:59:54, max mem: 20.9 GB 
[11/29 02:56:36 visual_prompt]: 	Training 400/553. train loss: 2.1459,	0.8293 s / batch. (data: 4.48e-03). ETA=10:44:11, max mem: 20.9 GB 
[11/29 02:58:32 visual_prompt]: 	Training 500/553. train loss: 13.0191,	2.2199 s / batch. (data: 1.40e+00). ETA=1 day, 4:40:36, max mem: 20.9 GB 
[11/29 02:59:33 visual_prompt]: Epoch 16 / 100: avg data time: 3.34e-01, avg batch time: 1.1681, average train loss: 8.9952
[11/29 03:00:40 visual_prompt]: Inference (val):avg data time: 7.12e-05, avg batch time: 0.3112, average loss: 15.6493
[11/29 03:00:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.58	
[11/29 03:00:40 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/29 03:02:41 visual_prompt]: 	Training 100/553. train loss: 3.8247,	0.8601 s / batch. (data: 9.63e-04). ETA=11:04:27, max mem: 20.9 GB 
[11/29 03:04:38 visual_prompt]: 	Training 200/553. train loss: 1.4259,	0.8324 s / batch. (data: 3.77e-04). ETA=10:41:41, max mem: 20.9 GB 
[11/29 03:06:32 visual_prompt]: 	Training 300/553. train loss: 10.5930,	0.8614 s / batch. (data: 5.45e-03). ETA=11:02:37, max mem: 20.9 GB 
[11/29 03:08:25 visual_prompt]: 	Training 400/553. train loss: 14.6420,	0.8292 s / batch. (data: 5.74e-03). ETA=10:36:24, max mem: 20.9 GB 
[11/29 03:10:21 visual_prompt]: 	Training 500/553. train loss: 5.6674,	2.1317 s / batch. (data: 1.30e+00). ETA=1 day, 3:12:37, max mem: 20.9 GB 
[11/29 03:11:22 visual_prompt]: Epoch 17 / 100: avg data time: 3.27e-01, avg batch time: 1.1616, average train loss: 7.7397
[11/29 03:12:30 visual_prompt]: Inference (val):avg data time: 9.22e-05, avg batch time: 0.3112, average loss: 1.6430
[11/29 03:12:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.38	
[11/29 03:12:30 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/29 03:14:31 visual_prompt]: 	Training 100/553. train loss: 17.1106,	0.8527 s / batch. (data: 2.40e-02). ETA=10:50:50, max mem: 20.9 GB 
[11/29 03:16:30 visual_prompt]: 	Training 200/553. train loss: 8.3440,	0.8705 s / batch. (data: 8.13e-03). ETA=11:03:01, max mem: 20.9 GB 
[11/29 03:18:27 visual_prompt]: 	Training 300/553. train loss: 0.9150,	0.8478 s / batch. (data: 9.20e-04). ETA=10:44:16, max mem: 20.9 GB 
[11/29 03:20:23 visual_prompt]: 	Training 400/553. train loss: 4.5661,	0.8268 s / batch. (data: 5.46e-03). ETA=10:27:00, max mem: 20.9 GB 
[11/29 03:22:16 visual_prompt]: 	Training 500/553. train loss: 0.8066,	0.8462 s / batch. (data: 4.73e-04). ETA=10:40:16, max mem: 20.9 GB 
[11/29 03:23:14 visual_prompt]: Epoch 18 / 100: avg data time: 3.30e-01, avg batch time: 1.1644, average train loss: 6.6641
[11/29 03:24:22 visual_prompt]: Inference (val):avg data time: 7.91e-05, avg batch time: 0.3112, average loss: 4.8269
[11/29 03:24:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.34	
[11/29 03:24:22 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[11/29 03:26:24 visual_prompt]: 	Training 100/553. train loss: 30.8825,	0.8335 s / batch. (data: 1.18e-02). ETA=10:28:32, max mem: 20.9 GB 
[11/29 03:28:21 visual_prompt]: 	Training 200/553. train loss: 9.3120,	0.8374 s / batch. (data: 4.10e-04). ETA=10:30:06, max mem: 20.9 GB 
[11/29 03:30:20 visual_prompt]: 	Training 300/553. train loss: 0.0079,	0.8458 s / batch. (data: 6.29e-03). ETA=10:34:58, max mem: 20.9 GB 
[11/29 03:32:17 visual_prompt]: 	Training 400/553. train loss: 5.2936,	0.8267 s / batch. (data: 3.68e-04). ETA=10:19:17, max mem: 20.9 GB 
[11/29 03:34:08 visual_prompt]: 	Training 500/553. train loss: 12.5619,	0.8484 s / batch. (data: 3.42e-04). ETA=10:34:05, max mem: 20.9 GB 
[11/29 03:35:10 visual_prompt]: Epoch 19 / 100: avg data time: 3.38e-01, avg batch time: 1.1719, average train loss: 7.0264
[11/29 03:36:18 visual_prompt]: Inference (val):avg data time: 7.11e-05, avg batch time: 0.3104, average loss: 26.7086
[11/29 03:36:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.51	
[11/29 03:36:18 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[11/29 03:38:17 visual_prompt]: 	Training 100/553. train loss: 0.7665,	0.8480 s / batch. (data: 7.87e-03). ETA=10:31:40, max mem: 20.9 GB 
[11/29 03:40:17 visual_prompt]: 	Training 200/553. train loss: 1.3021,	0.8401 s / batch. (data: 1.60e-02). ETA=10:24:23, max mem: 20.9 GB 
[11/29 03:42:14 visual_prompt]: 	Training 300/553. train loss: 2.7288,	0.8416 s / batch. (data: 7.46e-04). ETA=10:24:03, max mem: 20.9 GB 
[11/29 03:44:09 visual_prompt]: 	Training 400/553. train loss: 0.7817,	0.8322 s / batch. (data: 3.28e-04). ETA=10:15:41, max mem: 20.9 GB 
[11/29 03:46:05 visual_prompt]: 	Training 500/553. train loss: 8.7915,	0.8382 s / batch. (data: 5.89e-03). ETA=10:18:48, max mem: 20.9 GB 
[11/29 03:47:07 visual_prompt]: Epoch 20 / 100: avg data time: 3.37e-01, avg batch time: 1.1718, average train loss: 7.5887
[11/29 03:48:15 visual_prompt]: Inference (val):avg data time: 6.71e-05, avg batch time: 0.3113, average loss: 5.7085
[11/29 03:48:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.41	
[11/29 03:48:15 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[11/29 03:50:20 visual_prompt]: 	Training 100/553. train loss: 25.1690,	0.8616 s / batch. (data: 5.80e-03). ETA=10:33:51, max mem: 20.9 GB 
[11/29 03:52:17 visual_prompt]: 	Training 200/553. train loss: 19.9447,	0.8234 s / batch. (data: 4.12e-04). ETA=10:04:23, max mem: 20.9 GB 
[11/29 03:54:13 visual_prompt]: 	Training 300/553. train loss: 59.5267,	1.5421 s / batch. (data: 7.10e-01). ETA=18:49:19, max mem: 20.9 GB 
[11/29 03:56:10 visual_prompt]: 	Training 400/553. train loss: 2.0890,	0.8530 s / batch. (data: 1.15e-02). ETA=10:23:13, max mem: 20.9 GB 
[11/29 03:58:07 visual_prompt]: 	Training 500/553. train loss: 11.5200,	0.8283 s / batch. (data: 4.17e-04). ETA=10:03:50, max mem: 20.9 GB 
[11/29 03:59:06 visual_prompt]: Epoch 21 / 100: avg data time: 3.42e-01, avg batch time: 1.1772, average train loss: 8.5933
[11/29 04:00:15 visual_prompt]: Inference (val):avg data time: 9.10e-04, avg batch time: 0.3111, average loss: 11.3172
[11/29 04:00:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.94	
[11/29 04:00:15 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[11/29 04:02:17 visual_prompt]: 	Training 100/553. train loss: 3.0353,	0.8788 s / batch. (data: 2.29e-02). ETA=10:38:23, max mem: 20.9 GB 
[11/29 04:04:13 visual_prompt]: 	Training 200/553. train loss: 0.6691,	0.8570 s / batch. (data: 1.59e-02). ETA=10:21:08, max mem: 20.9 GB 
[11/29 04:06:07 visual_prompt]: 	Training 300/553. train loss: 4.7423,	0.8362 s / batch. (data: 6.71e-04). ETA=10:04:42, max mem: 20.9 GB 
[11/29 04:08:04 visual_prompt]: 	Training 400/553. train loss: 11.0472,	0.8202 s / batch. (data: 3.13e-04). ETA=9:51:44, max mem: 20.9 GB 
[11/29 04:10:00 visual_prompt]: 	Training 500/553. train loss: 3.0639,	0.8548 s / batch. (data: 5.41e-03). ETA=10:15:15, max mem: 20.9 GB 
[11/29 04:11:02 visual_prompt]: Epoch 22 / 100: avg data time: 3.35e-01, avg batch time: 1.1699, average train loss: 6.5814
[11/29 04:12:10 visual_prompt]: Inference (val):avg data time: 1.79e-04, avg batch time: 0.3128, average loss: 4.6498
[11/29 04:12:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.24	
[11/29 04:12:10 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[11/29 04:14:13 visual_prompt]: 	Training 100/553. train loss: 8.9007,	0.8559 s / batch. (data: 1.25e-03). ETA=10:13:53, max mem: 20.9 GB 
[11/29 04:16:11 visual_prompt]: 	Training 200/553. train loss: 0.6759,	1.5061 s / batch. (data: 6.82e-01). ETA=17:57:41, max mem: 20.9 GB 
[11/29 04:18:08 visual_prompt]: 	Training 300/553. train loss: 4.5584,	0.8560 s / batch. (data: 1.19e-03). ETA=10:11:04, max mem: 20.9 GB 
[11/29 04:20:01 visual_prompt]: 	Training 400/553. train loss: 2.1760,	0.8330 s / batch. (data: 3.17e-04). ETA=9:53:15, max mem: 20.9 GB 
[11/29 04:21:57 visual_prompt]: 	Training 500/553. train loss: 30.7379,	0.8629 s / batch. (data: 1.09e-03). ETA=10:13:10, max mem: 20.9 GB 
[11/29 04:22:58 visual_prompt]: Epoch 23 / 100: avg data time: 3.35e-01, avg batch time: 1.1698, average train loss: 7.8208
[11/29 04:24:05 visual_prompt]: Inference (val):avg data time: 2.18e-04, avg batch time: 0.3126, average loss: 6.2168
[11/29 04:24:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.19	
[11/29 04:24:05 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[11/29 04:26:04 visual_prompt]: 	Training 100/553. train loss: 0.4136,	1.3464 s / batch. (data: 5.23e-01). ETA=15:53:16, max mem: 20.9 GB 
[11/29 04:28:01 visual_prompt]: 	Training 200/553. train loss: 1.0560,	0.8200 s / batch. (data: 4.44e-04). ETA=9:39:10, max mem: 20.9 GB 
[11/29 04:29:55 visual_prompt]: 	Training 300/553. train loss: 18.3757,	1.4662 s / batch. (data: 6.43e-01). ETA=17:13:12, max mem: 20.9 GB 
[11/29 04:31:50 visual_prompt]: 	Training 400/553. train loss: 14.4224,	0.8189 s / batch. (data: 3.72e-04). ETA=9:35:43, max mem: 20.9 GB 
[11/29 04:33:48 visual_prompt]: 	Training 500/553. train loss: 1.6670,	0.8415 s / batch. (data: 7.79e-03). ETA=9:50:11, max mem: 20.9 GB 
[11/29 04:34:48 visual_prompt]: Epoch 24 / 100: avg data time: 3.29e-01, avg batch time: 1.1630, average train loss: 7.9286
[11/29 04:35:56 visual_prompt]: Inference (val):avg data time: 7.72e-05, avg batch time: 0.3103, average loss: 0.6890
[11/29 04:35:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.20	
[11/29 04:35:56 visual_prompt]: Best epoch 24: best metric: -0.689
[11/29 04:35:56 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[11/29 04:38:02 visual_prompt]: 	Training 100/553. train loss: 3.0734,	0.8375 s / batch. (data: 1.05e-03). ETA=9:45:15, max mem: 20.9 GB 
[11/29 04:39:51 visual_prompt]: 	Training 200/553. train loss: 8.3300,	0.8207 s / batch. (data: 9.43e-04). ETA=9:32:10, max mem: 20.9 GB 
[11/29 04:41:45 visual_prompt]: 	Training 300/553. train loss: 16.9463,	0.8372 s / batch. (data: 2.93e-04). ETA=9:42:15, max mem: 20.9 GB 
[11/29 04:43:38 visual_prompt]: 	Training 400/553. train loss: 2.5061,	1.9129 s / batch. (data: 1.08e+00). ETA=22:07:08, max mem: 20.9 GB 
[11/29 04:45:32 visual_prompt]: 	Training 500/553. train loss: 3.7947,	2.1677 s / batch. (data: 1.34e+00). ETA=1 day, 1:00:21, max mem: 20.9 GB 
[11/29 04:46:31 visual_prompt]: Epoch 25 / 100: avg data time: 3.16e-01, avg batch time: 1.1484, average train loss: 5.8817
[11/29 04:47:38 visual_prompt]: Inference (val):avg data time: 6.94e-05, avg batch time: 0.3102, average loss: 5.3982
[11/29 04:47:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.12	
[11/29 04:47:38 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[11/29 04:49:38 visual_prompt]: 	Training 100/553. train loss: 2.4094,	0.8389 s / batch. (data: 5.60e-03). ETA=9:38:27, max mem: 20.9 GB 
[11/29 04:51:34 visual_prompt]: 	Training 200/553. train loss: 18.5704,	2.4542 s / batch. (data: 1.63e+00). ETA=1 day, 4:08:15, max mem: 20.9 GB 
[11/29 04:53:30 visual_prompt]: 	Training 300/553. train loss: 1.3351,	0.8191 s / batch. (data: 4.68e-04). ETA=9:22:06, max mem: 20.9 GB 
[11/29 04:55:25 visual_prompt]: 	Training 400/553. train loss: 13.1680,	0.8537 s / batch. (data: 7.81e-03). ETA=9:44:24, max mem: 20.9 GB 
[11/29 04:57:17 visual_prompt]: 	Training 500/553. train loss: 0.8563,	0.8501 s / batch. (data: 3.16e-04). ETA=9:40:32, max mem: 20.9 GB 
[11/29 04:58:17 visual_prompt]: Epoch 26 / 100: avg data time: 3.22e-01, avg batch time: 1.1555, average train loss: 7.3773
[11/29 04:59:25 visual_prompt]: Inference (val):avg data time: 7.87e-05, avg batch time: 0.3097, average loss: 10.7353
[11/29 04:59:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.40	
[11/29 04:59:25 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[11/29 05:01:26 visual_prompt]: 	Training 100/553. train loss: 8.1316,	0.8162 s / batch. (data: 4.13e-04). ETA=9:15:18, max mem: 20.9 GB 
[11/29 05:03:21 visual_prompt]: 	Training 200/553. train loss: 17.1043,	2.3040 s / batch. (data: 1.47e+00). ETA=1 day, 2:03:41, max mem: 20.9 GB 
[11/29 05:05:16 visual_prompt]: 	Training 300/553. train loss: 6.4966,	1.2598 s / batch. (data: 4.20e-01). ETA=14:12:54, max mem: 20.9 GB 
[11/29 05:07:13 visual_prompt]: 	Training 400/553. train loss: 0.7360,	0.8282 s / batch. (data: 3.38e-04). ETA=9:19:21, max mem: 20.9 GB 
[11/29 05:09:09 visual_prompt]: 	Training 500/553. train loss: 0.7005,	0.8463 s / batch. (data: 7.38e-04). ETA=9:30:08, max mem: 20.9 GB 
[11/29 05:10:06 visual_prompt]: Epoch 27 / 100: avg data time: 3.25e-01, avg batch time: 1.1585, average train loss: 7.5118
[11/29 05:11:13 visual_prompt]: Inference (val):avg data time: 7.66e-05, avg batch time: 0.3116, average loss: 5.4981
[11/29 05:11:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.93	
[11/29 05:11:13 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[11/29 05:13:12 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8402 s / batch. (data: 8.01e-04). ETA=9:23:54, max mem: 20.9 GB 
[11/29 05:15:08 visual_prompt]: 	Training 200/553. train loss: 0.5731,	0.9164 s / batch. (data: 2.49e-02). ETA=10:13:31, max mem: 20.9 GB 
[11/29 05:17:04 visual_prompt]: 	Training 300/553. train loss: 5.0490,	2.1579 s / batch. (data: 1.33e+00). ETA=1 day, 0:01:04, max mem: 20.9 GB 
[11/29 05:18:58 visual_prompt]: 	Training 400/553. train loss: 19.3130,	0.8867 s / batch. (data: 2.65e-02). ETA=9:50:40, max mem: 20.9 GB 
[11/29 05:20:50 visual_prompt]: 	Training 500/553. train loss: 0.3478,	0.8313 s / batch. (data: 3.37e-04). ETA=9:12:24, max mem: 20.9 GB 
[11/29 05:21:51 visual_prompt]: Epoch 28 / 100: avg data time: 3.20e-01, avg batch time: 1.1539, average train loss: 7.7986
[11/29 05:22:59 visual_prompt]: Inference (val):avg data time: 7.93e-05, avg batch time: 0.3098, average loss: 2.5523
[11/29 05:22:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.50	
[11/29 05:22:59 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[11/29 05:25:06 visual_prompt]: 	Training 100/553. train loss: 8.1445,	0.8307 s / batch. (data: 3.14e-04). ETA=9:09:50, max mem: 20.9 GB 
[11/29 05:27:02 visual_prompt]: 	Training 200/553. train loss: 19.1221,	2.7178 s / batch. (data: 1.89e+00). ETA=1 day, 5:54:29, max mem: 20.9 GB 
[11/29 05:28:54 visual_prompt]: 	Training 300/553. train loss: 0.5006,	0.8394 s / batch. (data: 5.45e-03). ETA=9:12:51, max mem: 20.9 GB 
[11/29 05:30:46 visual_prompt]: 	Training 400/553. train loss: 28.6541,	1.8476 s / batch. (data: 1.02e+00). ETA=20:13:45, max mem: 20.9 GB 
[11/29 05:32:40 visual_prompt]: 	Training 500/553. train loss: 18.4631,	0.8489 s / batch. (data: 1.05e-02). ETA=9:16:13, max mem: 20.9 GB 
[11/29 05:33:40 visual_prompt]: Epoch 29 / 100: avg data time: 3.25e-01, avg batch time: 1.1592, average train loss: 7.6487
[11/29 05:34:48 visual_prompt]: Inference (val):avg data time: 7.77e-05, avg batch time: 0.3108, average loss: 7.2246
[11/29 05:34:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.46	
[11/29 05:34:48 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[11/29 05:36:46 visual_prompt]: 	Training 100/553. train loss: 10.0702,	0.8490 s / batch. (data: 2.02e-02). ETA=9:14:07, max mem: 20.9 GB 
[11/29 05:38:42 visual_prompt]: 	Training 200/553. train loss: 6.5274,	0.8599 s / batch. (data: 1.19e-02). ETA=9:19:51, max mem: 20.9 GB 
[11/29 05:40:36 visual_prompt]: 	Training 300/553. train loss: 0.0001,	1.1923 s / batch. (data: 3.75e-01). ETA=12:54:14, max mem: 20.9 GB 
[11/29 05:42:35 visual_prompt]: 	Training 400/553. train loss: 9.2713,	1.7457 s / batch. (data: 9.22e-01). ETA=18:50:44, max mem: 20.9 GB 
[11/29 05:44:29 visual_prompt]: 	Training 500/553. train loss: 0.9887,	2.0621 s / batch. (data: 1.23e+00). ETA=22:12:11, max mem: 20.9 GB 
[11/29 05:45:30 visual_prompt]: Epoch 30 / 100: avg data time: 3.27e-01, avg batch time: 1.1606, average train loss: 6.2163
[11/29 05:46:38 visual_prompt]: Inference (val):avg data time: 7.64e-05, avg batch time: 0.3106, average loss: 13.2799
[11/29 05:46:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.08	
[11/29 05:46:38 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[11/29 05:48:40 visual_prompt]: 	Training 100/553. train loss: 7.1848,	0.8264 s / batch. (data: 3.16e-04). ETA=8:51:45, max mem: 20.9 GB 
[11/29 05:50:39 visual_prompt]: 	Training 200/553. train loss: 1.2936,	0.8761 s / batch. (data: 7.61e-03). ETA=9:22:18, max mem: 20.9 GB 
[11/29 05:52:31 visual_prompt]: 	Training 300/553. train loss: 7.0803,	0.8439 s / batch. (data: 1.62e-03). ETA=9:00:14, max mem: 20.9 GB 
[11/29 05:54:25 visual_prompt]: 	Training 400/553. train loss: 2.0222,	0.8377 s / batch. (data: 1.10e-03). ETA=8:54:52, max mem: 20.9 GB 
[11/29 05:56:21 visual_prompt]: 	Training 500/553. train loss: 5.0059,	0.8320 s / batch. (data: 3.41e-04). ETA=8:49:50, max mem: 20.9 GB 
[11/29 05:57:20 visual_prompt]: Epoch 31 / 100: avg data time: 3.27e-01, avg batch time: 1.1612, average train loss: 6.7407
[11/29 05:58:28 visual_prompt]: Inference (val):avg data time: 1.76e-04, avg batch time: 0.3114, average loss: 12.3639
[11/29 05:58:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.44	
[11/29 05:58:28 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[11/29 06:00:30 visual_prompt]: 	Training 100/553. train loss: 0.7040,	0.8290 s / batch. (data: 3.40e-04). ETA=8:45:49, max mem: 20.9 GB 
[11/29 06:02:25 visual_prompt]: 	Training 200/553. train loss: 3.9722,	0.8617 s / batch. (data: 1.31e-02). ETA=9:05:09, max mem: 20.9 GB 
[11/29 06:04:24 visual_prompt]: 	Training 300/553. train loss: 12.6734,	0.8420 s / batch. (data: 1.13e-02). ETA=8:51:13, max mem: 20.9 GB 
[11/29 06:06:20 visual_prompt]: 	Training 400/553. train loss: 14.6780,	0.8249 s / batch. (data: 6.35e-03). ETA=8:39:04, max mem: 20.9 GB 
[11/29 06:08:12 visual_prompt]: 	Training 500/553. train loss: 1.0126,	0.8327 s / batch. (data: 8.75e-03). ETA=8:42:37, max mem: 20.9 GB 
[11/29 06:09:09 visual_prompt]: Epoch 32 / 100: avg data time: 3.26e-01, avg batch time: 1.1599, average train loss: 6.8522
[11/29 06:10:16 visual_prompt]: Inference (val):avg data time: 7.42e-05, avg batch time: 0.3096, average loss: 1.7569
[11/29 06:10:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.47	
[11/29 06:10:16 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[11/29 06:12:15 visual_prompt]: 	Training 100/553. train loss: 0.0002,	0.8402 s / batch. (data: 3.96e-04). ETA=8:45:10, max mem: 20.9 GB 
[11/29 06:14:12 visual_prompt]: 	Training 200/553. train loss: 22.1620,	2.4080 s / batch. (data: 1.59e+00). ETA=1 day, 1:01:10, max mem: 20.9 GB 
[11/29 06:16:04 visual_prompt]: 	Training 300/553. train loss: 9.6480,	0.8395 s / batch. (data: 5.46e-03). ETA=8:41:55, max mem: 20.9 GB 
[11/29 06:17:59 visual_prompt]: 	Training 400/553. train loss: 3.1186,	0.8263 s / batch. (data: 9.27e-03). ETA=8:32:23, max mem: 20.9 GB 
[11/29 06:19:54 visual_prompt]: 	Training 500/553. train loss: 0.9272,	1.8479 s / batch. (data: 1.01e+00). ETA=19:02:43, max mem: 20.9 GB 
[11/29 06:20:52 visual_prompt]: Epoch 33 / 100: avg data time: 3.16e-01, avg batch time: 1.1487, average train loss: 7.0732
[11/29 06:22:00 visual_prompt]: Inference (val):avg data time: 8.74e-05, avg batch time: 0.3098, average loss: 2.8704
[11/29 06:22:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.60	
[11/29 06:22:00 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[11/29 06:24:02 visual_prompt]: 	Training 100/553. train loss: 0.9340,	0.8385 s / batch. (data: 7.91e-03). ETA=8:36:23, max mem: 20.9 GB 
[11/29 06:25:55 visual_prompt]: 	Training 200/553. train loss: 7.4274,	2.0727 s / batch. (data: 1.25e+00). ETA=21:13:02, max mem: 20.9 GB 
[11/29 06:27:49 visual_prompt]: 	Training 300/553. train loss: 9.3686,	0.8618 s / batch. (data: 1.37e-02). ETA=8:47:51, max mem: 20.9 GB 
[11/29 06:29:46 visual_prompt]: 	Training 400/553. train loss: 0.9182,	0.8309 s / batch. (data: 7.94e-03). ETA=8:27:31, max mem: 20.9 GB 
[11/29 06:31:42 visual_prompt]: 	Training 500/553. train loss: 2.1674,	2.1803 s / batch. (data: 1.36e+00). ETA=22:08:12, max mem: 20.9 GB 
[11/29 06:32:40 visual_prompt]: Epoch 34 / 100: avg data time: 3.23e-01, avg batch time: 1.1572, average train loss: 5.9662
[11/29 06:33:48 visual_prompt]: Inference (val):avg data time: 2.06e-04, avg batch time: 0.3107, average loss: 4.5517
[11/29 06:33:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.40	
[11/29 06:33:48 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[11/29 06:35:51 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8284 s / batch. (data: 3.36e-04). ETA=8:22:33, max mem: 20.9 GB 
[11/29 06:37:47 visual_prompt]: 	Training 200/553. train loss: 0.9434,	0.9439 s / batch. (data: 8.69e-02). ETA=9:31:01, max mem: 20.9 GB 
[11/29 06:39:41 visual_prompt]: 	Training 300/553. train loss: 3.9342,	0.8394 s / batch. (data: 7.58e-04). ETA=8:26:23, max mem: 20.9 GB 
[11/29 06:41:34 visual_prompt]: 	Training 400/553. train loss: 3.4352,	1.2055 s / batch. (data: 3.73e-01). ETA=12:05:15, max mem: 20.9 GB 
[11/29 06:43:29 visual_prompt]: 	Training 500/553. train loss: 33.6728,	1.7659 s / batch. (data: 9.37e-01). ETA=17:39:28, max mem: 20.9 GB 
[11/29 06:44:28 visual_prompt]: Epoch 35 / 100: avg data time: 3.23e-01, avg batch time: 1.1579, average train loss: 6.2769
[11/29 06:45:35 visual_prompt]: Inference (val):avg data time: 2.74e-04, avg batch time: 0.3113, average loss: 6.2577
[11/29 06:45:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.13	
[11/29 06:45:35 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[11/29 06:47:35 visual_prompt]: 	Training 100/553. train loss: 23.1203,	0.9119 s / batch. (data: 5.72e-02). ETA=9:04:48, max mem: 20.9 GB 
[11/29 06:49:30 visual_prompt]: 	Training 200/553. train loss: 25.4850,	1.2089 s / batch. (data: 3.67e-01). ETA=12:00:11, max mem: 20.9 GB 
[11/29 06:51:28 visual_prompt]: 	Training 300/553. train loss: 0.0047,	0.8161 s / batch. (data: 3.05e-03). ETA=8:04:51, max mem: 20.9 GB 
[11/29 06:53:23 visual_prompt]: 	Training 400/553. train loss: 3.0664,	0.8444 s / batch. (data: 3.48e-04). ETA=8:20:15, max mem: 20.9 GB 
[11/29 06:55:19 visual_prompt]: 	Training 500/553. train loss: 6.5926,	1.4784 s / batch. (data: 6.53e-01). ETA=14:33:22, max mem: 20.9 GB 
[11/29 06:56:16 visual_prompt]: Epoch 36 / 100: avg data time: 3.24e-01, avg batch time: 1.1577, average train loss: 7.1413
[11/29 06:57:23 visual_prompt]: Inference (val):avg data time: 7.07e-05, avg batch time: 0.3103, average loss: 13.5050
[11/29 06:57:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.78	
[11/29 06:57:23 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[11/29 06:59:23 visual_prompt]: 	Training 100/553. train loss: 6.9923,	0.8376 s / batch. (data: 2.08e-02). ETA=8:12:41, max mem: 20.9 GB 
[11/29 07:01:18 visual_prompt]: 	Training 200/553. train loss: 11.8993,	0.8362 s / batch. (data: 2.75e-03). ETA=8:10:26, max mem: 20.9 GB 
[11/29 07:03:14 visual_prompt]: 	Training 300/553. train loss: 19.4043,	2.0241 s / batch. (data: 1.18e+00). ETA=19:43:51, max mem: 20.9 GB 
[11/29 07:05:12 visual_prompt]: 	Training 400/553. train loss: 43.1729,	2.4136 s / batch. (data: 1.60e+00). ETA=23:27:38, max mem: 20.9 GB 
[11/29 07:07:03 visual_prompt]: 	Training 500/553. train loss: 16.0144,	1.8227 s / batch. (data: 9.86e-01). ETA=17:39:57, max mem: 20.9 GB 
[11/29 07:08:04 visual_prompt]: Epoch 37 / 100: avg data time: 3.25e-01, avg batch time: 1.1595, average train loss: 6.2848
[11/29 07:09:11 visual_prompt]: Inference (val):avg data time: 7.13e-05, avg batch time: 0.3096, average loss: 1.5466
[11/29 07:09:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.55	
[11/29 07:09:11 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[11/29 07:11:09 visual_prompt]: 	Training 100/553. train loss: 1.1445,	1.4973 s / batch. (data: 6.63e-01). ETA=14:26:54, max mem: 20.9 GB 
[11/29 07:13:06 visual_prompt]: 	Training 200/553. train loss: 4.3302,	2.0142 s / batch. (data: 1.18e+00). ETA=19:22:49, max mem: 20.9 GB 
[11/29 07:15:01 visual_prompt]: 	Training 300/553. train loss: 1.0383,	0.8599 s / batch. (data: 1.58e-02). ETA=8:14:58, max mem: 20.9 GB 
[11/29 07:16:55 visual_prompt]: 	Training 400/553. train loss: 31.5564,	0.8280 s / batch. (data: 5.46e-03). ETA=7:55:15, max mem: 20.9 GB 
[11/29 07:18:52 visual_prompt]: 	Training 500/553. train loss: 42.8701,	0.8524 s / batch. (data: 1.24e-02). ETA=8:07:51, max mem: 20.9 GB 
[11/29 07:19:50 visual_prompt]: Epoch 38 / 100: avg data time: 3.22e-01, avg batch time: 1.1556, average train loss: 7.3106
[11/29 07:20:57 visual_prompt]: Inference (val):avg data time: 6.85e-05, avg batch time: 0.3121, average loss: 7.7741
[11/29 07:20:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.80	
[11/29 07:20:57 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[11/29 07:22:55 visual_prompt]: 	Training 100/553. train loss: 8.4772,	0.8404 s / batch. (data: 7.85e-03). ETA=7:58:48, max mem: 20.9 GB 
[11/29 07:24:55 visual_prompt]: 	Training 200/553. train loss: 1.2803,	0.8201 s / batch. (data: 3.24e-04). ETA=7:45:54, max mem: 20.9 GB 
[11/29 07:26:53 visual_prompt]: 	Training 300/553. train loss: 0.1741,	0.8201 s / batch. (data: 5.43e-03). ETA=7:44:33, max mem: 20.9 GB 
[11/29 07:28:45 visual_prompt]: 	Training 400/553. train loss: 30.8407,	1.3080 s / batch. (data: 4.63e-01). ETA=12:18:42, max mem: 20.9 GB 
[11/29 07:30:41 visual_prompt]: 	Training 500/553. train loss: 0.7097,	2.4679 s / batch. (data: 1.62e+00). ETA=23:09:40, max mem: 20.9 GB 
[11/29 07:31:37 visual_prompt]: Epoch 39 / 100: avg data time: 3.24e-01, avg batch time: 1.1582, average train loss: 5.7803
[11/29 07:32:44 visual_prompt]: Inference (val):avg data time: 4.37e-04, avg batch time: 0.3087, average loss: 3.7315
[11/29 07:32:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.46	
[11/29 07:32:44 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[11/29 07:34:44 visual_prompt]: 	Training 100/553. train loss: 15.7575,	0.8357 s / batch. (data: 3.87e-03). ETA=7:48:28, max mem: 20.9 GB 
[11/29 07:36:39 visual_prompt]: 	Training 200/553. train loss: 18.5758,	0.8743 s / batch. (data: 6.20e-03). ETA=8:08:38, max mem: 20.9 GB 
[11/29 07:38:35 visual_prompt]: 	Training 300/553. train loss: 40.2991,	0.8406 s / batch. (data: 6.15e-03). ETA=7:48:25, max mem: 20.9 GB 
[11/29 07:40:30 visual_prompt]: 	Training 400/553. train loss: 1.0436,	0.8227 s / batch. (data: 5.55e-03). ETA=7:37:03, max mem: 20.9 GB 
[11/29 07:42:23 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8341 s / batch. (data: 1.11e-03). ETA=7:42:00, max mem: 20.9 GB 
[11/29 07:43:24 visual_prompt]: Epoch 40 / 100: avg data time: 3.23e-01, avg batch time: 1.1562, average train loss: 6.4851
[11/29 07:44:30 visual_prompt]: Inference (val):avg data time: 7.23e-05, avg batch time: 0.3110, average loss: 7.4935
[11/29 07:44:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.66	
[11/29 07:44:30 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[11/29 07:46:36 visual_prompt]: 	Training 100/553. train loss: 1.9084,	0.8364 s / batch. (data: 3.46e-04). ETA=7:41:08, max mem: 20.9 GB 
[11/29 07:48:34 visual_prompt]: 	Training 200/553. train loss: 4.3705,	0.8925 s / batch. (data: 1.08e-02). ETA=8:10:34, max mem: 20.9 GB 
[11/29 07:50:28 visual_prompt]: 	Training 300/553. train loss: 5.5513,	0.8232 s / batch. (data: 4.93e-04). ETA=7:31:05, max mem: 20.9 GB 
[11/29 07:52:22 visual_prompt]: 	Training 400/553. train loss: 0.8021,	0.8283 s / batch. (data: 6.40e-04). ETA=7:32:31, max mem: 20.9 GB 
[11/29 07:54:12 visual_prompt]: 	Training 500/553. train loss: 2.9831,	0.8324 s / batch. (data: 3.14e-04). ETA=7:33:23, max mem: 20.9 GB 
[11/29 07:55:11 visual_prompt]: Epoch 41 / 100: avg data time: 3.23e-01, avg batch time: 1.1579, average train loss: 5.7236
[11/29 07:56:18 visual_prompt]: Inference (val):avg data time: 7.15e-05, avg batch time: 0.3110, average loss: 0.8251
[11/29 07:56:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.40	
[11/29 07:56:18 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[11/29 07:58:16 visual_prompt]: 	Training 100/553. train loss: 1.4763,	0.8322 s / batch. (data: 7.12e-04). ETA=7:31:09, max mem: 20.9 GB 
[11/29 08:00:12 visual_prompt]: 	Training 200/553. train loss: 5.3505,	1.1046 s / batch. (data: 2.63e-01). ETA=9:56:57, max mem: 20.9 GB 
[11/29 08:02:07 visual_prompt]: 	Training 300/553. train loss: 2.3067,	0.8353 s / batch. (data: 3.19e-04). ETA=7:30:01, max mem: 20.9 GB 
[11/29 08:04:01 visual_prompt]: 	Training 400/553. train loss: 2.1605,	0.8549 s / batch. (data: 8.51e-04). ETA=7:39:09, max mem: 20.9 GB 
[11/29 08:05:56 visual_prompt]: 	Training 500/553. train loss: 7.5252,	0.8359 s / batch. (data: 1.21e-03). ETA=7:27:33, max mem: 20.9 GB 
[11/29 08:06:56 visual_prompt]: Epoch 42 / 100: avg data time: 3.19e-01, avg batch time: 1.1533, average train loss: 4.9277
[11/29 08:08:03 visual_prompt]: Inference (val):avg data time: 5.86e-04, avg batch time: 0.3108, average loss: 5.3846
[11/29 08:08:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.26	
[11/29 08:08:03 visual_prompt]: Training 43 / 100 epoch, with learning rate 1.7979639334863466
[11/29 08:10:06 visual_prompt]: 	Training 100/553. train loss: 5.7172,	0.8290 s / batch. (data: 3.34e-04). ETA=7:21:46, max mem: 20.9 GB 
[11/29 08:11:59 visual_prompt]: 	Training 200/553. train loss: 6.9809,	0.8166 s / batch. (data: 3.46e-04). ETA=7:13:48, max mem: 20.9 GB 
[11/29 08:13:51 visual_prompt]: 	Training 300/553. train loss: 13.7929,	0.8357 s / batch. (data: 7.77e-03). ETA=7:22:32, max mem: 20.9 GB 
[11/29 08:15:44 visual_prompt]: 	Training 400/553. train loss: 3.5076,	0.8683 s / batch. (data: 3.62e-04). ETA=7:38:21, max mem: 20.9 GB 
[11/29 08:17:39 visual_prompt]: 	Training 500/553. train loss: 9.6055,	0.8251 s / batch. (data: 7.80e-03). ETA=7:14:11, max mem: 20.9 GB 
[11/29 08:18:41 visual_prompt]: Epoch 43 / 100: avg data time: 3.18e-01, avg batch time: 1.1526, average train loss: 6.3416
[11/29 08:19:47 visual_prompt]: Inference (val):avg data time: 9.22e-05, avg batch time: 0.3102, average loss: 7.8978
[11/29 08:19:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.69	
[11/29 08:19:47 visual_prompt]: Training 44 / 100 epoch, with learning rate 1.7584208038447504
[11/29 08:21:46 visual_prompt]: 	Training 100/553. train loss: 3.1416,	0.8158 s / batch. (data: 3.44e-04). ETA=7:07:12, max mem: 20.9 GB 
[11/29 08:23:44 visual_prompt]: 	Training 200/553. train loss: 2.1313,	0.8439 s / batch. (data: 1.20e-02). ETA=7:20:32, max mem: 20.9 GB 
[11/29 08:25:36 visual_prompt]: 	Training 300/553. train loss: 0.9869,	0.8482 s / batch. (data: 6.05e-03). ETA=7:21:20, max mem: 20.9 GB 
[11/29 08:27:29 visual_prompt]: 	Training 400/553. train loss: 12.2485,	0.8249 s / batch. (data: 3.61e-04). ETA=7:07:50, max mem: 20.9 GB 
[11/29 08:29:24 visual_prompt]: 	Training 500/553. train loss: 3.0475,	0.8310 s / batch. (data: 3.37e-04). ETA=7:09:39, max mem: 20.9 GB 
[11/29 08:30:24 visual_prompt]: Epoch 44 / 100: avg data time: 3.17e-01, avg batch time: 1.1512, average train loss: 5.4604
[11/29 08:31:31 visual_prompt]: Inference (val):avg data time: 8.14e-05, avg batch time: 0.3105, average loss: 5.5756
[11/29 08:31:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.95	
[11/29 08:31:31 visual_prompt]: Training 45 / 100 epoch, with learning rate 1.7182582417698902
[11/29 08:33:32 visual_prompt]: 	Training 100/553. train loss: 1.0534,	0.8259 s / batch. (data: 5.99e-04). ETA=7:04:55, max mem: 20.9 GB 
[11/29 08:35:23 visual_prompt]: 	Training 200/553. train loss: 2.0776,	0.8402 s / batch. (data: 5.21e-04). ETA=7:10:50, max mem: 20.9 GB 
[11/29 08:37:20 visual_prompt]: 	Training 300/553. train loss: 22.2744,	0.8324 s / batch. (data: 7.96e-03). ETA=7:05:26, max mem: 20.9 GB 
[11/29 08:39:13 visual_prompt]: 	Training 400/553. train loss: 6.2847,	0.8401 s / batch. (data: 1.42e-03). ETA=7:07:59, max mem: 20.9 GB 
[11/29 08:41:08 visual_prompt]: 	Training 500/553. train loss: 11.6673,	0.8338 s / batch. (data: 9.04e-03). ETA=7:03:24, max mem: 20.9 GB 
[11/29 08:42:08 visual_prompt]: Epoch 45 / 100: avg data time: 3.18e-01, avg batch time: 1.1517, average train loss: 5.4296
[11/29 08:43:14 visual_prompt]: Inference (val):avg data time: 7.76e-05, avg batch time: 0.3115, average loss: 7.6738
[11/29 08:43:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.64	
[11/29 08:43:14 visual_prompt]: Stopping early.
[11/29 08:43:15 visual_prompt]: Rank of current process: 0. World size: 1
[11/29 08:43:15 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/29 08:43:15 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/29 08:43:15 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/29 08:43:15 visual_prompt]: Training with config:
[11/29 08:43:15 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr2.5_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/29 08:43:15 visual_prompt]: Loading training data...
[11/29 08:43:15 visual_prompt]: Constructing mammo-cbis dataset train...
[11/29 08:43:15 visual_prompt]: Loading validation data...
[11/29 08:43:15 visual_prompt]: Constructing mammo-cbis dataset val...
[11/29 08:43:15 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/29 08:43:18 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/29 08:43:18 visual_prompt]: tuned percent:0.525
[11/29 08:43:19 visual_prompt]: Device used for model: 0
[11/29 08:43:19 visual_prompt]: Setting up Evaluator...
[11/29 08:43:19 visual_prompt]: Setting up Trainer...
[11/29 08:43:19 visual_prompt]: 	Setting up the optimizer...
[11/29 08:43:19 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/29 08:45:17 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8284 s / batch. (data: 1.23e-02). ETA=12:42:05, max mem: 20.9 GB 
[11/29 08:47:08 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8318 s / batch. (data: 6.63e-04). ETA=12:43:51, max mem: 20.9 GB 
[11/29 08:49:02 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.7319 s / batch. (data: 9.07e-01). ETA=1 day, 2:27:36, max mem: 20.9 GB 
[11/29 08:50:52 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8501 s / batch. (data: 2.68e-02). ETA=12:57:50, max mem: 20.9 GB 
[11/29 08:52:49 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8501 s / batch. (data: 2.06e-02). ETA=12:56:24, max mem: 20.9 GB 
[11/29 08:53:47 visual_prompt]: Epoch 1 / 100: avg data time: 3.03e-01, avg batch time: 1.1368, average train loss: 1.5403
[11/29 08:54:54 visual_prompt]: Inference (val):avg data time: 6.93e-05, avg batch time: 0.3124, average loss: 1.5201
[11/29 08:54:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/29 08:54:54 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/29 08:56:50 visual_prompt]: 	Training 100/553. train loss: 1.1712,	0.9322 s / batch. (data: 9.51e-02). ETA=14:08:59, max mem: 20.9 GB 
[11/29 08:58:43 visual_prompt]: 	Training 200/553. train loss: 0.0979,	0.8698 s / batch. (data: 5.12e-02). ETA=13:10:42, max mem: 20.9 GB 
[11/29 09:00:41 visual_prompt]: 	Training 300/553. train loss: 1.4903,	1.4523 s / batch. (data: 6.22e-01). ETA=21:57:54, max mem: 20.9 GB 
[11/29 09:02:35 visual_prompt]: 	Training 400/553. train loss: 0.5685,	0.8398 s / batch. (data: 8.11e-04). ETA=12:40:40, max mem: 20.9 GB 
[11/29 09:04:32 visual_prompt]: 	Training 500/553. train loss: 0.7332,	0.8485 s / batch. (data: 1.97e-03). ETA=12:47:07, max mem: 20.9 GB 
[11/29 09:05:30 visual_prompt]: Epoch 2 / 100: avg data time: 3.15e-01, avg batch time: 1.1490, average train loss: 1.2897
[11/29 09:06:35 visual_prompt]: Inference (val):avg data time: 4.79e-04, avg batch time: 0.3092, average loss: 2.6171
[11/29 09:06:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.83	
[11/29 09:06:35 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/29 09:08:31 visual_prompt]: 	Training 100/553. train loss: 1.3173,	0.8414 s / batch. (data: 5.45e-03). ETA=12:38:33, max mem: 20.9 GB 
[11/29 09:10:28 visual_prompt]: 	Training 200/553. train loss: 0.7036,	1.5218 s / batch. (data: 6.88e-01). ETA=22:49:27, max mem: 20.9 GB 
[11/29 09:12:22 visual_prompt]: 	Training 300/553. train loss: 1.2212,	0.8351 s / batch. (data: 5.47e-03). ETA=12:30:08, max mem: 20.9 GB 
[11/29 09:14:19 visual_prompt]: 	Training 400/553. train loss: 0.3644,	0.8363 s / batch. (data: 3.49e-04). ETA=12:29:47, max mem: 20.9 GB 
[11/29 09:16:16 visual_prompt]: 	Training 500/553. train loss: 0.7960,	1.9397 s / batch. (data: 1.07e+00). ETA=1 day, 4:55:52, max mem: 20.9 GB 
[11/29 09:17:13 visual_prompt]: Epoch 3 / 100: avg data time: 3.20e-01, avg batch time: 1.1532, average train loss: 1.3528
[11/29 09:18:21 visual_prompt]: Inference (val):avg data time: 7.59e-05, avg batch time: 0.3117, average loss: 1.8503
[11/29 09:18:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.62	
[11/29 09:18:21 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/29 09:20:23 visual_prompt]: 	Training 100/553. train loss: 1.2718,	0.8219 s / batch. (data: 1.03e-03). ETA=12:13:25, max mem: 20.9 GB 
[11/29 09:22:18 visual_prompt]: 	Training 200/553. train loss: 0.9991,	0.8400 s / batch. (data: 3.30e-04). ETA=12:28:10, max mem: 20.9 GB 
[11/29 09:24:12 visual_prompt]: 	Training 300/553. train loss: 0.6045,	1.5639 s / batch. (data: 7.45e-01). ETA=23:10:19, max mem: 20.9 GB 
[11/29 09:26:02 visual_prompt]: 	Training 400/553. train loss: 1.0981,	1.6920 s / batch. (data: 8.46e-01). ETA=1 day, 1:01:23, max mem: 20.9 GB 
[11/29 09:27:59 visual_prompt]: 	Training 500/553. train loss: 2.7090,	4.2440 s / batch. (data: 3.42e+00). ETA=2 days, 14:38:50, max mem: 20.9 GB 
[11/29 09:29:00 visual_prompt]: Epoch 4 / 100: avg data time: 3.20e-01, avg batch time: 1.1535, average train loss: 1.2842
[11/29 09:30:07 visual_prompt]: Inference (val):avg data time: 7.82e-05, avg batch time: 0.3099, average loss: 1.5540
[11/29 09:30:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.66	
[11/29 09:30:07 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/29 09:32:04 visual_prompt]: 	Training 100/553. train loss: 16.0523,	0.8335 s / batch. (data: 9.36e-04). ETA=12:16:04, max mem: 20.9 GB 
[11/29 09:33:58 visual_prompt]: 	Training 200/553. train loss: 2.0396,	2.1039 s / batch. (data: 1.28e+00). ETA=1 day, 6:54:32, max mem: 20.9 GB 
[11/29 09:35:53 visual_prompt]: 	Training 300/553. train loss: 0.7127,	0.8401 s / batch. (data: 7.00e-03). ETA=12:19:08, max mem: 20.9 GB 
[11/29 09:37:45 visual_prompt]: 	Training 400/553. train loss: 5.3987,	0.8524 s / batch. (data: 9.11e-04). ETA=12:28:29, max mem: 20.9 GB 
[11/29 09:39:38 visual_prompt]: 	Training 500/553. train loss: 2.3383,	0.8650 s / batch. (data: 7.79e-03). ETA=12:38:08, max mem: 20.9 GB 
[11/29 09:40:39 visual_prompt]: Epoch 5 / 100: avg data time: 3.09e-01, avg batch time: 1.1433, average train loss: 2.3967
[11/29 09:41:45 visual_prompt]: Inference (val):avg data time: 2.29e-04, avg batch time: 0.3125, average loss: 1.1091
[11/29 09:41:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.43	
[11/29 09:41:45 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/29 09:43:45 visual_prompt]: 	Training 100/553. train loss: 3.2001,	0.8611 s / batch. (data: 2.17e-03). ETA=12:32:31, max mem: 20.9 GB 
[11/29 09:45:37 visual_prompt]: 	Training 200/553. train loss: 12.0924,	0.8214 s / batch. (data: 5.46e-03). ETA=11:56:28, max mem: 20.9 GB 
[11/29 09:47:29 visual_prompt]: 	Training 300/553. train loss: 0.6165,	0.8349 s / batch. (data: 3.40e-04). ETA=12:06:52, max mem: 20.9 GB 
[11/29 09:49:26 visual_prompt]: 	Training 400/553. train loss: 9.0347,	0.8176 s / batch. (data: 6.17e-04). ETA=11:50:27, max mem: 20.9 GB 
[11/29 09:51:19 visual_prompt]: 	Training 500/553. train loss: 0.9203,	0.8583 s / batch. (data: 2.42e-02). ETA=12:24:21, max mem: 20.9 GB 
[11/29 09:52:18 visual_prompt]: Epoch 6 / 100: avg data time: 3.11e-01, avg batch time: 1.1443, average train loss: 3.5688
[11/29 09:53:25 visual_prompt]: Inference (val):avg data time: 3.22e-04, avg batch time: 0.3108, average loss: 2.7631
[11/29 09:53:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.84	
[11/29 09:53:25 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/29 09:55:22 visual_prompt]: 	Training 100/553. train loss: 3.1863,	0.8400 s / batch. (data: 1.85e-04). ETA=12:06:21, max mem: 20.9 GB 
[11/29 09:57:17 visual_prompt]: 	Training 200/553. train loss: 1.2053,	1.3738 s / batch. (data: 5.31e-01). ETA=19:45:36, max mem: 20.9 GB 
[11/29 09:59:17 visual_prompt]: 	Training 300/553. train loss: 2.6570,	2.7902 s / batch. (data: 1.97e+00). ETA=1 day, 16:03:21, max mem: 20.9 GB 
[11/29 10:01:12 visual_prompt]: 	Training 400/553. train loss: 1.3255,	2.3874 s / batch. (data: 1.57e+00). ETA=1 day, 10:12:29, max mem: 20.9 GB 
[11/29 10:03:03 visual_prompt]: 	Training 500/553. train loss: 2.3669,	0.8446 s / batch. (data: 3.91e-04). ETA=12:04:41, max mem: 20.9 GB 
[11/29 10:04:01 visual_prompt]: Epoch 7 / 100: avg data time: 3.17e-01, avg batch time: 1.1508, average train loss: 4.0666
[11/29 10:05:08 visual_prompt]: Inference (val):avg data time: 2.70e-04, avg batch time: 0.3120, average loss: 5.2876
[11/29 10:05:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.18	
[11/29 10:05:08 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/29 10:07:04 visual_prompt]: 	Training 100/553. train loss: 2.6818,	0.8348 s / batch. (data: 6.45e-03). ETA=11:54:10, max mem: 20.9 GB 
[11/29 10:09:00 visual_prompt]: 	Training 200/553. train loss: 0.5606,	0.8362 s / batch. (data: 3.28e-04). ETA=11:53:58, max mem: 20.9 GB 
[11/29 10:10:54 visual_prompt]: 	Training 300/553. train loss: 3.3904,	0.8760 s / batch. (data: 8.05e-03). ETA=12:26:30, max mem: 20.9 GB 
[11/29 10:12:48 visual_prompt]: 	Training 400/553. train loss: 3.0755,	1.3599 s / batch. (data: 5.09e-01). ETA=19:16:35, max mem: 20.9 GB 
[11/29 10:14:42 visual_prompt]: 	Training 500/553. train loss: 18.5463,	1.7480 s / batch. (data: 9.26e-01). ETA=1 day, 0:43:43, max mem: 20.9 GB 
[11/29 10:15:41 visual_prompt]: Epoch 8 / 100: avg data time: 3.10e-01, avg batch time: 1.1441, average train loss: 4.4663
[11/29 10:16:47 visual_prompt]: Inference (val):avg data time: 6.09e-05, avg batch time: 0.3093, average loss: 1.9721
[11/29 10:16:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.10	
[11/29 10:16:47 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/29 10:18:45 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8187 s / batch. (data: 3.65e-03). ETA=11:32:48, max mem: 20.9 GB 
[11/29 10:20:38 visual_prompt]: 	Training 200/553. train loss: 0.9589,	0.8521 s / batch. (data: 3.25e-04). ETA=11:59:40, max mem: 20.9 GB 
[11/29 10:22:33 visual_prompt]: 	Training 300/553. train loss: 11.3712,	1.9972 s / batch. (data: 1.17e+00). ETA=1 day, 4:03:28, max mem: 20.9 GB 
[11/29 10:24:28 visual_prompt]: 	Training 400/553. train loss: 2.1421,	0.8537 s / batch. (data: 3.54e-04). ETA=11:58:12, max mem: 20.9 GB 
[11/29 10:26:22 visual_prompt]: 	Training 500/553. train loss: 2.3915,	1.5111 s / batch. (data: 7.00e-01). ETA=21:08:43, max mem: 20.9 GB 
[11/29 10:27:20 visual_prompt]: Epoch 9 / 100: avg data time: 3.10e-01, avg batch time: 1.1449, average train loss: 4.8825
[11/29 10:28:27 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.3105, average loss: 0.7921
[11/29 10:28:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 40.65	rocauc: 41.97	
[11/29 10:28:27 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/29 10:30:29 visual_prompt]: 	Training 100/553. train loss: 5.6283,	0.8550 s / batch. (data: 1.10e-02). ETA=11:55:38, max mem: 20.9 GB 
[11/29 10:32:23 visual_prompt]: 	Training 200/553. train loss: 1.0567,	0.8560 s / batch. (data: 7.94e-03). ETA=11:55:05, max mem: 20.9 GB 
[11/29 10:34:17 visual_prompt]: 	Training 300/553. train loss: 7.1913,	0.8548 s / batch. (data: 1.05e-02). ETA=11:52:40, max mem: 20.9 GB 
[11/29 10:36:11 visual_prompt]: 	Training 400/553. train loss: 9.4222,	1.3838 s / batch. (data: 5.63e-01). ETA=19:11:21, max mem: 20.9 GB 
[11/29 10:38:06 visual_prompt]: 	Training 500/553. train loss: 12.0612,	1.3958 s / batch. (data: 5.43e-01). ETA=19:19:02, max mem: 20.9 GB 
[11/29 10:39:06 visual_prompt]: Epoch 10 / 100: avg data time: 3.20e-01, avg batch time: 1.1544, average train loss: 6.9467
[11/29 10:40:12 visual_prompt]: Inference (val):avg data time: 2.56e-04, avg batch time: 0.3091, average loss: 4.1296
[11/29 10:40:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.30	
[11/29 10:40:12 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/29 10:42:13 visual_prompt]: 	Training 100/553. train loss: 3.6753,	0.8555 s / batch. (data: 2.32e-02). ETA=11:48:11, max mem: 20.9 GB 
[11/29 10:44:10 visual_prompt]: 	Training 200/553. train loss: 4.9292,	0.8620 s / batch. (data: 1.16e-02). ETA=11:52:06, max mem: 20.9 GB 
[11/29 10:46:04 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.0592 s / batch. (data: 1.24e+00). ETA=1 day, 4:17:49, max mem: 20.9 GB 
[11/29 10:47:57 visual_prompt]: 	Training 400/553. train loss: 0.7602,	0.8498 s / batch. (data: 1.20e-02). ETA=11:39:17, max mem: 20.9 GB 
[11/29 10:49:51 visual_prompt]: 	Training 500/553. train loss: 3.7707,	0.8268 s / batch. (data: 6.41e-04). ETA=11:18:56, max mem: 20.9 GB 
[11/29 10:50:49 visual_prompt]: Epoch 11 / 100: avg data time: 3.18e-01, avg batch time: 1.1512, average train loss: 6.0666
[11/29 10:52:06 visual_prompt]: Inference (val):avg data time: 6.30e-05, avg batch time: 0.3086, average loss: 1.8684
[11/29 10:52:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.24	
[11/29 10:52:06 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/29 10:54:09 visual_prompt]: 	Training 100/553. train loss: 2.4200,	0.8541 s / batch. (data: 1.69e-02). ETA=11:39:09, max mem: 20.9 GB 
[11/29 10:56:03 visual_prompt]: 	Training 200/553. train loss: 2.7326,	0.8547 s / batch. (data: 1.56e-02). ETA=11:38:17, max mem: 20.9 GB 
[11/29 10:57:54 visual_prompt]: 	Training 300/553. train loss: 10.3288,	0.8312 s / batch. (data: 6.03e-04). ETA=11:17:41, max mem: 20.9 GB 
[11/29 10:59:49 visual_prompt]: 	Training 400/553. train loss: 6.6342,	0.8478 s / batch. (data: 1.07e-03). ETA=11:29:47, max mem: 20.9 GB 
[11/29 11:01:45 visual_prompt]: 	Training 500/553. train loss: 3.1335,	0.8313 s / batch. (data: 7.28e-04). ETA=11:14:58, max mem: 20.9 GB 
[11/29 11:02:43 visual_prompt]: Epoch 12 / 100: avg data time: 3.18e-01, avg batch time: 1.1505, average train loss: 6.9875
[11/29 11:03:50 visual_prompt]: Inference (val):avg data time: 6.77e-05, avg batch time: 0.3105, average loss: 1.9784
[11/29 11:03:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.19	
[11/29 11:03:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/29 11:05:52 visual_prompt]: 	Training 100/553. train loss: 6.9440,	0.8360 s / batch. (data: 7.75e-03). ETA=11:16:41, max mem: 20.9 GB 
[11/29 11:07:42 visual_prompt]: 	Training 200/553. train loss: 3.8198,	0.8337 s / batch. (data: 3.66e-04). ETA=11:13:24, max mem: 20.9 GB 
[11/29 11:09:39 visual_prompt]: 	Training 300/553. train loss: 3.7713,	2.3519 s / batch. (data: 1.51e+00). ETA=1 day, 7:35:47, max mem: 20.9 GB 
[11/29 11:11:33 visual_prompt]: 	Training 400/553. train loss: 33.9767,	0.8182 s / batch. (data: 1.03e-03). ETA=10:58:10, max mem: 20.9 GB 
[11/29 11:13:29 visual_prompt]: 	Training 500/553. train loss: 18.2886,	0.8166 s / batch. (data: 3.20e-04). ETA=10:55:32, max mem: 20.9 GB 
[11/29 11:14:29 visual_prompt]: Epoch 13 / 100: avg data time: 3.22e-01, avg batch time: 1.1560, average train loss: 8.3601
[11/29 11:15:37 visual_prompt]: Inference (val):avg data time: 9.96e-05, avg batch time: 0.3117, average loss: 2.4024
[11/29 11:15:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.31	
[11/29 11:15:37 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/29 11:17:36 visual_prompt]: 	Training 100/553. train loss: 5.6784,	0.8314 s / batch. (data: 4.49e-04). ETA=11:05:15, max mem: 20.9 GB 
[11/29 11:19:29 visual_prompt]: 	Training 200/553. train loss: 3.5062,	1.9183 s / batch. (data: 1.09e+00). ETA=1 day, 1:31:48, max mem: 20.9 GB 
[11/29 11:21:24 visual_prompt]: 	Training 300/553. train loss: 2.7180,	1.1513 s / batch. (data: 3.17e-01). ETA=15:17:26, max mem: 20.9 GB 
[11/29 11:23:17 visual_prompt]: 	Training 400/553. train loss: 2.3671,	0.8254 s / batch. (data: 5.73e-03). ETA=10:56:19, max mem: 20.9 GB 
[11/29 11:25:11 visual_prompt]: 	Training 500/553. train loss: 3.3707,	0.8231 s / batch. (data: 3.36e-04). ETA=10:53:08, max mem: 20.9 GB 
[11/29 11:26:11 visual_prompt]: Epoch 14 / 100: avg data time: 3.12e-01, avg batch time: 1.1457, average train loss: 7.4798
[11/29 11:27:17 visual_prompt]: Inference (val):avg data time: 6.83e-05, avg batch time: 0.3093, average loss: 6.0543
[11/29 11:27:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.13	
[11/29 11:27:17 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/29 11:29:18 visual_prompt]: 	Training 100/553. train loss: 7.5818,	0.8400 s / batch. (data: 1.20e-02). ETA=11:04:24, max mem: 20.9 GB 
[11/29 11:31:12 visual_prompt]: 	Training 200/553. train loss: 37.2050,	0.8607 s / batch. (data: 5.47e-03). ETA=11:19:23, max mem: 20.9 GB 
[11/29 11:33:08 visual_prompt]: 	Training 300/553. train loss: 0.9968,	0.8665 s / batch. (data: 3.66e-02). ETA=11:22:27, max mem: 20.9 GB 
[11/29 11:35:01 visual_prompt]: 	Training 400/553. train loss: 15.8997,	0.8473 s / batch. (data: 7.11e-03). ETA=11:05:57, max mem: 20.9 GB 
[11/29 11:36:57 visual_prompt]: 	Training 500/553. train loss: 0.9633,	0.8484 s / batch. (data: 8.38e-03). ETA=11:05:25, max mem: 20.9 GB 
[11/29 11:37:58 visual_prompt]: Epoch 15 / 100: avg data time: 3.25e-01, avg batch time: 1.1589, average train loss: 7.0170
[11/29 11:39:05 visual_prompt]: Inference (val):avg data time: 6.67e-05, avg batch time: 0.3096, average loss: 79.1104
[11/29 11:39:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.74	
[11/29 11:39:05 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/29 11:41:02 visual_prompt]: 	Training 100/553. train loss: 12.6038,	0.8260 s / batch. (data: 7.95e-03). ETA=10:45:43, max mem: 20.9 GB 
[11/29 11:42:54 visual_prompt]: 	Training 200/553. train loss: 41.1485,	0.8348 s / batch. (data: 5.47e-03). ETA=10:51:13, max mem: 20.9 GB 
[11/29 11:44:47 visual_prompt]: 	Training 300/553. train loss: 0.7949,	0.8165 s / batch. (data: 3.13e-04). ETA=10:35:32, max mem: 20.9 GB 
[11/29 11:46:40 visual_prompt]: 	Training 400/553. train loss: 1.5314,	0.8180 s / batch. (data: 7.45e-04). ETA=10:35:23, max mem: 20.9 GB 
[11/29 11:48:33 visual_prompt]: 	Training 500/553. train loss: 3.2104,	1.9677 s / batch. (data: 1.12e+00). ETA=1 day, 1:25:08, max mem: 20.9 GB 
[11/29 11:49:32 visual_prompt]: Epoch 16 / 100: avg data time: 3.02e-01, avg batch time: 1.1346, average train loss: 6.9581
[11/29 11:50:39 visual_prompt]: Inference (val):avg data time: 6.55e-05, avg batch time: 0.3113, average loss: 6.3874
[11/29 11:50:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.45	
[11/29 11:50:39 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/29 11:52:42 visual_prompt]: 	Training 100/553. train loss: 3.1416,	0.8392 s / batch. (data: 3.82e-04). ETA=10:48:19, max mem: 20.9 GB 
[11/29 11:54:40 visual_prompt]: 	Training 200/553. train loss: 1.7053,	0.8494 s / batch. (data: 1.15e-02). ETA=10:54:48, max mem: 20.9 GB 
[11/29 11:56:34 visual_prompt]: 	Training 300/553. train loss: 2.6357,	0.8279 s / batch. (data: 6.38e-03). ETA=10:36:49, max mem: 20.9 GB 
[11/29 11:58:30 visual_prompt]: 	Training 400/553. train loss: 7.6595,	1.8069 s / batch. (data: 9.56e-01). ETA=23:06:50, max mem: 20.9 GB 
[11/29 12:00:25 visual_prompt]: 	Training 500/553. train loss: 3.5254,	2.3637 s / batch. (data: 1.53e+00). ETA=1 day, 6:10:16, max mem: 20.9 GB 
[11/29 12:01:25 visual_prompt]: Epoch 17 / 100: avg data time: 3.36e-01, avg batch time: 1.1686, average train loss: 6.7164
[11/29 12:02:33 visual_prompt]: Inference (val):avg data time: 6.93e-05, avg batch time: 0.3089, average loss: 1.1450
[11/29 12:02:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.21	
[11/29 12:02:33 visual_prompt]: Best epoch 17: best metric: -1.145
[11/29 12:02:33 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/29 12:04:33 visual_prompt]: 	Training 100/553. train loss: 18.2026,	0.8574 s / batch. (data: 6.52e-03). ETA=10:54:27, max mem: 20.9 GB 
[11/29 12:06:32 visual_prompt]: 	Training 200/553. train loss: 4.7025,	0.8670 s / batch. (data: 1.10e-02). ETA=11:00:22, max mem: 20.9 GB 
[11/29 12:08:28 visual_prompt]: 	Training 300/553. train loss: 1.9836,	0.8223 s / batch. (data: 3.18e-04). ETA=10:24:55, max mem: 20.9 GB 
[11/29 12:10:22 visual_prompt]: 	Training 400/553. train loss: 5.2338,	0.8599 s / batch. (data: 1.05e-03). ETA=10:52:03, max mem: 20.9 GB 
[11/29 12:12:16 visual_prompt]: 	Training 500/553. train loss: 8.3156,	0.8301 s / batch. (data: 2.20e-04). ETA=10:28:04, max mem: 20.9 GB 
[11/29 12:13:14 visual_prompt]: Epoch 18 / 100: avg data time: 3.26e-01, avg batch time: 1.1587, average train loss: 7.3623
[11/29 12:14:20 visual_prompt]: Inference (val):avg data time: 7.13e-05, avg batch time: 0.3111, average loss: 9.6024
[11/29 12:14:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.61	
[11/29 12:14:20 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[11/29 12:16:24 visual_prompt]: 	Training 100/553. train loss: 0.8589,	0.8178 s / batch. (data: 3.23e-04). ETA=10:16:39, max mem: 20.9 GB 
[11/29 12:18:22 visual_prompt]: 	Training 200/553. train loss: 2.2295,	0.8193 s / batch. (data: 8.58e-04). ETA=10:16:28, max mem: 20.9 GB 
[11/29 12:20:18 visual_prompt]: 	Training 300/553. train loss: 17.0633,	0.8300 s / batch. (data: 1.24e-03). ETA=10:23:06, max mem: 20.9 GB 
[11/29 12:22:18 visual_prompt]: 	Training 400/553. train loss: 2.2336,	0.8436 s / batch. (data: 1.18e-03). ETA=10:31:56, max mem: 20.9 GB 
[11/29 12:24:14 visual_prompt]: 	Training 500/553. train loss: 7.0014,	0.8764 s / batch. (data: 2.44e-02). ETA=10:55:03, max mem: 20.9 GB 
[11/29 12:25:18 visual_prompt]: Epoch 19 / 100: avg data time: 3.56e-01, avg batch time: 1.1887, average train loss: 6.6630
[11/29 12:26:29 visual_prompt]: Inference (val):avg data time: 3.55e-04, avg batch time: 0.3107, average loss: 1.8048
[11/29 12:26:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.98	
[11/29 12:26:29 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[11/29 12:28:33 visual_prompt]: 	Training 100/553. train loss: 1.3331,	1.4321 s / batch. (data: 5.95e-01). ETA=17:46:46, max mem: 20.9 GB 
[11/29 12:30:35 visual_prompt]: 	Training 200/553. train loss: 3.7003,	0.8333 s / batch. (data: 7.65e-03). ETA=10:19:20, max mem: 20.9 GB 
[11/29 12:32:35 visual_prompt]: 	Training 300/553. train loss: 0.5766,	0.8524 s / batch. (data: 7.92e-03). ETA=10:32:08, max mem: 20.9 GB 
[11/29 12:34:35 visual_prompt]: 	Training 400/553. train loss: 2.2929,	0.8354 s / batch. (data: 1.16e-03). ETA=10:18:05, max mem: 20.9 GB 
[11/29 12:36:33 visual_prompt]: 	Training 500/553. train loss: 0.8760,	0.8401 s / batch. (data: 9.37e-04). ETA=10:20:11, max mem: 20.9 GB 
[11/29 12:37:37 visual_prompt]: Epoch 20 / 100: avg data time: 3.71e-01, avg batch time: 1.2071, average train loss: 7.9536
[11/29 12:38:47 visual_prompt]: Inference (val):avg data time: 2.82e-04, avg batch time: 0.3112, average loss: 0.7894
[11/29 12:38:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.55	
[11/29 12:38:47 visual_prompt]: Best epoch 20: best metric: -0.789
[11/29 12:38:47 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[11/29 12:40:54 visual_prompt]: 	Training 100/553. train loss: 3.4122,	1.2601 s / batch. (data: 4.21e-01). ETA=15:27:01, max mem: 20.9 GB 
[11/29 12:42:51 visual_prompt]: 	Training 200/553. train loss: 15.1083,	0.8267 s / batch. (data: 4.30e-04). ETA=10:06:49, max mem: 20.9 GB 
[11/29 12:44:51 visual_prompt]: 	Training 300/553. train loss: 36.6907,	1.7524 s / batch. (data: 9.13e-01). ETA=21:23:20, max mem: 20.9 GB 
[11/29 12:46:46 visual_prompt]: 	Training 400/553. train loss: 22.1858,	0.8231 s / batch. (data: 1.09e-03). ETA=10:01:25, max mem: 20.9 GB 
[11/29 12:48:42 visual_prompt]: 	Training 500/553. train loss: 1.8073,	0.8756 s / batch. (data: 3.35e-04). ETA=10:38:18, max mem: 20.9 GB 
[11/29 12:49:40 visual_prompt]: Epoch 21 / 100: avg data time: 3.45e-01, avg batch time: 1.1795, average train loss: 6.9093
[11/29 12:50:50 visual_prompt]: Inference (val):avg data time: 1.01e-04, avg batch time: 0.3132, average loss: 6.4880
[11/29 12:50:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.66	
[11/29 12:50:50 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[11/29 12:52:52 visual_prompt]: 	Training 100/553. train loss: 3.0801,	0.8261 s / batch. (data: 5.47e-03). ETA=10:00:09, max mem: 20.9 GB 
[11/29 12:54:48 visual_prompt]: 	Training 200/553. train loss: 7.0918,	0.8312 s / batch. (data: 3.23e-04). ETA=10:02:28, max mem: 20.9 GB 
[11/29 12:56:42 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8241 s / batch. (data: 5.47e-03). ETA=9:55:54, max mem: 20.9 GB 
[11/29 12:58:39 visual_prompt]: 	Training 400/553. train loss: 20.0441,	0.8477 s / batch. (data: 6.71e-04). ETA=10:11:33, max mem: 20.9 GB 
[11/29 13:00:37 visual_prompt]: 	Training 500/553. train loss: 2.1695,	0.8640 s / batch. (data: 7.96e-03). ETA=10:21:53, max mem: 20.9 GB 
[11/29 13:01:39 visual_prompt]: Epoch 22 / 100: avg data time: 3.39e-01, avg batch time: 1.1742, average train loss: 7.2443
[11/29 13:02:47 visual_prompt]: Inference (val):avg data time: 2.35e-04, avg batch time: 0.3103, average loss: 6.5371
[11/29 13:02:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.52	
[11/29 13:02:47 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[11/29 13:04:49 visual_prompt]: 	Training 100/553. train loss: 0.7161,	1.6928 s / batch. (data: 8.55e-01). ETA=20:14:09, max mem: 20.9 GB 
[11/29 13:06:47 visual_prompt]: 	Training 200/553. train loss: 0.8525,	1.4792 s / batch. (data: 6.52e-01). ETA=17:38:26, max mem: 20.9 GB 
[11/29 13:08:45 visual_prompt]: 	Training 300/553. train loss: 1.7063,	0.8649 s / batch. (data: 1.19e-03). ETA=10:17:26, max mem: 20.9 GB 
[11/29 13:10:37 visual_prompt]: 	Training 400/553. train loss: 7.9508,	0.8766 s / batch. (data: 3.97e-02). ETA=10:24:21, max mem: 20.9 GB 
[11/29 13:12:30 visual_prompt]: 	Training 500/553. train loss: 3.5188,	0.8696 s / batch. (data: 2.25e-02). ETA=10:17:53, max mem: 20.9 GB 
[11/29 13:13:29 visual_prompt]: Epoch 23 / 100: avg data time: 3.26e-01, avg batch time: 1.1615, average train loss: 6.2641
[11/29 13:14:38 visual_prompt]: Inference (val):avg data time: 8.04e-05, avg batch time: 0.3114, average loss: 9.3899
[11/29 13:14:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.78	
[11/29 13:14:38 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[11/29 13:16:34 visual_prompt]: 	Training 100/553. train loss: 0.9012,	0.8315 s / batch. (data: 1.39e-03). ETA=9:48:43, max mem: 20.9 GB 
[11/29 13:18:27 visual_prompt]: 	Training 200/553. train loss: 1.0382,	0.8630 s / batch. (data: 1.38e-03). ETA=10:09:36, max mem: 20.9 GB 
[11/29 13:20:22 visual_prompt]: 	Training 300/553. train loss: 5.4717,	1.3200 s / batch. (data: 4.70e-01). ETA=15:30:12, max mem: 20.9 GB 
[11/29 13:22:15 visual_prompt]: 	Training 400/553. train loss: 4.3375,	0.8184 s / batch. (data: 4.07e-04). ETA=9:35:20, max mem: 20.9 GB 
[11/29 13:24:10 visual_prompt]: 	Training 500/553. train loss: 11.1186,	0.8411 s / batch. (data: 1.13e-02). ETA=9:49:53, max mem: 20.9 GB 
[11/29 13:25:11 visual_prompt]: Epoch 24 / 100: avg data time: 3.12e-01, avg batch time: 1.1453, average train loss: 7.0337
[11/29 13:26:19 visual_prompt]: Inference (val):avg data time: 7.30e-05, avg batch time: 0.3118, average loss: 6.6399
[11/29 13:26:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.51	
[11/29 13:26:19 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[11/29 13:28:27 visual_prompt]: 	Training 100/553. train loss: 4.3921,	0.8306 s / batch. (data: 1.00e-03). ETA=9:40:26, max mem: 20.9 GB 
[11/29 13:30:23 visual_prompt]: 	Training 200/553. train loss: 1.0410,	0.8334 s / batch. (data: 6.61e-04). ETA=9:40:59, max mem: 20.9 GB 
[11/29 13:32:19 visual_prompt]: 	Training 300/553. train loss: 3.6681,	0.8162 s / batch. (data: 3.91e-04). ETA=9:27:38, max mem: 20.9 GB 
[11/29 13:34:14 visual_prompt]: 	Training 400/553. train loss: 0.9376,	0.8563 s / batch. (data: 7.93e-03). ETA=9:54:06, max mem: 20.9 GB 
[11/29 13:36:09 visual_prompt]: 	Training 500/553. train loss: 4.8600,	1.5581 s / batch. (data: 7.31e-01). ETA=17:58:24, max mem: 20.9 GB 
[11/29 13:37:12 visual_prompt]: Epoch 25 / 100: avg data time: 3.46e-01, avg batch time: 1.1809, average train loss: 4.7027
[11/29 13:38:19 visual_prompt]: Inference (val):avg data time: 1.02e-03, avg batch time: 0.3115, average loss: 8.6686
[11/29 13:38:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.27	
[11/29 13:38:19 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[11/29 13:40:24 visual_prompt]: 	Training 100/553. train loss: 13.1341,	0.8244 s / batch. (data: 7.86e-03). ETA=9:28:29, max mem: 20.9 GB 
[11/29 13:42:26 visual_prompt]: 	Training 200/553. train loss: 3.6936,	2.5627 s / batch. (data: 1.73e+00). ETA=1 day, 5:22:53, max mem: 20.9 GB 
[11/29 13:44:26 visual_prompt]: 	Training 300/553. train loss: 5.4963,	0.8681 s / batch. (data: 1.08e-03). ETA=9:55:41, max mem: 20.9 GB 
[11/29 13:46:23 visual_prompt]: 	Training 400/553. train loss: 2.1597,	0.9023 s / batch. (data: 4.63e-02). ETA=10:17:42, max mem: 20.9 GB 
[11/29 13:48:18 visual_prompt]: 	Training 500/553. train loss: 12.5324,	0.8268 s / batch. (data: 9.58e-04). ETA=9:24:39, max mem: 20.9 GB 
[11/29 13:49:19 visual_prompt]: Epoch 26 / 100: avg data time: 3.56e-01, avg batch time: 1.1922, average train loss: 4.8536
[11/29 13:50:30 visual_prompt]: Inference (val):avg data time: 5.16e-04, avg batch time: 0.3118, average loss: 8.5867
[11/29 13:50:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.31	
[11/29 13:50:30 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[11/29 13:52:31 visual_prompt]: 	Training 100/553. train loss: 1.0692,	0.8224 s / batch. (data: 4.79e-03). ETA=9:19:31, max mem: 20.9 GB 
[11/29 13:54:28 visual_prompt]: 	Training 200/553. train loss: 10.7444,	2.3776 s / batch. (data: 1.55e+00). ETA=1 day, 2:53:38, max mem: 20.9 GB 
[11/29 13:56:26 visual_prompt]: 	Training 300/553. train loss: 7.6850,	0.9280 s / batch. (data: 8.93e-02). ETA=10:28:18, max mem: 20.9 GB 
[11/29 13:58:25 visual_prompt]: 	Training 400/553. train loss: 2.4567,	0.8286 s / batch. (data: 1.88e-03). ETA=9:19:34, max mem: 20.9 GB 
[11/29 14:00:28 visual_prompt]: 	Training 500/553. train loss: 0.8005,	0.8478 s / batch. (data: 1.16e-03). ETA=9:31:11, max mem: 20.9 GB 
[11/29 14:01:29 visual_prompt]: Epoch 27 / 100: avg data time: 3.57e-01, avg batch time: 1.1920, average train loss: 5.8253
[11/29 14:02:41 visual_prompt]: Inference (val):avg data time: 8.06e-05, avg batch time: 0.3116, average loss: 11.6900
[11/29 14:02:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.86	
[11/29 14:02:41 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[11/29 14:04:45 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8455 s / batch. (data: 9.35e-03). ETA=9:27:26, max mem: 20.9 GB 
[11/29 14:06:40 visual_prompt]: 	Training 200/553. train loss: 0.6749,	0.8360 s / batch. (data: 4.31e-04). ETA=9:19:41, max mem: 20.9 GB 
[11/29 14:08:34 visual_prompt]: 	Training 300/553. train loss: 1.5065,	2.1702 s / batch. (data: 1.30e+00). ETA=1 day, 0:09:17, max mem: 20.9 GB 
[11/29 14:10:28 visual_prompt]: 	Training 400/553. train loss: 19.6463,	0.8692 s / batch. (data: 4.14e-02). ETA=9:39:02, max mem: 20.9 GB 
[11/29 14:12:21 visual_prompt]: 	Training 500/553. train loss: 15.3082,	0.8291 s / batch. (data: 5.65e-03). ETA=9:10:55, max mem: 20.9 GB 
[11/29 14:13:23 visual_prompt]: Epoch 28 / 100: avg data time: 3.27e-01, avg batch time: 1.1617, average train loss: 7.0804
[11/29 14:14:31 visual_prompt]: Inference (val):avg data time: 4.27e-04, avg batch time: 0.3115, average loss: 7.1892
[11/29 14:14:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.42	
[11/29 14:14:31 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[11/29 14:16:39 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8298 s / batch. (data: 1.16e-02). ETA=9:09:14, max mem: 20.9 GB 
[11/29 14:18:35 visual_prompt]: 	Training 200/553. train loss: 4.4561,	2.5371 s / batch. (data: 1.72e+00). ETA=1 day, 3:55:11, max mem: 20.9 GB 
[11/29 14:20:29 visual_prompt]: 	Training 300/553. train loss: 0.8421,	0.8150 s / batch. (data: 3.22e-04). ETA=8:56:46, max mem: 20.9 GB 
[11/29 14:22:31 visual_prompt]: 	Training 400/553. train loss: 10.4524,	1.1750 s / batch. (data: 3.39e-01). ETA=12:51:53, max mem: 20.9 GB 
[11/29 14:24:27 visual_prompt]: 	Training 500/553. train loss: 1.1496,	0.8286 s / batch. (data: 3.15e-04). ETA=9:02:58, max mem: 20.9 GB 
[11/29 14:25:26 visual_prompt]: Epoch 29 / 100: avg data time: 3.52e-01, avg batch time: 1.1851, average train loss: 5.8846
[11/29 14:26:43 visual_prompt]: Inference (val):avg data time: 8.40e-05, avg batch time: 0.3082, average loss: 2.1267
[11/29 14:26:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.62	
[11/29 14:26:43 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[11/29 14:28:50 visual_prompt]: 	Training 100/553. train loss: 1.9249,	1.0495 s / batch. (data: 2.20e-01). ETA=11:24:59, max mem: 20.9 GB 
[11/29 14:30:49 visual_prompt]: 	Training 200/553. train loss: 0.9541,	0.8295 s / batch. (data: 5.48e-03). ETA=9:00:02, max mem: 20.9 GB 
[11/29 14:32:58 visual_prompt]: 	Training 300/553. train loss: 0.8238,	0.8320 s / batch. (data: 3.52e-04). ETA=9:00:16, max mem: 20.9 GB 
[11/29 14:34:46 visual_prompt]: 	Training 400/553. train loss: 3.9622,	1.3291 s / batch. (data: 5.06e-01). ETA=14:20:52, max mem: 20.9 GB 
[11/29 14:36:21 visual_prompt]: 	Training 500/553. train loss: 10.2939,	1.3480 s / batch. (data: 5.10e-01). ETA=14:30:52, max mem: 20.9 GB 
[11/29 14:37:14 visual_prompt]: Epoch 30 / 100: avg data time: 3.08e-01, avg batch time: 1.1400, average train loss: 5.9626
[11/29 14:38:09 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3099, average loss: 8.9855
[11/29 14:38:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.66	
[11/29 14:38:09 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[11/29 14:39:50 visual_prompt]: 	Training 100/553. train loss: 3.8355,	0.8658 s / batch. (data: 2.51e-02). ETA=9:17:09, max mem: 20.9 GB 
[11/29 14:41:32 visual_prompt]: 	Training 200/553. train loss: 6.6480,	0.8266 s / batch. (data: 5.46e-03). ETA=8:50:33, max mem: 20.9 GB 
[11/29 14:43:05 visual_prompt]: 	Training 300/553. train loss: 5.1967,	0.8398 s / batch. (data: 5.42e-03). ETA=8:57:37, max mem: 20.9 GB 
[11/29 14:44:42 visual_prompt]: 	Training 400/553. train loss: 5.0988,	1.2245 s / batch. (data: 3.90e-01). ETA=13:01:49, max mem: 20.9 GB 
[11/29 14:46:26 visual_prompt]: 	Training 500/553. train loss: 15.8001,	0.8440 s / batch. (data: 3.01e-04). ETA=8:57:29, max mem: 20.9 GB 
[11/29 14:47:16 visual_prompt]: Epoch 31 / 100: avg data time: 1.58e-01, avg batch time: 0.9898, average train loss: 6.7488
[11/29 14:48:14 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3090, average loss: 5.5210
[11/29 14:48:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.26	
[11/29 14:48:14 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[11/29 14:49:58 visual_prompt]: 	Training 100/553. train loss: 1.5124,	0.8264 s / batch. (data: 3.29e-04). ETA=8:44:08, max mem: 20.9 GB 
[11/29 14:51:35 visual_prompt]: 	Training 200/553. train loss: 18.4035,	0.8313 s / batch. (data: 3.00e-04). ETA=8:45:52, max mem: 20.9 GB 
[11/29 14:53:14 visual_prompt]: 	Training 300/553. train loss: 5.6368,	0.8423 s / batch. (data: 2.39e-02). ETA=8:51:28, max mem: 20.9 GB 
[11/29 14:55:00 visual_prompt]: 	Training 400/553. train loss: 13.7159,	0.8319 s / batch. (data: 2.56e-04). ETA=8:43:29, max mem: 20.9 GB 
[11/29 14:56:33 visual_prompt]: 	Training 500/553. train loss: 16.9340,	0.8600 s / batch. (data: 1.20e-02). ETA=8:59:44, max mem: 20.9 GB 
[11/29 14:57:22 visual_prompt]: Epoch 32 / 100: avg data time: 1.57e-01, avg batch time: 0.9903, average train loss: 6.1576
[11/29 14:58:17 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3100, average loss: 5.3873
[11/29 14:58:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.64	
[11/29 14:58:17 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[11/29 14:59:58 visual_prompt]: 	Training 100/553. train loss: 0.0000,	1.0821 s / batch. (data: 2.47e-01). ETA=11:16:22, max mem: 20.9 GB 
[11/29 15:01:51 visual_prompt]: 	Training 200/553. train loss: 1.8919,	1.1550 s / batch. (data: 3.40e-01). ETA=12:00:00, max mem: 20.9 GB 
[11/29 15:03:27 visual_prompt]: 	Training 300/553. train loss: 12.5559,	0.8440 s / batch. (data: 2.83e-04). ETA=8:44:44, max mem: 20.9 GB 
[11/29 15:05:04 visual_prompt]: 	Training 400/553. train loss: 7.1457,	0.8537 s / batch. (data: 2.43e-02). ETA=8:49:20, max mem: 20.9 GB 
[11/29 15:06:39 visual_prompt]: 	Training 500/553. train loss: 5.0127,	0.8350 s / batch. (data: 3.34e-04). ETA=8:36:23, max mem: 20.9 GB 
[11/29 15:07:34 visual_prompt]: Epoch 33 / 100: avg data time: 1.73e-01, avg batch time: 1.0062, average train loss: 8.0582
[11/29 15:08:40 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3079, average loss: 5.3979
[11/29 15:08:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.53	
[11/29 15:08:40 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[11/29 15:10:22 visual_prompt]: 	Training 100/553. train loss: 1.6507,	0.8414 s / batch. (data: 3.48e-04). ETA=8:38:11, max mem: 20.9 GB 
[11/29 15:11:56 visual_prompt]: 	Training 200/553. train loss: 4.2793,	0.8639 s / batch. (data: 1.05e-02). ETA=8:50:36, max mem: 20.9 GB 
[11/29 15:13:31 visual_prompt]: 	Training 300/553. train loss: 11.0636,	0.8493 s / batch. (data: 1.60e-02). ETA=8:40:11, max mem: 20.9 GB 
[11/29 15:15:20 visual_prompt]: 	Training 400/553. train loss: 0.8228,	0.8400 s / batch. (data: 2.95e-04). ETA=8:33:06, max mem: 20.9 GB 
[11/29 15:16:55 visual_prompt]: 	Training 500/553. train loss: 1.5929,	1.3040 s / batch. (data: 4.70e-01). ETA=13:14:21, max mem: 20.9 GB 
[11/29 15:17:45 visual_prompt]: Epoch 34 / 100: avg data time: 1.52e-01, avg batch time: 0.9852, average train loss: 6.2087
[11/29 15:18:39 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3099, average loss: 5.2972
[11/29 15:18:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.36	
[11/29 15:18:39 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[11/29 15:20:21 visual_prompt]: 	Training 100/553. train loss: 0.0003,	0.8491 s / batch. (data: 1.56e-02). ETA=8:35:04, max mem: 20.9 GB 
[11/29 15:22:12 visual_prompt]: 	Training 200/553. train loss: 11.8373,	0.8227 s / batch. (data: 5.45e-03). ETA=8:17:43, max mem: 20.9 GB 
[11/29 15:23:46 visual_prompt]: 	Training 300/553. train loss: 7.7257,	0.8388 s / batch. (data: 1.05e-02). ETA=8:26:02, max mem: 20.9 GB 
[11/29 15:25:22 visual_prompt]: 	Training 400/553. train loss: 0.5574,	0.8212 s / batch. (data: 2.91e-04). ETA=8:14:04, max mem: 20.9 GB 
[11/29 15:26:57 visual_prompt]: 	Training 500/553. train loss: 0.6171,	0.9440 s / batch. (data: 8.80e-02). ETA=9:26:22, max mem: 20.9 GB 
[11/29 15:27:55 visual_prompt]: Epoch 35 / 100: avg data time: 1.72e-01, avg batch time: 1.0048, average train loss: 4.3686
[11/29 15:28:50 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3094, average loss: 4.6154
[11/29 15:28:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.12	
[11/29 15:28:50 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[11/29 15:30:30 visual_prompt]: 	Training 100/553. train loss: 0.8789,	0.8305 s / batch. (data: 4.88e-04). ETA=8:16:08, max mem: 20.9 GB 
[11/29 15:32:06 visual_prompt]: 	Training 200/553. train loss: 42.6628,	0.8407 s / batch. (data: 3.14e-04). ETA=8:20:50, max mem: 20.9 GB 
[11/29 15:33:45 visual_prompt]: 	Training 300/553. train loss: 0.0002,	0.8794 s / batch. (data: 2.99e-04). ETA=8:42:27, max mem: 20.9 GB 
[11/29 15:35:20 visual_prompt]: 	Training 400/553. train loss: 7.3354,	0.8405 s / batch. (data: 5.38e-03). ETA=8:17:56, max mem: 20.9 GB 
[11/29 15:36:57 visual_prompt]: 	Training 500/553. train loss: 8.6709,	0.8314 s / batch. (data: 3.18e-04). ETA=8:11:08, max mem: 20.9 GB 
[11/29 15:37:45 visual_prompt]: Epoch 36 / 100: avg data time: 1.32e-01, avg batch time: 0.9668, average train loss: 6.9534
[11/29 15:38:40 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3100, average loss: 16.3279
[11/29 15:38:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.22	
[11/29 15:38:40 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[11/29 15:40:20 visual_prompt]: 	Training 100/553. train loss: 0.9500,	0.8400 s / batch. (data: 7.94e-03). ETA=8:14:05, max mem: 20.9 GB 
[11/29 15:41:55 visual_prompt]: 	Training 200/553. train loss: 1.2989,	0.8314 s / batch. (data: 2.97e-04). ETA=8:07:38, max mem: 20.9 GB 
[11/29 15:43:32 visual_prompt]: 	Training 300/553. train loss: 9.7812,	1.0955 s / batch. (data: 2.66e-01). ETA=10:40:43, max mem: 20.9 GB 
[11/29 15:45:10 visual_prompt]: 	Training 400/553. train loss: 12.1231,	1.5280 s / batch. (data: 7.10e-01). ETA=14:51:07, max mem: 20.9 GB 
[11/29 15:46:42 visual_prompt]: 	Training 500/553. train loss: 3.6920,	0.9104 s / batch. (data: 6.24e-02). ETA=8:49:24, max mem: 20.9 GB 
[11/29 15:47:33 visual_prompt]: Epoch 37 / 100: avg data time: 1.30e-01, avg batch time: 0.9648, average train loss: 6.5884
[11/29 15:48:28 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3092, average loss: 7.1839
[11/29 15:48:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.78	
[11/29 15:48:28 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[11/29 15:50:06 visual_prompt]: 	Training 100/553. train loss: 3.6913,	0.8671 s / batch. (data: 3.47e-04). ETA=8:22:01, max mem: 20.9 GB 
[11/29 15:51:42 visual_prompt]: 	Training 200/553. train loss: 4.5708,	0.8192 s / batch. (data: 3.18e-04). ETA=7:52:57, max mem: 20.9 GB 
[11/29 15:53:19 visual_prompt]: 	Training 300/553. train loss: 0.7433,	0.8285 s / batch. (data: 3.32e-04). ETA=7:56:54, max mem: 20.9 GB 
[11/29 15:54:52 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8400 s / batch. (data: 3.12e-04). ETA=8:02:08, max mem: 20.9 GB 
[11/29 15:56:29 visual_prompt]: 	Training 500/553. train loss: 1.5066,	0.8336 s / batch. (data: 2.82e-04). ETA=7:57:04, max mem: 20.9 GB 
[11/29 15:57:18 visual_prompt]: Epoch 38 / 100: avg data time: 1.25e-01, avg batch time: 0.9590, average train loss: 5.5627
[11/29 15:58:12 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3101, average loss: 6.7872
[11/29 15:58:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.49	
[11/29 15:58:12 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[11/29 15:59:50 visual_prompt]: 	Training 100/553. train loss: 0.0126,	0.8193 s / batch. (data: 2.85e-04). ETA=7:46:49, max mem: 20.9 GB 
[11/29 16:01:30 visual_prompt]: 	Training 200/553. train loss: 1.0547,	0.8614 s / batch. (data: 1.10e-02). ETA=8:09:20, max mem: 20.9 GB 
[11/29 16:03:08 visual_prompt]: 	Training 300/553. train loss: 0.0113,	0.8320 s / batch. (data: 2.78e-04). ETA=7:51:17, max mem: 20.9 GB 
[11/29 16:04:44 visual_prompt]: 	Training 400/553. train loss: 1.2799,	0.8437 s / batch. (data: 1.16e-02). ETA=7:56:28, max mem: 20.9 GB 
[11/29 16:06:22 visual_prompt]: 	Training 500/553. train loss: 3.8063,	1.5825 s / batch. (data: 7.53e-01). ETA=14:51:06, max mem: 20.9 GB 
[11/29 16:07:11 visual_prompt]: Epoch 39 / 100: avg data time: 1.40e-01, avg batch time: 0.9743, average train loss: 3.8996
[11/29 16:08:07 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3104, average loss: 0.8770
[11/29 16:08:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.07	
[11/29 16:08:07 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[11/29 16:09:50 visual_prompt]: 	Training 100/553. train loss: 10.3424,	0.8489 s / batch. (data: 8.81e-03). ETA=7:55:50, max mem: 20.9 GB 
[11/29 16:11:27 visual_prompt]: 	Training 200/553. train loss: 1.4200,	0.8400 s / batch. (data: 1.20e-02). ETA=7:49:28, max mem: 20.9 GB 
[11/29 16:13:05 visual_prompt]: 	Training 300/553. train loss: 3.3010,	0.8360 s / batch. (data: 3.12e-04). ETA=7:45:49, max mem: 20.9 GB 
[11/29 16:14:41 visual_prompt]: 	Training 400/553. train loss: 0.8594,	0.8589 s / batch. (data: 3.55e-04). ETA=7:57:11, max mem: 20.9 GB 
[11/29 16:16:17 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8516 s / batch. (data: 7.94e-03). ETA=7:51:41, max mem: 20.9 GB 
[11/29 16:17:08 visual_prompt]: Epoch 40 / 100: avg data time: 1.45e-01, avg batch time: 0.9784, average train loss: 5.4063
[11/29 16:18:05 visual_prompt]: Inference (val):avg data time: 3.67e-04, avg batch time: 0.3095, average loss: 7.5626
[11/29 16:18:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.30	
[11/29 16:18:05 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[11/29 16:19:53 visual_prompt]: 	Training 100/553. train loss: 5.4121,	0.8480 s / batch. (data: 4.03e-04). ETA=7:47:30, max mem: 20.9 GB 
[11/29 16:21:33 visual_prompt]: 	Training 200/553. train loss: 5.0098,	0.8376 s / batch. (data: 8.02e-03). ETA=7:40:22, max mem: 20.9 GB 
[11/29 16:23:10 visual_prompt]: 	Training 300/553. train loss: 3.5845,	0.8251 s / batch. (data: 5.45e-03). ETA=7:32:10, max mem: 20.9 GB 
[11/29 16:24:47 visual_prompt]: 	Training 400/553. train loss: 0.7583,	0.8511 s / batch. (data: 1.56e-02). ETA=7:44:57, max mem: 20.9 GB 
[11/29 16:26:21 visual_prompt]: 	Training 500/553. train loss: 3.0201,	0.8600 s / batch. (data: 8.04e-04). ETA=7:48:23, max mem: 20.9 GB 
[11/29 16:27:10 visual_prompt]: Epoch 41 / 100: avg data time: 1.51e-01, avg batch time: 0.9856, average train loss: 6.7106
[11/29 16:28:05 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3095, average loss: 4.8890
[11/29 16:28:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.21	
[11/29 16:28:05 visual_prompt]: Stopping early.
[11/29 16:28:05 visual_prompt]: Rank of current process: 0. World size: 1
[11/29 16:28:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/29 16:28:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/29 16:28:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/29 16:28:05 visual_prompt]: Training with config:
[11/29 16:28:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr2.5_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/29 16:28:05 visual_prompt]: Loading training data...
[11/29 16:28:05 visual_prompt]: Constructing mammo-cbis dataset train...
[11/29 16:28:05 visual_prompt]: Loading validation data...
[11/29 16:28:05 visual_prompt]: Constructing mammo-cbis dataset val...
[11/29 16:28:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/29 16:28:08 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/29 16:28:08 visual_prompt]: tuned percent:0.525
[11/29 16:28:08 visual_prompt]: Device used for model: 0
[11/29 16:28:08 visual_prompt]: Setting up Evaluator...
[11/29 16:28:08 visual_prompt]: Setting up Trainer...
[11/29 16:28:08 visual_prompt]: 	Setting up the optimizer...
[11/29 16:28:08 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/29 16:29:47 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8400 s / batch. (data: 2.96e-04). ETA=12:52:47, max mem: 20.9 GB 
[11/29 16:31:22 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8360 s / batch. (data: 3.16e-04). ETA=12:47:43, max mem: 20.9 GB 
[11/29 16:33:00 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8640 s / batch. (data: 3.09e-04). ETA=13:12:02, max mem: 20.9 GB 
[11/29 16:34:34 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8200 s / batch. (data: 2.85e-04). ETA=12:30:18, max mem: 20.9 GB 
[11/29 16:36:11 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8203 s / batch. (data: 3.08e-04). ETA=12:29:11, max mem: 20.9 GB 
[11/29 16:37:02 visual_prompt]: Epoch 1 / 100: avg data time: 1.30e-01, avg batch time: 0.9661, average train loss: 1.5403
[11/29 16:37:57 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.3094, average loss: 1.5201
[11/29 16:37:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/29 16:37:57 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/29 16:39:36 visual_prompt]: 	Training 100/553. train loss: 1.2142,	0.8480 s / batch. (data: 1.59e-02). ETA=12:52:20, max mem: 20.9 GB 
[11/29 16:41:11 visual_prompt]: 	Training 200/553. train loss: 0.0920,	0.8255 s / batch. (data: 3.04e-04). ETA=12:30:28, max mem: 20.9 GB 
[11/29 16:42:49 visual_prompt]: 	Training 300/553. train loss: 1.6199,	0.9360 s / batch. (data: 9.62e-02). ETA=14:09:22, max mem: 20.9 GB 
[11/29 16:44:23 visual_prompt]: 	Training 400/553. train loss: 1.0970,	0.8480 s / batch. (data: 7.94e-03). ETA=12:48:05, max mem: 20.9 GB 
[11/29 16:46:01 visual_prompt]: 	Training 500/553. train loss: 0.5494,	0.8242 s / batch. (data: 2.83e-04). ETA=12:25:11, max mem: 20.9 GB 
[11/29 16:46:50 visual_prompt]: Epoch 2 / 100: avg data time: 1.30e-01, avg batch time: 0.9649, average train loss: 1.3718
[11/29 16:47:45 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3101, average loss: 4.0659
[11/29 16:47:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.87	
[11/29 16:47:45 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/29 16:49:24 visual_prompt]: 	Training 100/553. train loss: 0.9441,	0.8600 s / batch. (data: 5.43e-03). ETA=12:55:22, max mem: 20.9 GB 
[11/29 16:51:01 visual_prompt]: 	Training 200/553. train loss: 0.7023,	0.8322 s / batch. (data: 3.19e-04). ETA=12:28:51, max mem: 20.9 GB 
[11/29 16:52:36 visual_prompt]: 	Training 300/553. train loss: 1.4231,	0.8513 s / batch. (data: 5.41e-03). ETA=12:44:39, max mem: 20.9 GB 
[11/29 16:54:13 visual_prompt]: 	Training 400/553. train loss: 4.3751,	0.8480 s / batch. (data: 5.49e-03). ETA=12:40:15, max mem: 20.9 GB 
[11/29 16:55:50 visual_prompt]: 	Training 500/553. train loss: 0.7843,	1.1449 s / batch. (data: 3.21e-01). ETA=17:04:31, max mem: 20.9 GB 
[11/29 16:56:40 visual_prompt]: Epoch 3 / 100: avg data time: 1.30e-01, avg batch time: 0.9666, average train loss: 1.6100
[11/29 16:57:35 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3097, average loss: 1.9609
[11/29 16:57:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.84	
[11/29 16:57:35 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/29 16:59:16 visual_prompt]: 	Training 100/553. train loss: 2.2299,	0.8458 s / batch. (data: 2.89e-04). ETA=12:34:44, max mem: 20.9 GB 
[11/29 17:00:52 visual_prompt]: 	Training 200/553. train loss: 1.0641,	0.8366 s / batch. (data: 5.45e-03). ETA=12:25:10, max mem: 20.9 GB 
[11/29 17:02:29 visual_prompt]: 	Training 300/553. train loss: 0.8041,	1.2765 s / batch. (data: 4.49e-01). ETA=18:54:50, max mem: 20.9 GB 
[11/29 17:04:01 visual_prompt]: 	Training 400/553. train loss: 2.0559,	0.8536 s / batch. (data: 2.16e-02). ETA=12:37:29, max mem: 20.9 GB 
[11/29 17:05:39 visual_prompt]: 	Training 500/553. train loss: 0.0003,	3.2080 s / batch. (data: 2.38e+00). ETA=1 day, 23:21:16, max mem: 20.9 GB 
[11/29 17:06:30 visual_prompt]: Epoch 4 / 100: avg data time: 1.33e-01, avg batch time: 0.9681, average train loss: 2.5073
[11/29 17:07:25 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.3092, average loss: 1.3046
[11/29 17:07:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.22	
[11/29 17:07:25 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/29 17:09:04 visual_prompt]: 	Training 100/553. train loss: 0.0006,	0.8360 s / batch. (data: 3.05e-04). ETA=12:18:18, max mem: 20.9 GB 
[11/29 17:10:39 visual_prompt]: 	Training 200/553. train loss: 3.6976,	1.1198 s / batch. (data: 2.80e-01). ETA=16:27:03, max mem: 20.9 GB 
[11/29 17:12:18 visual_prompt]: 	Training 300/553. train loss: 8.8004,	0.8388 s / batch. (data: 3.08e-04). ETA=12:17:59, max mem: 20.9 GB 
[11/29 17:13:53 visual_prompt]: 	Training 400/553. train loss: 0.6081,	0.8200 s / batch. (data: 2.93e-04). ETA=12:00:05, max mem: 20.9 GB 
[11/29 17:15:30 visual_prompt]: 	Training 500/553. train loss: 1.3376,	0.8506 s / batch. (data: 5.44e-03). ETA=12:25:33, max mem: 20.9 GB 
[11/29 17:16:21 visual_prompt]: Epoch 5 / 100: avg data time: 1.34e-01, avg batch time: 0.9688, average train loss: 2.8567
[11/29 17:17:15 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3121, average loss: 5.3650
[11/29 17:17:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.52	
[11/29 17:17:15 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/29 17:18:56 visual_prompt]: 	Training 100/553. train loss: 1.1663,	0.8509 s / batch. (data: 3.05e-04). ETA=12:23:36, max mem: 20.9 GB 
[11/29 17:20:32 visual_prompt]: 	Training 200/553. train loss: 3.3154,	0.8247 s / batch. (data: 5.44e-03). ETA=11:59:22, max mem: 20.9 GB 
[11/29 17:22:07 visual_prompt]: 	Training 300/553. train loss: 0.7684,	0.8320 s / batch. (data: 1.19e-02). ETA=12:04:17, max mem: 20.9 GB 
[11/29 17:23:47 visual_prompt]: 	Training 400/553. train loss: 1.2629,	0.8600 s / batch. (data: 3.59e-04). ETA=12:27:18, max mem: 20.9 GB 
[11/29 17:25:21 visual_prompt]: 	Training 500/553. train loss: 5.1409,	0.8480 s / batch. (data: 3.53e-04). ETA=12:15:25, max mem: 20.9 GB 
[11/29 17:26:11 visual_prompt]: Epoch 6 / 100: avg data time: 1.33e-01, avg batch time: 0.9690, average train loss: 2.0619
[11/29 17:27:05 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3110, average loss: 0.9227
[11/29 17:27:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.91	
[11/29 17:27:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/29 17:28:43 visual_prompt]: 	Training 100/553. train loss: 4.5842,	0.8440 s / batch. (data: 2.81e-04). ETA=12:09:49, max mem: 20.9 GB 
[11/29 17:30:18 visual_prompt]: 	Training 200/553. train loss: 0.7425,	0.8360 s / batch. (data: 2.93e-04). ETA=12:01:27, max mem: 20.9 GB 
[11/29 17:31:57 visual_prompt]: 	Training 300/553. train loss: 1.6967,	1.7734 s / batch. (data: 9.46e-01). ETA=1 day, 1:27:31, max mem: 20.9 GB 
[11/29 17:33:34 visual_prompt]: 	Training 400/553. train loss: 0.6390,	1.8520 s / batch. (data: 1.01e+00). ETA=1 day, 2:32:10, max mem: 20.9 GB 
[11/29 17:35:09 visual_prompt]: 	Training 500/553. train loss: 0.6944,	0.8600 s / batch. (data: 5.30e-03). ETA=12:17:55, max mem: 20.9 GB 
[11/29 17:35:57 visual_prompt]: Epoch 7 / 100: avg data time: 1.27e-01, avg batch time: 0.9613, average train loss: 2.1026
[11/29 17:36:51 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3099, average loss: 1.9892
[11/29 17:36:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.76	
[11/29 17:36:51 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/29 17:38:29 visual_prompt]: 	Training 100/553. train loss: 8.7400,	0.8398 s / batch. (data: 2.52e-03). ETA=11:58:25, max mem: 20.9 GB 
[11/29 17:40:06 visual_prompt]: 	Training 200/553. train loss: 11.3453,	0.8373 s / batch. (data: 3.16e-04). ETA=11:54:55, max mem: 20.9 GB 
[11/29 17:41:42 visual_prompt]: 	Training 300/553. train loss: 2.3540,	0.8360 s / batch. (data: 2.90e-04). ETA=11:52:23, max mem: 20.9 GB 
[11/29 17:43:18 visual_prompt]: 	Training 400/553. train loss: 3.7570,	0.8373 s / batch. (data: 3.25e-04). ETA=11:52:07, max mem: 20.9 GB 
[11/29 17:44:53 visual_prompt]: 	Training 500/553. train loss: 6.8638,	1.1797 s / batch. (data: 3.27e-01). ETA=16:41:20, max mem: 20.9 GB 
[11/29 17:45:44 visual_prompt]: Epoch 8 / 100: avg data time: 1.27e-01, avg batch time: 0.9627, average train loss: 5.9094
[11/29 17:46:38 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3095, average loss: 3.1904
[11/29 17:46:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.02	
[11/29 17:46:38 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/29 17:48:17 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8280 s / batch. (data: 2.92e-04). ETA=11:40:42, max mem: 20.9 GB 
[11/29 17:49:52 visual_prompt]: 	Training 200/553. train loss: 0.5124,	0.8500 s / batch. (data: 5.43e-03). ETA=11:57:55, max mem: 20.9 GB 
[11/29 17:51:28 visual_prompt]: 	Training 300/553. train loss: 2.3567,	1.4680 s / batch. (data: 6.45e-01). ETA=20:37:23, max mem: 20.9 GB 
[11/29 17:53:07 visual_prompt]: 	Training 400/553. train loss: 2.7290,	0.8439 s / batch. (data: 9.50e-04). ETA=11:49:58, max mem: 20.9 GB 
[11/29 17:54:46 visual_prompt]: 	Training 500/553. train loss: 0.4908,	1.0440 s / batch. (data: 2.01e-01). ETA=14:36:32, max mem: 20.9 GB 
[11/29 17:55:36 visual_prompt]: Epoch 9 / 100: avg data time: 1.38e-01, avg batch time: 0.9723, average train loss: 2.7112
[11/29 17:56:32 visual_prompt]: Inference (val):avg data time: 3.63e-04, avg batch time: 0.3113, average loss: 0.8254
[11/29 17:56:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.13	
[11/29 17:56:32 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/29 17:58:16 visual_prompt]: 	Training 100/553. train loss: 11.5821,	0.8400 s / batch. (data: 8.12e-04). ETA=11:43:07, max mem: 20.9 GB 
[11/29 17:59:52 visual_prompt]: 	Training 200/553. train loss: 0.5792,	0.8437 s / batch. (data: 3.24e-04). ETA=11:44:47, max mem: 20.9 GB 
[11/29 18:01:28 visual_prompt]: 	Training 300/553. train loss: 1.7898,	2.1358 s / batch. (data: 1.29e+00). ETA=1 day, 5:40:37, max mem: 20.9 GB 
[11/29 18:03:03 visual_prompt]: 	Training 400/553. train loss: 4.9888,	0.8540 s / batch. (data: 1.05e-02). ETA=11:50:32, max mem: 20.9 GB 
[11/29 18:04:40 visual_prompt]: 	Training 500/553. train loss: 1.6759,	0.8405 s / batch. (data: 2.05e-02). ETA=11:37:57, max mem: 20.9 GB 
[11/29 18:05:31 visual_prompt]: Epoch 10 / 100: avg data time: 1.40e-01, avg batch time: 0.9743, average train loss: 4.3058
[11/29 18:06:25 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3104, average loss: 1.0591
[11/29 18:06:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.98	
[11/29 18:06:25 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/29 18:08:07 visual_prompt]: 	Training 100/553. train loss: 5.5329,	0.8429 s / batch. (data: 5.43e-03). ETA=11:37:48, max mem: 20.9 GB 
[11/29 18:09:46 visual_prompt]: 	Training 200/553. train loss: 8.8807,	0.8480 s / batch. (data: 5.85e-03). ETA=11:40:36, max mem: 20.9 GB 
[11/29 18:11:22 visual_prompt]: 	Training 300/553. train loss: 0.0029,	1.8434 s / batch. (data: 9.95e-01). ETA=1 day, 1:19:52, max mem: 20.9 GB 
[11/29 18:12:56 visual_prompt]: 	Training 400/553. train loss: 2.8477,	0.8350 s / batch. (data: 1.56e-02). ETA=11:27:06, max mem: 20.9 GB 
[11/29 18:14:31 visual_prompt]: 	Training 500/553. train loss: 2.4086,	0.8520 s / batch. (data: 3.06e-04). ETA=11:39:37, max mem: 20.9 GB 
[11/29 18:15:21 visual_prompt]: Epoch 11 / 100: avg data time: 1.33e-01, avg batch time: 0.9681, average train loss: 2.4816
[11/29 18:16:15 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3097, average loss: 4.5119
[11/29 18:16:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.09	
[11/29 18:16:15 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/29 18:17:56 visual_prompt]: 	Training 100/553. train loss: 1.4814,	0.8320 s / batch. (data: 3.13e-04). ETA=11:21:04, max mem: 20.9 GB 
[11/29 18:19:32 visual_prompt]: 	Training 200/553. train loss: 1.5926,	0.8476 s / batch. (data: 3.34e-04). ETA=11:32:26, max mem: 20.9 GB 
[11/29 18:21:07 visual_prompt]: 	Training 300/553. train loss: 4.6174,	0.8360 s / batch. (data: 2.84e-04). ETA=11:21:35, max mem: 20.9 GB 
[11/29 18:22:43 visual_prompt]: 	Training 400/553. train loss: 8.3687,	0.8345 s / batch. (data: 5.52e-03). ETA=11:18:57, max mem: 20.9 GB 
[11/29 18:24:19 visual_prompt]: 	Training 500/553. train loss: 23.9205,	0.8320 s / batch. (data: 2.76e-04). ETA=11:15:32, max mem: 20.9 GB 
[11/29 18:25:08 visual_prompt]: Epoch 12 / 100: avg data time: 1.28e-01, avg batch time: 0.9626, average train loss: 7.8640
[11/29 18:26:02 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3088, average loss: 0.8345
[11/29 18:26:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.96	
[11/29 18:26:02 visual_prompt]: Best epoch 12: best metric: -0.834
[11/29 18:26:02 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/29 18:27:44 visual_prompt]: 	Training 100/553. train loss: 4.5378,	0.8615 s / batch. (data: 3.75e-02). ETA=11:37:19, max mem: 20.9 GB 
[11/29 18:29:16 visual_prompt]: 	Training 200/553. train loss: 6.0635,	0.8400 s / batch. (data: 3.15e-04). ETA=11:18:29, max mem: 20.9 GB 
[11/29 18:30:52 visual_prompt]: 	Training 300/553. train loss: 3.3955,	1.5560 s / batch. (data: 7.35e-01). ETA=20:54:15, max mem: 20.9 GB 
[11/29 18:32:27 visual_prompt]: 	Training 400/553. train loss: 10.6050,	0.8243 s / batch. (data: 5.45e-03). ETA=11:03:03, max mem: 20.9 GB 
[11/29 18:34:04 visual_prompt]: 	Training 500/553. train loss: 9.7557,	0.8289 s / batch. (data: 3.04e-04). ETA=11:05:21, max mem: 20.9 GB 
[11/29 18:34:54 visual_prompt]: Epoch 13 / 100: avg data time: 1.28e-01, avg batch time: 0.9624, average train loss: 6.9726
[11/29 18:35:49 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3087, average loss: 1.6762
[11/29 18:35:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.13	
[11/29 18:35:49 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/29 18:37:30 visual_prompt]: 	Training 100/553. train loss: 21.1755,	0.8169 s / batch. (data: 2.68e-04). ETA=10:53:41, max mem: 20.9 GB 
[11/29 18:39:05 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.9920 s / batch. (data: 1.61e-01). ETA=13:12:07, max mem: 20.9 GB 
[11/29 18:40:41 visual_prompt]: 	Training 300/553. train loss: 2.2619,	0.8436 s / batch. (data: 1.05e-02). ETA=11:12:14, max mem: 20.9 GB 
[11/29 18:42:16 visual_prompt]: 	Training 400/553. train loss: 2.6300,	0.8520 s / batch. (data: 7.97e-03). ETA=11:17:30, max mem: 20.9 GB 
[11/29 18:43:51 visual_prompt]: 	Training 500/553. train loss: 4.8886,	0.8434 s / batch. (data: 5.57e-03). ETA=11:09:13, max mem: 20.9 GB 
[11/29 18:44:40 visual_prompt]: Epoch 14 / 100: avg data time: 1.26e-01, avg batch time: 0.9600, average train loss: 5.9034
[11/29 18:45:34 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3105, average loss: 5.0850
[11/29 18:45:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.40	
[11/29 18:45:34 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/29 18:47:14 visual_prompt]: 	Training 100/553. train loss: 6.5107,	0.8361 s / batch. (data: 3.07e-04). ETA=11:01:17, max mem: 20.9 GB 
[11/29 18:48:47 visual_prompt]: 	Training 200/553. train loss: 1.9310,	0.8280 s / batch. (data: 2.79e-04). ETA=10:53:32, max mem: 20.9 GB 
[11/29 18:50:25 visual_prompt]: 	Training 300/553. train loss: 13.6328,	0.8480 s / batch. (data: 7.72e-04). ETA=11:07:55, max mem: 20.9 GB 
[11/29 18:51:58 visual_prompt]: 	Training 400/553. train loss: 5.5660,	0.8494 s / batch. (data: 5.40e-03). ETA=11:07:34, max mem: 20.9 GB 
[11/29 18:53:35 visual_prompt]: 	Training 500/553. train loss: 13.2362,	0.8360 s / batch. (data: 2.93e-04). ETA=10:55:42, max mem: 20.9 GB 
[11/29 18:54:25 visual_prompt]: Epoch 15 / 100: avg data time: 1.25e-01, avg batch time: 0.9596, average train loss: 8.9296
[11/29 18:55:19 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3114, average loss: 0.7932
[11/29 18:55:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.85	
[11/29 18:55:19 visual_prompt]: Best epoch 15: best metric: -0.793
[11/29 18:55:19 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/29 18:56:58 visual_prompt]: 	Training 100/553. train loss: 6.6863,	0.8517 s / batch. (data: 2.35e-02). ETA=11:05:48, max mem: 20.9 GB 
[11/29 18:58:34 visual_prompt]: 	Training 200/553. train loss: 0.8400,	0.8439 s / batch. (data: 4.42e-04). ETA=10:58:20, max mem: 20.9 GB 
[11/29 19:00:09 visual_prompt]: 	Training 300/553. train loss: 1.1524,	0.8464 s / batch. (data: 5.43e-03). ETA=10:58:49, max mem: 20.9 GB 
[11/29 19:01:45 visual_prompt]: 	Training 400/553. train loss: 5.8702,	0.8487 s / batch. (data: 7.95e-04). ETA=10:59:11, max mem: 20.9 GB 
[11/29 19:03:20 visual_prompt]: 	Training 500/553. train loss: 10.1401,	1.3400 s / batch. (data: 5.09e-01). ETA=17:18:37, max mem: 20.9 GB 
[11/29 19:04:10 visual_prompt]: Epoch 16 / 100: avg data time: 1.25e-01, avg batch time: 0.9600, average train loss: 4.0607
[11/29 19:05:05 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.3122, average loss: 1.6048
[11/29 19:05:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.91	
[11/29 19:05:05 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/29 19:06:43 visual_prompt]: 	Training 100/553. train loss: 7.7257,	0.8183 s / batch. (data: 2.75e-04). ETA=10:32:09, max mem: 20.9 GB 
[11/29 19:08:20 visual_prompt]: 	Training 200/553. train loss: 2.0183,	0.8280 s / batch. (data: 2.61e-04). ETA=10:38:17, max mem: 20.9 GB 
[11/29 19:09:55 visual_prompt]: 	Training 300/553. train loss: 6.1852,	0.8445 s / batch. (data: 2.79e-04). ETA=10:49:36, max mem: 20.9 GB 
[11/29 19:11:30 visual_prompt]: 	Training 400/553. train loss: 0.6028,	0.9210 s / batch. (data: 9.40e-02). ETA=11:46:55, max mem: 20.9 GB 
[11/29 19:13:04 visual_prompt]: 	Training 500/553. train loss: 0.8317,	1.3360 s / batch. (data: 5.02e-01). ETA=17:03:13, max mem: 20.9 GB 
[11/29 19:13:55 visual_prompt]: Epoch 17 / 100: avg data time: 1.26e-01, avg batch time: 0.9595, average train loss: 5.6851
[11/29 19:14:50 visual_prompt]: Inference (val):avg data time: 5.71e-04, avg batch time: 0.3114, average loss: 3.2075
[11/29 19:14:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.88	
[11/29 19:14:50 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/29 19:16:29 visual_prompt]: 	Training 100/553. train loss: 8.7789,	0.8587 s / batch. (data: 3.03e-04). ETA=10:55:25, max mem: 20.9 GB 
[11/29 19:18:07 visual_prompt]: 	Training 200/553. train loss: 6.0809,	0.8478 s / batch. (data: 8.38e-04). ETA=10:45:44, max mem: 20.9 GB 
[11/29 19:19:42 visual_prompt]: 	Training 300/553. train loss: 2.3445,	0.8320 s / batch. (data: 3.02e-04). ETA=10:32:17, max mem: 20.9 GB 
[11/29 19:21:18 visual_prompt]: 	Training 400/553. train loss: 4.4757,	0.8319 s / batch. (data: 4.42e-04). ETA=10:30:51, max mem: 20.9 GB 
[11/29 19:22:52 visual_prompt]: 	Training 500/553. train loss: 1.6489,	0.8669 s / batch. (data: 1.08e-02). ETA=10:55:55, max mem: 20.9 GB 
[11/29 19:23:41 visual_prompt]: Epoch 18 / 100: avg data time: 1.26e-01, avg batch time: 0.9605, average train loss: 4.2608
[11/29 19:24:36 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3109, average loss: 2.0046
[11/29 19:24:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.59	
[11/29 19:24:36 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[11/29 19:26:15 visual_prompt]: 	Training 100/553. train loss: 1.2388,	1.2879 s / batch. (data: 4.70e-01). ETA=16:11:11, max mem: 20.9 GB 
[11/29 19:27:51 visual_prompt]: 	Training 200/553. train loss: 2.4127,	0.8475 s / batch. (data: 7.98e-04). ETA=10:37:39, max mem: 20.9 GB 
[11/29 19:29:25 visual_prompt]: 	Training 300/553. train loss: 8.9535,	0.8440 s / batch. (data: 7.97e-03). ETA=10:33:38, max mem: 20.9 GB 
[11/29 19:31:04 visual_prompt]: 	Training 400/553. train loss: 2.3611,	0.8197 s / batch. (data: 2.90e-04). ETA=10:14:00, max mem: 20.9 GB 
[11/29 19:32:36 visual_prompt]: 	Training 500/553. train loss: 0.6586,	0.8520 s / batch. (data: 1.19e-02). ETA=10:36:48, max mem: 20.9 GB 
[11/29 19:33:26 visual_prompt]: Epoch 19 / 100: avg data time: 1.26e-01, avg batch time: 0.9593, average train loss: 2.8640
[11/29 19:34:21 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3095, average loss: 8.3813
[11/29 19:34:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.01	
[11/29 19:34:21 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[11/29 19:36:00 visual_prompt]: 	Training 100/553. train loss: 0.6519,	0.8501 s / batch. (data: 5.43e-03). ETA=10:33:14, max mem: 20.9 GB 
[11/29 19:37:37 visual_prompt]: 	Training 200/553. train loss: 1.2496,	0.8520 s / batch. (data: 5.44e-03). ETA=10:33:12, max mem: 20.9 GB 
[11/29 19:39:13 visual_prompt]: 	Training 300/553. train loss: 0.8050,	0.8320 s / batch. (data: 3.12e-04). ETA=10:16:57, max mem: 20.9 GB 
[11/29 19:40:49 visual_prompt]: 	Training 400/553. train loss: 1.9979,	0.8299 s / batch. (data: 7.95e-03). ETA=10:14:00, max mem: 20.9 GB 
[11/29 19:42:25 visual_prompt]: 	Training 500/553. train loss: 1.2681,	0.8280 s / batch. (data: 3.22e-04). ETA=10:11:12, max mem: 20.9 GB 
[11/29 19:43:16 visual_prompt]: Epoch 20 / 100: avg data time: 1.34e-01, avg batch time: 0.9677, average train loss: 5.0237
[11/29 19:44:11 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.3096, average loss: 3.8102
[11/29 19:44:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.32	
[11/29 19:44:11 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[11/29 19:45:54 visual_prompt]: 	Training 100/553. train loss: 7.4528,	0.8320 s / batch. (data: 3.04e-04). ETA=10:12:03, max mem: 20.9 GB 
[11/29 19:47:29 visual_prompt]: 	Training 200/553. train loss: 5.4756,	0.8197 s / batch. (data: 2.99e-04). ETA=10:01:37, max mem: 20.9 GB 
[11/29 19:49:05 visual_prompt]: 	Training 300/553. train loss: 21.7955,	0.8760 s / batch. (data: 4.13e-02). ETA=10:41:30, max mem: 20.9 GB 
[11/29 19:50:40 visual_prompt]: 	Training 400/553. train loss: 6.5294,	0.8554 s / batch. (data: 3.30e-04). ETA=10:25:00, max mem: 20.9 GB 
[11/29 19:52:17 visual_prompt]: 	Training 500/553. train loss: 5.4555,	0.8278 s / batch. (data: 7.95e-03). ETA=10:03:27, max mem: 20.9 GB 
[11/29 19:53:07 visual_prompt]: Epoch 21 / 100: avg data time: 1.35e-01, avg batch time: 0.9688, average train loss: 5.0771
[11/29 19:54:02 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3108, average loss: 0.6898
[11/29 19:54:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.28	
[11/29 19:54:02 visual_prompt]: Best epoch 21: best metric: -0.690
[11/29 19:54:02 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[11/29 19:55:41 visual_prompt]: 	Training 100/553. train loss: 1.6547,	0.8405 s / batch. (data: 5.43e-03). ETA=10:10:36, max mem: 20.9 GB 
[11/29 19:57:17 visual_prompt]: 	Training 200/553. train loss: 4.8469,	0.8480 s / batch. (data: 2.95e-04). ETA=10:14:38, max mem: 20.9 GB 
[11/29 19:58:51 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8364 s / batch. (data: 7.96e-03). ETA=10:04:49, max mem: 20.9 GB 
[11/29 20:00:28 visual_prompt]: 	Training 400/553. train loss: 21.3525,	0.8440 s / batch. (data: 3.03e-04). ETA=10:08:55, max mem: 20.9 GB 
[11/29 20:02:04 visual_prompt]: 	Training 500/553. train loss: 3.4759,	0.8645 s / batch. (data: 2.45e-02). ETA=10:22:14, max mem: 20.9 GB 
[11/29 20:02:56 visual_prompt]: Epoch 22 / 100: avg data time: 1.31e-01, avg batch time: 0.9656, average train loss: 6.3588
[11/29 20:03:51 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3092, average loss: 1.1448
[11/29 20:03:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.40	
[11/29 20:03:51 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[11/29 20:05:31 visual_prompt]: 	Training 100/553. train loss: 1.8340,	0.8323 s / batch. (data: 3.06e-04). ETA=9:56:56, max mem: 20.9 GB 
[11/29 20:07:08 visual_prompt]: 	Training 200/553. train loss: 1.3014,	0.8670 s / batch. (data: 1.05e-02). ETA=10:20:23, max mem: 20.9 GB 
[11/29 20:08:46 visual_prompt]: 	Training 300/553. train loss: 1.6809,	0.8415 s / batch. (data: 2.93e-04). ETA=10:00:44, max mem: 20.9 GB 
[11/29 20:10:20 visual_prompt]: 	Training 400/553. train loss: 2.2285,	0.8606 s / batch. (data: 1.06e-02). ETA=10:12:56, max mem: 20.9 GB 
[11/29 20:11:55 visual_prompt]: 	Training 500/553. train loss: 0.0010,	0.8213 s / batch. (data: 3.08e-04). ETA=9:43:33, max mem: 20.9 GB 
[11/29 20:12:45 visual_prompt]: Epoch 23 / 100: avg data time: 1.32e-01, avg batch time: 0.9663, average train loss: 3.0160
[11/29 20:13:40 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3105, average loss: 1.4121
[11/29 20:13:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.34	
[11/29 20:13:40 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[11/29 20:15:18 visual_prompt]: 	Training 100/553. train loss: 5.5223,	0.8351 s / batch. (data: 3.00e-04). ETA=9:51:16, max mem: 20.9 GB 
[11/29 20:16:53 visual_prompt]: 	Training 200/553. train loss: 0.9238,	0.8403 s / batch. (data: 7.97e-03). ETA=9:53:34, max mem: 20.9 GB 
[11/29 20:18:30 visual_prompt]: 	Training 300/553. train loss: 2.2984,	0.8208 s / batch. (data: 3.14e-04). ETA=9:38:24, max mem: 20.9 GB 
[11/29 20:20:07 visual_prompt]: 	Training 400/553. train loss: 2.7778,	0.8480 s / batch. (data: 3.10e-04). ETA=9:56:09, max mem: 20.9 GB 
[11/29 20:21:44 visual_prompt]: 	Training 500/553. train loss: 8.6358,	0.8360 s / batch. (data: 3.12e-04). ETA=9:46:19, max mem: 20.9 GB 
[11/29 20:22:35 visual_prompt]: Epoch 24 / 100: avg data time: 1.33e-01, avg batch time: 0.9675, average train loss: 4.2892
[11/29 20:23:30 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.3107, average loss: 15.8930
[11/29 20:23:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.24	
[11/29 20:23:30 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[11/29 20:25:13 visual_prompt]: 	Training 100/553. train loss: 16.2428,	0.8270 s / batch. (data: 1.05e-02). ETA=9:37:56, max mem: 20.9 GB 
[11/29 20:26:45 visual_prompt]: 	Training 200/553. train loss: 2.1483,	0.8760 s / batch. (data: 3.83e-02). ETA=10:10:42, max mem: 20.9 GB 
[11/29 20:28:21 visual_prompt]: 	Training 300/553. train loss: 3.0458,	0.8360 s / batch. (data: 2.82e-04). ETA=9:41:23, max mem: 20.9 GB 
[11/29 20:29:56 visual_prompt]: 	Training 400/553. train loss: 3.4441,	1.1581 s / batch. (data: 3.15e-01). ETA=13:23:30, max mem: 20.9 GB 
[11/29 20:31:32 visual_prompt]: 	Training 500/553. train loss: 6.8680,	0.8600 s / batch. (data: 4.05e-02). ETA=9:55:14, max mem: 20.9 GB 
[11/29 20:32:22 visual_prompt]: Epoch 25 / 100: avg data time: 1.28e-01, avg batch time: 0.9626, average train loss: 6.1061
[11/29 20:33:17 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3103, average loss: 9.2456
[11/29 20:33:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.45	
[11/29 20:33:17 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[11/29 20:34:56 visual_prompt]: 	Training 100/553. train loss: 10.9275,	0.8360 s / batch. (data: 7.94e-03). ETA=9:36:28, max mem: 20.9 GB 
[11/29 20:36:33 visual_prompt]: 	Training 200/553. train loss: 27.0044,	1.4971 s / batch. (data: 6.81e-01). ETA=17:09:52, max mem: 20.9 GB 
[11/29 20:38:09 visual_prompt]: 	Training 300/553. train loss: 0.0091,	0.8704 s / batch. (data: 5.42e-03). ETA=9:57:18, max mem: 20.9 GB 
[11/29 20:39:43 visual_prompt]: 	Training 400/553. train loss: 0.7513,	0.8449 s / batch. (data: 1.99e-02). ETA=9:38:26, max mem: 20.9 GB 
[11/29 20:41:18 visual_prompt]: 	Training 500/553. train loss: 1.5391,	0.8479 s / batch. (data: 3.05e-04). ETA=9:39:03, max mem: 20.9 GB 
[11/29 20:42:08 visual_prompt]: Epoch 26 / 100: avg data time: 1.25e-01, avg batch time: 0.9602, average train loss: 3.9692
[11/29 20:43:03 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3117, average loss: 1.9148
[11/29 20:43:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.57	
[11/29 20:43:03 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[11/29 20:44:43 visual_prompt]: 	Training 100/553. train loss: 7.9751,	0.8612 s / batch. (data: 1.55e-02). ETA=9:45:57, max mem: 20.9 GB 
[11/29 20:46:19 visual_prompt]: 	Training 200/553. train loss: 4.2559,	1.1003 s / batch. (data: 2.67e-01). ETA=12:26:47, max mem: 20.9 GB 
[11/29 20:47:55 visual_prompt]: 	Training 300/553. train loss: 3.7082,	0.8361 s / batch. (data: 3.17e-04). ETA=9:26:02, max mem: 20.9 GB 
[11/29 20:49:32 visual_prompt]: 	Training 400/553. train loss: 12.4739,	0.8605 s / batch. (data: 7.97e-04). ETA=9:41:07, max mem: 20.9 GB 
[11/29 20:51:09 visual_prompt]: 	Training 500/553. train loss: 2.9141,	0.8520 s / batch. (data: 8.15e-04). ETA=9:34:01, max mem: 20.9 GB 
[11/29 20:51:57 visual_prompt]: Epoch 27 / 100: avg data time: 1.32e-01, avg batch time: 0.9669, average train loss: 5.5327
[11/29 20:52:52 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3111, average loss: 2.9857
[11/29 20:52:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.25	
[11/29 20:52:52 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[11/29 20:54:31 visual_prompt]: 	Training 100/553. train loss: 0.0005,	0.8440 s / batch. (data: 3.23e-04). ETA=9:26:26, max mem: 20.9 GB 
[11/29 20:56:08 visual_prompt]: 	Training 200/553. train loss: 14.3116,	0.8480 s / batch. (data: 2.89e-04). ETA=9:27:43, max mem: 20.9 GB 
[11/29 20:57:45 visual_prompt]: 	Training 300/553. train loss: 3.0044,	1.1702 s / batch. (data: 3.37e-01). ETA=13:01:28, max mem: 20.9 GB 
[11/29 20:59:20 visual_prompt]: 	Training 400/553. train loss: 7.5925,	0.8305 s / batch. (data: 2.96e-04). ETA=9:13:12, max mem: 20.9 GB 
[11/29 21:00:54 visual_prompt]: 	Training 500/553. train loss: 0.0967,	0.8400 s / batch. (data: 2.81e-04). ETA=9:18:10, max mem: 20.9 GB 
[11/29 21:01:45 visual_prompt]: Epoch 28 / 100: avg data time: 1.29e-01, avg batch time: 0.9636, average train loss: 3.8640
[11/29 21:02:40 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3112, average loss: 0.9024
[11/29 21:02:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.51	
[11/29 21:02:40 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[11/29 21:04:26 visual_prompt]: 	Training 100/553. train loss: 3.2007,	0.8364 s / batch. (data: 3.17e-04). ETA=9:13:36, max mem: 20.9 GB 
[11/29 21:06:01 visual_prompt]: 	Training 200/553. train loss: 3.3687,	1.6032 s / batch. (data: 7.74e-01). ETA=17:38:32, max mem: 20.9 GB 
[11/29 21:07:36 visual_prompt]: 	Training 300/553. train loss: 3.0620,	0.8367 s / batch. (data: 8.32e-04). ETA=9:11:01, max mem: 20.9 GB 
[11/29 21:09:08 visual_prompt]: 	Training 400/553. train loss: 5.5722,	1.1560 s / batch. (data: 3.12e-01). ETA=12:39:24, max mem: 20.9 GB 
[11/29 21:10:45 visual_prompt]: 	Training 500/553. train loss: 5.5548,	0.8542 s / batch. (data: 3.17e-04). ETA=9:19:42, max mem: 20.9 GB 
[11/29 21:11:35 visual_prompt]: Epoch 29 / 100: avg data time: 1.33e-01, avg batch time: 0.9668, average train loss: 4.2439
[11/29 21:12:29 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3100, average loss: 5.3311
[11/29 21:12:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.96	
[11/29 21:12:29 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[11/29 21:14:08 visual_prompt]: 	Training 100/553. train loss: 1.5523,	0.8240 s / batch. (data: 5.42e-03). ETA=8:57:49, max mem: 20.9 GB 
[11/29 21:15:45 visual_prompt]: 	Training 200/553. train loss: 2.2251,	0.8400 s / batch. (data: 1.20e-02). ETA=9:06:52, max mem: 20.9 GB 
[11/29 21:17:19 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8379 s / batch. (data: 7.95e-03). ETA=9:04:06, max mem: 20.9 GB 
[11/29 21:18:55 visual_prompt]: 	Training 400/553. train loss: 0.8466,	0.9521 s / batch. (data: 1.15e-01). ETA=10:16:39, max mem: 20.9 GB 
[11/29 21:20:30 visual_prompt]: 	Training 500/553. train loss: 1.1982,	1.1282 s / batch. (data: 3.08e-01). ETA=12:08:53, max mem: 20.9 GB 
[11/29 21:21:22 visual_prompt]: Epoch 30 / 100: avg data time: 1.29e-01, avg batch time: 0.9628, average train loss: 2.4570
[11/29 21:22:16 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3111, average loss: 0.8049
[11/29 21:22:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.99	
[11/29 21:22:16 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[11/29 21:23:57 visual_prompt]: 	Training 100/553. train loss: 1.4897,	0.8420 s / batch. (data: 7.82e-04). ETA=9:01:48, max mem: 20.9 GB 
[11/29 21:25:34 visual_prompt]: 	Training 200/553. train loss: 14.7473,	0.8572 s / batch. (data: 5.47e-03). ETA=9:10:09, max mem: 20.9 GB 
[11/29 21:27:07 visual_prompt]: 	Training 300/553. train loss: 4.5189,	0.8638 s / batch. (data: 1.57e-02). ETA=9:12:58, max mem: 20.9 GB 
[11/29 21:28:42 visual_prompt]: 	Training 400/553. train loss: 1.6754,	0.9680 s / batch. (data: 1.28e-01). ETA=10:18:04, max mem: 20.9 GB 
[11/29 21:30:17 visual_prompt]: 	Training 500/553. train loss: 0.7275,	0.8280 s / batch. (data: 2.91e-04). ETA=8:47:18, max mem: 20.9 GB 
[11/29 21:31:06 visual_prompt]: Epoch 31 / 100: avg data time: 1.25e-01, avg batch time: 0.9587, average train loss: 3.5094
[11/29 21:32:01 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.3107, average loss: 0.9829
[11/29 21:32:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.52	
[11/29 21:32:01 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[11/29 21:33:41 visual_prompt]: 	Training 100/553. train loss: 0.6376,	0.8273 s / batch. (data: 4.31e-04). ETA=8:44:45, max mem: 20.9 GB 
[11/29 21:35:15 visual_prompt]: 	Training 200/553. train loss: 0.6640,	0.8316 s / batch. (data: 3.07e-04). ETA=8:46:06, max mem: 20.9 GB 
[11/29 21:36:55 visual_prompt]: 	Training 300/553. train loss: 2.4566,	0.8184 s / batch. (data: 3.01e-04). ETA=8:36:21, max mem: 20.9 GB 
[11/29 21:38:39 visual_prompt]: 	Training 400/553. train loss: 9.4297,	0.8380 s / batch. (data: 5.72e-03). ETA=8:47:20, max mem: 20.9 GB 
[11/29 21:40:31 visual_prompt]: 	Training 500/553. train loss: 1.0623,	0.8177 s / batch. (data: 3.34e-04). ETA=8:33:13, max mem: 20.9 GB 
[11/29 21:41:27 visual_prompt]: Epoch 32 / 100: avg data time: 1.90e-01, avg batch time: 1.0239, average train loss: 4.0424
[11/29 21:42:32 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.3087, average loss: 4.6429
[11/29 21:42:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.22	
[11/29 21:42:32 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[11/29 21:44:46 visual_prompt]: 	Training 100/553. train loss: 0.0045,	2.2721 s / batch. (data: 1.44e+00). ETA=23:40:13, max mem: 20.9 GB 
[11/29 21:46:54 visual_prompt]: 	Training 200/553. train loss: 7.5363,	3.0971 s / batch. (data: 2.26e+00). ETA=1 day, 8:10:44, max mem: 20.9 GB 
[11/29 21:48:59 visual_prompt]: 	Training 300/553. train loss: 1.1980,	0.8415 s / batch. (data: 5.17e-04). ETA=8:43:10, max mem: 20.9 GB 
[11/29 21:51:04 visual_prompt]: 	Training 400/553. train loss: 1.8828,	0.8480 s / batch. (data: 6.01e-03). ETA=8:45:47, max mem: 20.9 GB 
[11/29 21:53:02 visual_prompt]: 	Training 500/553. train loss: 1.0904,	0.8756 s / batch. (data: 3.26e-02). ETA=9:01:29, max mem: 20.9 GB 
[11/29 21:54:07 visual_prompt]: Epoch 33 / 100: avg data time: 4.17e-01, avg batch time: 1.2558, average train loss: 4.0622
[11/29 21:55:20 visual_prompt]: Inference (val):avg data time: 1.28e-04, avg batch time: 0.3123, average loss: 9.3758
[11/29 21:55:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.62	
[11/29 21:55:20 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[11/29 21:57:27 visual_prompt]: 	Training 100/553. train loss: 7.7062,	1.4196 s / batch. (data: 5.71e-01). ETA=14:34:15, max mem: 20.9 GB 
[11/29 21:59:22 visual_prompt]: 	Training 200/553. train loss: 1.5949,	0.8390 s / batch. (data: 1.49e-02). ETA=8:35:16, max mem: 20.9 GB 
[11/29 22:01:25 visual_prompt]: 	Training 300/553. train loss: 3.7869,	0.8438 s / batch. (data: 1.42e-03). ETA=8:36:50, max mem: 20.9 GB 
[11/29 22:03:29 visual_prompt]: 	Training 400/553. train loss: 1.8363,	0.8635 s / batch. (data: 1.14e-02). ETA=8:47:27, max mem: 20.9 GB 
[11/29 22:05:30 visual_prompt]: 	Training 500/553. train loss: 0.7041,	2.0928 s / batch. (data: 1.26e+00). ETA=21:14:54, max mem: 20.9 GB 
[11/29 22:06:33 visual_prompt]: Epoch 34 / 100: avg data time: 3.78e-01, avg batch time: 1.2168, average train loss: 4.8986
[11/29 22:07:45 visual_prompt]: Inference (val):avg data time: 1.04e-04, avg batch time: 0.3117, average loss: 12.1164
[11/29 22:07:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.31	
[11/29 22:07:45 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[11/29 22:09:55 visual_prompt]: 	Training 100/553. train loss: 6.5282,	0.8505 s / batch. (data: 1.05e-02). ETA=8:35:55, max mem: 20.9 GB 
[11/29 22:12:01 visual_prompt]: 	Training 200/553. train loss: 1.3176,	0.8779 s / batch. (data: 1.57e-03). ETA=8:51:04, max mem: 20.9 GB 
[11/29 22:14:04 visual_prompt]: 	Training 300/553. train loss: 0.7490,	0.8516 s / batch. (data: 1.13e-02). ETA=8:33:46, max mem: 20.9 GB 
[11/29 22:16:04 visual_prompt]: 	Training 400/553. train loss: 3.2232,	1.4354 s / batch. (data: 5.83e-01). ETA=14:23:36, max mem: 20.9 GB 
[11/29 22:18:05 visual_prompt]: 	Training 500/553. train loss: 0.7879,	1.0476 s / batch. (data: 2.15e-01). ETA=10:28:33, max mem: 20.9 GB 
[11/29 22:19:08 visual_prompt]: Epoch 35 / 100: avg data time: 3.97e-01, avg batch time: 1.2356, average train loss: 3.0271
[11/29 22:20:21 visual_prompt]: Inference (val):avg data time: 1.02e-04, avg batch time: 0.3081, average loss: 2.3186
[11/29 22:20:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.02	
[11/29 22:20:21 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[11/29 22:22:29 visual_prompt]: 	Training 100/553. train loss: 4.3023,	1.4560 s / batch. (data: 6.06e-01). ETA=14:29:49, max mem: 20.9 GB 
[11/29 22:24:33 visual_prompt]: 	Training 200/553. train loss: 11.4812,	0.8603 s / batch. (data: 1.08e-02). ETA=8:32:30, max mem: 20.9 GB 
[11/29 22:26:38 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8522 s / batch. (data: 6.71e-03). ETA=8:26:18, max mem: 20.9 GB 
[11/29 22:28:42 visual_prompt]: 	Training 400/553. train loss: 4.0922,	0.8640 s / batch. (data: 1.18e-02). ETA=8:31:50, max mem: 20.9 GB 
[11/29 22:30:44 visual_prompt]: 	Training 500/553. train loss: 3.9295,	1.5303 s / batch. (data: 6.99e-01). ETA=15:04:02, max mem: 20.9 GB 
[11/29 22:31:43 visual_prompt]: Epoch 36 / 100: avg data time: 3.95e-01, avg batch time: 1.2340, average train loss: 4.9105
[11/29 22:32:56 visual_prompt]: Inference (val):avg data time: 1.12e-04, avg batch time: 0.3119, average loss: 16.6301
[11/29 22:32:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.63	
[11/29 22:32:56 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[11/29 22:35:04 visual_prompt]: 	Training 100/553. train loss: 0.7047,	0.8338 s / batch. (data: 9.91e-03). ETA=8:10:27, max mem: 20.9 GB 
[11/29 22:37:05 visual_prompt]: 	Training 200/553. train loss: 2.0787,	0.8397 s / batch. (data: 1.51e-03). ETA=8:12:30, max mem: 20.9 GB 
[11/29 22:39:08 visual_prompt]: 	Training 300/553. train loss: 10.6086,	2.3520 s / batch. (data: 1.52e+00). ETA=22:55:36, max mem: 20.9 GB 
[11/29 22:41:12 visual_prompt]: 	Training 400/553. train loss: 1.4300,	2.9343 s / batch. (data: 2.09e+00). ETA=1 day, 4:31:17, max mem: 20.9 GB 
[11/29 22:43:10 visual_prompt]: 	Training 500/553. train loss: 4.1304,	1.8429 s / batch. (data: 1.01e+00). ETA=17:51:43, max mem: 20.9 GB 
[11/29 22:44:15 visual_prompt]: Epoch 37 / 100: avg data time: 3.89e-01, avg batch time: 1.2278, average train loss: 3.1469
[11/29 22:45:26 visual_prompt]: Inference (val):avg data time: 1.05e-04, avg batch time: 0.3112, average loss: 1.5632
[11/29 22:45:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.37	
[11/29 22:45:26 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[11/29 22:47:29 visual_prompt]: 	Training 100/553. train loss: 0.6823,	1.6499 s / batch. (data: 8.14e-01). ETA=15:55:14, max mem: 20.9 GB 
[11/29 22:49:29 visual_prompt]: 	Training 200/553. train loss: 7.5107,	1.9920 s / batch. (data: 1.16e+00). ETA=19:10:01, max mem: 20.9 GB 
[11/29 22:51:29 visual_prompt]: 	Training 300/553. train loss: 8.8809,	0.8202 s / batch. (data: 8.08e-04). ETA=7:52:07, max mem: 20.9 GB 
[11/29 22:53:27 visual_prompt]: 	Training 400/553. train loss: 0.7410,	0.8373 s / batch. (data: 3.06e-04). ETA=8:00:35, max mem: 20.9 GB 
[11/29 22:55:30 visual_prompt]: 	Training 500/553. train loss: 1.1086,	0.8260 s / batch. (data: 3.66e-04). ETA=7:52:44, max mem: 20.9 GB 
[11/29 22:56:30 visual_prompt]: Epoch 38 / 100: avg data time: 3.63e-01, avg batch time: 1.1997, average train loss: 4.8043
[11/29 22:57:40 visual_prompt]: Inference (val):avg data time: 9.47e-05, avg batch time: 0.3122, average loss: 3.8242
[11/29 22:57:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.26	
[11/29 22:57:40 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[11/29 22:59:43 visual_prompt]: 	Training 100/553. train loss: 0.0031,	0.8432 s / batch. (data: 3.82e-04). ETA=8:00:24, max mem: 20.9 GB 
[11/29 23:01:47 visual_prompt]: 	Training 200/553. train loss: 0.9524,	0.8601 s / batch. (data: 5.53e-04). ETA=8:08:39, max mem: 20.9 GB 
[11/29 23:03:48 visual_prompt]: 	Training 300/553. train loss: 1.0382,	0.8554 s / batch. (data: 1.17e-02). ETA=8:04:32, max mem: 20.9 GB 
[11/29 23:05:42 visual_prompt]: 	Training 400/553. train loss: 2.6546,	0.8500 s / batch. (data: 3.01e-04). ETA=8:00:04, max mem: 20.9 GB 
[11/29 23:07:38 visual_prompt]: 	Training 500/553. train loss: 5.9861,	2.6160 s / batch. (data: 1.79e+00). ETA=1 day, 0:33:03, max mem: 20.9 GB 
[11/29 23:08:36 visual_prompt]: Epoch 39 / 100: avg data time: 3.51e-01, avg batch time: 1.1857, average train loss: 3.1505
[11/29 23:09:45 visual_prompt]: Inference (val):avg data time: 8.66e-05, avg batch time: 0.3107, average loss: 3.9882
[11/29 23:09:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.49	
[11/29 23:09:45 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[11/29 23:11:50 visual_prompt]: 	Training 100/553. train loss: 7.3343,	0.8520 s / batch. (data: 1.44e-03). ETA=7:57:35, max mem: 20.9 GB 
[11/29 23:13:49 visual_prompt]: 	Training 200/553. train loss: 4.9637,	0.8572 s / batch. (data: 4.20e-02). ETA=7:59:05, max mem: 20.9 GB 
[11/29 23:15:47 visual_prompt]: 	Training 300/553. train loss: 0.6640,	0.8594 s / batch. (data: 1.36e-03). ETA=7:58:53, max mem: 20.9 GB 
[11/29 23:17:46 visual_prompt]: 	Training 400/553. train loss: 1.4683,	0.8187 s / batch. (data: 2.79e-04). ETA=7:34:48, max mem: 20.9 GB 
[11/29 23:19:49 visual_prompt]: 	Training 500/553. train loss: 3.3919,	0.8518 s / batch. (data: 1.02e-03). ETA=7:51:46, max mem: 20.9 GB 
[11/29 23:20:53 visual_prompt]: Epoch 40 / 100: avg data time: 3.71e-01, avg batch time: 1.2055, average train loss: 3.5562
[11/29 23:22:02 visual_prompt]: Inference (val):avg data time: 9.60e-05, avg batch time: 0.3098, average loss: 1.2289
[11/29 23:22:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.55	
[11/29 23:22:02 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[11/29 23:24:12 visual_prompt]: 	Training 100/553. train loss: 0.6357,	0.8442 s / batch. (data: 1.22e-02). ETA=7:45:25, max mem: 20.9 GB 
[11/29 23:26:13 visual_prompt]: 	Training 200/553. train loss: 2.0259,	0.8631 s / batch. (data: 3.14e-04). ETA=7:54:24, max mem: 20.9 GB 
[11/29 23:28:13 visual_prompt]: 	Training 300/553. train loss: 1.9744,	0.8359 s / batch. (data: 6.41e-03). ETA=7:38:03, max mem: 20.9 GB 
[11/29 23:30:10 visual_prompt]: 	Training 400/553. train loss: 1.5477,	0.8437 s / batch. (data: 1.07e-03). ETA=7:40:55, max mem: 20.9 GB 
[11/29 23:32:05 visual_prompt]: 	Training 500/553. train loss: 1.2588,	0.8340 s / batch. (data: 3.19e-04). ETA=7:34:16, max mem: 20.9 GB 
[11/29 23:33:05 visual_prompt]: Epoch 41 / 100: avg data time: 3.64e-01, avg batch time: 1.1991, average train loss: 3.5518
[11/29 23:34:15 visual_prompt]: Inference (val):avg data time: 7.31e-05, avg batch time: 0.3109, average loss: 4.7325
[11/29 23:34:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.30	
[11/29 23:34:15 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[11/29 23:36:18 visual_prompt]: 	Training 100/553. train loss: 3.9658,	0.8628 s / batch. (data: 2.27e-02). ETA=7:47:43, max mem: 20.9 GB 
[11/29 23:38:16 visual_prompt]: 	Training 200/553. train loss: 22.3402,	0.8266 s / batch. (data: 7.72e-04). ETA=7:26:44, max mem: 20.9 GB 
[11/29 23:40:15 visual_prompt]: 	Training 300/553. train loss: 0.7048,	0.8481 s / batch. (data: 3.04e-04). ETA=7:36:55, max mem: 20.9 GB 
[11/29 23:42:15 visual_prompt]: 	Training 400/553. train loss: 2.1484,	0.8520 s / batch. (data: 7.96e-03). ETA=7:37:37, max mem: 20.9 GB 
[11/29 23:44:10 visual_prompt]: 	Training 500/553. train loss: 0.0977,	0.8248 s / batch. (data: 3.17e-04). ETA=7:21:39, max mem: 20.9 GB 
[11/29 23:45:12 visual_prompt]: Epoch 42 / 100: avg data time: 3.51e-01, avg batch time: 1.1870, average train loss: 4.5260
[11/29 23:46:21 visual_prompt]: Inference (val):avg data time: 8.94e-05, avg batch time: 0.3103, average loss: 5.1800
[11/29 23:46:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.63	
[11/29 23:46:21 visual_prompt]: Stopping early.
[11/29 23:46:21 visual_prompt]: Rank of current process: 0. World size: 1
[11/29 23:46:21 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/29 23:46:21 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/29 23:46:21 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/29 23:46:21 visual_prompt]: Training with config:
[11/29 23:46:21 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr2.5_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/29 23:46:21 visual_prompt]: Loading training data...
[11/29 23:46:21 visual_prompt]: Constructing mammo-cbis dataset train...
[11/29 23:46:21 visual_prompt]: Loading validation data...
[11/29 23:46:21 visual_prompt]: Constructing mammo-cbis dataset val...
[11/29 23:46:21 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/29 23:46:30 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/29 23:46:30 visual_prompt]: tuned percent:0.525
[11/29 23:46:30 visual_prompt]: Device used for model: 0
[11/29 23:46:30 visual_prompt]: Setting up Evaluator...
[11/29 23:46:30 visual_prompt]: Setting up Trainer...
[11/29 23:46:30 visual_prompt]: 	Setting up the optimizer...
[11/29 23:46:30 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/29 23:48:32 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8521 s / batch. (data: 3.03e-04). ETA=13:03:56, max mem: 20.9 GB 
[11/29 23:50:29 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8520 s / batch. (data: 2.87e-04). ETA=13:02:25, max mem: 20.9 GB 
[11/29 23:52:30 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.7004 s / batch. (data: 1.87e+00). ETA=1 day, 17:15:20, max mem: 20.9 GB 
[11/29 23:54:26 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8541 s / batch. (data: 2.22e-02). ETA=13:01:30, max mem: 20.9 GB 
[11/29 23:56:29 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8400 s / batch. (data: 3.08e-04). ETA=12:47:11, max mem: 20.9 GB 
[11/29 23:57:31 visual_prompt]: Epoch 1 / 100: avg data time: 3.57e-01, avg batch time: 1.1938, average train loss: 1.5403
[11/29 23:58:40 visual_prompt]: Inference (val):avg data time: 8.56e-05, avg batch time: 0.3102, average loss: 1.5201
[11/29 23:58:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/29 23:58:40 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/30 00:00:43 visual_prompt]: 	Training 100/553. train loss: 1.2469,	1.5784 s / batch. (data: 7.60e-01). ETA=23:57:35, max mem: 20.9 GB 
[11/30 00:02:41 visual_prompt]: 	Training 200/553. train loss: 0.1243,	0.8405 s / batch. (data: 4.39e-04). ETA=12:44:08, max mem: 20.9 GB 
[11/30 00:04:41 visual_prompt]: 	Training 300/553. train loss: 1.6256,	1.8202 s / batch. (data: 9.98e-01). ETA=1 day, 3:31:42, max mem: 20.9 GB 
[11/30 00:06:38 visual_prompt]: 	Training 400/553. train loss: 1.0894,	0.8180 s / batch. (data: 3.19e-04). ETA=12:20:53, max mem: 20.9 GB 
[11/30 00:08:42 visual_prompt]: 	Training 500/553. train loss: 0.5472,	0.8342 s / batch. (data: 1.04e-03). ETA=12:34:12, max mem: 20.9 GB 
[11/30 00:09:43 visual_prompt]: Epoch 2 / 100: avg data time: 3.62e-01, avg batch time: 1.1989, average train loss: 1.3787
[11/30 00:10:53 visual_prompt]: Inference (val):avg data time: 5.64e-04, avg batch time: 0.3111, average loss: 4.4359
[11/30 00:10:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.97	
[11/30 00:10:53 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/30 00:12:55 visual_prompt]: 	Training 100/553. train loss: 3.5525,	0.8520 s / batch. (data: 7.97e-03). ETA=12:48:08, max mem: 20.9 GB 
[11/30 00:14:54 visual_prompt]: 	Training 200/553. train loss: 0.7205,	1.2276 s / batch. (data: 3.85e-01). ETA=18:24:43, max mem: 20.9 GB 
[11/30 00:16:51 visual_prompt]: 	Training 300/553. train loss: 1.3486,	0.8496 s / batch. (data: 1.30e-03). ETA=12:43:10, max mem: 20.9 GB 
[11/30 00:18:51 visual_prompt]: 	Training 400/553. train loss: 5.5128,	0.8196 s / batch. (data: 3.08e-04). ETA=12:14:47, max mem: 20.9 GB 
[11/30 00:20:49 visual_prompt]: 	Training 500/553. train loss: 0.8341,	1.9356 s / batch. (data: 1.08e+00). ETA=1 day, 4:52:10, max mem: 20.9 GB 
[11/30 00:21:49 visual_prompt]: Epoch 3 / 100: avg data time: 3.50e-01, avg batch time: 1.1861, average train loss: 1.7772
[11/30 00:23:01 visual_prompt]: Inference (val):avg data time: 4.49e-04, avg batch time: 0.3134, average loss: 2.0011
[11/30 00:23:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.93	
[11/30 00:23:01 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/30 00:24:48 visual_prompt]: 	Training 100/553. train loss: 1.9634,	0.8480 s / batch. (data: 3.20e-04). ETA=12:36:42, max mem: 20.9 GB 
[11/30 00:26:24 visual_prompt]: 	Training 200/553. train loss: 1.2338,	1.3480 s / batch. (data: 5.31e-01). ETA=20:00:35, max mem: 20.9 GB 
[11/30 00:28:01 visual_prompt]: 	Training 300/553. train loss: 0.9824,	0.9878 s / batch. (data: 1.59e-01). ETA=14:38:08, max mem: 20.9 GB 
[11/30 00:29:33 visual_prompt]: 	Training 400/553. train loss: 0.5550,	0.8620 s / batch. (data: 5.45e-03). ETA=12:44:54, max mem: 20.9 GB 
[11/30 00:31:11 visual_prompt]: 	Training 500/553. train loss: 1.8908,	3.4983 s / batch. (data: 2.68e+00). ETA=2 days, 3:38:25, max mem: 20.9 GB 
[11/30 00:32:03 visual_prompt]: Epoch 4 / 100: avg data time: 1.45e-01, avg batch time: 0.9795, average train loss: 1.6000
[11/30 00:32:58 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3104, average loss: 0.9835
[11/30 00:32:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.53	
[11/30 00:32:58 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/30 00:34:37 visual_prompt]: 	Training 100/553. train loss: 0.3163,	0.8525 s / batch. (data: 8.46e-03). ETA=12:32:52, max mem: 20.9 GB 
[11/30 00:36:13 visual_prompt]: 	Training 200/553. train loss: 3.4402,	1.1860 s / batch. (data: 3.68e-01). ETA=17:25:24, max mem: 20.9 GB 
[11/30 00:37:51 visual_prompt]: 	Training 300/553. train loss: 11.1062,	0.8472 s / batch. (data: 3.04e-04). ETA=12:25:19, max mem: 20.9 GB 
[11/30 00:39:26 visual_prompt]: 	Training 400/553. train loss: 4.8068,	0.8320 s / batch. (data: 2.93e-04). ETA=12:10:36, max mem: 20.9 GB 
[11/30 00:41:03 visual_prompt]: 	Training 500/553. train loss: 3.0015,	0.8508 s / batch. (data: 1.08e-02). ETA=12:25:42, max mem: 20.9 GB 
[11/30 00:41:54 visual_prompt]: Epoch 5 / 100: avg data time: 1.34e-01, avg batch time: 0.9688, average train loss: 3.1452
[11/30 00:42:48 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3091, average loss: 7.8595
[11/30 00:42:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.86	
[11/30 00:42:48 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/30 00:44:28 visual_prompt]: 	Training 100/553. train loss: 1.0219,	0.8305 s / batch. (data: 1.27e-02). ETA=12:05:49, max mem: 20.9 GB 
[11/30 00:46:03 visual_prompt]: 	Training 200/553. train loss: 6.9314,	0.8354 s / batch. (data: 3.29e-04). ETA=12:08:42, max mem: 20.9 GB 
[11/30 00:47:38 visual_prompt]: 	Training 300/553. train loss: 2.0502,	0.8600 s / batch. (data: 5.44e-03). ETA=12:28:41, max mem: 20.9 GB 
[11/30 00:49:18 visual_prompt]: 	Training 400/553. train loss: 2.9298,	0.8685 s / batch. (data: 2.58e-02). ETA=12:34:38, max mem: 20.9 GB 
[11/30 00:50:53 visual_prompt]: 	Training 500/553. train loss: 8.1810,	0.8492 s / batch. (data: 3.24e-04). ETA=12:16:28, max mem: 20.9 GB 
[11/30 00:51:43 visual_prompt]: Epoch 6 / 100: avg data time: 1.35e-01, avg batch time: 0.9679, average train loss: 3.0157
[11/30 00:52:38 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3113, average loss: 1.0766
[11/30 00:52:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.61	
[11/30 00:52:38 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/30 00:54:17 visual_prompt]: 	Training 100/553. train loss: 4.1645,	0.8480 s / batch. (data: 3.06e-04). ETA=12:13:16, max mem: 20.9 GB 
[11/30 00:55:54 visual_prompt]: 	Training 200/553. train loss: 0.8076,	0.8200 s / batch. (data: 3.29e-04). ETA=11:47:42, max mem: 20.9 GB 
[11/30 00:57:34 visual_prompt]: 	Training 300/553. train loss: 1.0695,	1.5769 s / batch. (data: 7.48e-01). ETA=22:38:15, max mem: 20.9 GB 
[11/30 00:59:10 visual_prompt]: 	Training 400/553. train loss: 0.9894,	1.5714 s / batch. (data: 7.15e-01). ETA=22:30:58, max mem: 20.9 GB 
[11/30 01:00:45 visual_prompt]: 	Training 500/553. train loss: 1.4231,	0.8429 s / batch. (data: 1.07e-02). ETA=12:03:14, max mem: 20.9 GB 
[11/30 01:01:34 visual_prompt]: Epoch 7 / 100: avg data time: 1.33e-01, avg batch time: 0.9679, average train loss: 2.4139
[11/30 01:02:29 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3112, average loss: 1.2369
[11/30 01:02:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.16	
[11/30 01:02:29 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/30 01:04:07 visual_prompt]: 	Training 100/553. train loss: 3.9532,	0.8400 s / batch. (data: 3.13e-04). ETA=11:58:35, max mem: 20.9 GB 
[11/30 01:05:44 visual_prompt]: 	Training 200/553. train loss: 4.5629,	0.8184 s / batch. (data: 3.09e-04). ETA=11:38:47, max mem: 20.9 GB 
[11/30 01:07:21 visual_prompt]: 	Training 300/553. train loss: 10.5372,	0.8413 s / batch. (data: 6.91e-04). ETA=11:56:53, max mem: 20.9 GB 
[11/30 01:08:58 visual_prompt]: 	Training 400/553. train loss: 2.3542,	0.8280 s / batch. (data: 5.43e-03). ETA=11:44:11, max mem: 20.9 GB 
[11/30 01:10:33 visual_prompt]: 	Training 500/553. train loss: 6.8852,	1.2171 s / batch. (data: 3.85e-01). ETA=17:13:06, max mem: 20.9 GB 
[11/30 01:11:24 visual_prompt]: Epoch 8 / 100: avg data time: 1.34e-01, avg batch time: 0.9680, average train loss: 3.2039
[11/30 01:12:19 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3107, average loss: 0.7341
[11/30 01:12:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.82	
[11/30 01:12:19 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/30 01:13:59 visual_prompt]: 	Training 100/553. train loss: 0.0003,	0.8480 s / batch. (data: 3.10e-04). ETA=11:57:39, max mem: 20.9 GB 
[11/30 01:15:35 visual_prompt]: 	Training 200/553. train loss: 3.1278,	0.8256 s / batch. (data: 2.95e-04). ETA=11:37:17, max mem: 20.9 GB 
[11/30 01:17:10 visual_prompt]: 	Training 300/553. train loss: 0.8057,	1.4895 s / batch. (data: 6.61e-01). ETA=20:55:32, max mem: 20.9 GB 
[11/30 01:18:47 visual_prompt]: 	Training 400/553. train loss: 1.2837,	0.8509 s / batch. (data: 1.05e-02). ETA=11:55:49, max mem: 20.9 GB 
[11/30 01:20:22 visual_prompt]: 	Training 500/553. train loss: 0.6201,	0.8526 s / batch. (data: 1.47e-02). ETA=11:55:49, max mem: 20.9 GB 
[11/30 01:21:11 visual_prompt]: Epoch 9 / 100: avg data time: 1.28e-01, avg batch time: 0.9629, average train loss: 2.7151
[11/30 01:22:06 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3093, average loss: 0.7821
[11/30 01:22:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.84	
[11/30 01:22:06 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/30 01:23:47 visual_prompt]: 	Training 100/553. train loss: 6.0433,	0.8400 s / batch. (data: 2.77e-04). ETA=11:43:07, max mem: 20.9 GB 
[11/30 01:25:21 visual_prompt]: 	Training 200/553. train loss: 0.9900,	0.8227 s / batch. (data: 3.51e-04). ETA=11:27:14, max mem: 20.9 GB 
[11/30 01:26:56 visual_prompt]: 	Training 300/553. train loss: 2.5430,	0.8481 s / batch. (data: 7.96e-03). ETA=11:47:02, max mem: 20.9 GB 
[11/30 01:28:30 visual_prompt]: 	Training 400/553. train loss: 1.1189,	0.8295 s / batch. (data: 3.59e-04). ETA=11:30:12, max mem: 20.9 GB 
[11/30 01:30:06 visual_prompt]: 	Training 500/553. train loss: 0.8032,	0.8311 s / batch. (data: 1.16e-02). ETA=11:30:06, max mem: 20.9 GB 
[11/30 01:30:56 visual_prompt]: Epoch 10 / 100: avg data time: 1.25e-01, avg batch time: 0.9590, average train loss: 3.6532
[11/30 01:31:51 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3105, average loss: 0.7692
[11/30 01:31:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.96	
[11/30 01:31:51 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/30 01:33:33 visual_prompt]: 	Training 100/553. train loss: 1.9915,	0.8680 s / batch. (data: 7.99e-03). ETA=11:58:35, max mem: 20.9 GB 
[11/30 01:35:11 visual_prompt]: 	Training 200/553. train loss: 1.4055,	0.8679 s / batch. (data: 3.26e-04). ETA=11:56:59, max mem: 20.9 GB 
[11/30 01:36:45 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.5631 s / batch. (data: 7.14e-01). ETA=21:28:47, max mem: 20.9 GB 
[11/30 01:38:21 visual_prompt]: 	Training 400/553. train loss: 3.1107,	0.8197 s / batch. (data: 2.95e-04). ETA=11:14:27, max mem: 20.9 GB 
[11/30 01:39:55 visual_prompt]: 	Training 500/553. train loss: 2.3858,	0.8480 s / batch. (data: 3.00e-04). ETA=11:36:19, max mem: 20.9 GB 
[11/30 01:40:45 visual_prompt]: Epoch 11 / 100: avg data time: 1.33e-01, avg batch time: 0.9661, average train loss: 2.7395
[11/30 01:41:40 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3104, average loss: 2.1884
[11/30 01:41:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.54	
[11/30 01:41:40 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/30 01:43:22 visual_prompt]: 	Training 100/553. train loss: 0.7223,	0.8508 s / batch. (data: 1.47e-02). ETA=11:36:28, max mem: 20.9 GB 
[11/30 01:44:59 visual_prompt]: 	Training 200/553. train loss: 2.2740,	0.8360 s / batch. (data: 2.97e-04). ETA=11:22:58, max mem: 20.9 GB 
[11/30 01:46:34 visual_prompt]: 	Training 300/553. train loss: 8.0912,	0.8719 s / batch. (data: 4.61e-04). ETA=11:50:52, max mem: 20.9 GB 
[11/30 01:48:10 visual_prompt]: 	Training 400/553. train loss: 1.6407,	0.8194 s / batch. (data: 3.19e-04). ETA=11:06:42, max mem: 20.9 GB 
[11/30 01:49:46 visual_prompt]: 	Training 500/553. train loss: 12.1122,	0.8275 s / batch. (data: 3.00e-04). ETA=11:11:52, max mem: 20.9 GB 
[11/30 01:50:36 visual_prompt]: Epoch 12 / 100: avg data time: 1.34e-01, avg batch time: 0.9683, average train loss: 2.7763
[11/30 01:51:30 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3102, average loss: 7.2946
[11/30 01:51:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.00	
[11/30 01:51:30 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/30 01:53:12 visual_prompt]: 	Training 100/553. train loss: 1.2997,	0.8480 s / batch. (data: 3.23e-04). ETA=11:26:21, max mem: 20.9 GB 
[11/30 01:54:46 visual_prompt]: 	Training 200/553. train loss: 0.6424,	0.8520 s / batch. (data: 7.96e-03). ETA=11:28:11, max mem: 20.9 GB 
[11/30 01:56:22 visual_prompt]: 	Training 300/553. train loss: 1.9891,	1.5800 s / batch. (data: 7.61e-01). ETA=21:13:34, max mem: 20.9 GB 
[11/30 01:57:58 visual_prompt]: 	Training 400/553. train loss: 1.8692,	0.8744 s / batch. (data: 5.43e-03). ETA=11:43:24, max mem: 20.9 GB 
[11/30 01:59:35 visual_prompt]: 	Training 500/553. train loss: 5.3363,	0.8200 s / batch. (data: 3.33e-04). ETA=10:58:16, max mem: 20.9 GB 
[11/30 02:00:25 visual_prompt]: Epoch 13 / 100: avg data time: 1.32e-01, avg batch time: 0.9668, average train loss: 3.2620
[11/30 02:01:20 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3107, average loss: 2.3255
[11/30 02:01:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.87	
[11/30 02:01:20 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/30 02:03:02 visual_prompt]: 	Training 100/553. train loss: 0.6281,	0.8425 s / batch. (data: 2.83e-04). ETA=11:14:11, max mem: 20.9 GB 
[11/30 02:04:37 visual_prompt]: 	Training 200/553. train loss: 0.0045,	0.8200 s / batch. (data: 3.33e-04). ETA=10:54:45, max mem: 20.9 GB 
[11/30 02:06:14 visual_prompt]: 	Training 300/553. train loss: 1.0539,	0.8209 s / batch. (data: 3.06e-04). ETA=10:54:08, max mem: 20.9 GB 
[11/30 02:07:49 visual_prompt]: 	Training 400/553. train loss: 0.6801,	0.8560 s / batch. (data: 3.32e-04). ETA=11:20:40, max mem: 20.9 GB 
[11/30 02:09:26 visual_prompt]: 	Training 500/553. train loss: 2.5838,	0.8502 s / batch. (data: 1.42e-02). ETA=11:14:40, max mem: 20.9 GB 
[11/30 02:10:14 visual_prompt]: Epoch 14 / 100: avg data time: 1.31e-01, avg batch time: 0.9659, average train loss: 2.5301
[11/30 02:11:09 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3100, average loss: 0.6842
[11/30 02:11:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 61.50	
[11/30 02:11:09 visual_prompt]: Best epoch 14: best metric: -0.684
[11/30 02:11:09 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/30 02:12:49 visual_prompt]: 	Training 100/553. train loss: 4.3418,	0.8349 s / batch. (data: 5.47e-03). ETA=11:00:21, max mem: 20.9 GB 
[11/30 02:14:24 visual_prompt]: 	Training 200/553. train loss: 1.6729,	0.8218 s / batch. (data: 2.74e-04). ETA=10:48:38, max mem: 20.9 GB 
[11/30 02:16:03 visual_prompt]: 	Training 300/553. train loss: 2.9823,	0.8434 s / batch. (data: 1.05e-02). ETA=11:04:18, max mem: 20.9 GB 
[11/30 02:17:37 visual_prompt]: 	Training 400/553. train loss: 0.5084,	1.1280 s / batch. (data: 3.03e-01). ETA=14:46:35, max mem: 20.9 GB 
[11/30 02:19:13 visual_prompt]: 	Training 500/553. train loss: 1.0066,	1.1194 s / batch. (data: 2.88e-01). ETA=14:37:54, max mem: 20.9 GB 
[11/30 02:20:05 visual_prompt]: Epoch 15 / 100: avg data time: 1.32e-01, avg batch time: 0.9678, average train loss: 3.7963
[11/30 02:20:59 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3108, average loss: 3.8092
[11/30 02:20:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.50	
[11/30 02:20:59 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/30 02:22:38 visual_prompt]: 	Training 100/553. train loss: 5.3203,	0.8407 s / batch. (data: 7.76e-03). ETA=10:57:11, max mem: 20.9 GB 
[11/30 02:24:15 visual_prompt]: 	Training 200/553. train loss: 0.4610,	0.8544 s / batch. (data: 2.64e-02). ETA=11:06:32, max mem: 20.9 GB 
[11/30 02:25:51 visual_prompt]: 	Training 300/553. train loss: 3.5131,	0.8219 s / batch. (data: 3.22e-04). ETA=10:39:46, max mem: 20.9 GB 
[11/30 02:27:27 visual_prompt]: 	Training 400/553. train loss: 7.2067,	0.8360 s / batch. (data: 7.95e-03). ETA=10:49:20, max mem: 20.9 GB 
[11/30 02:29:03 visual_prompt]: 	Training 500/553. train loss: 1.3315,	0.8355 s / batch. (data: 5.43e-03). ETA=10:47:36, max mem: 20.9 GB 
[11/30 02:29:53 visual_prompt]: Epoch 16 / 100: avg data time: 1.31e-01, avg batch time: 0.9652, average train loss: 2.9806
[11/30 02:30:48 visual_prompt]: Inference (val):avg data time: 1.68e-04, avg batch time: 0.3093, average loss: 0.7077
[11/30 02:30:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.44	
[11/30 02:30:48 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/30 02:32:27 visual_prompt]: 	Training 100/553. train loss: 6.2435,	0.8427 s / batch. (data: 1.56e-02). ETA=10:51:02, max mem: 20.9 GB 
[11/30 02:34:04 visual_prompt]: 	Training 200/553. train loss: 8.1347,	0.8520 s / batch. (data: 2.91e-04). ETA=10:56:47, max mem: 20.9 GB 
[11/30 02:35:40 visual_prompt]: 	Training 300/553. train loss: 12.6241,	0.8560 s / batch. (data: 7.96e-03). ETA=10:58:27, max mem: 20.9 GB 
[11/30 02:37:16 visual_prompt]: 	Training 400/553. train loss: 3.3180,	1.0477 s / batch. (data: 2.16e-01). ETA=13:24:07, max mem: 20.9 GB 
[11/30 02:38:51 visual_prompt]: 	Training 500/553. train loss: 2.3273,	1.4814 s / batch. (data: 6.64e-01). ETA=18:54:34, max mem: 20.9 GB 
[11/30 02:39:43 visual_prompt]: Epoch 17 / 100: avg data time: 1.32e-01, avg batch time: 0.9673, average train loss: 3.0233
[11/30 02:40:38 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3113, average loss: 3.4167
[11/30 02:40:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.38	
[11/30 02:40:38 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/30 02:42:18 visual_prompt]: 	Training 100/553. train loss: 1.2508,	0.8360 s / batch. (data: 2.99e-04). ETA=10:38:06, max mem: 20.9 GB 
[11/30 02:43:56 visual_prompt]: 	Training 200/553. train loss: 5.2008,	0.8179 s / batch. (data: 3.05e-04). ETA=10:22:59, max mem: 20.9 GB 
[11/30 02:45:32 visual_prompt]: 	Training 300/553. train loss: 1.5172,	0.8426 s / batch. (data: 1.06e-02). ETA=10:40:21, max mem: 20.9 GB 
[11/30 02:47:09 visual_prompt]: 	Training 400/553. train loss: 2.1699,	0.8640 s / batch. (data: 7.95e-03). ETA=10:55:10, max mem: 20.9 GB 
[11/30 02:48:44 visual_prompt]: 	Training 500/553. train loss: 2.3278,	0.8683 s / batch. (data: 3.23e-04). ETA=10:57:00, max mem: 20.9 GB 
[11/30 02:49:33 visual_prompt]: Epoch 18 / 100: avg data time: 1.35e-01, avg batch time: 0.9681, average train loss: 3.2838
[11/30 02:50:28 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3104, average loss: 3.8529
[11/30 02:50:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.17	
[11/30 02:50:28 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[11/30 02:52:08 visual_prompt]: 	Training 100/553. train loss: 1.1073,	1.3163 s / batch. (data: 4.91e-01). ETA=16:32:35, max mem: 20.9 GB 
[11/30 02:53:45 visual_prompt]: 	Training 200/553. train loss: 0.5272,	0.8240 s / batch. (data: 3.38e-04). ETA=10:20:01, max mem: 20.9 GB 
[11/30 02:55:21 visual_prompt]: 	Training 300/553. train loss: 0.7648,	0.8246 s / batch. (data: 5.44e-03). ETA=10:19:03, max mem: 20.9 GB 
[11/30 02:56:59 visual_prompt]: 	Training 400/553. train loss: 0.9542,	0.8492 s / batch. (data: 7.57e-04). ETA=10:36:07, max mem: 20.9 GB 
[11/30 02:58:31 visual_prompt]: 	Training 500/553. train loss: 1.7440,	0.8502 s / batch. (data: 5.45e-03). ETA=10:35:29, max mem: 20.9 GB 
[11/30 02:59:21 visual_prompt]: Epoch 19 / 100: avg data time: 1.29e-01, avg batch time: 0.9633, average train loss: 2.5396
[11/30 03:00:15 visual_prompt]: Inference (val):avg data time: 2.79e-04, avg batch time: 0.3120, average loss: 8.6541
[11/30 03:00:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.80	
[11/30 03:00:15 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[11/30 03:01:54 visual_prompt]: 	Training 100/553. train loss: 1.4667,	0.8519 s / batch. (data: 2.38e-02). ETA=10:34:33, max mem: 20.9 GB 
[11/30 03:03:31 visual_prompt]: 	Training 200/553. train loss: 0.4625,	0.8483 s / batch. (data: 3.04e-03). ETA=10:30:26, max mem: 20.9 GB 
[11/30 03:05:07 visual_prompt]: 	Training 300/553. train loss: 11.2653,	0.8371 s / batch. (data: 1.20e-02). ETA=10:20:46, max mem: 20.9 GB 
[11/30 03:06:43 visual_prompt]: 	Training 400/553. train loss: 2.3421,	0.8299 s / batch. (data: 5.45e-03). ETA=10:14:02, max mem: 20.9 GB 
[11/30 03:08:19 visual_prompt]: 	Training 500/553. train loss: 2.3442,	0.8400 s / batch. (data: 3.03e-04). ETA=10:20:04, max mem: 20.9 GB 
[11/30 03:09:10 visual_prompt]: Epoch 20 / 100: avg data time: 1.31e-01, avg batch time: 0.9667, average train loss: 3.0663
[11/30 03:10:05 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3101, average loss: 1.1841
[11/30 03:10:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 66.69	
[11/30 03:10:05 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[11/30 03:11:47 visual_prompt]: 	Training 100/553. train loss: 4.4882,	0.8357 s / batch. (data: 2.74e-04). ETA=10:14:47, max mem: 20.9 GB 
[11/30 03:13:22 visual_prompt]: 	Training 200/553. train loss: 0.4055,	0.8574 s / batch. (data: 2.91e-04). ETA=10:29:21, max mem: 20.9 GB 
[11/30 03:14:58 visual_prompt]: 	Training 300/553. train loss: 11.5161,	0.9419 s / batch. (data: 9.47e-02). ETA=11:29:48, max mem: 20.9 GB 
[11/30 03:16:34 visual_prompt]: 	Training 400/553. train loss: 3.4505,	0.8291 s / batch. (data: 1.05e-02). ETA=10:05:46, max mem: 20.9 GB 
[11/30 03:18:11 visual_prompt]: 	Training 500/553. train loss: 4.3983,	0.8360 s / batch. (data: 2.88e-04). ETA=10:09:26, max mem: 20.9 GB 
[11/30 03:19:01 visual_prompt]: Epoch 21 / 100: avg data time: 1.33e-01, avg batch time: 0.9682, average train loss: 3.2743
[11/30 03:19:55 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3105, average loss: 1.0218
[11/30 03:19:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 64.63	
[11/30 03:19:55 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[11/30 03:21:35 visual_prompt]: 	Training 100/553. train loss: 2.2802,	0.8440 s / batch. (data: 2.97e-04). ETA=10:13:07, max mem: 20.9 GB 
[11/30 03:23:11 visual_prompt]: 	Training 200/553. train loss: 2.4901,	0.8203 s / batch. (data: 2.85e-04). ETA=9:54:33, max mem: 20.9 GB 
[11/30 03:24:45 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8440 s / batch. (data: 3.10e-04). ETA=10:10:18, max mem: 20.9 GB 
[11/30 03:26:23 visual_prompt]: 	Training 400/553. train loss: 0.4690,	0.8444 s / batch. (data: 4.48e-04). ETA=10:09:11, max mem: 20.9 GB 
[11/30 03:27:58 visual_prompt]: 	Training 500/553. train loss: 2.7315,	0.8434 s / batch. (data: 1.05e-02). ETA=10:07:04, max mem: 20.9 GB 
[11/30 03:28:49 visual_prompt]: Epoch 22 / 100: avg data time: 1.29e-01, avg batch time: 0.9643, average train loss: 2.5661
[11/30 03:29:43 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3110, average loss: 2.9954
[11/30 03:29:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.53	
[11/30 03:29:43 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[11/30 03:31:24 visual_prompt]: 	Training 100/553. train loss: 3.3062,	0.8243 s / batch. (data: 3.22e-04). ETA=9:51:13, max mem: 20.9 GB 
[11/30 03:33:00 visual_prompt]: 	Training 200/553. train loss: 7.6764,	0.8473 s / batch. (data: 1.13e-02). ETA=10:06:19, max mem: 20.9 GB 
[11/30 03:34:37 visual_prompt]: 	Training 300/553. train loss: 2.3995,	0.8487 s / batch. (data: 2.88e-04). ETA=10:05:52, max mem: 20.9 GB 
[11/30 03:36:10 visual_prompt]: 	Training 400/553. train loss: 0.3571,	0.8711 s / batch. (data: 2.59e-02). ETA=10:20:24, max mem: 20.9 GB 
[11/30 03:37:44 visual_prompt]: 	Training 500/553. train loss: 0.0222,	0.8360 s / batch. (data: 3.02e-04). ETA=9:54:01, max mem: 20.9 GB 
[11/30 03:38:33 visual_prompt]: Epoch 23 / 100: avg data time: 1.24e-01, avg batch time: 0.9589, average train loss: 2.4915
[11/30 03:39:28 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.3106, average loss: 1.4835
[11/30 03:39:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 66.13	
[11/30 03:39:28 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[11/30 03:41:05 visual_prompt]: 	Training 100/553. train loss: 2.2103,	0.8187 s / batch. (data: 3.23e-04). ETA=9:39:40, max mem: 20.9 GB 
[11/30 03:42:40 visual_prompt]: 	Training 200/553. train loss: 2.7661,	0.8246 s / batch. (data: 3.09e-04). ETA=9:42:25, max mem: 20.9 GB 
[11/30 03:44:16 visual_prompt]: 	Training 300/553. train loss: 0.9720,	0.8367 s / batch. (data: 3.09e-04). ETA=9:49:38, max mem: 20.9 GB 
[11/30 03:45:52 visual_prompt]: 	Training 400/553. train loss: 3.7028,	0.8480 s / batch. (data: 2.91e-04). ETA=9:56:09, max mem: 20.9 GB 
[11/30 03:47:29 visual_prompt]: 	Training 500/553. train loss: 1.4318,	0.8400 s / batch. (data: 3.07e-04). ETA=9:49:08, max mem: 20.9 GB 
[11/30 03:48:19 visual_prompt]: Epoch 24 / 100: avg data time: 1.26e-01, avg batch time: 0.9606, average train loss: 3.6031
[11/30 03:49:13 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.3090, average loss: 6.2080
[11/30 03:49:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.38	
[11/30 03:49:13 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[11/30 03:50:56 visual_prompt]: 	Training 100/553. train loss: 1.1305,	0.8240 s / batch. (data: 3.06e-04). ETA=9:35:47, max mem: 20.9 GB 
[11/30 03:52:28 visual_prompt]: 	Training 200/553. train loss: 2.1433,	0.9616 s / batch. (data: 1.31e-01). ETA=11:10:20, max mem: 20.9 GB 
[11/30 03:54:04 visual_prompt]: 	Training 300/553. train loss: 1.3771,	0.8364 s / batch. (data: 5.45e-03). ETA=9:41:42, max mem: 20.9 GB 
[11/30 03:55:40 visual_prompt]: 	Training 400/553. train loss: 2.0029,	1.0600 s / batch. (data: 2.42e-01). ETA=12:15:24, max mem: 20.9 GB 
[11/30 03:57:16 visual_prompt]: 	Training 500/553. train loss: 3.5644,	1.2734 s / batch. (data: 4.44e-01). ETA=14:41:20, max mem: 20.9 GB 
[11/30 03:58:06 visual_prompt]: Epoch 25 / 100: avg data time: 1.29e-01, avg batch time: 0.9637, average train loss: 2.5629
[11/30 03:59:01 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3086, average loss: 11.4234
[11/30 03:59:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 68.88	
[11/30 03:59:01 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[11/30 04:00:41 visual_prompt]: 	Training 100/553. train loss: 1.9308,	0.8280 s / batch. (data: 3.47e-04). ETA=9:30:57, max mem: 20.9 GB 
[11/30 04:02:19 visual_prompt]: 	Training 200/553. train loss: 5.0691,	1.7437 s / batch. (data: 9.08e-01). ETA=19:59:31, max mem: 20.9 GB 
[11/30 04:03:56 visual_prompt]: 	Training 300/553. train loss: 0.0269,	0.8480 s / batch. (data: 7.33e-04). ETA=9:41:55, max mem: 20.9 GB 
[11/30 04:05:31 visual_prompt]: 	Training 400/553. train loss: 1.0313,	0.8576 s / batch. (data: 2.86e-04). ETA=9:47:07, max mem: 20.9 GB 
[11/30 04:07:06 visual_prompt]: 	Training 500/553. train loss: 1.3334,	0.8196 s / batch. (data: 2.97e-04). ETA=9:19:41, max mem: 20.9 GB 
[11/30 04:07:56 visual_prompt]: Epoch 26 / 100: avg data time: 1.34e-01, avg batch time: 0.9670, average train loss: 3.2823
[11/30 04:08:51 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3115, average loss: 1.6341
[11/30 04:08:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 69.17	
[11/30 04:08:51 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[11/30 04:10:32 visual_prompt]: 	Training 100/553. train loss: 0.1812,	0.8287 s / batch. (data: 2.95e-04). ETA=9:23:51, max mem: 20.9 GB 
[11/30 04:12:07 visual_prompt]: 	Training 200/553. train loss: 8.5601,	0.8440 s / batch. (data: 3.06e-04). ETA=9:32:49, max mem: 20.9 GB 
[11/30 04:13:43 visual_prompt]: 	Training 300/553. train loss: 3.8854,	0.8489 s / batch. (data: 3.09e-02). ETA=9:34:45, max mem: 20.9 GB 
[11/30 04:15:21 visual_prompt]: 	Training 400/553. train loss: 0.5293,	0.8440 s / batch. (data: 1.60e-02). ETA=9:30:00, max mem: 20.9 GB 
[11/30 04:16:58 visual_prompt]: 	Training 500/553. train loss: 2.6149,	0.8200 s / batch. (data: 2.93e-04). ETA=9:12:26, max mem: 20.9 GB 
[11/30 04:17:47 visual_prompt]: Epoch 27 / 100: avg data time: 1.36e-01, avg batch time: 0.9692, average train loss: 2.6397
[11/30 04:18:42 visual_prompt]: Inference (val):avg data time: 3.85e-04, avg batch time: 0.3116, average loss: 0.7752
[11/30 04:18:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 70.10	
[11/30 04:18:42 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[11/30 04:20:22 visual_prompt]: 	Training 100/553. train loss: 1.3799,	0.8495 s / batch. (data: 2.21e-02). ETA=9:30:08, max mem: 20.9 GB 
[11/30 04:21:59 visual_prompt]: 	Training 200/553. train loss: 10.3189,	0.8485 s / batch. (data: 2.06e-02). ETA=9:28:01, max mem: 20.9 GB 
[11/30 04:23:34 visual_prompt]: 	Training 300/553. train loss: 0.6879,	1.2738 s / batch. (data: 4.56e-01). ETA=14:10:41, max mem: 20.9 GB 
[11/30 04:25:10 visual_prompt]: 	Training 400/553. train loss: 0.7422,	0.8240 s / batch. (data: 2.66e-04). ETA=9:08:55, max mem: 20.9 GB 
[11/30 04:26:45 visual_prompt]: 	Training 500/553. train loss: 7.5876,	0.8242 s / batch. (data: 3.04e-04). ETA=9:07:38, max mem: 20.9 GB 
[11/30 04:27:37 visual_prompt]: Epoch 28 / 100: avg data time: 1.33e-01, avg batch time: 0.9674, average train loss: 2.5582
[11/30 04:28:31 visual_prompt]: Inference (val):avg data time: 2.15e-04, avg batch time: 0.3123, average loss: 1.6362
[11/30 04:28:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 69.12	
[11/30 04:28:31 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[11/30 04:30:18 visual_prompt]: 	Training 100/553. train loss: 0.3600,	0.8440 s / batch. (data: 8.71e-04). ETA=9:18:38, max mem: 20.9 GB 
[11/30 04:31:53 visual_prompt]: 	Training 200/553. train loss: 0.6727,	1.7160 s / batch. (data: 8.78e-01). ETA=18:53:01, max mem: 20.9 GB 
[11/30 04:33:27 visual_prompt]: 	Training 300/553. train loss: 3.2199,	0.8237 s / batch. (data: 5.41e-03). ETA=9:02:31, max mem: 20.9 GB 
[11/30 04:35:01 visual_prompt]: 	Training 400/553. train loss: 7.5341,	1.2000 s / batch. (data: 3.78e-01). ETA=13:08:19, max mem: 20.9 GB 
[11/30 04:36:37 visual_prompt]: 	Training 500/553. train loss: 1.2319,	0.8200 s / batch. (data: 3.00e-04). ETA=8:57:19, max mem: 20.9 GB 
[11/30 04:37:27 visual_prompt]: Epoch 29 / 100: avg data time: 1.35e-01, avg batch time: 0.9687, average train loss: 3.0130
[11/30 04:38:22 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3091, average loss: 3.1624
[11/30 04:38:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.84	
[11/30 04:38:22 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[11/30 04:40:01 visual_prompt]: 	Training 100/553. train loss: 2.8780,	0.8395 s / batch. (data: 1.06e-02). ETA=9:07:57, max mem: 20.9 GB 
[11/30 04:41:38 visual_prompt]: 	Training 200/553. train loss: 0.5284,	0.8271 s / batch. (data: 3.00e-04). ETA=8:58:28, max mem: 20.9 GB 
[11/30 04:43:12 visual_prompt]: 	Training 300/553. train loss: 0.7742,	1.1160 s / batch. (data: 2.94e-01). ETA=12:04:42, max mem: 20.9 GB 
[11/30 04:44:51 visual_prompt]: 	Training 400/553. train loss: 1.6441,	1.1459 s / batch. (data: 3.10e-01). ETA=12:22:12, max mem: 20.9 GB 
[11/30 04:46:25 visual_prompt]: 	Training 500/553. train loss: 4.2872,	1.2821 s / batch. (data: 4.40e-01). ETA=13:48:18, max mem: 20.9 GB 
[11/30 04:47:17 visual_prompt]: Epoch 30 / 100: avg data time: 1.33e-01, avg batch time: 0.9674, average train loss: 2.0951
[11/30 04:48:12 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3122, average loss: 1.0392
[11/30 04:48:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 70.74	
[11/30 04:48:12 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[11/30 04:49:53 visual_prompt]: 	Training 100/553. train loss: 0.2264,	0.8418 s / batch. (data: 1.35e-02). ETA=9:01:42, max mem: 20.9 GB 
[11/30 04:51:31 visual_prompt]: 	Training 200/553. train loss: 4.6332,	0.8575 s / batch. (data: 3.50e-04). ETA=9:10:21, max mem: 20.9 GB 
[11/30 04:53:05 visual_prompt]: 	Training 300/553. train loss: 2.4472,	0.8660 s / batch. (data: 5.45e-03). ETA=9:14:23, max mem: 20.9 GB 
[11/30 04:54:40 visual_prompt]: 	Training 400/553. train loss: 2.4227,	1.0565 s / batch. (data: 2.20e-01). ETA=11:14:33, max mem: 20.9 GB 
[11/30 04:56:16 visual_prompt]: 	Training 500/553. train loss: 2.3335,	0.8286 s / batch. (data: 9.95e-03). ETA=8:47:40, max mem: 20.9 GB 
[11/30 04:57:05 visual_prompt]: Epoch 31 / 100: avg data time: 1.31e-01, avg batch time: 0.9648, average train loss: 2.7274
[11/30 04:58:00 visual_prompt]: Inference (val):avg data time: 2.09e-04, avg batch time: 0.3103, average loss: 1.0498
[11/30 04:58:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.54	
[11/30 04:58:00 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[11/30 04:59:41 visual_prompt]: 	Training 100/553. train loss: 0.6479,	0.8320 s / batch. (data: 3.00e-04). ETA=8:47:42, max mem: 20.9 GB 
[11/30 05:01:17 visual_prompt]: 	Training 200/553. train loss: 0.2726,	0.8305 s / batch. (data: 3.40e-04). ETA=8:45:24, max mem: 20.9 GB 
[11/30 05:02:56 visual_prompt]: 	Training 300/553. train loss: 3.7536,	0.8185 s / batch. (data: 2.94e-04). ETA=8:36:25, max mem: 20.9 GB 
[11/30 05:04:33 visual_prompt]: 	Training 400/553. train loss: 1.7283,	0.8298 s / batch. (data: 7.96e-03). ETA=8:42:11, max mem: 20.9 GB 
[11/30 05:06:06 visual_prompt]: 	Training 500/553. train loss: 1.9451,	0.8501 s / batch. (data: 7.95e-03). ETA=8:53:32, max mem: 20.9 GB 
[11/30 05:06:55 visual_prompt]: Epoch 32 / 100: avg data time: 1.33e-01, avg batch time: 0.9669, average train loss: 1.9695
[11/30 05:07:50 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3097, average loss: 0.9019
[11/30 05:07:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 71.43	
[11/30 05:07:50 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[11/30 05:09:28 visual_prompt]: 	Training 100/553. train loss: 0.0012,	0.8320 s / batch. (data: 2.58e-04). ETA=8:40:04, max mem: 20.9 GB 
[11/30 05:11:06 visual_prompt]: 	Training 200/553. train loss: 0.1928,	0.9803 s / batch. (data: 1.55e-01). ETA=10:11:08, max mem: 20.9 GB 
[11/30 05:12:42 visual_prompt]: 	Training 300/553. train loss: 0.5559,	0.8295 s / batch. (data: 3.06e-04). ETA=8:35:42, max mem: 20.9 GB 
[11/30 05:14:18 visual_prompt]: 	Training 400/553. train loss: 0.2377,	0.8360 s / batch. (data: 3.06e-04). ETA=8:38:22, max mem: 20.9 GB 
[11/30 05:15:54 visual_prompt]: 	Training 500/553. train loss: 1.7838,	0.8453 s / batch. (data: 3.34e-04). ETA=8:42:42, max mem: 20.9 GB 
[11/30 05:16:43 visual_prompt]: Epoch 33 / 100: avg data time: 1.31e-01, avg batch time: 0.9649, average train loss: 2.4090
[11/30 05:17:38 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3102, average loss: 3.1907
[11/30 05:17:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.72	
[11/30 05:17:38 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[11/30 05:19:20 visual_prompt]: 	Training 100/553. train loss: 2.6494,	0.8520 s / batch. (data: 1.19e-02). ETA=8:44:41, max mem: 20.9 GB 
[11/30 05:20:54 visual_prompt]: 	Training 200/553. train loss: 5.9411,	0.8240 s / batch. (data: 3.19e-04). ETA=8:26:04, max mem: 20.9 GB 
[11/30 05:22:30 visual_prompt]: 	Training 300/553. train loss: 0.0310,	0.8525 s / batch. (data: 8.61e-03). ETA=8:42:11, max mem: 20.9 GB 
[11/30 05:24:06 visual_prompt]: 	Training 400/553. train loss: 4.6238,	0.8705 s / batch. (data: 2.26e-02). ETA=8:51:46, max mem: 20.9 GB 
[11/30 05:25:43 visual_prompt]: 	Training 500/553. train loss: 0.2215,	1.4240 s / batch. (data: 6.03e-01). ETA=14:27:29, max mem: 20.9 GB 
[11/30 05:26:33 visual_prompt]: Epoch 34 / 100: avg data time: 1.32e-01, avg batch time: 0.9662, average train loss: 2.9590
[11/30 05:27:27 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3099, average loss: 0.8867
[11/30 05:27:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 69.72	
[11/30 05:27:27 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[11/30 05:29:07 visual_prompt]: 	Training 100/553. train loss: 5.9347,	0.8286 s / batch. (data: 1.05e-02). ETA=8:22:38, max mem: 20.9 GB 
[11/30 05:30:45 visual_prompt]: 	Training 200/553. train loss: 3.2823,	0.8556 s / batch. (data: 3.56e-04). ETA=8:37:37, max mem: 20.9 GB 
[11/30 05:32:18 visual_prompt]: 	Training 300/553. train loss: 0.4184,	0.8655 s / batch. (data: 2.95e-02). ETA=8:42:10, max mem: 20.9 GB 
[11/30 05:33:52 visual_prompt]: 	Training 400/553. train loss: 4.9845,	0.8360 s / batch. (data: 3.43e-04). ETA=8:22:56, max mem: 20.9 GB 
[11/30 05:35:27 visual_prompt]: 	Training 500/553. train loss: 2.0137,	0.9570 s / batch. (data: 1.35e-01). ETA=9:34:10, max mem: 20.9 GB 
[11/30 05:36:18 visual_prompt]: Epoch 35 / 100: avg data time: 1.25e-01, avg batch time: 0.9592, average train loss: 2.2316
[11/30 05:37:12 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3108, average loss: 6.1489
[11/30 05:37:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.89	
[11/30 05:37:12 visual_prompt]: Stopping early.
[11/30 05:37:12 visual_prompt]: Rank of current process: 0. World size: 1
[11/30 05:37:12 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/30 05:37:12 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/30 05:37:12 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/30 05:37:12 visual_prompt]: Training with config:
[11/30 05:37:12 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr1.0_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/30 05:37:12 visual_prompt]: Loading training data...
[11/30 05:37:12 visual_prompt]: Constructing mammo-cbis dataset train...
[11/30 05:37:12 visual_prompt]: Loading validation data...
[11/30 05:37:12 visual_prompt]: Constructing mammo-cbis dataset val...
[11/30 05:37:12 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/30 05:37:14 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/30 05:37:14 visual_prompt]: tuned percent:0.525
[11/30 05:37:15 visual_prompt]: Device used for model: 0
[11/30 05:37:15 visual_prompt]: Setting up Evaluator...
[11/30 05:37:15 visual_prompt]: Setting up Trainer...
[11/30 05:37:15 visual_prompt]: 	Setting up the optimizer...
[11/30 05:37:15 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/30 05:38:53 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8173 s / batch. (data: 3.02e-04). ETA=12:31:53, max mem: 20.9 GB 
[11/30 05:40:27 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8480 s / batch. (data: 7.95e-03). ETA=12:58:44, max mem: 20.9 GB 
[11/30 05:42:05 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.9082 s / batch. (data: 8.30e-02). ETA=13:52:30, max mem: 20.9 GB 
[11/30 05:43:38 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8421 s / batch. (data: 3.30e-04). ETA=12:50:32, max mem: 20.9 GB 
[11/30 05:45:16 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8400 s / batch. (data: 7.94e-03). ETA=12:47:10, max mem: 20.9 GB 
[11/30 05:46:07 visual_prompt]: Epoch 1 / 100: avg data time: 1.28e-01, avg batch time: 0.9618, average train loss: 1.5403
[11/30 05:47:01 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3096, average loss: 1.5201
[11/30 05:47:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/30 05:47:01 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[11/30 05:48:40 visual_prompt]: 	Training 100/553. train loss: 0.7319,	0.8201 s / batch. (data: 2.99e-04). ETA=12:26:56, max mem: 20.9 GB 
[11/30 05:50:14 visual_prompt]: 	Training 200/553. train loss: 0.0588,	0.8402 s / batch. (data: 3.35e-04). ETA=12:43:47, max mem: 20.9 GB 
[11/30 05:51:52 visual_prompt]: 	Training 300/553. train loss: 0.7024,	0.9047 s / batch. (data: 6.55e-02). ETA=13:40:59, max mem: 20.9 GB 
[11/30 05:53:26 visual_prompt]: 	Training 400/553. train loss: 1.2103,	0.8318 s / batch. (data: 2.90e-04). ETA=12:33:28, max mem: 20.9 GB 
[11/30 05:55:03 visual_prompt]: 	Training 500/553. train loss: 0.6010,	0.8520 s / batch. (data: 2.93e-04). ETA=12:50:18, max mem: 20.9 GB 
[11/30 05:55:51 visual_prompt]: Epoch 2 / 100: avg data time: 1.25e-01, avg batch time: 0.9593, average train loss: 0.9073
[11/30 05:56:46 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.3103, average loss: 1.4984
[11/30 05:56:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.17	
[11/30 05:56:46 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[11/30 05:58:24 visual_prompt]: 	Training 100/553. train loss: 1.1307,	0.8236 s / batch. (data: 5.40e-03). ETA=12:22:31, max mem: 20.9 GB 
[11/30 06:00:00 visual_prompt]: 	Training 200/553. train loss: 0.7188,	1.4400 s / batch. (data: 6.05e-01). ETA=21:35:51, max mem: 20.9 GB 
[11/30 06:01:35 visual_prompt]: 	Training 300/553. train loss: 1.0751,	0.8338 s / batch. (data: 2.77e-04). ETA=12:28:54, max mem: 20.9 GB 
[11/30 06:03:11 visual_prompt]: 	Training 400/553. train loss: 1.2364,	0.8480 s / batch. (data: 3.90e-03). ETA=12:40:16, max mem: 20.9 GB 
[11/30 06:04:48 visual_prompt]: 	Training 500/553. train loss: 0.7146,	1.1821 s / batch. (data: 3.43e-01). ETA=17:37:53, max mem: 20.9 GB 
[11/30 06:05:38 visual_prompt]: Epoch 3 / 100: avg data time: 1.28e-01, avg batch time: 0.9618, average train loss: 1.0137
[11/30 06:06:33 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3119, average loss: 0.7381
[11/30 06:06:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.10	
[11/30 06:06:33 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[11/30 06:08:15 visual_prompt]: 	Training 100/553. train loss: 0.7098,	0.8539 s / batch. (data: 3.52e-04). ETA=12:42:01, max mem: 20.9 GB 
[11/30 06:09:51 visual_prompt]: 	Training 200/553. train loss: 0.6045,	0.8211 s / batch. (data: 3.26e-04). ETA=12:11:20, max mem: 20.9 GB 
[11/30 06:11:28 visual_prompt]: 	Training 300/553. train loss: 0.6077,	1.3784 s / batch. (data: 5.49e-01). ETA=20:25:25, max mem: 20.9 GB 
[11/30 06:13:00 visual_prompt]: 	Training 400/553. train loss: 0.5648,	1.2271 s / batch. (data: 4.09e-01). ETA=18:08:51, max mem: 20.9 GB 
[11/30 06:14:39 visual_prompt]: 	Training 500/553. train loss: 1.8408,	3.1369 s / batch. (data: 2.32e+00). ETA=1 day, 22:18:17, max mem: 20.9 GB 
[11/30 06:15:29 visual_prompt]: Epoch 4 / 100: avg data time: 1.35e-01, avg batch time: 0.9698, average train loss: 1.0741
[11/30 06:16:24 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3122, average loss: 0.6947
[11/30 06:16:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.97	
[11/30 06:16:24 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[11/30 06:18:03 visual_prompt]: 	Training 100/553. train loss: 3.1885,	0.8414 s / batch. (data: 1.05e-02). ETA=12:23:06, max mem: 20.9 GB 
[11/30 06:19:39 visual_prompt]: 	Training 200/553. train loss: 0.6860,	1.0336 s / batch. (data: 1.87e-01). ETA=15:11:05, max mem: 20.9 GB 
[11/30 06:21:17 visual_prompt]: 	Training 300/553. train loss: 0.9858,	0.8362 s / batch. (data: 1.05e-02). ETA=12:15:41, max mem: 20.9 GB 
[11/30 06:22:52 visual_prompt]: 	Training 400/553. train loss: 2.0841,	0.8408 s / batch. (data: 2.96e-04). ETA=12:18:20, max mem: 20.9 GB 
[11/30 06:24:29 visual_prompt]: 	Training 500/553. train loss: 0.5765,	0.8398 s / batch. (data: 4.50e-04). ETA=12:16:04, max mem: 20.9 GB 
[11/30 06:25:20 visual_prompt]: Epoch 5 / 100: avg data time: 1.33e-01, avg batch time: 0.9679, average train loss: 1.3406
[11/30 06:26:15 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3108, average loss: 1.1708
[11/30 06:26:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.42	
[11/30 06:26:15 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[11/30 06:27:56 visual_prompt]: 	Training 100/553. train loss: 1.4281,	0.8435 s / batch. (data: 1.05e-02). ETA=12:17:09, max mem: 20.9 GB 
[11/30 06:29:32 visual_prompt]: 	Training 200/553. train loss: 0.3868,	0.8331 s / batch. (data: 5.41e-03). ETA=12:06:41, max mem: 20.9 GB 
[11/30 06:31:07 visual_prompt]: 	Training 300/553. train loss: 0.5484,	0.8507 s / batch. (data: 3.19e-04). ETA=12:20:38, max mem: 20.9 GB 
[11/30 06:32:47 visual_prompt]: 	Training 400/553. train loss: 1.1725,	0.8600 s / batch. (data: 3.68e-04). ETA=12:27:16, max mem: 20.9 GB 
[11/30 06:34:22 visual_prompt]: 	Training 500/553. train loss: 1.1503,	0.8320 s / batch. (data: 3.27e-04). ETA=12:01:32, max mem: 20.9 GB 
[11/30 06:35:11 visual_prompt]: Epoch 6 / 100: avg data time: 1.37e-01, avg batch time: 0.9704, average train loss: 1.4767
[11/30 06:36:06 visual_prompt]: Inference (val):avg data time: 2.15e-04, avg batch time: 0.3096, average loss: 0.7122
[11/30 06:36:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.23	
[11/30 06:36:06 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[11/30 06:37:45 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8525 s / batch. (data: 2.04e-02). ETA=12:17:07, max mem: 20.9 GB 
[11/30 06:39:21 visual_prompt]: 	Training 200/553. train loss: 0.5725,	0.8559 s / batch. (data: 3.01e-02). ETA=12:18:41, max mem: 20.9 GB 
[11/30 06:41:00 visual_prompt]: 	Training 300/553. train loss: 1.1049,	1.8115 s / batch. (data: 9.59e-01). ETA=1 day, 2:00:24, max mem: 20.9 GB 
[11/30 06:42:36 visual_prompt]: 	Training 400/553. train loss: 0.8149,	1.7275 s / batch. (data: 8.88e-01). ETA=1 day, 0:45:08, max mem: 20.9 GB 
[11/30 06:44:11 visual_prompt]: 	Training 500/553. train loss: 2.5716,	0.8280 s / batch. (data: 5.43e-03). ETA=11:50:26, max mem: 20.9 GB 
[11/30 06:45:00 visual_prompt]: Epoch 7 / 100: avg data time: 1.30e-01, avg batch time: 0.9650, average train loss: 1.7214
[11/30 06:45:55 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3109, average loss: 2.4383
[11/30 06:45:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.17	
[11/30 06:45:55 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[11/30 06:47:34 visual_prompt]: 	Training 100/553. train loss: 1.4058,	0.8394 s / batch. (data: 1.10e-02). ETA=11:58:05, max mem: 20.9 GB 
[11/30 06:49:12 visual_prompt]: 	Training 200/553. train loss: 1.2943,	0.8453 s / batch. (data: 7.96e-03). ETA=12:01:41, max mem: 20.9 GB 
[11/30 06:50:48 visual_prompt]: 	Training 300/553. train loss: 1.1910,	0.8194 s / batch. (data: 3.16e-04). ETA=11:38:13, max mem: 20.9 GB 
[11/30 06:52:25 visual_prompt]: 	Training 400/553. train loss: 1.4699,	0.8267 s / batch. (data: 7.95e-03). ETA=11:43:07, max mem: 20.9 GB 
[11/30 06:54:01 visual_prompt]: 	Training 500/553. train loss: 2.6714,	1.2555 s / batch. (data: 4.28e-01). ETA=17:45:42, max mem: 20.9 GB 
[11/30 06:54:51 visual_prompt]: Epoch 8 / 100: avg data time: 1.35e-01, avg batch time: 0.9696, average train loss: 1.7855
[11/30 06:55:46 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.3095, average loss: 1.4725
[11/30 06:55:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.95	
[11/30 06:55:46 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[11/30 06:57:26 visual_prompt]: 	Training 100/553. train loss: 0.1074,	0.8231 s / batch. (data: 5.42e-03). ETA=11:36:35, max mem: 20.9 GB 
[11/30 06:59:02 visual_prompt]: 	Training 200/553. train loss: 0.8484,	0.8802 s / batch. (data: 2.82e-02). ETA=12:23:26, max mem: 20.9 GB 
[11/30 07:00:38 visual_prompt]: 	Training 300/553. train loss: 0.5530,	1.6147 s / batch. (data: 7.86e-01). ETA=22:41:02, max mem: 20.9 GB 
[11/30 07:02:16 visual_prompt]: 	Training 400/553. train loss: 1.3158,	0.8349 s / batch. (data: 3.36e-04). ETA=11:42:24, max mem: 20.9 GB 
[11/30 07:03:53 visual_prompt]: 	Training 500/553. train loss: 1.6353,	0.8421 s / batch. (data: 1.40e-02). ETA=11:47:00, max mem: 20.9 GB 
[11/30 07:04:42 visual_prompt]: Epoch 9 / 100: avg data time: 1.34e-01, avg batch time: 0.9681, average train loss: 2.0420
[11/30 07:05:36 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3107, average loss: 2.6569
[11/30 07:05:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.63	
[11/30 07:05:36 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[11/30 07:07:19 visual_prompt]: 	Training 100/553. train loss: 4.6758,	0.8240 s / batch. (data: 3.32e-04). ETA=11:29:43, max mem: 20.9 GB 
[11/30 07:08:53 visual_prompt]: 	Training 200/553. train loss: 4.1651,	0.8327 s / batch. (data: 3.22e-04). ETA=11:35:35, max mem: 20.9 GB 
[11/30 07:10:29 visual_prompt]: 	Training 300/553. train loss: 1.9748,	1.6458 s / batch. (data: 8.16e-01). ETA=22:52:08, max mem: 20.9 GB 
[11/30 07:12:04 visual_prompt]: 	Training 400/553. train loss: 0.8070,	0.8360 s / batch. (data: 7.96e-03). ETA=11:35:35, max mem: 20.9 GB 
[11/30 07:13:41 visual_prompt]: 	Training 500/553. train loss: 0.5712,	0.8320 s / batch. (data: 3.48e-04). ETA=11:30:53, max mem: 20.9 GB 
[11/30 07:14:32 visual_prompt]: Epoch 10 / 100: avg data time: 1.33e-01, avg batch time: 0.9674, average train loss: 2.7223
[11/30 07:15:27 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3100, average loss: 0.9930
[11/30 07:15:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.69	
[11/30 07:15:27 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[11/30 07:17:09 visual_prompt]: 	Training 100/553. train loss: 1.3468,	0.8440 s / batch. (data: 2.91e-04). ETA=11:38:43, max mem: 20.9 GB 
[11/30 07:18:47 visual_prompt]: 	Training 200/553. train loss: 7.5440,	0.8248 s / batch. (data: 3.17e-04). ETA=11:21:27, max mem: 20.9 GB 
[11/30 07:20:23 visual_prompt]: 	Training 300/553. train loss: 0.0002,	1.9760 s / batch. (data: 1.13e+00). ETA=1 day, 3:09:11, max mem: 20.9 GB 
[11/30 07:21:57 visual_prompt]: 	Training 400/553. train loss: 5.0701,	0.8593 s / batch. (data: 4.14e-04). ETA=11:47:04, max mem: 20.9 GB 
[11/30 07:23:31 visual_prompt]: 	Training 500/553. train loss: 4.2936,	0.8392 s / batch. (data: 3.14e-04). ETA=11:29:06, max mem: 20.9 GB 
[11/30 07:24:21 visual_prompt]: Epoch 11 / 100: avg data time: 1.32e-01, avg batch time: 0.9659, average train loss: 2.7813
[11/30 07:25:15 visual_prompt]: Inference (val):avg data time: 6.93e-05, avg batch time: 0.3095, average loss: 0.7190
[11/30 07:25:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.25	
[11/30 07:25:15 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[11/30 07:26:57 visual_prompt]: 	Training 100/553. train loss: 1.2974,	0.8271 s / batch. (data: 3.09e-04). ETA=11:17:06, max mem: 20.9 GB 
[11/30 07:28:33 visual_prompt]: 	Training 200/553. train loss: 1.2570,	0.8551 s / batch. (data: 5.42e-03). ETA=11:38:33, max mem: 20.9 GB 
[11/30 07:30:07 visual_prompt]: 	Training 300/553. train loss: 1.0898,	0.8600 s / batch. (data: 3.33e-04). ETA=11:41:07, max mem: 20.9 GB 
[11/30 07:31:44 visual_prompt]: 	Training 400/553. train loss: 2.4149,	0.8520 s / batch. (data: 1.20e-02). ETA=11:33:11, max mem: 20.9 GB 
[11/30 07:33:19 visual_prompt]: 	Training 500/553. train loss: 0.8156,	0.8404 s / batch. (data: 7.67e-04). ETA=11:22:21, max mem: 20.9 GB 
[11/30 07:34:08 visual_prompt]: Epoch 12 / 100: avg data time: 1.29e-01, avg batch time: 0.9633, average train loss: 2.2124
[11/30 07:35:03 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3111, average loss: 1.9518
[11/30 07:35:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 42.65	
[11/30 07:35:03 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[11/30 07:36:44 visual_prompt]: 	Training 100/553. train loss: 1.6673,	0.8440 s / batch. (data: 2.96e-04). ETA=11:23:07, max mem: 20.9 GB 
[11/30 07:38:16 visual_prompt]: 	Training 200/553. train loss: 1.0907,	0.8560 s / batch. (data: 5.44e-03). ETA=11:31:24, max mem: 20.9 GB 
[11/30 07:39:53 visual_prompt]: 	Training 300/553. train loss: 11.3819,	1.3950 s / batch. (data: 5.76e-01). ETA=18:44:28, max mem: 20.9 GB 
[11/30 07:41:27 visual_prompt]: 	Training 400/553. train loss: 0.8201,	0.8476 s / batch. (data: 2.27e-02). ETA=11:21:46, max mem: 20.9 GB 
[11/30 07:43:04 visual_prompt]: 	Training 500/553. train loss: 4.5766,	0.8196 s / batch. (data: 2.96e-04). ETA=10:57:56, max mem: 20.9 GB 
[11/30 07:43:54 visual_prompt]: Epoch 13 / 100: avg data time: 1.25e-01, avg batch time: 0.9598, average train loss: 3.3906
[11/30 07:44:48 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3100, average loss: 1.8345
[11/30 07:44:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 54.85	
[11/30 07:44:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[11/30 07:46:28 visual_prompt]: 	Training 100/553. train loss: 1.4706,	0.8680 s / batch. (data: 5.90e-03). ETA=11:34:32, max mem: 20.9 GB 
[11/30 07:48:05 visual_prompt]: 	Training 200/553. train loss: 0.0002,	0.8520 s / batch. (data: 3.20e-04). ETA=11:20:21, max mem: 20.9 GB 
[11/30 07:49:40 visual_prompt]: 	Training 300/553. train loss: 0.9847,	0.8214 s / batch. (data: 3.39e-04). ETA=10:54:32, max mem: 20.9 GB 
[11/30 07:51:15 visual_prompt]: 	Training 400/553. train loss: 0.6098,	0.8284 s / batch. (data: 3.34e-04). ETA=10:58:43, max mem: 20.9 GB 
[11/30 07:52:50 visual_prompt]: 	Training 500/553. train loss: 0.8445,	0.8667 s / batch. (data: 1.55e-02). ETA=11:27:44, max mem: 20.9 GB 
[11/30 07:53:40 visual_prompt]: Epoch 14 / 100: avg data time: 1.27e-01, avg batch time: 0.9614, average train loss: 2.9013
[11/30 07:54:34 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3106, average loss: 2.6851
[11/30 07:54:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.51	
[11/30 07:54:34 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[11/30 07:56:14 visual_prompt]: 	Training 100/553. train loss: 0.7502,	0.8473 s / batch. (data: 2.99e-04). ETA=11:10:10, max mem: 20.9 GB 
[11/30 07:57:54 visual_prompt]: 	Training 200/553. train loss: 6.2084,	0.8317 s / batch. (data: 3.21e-04). ETA=10:56:28, max mem: 20.9 GB 
[11/30 07:59:31 visual_prompt]: 	Training 300/553. train loss: 6.2141,	0.8330 s / batch. (data: 3.09e-04). ETA=10:56:03, max mem: 20.9 GB 
[11/30 08:01:05 visual_prompt]: 	Training 400/553. train loss: 4.9723,	1.2274 s / batch. (data: 3.92e-01). ETA=16:04:39, max mem: 20.9 GB 
[11/30 08:02:41 visual_prompt]: 	Training 500/553. train loss: 0.5792,	1.1160 s / batch. (data: 2.60e-01). ETA=14:35:15, max mem: 20.9 GB 
[11/30 08:03:32 visual_prompt]: Epoch 15 / 100: avg data time: 1.39e-01, avg batch time: 0.9724, average train loss: 2.8800
[11/30 08:04:26 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3085, average loss: 1.1222
[11/30 08:04:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.96	
[11/30 08:04:26 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[11/30 08:06:05 visual_prompt]: 	Training 100/553. train loss: 0.9188,	0.8185 s / batch. (data: 3.04e-04). ETA=10:39:52, max mem: 20.9 GB 
[11/30 08:07:41 visual_prompt]: 	Training 200/553. train loss: 1.6138,	0.8425 s / batch. (data: 1.05e-02). ETA=10:57:13, max mem: 20.9 GB 
[11/30 08:09:17 visual_prompt]: 	Training 300/553. train loss: 2.6054,	0.8671 s / batch. (data: 2.30e-02). ETA=11:14:59, max mem: 20.9 GB 
[11/30 08:10:52 visual_prompt]: 	Training 400/553. train loss: 11.0380,	0.8221 s / batch. (data: 3.26e-04). ETA=10:38:33, max mem: 20.9 GB 
[11/30 08:12:27 visual_prompt]: 	Training 500/553. train loss: 1.0261,	1.0960 s / batch. (data: 2.55e-01). ETA=14:09:29, max mem: 20.9 GB 
[11/30 08:13:17 visual_prompt]: Epoch 16 / 100: avg data time: 1.25e-01, avg batch time: 0.9596, average train loss: 3.0492
[11/30 08:14:12 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3102, average loss: 1.8886
[11/30 08:14:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.51	
[11/30 08:14:12 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[11/30 08:15:51 visual_prompt]: 	Training 100/553. train loss: 3.0164,	0.8400 s / batch. (data: 3.01e-04). ETA=10:48:56, max mem: 20.9 GB 
[11/30 08:17:29 visual_prompt]: 	Training 200/553. train loss: 0.7377,	0.8357 s / batch. (data: 5.43e-03). ETA=10:44:14, max mem: 20.9 GB 
[11/30 08:19:04 visual_prompt]: 	Training 300/553. train loss: 2.3622,	0.8278 s / batch. (data: 3.11e-04). ETA=10:36:45, max mem: 20.9 GB 
[11/30 08:20:40 visual_prompt]: 	Training 400/553. train loss: 3.6778,	1.0080 s / batch. (data: 1.66e-01). ETA=12:53:39, max mem: 20.9 GB 
[11/30 08:22:15 visual_prompt]: 	Training 500/553. train loss: 2.8617,	1.3228 s / batch. (data: 4.83e-01). ETA=16:53:06, max mem: 20.9 GB 
[11/30 08:23:07 visual_prompt]: Epoch 17 / 100: avg data time: 1.34e-01, avg batch time: 0.9677, average train loss: 3.0789
[11/30 08:24:02 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3098, average loss: 2.8243
[11/30 08:24:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.42	
[11/30 08:24:02 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[11/30 08:25:42 visual_prompt]: 	Training 100/553. train loss: 2.2617,	0.8259 s / batch. (data: 2.81e-04). ETA=10:30:25, max mem: 20.9 GB 
[11/30 08:27:21 visual_prompt]: 	Training 200/553. train loss: 1.3063,	0.8284 s / batch. (data: 2.68e-04). ETA=10:30:59, max mem: 20.9 GB 
[11/30 08:28:57 visual_prompt]: 	Training 300/553. train loss: 1.5358,	0.8495 s / batch. (data: 9.43e-03). ETA=10:45:35, max mem: 20.9 GB 
[11/30 08:30:33 visual_prompt]: 	Training 400/553. train loss: 4.9620,	0.8472 s / batch. (data: 5.43e-03). ETA=10:42:24, max mem: 20.9 GB 
[11/30 08:32:09 visual_prompt]: 	Training 500/553. train loss: 1.2236,	1.4080 s / batch. (data: 5.85e-01). ETA=17:45:23, max mem: 20.9 GB 
[11/30 08:32:58 visual_prompt]: Epoch 18 / 100: avg data time: 1.34e-01, avg batch time: 0.9687, average train loss: 3.1646
[11/30 08:33:53 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.3113, average loss: 2.9035
[11/30 08:33:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.02	
[11/30 08:33:53 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[11/30 08:35:33 visual_prompt]: 	Training 100/553. train loss: 1.2002,	0.8396 s / batch. (data: 3.09e-04). ETA=10:33:06, max mem: 20.9 GB 
[11/30 08:37:09 visual_prompt]: 	Training 200/553. train loss: 1.4388,	0.8280 s / batch. (data: 3.15e-04). ETA=10:23:01, max mem: 20.9 GB 
[11/30 08:38:45 visual_prompt]: 	Training 300/553. train loss: 12.4888,	0.8440 s / batch. (data: 1.20e-02). ETA=10:33:38, max mem: 20.9 GB 
[11/30 08:40:23 visual_prompt]: 	Training 400/553. train loss: 1.7336,	0.8384 s / batch. (data: 2.69e-04). ETA=10:28:00, max mem: 20.9 GB 
[11/30 08:41:55 visual_prompt]: 	Training 500/553. train loss: 0.7673,	0.8440 s / batch. (data: 3.13e-04). ETA=10:30:49, max mem: 20.9 GB 
[11/30 08:42:45 visual_prompt]: Epoch 19 / 100: avg data time: 1.29e-01, avg batch time: 0.9635, average train loss: 2.8470
[11/30 08:43:40 visual_prompt]: Inference (val):avg data time: 2.28e-04, avg batch time: 0.3121, average loss: 7.4800
[11/30 08:43:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.49	
[11/30 08:43:40 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[11/30 08:45:19 visual_prompt]: 	Training 100/553. train loss: 4.2764,	0.8360 s / batch. (data: 3.14e-04). ETA=10:22:42, max mem: 20.9 GB 
[11/30 08:46:56 visual_prompt]: 	Training 200/553. train loss: 0.8867,	0.9250 s / batch. (data: 1.01e-01). ETA=11:27:26, max mem: 20.9 GB 
[11/30 08:48:33 visual_prompt]: 	Training 300/553. train loss: 0.9475,	0.8674 s / batch. (data: 8.13e-04). ETA=10:43:13, max mem: 20.9 GB 
[11/30 08:50:09 visual_prompt]: 	Training 400/553. train loss: 0.5581,	0.8600 s / batch. (data: 3.55e-04). ETA=10:36:17, max mem: 20.9 GB 
[11/30 08:51:44 visual_prompt]: 	Training 500/553. train loss: 3.0633,	0.8444 s / batch. (data: 3.01e-04). ETA=10:23:21, max mem: 20.9 GB 
[11/30 08:52:36 visual_prompt]: Epoch 20 / 100: avg data time: 1.35e-01, avg batch time: 0.9689, average train loss: 2.7796
[11/30 08:53:31 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3093, average loss: 0.8203
[11/30 08:53:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.48	
[11/30 08:53:31 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[11/30 08:55:13 visual_prompt]: 	Training 100/553. train loss: 4.5754,	0.8230 s / batch. (data: 3.45e-04). ETA=10:05:27, max mem: 20.9 GB 
[11/30 08:56:51 visual_prompt]: 	Training 200/553. train loss: 4.4108,	0.8379 s / batch. (data: 5.88e-04). ETA=10:14:59, max mem: 20.9 GB 
[11/30 08:58:30 visual_prompt]: 	Training 300/553. train loss: 3.5463,	1.1006 s / batch. (data: 2.62e-01). ETA=13:26:02, max mem: 20.9 GB 
[11/30 09:00:07 visual_prompt]: 	Training 400/553. train loss: 5.4283,	0.8320 s / batch. (data: 3.06e-04). ETA=10:07:54, max mem: 20.9 GB 
[11/30 09:01:45 visual_prompt]: 	Training 500/553. train loss: 1.6179,	0.8512 s / batch. (data: 1.17e-02). ETA=10:20:29, max mem: 20.9 GB 
[11/30 09:02:34 visual_prompt]: Epoch 21 / 100: avg data time: 1.48e-01, avg batch time: 0.9814, average train loss: 2.9148
[11/30 09:03:29 visual_prompt]: Inference (val):avg data time: 2.09e-04, avg batch time: 0.3094, average loss: 15.3549
[11/30 09:03:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.30	
[11/30 09:03:29 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[11/30 09:05:09 visual_prompt]: 	Training 100/553. train loss: 1.5224,	0.8390 s / batch. (data: 2.78e-04). ETA=10:09:27, max mem: 20.9 GB 
[11/30 09:06:45 visual_prompt]: 	Training 200/553. train loss: 13.7409,	0.8482 s / batch. (data: 3.02e-04). ETA=10:14:47, max mem: 20.9 GB 
[11/30 09:08:20 visual_prompt]: 	Training 300/553. train loss: 0.0334,	0.8350 s / batch. (data: 1.05e-02). ETA=10:03:50, max mem: 20.9 GB 
[11/30 09:09:57 visual_prompt]: 	Training 400/553. train loss: 1.1310,	0.8280 s / batch. (data: 3.14e-04). ETA=9:57:20, max mem: 20.9 GB 
[11/30 09:11:34 visual_prompt]: 	Training 500/553. train loss: 22.3371,	0.8440 s / batch. (data: 1.06e-03). ETA=10:07:29, max mem: 20.9 GB 
[11/30 09:12:26 visual_prompt]: Epoch 22 / 100: avg data time: 1.36e-01, avg batch time: 0.9703, average train loss: 3.2062
[11/30 09:13:20 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3099, average loss: 4.0187
[11/30 09:13:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.98	
[11/30 09:13:20 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[11/30 09:15:02 visual_prompt]: 	Training 100/553. train loss: 4.3435,	0.8653 s / batch. (data: 2.58e-02). ETA=10:20:38, max mem: 20.9 GB 
[11/30 09:16:39 visual_prompt]: 	Training 200/553. train loss: 0.8062,	0.8457 s / batch. (data: 5.45e-03). ETA=10:05:07, max mem: 20.9 GB 
[11/30 09:18:17 visual_prompt]: 	Training 300/553. train loss: 0.5328,	0.8638 s / batch. (data: 5.58e-03). ETA=10:16:41, max mem: 20.9 GB 
[11/30 09:19:51 visual_prompt]: 	Training 400/553. train loss: 2.7154,	0.8360 s / batch. (data: 2.97e-04). ETA=9:55:25, max mem: 20.9 GB 
[11/30 09:21:26 visual_prompt]: 	Training 500/553. train loss: 1.2378,	0.8674 s / batch. (data: 1.14e-02). ETA=10:16:20, max mem: 20.9 GB 
[11/30 09:22:16 visual_prompt]: Epoch 23 / 100: avg data time: 1.34e-01, avg batch time: 0.9681, average train loss: 2.7538
[11/30 09:23:11 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3097, average loss: 0.8995
[11/30 09:23:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.44	
[11/30 09:23:11 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[11/30 09:24:49 visual_prompt]: 	Training 100/553. train loss: 1.7059,	0.8452 s / batch. (data: 1.06e-02). ETA=9:58:25, max mem: 20.9 GB 
[11/30 09:26:24 visual_prompt]: 	Training 200/553. train loss: 2.1721,	0.8560 s / batch. (data: 1.20e-02). ETA=10:04:38, max mem: 20.9 GB 
[11/30 09:28:01 visual_prompt]: 	Training 300/553. train loss: 5.1876,	0.8589 s / batch. (data: 7.03e-03). ETA=10:05:14, max mem: 20.9 GB 
[11/30 09:29:38 visual_prompt]: 	Training 400/553. train loss: 0.9383,	0.8514 s / batch. (data: 1.13e-02). ETA=9:58:31, max mem: 20.9 GB 
[11/30 09:31:16 visual_prompt]: 	Training 500/553. train loss: 0.6994,	0.8468 s / batch. (data: 3.36e-04). ETA=9:53:53, max mem: 20.9 GB 
[11/30 09:32:06 visual_prompt]: Epoch 24 / 100: avg data time: 1.32e-01, avg batch time: 0.9677, average train loss: 2.7766
[11/30 09:33:01 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3101, average loss: 0.6933
[11/30 09:33:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 51.58	
[11/30 09:33:01 visual_prompt]: Best epoch 24: best metric: -0.693
[11/30 09:33:01 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[11/30 09:34:43 visual_prompt]: 	Training 100/553. train loss: 0.9825,	0.8240 s / batch. (data: 3.15e-04). ETA=9:35:48, max mem: 20.9 GB 
[11/30 09:36:17 visual_prompt]: 	Training 200/553. train loss: 2.4278,	0.8567 s / batch. (data: 1.05e-02). ETA=9:57:15, max mem: 20.9 GB 
[11/30 09:37:55 visual_prompt]: 	Training 300/553. train loss: 0.6883,	0.8520 s / batch. (data: 3.06e-04). ETA=9:52:30, max mem: 20.9 GB 
[11/30 09:39:31 visual_prompt]: 	Training 400/553. train loss: 10.6437,	0.8398 s / batch. (data: 3.37e-04). ETA=9:42:39, max mem: 20.9 GB 
[11/30 09:41:07 visual_prompt]: 	Training 500/553. train loss: 1.9651,	1.0520 s / batch. (data: 1.90e-01). ETA=12:08:05, max mem: 20.9 GB 
[11/30 09:41:58 visual_prompt]: Epoch 25 / 100: avg data time: 1.37e-01, avg batch time: 0.9715, average train loss: 2.5519
[11/30 09:42:53 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3100, average loss: 13.9499
[11/30 09:42:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.81	
[11/30 09:42:53 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[11/30 09:44:32 visual_prompt]: 	Training 100/553. train loss: 1.1473,	0.8400 s / batch. (data: 3.09e-04). ETA=9:39:14, max mem: 20.9 GB 
[11/30 09:46:11 visual_prompt]: 	Training 200/553. train loss: 0.6540,	1.5680 s / batch. (data: 7.45e-01). ETA=17:58:38, max mem: 20.9 GB 
[11/30 09:47:48 visual_prompt]: 	Training 300/553. train loss: 0.3755,	0.8500 s / batch. (data: 3.20e-04). ETA=9:43:16, max mem: 20.9 GB 
[11/30 09:49:23 visual_prompt]: 	Training 400/553. train loss: 3.4730,	0.8254 s / batch. (data: 2.90e-04). ETA=9:25:02, max mem: 20.9 GB 
[11/30 09:50:57 visual_prompt]: 	Training 500/553. train loss: 0.6244,	0.8551 s / batch. (data: 4.18e-04). ETA=9:43:57, max mem: 20.9 GB 
[11/30 09:51:47 visual_prompt]: Epoch 26 / 100: avg data time: 1.31e-01, avg batch time: 0.9658, average train loss: 2.1947
[11/30 09:52:42 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3107, average loss: 3.6868
[11/30 09:52:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.12	
[11/30 09:52:42 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[11/30 09:54:24 visual_prompt]: 	Training 100/553. train loss: 1.7660,	0.8524 s / batch. (data: 8.43e-03). ETA=9:39:58, max mem: 20.9 GB 
[11/30 09:55:59 visual_prompt]: 	Training 200/553. train loss: 2.8494,	1.0840 s / batch. (data: 2.43e-01). ETA=12:15:42, max mem: 20.9 GB 
[11/30 09:57:35 visual_prompt]: 	Training 300/553. train loss: 5.5061,	0.8440 s / batch. (data: 3.13e-04). ETA=9:31:25, max mem: 20.9 GB 
[11/30 09:59:12 visual_prompt]: 	Training 400/553. train loss: 1.2970,	0.8185 s / batch. (data: 3.05e-04). ETA=9:12:46, max mem: 20.9 GB 
[11/30 10:00:49 visual_prompt]: 	Training 500/553. train loss: 0.5721,	0.8471 s / batch. (data: 3.05e-04). ETA=9:30:42, max mem: 20.9 GB 
[11/30 10:01:37 visual_prompt]: Epoch 27 / 100: avg data time: 1.33e-01, avg batch time: 0.9668, average train loss: 2.6540
[11/30 10:02:32 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3126, average loss: 3.5891
[11/30 10:02:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.28	
[11/30 10:02:32 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[11/30 10:04:12 visual_prompt]: 	Training 100/553. train loss: 0.0002,	1.5583 s / batch. (data: 7.22e-01). ETA=17:25:51, max mem: 20.9 GB 
[11/30 10:05:48 visual_prompt]: 	Training 200/553. train loss: 0.6356,	0.8321 s / batch. (data: 3.32e-04). ETA=9:17:05, max mem: 20.9 GB 
[11/30 10:07:26 visual_prompt]: 	Training 300/553. train loss: 2.6102,	1.3879 s / batch. (data: 5.62e-01). ETA=15:26:52, max mem: 20.9 GB 
[11/30 10:09:00 visual_prompt]: 	Training 400/553. train loss: 3.8450,	0.8788 s / batch. (data: 2.69e-02). ETA=9:45:25, max mem: 20.9 GB 
[11/30 10:10:35 visual_prompt]: 	Training 500/553. train loss: 0.6316,	0.8675 s / batch. (data: 2.34e-02). ETA=9:36:25, max mem: 20.9 GB 
[11/30 10:11:26 visual_prompt]: Epoch 28 / 100: avg data time: 1.33e-01, avg batch time: 0.9668, average train loss: 2.5020
[11/30 10:12:21 visual_prompt]: Inference (val):avg data time: 1.53e-04, avg batch time: 0.3100, average loss: 2.3042
[11/30 10:12:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.51	
[11/30 10:12:21 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[11/30 10:14:07 visual_prompt]: 	Training 100/553. train loss: 3.3383,	0.8560 s / batch. (data: 2.88e-04). ETA=9:26:36, max mem: 20.9 GB 
[11/30 10:15:43 visual_prompt]: 	Training 200/553. train loss: 1.6855,	1.5040 s / batch. (data: 6.74e-01). ETA=16:33:02, max mem: 20.9 GB 
[11/30 10:17:17 visual_prompt]: 	Training 300/553. train loss: 0.6887,	0.8560 s / batch. (data: 7.82e-04). ETA=9:23:45, max mem: 20.9 GB 
[11/30 10:18:50 visual_prompt]: 	Training 400/553. train loss: 6.9906,	1.1760 s / batch. (data: 3.21e-01). ETA=12:52:32, max mem: 20.9 GB 
[11/30 10:20:26 visual_prompt]: 	Training 500/553. train loss: 2.3491,	0.8280 s / batch. (data: 5.42e-03). ETA=9:02:33, max mem: 20.9 GB 
[11/30 10:21:16 visual_prompt]: Epoch 29 / 100: avg data time: 1.32e-01, avg batch time: 0.9673, average train loss: 2.7083
[11/30 10:22:11 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3096, average loss: 2.2601
[11/30 10:22:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.89	
[11/30 10:22:11 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[11/30 10:23:50 visual_prompt]: 	Training 100/553. train loss: 3.3582,	0.8422 s / batch. (data: 1.19e-02). ETA=9:09:43, max mem: 20.9 GB 
[11/30 10:25:28 visual_prompt]: 	Training 200/553. train loss: 0.6117,	0.8187 s / batch. (data: 3.29e-04). ETA=8:53:01, max mem: 20.9 GB 
[11/30 10:27:02 visual_prompt]: 	Training 300/553. train loss: 0.0323,	1.4880 s / batch. (data: 6.60e-01). ETA=16:06:16, max mem: 20.9 GB 
[11/30 10:28:40 visual_prompt]: 	Training 400/553. train loss: 0.7059,	0.9840 s / batch. (data: 1.52e-01). ETA=10:37:21, max mem: 20.9 GB 
[11/30 10:30:15 visual_prompt]: 	Training 500/553. train loss: 4.8829,	1.2883 s / batch. (data: 4.71e-01). ETA=13:52:19, max mem: 20.9 GB 
[11/30 10:31:07 visual_prompt]: Epoch 30 / 100: avg data time: 1.35e-01, avg batch time: 0.9690, average train loss: 2.6531
[11/30 10:32:02 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3116, average loss: 2.0257
[11/30 10:32:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.52	
[11/30 10:32:02 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[11/30 10:33:43 visual_prompt]: 	Training 100/553. train loss: 0.9641,	0.8583 s / batch. (data: 7.94e-03). ETA=9:12:18, max mem: 20.9 GB 
[11/30 10:35:20 visual_prompt]: 	Training 200/553. train loss: 3.9513,	0.8409 s / batch. (data: 8.65e-03). ETA=8:59:41, max mem: 20.9 GB 
[11/30 10:36:53 visual_prompt]: 	Training 300/553. train loss: 10.7418,	0.8526 s / batch. (data: 7.96e-03). ETA=9:05:47, max mem: 20.9 GB 
[11/30 10:38:28 visual_prompt]: 	Training 400/553. train loss: 6.2864,	0.8720 s / batch. (data: 3.03e-02). ETA=9:16:45, max mem: 20.9 GB 
[11/30 10:40:03 visual_prompt]: 	Training 500/553. train loss: 0.9899,	0.8668 s / batch. (data: 2.71e-02). ETA=9:11:59, max mem: 20.9 GB 
[11/30 10:40:52 visual_prompt]: Epoch 31 / 100: avg data time: 1.23e-01, avg batch time: 0.9581, average train loss: 2.2740
[11/30 10:41:46 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3095, average loss: 0.7490
[11/30 10:41:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.76	
[11/30 10:41:46 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[11/30 10:43:27 visual_prompt]: 	Training 100/553. train loss: 0.6049,	0.8464 s / batch. (data: 8.66e-04). ETA=8:56:50, max mem: 20.9 GB 
[11/30 10:45:02 visual_prompt]: 	Training 200/553. train loss: 0.5663,	0.8760 s / batch. (data: 1.05e-02). ETA=9:14:09, max mem: 20.9 GB 
[11/30 10:46:40 visual_prompt]: 	Training 300/553. train loss: 7.0590,	1.6837 s / batch. (data: 8.51e-01). ETA=17:42:19, max mem: 20.9 GB 
[11/30 10:48:16 visual_prompt]: 	Training 400/553. train loss: 0.7622,	0.8323 s / batch. (data: 3.01e-04). ETA=8:43:45, max mem: 20.9 GB 
[11/30 10:49:51 visual_prompt]: 	Training 500/553. train loss: 0.6783,	0.8440 s / batch. (data: 7.94e-03). ETA=8:49:42, max mem: 20.9 GB 
[11/30 10:50:39 visual_prompt]: Epoch 32 / 100: avg data time: 1.28e-01, avg batch time: 0.9626, average train loss: 2.2939
[11/30 10:51:33 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3120, average loss: 2.3057
[11/30 10:51:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.82	
[11/30 10:51:33 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[11/30 10:53:11 visual_prompt]: 	Training 100/553. train loss: 0.0002,	1.1720 s / batch. (data: 3.24e-01). ETA=12:12:34, max mem: 20.9 GB 
[11/30 10:54:49 visual_prompt]: 	Training 200/553. train loss: 2.4458,	0.8360 s / batch. (data: 3.23e-04). ETA=8:41:10, max mem: 20.9 GB 
[11/30 10:56:24 visual_prompt]: 	Training 300/553. train loss: 0.7954,	0.8212 s / batch. (data: 2.95e-04). ETA=8:30:35, max mem: 20.9 GB 
[11/30 10:58:00 visual_prompt]: 	Training 400/553. train loss: 1.3548,	0.8640 s / batch. (data: 2.80e-04). ETA=8:55:44, max mem: 20.9 GB 
[11/30 10:59:34 visual_prompt]: 	Training 500/553. train loss: 0.6328,	0.8434 s / batch. (data: 5.43e-03). ETA=8:41:31, max mem: 20.9 GB 
[11/30 11:00:24 visual_prompt]: Epoch 33 / 100: avg data time: 1.25e-01, avg batch time: 0.9591, average train loss: 2.1522
[11/30 11:01:18 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3090, average loss: 0.6910
[11/30 11:01:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.83	
[11/30 11:01:18 visual_prompt]: Best epoch 33: best metric: -0.691
[11/30 11:01:18 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[11/30 11:02:59 visual_prompt]: 	Training 100/553. train loss: 0.7138,	0.8363 s / batch. (data: 7.95e-03). ETA=8:35:02, max mem: 20.9 GB 
[11/30 11:04:33 visual_prompt]: 	Training 200/553. train loss: 3.4925,	0.8193 s / batch. (data: 3.55e-04). ETA=8:23:10, max mem: 20.9 GB 
[11/30 11:06:22 visual_prompt]: 	Training 300/553. train loss: 2.9661,	1.3043 s / batch. (data: 4.83e-01). ETA=13:18:54, max mem: 20.9 GB 
[11/30 11:08:34 visual_prompt]: 	Training 400/553. train loss: 1.7802,	0.8469 s / batch. (data: 2.30e-03). ETA=8:37:18, max mem: 20.9 GB 
[11/30 11:10:41 visual_prompt]: 	Training 500/553. train loss: 0.8462,	2.4895 s / batch. (data: 1.65e+00). ETA=1 day, 1:16:32, max mem: 20.9 GB 
[11/30 11:11:40 visual_prompt]: Epoch 34 / 100: avg data time: 2.88e-01, avg batch time: 1.1245, average train loss: 2.6615
[11/30 11:12:54 visual_prompt]: Inference (val):avg data time: 2.96e-04, avg batch time: 0.3118, average loss: 1.1740
[11/30 11:12:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.93	
[11/30 11:12:54 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[11/30 11:15:14 visual_prompt]: 	Training 100/553. train loss: 3.9830,	1.2366 s / batch. (data: 4.00e-01). ETA=12:30:09, max mem: 20.9 GB 
[11/30 11:17:18 visual_prompt]: 	Training 200/553. train loss: 1.6856,	1.0155 s / batch. (data: 1.90e-01). ETA=10:14:21, max mem: 20.9 GB 
[11/30 11:19:16 visual_prompt]: 	Training 300/553. train loss: 1.6449,	0.8323 s / batch. (data: 8.61e-04). ETA=8:22:08, max mem: 20.9 GB 
[11/30 11:21:19 visual_prompt]: 	Training 400/553. train loss: 0.6149,	1.1601 s / batch. (data: 3.18e-01). ETA=11:37:55, max mem: 20.9 GB 
[11/30 11:23:18 visual_prompt]: 	Training 500/553. train loss: 0.6931,	0.9701 s / batch. (data: 1.45e-01). ETA=9:42:03, max mem: 20.9 GB 
[11/30 11:24:22 visual_prompt]: Epoch 35 / 100: avg data time: 4.07e-01, avg batch time: 1.2440, average train loss: 1.8298
[11/30 11:25:34 visual_prompt]: Inference (val):avg data time: 3.25e-04, avg batch time: 0.3124, average loss: 1.8427
[11/30 11:25:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.98	
[11/30 11:25:34 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[11/30 11:27:39 visual_prompt]: 	Training 100/553. train loss: 1.1378,	1.1524 s / batch. (data: 3.00e-01). ETA=11:28:26, max mem: 20.9 GB 
[11/30 11:29:45 visual_prompt]: 	Training 200/553. train loss: 1.7021,	0.8548 s / batch. (data: 1.09e-02). ETA=8:29:13, max mem: 20.9 GB 
[11/30 11:31:48 visual_prompt]: 	Training 300/553. train loss: 3.7361,	0.8200 s / batch. (data: 5.56e-03). ETA=8:07:09, max mem: 20.9 GB 
[11/30 11:33:42 visual_prompt]: 	Training 400/553. train loss: 1.4654,	0.8554 s / batch. (data: 1.44e-03). ETA=8:26:46, max mem: 20.9 GB 
[11/30 11:35:39 visual_prompt]: 	Training 500/553. train loss: 0.5981,	1.1895 s / batch. (data: 3.41e-01). ETA=11:42:41, max mem: 20.9 GB 
[11/30 11:36:36 visual_prompt]: Epoch 36 / 100: avg data time: 3.60e-01, avg batch time: 1.1964, average train loss: 2.2093
[11/30 11:37:42 visual_prompt]: Inference (val):avg data time: 1.74e-04, avg batch time: 0.3116, average loss: 5.9647
[11/30 11:37:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.15	
[11/30 11:37:42 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[11/30 11:39:42 visual_prompt]: 	Training 100/553. train loss: 2.6822,	0.8322 s / batch. (data: 1.00e-03). ETA=8:09:28, max mem: 20.9 GB 
[11/30 11:41:34 visual_prompt]: 	Training 200/553. train loss: 0.6890,	0.8467 s / batch. (data: 6.09e-03). ETA=8:16:37, max mem: 20.9 GB 
[11/30 11:43:28 visual_prompt]: 	Training 300/553. train loss: 1.9766,	1.9240 s / batch. (data: 1.10e+00). ETA=18:45:17, max mem: 20.9 GB 
[11/30 11:45:24 visual_prompt]: 	Training 400/553. train loss: 1.1788,	2.6643 s / batch. (data: 1.85e+00). ETA=1 day, 1:53:49, max mem: 20.9 GB 
[11/30 11:47:13 visual_prompt]: 	Training 500/553. train loss: 4.2658,	1.0203 s / batch. (data: 1.87e-01). ETA=9:53:21, max mem: 20.9 GB 
[11/30 11:48:16 visual_prompt]: Epoch 37 / 100: avg data time: 3.11e-01, avg batch time: 1.1450, average train loss: 2.0978
[11/30 11:49:28 visual_prompt]: Inference (val):avg data time: 2.94e-04, avg batch time: 0.3086, average loss: 1.5988
[11/30 11:49:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.77	
[11/30 11:49:28 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[11/30 11:51:30 visual_prompt]: 	Training 100/553. train loss: 0.8851,	0.8224 s / batch. (data: 7.97e-03). ETA=7:56:08, max mem: 20.9 GB 
[11/30 11:53:31 visual_prompt]: 	Training 200/553. train loss: 2.0352,	2.0360 s / batch. (data: 1.19e+00). ETA=19:35:25, max mem: 20.9 GB 
[11/30 11:55:30 visual_prompt]: 	Training 300/553. train loss: 1.5409,	0.8177 s / batch. (data: 3.30e-04). ETA=7:50:43, max mem: 20.9 GB 
[11/30 11:57:30 visual_prompt]: 	Training 400/553. train loss: 0.6111,	1.8197 s / batch. (data: 9.98e-01). ETA=17:24:27, max mem: 20.9 GB 
[11/30 11:59:34 visual_prompt]: 	Training 500/553. train loss: 22.3139,	2.2395 s / batch. (data: 1.41e+00). ETA=21:21:41, max mem: 20.9 GB 
[11/30 12:00:36 visual_prompt]: Epoch 38 / 100: avg data time: 3.70e-01, avg batch time: 1.2065, average train loss: 2.1677
[11/30 12:01:46 visual_prompt]: Inference (val):avg data time: 8.57e-05, avg batch time: 0.3118, average loss: 4.7694
[11/30 12:01:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.79	
[11/30 12:01:46 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[11/30 12:03:47 visual_prompt]: 	Training 100/553. train loss: 0.0604,	0.8209 s / batch. (data: 5.99e-03). ETA=7:47:43, max mem: 20.9 GB 
[11/30 12:05:50 visual_prompt]: 	Training 200/553. train loss: 0.6759,	0.8320 s / batch. (data: 8.93e-04). ETA=7:52:39, max mem: 20.9 GB 
[11/30 12:07:51 visual_prompt]: 	Training 300/553. train loss: 3.5685,	0.8589 s / batch. (data: 1.14e-02). ETA=8:06:29, max mem: 20.9 GB 
[11/30 12:09:42 visual_prompt]: 	Training 400/553. train loss: 2.7708,	0.8301 s / batch. (data: 2.82e-04). ETA=7:48:50, max mem: 20.9 GB 
[11/30 12:11:37 visual_prompt]: 	Training 500/553. train loss: 1.4081,	2.4674 s / batch. (data: 1.64e+00). ETA=23:09:22, max mem: 20.9 GB 
[11/30 12:12:32 visual_prompt]: Epoch 39 / 100: avg data time: 3.33e-01, avg batch time: 1.1677, average train loss: 1.9088
[11/30 12:13:39 visual_prompt]: Inference (val):avg data time: 1.90e-04, avg batch time: 0.3104, average loss: 0.8640
[11/30 12:13:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.66	
[11/30 12:13:39 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[11/30 12:15:42 visual_prompt]: 	Training 100/553. train loss: 4.1731,	0.8488 s / batch. (data: 2.45e-02). ETA=7:55:49, max mem: 20.9 GB 
[11/30 12:17:35 visual_prompt]: 	Training 200/553. train loss: 1.3754,	0.9111 s / batch. (data: 2.23e-02). ETA=8:29:11, max mem: 20.9 GB 
[11/30 12:19:29 visual_prompt]: 	Training 300/553. train loss: 5.2486,	0.8582 s / batch. (data: 3.43e-04). ETA=7:58:11, max mem: 20.9 GB 
[11/30 12:21:23 visual_prompt]: 	Training 400/553. train loss: 0.7807,	0.8333 s / batch. (data: 2.57e-03). ETA=7:42:54, max mem: 20.9 GB 
[11/30 12:23:16 visual_prompt]: 	Training 500/553. train loss: 1.5303,	0.8260 s / batch. (data: 3.10e-04). ETA=7:37:31, max mem: 20.9 GB 
[11/30 12:24:17 visual_prompt]: Epoch 40 / 100: avg data time: 3.20e-01, avg batch time: 1.1535, average train loss: 2.1051
[11/30 12:25:23 visual_prompt]: Inference (val):avg data time: 6.50e-05, avg batch time: 0.3091, average loss: 0.8930
[11/30 12:25:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.54	
[11/30 12:25:23 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[11/30 12:27:28 visual_prompt]: 	Training 100/553. train loss: 0.6960,	0.8425 s / batch. (data: 1.05e-02). ETA=7:44:29, max mem: 20.9 GB 
[11/30 12:29:26 visual_prompt]: 	Training 200/553. train loss: 1.1739,	0.8401 s / batch. (data: 7.86e-04). ETA=7:41:48, max mem: 20.9 GB 
[11/30 12:31:20 visual_prompt]: 	Training 300/553. train loss: 2.3497,	0.8556 s / batch. (data: 1.10e-02). ETA=7:48:52, max mem: 20.9 GB 
[11/30 12:33:14 visual_prompt]: 	Training 400/553. train loss: 0.9638,	0.8402 s / batch. (data: 2.91e-04). ETA=7:39:02, max mem: 20.9 GB 
[11/30 12:35:06 visual_prompt]: 	Training 500/553. train loss: 2.8615,	0.8485 s / batch. (data: 7.75e-04). ETA=7:42:10, max mem: 20.9 GB 
[11/30 12:36:03 visual_prompt]: Epoch 41 / 100: avg data time: 3.22e-01, avg batch time: 1.1565, average train loss: 1.9298
[11/30 12:37:09 visual_prompt]: Inference (val):avg data time: 5.44e-05, avg batch time: 0.3083, average loss: 1.0720
[11/30 12:37:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 55.11	
[11/30 12:37:09 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[11/30 12:39:06 visual_prompt]: 	Training 100/553. train loss: 2.6607,	0.8232 s / batch. (data: 7.20e-03). ETA=7:26:17, max mem: 20.9 GB 
[11/30 12:40:59 visual_prompt]: 	Training 200/553. train loss: 3.9794,	0.8908 s / batch. (data: 6.20e-02). ETA=8:01:26, max mem: 20.9 GB 
[11/30 12:42:55 visual_prompt]: 	Training 300/553. train loss: 0.8868,	0.8293 s / batch. (data: 4.21e-04). ETA=7:26:49, max mem: 20.9 GB 
[11/30 12:44:49 visual_prompt]: 	Training 400/553. train loss: 0.9817,	0.8479 s / batch. (data: 3.39e-04). ETA=7:35:26, max mem: 20.9 GB 
[11/30 12:46:43 visual_prompt]: 	Training 500/553. train loss: 2.0830,	0.8284 s / batch. (data: 3.40e-04). ETA=7:23:34, max mem: 20.9 GB 
[11/30 12:47:42 visual_prompt]: Epoch 42 / 100: avg data time: 3.11e-01, avg batch time: 1.1445, average train loss: 2.3765
[11/30 12:48:49 visual_prompt]: Inference (val):avg data time: 6.40e-05, avg batch time: 0.3111, average loss: 2.6654
[11/30 12:48:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.41	
[11/30 12:48:49 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[11/30 12:50:50 visual_prompt]: 	Training 100/553. train loss: 0.6919,	0.8449 s / batch. (data: 5.64e-03). ETA=7:30:14, max mem: 20.9 GB 
[11/30 12:52:44 visual_prompt]: 	Training 200/553. train loss: 4.3715,	0.8310 s / batch. (data: 3.70e-04). ETA=7:21:27, max mem: 20.9 GB 
[11/30 12:54:37 visual_prompt]: 	Training 300/553. train loss: 3.3153,	0.8385 s / batch. (data: 7.03e-03). ETA=7:24:03, max mem: 20.9 GB 
[11/30 12:56:32 visual_prompt]: 	Training 400/553. train loss: 0.5495,	0.8400 s / batch. (data: 3.23e-04). ETA=7:23:25, max mem: 20.9 GB 
[11/30 12:58:32 visual_prompt]: 	Training 500/553. train loss: 2.3719,	0.8875 s / batch. (data: 3.82e-03). ETA=7:47:03, max mem: 20.9 GB 
[11/30 12:59:36 visual_prompt]: Epoch 43 / 100: avg data time: 3.34e-01, avg batch time: 1.1696, average train loss: 2.1711
[11/30 13:00:46 visual_prompt]: Inference (val):avg data time: 8.96e-05, avg batch time: 0.3108, average loss: 1.0611
[11/30 13:00:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.77	
[11/30 13:00:46 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[11/30 13:02:53 visual_prompt]: 	Training 100/553. train loss: 0.6536,	1.2363 s / batch. (data: 4.05e-01). ETA=10:47:25, max mem: 20.9 GB 
[11/30 13:04:57 visual_prompt]: 	Training 200/553. train loss: 1.2334,	0.8401 s / batch. (data: 1.30e-03). ETA=7:18:32, max mem: 20.9 GB 
[11/30 13:06:52 visual_prompt]: 	Training 300/553. train loss: 0.8066,	0.8611 s / batch. (data: 5.45e-03). ETA=7:28:04, max mem: 20.9 GB 
[11/30 13:08:47 visual_prompt]: 	Training 400/553. train loss: 1.1292,	0.8636 s / batch. (data: 2.60e-02). ETA=7:27:55, max mem: 20.9 GB 
[11/30 13:10:40 visual_prompt]: 	Training 500/553. train loss: 1.5931,	0.9390 s / batch. (data: 1.15e-01). ETA=8:05:28, max mem: 20.9 GB 
[11/30 13:11:41 visual_prompt]: Epoch 44 / 100: avg data time: 3.47e-01, avg batch time: 1.1836, average train loss: 2.0941
[11/30 13:12:47 visual_prompt]: Inference (val):avg data time: 6.65e-05, avg batch time: 0.3081, average loss: 3.0344
[11/30 13:12:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.45	
[11/30 13:12:47 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[11/30 13:14:47 visual_prompt]: 	Training 100/553. train loss: 2.6431,	0.8623 s / batch. (data: 8.47e-04). ETA=7:23:37, max mem: 20.9 GB 
[11/30 13:16:38 visual_prompt]: 	Training 200/553. train loss: 0.6821,	0.8483 s / batch. (data: 1.20e-02). ETA=7:15:01, max mem: 20.9 GB 
[11/30 13:18:33 visual_prompt]: 	Training 300/553. train loss: 6.4774,	0.8551 s / batch. (data: 5.46e-03). ETA=7:17:03, max mem: 20.9 GB 
[11/30 13:20:25 visual_prompt]: 	Training 400/553. train loss: 2.2171,	0.8295 s / batch. (data: 7.83e-03). ETA=7:02:35, max mem: 20.9 GB 
[11/30 13:22:22 visual_prompt]: 	Training 500/553. train loss: 0.6980,	0.8160 s / batch. (data: 3.47e-04). ETA=6:54:20, max mem: 20.9 GB 
[11/30 13:23:21 visual_prompt]: Epoch 45 / 100: avg data time: 3.13e-01, avg batch time: 1.1469, average train loss: 1.9767
[11/30 13:24:29 visual_prompt]: Inference (val):avg data time: 7.75e-05, avg batch time: 0.3104, average loss: 1.0924
[11/30 13:24:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.41	
[11/30 13:24:29 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[11/30 13:26:30 visual_prompt]: 	Training 100/553. train loss: 3.2458,	1.6893 s / batch. (data: 8.30e-01). ETA=14:13:30, max mem: 20.9 GB 
[11/30 13:28:27 visual_prompt]: 	Training 200/553. train loss: 2.2857,	0.8418 s / batch. (data: 1.94e-03). ETA=7:03:55, max mem: 20.9 GB 
[11/30 13:30:21 visual_prompt]: 	Training 300/553. train loss: 1.5727,	0.8294 s / batch. (data: 1.16e-02). ETA=6:56:17, max mem: 20.9 GB 
[11/30 13:32:16 visual_prompt]: 	Training 400/553. train loss: 0.7820,	0.8441 s / batch. (data: 6.45e-03). ETA=7:02:14, max mem: 20.9 GB 
[11/30 13:34:08 visual_prompt]: 	Training 500/553. train loss: 5.5864,	0.8185 s / batch. (data: 1.36e-03). ETA=6:48:05, max mem: 20.9 GB 
[11/30 13:35:08 visual_prompt]: Epoch 46 / 100: avg data time: 3.21e-01, avg batch time: 1.1549, average train loss: 1.7843
[11/30 13:36:16 visual_prompt]: Inference (val):avg data time: 7.21e-05, avg batch time: 0.3093, average loss: 1.3203
[11/30 13:36:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[11/30 13:36:16 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[11/30 13:38:19 visual_prompt]: 	Training 100/553. train loss: 3.5818,	0.8549 s / batch. (data: 3.41e-02). ETA=7:04:04, max mem: 20.9 GB 
[11/30 13:40:10 visual_prompt]: 	Training 200/553. train loss: 0.7208,	1.6160 s / batch. (data: 7.46e-01). ETA=13:18:54, max mem: 20.9 GB 
[11/30 13:42:11 visual_prompt]: 	Training 300/553. train loss: 3.1863,	0.8415 s / batch. (data: 8.52e-04). ETA=6:54:37, max mem: 20.9 GB 
[11/30 13:44:12 visual_prompt]: 	Training 400/553. train loss: 0.4958,	0.8683 s / batch. (data: 6.61e-03). ETA=7:06:23, max mem: 20.9 GB 
[11/30 13:46:12 visual_prompt]: 	Training 500/553. train loss: 1.8249,	0.8336 s / batch. (data: 5.60e-03). ETA=6:47:55, max mem: 20.9 GB 
[11/30 13:47:15 visual_prompt]: Epoch 47 / 100: avg data time: 3.56e-01, avg batch time: 1.1918, average train loss: 2.1250
[11/30 13:48:28 visual_prompt]: Inference (val):avg data time: 2.40e-04, avg batch time: 0.3119, average loss: 0.7435
[11/30 13:48:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.28	
[11/30 13:48:28 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[11/30 13:50:34 visual_prompt]: 	Training 100/553. train loss: 0.9621,	0.8442 s / batch. (data: 7.69e-04). ETA=6:50:57, max mem: 20.9 GB 
[11/30 13:52:37 visual_prompt]: 	Training 200/553. train loss: 0.7486,	0.8718 s / batch. (data: 1.18e-02). ETA=7:02:56, max mem: 20.9 GB 
[11/30 13:54:41 visual_prompt]: 	Training 300/553. train loss: 2.1024,	2.7479 s / batch. (data: 1.93e+00). ETA=22:08:32, max mem: 20.9 GB 
[11/30 13:56:39 visual_prompt]: 	Training 400/553. train loss: 0.3197,	1.0652 s / batch. (data: 2.39e-01). ETA=8:33:14, max mem: 20.9 GB 
[11/30 13:58:40 visual_prompt]: 	Training 500/553. train loss: 2.1887,	0.8274 s / batch. (data: 1.17e-03). ETA=6:37:17, max mem: 20.9 GB 
[11/30 13:59:41 visual_prompt]: Epoch 48 / 100: avg data time: 3.79e-01, avg batch time: 1.2176, average train loss: 1.6458
[11/30 14:00:51 visual_prompt]: Inference (val):avg data time: 8.50e-05, avg batch time: 0.3115, average loss: 3.0819
[11/30 14:00:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.79	
[11/30 14:00:51 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[11/30 14:03:02 visual_prompt]: 	Training 100/553. train loss: 1.1156,	0.8559 s / batch. (data: 3.14e-03). ETA=6:48:46, max mem: 20.9 GB 
[11/30 14:04:59 visual_prompt]: 	Training 200/553. train loss: 3.1905,	0.8601 s / batch. (data: 6.28e-03). ETA=6:49:19, max mem: 20.9 GB 
[11/30 14:07:00 visual_prompt]: 	Training 300/553. train loss: 3.1327,	1.0960 s / batch. (data: 2.73e-01). ETA=8:39:47, max mem: 20.9 GB 
[11/30 14:09:02 visual_prompt]: 	Training 400/553. train loss: 1.0822,	0.8260 s / batch. (data: 1.14e-03). ETA=6:30:21, max mem: 20.9 GB 
[11/30 14:11:06 visual_prompt]: 	Training 500/553. train loss: 0.9880,	0.8781 s / batch. (data: 2.38e-02). ETA=6:53:31, max mem: 20.9 GB 
[11/30 14:12:08 visual_prompt]: Epoch 49 / 100: avg data time: 3.86e-01, avg batch time: 1.2247, average train loss: 1.7943
[11/30 14:13:22 visual_prompt]: Inference (val):avg data time: 4.56e-04, avg batch time: 0.3168, average loss: 1.5640
[11/30 14:13:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.78	
[11/30 14:13:22 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[11/30 14:15:31 visual_prompt]: 	Training 100/553. train loss: 1.3883,	0.8360 s / batch. (data: 3.23e-04). ETA=6:31:34, max mem: 20.9 GB 
[11/30 14:17:34 visual_prompt]: 	Training 200/553. train loss: 2.9734,	0.8917 s / batch. (data: 2.37e-02). ETA=6:56:11, max mem: 20.9 GB 
[11/30 14:19:34 visual_prompt]: 	Training 300/553. train loss: 0.7607,	0.8585 s / batch. (data: 2.55e-03). ETA=6:39:13, max mem: 20.9 GB 
[11/30 14:21:33 visual_prompt]: 	Training 400/553. train loss: 0.0439,	0.8805 s / batch. (data: 1.50e-03). ETA=6:47:59, max mem: 20.9 GB 
[11/30 14:23:34 visual_prompt]: 	Training 500/553. train loss: 2.9308,	0.8459 s / batch. (data: 1.48e-03). ETA=6:30:33, max mem: 20.9 GB 
[11/30 14:24:37 visual_prompt]: Epoch 50 / 100: avg data time: 3.80e-01, avg batch time: 1.2209, average train loss: 1.6563
[11/30 14:25:46 visual_prompt]: Inference (val):avg data time: 6.36e-05, avg batch time: 0.3119, average loss: 2.6452
[11/30 14:25:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.17	
[11/30 14:25:46 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[11/30 14:27:48 visual_prompt]: 	Training 100/553. train loss: 1.0896,	1.8958 s / batch. (data: 1.05e+00). ETA=14:30:29, max mem: 20.9 GB 
[11/30 14:29:50 visual_prompt]: 	Training 200/553. train loss: 3.8811,	0.8469 s / batch. (data: 6.11e-03). ETA=6:27:28, max mem: 20.9 GB 
[11/30 14:31:50 visual_prompt]: 	Training 300/553. train loss: 1.1684,	0.8306 s / batch. (data: 1.09e-03). ETA=6:18:37, max mem: 20.9 GB 
[11/30 14:33:54 visual_prompt]: 	Training 400/553. train loss: 0.5511,	2.3760 s / batch. (data: 1.54e+00). ETA=17:59:05, max mem: 20.9 GB 
[11/30 14:35:51 visual_prompt]: 	Training 500/553. train loss: 1.8281,	0.8543 s / batch. (data: 9.04e-04). ETA=6:26:35, max mem: 20.9 GB 
[11/30 14:36:51 visual_prompt]: Epoch 51 / 100: avg data time: 3.64e-01, avg batch time: 1.2030, average train loss: 1.4512
[11/30 14:38:01 visual_prompt]: Inference (val):avg data time: 8.32e-05, avg batch time: 0.3098, average loss: 1.2400
[11/30 14:38:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.66	
[11/30 14:38:01 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[11/30 14:40:10 visual_prompt]: 	Training 100/553. train loss: 3.5407,	0.8637 s / batch. (data: 1.12e-02). ETA=6:28:37, max mem: 20.9 GB 
[11/30 14:42:10 visual_prompt]: 	Training 200/553. train loss: 1.2539,	0.8539 s / batch. (data: 3.59e-04). ETA=6:22:48, max mem: 20.9 GB 
[11/30 14:44:11 visual_prompt]: 	Training 300/553. train loss: 1.7346,	0.8639 s / batch. (data: 1.18e-02). ETA=6:25:49, max mem: 20.9 GB 
[11/30 14:46:11 visual_prompt]: 	Training 400/553. train loss: 0.0008,	0.8504 s / batch. (data: 2.28e-02). ETA=6:18:23, max mem: 20.9 GB 
[11/30 14:48:01 visual_prompt]: 	Training 500/553. train loss: 2.5077,	0.8566 s / batch. (data: 3.47e-04). ETA=6:19:42, max mem: 20.9 GB 
[11/30 14:49:00 visual_prompt]: Epoch 52 / 100: avg data time: 3.53e-01, avg batch time: 1.1906, average train loss: 1.8658
[11/30 14:50:06 visual_prompt]: Inference (val):avg data time: 6.83e-05, avg batch time: 0.3115, average loss: 5.0050
[11/30 14:50:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.26	
[11/30 14:50:06 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[11/30 14:52:04 visual_prompt]: 	Training 100/553. train loss: 0.7898,	0.8328 s / batch. (data: 1.01e-02). ETA=6:07:01, max mem: 20.9 GB 
[11/30 14:53:57 visual_prompt]: 	Training 200/553. train loss: 1.5885,	0.8384 s / batch. (data: 5.49e-03). ETA=6:08:06, max mem: 20.9 GB 
[11/30 14:55:50 visual_prompt]: 	Training 300/553. train loss: 1.4154,	0.8519 s / batch. (data: 1.22e-03). ETA=6:12:36, max mem: 20.9 GB 
[11/30 14:57:47 visual_prompt]: 	Training 400/553. train loss: 1.0066,	1.2679 s / batch. (data: 4.33e-01). ETA=9:12:28, max mem: 20.9 GB 
[11/30 14:59:39 visual_prompt]: 	Training 500/553. train loss: 2.0576,	0.8730 s / batch. (data: 1.29e-02). ETA=6:18:55, max mem: 20.9 GB 
[11/30 15:00:40 visual_prompt]: Epoch 53 / 100: avg data time: 3.13e-01, avg batch time: 1.1464, average train loss: 1.5731
[11/30 15:01:48 visual_prompt]: Inference (val):avg data time: 2.54e-04, avg batch time: 0.3101, average loss: 5.4345
[11/30 15:01:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.49	
[11/30 15:01:48 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[11/30 15:03:50 visual_prompt]: 	Training 100/553. train loss: 1.1204,	0.8405 s / batch. (data: 1.28e-03). ETA=6:02:40, max mem: 20.9 GB 
[11/30 15:05:47 visual_prompt]: 	Training 200/553. train loss: 0.6302,	0.8726 s / batch. (data: 4.12e-02). ETA=6:15:04, max mem: 20.9 GB 
[11/30 15:07:38 visual_prompt]: 	Training 300/553. train loss: 3.3439,	1.2163 s / batch. (data: 3.86e-01). ETA=8:40:47, max mem: 20.9 GB 
[11/30 15:09:33 visual_prompt]: 	Training 400/553. train loss: 4.6110,	0.8314 s / batch. (data: 7.87e-04). ETA=5:54:37, max mem: 20.9 GB 
[11/30 15:11:27 visual_prompt]: 	Training 500/553. train loss: 0.5944,	0.8380 s / batch. (data: 3.17e-04). ETA=5:56:00, max mem: 20.9 GB 
[11/30 15:12:26 visual_prompt]: Epoch 54 / 100: avg data time: 3.20e-01, avg batch time: 1.1547, average train loss: 1.5220
[11/30 15:13:34 visual_prompt]: Inference (val):avg data time: 5.95e-04, avg batch time: 0.3109, average loss: 3.7185
[11/30 15:13:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.83	
[11/30 15:13:34 visual_prompt]: Stopping early.
[11/30 15:13:34 visual_prompt]: Rank of current process: 0. World size: 1
[11/30 15:13:34 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/30 15:13:34 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/30 15:13:34 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/30 15:13:34 visual_prompt]: Training with config:
[11/30 15:13:34 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr1.0_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/30 15:13:34 visual_prompt]: Loading training data...
[11/30 15:13:34 visual_prompt]: Constructing mammo-cbis dataset train...
[11/30 15:13:34 visual_prompt]: Loading validation data...
[11/30 15:13:34 visual_prompt]: Constructing mammo-cbis dataset val...
[11/30 15:13:34 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/30 15:13:39 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/30 15:13:39 visual_prompt]: tuned percent:0.525
[11/30 15:13:39 visual_prompt]: Device used for model: 0
[11/30 15:13:39 visual_prompt]: Setting up Evaluator...
[11/30 15:13:39 visual_prompt]: Setting up Trainer...
[11/30 15:13:39 visual_prompt]: 	Setting up the optimizer...
[11/30 15:13:39 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/30 15:15:37 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8324 s / batch. (data: 8.05e-03). ETA=12:45:46, max mem: 20.9 GB 
[11/30 15:17:31 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8299 s / batch. (data: 6.67e-04). ETA=12:42:09, max mem: 20.9 GB 
[11/30 15:19:28 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.1629 s / batch. (data: 1.34e+00). ETA=1 day, 9:02:39, max mem: 20.9 GB 
[11/30 15:21:20 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8361 s / batch. (data: 9.61e-04). ETA=12:44:59, max mem: 20.9 GB 
[11/30 15:23:18 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8510 s / batch. (data: 1.10e-02). ETA=12:57:15, max mem: 20.9 GB 
[11/30 15:24:18 visual_prompt]: Epoch 1 / 100: avg data time: 3.20e-01, avg batch time: 1.1553, average train loss: 1.5403
[11/30 15:25:26 visual_prompt]: Inference (val):avg data time: 7.29e-05, avg batch time: 0.3111, average loss: 1.5201
[11/30 15:25:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/30 15:25:26 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[11/30 15:27:25 visual_prompt]: 	Training 100/553. train loss: 0.7431,	1.1080 s / batch. (data: 2.88e-01). ETA=16:49:08, max mem: 20.9 GB 
[11/30 15:29:20 visual_prompt]: 	Training 200/553. train loss: 0.0229,	0.8569 s / batch. (data: 1.21e-02). ETA=12:59:02, max mem: 20.9 GB 
[11/30 15:31:17 visual_prompt]: 	Training 300/553. train loss: 0.7115,	1.5125 s / batch. (data: 6.64e-01). ETA=22:52:31, max mem: 20.9 GB 
[11/30 15:33:10 visual_prompt]: 	Training 400/553. train loss: 1.0806,	0.8444 s / batch. (data: 5.97e-04). ETA=12:44:50, max mem: 20.9 GB 
[11/30 15:35:13 visual_prompt]: 	Training 500/553. train loss: 0.6006,	0.8406 s / batch. (data: 1.08e-02). ETA=12:40:01, max mem: 20.9 GB 
[11/30 15:36:13 visual_prompt]: Epoch 2 / 100: avg data time: 3.35e-01, avg batch time: 1.1704, average train loss: 0.9692
[11/30 15:37:21 visual_prompt]: Inference (val):avg data time: 8.71e-05, avg batch time: 0.3102, average loss: 1.2669
[11/30 15:37:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.96	
[11/30 15:37:21 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[11/30 15:39:24 visual_prompt]: 	Training 100/553. train loss: 1.2298,	0.9083 s / batch. (data: 7.53e-02). ETA=13:38:53, max mem: 20.9 GB 
[11/30 15:41:25 visual_prompt]: 	Training 200/553. train loss: 0.9757,	1.9667 s / batch. (data: 1.13e+00). ETA=1 day, 5:29:50, max mem: 20.9 GB 
[11/30 15:43:25 visual_prompt]: 	Training 300/553. train loss: 0.7386,	0.8520 s / batch. (data: 3.64e-04). ETA=12:45:18, max mem: 20.9 GB 
[11/30 15:45:38 visual_prompt]: 	Training 400/553. train loss: 3.2934,	0.8752 s / batch. (data: 1.47e-02). ETA=13:04:39, max mem: 20.9 GB 
[11/30 15:47:54 visual_prompt]: 	Training 500/553. train loss: 0.6973,	2.5906 s / batch. (data: 1.73e+00). ETA=1 day, 14:38:19, max mem: 20.9 GB 
[11/30 15:49:04 visual_prompt]: Epoch 3 / 100: avg data time: 4.21e-01, avg batch time: 1.2707, average train loss: 0.9728
[11/30 15:50:23 visual_prompt]: Inference (val):avg data time: 3.09e-04, avg batch time: 0.3280, average loss: 0.8207
[11/30 15:50:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.52	
[11/30 15:50:23 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[11/30 15:52:49 visual_prompt]: 	Training 100/553. train loss: 0.7498,	0.8701 s / batch. (data: 2.22e-02). ETA=12:56:26, max mem: 20.9 GB 
[11/30 15:54:58 visual_prompt]: 	Training 200/553. train loss: 0.6275,	0.8820 s / batch. (data: 2.16e-03). ETA=13:05:34, max mem: 20.9 GB 
[11/30 15:57:00 visual_prompt]: 	Training 300/553. train loss: 0.6057,	1.6892 s / batch. (data: 8.61e-01). ETA=1 day, 1:01:43, max mem: 20.9 GB 
[11/30 15:58:56 visual_prompt]: 	Training 400/553. train loss: 0.5973,	2.2792 s / batch. (data: 1.37e+00). ETA=1 day, 9:42:24, max mem: 20.9 GB 
[11/30 16:00:58 visual_prompt]: 	Training 500/553. train loss: 0.3909,	4.7560 s / batch. (data: 3.93e+00). ETA=2 days, 22:12:17, max mem: 20.9 GB 
[11/30 16:02:00 visual_prompt]: Epoch 4 / 100: avg data time: 4.14e-01, avg batch time: 1.2603, average train loss: 1.0162
[11/30 16:03:13 visual_prompt]: Inference (val):avg data time: 1.21e-04, avg batch time: 0.3148, average loss: 0.6778
[11/30 16:03:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 58.80	
[11/30 16:03:13 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[11/30 16:05:15 visual_prompt]: 	Training 100/553. train loss: 3.2099,	0.8396 s / batch. (data: 4.69e-04). ETA=12:21:30, max mem: 20.9 GB 
[11/30 16:07:13 visual_prompt]: 	Training 200/553. train loss: 0.9763,	0.9280 s / batch. (data: 7.55e-02). ETA=13:37:58, max mem: 20.9 GB 
[11/30 16:09:17 visual_prompt]: 	Training 300/553. train loss: 2.7767,	0.8200 s / batch. (data: 4.91e-04). ETA=12:01:27, max mem: 20.9 GB 
[11/30 16:11:15 visual_prompt]: 	Training 400/553. train loss: 2.3731,	0.8523 s / batch. (data: 5.61e-03). ETA=12:28:25, max mem: 20.9 GB 
[11/30 16:13:15 visual_prompt]: 	Training 500/553. train loss: 0.6466,	0.8280 s / batch. (data: 1.39e-03). ETA=12:05:40, max mem: 20.9 GB 
[11/30 16:14:18 visual_prompt]: Epoch 5 / 100: avg data time: 3.65e-01, avg batch time: 1.2029, average train loss: 1.1417
[11/30 16:15:33 visual_prompt]: Inference (val):avg data time: 1.45e-04, avg batch time: 0.3158, average loss: 2.2109
[11/30 16:15:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.25	
[11/30 16:15:33 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[11/30 16:17:48 visual_prompt]: 	Training 100/553. train loss: 1.4888,	0.8142 s / batch. (data: 4.78e-04). ETA=11:51:31, max mem: 20.9 GB 
[11/30 16:19:54 visual_prompt]: 	Training 200/553. train loss: 2.5385,	0.8662 s / batch. (data: 1.36e-02). ETA=12:35:34, max mem: 20.9 GB 
[11/30 16:21:58 visual_prompt]: 	Training 300/553. train loss: 2.6572,	0.8353 s / batch. (data: 3.47e-04). ETA=12:07:14, max mem: 20.9 GB 
[11/30 16:24:08 visual_prompt]: 	Training 400/553. train loss: 2.0656,	0.8755 s / batch. (data: 6.98e-03). ETA=12:40:42, max mem: 20.9 GB 
[11/30 16:26:11 visual_prompt]: 	Training 500/553. train loss: 2.2061,	1.5889 s / batch. (data: 7.60e-01). ETA=22:58:00, max mem: 20.9 GB 
[11/30 16:27:13 visual_prompt]: Epoch 6 / 100: avg data time: 4.18e-01, avg batch time: 1.2642, average train loss: 1.4448
[11/30 16:28:29 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3165, average loss: 0.7829
[11/30 16:28:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.25	
[11/30 16:28:29 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[11/30 16:30:39 visual_prompt]: 	Training 100/553. train loss: 2.1696,	0.8320 s / batch. (data: 1.19e-02). ETA=11:59:26, max mem: 20.9 GB 
[11/30 16:32:47 visual_prompt]: 	Training 200/553. train loss: 0.5902,	0.8681 s / batch. (data: 2.77e-03). ETA=12:29:10, max mem: 20.9 GB 
[11/30 16:34:55 visual_prompt]: 	Training 300/553. train loss: 1.1093,	2.6999 s / batch. (data: 1.87e+00). ETA=1 day, 14:45:37, max mem: 20.9 GB 
[11/30 16:36:56 visual_prompt]: 	Training 400/553. train loss: 1.1678,	2.6679 s / batch. (data: 1.82e+00). ETA=1 day, 14:13:37, max mem: 20.9 GB 
[11/30 16:38:54 visual_prompt]: 	Training 500/553. train loss: 0.6711,	2.1020 s / batch. (data: 1.26e+00). ETA=1 day, 6:03:34, max mem: 20.9 GB 
[11/30 16:39:55 visual_prompt]: Epoch 7 / 100: avg data time: 3.96e-01, avg batch time: 1.2403, average train loss: 1.4688
[11/30 16:41:07 visual_prompt]: Inference (val):avg data time: 2.63e-04, avg batch time: 0.3114, average loss: 0.9110
[11/30 16:41:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.41	
[11/30 16:41:07 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[11/30 16:43:11 visual_prompt]: 	Training 100/553. train loss: 3.7135,	0.8280 s / batch. (data: 5.38e-04). ETA=11:48:19, max mem: 20.9 GB 
[11/30 16:45:15 visual_prompt]: 	Training 200/553. train loss: 0.6631,	0.8297 s / batch. (data: 1.74e-03). ETA=11:48:24, max mem: 20.9 GB 
[11/30 16:47:19 visual_prompt]: 	Training 300/553. train loss: 1.7093,	0.8255 s / batch. (data: 4.89e-04). ETA=11:43:25, max mem: 20.9 GB 
[11/30 16:49:21 visual_prompt]: 	Training 400/553. train loss: 3.7947,	1.6253 s / batch. (data: 8.05e-01). ETA=23:02:19, max mem: 20.9 GB 
[11/30 16:51:22 visual_prompt]: 	Training 500/553. train loss: 4.6051,	2.1709 s / batch. (data: 1.34e+00). ETA=1 day, 6:42:44, max mem: 20.9 GB 
[11/30 16:52:25 visual_prompt]: Epoch 8 / 100: avg data time: 3.83e-01, avg batch time: 1.2255, average train loss: 1.9594
[11/30 16:53:35 visual_prompt]: Inference (val):avg data time: 1.15e-04, avg batch time: 0.3114, average loss: 0.8549
[11/30 16:53:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[11/30 16:53:35 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[11/30 16:55:42 visual_prompt]: 	Training 100/553. train loss: 0.9613,	0.8232 s / batch. (data: 6.29e-04). ETA=11:36:39, max mem: 20.9 GB 
[11/30 16:57:43 visual_prompt]: 	Training 200/553. train loss: 0.5460,	0.8586 s / batch. (data: 7.93e-03). ETA=12:05:08, max mem: 20.9 GB 
[11/30 16:59:42 visual_prompt]: 	Training 300/553. train loss: 0.7530,	2.0405 s / batch. (data: 1.17e+00). ETA=1 day, 4:40:00, max mem: 20.9 GB 
[11/30 17:01:48 visual_prompt]: 	Training 400/553. train loss: 0.6412,	0.8850 s / batch. (data: 1.15e-02). ETA=12:24:29, max mem: 20.9 GB 
[11/30 17:03:50 visual_prompt]: 	Training 500/553. train loss: 0.6179,	1.7787 s / batch. (data: 9.42e-01). ETA=1 day, 0:53:23, max mem: 20.9 GB 
[11/30 17:04:50 visual_prompt]: Epoch 9 / 100: avg data time: 3.79e-01, avg batch time: 1.2188, average train loss: 1.6057
[11/30 17:06:01 visual_prompt]: Inference (val):avg data time: 1.19e-04, avg batch time: 0.3130, average loss: 0.8593
[11/30 17:06:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.55	
[11/30 17:06:01 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[11/30 17:08:09 visual_prompt]: 	Training 100/553. train loss: 4.1423,	0.8675 s / batch. (data: 1.16e-02). ETA=12:06:06, max mem: 20.9 GB 
[11/30 17:10:08 visual_prompt]: 	Training 200/553. train loss: 0.9189,	0.8478 s / batch. (data: 1.17e-02). ETA=11:48:16, max mem: 20.9 GB 
[11/30 17:12:08 visual_prompt]: 	Training 300/553. train loss: 2.8276,	0.8397 s / batch. (data: 4.13e-03). ETA=11:40:06, max mem: 20.9 GB 
[11/30 17:14:06 visual_prompt]: 	Training 400/553. train loss: 1.9134,	1.5610 s / batch. (data: 7.17e-01). ETA=21:38:51, max mem: 20.9 GB 
[11/30 17:16:13 visual_prompt]: 	Training 500/553. train loss: 0.9033,	1.6684 s / batch. (data: 8.45e-01). ETA=23:05:26, max mem: 20.9 GB 
[11/30 17:17:17 visual_prompt]: Epoch 10 / 100: avg data time: 3.82e-01, avg batch time: 1.2228, average train loss: 2.1337
[11/30 17:18:29 visual_prompt]: Inference (val):avg data time: 1.07e-04, avg batch time: 0.3137, average loss: 1.3788
[11/30 17:18:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.90	
[11/30 17:18:29 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[11/30 17:20:42 visual_prompt]: 	Training 100/553. train loss: 2.5256,	0.8612 s / batch. (data: 2.90e-02). ETA=11:52:55, max mem: 20.9 GB 
[11/30 17:22:47 visual_prompt]: 	Training 200/553. train loss: 1.4719,	0.8447 s / batch. (data: 9.98e-04). ETA=11:37:50, max mem: 20.9 GB 
[11/30 17:24:49 visual_prompt]: 	Training 300/553. train loss: 0.0326,	1.4812 s / batch. (data: 6.64e-01). ETA=20:21:15, max mem: 20.9 GB 
[11/30 17:26:51 visual_prompt]: 	Training 400/553. train loss: 1.0791,	0.9363 s / batch. (data: 2.23e-02). ETA=12:50:26, max mem: 20.9 GB 
[11/30 17:28:54 visual_prompt]: 	Training 500/553. train loss: 2.7138,	0.8879 s / batch. (data: 1.17e-02). ETA=12:09:05, max mem: 20.9 GB 
[11/30 17:29:59 visual_prompt]: Epoch 11 / 100: avg data time: 4.03e-01, avg batch time: 1.2473, average train loss: 2.4819
[11/30 17:31:13 visual_prompt]: Inference (val):avg data time: 1.43e-04, avg batch time: 0.3134, average loss: 1.0858
[11/30 17:31:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.37	
[11/30 17:31:13 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[11/30 17:33:22 visual_prompt]: 	Training 100/553. train loss: 0.6847,	1.4476 s / batch. (data: 6.12e-01). ETA=19:45:02, max mem: 20.9 GB 
[11/30 17:35:26 visual_prompt]: 	Training 200/553. train loss: 0.5447,	0.8320 s / batch. (data: 1.75e-03). ETA=11:19:41, max mem: 20.9 GB 
[11/30 17:37:26 visual_prompt]: 	Training 300/553. train loss: 1.9428,	0.8472 s / batch. (data: 1.57e-03). ETA=11:30:40, max mem: 20.9 GB 
[11/30 17:39:31 visual_prompt]: 	Training 400/553. train loss: 1.3645,	0.8865 s / batch. (data: 1.80e-03). ETA=12:01:14, max mem: 20.9 GB 
[11/30 17:41:34 visual_prompt]: 	Training 500/553. train loss: 5.1667,	0.8683 s / batch. (data: 1.65e-02). ETA=11:45:00, max mem: 20.9 GB 
[11/30 17:42:36 visual_prompt]: Epoch 12 / 100: avg data time: 3.92e-01, avg batch time: 1.2346, average train loss: 1.8395
[11/30 17:43:49 visual_prompt]: Inference (val):avg data time: 1.27e-04, avg batch time: 0.3123, average loss: 1.0636
[11/30 17:43:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.40	
[11/30 17:43:49 visual_prompt]: Best epoch 12: best metric: -1.064
[11/30 17:43:49 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[11/30 17:46:03 visual_prompt]: 	Training 100/553. train loss: 0.5006,	0.8366 s / batch. (data: 5.02e-04). ETA=11:17:07, max mem: 20.9 GB 
[11/30 17:48:06 visual_prompt]: 	Training 200/553. train loss: 0.7680,	0.8518 s / batch. (data: 1.09e-02). ETA=11:28:02, max mem: 20.9 GB 
[11/30 17:50:11 visual_prompt]: 	Training 300/553. train loss: 0.7476,	2.5274 s / batch. (data: 1.68e+00). ETA=1 day, 9:57:16, max mem: 20.9 GB 
[11/30 17:52:11 visual_prompt]: 	Training 400/553. train loss: 1.3625,	0.8720 s / batch. (data: 5.67e-03). ETA=11:41:28, max mem: 20.9 GB 
[11/30 17:54:13 visual_prompt]: 	Training 500/553. train loss: 3.0766,	0.8638 s / batch. (data: 1.73e-03). ETA=11:33:24, max mem: 20.9 GB 
[11/30 17:55:18 visual_prompt]: Epoch 13 / 100: avg data time: 4.02e-01, avg batch time: 1.2460, average train loss: 2.0376
[11/30 17:56:33 visual_prompt]: Inference (val):avg data time: 3.58e-04, avg batch time: 0.3144, average loss: 0.8844
[11/30 17:56:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.78	
[11/30 17:56:33 visual_prompt]: Best epoch 13: best metric: -0.884
[11/30 17:56:33 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[11/30 17:58:40 visual_prompt]: 	Training 100/553. train loss: 8.4550,	0.8314 s / batch. (data: 1.64e-03). ETA=11:05:16, max mem: 20.9 GB 
[11/30 18:00:40 visual_prompt]: 	Training 200/553. train loss: 0.1337,	1.2693 s / batch. (data: 4.16e-01). ETA=16:53:33, max mem: 20.9 GB 
[11/30 18:02:40 visual_prompt]: 	Training 300/553. train loss: 0.7719,	1.3597 s / batch. (data: 5.27e-01). ETA=18:03:30, max mem: 20.9 GB 
[11/30 18:04:39 visual_prompt]: 	Training 400/553. train loss: 0.6683,	0.8516 s / batch. (data: 1.17e-02). ETA=11:17:10, max mem: 20.9 GB 
[11/30 18:06:37 visual_prompt]: 	Training 500/553. train loss: 3.9133,	0.8626 s / batch. (data: 1.08e-02). ETA=11:24:30, max mem: 20.9 GB 
[11/30 18:07:40 visual_prompt]: Epoch 14 / 100: avg data time: 3.65e-01, avg batch time: 1.2053, average train loss: 1.7534
[11/30 18:08:49 visual_prompt]: Inference (val):avg data time: 9.21e-05, avg batch time: 0.3120, average loss: 1.6784
[11/30 18:08:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.75	
[11/30 18:08:49 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[11/30 18:10:55 visual_prompt]: 	Training 100/553. train loss: 0.9750,	0.8361 s / batch. (data: 4.67e-04). ETA=11:01:18, max mem: 20.9 GB 
[11/30 18:12:52 visual_prompt]: 	Training 200/553. train loss: 12.8571,	0.8297 s / batch. (data: 4.99e-04). ETA=10:54:52, max mem: 20.9 GB 
[11/30 18:14:53 visual_prompt]: 	Training 300/553. train loss: 1.9245,	0.9006 s / batch. (data: 6.98e-03). ETA=11:49:20, max mem: 20.9 GB 
[11/30 18:16:53 visual_prompt]: 	Training 400/553. train loss: 0.6197,	0.8815 s / batch. (data: 1.97e-03). ETA=11:32:49, max mem: 20.9 GB 
[11/30 18:18:53 visual_prompt]: 	Training 500/553. train loss: 1.6630,	0.8676 s / batch. (data: 1.19e-02). ETA=11:20:25, max mem: 20.9 GB 
[11/30 18:19:58 visual_prompt]: Epoch 15 / 100: avg data time: 3.67e-01, avg batch time: 1.2083, average train loss: 2.6104
[11/30 18:21:10 visual_prompt]: Inference (val):avg data time: 1.25e-04, avg batch time: 0.3125, average loss: 2.2748
[11/30 18:21:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.09	
[11/30 18:21:10 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[11/30 18:23:15 visual_prompt]: 	Training 100/553. train loss: 0.5710,	0.8610 s / batch. (data: 5.74e-04). ETA=11:13:04, max mem: 20.9 GB 
[11/30 18:25:18 visual_prompt]: 	Training 200/553. train loss: 2.6514,	0.8315 s / batch. (data: 1.34e-03). ETA=10:48:36, max mem: 20.9 GB 
[11/30 18:27:16 visual_prompt]: 	Training 300/553. train loss: 5.6168,	0.8480 s / batch. (data: 1.28e-03). ETA=11:00:06, max mem: 20.9 GB 
[11/30 18:29:15 visual_prompt]: 	Training 400/553. train loss: 4.9681,	0.8634 s / batch. (data: 1.72e-03). ETA=11:10:38, max mem: 20.9 GB 
[11/30 18:31:15 visual_prompt]: 	Training 500/553. train loss: 0.8915,	1.6966 s / batch. (data: 8.49e-01). ETA=21:55:02, max mem: 20.9 GB 
[11/30 18:32:18 visual_prompt]: Epoch 16 / 100: avg data time: 3.67e-01, avg batch time: 1.2077, average train loss: 2.6295
[11/30 18:33:27 visual_prompt]: Inference (val):avg data time: 2.81e-04, avg batch time: 0.3109, average loss: 0.7354
[11/30 18:33:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.72	
[11/30 18:33:27 visual_prompt]: Best epoch 16: best metric: -0.735
[11/30 18:33:27 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[11/30 18:35:31 visual_prompt]: 	Training 100/553. train loss: 0.5410,	0.8397 s / batch. (data: 1.27e-03). ETA=10:48:39, max mem: 20.9 GB 
[11/30 18:37:34 visual_prompt]: 	Training 200/553. train loss: 0.8182,	0.8363 s / batch. (data: 3.63e-04). ETA=10:44:40, max mem: 20.9 GB 
[11/30 18:39:32 visual_prompt]: 	Training 300/553. train loss: 1.0049,	0.8320 s / batch. (data: 7.88e-03). ETA=10:39:58, max mem: 20.9 GB 
[11/30 18:41:30 visual_prompt]: 	Training 400/553. train loss: 6.2289,	0.8361 s / batch. (data: 7.53e-04). ETA=10:41:44, max mem: 20.9 GB 
[11/30 18:43:29 visual_prompt]: 	Training 500/553. train loss: 4.5001,	2.2963 s / batch. (data: 1.45e+00). ETA=1 day, 5:18:38, max mem: 20.9 GB 
[11/30 18:44:32 visual_prompt]: Epoch 17 / 100: avg data time: 3.62e-01, avg batch time: 1.2010, average train loss: 2.3484
[11/30 18:45:45 visual_prompt]: Inference (val):avg data time: 1.29e-04, avg batch time: 0.3163, average loss: 3.8605
[11/30 18:45:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.36	
[11/30 18:45:45 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[11/30 18:47:50 visual_prompt]: 	Training 100/553. train loss: 5.0259,	0.8353 s / batch. (data: 3.38e-03). ETA=10:37:34, max mem: 20.9 GB 
[11/30 18:49:54 visual_prompt]: 	Training 200/553. train loss: 16.7930,	0.8512 s / batch. (data: 1.28e-03). ETA=10:48:19, max mem: 20.9 GB 
[11/30 18:51:56 visual_prompt]: 	Training 300/553. train loss: 0.7122,	0.8302 s / batch. (data: 1.19e-02). ETA=10:30:56, max mem: 20.9 GB 
[11/30 18:53:56 visual_prompt]: 	Training 400/553. train loss: 1.0092,	0.8251 s / batch. (data: 5.72e-03). ETA=10:25:40, max mem: 20.9 GB 
[11/30 18:55:57 visual_prompt]: 	Training 500/553. train loss: 0.7725,	0.8918 s / batch. (data: 1.59e-03). ETA=11:14:45, max mem: 20.9 GB 
[11/30 18:56:57 visual_prompt]: Epoch 18 / 100: avg data time: 3.73e-01, avg batch time: 1.2138, average train loss: 3.1650
[11/30 18:58:07 visual_prompt]: Inference (val):avg data time: 9.84e-05, avg batch time: 0.3115, average loss: 0.9248
[11/30 18:58:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.60	
[11/30 18:58:07 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[11/30 19:00:15 visual_prompt]: 	Training 100/553. train loss: 0.6262,	1.1195 s / batch. (data: 2.84e-01). ETA=14:04:10, max mem: 20.9 GB 
[11/30 19:02:16 visual_prompt]: 	Training 200/553. train loss: 1.7280,	0.8442 s / batch. (data: 1.48e-03). ETA=10:35:10, max mem: 20.9 GB 
[11/30 19:04:17 visual_prompt]: 	Training 300/553. train loss: 5.0892,	0.8395 s / batch. (data: 3.13e-03). ETA=10:30:16, max mem: 20.9 GB 
[11/30 19:06:20 visual_prompt]: 	Training 400/553. train loss: 1.3262,	0.8359 s / batch. (data: 5.53e-04). ETA=10:26:08, max mem: 20.9 GB 
[11/30 19:08:15 visual_prompt]: 	Training 500/553. train loss: 1.1437,	0.8560 s / batch. (data: 5.00e-04). ETA=10:39:48, max mem: 20.9 GB 
[11/30 19:09:18 visual_prompt]: Epoch 19 / 100: avg data time: 3.72e-01, avg batch time: 1.2127, average train loss: 1.7645
[11/30 19:10:29 visual_prompt]: Inference (val):avg data time: 1.35e-04, avg batch time: 0.3151, average loss: 3.4530
[11/30 19:10:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.46	
[11/30 19:10:29 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[11/30 19:12:31 visual_prompt]: 	Training 100/553. train loss: 0.5834,	1.1662 s / batch. (data: 3.39e-01). ETA=14:28:39, max mem: 20.9 GB 
[11/30 19:14:33 visual_prompt]: 	Training 200/553. train loss: 0.6940,	0.8477 s / batch. (data: 5.48e-04). ETA=10:30:02, max mem: 20.9 GB 
[11/30 19:16:33 visual_prompt]: 	Training 300/553. train loss: 1.6089,	0.8304 s / batch. (data: 1.04e-03). ETA=10:15:46, max mem: 20.9 GB 
[11/30 19:18:34 visual_prompt]: 	Training 400/553. train loss: 0.6775,	0.8280 s / batch. (data: 6.18e-04). ETA=10:12:38, max mem: 20.9 GB 
[11/30 19:20:35 visual_prompt]: 	Training 500/553. train loss: 1.3922,	0.8449 s / batch. (data: 8.68e-03). ETA=10:23:41, max mem: 20.9 GB 
[11/30 19:21:40 visual_prompt]: Epoch 20 / 100: avg data time: 3.71e-01, avg batch time: 1.2129, average train loss: 2.1300
[11/30 19:22:51 visual_prompt]: Inference (val):avg data time: 2.86e-04, avg batch time: 0.3129, average loss: 0.7118
[11/30 19:22:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.56	
[11/30 19:22:51 visual_prompt]: Best epoch 20: best metric: -0.712
[11/30 19:22:51 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[11/30 19:24:58 visual_prompt]: 	Training 100/553. train loss: 0.5810,	1.0994 s / batch. (data: 2.70e-01). ETA=13:28:45, max mem: 20.9 GB 
[11/30 19:26:59 visual_prompt]: 	Training 200/553. train loss: 6.4126,	0.8307 s / batch. (data: 1.07e-02). ETA=10:09:42, max mem: 20.9 GB 
[11/30 19:28:58 visual_prompt]: 	Training 300/553. train loss: 0.8785,	0.8561 s / batch. (data: 1.70e-03). ETA=10:26:54, max mem: 20.9 GB 
[11/30 19:31:01 visual_prompt]: 	Training 400/553. train loss: 3.0139,	0.8538 s / batch. (data: 1.26e-02). ETA=10:23:48, max mem: 20.9 GB 
[11/30 19:33:02 visual_prompt]: 	Training 500/553. train loss: 2.0082,	0.8218 s / batch. (data: 3.45e-04). ETA=9:59:05, max mem: 20.9 GB 
[11/30 19:34:03 visual_prompt]: Epoch 21 / 100: avg data time: 3.72e-01, avg batch time: 1.2138, average train loss: 2.1354
[11/30 19:35:15 visual_prompt]: Inference (val):avg data time: 1.24e-04, avg batch time: 0.3122, average loss: 1.3491
[11/30 19:35:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.65	
[11/30 19:35:15 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[11/30 19:37:21 visual_prompt]: 	Training 100/553. train loss: 1.5063,	0.8999 s / batch. (data: 1.16e-02). ETA=10:53:44, max mem: 20.9 GB 
[11/30 19:39:23 visual_prompt]: 	Training 200/553. train loss: 0.6319,	0.8376 s / batch. (data: 5.07e-04). ETA=10:07:04, max mem: 20.9 GB 
[11/30 19:41:21 visual_prompt]: 	Training 300/553. train loss: 0.0657,	1.0602 s / batch. (data: 2.19e-01). ETA=12:46:36, max mem: 20.9 GB 
[11/30 19:43:20 visual_prompt]: 	Training 400/553. train loss: 10.1777,	0.8357 s / batch. (data: 7.65e-04). ETA=10:02:56, max mem: 20.9 GB 
[11/30 19:45:20 visual_prompt]: 	Training 500/553. train loss: 0.8846,	0.8247 s / batch. (data: 5.70e-04). ETA=9:53:35, max mem: 20.9 GB 
[11/30 19:46:24 visual_prompt]: Epoch 22 / 100: avg data time: 3.68e-01, avg batch time: 1.2081, average train loss: 1.8319
[11/30 19:47:33 visual_prompt]: Inference (val):avg data time: 8.80e-05, avg batch time: 0.3107, average loss: 3.1889
[11/30 19:47:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.64	
[11/30 19:47:33 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[11/30 19:49:40 visual_prompt]: 	Training 100/553. train loss: 1.2534,	1.1175 s / batch. (data: 2.85e-01). ETA=13:21:32, max mem: 20.9 GB 
[11/30 19:51:43 visual_prompt]: 	Training 200/553. train loss: 0.5634,	1.1560 s / batch. (data: 3.20e-01). ETA=13:47:12, max mem: 20.9 GB 
[11/30 19:53:45 visual_prompt]: 	Training 300/553. train loss: 0.5942,	0.8361 s / batch. (data: 5.44e-04). ETA=9:56:54, max mem: 20.9 GB 
[11/30 19:55:42 visual_prompt]: 	Training 400/553. train loss: 1.5386,	0.8451 s / batch. (data: 1.09e-02). ETA=10:01:56, max mem: 20.9 GB 
[11/30 19:57:37 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8570 s / batch. (data: 8.66e-03). ETA=10:08:55, max mem: 20.9 GB 
[11/30 19:58:41 visual_prompt]: Epoch 23 / 100: avg data time: 3.65e-01, avg batch time: 1.2066, average train loss: 2.2013
[11/30 19:59:49 visual_prompt]: Inference (val):avg data time: 8.93e-05, avg batch time: 0.3104, average loss: 2.4110
[11/30 19:59:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.72	
[11/30 19:59:49 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[11/30 20:01:52 visual_prompt]: 	Training 100/553. train loss: 6.5537,	0.8760 s / batch. (data: 1.19e-02). ETA=10:20:14, max mem: 20.9 GB 
[11/30 20:03:51 visual_prompt]: 	Training 200/553. train loss: 0.6106,	0.8768 s / batch. (data: 1.41e-02). ETA=10:19:20, max mem: 20.9 GB 
[11/30 20:05:50 visual_prompt]: 	Training 300/553. train loss: 1.3971,	1.5407 s / batch. (data: 7.04e-01). ETA=18:05:41, max mem: 20.9 GB 
[11/30 20:07:47 visual_prompt]: 	Training 400/553. train loss: 0.6552,	0.8244 s / batch. (data: 7.91e-03). ETA=9:39:33, max mem: 20.9 GB 
[11/30 20:09:46 visual_prompt]: 	Training 500/553. train loss: 0.9322,	0.8240 s / batch. (data: 5.20e-04). ETA=9:37:54, max mem: 20.9 GB 
[11/30 20:10:48 visual_prompt]: Epoch 24 / 100: avg data time: 3.51e-01, avg batch time: 1.1903, average train loss: 2.2871
[11/30 20:11:58 visual_prompt]: Inference (val):avg data time: 1.88e-04, avg batch time: 0.3109, average loss: 0.7321
[11/30 20:11:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.61	
[11/30 20:11:58 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[11/30 20:14:06 visual_prompt]: 	Training 100/553. train loss: 2.2837,	0.8400 s / batch. (data: 8.07e-03). ETA=9:47:00, max mem: 20.9 GB 
[11/30 20:16:00 visual_prompt]: 	Training 200/553. train loss: 1.1692,	1.1765 s / batch. (data: 3.41e-01). ETA=13:40:09, max mem: 20.9 GB 
[11/30 20:18:00 visual_prompt]: 	Training 300/553. train loss: 1.5923,	1.6134 s / batch. (data: 7.62e-01). ETA=18:42:02, max mem: 20.9 GB 
[11/30 20:19:57 visual_prompt]: 	Training 400/553. train loss: 1.2355,	1.6832 s / batch. (data: 8.52e-01). ETA=19:27:47, max mem: 20.9 GB 
[11/30 20:21:56 visual_prompt]: 	Training 500/553. train loss: 0.7242,	2.3690 s / batch. (data: 1.53e+00). ETA=1 day, 3:19:40, max mem: 20.9 GB 
[11/30 20:22:58 visual_prompt]: Epoch 25 / 100: avg data time: 3.53e-01, avg batch time: 1.1923, average train loss: 1.6836
[11/30 20:24:07 visual_prompt]: Inference (val):avg data time: 9.11e-05, avg batch time: 0.3121, average loss: 3.6299
[11/30 20:24:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.64	
[11/30 20:24:07 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[11/30 20:26:12 visual_prompt]: 	Training 100/553. train loss: 2.0128,	0.8347 s / batch. (data: 1.70e-03). ETA=9:35:34, max mem: 20.9 GB 
[11/30 20:28:14 visual_prompt]: 	Training 200/553. train loss: 11.8396,	2.6429 s / batch. (data: 1.82e+00). ETA=1 day, 6:18:04, max mem: 20.9 GB 
[11/30 20:30:13 visual_prompt]: 	Training 300/553. train loss: 0.1117,	0.8280 s / batch. (data: 1.20e-02). ETA=9:28:14, max mem: 20.9 GB 
[11/30 20:32:09 visual_prompt]: 	Training 400/553. train loss: 0.5824,	0.8399 s / batch. (data: 7.98e-04). ETA=9:34:58, max mem: 20.9 GB 
[11/30 20:34:07 visual_prompt]: 	Training 500/553. train loss: 5.2843,	0.8360 s / batch. (data: 4.84e-04). ETA=9:30:54, max mem: 20.9 GB 
[11/30 20:35:08 visual_prompt]: Epoch 26 / 100: avg data time: 3.56e-01, avg batch time: 1.1951, average train loss: 2.4047
[11/30 20:36:16 visual_prompt]: Inference (val):avg data time: 9.27e-05, avg batch time: 0.3110, average loss: 0.8601
[11/30 20:36:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.71	
[11/30 20:36:16 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[11/30 20:38:20 visual_prompt]: 	Training 100/553. train loss: 0.6568,	0.8655 s / batch. (data: 1.23e-03). ETA=9:48:53, max mem: 20.9 GB 
[11/30 20:40:18 visual_prompt]: 	Training 200/553. train loss: 5.6375,	2.5520 s / batch. (data: 1.71e+00). ETA=1 day, 4:52:03, max mem: 20.9 GB 
[11/30 20:42:15 visual_prompt]: 	Training 300/553. train loss: 2.6742,	1.2538 s / batch. (data: 3.86e-01). ETA=14:08:51, max mem: 20.9 GB 
[11/30 20:44:15 visual_prompt]: 	Training 400/553. train loss: 22.6630,	0.8799 s / batch. (data: 1.54e-02). ETA=9:54:13, max mem: 20.9 GB 
[11/30 20:46:14 visual_prompt]: 	Training 500/553. train loss: 0.7050,	0.8641 s / batch. (data: 3.36e-03). ETA=9:42:09, max mem: 20.9 GB 
[11/30 20:47:13 visual_prompt]: Epoch 27 / 100: avg data time: 3.48e-01, avg batch time: 1.1864, average train loss: 2.5300
[11/30 20:48:22 visual_prompt]: Inference (val):avg data time: 7.40e-04, avg batch time: 0.3129, average loss: 4.6716
[11/30 20:48:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.49	
[11/30 20:48:22 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[11/30 20:50:26 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8769 s / batch. (data: 5.63e-03). ETA=9:48:31, max mem: 20.9 GB 
[11/30 20:52:25 visual_prompt]: 	Training 200/553. train loss: 0.6092,	0.8402 s / batch. (data: 9.42e-04). ETA=9:22:28, max mem: 20.9 GB 
[11/30 20:54:21 visual_prompt]: 	Training 300/553. train loss: 1.4800,	2.0994 s / batch. (data: 1.24e+00). ETA=23:22:01, max mem: 20.9 GB 
[11/30 20:56:18 visual_prompt]: 	Training 400/553. train loss: 3.4777,	0.8678 s / batch. (data: 1.85e-03). ETA=9:38:05, max mem: 20.9 GB 
[11/30 20:58:16 visual_prompt]: 	Training 500/553. train loss: 3.5916,	0.8459 s / batch. (data: 1.13e-02). ETA=9:22:04, max mem: 20.9 GB 
[11/30 20:59:18 visual_prompt]: Epoch 28 / 100: avg data time: 3.49e-01, avg batch time: 1.1869, average train loss: 2.3100
[11/30 21:00:28 visual_prompt]: Inference (val):avg data time: 4.82e-04, avg batch time: 0.3128, average loss: 0.7591
[11/30 21:00:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.65	
[11/30 21:00:28 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[11/30 21:02:41 visual_prompt]: 	Training 100/553. train loss: 1.9452,	0.8640 s / batch. (data: 5.50e-04). ETA=9:31:53, max mem: 20.9 GB 
[11/30 21:04:40 visual_prompt]: 	Training 200/553. train loss: 0.9163,	2.5196 s / batch. (data: 1.69e+00). ETA=1 day, 3:43:36, max mem: 20.9 GB 
[11/30 21:06:38 visual_prompt]: 	Training 300/553. train loss: 1.0176,	0.8476 s / batch. (data: 8.53e-04). ETA=9:18:14, max mem: 20.9 GB 
[11/30 21:08:34 visual_prompt]: 	Training 400/553. train loss: 2.4156,	2.2046 s / batch. (data: 1.36e+00). ETA=1 day, 0:08:14, max mem: 20.9 GB 
[11/30 21:10:34 visual_prompt]: 	Training 500/553. train loss: 2.2743,	0.8284 s / batch. (data: 1.76e-03). ETA=9:02:48, max mem: 20.9 GB 
[11/30 21:11:38 visual_prompt]: Epoch 29 / 100: avg data time: 3.69e-01, avg batch time: 1.2105, average train loss: 1.7621
[11/30 21:12:50 visual_prompt]: Inference (val):avg data time: 1.15e-04, avg batch time: 0.3130, average loss: 1.4689
[11/30 21:12:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.46	
[11/30 21:12:50 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[11/30 21:14:52 visual_prompt]: 	Training 100/553. train loss: 1.3579,	0.8734 s / batch. (data: 5.67e-03). ETA=9:30:03, max mem: 20.9 GB 
[11/30 21:16:53 visual_prompt]: 	Training 200/553. train loss: 1.5283,	0.8701 s / batch. (data: 1.56e-03). ETA=9:26:27, max mem: 20.9 GB 
[11/30 21:18:51 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.2563 s / batch. (data: 4.38e-01). ETA=13:35:50, max mem: 20.9 GB 
[11/30 21:20:54 visual_prompt]: 	Training 400/553. train loss: 0.8073,	2.0079 s / batch. (data: 1.15e+00). ETA=21:40:31, max mem: 20.9 GB 
[11/30 21:22:54 visual_prompt]: 	Training 500/553. train loss: 2.9417,	2.4055 s / batch. (data: 1.57e+00). ETA=1 day, 1:54:02, max mem: 20.9 GB 
[11/30 21:23:57 visual_prompt]: Epoch 30 / 100: avg data time: 3.63e-01, avg batch time: 1.2051, average train loss: 1.7273
[11/30 21:25:09 visual_prompt]: Inference (val):avg data time: 1.15e-04, avg batch time: 0.3121, average loss: 0.7019
[11/30 21:25:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.84	
[11/30 21:25:09 visual_prompt]: Best epoch 30: best metric: -0.702
[11/30 21:25:09 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[11/30 21:27:17 visual_prompt]: 	Training 100/553. train loss: 2.5419,	0.8212 s / batch. (data: 1.06e-03). ETA=8:48:24, max mem: 20.9 GB 
[11/30 21:29:23 visual_prompt]: 	Training 200/553. train loss: 1.6897,	0.8742 s / batch. (data: 2.33e-03). ETA=9:21:05, max mem: 20.9 GB 
[11/30 21:31:24 visual_prompt]: 	Training 300/553. train loss: 3.9643,	0.8842 s / batch. (data: 1.15e-02). ETA=9:26:01, max mem: 20.9 GB 
[11/30 21:33:26 visual_prompt]: 	Training 400/553. train loss: 1.2880,	1.4803 s / batch. (data: 6.46e-01). ETA=15:45:11, max mem: 20.9 GB 
[11/30 21:35:32 visual_prompt]: 	Training 500/553. train loss: 0.6039,	0.8360 s / batch. (data: 1.53e-03). ETA=8:52:22, max mem: 20.9 GB 
[11/30 21:36:35 visual_prompt]: Epoch 31 / 100: avg data time: 3.96e-01, avg batch time: 1.2406, average train loss: 2.2001
[11/30 21:37:46 visual_prompt]: Inference (val):avg data time: 1.07e-04, avg batch time: 0.3133, average loss: 0.8232
[11/30 21:37:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.39	
[11/30 21:37:46 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[11/30 21:39:59 visual_prompt]: 	Training 100/553. train loss: 0.9455,	0.8845 s / batch. (data: 6.33e-04). ETA=9:21:00, max mem: 20.9 GB 
[11/30 21:42:05 visual_prompt]: 	Training 200/553. train loss: 1.8196,	0.8840 s / batch. (data: 1.92e-03). ETA=9:19:13, max mem: 20.9 GB 
[11/30 21:44:18 visual_prompt]: 	Training 300/553. train loss: 2.4443,	0.8271 s / batch. (data: 1.58e-03). ETA=8:41:50, max mem: 20.9 GB 
[11/30 21:46:23 visual_prompt]: 	Training 400/553. train loss: 0.7317,	0.8253 s / batch. (data: 1.65e-03). ETA=8:39:20, max mem: 20.9 GB 
[11/30 21:48:24 visual_prompt]: 	Training 500/553. train loss: 1.1630,	0.8640 s / batch. (data: 1.50e-03). ETA=9:02:15, max mem: 20.9 GB 
[11/30 21:49:27 visual_prompt]: Epoch 32 / 100: avg data time: 4.20e-01, avg batch time: 1.2667, average train loss: 1.5822
[11/30 21:50:39 visual_prompt]: Inference (val):avg data time: 3.13e-04, avg batch time: 0.3138, average loss: 2.1389
[11/30 21:50:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.80	
[11/30 21:50:39 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[11/30 21:52:45 visual_prompt]: 	Training 100/553. train loss: 0.0169,	1.8787 s / batch. (data: 1.03e+00). ETA=19:34:20, max mem: 20.9 GB 
[11/30 21:54:55 visual_prompt]: 	Training 200/553. train loss: 2.2373,	2.6183 s / batch. (data: 1.79e+00). ETA=1 day, 3:12:14, max mem: 20.9 GB 
[11/30 21:56:59 visual_prompt]: 	Training 300/553. train loss: 0.6407,	0.8624 s / batch. (data: 1.56e-03). ETA=8:56:09, max mem: 20.9 GB 
[11/30 21:59:07 visual_prompt]: 	Training 400/553. train loss: 3.1459,	0.8278 s / batch. (data: 1.52e-03). ETA=8:33:19, max mem: 20.9 GB 
[11/30 22:01:11 visual_prompt]: 	Training 500/553. train loss: 1.8285,	0.8385 s / batch. (data: 1.32e-03). ETA=8:38:31, max mem: 20.9 GB 
[11/30 22:02:19 visual_prompt]: Epoch 33 / 100: avg data time: 4.19e-01, avg batch time: 1.2649, average train loss: 2.2029
[11/30 22:03:30 visual_prompt]: Inference (val):avg data time: 2.51e-04, avg batch time: 0.3127, average loss: 1.7899
[11/30 22:03:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.28	
[11/30 22:03:31 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[11/30 22:05:37 visual_prompt]: 	Training 100/553. train loss: 1.7802,	1.0314 s / batch. (data: 1.87e-01). ETA=10:35:12, max mem: 20.9 GB 
[11/30 22:07:39 visual_prompt]: 	Training 200/553. train loss: 0.7059,	0.8241 s / batch. (data: 4.27e-04). ETA=8:26:08, max mem: 20.9 GB 
[11/30 22:09:37 visual_prompt]: 	Training 300/553. train loss: 0.6871,	1.6155 s / batch. (data: 7.58e-01). ETA=16:29:31, max mem: 20.9 GB 
[11/30 22:11:41 visual_prompt]: 	Training 400/553. train loss: 8.1180,	0.8600 s / batch. (data: 2.34e-03). ETA=8:45:18, max mem: 20.9 GB 
[11/30 22:13:42 visual_prompt]: 	Training 500/553. train loss: 1.1805,	2.4158 s / batch. (data: 1.59e+00). ETA=1 day, 0:31:39, max mem: 20.9 GB 
[11/30 22:14:42 visual_prompt]: Epoch 34 / 100: avg data time: 3.72e-01, avg batch time: 1.2137, average train loss: 2.4220
[11/30 22:15:57 visual_prompt]: Inference (val):avg data time: 1.16e-03, avg batch time: 0.3165, average loss: 0.6973
[11/30 22:15:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.53	
[11/30 22:15:57 visual_prompt]: Best epoch 34: best metric: -0.697
[11/30 22:15:57 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[11/30 22:18:04 visual_prompt]: 	Training 100/553. train loss: 3.9472,	0.8634 s / batch. (data: 1.19e-02). ETA=8:43:46, max mem: 20.9 GB 
[11/30 22:20:07 visual_prompt]: 	Training 200/553. train loss: 0.7024,	0.9537 s / batch. (data: 1.01e-01). ETA=9:36:56, max mem: 20.9 GB 
[11/30 22:22:09 visual_prompt]: 	Training 300/553. train loss: 3.0088,	0.8240 s / batch. (data: 5.30e-04). ETA=8:17:07, max mem: 20.9 GB 
[11/30 22:24:07 visual_prompt]: 	Training 400/553. train loss: 3.9492,	1.6443 s / batch. (data: 8.23e-01). ETA=16:29:14, max mem: 20.9 GB 
[11/30 22:26:05 visual_prompt]: 	Training 500/553. train loss: 0.7027,	1.9105 s / batch. (data: 1.08e+00). ETA=19:06:12, max mem: 20.9 GB 
[11/30 22:27:07 visual_prompt]: Epoch 35 / 100: avg data time: 3.70e-01, avg batch time: 1.2110, average train loss: 1.8787
[11/30 22:28:18 visual_prompt]: Inference (val):avg data time: 3.34e-04, avg batch time: 0.3141, average loss: 4.9512
[11/30 22:28:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.78	
[11/30 22:28:18 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[11/30 22:30:22 visual_prompt]: 	Training 100/553. train loss: 1.8079,	0.8509 s / batch. (data: 1.73e-03). ETA=8:28:19, max mem: 20.9 GB 
[11/30 22:32:26 visual_prompt]: 	Training 200/553. train loss: 1.8903,	0.8721 s / batch. (data: 1.58e-02). ETA=8:39:33, max mem: 20.9 GB 
[11/30 22:34:25 visual_prompt]: 	Training 300/553. train loss: 0.0066,	0.8613 s / batch. (data: 6.95e-03). ETA=8:31:40, max mem: 20.9 GB 
[11/30 22:36:24 visual_prompt]: 	Training 400/553. train loss: 0.7187,	1.4318 s / batch. (data: 6.12e-01). ETA=14:08:13, max mem: 20.9 GB 
[11/30 22:38:22 visual_prompt]: 	Training 500/553. train loss: 1.8388,	1.5121 s / batch. (data: 6.91e-01). ETA=14:53:14, max mem: 20.9 GB 
[11/30 22:39:19 visual_prompt]: Epoch 36 / 100: avg data time: 3.55e-01, avg batch time: 1.1950, average train loss: 2.0146
[11/30 22:40:29 visual_prompt]: Inference (val):avg data time: 3.28e-04, avg batch time: 0.3147, average loss: 1.5923
[11/30 22:40:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.21	
[11/30 22:40:29 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[11/30 22:42:34 visual_prompt]: 	Training 100/553. train loss: 0.5348,	0.8399 s / batch. (data: 1.47e-03). ETA=8:14:02, max mem: 20.9 GB 
[11/30 22:44:31 visual_prompt]: 	Training 200/553. train loss: 0.8555,	0.8240 s / batch. (data: 4.93e-04). ETA=8:03:18, max mem: 20.9 GB 
[11/30 22:46:31 visual_prompt]: 	Training 300/553. train loss: 3.8998,	2.2000 s / batch. (data: 1.36e+00). ETA=21:26:43, max mem: 20.9 GB 
[11/30 22:48:31 visual_prompt]: 	Training 400/553. train loss: 0.7419,	2.2363 s / batch. (data: 1.40e+00). ETA=21:44:12, max mem: 20.9 GB 
[11/30 22:50:25 visual_prompt]: 	Training 500/553. train loss: 1.4059,	1.8487 s / batch. (data: 9.73e-01). ETA=17:55:04, max mem: 20.9 GB 
[11/30 22:51:28 visual_prompt]: Epoch 37 / 100: avg data time: 3.53e-01, avg batch time: 1.1907, average train loss: 1.9589
[11/30 22:52:36 visual_prompt]: Inference (val):avg data time: 3.39e-04, avg batch time: 0.3144, average loss: 0.7793
[11/30 22:52:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.97	
[11/30 22:52:36 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[11/30 22:54:35 visual_prompt]: 	Training 100/553. train loss: 0.5645,	0.8481 s / batch. (data: 6.50e-03). ETA=8:11:03, max mem: 20.9 GB 
[11/30 22:56:35 visual_prompt]: 	Training 200/553. train loss: 1.0203,	2.0074 s / batch. (data: 1.16e+00). ETA=19:18:54, max mem: 20.9 GB 
[11/30 22:58:35 visual_prompt]: 	Training 300/553. train loss: 2.2659,	0.8525 s / batch. (data: 7.87e-03). ETA=8:10:44, max mem: 20.9 GB 
[11/30 23:00:30 visual_prompt]: 	Training 400/553. train loss: 0.4156,	0.8273 s / batch. (data: 5.26e-04). ETA=7:54:50, max mem: 20.9 GB 
[11/30 23:02:31 visual_prompt]: 	Training 500/553. train loss: 0.5712,	0.8709 s / batch. (data: 2.12e-03). ETA=8:18:24, max mem: 20.9 GB 
[11/30 23:03:31 visual_prompt]: Epoch 38 / 100: avg data time: 3.45e-01, avg batch time: 1.1839, average train loss: 1.9836
[11/30 23:04:41 visual_prompt]: Inference (val):avg data time: 9.98e-05, avg batch time: 0.3121, average loss: 2.0595
[11/30 23:04:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.40	
[11/30 23:04:41 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[11/30 23:06:43 visual_prompt]: 	Training 100/553. train loss: 0.0233,	0.8498 s / batch. (data: 5.70e-03). ETA=8:04:10, max mem: 20.9 GB 
[11/30 23:08:47 visual_prompt]: 	Training 200/553. train loss: 2.2022,	0.8373 s / batch. (data: 5.92e-04). ETA=7:55:41, max mem: 20.9 GB 
[11/30 23:10:49 visual_prompt]: 	Training 300/553. train loss: 1.7887,	0.8440 s / batch. (data: 5.25e-04). ETA=7:58:04, max mem: 20.9 GB 
[11/30 23:12:46 visual_prompt]: 	Training 400/553. train loss: 1.1589,	0.8484 s / batch. (data: 7.00e-04). ETA=7:59:10, max mem: 20.9 GB 
[11/30 23:14:43 visual_prompt]: 	Training 500/553. train loss: 1.4047,	2.6127 s / batch. (data: 1.78e+00). ETA=1 day, 0:31:12, max mem: 20.9 GB 
[11/30 23:15:45 visual_prompt]: Epoch 39 / 100: avg data time: 3.61e-01, avg batch time: 1.2002, average train loss: 1.9424
[11/30 23:16:54 visual_prompt]: Inference (val):avg data time: 9.74e-05, avg batch time: 0.3140, average loss: 0.6912
[11/30 23:16:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 49.58	
[11/30 23:16:54 visual_prompt]: Best epoch 39: best metric: -0.691
[11/30 23:16:54 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[11/30 23:19:03 visual_prompt]: 	Training 100/553. train loss: 2.9837,	0.8392 s / batch. (data: 3.45e-03). ETA=7:50:24, max mem: 20.9 GB 
[11/30 23:21:01 visual_prompt]: 	Training 200/553. train loss: 2.2992,	0.8534 s / batch. (data: 1.92e-02). ETA=7:56:57, max mem: 20.9 GB 
[11/30 23:23:03 visual_prompt]: 	Training 300/553. train loss: 3.2350,	0.8360 s / batch. (data: 1.78e-03). ETA=7:45:49, max mem: 20.9 GB 
[11/30 23:25:05 visual_prompt]: 	Training 400/553. train loss: 3.3173,	0.8806 s / batch. (data: 1.10e-02). ETA=8:09:12, max mem: 20.9 GB 
[11/30 23:27:04 visual_prompt]: 	Training 500/553. train loss: 1.2729,	0.8922 s / batch. (data: 3.56e-02). ETA=8:14:10, max mem: 20.9 GB 
[11/30 23:28:08 visual_prompt]: Epoch 40 / 100: avg data time: 3.78e-01, avg batch time: 1.2187, average train loss: 2.1331
[11/30 23:29:20 visual_prompt]: Inference (val):avg data time: 4.52e-04, avg batch time: 0.3135, average loss: 0.6875
[11/30 23:29:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.70	
[11/30 23:29:20 visual_prompt]: Best epoch 40: best metric: -0.687
[11/30 23:29:20 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[11/30 23:31:33 visual_prompt]: 	Training 100/553. train loss: 1.0026,	0.8440 s / batch. (data: 5.51e-04). ETA=7:45:18, max mem: 20.9 GB 
[11/30 23:33:38 visual_prompt]: 	Training 200/553. train loss: 0.5577,	0.8478 s / batch. (data: 6.33e-04). ETA=7:46:01, max mem: 20.9 GB 
[11/30 23:35:36 visual_prompt]: 	Training 300/553. train loss: 0.8985,	0.8298 s / batch. (data: 5.90e-04). ETA=7:34:43, max mem: 20.9 GB 
[11/30 23:37:35 visual_prompt]: 	Training 400/553. train loss: 0.7687,	0.8396 s / batch. (data: 1.18e-02). ETA=7:38:40, max mem: 20.9 GB 
[11/30 23:39:30 visual_prompt]: 	Training 500/553. train loss: 0.9609,	0.8438 s / batch. (data: 1.44e-03). ETA=7:39:33, max mem: 20.9 GB 
[11/30 23:40:32 visual_prompt]: Epoch 41 / 100: avg data time: 3.74e-01, avg batch time: 1.2158, average train loss: 1.8076
[11/30 23:41:41 visual_prompt]: Inference (val):avg data time: 2.98e-04, avg batch time: 0.3111, average loss: 0.8254
[11/30 23:41:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.25	
[11/30 23:41:41 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[11/30 23:43:47 visual_prompt]: 	Training 100/553. train loss: 3.5121,	0.8198 s / batch. (data: 5.68e-04). ETA=7:24:25, max mem: 20.9 GB 
[11/30 23:45:48 visual_prompt]: 	Training 200/553. train loss: 2.8598,	0.8168 s / batch. (data: 5.03e-04). ETA=7:21:27, max mem: 20.9 GB 
[11/30 23:47:49 visual_prompt]: 	Training 300/553. train loss: 1.5575,	0.8694 s / batch. (data: 7.83e-03). ETA=7:48:24, max mem: 20.9 GB 
[11/30 23:49:49 visual_prompt]: 	Training 400/553. train loss: 0.8109,	0.8610 s / batch. (data: 1.38e-02). ETA=7:42:28, max mem: 20.9 GB 
[11/30 23:51:47 visual_prompt]: 	Training 500/553. train loss: 1.9449,	1.3200 s / batch. (data: 4.96e-01). ETA=11:46:46, max mem: 20.9 GB 
[11/30 23:52:51 visual_prompt]: Epoch 42 / 100: avg data time: 3.69e-01, avg batch time: 1.2110, average train loss: 2.8876
[11/30 23:54:03 visual_prompt]: Inference (val):avg data time: 1.14e-04, avg batch time: 0.3118, average loss: 3.6646
[11/30 23:54:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.18	
[11/30 23:54:03 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[11/30 23:56:10 visual_prompt]: 	Training 100/553. train loss: 0.5826,	0.8321 s / batch. (data: 1.19e-02). ETA=7:23:24, max mem: 20.9 GB 
[11/30 23:58:09 visual_prompt]: 	Training 200/553. train loss: 2.6250,	0.8603 s / batch. (data: 1.55e-03). ETA=7:36:59, max mem: 20.9 GB 
[12/01 00:00:07 visual_prompt]: 	Training 300/553. train loss: 0.8087,	0.8521 s / batch. (data: 1.43e-02). ETA=7:31:15, max mem: 20.9 GB 
[12/01 00:02:05 visual_prompt]: 	Training 400/553. train loss: 0.8170,	0.8364 s / batch. (data: 5.20e-04). ETA=7:21:32, max mem: 20.9 GB 
[12/01 00:04:09 visual_prompt]: 	Training 500/553. train loss: 2.3906,	0.8554 s / batch. (data: 5.21e-04). ETA=7:30:09, max mem: 20.9 GB 
[12/01 00:05:12 visual_prompt]: Epoch 43 / 100: avg data time: 3.71e-01, avg batch time: 1.2088, average train loss: 1.7912
[12/01 00:06:22 visual_prompt]: Inference (val):avg data time: 8.67e-05, avg batch time: 0.3112, average loss: 0.7125
[12/01 00:06:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.82	
[12/01 00:06:22 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[12/01 00:08:29 visual_prompt]: 	Training 100/553. train loss: 0.6620,	0.8319 s / batch. (data: 5.50e-04). ETA=7:15:39, max mem: 20.9 GB 
[12/01 00:10:33 visual_prompt]: 	Training 200/553. train loss: 1.5963,	0.8435 s / batch. (data: 5.47e-04). ETA=7:20:18, max mem: 20.9 GB 
[12/01 00:12:30 visual_prompt]: 	Training 300/553. train loss: 1.1343,	0.8444 s / batch. (data: 7.89e-03). ETA=7:19:22, max mem: 20.9 GB 
[12/01 00:14:26 visual_prompt]: 	Training 400/553. train loss: 0.7961,	0.8530 s / batch. (data: 1.58e-02). ETA=7:22:25, max mem: 20.9 GB 
[12/01 00:16:25 visual_prompt]: 	Training 500/553. train loss: 1.4189,	0.8283 s / batch. (data: 1.62e-03). ETA=7:08:13, max mem: 20.9 GB 
[12/01 00:17:29 visual_prompt]: Epoch 44 / 100: avg data time: 3.62e-01, avg batch time: 1.2044, average train loss: 1.7323
[12/01 00:18:41 visual_prompt]: Inference (val):avg data time: 1.90e-04, avg batch time: 0.3146, average loss: 3.6322
[12/01 00:18:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.28	
[12/01 00:18:41 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[12/01 00:20:50 visual_prompt]: 	Training 100/553. train loss: 1.9493,	0.8359 s / batch. (data: 1.25e-03). ETA=7:10:02, max mem: 20.9 GB 
[12/01 00:22:45 visual_prompt]: 	Training 200/553. train loss: 1.9173,	2.1401 s / batch. (data: 1.31e+00). ETA=18:17:25, max mem: 20.9 GB 
[12/01 00:24:49 visual_prompt]: 	Training 300/553. train loss: 4.5470,	0.8627 s / batch. (data: 1.06e-02). ETA=7:20:58, max mem: 20.9 GB 
[12/01 00:26:47 visual_prompt]: 	Training 400/553. train loss: 0.9918,	0.8554 s / batch. (data: 6.27e-03). ETA=7:15:46, max mem: 20.9 GB 
[12/01 00:28:53 visual_prompt]: 	Training 500/553. train loss: 0.5472,	0.8324 s / batch. (data: 7.72e-04). ETA=7:02:41, max mem: 20.9 GB 
[12/01 00:29:56 visual_prompt]: Epoch 45 / 100: avg data time: 3.79e-01, avg batch time: 1.2203, average train loss: 1.2542
[12/01 00:31:06 visual_prompt]: Inference (val):avg data time: 3.30e-04, avg batch time: 0.3134, average loss: 2.2121
[12/01 00:31:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.60	
[12/01 00:31:06 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[12/01 00:33:14 visual_prompt]: 	Training 100/553. train loss: 1.0741,	1.9555 s / batch. (data: 1.12e+00). ETA=16:27:59, max mem: 20.9 GB 
[12/01 00:35:16 visual_prompt]: 	Training 200/553. train loss: 1.3387,	0.8338 s / batch. (data: 5.19e-04). ETA=6:59:53, max mem: 20.9 GB 
[12/01 00:37:15 visual_prompt]: 	Training 300/553. train loss: 1.0882,	0.8597 s / batch. (data: 6.84e-03). ETA=7:11:30, max mem: 20.9 GB 
[12/01 00:39:15 visual_prompt]: 	Training 400/553. train loss: 1.5967,	0.8440 s / batch. (data: 1.50e-03). ETA=7:02:13, max mem: 20.9 GB 
[12/01 00:41:10 visual_prompt]: 	Training 500/553. train loss: 1.9018,	0.8486 s / batch. (data: 2.39e-02). ETA=7:03:05, max mem: 20.9 GB 
[12/01 00:42:16 visual_prompt]: Epoch 46 / 100: avg data time: 3.70e-01, avg batch time: 1.2102, average train loss: 1.1625
[12/01 00:43:27 visual_prompt]: Inference (val):avg data time: 1.11e-04, avg batch time: 0.3127, average loss: 0.7888
[12/01 00:43:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.65	
[12/01 00:43:27 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[12/01 00:45:34 visual_prompt]: 	Training 100/553. train loss: 3.3990,	0.8256 s / batch. (data: 6.40e-04). ETA=6:49:32, max mem: 20.9 GB 
[12/01 00:47:30 visual_prompt]: 	Training 200/553. train loss: 2.1580,	1.5682 s / batch. (data: 7.33e-01). ETA=12:55:17, max mem: 20.9 GB 
[12/01 00:49:32 visual_prompt]: 	Training 300/553. train loss: 2.5841,	0.8594 s / batch. (data: 6.10e-03). ETA=7:03:26, max mem: 20.9 GB 
[12/01 00:51:35 visual_prompt]: 	Training 400/553. train loss: 1.3300,	0.8547 s / batch. (data: 2.58e-02). ETA=6:59:42, max mem: 20.9 GB 
[12/01 00:53:35 visual_prompt]: 	Training 500/553. train loss: 1.0709,	0.8793 s / batch. (data: 6.84e-03). ETA=7:10:19, max mem: 20.9 GB 
[12/01 00:54:37 visual_prompt]: Epoch 47 / 100: avg data time: 3.70e-01, avg batch time: 1.2120, average train loss: 1.8019
[12/01 00:55:46 visual_prompt]: Inference (val):avg data time: 8.18e-05, avg batch time: 0.3111, average loss: 1.6289
[12/01 00:55:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.55	
[12/01 00:55:46 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[12/01 00:57:53 visual_prompt]: 	Training 100/553. train loss: 1.5418,	0.8549 s / batch. (data: 5.59e-03). ETA=6:56:10, max mem: 20.9 GB 
[12/01 00:59:55 visual_prompt]: 	Training 200/553. train loss: 1.7255,	0.8283 s / batch. (data: 6.53e-04). ETA=6:41:51, max mem: 20.9 GB 
[12/01 01:01:55 visual_prompt]: 	Training 300/553. train loss: 0.7735,	3.1916 s / batch. (data: 2.36e+00). ETA=1 day, 1:43:05, max mem: 20.9 GB 
[12/01 01:03:53 visual_prompt]: 	Training 400/553. train loss: 0.0003,	1.0999 s / batch. (data: 2.70e-01). ETA=8:49:57, max mem: 20.9 GB 
[12/01 01:05:52 visual_prompt]: 	Training 500/553. train loss: 0.8329,	0.8302 s / batch. (data: 7.09e-04). ETA=6:38:38, max mem: 20.9 GB 
[12/01 01:06:53 visual_prompt]: Epoch 48 / 100: avg data time: 3.65e-01, avg batch time: 1.2057, average train loss: 1.3961
[12/01 01:08:02 visual_prompt]: Inference (val):avg data time: 2.69e-04, avg batch time: 0.3127, average loss: 1.5958
[12/01 01:08:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.42	
[12/01 01:08:02 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[12/01 01:10:06 visual_prompt]: 	Training 100/553. train loss: 0.7296,	0.8696 s / batch. (data: 1.07e-02). ETA=6:55:18, max mem: 20.9 GB 
[12/01 01:12:03 visual_prompt]: 	Training 200/553. train loss: 1.0577,	0.8245 s / batch. (data: 5.43e-04). ETA=6:32:25, max mem: 20.9 GB 
[12/01 01:14:02 visual_prompt]: 	Training 300/553. train loss: 3.9160,	0.8600 s / batch. (data: 7.74e-03). ETA=6:47:51, max mem: 20.9 GB 
[12/01 01:16:06 visual_prompt]: 	Training 400/553. train loss: 0.8221,	0.8322 s / batch. (data: 9.08e-04). ETA=6:33:18, max mem: 20.9 GB 
[12/01 01:18:04 visual_prompt]: 	Training 500/553. train loss: 0.5410,	0.8415 s / batch. (data: 1.02e-03). ETA=6:36:17, max mem: 20.9 GB 
[12/01 01:19:05 visual_prompt]: Epoch 49 / 100: avg data time: 3.59e-01, avg batch time: 1.1975, average train loss: 1.3092
[12/01 01:20:14 visual_prompt]: Inference (val):avg data time: 8.33e-05, avg batch time: 0.3115, average loss: 1.1443
[12/01 01:20:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.26	
[12/01 01:20:14 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[12/01 01:22:20 visual_prompt]: 	Training 100/553. train loss: 0.7521,	0.8815 s / batch. (data: 2.23e-02). ETA=6:52:52, max mem: 20.9 GB 
[12/01 01:24:22 visual_prompt]: 	Training 200/553. train loss: 4.1074,	0.8460 s / batch. (data: 5.50e-03). ETA=6:34:51, max mem: 20.9 GB 
[12/01 01:26:20 visual_prompt]: 	Training 300/553. train loss: 0.7680,	0.8448 s / batch. (data: 8.68e-03). ETA=6:32:51, max mem: 20.9 GB 
[12/01 01:28:17 visual_prompt]: 	Training 400/553. train loss: 1.1112,	0.8635 s / batch. (data: 1.21e-02). ETA=6:40:07, max mem: 20.9 GB 
[12/01 01:30:19 visual_prompt]: 	Training 500/553. train loss: 3.2906,	0.8773 s / batch. (data: 2.18e-02). ETA=6:45:04, max mem: 20.9 GB 
[12/01 01:31:22 visual_prompt]: Epoch 50 / 100: avg data time: 3.68e-01, avg batch time: 1.2085, average train loss: 1.3716
[12/01 01:32:33 visual_prompt]: Inference (val):avg data time: 1.22e-04, avg batch time: 0.3119, average loss: 1.5215
[12/01 01:32:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.37	
[12/01 01:32:33 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[12/01 01:34:40 visual_prompt]: 	Training 100/553. train loss: 0.7612,	1.6640 s / batch. (data: 8.33e-01). ETA=12:44:02, max mem: 20.9 GB 
[12/01 01:36:39 visual_prompt]: 	Training 200/553. train loss: 1.4728,	0.8498 s / batch. (data: 2.22e-02). ETA=6:28:46, max mem: 20.9 GB 
[12/01 01:38:41 visual_prompt]: 	Training 300/553. train loss: 1.2629,	2.1561 s / batch. (data: 1.29e+00). ETA=16:22:48, max mem: 20.9 GB 
[12/01 01:40:45 visual_prompt]: 	Training 400/553. train loss: 2.5016,	2.5584 s / batch. (data: 1.74e+00). ETA=19:21:56, max mem: 20.9 GB 
[12/01 01:42:42 visual_prompt]: 	Training 500/553. train loss: 0.6205,	0.8639 s / batch. (data: 1.36e-03). ETA=6:30:54, max mem: 20.9 GB 
[12/01 01:43:45 visual_prompt]: Epoch 51 / 100: avg data time: 3.73e-01, avg batch time: 1.2137, average train loss: 1.2642
[12/01 01:44:56 visual_prompt]: Inference (val):avg data time: 1.17e-04, avg batch time: 0.3125, average loss: 0.7854
[12/01 01:44:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.11	
[12/01 01:44:56 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[12/01 01:47:08 visual_prompt]: 	Training 100/553. train loss: 2.8758,	0.8388 s / batch. (data: 1.47e-03). ETA=6:17:25, max mem: 20.9 GB 
[12/01 01:49:08 visual_prompt]: 	Training 200/553. train loss: 0.7447,	0.8725 s / batch. (data: 5.87e-04). ETA=6:31:07, max mem: 20.9 GB 
[12/01 01:51:10 visual_prompt]: 	Training 300/553. train loss: 1.9115,	0.8538 s / batch. (data: 1.60e-03). ETA=6:21:19, max mem: 20.9 GB 
[12/01 01:53:13 visual_prompt]: 	Training 400/553. train loss: 0.0999,	0.8519 s / batch. (data: 1.37e-03). ETA=6:19:02, max mem: 20.9 GB 
[12/01 01:55:07 visual_prompt]: 	Training 500/553. train loss: 1.4158,	0.8408 s / batch. (data: 5.24e-04). ETA=6:12:41, max mem: 20.9 GB 
[12/01 01:56:10 visual_prompt]: Epoch 52 / 100: avg data time: 3.77e-01, avg batch time: 1.2186, average train loss: 1.6700
[12/01 01:57:24 visual_prompt]: Inference (val):avg data time: 1.39e-04, avg batch time: 0.3154, average loss: 1.0005
[12/01 01:57:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.52	
[12/01 01:57:24 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[12/01 01:59:30 visual_prompt]: 	Training 100/553. train loss: 2.1284,	0.8437 s / batch. (data: 7.92e-03). ETA=6:11:51, max mem: 20.9 GB 
[12/01 02:01:31 visual_prompt]: 	Training 200/553. train loss: 0.5101,	0.8824 s / batch. (data: 1.06e-02). ETA=6:27:26, max mem: 20.9 GB 
[12/01 02:03:30 visual_prompt]: 	Training 300/553. train loss: 0.8134,	0.8314 s / batch. (data: 4.66e-04). ETA=6:03:38, max mem: 20.9 GB 
[12/01 02:05:31 visual_prompt]: 	Training 400/553. train loss: 0.8493,	1.7898 s / batch. (data: 9.71e-01). ETA=12:59:52, max mem: 20.9 GB 
[12/01 02:07:29 visual_prompt]: 	Training 500/553. train loss: 2.9513,	0.8582 s / batch. (data: 7.06e-03). ETA=6:12:30, max mem: 20.9 GB 
[12/01 02:08:30 visual_prompt]: Epoch 53 / 100: avg data time: 3.64e-01, avg batch time: 1.2043, average train loss: 1.3348
[12/01 02:09:40 visual_prompt]: Inference (val):avg data time: 2.73e-04, avg batch time: 0.3135, average loss: 8.3878
[12/01 02:09:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.33	
[12/01 02:09:40 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[12/01 02:11:47 visual_prompt]: 	Training 100/553. train loss: 0.7603,	0.8521 s / batch. (data: 1.58e-02). ETA=6:07:42, max mem: 20.9 GB 
[12/01 02:13:45 visual_prompt]: 	Training 200/553. train loss: 0.6690,	0.8402 s / batch. (data: 1.06e-03). ETA=6:01:09, max mem: 20.9 GB 
[12/01 02:15:43 visual_prompt]: 	Training 300/553. train loss: 1.4122,	1.6062 s / batch. (data: 7.72e-01). ETA=11:27:45, max mem: 20.9 GB 
[12/01 02:17:44 visual_prompt]: 	Training 400/553. train loss: 2.2229,	0.8486 s / batch. (data: 7.89e-03). ETA=6:01:55, max mem: 20.9 GB 
[12/01 02:19:45 visual_prompt]: 	Training 500/553. train loss: 0.5575,	0.8367 s / batch. (data: 1.18e-02). ETA=5:55:29, max mem: 20.9 GB 
[12/01 02:20:47 visual_prompt]: Epoch 54 / 100: avg data time: 3.66e-01, avg batch time: 1.2058, average train loss: 1.5471
[12/01 02:21:59 visual_prompt]: Inference (val):avg data time: 2.87e-04, avg batch time: 0.3145, average loss: 3.9482
[12/01 02:21:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.72	
[12/01 02:21:59 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[12/01 02:24:05 visual_prompt]: 	Training 100/553. train loss: 0.5613,	0.8527 s / batch. (data: 9.38e-03). ETA=6:00:06, max mem: 20.9 GB 
[12/01 02:26:03 visual_prompt]: 	Training 200/553. train loss: 2.3524,	0.8541 s / batch. (data: 7.84e-03). ETA=5:59:15, max mem: 20.9 GB 
[12/01 02:28:09 visual_prompt]: 	Training 300/553. train loss: 0.6965,	0.8463 s / batch. (data: 1.59e-02). ETA=5:54:34, max mem: 20.9 GB 
[12/01 02:30:12 visual_prompt]: 	Training 400/553. train loss: 1.4366,	2.3233 s / batch. (data: 1.46e+00). ETA=16:09:29, max mem: 20.9 GB 
[12/01 02:32:13 visual_prompt]: 	Training 500/553. train loss: 0.9577,	1.9786 s / batch. (data: 1.15e+00). ETA=13:42:23, max mem: 20.9 GB 
[12/01 02:33:18 visual_prompt]: Epoch 55 / 100: avg data time: 3.84e-01, avg batch time: 1.2278, average train loss: 1.4788
[12/01 02:34:27 visual_prompt]: Inference (val):avg data time: 3.12e-04, avg batch time: 0.3131, average loss: 1.8985
[12/01 02:34:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.87	
[12/01 02:34:27 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[12/01 02:36:37 visual_prompt]: 	Training 100/553. train loss: 1.5224,	0.8286 s / batch. (data: 1.20e-02). ETA=5:42:17, max mem: 20.9 GB 
[12/01 02:38:36 visual_prompt]: 	Training 200/553. train loss: 0.1610,	0.8240 s / batch. (data: 5.37e-04). ETA=5:38:59, max mem: 20.9 GB 
[12/01 02:40:38 visual_prompt]: 	Training 300/553. train loss: 0.9919,	0.8434 s / batch. (data: 7.82e-04). ETA=5:45:35, max mem: 20.9 GB 
[12/01 02:42:39 visual_prompt]: 	Training 400/553. train loss: 0.7902,	0.8503 s / batch. (data: 1.22e-02). ETA=5:47:00, max mem: 20.9 GB 
[12/01 02:44:38 visual_prompt]: 	Training 500/553. train loss: 1.3092,	3.6707 s / batch. (data: 2.84e+00). ETA=1 day, 0:51:49, max mem: 20.9 GB 
[12/01 02:45:38 visual_prompt]: Epoch 56 / 100: avg data time: 3.72e-01, avg batch time: 1.2126, average train loss: 1.6878
[12/01 02:46:49 visual_prompt]: Inference (val):avg data time: 1.05e-04, avg batch time: 0.3116, average loss: 1.0191
[12/01 02:46:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.40	
[12/01 02:46:49 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[12/01 02:48:58 visual_prompt]: 	Training 100/553. train loss: 1.0296,	0.8786 s / batch. (data: 2.20e-02). ETA=5:54:50, max mem: 20.9 GB 
[12/01 02:50:58 visual_prompt]: 	Training 200/553. train loss: 1.5303,	1.2194 s / batch. (data: 3.95e-01). ETA=8:10:27, max mem: 20.9 GB 
[12/01 02:52:58 visual_prompt]: 	Training 300/553. train loss: 1.3516,	0.8545 s / batch. (data: 5.92e-03). ETA=5:42:14, max mem: 20.9 GB 
[12/01 02:54:58 visual_prompt]: 	Training 400/553. train loss: 1.0038,	0.8528 s / batch. (data: 5.97e-04). ETA=5:40:08, max mem: 20.9 GB 
[12/01 02:56:57 visual_prompt]: 	Training 500/553. train loss: 1.0796,	0.8325 s / batch. (data: 5.30e-04). ETA=5:30:40, max mem: 20.9 GB 
[12/01 02:58:00 visual_prompt]: Epoch 57 / 100: avg data time: 3.72e-01, avg batch time: 1.2130, average train loss: 1.2450
[12/01 02:59:09 visual_prompt]: Inference (val):avg data time: 9.38e-05, avg batch time: 0.3112, average loss: 0.6973
[12/01 02:59:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.53	
[12/01 02:59:09 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[12/01 03:01:12 visual_prompt]: 	Training 100/553. train loss: 0.9538,	1.9089 s / batch. (data: 1.10e+00). ETA=12:33:20, max mem: 20.9 GB 
[12/01 03:03:09 visual_prompt]: 	Training 200/553. train loss: 0.7651,	0.8279 s / batch. (data: 6.87e-04). ETA=5:25:21, max mem: 20.9 GB 
[12/01 03:05:12 visual_prompt]: 	Training 300/553. train loss: 0.8147,	0.8293 s / batch. (data: 1.44e-03). ETA=5:24:30, max mem: 20.9 GB 
[12/01 03:07:10 visual_prompt]: 	Training 400/553. train loss: 0.8507,	0.8407 s / batch. (data: 4.50e-04). ETA=5:27:34, max mem: 20.9 GB 
[12/01 03:09:07 visual_prompt]: 	Training 500/553. train loss: 0.7717,	0.8518 s / batch. (data: 1.42e-03). ETA=5:30:29, max mem: 20.9 GB 
[12/01 03:10:07 visual_prompt]: Epoch 58 / 100: avg data time: 3.51e-01, avg batch time: 1.1895, average train loss: 1.2145
[12/01 03:11:17 visual_prompt]: Inference (val):avg data time: 9.72e-05, avg batch time: 0.3106, average loss: 2.6693
[12/01 03:11:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.18	
[12/01 03:11:17 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[12/01 03:13:24 visual_prompt]: 	Training 100/553. train loss: 0.6862,	0.8335 s / batch. (data: 6.12e-03). ETA=5:21:14, max mem: 20.9 GB 
[12/01 03:15:24 visual_prompt]: 	Training 200/553. train loss: 0.0471,	0.8477 s / batch. (data: 7.27e-03). ETA=5:25:18, max mem: 20.9 GB 
[12/01 03:17:21 visual_prompt]: 	Training 300/553. train loss: 1.4638,	0.8870 s / batch. (data: 1.15e-02). ETA=5:38:54, max mem: 20.9 GB 
[12/01 03:19:20 visual_prompt]: 	Training 400/553. train loss: 0.6033,	1.3094 s / batch. (data: 4.77e-01). ETA=8:18:08, max mem: 20.9 GB 
[12/01 03:21:21 visual_prompt]: 	Training 500/553. train loss: 1.2148,	0.8559 s / batch. (data: 1.39e-03). ETA=5:24:11, max mem: 20.9 GB 
[12/01 03:22:21 visual_prompt]: Epoch 59 / 100: avg data time: 3.60e-01, avg batch time: 1.1998, average train loss: 1.2319
[12/01 03:23:31 visual_prompt]: Inference (val):avg data time: 1.18e-04, avg batch time: 0.3116, average loss: 1.1755
[12/01 03:23:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.31	
[12/01 03:23:31 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[12/01 03:25:35 visual_prompt]: 	Training 100/553. train loss: 0.5628,	0.8548 s / batch. (data: 3.69e-02). ETA=5:21:34, max mem: 20.9 GB 
[12/01 03:27:34 visual_prompt]: 	Training 200/553. train loss: 0.7739,	0.8580 s / batch. (data: 1.15e-03). ETA=5:21:20, max mem: 20.9 GB 
[12/01 03:29:32 visual_prompt]: 	Training 300/553. train loss: 0.0036,	2.4261 s / batch. (data: 1.60e+00). ETA=15:04:38, max mem: 20.9 GB 
[12/01 03:31:34 visual_prompt]: 	Training 400/553. train loss: 0.8494,	1.5774 s / batch. (data: 7.49e-01). ETA=9:45:32, max mem: 20.9 GB 
[12/01 03:33:33 visual_prompt]: 	Training 500/553. train loss: 0.6277,	0.8491 s / batch. (data: 1.90e-03). ETA=5:13:47, max mem: 20.9 GB 
[12/01 03:34:36 visual_prompt]: Epoch 60 / 100: avg data time: 3.62e-01, avg batch time: 1.2022, average train loss: 1.0851
[12/01 03:35:46 visual_prompt]: Inference (val):avg data time: 2.24e-04, avg batch time: 0.3134, average loss: 0.9783
[12/01 03:35:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.83	
[12/01 03:35:46 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[12/01 03:37:52 visual_prompt]: 	Training 100/553. train loss: 1.6608,	0.8375 s / batch. (data: 4.62e-04). ETA=5:07:21, max mem: 20.9 GB 
[12/01 03:39:52 visual_prompt]: 	Training 200/553. train loss: 1.1164,	0.8429 s / batch. (data: 1.19e-02). ETA=5:07:56, max mem: 20.9 GB 
[12/01 03:41:51 visual_prompt]: 	Training 300/553. train loss: 1.1284,	1.3065 s / batch. (data: 4.60e-01). ETA=7:55:07, max mem: 20.9 GB 
[12/01 03:43:50 visual_prompt]: 	Training 400/553. train loss: 0.9146,	0.8448 s / batch. (data: 5.69e-03). ETA=5:05:49, max mem: 20.9 GB 
[12/01 03:45:52 visual_prompt]: 	Training 500/553. train loss: 1.5714,	3.7240 s / batch. (data: 2.91e+00). ETA=22:21:53, max mem: 20.9 GB 
[12/01 03:46:51 visual_prompt]: Epoch 61 / 100: avg data time: 3.60e-01, avg batch time: 1.2012, average train loss: 1.0691
[12/01 03:48:02 visual_prompt]: Inference (val):avg data time: 2.44e-04, avg batch time: 0.3128, average loss: 0.6913
[12/01 03:48:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.51	
[12/01 03:48:02 visual_prompt]: Stopping early.
[12/01 03:48:03 visual_prompt]: Rank of current process: 0. World size: 1
[12/01 03:48:03 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/01 03:48:03 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/01 03:48:03 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/01 03:48:03 visual_prompt]: Training with config:
[12/01 03:48:03 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr1.0_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/01 03:48:03 visual_prompt]: Loading training data...
[12/01 03:48:03 visual_prompt]: Constructing mammo-cbis dataset train...
[12/01 03:48:03 visual_prompt]: Loading validation data...
[12/01 03:48:03 visual_prompt]: Constructing mammo-cbis dataset val...
[12/01 03:48:03 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[12/01 03:48:07 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[12/01 03:48:07 visual_prompt]: tuned percent:0.525
[12/01 03:48:07 visual_prompt]: Device used for model: 0
[12/01 03:48:07 visual_prompt]: Setting up Evaluator...
[12/01 03:48:07 visual_prompt]: Setting up Trainer...
[12/01 03:48:07 visual_prompt]: 	Setting up the optimizer...
[12/01 03:48:07 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/01 03:50:12 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8440 s / batch. (data: 5.55e-03). ETA=12:56:30, max mem: 20.9 GB 
[12/01 03:52:10 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8475 s / batch. (data: 7.45e-03). ETA=12:58:18, max mem: 20.9 GB 
[12/01 03:54:14 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.4969 s / batch. (data: 1.67e+00). ETA=1 day, 14:08:49, max mem: 20.9 GB 
[12/01 03:56:11 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8709 s / batch. (data: 1.74e-03). ETA=13:16:52, max mem: 20.9 GB 
[12/01 03:58:19 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8441 s / batch. (data: 4.41e-04). ETA=12:50:58, max mem: 20.9 GB 
[12/01 03:59:22 visual_prompt]: Epoch 1 / 100: avg data time: 3.78e-01, avg batch time: 1.2196, average train loss: 1.5403
[12/01 04:00:32 visual_prompt]: Inference (val):avg data time: 4.09e-04, avg batch time: 0.3125, average loss: 1.5201
[12/01 04:00:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[12/01 04:00:32 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[12/01 04:02:42 visual_prompt]: 	Training 100/553. train loss: 0.7444,	0.8432 s / batch. (data: 5.52e-03). ETA=12:47:58, max mem: 20.9 GB 
[12/01 04:04:44 visual_prompt]: 	Training 200/553. train loss: 0.0235,	1.0151 s / batch. (data: 1.70e-01). ETA=15:22:51, max mem: 20.9 GB 
[12/01 04:06:51 visual_prompt]: 	Training 300/553. train loss: 0.7303,	1.9322 s / batch. (data: 1.08e+00). ETA=1 day, 5:13:21, max mem: 20.9 GB 
[12/01 04:08:50 visual_prompt]: 	Training 400/553. train loss: 1.0344,	0.8488 s / batch. (data: 8.76e-03). ETA=12:48:51, max mem: 20.9 GB 
[12/01 04:10:52 visual_prompt]: 	Training 500/553. train loss: 0.6514,	0.8469 s / batch. (data: 1.98e-03). ETA=12:45:40, max mem: 20.9 GB 
[12/01 04:11:53 visual_prompt]: Epoch 2 / 100: avg data time: 3.88e-01, avg batch time: 1.2317, average train loss: 0.9801
[12/01 04:13:05 visual_prompt]: Inference (val):avg data time: 4.09e-04, avg batch time: 0.3145, average loss: 1.2716
[12/01 04:13:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.56	
[12/01 04:13:05 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[12/01 04:15:07 visual_prompt]: 	Training 100/553. train loss: 1.4377,	0.8466 s / batch. (data: 5.70e-03). ETA=12:43:15, max mem: 20.9 GB 
[12/01 04:17:09 visual_prompt]: 	Training 200/553. train loss: 0.8695,	1.6643 s / batch. (data: 8.50e-01). ETA=1 day, 0:57:42, max mem: 20.9 GB 
[12/01 04:19:07 visual_prompt]: 	Training 300/553. train loss: 0.6381,	0.8652 s / batch. (data: 1.16e-03). ETA=12:57:08, max mem: 20.9 GB 
[12/01 04:21:11 visual_prompt]: 	Training 400/553. train loss: 3.7800,	0.8405 s / batch. (data: 2.46e-02). ETA=12:33:35, max mem: 20.9 GB 
[12/01 04:23:09 visual_prompt]: 	Training 500/553. train loss: 0.7114,	1.8616 s / batch. (data: 1.04e+00). ETA=1 day, 3:45:56, max mem: 20.9 GB 
[12/01 04:24:11 visual_prompt]: Epoch 3 / 100: avg data time: 3.65e-01, avg batch time: 1.2039, average train loss: 1.0415
[12/01 04:25:22 visual_prompt]: Inference (val):avg data time: 1.20e-04, avg batch time: 0.3137, average loss: 0.7156
[12/01 04:25:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.31	rocauc: 59.24	
[12/01 04:25:22 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[12/01 04:27:28 visual_prompt]: 	Training 100/553. train loss: 0.7622,	0.8304 s / batch. (data: 1.11e-03). ETA=12:20:59, max mem: 20.9 GB 
[12/01 04:29:29 visual_prompt]: 	Training 200/553. train loss: 0.5349,	0.8358 s / batch. (data: 4.76e-04). ETA=12:24:25, max mem: 20.9 GB 
[12/01 04:31:29 visual_prompt]: 	Training 300/553. train loss: 0.5132,	2.3442 s / batch. (data: 1.48e+00). ETA=1 day, 10:44:03, max mem: 20.9 GB 
[12/01 04:33:24 visual_prompt]: 	Training 400/553. train loss: 1.0355,	1.9394 s / batch. (data: 1.09e+00). ETA=1 day, 4:40:53, max mem: 20.9 GB 
[12/01 04:35:28 visual_prompt]: 	Training 500/553. train loss: 0.2275,	3.3822 s / batch. (data: 2.56e+00). ETA=2 days, 1:55:35, max mem: 20.9 GB 
[12/01 04:36:31 visual_prompt]: Epoch 4 / 100: avg data time: 3.70e-01, avg batch time: 1.2097, average train loss: 1.0871
[12/01 04:37:40 visual_prompt]: Inference (val):avg data time: 9.89e-05, avg batch time: 0.3129, average loss: 1.6708
[12/01 04:37:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.13	
[12/01 04:37:40 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[12/01 04:39:46 visual_prompt]: 	Training 100/553. train loss: 2.6226,	0.8403 s / batch. (data: 1.19e-03). ETA=12:22:07, max mem: 20.9 GB 
[12/01 04:41:47 visual_prompt]: 	Training 200/553. train loss: 1.5608,	1.9921 s / batch. (data: 1.15e+00). ETA=1 day, 5:16:00, max mem: 20.9 GB 
[12/01 04:43:49 visual_prompt]: 	Training 300/553. train loss: 1.2797,	0.8556 s / batch. (data: 4.12e-03). ETA=12:32:45, max mem: 20.9 GB 
[12/01 04:45:49 visual_prompt]: 	Training 400/553. train loss: 2.5061,	0.8436 s / batch. (data: 1.21e-03). ETA=12:20:45, max mem: 20.9 GB 
[12/01 04:47:50 visual_prompt]: 	Training 500/553. train loss: 1.0235,	0.8393 s / batch. (data: 4.76e-04). ETA=12:15:38, max mem: 20.9 GB 
[12/01 04:48:54 visual_prompt]: Epoch 5 / 100: avg data time: 3.75e-01, avg batch time: 1.2172, average train loss: 1.4804
[12/01 04:50:04 visual_prompt]: Inference (val):avg data time: 2.33e-04, avg batch time: 0.3112, average loss: 3.6272
[12/01 04:50:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.28	
[12/01 04:50:04 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[12/01 04:52:13 visual_prompt]: 	Training 100/553. train loss: 0.5887,	0.8529 s / batch. (data: 6.93e-03). ETA=12:25:21, max mem: 20.9 GB 
[12/01 04:54:13 visual_prompt]: 	Training 200/553. train loss: 3.9305,	0.8190 s / batch. (data: 3.13e-04). ETA=11:54:21, max mem: 20.9 GB 
[12/01 04:56:10 visual_prompt]: 	Training 300/553. train loss: 1.8119,	0.8365 s / batch. (data: 5.46e-04). ETA=12:08:14, max mem: 20.9 GB 
[12/01 04:58:16 visual_prompt]: 	Training 400/553. train loss: 1.6268,	0.8618 s / batch. (data: 9.50e-03). ETA=12:28:49, max mem: 20.9 GB 
[12/01 05:00:15 visual_prompt]: 	Training 500/553. train loss: 3.0176,	1.7595 s / batch. (data: 9.41e-01). ETA=1 day, 1:25:55, max mem: 20.9 GB 
[12/01 05:01:15 visual_prompt]: Epoch 6 / 100: avg data time: 3.73e-01, avg batch time: 1.2126, average train loss: 1.3675
[12/01 05:02:27 visual_prompt]: Inference (val):avg data time: 2.62e-04, avg batch time: 0.3125, average loss: 0.9233
[12/01 05:02:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.50	
[12/01 05:02:27 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[12/01 05:04:32 visual_prompt]: 	Training 100/553. train loss: 1.1389,	0.8361 s / batch. (data: 4.72e-04). ETA=12:02:55, max mem: 20.9 GB 
[12/01 05:06:32 visual_prompt]: 	Training 200/553. train loss: 0.5374,	0.8993 s / batch. (data: 3.01e-02). ETA=12:56:06, max mem: 20.9 GB 
[12/01 05:08:36 visual_prompt]: 	Training 300/553. train loss: 0.6987,	2.6685 s / batch. (data: 1.85e+00). ETA=1 day, 14:18:31, max mem: 20.9 GB 
[12/01 05:10:34 visual_prompt]: 	Training 400/553. train loss: 0.8380,	2.6000 s / batch. (data: 1.78e+00). ETA=1 day, 13:15:12, max mem: 20.9 GB 
[12/01 05:12:34 visual_prompt]: 	Training 500/553. train loss: 1.3916,	0.8680 s / batch. (data: 1.63e-03). ETA=12:24:43, max mem: 20.9 GB 
[12/01 05:13:33 visual_prompt]: Epoch 7 / 100: avg data time: 3.65e-01, avg batch time: 1.2041, average train loss: 1.1268
[12/01 05:14:45 visual_prompt]: Inference (val):avg data time: 1.09e-04, avg batch time: 0.3125, average loss: 0.6818
[12/01 05:14:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 61.98	
[12/01 05:14:45 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[12/01 05:16:47 visual_prompt]: 	Training 100/553. train loss: 2.6913,	0.8434 s / batch. (data: 1.19e-02). ETA=12:01:32, max mem: 20.9 GB 
[12/01 05:18:50 visual_prompt]: 	Training 200/553. train loss: 1.1695,	0.8354 s / batch. (data: 1.28e-03). ETA=11:53:15, max mem: 20.9 GB 
[12/01 05:20:49 visual_prompt]: 	Training 300/553. train loss: 0.5651,	0.8476 s / batch. (data: 1.38e-03). ETA=12:02:16, max mem: 20.9 GB 
[12/01 05:22:49 visual_prompt]: 	Training 400/553. train loss: 0.8290,	0.8759 s / batch. (data: 1.72e-03). ETA=12:24:57, max mem: 20.9 GB 
[12/01 05:24:49 visual_prompt]: 	Training 500/553. train loss: 1.5774,	2.2199 s / batch. (data: 1.36e+00). ETA=1 day, 7:24:19, max mem: 20.9 GB 
[12/01 05:25:51 visual_prompt]: Epoch 8 / 100: avg data time: 3.64e-01, avg batch time: 1.2052, average train loss: 1.4651
[12/01 05:27:03 visual_prompt]: Inference (val):avg data time: 1.19e-04, avg batch time: 0.3115, average loss: 1.0596
[12/01 05:27:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.58	
[12/01 05:27:03 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[12/01 05:29:06 visual_prompt]: 	Training 100/553. train loss: 0.0075,	0.8357 s / batch. (data: 1.19e-02). ETA=11:47:11, max mem: 20.9 GB 
[12/01 05:31:07 visual_prompt]: 	Training 200/553. train loss: 0.5798,	0.8245 s / batch. (data: 4.83e-04). ETA=11:36:24, max mem: 20.9 GB 
[12/01 05:33:06 visual_prompt]: 	Training 300/553. train loss: 0.9115,	2.2043 s / batch. (data: 1.35e+00). ETA=1 day, 6:58:05, max mem: 20.9 GB 
[12/01 05:35:07 visual_prompt]: 	Training 400/553. train loss: 1.0908,	0.8385 s / batch. (data: 1.15e-03). ETA=11:45:22, max mem: 20.9 GB 
[12/01 05:37:06 visual_prompt]: 	Training 500/553. train loss: 0.8527,	0.9766 s / batch. (data: 1.57e-01). ETA=13:39:54, max mem: 20.9 GB 
[12/01 05:38:08 visual_prompt]: Epoch 9 / 100: avg data time: 3.64e-01, avg batch time: 1.2027, average train loss: 1.1514
[12/01 05:39:17 visual_prompt]: Inference (val):avg data time: 2.93e-04, avg batch time: 0.3141, average loss: 0.8764
[12/01 05:39:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.72	
[12/01 05:39:17 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[12/01 05:41:25 visual_prompt]: 	Training 100/553. train loss: 2.6237,	0.8652 s / batch. (data: 1.57e-03). ETA=12:04:11, max mem: 20.9 GB 
[12/01 05:43:23 visual_prompt]: 	Training 200/553. train loss: 1.0801,	0.8234 s / batch. (data: 1.82e-03). ETA=11:27:49, max mem: 20.9 GB 
[12/01 05:45:26 visual_prompt]: 	Training 300/553. train loss: 0.8928,	1.2442 s / batch. (data: 4.10e-01). ETA=17:17:18, max mem: 20.9 GB 
[12/01 05:47:24 visual_prompt]: 	Training 400/553. train loss: 3.2987,	1.0240 s / batch. (data: 1.69e-01). ETA=14:12:01, max mem: 20.9 GB 
[12/01 05:49:24 visual_prompt]: 	Training 500/553. train loss: 0.9837,	1.7507 s / batch. (data: 8.86e-01). ETA=1 day, 0:13:44, max mem: 20.9 GB 
[12/01 05:50:26 visual_prompt]: Epoch 10 / 100: avg data time: 3.69e-01, avg batch time: 1.2082, average train loss: 1.7488
[12/01 05:51:37 visual_prompt]: Inference (val):avg data time: 1.11e-04, avg batch time: 0.3129, average loss: 0.7217
[12/01 05:51:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.37	
[12/01 05:51:37 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[12/01 05:53:44 visual_prompt]: 	Training 100/553. train loss: 0.7583,	0.8646 s / batch. (data: 2.23e-02). ETA=11:55:44, max mem: 20.9 GB 
[12/01 05:55:46 visual_prompt]: 	Training 200/553. train loss: 0.4333,	0.8559 s / batch. (data: 7.79e-03). ETA=11:47:05, max mem: 20.9 GB 
[12/01 05:57:45 visual_prompt]: 	Training 300/553. train loss: 0.0232,	3.0316 s / batch. (data: 2.18e+00). ETA=1 day, 17:39:33, max mem: 20.9 GB 
[12/01 05:59:43 visual_prompt]: 	Training 400/553. train loss: 0.6679,	0.8590 s / batch. (data: 1.19e-03). ETA=11:46:49, max mem: 20.9 GB 
[12/01 06:01:41 visual_prompt]: 	Training 500/553. train loss: 0.6420,	0.8547 s / batch. (data: 1.08e-02). ETA=11:41:53, max mem: 20.9 GB 
[12/01 06:02:44 visual_prompt]: Epoch 11 / 100: avg data time: 3.65e-01, avg batch time: 1.2048, average train loss: 1.3496
[12/01 06:03:55 visual_prompt]: Inference (val):avg data time: 1.23e-04, avg batch time: 0.3134, average loss: 2.7328
[12/01 06:03:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.00	
[12/01 06:03:55 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[12/01 06:06:00 visual_prompt]: 	Training 100/553. train loss: 0.7005,	0.8581 s / batch. (data: 9.57e-03). ETA=11:42:28, max mem: 20.9 GB 
[12/01 06:08:05 visual_prompt]: 	Training 200/553. train loss: 0.5731,	0.8363 s / batch. (data: 4.71e-03). ETA=11:23:12, max mem: 20.9 GB 
[12/01 06:10:04 visual_prompt]: 	Training 300/553. train loss: 0.7164,	0.8280 s / batch. (data: 6.48e-04). ETA=11:15:01, max mem: 20.9 GB 
[12/01 06:12:08 visual_prompt]: 	Training 400/553. train loss: 1.8411,	0.8437 s / batch. (data: 1.20e-02). ETA=11:26:25, max mem: 20.9 GB 
[12/01 06:14:07 visual_prompt]: 	Training 500/553. train loss: 7.3812,	0.8325 s / batch. (data: 1.13e-03). ETA=11:15:55, max mem: 20.9 GB 
[12/01 06:15:01 visual_prompt]: Epoch 12 / 100: avg data time: 3.65e-01, avg batch time: 1.2044, average train loss: 1.7641
[12/01 06:15:56 visual_prompt]: Inference (val):avg data time: 2.07e-04, avg batch time: 0.3094, average loss: 5.1254
[12/01 06:15:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.50	
[12/01 06:15:56 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[12/01 06:17:38 visual_prompt]: 	Training 100/553. train loss: 1.0007,	0.8400 s / batch. (data: 4.84e-04). ETA=11:19:53, max mem: 20.9 GB 
[12/01 06:19:11 visual_prompt]: 	Training 200/553. train loss: 0.7884,	0.8368 s / batch. (data: 1.07e-02). ETA=11:15:53, max mem: 20.9 GB 
[12/01 06:20:49 visual_prompt]: 	Training 300/553. train loss: 1.5693,	1.6719 s / batch. (data: 8.35e-01). ETA=22:27:39, max mem: 20.9 GB 
[12/01 06:22:25 visual_prompt]: 	Training 400/553. train loss: 2.8366,	0.8440 s / batch. (data: 5.04e-04). ETA=11:18:53, max mem: 20.9 GB 
[12/01 06:24:02 visual_prompt]: 	Training 500/553. train loss: 3.0108,	0.8576 s / batch. (data: 1.20e-02). ETA=11:28:23, max mem: 20.9 GB 
[12/01 06:24:53 visual_prompt]: Epoch 13 / 100: avg data time: 1.34e-01, avg batch time: 0.9706, average train loss: 1.9001
[12/01 06:25:48 visual_prompt]: Inference (val):avg data time: 4.90e-05, avg batch time: 0.3124, average loss: 1.2067
[12/01 06:25:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.46	
[12/01 06:25:48 visual_prompt]: Best epoch 13: best metric: -1.207
[12/01 06:25:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[12/01 06:27:30 visual_prompt]: 	Training 100/553. train loss: 1.5523,	0.8767 s / batch. (data: 1.66e-02). ETA=11:41:29, max mem: 20.9 GB 
[12/01 06:29:06 visual_prompt]: 	Training 200/553. train loss: 0.0387,	0.9160 s / batch. (data: 4.05e-02). ETA=12:11:27, max mem: 20.9 GB 
[12/01 06:30:43 visual_prompt]: 	Training 300/553. train loss: 0.9315,	0.8360 s / batch. (data: 4.63e-04). ETA=11:06:11, max mem: 20.9 GB 
[12/01 06:32:19 visual_prompt]: 	Training 400/553. train loss: 0.7454,	0.8719 s / batch. (data: 1.19e-02). ETA=11:33:18, max mem: 20.9 GB 
[12/01 06:33:56 visual_prompt]: 	Training 500/553. train loss: 1.9779,	0.8336 s / batch. (data: 5.55e-03). ETA=11:01:28, max mem: 20.9 GB 
[12/01 06:34:45 visual_prompt]: Epoch 14 / 100: avg data time: 1.35e-01, avg batch time: 0.9713, average train loss: 1.2273
[12/01 06:35:40 visual_prompt]: Inference (val):avg data time: 5.26e-05, avg batch time: 0.3113, average loss: 0.7427
[12/01 06:35:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.20	
[12/01 06:35:40 visual_prompt]: Best epoch 14: best metric: -0.743
[12/01 06:35:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[12/01 06:37:21 visual_prompt]: 	Training 100/553. train loss: 1.0834,	0.8271 s / batch. (data: 5.49e-04). ETA=10:54:13, max mem: 20.9 GB 
[12/01 06:38:56 visual_prompt]: 	Training 200/553. train loss: 4.6378,	0.8440 s / batch. (data: 5.30e-04). ETA=11:06:07, max mem: 20.9 GB 
[12/01 06:40:35 visual_prompt]: 	Training 300/553. train loss: 4.5705,	0.8600 s / batch. (data: 1.31e-03). ETA=11:17:20, max mem: 20.9 GB 
[12/01 06:42:09 visual_prompt]: 	Training 400/553. train loss: 0.6600,	1.0320 s / batch. (data: 1.81e-01). ETA=13:31:05, max mem: 20.9 GB 
[12/01 06:43:47 visual_prompt]: 	Training 500/553. train loss: 1.6384,	0.8279 s / batch. (data: 5.61e-04). ETA=10:49:18, max mem: 20.9 GB 
[12/01 06:44:38 visual_prompt]: Epoch 15 / 100: avg data time: 1.35e-01, avg batch time: 0.9716, average train loss: 2.1151
[12/01 06:45:33 visual_prompt]: Inference (val):avg data time: 4.62e-05, avg batch time: 0.3108, average loss: 1.9401
[12/01 06:45:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.86	
[12/01 06:45:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[12/01 06:47:13 visual_prompt]: 	Training 100/553. train loss: 0.5644,	0.8575 s / batch. (data: 1.06e-02). ETA=11:10:19, max mem: 20.9 GB 
[12/01 06:48:50 visual_prompt]: 	Training 200/553. train loss: 2.3663,	0.8280 s / batch. (data: 4.43e-04). ETA=10:45:55, max mem: 20.9 GB 
[12/01 06:50:27 visual_prompt]: 	Training 300/553. train loss: 1.1997,	0.8300 s / batch. (data: 5.35e-04). ETA=10:46:07, max mem: 20.9 GB 
[12/01 06:52:03 visual_prompt]: 	Training 400/553. train loss: 1.6563,	0.8263 s / batch. (data: 7.91e-03). ETA=10:41:49, max mem: 20.9 GB 
[12/01 06:53:39 visual_prompt]: 	Training 500/553. train loss: 0.5822,	0.8920 s / batch. (data: 5.84e-02). ETA=11:31:23, max mem: 20.9 GB 
[12/01 06:54:30 visual_prompt]: Epoch 16 / 100: avg data time: 1.35e-01, avg batch time: 0.9706, average train loss: 1.4400
[12/01 06:55:25 visual_prompt]: Inference (val):avg data time: 4.64e-05, avg batch time: 0.3100, average loss: 0.7450
[12/01 06:55:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.94	
[12/01 06:55:25 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[12/01 06:57:05 visual_prompt]: 	Training 100/553. train loss: 1.6136,	0.8193 s / batch. (data: 5.17e-04). ETA=10:32:58, max mem: 20.9 GB 
[12/01 06:58:43 visual_prompt]: 	Training 200/553. train loss: 3.2109,	0.8315 s / batch. (data: 5.31e-04). ETA=10:40:56, max mem: 20.9 GB 
[12/01 07:00:19 visual_prompt]: 	Training 300/553. train loss: 2.5880,	0.8440 s / batch. (data: 3.96e-04). ETA=10:49:11, max mem: 20.9 GB 
[12/01 07:01:55 visual_prompt]: 	Training 400/553. train loss: 0.9729,	1.1785 s / batch. (data: 3.44e-01). ETA=15:04:34, max mem: 20.9 GB 
[12/01 07:03:32 visual_prompt]: 	Training 500/553. train loss: 1.1869,	1.5524 s / batch. (data: 7.06e-01). ETA=19:48:56, max mem: 20.9 GB 
[12/01 07:04:23 visual_prompt]: Epoch 17 / 100: avg data time: 1.35e-01, avg batch time: 0.9721, average train loss: 1.4579
[12/01 07:05:18 visual_prompt]: Inference (val):avg data time: 1.64e-04, avg batch time: 0.3108, average loss: 1.4424
[12/01 07:05:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.36	
[12/01 07:05:18 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[12/01 07:06:59 visual_prompt]: 	Training 100/553. train loss: 0.6193,	0.8348 s / batch. (data: 4.16e-03). ETA=10:37:10, max mem: 20.9 GB 
[12/01 07:08:38 visual_prompt]: 	Training 200/553. train loss: 1.5820,	0.8425 s / batch. (data: 8.24e-04). ETA=10:41:39, max mem: 20.9 GB 
[12/01 07:10:14 visual_prompt]: 	Training 300/553. train loss: 0.5427,	0.8480 s / batch. (data: 4.71e-04). ETA=10:44:26, max mem: 20.9 GB 
[12/01 07:11:51 visual_prompt]: 	Training 400/553. train loss: 1.4705,	0.8320 s / batch. (data: 5.35e-04). ETA=10:30:54, max mem: 20.9 GB 
[12/01 07:13:26 visual_prompt]: 	Training 500/553. train loss: 0.7507,	0.8392 s / batch. (data: 1.07e-02). ETA=10:34:57, max mem: 20.9 GB 
[12/01 07:14:16 visual_prompt]: Epoch 18 / 100: avg data time: 1.37e-01, avg batch time: 0.9725, average train loss: 1.3950
[12/01 07:15:12 visual_prompt]: Inference (val):avg data time: 5.36e-05, avg batch time: 0.3114, average loss: 0.8686
[12/01 07:15:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.83	
[12/01 07:15:12 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[12/01 07:16:52 visual_prompt]: 	Training 100/553. train loss: 0.5782,	1.2640 s / batch. (data: 4.26e-01). ETA=15:53:10, max mem: 20.9 GB 
[12/01 07:18:28 visual_prompt]: 	Training 200/553. train loss: 0.7036,	0.8303 s / batch. (data: 5.42e-03). ETA=10:24:45, max mem: 20.9 GB 
[12/01 07:20:06 visual_prompt]: 	Training 300/553. train loss: 3.1288,	0.8621 s / batch. (data: 5.66e-03). ETA=10:47:14, max mem: 20.9 GB 
[12/01 07:21:43 visual_prompt]: 	Training 400/553. train loss: 0.5579,	0.8376 s / batch. (data: 5.22e-04). ETA=10:27:24, max mem: 20.9 GB 
[12/01 07:23:17 visual_prompt]: 	Training 500/553. train loss: 0.6502,	0.8337 s / batch. (data: 9.65e-03). ETA=10:23:08, max mem: 20.9 GB 
[12/01 07:24:07 visual_prompt]: Epoch 19 / 100: avg data time: 1.32e-01, avg batch time: 0.9677, average train loss: 1.2007
[12/01 07:25:02 visual_prompt]: Inference (val):avg data time: 5.29e-05, avg batch time: 0.3113, average loss: 2.6943
[12/01 07:25:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.90	
[12/01 07:25:02 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[12/01 07:26:41 visual_prompt]: 	Training 100/553. train loss: 0.5632,	0.8299 s / batch. (data: 5.36e-04). ETA=10:18:09, max mem: 20.9 GB 
[12/01 07:28:18 visual_prompt]: 	Training 200/553. train loss: 0.8049,	0.8202 s / batch. (data: 4.79e-04). ETA=10:09:35, max mem: 20.9 GB 
[12/01 07:29:55 visual_prompt]: 	Training 300/553. train loss: 3.2648,	0.8521 s / batch. (data: 9.81e-04). ETA=10:31:53, max mem: 20.9 GB 
[12/01 07:31:33 visual_prompt]: 	Training 400/553. train loss: 0.8737,	0.8240 s / batch. (data: 3.78e-04). ETA=10:09:40, max mem: 20.9 GB 
[12/01 07:33:09 visual_prompt]: 	Training 500/553. train loss: 1.3796,	0.8480 s / batch. (data: 5.07e-04). ETA=10:25:59, max mem: 20.9 GB 
[12/01 07:34:01 visual_prompt]: Epoch 20 / 100: avg data time: 1.39e-01, avg batch time: 0.9741, average train loss: 1.4975
[12/01 07:34:56 visual_prompt]: Inference (val):avg data time: 4.61e-05, avg batch time: 0.3092, average loss: 0.8022
[12/01 07:34:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.56	
[12/01 07:34:56 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[12/01 07:36:39 visual_prompt]: 	Training 100/553. train loss: 1.7491,	0.8381 s / batch. (data: 1.69e-02). ETA=10:16:34, max mem: 20.9 GB 
[12/01 07:38:14 visual_prompt]: 	Training 200/553. train loss: 1.3258,	0.8349 s / batch. (data: 5.03e-04). ETA=10:12:50, max mem: 20.9 GB 
[12/01 07:39:51 visual_prompt]: 	Training 300/553. train loss: 1.9252,	0.9015 s / batch. (data: 6.65e-02). ETA=11:00:11, max mem: 20.9 GB 
[12/01 07:41:26 visual_prompt]: 	Training 400/553. train loss: 1.4301,	0.9381 s / batch. (data: 1.01e-01). ETA=11:25:24, max mem: 20.9 GB 
[12/01 07:43:04 visual_prompt]: 	Training 500/553. train loss: 0.6992,	0.8335 s / batch. (data: 5.60e-03). ETA=10:07:36, max mem: 20.9 GB 
[12/01 07:43:54 visual_prompt]: Epoch 21 / 100: avg data time: 1.39e-01, avg batch time: 0.9730, average train loss: 1.3935
[12/01 07:44:49 visual_prompt]: Inference (val):avg data time: 4.69e-05, avg batch time: 0.3113, average loss: 1.4628
[12/01 07:44:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.11	
[12/01 07:44:49 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[12/01 07:46:29 visual_prompt]: 	Training 100/553. train loss: 0.7020,	0.8233 s / batch. (data: 4.62e-04). ETA=9:58:07, max mem: 20.9 GB 
[12/01 07:48:06 visual_prompt]: 	Training 200/553. train loss: 0.6173,	0.8280 s / batch. (data: 4.47e-04). ETA=10:00:07, max mem: 20.9 GB 
[12/01 07:49:40 visual_prompt]: 	Training 300/553. train loss: 0.0023,	0.8400 s / batch. (data: 5.43e-03). ETA=10:07:23, max mem: 20.9 GB 
[12/01 07:51:18 visual_prompt]: 	Training 400/553. train loss: 0.7821,	0.8218 s / batch. (data: 5.21e-04). ETA=9:52:53, max mem: 20.9 GB 
[12/01 07:52:55 visual_prompt]: 	Training 500/553. train loss: 0.5836,	0.8229 s / batch. (data: 4.96e-04). ETA=9:52:16, max mem: 20.9 GB 
[12/01 07:53:46 visual_prompt]: Epoch 22 / 100: avg data time: 1.35e-01, avg batch time: 0.9702, average train loss: 1.7404
[12/01 07:54:41 visual_prompt]: Inference (val):avg data time: 5.02e-05, avg batch time: 0.3126, average loss: 0.6882
[12/01 07:54:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.23	
[12/01 07:54:41 visual_prompt]: Best epoch 22: best metric: -0.688
[12/01 07:54:41 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[12/01 07:56:23 visual_prompt]: 	Training 100/553. train loss: 0.7659,	0.8560 s / batch. (data: 5.52e-04). ETA=10:13:56, max mem: 20.9 GB 
[12/01 07:58:00 visual_prompt]: 	Training 200/553. train loss: 1.9202,	0.8280 s / batch. (data: 4.85e-04). ETA=9:52:30, max mem: 20.9 GB 
[12/01 07:59:39 visual_prompt]: 	Training 300/553. train loss: 0.6428,	0.8744 s / batch. (data: 6.13e-03). ETA=10:24:13, max mem: 20.9 GB 
[12/01 08:01:14 visual_prompt]: 	Training 400/553. train loss: 0.9562,	0.8772 s / batch. (data: 1.14e-02). ETA=10:24:47, max mem: 20.9 GB 
[12/01 08:02:48 visual_prompt]: 	Training 500/553. train loss: 0.7178,	0.8510 s / batch. (data: 1.19e-02). ETA=10:04:39, max mem: 20.9 GB 
[12/01 08:03:39 visual_prompt]: Epoch 23 / 100: avg data time: 1.36e-01, avg batch time: 0.9718, average train loss: 1.2195
[12/01 08:04:34 visual_prompt]: Inference (val):avg data time: 5.13e-05, avg batch time: 0.3117, average loss: 0.7635
[12/01 08:04:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.16	
[12/01 08:04:34 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[12/01 08:06:12 visual_prompt]: 	Training 100/553. train loss: 3.1249,	0.8258 s / batch. (data: 5.43e-04). ETA=9:44:40, max mem: 20.9 GB 
[12/01 08:07:48 visual_prompt]: 	Training 200/553. train loss: 0.9423,	0.8560 s / batch. (data: 5.31e-04). ETA=10:04:37, max mem: 20.9 GB 
[12/01 08:09:25 visual_prompt]: 	Training 300/553. train loss: 0.7538,	0.8282 s / batch. (data: 7.91e-03). ETA=9:43:35, max mem: 20.9 GB 
[12/01 08:11:02 visual_prompt]: 	Training 400/553. train loss: 1.4100,	0.8464 s / batch. (data: 1.07e-02). ETA=9:55:02, max mem: 20.9 GB 
[12/01 08:12:41 visual_prompt]: 	Training 500/553. train loss: 1.5143,	0.8284 s / batch. (data: 3.74e-04). ETA=9:41:01, max mem: 20.9 GB 
[12/01 08:13:31 visual_prompt]: Epoch 24 / 100: avg data time: 1.35e-01, avg batch time: 0.9719, average train loss: 1.4541
[12/01 08:14:27 visual_prompt]: Inference (val):avg data time: 5.15e-05, avg batch time: 0.3105, average loss: 2.3475
[12/01 08:14:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.11	
[12/01 08:14:27 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[12/01 08:16:11 visual_prompt]: 	Training 100/553. train loss: 1.6118,	0.8228 s / batch. (data: 4.96e-04). ETA=9:34:59, max mem: 20.9 GB 
[12/01 08:17:45 visual_prompt]: 	Training 200/553. train loss: 1.5407,	0.8431 s / batch. (data: 4.54e-04). ETA=9:47:43, max mem: 20.9 GB 
[12/01 08:19:21 visual_prompt]: 	Training 300/553. train loss: 2.3506,	0.8292 s / batch. (data: 4.58e-04). ETA=9:36:39, max mem: 20.9 GB 
[12/01 08:20:57 visual_prompt]: 	Training 400/553. train loss: 0.6104,	1.1800 s / batch. (data: 3.45e-01). ETA=13:38:40, max mem: 20.9 GB 
[12/01 08:22:34 visual_prompt]: 	Training 500/553. train loss: 0.6991,	1.5615 s / batch. (data: 7.33e-01). ETA=18:00:48, max mem: 20.9 GB 
[12/01 08:23:25 visual_prompt]: Epoch 25 / 100: avg data time: 1.38e-01, avg batch time: 0.9736, average train loss: 1.9322
[12/01 08:24:20 visual_prompt]: Inference (val):avg data time: 4.70e-05, avg batch time: 0.3107, average loss: 4.2951
[12/01 08:24:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.22	
[12/01 08:24:20 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[12/01 08:26:01 visual_prompt]: 	Training 100/553. train loss: 1.3717,	0.8559 s / batch. (data: 5.85e-04). ETA=9:50:12, max mem: 20.9 GB 
[12/01 08:27:39 visual_prompt]: 	Training 200/553. train loss: 4.0646,	1.6691 s / batch. (data: 8.39e-01). ETA=19:08:10, max mem: 20.9 GB 
[12/01 08:29:16 visual_prompt]: 	Training 300/553. train loss: 0.0216,	0.8222 s / batch. (data: 1.04e-03). ETA=9:24:16, max mem: 20.9 GB 
[12/01 08:30:52 visual_prompt]: 	Training 400/553. train loss: 2.7905,	0.8195 s / batch. (data: 5.10e-04). ETA=9:21:01, max mem: 20.9 GB 
[12/01 08:32:27 visual_prompt]: 	Training 500/553. train loss: 1.7160,	0.8337 s / batch. (data: 2.97e-04). ETA=9:29:20, max mem: 20.9 GB 
[12/01 08:33:17 visual_prompt]: Epoch 26 / 100: avg data time: 1.36e-01, avg batch time: 0.9715, average train loss: 1.4642
[12/01 08:34:13 visual_prompt]: Inference (val):avg data time: 2.22e-04, avg batch time: 0.3108, average loss: 0.6935
[12/01 08:34:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 58.71	
[12/01 08:34:13 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[12/01 08:35:54 visual_prompt]: 	Training 100/553. train loss: 1.0322,	0.8404 s / batch. (data: 3.95e-04). ETA=9:31:45, max mem: 20.9 GB 
[12/01 08:37:31 visual_prompt]: 	Training 200/553. train loss: 4.8499,	0.9973 s / batch. (data: 1.57e-01). ETA=11:16:50, max mem: 20.9 GB 
[12/01 08:39:07 visual_prompt]: 	Training 300/553. train loss: 0.8878,	0.8257 s / batch. (data: 5.40e-04). ETA=9:19:00, max mem: 20.9 GB 
[12/01 08:40:45 visual_prompt]: 	Training 400/553. train loss: 0.6557,	0.8207 s / batch. (data: 4.67e-04). ETA=9:14:14, max mem: 20.9 GB 
[12/01 08:42:22 visual_prompt]: 	Training 500/553. train loss: 0.9570,	0.8385 s / batch. (data: 6.47e-03). ETA=9:24:53, max mem: 20.9 GB 
[12/01 08:43:11 visual_prompt]: Epoch 27 / 100: avg data time: 1.37e-01, avg batch time: 0.9731, average train loss: 1.2158
[12/01 08:44:06 visual_prompt]: Inference (val):avg data time: 5.37e-05, avg batch time: 0.3106, average loss: 0.6990
[12/01 08:44:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.84	
[12/01 08:44:06 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[12/01 08:45:46 visual_prompt]: 	Training 100/553. train loss: 1.4399,	0.9960 s / batch. (data: 1.44e-01). ETA=11:08:27, max mem: 20.9 GB 
[12/01 08:47:23 visual_prompt]: 	Training 200/553. train loss: 4.6834,	0.8400 s / batch. (data: 4.67e-04). ETA=9:22:22, max mem: 20.9 GB 
[12/01 08:49:00 visual_prompt]: 	Training 300/553. train loss: 0.4759,	1.4005 s / batch. (data: 5.76e-01). ETA=15:35:16, max mem: 20.9 GB 
[12/01 08:50:37 visual_prompt]: 	Training 400/553. train loss: 0.7004,	0.8720 s / batch. (data: 1.24e-03). ETA=9:40:54, max mem: 20.9 GB 
[12/01 08:52:12 visual_prompt]: 	Training 500/553. train loss: 2.9759,	0.8467 s / batch. (data: 5.29e-04). ETA=9:22:37, max mem: 20.9 GB 
[12/01 08:53:02 visual_prompt]: Epoch 28 / 100: avg data time: 1.34e-01, avg batch time: 0.9698, average train loss: 1.2730
[12/01 08:53:57 visual_prompt]: Inference (val):avg data time: 5.21e-05, avg batch time: 0.3120, average loss: 0.6871
[12/01 08:53:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.59	
[12/01 08:53:57 visual_prompt]: Best epoch 28: best metric: -0.687
[12/01 08:53:57 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[12/01 08:55:43 visual_prompt]: 	Training 100/553. train loss: 1.8330,	0.8287 s / batch. (data: 5.53e-03). ETA=9:08:32, max mem: 20.9 GB 
[12/01 08:57:19 visual_prompt]: 	Training 200/553. train loss: 0.7838,	1.7697 s / batch. (data: 9.24e-01). ETA=19:28:28, max mem: 20.9 GB 
[12/01 08:58:54 visual_prompt]: 	Training 300/553. train loss: 0.8803,	0.8720 s / batch. (data: 1.24e-03). ETA=9:34:16, max mem: 20.9 GB 
[12/01 09:00:27 visual_prompt]: 	Training 400/553. train loss: 1.4903,	0.8253 s / batch. (data: 5.44e-03). ETA=9:02:09, max mem: 20.9 GB 
[12/01 09:02:05 visual_prompt]: 	Training 500/553. train loss: 0.8697,	0.8399 s / batch. (data: 5.02e-04). ETA=9:10:22, max mem: 20.9 GB 
[12/01 09:02:55 visual_prompt]: Epoch 29 / 100: avg data time: 1.36e-01, avg batch time: 0.9712, average train loss: 1.2836
[12/01 09:03:49 visual_prompt]: Inference (val):avg data time: 2.07e-04, avg batch time: 0.3115, average loss: 1.4331
[12/01 09:03:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.20	
[12/01 09:03:49 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[12/01 09:05:29 visual_prompt]: 	Training 100/553. train loss: 0.8284,	0.8465 s / batch. (data: 3.60e-04). ETA=9:12:29, max mem: 20.9 GB 
[12/01 09:07:05 visual_prompt]: 	Training 200/553. train loss: 0.9494,	0.8520 s / batch. (data: 4.55e-04). ETA=9:14:42, max mem: 20.9 GB 
[12/01 09:08:41 visual_prompt]: 	Training 300/553. train loss: 0.0349,	1.6600 s / batch. (data: 8.18e-01). ETA=17:57:57, max mem: 20.9 GB 
[12/01 09:10:19 visual_prompt]: 	Training 400/553. train loss: 0.7079,	1.0410 s / batch. (data: 2.12e-01). ETA=11:14:15, max mem: 20.9 GB 
[12/01 09:11:55 visual_prompt]: 	Training 500/553. train loss: 0.5656,	1.3262 s / batch. (data: 4.86e-01). ETA=14:16:48, max mem: 20.9 GB 
[12/01 09:12:47 visual_prompt]: Epoch 30 / 100: avg data time: 1.37e-01, avg batch time: 0.9721, average train loss: 1.1249
[12/01 09:13:42 visual_prompt]: Inference (val):avg data time: 5.10e-05, avg batch time: 0.3117, average loss: 0.7190
[12/01 09:13:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.52	
[12/01 09:13:42 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[12/01 09:15:25 visual_prompt]: 	Training 100/553. train loss: 0.9726,	0.8355 s / batch. (data: 4.95e-04). ETA=8:57:37, max mem: 20.9 GB 
[12/01 09:17:03 visual_prompt]: 	Training 200/553. train loss: 2.1719,	0.8480 s / batch. (data: 1.65e-03). ETA=9:04:17, max mem: 20.9 GB 
[12/01 09:18:38 visual_prompt]: 	Training 300/553. train loss: 1.9726,	0.8783 s / batch. (data: 2.47e-02). ETA=9:22:15, max mem: 20.9 GB 
[12/01 09:20:13 visual_prompt]: 	Training 400/553. train loss: 0.5531,	0.8262 s / batch. (data: 4.89e-04). ETA=8:47:33, max mem: 20.9 GB 
[12/01 09:21:50 visual_prompt]: 	Training 500/553. train loss: 0.8325,	0.8441 s / batch. (data: 4.52e-04). ETA=8:57:31, max mem: 20.9 GB 
[12/01 09:22:40 visual_prompt]: Epoch 31 / 100: avg data time: 1.36e-01, avg batch time: 0.9714, average train loss: 1.2540
[12/01 09:23:35 visual_prompt]: Inference (val):avg data time: 4.84e-05, avg batch time: 0.3087, average loss: 0.7238
[12/01 09:23:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.56	
[12/01 09:23:35 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[12/01 09:25:16 visual_prompt]: 	Training 100/553. train loss: 0.6575,	0.8469 s / batch. (data: 5.80e-03). ETA=8:57:09, max mem: 20.9 GB 
[12/01 09:26:52 visual_prompt]: 	Training 200/553. train loss: 0.5992,	0.8455 s / batch. (data: 5.52e-03). ETA=8:54:51, max mem: 20.9 GB 
[12/01 09:28:33 visual_prompt]: 	Training 300/553. train loss: 1.3649,	0.8221 s / batch. (data: 3.68e-03). ETA=8:38:40, max mem: 20.9 GB 
[12/01 09:30:09 visual_prompt]: 	Training 400/553. train loss: 1.0292,	0.8600 s / batch. (data: 4.87e-04). ETA=9:01:10, max mem: 20.9 GB 
[12/01 09:31:43 visual_prompt]: 	Training 500/553. train loss: 0.7414,	0.8442 s / batch. (data: 1.22e-02). ETA=8:49:49, max mem: 20.9 GB 
[12/01 09:32:32 visual_prompt]: Epoch 32 / 100: avg data time: 1.37e-01, avg batch time: 0.9723, average train loss: 1.0378
[12/01 09:33:28 visual_prompt]: Inference (val):avg data time: 4.90e-05, avg batch time: 0.3110, average loss: 1.1125
[12/01 09:33:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.76	
[12/01 09:33:28 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[12/01 09:35:07 visual_prompt]: 	Training 100/553. train loss: 0.1837,	0.8280 s / batch. (data: 4.63e-04). ETA=8:37:33, max mem: 20.9 GB 
[12/01 09:36:45 visual_prompt]: 	Training 200/553. train loss: 2.0197,	0.8280 s / batch. (data: 7.92e-03). ETA=8:36:10, max mem: 20.9 GB 
[12/01 09:38:21 visual_prompt]: 	Training 300/553. train loss: 0.6138,	0.8414 s / batch. (data: 3.02e-04). ETA=8:43:08, max mem: 20.9 GB 
[12/01 09:39:58 visual_prompt]: 	Training 400/553. train loss: 1.0919,	0.8237 s / batch. (data: 3.07e-04). ETA=8:30:43, max mem: 20.9 GB 
[12/01 09:41:39 visual_prompt]: 	Training 500/553. train loss: 0.8687,	1.4239 s / batch. (data: 5.73e-01). ETA=14:40:33, max mem: 20.9 GB 
[12/01 09:42:47 visual_prompt]: Epoch 33 / 100: avg data time: 1.77e-01, avg batch time: 1.0103, average train loss: 1.4306
[12/01 09:44:05 visual_prompt]: Inference (val):avg data time: 2.98e-04, avg batch time: 0.3132, average loss: 0.7960
[12/01 09:44:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.11	
[12/01 09:44:05 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[12/01 09:46:26 visual_prompt]: 	Training 100/553. train loss: 1.9688,	1.5026 s / batch. (data: 6.78e-01). ETA=15:25:23, max mem: 20.9 GB 
[12/01 09:48:21 visual_prompt]: 	Training 200/553. train loss: 1.7306,	0.9840 s / batch. (data: 1.40e-01). ETA=10:04:20, max mem: 20.9 GB 
[12/01 09:50:18 visual_prompt]: 	Training 300/553. train loss: 0.7064,	1.0242 s / batch. (data: 1.77e-01). ETA=10:27:22, max mem: 20.9 GB 
[12/01 09:52:19 visual_prompt]: 	Training 400/553. train loss: 3.1435,	0.8180 s / batch. (data: 3.91e-04). ETA=8:19:40, max mem: 20.9 GB 
[12/01 09:54:18 visual_prompt]: 	Training 500/553. train loss: 0.5808,	2.2485 s / batch. (data: 1.43e+00). ETA=22:49:44, max mem: 20.9 GB 
[12/01 09:55:19 visual_prompt]: Epoch 34 / 100: avg data time: 3.82e-01, avg batch time: 1.2169, average train loss: 1.4370
[12/01 09:56:27 visual_prompt]: Inference (val):avg data time: 8.25e-05, avg batch time: 0.3092, average loss: 0.7245
[12/01 09:56:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.37	
[12/01 09:56:27 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[12/01 09:58:33 visual_prompt]: 	Training 100/553. train loss: 0.3701,	0.8416 s / batch. (data: 1.06e-02). ETA=8:30:31, max mem: 20.9 GB 
[12/01 10:00:31 visual_prompt]: 	Training 200/553. train loss: 1.1650,	0.8400 s / batch. (data: 3.96e-04). ETA=8:28:10, max mem: 20.9 GB 
[12/01 10:02:28 visual_prompt]: 	Training 300/553. train loss: 0.6866,	0.8154 s / batch. (data: 3.19e-04). ETA=8:11:54, max mem: 20.9 GB 
[12/01 10:04:24 visual_prompt]: 	Training 400/553. train loss: 2.5259,	0.9420 s / batch. (data: 8.04e-02). ETA=9:26:42, max mem: 20.9 GB 
[12/01 10:06:20 visual_prompt]: 	Training 500/553. train loss: 0.6820,	1.3874 s / batch. (data: 5.75e-01). ETA=13:52:24, max mem: 20.9 GB 
[12/01 10:07:19 visual_prompt]: Epoch 35 / 100: avg data time: 3.42e-01, avg batch time: 1.1780, average train loss: 1.4965
[12/01 10:08:24 visual_prompt]: Inference (val):avg data time: 5.48e-05, avg batch time: 0.3087, average loss: 4.1198
[12/01 10:08:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.79	
[12/01 10:08:24 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[12/01 10:10:20 visual_prompt]: 	Training 100/553. train loss: 2.8035,	1.1075 s / batch. (data: 2.69e-01). ETA=11:01:39, max mem: 20.9 GB 
[12/01 10:12:15 visual_prompt]: 	Training 200/553. train loss: 2.2939,	0.8534 s / batch. (data: 5.61e-03). ETA=8:28:26, max mem: 20.9 GB 
[12/01 10:14:12 visual_prompt]: 	Training 300/553. train loss: 0.0959,	0.8360 s / batch. (data: 3.17e-04). ETA=8:16:39, max mem: 20.9 GB 
[12/01 10:16:05 visual_prompt]: 	Training 400/553. train loss: 2.7227,	0.8342 s / batch. (data: 3.40e-04). ETA=8:14:13, max mem: 20.9 GB 
[12/01 10:17:59 visual_prompt]: 	Training 500/553. train loss: 4.2440,	1.5434 s / batch. (data: 7.29e-01). ETA=15:11:45, max mem: 20.9 GB 
[12/01 10:18:55 visual_prompt]: Epoch 36 / 100: avg data time: 3.06e-01, avg batch time: 1.1396, average train loss: 1.7918
[12/01 10:20:00 visual_prompt]: Inference (val):avg data time: 5.74e-05, avg batch time: 0.3094, average loss: 1.7960
[12/01 10:20:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.40	
[12/01 10:20:00 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[12/01 10:21:58 visual_prompt]: 	Training 100/553. train loss: 0.5411,	0.8667 s / batch. (data: 4.83e-02). ETA=8:29:47, max mem: 20.9 GB 
[12/01 10:23:51 visual_prompt]: 	Training 200/553. train loss: 1.6018,	0.8409 s / batch. (data: 8.69e-03). ETA=8:13:13, max mem: 20.9 GB 
[12/01 10:25:46 visual_prompt]: 	Training 300/553. train loss: 4.2549,	1.7600 s / batch. (data: 9.32e-01). ETA=17:09:21, max mem: 20.9 GB 
[12/01 10:27:42 visual_prompt]: 	Training 400/553. train loss: 1.0434,	2.3081 s / batch. (data: 1.48e+00). ETA=22:26:05, max mem: 20.9 GB 
[12/01 10:29:32 visual_prompt]: 	Training 500/553. train loss: 1.9251,	1.8002 s / batch. (data: 9.68e-01). ETA=17:26:52, max mem: 20.9 GB 
[12/01 10:30:32 visual_prompt]: Epoch 37 / 100: avg data time: 3.09e-01, avg batch time: 1.1423, average train loss: 1.4903
[12/01 10:31:37 visual_prompt]: Inference (val):avg data time: 5.60e-05, avg batch time: 0.3101, average loss: 1.1598
[12/01 10:31:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.07	
[12/01 10:31:37 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[12/01 10:33:33 visual_prompt]: 	Training 100/553. train loss: 0.8273,	1.1723 s / batch. (data: 3.32e-01). ETA=11:18:46, max mem: 20.9 GB 
[12/01 10:35:27 visual_prompt]: 	Training 200/553. train loss: 0.7999,	1.9143 s / batch. (data: 1.08e+00). ETA=18:25:10, max mem: 20.9 GB 
[12/01 10:37:21 visual_prompt]: 	Training 300/553. train loss: 1.0808,	0.8651 s / batch. (data: 7.96e-03). ETA=8:17:58, max mem: 20.9 GB 
[12/01 10:39:11 visual_prompt]: 	Training 400/553. train loss: 0.0853,	0.8565 s / batch. (data: 3.04e-04). ETA=8:11:37, max mem: 20.9 GB 
[12/01 10:41:06 visual_prompt]: 	Training 500/553. train loss: 0.7028,	0.8462 s / batch. (data: 3.92e-04). ETA=8:04:18, max mem: 20.9 GB 
[12/01 10:42:03 visual_prompt]: Epoch 38 / 100: avg data time: 2.97e-01, avg batch time: 1.1304, average train loss: 1.4671
[12/01 10:43:08 visual_prompt]: Inference (val):avg data time: 3.35e-04, avg batch time: 0.3098, average loss: 2.2161
[12/01 10:43:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.28	
[12/01 10:43:08 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[12/01 10:45:04 visual_prompt]: 	Training 100/553. train loss: 0.3800,	0.8149 s / batch. (data: 3.33e-04). ETA=7:44:16, max mem: 20.9 GB 
[12/01 10:47:00 visual_prompt]: 	Training 200/553. train loss: 1.0805,	0.8316 s / batch. (data: 1.29e-03). ETA=7:52:24, max mem: 20.9 GB 
[12/01 10:48:56 visual_prompt]: 	Training 300/553. train loss: 5.5154,	0.8162 s / batch. (data: 3.33e-04). ETA=7:42:20, max mem: 20.9 GB 
[12/01 10:50:46 visual_prompt]: 	Training 400/553. train loss: 1.1481,	0.8320 s / batch. (data: 3.52e-04). ETA=7:49:52, max mem: 20.9 GB 
[12/01 10:52:39 visual_prompt]: 	Training 500/553. train loss: 0.7273,	2.1120 s / batch. (data: 1.27e+00). ETA=19:49:16, max mem: 20.9 GB 
[12/01 10:53:35 visual_prompt]: Epoch 39 / 100: avg data time: 3.00e-01, avg batch time: 1.1337, average train loss: 1.3382
[12/01 10:54:42 visual_prompt]: Inference (val):avg data time: 6.17e-05, avg batch time: 0.3104, average loss: 1.5693
[12/01 10:54:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.72	
[12/01 10:54:42 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[12/01 10:56:41 visual_prompt]: 	Training 100/553. train loss: 1.3164,	0.8183 s / batch. (data: 8.56e-04). ETA=7:38:41, max mem: 20.9 GB 
[12/01 10:58:32 visual_prompt]: 	Training 200/553. train loss: 2.5917,	0.8563 s / batch. (data: 3.58e-04). ETA=7:58:35, max mem: 20.9 GB 
[12/01 11:00:27 visual_prompt]: 	Training 300/553. train loss: 2.0996,	0.8622 s / batch. (data: 1.05e-02). ETA=8:00:24, max mem: 20.9 GB 
[12/01 11:02:21 visual_prompt]: 	Training 400/553. train loss: 1.2853,	0.8360 s / batch. (data: 3.19e-04). ETA=7:44:27, max mem: 20.9 GB 
[12/01 11:04:13 visual_prompt]: 	Training 500/553. train loss: 0.0488,	0.8280 s / batch. (data: 3.70e-04). ETA=7:38:36, max mem: 20.9 GB 
[12/01 11:05:13 visual_prompt]: Epoch 40 / 100: avg data time: 3.07e-01, avg batch time: 1.1419, average train loss: 1.4414
[12/01 11:06:19 visual_prompt]: Inference (val):avg data time: 5.43e-05, avg batch time: 0.3109, average loss: 0.7913
[12/01 11:06:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.57	
[12/01 11:06:19 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[12/01 11:08:22 visual_prompt]: 	Training 100/553. train loss: 1.1697,	0.8318 s / batch. (data: 3.63e-03). ETA=7:38:36, max mem: 20.9 GB 
[12/01 11:10:17 visual_prompt]: 	Training 200/553. train loss: 3.3325,	0.8420 s / batch. (data: 9.31e-04). ETA=7:42:49, max mem: 20.9 GB 
[12/01 11:12:10 visual_prompt]: 	Training 300/553. train loss: 1.0434,	0.8187 s / batch. (data: 5.37e-04). ETA=7:28:39, max mem: 20.9 GB 
[12/01 11:14:00 visual_prompt]: 	Training 400/553. train loss: 0.7200,	0.8251 s / batch. (data: 7.96e-03). ETA=7:30:45, max mem: 20.9 GB 
[12/01 11:15:50 visual_prompt]: 	Training 500/553. train loss: 0.6739,	0.8411 s / batch. (data: 5.91e-03). ETA=7:38:05, max mem: 20.9 GB 
[12/01 11:16:46 visual_prompt]: Epoch 41 / 100: avg data time: 3.00e-01, avg batch time: 1.1337, average train loss: 1.7543
[12/01 11:17:51 visual_prompt]: Inference (val):avg data time: 5.45e-05, avg batch time: 0.3108, average loss: 1.0916
[12/01 11:17:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.43	
[12/01 11:17:51 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[12/01 11:19:45 visual_prompt]: 	Training 100/553. train loss: 3.8636,	0.8764 s / batch. (data: 2.03e-02). ETA=7:55:06, max mem: 20.9 GB 
[12/01 11:21:38 visual_prompt]: 	Training 200/553. train loss: 4.4115,	1.0240 s / batch. (data: 1.85e-01). ETA=9:13:26, max mem: 20.9 GB 
[12/01 11:23:31 visual_prompt]: 	Training 300/553. train loss: 0.7441,	0.8533 s / batch. (data: 1.17e-02). ETA=7:39:46, max mem: 20.9 GB 
[12/01 11:25:23 visual_prompt]: 	Training 400/553. train loss: 0.6954,	0.8390 s / batch. (data: 1.06e-02). ETA=7:30:39, max mem: 20.9 GB 
[12/01 11:27:15 visual_prompt]: 	Training 500/553. train loss: 0.5426,	0.8437 s / batch. (data: 5.84e-03). ETA=7:31:46, max mem: 20.9 GB 
[12/01 11:28:14 visual_prompt]: Epoch 42 / 100: avg data time: 2.93e-01, avg batch time: 1.1258, average train loss: 1.3538
[12/01 11:29:19 visual_prompt]: Inference (val):avg data time: 6.14e-05, avg batch time: 0.3092, average loss: 1.4551
[12/01 11:29:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.13	
[12/01 11:29:19 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[12/01 11:31:17 visual_prompt]: 	Training 100/553. train loss: 2.0815,	0.8606 s / batch. (data: 6.63e-03). ETA=7:38:37, max mem: 20.9 GB 
[12/01 11:33:09 visual_prompt]: 	Training 200/553. train loss: 0.7445,	0.8506 s / batch. (data: 2.20e-02). ETA=7:31:50, max mem: 20.9 GB 
[12/01 11:35:00 visual_prompt]: 	Training 300/553. train loss: 1.5763,	0.8187 s / batch. (data: 7.89e-04). ETA=7:13:33, max mem: 20.9 GB 
[12/01 11:36:50 visual_prompt]: 	Training 400/553. train loss: 0.6066,	0.8315 s / batch. (data: 3.29e-04). ETA=7:18:55, max mem: 20.9 GB 
[12/01 11:38:44 visual_prompt]: 	Training 500/553. train loss: 1.4486,	0.8236 s / batch. (data: 3.45e-04). ETA=7:13:24, max mem: 20.9 GB 
[12/01 11:39:44 visual_prompt]: Epoch 43 / 100: avg data time: 2.97e-01, avg batch time: 1.1297, average train loss: 1.1745
[12/01 11:40:49 visual_prompt]: Inference (val):avg data time: 5.55e-05, avg batch time: 0.3084, average loss: 0.7113
[12/01 11:40:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.01	
[12/01 11:40:49 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[12/01 11:42:46 visual_prompt]: 	Training 100/553. train loss: 0.6241,	0.8440 s / batch. (data: 3.71e-04). ETA=7:21:59, max mem: 20.9 GB 
[12/01 11:44:50 visual_prompt]: 	Training 200/553. train loss: 0.7946,	0.8240 s / batch. (data: 3.40e-04). ETA=7:10:08, max mem: 20.9 GB 
[12/01 11:46:43 visual_prompt]: 	Training 300/553. train loss: 0.5483,	0.8376 s / batch. (data: 4.95e-03). ETA=7:15:49, max mem: 20.9 GB 
[12/01 11:48:42 visual_prompt]: 	Training 400/553. train loss: 0.7269,	0.8374 s / batch. (data: 5.73e-03). ETA=7:14:21, max mem: 20.9 GB 
[12/01 11:50:37 visual_prompt]: 	Training 500/553. train loss: 0.7319,	0.9826 s / batch. (data: 1.57e-01). ETA=8:28:01, max mem: 20.9 GB 
[12/01 11:51:39 visual_prompt]: Epoch 44 / 100: avg data time: 3.44e-01, avg batch time: 1.1747, average train loss: 1.0928
[12/01 11:52:49 visual_prompt]: Inference (val):avg data time: 6.56e-05, avg batch time: 0.3078, average loss: 0.7601
[12/01 11:52:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.35	
[12/01 11:52:49 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[12/01 11:54:51 visual_prompt]: 	Training 100/553. train loss: 0.8628,	0.8560 s / batch. (data: 3.76e-04). ETA=7:20:22, max mem: 20.9 GB 
[12/01 11:56:40 visual_prompt]: 	Training 200/553. train loss: 0.7001,	0.8423 s / batch. (data: 5.46e-03). ETA=7:11:56, max mem: 20.9 GB 
[12/01 11:58:35 visual_prompt]: 	Training 300/553. train loss: 1.3440,	0.8234 s / batch. (data: 4.29e-04). ETA=7:00:51, max mem: 20.9 GB 
[12/01 12:00:25 visual_prompt]: 	Training 400/553. train loss: 0.8332,	0.8236 s / batch. (data: 1.36e-03). ETA=6:59:34, max mem: 20.9 GB 
[12/01 12:02:20 visual_prompt]: 	Training 500/553. train loss: 2.0294,	0.8416 s / batch. (data: 3.41e-04). ETA=7:07:21, max mem: 20.9 GB 
[12/01 12:03:21 visual_prompt]: Epoch 45 / 100: avg data time: 3.09e-01, avg batch time: 1.1416, average train loss: 0.9599
[12/01 12:04:26 visual_prompt]: Inference (val):avg data time: 4.87e-05, avg batch time: 0.3107, average loss: 1.5719
[12/01 12:04:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.47	
[12/01 12:04:26 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[12/01 12:06:26 visual_prompt]: 	Training 100/553. train loss: 0.6075,	1.9844 s / batch. (data: 1.13e+00). ETA=16:42:37, max mem: 20.9 GB 
[12/01 12:08:21 visual_prompt]: 	Training 200/553. train loss: 0.9347,	0.8192 s / batch. (data: 3.19e-04). ETA=6:52:31, max mem: 20.9 GB 
[12/01 12:10:16 visual_prompt]: 	Training 300/553. train loss: 1.7092,	0.8380 s / batch. (data: 1.05e-02). ETA=7:00:37, max mem: 20.9 GB 
[12/01 12:12:12 visual_prompt]: 	Training 400/553. train loss: 0.7330,	0.8510 s / batch. (data: 2.18e-02). ETA=7:05:43, max mem: 20.9 GB 
[12/01 12:14:02 visual_prompt]: 	Training 500/553. train loss: 2.5807,	0.8301 s / batch. (data: 5.57e-03). ETA=6:53:53, max mem: 20.9 GB 
[12/01 12:15:02 visual_prompt]: Epoch 46 / 100: avg data time: 3.16e-01, avg batch time: 1.1492, average train loss: 1.1383
[12/01 12:16:07 visual_prompt]: Inference (val):avg data time: 5.46e-05, avg batch time: 0.3096, average loss: 1.3494
[12/01 12:16:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.01	
[12/01 12:16:07 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[12/01 12:18:06 visual_prompt]: 	Training 100/553. train loss: 1.2163,	0.8258 s / batch. (data: 3.09e-04). ETA=6:49:36, max mem: 20.9 GB 
[12/01 12:19:53 visual_prompt]: 	Training 200/553. train loss: 0.7277,	0.9328 s / batch. (data: 1.15e-01). ETA=7:41:09, max mem: 20.9 GB 
[12/01 12:21:46 visual_prompt]: 	Training 300/553. train loss: 2.2372,	0.8387 s / batch. (data: 8.21e-04). ETA=6:53:12, max mem: 20.9 GB 
[12/01 12:23:38 visual_prompt]: 	Training 400/553. train loss: 2.3650,	0.8514 s / batch. (data: 1.89e-03). ETA=6:58:04, max mem: 20.9 GB 
[12/01 12:25:30 visual_prompt]: 	Training 500/553. train loss: 0.9599,	0.8372 s / batch. (data: 2.07e-02). ETA=6:49:43, max mem: 20.9 GB 
[12/01 12:26:30 visual_prompt]: Epoch 47 / 100: avg data time: 2.95e-01, avg batch time: 1.1275, average train loss: 1.6888
[12/01 12:27:37 visual_prompt]: Inference (val):avg data time: 6.29e-05, avg batch time: 0.3101, average loss: 1.2614
[12/01 12:27:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.40	
[12/01 12:27:37 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[12/01 12:29:36 visual_prompt]: 	Training 100/553. train loss: 2.0780,	0.8404 s / batch. (data: 1.05e-02). ETA=6:49:07, max mem: 20.9 GB 
[12/01 12:31:28 visual_prompt]: 	Training 200/553. train loss: 0.8790,	0.8348 s / batch. (data: 3.20e-04). ETA=6:45:01, max mem: 20.9 GB 
[12/01 12:33:23 visual_prompt]: 	Training 300/553. train loss: 0.7086,	2.1631 s / batch. (data: 1.34e+00). ETA=17:25:50, max mem: 20.9 GB 
[12/01 12:35:12 visual_prompt]: 	Training 400/553. train loss: 0.0159,	1.0280 s / batch. (data: 1.97e-01). ETA=8:15:19, max mem: 20.9 GB 
[12/01 12:37:05 visual_prompt]: 	Training 500/553. train loss: 0.9275,	0.8385 s / batch. (data: 3.20e-04). ETA=6:42:35, max mem: 20.9 GB 
[12/01 12:38:03 visual_prompt]: Epoch 48 / 100: avg data time: 3.00e-01, avg batch time: 1.1326, average train loss: 1.1075
[12/01 12:39:08 visual_prompt]: Inference (val):avg data time: 5.30e-05, avg batch time: 0.3089, average loss: 0.9906
[12/01 12:39:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.12	
[12/01 12:39:08 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[12/01 12:41:05 visual_prompt]: 	Training 100/553. train loss: 1.0602,	0.8263 s / batch. (data: 9.84e-03). ETA=6:34:38, max mem: 20.9 GB 
[12/01 12:42:55 visual_prompt]: 	Training 200/553. train loss: 0.6877,	0.8760 s / batch. (data: 5.43e-03). ETA=6:56:55, max mem: 20.9 GB 
[12/01 12:44:47 visual_prompt]: 	Training 300/553. train loss: 2.1534,	0.8422 s / batch. (data: 5.77e-04). ETA=6:39:26, max mem: 20.9 GB 
[12/01 12:46:41 visual_prompt]: 	Training 400/553. train loss: 0.8271,	0.8267 s / batch. (data: 3.36e-04). ETA=6:30:40, max mem: 20.9 GB 
[12/01 12:48:34 visual_prompt]: 	Training 500/553. train loss: 0.9608,	0.8283 s / batch. (data: 3.51e-04). ETA=6:30:03, max mem: 20.9 GB 
[12/01 12:49:33 visual_prompt]: Epoch 49 / 100: avg data time: 2.96e-01, avg batch time: 1.1291, average train loss: 1.0446
[12/01 12:50:38 visual_prompt]: Inference (val):avg data time: 5.50e-05, avg batch time: 0.3105, average loss: 0.6848
[12/01 12:50:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.03	
[12/01 12:50:38 visual_prompt]: Best epoch 49: best metric: -0.685
[12/01 12:50:38 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[12/01 12:52:35 visual_prompt]: 	Training 100/553. train loss: 0.8385,	0.8306 s / batch. (data: 4.19e-04). ETA=6:29:03, max mem: 20.9 GB 
[12/01 12:54:28 visual_prompt]: 	Training 200/553. train loss: 3.6089,	0.8439 s / batch. (data: 8.03e-04). ETA=6:33:51, max mem: 20.9 GB 
[12/01 12:56:18 visual_prompt]: 	Training 300/553. train loss: 0.8023,	0.8561 s / batch. (data: 1.05e-03). ETA=6:38:07, max mem: 20.9 GB 
[12/01 12:58:08 visual_prompt]: 	Training 400/553. train loss: 1.4369,	0.8390 s / batch. (data: 5.55e-03). ETA=6:28:46, max mem: 20.9 GB 
[12/01 13:00:00 visual_prompt]: 	Training 500/553. train loss: 0.7205,	0.8520 s / batch. (data: 7.91e-03). ETA=6:33:23, max mem: 20.9 GB 
[12/01 13:00:58 visual_prompt]: Epoch 50 / 100: avg data time: 2.88e-01, avg batch time: 1.1214, average train loss: 1.0717
[12/01 13:02:04 visual_prompt]: Inference (val):avg data time: 6.96e-05, avg batch time: 0.3117, average loss: 0.9001
[12/01 13:02:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.08	
[12/01 13:02:04 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[12/01 13:04:02 visual_prompt]: 	Training 100/553. train loss: 0.7063,	1.6690 s / batch. (data: 8.33e-01). ETA=12:46:21, max mem: 20.9 GB 
[12/01 13:05:54 visual_prompt]: 	Training 200/553. train loss: 1.1182,	0.8538 s / batch. (data: 1.56e-02). ETA=6:30:36, max mem: 20.9 GB 
[12/01 13:07:48 visual_prompt]: 	Training 300/553. train loss: 0.4876,	1.8606 s / batch. (data: 1.05e+00). ETA=14:08:08, max mem: 20.9 GB 
[12/01 13:09:41 visual_prompt]: 	Training 400/553. train loss: 2.2931,	1.9260 s / batch. (data: 1.07e+00). ETA=14:34:43, max mem: 20.9 GB 
[12/01 13:11:31 visual_prompt]: 	Training 500/553. train loss: 0.6059,	0.8480 s / batch. (data: 8.71e-04). ETA=6:23:43, max mem: 20.9 GB 
[12/01 13:12:27 visual_prompt]: Epoch 51 / 100: avg data time: 2.92e-01, avg batch time: 1.1253, average train loss: 0.9664
[12/01 13:13:32 visual_prompt]: Inference (val):avg data time: 5.45e-05, avg batch time: 0.3100, average loss: 0.7036
[12/01 13:13:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 58.73	
[12/01 13:13:32 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[12/01 13:15:33 visual_prompt]: 	Training 100/553. train loss: 1.0014,	0.8396 s / batch. (data: 7.96e-03). ETA=6:17:47, max mem: 20.9 GB 
[12/01 13:17:24 visual_prompt]: 	Training 200/553. train loss: 1.1307,	0.8479 s / batch. (data: 5.09e-04). ETA=6:20:07, max mem: 20.9 GB 
[12/01 13:19:16 visual_prompt]: 	Training 300/553. train loss: 1.9650,	0.8167 s / batch. (data: 2.90e-04). ETA=6:04:45, max mem: 20.9 GB 
[12/01 13:21:11 visual_prompt]: 	Training 400/553. train loss: 0.3705,	0.8310 s / batch. (data: 3.09e-04). ETA=6:09:45, max mem: 20.9 GB 
[12/01 13:22:57 visual_prompt]: 	Training 500/553. train loss: 0.0059,	0.8359 s / batch. (data: 1.38e-03). ETA=6:10:31, max mem: 20.9 GB 
[12/01 13:23:55 visual_prompt]: Epoch 52 / 100: avg data time: 2.92e-01, avg batch time: 1.1250, average train loss: 1.0552
[12/01 13:24:59 visual_prompt]: Inference (val):avg data time: 5.06e-05, avg batch time: 0.3108, average loss: 2.0058
[12/01 13:24:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.05	
[12/01 13:24:59 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[12/01 13:26:56 visual_prompt]: 	Training 100/553. train loss: 1.2796,	0.8440 s / batch. (data: 3.40e-04). ETA=6:11:58, max mem: 20.9 GB 
[12/01 13:28:48 visual_prompt]: 	Training 200/553. train loss: 0.4079,	0.8355 s / batch. (data: 8.66e-04). ETA=6:06:51, max mem: 20.9 GB 
[12/01 13:30:40 visual_prompt]: 	Training 300/553. train loss: 0.6331,	0.8801 s / batch. (data: 3.20e-02). ETA=6:24:56, max mem: 20.9 GB 
[12/01 13:32:34 visual_prompt]: 	Training 400/553. train loss: 0.6461,	0.8544 s / batch. (data: 3.16e-04). ETA=6:12:18, max mem: 20.9 GB 
[12/01 13:34:27 visual_prompt]: 	Training 500/553. train loss: 0.8141,	0.8448 s / batch. (data: 1.58e-02). ETA=6:06:42, max mem: 20.9 GB 
[12/01 13:35:26 visual_prompt]: Epoch 53 / 100: avg data time: 3.00e-01, avg batch time: 1.1338, average train loss: 1.0234
[12/01 13:36:31 visual_prompt]: Inference (val):avg data time: 5.78e-05, avg batch time: 0.3101, average loss: 1.8954
[12/01 13:36:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.10	
[12/01 13:36:31 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[12/01 13:38:31 visual_prompt]: 	Training 100/553. train loss: 1.1553,	0.8388 s / batch. (data: 1.05e-02). ETA=6:01:57, max mem: 20.9 GB 
[12/01 13:40:24 visual_prompt]: 	Training 200/553. train loss: 0.6021,	0.8520 s / batch. (data: 1.60e-02). ETA=6:06:14, max mem: 20.9 GB 
[12/01 13:42:15 visual_prompt]: 	Training 300/553. train loss: 0.1668,	1.2440 s / batch. (data: 3.94e-01). ETA=8:52:39, max mem: 20.9 GB 
[12/01 13:44:07 visual_prompt]: 	Training 400/553. train loss: 2.7481,	0.8516 s / batch. (data: 1.19e-02). ETA=6:03:12, max mem: 20.9 GB 
[12/01 13:45:58 visual_prompt]: 	Training 500/553. train loss: 0.4557,	0.8386 s / batch. (data: 3.38e-04). ETA=5:56:16, max mem: 20.9 GB 
[12/01 13:46:57 visual_prompt]: Epoch 54 / 100: avg data time: 2.98e-01, avg batch time: 1.1306, average train loss: 1.0393
[12/01 13:48:01 visual_prompt]: Inference (val):avg data time: 5.44e-05, avg batch time: 0.3107, average loss: 2.1879
[12/01 13:48:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.73	
[12/01 13:48:01 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[12/01 13:49:58 visual_prompt]: 	Training 100/553. train loss: 0.6010,	1.4080 s / batch. (data: 5.75e-01). ETA=9:54:35, max mem: 20.9 GB 
[12/01 13:51:49 visual_prompt]: 	Training 200/553. train loss: 1.4890,	0.8358 s / batch. (data: 5.97e-03). ETA=5:51:34, max mem: 20.9 GB 
[12/01 13:53:41 visual_prompt]: 	Training 300/553. train loss: 1.5187,	0.8361 s / batch. (data: 5.60e-03). ETA=5:50:18, max mem: 20.9 GB 
[12/01 13:55:33 visual_prompt]: 	Training 400/553. train loss: 0.9575,	0.8440 s / batch. (data: 7.96e-03). ETA=5:52:12, max mem: 20.9 GB 
[12/01 13:57:24 visual_prompt]: 	Training 500/553. train loss: 0.9064,	1.4810 s / batch. (data: 6.58e-01). ETA=10:15:32, max mem: 20.9 GB 
[12/01 13:58:23 visual_prompt]: Epoch 55 / 100: avg data time: 2.91e-01, avg batch time: 1.1248, average train loss: 0.9291
[12/01 13:59:28 visual_prompt]: Inference (val):avg data time: 1.22e-04, avg batch time: 0.3111, average loss: 0.7386
[12/01 13:59:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.05	
[12/01 13:59:28 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[12/01 14:01:26 visual_prompt]: 	Training 100/553. train loss: 1.2588,	0.8194 s / batch. (data: 3.23e-04). ETA=5:38:29, max mem: 20.9 GB 
[12/01 14:03:16 visual_prompt]: 	Training 200/553. train loss: 0.9217,	0.8429 s / batch. (data: 3.42e-04). ETA=5:46:46, max mem: 20.9 GB 
[12/01 14:05:10 visual_prompt]: 	Training 300/553. train loss: 0.7833,	0.8309 s / batch. (data: 2.90e-04). ETA=5:40:27, max mem: 20.9 GB 
[12/01 14:07:03 visual_prompt]: 	Training 400/553. train loss: 0.9919,	0.8449 s / batch. (data: 1.10e-03). ETA=5:44:46, max mem: 20.9 GB 
[12/01 14:08:55 visual_prompt]: 	Training 500/553. train loss: 1.0380,	2.8285 s / batch. (data: 2.00e+00). ETA=19:09:33, max mem: 20.9 GB 
[12/01 14:09:51 visual_prompt]: Epoch 56 / 100: avg data time: 2.92e-01, avg batch time: 1.1260, average train loss: 0.8924
[12/01 14:10:57 visual_prompt]: Inference (val):avg data time: 6.64e-05, avg batch time: 0.3100, average loss: 1.5013
[12/01 14:10:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.48	
[12/01 14:10:57 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[12/01 14:12:56 visual_prompt]: 	Training 100/553. train loss: 1.1930,	0.8490 s / batch. (data: 1.33e-02). ETA=5:42:52, max mem: 20.9 GB 
[12/01 14:14:46 visual_prompt]: 	Training 200/553. train loss: 0.7857,	0.8224 s / batch. (data: 4.14e-04). ETA=5:30:46, max mem: 20.9 GB 
[12/01 14:16:39 visual_prompt]: 	Training 300/553. train loss: 0.5804,	0.8649 s / batch. (data: 1.07e-02). ETA=5:46:24, max mem: 20.9 GB 
[12/01 14:18:31 visual_prompt]: 	Training 400/553. train loss: 0.8714,	0.8480 s / batch. (data: 3.18e-04). ETA=5:38:14, max mem: 20.9 GB 
[12/01 14:20:21 visual_prompt]: 	Training 500/553. train loss: 0.6324,	0.8280 s / batch. (data: 3.09e-04). ETA=5:28:52, max mem: 20.9 GB 
[12/01 14:21:19 visual_prompt]: Epoch 57 / 100: avg data time: 2.92e-01, avg batch time: 1.1260, average train loss: 0.8856
[12/01 14:22:25 visual_prompt]: Inference (val):avg data time: 5.43e-05, avg batch time: 0.3103, average loss: 0.7279
[12/01 14:22:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.59	
[12/01 14:22:25 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[12/01 14:24:21 visual_prompt]: 	Training 100/553. train loss: 1.3460,	1.5270 s / batch. (data: 7.12e-01). ETA=10:02:37, max mem: 20.9 GB 
[12/01 14:26:13 visual_prompt]: 	Training 200/553. train loss: 1.1418,	0.8935 s / batch. (data: 4.15e-02). ETA=5:51:07, max mem: 20.9 GB 
[12/01 14:28:09 visual_prompt]: 	Training 300/553. train loss: 0.6611,	0.8705 s / batch. (data: 2.35e-03). ETA=5:40:38, max mem: 20.9 GB 
[12/01 14:29:59 visual_prompt]: 	Training 400/553. train loss: 0.7039,	0.8268 s / batch. (data: 4.07e-04). ETA=5:22:08, max mem: 20.9 GB 
[12/01 14:31:51 visual_prompt]: 	Training 500/553. train loss: 0.5844,	0.8245 s / batch. (data: 7.20e-04). ETA=5:19:52, max mem: 20.9 GB 
[12/01 14:32:47 visual_prompt]: Epoch 58 / 100: avg data time: 2.93e-01, avg batch time: 1.1255, average train loss: 0.9257
[12/01 14:33:52 visual_prompt]: Inference (val):avg data time: 5.05e-05, avg batch time: 0.3100, average loss: 1.3509
[12/01 14:33:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.16	
[12/01 14:33:52 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[12/01 14:35:51 visual_prompt]: 	Training 100/553. train loss: 0.5540,	0.8298 s / batch. (data: 2.79e-04). ETA=5:19:48, max mem: 20.9 GB 
[12/01 14:37:44 visual_prompt]: 	Training 200/553. train loss: 0.2182,	0.8520 s / batch. (data: 3.96e-04). ETA=5:26:58, max mem: 20.9 GB 
[12/01 14:39:33 visual_prompt]: 	Training 300/553. train loss: 0.5867,	0.8435 s / batch. (data: 3.37e-04). ETA=5:22:17, max mem: 20.9 GB 
[12/01 14:41:25 visual_prompt]: 	Training 400/553. train loss: 0.6023,	1.3237 s / batch. (data: 4.56e-01). ETA=8:23:34, max mem: 20.9 GB 
[12/01 14:43:21 visual_prompt]: 	Training 500/553. train loss: 0.8391,	0.8349 s / batch. (data: 6.20e-03). ETA=5:16:13, max mem: 20.9 GB 
[12/01 14:44:16 visual_prompt]: Epoch 59 / 100: avg data time: 2.93e-01, avg batch time: 1.1280, average train loss: 0.9231
[12/01 14:45:21 visual_prompt]: Inference (val):avg data time: 4.80e-05, avg batch time: 0.3082, average loss: 0.7057
[12/01 14:45:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.81	
[12/01 14:45:21 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[12/01 14:47:19 visual_prompt]: 	Training 100/553. train loss: 0.5912,	0.8499 s / batch. (data: 3.65e-04). ETA=5:19:44, max mem: 20.9 GB 
[12/01 14:49:11 visual_prompt]: 	Training 200/553. train loss: 0.9063,	0.8273 s / batch. (data: 3.22e-04). ETA=5:09:52, max mem: 20.9 GB 
[12/01 14:51:01 visual_prompt]: 	Training 300/553. train loss: 0.0241,	2.3308 s / batch. (data: 1.50e+00). ETA=14:29:06, max mem: 20.9 GB 
[12/01 14:52:55 visual_prompt]: 	Training 400/553. train loss: 1.1826,	1.5640 s / batch. (data: 7.28e-01). ETA=9:40:34, max mem: 20.9 GB 
[12/01 14:54:47 visual_prompt]: 	Training 500/553. train loss: 0.8388,	0.8360 s / batch. (data: 3.77e-04). ETA=5:08:56, max mem: 20.9 GB 
[12/01 14:55:46 visual_prompt]: Epoch 60 / 100: avg data time: 2.96e-01, avg batch time: 1.1298, average train loss: 0.8315
[12/01 14:56:51 visual_prompt]: Inference (val):avg data time: 5.53e-05, avg batch time: 0.3113, average loss: 0.8650
[12/01 14:56:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.57	
[12/01 14:56:51 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[12/01 14:58:49 visual_prompt]: 	Training 100/553. train loss: 1.9142,	0.8446 s / batch. (data: 3.68e-04). ETA=5:09:58, max mem: 20.9 GB 
[12/01 15:00:43 visual_prompt]: 	Training 200/553. train loss: 1.0726,	0.8240 s / batch. (data: 7.98e-03). ETA=5:01:01, max mem: 20.9 GB 
[12/01 15:02:37 visual_prompt]: 	Training 300/553. train loss: 0.8136,	0.8400 s / batch. (data: 3.69e-04). ETA=5:05:28, max mem: 20.9 GB 
[12/01 15:04:27 visual_prompt]: 	Training 400/553. train loss: 0.8267,	0.8475 s / batch. (data: 1.36e-03). ETA=5:06:48, max mem: 20.9 GB 
[12/01 15:06:19 visual_prompt]: 	Training 500/553. train loss: 1.5411,	3.1988 s / batch. (data: 2.37e+00). ETA=19:12:38, max mem: 20.9 GB 
[12/01 15:07:16 visual_prompt]: Epoch 61 / 100: avg data time: 2.95e-01, avg batch time: 1.1300, average train loss: 0.9905
[12/01 15:08:21 visual_prompt]: Inference (val):avg data time: 1.70e-04, avg batch time: 0.3099, average loss: 0.7002
[12/01 15:08:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 63.02	
[12/01 15:08:21 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[12/01 15:10:16 visual_prompt]: 	Training 100/553. train loss: 1.2991,	0.8281 s / batch. (data: 5.11e-04). ETA=4:56:16, max mem: 20.9 GB 
[12/01 15:12:07 visual_prompt]: 	Training 200/553. train loss: 1.4442,	0.8312 s / batch. (data: 2.99e-04). ETA=4:56:00, max mem: 20.9 GB 
[12/01 15:13:57 visual_prompt]: 	Training 300/553. train loss: 0.3993,	0.8483 s / batch. (data: 7.98e-03). ETA=5:00:40, max mem: 20.9 GB 
[12/01 15:15:50 visual_prompt]: 	Training 400/553. train loss: 0.7722,	0.8177 s / batch. (data: 3.17e-04). ETA=4:48:27, max mem: 20.9 GB 
[12/01 15:17:40 visual_prompt]: 	Training 500/553. train loss: 0.9074,	0.8632 s / batch. (data: 1.05e-02). ETA=5:03:05, max mem: 20.9 GB 
[12/01 15:18:41 visual_prompt]: Epoch 62 / 100: avg data time: 2.87e-01, avg batch time: 1.1208, average train loss: 0.8295
[12/01 15:19:46 visual_prompt]: Inference (val):avg data time: 5.55e-05, avg batch time: 0.3104, average loss: 0.6724
[12/01 15:19:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 63.68	
[12/01 15:19:46 visual_prompt]: Best epoch 62: best metric: -0.672
[12/01 15:19:46 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[12/01 15:21:49 visual_prompt]: 	Training 100/553. train loss: 0.7518,	0.8527 s / batch. (data: 3.60e-04). ETA=4:57:12, max mem: 20.9 GB 
[12/01 15:23:45 visual_prompt]: 	Training 200/553. train loss: 1.5609,	0.8517 s / batch. (data: 1.17e-03). ETA=4:55:27, max mem: 20.9 GB 
[12/01 15:25:35 visual_prompt]: 	Training 300/553. train loss: 0.5662,	0.8286 s / batch. (data: 3.45e-04). ETA=4:46:03, max mem: 20.9 GB 
[12/01 15:27:22 visual_prompt]: 	Training 400/553. train loss: 0.5996,	0.8387 s / batch. (data: 5.49e-03). ETA=4:48:09, max mem: 20.9 GB 
[12/01 15:29:12 visual_prompt]: 	Training 500/553. train loss: 0.6165,	0.8380 s / batch. (data: 5.15e-04). ETA=4:46:30, max mem: 20.9 GB 
[12/01 15:30:09 visual_prompt]: Epoch 63 / 100: avg data time: 2.93e-01, avg batch time: 1.1262, average train loss: 0.8953
[12/01 15:31:15 visual_prompt]: Inference (val):avg data time: 5.87e-05, avg batch time: 0.3111, average loss: 0.8298
[12/01 15:31:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.34	
[12/01 15:31:15 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[12/01 15:33:14 visual_prompt]: 	Training 100/553. train loss: 1.1826,	0.8371 s / batch. (data: 5.94e-03). ETA=4:44:04, max mem: 20.9 GB 
[12/01 15:35:07 visual_prompt]: 	Training 200/553. train loss: 1.0097,	0.8235 s / batch. (data: 5.84e-04). ETA=4:38:04, max mem: 20.9 GB 
[12/01 15:36:55 visual_prompt]: 	Training 300/553. train loss: 0.5600,	0.8677 s / batch. (data: 5.46e-03). ETA=4:51:33, max mem: 20.9 GB 
[12/01 15:38:46 visual_prompt]: 	Training 400/553. train loss: 0.6988,	0.9835 s / batch. (data: 1.44e-01). ETA=5:28:50, max mem: 20.9 GB 
[12/01 15:40:39 visual_prompt]: 	Training 500/553. train loss: 0.4179,	0.8360 s / batch. (data: 3.13e-04). ETA=4:38:07, max mem: 20.9 GB 
[12/01 15:41:36 visual_prompt]: Epoch 64 / 100: avg data time: 2.91e-01, avg batch time: 1.1230, average train loss: 0.8700
[12/01 15:42:41 visual_prompt]: Inference (val):avg data time: 5.12e-05, avg batch time: 0.3128, average loss: 0.6944
[12/01 15:42:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.35	
[12/01 15:42:41 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[12/01 15:44:43 visual_prompt]: 	Training 100/553. train loss: 1.1999,	1.2083 s / batch. (data: 3.93e-01). ETA=6:38:54, max mem: 20.9 GB 
[12/01 15:46:36 visual_prompt]: 	Training 200/553. train loss: 0.5161,	1.9747 s / batch. (data: 1.15e+00). ETA=10:48:36, max mem: 20.9 GB 
[12/01 15:48:25 visual_prompt]: 	Training 300/553. train loss: 0.7307,	1.5040 s / batch. (data: 6.76e-01). ETA=8:11:30, max mem: 20.9 GB 
[12/01 15:50:16 visual_prompt]: 	Training 400/553. train loss: 0.9739,	0.8212 s / batch. (data: 3.71e-04). ETA=4:27:00, max mem: 20.9 GB 
[12/01 15:52:07 visual_prompt]: 	Training 500/553. train loss: 0.7350,	0.8513 s / batch. (data: 1.05e-02). ETA=4:35:22, max mem: 20.9 GB 
[12/01 15:53:02 visual_prompt]: Epoch 65 / 100: avg data time: 2.89e-01, avg batch time: 1.1226, average train loss: 0.8378
[12/01 15:54:07 visual_prompt]: Inference (val):avg data time: 6.04e-05, avg batch time: 0.3098, average loss: 0.7525
[12/01 15:54:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.78	
[12/01 15:54:08 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[12/01 15:56:05 visual_prompt]: 	Training 100/553. train loss: 0.5836,	0.8600 s / batch. (data: 7.94e-03). ETA=4:35:59, max mem: 20.9 GB 
[12/01 15:57:57 visual_prompt]: 	Training 200/553. train loss: 0.6359,	2.0437 s / batch. (data: 1.22e+00). ETA=10:52:27, max mem: 20.9 GB 
[12/01 15:59:51 visual_prompt]: 	Training 300/553. train loss: 0.7470,	0.8192 s / batch. (data: 3.54e-04). ETA=4:20:10, max mem: 20.9 GB 
[12/01 16:01:43 visual_prompt]: 	Training 400/553. train loss: 0.5716,	0.8212 s / batch. (data: 3.83e-04). ETA=4:19:25, max mem: 20.9 GB 
[12/01 16:03:33 visual_prompt]: 	Training 500/553. train loss: 1.2685,	0.8400 s / batch. (data: 3.78e-04). ETA=4:23:57, max mem: 20.9 GB 
[12/01 16:04:33 visual_prompt]: Epoch 66 / 100: avg data time: 2.97e-01, avg batch time: 1.1309, average train loss: 0.9078
[12/01 16:05:39 visual_prompt]: Inference (val):avg data time: 6.26e-05, avg batch time: 0.3122, average loss: 0.6883
[12/01 16:05:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.25	
[12/01 16:05:39 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[12/01 16:07:40 visual_prompt]: 	Training 100/553. train loss: 0.5161,	1.0615 s / batch. (data: 2.21e-01). ETA=5:30:52, max mem: 20.9 GB 
[12/01 16:09:33 visual_prompt]: 	Training 200/553. train loss: 0.2825,	0.8325 s / batch. (data: 3.39e-04). ETA=4:18:05, max mem: 20.9 GB 
[12/01 16:11:21 visual_prompt]: 	Training 300/553. train loss: 0.6899,	0.8400 s / batch. (data: 3.31e-04). ETA=4:19:01, max mem: 20.9 GB 
[12/01 16:13:09 visual_prompt]: 	Training 400/553. train loss: 0.8005,	0.8200 s / batch. (data: 3.23e-04). ETA=4:11:29, max mem: 20.9 GB 
[12/01 16:15:02 visual_prompt]: 	Training 500/553. train loss: 0.9587,	1.5670 s / batch. (data: 7.29e-01). ETA=7:57:59, max mem: 20.9 GB 
[12/01 16:16:02 visual_prompt]: Epoch 67 / 100: avg data time: 2.92e-01, avg batch time: 1.1254, average train loss: 0.8017
[12/01 16:17:07 visual_prompt]: Inference (val):avg data time: 5.45e-05, avg batch time: 0.3087, average loss: 0.6943
[12/01 16:17:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.07	
[12/01 16:17:07 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[12/01 16:19:04 visual_prompt]: 	Training 100/553. train loss: 1.7715,	0.8388 s / batch. (data: 1.07e-02). ETA=4:13:42, max mem: 20.9 GB 
[12/01 16:20:59 visual_prompt]: 	Training 200/553. train loss: 1.2503,	1.2120 s / batch. (data: 3.94e-01). ETA=6:04:35, max mem: 20.9 GB 
[12/01 16:22:48 visual_prompt]: 	Training 300/553. train loss: 0.6777,	0.8179 s / batch. (data: 3.35e-04). ETA=4:04:40, max mem: 20.9 GB 
[12/01 16:24:40 visual_prompt]: 	Training 400/553. train loss: 0.6658,	0.8325 s / batch. (data: 3.42e-04). ETA=4:07:40, max mem: 20.9 GB 
[12/01 16:26:33 visual_prompt]: 	Training 500/553. train loss: 0.6796,	0.8400 s / batch. (data: 3.23e-04). ETA=4:08:29, max mem: 20.9 GB 
[12/01 16:27:32 visual_prompt]: Epoch 68 / 100: avg data time: 2.96e-01, avg batch time: 1.1298, average train loss: 0.8712
[12/01 16:28:38 visual_prompt]: Inference (val):avg data time: 6.58e-05, avg batch time: 0.3102, average loss: 0.8419
[12/01 16:28:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.68	
[12/01 16:28:38 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[12/01 16:30:35 visual_prompt]: 	Training 100/553. train loss: 1.0139,	0.8480 s / batch. (data: 6.92e-03). ETA=4:08:41, max mem: 20.9 GB 
[12/01 16:32:28 visual_prompt]: 	Training 200/553. train loss: 1.4849,	0.8292 s / batch. (data: 1.39e-03). ETA=4:01:47, max mem: 20.9 GB 
[12/01 16:34:20 visual_prompt]: 	Training 300/553. train loss: 0.6208,	0.8173 s / batch. (data: 3.07e-04). ETA=3:56:58, max mem: 20.9 GB 
[12/01 16:36:12 visual_prompt]: 	Training 400/553. train loss: 0.5946,	0.8411 s / batch. (data: 5.44e-03). ETA=4:02:28, max mem: 20.9 GB 
[12/01 16:38:04 visual_prompt]: 	Training 500/553. train loss: 0.5688,	0.8220 s / batch. (data: 3.09e-04). ETA=3:55:34, max mem: 20.9 GB 
[12/01 16:39:03 visual_prompt]: Epoch 69 / 100: avg data time: 2.96e-01, avg batch time: 1.1294, average train loss: 0.8249
[12/01 16:40:08 visual_prompt]: Inference (val):avg data time: 1.66e-04, avg batch time: 0.3079, average loss: 0.6906
[12/01 16:40:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 61.82	
[12/01 16:40:08 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[12/01 16:42:04 visual_prompt]: 	Training 100/553. train loss: 0.4917,	0.8280 s / batch. (data: 3.45e-04). ETA=3:55:11, max mem: 20.9 GB 
[12/01 16:43:58 visual_prompt]: 	Training 200/553. train loss: 1.2882,	1.4090 s / batch. (data: 5.85e-01). ETA=6:37:53, max mem: 20.9 GB 
[12/01 16:45:51 visual_prompt]: 	Training 300/553. train loss: 0.9759,	1.5560 s / batch. (data: 7.38e-01). ETA=7:16:48, max mem: 20.9 GB 
[12/01 16:47:48 visual_prompt]: 	Training 400/553. train loss: 0.8910,	1.1765 s / batch. (data: 3.56e-01). ETA=5:28:18, max mem: 20.9 GB 
[12/01 16:49:39 visual_prompt]: 	Training 500/553. train loss: 1.1098,	0.8174 s / batch. (data: 4.69e-04). ETA=3:46:44, max mem: 20.9 GB 
[12/01 16:50:38 visual_prompt]: Epoch 70 / 100: avg data time: 3.07e-01, avg batch time: 1.1392, average train loss: 0.7856
[12/01 16:51:43 visual_prompt]: Inference (val):avg data time: 4.83e-05, avg batch time: 0.3090, average loss: 0.7192
[12/01 16:51:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.51	
[12/01 16:51:43 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[12/01 16:53:40 visual_prompt]: 	Training 100/553. train loss: 0.7738,	0.8185 s / batch. (data: 1.47e-03). ETA=3:44:56, max mem: 20.9 GB 
[12/01 16:55:34 visual_prompt]: 	Training 200/553. train loss: 0.6907,	0.8519 s / batch. (data: 4.62e-04). ETA=3:52:43, max mem: 20.9 GB 
[12/01 16:57:27 visual_prompt]: 	Training 300/553. train loss: 0.6301,	0.8262 s / batch. (data: 3.59e-04). ETA=3:44:18, max mem: 20.9 GB 
[12/01 16:59:19 visual_prompt]: 	Training 400/553. train loss: 0.8007,	0.8440 s / batch. (data: 1.09e-02). ETA=3:47:45, max mem: 20.9 GB 
[12/01 17:01:12 visual_prompt]: 	Training 500/553. train loss: 0.8443,	0.8425 s / batch. (data: 7.44e-03). ETA=3:45:56, max mem: 20.9 GB 
[12/01 17:02:12 visual_prompt]: Epoch 71 / 100: avg data time: 3.03e-01, avg batch time: 1.1360, average train loss: 0.7982
[12/01 17:03:19 visual_prompt]: Inference (val):avg data time: 5.18e-05, avg batch time: 0.3097, average loss: 1.1313
[12/01 17:03:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.90	
[12/01 17:03:19 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[12/01 17:05:20 visual_prompt]: 	Training 100/553. train loss: 0.5571,	1.6328 s / batch. (data: 7.87e-01). ETA=7:13:42, max mem: 20.9 GB 
[12/01 17:07:11 visual_prompt]: 	Training 200/553. train loss: 0.7734,	0.8269 s / batch. (data: 3.23e-04). ETA=3:38:15, max mem: 20.9 GB 
[12/01 17:09:08 visual_prompt]: 	Training 300/553. train loss: 1.0388,	0.8244 s / batch. (data: 6.34e-04). ETA=3:36:13, max mem: 20.9 GB 
[12/01 17:10:59 visual_prompt]: 	Training 400/553. train loss: 0.9353,	1.6772 s / batch. (data: 8.42e-01). ETA=7:17:05, max mem: 20.9 GB 
[12/01 17:12:49 visual_prompt]: 	Training 500/553. train loss: 0.7379,	0.8364 s / batch. (data: 5.02e-04). ETA=3:36:34, max mem: 20.9 GB 
[12/01 17:13:48 visual_prompt]: Epoch 72 / 100: avg data time: 3.04e-01, avg batch time: 1.1379, average train loss: 0.8186
[12/01 17:14:55 visual_prompt]: Inference (val):avg data time: 5.40e-05, avg batch time: 0.3099, average loss: 0.7344
[12/01 17:14:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 65.08	
[12/01 17:14:55 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[12/01 17:16:53 visual_prompt]: 	Training 100/553. train loss: 0.5257,	0.8236 s / batch. (data: 1.30e-03). ETA=3:31:10, max mem: 20.9 GB 
[12/01 17:18:48 visual_prompt]: 	Training 200/553. train loss: 0.6372,	0.8273 s / batch. (data: 3.09e-03). ETA=3:30:44, max mem: 20.9 GB 
[12/01 17:20:36 visual_prompt]: 	Training 300/553. train loss: 0.8362,	0.8280 s / batch. (data: 3.54e-04). ETA=3:29:31, max mem: 20.9 GB 
[12/01 17:22:31 visual_prompt]: 	Training 400/553. train loss: 0.6147,	0.8565 s / batch. (data: 2.96e-04). ETA=3:35:19, max mem: 20.9 GB 
[12/01 17:24:24 visual_prompt]: 	Training 500/553. train loss: 0.6047,	0.8388 s / batch. (data: 1.50e-02). ETA=3:29:29, max mem: 20.9 GB 
[12/01 17:25:22 visual_prompt]: Epoch 73 / 100: avg data time: 3.00e-01, avg batch time: 1.1338, average train loss: 0.7861
[12/01 17:26:28 visual_prompt]: Inference (val):avg data time: 5.30e-05, avg batch time: 0.3109, average loss: 0.6789
[12/01 17:26:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 66.67	
[12/01 17:26:28 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[12/01 17:28:31 visual_prompt]: 	Training 100/553. train loss: 0.8124,	0.8439 s / batch. (data: 2.44e-02). ETA=3:28:35, max mem: 20.9 GB 
[12/01 17:30:24 visual_prompt]: 	Training 200/553. train loss: 0.9233,	0.8560 s / batch. (data: 1.20e-02). ETA=3:30:09, max mem: 20.9 GB 
[12/01 17:32:16 visual_prompt]: 	Training 300/553. train loss: 0.6351,	0.8378 s / batch. (data: 3.55e-04). ETA=3:24:17, max mem: 20.9 GB 
[12/01 17:34:06 visual_prompt]: 	Training 400/553. train loss: 1.9750,	1.3192 s / batch. (data: 4.90e-01). ETA=5:19:29, max mem: 20.9 GB 
[12/01 17:35:57 visual_prompt]: 	Training 500/553. train loss: 0.7002,	2.7088 s / batch. (data: 1.89e+00). ETA=10:51:30, max mem: 20.9 GB 
[12/01 17:36:55 visual_prompt]: Epoch 74 / 100: avg data time: 2.99e-01, avg batch time: 1.1328, average train loss: 0.7581
[12/01 17:38:01 visual_prompt]: Inference (val):avg data time: 6.13e-05, avg batch time: 0.3089, average loss: 0.6723
[12/01 17:38:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 65.63	
[12/01 17:38:01 visual_prompt]: Best epoch 74: best metric: -0.672
[12/01 17:38:01 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[12/01 17:39:59 visual_prompt]: 	Training 100/553. train loss: 0.7619,	1.2493 s / batch. (data: 4.13e-01). ETA=4:57:17, max mem: 20.9 GB 
[12/01 17:41:50 visual_prompt]: 	Training 200/553. train loss: 0.8989,	0.8287 s / batch. (data: 3.56e-04). ETA=3:15:49, max mem: 20.9 GB 
[12/01 17:43:41 visual_prompt]: 	Training 300/553. train loss: 0.7406,	0.8440 s / batch. (data: 3.09e-04). ETA=3:18:02, max mem: 20.9 GB 
[12/01 17:45:36 visual_prompt]: 	Training 400/553. train loss: 1.4057,	2.6039 s / batch. (data: 1.79e+00). ETA=10:06:37, max mem: 20.9 GB 
[12/01 17:47:27 visual_prompt]: 	Training 500/553. train loss: 0.6614,	0.8251 s / batch. (data: 3.28e-04). ETA=3:10:50, max mem: 20.9 GB 
[12/01 17:48:27 visual_prompt]: Epoch 75 / 100: avg data time: 2.99e-01, avg batch time: 1.1316, average train loss: 0.7415
[12/01 17:49:32 visual_prompt]: Inference (val):avg data time: 5.14e-05, avg batch time: 0.3084, average loss: 1.1315
[12/01 17:49:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.32	
[12/01 17:49:32 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[12/01 17:51:34 visual_prompt]: 	Training 100/553. train loss: 0.9384,	0.8577 s / batch. (data: 7.12e-03). ETA=3:16:12, max mem: 20.9 GB 
[12/01 17:53:24 visual_prompt]: 	Training 200/553. train loss: 0.6976,	1.1292 s / batch. (data: 2.99e-01). ETA=4:16:25, max mem: 20.9 GB 
[12/01 17:55:14 visual_prompt]: 	Training 300/553. train loss: 0.8142,	0.8600 s / batch. (data: 4.95e-04). ETA=3:13:52, max mem: 20.9 GB 
[12/01 17:57:05 visual_prompt]: 	Training 400/553. train loss: 0.7100,	0.8354 s / batch. (data: 9.91e-04). ETA=3:06:55, max mem: 20.9 GB 
[12/01 17:58:56 visual_prompt]: 	Training 500/553. train loss: 0.5735,	0.8206 s / batch. (data: 3.55e-04). ETA=3:02:13, max mem: 20.9 GB 
[12/01 17:59:56 visual_prompt]: Epoch 76 / 100: avg data time: 2.94e-01, avg batch time: 1.1275, average train loss: 0.7498
[12/01 18:01:01 visual_prompt]: Inference (val):avg data time: 1.53e-04, avg batch time: 0.3093, average loss: 1.0890
[12/01 18:01:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.30	
[12/01 18:01:01 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[12/01 18:02:59 visual_prompt]: 	Training 100/553. train loss: 0.6749,	1.1167 s / batch. (data: 2.86e-01). ETA=4:05:09, max mem: 20.9 GB 
[12/01 18:04:53 visual_prompt]: 	Training 200/553. train loss: 0.7458,	0.8439 s / batch. (data: 1.62e-02). ETA=3:03:51, max mem: 20.9 GB 
[12/01 18:06:41 visual_prompt]: 	Training 300/553. train loss: 0.5785,	0.8317 s / batch. (data: 3.99e-04). ETA=2:59:48, max mem: 20.9 GB 
[12/01 18:08:34 visual_prompt]: 	Training 400/553. train loss: 0.5226,	0.8356 s / batch. (data: 3.22e-04). ETA=2:59:16, max mem: 20.9 GB 
[12/01 18:10:24 visual_prompt]: 	Training 500/553. train loss: 0.8049,	0.8365 s / batch. (data: 3.26e-04). ETA=2:58:03, max mem: 20.9 GB 
[12/01 18:11:22 visual_prompt]: Epoch 77 / 100: avg data time: 2.90e-01, avg batch time: 1.1232, average train loss: 0.7139
[12/01 18:12:27 visual_prompt]: Inference (val):avg data time: 5.33e-05, avg batch time: 0.3101, average loss: 0.6598
[12/01 18:12:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 69.88	
[12/01 18:12:27 visual_prompt]: Best epoch 77: best metric: -0.660
[12/01 18:12:27 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[12/01 18:14:23 visual_prompt]: 	Training 100/553. train loss: 0.5322,	0.8519 s / batch. (data: 8.04e-03). ETA=2:59:10, max mem: 20.9 GB 
[12/01 18:16:16 visual_prompt]: 	Training 200/553. train loss: 0.7188,	0.8341 s / batch. (data: 3.39e-04). ETA=2:54:01, max mem: 20.9 GB 
[12/01 18:18:08 visual_prompt]: 	Training 300/553. train loss: 0.5901,	0.8304 s / batch. (data: 3.55e-04). ETA=2:51:52, max mem: 20.9 GB 
[12/01 18:20:01 visual_prompt]: 	Training 400/553. train loss: 0.9560,	0.8280 s / batch. (data: 4.03e-04). ETA=2:50:00, max mem: 20.9 GB 
[12/01 18:21:49 visual_prompt]: 	Training 500/553. train loss: 0.6411,	1.2202 s / batch. (data: 3.73e-01). ETA=4:08:29, max mem: 20.9 GB 
[12/01 18:22:50 visual_prompt]: Epoch 78 / 100: avg data time: 2.93e-01, avg batch time: 1.1258, average train loss: 0.7757
[12/01 18:23:55 visual_prompt]: Inference (val):avg data time: 4.88e-05, avg batch time: 0.3094, average loss: 0.6484
[12/01 18:23:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 69.63	
[12/01 18:23:55 visual_prompt]: Best epoch 78: best metric: -0.648
[12/01 18:23:55 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[12/01 18:25:53 visual_prompt]: 	Training 100/553. train loss: 0.5406,	0.8560 s / batch. (data: 5.46e-03). ETA=2:52:08, max mem: 20.9 GB 
[12/01 18:27:44 visual_prompt]: 	Training 200/553. train loss: 0.8816,	0.8168 s / batch. (data: 3.18e-04). ETA=2:42:53, max mem: 20.9 GB 
[12/01 18:29:34 visual_prompt]: 	Training 300/553. train loss: 1.0488,	2.1503 s / batch. (data: 1.32e+00). ETA=7:05:15, max mem: 20.9 GB 
[12/01 18:31:30 visual_prompt]: 	Training 400/553. train loss: 0.8108,	0.8436 s / batch. (data: 1.22e-03). ETA=2:45:25, max mem: 20.9 GB 
[12/01 18:33:22 visual_prompt]: 	Training 500/553. train loss: 0.8816,	0.8320 s / batch. (data: 3.27e-04). ETA=2:41:45, max mem: 20.9 GB 
[12/01 18:34:20 visual_prompt]: Epoch 79 / 100: avg data time: 2.96e-01, avg batch time: 1.1306, average train loss: 0.7205
[12/01 18:35:25 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.3091, average loss: 0.6835
[12/01 18:35:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 66.71	
[12/01 18:35:25 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[12/01 18:37:21 visual_prompt]: 	Training 100/553. train loss: 0.5732,	0.8302 s / batch. (data: 7.84e-04). ETA=2:39:18, max mem: 20.9 GB 
[12/01 18:39:14 visual_prompt]: 	Training 200/553. train loss: 0.5974,	0.8592 s / batch. (data: 3.05e-04). ETA=2:43:25, max mem: 20.9 GB 
[12/01 18:41:05 visual_prompt]: 	Training 300/553. train loss: 0.7457,	1.8597 s / batch. (data: 1.02e+00). ETA=5:50:39, max mem: 20.9 GB 
[12/01 18:43:02 visual_prompt]: 	Training 400/553. train loss: 0.7934,	0.8248 s / batch. (data: 9.79e-04). ETA=2:34:08, max mem: 20.9 GB 
[12/01 18:44:53 visual_prompt]: 	Training 500/553. train loss: 1.0668,	1.8005 s / batch. (data: 9.51e-01). ETA=5:33:28, max mem: 20.9 GB 
[12/01 18:45:52 visual_prompt]: Epoch 80 / 100: avg data time: 3.00e-01, avg batch time: 1.1332, average train loss: 0.7210
[12/01 18:46:57 visual_prompt]: Inference (val):avg data time: 1.87e-04, avg batch time: 0.3113, average loss: 0.8517
[12/01 18:46:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.34	
[12/01 18:46:57 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[12/01 18:48:55 visual_prompt]: 	Training 100/553. train loss: 0.7026,	0.8360 s / batch. (data: 3.62e-04). ETA=2:32:42, max mem: 20.9 GB 
[12/01 18:50:48 visual_prompt]: 	Training 200/553. train loss: 0.7554,	0.8240 s / batch. (data: 3.45e-04). ETA=2:29:08, max mem: 20.9 GB 
[12/01 18:52:40 visual_prompt]: 	Training 300/553. train loss: 0.5832,	0.8769 s / batch. (data: 8.40e-03). ETA=2:37:15, max mem: 20.9 GB 
[12/01 18:54:33 visual_prompt]: 	Training 400/553. train loss: 0.7282,	1.8284 s / batch. (data: 9.99e-01). ETA=5:24:50, max mem: 20.9 GB 
[12/01 18:56:22 visual_prompt]: 	Training 500/553. train loss: 0.6736,	2.5550 s / batch. (data: 1.74e+00). ETA=7:29:41, max mem: 20.9 GB 
[12/01 18:57:21 visual_prompt]: Epoch 81 / 100: avg data time: 2.95e-01, avg batch time: 1.1273, average train loss: 0.7004
[12/01 18:58:26 visual_prompt]: Inference (val):avg data time: 6.00e-05, avg batch time: 0.3101, average loss: 0.6901
[12/01 18:58:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 67.26	
[12/01 18:58:26 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[12/01 19:00:23 visual_prompt]: 	Training 100/553. train loss: 0.3198,	0.8773 s / batch. (data: 2.15e-02). ETA=2:32:09, max mem: 20.9 GB 
[12/01 19:02:17 visual_prompt]: 	Training 200/553. train loss: 1.0443,	1.0533 s / batch. (data: 2.33e-01). ETA=3:00:56, max mem: 20.9 GB 
[12/01 19:04:08 visual_prompt]: 	Training 300/553. train loss: 1.4074,	2.5439 s / batch. (data: 1.71e+00). ETA=7:12:46, max mem: 20.9 GB 
[12/01 19:05:58 visual_prompt]: 	Training 400/553. train loss: 0.8122,	2.1242 s / batch. (data: 1.31e+00). ETA=5:57:49, max mem: 20.9 GB 
[12/01 19:07:53 visual_prompt]: 	Training 500/553. train loss: 0.6064,	0.8292 s / batch. (data: 5.22e-04). ETA=2:18:17, max mem: 20.9 GB 
[12/01 19:08:51 visual_prompt]: Epoch 82 / 100: avg data time: 2.96e-01, avg batch time: 1.1302, average train loss: 0.6851
[12/01 19:09:57 visual_prompt]: Inference (val):avg data time: 6.08e-05, avg batch time: 0.3103, average loss: 0.7530
[12/01 19:09:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 72.21	
[12/01 19:09:57 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[12/01 19:11:54 visual_prompt]: 	Training 100/553. train loss: 0.6926,	0.8445 s / batch. (data: 3.38e-04). ETA=2:18:41, max mem: 20.9 GB 
[12/01 19:13:47 visual_prompt]: 	Training 200/553. train loss: 0.5952,	0.8388 s / batch. (data: 7.95e-03). ETA=2:16:21, max mem: 20.9 GB 
[12/01 19:15:38 visual_prompt]: 	Training 300/553. train loss: 0.5727,	0.8560 s / batch. (data: 3.26e-04). ETA=2:17:43, max mem: 20.9 GB 
[12/01 19:17:32 visual_prompt]: 	Training 400/553. train loss: 0.8701,	0.8560 s / batch. (data: 3.24e-04). ETA=2:16:18, max mem: 20.9 GB 
[12/01 19:19:24 visual_prompt]: 	Training 500/553. train loss: 0.7401,	0.8670 s / batch. (data: 1.05e-02). ETA=2:16:36, max mem: 20.9 GB 
[12/01 19:20:20 visual_prompt]: Epoch 83 / 100: avg data time: 2.92e-01, avg batch time: 1.1258, average train loss: 0.6907
[12/01 19:21:24 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.3098, average loss: 0.6592
[12/01 19:21:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 71.27	
[12/01 19:21:24 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[12/01 19:23:22 visual_prompt]: 	Training 100/553. train loss: 0.4770,	0.9359 s / batch. (data: 1.08e-01). ETA=2:25:04, max mem: 20.9 GB 
[12/01 19:25:14 visual_prompt]: 	Training 200/553. train loss: 0.8109,	2.1617 s / batch. (data: 1.34e+00). ETA=5:31:29, max mem: 20.9 GB 
[12/01 19:27:05 visual_prompt]: 	Training 300/553. train loss: 1.1348,	0.8600 s / batch. (data: 5.46e-03). ETA=2:10:26, max mem: 20.9 GB 
[12/01 19:28:59 visual_prompt]: 	Training 400/553. train loss: 0.7153,	0.8440 s / batch. (data: 3.33e-04). ETA=2:06:36, max mem: 20.9 GB 
[12/01 19:30:49 visual_prompt]: 	Training 500/553. train loss: 0.4910,	0.8428 s / batch. (data: 1.21e-02). ETA=2:05:01, max mem: 20.9 GB 
[12/01 19:31:51 visual_prompt]: Epoch 84 / 100: avg data time: 3.00e-01, avg batch time: 1.1330, average train loss: 0.6598
[12/01 19:32:57 visual_prompt]: Inference (val):avg data time: 6.61e-05, avg batch time: 0.3091, average loss: 0.7924
[12/01 19:32:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 71.08	
[12/01 19:32:57 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[12/01 19:34:58 visual_prompt]: 	Training 100/553. train loss: 0.6172,	4.0265 s / batch. (data: 3.19e+00). ETA=9:47:04, max mem: 20.9 GB 
[12/01 19:36:51 visual_prompt]: 	Training 200/553. train loss: 0.4731,	1.6104 s / batch. (data: 7.96e-01). ETA=3:52:06, max mem: 20.9 GB 
[12/01 19:38:41 visual_prompt]: 	Training 300/553. train loss: 0.5386,	0.8336 s / batch. (data: 5.00e-04). ETA=1:58:45, max mem: 20.9 GB 
[12/01 19:40:30 visual_prompt]: 	Training 400/553. train loss: 0.7141,	0.8173 s / batch. (data: 3.27e-04). ETA=1:55:04, max mem: 20.9 GB 
[12/01 19:42:25 visual_prompt]: 	Training 500/553. train loss: 0.9524,	0.8221 s / batch. (data: 4.89e-04). ETA=1:54:23, max mem: 20.9 GB 
[12/01 19:43:22 visual_prompt]: Epoch 85 / 100: avg data time: 2.98e-01, avg batch time: 1.1300, average train loss: 0.6666
[12/01 19:44:28 visual_prompt]: Inference (val):avg data time: 5.72e-05, avg batch time: 0.3090, average loss: 0.6352
[12/01 19:44:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 71.16	
[12/01 19:44:28 visual_prompt]: Best epoch 85: best metric: -0.635
[12/01 19:44:28 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[12/01 19:46:27 visual_prompt]: 	Training 100/553. train loss: 0.5703,	3.9839 s / batch. (data: 3.16e+00). ETA=9:04:08, max mem: 20.9 GB 
[12/01 19:48:17 visual_prompt]: 	Training 200/553. train loss: 0.6650,	1.5160 s / batch. (data: 6.77e-01). ETA=3:24:32, max mem: 20.9 GB 
[12/01 19:50:06 visual_prompt]: 	Training 300/553. train loss: 0.6166,	0.8358 s / batch. (data: 8.00e-04). ETA=1:51:21, max mem: 20.9 GB 
[12/01 19:52:00 visual_prompt]: 	Training 400/553. train loss: 0.8643,	0.8387 s / batch. (data: 8.69e-04). ETA=1:50:21, max mem: 20.9 GB 
[12/01 19:53:53 visual_prompt]: 	Training 500/553. train loss: 0.3777,	1.5545 s / batch. (data: 7.26e-01). ETA=3:21:57, max mem: 20.9 GB 
[12/01 19:54:52 visual_prompt]: Epoch 86 / 100: avg data time: 2.94e-01, avg batch time: 1.1276, average train loss: 0.6586
[12/01 19:55:58 visual_prompt]: Inference (val):avg data time: 6.35e-05, avg batch time: 0.3093, average loss: 0.6307
[12/01 19:55:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 71.46	
[12/01 19:55:58 visual_prompt]: Best epoch 86: best metric: -0.631
[12/01 19:55:58 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[12/01 19:57:58 visual_prompt]: 	Training 100/553. train loss: 0.7061,	0.8240 s / batch. (data: 3.33e-04). ETA=1:44:56, max mem: 20.9 GB 
[12/01 19:59:52 visual_prompt]: 	Training 200/553. train loss: 0.3909,	0.8392 s / batch. (data: 3.52e-04). ETA=1:45:29, max mem: 20.9 GB 
[12/01 20:01:44 visual_prompt]: 	Training 300/553. train loss: 0.6450,	2.0523 s / batch. (data: 1.24e+00). ETA=4:14:32, max mem: 20.9 GB 
[12/01 20:03:33 visual_prompt]: 	Training 400/553. train loss: 0.8352,	0.8462 s / batch. (data: 6.14e-03). ETA=1:43:32, max mem: 20.9 GB 
[12/01 20:05:25 visual_prompt]: 	Training 500/553. train loss: 0.7257,	0.8365 s / batch. (data: 3.29e-04). ETA=1:40:57, max mem: 20.9 GB 
[12/01 20:06:23 visual_prompt]: Epoch 87 / 100: avg data time: 2.96e-01, avg batch time: 1.1300, average train loss: 0.6455
[12/01 20:07:29 visual_prompt]: Inference (val):avg data time: 1.56e-04, avg batch time: 0.3093, average loss: 0.7614
[12/01 20:07:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 68.64	
[12/01 20:07:29 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[12/01 20:09:25 visual_prompt]: 	Training 100/553. train loss: 0.3209,	0.8406 s / batch. (data: 8.49e-03). ETA=1:39:18, max mem: 20.9 GB 
[12/01 20:11:17 visual_prompt]: 	Training 200/553. train loss: 0.3696,	0.8480 s / batch. (data: 3.47e-04). ETA=1:38:46, max mem: 20.9 GB 
[12/01 20:13:11 visual_prompt]: 	Training 300/553. train loss: 0.3727,	0.8162 s / batch. (data: 4.56e-04). ETA=1:33:42, max mem: 20.9 GB 
[12/01 20:15:06 visual_prompt]: 	Training 400/553. train loss: 0.5437,	2.6923 s / batch. (data: 1.86e+00). ETA=5:04:38, max mem: 20.9 GB 
[12/01 20:16:55 visual_prompt]: 	Training 500/553. train loss: 0.9720,	1.4980 s / batch. (data: 6.40e-01). ETA=2:47:00, max mem: 20.9 GB 
[12/01 20:17:53 visual_prompt]: Epoch 88 / 100: avg data time: 2.94e-01, avg batch time: 1.1273, average train loss: 0.6463
[12/01 20:18:59 visual_prompt]: Inference (val):avg data time: 6.16e-05, avg batch time: 0.3107, average loss: 0.6165
[12/01 20:18:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 72.63	
[12/01 20:18:59 visual_prompt]: Best epoch 88: best metric: -0.616
[12/01 20:18:59 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[12/01 20:20:57 visual_prompt]: 	Training 100/553. train loss: 0.5181,	0.8324 s / batch. (data: 5.44e-03). ETA=1:30:40, max mem: 20.9 GB 
[12/01 20:22:47 visual_prompt]: 	Training 200/553. train loss: 0.7454,	0.8275 s / batch. (data: 6.28e-03). ETA=1:28:45, max mem: 20.9 GB 
[12/01 20:24:41 visual_prompt]: 	Training 300/553. train loss: 0.6094,	0.8314 s / batch. (data: 6.58e-04). ETA=1:27:47, max mem: 20.9 GB 
[12/01 20:26:35 visual_prompt]: 	Training 400/553. train loss: 0.8542,	0.8675 s / batch. (data: 6.81e-03). ETA=1:30:09, max mem: 20.9 GB 
[12/01 20:28:28 visual_prompt]: 	Training 500/553. train loss: 0.8532,	0.8295 s / batch. (data: 9.36e-04). ETA=1:24:49, max mem: 20.9 GB 
[12/01 20:29:25 visual_prompt]: Epoch 89 / 100: avg data time: 2.98e-01, avg batch time: 1.1320, average train loss: 0.6345
[12/01 20:30:31 visual_prompt]: Inference (val):avg data time: 6.00e-05, avg batch time: 0.3100, average loss: 0.6135
[12/01 20:30:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 72.97	
[12/01 20:30:31 visual_prompt]: Best epoch 89: best metric: -0.613
[12/01 20:30:31 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[12/01 20:32:31 visual_prompt]: 	Training 100/553. train loss: 0.7507,	0.8141 s / batch. (data: 3.54e-04). ETA=1:21:10, max mem: 20.9 GB 
[12/01 20:34:21 visual_prompt]: 	Training 200/553. train loss: 0.4661,	0.8184 s / batch. (data: 3.39e-04). ETA=1:20:14, max mem: 20.9 GB 
[12/01 20:36:13 visual_prompt]: 	Training 300/553. train loss: 1.1158,	0.8532 s / batch. (data: 1.05e-02). ETA=1:22:13, max mem: 20.9 GB 
[12/01 20:38:05 visual_prompt]: 	Training 400/553. train loss: 1.0277,	0.8565 s / batch. (data: 2.05e-02). ETA=1:21:07, max mem: 20.9 GB 
[12/01 20:39:57 visual_prompt]: 	Training 500/553. train loss: 1.2300,	0.8327 s / batch. (data: 3.16e-04). ETA=1:17:29, max mem: 20.9 GB 
[12/01 20:40:53 visual_prompt]: Epoch 90 / 100: avg data time: 2.91e-01, avg batch time: 1.1250, average train loss: 0.6280
[12/01 20:41:58 visual_prompt]: Inference (val):avg data time: 2.21e-04, avg batch time: 0.3094, average loss: 0.6149
[12/01 20:41:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 71.65	
[12/01 20:41:58 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[12/01 20:43:56 visual_prompt]: 	Training 100/553. train loss: 0.9312,	0.8531 s / batch. (data: 2.11e-02). ETA=1:17:12, max mem: 20.9 GB 
[12/01 20:45:51 visual_prompt]: 	Training 200/553. train loss: 0.7834,	0.8286 s / batch. (data: 9.04e-03). ETA=1:13:36, max mem: 20.9 GB 
[12/01 20:47:46 visual_prompt]: 	Training 300/553. train loss: 1.2897,	0.8192 s / batch. (data: 4.01e-04). ETA=1:11:24, max mem: 20.9 GB 
[12/01 20:49:40 visual_prompt]: 	Training 400/553. train loss: 0.5518,	2.2440 s / batch. (data: 1.42e+00). ETA=3:11:51, max mem: 20.9 GB 
[12/01 20:51:28 visual_prompt]: 	Training 500/553. train loss: 0.6778,	1.4633 s / batch. (data: 6.29e-01). ETA=2:02:40, max mem: 20.9 GB 
[12/01 20:52:25 visual_prompt]: Epoch 91 / 100: avg data time: 3.00e-01, avg batch time: 1.1330, average train loss: 0.6197
[12/01 20:53:30 visual_prompt]: Inference (val):avg data time: 4.94e-05, avg batch time: 0.3100, average loss: 0.6227
[12/01 20:53:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 71.77	
[12/01 20:53:30 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[12/01 20:55:30 visual_prompt]: 	Training 100/553. train loss: 0.5841,	2.6509 s / batch. (data: 1.81e+00). ETA=3:35:28, max mem: 20.9 GB 
[12/01 20:57:22 visual_prompt]: 	Training 200/553. train loss: 0.5266,	0.8635 s / batch. (data: 7.67e-03). ETA=1:08:44, max mem: 20.9 GB 
[12/01 20:59:13 visual_prompt]: 	Training 300/553. train loss: 0.6199,	0.8275 s / batch. (data: 5.56e-04). ETA=1:04:30, max mem: 20.9 GB 
[12/01 21:01:09 visual_prompt]: 	Training 400/553. train loss: 0.9567,	0.8287 s / batch. (data: 1.22e-02). ETA=1:03:12, max mem: 20.9 GB 
[12/01 21:03:00 visual_prompt]: 	Training 500/553. train loss: 0.5725,	0.8400 s / batch. (data: 3.56e-04). ETA=1:02:40, max mem: 20.9 GB 
[12/01 21:03:59 visual_prompt]: Epoch 92 / 100: avg data time: 3.04e-01, avg batch time: 1.1383, average train loss: 0.6084
[12/01 21:05:05 visual_prompt]: Inference (val):avg data time: 6.76e-04, avg batch time: 0.3093, average loss: 0.6714
[12/01 21:05:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 69.60	
[12/01 21:05:05 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[12/01 21:07:02 visual_prompt]: 	Training 100/553. train loss: 0.4266,	0.8289 s / batch. (data: 7.96e-03). ETA=0:59:44, max mem: 20.9 GB 
[12/01 21:08:55 visual_prompt]: 	Training 200/553. train loss: 0.9783,	1.5310 s / batch. (data: 6.88e-01). ETA=1:47:46, max mem: 20.9 GB 
[12/01 21:10:47 visual_prompt]: 	Training 300/553. train loss: 0.7009,	1.3725 s / batch. (data: 5.43e-01). ETA=1:34:19, max mem: 20.9 GB 
[12/01 21:12:39 visual_prompt]: 	Training 400/553. train loss: 0.4904,	1.7481 s / batch. (data: 9.14e-01). ETA=1:57:14, max mem: 20.9 GB 
[12/01 21:14:33 visual_prompt]: 	Training 500/553. train loss: 0.6211,	0.8206 s / batch. (data: 3.11e-04). ETA=0:53:39, max mem: 20.9 GB 
[12/01 21:15:29 visual_prompt]: Epoch 93 / 100: avg data time: 2.96e-01, avg batch time: 1.1286, average train loss: 0.5906
[12/01 21:16:35 visual_prompt]: Inference (val):avg data time: 6.24e-05, avg batch time: 0.3098, average loss: 0.6100
[12/01 21:16:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 71.54	rocauc: 75.28	
[12/01 21:16:35 visual_prompt]: Best epoch 93: best metric: -0.610
[12/01 21:16:35 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[12/01 21:18:32 visual_prompt]: 	Training 100/553. train loss: 0.7136,	0.9996 s / batch. (data: 1.61e-01). ETA=1:02:49, max mem: 20.9 GB 
[12/01 21:20:23 visual_prompt]: 	Training 200/553. train loss: 1.1199,	1.4400 s / batch. (data: 6.13e-01). ETA=1:28:06, max mem: 20.9 GB 
[12/01 21:22:20 visual_prompt]: 	Training 300/553. train loss: 1.0316,	0.8439 s / batch. (data: 1.42e-03). ETA=0:50:13, max mem: 20.9 GB 
[12/01 21:24:11 visual_prompt]: 	Training 400/553. train loss: 0.8759,	1.1368 s / batch. (data: 3.17e-01). ETA=1:05:45, max mem: 20.9 GB 
[12/01 21:26:01 visual_prompt]: 	Training 500/553. train loss: 0.5165,	1.0478 s / batch. (data: 2.05e-01). ETA=0:58:52, max mem: 20.9 GB 
[12/01 21:27:03 visual_prompt]: Epoch 94 / 100: avg data time: 3.02e-01, avg batch time: 1.1362, average train loss: 0.5748
[12/01 21:28:10 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.3117, average loss: 0.6560
[12/01 21:28:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 72.46	
[12/01 21:28:10 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[12/01 21:30:08 visual_prompt]: 	Training 100/553. train loss: 0.4243,	0.9378 s / batch. (data: 9.77e-02). ETA=0:50:17, max mem: 20.9 GB 
[12/01 21:32:03 visual_prompt]: 	Training 200/553. train loss: 0.4114,	1.3321 s / batch. (data: 5.03e-01). ETA=1:09:13, max mem: 20.9 GB 
[12/01 21:33:55 visual_prompt]: 	Training 300/553. train loss: 0.5563,	2.1733 s / batch. (data: 1.35e+00). ETA=1:49:19, max mem: 20.9 GB 
[12/01 21:35:48 visual_prompt]: 	Training 400/553. train loss: 0.8174,	2.4880 s / batch. (data: 1.67e+00). ETA=2:00:59, max mem: 20.9 GB 
[12/01 21:37:40 visual_prompt]: 	Training 500/553. train loss: 0.8099,	0.8526 s / batch. (data: 4.10e-04). ETA=0:40:02, max mem: 20.9 GB 
[12/01 21:38:38 visual_prompt]: Epoch 95 / 100: avg data time: 3.02e-01, avg batch time: 1.1360, average train loss: 0.5709
[12/01 21:39:43 visual_prompt]: Inference (val):avg data time: 4.73e-05, avg batch time: 0.3088, average loss: 0.6438
[12/01 21:39:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 72.91	
[12/01 21:39:43 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[12/01 21:41:43 visual_prompt]: 	Training 100/553. train loss: 0.5854,	0.8285 s / batch. (data: 3.45e-04). ETA=0:36:47, max mem: 20.9 GB 
[12/01 21:43:35 visual_prompt]: 	Training 200/553. train loss: 1.0042,	0.8161 s / batch. (data: 9.13e-04). ETA=0:34:53, max mem: 20.9 GB 
[12/01 21:45:25 visual_prompt]: 	Training 300/553. train loss: 0.5373,	1.2120 s / batch. (data: 3.85e-01). ETA=0:49:47, max mem: 20.9 GB 
[12/01 21:47:17 visual_prompt]: 	Training 400/553. train loss: 0.4507,	0.8738 s / batch. (data: 5.31e-03). ETA=0:34:26, max mem: 20.9 GB 
[12/01 21:49:09 visual_prompt]: 	Training 500/553. train loss: 0.5478,	0.8557 s / batch. (data: 1.21e-03). ETA=0:32:18, max mem: 20.9 GB 
[12/01 21:50:06 visual_prompt]: Epoch 96 / 100: avg data time: 2.95e-01, avg batch time: 1.1272, average train loss: 0.5648
[12/01 21:51:13 visual_prompt]: Inference (val):avg data time: 2.51e-04, avg batch time: 0.3102, average loss: 0.6012
[12/01 21:51:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.07	rocauc: 73.67	
[12/01 21:51:13 visual_prompt]: Best epoch 96: best metric: -0.601
[12/01 21:51:13 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[12/01 21:53:09 visual_prompt]: 	Training 100/553. train loss: 0.6683,	1.6131 s / batch. (data: 7.89e-01). ETA=0:56:46, max mem: 20.9 GB 
[12/01 21:55:03 visual_prompt]: 	Training 200/553. train loss: 0.3656,	1.4414 s / batch. (data: 6.24e-01). ETA=0:48:20, max mem: 20.9 GB 
[12/01 21:56:56 visual_prompt]: 	Training 300/553. train loss: 0.6272,	0.8320 s / batch. (data: 3.19e-04). ETA=0:26:30, max mem: 20.9 GB 
[12/01 21:58:49 visual_prompt]: 	Training 400/553. train loss: 0.2865,	0.8598 s / batch. (data: 7.98e-03). ETA=0:25:57, max mem: 20.9 GB 
[12/01 22:00:40 visual_prompt]: 	Training 500/553. train loss: 0.9724,	0.8452 s / batch. (data: 1.03e-03). ETA=0:24:06, max mem: 20.9 GB 
[12/01 22:01:40 visual_prompt]: Epoch 97 / 100: avg data time: 3.01e-01, avg batch time: 1.1335, average train loss: 0.5494
[12/01 22:02:46 visual_prompt]: Inference (val):avg data time: 6.05e-05, avg batch time: 0.3111, average loss: 0.6139
[12/01 22:02:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 73.22	
[12/01 22:02:46 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[12/01 22:04:45 visual_prompt]: 	Training 100/553. train loss: 0.2786,	0.8363 s / batch. (data: 8.18e-04). ETA=0:21:43, max mem: 20.9 GB 
[12/01 22:06:36 visual_prompt]: 	Training 200/553. train loss: 0.2907,	0.8337 s / batch. (data: 3.09e-03). ETA=0:20:16, max mem: 20.9 GB 
[12/01 22:08:29 visual_prompt]: 	Training 300/553. train loss: 0.3585,	3.0262 s / batch. (data: 2.21e+00). ETA=1:08:32, max mem: 20.9 GB 
[12/01 22:10:23 visual_prompt]: 	Training 400/553. train loss: 0.3514,	2.4040 s / batch. (data: 1.57e+00). ETA=0:50:26, max mem: 20.9 GB 
[12/01 22:12:15 visual_prompt]: 	Training 500/553. train loss: 0.7620,	0.8176 s / batch. (data: 3.45e-04). ETA=0:15:47, max mem: 20.9 GB 
[12/01 22:13:13 visual_prompt]: Epoch 98 / 100: avg data time: 3.00e-01, avg batch time: 1.1345, average train loss: 0.5420
[12/01 22:14:19 visual_prompt]: Inference (val):avg data time: 2.32e-04, avg batch time: 0.3096, average loss: 0.6379
[12/01 22:14:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 72.28	
[12/01 22:14:19 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[12/01 22:16:16 visual_prompt]: 	Training 100/553. train loss: 0.4700,	0.8360 s / batch. (data: 1.20e-02). ETA=0:14:01, max mem: 20.9 GB 
[12/01 22:18:10 visual_prompt]: 	Training 200/553. train loss: 0.1952,	1.3566 s / batch. (data: 5.26e-01). ETA=0:20:29, max mem: 20.9 GB 
[12/01 22:20:05 visual_prompt]: 	Training 300/553. train loss: 0.7354,	1.7880 s / batch. (data: 9.47e-01). ETA=0:24:01, max mem: 20.9 GB 
[12/01 22:21:54 visual_prompt]: 	Training 400/553. train loss: 0.5205,	0.8708 s / batch. (data: 5.51e-03). ETA=0:10:14, max mem: 20.9 GB 
[12/01 22:23:45 visual_prompt]: 	Training 500/553. train loss: 0.2793,	0.8355 s / batch. (data: 1.29e-03). ETA=0:08:26, max mem: 20.9 GB 
[12/01 22:24:43 visual_prompt]: Epoch 99 / 100: avg data time: 2.94e-01, avg batch time: 1.1278, average train loss: 0.5320
[12/01 22:25:48 visual_prompt]: Inference (val):avg data time: 5.69e-05, avg batch time: 0.3083, average loss: 0.6301
[12/01 22:25:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 72.18	
[12/01 22:25:48 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[12/01 22:27:47 visual_prompt]: 	Training 100/553. train loss: 0.3905,	1.2359 s / batch. (data: 4.07e-01). ETA=0:09:19, max mem: 20.9 GB 
[12/01 22:29:38 visual_prompt]: 	Training 200/553. train loss: 0.5387,	0.8352 s / batch. (data: 6.67e-04). ETA=0:04:54, max mem: 20.9 GB 
[12/01 22:31:30 visual_prompt]: 	Training 300/553. train loss: 0.2897,	1.0319 s / batch. (data: 1.96e-01). ETA=0:04:21, max mem: 20.9 GB 
[12/01 22:33:22 visual_prompt]: 	Training 400/553. train loss: 0.4765,	0.8431 s / batch. (data: 8.39e-03). ETA=0:02:08, max mem: 20.9 GB 
[12/01 22:35:13 visual_prompt]: 	Training 500/553. train loss: 0.2546,	0.8205 s / batch. (data: 3.21e-04). ETA=0:00:43, max mem: 20.9 GB 
[12/01 22:36:10 visual_prompt]: Epoch 100 / 100: avg data time: 2.93e-01, avg batch time: 1.1250, average train loss: 0.5299
[12/01 22:37:15 visual_prompt]: Inference (val):avg data time: 5.23e-05, avg batch time: 0.3089, average loss: 0.6420
[12/01 22:37:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 72.24	
[12/01 22:37:15 visual_prompt]: Rank of current process: 0. World size: 1
[12/01 22:37:15 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/01 22:37:15 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/01 22:37:15 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/01 22:37:15 visual_prompt]: Training with config:
[12/01 22:37:15 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr1.0_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/01 22:37:15 visual_prompt]: Loading training data...
[12/01 22:37:15 visual_prompt]: Constructing mammo-cbis dataset train...
[12/01 22:37:15 visual_prompt]: Loading validation data...
[12/01 22:37:15 visual_prompt]: Constructing mammo-cbis dataset val...
[12/01 22:37:15 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[12/01 22:37:19 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[12/01 22:37:19 visual_prompt]: tuned percent:0.525
[12/01 22:37:19 visual_prompt]: Device used for model: 0
[12/01 22:37:19 visual_prompt]: Setting up Evaluator...
[12/01 22:37:19 visual_prompt]: Setting up Trainer...
[12/01 22:37:19 visual_prompt]: 	Setting up the optimizer...
[12/01 22:37:19 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/01 22:39:16 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8291 s / batch. (data: 3.53e-04). ETA=12:42:44, max mem: 20.9 GB 
[12/01 22:41:07 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8319 s / batch. (data: 5.76e-04). ETA=12:43:56, max mem: 20.9 GB 
[12/01 22:43:02 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.2709 s / batch. (data: 1.44e+00). ETA=1 day, 10:41:39, max mem: 20.9 GB 
[12/01 22:44:52 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8167 s / batch. (data: 3.74e-04). ETA=12:27:15, max mem: 20.9 GB 
[12/01 22:46:47 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8373 s / batch. (data: 3.23e-04). ETA=12:44:44, max mem: 20.9 GB 
[12/01 22:47:45 visual_prompt]: Epoch 1 / 100: avg data time: 2.98e-01, avg batch time: 1.1319, average train loss: 1.5403
[12/01 22:48:51 visual_prompt]: Inference (val):avg data time: 1.28e-04, avg batch time: 0.3110, average loss: 1.5201
[12/01 22:48:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[12/01 22:48:51 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[12/01 22:50:47 visual_prompt]: 	Training 100/553. train loss: 0.7442,	0.8225 s / batch. (data: 3.81e-04). ETA=12:29:08, max mem: 20.9 GB 
[12/01 22:52:38 visual_prompt]: 	Training 200/553. train loss: 0.0236,	1.4800 s / batch. (data: 6.48e-01). ETA=22:25:28, max mem: 20.9 GB 
[12/01 22:54:33 visual_prompt]: 	Training 300/553. train loss: 0.7331,	1.4699 s / batch. (data: 6.44e-01). ETA=22:13:51, max mem: 20.9 GB 
[12/01 22:56:23 visual_prompt]: 	Training 400/553. train loss: 1.0297,	0.8440 s / batch. (data: 3.50e-04). ETA=12:44:28, max mem: 20.9 GB 
[12/01 22:58:18 visual_prompt]: 	Training 500/553. train loss: 0.6588,	0.8445 s / batch. (data: 3.29e-04). ETA=12:43:30, max mem: 20.9 GB 
[12/01 22:59:15 visual_prompt]: Epoch 2 / 100: avg data time: 2.96e-01, avg batch time: 1.1290, average train loss: 0.9814
[12/01 23:00:21 visual_prompt]: Inference (val):avg data time: 5.62e-05, avg batch time: 0.3110, average loss: 1.2745
[12/01 23:00:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.69	
[12/01 23:00:21 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[12/01 23:02:17 visual_prompt]: 	Training 100/553. train loss: 1.4617,	0.8319 s / batch. (data: 7.88e-04). ETA=12:29:58, max mem: 20.9 GB 
[12/01 23:04:11 visual_prompt]: 	Training 200/553. train loss: 0.8717,	0.8373 s / batch. (data: 3.36e-04). ETA=12:33:28, max mem: 20.9 GB 
[12/01 23:06:02 visual_prompt]: 	Training 300/553. train loss: 0.6260,	0.8279 s / batch. (data: 4.92e-04). ETA=12:23:40, max mem: 20.9 GB 
[12/01 23:07:56 visual_prompt]: 	Training 400/553. train loss: 3.8072,	0.8559 s / batch. (data: 6.33e-04). ETA=12:47:21, max mem: 20.9 GB 
[12/01 23:09:50 visual_prompt]: 	Training 500/553. train loss: 0.7178,	1.7597 s / batch. (data: 9.22e-01). ETA=1 day, 2:14:46, max mem: 20.9 GB 
[12/01 23:10:46 visual_prompt]: Epoch 3 / 100: avg data time: 2.96e-01, avg batch time: 1.1298, average train loss: 1.0506
[12/01 23:11:52 visual_prompt]: Inference (val):avg data time: 6.38e-05, avg batch time: 0.3099, average loss: 0.7089
[12/01 23:11:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 59.41	
[12/01 23:11:52 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[12/01 23:13:50 visual_prompt]: 	Training 100/553. train loss: 0.7360,	0.8380 s / batch. (data: 3.27e-04). ETA=12:27:47, max mem: 20.9 GB 
[12/01 23:15:43 visual_prompt]: 	Training 200/553. train loss: 0.5707,	0.8520 s / batch. (data: 7.92e-03). ETA=12:38:54, max mem: 20.9 GB 
[12/01 23:17:37 visual_prompt]: 	Training 300/553. train loss: 0.6907,	1.9469 s / batch. (data: 1.12e+00). ETA=1 day, 4:50:50, max mem: 20.9 GB 
[12/01 23:19:24 visual_prompt]: 	Training 400/553. train loss: 0.9989,	1.9828 s / batch. (data: 1.16e+00). ETA=1 day, 5:19:26, max mem: 20.9 GB 
[12/01 23:21:18 visual_prompt]: 	Training 500/553. train loss: 0.1562,	4.0929 s / batch. (data: 3.27e+00). ETA=2 days, 12:24:59, max mem: 20.9 GB 
[12/01 23:22:17 visual_prompt]: Epoch 4 / 100: avg data time: 2.96e-01, avg batch time: 1.1296, average train loss: 1.1074
[12/01 23:23:21 visual_prompt]: Inference (val):avg data time: 4.49e-05, avg batch time: 0.3102, average loss: 1.4242
[12/01 23:23:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.75	
[12/01 23:23:21 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[12/01 23:25:16 visual_prompt]: 	Training 100/553. train loss: 2.8956,	0.8240 s / batch. (data: 3.35e-04). ETA=12:07:42, max mem: 20.9 GB 
[12/01 23:27:09 visual_prompt]: 	Training 200/553. train loss: 1.7048,	1.6479 s / batch. (data: 7.65e-01). ETA=1 day, 0:12:35, max mem: 20.9 GB 
[12/01 23:29:03 visual_prompt]: 	Training 300/553. train loss: 3.8031,	0.8645 s / batch. (data: 3.13e-04). ETA=12:40:35, max mem: 20.9 GB 
[12/01 23:30:54 visual_prompt]: 	Training 400/553. train loss: 2.9447,	0.8494 s / batch. (data: 2.29e-03). ETA=12:25:50, max mem: 20.9 GB 
[12/01 23:32:46 visual_prompt]: 	Training 500/553. train loss: 1.0125,	0.8407 s / batch. (data: 3.12e-04). ETA=12:16:49, max mem: 20.9 GB 
[12/01 23:33:45 visual_prompt]: Epoch 5 / 100: avg data time: 2.95e-01, avg batch time: 1.1277, average train loss: 1.3246
[12/01 23:34:50 visual_prompt]: Inference (val):avg data time: 5.13e-05, avg batch time: 0.3088, average loss: 1.9187
[12/01 23:34:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.00	
[12/01 23:34:50 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[12/01 23:36:49 visual_prompt]: 	Training 100/553. train loss: 0.7886,	0.8532 s / batch. (data: 3.83e-04). ETA=12:25:36, max mem: 20.9 GB 
[12/01 23:38:40 visual_prompt]: 	Training 200/553. train loss: 4.9891,	0.8535 s / batch. (data: 2.95e-02). ETA=12:24:27, max mem: 20.9 GB 
[12/01 23:40:30 visual_prompt]: 	Training 300/553. train loss: 0.6830,	0.8237 s / batch. (data: 5.44e-03). ETA=11:57:04, max mem: 20.9 GB 
[12/01 23:42:26 visual_prompt]: 	Training 400/553. train loss: 0.6291,	0.8169 s / batch. (data: 3.09e-04). ETA=11:49:48, max mem: 20.9 GB 
[12/01 23:44:15 visual_prompt]: 	Training 500/553. train loss: 3.6114,	0.9859 s / batch. (data: 1.54e-01). ETA=14:15:02, max mem: 20.9 GB 
[12/01 23:45:12 visual_prompt]: Epoch 6 / 100: avg data time: 2.92e-01, avg batch time: 1.1247, average train loss: 1.4182
[12/01 23:46:17 visual_prompt]: Inference (val):avg data time: 2.31e-04, avg batch time: 0.3103, average loss: 1.3055
[12/01 23:46:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.08	
[12/01 23:46:17 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[12/01 23:48:11 visual_prompt]: 	Training 100/553. train loss: 1.0775,	0.8345 s / batch. (data: 1.32e-03). ETA=12:01:35, max mem: 20.9 GB 
[12/01 23:50:02 visual_prompt]: 	Training 200/553. train loss: 0.6079,	1.6838 s / batch. (data: 8.53e-01). ETA=1 day, 0:13:10, max mem: 20.9 GB 
[12/01 23:52:00 visual_prompt]: 	Training 300/553. train loss: 0.7782,	2.5368 s / batch. (data: 1.70e+00). ETA=1 day, 12:25:05, max mem: 20.9 GB 
[12/01 23:53:52 visual_prompt]: 	Training 400/553. train loss: 0.8291,	2.2728 s / batch. (data: 1.45e+00). ETA=1 day, 8:33:56, max mem: 20.9 GB 
[12/01 23:55:42 visual_prompt]: 	Training 500/553. train loss: 1.8916,	1.5714 s / batch. (data: 7.43e-01). ETA=22:28:16, max mem: 20.9 GB 
[12/01 23:56:39 visual_prompt]: Epoch 7 / 100: avg data time: 2.90e-01, avg batch time: 1.1230, average train loss: 1.3133
[12/01 23:57:43 visual_prompt]: Inference (val):avg data time: 5.13e-05, avg batch time: 0.3113, average loss: 0.6972
[12/01 23:57:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 64.01	
[12/01 23:57:43 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[12/01 23:59:38 visual_prompt]: 	Training 100/553. train loss: 3.1443,	0.8382 s / batch. (data: 4.55e-04). ETA=11:57:06, max mem: 20.9 GB 
[12/02 00:01:32 visual_prompt]: 	Training 200/553. train loss: 1.8945,	0.8371 s / batch. (data: 1.07e-02). ETA=11:54:42, max mem: 20.9 GB 
[12/02 00:03:25 visual_prompt]: 	Training 300/553. train loss: 3.2202,	0.8278 s / batch. (data: 1.42e-03). ETA=11:45:23, max mem: 20.9 GB 
[12/02 00:05:17 visual_prompt]: 	Training 400/553. train loss: 0.7549,	1.5160 s / batch. (data: 6.81e-01). ETA=21:29:18, max mem: 20.9 GB 
[12/02 00:07:10 visual_prompt]: 	Training 500/553. train loss: 2.4947,	1.7791 s / batch. (data: 9.33e-01). ETA=1 day, 1:10:05, max mem: 20.9 GB 
[12/02 00:08:08 visual_prompt]: Epoch 8 / 100: avg data time: 2.98e-01, avg batch time: 1.1306, average train loss: 1.8896
[12/02 00:09:13 visual_prompt]: Inference (val):avg data time: 2.45e-04, avg batch time: 0.3102, average loss: 1.0552
[12/02 00:09:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.93	
[12/02 00:09:13 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[12/02 00:11:10 visual_prompt]: 	Training 100/553. train loss: 0.0021,	0.8332 s / batch. (data: 1.08e-02). ETA=11:45:08, max mem: 20.9 GB 
[12/02 00:13:01 visual_prompt]: 	Training 200/553. train loss: 0.5775,	0.8372 s / batch. (data: 9.16e-03). ETA=11:47:04, max mem: 20.9 GB 
[12/02 00:14:54 visual_prompt]: 	Training 300/553. train loss: 1.3700,	2.4655 s / batch. (data: 1.63e+00). ETA=1 day, 10:38:14, max mem: 20.9 GB 
[12/02 00:16:47 visual_prompt]: 	Training 400/553. train loss: 1.1137,	0.8359 s / batch. (data: 5.39e-04). ETA=11:43:13, max mem: 20.9 GB 
[12/02 00:18:41 visual_prompt]: 	Training 500/553. train loss: 1.7083,	1.4518 s / batch. (data: 6.10e-01). ETA=20:18:53, max mem: 20.9 GB 
[12/02 00:19:38 visual_prompt]: Epoch 9 / 100: avg data time: 2.96e-01, avg batch time: 1.1297, average train loss: 1.4716
[12/02 00:20:43 visual_prompt]: Inference (val):avg data time: 4.50e-05, avg batch time: 0.3078, average loss: 1.2125
[12/02 00:20:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.94	
[12/02 00:20:43 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[12/02 00:22:41 visual_prompt]: 	Training 100/553. train loss: 3.3886,	0.8281 s / batch. (data: 3.59e-04). ETA=11:33:08, max mem: 20.9 GB 
[12/02 00:24:33 visual_prompt]: 	Training 200/553. train loss: 0.6082,	0.8410 s / batch. (data: 1.32e-02). ETA=11:42:35, max mem: 20.9 GB 
[12/02 00:26:24 visual_prompt]: 	Training 300/553. train loss: 0.6965,	0.8355 s / batch. (data: 1.31e-03). ETA=11:36:36, max mem: 20.9 GB 
[12/02 00:28:12 visual_prompt]: 	Training 400/553. train loss: 1.5656,	0.8257 s / batch. (data: 3.80e-04). ETA=11:27:00, max mem: 20.9 GB 
[12/02 00:30:07 visual_prompt]: 	Training 500/553. train loss: 0.6589,	1.4240 s / batch. (data: 5.93e-01). ETA=19:42:29, max mem: 20.9 GB 
[12/02 00:31:06 visual_prompt]: Epoch 10 / 100: avg data time: 2.93e-01, avg batch time: 1.1265, average train loss: 2.1908
[12/02 00:32:11 visual_prompt]: Inference (val):avg data time: 4.74e-05, avg batch time: 0.3102, average loss: 1.2408
[12/02 00:32:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.24	
[12/02 00:32:11 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[12/02 00:34:11 visual_prompt]: 	Training 100/553. train loss: 0.5043,	0.8257 s / batch. (data: 3.23e-04). ETA=11:23:33, max mem: 20.9 GB 
[12/02 00:36:06 visual_prompt]: 	Training 200/553. train loss: 1.3819,	0.8530 s / batch. (data: 8.69e-04). ETA=11:44:44, max mem: 20.9 GB 
[12/02 00:37:56 visual_prompt]: 	Training 300/553. train loss: 0.1835,	2.7047 s / batch. (data: 1.87e+00). ETA=1 day, 13:09:59, max mem: 20.9 GB 
[12/02 00:39:46 visual_prompt]: 	Training 400/553. train loss: 0.8650,	0.8687 s / batch. (data: 1.05e-02). ETA=11:54:49, max mem: 20.9 GB 
[12/02 00:41:36 visual_prompt]: 	Training 500/553. train loss: 1.4664,	0.8320 s / batch. (data: 3.34e-04). ETA=11:23:12, max mem: 20.9 GB 
[12/02 00:42:34 visual_prompt]: Epoch 11 / 100: avg data time: 2.94e-01, avg batch time: 1.1264, average train loss: 1.3125
[12/02 00:43:39 visual_prompt]: Inference (val):avg data time: 6.16e-05, avg batch time: 0.3101, average loss: 0.6698
[12/02 00:43:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 63.50	
[12/02 00:43:39 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[12/02 00:45:38 visual_prompt]: 	Training 100/553. train loss: 1.0039,	0.8235 s / batch. (data: 7.36e-03). ETA=11:14:09, max mem: 20.9 GB 
[12/02 00:47:31 visual_prompt]: 	Training 200/553. train loss: 1.3046,	1.3802 s / batch. (data: 5.36e-01). ETA=18:47:35, max mem: 20.9 GB 
[12/02 00:49:20 visual_prompt]: 	Training 300/553. train loss: 2.3753,	0.8413 s / batch. (data: 3.00e-04). ETA=11:25:55, max mem: 20.9 GB 
[12/02 00:51:12 visual_prompt]: 	Training 400/553. train loss: 2.2551,	0.8279 s / batch. (data: 1.20e-02). ETA=11:13:34, max mem: 20.9 GB 
[12/02 00:53:05 visual_prompt]: 	Training 500/553. train loss: 7.8036,	0.8300 s / batch. (data: 3.84e-04). ETA=11:13:56, max mem: 20.9 GB 
[12/02 00:54:02 visual_prompt]: Epoch 12 / 100: avg data time: 2.91e-01, avg batch time: 1.1249, average train loss: 1.5844
[12/02 00:55:07 visual_prompt]: Inference (val):avg data time: 5.16e-05, avg batch time: 0.3103, average loss: 4.4920
[12/02 00:55:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.22	
[12/02 00:55:07 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[12/02 00:57:05 visual_prompt]: 	Training 100/553. train loss: 0.9622,	0.8469 s / batch. (data: 1.08e-02). ETA=11:25:27, max mem: 20.9 GB 
[12/02 00:58:53 visual_prompt]: 	Training 200/553. train loss: 0.6140,	0.8480 s / batch. (data: 1.20e-02). ETA=11:24:57, max mem: 20.9 GB 
[12/02 01:00:46 visual_prompt]: 	Training 300/553. train loss: 0.6270,	2.1586 s / batch. (data: 1.33e+00). ETA=1 day, 4:59:59, max mem: 20.9 GB 
[12/02 01:02:37 visual_prompt]: 	Training 400/553. train loss: 0.5155,	0.8593 s / batch. (data: 1.56e-02). ETA=11:31:13, max mem: 20.9 GB 
[12/02 01:04:30 visual_prompt]: 	Training 500/553. train loss: 3.0772,	0.8330 s / batch. (data: 2.16e-03). ETA=11:08:41, max mem: 20.9 GB 
[12/02 01:05:29 visual_prompt]: Epoch 13 / 100: avg data time: 2.92e-01, avg batch time: 1.1249, average train loss: 1.5807
[12/02 01:06:35 visual_prompt]: Inference (val):avg data time: 6.31e-05, avg batch time: 0.3103, average loss: 0.7782
[12/02 01:06:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 62.49	
[12/02 01:06:35 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[12/02 01:08:34 visual_prompt]: 	Training 100/553. train loss: 0.9836,	0.8282 s / batch. (data: 3.43e-04). ETA=11:02:44, max mem: 20.9 GB 
[12/02 01:10:26 visual_prompt]: 	Training 200/553. train loss: 0.0590,	1.8419 s / batch. (data: 1.01e+00). ETA=1 day, 0:30:49, max mem: 20.9 GB 
[12/02 01:12:18 visual_prompt]: 	Training 300/553. train loss: 0.4164,	1.0800 s / batch. (data: 2.29e-01). ETA=14:20:35, max mem: 20.9 GB 
[12/02 01:14:10 visual_prompt]: 	Training 400/553. train loss: 0.8832,	0.8280 s / batch. (data: 3.52e-04). ETA=10:58:24, max mem: 20.9 GB 
[12/02 01:16:01 visual_prompt]: 	Training 500/553. train loss: 4.5468,	0.8724 s / batch. (data: 1.74e-02). ETA=11:32:14, max mem: 20.9 GB 
[12/02 01:16:59 visual_prompt]: Epoch 14 / 100: avg data time: 2.95e-01, avg batch time: 1.1291, average train loss: 1.4852
[12/02 01:18:05 visual_prompt]: Inference (val):avg data time: 6.52e-05, avg batch time: 0.3100, average loss: 0.9987
[12/02 01:18:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 66.16	
[12/02 01:18:05 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[12/02 01:20:02 visual_prompt]: 	Training 100/553. train loss: 1.3619,	0.8305 s / batch. (data: 1.02e-02). ETA=10:56:54, max mem: 20.9 GB 
[12/02 01:21:52 visual_prompt]: 	Training 200/553. train loss: 0.2890,	0.8278 s / batch. (data: 7.46e-04). ETA=10:53:23, max mem: 20.9 GB 
[12/02 01:23:46 visual_prompt]: 	Training 300/553. train loss: 1.6031,	0.8417 s / batch. (data: 7.95e-03). ETA=11:02:54, max mem: 20.9 GB 
[12/02 01:25:36 visual_prompt]: 	Training 400/553. train loss: 1.2904,	1.5532 s / batch. (data: 7.27e-01). ETA=20:20:46, max mem: 20.9 GB 
[12/02 01:27:29 visual_prompt]: 	Training 500/553. train loss: 0.8546,	0.8445 s / batch. (data: 3.45e-04). ETA=11:02:22, max mem: 20.9 GB 
[12/02 01:28:28 visual_prompt]: Epoch 15 / 100: avg data time: 2.92e-01, avg batch time: 1.1249, average train loss: 1.8137
[12/02 01:29:33 visual_prompt]: Inference (val):avg data time: 5.16e-05, avg batch time: 0.3106, average loss: 1.5462
[12/02 01:29:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.27	
[12/02 01:29:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[12/02 01:31:30 visual_prompt]: 	Training 100/553. train loss: 1.7917,	0.8231 s / batch. (data: 1.15e-03). ETA=10:43:25, max mem: 20.9 GB 
[12/02 01:33:23 visual_prompt]: 	Training 200/553. train loss: 0.9200,	0.8311 s / batch. (data: 3.55e-04). ETA=10:48:20, max mem: 20.9 GB 
[12/02 01:35:15 visual_prompt]: 	Training 300/553. train loss: 2.8723,	0.8387 s / batch. (data: 3.19e-04). ETA=10:52:49, max mem: 20.9 GB 
[12/02 01:37:07 visual_prompt]: 	Training 400/553. train loss: 1.8306,	0.8356 s / batch. (data: 1.05e-02). ETA=10:49:03, max mem: 20.9 GB 
[12/02 01:38:59 visual_prompt]: 	Training 500/553. train loss: 0.5853,	1.7084 s / batch. (data: 8.75e-01). ETA=22:04:07, max mem: 20.9 GB 
[12/02 01:39:58 visual_prompt]: Epoch 16 / 100: avg data time: 2.96e-01, avg batch time: 1.1294, average train loss: 1.3513
[12/02 01:41:04 visual_prompt]: Inference (val):avg data time: 5.90e-05, avg batch time: 0.3108, average loss: 0.8820
[12/02 01:41:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.44	
[12/02 01:41:04 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[12/02 01:43:01 visual_prompt]: 	Training 100/553. train loss: 0.9979,	0.8564 s / batch. (data: 7.95e-03). ETA=11:01:35, max mem: 20.9 GB 
[12/02 01:44:56 visual_prompt]: 	Training 200/553. train loss: 5.0759,	0.8308 s / batch. (data: 5.52e-03). ETA=10:40:25, max mem: 20.9 GB 
[12/02 01:46:47 visual_prompt]: 	Training 300/553. train loss: 1.2097,	0.8600 s / batch. (data: 1.19e-02). ETA=11:01:32, max mem: 20.9 GB 
[12/02 01:48:38 visual_prompt]: 	Training 400/553. train loss: 0.6966,	1.5505 s / batch. (data: 7.18e-01). ETA=19:50:01, max mem: 20.9 GB 
[12/02 01:50:29 visual_prompt]: 	Training 500/553. train loss: 0.9693,	2.1919 s / batch. (data: 1.34e+00). ETA=1 day, 3:58:44, max mem: 20.9 GB 
[12/02 01:51:29 visual_prompt]: Epoch 17 / 100: avg data time: 2.96e-01, avg batch time: 1.1289, average train loss: 1.5792
[12/02 01:52:35 visual_prompt]: Inference (val):avg data time: 5.24e-05, avg batch time: 0.3090, average loss: 0.6952
[12/02 01:52:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 69.91	
[12/02 01:52:35 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[12/02 01:54:33 visual_prompt]: 	Training 100/553. train loss: 0.2510,	0.8269 s / batch. (data: 3.19e-04). ETA=10:31:12, max mem: 20.9 GB 
[12/02 01:56:28 visual_prompt]: 	Training 200/553. train loss: 2.5037,	0.8268 s / batch. (data: 3.47e-04). ETA=10:29:43, max mem: 20.9 GB 
[12/02 01:58:20 visual_prompt]: 	Training 300/553. train loss: 0.5159,	0.8398 s / batch. (data: 6.22e-04). ETA=10:38:15, max mem: 20.9 GB 
[12/02 02:00:11 visual_prompt]: 	Training 400/553. train loss: 1.4378,	0.8405 s / batch. (data: 3.31e-04). ETA=10:37:20, max mem: 20.9 GB 
[12/02 02:02:03 visual_prompt]: 	Training 500/553. train loss: 1.6077,	0.8319 s / batch. (data: 4.75e-04). ETA=10:29:26, max mem: 20.9 GB 
[12/02 02:03:00 visual_prompt]: Epoch 18 / 100: avg data time: 2.96e-01, avg batch time: 1.1302, average train loss: 1.5942
[12/02 02:04:05 visual_prompt]: Inference (val):avg data time: 5.15e-05, avg batch time: 0.3095, average loss: 0.9914
[12/02 02:04:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.78	
[12/02 02:04:05 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[12/02 02:06:02 visual_prompt]: 	Training 100/553. train loss: 0.8349,	0.8412 s / batch. (data: 1.19e-02). ETA=10:34:21, max mem: 20.9 GB 
[12/02 02:07:56 visual_prompt]: 	Training 200/553. train loss: 0.4752,	0.8326 s / batch. (data: 3.42e-04). ETA=10:26:27, max mem: 20.9 GB 
[12/02 02:09:49 visual_prompt]: 	Training 300/553. train loss: 2.0306,	0.8513 s / batch. (data: 5.49e-03). ETA=10:39:08, max mem: 20.9 GB 
[12/02 02:11:43 visual_prompt]: 	Training 400/553. train loss: 0.4793,	0.8327 s / batch. (data: 8.17e-04). ETA=10:23:45, max mem: 20.9 GB 
[12/02 02:13:31 visual_prompt]: 	Training 500/553. train loss: 0.9482,	0.8244 s / batch. (data: 3.35e-04). ETA=10:16:12, max mem: 20.9 GB 
[12/02 02:14:30 visual_prompt]: Epoch 19 / 100: avg data time: 2.96e-01, avg batch time: 1.1302, average train loss: 1.2877
[12/02 02:15:36 visual_prompt]: Inference (val):avg data time: 4.80e-05, avg batch time: 0.3083, average loss: 3.3269
[12/02 02:15:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.71	
[12/02 02:15:36 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[12/02 02:17:29 visual_prompt]: 	Training 100/553. train loss: 0.6429,	0.8372 s / batch. (data: 5.46e-04). ETA=10:23:38, max mem: 20.9 GB 
[12/02 02:19:24 visual_prompt]: 	Training 200/553. train loss: 0.4377,	0.8242 s / batch. (data: 7.47e-04). ETA=10:12:31, max mem: 20.9 GB 
[12/02 02:21:17 visual_prompt]: 	Training 300/553. train loss: 4.3347,	0.8412 s / batch. (data: 5.43e-03). ETA=10:23:49, max mem: 20.9 GB 
[12/02 02:23:07 visual_prompt]: 	Training 400/553. train loss: 0.5259,	0.8322 s / batch. (data: 3.43e-04). ETA=10:15:43, max mem: 20.9 GB 
[12/02 02:24:57 visual_prompt]: 	Training 500/553. train loss: 1.7503,	0.8560 s / batch. (data: 7.94e-03). ETA=10:31:52, max mem: 20.9 GB 
[12/02 02:25:58 visual_prompt]: Epoch 20 / 100: avg data time: 2.91e-01, avg batch time: 1.1250, average train loss: 1.6240
[12/02 02:27:03 visual_prompt]: Inference (val):avg data time: 2.71e-04, avg batch time: 0.3097, average loss: 1.1141
[12/02 02:27:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.32	
[12/02 02:27:03 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[12/02 02:29:03 visual_prompt]: 	Training 100/553. train loss: 0.4350,	0.8232 s / batch. (data: 7.93e-03). ETA=10:05:36, max mem: 20.9 GB 
[12/02 02:30:54 visual_prompt]: 	Training 200/553. train loss: 0.1791,	0.8480 s / batch. (data: 3.67e-04). ETA=10:22:25, max mem: 20.9 GB 
[12/02 02:32:48 visual_prompt]: 	Training 300/553. train loss: 4.4692,	1.4308 s / batch. (data: 6.03e-01). ETA=17:27:49, max mem: 20.9 GB 
[12/02 02:34:38 visual_prompt]: 	Training 400/553. train loss: 4.0793,	0.8235 s / batch. (data: 7.95e-03). ETA=10:01:44, max mem: 20.9 GB 
[12/02 02:36:31 visual_prompt]: 	Training 500/553. train loss: 1.5386,	0.8186 s / batch. (data: 2.96e-04). ETA=9:56:43, max mem: 20.9 GB 
[12/02 02:37:28 visual_prompt]: Epoch 21 / 100: avg data time: 2.96e-01, avg batch time: 1.1301, average train loss: 1.3973
[12/02 02:38:34 visual_prompt]: Inference (val):avg data time: 2.00e-04, avg batch time: 0.3108, average loss: 0.7684
[12/02 02:38:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 69.66	
[12/02 02:38:34 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[12/02 02:40:30 visual_prompt]: 	Training 100/553. train loss: 1.9408,	0.8603 s / batch. (data: 3.31e-04). ETA=10:24:56, max mem: 20.9 GB 
[12/02 02:42:23 visual_prompt]: 	Training 200/553. train loss: 0.4545,	0.8284 s / batch. (data: 3.06e-04). ETA=10:00:23, max mem: 20.9 GB 
[12/02 02:44:13 visual_prompt]: 	Training 300/553. train loss: 0.1374,	0.8687 s / batch. (data: 1.66e-02). ETA=10:28:11, max mem: 20.9 GB 
[12/02 02:46:07 visual_prompt]: 	Training 400/553. train loss: 0.6139,	0.8600 s / batch. (data: 3.13e-04). ETA=10:20:27, max mem: 20.9 GB 
[12/02 02:48:00 visual_prompt]: 	Training 500/553. train loss: 1.2890,	0.8222 s / batch. (data: 5.45e-04). ETA=9:51:46, max mem: 20.9 GB 
[12/02 02:49:00 visual_prompt]: Epoch 22 / 100: avg data time: 2.98e-01, avg batch time: 1.1318, average train loss: 1.2825
[12/02 02:50:06 visual_prompt]: Inference (val):avg data time: 5.67e-05, avg batch time: 0.3084, average loss: 1.7864
[12/02 02:50:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 70.46	
[12/02 02:50:06 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[12/02 02:52:04 visual_prompt]: 	Training 100/553. train loss: 1.4544,	0.8372 s / batch. (data: 1.13e-02). ETA=10:00:27, max mem: 20.9 GB 
[12/02 02:53:58 visual_prompt]: 	Training 200/553. train loss: 2.4655,	1.4240 s / batch. (data: 5.93e-01). ETA=16:58:57, max mem: 20.9 GB 
[12/02 02:55:52 visual_prompt]: 	Training 300/553. train loss: 0.5708,	0.8370 s / batch. (data: 7.54e-04). ETA=9:57:31, max mem: 20.9 GB 
[12/02 02:57:43 visual_prompt]: 	Training 400/553. train loss: 0.6146,	0.8560 s / batch. (data: 3.17e-04). ETA=10:09:39, max mem: 20.9 GB 
[12/02 02:59:33 visual_prompt]: 	Training 500/553. train loss: 0.9569,	0.8408 s / batch. (data: 3.04e-04). ETA=9:57:24, max mem: 20.9 GB 
[12/02 03:00:31 visual_prompt]: Epoch 23 / 100: avg data time: 2.97e-01, avg batch time: 1.1303, average train loss: 1.2626
[12/02 03:01:36 visual_prompt]: Inference (val):avg data time: 5.89e-05, avg batch time: 0.3115, average loss: 0.9033
[12/02 03:01:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 69.94	
[12/02 03:01:36 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[12/02 03:03:29 visual_prompt]: 	Training 100/553. train loss: 0.4386,	0.8310 s / batch. (data: 1.20e-02). ETA=9:48:23, max mem: 20.9 GB 
[12/02 03:05:20 visual_prompt]: 	Training 200/553. train loss: 1.4972,	0.8402 s / batch. (data: 2.75e-04). ETA=9:53:26, max mem: 20.9 GB 
[12/02 03:07:12 visual_prompt]: 	Training 300/553. train loss: 0.7559,	1.5040 s / batch. (data: 6.52e-01). ETA=17:39:50, max mem: 20.9 GB 
[12/02 03:09:04 visual_prompt]: 	Training 400/553. train loss: 1.0931,	0.8439 s / batch. (data: 5.15e-04). ETA=9:53:17, max mem: 20.9 GB 
[12/02 03:10:57 visual_prompt]: 	Training 500/553. train loss: 0.9253,	0.8320 s / batch. (data: 3.84e-04). ETA=9:43:30, max mem: 20.9 GB 
[12/02 03:11:57 visual_prompt]: Epoch 24 / 100: avg data time: 2.89e-01, avg batch time: 1.1218, average train loss: 1.4870
[12/02 03:13:01 visual_prompt]: Inference (val):avg data time: 4.63e-05, avg batch time: 0.3094, average loss: 1.2143
[12/02 03:13:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 69.19	
[12/02 03:13:01 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[12/02 03:15:02 visual_prompt]: 	Training 100/553. train loss: 2.9093,	0.8159 s / batch. (data: 4.41e-04). ETA=9:30:10, max mem: 20.9 GB 
[12/02 03:16:50 visual_prompt]: 	Training 200/553. train loss: 1.0024,	0.9693 s / batch. (data: 1.54e-01). ETA=11:15:45, max mem: 20.9 GB 
[12/02 03:18:40 visual_prompt]: 	Training 300/553. train loss: 0.7224,	0.8341 s / batch. (data: 1.07e-02). ETA=9:40:06, max mem: 20.9 GB 
[12/02 03:20:32 visual_prompt]: 	Training 400/553. train loss: 0.4022,	1.5044 s / batch. (data: 6.66e-01). ETA=17:23:43, max mem: 20.9 GB 
[12/02 03:22:25 visual_prompt]: 	Training 500/553. train loss: 0.8279,	2.3037 s / batch. (data: 1.46e+00). ETA=1 day, 2:34:28, max mem: 20.9 GB 
[12/02 03:23:22 visual_prompt]: Epoch 25 / 100: avg data time: 2.89e-01, avg batch time: 1.1225, average train loss: 1.5533
[12/02 03:24:27 visual_prompt]: Inference (val):avg data time: 4.81e-05, avg batch time: 0.3106, average loss: 1.8714
[12/02 03:24:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 69.58	
[12/02 03:24:27 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[12/02 03:26:23 visual_prompt]: 	Training 100/553. train loss: 0.6592,	0.8320 s / batch. (data: 7.94e-03). ETA=9:33:43, max mem: 20.9 GB 
[12/02 03:28:16 visual_prompt]: 	Training 200/553. train loss: 4.6227,	2.2469 s / batch. (data: 1.43e+00). ETA=1 day, 1:45:39, max mem: 20.9 GB 
[12/02 03:30:09 visual_prompt]: 	Training 300/553. train loss: 0.0477,	0.8520 s / batch. (data: 8.01e-03). ETA=9:44:41, max mem: 20.9 GB 
[12/02 03:31:59 visual_prompt]: 	Training 400/553. train loss: 1.0129,	0.8437 s / batch. (data: 7.65e-03). ETA=9:37:32, max mem: 20.9 GB 
[12/02 03:33:50 visual_prompt]: 	Training 500/553. train loss: 1.1313,	0.8309 s / batch. (data: 3.27e-04). ETA=9:27:24, max mem: 20.9 GB 
[12/02 03:34:48 visual_prompt]: Epoch 26 / 100: avg data time: 2.91e-01, avg batch time: 1.1226, average train loss: 1.2684
[12/02 03:35:53 visual_prompt]: Inference (val):avg data time: 4.53e-05, avg batch time: 0.3105, average loss: 1.4610
[12/02 03:35:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 71.04	
[12/02 03:35:53 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[12/02 03:37:50 visual_prompt]: 	Training 100/553. train loss: 0.2323,	0.8444 s / batch. (data: 2.14e-02). ETA=9:34:30, max mem: 20.9 GB 
[12/02 03:39:41 visual_prompt]: 	Training 200/553. train loss: 0.3508,	2.1179 s / batch. (data: 1.30e+00). ETA=23:57:26, max mem: 20.9 GB 
[12/02 03:41:33 visual_prompt]: 	Training 300/553. train loss: 1.7402,	0.8439 s / batch. (data: 3.29e-04). ETA=9:31:22, max mem: 20.9 GB 
[12/02 03:43:26 visual_prompt]: 	Training 400/553. train loss: 0.3210,	0.8529 s / batch. (data: 7.78e-04). ETA=9:36:00, max mem: 20.9 GB 
[12/02 03:45:18 visual_prompt]: 	Training 500/553. train loss: 1.8996,	0.8335 s / batch. (data: 3.28e-04). ETA=9:21:30, max mem: 20.9 GB 
[12/02 03:46:15 visual_prompt]: Epoch 27 / 100: avg data time: 2.91e-01, avg batch time: 1.1236, average train loss: 1.4210
[12/02 03:47:20 visual_prompt]: Inference (val):avg data time: 5.68e-05, avg batch time: 0.3093, average loss: 0.7539
[12/02 03:47:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 69.83	
[12/02 03:47:20 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[12/02 03:49:15 visual_prompt]: 	Training 100/553. train loss: 1.6605,	0.8165 s / batch. (data: 3.16e-04). ETA=9:08:00, max mem: 20.9 GB 
[12/02 03:51:07 visual_prompt]: 	Training 200/553. train loss: 2.2914,	0.8308 s / batch. (data: 5.51e-03). ETA=9:16:14, max mem: 20.9 GB 
[12/02 03:53:00 visual_prompt]: 	Training 300/553. train loss: 1.0838,	2.0001 s / batch. (data: 1.17e+00). ETA=22:15:40, max mem: 20.9 GB 
[12/02 03:54:51 visual_prompt]: 	Training 400/553. train loss: 0.9004,	0.8240 s / batch. (data: 5.45e-03). ETA=9:08:55, max mem: 20.9 GB 
[12/02 03:56:41 visual_prompt]: 	Training 500/553. train loss: 4.4023,	0.8360 s / batch. (data: 3.30e-04). ETA=9:15:30, max mem: 20.9 GB 
[12/02 03:57:39 visual_prompt]: Epoch 28 / 100: avg data time: 2.87e-01, avg batch time: 1.1201, average train loss: 1.2425
[12/02 03:58:45 visual_prompt]: Inference (val):avg data time: 1.94e-04, avg batch time: 0.3114, average loss: 0.6959
[12/02 03:58:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 70.70	
[12/02 03:58:45 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[12/02 04:00:49 visual_prompt]: 	Training 100/553. train loss: 1.9331,	0.8259 s / batch. (data: 2.66e-04). ETA=9:06:42, max mem: 20.9 GB 
[12/02 04:02:39 visual_prompt]: 	Training 200/553. train loss: 1.6660,	2.2723 s / batch. (data: 1.44e+00). ETA=1 day, 1:00:18, max mem: 20.9 GB 
[12/02 04:04:28 visual_prompt]: 	Training 300/553. train loss: 0.6345,	0.8278 s / batch. (data: 4.84e-04). ETA=9:05:09, max mem: 20.9 GB 
[12/02 04:06:16 visual_prompt]: 	Training 400/553. train loss: 2.8436,	1.7148 s / batch. (data: 8.71e-01). ETA=18:46:31, max mem: 20.9 GB 
[12/02 04:08:07 visual_prompt]: 	Training 500/553. train loss: 1.8195,	0.8431 s / batch. (data: 1.18e-02). ETA=9:12:27, max mem: 20.9 GB 
[12/02 04:09:05 visual_prompt]: Epoch 29 / 100: avg data time: 2.88e-01, avg batch time: 1.1206, average train loss: 1.3507
[12/02 04:10:10 visual_prompt]: Inference (val):avg data time: 5.68e-05, avg batch time: 0.3088, average loss: 0.6417
[12/02 04:10:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 70.33	
[12/02 04:10:10 visual_prompt]: Best epoch 29: best metric: -0.642
[12/02 04:10:10 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[12/02 04:12:04 visual_prompt]: 	Training 100/553. train loss: 2.0049,	0.8399 s / batch. (data: 7.91e-03). ETA=9:08:13, max mem: 20.9 GB 
[12/02 04:13:56 visual_prompt]: 	Training 200/553. train loss: 1.0563,	0.8339 s / batch. (data: 3.66e-04). ETA=9:02:56, max mem: 20.9 GB 
[12/02 04:15:47 visual_prompt]: 	Training 300/553. train loss: 0.0074,	2.0269 s / batch. (data: 1.19e+00). ETA=21:56:13, max mem: 20.9 GB 
[12/02 04:17:40 visual_prompt]: 	Training 400/553. train loss: 1.1160,	1.6115 s / batch. (data: 7.79e-01). ETA=17:23:47, max mem: 20.9 GB 
[12/02 04:19:30 visual_prompt]: 	Training 500/553. train loss: 2.4017,	1.8256 s / batch. (data: 9.89e-01). ETA=19:39:26, max mem: 20.9 GB 
[12/02 04:20:29 visual_prompt]: Epoch 30 / 100: avg data time: 2.87e-01, avg batch time: 1.1196, average train loss: 1.1987
[12/02 04:21:34 visual_prompt]: Inference (val):avg data time: 4.79e-05, avg batch time: 0.3103, average loss: 1.0773
[12/02 04:21:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 72.88	
[12/02 04:21:34 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[12/02 04:23:32 visual_prompt]: 	Training 100/553. train loss: 0.4903,	0.8418 s / batch. (data: 1.14e-02). ETA=9:01:42, max mem: 20.9 GB 
[12/02 04:25:25 visual_prompt]: 	Training 200/553. train loss: 1.6626,	0.8603 s / batch. (data: 3.33e-02). ETA=9:12:11, max mem: 20.9 GB 
[12/02 04:27:14 visual_prompt]: 	Training 300/553. train loss: 1.0317,	0.8393 s / batch. (data: 1.09e-02). ETA=8:57:17, max mem: 20.9 GB 
[12/02 04:29:03 visual_prompt]: 	Training 400/553. train loss: 1.1188,	1.2680 s / batch. (data: 4.40e-01). ETA=13:29:37, max mem: 20.9 GB 
[12/02 04:30:55 visual_prompt]: 	Training 500/553. train loss: 0.8082,	0.8285 s / batch. (data: 1.20e-02). ETA=8:47:36, max mem: 20.9 GB 
[12/02 04:31:51 visual_prompt]: Epoch 31 / 100: avg data time: 2.81e-01, avg batch time: 1.1150, average train loss: 1.2618
[12/02 04:32:56 visual_prompt]: Inference (val):avg data time: 2.81e-04, avg batch time: 0.3109, average loss: 0.7294
[12/02 04:32:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 69.39	
[12/02 04:32:56 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[12/02 04:34:54 visual_prompt]: 	Training 100/553. train loss: 0.3264,	0.8309 s / batch. (data: 8.18e-04). ETA=8:47:02, max mem: 20.9 GB 
[12/02 04:36:45 visual_prompt]: 	Training 200/553. train loss: 0.3521,	0.8479 s / batch. (data: 1.38e-03). ETA=8:56:22, max mem: 20.9 GB 
[12/02 04:38:40 visual_prompt]: 	Training 300/553. train loss: 1.2200,	0.8383 s / batch. (data: 6.24e-04). ETA=8:48:55, max mem: 20.9 GB 
[12/02 04:40:32 visual_prompt]: 	Training 400/553. train loss: 1.3647,	0.8308 s / batch. (data: 1.10e-03). ETA=8:42:48, max mem: 20.9 GB 
[12/02 04:42:21 visual_prompt]: 	Training 500/553. train loss: 1.4383,	0.8363 s / batch. (data: 5.46e-03). ETA=8:44:54, max mem: 20.9 GB 
[12/02 04:43:17 visual_prompt]: Epoch 32 / 100: avg data time: 2.90e-01, avg batch time: 1.1227, average train loss: 1.0905
[12/02 04:44:22 visual_prompt]: Inference (val):avg data time: 5.36e-05, avg batch time: 0.3086, average loss: 1.1206
[12/02 04:44:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 70.38	
[12/02 04:44:22 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[12/02 04:46:16 visual_prompt]: 	Training 100/553. train loss: 0.8567,	1.4680 s / batch. (data: 6.24e-01). ETA=15:17:35, max mem: 20.9 GB 
[12/02 04:48:11 visual_prompt]: 	Training 200/553. train loss: 0.6552,	2.2604 s / batch. (data: 1.43e+00). ETA=23:29:06, max mem: 20.9 GB 
[12/02 04:50:01 visual_prompt]: 	Training 300/553. train loss: 0.4038,	0.8243 s / batch. (data: 3.59e-04). ETA=8:32:31, max mem: 20.9 GB 
[12/02 04:51:54 visual_prompt]: 	Training 400/553. train loss: 0.4884,	0.8744 s / batch. (data: 6.73e-03). ETA=9:02:12, max mem: 20.9 GB 
[12/02 04:53:44 visual_prompt]: 	Training 500/553. train loss: 1.8348,	1.3356 s / batch. (data: 4.99e-01). ETA=13:45:57, max mem: 20.9 GB 
[12/02 04:54:42 visual_prompt]: Epoch 33 / 100: avg data time: 2.87e-01, avg batch time: 1.1202, average train loss: 1.3352
[12/02 04:55:47 visual_prompt]: Inference (val):avg data time: 1.68e-04, avg batch time: 0.3115, average loss: 1.0432
[12/02 04:55:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 71.81	
[12/02 04:55:47 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[12/02 04:57:44 visual_prompt]: 	Training 100/553. train loss: 1.2060,	0.8381 s / batch. (data: 1.05e-02). ETA=8:36:09, max mem: 20.9 GB 
[12/02 04:59:34 visual_prompt]: 	Training 200/553. train loss: 1.5440,	1.0041 s / batch. (data: 1.40e-01). ETA=10:16:42, max mem: 20.9 GB 
[12/02 05:01:25 visual_prompt]: 	Training 300/553. train loss: 0.1461,	0.8338 s / batch. (data: 3.72e-04). ETA=8:30:42, max mem: 20.9 GB 
[12/02 05:03:17 visual_prompt]: 	Training 400/553. train loss: 0.9827,	0.8198 s / batch. (data: 3.49e-04). ETA=8:20:45, max mem: 20.9 GB 
[12/02 05:05:08 visual_prompt]: 	Training 500/553. train loss: 0.3339,	2.0392 s / batch. (data: 1.22e+00). ETA=20:42:15, max mem: 20.9 GB 
[12/02 05:06:06 visual_prompt]: Epoch 34 / 100: avg data time: 2.87e-01, avg batch time: 1.1194, average train loss: 1.1495
[12/02 05:07:10 visual_prompt]: Inference (val):avg data time: 5.09e-05, avg batch time: 0.3088, average loss: 0.6929
[12/02 05:07:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 71.45	
[12/02 05:07:10 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[12/02 05:09:08 visual_prompt]: 	Training 100/553. train loss: 1.2604,	0.8439 s / batch. (data: 1.08e-02). ETA=8:31:57, max mem: 20.9 GB 
[12/02 05:11:01 visual_prompt]: 	Training 200/553. train loss: 1.7051,	0.8364 s / batch. (data: 6.62e-04). ETA=8:25:58, max mem: 20.9 GB 
[12/02 05:12:52 visual_prompt]: 	Training 300/553. train loss: 0.4288,	0.8168 s / batch. (data: 3.99e-04). ETA=8:12:45, max mem: 20.9 GB 
[12/02 05:14:42 visual_prompt]: 	Training 400/553. train loss: 2.3022,	0.8320 s / batch. (data: 3.25e-04). ETA=8:20:33, max mem: 20.9 GB 
[12/02 05:16:32 visual_prompt]: 	Training 500/553. train loss: 1.8509,	1.2618 s / batch. (data: 4.35e-01). ETA=12:37:01, max mem: 20.9 GB 
[12/02 05:17:29 visual_prompt]: Epoch 35 / 100: avg data time: 2.86e-01, avg batch time: 1.1192, average train loss: 1.5303
[12/02 05:18:34 visual_prompt]: Inference (val):avg data time: 5.02e-05, avg batch time: 0.3097, average loss: 1.5124
[12/02 05:18:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 68.10	
[12/02 05:18:34 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[12/02 05:20:29 visual_prompt]: 	Training 100/553. train loss: 1.7113,	0.8161 s / batch. (data: 3.87e-04). ETA=8:07:31, max mem: 20.9 GB 
[12/02 05:22:23 visual_prompt]: 	Training 200/553. train loss: 3.6726,	1.3373 s / batch. (data: 5.04e-01). ETA=13:16:42, max mem: 20.9 GB 
[12/02 05:24:16 visual_prompt]: 	Training 300/553. train loss: 0.2190,	0.8402 s / batch. (data: 1.19e-02). ETA=8:19:07, max mem: 20.9 GB 
[12/02 05:26:07 visual_prompt]: 	Training 400/553. train loss: 1.9942,	0.8325 s / batch. (data: 3.55e-04). ETA=8:13:10, max mem: 20.9 GB 
[12/02 05:28:00 visual_prompt]: 	Training 500/553. train loss: 0.3903,	1.5731 s / batch. (data: 7.44e-01). ETA=15:29:19, max mem: 20.9 GB 
[12/02 05:28:55 visual_prompt]: Epoch 36 / 100: avg data time: 2.89e-01, avg batch time: 1.1228, average train loss: 1.1051
[12/02 05:30:00 visual_prompt]: Inference (val):avg data time: 2.27e-04, avg batch time: 0.3105, average loss: 1.1978
[12/02 05:30:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 72.27	
[12/02 05:30:00 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[12/02 05:31:57 visual_prompt]: 	Training 100/553. train loss: 2.3738,	0.8318 s / batch. (data: 6.18e-03). ETA=8:09:16, max mem: 20.9 GB 
[12/02 05:33:48 visual_prompt]: 	Training 200/553. train loss: 0.5846,	0.8769 s / batch. (data: 1.82e-03). ETA=8:34:18, max mem: 20.9 GB 
[12/02 05:35:41 visual_prompt]: 	Training 300/553. train loss: 4.1251,	1.7439 s / batch. (data: 9.02e-01). ETA=16:59:58, max mem: 20.9 GB 
[12/02 05:37:36 visual_prompt]: 	Training 400/553. train loss: 1.3368,	2.2200 s / batch. (data: 1.39e+00). ETA=21:34:42, max mem: 20.9 GB 
[12/02 05:39:25 visual_prompt]: 	Training 500/553. train loss: 1.4077,	1.7015 s / batch. (data: 8.84e-01). ETA=16:29:28, max mem: 20.9 GB 
[12/02 05:40:24 visual_prompt]: Epoch 37 / 100: avg data time: 2.95e-01, avg batch time: 1.1286, average train loss: 1.2632
[12/02 05:41:30 visual_prompt]: Inference (val):avg data time: 3.86e-04, avg batch time: 0.3096, average loss: 0.7242
[12/02 05:41:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 72.82	
[12/02 05:41:30 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[12/02 05:43:24 visual_prompt]: 	Training 100/553. train loss: 0.4553,	1.1082 s / batch. (data: 2.79e-01). ETA=10:41:37, max mem: 20.9 GB 
[12/02 05:45:18 visual_prompt]: 	Training 200/553. train loss: 1.9488,	2.0224 s / batch. (data: 1.17e+00). ETA=19:27:34, max mem: 20.9 GB 
[12/02 05:47:11 visual_prompt]: 	Training 300/553. train loss: 0.6312,	0.8315 s / batch. (data: 1.29e-03). ETA=7:58:40, max mem: 20.9 GB 
[12/02 05:49:01 visual_prompt]: 	Training 400/553. train loss: 0.4676,	0.8355 s / batch. (data: 3.34e-04). ETA=7:59:33, max mem: 20.9 GB 
[12/02 05:50:56 visual_prompt]: 	Training 500/553. train loss: 0.4923,	0.8672 s / batch. (data: 2.00e-03). ETA=8:16:19, max mem: 20.9 GB 
[12/02 05:51:53 visual_prompt]: Epoch 38 / 100: avg data time: 2.93e-01, avg batch time: 1.1263, average train loss: 1.0021
[12/02 05:52:58 visual_prompt]: Inference (val):avg data time: 6.16e-05, avg batch time: 0.3109, average loss: 1.9276
[12/02 05:52:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 71.44	
[12/02 05:52:58 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[12/02 05:54:52 visual_prompt]: 	Training 100/553. train loss: 0.3819,	0.8268 s / batch. (data: 5.56e-04). ETA=7:51:04, max mem: 20.9 GB 
[12/02 05:56:49 visual_prompt]: 	Training 200/553. train loss: 0.9766,	0.8604 s / batch. (data: 6.82e-03). ETA=8:08:48, max mem: 20.9 GB 
[12/02 05:58:44 visual_prompt]: 	Training 300/553. train loss: 0.7392,	0.8480 s / batch. (data: 3.97e-03). ETA=8:00:19, max mem: 20.9 GB 
[12/02 06:00:33 visual_prompt]: 	Training 400/553. train loss: 0.1125,	0.9894 s / batch. (data: 1.73e-01). ETA=9:18:46, max mem: 20.9 GB 
[12/02 06:02:25 visual_prompt]: 	Training 500/553. train loss: 1.1911,	2.4627 s / batch. (data: 1.63e+00). ETA=23:06:44, max mem: 20.9 GB 
[12/02 06:03:21 visual_prompt]: Epoch 39 / 100: avg data time: 2.93e-01, avg batch time: 1.1266, average train loss: 0.9236
[12/02 06:04:26 visual_prompt]: Inference (val):avg data time: 4.65e-05, avg batch time: 0.3111, average loss: 0.9346
[12/02 06:04:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 69.52	
[12/02 06:04:26 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[12/02 06:06:23 visual_prompt]: 	Training 100/553. train loss: 1.5997,	0.8240 s / batch. (data: 3.08e-04). ETA=7:41:53, max mem: 20.9 GB 
[12/02 06:08:14 visual_prompt]: 	Training 200/553. train loss: 1.3700,	0.8401 s / batch. (data: 3.33e-04). ETA=7:49:30, max mem: 20.9 GB 
[12/02 06:10:06 visual_prompt]: 	Training 300/553. train loss: 2.6526,	0.8477 s / batch. (data: 1.17e-02). ETA=7:52:20, max mem: 20.9 GB 
[12/02 06:11:57 visual_prompt]: 	Training 400/553. train loss: 0.3563,	0.8598 s / batch. (data: 4.06e-02). ETA=7:57:39, max mem: 20.9 GB 
[12/02 06:13:47 visual_prompt]: 	Training 500/553. train loss: 0.3543,	0.8294 s / batch. (data: 1.17e-02). ETA=7:39:23, max mem: 20.9 GB 
[12/02 06:14:47 visual_prompt]: Epoch 40 / 100: avg data time: 2.90e-01, avg batch time: 1.1227, average train loss: 1.1855
[12/02 06:15:52 visual_prompt]: Inference (val):avg data time: 4.80e-05, avg batch time: 0.3106, average loss: 0.8158
[12/02 06:15:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 73.85	
[12/02 06:15:52 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[12/02 06:17:53 visual_prompt]: 	Training 100/553. train loss: 1.4972,	0.8363 s / batch. (data: 3.61e-04). ETA=7:41:05, max mem: 20.9 GB 
[12/02 06:19:48 visual_prompt]: 	Training 200/553. train loss: 4.9335,	0.8301 s / batch. (data: 5.48e-03). ETA=7:36:17, max mem: 20.9 GB 
[12/02 06:21:38 visual_prompt]: 	Training 300/553. train loss: 0.5921,	0.8440 s / batch. (data: 1.20e-02). ETA=7:42:30, max mem: 20.9 GB 
[12/02 06:23:29 visual_prompt]: 	Training 400/553. train loss: 0.9874,	0.8437 s / batch. (data: 1.28e-02). ETA=7:40:56, max mem: 20.9 GB 
[12/02 06:25:16 visual_prompt]: 	Training 500/553. train loss: 1.2539,	0.8285 s / batch. (data: 1.09e-02). ETA=7:31:15, max mem: 20.9 GB 
[12/02 06:26:13 visual_prompt]: Epoch 41 / 100: avg data time: 2.89e-01, avg batch time: 1.1223, average train loss: 1.1928
[12/02 06:27:19 visual_prompt]: Inference (val):avg data time: 5.73e-05, avg batch time: 0.3092, average loss: 0.8494
[12/02 06:27:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 75.74	
[12/02 06:27:19 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[12/02 06:29:14 visual_prompt]: 	Training 100/553. train loss: 1.6373,	0.8476 s / batch. (data: 7.81e-03). ETA=7:39:30, max mem: 20.9 GB 
[12/02 06:31:09 visual_prompt]: 	Training 200/553. train loss: 1.6807,	0.8440 s / batch. (data: 7.95e-03). ETA=7:36:08, max mem: 20.9 GB 
[12/02 06:33:01 visual_prompt]: 	Training 300/553. train loss: 2.0106,	0.8403 s / batch. (data: 3.44e-04). ETA=7:32:44, max mem: 20.9 GB 
[12/02 06:34:53 visual_prompt]: 	Training 400/553. train loss: 0.4161,	0.8696 s / batch. (data: 2.57e-02). ETA=7:47:06, max mem: 20.9 GB 
[12/02 06:36:45 visual_prompt]: 	Training 500/553. train loss: 0.4438,	1.3086 s / batch. (data: 4.91e-01). ETA=11:40:41, max mem: 20.9 GB 
[12/02 06:37:45 visual_prompt]: Epoch 42 / 100: avg data time: 2.97e-01, avg batch time: 1.1306, average train loss: 1.0429
[12/02 06:38:50 visual_prompt]: Inference (val):avg data time: 1.28e-04, avg batch time: 0.3091, average loss: 0.8416
[12/02 06:38:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 73.95	
[12/02 06:38:50 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[12/02 06:40:50 visual_prompt]: 	Training 100/553. train loss: 0.3724,	0.8383 s / batch. (data: 2.95e-04). ETA=7:26:45, max mem: 20.9 GB 
[12/02 06:42:40 visual_prompt]: 	Training 200/553. train loss: 1.5967,	0.8467 s / batch. (data: 1.18e-02). ETA=7:29:48, max mem: 20.9 GB 
[12/02 06:44:30 visual_prompt]: 	Training 300/553. train loss: 0.3267,	0.8280 s / batch. (data: 5.45e-03). ETA=7:18:29, max mem: 20.9 GB 
[12/02 06:46:19 visual_prompt]: 	Training 400/553. train loss: 0.1831,	0.8392 s / batch. (data: 3.27e-04). ETA=7:23:01, max mem: 20.9 GB 
[12/02 06:48:13 visual_prompt]: 	Training 500/553. train loss: 0.2348,	0.8496 s / batch. (data: 5.47e-03). ETA=7:27:05, max mem: 20.9 GB 
[12/02 06:49:11 visual_prompt]: Epoch 43 / 100: avg data time: 2.90e-01, avg batch time: 1.1233, average train loss: 0.8600
[12/02 06:50:17 visual_prompt]: Inference (val):avg data time: 5.69e-05, avg batch time: 0.3097, average loss: 0.6792
[12/02 06:50:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.92	rocauc: 76.43	
[12/02 06:50:17 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[12/02 06:52:15 visual_prompt]: 	Training 100/553. train loss: 0.5801,	1.0121 s / batch. (data: 1.65e-01). ETA=8:50:01, max mem: 20.9 GB 
[12/02 06:54:09 visual_prompt]: 	Training 200/553. train loss: 0.2154,	0.8616 s / batch. (data: 1.05e-02). ETA=7:29:44, max mem: 20.9 GB 
[12/02 06:56:00 visual_prompt]: 	Training 300/553. train loss: 0.6173,	0.8494 s / batch. (data: 3.57e-04). ETA=7:21:59, max mem: 20.9 GB 
[12/02 06:57:50 visual_prompt]: 	Training 400/553. train loss: 0.6617,	0.8170 s / batch. (data: 4.15e-04). ETA=7:03:46, max mem: 20.9 GB 
[12/02 06:59:43 visual_prompt]: 	Training 500/553. train loss: 0.9854,	0.8781 s / batch. (data: 3.34e-04). ETA=7:34:00, max mem: 20.9 GB 
[12/02 07:00:42 visual_prompt]: Epoch 44 / 100: avg data time: 2.96e-01, avg batch time: 1.1299, average train loss: 0.9651
[12/02 07:01:47 visual_prompt]: Inference (val):avg data time: 5.92e-05, avg batch time: 0.3117, average loss: 0.7643
[12/02 07:01:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.61	
[12/02 07:01:47 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[12/02 07:03:47 visual_prompt]: 	Training 100/553. train loss: 1.5978,	0.8207 s / batch. (data: 6.03e-03). ETA=7:02:13, max mem: 20.9 GB 
[12/02 07:05:35 visual_prompt]: 	Training 200/553. train loss: 0.5744,	0.8559 s / batch. (data: 7.73e-03). ETA=7:18:54, max mem: 20.9 GB 
[12/02 07:07:30 visual_prompt]: 	Training 300/553. train loss: 0.8832,	0.8279 s / batch. (data: 4.97e-04). ETA=7:03:10, max mem: 20.9 GB 
[12/02 07:09:20 visual_prompt]: 	Training 400/553. train loss: 0.3005,	0.8200 s / batch. (data: 3.14e-04). ETA=6:57:45, max mem: 20.9 GB 
[12/02 07:11:15 visual_prompt]: 	Training 500/553. train loss: 1.0384,	0.8433 s / batch. (data: 1.20e-02). ETA=7:08:14, max mem: 20.9 GB 
[12/02 07:12:14 visual_prompt]: Epoch 45 / 100: avg data time: 3.00e-01, avg batch time: 1.1331, average train loss: 0.8418
[12/02 07:13:20 visual_prompt]: Inference (val):avg data time: 6.03e-05, avg batch time: 0.3105, average loss: 2.0009
[12/02 07:13:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 71.28	
[12/02 07:13:20 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[12/02 07:15:19 visual_prompt]: 	Training 100/553. train loss: 2.1590,	2.0361 s / batch. (data: 1.22e+00). ETA=17:08:43, max mem: 20.9 GB 
[12/02 07:17:12 visual_prompt]: 	Training 200/553. train loss: 0.5292,	0.8557 s / batch. (data: 1.09e-03). ETA=7:10:53, max mem: 20.9 GB 
[12/02 07:19:03 visual_prompt]: 	Training 300/553. train loss: 1.4834,	0.8174 s / batch. (data: 4.22e-04). ETA=6:50:16, max mem: 20.9 GB 
[12/02 07:20:55 visual_prompt]: 	Training 400/553. train loss: 1.0372,	0.8371 s / batch. (data: 5.46e-03). ETA=6:58:46, max mem: 20.9 GB 
[12/02 07:22:43 visual_prompt]: 	Training 500/553. train loss: 2.9249,	0.8434 s / batch. (data: 7.30e-03). ETA=7:00:30, max mem: 20.9 GB 
[12/02 07:23:43 visual_prompt]: Epoch 46 / 100: avg data time: 2.94e-01, avg batch time: 1.1271, average train loss: 0.9696
[12/02 07:24:48 visual_prompt]: Inference (val):avg data time: 1.71e-04, avg batch time: 0.3082, average loss: 2.1637
[12/02 07:24:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.36	
[12/02 07:24:48 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[12/02 07:26:46 visual_prompt]: 	Training 100/553. train loss: 0.9405,	0.8261 s / batch. (data: 5.47e-03). ETA=6:49:45, max mem: 20.9 GB 
[12/02 07:28:34 visual_prompt]: 	Training 200/553. train loss: 0.9820,	0.8665 s / batch. (data: 4.25e-02). ETA=7:08:22, max mem: 20.9 GB 
[12/02 07:30:29 visual_prompt]: 	Training 300/553. train loss: 0.5535,	0.8876 s / batch. (data: 1.57e-02). ETA=7:17:20, max mem: 20.9 GB 
[12/02 07:32:21 visual_prompt]: 	Training 400/553. train loss: 2.5532,	0.8349 s / batch. (data: 4.91e-04). ETA=6:49:58, max mem: 20.9 GB 
[12/02 07:34:12 visual_prompt]: 	Training 500/553. train loss: 6.8177,	0.8440 s / batch. (data: 3.52e-04). ETA=6:53:01, max mem: 20.9 GB 
[12/02 07:35:11 visual_prompt]: Epoch 47 / 100: avg data time: 2.91e-01, avg batch time: 1.1248, average train loss: 1.1701
[12/02 07:36:16 visual_prompt]: Inference (val):avg data time: 5.45e-05, avg batch time: 0.3093, average loss: 0.9397
[12/02 07:36:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 69.28	
[12/02 07:36:16 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[12/02 07:38:14 visual_prompt]: 	Training 100/553. train loss: 0.8131,	0.8238 s / batch. (data: 5.47e-03). ETA=6:41:03, max mem: 20.9 GB 
[12/02 07:40:06 visual_prompt]: 	Training 200/553. train loss: 0.0261,	0.8405 s / batch. (data: 3.40e-04). ETA=6:47:45, max mem: 20.9 GB 
[12/02 07:42:01 visual_prompt]: 	Training 300/553. train loss: 2.0191,	2.0613 s / batch. (data: 1.23e+00). ETA=16:36:36, max mem: 20.9 GB 
[12/02 07:43:49 visual_prompt]: 	Training 400/553. train loss: 0.2453,	1.1238 s / batch. (data: 2.89e-01). ETA=9:01:28, max mem: 20.9 GB 
[12/02 07:45:43 visual_prompt]: 	Training 500/553. train loss: 0.8320,	0.8496 s / batch. (data: 5.44e-03). ETA=6:47:56, max mem: 20.9 GB 
[12/02 07:46:40 visual_prompt]: Epoch 48 / 100: avg data time: 2.95e-01, avg batch time: 1.1284, average train loss: 0.9310
[12/02 07:47:46 visual_prompt]: Inference (val):avg data time: 2.39e-04, avg batch time: 0.3115, average loss: 0.9317
[12/02 07:47:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 73.50	
[12/02 07:47:46 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[12/02 07:49:43 visual_prompt]: 	Training 100/553. train loss: 0.2374,	0.8787 s / batch. (data: 3.47e-02). ETA=6:59:40, max mem: 20.9 GB 
[12/02 07:51:33 visual_prompt]: 	Training 200/553. train loss: 0.3224,	0.8497 s / batch. (data: 1.08e-02). ETA=6:44:23, max mem: 20.9 GB 
[12/02 07:53:26 visual_prompt]: 	Training 300/553. train loss: 0.9482,	0.8242 s / batch. (data: 4.10e-04). ETA=6:30:54, max mem: 20.9 GB 
[12/02 07:55:20 visual_prompt]: 	Training 400/553. train loss: 0.4818,	0.8315 s / batch. (data: 7.84e-04). ETA=6:32:58, max mem: 20.9 GB 
[12/02 07:57:13 visual_prompt]: 	Training 500/553. train loss: 0.4828,	0.8720 s / batch. (data: 1.60e-02). ETA=6:50:39, max mem: 20.9 GB 
[12/02 07:58:11 visual_prompt]: Epoch 49 / 100: avg data time: 2.97e-01, avg batch time: 1.1310, average train loss: 0.8215
[12/02 07:59:18 visual_prompt]: Inference (val):avg data time: 7.60e-05, avg batch time: 0.3098, average loss: 0.7394
[12/02 07:59:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 72.14	
[12/02 07:59:18 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[12/02 08:01:16 visual_prompt]: 	Training 100/553. train loss: 1.0234,	0.8600 s / batch. (data: 5.45e-03). ETA=6:42:48, max mem: 20.9 GB 
[12/02 08:03:08 visual_prompt]: 	Training 200/553. train loss: 2.1429,	0.8165 s / batch. (data: 3.49e-04). ETA=6:21:03, max mem: 20.9 GB 
[12/02 08:05:00 visual_prompt]: 	Training 300/553. train loss: 0.2443,	0.8341 s / batch. (data: 9.33e-04). ETA=6:27:53, max mem: 20.9 GB 
[12/02 08:06:49 visual_prompt]: 	Training 400/553. train loss: 1.9235,	0.8208 s / batch. (data: 5.64e-04). ETA=6:20:21, max mem: 20.9 GB 
[12/02 08:08:42 visual_prompt]: 	Training 500/553. train loss: 0.2269,	0.8369 s / batch. (data: 3.64e-04). ETA=6:26:25, max mem: 20.9 GB 
[12/02 08:09:39 visual_prompt]: Epoch 50 / 100: avg data time: 2.91e-01, avg batch time: 1.1233, average train loss: 0.8887
[12/02 08:10:44 visual_prompt]: Inference (val):avg data time: 5.88e-05, avg batch time: 0.3097, average loss: 1.9807
[12/02 08:10:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 74.15	
[12/02 08:10:44 visual_prompt]: Stopping early.
[12/02 08:10:44 visual_prompt]: Rank of current process: 0. World size: 1
[12/02 08:10:44 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/02 08:10:44 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/02 08:10:44 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/02 08:10:44 visual_prompt]: Training with config:
[12/02 08:10:44 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.5_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/02 08:10:44 visual_prompt]: Loading training data...
[12/02 08:10:44 visual_prompt]: Constructing mammo-cbis dataset train...
[12/02 08:10:44 visual_prompt]: Loading validation data...
[12/02 08:10:44 visual_prompt]: Constructing mammo-cbis dataset val...
[12/02 08:10:44 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[12/02 08:10:47 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[12/02 08:10:47 visual_prompt]: tuned percent:0.525
[12/02 08:10:47 visual_prompt]: Device used for model: 0
[12/02 08:10:47 visual_prompt]: Setting up Evaluator...
[12/02 08:10:47 visual_prompt]: Setting up Trainer...
[12/02 08:10:47 visual_prompt]: 	Setting up the optimizer...
[12/02 08:10:47 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/02 08:12:43 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8386 s / batch. (data: 3.15e-04). ETA=12:51:33, max mem: 20.9 GB 
[12/02 08:14:32 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8600 s / batch. (data: 5.47e-03). ETA=13:09:46, max mem: 20.9 GB 
[12/02 08:16:28 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.2286 s / batch. (data: 1.39e+00). ETA=1 day, 10:02:52, max mem: 20.9 GB 
[12/02 08:18:17 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8240 s / batch. (data: 3.16e-04). ETA=12:33:58, max mem: 20.9 GB 
[12/02 08:20:11 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8317 s / batch. (data: 1.17e-03). ETA=12:39:35, max mem: 20.9 GB 
[12/02 08:21:10 visual_prompt]: Epoch 1 / 100: avg data time: 2.93e-01, avg batch time: 1.1258, average train loss: 1.5403
[12/02 08:22:15 visual_prompt]: Inference (val):avg data time: 4.57e-04, avg batch time: 0.3105, average loss: 1.5201
[12/02 08:22:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[12/02 08:22:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[12/02 08:24:11 visual_prompt]: 	Training 100/553. train loss: 0.7700,	1.2792 s / batch. (data: 4.60e-01). ETA=19:25:04, max mem: 20.9 GB 
[12/02 08:26:03 visual_prompt]: 	Training 200/553. train loss: 0.1688,	0.8509 s / batch. (data: 3.11e-02). ETA=12:53:36, max mem: 20.9 GB 
[12/02 08:27:57 visual_prompt]: 	Training 300/553. train loss: 0.9670,	1.3955 s / batch. (data: 5.57e-01). ETA=21:06:21, max mem: 20.9 GB 
[12/02 08:29:47 visual_prompt]: 	Training 400/553. train loss: 1.0565,	0.8252 s / batch. (data: 3.60e-04). ETA=12:27:28, max mem: 20.9 GB 
[12/02 08:31:42 visual_prompt]: 	Training 500/553. train loss: 0.6524,	0.8360 s / batch. (data: 4.69e-04). ETA=12:35:49, max mem: 20.9 GB 
[12/02 08:32:39 visual_prompt]: Epoch 2 / 100: avg data time: 2.94e-01, avg batch time: 1.1268, average train loss: 0.8609
[12/02 08:33:45 visual_prompt]: Inference (val):avg data time: 6.47e-05, avg batch time: 0.3121, average loss: 0.7371
[12/02 08:33:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.87	
[12/02 08:33:45 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[12/02 08:35:41 visual_prompt]: 	Training 100/553. train loss: 0.7855,	0.8200 s / batch. (data: 4.33e-04). ETA=12:19:14, max mem: 20.9 GB 
[12/02 08:37:35 visual_prompt]: 	Training 200/553. train loss: 0.8414,	1.5200 s / batch. (data: 6.95e-01). ETA=22:47:53, max mem: 20.9 GB 
[12/02 08:39:27 visual_prompt]: 	Training 300/553. train loss: 0.6527,	0.8521 s / batch. (data: 1.60e-02). ETA=12:45:22, max mem: 20.9 GB 
[12/02 08:41:20 visual_prompt]: 	Training 400/553. train loss: 0.9671,	0.8324 s / batch. (data: 3.17e-04). ETA=12:26:16, max mem: 20.9 GB 
[12/02 08:43:15 visual_prompt]: 	Training 500/553. train loss: 0.7584,	1.7068 s / batch. (data: 8.92e-01). ETA=1 day, 1:27:27, max mem: 20.9 GB 
[12/02 08:44:12 visual_prompt]: Epoch 3 / 100: avg data time: 2.98e-01, avg batch time: 1.1319, average train loss: 0.7676
[12/02 08:45:17 visual_prompt]: Inference (val):avg data time: 4.79e-05, avg batch time: 0.3097, average loss: 0.7342
[12/02 08:45:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.38	
[12/02 08:45:17 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[12/02 08:47:16 visual_prompt]: 	Training 100/553. train loss: 0.8624,	0.8433 s / batch. (data: 1.16e-02). ETA=12:32:30, max mem: 20.9 GB 
[12/02 08:49:09 visual_prompt]: 	Training 200/553. train loss: 0.9227,	0.8460 s / batch. (data: 5.52e-03). ETA=12:33:29, max mem: 20.9 GB 
[12/02 08:51:02 visual_prompt]: 	Training 300/553. train loss: 0.7043,	2.0569 s / batch. (data: 1.22e+00). ETA=1 day, 6:28:39, max mem: 20.9 GB 
[12/02 08:52:50 visual_prompt]: 	Training 400/553. train loss: 0.6346,	1.8595 s / batch. (data: 1.02e+00). ETA=1 day, 3:30:00, max mem: 20.9 GB 
[12/02 08:54:45 visual_prompt]: 	Training 500/553. train loss: 0.1718,	4.1504 s / batch. (data: 3.32e+00). ETA=2 days, 13:15:57, max mem: 20.9 GB 
[12/02 08:55:44 visual_prompt]: Epoch 4 / 100: avg data time: 2.99e-01, avg batch time: 1.1329, average train loss: 0.9653
[12/02 08:56:49 visual_prompt]: Inference (val):avg data time: 5.55e-05, avg batch time: 0.3109, average loss: 1.0900
[12/02 08:56:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.41	
[12/02 08:56:49 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[12/02 08:58:44 visual_prompt]: 	Training 100/553. train loss: 2.2221,	0.8247 s / batch. (data: 4.46e-04). ETA=12:08:20, max mem: 20.9 GB 
[12/02 09:00:36 visual_prompt]: 	Training 200/553. train loss: 0.6535,	1.5172 s / batch. (data: 6.90e-01). ETA=22:17:22, max mem: 20.9 GB 
[12/02 09:02:29 visual_prompt]: 	Training 300/553. train loss: 1.4512,	0.8234 s / batch. (data: 3.03e-04). ETA=12:04:23, max mem: 20.9 GB 
[12/02 09:04:20 visual_prompt]: 	Training 400/553. train loss: 2.0240,	0.8527 s / batch. (data: 5.45e-03). ETA=12:28:48, max mem: 20.9 GB 
[12/02 09:06:13 visual_prompt]: 	Training 500/553. train loss: 0.5817,	0.8400 s / batch. (data: 3.50e-04). ETA=12:16:15, max mem: 20.9 GB 
[12/02 09:07:11 visual_prompt]: Epoch 5 / 100: avg data time: 2.94e-01, avg batch time: 1.1258, average train loss: 0.9193
[12/02 09:08:17 visual_prompt]: Inference (val):avg data time: 5.29e-05, avg batch time: 0.3094, average loss: 1.2579
[12/02 09:08:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.52	
[12/02 09:08:17 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[12/02 09:10:17 visual_prompt]: 	Training 100/553. train loss: 0.7583,	0.8330 s / batch. (data: 3.77e-04). ETA=12:07:57, max mem: 20.9 GB 
[12/02 09:12:07 visual_prompt]: 	Training 200/553. train loss: 2.2897,	0.8570 s / batch. (data: 9.71e-03). ETA=12:27:29, max mem: 20.9 GB 
[12/02 09:13:56 visual_prompt]: 	Training 300/553. train loss: 0.5654,	0.8400 s / batch. (data: 1.20e-02). ETA=12:11:17, max mem: 20.9 GB 
[12/02 09:15:51 visual_prompt]: 	Training 400/553. train loss: 0.5669,	0.8215 s / batch. (data: 5.44e-03). ETA=11:53:46, max mem: 20.9 GB 
[12/02 09:17:40 visual_prompt]: 	Training 500/553. train loss: 0.7552,	1.0080 s / batch. (data: 1.59e-01). ETA=14:34:11, max mem: 20.9 GB 
[12/02 09:18:37 visual_prompt]: Epoch 6 / 100: avg data time: 2.88e-01, avg batch time: 1.1208, average train loss: 0.9855
[12/02 09:19:42 visual_prompt]: Inference (val):avg data time: 5.10e-05, avg batch time: 0.3107, average loss: 0.9076
[12/02 09:19:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.66	
[12/02 09:19:42 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[12/02 09:21:38 visual_prompt]: 	Training 100/553. train loss: 2.4193,	0.8397 s / batch. (data: 1.56e-02). ETA=12:06:05, max mem: 20.9 GB 
[12/02 09:23:30 visual_prompt]: 	Training 200/553. train loss: 0.5644,	0.8357 s / batch. (data: 9.44e-04). ETA=12:01:13, max mem: 20.9 GB 
[12/02 09:25:26 visual_prompt]: 	Training 300/553. train loss: 0.5432,	2.4816 s / batch. (data: 1.65e+00). ETA=1 day, 11:37:31, max mem: 20.9 GB 
[12/02 09:27:17 visual_prompt]: 	Training 400/553. train loss: 0.5758,	2.3973 s / batch. (data: 1.57e+00). ETA=1 day, 10:20:58, max mem: 20.9 GB 
[12/02 09:29:05 visual_prompt]: 	Training 500/553. train loss: 1.1166,	0.8315 s / batch. (data: 3.40e-04). ETA=11:53:29, max mem: 20.9 GB 
[12/02 09:30:02 visual_prompt]: Epoch 7 / 100: avg data time: 2.88e-01, avg batch time: 1.1209, average train loss: 1.0383
[12/02 09:31:07 visual_prompt]: Inference (val):avg data time: 4.56e-05, avg batch time: 0.3106, average loss: 0.7151
[12/02 09:31:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.52	
[12/02 09:31:07 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[12/02 09:33:01 visual_prompt]: 	Training 100/553. train loss: 2.1714,	0.8297 s / batch. (data: 6.55e-03). ETA=11:49:46, max mem: 20.9 GB 
[12/02 09:34:54 visual_prompt]: 	Training 200/553. train loss: 0.6697,	0.8191 s / batch. (data: 1.23e-03). ETA=11:39:19, max mem: 20.9 GB 
[12/02 09:36:47 visual_prompt]: 	Training 300/553. train loss: 0.6618,	0.8275 s / batch. (data: 2.06e-04). ETA=11:45:06, max mem: 20.9 GB 
[12/02 09:38:39 visual_prompt]: 	Training 400/553. train loss: 0.6970,	0.8271 s / batch. (data: 3.26e-04). ETA=11:43:25, max mem: 20.9 GB 
[12/02 09:40:31 visual_prompt]: 	Training 500/553. train loss: 3.6742,	1.8285 s / batch. (data: 1.00e+00). ETA=1 day, 1:52:02, max mem: 20.9 GB 
[12/02 09:41:28 visual_prompt]: Epoch 8 / 100: avg data time: 2.91e-01, avg batch time: 1.1237, average train loss: 1.1705
[12/02 09:42:33 visual_prompt]: Inference (val):avg data time: 4.48e-05, avg batch time: 0.3108, average loss: 0.9263
[12/02 09:42:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.42	
[12/02 09:42:33 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[12/02 09:44:29 visual_prompt]: 	Training 100/553. train loss: 0.3339,	0.8400 s / batch. (data: 7.94e-03). ETA=11:50:51, max mem: 20.9 GB 
[12/02 09:46:21 visual_prompt]: 	Training 200/553. train loss: 0.8443,	0.8357 s / batch. (data: 3.29e-04). ETA=11:45:52, max mem: 20.9 GB 
[12/02 09:48:13 visual_prompt]: 	Training 300/553. train loss: 0.6550,	2.0600 s / batch. (data: 1.22e+00). ETA=1 day, 4:56:24, max mem: 20.9 GB 
[12/02 09:50:05 visual_prompt]: 	Training 400/553. train loss: 0.9674,	0.8254 s / batch. (data: 3.29e-04). ETA=11:34:20, max mem: 20.9 GB 
[12/02 09:51:57 visual_prompt]: 	Training 500/553. train loss: 0.7006,	1.2569 s / batch. (data: 4.41e-01). ETA=17:35:15, max mem: 20.9 GB 
[12/02 09:52:54 visual_prompt]: Epoch 9 / 100: avg data time: 2.90e-01, avg batch time: 1.1222, average train loss: 1.0496
[12/02 09:53:58 visual_prompt]: Inference (val):avg data time: 4.50e-05, avg batch time: 0.3114, average loss: 0.6961
[12/02 09:53:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.60	
[12/02 09:53:58 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[12/02 09:55:56 visual_prompt]: 	Training 100/553. train loss: 3.6842,	0.8319 s / batch. (data: 4.37e-04). ETA=11:36:22, max mem: 20.9 GB 
[12/02 09:57:47 visual_prompt]: 	Training 200/553. train loss: 2.0461,	0.8440 s / batch. (data: 3.57e-04). ETA=11:45:03, max mem: 20.9 GB 
[12/02 09:59:37 visual_prompt]: 	Training 300/553. train loss: 3.3785,	1.4510 s / batch. (data: 6.35e-01). ETA=20:09:45, max mem: 20.9 GB 
[12/02 10:01:27 visual_prompt]: 	Training 400/553. train loss: 0.9255,	1.2249 s / batch. (data: 3.82e-01). ETA=16:59:11, max mem: 20.9 GB 
[12/02 10:03:20 visual_prompt]: 	Training 500/553. train loss: 0.5967,	1.4650 s / batch. (data: 6.24e-01). ETA=20:16:30, max mem: 20.9 GB 
[12/02 10:04:19 visual_prompt]: Epoch 10 / 100: avg data time: 2.90e-01, avg batch time: 1.1216, average train loss: 1.3934
[12/02 10:05:24 visual_prompt]: Inference (val):avg data time: 4.91e-05, avg batch time: 0.3092, average loss: 0.7303
[12/02 10:05:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.89	
[12/02 10:05:24 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[12/02 10:07:22 visual_prompt]: 	Training 100/553. train loss: 0.8313,	0.8290 s / batch. (data: 1.60e-02). ETA=11:26:17, max mem: 20.9 GB 
[12/02 10:09:16 visual_prompt]: 	Training 200/553. train loss: 0.6824,	0.8433 s / batch. (data: 1.52e-02). ETA=11:36:41, max mem: 20.9 GB 
[12/02 10:11:09 visual_prompt]: 	Training 300/553. train loss: 0.1487,	2.8798 s / batch. (data: 2.06e+00). ETA=1 day, 15:34:25, max mem: 20.9 GB 
[12/02 10:12:59 visual_prompt]: 	Training 400/553. train loss: 0.6376,	0.8326 s / batch. (data: 3.02e-04). ETA=11:25:04, max mem: 20.9 GB 
[12/02 10:14:49 visual_prompt]: 	Training 500/553. train loss: 1.5787,	0.8755 s / batch. (data: 1.67e-03). ETA=11:58:53, max mem: 20.9 GB 
[12/02 10:15:45 visual_prompt]: Epoch 11 / 100: avg data time: 2.91e-01, avg batch time: 1.1241, average train loss: 1.1421
[12/02 10:16:50 visual_prompt]: Inference (val):avg data time: 4.53e-05, avg batch time: 0.3099, average loss: 1.3272
[12/02 10:16:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.44	
[12/02 10:16:50 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[12/02 10:18:48 visual_prompt]: 	Training 100/553. train loss: 0.7520,	0.8617 s / batch. (data: 3.41e-04). ETA=11:45:23, max mem: 20.9 GB 
[12/02 10:20:40 visual_prompt]: 	Training 200/553. train loss: 0.8649,	0.8659 s / batch. (data: 1.73e-02). ETA=11:47:24, max mem: 20.9 GB 
[12/02 10:22:31 visual_prompt]: 	Training 300/553. train loss: 1.3444,	0.8365 s / batch. (data: 3.18e-04). ETA=11:22:01, max mem: 20.9 GB 
[12/02 10:24:22 visual_prompt]: 	Training 400/553. train loss: 0.7673,	0.8600 s / batch. (data: 7.95e-03). ETA=11:39:42, max mem: 20.9 GB 
[12/02 10:26:14 visual_prompt]: 	Training 500/553. train loss: 1.1665,	0.8280 s / batch. (data: 3.13e-04). ETA=11:12:18, max mem: 20.9 GB 
[12/02 10:27:11 visual_prompt]: Epoch 12 / 100: avg data time: 2.88e-01, avg batch time: 1.1222, average train loss: 1.3947
[12/02 10:28:16 visual_prompt]: Inference (val):avg data time: 5.18e-05, avg batch time: 0.3095, average loss: 0.8203
[12/02 10:28:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.77	
[12/02 10:28:16 visual_prompt]: Best epoch 12: best metric: -0.820
[12/02 10:28:16 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[12/02 10:30:14 visual_prompt]: 	Training 100/553. train loss: 0.7714,	0.8149 s / batch. (data: 3.53e-04). ETA=10:59:34, max mem: 20.9 GB 
[12/02 10:32:02 visual_prompt]: 	Training 200/553. train loss: 0.7119,	0.8478 s / batch. (data: 1.12e-02). ETA=11:24:48, max mem: 20.9 GB 
[12/02 10:33:55 visual_prompt]: 	Training 300/553. train loss: 0.6117,	2.3487 s / batch. (data: 1.52e+00). ETA=1 day, 7:33:10, max mem: 20.9 GB 
[12/02 10:35:45 visual_prompt]: 	Training 400/553. train loss: 3.0195,	0.8320 s / batch. (data: 7.96e-03). ETA=11:09:15, max mem: 20.9 GB 
[12/02 10:37:37 visual_prompt]: 	Training 500/553. train loss: 2.4177,	0.8316 s / batch. (data: 3.02e-04). ETA=11:07:31, max mem: 20.9 GB 
[12/02 10:38:35 visual_prompt]: Epoch 13 / 100: avg data time: 2.87e-01, avg batch time: 1.1194, average train loss: 1.3389
[12/02 10:39:40 visual_prompt]: Inference (val):avg data time: 3.17e-04, avg batch time: 0.3113, average loss: 0.8071
[12/02 10:39:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 45.86	
[12/02 10:39:40 visual_prompt]: Best epoch 13: best metric: -0.807
[12/02 10:39:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[12/02 10:41:37 visual_prompt]: 	Training 100/553. train loss: 0.5203,	0.8358 s / batch. (data: 7.12e-04). ETA=11:08:48, max mem: 20.9 GB 
[12/02 10:43:29 visual_prompt]: 	Training 200/553. train loss: 0.1359,	1.6839 s / batch. (data: 8.52e-01). ETA=22:24:36, max mem: 20.9 GB 
[12/02 10:45:19 visual_prompt]: 	Training 300/553. train loss: 0.7159,	0.9626 s / batch. (data: 1.44e-01). ETA=12:47:04, max mem: 20.9 GB 
[12/02 10:47:11 visual_prompt]: 	Training 400/553. train loss: 0.4982,	0.8800 s / batch. (data: 7.95e-03). ETA=11:39:45, max mem: 20.9 GB 
[12/02 10:49:02 visual_prompt]: 	Training 500/553. train loss: 1.7130,	0.8436 s / batch. (data: 2.69e-04). ETA=11:09:22, max mem: 20.9 GB 
[12/02 10:49:59 visual_prompt]: Epoch 14 / 100: avg data time: 2.85e-01, avg batch time: 1.1184, average train loss: 1.2353
[12/02 10:51:04 visual_prompt]: Inference (val):avg data time: 2.88e-04, avg batch time: 0.3094, average loss: 0.8232
[12/02 10:51:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.43	
[12/02 10:51:04 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[12/02 10:53:00 visual_prompt]: 	Training 100/553. train loss: 0.7051,	0.8560 s / batch. (data: 7.95e-03). ETA=11:17:04, max mem: 20.9 GB 
[12/02 10:54:49 visual_prompt]: 	Training 200/553. train loss: 5.6507,	0.8598 s / batch. (data: 1.28e-03). ETA=11:18:37, max mem: 20.9 GB 
[12/02 10:56:42 visual_prompt]: 	Training 300/553. train loss: 1.0801,	0.8282 s / batch. (data: 2.84e-04). ETA=10:52:17, max mem: 20.9 GB 
[12/02 10:58:31 visual_prompt]: 	Training 400/553. train loss: 1.5442,	0.8560 s / batch. (data: 7.93e-03). ETA=11:12:48, max mem: 20.9 GB 
[12/02 11:00:24 visual_prompt]: 	Training 500/553. train loss: 0.6505,	0.8411 s / batch. (data: 1.05e-02). ETA=10:59:39, max mem: 20.9 GB 
[12/02 11:01:23 visual_prompt]: Epoch 15 / 100: avg data time: 2.88e-01, avg batch time: 1.1197, average train loss: 1.3959
[12/02 11:02:27 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.3100, average loss: 1.1552
[12/02 11:02:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.79	
[12/02 11:02:27 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[12/02 11:04:22 visual_prompt]: 	Training 100/553. train loss: 3.9407,	0.8265 s / batch. (data: 3.09e-04). ETA=10:46:05, max mem: 20.9 GB 
[12/02 11:06:13 visual_prompt]: 	Training 200/553. train loss: 2.2138,	0.8501 s / batch. (data: 3.18e-04). ETA=11:03:07, max mem: 20.9 GB 
[12/02 11:08:06 visual_prompt]: 	Training 300/553. train loss: 0.6724,	0.8614 s / batch. (data: 6.70e-04). ETA=11:10:30, max mem: 20.9 GB 
[12/02 11:09:57 visual_prompt]: 	Training 400/553. train loss: 1.2174,	0.8352 s / batch. (data: 4.13e-04). ETA=10:48:44, max mem: 20.9 GB 
[12/02 11:11:49 visual_prompt]: 	Training 500/553. train loss: 0.8232,	1.8200 s / batch. (data: 9.88e-01). ETA=23:30:39, max mem: 20.9 GB 
[12/02 11:12:47 visual_prompt]: Epoch 16 / 100: avg data time: 2.87e-01, avg batch time: 1.1200, average train loss: 1.4743
[12/02 11:13:52 visual_prompt]: Inference (val):avg data time: 1.23e-04, avg batch time: 0.3109, average loss: 0.6890
[12/02 11:13:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.91	
[12/02 11:13:52 visual_prompt]: Best epoch 16: best metric: -0.689
[12/02 11:13:52 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[12/02 11:15:47 visual_prompt]: 	Training 100/553. train loss: 0.8575,	0.8283 s / batch. (data: 5.51e-03). ETA=10:39:53, max mem: 20.9 GB 
[12/02 11:17:39 visual_prompt]: 	Training 200/553. train loss: 3.3039,	0.8468 s / batch. (data: 5.43e-03). ETA=10:52:44, max mem: 20.9 GB 
[12/02 11:19:30 visual_prompt]: 	Training 300/553. train loss: 0.9985,	0.8440 s / batch. (data: 1.20e-02). ETA=10:49:12, max mem: 20.9 GB 
[12/02 11:21:20 visual_prompt]: 	Training 400/553. train loss: 2.4057,	1.4825 s / batch. (data: 6.68e-01). ETA=18:57:52, max mem: 20.9 GB 
[12/02 11:23:12 visual_prompt]: 	Training 500/553. train loss: 0.5941,	2.1072 s / batch. (data: 1.29e+00). ETA=1 day, 2:53:52, max mem: 20.9 GB 
[12/02 11:24:11 visual_prompt]: Epoch 17 / 100: avg data time: 2.87e-01, avg batch time: 1.1195, average train loss: 1.4583
[12/02 11:25:16 visual_prompt]: Inference (val):avg data time: 3.81e-04, avg batch time: 0.3104, average loss: 1.1845
[12/02 11:25:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.50	
[12/02 11:25:16 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[12/02 11:27:12 visual_prompt]: 	Training 100/553. train loss: 0.8823,	0.8530 s / batch. (data: 5.48e-03). ETA=10:51:08, max mem: 20.9 GB 
[12/02 11:29:08 visual_prompt]: 	Training 200/553. train loss: 2.5699,	0.8640 s / batch. (data: 3.73e-04). ETA=10:58:06, max mem: 20.9 GB 
[12/02 11:31:00 visual_prompt]: 	Training 300/553. train loss: 0.5949,	0.8207 s / batch. (data: 3.22e-04). ETA=10:23:43, max mem: 20.9 GB 
[12/02 11:32:50 visual_prompt]: 	Training 400/553. train loss: 0.9589,	0.8400 s / batch. (data: 3.38e-04). ETA=10:36:59, max mem: 20.9 GB 
[12/02 11:34:42 visual_prompt]: 	Training 500/553. train loss: 0.9227,	0.8400 s / batch. (data: 8.67e-04). ETA=10:35:36, max mem: 20.9 GB 
[12/02 11:35:38 visual_prompt]: Epoch 18 / 100: avg data time: 2.91e-01, avg batch time: 1.1241, average train loss: 1.4059
[12/02 11:36:44 visual_prompt]: Inference (val):avg data time: 5.68e-05, avg batch time: 0.3091, average loss: 1.4272
[12/02 11:36:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.14	
[12/02 11:36:44 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[12/02 11:38:39 visual_prompt]: 	Training 100/553. train loss: 0.5746,	0.8385 s / batch. (data: 1.05e-02). ETA=10:32:20, max mem: 20.9 GB 
[12/02 11:40:31 visual_prompt]: 	Training 200/553. train loss: 0.7530,	0.8170 s / batch. (data: 5.25e-04). ETA=10:14:45, max mem: 20.9 GB 
[12/02 11:42:23 visual_prompt]: 	Training 300/553. train loss: 0.3898,	0.8343 s / batch. (data: 1.34e-03). ETA=10:26:21, max mem: 20.9 GB 
[12/02 11:44:16 visual_prompt]: 	Training 400/553. train loss: 0.9271,	0.8170 s / batch. (data: 4.17e-04). ETA=10:11:59, max mem: 20.9 GB 
[12/02 11:46:03 visual_prompt]: 	Training 500/553. train loss: 0.5652,	0.8461 s / batch. (data: 3.43e-04). ETA=10:32:25, max mem: 20.9 GB 
[12/02 11:47:02 visual_prompt]: Epoch 19 / 100: avg data time: 2.84e-01, avg batch time: 1.1171, average train loss: 1.3115
[12/02 11:48:07 visual_prompt]: Inference (val):avg data time: 5.10e-05, avg batch time: 0.3100, average loss: 5.8312
[12/02 11:48:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.63	
[12/02 11:48:07 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[12/02 11:50:02 visual_prompt]: 	Training 100/553. train loss: 0.7514,	0.8330 s / batch. (data: 3.40e-04). ETA=10:20:28, max mem: 20.9 GB 
[12/02 11:51:57 visual_prompt]: 	Training 200/553. train loss: 0.6134,	0.8476 s / batch. (data: 1.37e-03). ETA=10:29:54, max mem: 20.9 GB 
[12/02 11:53:49 visual_prompt]: 	Training 300/553. train loss: 0.6458,	0.8280 s / batch. (data: 6.47e-03). ETA=10:13:59, max mem: 20.9 GB 
[12/02 11:55:41 visual_prompt]: 	Training 400/553. train loss: 1.0217,	0.8520 s / batch. (data: 3.29e-04). ETA=10:30:23, max mem: 20.9 GB 
[12/02 11:57:31 visual_prompt]: 	Training 500/553. train loss: 1.3167,	0.8297 s / batch. (data: 3.32e-04). ETA=10:12:31, max mem: 20.9 GB 
[12/02 11:58:31 visual_prompt]: Epoch 20 / 100: avg data time: 2.96e-01, avg batch time: 1.1292, average train loss: 1.2826
[12/02 11:59:36 visual_prompt]: Inference (val):avg data time: 5.52e-05, avg batch time: 0.3114, average loss: 0.6973
[12/02 11:59:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.44	
[12/02 11:59:36 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[12/02 12:01:37 visual_prompt]: 	Training 100/553. train loss: 1.3835,	0.8458 s / batch. (data: 5.46e-03). ETA=10:22:11, max mem: 20.9 GB 
[12/02 12:03:28 visual_prompt]: 	Training 200/553. train loss: 1.9942,	0.8401 s / batch. (data: 5.46e-03). ETA=10:16:36, max mem: 20.9 GB 
[12/02 12:05:19 visual_prompt]: 	Training 300/553. train loss: 1.2501,	1.1041 s / batch. (data: 2.75e-01). ETA=13:28:35, max mem: 20.9 GB 
[12/02 12:07:11 visual_prompt]: 	Training 400/553. train loss: 1.1687,	0.8295 s / batch. (data: 3.04e-04). ETA=10:06:03, max mem: 20.9 GB 
[12/02 12:09:03 visual_prompt]: 	Training 500/553. train loss: 1.5510,	0.8252 s / batch. (data: 5.15e-03). ETA=10:01:33, max mem: 20.9 GB 
[12/02 12:10:00 visual_prompt]: Epoch 21 / 100: avg data time: 2.96e-01, avg batch time: 1.1282, average train loss: 1.3929
[12/02 12:11:05 visual_prompt]: Inference (val):avg data time: 5.26e-05, avg batch time: 0.3083, average loss: 0.8058
[12/02 12:11:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.46	
[12/02 12:11:05 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[12/02 12:13:01 visual_prompt]: 	Training 100/553. train loss: 0.7388,	0.8320 s / batch. (data: 3.51e-04). ETA=10:04:24, max mem: 20.9 GB 
[12/02 12:14:53 visual_prompt]: 	Training 200/553. train loss: 0.6400,	0.8720 s / batch. (data: 3.24e-04). ETA=10:32:00, max mem: 20.9 GB 
[12/02 12:16:44 visual_prompt]: 	Training 300/553. train loss: 0.2082,	0.8336 s / batch. (data: 9.94e-03). ETA=10:02:48, max mem: 20.9 GB 
[12/02 12:18:37 visual_prompt]: 	Training 400/553. train loss: 3.0826,	0.8302 s / batch. (data: 1.28e-02). ETA=9:58:55, max mem: 20.9 GB 
[12/02 12:20:30 visual_prompt]: 	Training 500/553. train loss: 0.8451,	0.8315 s / batch. (data: 1.56e-02). ETA=9:58:32, max mem: 20.9 GB 
[12/02 12:21:29 visual_prompt]: Epoch 22 / 100: avg data time: 2.95e-01, avg batch time: 1.1282, average train loss: 1.4194
[12/02 12:22:35 visual_prompt]: Inference (val):avg data time: 5.19e-05, avg batch time: 0.3083, average loss: 1.3268
[12/02 12:22:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.88	
[12/02 12:22:35 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[12/02 12:24:33 visual_prompt]: 	Training 100/553. train loss: 0.9541,	1.4052 s / batch. (data: 5.71e-01). ETA=16:47:51, max mem: 20.9 GB 
[12/02 12:26:27 visual_prompt]: 	Training 200/553. train loss: 0.5850,	0.8684 s / batch. (data: 5.29e-02). ETA=10:21:25, max mem: 20.9 GB 
[12/02 12:28:21 visual_prompt]: 	Training 300/553. train loss: 0.5825,	0.8400 s / batch. (data: 3.41e-04). ETA=9:59:39, max mem: 20.9 GB 
[12/02 12:30:09 visual_prompt]: 	Training 400/553. train loss: 0.9522,	0.8500 s / batch. (data: 3.18e-04). ETA=10:05:22, max mem: 20.9 GB 
[12/02 12:31:59 visual_prompt]: 	Training 500/553. train loss: 0.2300,	0.8408 s / batch. (data: 1.24e-02). ETA=9:57:27, max mem: 20.9 GB 
[12/02 12:32:56 visual_prompt]: Epoch 23 / 100: avg data time: 2.92e-01, avg batch time: 1.1243, average train loss: 1.2357
[12/02 12:34:01 visual_prompt]: Inference (val):avg data time: 5.22e-05, avg batch time: 0.3084, average loss: 0.9518
[12/02 12:34:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.56	
[12/02 12:34:01 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[12/02 12:35:55 visual_prompt]: 	Training 100/553. train loss: 1.1287,	0.8444 s / batch. (data: 3.23e-04). ETA=9:57:49, max mem: 20.9 GB 
[12/02 12:37:47 visual_prompt]: 	Training 200/553. train loss: 0.8943,	0.8368 s / batch. (data: 1.36e-03). ETA=9:51:03, max mem: 20.9 GB 
[12/02 12:39:38 visual_prompt]: 	Training 300/553. train loss: 0.8214,	1.2489 s / batch. (data: 4.29e-01). ETA=14:40:04, max mem: 20.9 GB 
[12/02 12:41:31 visual_prompt]: 	Training 400/553. train loss: 0.5857,	0.8200 s / batch. (data: 3.37e-04). ETA=9:36:28, max mem: 20.9 GB 
[12/02 12:43:23 visual_prompt]: 	Training 500/553. train loss: 0.7096,	0.8837 s / batch. (data: 1.45e-03). ETA=10:19:46, max mem: 20.9 GB 
[12/02 12:44:22 visual_prompt]: Epoch 24 / 100: avg data time: 2.89e-01, avg batch time: 1.1227, average train loss: 1.4594
[12/02 12:45:28 visual_prompt]: Inference (val):avg data time: 3.03e-04, avg batch time: 0.3101, average loss: 0.6917
[12/02 12:45:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.10	
[12/02 12:45:28 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[12/02 12:47:28 visual_prompt]: 	Training 100/553. train loss: 0.8401,	0.8485 s / batch. (data: 3.14e-04). ETA=9:52:57, max mem: 20.9 GB 
[12/02 12:49:16 visual_prompt]: 	Training 200/553. train loss: 0.6188,	1.0559 s / batch. (data: 2.40e-01). ETA=12:16:08, max mem: 20.9 GB 
[12/02 12:51:08 visual_prompt]: 	Training 300/553. train loss: 1.1996,	0.8484 s / batch. (data: 3.54e-04). ETA=9:50:03, max mem: 20.9 GB 
[12/02 12:52:59 visual_prompt]: 	Training 400/553. train loss: 0.6409,	1.6975 s / batch. (data: 8.30e-01). ETA=19:37:42, max mem: 20.9 GB 
[12/02 12:54:51 visual_prompt]: 	Training 500/553. train loss: 1.3159,	1.3320 s / batch. (data: 5.02e-01). ETA=15:21:55, max mem: 20.9 GB 
[12/02 12:55:50 visual_prompt]: Epoch 25 / 100: avg data time: 2.91e-01, avg batch time: 1.1252, average train loss: 1.3519
[12/02 12:56:55 visual_prompt]: Inference (val):avg data time: 3.04e-04, avg batch time: 0.3109, average loss: 1.5980
[12/02 12:56:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.26	
[12/02 12:56:55 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[12/02 12:58:51 visual_prompt]: 	Training 100/553. train loss: 2.1431,	0.8361 s / batch. (data: 5.44e-03). ETA=9:36:32, max mem: 20.9 GB 
[12/02 13:00:45 visual_prompt]: 	Training 200/553. train loss: 0.6065,	2.3582 s / batch. (data: 1.55e+00). ETA=1 day, 3:02:14, max mem: 20.9 GB 
[12/02 13:02:38 visual_prompt]: 	Training 300/553. train loss: 1.4141,	0.8400 s / batch. (data: 3.40e-04). ETA=9:36:26, max mem: 20.9 GB 
[12/02 13:04:29 visual_prompt]: 	Training 400/553. train loss: 0.6268,	0.8485 s / batch. (data: 3.29e-04). ETA=9:40:52, max mem: 20.9 GB 
[12/02 13:06:18 visual_prompt]: 	Training 500/553. train loss: 1.7169,	0.8560 s / batch. (data: 3.39e-04). ETA=9:44:33, max mem: 20.9 GB 
[12/02 13:07:16 visual_prompt]: Epoch 26 / 100: avg data time: 2.89e-01, avg batch time: 1.1220, average train loss: 1.1621
[12/02 13:08:20 visual_prompt]: Inference (val):avg data time: 4.64e-05, avg batch time: 0.3086, average loss: 1.3134
[12/02 13:08:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.72	
[12/02 13:08:20 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[12/02 13:10:17 visual_prompt]: 	Training 100/553. train loss: 0.9940,	0.8307 s / batch. (data: 3.19e-04). ETA=9:25:12, max mem: 20.9 GB 
[12/02 13:12:09 visual_prompt]: 	Training 200/553. train loss: 4.5847,	1.9720 s / batch. (data: 1.15e+00). ETA=22:18:24, max mem: 20.9 GB 
[12/02 13:14:00 visual_prompt]: 	Training 300/553. train loss: 1.9995,	0.8375 s / batch. (data: 4.03e-04). ETA=9:26:59, max mem: 20.9 GB 
[12/02 13:15:54 visual_prompt]: 	Training 400/553. train loss: 0.6904,	0.8374 s / batch. (data: 6.11e-03). ETA=9:25:32, max mem: 20.9 GB 
[12/02 13:17:46 visual_prompt]: 	Training 500/553. train loss: 0.5809,	0.8211 s / batch. (data: 8.34e-04). ETA=9:13:09, max mem: 20.9 GB 
[12/02 13:18:42 visual_prompt]: Epoch 27 / 100: avg data time: 2.91e-01, avg batch time: 1.1236, average train loss: 1.3266
[12/02 13:19:47 visual_prompt]: Inference (val):avg data time: 5.77e-05, avg batch time: 0.3120, average loss: 1.2304
[12/02 13:19:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.60	
[12/02 13:19:47 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[12/02 13:21:42 visual_prompt]: 	Training 100/553. train loss: 0.0578,	0.8211 s / batch. (data: 3.08e-04). ETA=9:11:03, max mem: 20.9 GB 
[12/02 13:23:34 visual_prompt]: 	Training 200/553. train loss: 3.9587,	0.8200 s / batch. (data: 3.39e-04). ETA=9:08:58, max mem: 20.9 GB 
[12/02 13:25:27 visual_prompt]: 	Training 300/553. train loss: 0.9422,	1.9789 s / batch. (data: 1.14e+00). ETA=22:01:34, max mem: 20.9 GB 
[12/02 13:27:17 visual_prompt]: 	Training 400/553. train loss: 1.7290,	0.8571 s / batch. (data: 1.56e-02). ETA=9:30:58, max mem: 20.9 GB 
[12/02 13:29:07 visual_prompt]: 	Training 500/553. train loss: 0.0043,	0.8196 s / batch. (data: 3.04e-04). ETA=9:04:38, max mem: 20.9 GB 
[12/02 13:30:06 visual_prompt]: Epoch 28 / 100: avg data time: 2.87e-01, avg batch time: 1.1203, average train loss: 1.4301
[12/02 13:31:11 visual_prompt]: Inference (val):avg data time: 5.34e-04, avg batch time: 0.3095, average loss: 0.7544
[12/02 13:31:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.30	
[12/02 13:31:11 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[12/02 13:33:15 visual_prompt]: 	Training 100/553. train loss: 0.3826,	0.8512 s / batch. (data: 1.12e-02). ETA=9:23:27, max mem: 20.9 GB 
[12/02 13:35:07 visual_prompt]: 	Training 200/553. train loss: 1.1219,	2.2608 s / batch. (data: 1.45e+00). ETA=1 day, 0:52:41, max mem: 20.9 GB 
[12/02 13:36:55 visual_prompt]: 	Training 300/553. train loss: 1.3036,	0.8185 s / batch. (data: 5.48e-03). ETA=8:59:04, max mem: 20.9 GB 
[12/02 13:38:43 visual_prompt]: 	Training 400/553. train loss: 2.4949,	1.2309 s / batch. (data: 4.15e-01). ETA=13:28:38, max mem: 20.9 GB 
[12/02 13:40:36 visual_prompt]: 	Training 500/553. train loss: 1.1566,	0.8199 s / batch. (data: 4.86e-04). ETA=8:57:15, max mem: 20.9 GB 
[12/02 13:41:34 visual_prompt]: Epoch 29 / 100: avg data time: 2.94e-01, avg batch time: 1.1266, average train loss: 1.3371
[12/02 13:42:40 visual_prompt]: Inference (val):avg data time: 1.80e-04, avg batch time: 0.3111, average loss: 2.8916
[12/02 13:42:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 38.71	
[12/02 13:42:40 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[12/02 13:44:35 visual_prompt]: 	Training 100/553. train loss: 2.1121,	0.8590 s / batch. (data: 3.66e-02). ETA=9:20:39, max mem: 20.9 GB 
[12/02 13:46:28 visual_prompt]: 	Training 200/553. train loss: 0.8000,	0.8360 s / batch. (data: 7.71e-03). ETA=9:04:18, max mem: 20.9 GB 
[12/02 13:48:18 visual_prompt]: 	Training 300/553. train loss: 0.4499,	1.4680 s / batch. (data: 6.39e-01). ETA=15:53:18, max mem: 20.9 GB 
[12/02 13:50:12 visual_prompt]: 	Training 400/553. train loss: 0.8217,	1.5077 s / batch. (data: 6.84e-01). ETA=16:16:32, max mem: 20.9 GB 
[12/02 13:52:04 visual_prompt]: 	Training 500/553. train loss: 0.8218,	2.1440 s / batch. (data: 1.32e+00). ETA=23:05:07, max mem: 20.9 GB 
[12/02 13:53:03 visual_prompt]: Epoch 30 / 100: avg data time: 2.94e-01, avg batch time: 1.1266, average train loss: 1.3557
[12/02 13:54:09 visual_prompt]: Inference (val):avg data time: 4.67e-05, avg batch time: 0.3108, average loss: 0.7049
[12/02 13:54:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.05	
[12/02 13:54:09 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[12/02 13:56:07 visual_prompt]: 	Training 100/553. train loss: 1.1711,	0.8388 s / batch. (data: 1.05e-02). ETA=8:59:45, max mem: 20.9 GB 
[12/02 13:58:01 visual_prompt]: 	Training 200/553. train loss: 0.7528,	0.8515 s / batch. (data: 1.06e-02). ETA=9:06:31, max mem: 20.9 GB 
[12/02 13:59:50 visual_prompt]: 	Training 300/553. train loss: 2.0702,	0.8400 s / batch. (data: 3.29e-04). ETA=8:57:43, max mem: 20.9 GB 
[12/02 14:01:40 visual_prompt]: 	Training 400/553. train loss: 2.2690,	0.8271 s / batch. (data: 3.51e-04). ETA=8:48:07, max mem: 20.9 GB 
[12/02 14:03:32 visual_prompt]: 	Training 500/553. train loss: 1.0295,	0.8240 s / batch. (data: 3.19e-04). ETA=8:44:44, max mem: 20.9 GB 
[12/02 14:04:29 visual_prompt]: Epoch 31 / 100: avg data time: 2.86e-01, avg batch time: 1.1205, average train loss: 1.2920
[12/02 14:05:34 visual_prompt]: Inference (val):avg data time: 5.67e-05, avg batch time: 0.3113, average loss: 1.2160
[12/02 14:05:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.52	
[12/02 14:05:34 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[12/02 14:07:32 visual_prompt]: 	Training 100/553. train loss: 0.5619,	0.8280 s / batch. (data: 2.59e-04). ETA=8:45:11, max mem: 20.9 GB 
[12/02 14:09:22 visual_prompt]: 	Training 200/553. train loss: 0.9662,	0.8173 s / batch. (data: 5.02e-04). ETA=8:37:01, max mem: 20.9 GB 
[12/02 14:11:18 visual_prompt]: 	Training 300/553. train loss: 2.2385,	0.8317 s / batch. (data: 1.74e-02). ETA=8:44:46, max mem: 20.9 GB 
[12/02 14:13:09 visual_prompt]: 	Training 400/553. train loss: 0.7085,	0.8523 s / batch. (data: 2.38e-02). ETA=8:56:19, max mem: 20.9 GB 
[12/02 14:14:58 visual_prompt]: 	Training 500/553. train loss: 0.7105,	0.8562 s / batch. (data: 2.42e-02). ETA=8:57:23, max mem: 20.9 GB 
[12/02 14:15:54 visual_prompt]: Epoch 32 / 100: avg data time: 2.89e-01, avg batch time: 1.1214, average train loss: 1.2053
[12/02 14:17:00 visual_prompt]: Inference (val):avg data time: 4.98e-05, avg batch time: 0.3112, average loss: 1.4430
[12/02 14:17:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.51	
[12/02 14:17:00 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[12/02 14:18:54 visual_prompt]: 	Training 100/553. train loss: 0.0348,	0.8216 s / batch. (data: 3.21e-04). ETA=8:33:35, max mem: 20.9 GB 
[12/02 14:20:50 visual_prompt]: 	Training 200/553. train loss: 2.4286,	2.1520 s / batch. (data: 1.32e+00). ETA=22:21:33, max mem: 20.9 GB 
[12/02 14:22:41 visual_prompt]: 	Training 300/553. train loss: 0.5720,	0.8391 s / batch. (data: 4.12e-03). ETA=8:41:42, max mem: 20.9 GB 
[12/02 14:24:34 visual_prompt]: 	Training 400/553. train loss: 5.5935,	0.8606 s / batch. (data: 8.44e-03). ETA=8:53:37, max mem: 20.9 GB 
[12/02 14:26:26 visual_prompt]: 	Training 500/553. train loss: 0.5585,	1.2776 s / batch. (data: 4.51e-01). ETA=13:10:02, max mem: 20.9 GB 
[12/02 14:27:24 visual_prompt]: Epoch 33 / 100: avg data time: 2.95e-01, avg batch time: 1.1286, average train loss: 1.2378
[12/02 14:28:29 visual_prompt]: Inference (val):avg data time: 2.27e-04, avg batch time: 0.3093, average loss: 1.3795
[12/02 14:28:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.65	
[12/02 14:28:29 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[12/02 14:30:28 visual_prompt]: 	Training 100/553. train loss: 0.7157,	0.8454 s / batch. (data: 5.20e-04). ETA=8:40:36, max mem: 20.9 GB 
[12/02 14:32:17 visual_prompt]: 	Training 200/553. train loss: 1.0588,	0.8525 s / batch. (data: 3.25e-04). ETA=8:43:33, max mem: 20.9 GB 
[12/02 14:34:07 visual_prompt]: 	Training 300/553. train loss: 11.3398,	0.8513 s / batch. (data: 7.95e-03). ETA=8:41:26, max mem: 20.9 GB 
[12/02 14:36:00 visual_prompt]: 	Training 400/553. train loss: 1.2482,	0.8418 s / batch. (data: 3.19e-04). ETA=8:34:11, max mem: 20.9 GB 
[12/02 14:37:52 visual_prompt]: 	Training 500/553. train loss: 0.5747,	1.8560 s / batch. (data: 1.04e+00). ETA=18:50:39, max mem: 20.9 GB 
[12/02 14:38:49 visual_prompt]: Epoch 34 / 100: avg data time: 2.88e-01, avg batch time: 1.1204, average train loss: 1.2830
[12/02 14:39:55 visual_prompt]: Inference (val):avg data time: 5.14e-05, avg batch time: 0.3099, average loss: 0.9455
[12/02 14:39:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.51	
[12/02 14:39:55 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[12/02 14:41:54 visual_prompt]: 	Training 100/553. train loss: 0.3765,	0.8530 s / batch. (data: 8.92e-03). ETA=8:37:26, max mem: 20.9 GB 
[12/02 14:43:46 visual_prompt]: 	Training 200/553. train loss: 0.7583,	0.8320 s / batch. (data: 3.37e-04). ETA=8:23:19, max mem: 20.9 GB 
[12/02 14:45:36 visual_prompt]: 	Training 300/553. train loss: 0.8936,	0.8400 s / batch. (data: 1.20e-02). ETA=8:26:46, max mem: 20.9 GB 
[12/02 14:47:26 visual_prompt]: 	Training 400/553. train loss: 1.9171,	1.0598 s / batch. (data: 2.30e-01). ETA=10:37:38, max mem: 20.9 GB 
[12/02 14:49:17 visual_prompt]: 	Training 500/553. train loss: 0.7005,	1.5041 s / batch. (data: 6.66e-01). ETA=15:02:23, max mem: 20.9 GB 
[12/02 14:50:15 visual_prompt]: Epoch 35 / 100: avg data time: 2.88e-01, avg batch time: 1.1208, average train loss: 1.1540
[12/02 14:51:20 visual_prompt]: Inference (val):avg data time: 5.54e-05, avg batch time: 0.3111, average loss: 0.8986
[12/02 14:51:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.91	
[12/02 14:51:20 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[12/02 14:53:16 visual_prompt]: 	Training 100/553. train loss: 0.5873,	0.8400 s / batch. (data: 3.33e-04). ETA=8:21:49, max mem: 20.9 GB 
[12/02 14:55:08 visual_prompt]: 	Training 200/553. train loss: 3.1401,	0.8174 s / batch. (data: 3.14e-04). ETA=8:06:58, max mem: 20.9 GB 
[12/02 14:57:03 visual_prompt]: 	Training 300/553. train loss: 0.9643,	0.8493 s / batch. (data: 5.30e-03). ETA=8:24:31, max mem: 20.9 GB 
[12/02 14:58:53 visual_prompt]: 	Training 400/553. train loss: 0.8377,	0.8379 s / batch. (data: 3.35e-04). ETA=8:16:24, max mem: 20.9 GB 
[12/02 15:00:46 visual_prompt]: 	Training 500/553. train loss: 0.7386,	1.3444 s / batch. (data: 5.28e-01). ETA=13:14:13, max mem: 20.9 GB 
[12/02 15:01:41 visual_prompt]: Epoch 36 / 100: avg data time: 2.91e-01, avg batch time: 1.1234, average train loss: 1.4066
[12/02 15:02:46 visual_prompt]: Inference (val):avg data time: 4.80e-05, avg batch time: 0.3102, average loss: 1.8359
[12/02 15:02:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 39.97	
[12/02 15:02:46 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[12/02 15:04:43 visual_prompt]: 	Training 100/553. train loss: 0.7353,	0.8162 s / batch. (data: 3.89e-04). ETA=8:00:05, max mem: 20.9 GB 
[12/02 15:06:34 visual_prompt]: 	Training 200/553. train loss: 0.7840,	0.8360 s / batch. (data: 3.37e-04). ETA=8:10:20, max mem: 20.9 GB 
[12/02 15:08:26 visual_prompt]: 	Training 300/553. train loss: 2.6184,	1.9912 s / batch. (data: 1.17e+00). ETA=19:24:34, max mem: 20.9 GB 
[12/02 15:10:20 visual_prompt]: 	Training 400/553. train loss: 0.8362,	2.2999 s / batch. (data: 1.48e+00). ETA=22:21:19, max mem: 20.9 GB 
[12/02 15:12:07 visual_prompt]: 	Training 500/553. train loss: 0.6691,	1.4920 s / batch. (data: 6.72e-01). ETA=14:27:39, max mem: 20.9 GB 
[12/02 15:13:06 visual_prompt]: Epoch 37 / 100: avg data time: 2.89e-01, avg batch time: 1.1218, average train loss: 1.3115
[12/02 15:14:11 visual_prompt]: Inference (val):avg data time: 5.02e-05, avg batch time: 0.3097, average loss: 0.8791
[12/02 15:14:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.98	
[12/02 15:14:11 visual_prompt]: Stopping early.
[12/02 15:14:11 visual_prompt]: Rank of current process: 0. World size: 1
[12/02 15:14:11 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/02 15:14:11 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/02 15:14:11 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/02 15:14:11 visual_prompt]: Training with config:
[12/02 15:14:11 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.5_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/02 15:14:11 visual_prompt]: Loading training data...
[12/02 15:14:11 visual_prompt]: Constructing mammo-cbis dataset train...
[12/02 15:14:11 visual_prompt]: Loading validation data...
[12/02 15:14:11 visual_prompt]: Constructing mammo-cbis dataset val...
[12/02 15:14:11 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[12/02 15:14:15 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[12/02 15:14:15 visual_prompt]: tuned percent:0.525
[12/02 15:14:15 visual_prompt]: Device used for model: 0
[12/02 15:14:15 visual_prompt]: Setting up Evaluator...
[12/02 15:14:15 visual_prompt]: Setting up Trainer...
[12/02 15:14:15 visual_prompt]: 	Setting up the optimizer...
[12/02 15:14:15 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/02 15:16:11 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8521 s / batch. (data: 3.19e-04). ETA=13:03:56, max mem: 20.9 GB 
[12/02 15:18:01 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8205 s / batch. (data: 3.21e-04). ETA=12:33:31, max mem: 20.9 GB 
[12/02 15:20:13 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.9103 s / batch. (data: 2.09e+00). ETA=1 day, 20:27:48, max mem: 20.9 GB 
[12/02 15:22:20 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8265 s / batch. (data: 4.97e-04). ETA=12:36:16, max mem: 20.9 GB 
[12/02 15:24:32 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8420 s / batch. (data: 3.18e-04). ETA=12:49:01, max mem: 20.9 GB 
[12/02 15:25:39 visual_prompt]: Epoch 1 / 100: avg data time: 4.09e-01, avg batch time: 1.2379, average train loss: 1.5403
[12/02 15:26:55 visual_prompt]: Inference (val):avg data time: 5.20e-05, avg batch time: 0.3066, average loss: 1.5201
[12/02 15:26:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[12/02 15:26:55 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[12/02 15:28:57 visual_prompt]: 	Training 100/553. train loss: 0.7742,	0.8143 s / batch. (data: 2.98e-04). ETA=12:21:41, max mem: 20.9 GB 
[12/02 15:30:57 visual_prompt]: 	Training 200/553. train loss: 0.1321,	0.8639 s / batch. (data: 5.71e-04). ETA=13:05:21, max mem: 20.9 GB 
[12/02 15:33:02 visual_prompt]: 	Training 300/553. train loss: 0.9265,	1.7681 s / batch. (data: 9.44e-01). ETA=1 day, 2:44:26, max mem: 20.9 GB 
[12/02 15:35:05 visual_prompt]: 	Training 400/553. train loss: 1.5258,	0.8304 s / batch. (data: 4.08e-04). ETA=12:32:08, max mem: 20.9 GB 
[12/02 15:37:09 visual_prompt]: 	Training 500/553. train loss: 0.5747,	0.8320 s / batch. (data: 1.23e-03). ETA=12:32:12, max mem: 20.9 GB 
[12/02 15:38:13 visual_prompt]: Epoch 2 / 100: avg data time: 3.96e-01, avg batch time: 1.2243, average train loss: 0.9021
[12/02 15:39:23 visual_prompt]: Inference (val):avg data time: 6.05e-05, avg batch time: 0.3092, average loss: 1.0807
[12/02 15:39:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.77	
[12/02 15:39:23 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[12/02 15:41:23 visual_prompt]: 	Training 100/553. train loss: 0.7503,	1.6029 s / batch. (data: 7.76e-01). ETA=1 day, 0:05:08, max mem: 20.9 GB 
[12/02 15:43:21 visual_prompt]: 	Training 200/553. train loss: 0.7792,	3.3013 s / batch. (data: 2.48e+00). ETA=2 days, 1:30:52, max mem: 20.9 GB 
[12/02 15:45:14 visual_prompt]: 	Training 300/553. train loss: 0.5905,	0.8427 s / batch. (data: 8.50e-04). ETA=12:36:55, max mem: 20.9 GB 
[12/02 15:47:09 visual_prompt]: 	Training 400/553. train loss: 1.6339,	0.8267 s / batch. (data: 4.68e-04). ETA=12:21:09, max mem: 20.9 GB 
[12/02 15:49:04 visual_prompt]: 	Training 500/553. train loss: 0.8624,	1.5080 s / batch. (data: 6.79e-01). ETA=22:29:30, max mem: 20.9 GB 
[12/02 15:50:06 visual_prompt]: Epoch 3 / 100: avg data time: 3.31e-01, avg batch time: 1.1625, average train loss: 0.8498
[12/02 15:51:11 visual_prompt]: Inference (val):avg data time: 6.59e-05, avg batch time: 0.3098, average loss: 0.7327
[12/02 15:51:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.58	
[12/02 15:51:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[12/02 15:53:11 visual_prompt]: 	Training 100/553. train loss: 0.8313,	0.8354 s / batch. (data: 5.46e-03). ETA=12:25:25, max mem: 20.9 GB 
[12/02 15:55:03 visual_prompt]: 	Training 200/553. train loss: 1.4157,	0.8313 s / batch. (data: 6.43e-03). ETA=12:20:26, max mem: 20.9 GB 
[12/02 15:56:56 visual_prompt]: 	Training 300/553. train loss: 0.7560,	1.4119 s / batch. (data: 5.79e-01). ETA=20:55:13, max mem: 20.9 GB 
[12/02 15:58:44 visual_prompt]: 	Training 400/553. train loss: 0.6816,	1.7264 s / batch. (data: 8.90e-01). ETA=1 day, 1:31:57, max mem: 20.9 GB 
[12/02 16:00:39 visual_prompt]: 	Training 500/553. train loss: 0.6022,	4.2220 s / batch. (data: 3.40e+00). ETA=2 days, 14:19:22, max mem: 20.9 GB 
[12/02 16:01:38 visual_prompt]: Epoch 4 / 100: avg data time: 3.00e-01, avg batch time: 1.1335, average train loss: 0.9386
[12/02 16:02:43 visual_prompt]: Inference (val):avg data time: 5.59e-05, avg batch time: 0.3085, average loss: 1.0216
[12/02 16:02:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.34	
[12/02 16:02:43 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[12/02 16:04:39 visual_prompt]: 	Training 100/553. train loss: 0.6818,	0.8356 s / batch. (data: 3.02e-04). ETA=12:17:55, max mem: 20.9 GB 
[12/02 16:06:32 visual_prompt]: 	Training 200/553. train loss: 0.6144,	1.4772 s / batch. (data: 6.62e-01). ETA=21:42:04, max mem: 20.9 GB 
[12/02 16:08:25 visual_prompt]: 	Training 300/553. train loss: 1.4252,	0.8510 s / batch. (data: 2.09e-02). ETA=12:28:43, max mem: 20.9 GB 
[12/02 16:10:17 visual_prompt]: 	Training 400/553. train loss: 1.0239,	0.8399 s / batch. (data: 9.04e-04). ETA=12:17:33, max mem: 20.9 GB 
[12/02 16:12:08 visual_prompt]: 	Training 500/553. train loss: 0.5849,	0.8320 s / batch. (data: 3.60e-04). ETA=12:09:12, max mem: 20.9 GB 
[12/02 16:13:07 visual_prompt]: Epoch 5 / 100: avg data time: 2.94e-01, avg batch time: 1.1266, average train loss: 0.8772
[12/02 16:14:11 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.3095, average loss: 1.6199
[12/02 16:14:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.20	
[12/02 16:14:11 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[12/02 16:16:08 visual_prompt]: 	Training 100/553. train loss: 0.5677,	0.8573 s / batch. (data: 5.97e-03). ETA=12:29:14, max mem: 20.9 GB 
[12/02 16:17:59 visual_prompt]: 	Training 200/553. train loss: 0.9987,	0.8412 s / batch. (data: 9.08e-03). ETA=12:13:46, max mem: 20.9 GB 
[12/02 16:19:49 visual_prompt]: 	Training 300/553. train loss: 0.5979,	0.8320 s / batch. (data: 3.39e-04). ETA=12:04:19, max mem: 20.9 GB 
[12/02 16:21:46 visual_prompt]: 	Training 400/553. train loss: 0.6772,	0.8352 s / batch. (data: 7.84e-03). ETA=12:05:42, max mem: 20.9 GB 
[12/02 16:23:36 visual_prompt]: 	Training 500/553. train loss: 0.9436,	1.0210 s / batch. (data: 2.03e-01). ETA=14:45:30, max mem: 20.9 GB 
[12/02 16:24:34 visual_prompt]: Epoch 6 / 100: avg data time: 2.92e-01, avg batch time: 1.1262, average train loss: 0.9098
[12/02 16:25:39 visual_prompt]: Inference (val):avg data time: 7.83e-05, avg batch time: 0.3112, average loss: 1.0490
[12/02 16:25:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.71	
[12/02 16:25:39 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[12/02 16:27:33 visual_prompt]: 	Training 100/553. train loss: 2.2867,	0.8404 s / batch. (data: 3.45e-04). ETA=12:06:43, max mem: 20.9 GB 
[12/02 16:29:25 visual_prompt]: 	Training 200/553. train loss: 0.5891,	1.4520 s / batch. (data: 6.13e-01). ETA=20:53:07, max mem: 20.9 GB 
[12/02 16:31:21 visual_prompt]: 	Training 300/553. train loss: 0.5617,	2.4249 s / batch. (data: 1.58e+00). ETA=1 day, 10:48:44, max mem: 20.9 GB 
[12/02 16:33:13 visual_prompt]: 	Training 400/553. train loss: 0.5960,	2.3858 s / batch. (data: 1.54e+00). ETA=1 day, 10:11:04, max mem: 20.9 GB 
[12/02 16:35:04 visual_prompt]: 	Training 500/553. train loss: 1.2674,	0.8275 s / batch. (data: 3.56e-04). ETA=11:49:59, max mem: 20.9 GB 
[12/02 16:36:00 visual_prompt]: Epoch 7 / 100: avg data time: 2.91e-01, avg batch time: 1.1237, average train loss: 0.9300
[12/02 16:37:04 visual_prompt]: Inference (val):avg data time: 4.49e-05, avg batch time: 0.3099, average loss: 0.7171
[12/02 16:37:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.19	
[12/02 16:37:04 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[12/02 16:38:59 visual_prompt]: 	Training 100/553. train loss: 0.7912,	0.8453 s / batch. (data: 1.05e-02). ETA=12:03:10, max mem: 20.9 GB 
[12/02 16:40:53 visual_prompt]: 	Training 200/553. train loss: 0.7433,	0.8312 s / batch. (data: 5.64e-03). ETA=11:49:41, max mem: 20.9 GB 
[12/02 16:42:45 visual_prompt]: 	Training 300/553. train loss: 0.7953,	0.8574 s / batch. (data: 5.48e-03). ETA=12:10:39, max mem: 20.9 GB 
[12/02 16:44:38 visual_prompt]: 	Training 400/553. train loss: 0.7159,	0.8556 s / batch. (data: 8.34e-04). ETA=12:07:42, max mem: 20.9 GB 
[12/02 16:46:32 visual_prompt]: 	Training 500/553. train loss: 1.2213,	1.9460 s / batch. (data: 1.13e+00). ETA=1 day, 3:31:47, max mem: 20.9 GB 
[12/02 16:47:29 visual_prompt]: Epoch 8 / 100: avg data time: 2.96e-01, avg batch time: 1.1292, average train loss: 0.9477
[12/02 16:48:34 visual_prompt]: Inference (val):avg data time: 4.62e-05, avg batch time: 0.3098, average loss: 1.1595
[12/02 16:48:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.49	
[12/02 16:48:34 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[12/02 16:50:30 visual_prompt]: 	Training 100/553. train loss: 0.1267,	0.8297 s / batch. (data: 5.44e-03). ETA=11:42:08, max mem: 20.9 GB 
[12/02 16:52:22 visual_prompt]: 	Training 200/553. train loss: 0.7180,	0.8411 s / batch. (data: 3.18e-04). ETA=11:50:22, max mem: 20.9 GB 
[12/02 16:54:13 visual_prompt]: 	Training 300/553. train loss: 0.5589,	2.0708 s / batch. (data: 1.24e+00). ETA=1 day, 5:05:34, max mem: 20.9 GB 
[12/02 16:56:06 visual_prompt]: 	Training 400/553. train loss: 0.6192,	0.8438 s / batch. (data: 1.25e-03). ETA=11:49:51, max mem: 20.9 GB 
[12/02 16:57:59 visual_prompt]: 	Training 500/553. train loss: 0.6840,	1.3641 s / batch. (data: 5.17e-01). ETA=19:05:16, max mem: 20.9 GB 
[12/02 16:58:54 visual_prompt]: Epoch 9 / 100: avg data time: 2.88e-01, avg batch time: 1.1212, average train loss: 0.9406
[12/02 16:59:59 visual_prompt]: Inference (val):avg data time: 6.60e-05, avg batch time: 0.3088, average loss: 0.6886
[12/02 16:59:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.76	
[12/02 16:59:59 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[12/02 17:01:58 visual_prompt]: 	Training 100/553. train loss: 1.4953,	0.8151 s / batch. (data: 3.67e-04). ETA=11:22:14, max mem: 20.9 GB 
[12/02 17:03:49 visual_prompt]: 	Training 200/553. train loss: 0.7161,	0.8322 s / batch. (data: 3.25e-04). ETA=11:35:14, max mem: 20.9 GB 
[12/02 17:05:40 visual_prompt]: 	Training 300/553. train loss: 0.5708,	1.1406 s / batch. (data: 3.26e-01). ETA=15:50:56, max mem: 20.9 GB 
[12/02 17:07:30 visual_prompt]: 	Training 400/553. train loss: 0.8661,	1.2870 s / batch. (data: 4.70e-01). ETA=17:50:48, max mem: 20.9 GB 
[12/02 17:09:23 visual_prompt]: 	Training 500/553. train loss: 0.5701,	1.0279 s / batch. (data: 1.99e-01). ETA=14:13:35, max mem: 20.9 GB 
[12/02 17:10:22 visual_prompt]: Epoch 10 / 100: avg data time: 2.92e-01, avg batch time: 1.1247, average train loss: 1.1865
[12/02 17:11:27 visual_prompt]: Inference (val):avg data time: 5.66e-05, avg batch time: 0.3115, average loss: 0.6896
[12/02 17:11:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.88	
[12/02 17:11:27 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[12/02 17:13:25 visual_prompt]: 	Training 100/553. train loss: 1.0289,	0.8374 s / batch. (data: 2.99e-04). ETA=11:33:14, max mem: 20.9 GB 
[12/02 17:15:18 visual_prompt]: 	Training 200/553. train loss: 1.3758,	0.8562 s / batch. (data: 3.55e-04). ETA=11:47:19, max mem: 20.9 GB 
[12/02 17:17:10 visual_prompt]: 	Training 300/553. train loss: 0.1677,	2.7465 s / batch. (data: 1.93e+00). ETA=1 day, 13:44:29, max mem: 20.9 GB 
[12/02 17:19:00 visual_prompt]: 	Training 400/553. train loss: 0.6386,	0.8368 s / batch. (data: 2.74e-03). ETA=11:28:30, max mem: 20.9 GB 
[12/02 17:20:50 visual_prompt]: 	Training 500/553. train loss: 0.8986,	0.8747 s / batch. (data: 3.47e-02). ETA=11:58:15, max mem: 20.9 GB 
[12/02 17:21:47 visual_prompt]: Epoch 11 / 100: avg data time: 2.87e-01, avg batch time: 1.1207, average train loss: 0.9486
[12/02 17:22:52 visual_prompt]: Inference (val):avg data time: 3.76e-04, avg batch time: 0.3099, average loss: 0.7804
[12/02 17:22:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.26	
[12/02 17:22:52 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[12/02 17:24:50 visual_prompt]: 	Training 100/553. train loss: 0.8570,	0.8163 s / batch. (data: 3.47e-04). ETA=11:08:14, max mem: 20.9 GB 
[12/02 17:26:42 visual_prompt]: 	Training 200/553. train loss: 0.5702,	1.6315 s / batch. (data: 8.18e-01). ETA=22:12:53, max mem: 20.9 GB 
[12/02 17:28:32 visual_prompt]: 	Training 300/553. train loss: 1.0247,	0.8511 s / batch. (data: 1.19e-02). ETA=11:33:53, max mem: 20.9 GB 
[12/02 17:30:23 visual_prompt]: 	Training 400/553. train loss: 0.7868,	0.8237 s / batch. (data: 1.26e-03). ETA=11:10:12, max mem: 20.9 GB 
[12/02 17:32:15 visual_prompt]: 	Training 500/553. train loss: 3.7251,	0.8206 s / batch. (data: 3.08e-04). ETA=11:06:18, max mem: 20.9 GB 
[12/02 17:33:12 visual_prompt]: Epoch 12 / 100: avg data time: 2.88e-01, avg batch time: 1.1203, average train loss: 1.0984
[12/02 17:34:17 visual_prompt]: Inference (val):avg data time: 4.44e-05, avg batch time: 0.3100, average loss: 1.8046
[12/02 17:34:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.26	
[12/02 17:34:17 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[12/02 17:36:15 visual_prompt]: 	Training 100/553. train loss: 0.6844,	0.8371 s / batch. (data: 3.58e-04). ETA=11:17:34, max mem: 20.9 GB 
[12/02 17:38:04 visual_prompt]: 	Training 200/553. train loss: 0.7033,	0.8440 s / batch. (data: 9.46e-04). ETA=11:21:43, max mem: 20.9 GB 
[12/02 17:39:57 visual_prompt]: 	Training 300/553. train loss: 0.9773,	2.2639 s / batch. (data: 1.43e+00). ETA=1 day, 6:24:49, max mem: 20.9 GB 
[12/02 17:41:46 visual_prompt]: 	Training 400/553. train loss: 3.7853,	0.8400 s / batch. (data: 7.95e-03). ETA=11:15:41, max mem: 20.9 GB 
[12/02 17:43:38 visual_prompt]: 	Training 500/553. train loss: 1.1160,	0.8478 s / batch. (data: 1.18e-02). ETA=11:20:31, max mem: 20.9 GB 
[12/02 17:44:36 visual_prompt]: Epoch 13 / 100: avg data time: 2.87e-01, avg batch time: 1.1197, average train loss: 1.2169
[12/02 17:45:41 visual_prompt]: Inference (val):avg data time: 4.95e-05, avg batch time: 0.3112, average loss: 0.9227
[12/02 17:45:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.10	
[12/02 17:45:41 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[12/02 17:47:39 visual_prompt]: 	Training 100/553. train loss: 0.9061,	0.8931 s / batch. (data: 2.15e-02). ETA=11:54:37, max mem: 20.9 GB 
[12/02 17:49:33 visual_prompt]: 	Training 200/553. train loss: 0.0361,	1.9590 s / batch. (data: 1.13e+00). ETA=1 day, 2:04:15, max mem: 20.9 GB 
[12/02 17:51:23 visual_prompt]: 	Training 300/553. train loss: 0.6723,	1.1003 s / batch. (data: 2.49e-01). ETA=14:36:45, max mem: 20.9 GB 
[12/02 17:53:14 visual_prompt]: 	Training 400/553. train loss: 0.6289,	0.8171 s / batch. (data: 3.38e-04). ETA=10:49:44, max mem: 20.9 GB 
[12/02 17:55:05 visual_prompt]: 	Training 500/553. train loss: 1.1140,	0.8240 s / batch. (data: 3.34e-04). ETA=10:53:51, max mem: 20.9 GB 
[12/02 17:56:01 visual_prompt]: Epoch 14 / 100: avg data time: 2.88e-01, avg batch time: 1.1214, average train loss: 1.2065
[12/02 17:57:06 visual_prompt]: Inference (val):avg data time: 5.32e-05, avg batch time: 0.3097, average loss: 0.6908
[12/02 17:57:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.98	
[12/02 17:57:06 visual_prompt]: Best epoch 14: best metric: -0.691
[12/02 17:57:06 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[12/02 17:59:02 visual_prompt]: 	Training 100/553. train loss: 0.8596,	1.3065 s / batch. (data: 4.88e-01). ETA=17:13:22, max mem: 20.9 GB 
[12/02 18:00:52 visual_prompt]: 	Training 200/553. train loss: 4.4659,	0.8320 s / batch. (data: 4.90e-04). ETA=10:56:39, max mem: 20.9 GB 
[12/02 18:02:46 visual_prompt]: 	Training 300/553. train loss: 0.6629,	0.8506 s / batch. (data: 1.05e-02). ETA=11:09:58, max mem: 20.9 GB 
[12/02 18:04:34 visual_prompt]: 	Training 400/553. train loss: 1.2398,	0.8414 s / batch. (data: 4.78e-04). ETA=11:01:17, max mem: 20.9 GB 
[12/02 18:06:27 visual_prompt]: 	Training 500/553. train loss: 0.5912,	0.8400 s / batch. (data: 3.45e-04). ETA=10:58:50, max mem: 20.9 GB 
[12/02 18:07:26 visual_prompt]: Epoch 15 / 100: avg data time: 2.87e-01, avg batch time: 1.1195, average train loss: 1.2034
[12/02 18:08:30 visual_prompt]: Inference (val):avg data time: 4.41e-05, avg batch time: 0.3080, average loss: 1.0720
[12/02 18:08:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.47	
[12/02 18:08:30 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[12/02 18:10:25 visual_prompt]: 	Training 100/553. train loss: 0.7801,	0.8320 s / batch. (data: 1.19e-02). ETA=10:50:25, max mem: 20.9 GB 
[12/02 18:12:17 visual_prompt]: 	Training 200/553. train loss: 1.8266,	0.8252 s / batch. (data: 3.15e-04). ETA=10:43:42, max mem: 20.9 GB 
[12/02 18:14:10 visual_prompt]: 	Training 300/553. train loss: 1.1724,	0.8443 s / batch. (data: 7.94e-03). ETA=10:57:12, max mem: 20.9 GB 
[12/02 18:16:01 visual_prompt]: 	Training 400/553. train loss: 0.9592,	0.8320 s / batch. (data: 3.40e-04). ETA=10:46:14, max mem: 20.9 GB 
[12/02 18:17:52 visual_prompt]: 	Training 500/553. train loss: 0.6301,	1.9635 s / batch. (data: 1.15e+00). ETA=1 day, 1:21:51, max mem: 20.9 GB 
[12/02 18:18:51 visual_prompt]: Epoch 16 / 100: avg data time: 2.90e-01, avg batch time: 1.1224, average train loss: 0.9768
[12/02 18:19:55 visual_prompt]: Inference (val):avg data time: 4.79e-05, avg batch time: 0.3087, average loss: 0.8237
[12/02 18:19:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.02	
[12/02 18:19:55 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[12/02 18:21:50 visual_prompt]: 	Training 100/553. train loss: 0.9535,	0.8492 s / batch. (data: 2.11e-02). ETA=10:56:03, max mem: 20.9 GB 
[12/02 18:23:43 visual_prompt]: 	Training 200/553. train loss: 2.9774,	0.8417 s / batch. (data: 3.29e-04). ETA=10:48:51, max mem: 20.9 GB 
[12/02 18:25:34 visual_prompt]: 	Training 300/553. train loss: 1.5363,	0.8323 s / batch. (data: 3.40e-04). ETA=10:40:14, max mem: 20.9 GB 
[12/02 18:27:26 visual_prompt]: 	Training 400/553. train loss: 0.6225,	1.5598 s / batch. (data: 6.96e-01). ETA=19:57:12, max mem: 20.9 GB 
[12/02 18:29:18 visual_prompt]: 	Training 500/553. train loss: 1.1883,	1.8240 s / batch. (data: 9.86e-01). ETA=23:16:56, max mem: 20.9 GB 
[12/02 18:30:17 visual_prompt]: Epoch 17 / 100: avg data time: 2.91e-01, avg batch time: 1.1241, average train loss: 1.1318
[12/02 18:31:23 visual_prompt]: Inference (val):avg data time: 4.38e-04, avg batch time: 0.3112, average loss: 0.8863
[12/02 18:31:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.78	
[12/02 18:31:23 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[12/02 18:33:20 visual_prompt]: 	Training 100/553. train loss: 0.7490,	0.8248 s / batch. (data: 6.00e-04). ETA=10:29:33, max mem: 20.9 GB 
[12/02 18:35:15 visual_prompt]: 	Training 200/553. train loss: 0.6182,	0.8405 s / batch. (data: 3.06e-04). ETA=10:40:10, max mem: 20.9 GB 
[12/02 18:37:08 visual_prompt]: 	Training 300/553. train loss: 0.6582,	0.8399 s / batch. (data: 7.92e-03). ETA=10:38:20, max mem: 20.9 GB 
[12/02 18:38:59 visual_prompt]: 	Training 400/553. train loss: 2.0270,	0.8348 s / batch. (data: 5.46e-03). ETA=10:33:02, max mem: 20.9 GB 
[12/02 18:40:49 visual_prompt]: 	Training 500/553. train loss: 0.7826,	0.8365 s / batch. (data: 2.38e-03). ETA=10:32:54, max mem: 20.9 GB 
[12/02 18:41:45 visual_prompt]: Epoch 18 / 100: avg data time: 2.93e-01, avg batch time: 1.1259, average train loss: 1.3795
[12/02 18:42:50 visual_prompt]: Inference (val):avg data time: 4.91e-05, avg batch time: 0.3094, average loss: 0.6943
[12/02 18:42:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.74	
[12/02 18:42:50 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[12/02 18:44:47 visual_prompt]: 	Training 100/553. train loss: 0.6935,	0.8284 s / batch. (data: 3.40e-04). ETA=10:24:39, max mem: 20.9 GB 
[12/02 18:46:39 visual_prompt]: 	Training 200/553. train loss: 0.7440,	0.8381 s / batch. (data: 5.44e-03). ETA=10:30:36, max mem: 20.9 GB 
[12/02 18:48:31 visual_prompt]: 	Training 300/553. train loss: 0.3663,	1.1436 s / batch. (data: 3.14e-01). ETA=14:18:35, max mem: 20.9 GB 
[12/02 18:50:25 visual_prompt]: 	Training 400/553. train loss: 0.6592,	0.8600 s / batch. (data: 3.11e-04). ETA=10:44:14, max mem: 20.9 GB 
[12/02 18:52:13 visual_prompt]: 	Training 500/553. train loss: 0.8467,	0.8291 s / batch. (data: 5.50e-03). ETA=10:19:43, max mem: 20.9 GB 
[12/02 18:53:11 visual_prompt]: Epoch 19 / 100: avg data time: 2.90e-01, avg batch time: 1.1226, average train loss: 1.1908
[12/02 18:54:16 visual_prompt]: Inference (val):avg data time: 5.41e-05, avg batch time: 0.3105, average loss: 4.5945
[12/02 18:54:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.63	
[12/02 18:54:16 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[12/02 18:56:10 visual_prompt]: 	Training 100/553. train loss: 0.6789,	0.8546 s / batch. (data: 2.66e-02). ETA=10:36:36, max mem: 20.9 GB 
[12/02 18:58:04 visual_prompt]: 	Training 200/553. train loss: 0.6506,	0.8393 s / batch. (data: 3.23e-04). ETA=10:23:46, max mem: 20.9 GB 
[12/02 18:59:56 visual_prompt]: 	Training 300/553. train loss: 0.6597,	0.8533 s / batch. (data: 1.05e-02). ETA=10:32:46, max mem: 20.9 GB 
[12/02 19:01:48 visual_prompt]: 	Training 400/553. train loss: 0.7239,	0.8359 s / batch. (data: 1.20e-03). ETA=10:18:30, max mem: 20.9 GB 
[12/02 19:03:38 visual_prompt]: 	Training 500/553. train loss: 0.7731,	0.8757 s / batch. (data: 3.05e-04). ETA=10:46:27, max mem: 20.9 GB 
[12/02 19:04:37 visual_prompt]: Epoch 20 / 100: avg data time: 2.90e-01, avg batch time: 1.1231, average train loss: 1.4464
[12/02 19:05:42 visual_prompt]: Inference (val):avg data time: 5.41e-05, avg batch time: 0.3094, average loss: 0.8541
[12/02 19:05:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.39	
[12/02 19:05:42 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[12/02 19:07:40 visual_prompt]: 	Training 100/553. train loss: 1.5329,	0.8340 s / batch. (data: 3.35e-04). ETA=10:13:31, max mem: 20.9 GB 
[12/02 19:09:31 visual_prompt]: 	Training 200/553. train loss: 0.8960,	0.8323 s / batch. (data: 5.28e-04). ETA=10:10:55, max mem: 20.9 GB 
[12/02 19:11:22 visual_prompt]: 	Training 300/553. train loss: 3.4251,	1.1840 s / batch. (data: 3.50e-01). ETA=14:27:04, max mem: 20.9 GB 
[12/02 19:13:14 visual_prompt]: 	Training 400/553. train loss: 1.6836,	0.8480 s / batch. (data: 1.10e-03). ETA=10:19:36, max mem: 20.9 GB 
[12/02 19:15:06 visual_prompt]: 	Training 500/553. train loss: 0.7203,	0.8264 s / batch. (data: 3.16e-04). ETA=10:02:24, max mem: 20.9 GB 
[12/02 19:16:03 visual_prompt]: Epoch 21 / 100: avg data time: 2.90e-01, avg batch time: 1.1221, average train loss: 1.2872
[12/02 19:17:07 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.3089, average loss: 0.8163
[12/02 19:17:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.15	
[12/02 19:17:07 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[12/02 19:19:03 visual_prompt]: 	Training 100/553. train loss: 1.2997,	0.8323 s / batch. (data: 2.62e-04). ETA=10:04:39, max mem: 20.9 GB 
[12/02 19:20:54 visual_prompt]: 	Training 200/553. train loss: 0.6311,	0.8363 s / batch. (data: 3.47e-04). ETA=10:06:08, max mem: 20.9 GB 
[12/02 19:22:43 visual_prompt]: 	Training 300/553. train loss: 0.2748,	0.8175 s / batch. (data: 4.39e-04). ETA=9:51:06, max mem: 20.9 GB 
[12/02 19:24:36 visual_prompt]: 	Training 400/553. train loss: 0.8418,	0.8360 s / batch. (data: 3.96e-04). ETA=10:03:06, max mem: 20.9 GB 
[12/02 19:26:27 visual_prompt]: 	Training 500/553. train loss: 0.5765,	0.8440 s / batch. (data: 3.54e-04). ETA=10:07:29, max mem: 20.9 GB 
[12/02 19:27:27 visual_prompt]: Epoch 22 / 100: avg data time: 2.88e-01, avg batch time: 1.1214, average train loss: 1.2951
[12/02 19:28:32 visual_prompt]: Inference (val):avg data time: 4.58e-05, avg batch time: 0.3088, average loss: 0.6884
[12/02 19:28:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.37	
[12/02 19:28:32 visual_prompt]: Best epoch 22: best metric: -0.688
[12/02 19:28:32 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[12/02 19:30:29 visual_prompt]: 	Training 100/553. train loss: 1.0762,	0.8560 s / batch. (data: 7.94e-03). ETA=10:13:56, max mem: 20.9 GB 
[12/02 19:32:21 visual_prompt]: 	Training 200/553. train loss: 2.0692,	0.8271 s / batch. (data: 2.76e-04). ETA=9:51:50, max mem: 20.9 GB 
[12/02 19:34:14 visual_prompt]: 	Training 300/553. train loss: 0.6199,	0.8479 s / batch. (data: 6.38e-03). ETA=10:05:17, max mem: 20.9 GB 
[12/02 19:36:04 visual_prompt]: 	Training 400/553. train loss: 0.7393,	0.8220 s / batch. (data: 3.45e-04). ETA=9:45:26, max mem: 20.9 GB 
[12/02 19:37:53 visual_prompt]: 	Training 500/553. train loss: 0.1916,	0.8545 s / batch. (data: 2.45e-02). ETA=10:07:10, max mem: 20.9 GB 
[12/02 19:38:52 visual_prompt]: Epoch 23 / 100: avg data time: 2.87e-01, avg batch time: 1.1208, average train loss: 1.0443
[12/02 19:39:56 visual_prompt]: Inference (val):avg data time: 4.91e-05, avg batch time: 0.3089, average loss: 1.1503
[12/02 19:39:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.70	
[12/02 19:39:56 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[12/02 19:41:49 visual_prompt]: 	Training 100/553. train loss: 1.8344,	0.8320 s / batch. (data: 2.95e-04). ETA=9:49:04, max mem: 20.9 GB 
[12/02 19:43:40 visual_prompt]: 	Training 200/553. train loss: 1.3425,	0.8237 s / batch. (data: 3.96e-04). ETA=9:41:50, max mem: 20.9 GB 
[12/02 19:45:32 visual_prompt]: 	Training 300/553. train loss: 0.7916,	1.4010 s / batch. (data: 5.82e-01). ETA=16:27:17, max mem: 20.9 GB 
[12/02 19:47:24 visual_prompt]: 	Training 400/553. train loss: 0.5746,	0.8330 s / batch. (data: 1.09e-03). ETA=9:45:38, max mem: 20.9 GB 
[12/02 19:49:17 visual_prompt]: 	Training 500/553. train loss: 1.3705,	0.8706 s / batch. (data: 2.67e-02). ETA=10:10:36, max mem: 20.9 GB 
[12/02 19:50:15 visual_prompt]: Epoch 24 / 100: avg data time: 2.86e-01, avg batch time: 1.1186, average train loss: 1.4314
[12/02 19:51:21 visual_prompt]: Inference (val):avg data time: 2.24e-04, avg batch time: 0.3091, average loss: 1.9105
[12/02 19:51:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.09	
[12/02 19:51:21 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[12/02 19:53:22 visual_prompt]: 	Training 100/553. train loss: 1.8765,	0.8363 s / batch. (data: 1.56e-02). ETA=9:44:26, max mem: 20.9 GB 
[12/02 19:55:10 visual_prompt]: 	Training 200/553. train loss: 0.6737,	0.8299 s / batch. (data: 5.67e-04). ETA=9:38:32, max mem: 20.9 GB 
[12/02 19:57:02 visual_prompt]: 	Training 300/553. train loss: 0.7955,	0.8253 s / batch. (data: 6.55e-04). ETA=9:33:59, max mem: 20.9 GB 
[12/02 19:58:54 visual_prompt]: 	Training 400/553. train loss: 0.5555,	1.6144 s / batch. (data: 7.96e-01). ETA=18:40:06, max mem: 20.9 GB 
[12/02 20:00:46 visual_prompt]: 	Training 500/553. train loss: 0.7783,	2.0264 s / batch. (data: 1.19e+00). ETA=23:22:32, max mem: 20.9 GB 
[12/02 20:01:43 visual_prompt]: Epoch 25 / 100: avg data time: 2.91e-01, avg batch time: 1.1239, average train loss: 1.1830
[12/02 20:02:48 visual_prompt]: Inference (val):avg data time: 6.01e-05, avg batch time: 0.3082, average loss: 2.2345
[12/02 20:02:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.31	
[12/02 20:02:48 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[12/02 20:04:46 visual_prompt]: 	Training 100/553. train loss: 0.6476,	0.8320 s / batch. (data: 1.30e-03). ETA=9:33:45, max mem: 20.9 GB 
[12/02 20:06:46 visual_prompt]: 	Training 200/553. train loss: 2.7143,	2.1984 s / batch. (data: 1.38e+00). ETA=1 day, 1:12:19, max mem: 20.9 GB 
[12/02 20:08:43 visual_prompt]: 	Training 300/553. train loss: 1.5424,	0.8253 s / batch. (data: 5.76e-04). ETA=9:26:21, max mem: 20.9 GB 
[12/02 20:10:34 visual_prompt]: 	Training 400/553. train loss: 2.0193,	0.8404 s / batch. (data: 1.17e-02). ETA=9:35:20, max mem: 20.9 GB 
[12/02 20:12:24 visual_prompt]: 	Training 500/553. train loss: 1.8188,	0.8398 s / batch. (data: 8.04e-04). ETA=9:33:30, max mem: 20.9 GB 
[12/02 20:13:22 visual_prompt]: Epoch 26 / 100: avg data time: 3.13e-01, avg batch time: 1.1454, average train loss: 1.1821
[12/02 20:14:28 visual_prompt]: Inference (val):avg data time: 5.39e-05, avg batch time: 0.3101, average loss: 0.6895
[12/02 20:14:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.57	
[12/02 20:14:28 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[12/02 20:16:31 visual_prompt]: 	Training 100/553. train loss: 0.5583,	0.8280 s / batch. (data: 3.04e-04). ETA=9:23:20, max mem: 20.9 GB 
[12/02 20:18:22 visual_prompt]: 	Training 200/553. train loss: 4.4085,	1.3720 s / batch. (data: 5.22e-01). ETA=15:31:10, max mem: 20.9 GB 
[12/02 20:20:15 visual_prompt]: 	Training 300/553. train loss: 1.0742,	0.8440 s / batch. (data: 3.51e-04). ETA=9:31:24, max mem: 20.9 GB 
[12/02 20:22:08 visual_prompt]: 	Training 400/553. train loss: 1.0918,	0.8515 s / batch. (data: 1.56e-02). ETA=9:35:05, max mem: 20.9 GB 
[12/02 20:24:01 visual_prompt]: 	Training 500/553. train loss: 0.8502,	0.8877 s / batch. (data: 1.57e-02). ETA=9:58:04, max mem: 20.9 GB 
[12/02 20:24:57 visual_prompt]: Epoch 27 / 100: avg data time: 3.04e-01, avg batch time: 1.1365, average train loss: 1.2770
[12/02 20:26:02 visual_prompt]: Inference (val):avg data time: 5.89e-05, avg batch time: 0.3100, average loss: 1.3220
[12/02 20:26:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.40	
[12/02 20:26:02 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[12/02 20:27:58 visual_prompt]: 	Training 100/553. train loss: 0.8539,	0.8229 s / batch. (data: 3.52e-04). ETA=9:12:17, max mem: 20.9 GB 
[12/02 20:29:53 visual_prompt]: 	Training 200/553. train loss: 1.2059,	0.8462 s / batch. (data: 1.58e-02). ETA=9:26:32, max mem: 20.9 GB 
[12/02 20:31:46 visual_prompt]: 	Training 300/553. train loss: 0.5361,	2.0349 s / batch. (data: 1.21e+00). ETA=22:38:54, max mem: 20.9 GB 
[12/02 20:33:36 visual_prompt]: 	Training 400/553. train loss: 1.3699,	0.8610 s / batch. (data: 6.58e-03). ETA=9:33:33, max mem: 20.9 GB 
[12/02 20:35:26 visual_prompt]: 	Training 500/553. train loss: 2.3905,	0.8603 s / batch. (data: 2.44e-02). ETA=9:31:39, max mem: 20.9 GB 
[12/02 20:36:25 visual_prompt]: Epoch 28 / 100: avg data time: 2.92e-01, avg batch time: 1.1248, average train loss: 1.1732
[12/02 20:37:30 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3080, average loss: 0.6995
[12/02 20:37:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.27	
[12/02 20:37:30 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[12/02 20:39:33 visual_prompt]: 	Training 100/553. train loss: 1.0199,	0.8298 s / batch. (data: 3.40e-04). ETA=9:09:15, max mem: 20.9 GB 
[12/02 20:41:24 visual_prompt]: 	Training 200/553. train loss: 0.9454,	2.3796 s / batch. (data: 1.56e+00). ETA=1 day, 2:11:11, max mem: 20.9 GB 
[12/02 20:43:13 visual_prompt]: 	Training 300/553. train loss: 0.9287,	0.8560 s / batch. (data: 3.83e-04). ETA=9:23:44, max mem: 20.9 GB 
[12/02 20:45:00 visual_prompt]: 	Training 400/553. train loss: 2.0555,	1.8722 s / batch. (data: 1.04e+00). ETA=20:29:53, max mem: 20.9 GB 
[12/02 20:46:51 visual_prompt]: 	Training 500/553. train loss: 1.2652,	0.8681 s / batch. (data: 4.04e-04). ETA=9:28:48, max mem: 20.9 GB 
[12/02 20:47:48 visual_prompt]: Epoch 29 / 100: avg data time: 2.86e-01, avg batch time: 1.1183, average train loss: 1.3232
[12/02 20:48:53 visual_prompt]: Inference (val):avg data time: 2.52e-04, avg batch time: 0.3089, average loss: 1.3718
[12/02 20:48:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.95	
[12/02 20:48:53 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[12/02 20:50:48 visual_prompt]: 	Training 100/553. train loss: 0.7049,	0.8422 s / batch. (data: 7.97e-03). ETA=9:09:42, max mem: 20.9 GB 
[12/02 20:52:40 visual_prompt]: 	Training 200/553. train loss: 0.9930,	0.8401 s / batch. (data: 3.21e-04). ETA=9:06:57, max mem: 20.9 GB 
[12/02 20:54:32 visual_prompt]: 	Training 300/553. train loss: 0.1686,	2.0828 s / batch. (data: 1.26e+00). ETA=22:32:31, max mem: 20.9 GB 
[12/02 20:56:26 visual_prompt]: 	Training 400/553. train loss: 0.8886,	1.7763 s / batch. (data: 9.58e-01). ETA=19:10:32, max mem: 20.9 GB 
[12/02 20:58:16 visual_prompt]: 	Training 500/553. train loss: 0.7784,	2.0196 s / batch. (data: 1.20e+00). ETA=21:44:47, max mem: 20.9 GB 
[12/02 20:59:16 visual_prompt]: Epoch 30 / 100: avg data time: 2.94e-01, avg batch time: 1.1262, average train loss: 0.9486
[12/02 21:00:21 visual_prompt]: Inference (val):avg data time: 4.34e-04, avg batch time: 0.3106, average loss: 0.6880
[12/02 21:00:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.49	
[12/02 21:00:21 visual_prompt]: Best epoch 30: best metric: -0.688
[12/02 21:00:21 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[12/02 21:02:20 visual_prompt]: 	Training 100/553. train loss: 0.9790,	0.8517 s / batch. (data: 1.11e-03). ETA=9:08:05, max mem: 20.9 GB 
[12/02 21:04:13 visual_prompt]: 	Training 200/553. train loss: 1.7139,	0.8681 s / batch. (data: 3.51e-04). ETA=9:17:08, max mem: 20.9 GB 
[12/02 21:06:02 visual_prompt]: 	Training 300/553. train loss: 0.8648,	0.8200 s / batch. (data: 3.41e-04). ETA=8:44:55, max mem: 20.9 GB 
[12/02 21:07:52 visual_prompt]: 	Training 400/553. train loss: 0.5830,	0.8544 s / batch. (data: 1.17e-02). ETA=9:05:31, max mem: 20.9 GB 
[12/02 21:09:45 visual_prompt]: 	Training 500/553. train loss: 0.5741,	0.8434 s / batch. (data: 5.45e-03). ETA=8:57:06, max mem: 20.9 GB 
[12/02 21:10:42 visual_prompt]: Epoch 31 / 100: avg data time: 2.90e-01, avg batch time: 1.1231, average train loss: 1.0939
[12/02 21:11:46 visual_prompt]: Inference (val):avg data time: 4.80e-05, avg batch time: 0.3112, average loss: 1.6096
[12/02 21:11:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.88	
[12/02 21:11:46 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[12/02 21:13:44 visual_prompt]: 	Training 100/553. train loss: 0.5697,	0.8668 s / batch. (data: 1.11e-02). ETA=9:09:49, max mem: 20.9 GB 
[12/02 21:15:34 visual_prompt]: 	Training 200/553. train loss: 0.6193,	0.8641 s / batch. (data: 8.62e-04). ETA=9:06:38, max mem: 20.9 GB 
[12/02 21:17:31 visual_prompt]: 	Training 300/553. train loss: 0.9712,	0.8400 s / batch. (data: 3.81e-04). ETA=8:49:59, max mem: 20.9 GB 
[12/02 21:19:22 visual_prompt]: 	Training 400/553. train loss: 0.9305,	0.8410 s / batch. (data: 1.05e-02). ETA=8:49:13, max mem: 20.9 GB 
[12/02 21:21:11 visual_prompt]: 	Training 500/553. train loss: 0.6974,	0.8350 s / batch. (data: 1.16e-03). ETA=8:44:04, max mem: 20.9 GB 
[12/02 21:22:06 visual_prompt]: Epoch 32 / 100: avg data time: 2.87e-01, avg batch time: 1.1210, average train loss: 0.8809
[12/02 21:23:11 visual_prompt]: Inference (val):avg data time: 4.47e-05, avg batch time: 0.3086, average loss: 0.7373
[12/02 21:23:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.74	
[12/02 21:23:11 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[12/02 21:25:05 visual_prompt]: 	Training 100/553. train loss: 0.0345,	1.1680 s / batch. (data: 3.20e-01). ETA=12:10:05, max mem: 20.9 GB 
[12/02 21:26:58 visual_prompt]: 	Training 200/553. train loss: 3.4234,	1.8330 s / batch. (data: 1.02e+00). ETA=19:02:40, max mem: 20.9 GB 
[12/02 21:28:49 visual_prompt]: 	Training 300/553. train loss: 0.6207,	0.8388 s / batch. (data: 3.04e-04). ETA=8:41:29, max mem: 20.9 GB 
[12/02 21:30:42 visual_prompt]: 	Training 400/553. train loss: 0.9057,	0.8315 s / batch. (data: 1.29e-03). ETA=8:35:35, max mem: 20.9 GB 
[12/02 21:32:32 visual_prompt]: 	Training 500/553. train loss: 0.5455,	1.2482 s / batch. (data: 4.32e-01). ETA=12:51:54, max mem: 20.9 GB 
[12/02 21:33:30 visual_prompt]: Epoch 33 / 100: avg data time: 2.86e-01, avg batch time: 1.1192, average train loss: 1.1526
[12/02 21:34:35 visual_prompt]: Inference (val):avg data time: 1.85e-04, avg batch time: 0.3082, average loss: 1.1265
[12/02 21:34:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.90	
[12/02 21:34:35 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[12/02 21:36:33 visual_prompt]: 	Training 100/553. train loss: 0.9483,	1.2551 s / batch. (data: 4.41e-01). ETA=12:52:56, max mem: 20.9 GB 
[12/02 21:38:22 visual_prompt]: 	Training 200/553. train loss: 0.8462,	0.8604 s / batch. (data: 2.52e-02). ETA=8:48:27, max mem: 20.9 GB 
[12/02 21:40:14 visual_prompt]: 	Training 300/553. train loss: 0.9869,	1.1280 s / batch. (data: 2.80e-01). ETA=11:30:55, max mem: 20.9 GB 
[12/02 21:42:06 visual_prompt]: 	Training 400/553. train loss: 0.8354,	0.8400 s / batch. (data: 5.13e-04). ETA=8:33:06, max mem: 20.9 GB 
[12/02 21:43:58 visual_prompt]: 	Training 500/553. train loss: 0.6489,	2.0000 s / batch. (data: 1.17e+00). ETA=20:18:22, max mem: 20.9 GB 
[12/02 21:44:56 visual_prompt]: Epoch 34 / 100: avg data time: 2.90e-01, avg batch time: 1.1225, average train loss: 1.0422
[12/02 21:46:01 visual_prompt]: Inference (val):avg data time: 5.25e-05, avg batch time: 0.3095, average loss: 0.6918
[12/02 21:46:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.79	
[12/02 21:46:01 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[12/02 21:47:58 visual_prompt]: 	Training 100/553. train loss: 1.3668,	0.8290 s / batch. (data: 3.85e-04). ETA=8:22:53, max mem: 20.9 GB 
[12/02 21:49:50 visual_prompt]: 	Training 200/553. train loss: 1.0887,	0.8421 s / batch. (data: 3.41e-04). ETA=8:29:27, max mem: 20.9 GB 
[12/02 21:51:40 visual_prompt]: 	Training 300/553. train loss: 1.1288,	0.8516 s / batch. (data: 1.26e-02). ETA=8:33:45, max mem: 20.9 GB 
[12/02 21:53:31 visual_prompt]: 	Training 400/553. train loss: 1.5932,	1.2597 s / batch. (data: 4.23e-01). ETA=12:37:52, max mem: 20.9 GB 
[12/02 21:55:20 visual_prompt]: 	Training 500/553. train loss: 0.7116,	0.8304 s / batch. (data: 4.13e-04). ETA=8:18:12, max mem: 20.9 GB 
[12/02 21:56:19 visual_prompt]: Epoch 35 / 100: avg data time: 2.85e-01, avg batch time: 1.1174, average train loss: 1.0545
[12/02 21:57:24 visual_prompt]: Inference (val):avg data time: 5.42e-05, avg batch time: 0.3091, average loss: 2.2827
[12/02 21:57:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.91	
[12/02 21:57:24 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[12/02 21:59:19 visual_prompt]: 	Training 100/553. train loss: 0.6815,	0.8390 s / batch. (data: 9.01e-03). ETA=8:21:14, max mem: 20.9 GB 
[12/02 22:01:12 visual_prompt]: 	Training 200/553. train loss: 2.7106,	0.8520 s / batch. (data: 3.35e-04). ETA=8:27:35, max mem: 20.9 GB 
[12/02 22:03:05 visual_prompt]: 	Training 300/553. train loss: 0.0736,	0.8480 s / batch. (data: 3.17e-04). ETA=8:23:47, max mem: 20.9 GB 
[12/02 22:04:56 visual_prompt]: 	Training 400/553. train loss: 0.8018,	0.8161 s / batch. (data: 3.34e-04). ETA=8:03:29, max mem: 20.9 GB 
[12/02 22:06:50 visual_prompt]: 	Training 500/553. train loss: 0.7004,	1.2604 s / batch. (data: 4.45e-01). ETA=12:24:34, max mem: 20.9 GB 
[12/02 22:07:43 visual_prompt]: Epoch 36 / 100: avg data time: 2.87e-01, avg batch time: 1.1208, average train loss: 1.1824
[12/02 22:08:48 visual_prompt]: Inference (val):avg data time: 4.50e-05, avg batch time: 0.3095, average loss: 2.6534
[12/02 22:08:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.09	
[12/02 22:08:48 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[12/02 22:10:45 visual_prompt]: 	Training 100/553. train loss: 1.0975,	0.8263 s / batch. (data: 3.02e-04). ETA=8:06:01, max mem: 20.9 GB 
[12/02 22:12:35 visual_prompt]: 	Training 200/553. train loss: 0.7176,	0.8513 s / batch. (data: 1.13e-02). ETA=8:19:19, max mem: 20.9 GB 
[12/02 22:14:28 visual_prompt]: 	Training 300/553. train loss: 1.9940,	2.0495 s / batch. (data: 1.20e+00). ETA=19:58:41, max mem: 20.9 GB 
[12/02 22:16:23 visual_prompt]: 	Training 400/553. train loss: 0.6749,	2.1101 s / batch. (data: 1.29e+00). ETA=20:30:35, max mem: 20.9 GB 
[12/02 22:18:11 visual_prompt]: 	Training 500/553. train loss: 1.3532,	1.5528 s / batch. (data: 7.17e-01). ETA=15:02:59, max mem: 20.9 GB 
[12/02 22:19:10 visual_prompt]: Epoch 37 / 100: avg data time: 2.91e-01, avg batch time: 1.1246, average train loss: 1.2433
[12/02 22:20:15 visual_prompt]: Inference (val):avg data time: 5.06e-05, avg batch time: 0.3105, average loss: 0.7133
[12/02 22:20:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.38	
[12/02 22:20:15 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[12/02 22:22:10 visual_prompt]: 	Training 100/553. train loss: 0.5807,	1.2548 s / batch. (data: 4.40e-01). ETA=12:06:31, max mem: 20.9 GB 
[12/02 22:24:02 visual_prompt]: 	Training 200/553. train loss: 0.8502,	1.5702 s / batch. (data: 7.44e-01). ETA=15:06:31, max mem: 20.9 GB 
[12/02 22:25:55 visual_prompt]: 	Training 300/553. train loss: 0.8143,	0.8579 s / batch. (data: 2.95e-02). ETA=8:13:51, max mem: 20.9 GB 
[12/02 22:27:45 visual_prompt]: 	Training 400/553. train loss: 0.6667,	0.8601 s / batch. (data: 1.19e-02). ETA=8:13:40, max mem: 20.9 GB 
[12/02 22:29:39 visual_prompt]: 	Training 500/553. train loss: 1.4159,	0.8641 s / batch. (data: 2.99e-04). ETA=8:14:31, max mem: 20.9 GB 
[12/02 22:30:35 visual_prompt]: Epoch 38 / 100: avg data time: 2.87e-01, avg batch time: 1.1209, average train loss: 1.0341
[12/02 22:31:41 visual_prompt]: Inference (val):avg data time: 3.10e-04, avg batch time: 0.3089, average loss: 1.5270
[12/02 22:31:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.81	
[12/02 22:31:41 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[12/02 22:33:36 visual_prompt]: 	Training 100/553. train loss: 0.0641,	0.8360 s / batch. (data: 3.75e-04). ETA=7:56:18, max mem: 20.9 GB 
[12/02 22:35:32 visual_prompt]: 	Training 200/553. train loss: 1.1026,	0.8259 s / batch. (data: 9.78e-03). ETA=7:49:13, max mem: 20.9 GB 
[12/02 22:37:27 visual_prompt]: 	Training 300/553. train loss: 1.9701,	0.8429 s / batch. (data: 3.34e-04). ETA=7:57:26, max mem: 20.9 GB 
[12/02 22:39:16 visual_prompt]: 	Training 400/553. train loss: 0.6570,	0.8200 s / batch. (data: 3.40e-04). ETA=7:43:06, max mem: 20.9 GB 
[12/02 22:41:08 visual_prompt]: 	Training 500/553. train loss: 0.5838,	2.1244 s / batch. (data: 1.30e+00). ETA=19:56:14, max mem: 20.9 GB 
[12/02 22:42:03 visual_prompt]: Epoch 39 / 100: avg data time: 2.91e-01, avg batch time: 1.1249, average train loss: 0.9864
[12/02 22:43:09 visual_prompt]: Inference (val):avg data time: 4.75e-04, avg batch time: 0.3089, average loss: 0.9107
[12/02 22:43:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.25	
[12/02 22:43:09 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[12/02 22:45:07 visual_prompt]: 	Training 100/553. train loss: 1.6172,	0.8494 s / batch. (data: 2.08e-02). ETA=7:56:06, max mem: 20.9 GB 
[12/02 22:46:57 visual_prompt]: 	Training 200/553. train loss: 2.2256,	0.8278 s / batch. (data: 5.80e-04). ETA=7:42:39, max mem: 20.9 GB 
[12/02 22:48:50 visual_prompt]: 	Training 300/553. train loss: 2.0742,	0.8304 s / batch. (data: 8.78e-04). ETA=7:42:41, max mem: 20.9 GB 
[12/02 22:50:42 visual_prompt]: 	Training 400/553. train loss: 0.7869,	0.8589 s / batch. (data: 7.94e-04). ETA=7:57:10, max mem: 20.9 GB 
[12/02 22:52:33 visual_prompt]: 	Training 500/553. train loss: 0.5239,	0.8495 s / batch. (data: 2.36e-02). ETA=7:50:30, max mem: 20.9 GB 
[12/02 22:53:32 visual_prompt]: Epoch 40 / 100: avg data time: 2.93e-01, avg batch time: 1.1264, average train loss: 1.2190
[12/02 22:54:37 visual_prompt]: Inference (val):avg data time: 5.01e-05, avg batch time: 0.3084, average loss: 0.6916
[12/02 22:54:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 54.74	
[12/02 22:54:37 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[12/02 22:56:40 visual_prompt]: 	Training 100/553. train loss: 1.1397,	0.8560 s / batch. (data: 3.12e-04). ETA=7:51:56, max mem: 20.9 GB 
[12/02 22:58:35 visual_prompt]: 	Training 200/553. train loss: 1.6828,	0.8317 s / batch. (data: 5.16e-04). ETA=7:37:10, max mem: 20.9 GB 
[12/02 23:00:26 visual_prompt]: 	Training 300/553. train loss: 0.7830,	0.8509 s / batch. (data: 1.18e-02). ETA=7:46:16, max mem: 20.9 GB 
[12/02 23:02:19 visual_prompt]: 	Training 400/553. train loss: 0.8210,	0.8320 s / batch. (data: 2.85e-04). ETA=7:34:33, max mem: 20.9 GB 
[12/02 23:04:06 visual_prompt]: 	Training 500/553. train loss: 0.7016,	0.8512 s / batch. (data: 5.63e-03). ETA=7:43:38, max mem: 20.9 GB 
[12/02 23:05:03 visual_prompt]: Epoch 41 / 100: avg data time: 2.99e-01, avg batch time: 1.1317, average train loss: 1.2086
[12/02 23:06:08 visual_prompt]: Inference (val):avg data time: 5.18e-05, avg batch time: 0.3084, average loss: 1.9049
[12/02 23:06:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.74	
[12/02 23:06:08 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[12/02 23:08:04 visual_prompt]: 	Training 100/553. train loss: 1.2751,	0.8480 s / batch. (data: 5.47e-03). ETA=7:39:42, max mem: 20.9 GB 
[12/02 23:09:57 visual_prompt]: 	Training 200/553. train loss: 4.0036,	0.9021 s / batch. (data: 8.66e-02). ETA=8:07:31, max mem: 20.9 GB 
[12/02 23:11:50 visual_prompt]: 	Training 300/553. train loss: 0.6465,	0.8708 s / batch. (data: 1.73e-02). ETA=7:49:11, max mem: 20.9 GB 
[12/02 23:13:43 visual_prompt]: 	Training 400/553. train loss: 0.7100,	0.8300 s / batch. (data: 4.63e-04). ETA=7:25:47, max mem: 20.9 GB 
[12/02 23:15:34 visual_prompt]: 	Training 500/553. train loss: 1.0262,	0.8198 s / batch. (data: 3.21e-04). ETA=7:18:57, max mem: 20.9 GB 
[12/02 23:16:32 visual_prompt]: Epoch 42 / 100: avg data time: 2.95e-01, avg batch time: 1.1287, average train loss: 1.3379
[12/02 23:17:37 visual_prompt]: Inference (val):avg data time: 5.14e-05, avg batch time: 0.3106, average loss: 0.7310
[12/02 23:17:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.59	
[12/02 23:17:37 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[12/02 23:19:36 visual_prompt]: 	Training 100/553. train loss: 0.5841,	0.8323 s / batch. (data: 6.33e-04). ETA=7:23:30, max mem: 20.9 GB 
[12/02 23:21:28 visual_prompt]: 	Training 200/553. train loss: 0.7191,	0.8480 s / batch. (data: 7.94e-03). ETA=7:30:29, max mem: 20.9 GB 
[12/02 23:23:17 visual_prompt]: 	Training 300/553. train loss: 1.3362,	0.8560 s / batch. (data: 2.86e-04). ETA=7:33:17, max mem: 20.9 GB 
[12/02 23:25:07 visual_prompt]: 	Training 400/553. train loss: 0.6597,	0.8314 s / batch. (data: 3.82e-04). ETA=7:18:53, max mem: 20.9 GB 
[12/02 23:27:00 visual_prompt]: 	Training 500/553. train loss: 0.6791,	0.8522 s / batch. (data: 1.64e-02). ETA=7:28:27, max mem: 20.9 GB 
[12/02 23:28:01 visual_prompt]: Epoch 43 / 100: avg data time: 2.94e-01, avg batch time: 1.1273, average train loss: 1.1429
[12/02 23:29:05 visual_prompt]: Inference (val):avg data time: 5.76e-05, avg batch time: 0.3088, average loss: 0.8117
[12/02 23:29:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.13	
[12/02 23:29:05 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[12/02 23:31:03 visual_prompt]: 	Training 100/553. train loss: 0.5694,	0.8481 s / batch. (data: 5.46e-03). ETA=7:24:09, max mem: 20.9 GB 
[12/02 23:32:58 visual_prompt]: 	Training 200/553. train loss: 0.8311,	0.8381 s / batch. (data: 4.88e-04). ETA=7:17:29, max mem: 20.9 GB 
[12/02 23:34:48 visual_prompt]: 	Training 300/553. train loss: 0.6640,	0.8477 s / batch. (data: 6.23e-03). ETA=7:21:05, max mem: 20.9 GB 
[12/02 23:36:38 visual_prompt]: 	Training 400/553. train loss: 0.7252,	0.8478 s / batch. (data: 7.89e-04). ETA=7:19:45, max mem: 20.9 GB 
[12/02 23:38:30 visual_prompt]: 	Training 500/553. train loss: 0.6982,	0.8282 s / batch. (data: 1.18e-02). ETA=7:08:12, max mem: 20.9 GB 
[12/02 23:39:29 visual_prompt]: Epoch 44 / 100: avg data time: 2.95e-01, avg batch time: 1.1278, average train loss: 1.0470
[12/02 23:40:35 visual_prompt]: Inference (val):avg data time: 6.92e-05, avg batch time: 0.3114, average loss: 1.0421
[12/02 23:40:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.24	
[12/02 23:40:35 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[12/02 23:42:33 visual_prompt]: 	Training 100/553. train loss: 0.7681,	0.8223 s / batch. (data: 5.99e-04). ETA=7:03:03, max mem: 20.9 GB 
[12/02 23:44:22 visual_prompt]: 	Training 200/553. train loss: 0.5656,	0.8161 s / batch. (data: 6.60e-04). ETA=6:58:30, max mem: 20.9 GB 
[12/02 23:46:16 visual_prompt]: 	Training 300/553. train loss: 2.6646,	0.8426 s / batch. (data: 5.46e-03). ETA=7:10:42, max mem: 20.9 GB 
[12/02 23:48:05 visual_prompt]: 	Training 400/553. train loss: 1.1059,	0.8615 s / batch. (data: 3.49e-04). ETA=7:18:55, max mem: 20.9 GB 
[12/02 23:50:00 visual_prompt]: 	Training 500/553. train loss: 1.4157,	0.8658 s / batch. (data: 1.95e-02). ETA=7:19:37, max mem: 20.9 GB 
[12/02 23:50:58 visual_prompt]: Epoch 45 / 100: avg data time: 2.95e-01, avg batch time: 1.1273, average train loss: 0.8904
[12/02 23:52:03 visual_prompt]: Inference (val):avg data time: 5.24e-05, avg batch time: 0.3113, average loss: 0.9534
[12/02 23:52:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.47	
[12/02 23:52:03 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[12/02 23:54:01 visual_prompt]: 	Training 100/553. train loss: 0.6011,	1.7280 s / batch. (data: 8.93e-01). ETA=14:33:04, max mem: 20.9 GB 
[12/02 23:55:54 visual_prompt]: 	Training 200/553. train loss: 1.3967,	0.8152 s / batch. (data: 3.19e-04). ETA=6:50:31, max mem: 20.9 GB 
[12/02 23:57:44 visual_prompt]: 	Training 300/553. train loss: 1.0838,	0.8280 s / batch. (data: 3.35e-04). ETA=6:55:35, max mem: 20.9 GB 
[12/02 23:59:38 visual_prompt]: 	Training 400/553. train loss: 0.7077,	0.8271 s / batch. (data: 3.33e-04). ETA=6:53:46, max mem: 20.9 GB 
[12/03 00:01:25 visual_prompt]: 	Training 500/553. train loss: 2.4018,	0.8200 s / batch. (data: 3.70e-04). ETA=6:48:49, max mem: 20.9 GB 
[12/03 00:02:25 visual_prompt]: Epoch 46 / 100: avg data time: 2.92e-01, avg batch time: 1.1240, average train loss: 1.1090
[12/03 00:03:29 visual_prompt]: Inference (val):avg data time: 5.10e-05, avg batch time: 0.3090, average loss: 0.7941
[12/03 00:03:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.31	
[12/03 00:03:29 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[12/03 00:05:27 visual_prompt]: 	Training 100/553. train loss: 1.0026,	0.8192 s / batch. (data: 3.37e-04). ETA=6:46:21, max mem: 20.9 GB 
[12/03 00:07:16 visual_prompt]: 	Training 200/553. train loss: 0.9180,	1.4874 s / batch. (data: 6.73e-01). ETA=12:15:18, max mem: 20.9 GB 
[12/03 00:09:08 visual_prompt]: 	Training 300/553. train loss: 1.0089,	0.8442 s / batch. (data: 1.55e-02). ETA=6:55:54, max mem: 20.9 GB 
[12/03 00:10:59 visual_prompt]: 	Training 400/553. train loss: 0.7003,	0.8357 s / batch. (data: 1.57e-02). ETA=6:50:20, max mem: 20.9 GB 
[12/03 00:12:50 visual_prompt]: 	Training 500/553. train loss: 0.5744,	0.8560 s / batch. (data: 3.04e-04). ETA=6:58:54, max mem: 20.9 GB 
[12/03 00:13:49 visual_prompt]: Epoch 47 / 100: avg data time: 2.87e-01, avg batch time: 1.1197, average train loss: 1.0548
[12/03 00:14:54 visual_prompt]: Inference (val):avg data time: 4.91e-05, avg batch time: 0.3090, average loss: 0.7879
[12/03 00:14:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.51	
[12/03 00:14:54 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.3189093389542498
[12/03 00:16:51 visual_prompt]: 	Training 100/553. train loss: 0.6988,	0.8258 s / batch. (data: 3.40e-04). ETA=6:41:59, max mem: 20.9 GB 
[12/03 00:18:43 visual_prompt]: 	Training 200/553. train loss: 0.9757,	0.8342 s / batch. (data: 2.58e-03). ETA=6:44:41, max mem: 20.9 GB 
[12/03 00:20:37 visual_prompt]: 	Training 300/553. train loss: 0.9183,	1.9040 s / batch. (data: 1.07e+00). ETA=15:20:32, max mem: 20.9 GB 
[12/03 00:22:25 visual_prompt]: 	Training 400/553. train loss: 0.5316,	1.0174 s / batch. (data: 2.00e-01). ETA=8:10:11, max mem: 20.9 GB 
[12/03 00:24:17 visual_prompt]: 	Training 500/553. train loss: 0.8235,	0.8311 s / batch. (data: 5.44e-03). ETA=6:39:02, max mem: 20.9 GB 
[12/03 00:25:15 visual_prompt]: Epoch 48 / 100: avg data time: 2.90e-01, avg batch time: 1.1239, average train loss: 0.9828
[12/03 00:26:20 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.3101, average loss: 0.6958
[12/03 00:26:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.39	
[12/03 00:26:20 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.3104804738999169
[12/03 00:28:16 visual_prompt]: 	Training 100/553. train loss: 0.7831,	0.8146 s / batch. (data: 3.29e-04). ETA=6:29:04, max mem: 20.9 GB 
[12/03 00:30:07 visual_prompt]: 	Training 200/553. train loss: 0.7013,	0.8355 s / batch. (data: 1.26e-03). ETA=6:37:38, max mem: 20.9 GB 
[12/03 00:31:58 visual_prompt]: 	Training 300/553. train loss: 3.6333,	0.8379 s / batch. (data: 4.14e-04). ETA=6:37:24, max mem: 20.9 GB 
[12/03 00:33:52 visual_prompt]: 	Training 400/553. train loss: 0.7431,	0.8202 s / batch. (data: 3.11e-04). ETA=6:27:36, max mem: 20.9 GB 
[12/03 00:35:45 visual_prompt]: 	Training 500/553. train loss: 0.5814,	0.8380 s / batch. (data: 3.06e-04). ETA=6:34:39, max mem: 20.9 GB 
[12/03 00:36:43 visual_prompt]: Epoch 49 / 100: avg data time: 2.93e-01, avg batch time: 1.1261, average train loss: 0.9772
[12/03 00:37:48 visual_prompt]: Inference (val):avg data time: 4.60e-05, avg batch time: 0.3093, average loss: 0.6903
[12/03 00:37:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.69	
[12/03 00:37:48 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.3019779227044398
[12/03 00:39:44 visual_prompt]: 	Training 100/553. train loss: 1.0747,	0.8636 s / batch. (data: 9.80e-04). ETA=6:44:30, max mem: 20.9 GB 
[12/03 00:41:37 visual_prompt]: 	Training 200/553. train loss: 5.4372,	0.8386 s / batch. (data: 1.60e-02). ETA=6:31:23, max mem: 20.9 GB 
[12/03 00:43:27 visual_prompt]: 	Training 300/553. train loss: 1.1794,	0.8304 s / batch. (data: 1.42e-03). ETA=6:26:10, max mem: 20.9 GB 
[12/03 00:45:17 visual_prompt]: 	Training 400/553. train loss: 0.8212,	0.8316 s / batch. (data: 3.56e-04). ETA=6:25:21, max mem: 20.9 GB 
[12/03 00:47:09 visual_prompt]: 	Training 500/553. train loss: 0.7600,	0.8429 s / batch. (data: 5.71e-03). ETA=6:29:10, max mem: 20.9 GB 
[12/03 00:48:07 visual_prompt]: Epoch 50 / 100: avg data time: 2.88e-01, avg batch time: 1.1200, average train loss: 1.0001
[12/03 00:49:11 visual_prompt]: Inference (val):avg data time: 4.70e-05, avg batch time: 0.3105, average loss: 0.9976
[12/03 00:49:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.57	
[12/03 00:49:11 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.29341204441673263
[12/03 00:51:07 visual_prompt]: 	Training 100/553. train loss: 1.1656,	1.7280 s / batch. (data: 9.11e-01). ETA=13:13:27, max mem: 20.9 GB 
[12/03 00:52:59 visual_prompt]: 	Training 200/553. train loss: 1.0153,	0.8294 s / batch. (data: 3.37e-04). ETA=6:19:27, max mem: 20.9 GB 
[12/03 00:54:51 visual_prompt]: 	Training 300/553. train loss: 0.5555,	0.8235 s / batch. (data: 3.26e-04). ETA=6:15:23, max mem: 20.9 GB 
[12/03 00:56:43 visual_prompt]: 	Training 400/553. train loss: 0.9438,	2.0885 s / batch. (data: 1.27e+00). ETA=15:48:30, max mem: 20.9 GB 
[12/03 00:58:34 visual_prompt]: 	Training 500/553. train loss: 0.6385,	0.8440 s / batch. (data: 8.07e-04). ETA=6:21:55, max mem: 20.9 GB 
[12/03 00:59:30 visual_prompt]: Epoch 51 / 100: avg data time: 2.86e-01, avg batch time: 1.1184, average train loss: 0.8635
[12/03 01:00:35 visual_prompt]: Inference (val):avg data time: 5.19e-05, avg batch time: 0.3104, average loss: 0.9174
[12/03 01:00:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.30	
[12/03 01:00:35 visual_prompt]: Stopping early.
[12/03 01:00:35 visual_prompt]: Rank of current process: 0. World size: 1
[12/03 01:00:35 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/03 01:00:35 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/03 01:00:35 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/03 01:00:35 visual_prompt]: Training with config:
[12/03 01:00:35 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.5_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/03 01:00:35 visual_prompt]: Loading training data...
[12/03 01:00:35 visual_prompt]: Constructing mammo-cbis dataset train...
[12/03 01:00:35 visual_prompt]: Loading validation data...
[12/03 01:00:35 visual_prompt]: Constructing mammo-cbis dataset val...
[12/03 01:00:35 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[12/03 01:00:38 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[12/03 01:00:38 visual_prompt]: tuned percent:0.525
[12/03 01:00:38 visual_prompt]: Device used for model: 0
[12/03 01:00:38 visual_prompt]: Setting up Evaluator...
[12/03 01:00:38 visual_prompt]: Setting up Trainer...
[12/03 01:00:38 visual_prompt]: 	Setting up the optimizer...
[12/03 01:00:39 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/03 01:02:35 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8259 s / batch. (data: 1.16e-02). ETA=12:39:47, max mem: 20.9 GB 
[12/03 01:04:25 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8958 s / batch. (data: 2.42e-02). ETA=13:42:39, max mem: 20.9 GB 
[12/03 01:06:20 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.7076 s / batch. (data: 1.87e+00). ETA=1 day, 17:21:56, max mem: 20.9 GB 
[12/03 01:08:10 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8192 s / batch. (data: 9.26e-04). ETA=12:29:34, max mem: 20.9 GB 
[12/03 01:10:04 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8260 s / batch. (data: 3.51e-04). ETA=12:34:24, max mem: 20.9 GB 
[12/03 01:11:03 visual_prompt]: Epoch 1 / 100: avg data time: 2.96e-01, avg batch time: 1.1298, average train loss: 1.5403
[12/03 01:12:08 visual_prompt]: Inference (val):avg data time: 5.66e-05, avg batch time: 0.3088, average loss: 1.5201
[12/03 01:12:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[12/03 01:12:08 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[12/03 01:14:04 visual_prompt]: 	Training 100/553. train loss: 0.7679,	1.0159 s / batch. (data: 1.90e-01). ETA=15:25:16, max mem: 20.9 GB 
[12/03 01:15:55 visual_prompt]: 	Training 200/553. train loss: 0.0953,	0.8275 s / batch. (data: 7.92e-03). ETA=12:32:15, max mem: 20.9 GB 
[12/03 01:17:49 visual_prompt]: 	Training 300/553. train loss: 0.8468,	1.4905 s / batch. (data: 6.57e-01). ETA=22:32:30, max mem: 20.9 GB 
[12/03 01:19:39 visual_prompt]: 	Training 400/553. train loss: 1.5797,	0.8275 s / batch. (data: 5.51e-03). ETA=12:29:32, max mem: 20.9 GB 
[12/03 01:21:32 visual_prompt]: 	Training 500/553. train loss: 0.5381,	0.8482 s / batch. (data: 3.92e-04). ETA=12:46:51, max mem: 20.9 GB 
[12/03 01:22:29 visual_prompt]: Epoch 2 / 100: avg data time: 2.88e-01, avg batch time: 1.1215, average train loss: 0.9151
[12/03 01:23:34 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.3101, average loss: 1.2192
[12/03 01:23:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.14	
[12/03 01:23:34 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[12/03 01:25:28 visual_prompt]: 	Training 100/553. train loss: 0.7689,	0.9460 s / batch. (data: 1.19e-01). ETA=14:12:54, max mem: 20.9 GB 
[12/03 01:27:21 visual_prompt]: 	Training 200/553. train loss: 0.8610,	1.1015 s / batch. (data: 2.71e-01). ETA=16:31:16, max mem: 20.9 GB 
[12/03 01:29:12 visual_prompt]: 	Training 300/553. train loss: 0.6059,	0.8480 s / batch. (data: 3.16e-04). ETA=12:41:39, max mem: 20.9 GB 
[12/03 01:31:05 visual_prompt]: 	Training 400/553. train loss: 1.8079,	0.8494 s / batch. (data: 1.26e-02). ETA=12:41:33, max mem: 20.9 GB 
[12/03 01:32:58 visual_prompt]: 	Training 500/553. train loss: 0.8238,	1.8004 s / batch. (data: 9.54e-01). ETA=1 day, 2:51:11, max mem: 20.9 GB 
[12/03 01:33:54 visual_prompt]: Epoch 3 / 100: avg data time: 2.89e-01, avg batch time: 1.1222, average train loss: 0.8850
[12/03 01:35:00 visual_prompt]: Inference (val):avg data time: 5.77e-05, avg batch time: 0.3116, average loss: 0.7381
[12/03 01:35:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.87	
[12/03 01:35:00 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[12/03 01:36:58 visual_prompt]: 	Training 100/553. train loss: 0.8136,	0.8400 s / batch. (data: 3.85e-03). ETA=12:29:34, max mem: 20.9 GB 
[12/03 01:38:50 visual_prompt]: 	Training 200/553. train loss: 0.5487,	0.8558 s / batch. (data: 7.71e-04). ETA=12:42:15, max mem: 20.9 GB 
[12/03 01:40:41 visual_prompt]: 	Training 300/553. train loss: 0.9565,	1.4776 s / batch. (data: 6.49e-01). ETA=21:53:36, max mem: 20.9 GB 
[12/03 01:42:27 visual_prompt]: 	Training 400/553. train loss: 1.2003,	1.7280 s / batch. (data: 9.02e-01). ETA=1 day, 1:33:20, max mem: 20.9 GB 
[12/03 01:44:21 visual_prompt]: 	Training 500/553. train loss: 0.2921,	3.9845 s / batch. (data: 3.17e+00). ETA=2 days, 10:49:00, max mem: 20.9 GB 
[12/03 01:45:20 visual_prompt]: Epoch 4 / 100: avg data time: 2.88e-01, avg batch time: 1.1206, average train loss: 0.9867
[12/03 01:46:25 visual_prompt]: Inference (val):avg data time: 5.15e-05, avg batch time: 0.3108, average loss: 1.5527
[12/03 01:46:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.95	
[12/03 01:46:25 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[12/03 01:48:20 visual_prompt]: 	Training 100/553. train loss: 2.0901,	0.8603 s / batch. (data: 1.74e-02). ETA=12:39:44, max mem: 20.9 GB 
[12/03 01:50:11 visual_prompt]: 	Training 200/553. train loss: 0.8485,	1.6436 s / batch. (data: 7.98e-01). ETA=1 day, 0:08:45, max mem: 20.9 GB 
[12/03 01:52:04 visual_prompt]: 	Training 300/553. train loss: 1.4419,	0.8357 s / batch. (data: 1.08e-03). ETA=12:15:13, max mem: 20.9 GB 
[12/03 01:53:55 visual_prompt]: 	Training 400/553. train loss: 1.5371,	0.8567 s / batch. (data: 1.05e-02). ETA=12:32:17, max mem: 20.9 GB 
[12/03 01:55:47 visual_prompt]: 	Training 500/553. train loss: 0.5277,	0.8316 s / batch. (data: 1.19e-02). ETA=12:08:52, max mem: 20.9 GB 
[12/03 01:56:46 visual_prompt]: Epoch 5 / 100: avg data time: 2.90e-01, avg batch time: 1.1229, average train loss: 1.0075
[12/03 01:57:51 visual_prompt]: Inference (val):avg data time: 5.64e-05, avg batch time: 0.3092, average loss: 1.4005
[12/03 01:57:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.86	
[12/03 01:57:51 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[12/03 01:59:49 visual_prompt]: 	Training 100/553. train loss: 0.6448,	0.8370 s / batch. (data: 8.81e-04). ETA=12:11:28, max mem: 20.9 GB 
[12/03 02:01:40 visual_prompt]: 	Training 200/553. train loss: 2.5418,	0.8431 s / batch. (data: 7.96e-03). ETA=12:15:25, max mem: 20.9 GB 
[12/03 02:03:31 visual_prompt]: 	Training 300/553. train loss: 0.5570,	0.8320 s / batch. (data: 4.08e-04). ETA=12:04:20, max mem: 20.9 GB 
[12/03 02:05:26 visual_prompt]: 	Training 400/553. train loss: 0.8850,	0.8586 s / batch. (data: 1.05e-02). ETA=12:26:00, max mem: 20.9 GB 
[12/03 02:07:16 visual_prompt]: 	Training 500/553. train loss: 1.2508,	0.9920 s / batch. (data: 1.51e-01). ETA=14:20:19, max mem: 20.9 GB 
[12/03 02:08:13 visual_prompt]: Epoch 6 / 100: avg data time: 2.92e-01, avg batch time: 1.1254, average train loss: 1.0922
[12/03 02:09:18 visual_prompt]: Inference (val):avg data time: 5.14e-05, avg batch time: 0.3077, average loss: 1.2903
[12/03 02:09:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.99	
[12/03 02:09:18 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[12/03 02:11:13 visual_prompt]: 	Training 100/553. train loss: 2.6519,	0.8520 s / batch. (data: 2.98e-04). ETA=12:16:43, max mem: 20.9 GB 
[12/03 02:13:05 visual_prompt]: 	Training 200/553. train loss: 0.5422,	0.8366 s / batch. (data: 5.47e-03). ETA=12:01:59, max mem: 20.9 GB 
[12/03 02:15:00 visual_prompt]: 	Training 300/553. train loss: 0.7642,	2.4510 s / batch. (data: 1.61e+00). ETA=1 day, 11:11:12, max mem: 20.9 GB 
[12/03 02:16:52 visual_prompt]: 	Training 400/553. train loss: 0.5958,	2.3287 s / batch. (data: 1.51e+00). ETA=1 day, 9:22:00, max mem: 20.9 GB 
[12/03 02:18:40 visual_prompt]: 	Training 500/553. train loss: 1.4260,	0.8480 s / batch. (data: 7.91e-03). ETA=12:07:36, max mem: 20.9 GB 
[12/03 02:19:37 visual_prompt]: Epoch 7 / 100: avg data time: 2.85e-01, avg batch time: 1.1179, average train loss: 0.9920
[12/03 02:20:42 visual_prompt]: Inference (val):avg data time: 5.07e-05, avg batch time: 0.3105, average loss: 0.6865
[12/03 02:20:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 63.35	
[12/03 02:20:42 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[12/03 02:22:36 visual_prompt]: 	Training 100/553. train loss: 1.1387,	0.8490 s / batch. (data: 1.68e-02). ETA=12:06:18, max mem: 20.9 GB 
[12/03 02:24:30 visual_prompt]: 	Training 200/553. train loss: 1.0875,	0.8361 s / batch. (data: 3.19e-04). ETA=11:53:50, max mem: 20.9 GB 
[12/03 02:26:23 visual_prompt]: 	Training 300/553. train loss: 1.8679,	0.8575 s / batch. (data: 5.45e-03). ETA=12:10:45, max mem: 20.9 GB 
[12/03 02:28:13 visual_prompt]: 	Training 400/553. train loss: 0.7211,	1.2680 s / batch. (data: 4.47e-01). ETA=17:58:24, max mem: 20.9 GB 
[12/03 02:30:05 visual_prompt]: 	Training 500/553. train loss: 1.6014,	1.7701 s / batch. (data: 9.30e-01). ETA=1 day, 1:02:28, max mem: 20.9 GB 
[12/03 02:31:03 visual_prompt]: Epoch 8 / 100: avg data time: 2.89e-01, avg batch time: 1.1218, average train loss: 1.1053
[12/03 02:32:07 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.3088, average loss: 0.6950
[12/03 02:32:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.56	rocauc: 60.88	
[12/03 02:32:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[12/03 02:34:03 visual_prompt]: 	Training 100/553. train loss: 0.1361,	0.8454 s / batch. (data: 3.59e-04). ETA=11:55:26, max mem: 20.9 GB 
[12/03 02:35:54 visual_prompt]: 	Training 200/553. train loss: 0.7272,	0.8328 s / batch. (data: 1.60e-02). ETA=11:43:25, max mem: 20.9 GB 
[12/03 02:37:46 visual_prompt]: 	Training 300/553. train loss: 0.7011,	2.1840 s / batch. (data: 1.35e+00). ETA=1 day, 6:40:57, max mem: 20.9 GB 
[12/03 02:39:37 visual_prompt]: 	Training 400/553. train loss: 0.6741,	0.8200 s / batch. (data: 5.09e-04). ETA=11:29:49, max mem: 20.9 GB 
[12/03 02:41:30 visual_prompt]: 	Training 500/553. train loss: 1.0512,	0.8634 s / batch. (data: 1.53e-03). ETA=12:04:55, max mem: 20.9 GB 
[12/03 02:42:27 visual_prompt]: Epoch 9 / 100: avg data time: 2.87e-01, avg batch time: 1.1203, average train loss: 0.8821
[12/03 02:43:32 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.3101, average loss: 1.2095
[12/03 02:43:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.04	
[12/03 02:43:32 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[12/03 02:45:31 visual_prompt]: 	Training 100/553. train loss: 1.7716,	0.8423 s / batch. (data: 3.18e-04). ETA=11:45:04, max mem: 20.9 GB 
[12/03 02:47:21 visual_prompt]: 	Training 200/553. train loss: 0.7371,	0.8445 s / batch. (data: 1.56e-02). ETA=11:45:28, max mem: 20.9 GB 
[12/03 02:49:14 visual_prompt]: 	Training 300/553. train loss: 0.4131,	1.7746 s / batch. (data: 9.42e-01). ETA=1 day, 0:39:28, max mem: 20.9 GB 
[12/03 02:51:04 visual_prompt]: 	Training 400/553. train loss: 0.6812,	0.8600 s / batch. (data: 3.45e-04). ETA=11:55:33, max mem: 20.9 GB 
[12/03 02:52:58 visual_prompt]: 	Training 500/553. train loss: 0.5299,	1.4607 s / batch. (data: 6.46e-01). ETA=20:12:57, max mem: 20.9 GB 
[12/03 02:53:56 visual_prompt]: Epoch 10 / 100: avg data time: 2.96e-01, avg batch time: 1.1293, average train loss: 1.1561
[12/03 02:55:01 visual_prompt]: Inference (val):avg data time: 4.44e-05, avg batch time: 0.3104, average loss: 0.7289
[12/03 02:55:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.17	
[12/03 02:55:01 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[12/03 02:57:00 visual_prompt]: 	Training 100/553. train loss: 2.1311,	0.8440 s / batch. (data: 3.24e-04). ETA=11:38:41, max mem: 20.9 GB 
[12/03 02:58:54 visual_prompt]: 	Training 200/553. train loss: 1.6184,	0.8468 s / batch. (data: 1.20e-03). ETA=11:39:38, max mem: 20.9 GB 
[12/03 03:00:46 visual_prompt]: 	Training 300/553. train loss: 0.0302,	2.6410 s / batch. (data: 1.80e+00). ETA=1 day, 12:17:31, max mem: 20.9 GB 
[12/03 03:02:37 visual_prompt]: 	Training 400/553. train loss: 0.6176,	0.8267 s / batch. (data: 7.08e-04). ETA=11:20:15, max mem: 20.9 GB 
[12/03 03:04:31 visual_prompt]: 	Training 500/553. train loss: 0.7791,	0.8157 s / batch. (data: 3.17e-04). ETA=11:09:49, max mem: 20.9 GB 
[12/03 03:05:28 visual_prompt]: Epoch 11 / 100: avg data time: 3.01e-01, avg batch time: 1.1341, average train loss: 0.9843
[12/03 03:06:35 visual_prompt]: Inference (val):avg data time: 5.52e-05, avg batch time: 0.3120, average loss: 0.6849
[12/03 03:06:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.17	
[12/03 03:06:35 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[12/03 03:08:36 visual_prompt]: 	Training 100/553. train loss: 0.7765,	0.8306 s / batch. (data: 7.17e-03). ETA=11:19:57, max mem: 20.9 GB 
[12/03 03:10:30 visual_prompt]: 	Training 200/553. train loss: 0.6089,	0.8343 s / batch. (data: 3.54e-04). ETA=11:21:33, max mem: 20.9 GB 
[12/03 03:12:22 visual_prompt]: 	Training 300/553. train loss: 0.7602,	0.8318 s / batch. (data: 6.57e-04). ETA=11:18:08, max mem: 20.9 GB 
[12/03 03:14:15 visual_prompt]: 	Training 400/553. train loss: 0.5513,	0.8303 s / batch. (data: 3.23e-04). ETA=11:15:32, max mem: 20.9 GB 
[12/03 03:16:08 visual_prompt]: 	Training 500/553. train loss: 4.2357,	0.8387 s / batch. (data: 9.70e-04). ETA=11:21:00, max mem: 20.9 GB 
[12/03 03:17:04 visual_prompt]: Epoch 12 / 100: avg data time: 3.06e-01, avg batch time: 1.1381, average train loss: 1.0818
[12/03 03:18:09 visual_prompt]: Inference (val):avg data time: 5.41e-05, avg batch time: 0.3101, average loss: 1.6687
[12/03 03:18:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.49	
[12/03 03:18:09 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[12/03 03:20:09 visual_prompt]: 	Training 100/553. train loss: 0.5888,	2.2777 s / batch. (data: 1.45e+00). ETA=1 day, 6:43:33, max mem: 20.9 GB 
[12/03 03:21:58 visual_prompt]: 	Training 200/553. train loss: 0.6360,	0.8311 s / batch. (data: 3.31e-04). ETA=11:11:18, max mem: 20.9 GB 
[12/03 03:23:52 visual_prompt]: 	Training 300/553. train loss: 0.5082,	2.1840 s / batch. (data: 1.34e+00). ETA=1 day, 5:20:28, max mem: 20.9 GB 
[12/03 03:25:43 visual_prompt]: 	Training 400/553. train loss: 4.0660,	0.8294 s / batch. (data: 3.60e-04). ETA=11:07:11, max mem: 20.9 GB 
[12/03 03:27:36 visual_prompt]: 	Training 500/553. train loss: 0.8391,	0.8592 s / batch. (data: 7.21e-03). ETA=11:29:44, max mem: 20.9 GB 
[12/03 03:28:34 visual_prompt]: Epoch 13 / 100: avg data time: 2.96e-01, avg batch time: 1.1289, average train loss: 1.0679
[12/03 03:29:39 visual_prompt]: Inference (val):avg data time: 5.38e-05, avg batch time: 0.3082, average loss: 1.2025
[12/03 03:29:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.58	
[12/03 03:29:39 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[12/03 03:31:36 visual_prompt]: 	Training 100/553. train loss: 0.6127,	0.8362 s / batch. (data: 3.17e-04). ETA=11:09:05, max mem: 20.9 GB 
[12/03 03:33:29 visual_prompt]: 	Training 200/553. train loss: 0.1792,	1.7622 s / batch. (data: 9.39e-01). ETA=23:27:09, max mem: 20.9 GB 
[12/03 03:35:19 visual_prompt]: 	Training 300/553. train loss: 0.7110,	1.1361 s / batch. (data: 3.19e-01). ETA=15:05:17, max mem: 20.9 GB 
[12/03 03:37:10 visual_prompt]: 	Training 400/553. train loss: 0.6591,	0.8378 s / batch. (data: 1.20e-02). ETA=11:06:11, max mem: 20.9 GB 
[12/03 03:39:02 visual_prompt]: 	Training 500/553. train loss: 2.0891,	0.8160 s / batch. (data: 3.50e-04). ETA=10:47:30, max mem: 20.9 GB 
[12/03 03:40:00 visual_prompt]: Epoch 14 / 100: avg data time: 2.92e-01, avg batch time: 1.1238, average train loss: 1.1317
[12/03 03:41:04 visual_prompt]: Inference (val):avg data time: 4.63e-05, avg batch time: 0.3086, average loss: 0.7286
[12/03 03:41:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.69	
[12/03 03:41:04 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[12/03 03:43:00 visual_prompt]: 	Training 100/553. train loss: 1.1146,	0.8360 s / batch. (data: 7.97e-03). ETA=11:01:13, max mem: 20.9 GB 
[12/03 03:44:51 visual_prompt]: 	Training 200/553. train loss: 6.5137,	0.8468 s / batch. (data: 1.20e-02). ETA=11:08:23, max mem: 20.9 GB 
[12/03 03:46:44 visual_prompt]: 	Training 300/553. train loss: 1.6192,	0.8168 s / batch. (data: 3.31e-04). ETA=10:43:22, max mem: 20.9 GB 
[12/03 03:48:32 visual_prompt]: 	Training 400/553. train loss: 1.1032,	0.8640 s / batch. (data: 2.06e-02). ETA=11:19:05, max mem: 20.9 GB 
[12/03 03:50:26 visual_prompt]: 	Training 500/553. train loss: 0.6259,	0.8246 s / batch. (data: 6.69e-03). ETA=10:46:46, max mem: 20.9 GB 
[12/03 03:51:25 visual_prompt]: Epoch 15 / 100: avg data time: 2.88e-01, avg batch time: 1.1212, average train loss: 1.3634
[12/03 03:52:30 visual_prompt]: Inference (val):avg data time: 2.15e-04, avg batch time: 0.3104, average loss: 2.0656
[12/03 03:52:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.88	
[12/03 03:52:30 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[12/03 03:54:27 visual_prompt]: 	Training 100/553. train loss: 0.8009,	0.8629 s / batch. (data: 1.56e-02). ETA=11:14:32, max mem: 20.9 GB 
[12/03 03:56:19 visual_prompt]: 	Training 200/553. train loss: 1.4988,	0.8440 s / batch. (data: 3.40e-04). ETA=10:58:23, max mem: 20.9 GB 
[12/03 03:58:10 visual_prompt]: 	Training 300/553. train loss: 1.0280,	0.8520 s / batch. (data: 1.06e-02). ETA=11:03:13, max mem: 20.9 GB 
[12/03 04:00:02 visual_prompt]: 	Training 400/553. train loss: 0.6460,	0.8166 s / batch. (data: 3.32e-04). ETA=10:34:18, max mem: 20.9 GB 
[12/03 04:01:52 visual_prompt]: 	Training 500/553. train loss: 0.9618,	1.5506 s / batch. (data: 7.12e-01). ETA=20:01:49, max mem: 20.9 GB 
[12/03 04:02:52 visual_prompt]: Epoch 16 / 100: avg data time: 2.90e-01, avg batch time: 1.1236, average train loss: 0.9734
[12/03 04:03:58 visual_prompt]: Inference (val):avg data time: 2.57e-04, avg batch time: 0.3089, average loss: 0.6828
[12/03 04:03:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 64.31	
[12/03 04:03:58 visual_prompt]: Best epoch 16: best metric: -0.683
[12/03 04:03:58 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[12/03 04:05:53 visual_prompt]: 	Training 100/553. train loss: 0.8205,	0.8405 s / batch. (data: 1.60e-02). ETA=10:49:16, max mem: 20.9 GB 
[12/03 04:07:47 visual_prompt]: 	Training 200/553. train loss: 1.7765,	0.8360 s / batch. (data: 2.99e-04). ETA=10:44:26, max mem: 20.9 GB 
[12/03 04:09:37 visual_prompt]: 	Training 300/553. train loss: 1.4992,	0.8388 s / batch. (data: 3.01e-04). ETA=10:45:13, max mem: 20.9 GB 
[12/03 04:11:28 visual_prompt]: 	Training 400/553. train loss: 0.9255,	1.5880 s / batch. (data: 7.65e-01). ETA=20:18:48, max mem: 20.9 GB 
[12/03 04:13:19 visual_prompt]: 	Training 500/553. train loss: 0.6938,	2.1266 s / batch. (data: 1.30e+00). ETA=1 day, 3:08:40, max mem: 20.9 GB 
[12/03 04:14:18 visual_prompt]: Epoch 17 / 100: avg data time: 2.89e-01, avg batch time: 1.1221, average train loss: 0.9912
[12/03 04:15:23 visual_prompt]: Inference (val):avg data time: 4.41e-05, avg batch time: 0.3092, average loss: 0.7131
[12/03 04:15:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.57	
[12/03 04:15:23 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[12/03 04:17:21 visual_prompt]: 	Training 100/553. train loss: 0.7639,	0.8232 s / batch. (data: 3.21e-04). ETA=10:28:19, max mem: 20.9 GB 
[12/03 04:19:15 visual_prompt]: 	Training 200/553. train loss: 1.1218,	0.8189 s / batch. (data: 5.63e-04). ETA=10:23:43, max mem: 20.9 GB 
[12/03 04:21:08 visual_prompt]: 	Training 300/553. train loss: 0.4815,	0.8306 s / batch. (data: 3.19e-04). ETA=10:31:16, max mem: 20.9 GB 
[12/03 04:22:59 visual_prompt]: 	Training 400/553. train loss: 0.9788,	0.8305 s / batch. (data: 3.13e-04). ETA=10:29:48, max mem: 20.9 GB 
[12/03 04:24:49 visual_prompt]: 	Training 500/553. train loss: 1.5766,	0.8242 s / batch. (data: 4.10e-04). ETA=10:23:36, max mem: 20.9 GB 
[12/03 04:25:46 visual_prompt]: Epoch 18 / 100: avg data time: 2.93e-01, avg batch time: 1.1254, average train loss: 1.3131
[12/03 04:26:51 visual_prompt]: Inference (val):avg data time: 5.33e-05, avg batch time: 0.3096, average loss: 0.9938
[12/03 04:26:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.91	
[12/03 04:26:51 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[12/03 04:28:49 visual_prompt]: 	Training 100/553. train loss: 0.8804,	0.8497 s / batch. (data: 1.18e-03). ETA=10:40:45, max mem: 20.9 GB 
[12/03 04:30:41 visual_prompt]: 	Training 200/553. train loss: 0.7377,	0.8233 s / batch. (data: 5.14e-04). ETA=10:19:26, max mem: 20.9 GB 
[12/03 04:32:33 visual_prompt]: 	Training 300/553. train loss: 3.0461,	0.8278 s / batch. (data: 3.26e-04). ETA=10:21:31, max mem: 20.9 GB 
[12/03 04:34:24 visual_prompt]: 	Training 400/553. train loss: 0.6691,	0.8560 s / batch. (data: 5.95e-03). ETA=10:41:15, max mem: 20.9 GB 
[12/03 04:36:11 visual_prompt]: 	Training 500/553. train loss: 0.5790,	0.8578 s / batch. (data: 2.19e-02). ETA=10:41:09, max mem: 20.9 GB 
[12/03 04:37:09 visual_prompt]: Epoch 19 / 100: avg data time: 2.84e-01, avg batch time: 1.1172, average train loss: 0.9104
[12/03 04:38:13 visual_prompt]: Inference (val):avg data time: 4.75e-05, avg batch time: 0.3092, average loss: 1.7240
[12/03 04:38:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.08	
[12/03 04:38:13 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[12/03 04:40:08 visual_prompt]: 	Training 100/553. train loss: 0.7645,	0.8304 s / batch. (data: 3.36e-04). ETA=10:18:30, max mem: 20.9 GB 
[12/03 04:42:01 visual_prompt]: 	Training 200/553. train loss: 0.4673,	0.8565 s / batch. (data: 2.53e-02). ETA=10:36:34, max mem: 20.9 GB 
[12/03 04:43:52 visual_prompt]: 	Training 300/553. train loss: 1.8378,	0.8505 s / batch. (data: 3.02e-04). ETA=10:30:43, max mem: 20.9 GB 
[12/03 04:45:44 visual_prompt]: 	Training 400/553. train loss: 0.5997,	0.8431 s / batch. (data: 1.60e-02). ETA=10:23:48, max mem: 20.9 GB 
[12/03 04:47:34 visual_prompt]: 	Training 500/553. train loss: 0.7122,	0.8169 s / batch. (data: 3.01e-04). ETA=10:03:02, max mem: 20.9 GB 
[12/03 04:48:34 visual_prompt]: Epoch 20 / 100: avg data time: 2.87e-01, avg batch time: 1.1210, average train loss: 0.9603
[12/03 04:49:39 visual_prompt]: Inference (val):avg data time: 4.73e-05, avg batch time: 0.3111, average loss: 0.6917
[12/03 04:49:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 64.64	
[12/03 04:49:39 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[12/03 04:51:38 visual_prompt]: 	Training 100/553. train loss: 0.9778,	1.5252 s / batch. (data: 7.07e-01). ETA=18:42:01, max mem: 20.9 GB 
[12/03 04:53:27 visual_prompt]: 	Training 200/553. train loss: 1.2474,	0.8463 s / batch. (data: 5.45e-03). ETA=10:21:09, max mem: 20.9 GB 
[12/03 04:55:19 visual_prompt]: 	Training 300/553. train loss: 2.5566,	1.3734 s / batch. (data: 5.18e-01). ETA=16:45:47, max mem: 20.9 GB 
[12/03 04:57:09 visual_prompt]: 	Training 400/553. train loss: 1.3980,	0.8340 s / batch. (data: 3.43e-04). ETA=10:09:21, max mem: 20.9 GB 
[12/03 04:59:01 visual_prompt]: 	Training 500/553. train loss: 0.8321,	0.8513 s / batch. (data: 6.60e-03). ETA=10:20:37, max mem: 20.9 GB 
[12/03 04:59:57 visual_prompt]: Epoch 21 / 100: avg data time: 2.86e-01, avg batch time: 1.1190, average train loss: 0.9988
[12/03 05:01:02 visual_prompt]: Inference (val):avg data time: 4.19e-05, avg batch time: 0.3095, average loss: 0.7338
[12/03 05:01:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.85	rocauc: 59.84	
[12/03 05:01:02 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[12/03 05:02:58 visual_prompt]: 	Training 100/553. train loss: 1.1736,	0.8186 s / batch. (data: 9.95e-04). ETA=9:54:40, max mem: 20.9 GB 
[12/03 05:04:51 visual_prompt]: 	Training 200/553. train loss: 0.6435,	0.8462 s / batch. (data: 1.20e-02). ETA=10:13:17, max mem: 20.9 GB 
[12/03 05:06:39 visual_prompt]: 	Training 300/553. train loss: 0.2492,	0.8477 s / batch. (data: 8.40e-03). ETA=10:12:59, max mem: 20.9 GB 
[12/03 05:08:31 visual_prompt]: 	Training 400/553. train loss: 0.5551,	0.8418 s / batch. (data: 4.13e-04). ETA=10:07:19, max mem: 20.9 GB 
[12/03 05:10:22 visual_prompt]: 	Training 500/553. train loss: 0.5989,	0.8317 s / batch. (data: 9.41e-04). ETA=9:58:39, max mem: 20.9 GB 
[12/03 05:11:22 visual_prompt]: Epoch 22 / 100: avg data time: 2.87e-01, avg batch time: 1.1201, average train loss: 0.9580
[12/03 05:12:27 visual_prompt]: Inference (val):avg data time: 4.98e-05, avg batch time: 0.3093, average loss: 0.8673
[12/03 05:12:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.81	
[12/03 05:12:27 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[12/03 05:14:26 visual_prompt]: 	Training 100/553. train loss: 1.5493,	0.8400 s / batch. (data: 3.21e-04). ETA=10:02:28, max mem: 20.9 GB 
[12/03 05:16:19 visual_prompt]: 	Training 200/553. train loss: 1.3994,	1.1621 s / batch. (data: 3.30e-01). ETA=13:51:34, max mem: 20.9 GB 
[12/03 05:18:13 visual_prompt]: 	Training 300/553. train loss: 0.6830,	0.8321 s / batch. (data: 9.25e-04). ETA=9:54:00, max mem: 20.9 GB 
[12/03 05:20:02 visual_prompt]: 	Training 400/553. train loss: 0.5141,	0.8825 s / batch. (data: 1.78e-02). ETA=10:28:30, max mem: 20.9 GB 
[12/03 05:21:52 visual_prompt]: 	Training 500/553. train loss: 1.0323,	0.8556 s / batch. (data: 3.82e-02). ETA=10:07:58, max mem: 20.9 GB 
[12/03 05:22:50 visual_prompt]: Epoch 23 / 100: avg data time: 2.94e-01, avg batch time: 1.1266, average train loss: 0.9491
[12/03 05:23:55 visual_prompt]: Inference (val):avg data time: 5.56e-05, avg batch time: 0.3090, average loss: 0.7012
[12/03 05:23:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.77	
[12/03 05:23:55 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[12/03 05:25:48 visual_prompt]: 	Training 100/553. train loss: 1.1732,	0.8351 s / batch. (data: 5.20e-04). ETA=9:51:16, max mem: 20.9 GB 
[12/03 05:27:39 visual_prompt]: 	Training 200/553. train loss: 0.7263,	0.8345 s / batch. (data: 1.42e-02). ETA=9:49:27, max mem: 20.9 GB 
[12/03 05:29:32 visual_prompt]: 	Training 300/553. train loss: 0.6324,	1.5134 s / batch. (data: 6.53e-01). ETA=17:46:26, max mem: 20.9 GB 
[12/03 05:31:24 visual_prompt]: 	Training 400/553. train loss: 0.5983,	0.8280 s / batch. (data: 3.30e-04). ETA=9:42:05, max mem: 20.9 GB 
[12/03 05:33:19 visual_prompt]: 	Training 500/553. train loss: 0.6307,	0.8680 s / batch. (data: 7.94e-03). ETA=10:08:45, max mem: 20.9 GB 
[12/03 05:34:18 visual_prompt]: Epoch 24 / 100: avg data time: 2.94e-01, avg batch time: 1.1264, average train loss: 0.9542
[12/03 05:35:23 visual_prompt]: Inference (val):avg data time: 5.35e-05, avg batch time: 0.3097, average loss: 1.0284
[12/03 05:35:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.76	
[12/03 05:35:23 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[12/03 05:37:24 visual_prompt]: 	Training 100/553. train loss: 0.7718,	0.8523 s / batch. (data: 3.28e-04). ETA=9:55:36, max mem: 20.9 GB 
[12/03 05:39:12 visual_prompt]: 	Training 200/553. train loss: 1.9327,	1.1751 s / batch. (data: 3.47e-01). ETA=13:39:10, max mem: 20.9 GB 
[12/03 05:41:03 visual_prompt]: 	Training 300/553. train loss: 0.6931,	1.5372 s / batch. (data: 6.68e-01). ETA=17:49:03, max mem: 20.9 GB 
[12/03 05:42:55 visual_prompt]: 	Training 400/553. train loss: 0.6703,	1.7441 s / batch. (data: 9.24e-01). ETA=20:10:03, max mem: 20.9 GB 
[12/03 05:44:47 visual_prompt]: 	Training 500/553. train loss: 1.3267,	2.1717 s / batch. (data: 1.33e+00). ETA=1 day, 1:03:06, max mem: 20.9 GB 
[12/03 05:45:44 visual_prompt]: Epoch 25 / 100: avg data time: 2.89e-01, avg batch time: 1.1218, average train loss: 0.9651
[12/03 05:46:49 visual_prompt]: Inference (val):avg data time: 4.61e-05, avg batch time: 0.3087, average loss: 1.7048
[12/03 05:46:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.19	
[12/03 05:46:49 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[12/03 05:48:45 visual_prompt]: 	Training 100/553. train loss: 0.5803,	0.8410 s / batch. (data: 1.36e-03). ETA=9:39:54, max mem: 20.9 GB 
[12/03 05:50:38 visual_prompt]: 	Training 200/553. train loss: 1.9303,	2.3150 s / batch. (data: 1.50e+00). ETA=1 day, 2:32:31, max mem: 20.9 GB 
[12/03 05:52:31 visual_prompt]: 	Training 300/553. train loss: 0.4806,	0.8449 s / batch. (data: 1.05e-02). ETA=9:39:49, max mem: 20.9 GB 
[12/03 05:54:22 visual_prompt]: 	Training 400/553. train loss: 1.2638,	0.8548 s / batch. (data: 9.83e-03). ETA=9:45:12, max mem: 20.9 GB 
[12/03 05:56:10 visual_prompt]: 	Training 500/553. train loss: 0.5712,	0.8289 s / batch. (data: 3.59e-04). ETA=9:26:03, max mem: 20.9 GB 
[12/03 05:57:10 visual_prompt]: Epoch 26 / 100: avg data time: 2.90e-01, avg batch time: 1.1228, average train loss: 0.9355
[12/03 05:58:15 visual_prompt]: Inference (val):avg data time: 1.65e-04, avg batch time: 0.3103, average loss: 0.6400
[12/03 05:58:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 68.85	
[12/03 05:58:15 visual_prompt]: Best epoch 26: best metric: -0.640
[12/03 05:58:15 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[12/03 06:00:13 visual_prompt]: 	Training 100/553. train loss: 0.5589,	0.8320 s / batch. (data: 4.15e-04). ETA=9:26:04, max mem: 20.9 GB 
[12/03 06:02:04 visual_prompt]: 	Training 200/553. train loss: 0.9818,	2.1850 s / batch. (data: 1.37e+00). ETA=1 day, 0:42:55, max mem: 20.9 GB 
[12/03 06:03:56 visual_prompt]: 	Training 300/553. train loss: 0.6183,	0.9550 s / batch. (data: 1.40e-01). ETA=10:46:35, max mem: 20.9 GB 
[12/03 06:05:51 visual_prompt]: 	Training 400/553. train loss: 0.5808,	0.8345 s / batch. (data: 5.49e-03). ETA=9:23:36, max mem: 20.9 GB 
[12/03 06:07:42 visual_prompt]: 	Training 500/553. train loss: 0.8439,	0.8439 s / batch. (data: 5.82e-04). ETA=9:28:31, max mem: 20.9 GB 
[12/03 06:08:38 visual_prompt]: Epoch 27 / 100: avg data time: 2.94e-01, avg batch time: 1.1259, average train loss: 0.8934
[12/03 06:09:43 visual_prompt]: Inference (val):avg data time: 6.21e-05, avg batch time: 0.3094, average loss: 1.1167
[12/03 06:09:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 68.13	
[12/03 06:09:43 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[12/03 06:11:39 visual_prompt]: 	Training 100/553. train loss: 0.5879,	0.8700 s / batch. (data: 3.36e-02). ETA=9:43:54, max mem: 20.9 GB 
[12/03 06:13:30 visual_prompt]: 	Training 200/553. train loss: 1.5727,	0.8264 s / batch. (data: 3.12e-04). ETA=9:13:16, max mem: 20.9 GB 
[12/03 06:15:24 visual_prompt]: 	Training 300/553. train loss: 1.3673,	2.1983 s / batch. (data: 1.37e+00). ETA=1 day, 0:28:03, max mem: 20.9 GB 
[12/03 06:17:13 visual_prompt]: 	Training 400/553. train loss: 0.5791,	0.8303 s / batch. (data: 3.13e-04). ETA=9:13:06, max mem: 20.9 GB 
[12/03 06:19:03 visual_prompt]: 	Training 500/553. train loss: 1.0643,	0.8438 s / batch. (data: 1.05e-03). ETA=9:20:39, max mem: 20.9 GB 
[12/03 06:20:01 visual_prompt]: Epoch 28 / 100: avg data time: 2.86e-01, avg batch time: 1.1182, average train loss: 0.9082
[12/03 06:21:06 visual_prompt]: Inference (val):avg data time: 4.95e-05, avg batch time: 0.3111, average loss: 0.9102
[12/03 06:21:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 69.08	
[12/03 06:21:06 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[12/03 06:23:09 visual_prompt]: 	Training 100/553. train loss: 0.9330,	0.8480 s / batch. (data: 3.26e-04). ETA=9:21:19, max mem: 20.9 GB 
[12/03 06:25:00 visual_prompt]: 	Training 200/553. train loss: 1.0444,	2.2608 s / batch. (data: 1.41e+00). ETA=1 day, 0:52:42, max mem: 20.9 GB 
[12/03 06:26:49 visual_prompt]: 	Training 300/553. train loss: 0.6366,	0.8457 s / batch. (data: 1.11e-02). ETA=9:16:58, max mem: 20.9 GB 
[12/03 06:28:36 visual_prompt]: 	Training 400/553. train loss: 1.2541,	1.5965 s / batch. (data: 7.77e-01). ETA=17:28:46, max mem: 20.9 GB 
[12/03 06:30:28 visual_prompt]: 	Training 500/553. train loss: 0.7749,	0.8380 s / batch. (data: 1.05e-02). ETA=9:09:07, max mem: 20.9 GB 
[12/03 06:31:26 visual_prompt]: Epoch 29 / 100: avg data time: 2.89e-01, avg batch time: 1.1204, average train loss: 0.9702
[12/03 06:32:30 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3078, average loss: 1.1292
[12/03 06:32:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.18	
[12/03 06:32:30 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[12/03 06:34:25 visual_prompt]: 	Training 100/553. train loss: 0.7343,	0.8280 s / batch. (data: 2.75e-04). ETA=9:00:26, max mem: 20.9 GB 
[12/03 06:36:17 visual_prompt]: 	Training 200/553. train loss: 1.0467,	0.8312 s / batch. (data: 7.96e-03). ETA=9:01:07, max mem: 20.9 GB 
[12/03 06:38:07 visual_prompt]: 	Training 300/553. train loss: 0.1571,	1.7223 s / batch. (data: 8.86e-01). ETA=18:38:24, max mem: 20.9 GB 
[12/03 06:40:00 visual_prompt]: 	Training 400/553. train loss: 0.8606,	1.5406 s / batch. (data: 7.09e-01). ETA=16:37:51, max mem: 20.9 GB 
[12/03 06:41:51 visual_prompt]: 	Training 500/553. train loss: 0.9234,	2.0437 s / batch. (data: 1.21e+00). ETA=22:00:20, max mem: 20.9 GB 
[12/03 06:42:51 visual_prompt]: Epoch 30 / 100: avg data time: 2.89e-01, avg batch time: 1.1220, average train loss: 0.8483
[12/03 06:43:56 visual_prompt]: Inference (val):avg data time: 6.18e-05, avg batch time: 0.3096, average loss: 0.6776
[12/03 06:43:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 65.34	
[12/03 06:43:56 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[12/03 06:45:53 visual_prompt]: 	Training 100/553. train loss: 0.5471,	0.8280 s / batch. (data: 3.22e-04). ETA=8:52:49, max mem: 20.9 GB 
[12/03 06:47:48 visual_prompt]: 	Training 200/553. train loss: 1.2623,	0.8227 s / batch. (data: 5.76e-03). ETA=8:48:03, max mem: 20.9 GB 
[12/03 06:49:36 visual_prompt]: 	Training 300/553. train loss: 0.8930,	0.8374 s / batch. (data: 3.40e-04). ETA=8:56:05, max mem: 20.9 GB 
[12/03 06:51:26 visual_prompt]: 	Training 400/553. train loss: 0.4858,	1.8453 s / batch. (data: 1.03e+00). ETA=19:38:15, max mem: 20.9 GB 
[12/03 06:53:17 visual_prompt]: 	Training 500/553. train loss: 0.5799,	0.8235 s / batch. (data: 3.44e-04). ETA=8:44:25, max mem: 20.9 GB 
[12/03 06:54:13 visual_prompt]: Epoch 31 / 100: avg data time: 2.84e-01, avg batch time: 1.1165, average train loss: 0.9186
[12/03 06:55:18 visual_prompt]: Inference (val):avg data time: 4.45e-05, avg batch time: 0.3101, average loss: 0.8886
[12/03 06:55:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.46	
[12/03 06:55:18 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[12/03 06:57:15 visual_prompt]: 	Training 100/553. train loss: 0.5733,	0.8282 s / batch. (data: 3.84e-04). ETA=8:45:17, max mem: 20.9 GB 
[12/03 06:59:09 visual_prompt]: 	Training 200/553. train loss: 0.4370,	0.8451 s / batch. (data: 1.47e-03). ETA=8:54:39, max mem: 20.9 GB 
[12/03 07:01:05 visual_prompt]: 	Training 300/553. train loss: 1.3144,	0.8136 s / batch. (data: 3.02e-04). ETA=8:33:19, max mem: 20.9 GB 
[12/03 07:02:59 visual_prompt]: 	Training 400/553. train loss: 0.8571,	0.8584 s / batch. (data: 1.05e-02). ETA=9:00:10, max mem: 20.9 GB 
[12/03 07:04:49 visual_prompt]: 	Training 500/553. train loss: 0.9784,	0.8605 s / batch. (data: 3.09e-04). ETA=9:00:04, max mem: 20.9 GB 
[12/03 07:05:46 visual_prompt]: Epoch 32 / 100: avg data time: 3.02e-01, avg batch time: 1.1361, average train loss: 0.8385
[12/03 07:06:53 visual_prompt]: Inference (val):avg data time: 5.67e-05, avg batch time: 0.3112, average loss: 0.6622
[12/03 07:06:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 67.72	
[12/03 07:06:53 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[12/03 07:08:49 visual_prompt]: 	Training 100/553. train loss: 0.6371,	1.1133 s / batch. (data: 3.00e-01). ETA=11:35:54, max mem: 20.9 GB 
[12/03 07:10:46 visual_prompt]: 	Training 200/553. train loss: 1.1876,	2.2625 s / batch. (data: 1.42e+00). ETA=23:30:27, max mem: 20.9 GB 
[12/03 07:12:37 visual_prompt]: 	Training 300/553. train loss: 0.6934,	0.8360 s / batch. (data: 3.32e-04). ETA=8:39:45, max mem: 20.9 GB 
[12/03 07:14:31 visual_prompt]: 	Training 400/553. train loss: 0.4740,	0.8165 s / batch. (data: 3.23e-04). ETA=8:26:15, max mem: 20.9 GB 
[12/03 07:16:21 visual_prompt]: 	Training 500/553. train loss: 0.6223,	1.1341 s / batch. (data: 2.95e-01). ETA=11:41:18, max mem: 20.9 GB 
[12/03 07:17:19 visual_prompt]: Epoch 33 / 100: avg data time: 2.99e-01, avg batch time: 1.1320, average train loss: 0.9891
[12/03 07:18:24 visual_prompt]: Inference (val):avg data time: 5.18e-05, avg batch time: 0.3097, average loss: 1.2315
[12/03 07:18:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 68.00	
[12/03 07:18:24 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[12/03 07:20:23 visual_prompt]: 	Training 100/553. train loss: 0.6903,	1.0228 s / batch. (data: 1.93e-01). ETA=10:29:52, max mem: 20.9 GB 
[12/03 07:22:14 visual_prompt]: 	Training 200/553. train loss: 0.8068,	0.8600 s / batch. (data: 3.10e-04). ETA=8:48:11, max mem: 20.9 GB 
[12/03 07:24:05 visual_prompt]: 	Training 300/553. train loss: 0.5800,	0.8640 s / batch. (data: 1.60e-02). ETA=8:49:13, max mem: 20.9 GB 
[12/03 07:25:59 visual_prompt]: 	Training 400/553. train loss: 2.1472,	0.8400 s / batch. (data: 1.59e-02). ETA=8:33:06, max mem: 20.9 GB 
[12/03 07:27:50 visual_prompt]: 	Training 500/553. train loss: 0.5459,	1.8329 s / batch. (data: 1.02e+00). ETA=18:36:35, max mem: 20.9 GB 
[12/03 07:28:49 visual_prompt]: Epoch 34 / 100: avg data time: 2.95e-01, avg batch time: 1.1293, average train loss: 0.9102
[12/03 07:29:54 visual_prompt]: Inference (val):avg data time: 3.77e-04, avg batch time: 0.3095, average loss: 0.6606
[12/03 07:29:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 67.18	
[12/03 07:29:54 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[12/03 07:31:52 visual_prompt]: 	Training 100/553. train loss: 1.0932,	0.8464 s / batch. (data: 2.23e-02). ETA=8:33:26, max mem: 20.9 GB 
[12/03 07:33:43 visual_prompt]: 	Training 200/553. train loss: 1.0703,	0.8402 s / batch. (data: 1.21e-02). ETA=8:28:18, max mem: 20.9 GB 
[12/03 07:35:34 visual_prompt]: 	Training 300/553. train loss: 1.3418,	0.8494 s / batch. (data: 1.85e-02). ETA=8:32:27, max mem: 20.9 GB 
[12/03 07:37:25 visual_prompt]: 	Training 400/553. train loss: 0.8366,	1.1600 s / batch. (data: 3.08e-01). ETA=11:37:53, max mem: 20.9 GB 
[12/03 07:39:14 visual_prompt]: 	Training 500/553. train loss: 0.6799,	1.5894 s / batch. (data: 7.51e-01). ETA=15:53:36, max mem: 20.9 GB 
[12/03 07:40:14 visual_prompt]: Epoch 35 / 100: avg data time: 2.89e-01, avg batch time: 1.1213, average train loss: 0.9424
[12/03 07:41:19 visual_prompt]: Inference (val):avg data time: 6.73e-05, avg batch time: 0.3105, average loss: 1.0639
[12/03 07:41:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.83	
[12/03 07:41:19 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[12/03 07:43:16 visual_prompt]: 	Training 100/553. train loss: 0.5993,	0.8878 s / batch. (data: 4.14e-02). ETA=8:50:24, max mem: 20.9 GB 
[12/03 07:45:10 visual_prompt]: 	Training 200/553. train loss: 1.8298,	0.8294 s / batch. (data: 6.75e-03). ETA=8:14:05, max mem: 20.9 GB 
[12/03 07:47:03 visual_prompt]: 	Training 300/553. train loss: 0.1840,	0.8526 s / batch. (data: 8.54e-03). ETA=8:26:30, max mem: 20.9 GB 
[12/03 07:48:55 visual_prompt]: 	Training 400/553. train loss: 1.3101,	0.8488 s / batch. (data: 1.05e-02). ETA=8:22:48, max mem: 20.9 GB 
[12/03 07:50:47 visual_prompt]: 	Training 500/553. train loss: 0.4038,	1.4080 s / batch. (data: 5.75e-01). ETA=13:51:46, max mem: 20.9 GB 
[12/03 07:51:41 visual_prompt]: Epoch 36 / 100: avg data time: 2.91e-01, avg batch time: 1.1244, average train loss: 0.9894
[12/03 07:52:47 visual_prompt]: Inference (val):avg data time: 6.70e-05, avg batch time: 0.3103, average loss: 0.7526
[12/03 07:52:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 66.51	
[12/03 07:52:47 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[12/03 07:54:44 visual_prompt]: 	Training 100/553. train loss: 0.6345,	0.8397 s / batch. (data: 9.83e-04). ETA=8:13:55, max mem: 20.9 GB 
[12/03 07:56:35 visual_prompt]: 	Training 200/553. train loss: 0.6730,	0.8304 s / batch. (data: 5.44e-03). ETA=8:07:01, max mem: 20.9 GB 
[12/03 07:58:26 visual_prompt]: 	Training 300/553. train loss: 1.8157,	2.0520 s / batch. (data: 1.23e+00). ETA=20:00:07, max mem: 20.9 GB 
[12/03 08:00:21 visual_prompt]: 	Training 400/553. train loss: 0.6560,	2.4241 s / batch. (data: 1.60e+00). ETA=23:33:42, max mem: 20.9 GB 
[12/03 08:02:09 visual_prompt]: 	Training 500/553. train loss: 0.8414,	1.4930 s / batch. (data: 6.35e-01). ETA=14:28:13, max mem: 20.9 GB 
[12/03 08:03:08 visual_prompt]: Epoch 37 / 100: avg data time: 2.90e-01, avg batch time: 1.1230, average train loss: 0.9035
[12/03 08:04:13 visual_prompt]: Inference (val):avg data time: 5.06e-05, avg batch time: 0.3097, average loss: 0.6616
[12/03 08:04:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.65	
[12/03 08:04:13 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[12/03 08:06:08 visual_prompt]: 	Training 100/553. train loss: 0.4671,	1.2137 s / batch. (data: 3.93e-01). ETA=11:42:41, max mem: 20.9 GB 
[12/03 08:08:00 visual_prompt]: 	Training 200/553. train loss: 1.0126,	1.4360 s / batch. (data: 6.17e-01). ETA=13:49:02, max mem: 20.9 GB 
[12/03 08:09:54 visual_prompt]: 	Training 300/553. train loss: 0.5074,	0.8390 s / batch. (data: 8.09e-04). ETA=8:02:58, max mem: 20.9 GB 
[12/03 08:11:43 visual_prompt]: 	Training 400/553. train loss: 0.9426,	0.8792 s / batch. (data: 6.82e-03). ETA=8:24:38, max mem: 20.9 GB 
[12/03 08:13:36 visual_prompt]: 	Training 500/553. train loss: 0.8465,	0.8520 s / batch. (data: 3.32e-04). ETA=8:07:36, max mem: 20.9 GB 
[12/03 08:14:32 visual_prompt]: Epoch 38 / 100: avg data time: 2.86e-01, avg batch time: 1.1183, average train loss: 0.9061
[12/03 08:15:38 visual_prompt]: Inference (val):avg data time: 6.38e-05, avg batch time: 0.3112, average loss: 1.2208
[12/03 08:15:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.50	
[12/03 08:15:38 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[12/03 08:17:32 visual_prompt]: 	Training 100/553. train loss: 0.0895,	0.8383 s / batch. (data: 1.05e-02). ETA=7:57:38, max mem: 20.9 GB 
[12/03 08:19:29 visual_prompt]: 	Training 200/553. train loss: 1.0788,	0.8344 s / batch. (data: 3.24e-04). ETA=7:54:00, max mem: 20.9 GB 
[12/03 08:21:23 visual_prompt]: 	Training 300/553. train loss: 2.9042,	0.8288 s / batch. (data: 7.01e-04). ETA=7:49:28, max mem: 20.9 GB 
[12/03 08:23:12 visual_prompt]: 	Training 400/553. train loss: 0.6974,	0.8211 s / batch. (data: 5.53e-03). ETA=7:43:45, max mem: 20.9 GB 
[12/03 08:25:03 visual_prompt]: 	Training 500/553. train loss: 0.7140,	2.0520 s / batch. (data: 1.21e+00). ETA=19:15:28, max mem: 20.9 GB 
[12/03 08:25:59 visual_prompt]: Epoch 39 / 100: avg data time: 2.90e-01, avg batch time: 1.1223, average train loss: 0.9755
[12/03 08:27:04 visual_prompt]: Inference (val):avg data time: 4.84e-05, avg batch time: 0.3107, average loss: 0.9354
[12/03 08:27:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.61	
[12/03 08:27:04 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[12/03 08:29:00 visual_prompt]: 	Training 100/553. train loss: 1.0745,	0.8355 s / batch. (data: 3.57e-04). ETA=7:48:18, max mem: 20.9 GB 
[12/03 08:30:52 visual_prompt]: 	Training 200/553. train loss: 1.4086,	0.8214 s / batch. (data: 3.29e-04). ETA=7:39:03, max mem: 20.9 GB 
[12/03 08:32:45 visual_prompt]: 	Training 300/553. train loss: 1.1136,	0.8297 s / batch. (data: 4.94e-04). ETA=7:42:17, max mem: 20.9 GB 
[12/03 08:34:37 visual_prompt]: 	Training 400/553. train loss: 0.5240,	0.8160 s / batch. (data: 4.49e-04). ETA=7:33:20, max mem: 20.9 GB 
[12/03 08:36:27 visual_prompt]: 	Training 500/553. train loss: 0.3036,	0.8174 s / batch. (data: 3.13e-04). ETA=7:32:45, max mem: 20.9 GB 
[12/03 08:37:27 visual_prompt]: Epoch 40 / 100: avg data time: 2.94e-01, avg batch time: 1.1261, average train loss: 0.9345
[12/03 08:38:31 visual_prompt]: Inference (val):avg data time: 5.09e-05, avg batch time: 0.3095, average loss: 0.6589
[12/03 08:38:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 65.21	
[12/03 08:38:31 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[12/03 08:40:31 visual_prompt]: 	Training 100/553. train loss: 0.6940,	0.8232 s / batch. (data: 3.13e-04). ETA=7:33:52, max mem: 20.9 GB 
[12/03 08:42:26 visual_prompt]: 	Training 200/553. train loss: 1.2333,	0.8360 s / batch. (data: 3.39e-04). ETA=7:39:31, max mem: 20.9 GB 
[12/03 08:44:15 visual_prompt]: 	Training 300/553. train loss: 0.7720,	0.8560 s / batch. (data: 9.02e-03). ETA=7:49:04, max mem: 20.9 GB 
[12/03 08:46:05 visual_prompt]: 	Training 400/553. train loss: 0.9195,	0.8208 s / batch. (data: 5.64e-03). ETA=7:28:25, max mem: 20.9 GB 
[12/03 08:47:54 visual_prompt]: 	Training 500/553. train loss: 0.6610,	0.8337 s / batch. (data: 7.97e-03). ETA=7:34:04, max mem: 20.9 GB 
[12/03 08:48:49 visual_prompt]: Epoch 41 / 100: avg data time: 2.86e-01, avg batch time: 1.1180, average train loss: 0.9346
[12/03 08:49:54 visual_prompt]: Inference (val):avg data time: 5.35e-05, avg batch time: 0.3115, average loss: 0.9945
[12/03 08:49:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.63	
[12/03 08:49:54 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[12/03 08:51:50 visual_prompt]: 	Training 100/553. train loss: 1.2279,	0.8251 s / batch. (data: 3.33e-04). ETA=7:27:18, max mem: 20.9 GB 
[12/03 08:53:44 visual_prompt]: 	Training 200/553. train loss: 1.7115,	0.8360 s / batch. (data: 3.25e-04). ETA=7:31:48, max mem: 20.9 GB 
[12/03 08:55:36 visual_prompt]: 	Training 300/553. train loss: 1.0928,	0.8360 s / batch. (data: 7.95e-03). ETA=7:30:25, max mem: 20.9 GB 
[12/03 08:57:28 visual_prompt]: 	Training 400/553. train loss: 0.8035,	1.3920 s / batch. (data: 5.44e-01). ETA=12:27:39, max mem: 20.9 GB 
[12/03 08:59:20 visual_prompt]: 	Training 500/553. train loss: 1.7607,	1.8524 s / batch. (data: 1.02e+00). ETA=16:31:51, max mem: 20.9 GB 
[12/03 09:00:20 visual_prompt]: Epoch 42 / 100: avg data time: 2.98e-01, avg batch time: 1.1309, average train loss: 0.9880
[12/03 09:01:26 visual_prompt]: Inference (val):avg data time: 1.67e-04, avg batch time: 0.3115, average loss: 0.7403
[12/03 09:01:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.98	
[12/03 09:01:26 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[12/03 09:03:24 visual_prompt]: 	Training 100/553. train loss: 0.8435,	0.8646 s / batch. (data: 2.08e-02). ETA=7:40:44, max mem: 20.9 GB 
[12/03 09:05:15 visual_prompt]: 	Training 200/553. train loss: 0.8906,	0.8390 s / batch. (data: 3.68e-04). ETA=7:25:41, max mem: 20.9 GB 
[12/03 09:07:04 visual_prompt]: 	Training 300/553. train loss: 0.1807,	0.8229 s / batch. (data: 5.57e-03). ETA=7:15:48, max mem: 20.9 GB 
[12/03 09:08:55 visual_prompt]: 	Training 400/553. train loss: 1.5686,	0.8263 s / batch. (data: 1.08e-02). ETA=7:16:13, max mem: 20.9 GB 
[12/03 09:10:48 visual_prompt]: 	Training 500/553. train loss: 0.5713,	0.8402 s / batch. (data: 1.56e-02). ETA=7:22:09, max mem: 20.9 GB 
[12/03 09:11:48 visual_prompt]: Epoch 43 / 100: avg data time: 2.93e-01, avg batch time: 1.1251, average train loss: 0.8848
[12/03 09:12:53 visual_prompt]: Inference (val):avg data time: 5.21e-05, avg batch time: 0.3117, average loss: 0.9790
[12/03 09:12:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.14	
[12/03 09:12:53 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[12/03 09:14:50 visual_prompt]: 	Training 100/553. train loss: 0.6897,	0.8144 s / batch. (data: 3.54e-04). ETA=7:06:30, max mem: 20.9 GB 
[12/03 09:16:44 visual_prompt]: 	Training 200/553. train loss: 0.6593,	0.8519 s / batch. (data: 6.78e-04). ETA=7:24:41, max mem: 20.9 GB 
[12/03 09:18:33 visual_prompt]: 	Training 300/553. train loss: 0.8174,	0.8321 s / batch. (data: 3.34e-04). ETA=7:12:59, max mem: 20.9 GB 
[12/03 09:20:25 visual_prompt]: 	Training 400/553. train loss: 0.9755,	0.8368 s / batch. (data: 3.32e-04). ETA=7:14:02, max mem: 20.9 GB 
[12/03 09:22:17 visual_prompt]: 	Training 500/553. train loss: 0.7160,	0.8253 s / batch. (data: 1.06e-02). ETA=7:06:40, max mem: 20.9 GB 
[12/03 09:23:16 visual_prompt]: Epoch 44 / 100: avg data time: 2.92e-01, avg batch time: 1.1249, average train loss: 0.8916
[12/03 09:24:21 visual_prompt]: Inference (val):avg data time: 5.11e-05, avg batch time: 0.3100, average loss: 1.0274
[12/03 09:24:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.49	
[12/03 09:24:21 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[12/03 09:26:20 visual_prompt]: 	Training 100/553. train loss: 0.7761,	0.8236 s / batch. (data: 1.41e-03). ETA=7:03:43, max mem: 20.9 GB 
[12/03 09:28:08 visual_prompt]: 	Training 200/553. train loss: 0.9075,	0.8359 s / batch. (data: 5.33e-04). ETA=7:08:39, max mem: 20.9 GB 
[12/03 09:30:03 visual_prompt]: 	Training 300/553. train loss: 1.0822,	0.8516 s / batch. (data: 1.17e-02). ETA=7:15:16, max mem: 20.9 GB 
[12/03 09:31:52 visual_prompt]: 	Training 400/553. train loss: 0.8528,	0.8323 s / batch. (data: 3.36e-04). ETA=7:04:01, max mem: 20.9 GB 
[12/03 09:33:48 visual_prompt]: 	Training 500/553. train loss: 1.4698,	0.8513 s / batch. (data: 1.06e-02). ETA=7:12:16, max mem: 20.9 GB 
[12/03 09:34:46 visual_prompt]: Epoch 45 / 100: avg data time: 2.97e-01, avg batch time: 1.1301, average train loss: 0.8101
[12/03 09:35:51 visual_prompt]: Inference (val):avg data time: 6.33e-05, avg batch time: 0.3088, average loss: 0.8189
[12/03 09:35:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.16	
[12/03 09:35:51 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[12/03 09:37:49 visual_prompt]: 	Training 100/553. train loss: 0.7142,	1.4589 s / batch. (data: 6.13e-01). ETA=12:17:06, max mem: 20.9 GB 
[12/03 09:39:41 visual_prompt]: 	Training 200/553. train loss: 0.8873,	0.8418 s / batch. (data: 1.85e-03). ETA=7:03:55, max mem: 20.9 GB 
[12/03 09:41:30 visual_prompt]: 	Training 300/553. train loss: 0.7855,	0.8480 s / batch. (data: 3.51e-04). ETA=7:05:37, max mem: 20.9 GB 
[12/03 09:43:22 visual_prompt]: 	Training 400/553. train loss: 0.7190,	0.8604 s / batch. (data: 2.09e-02). ETA=7:10:26, max mem: 20.9 GB 
[12/03 09:45:11 visual_prompt]: 	Training 500/553. train loss: 1.2554,	0.8314 s / batch. (data: 4.34e-04). ETA=6:54:31, max mem: 20.9 GB 
[12/03 09:46:10 visual_prompt]: Epoch 46 / 100: avg data time: 2.87e-01, avg batch time: 1.1193, average train loss: 0.9142
[12/03 09:47:15 visual_prompt]: Inference (val):avg data time: 6.30e-05, avg batch time: 0.3106, average loss: 0.6800
[12/03 09:47:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.71	
[12/03 09:47:15 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[12/03 09:49:15 visual_prompt]: 	Training 100/553. train loss: 0.8132,	0.8160 s / batch. (data: 3.57e-04). ETA=6:44:46, max mem: 20.9 GB 
[12/03 09:51:02 visual_prompt]: 	Training 200/553. train loss: 0.8544,	1.6314 s / batch. (data: 8.08e-01). ETA=13:26:29, max mem: 20.9 GB 
[12/03 09:52:56 visual_prompt]: 	Training 300/553. train loss: 0.7774,	0.8494 s / batch. (data: 2.80e-03). ETA=6:58:31, max mem: 20.9 GB 
[12/03 09:54:46 visual_prompt]: 	Training 400/553. train loss: 1.1223,	0.8481 s / batch. (data: 2.90e-02). ETA=6:56:25, max mem: 20.9 GB 
[12/03 09:56:37 visual_prompt]: 	Training 500/553. train loss: 2.4051,	0.8171 s / batch. (data: 3.32e-04). ETA=6:39:52, max mem: 20.9 GB 
[12/03 09:57:37 visual_prompt]: Epoch 47 / 100: avg data time: 2.90e-01, avg batch time: 1.1234, average train loss: 0.8894
[12/03 09:58:42 visual_prompt]: Inference (val):avg data time: 4.65e-05, avg batch time: 0.3097, average loss: 0.6911
[12/03 09:58:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 63.68	
[12/03 09:58:42 visual_prompt]: Stopping early.
[12/03 09:58:42 visual_prompt]: Rank of current process: 0. World size: 1
[12/03 09:58:42 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/03 09:58:42 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/03 09:58:42 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/03 09:58:42 visual_prompt]: Training with config:
[12/03 09:58:42 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.5_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/03 09:58:42 visual_prompt]: Loading training data...
[12/03 09:58:42 visual_prompt]: Constructing mammo-cbis dataset train...
[12/03 09:58:42 visual_prompt]: Loading validation data...
[12/03 09:58:42 visual_prompt]: Constructing mammo-cbis dataset val...
[12/03 09:58:42 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[12/03 09:58:45 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[12/03 09:58:45 visual_prompt]: tuned percent:0.525
[12/03 09:58:45 visual_prompt]: Device used for model: 0
[12/03 09:58:45 visual_prompt]: Setting up Evaluator...
[12/03 09:58:45 visual_prompt]: Setting up Trainer...
[12/03 09:58:45 visual_prompt]: 	Setting up the optimizer...
[12/03 09:58:45 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/03 10:00:41 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8287 s / batch. (data: 6.02e-03). ETA=12:42:26, max mem: 20.9 GB 
[12/03 10:02:31 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8320 s / batch. (data: 3.19e-04). ETA=12:44:02, max mem: 20.9 GB 
[12/03 10:04:27 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.2139 s / batch. (data: 1.37e+00). ETA=1 day, 9:49:23, max mem: 20.9 GB 
[12/03 10:06:17 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8372 s / batch. (data: 1.06e-02). ETA=12:45:59, max mem: 20.9 GB 
[12/03 10:08:11 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8560 s / batch. (data: 8.16e-04). ETA=13:01:49, max mem: 20.9 GB 
[12/03 10:09:09 visual_prompt]: Epoch 1 / 100: avg data time: 2.96e-01, avg batch time: 1.1282, average train loss: 1.5403
[12/03 10:10:15 visual_prompt]: Inference (val):avg data time: 5.33e-05, avg batch time: 0.3102, average loss: 1.5201
[12/03 10:10:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[12/03 10:10:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[12/03 10:12:10 visual_prompt]: 	Training 100/553. train loss: 0.7692,	1.1683 s / batch. (data: 3.42e-01). ETA=17:44:04, max mem: 20.9 GB 
[12/03 10:14:00 visual_prompt]: 	Training 200/553. train loss: 0.0948,	1.4038 s / batch. (data: 5.90e-01). ETA=21:16:10, max mem: 20.9 GB 
[12/03 10:15:55 visual_prompt]: 	Training 300/553. train loss: 0.8429,	1.3371 s / batch. (data: 5.16e-01). ETA=20:13:22, max mem: 20.9 GB 
[12/03 10:17:45 visual_prompt]: 	Training 400/553. train loss: 1.5886,	0.8287 s / batch. (data: 3.33e-04). ETA=12:30:35, max mem: 20.9 GB 
[12/03 10:19:39 visual_prompt]: 	Training 500/553. train loss: 0.5426,	0.8442 s / batch. (data: 3.14e-04). ETA=12:43:12, max mem: 20.9 GB 
[12/03 10:20:36 visual_prompt]: Epoch 2 / 100: avg data time: 2.91e-01, avg batch time: 1.1227, average train loss: 0.9154
[12/03 10:21:41 visual_prompt]: Inference (val):avg data time: 5.28e-05, avg batch time: 0.3101, average loss: 1.2315
[12/03 10:21:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.98	
[12/03 10:21:41 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[12/03 10:23:38 visual_prompt]: 	Training 100/553. train loss: 0.7618,	0.8143 s / batch. (data: 3.29e-04). ETA=12:14:11, max mem: 20.9 GB 
[12/03 10:25:31 visual_prompt]: 	Training 200/553. train loss: 0.8719,	0.8237 s / batch. (data: 1.01e-03). ETA=12:21:12, max mem: 20.9 GB 
[12/03 10:27:23 visual_prompt]: 	Training 300/553. train loss: 0.6061,	0.8512 s / batch. (data: 2.07e-02). ETA=12:44:35, max mem: 20.9 GB 
[12/03 10:29:15 visual_prompt]: 	Training 400/553. train loss: 1.8251,	0.8487 s / batch. (data: 1.42e-02). ETA=12:40:54, max mem: 20.9 GB 
[12/03 10:31:09 visual_prompt]: 	Training 500/553. train loss: 0.8345,	1.6800 s / batch. (data: 8.48e-01). ETA=1 day, 1:03:24, max mem: 20.9 GB 
[12/03 10:32:06 visual_prompt]: Epoch 3 / 100: avg data time: 2.96e-01, avg batch time: 1.1296, average train loss: 0.8886
[12/03 10:33:12 visual_prompt]: Inference (val):avg data time: 5.80e-05, avg batch time: 0.3087, average loss: 0.7379
[12/03 10:33:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.87	
[12/03 10:33:12 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[12/03 10:35:12 visual_prompt]: 	Training 100/553. train loss: 0.8040,	0.8597 s / batch. (data: 7.88e-03). ETA=12:47:09, max mem: 20.9 GB 
[12/03 10:37:05 visual_prompt]: 	Training 200/553. train loss: 0.5810,	0.8365 s / batch. (data: 3.51e-04). ETA=12:25:04, max mem: 20.9 GB 
[12/03 10:38:57 visual_prompt]: 	Training 300/553. train loss: 0.9695,	1.7830 s / batch. (data: 9.58e-01). ETA=1 day, 2:25:08, max mem: 20.9 GB 
[12/03 10:40:45 visual_prompt]: 	Training 400/553. train loss: 1.2132,	1.8842 s / batch. (data: 1.05e+00). ETA=1 day, 3:51:54, max mem: 20.9 GB 
[12/03 10:42:40 visual_prompt]: 	Training 500/553. train loss: 0.2903,	4.4980 s / batch. (data: 3.68e+00). ETA=2 days, 18:23:48, max mem: 20.9 GB 
[12/03 10:43:38 visual_prompt]: Epoch 4 / 100: avg data time: 3.00e-01, avg batch time: 1.1331, average train loss: 0.9992
[12/03 10:44:44 visual_prompt]: Inference (val):avg data time: 5.65e-05, avg batch time: 0.3095, average loss: 1.5824
[12/03 10:44:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.18	
[12/03 10:44:44 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[12/03 10:46:40 visual_prompt]: 	Training 100/553. train loss: 2.2033,	0.8240 s / batch. (data: 4.80e-04). ETA=12:07:42, max mem: 20.9 GB 
[12/03 10:48:32 visual_prompt]: 	Training 200/553. train loss: 0.8827,	1.3960 s / batch. (data: 5.57e-01). ETA=20:30:31, max mem: 20.9 GB 
[12/03 10:50:25 visual_prompt]: 	Training 300/553. train loss: 1.2643,	0.8406 s / batch. (data: 5.45e-03). ETA=12:19:35, max mem: 20.9 GB 
[12/03 10:52:16 visual_prompt]: 	Training 400/553. train loss: 1.5535,	0.8426 s / batch. (data: 3.69e-04). ETA=12:19:56, max mem: 20.9 GB 
[12/03 10:54:08 visual_prompt]: 	Training 500/553. train loss: 0.5232,	0.8472 s / batch. (data: 3.45e-04). ETA=12:22:33, max mem: 20.9 GB 
[12/03 10:55:06 visual_prompt]: Epoch 5 / 100: avg data time: 2.91e-01, avg batch time: 1.1248, average train loss: 0.9994
[12/03 10:56:12 visual_prompt]: Inference (val):avg data time: 5.16e-05, avg batch time: 0.3101, average loss: 1.8342
[12/03 10:56:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.22	
[12/03 10:56:12 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[12/03 10:58:10 visual_prompt]: 	Training 100/553. train loss: 0.6791,	0.8252 s / batch. (data: 4.15e-04). ETA=12:01:10, max mem: 20.9 GB 
[12/03 11:00:01 visual_prompt]: 	Training 200/553. train loss: 2.7447,	0.8387 s / batch. (data: 7.94e-03). ETA=12:11:31, max mem: 20.9 GB 
[12/03 11:01:50 visual_prompt]: 	Training 300/553. train loss: 0.5407,	0.8276 s / batch. (data: 5.41e-04). ETA=12:00:27, max mem: 20.9 GB 
[12/03 11:03:46 visual_prompt]: 	Training 400/553. train loss: 0.8629,	0.8240 s / batch. (data: 3.27e-04). ETA=11:55:58, max mem: 20.9 GB 
[12/03 11:05:36 visual_prompt]: 	Training 500/553. train loss: 1.2590,	1.2112 s / batch. (data: 3.81e-01). ETA=17:30:22, max mem: 20.9 GB 
[12/03 11:06:33 visual_prompt]: Epoch 6 / 100: avg data time: 2.91e-01, avg batch time: 1.1233, average train loss: 1.1331
[12/03 11:07:38 visual_prompt]: Inference (val):avg data time: 5.44e-05, avg batch time: 0.3115, average loss: 1.3537
[12/03 11:07:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.22	
[12/03 11:07:38 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[12/03 11:09:34 visual_prompt]: 	Training 100/553. train loss: 2.4068,	0.8210 s / batch. (data: 3.25e-04). ETA=11:49:53, max mem: 20.9 GB 
[12/03 11:11:25 visual_prompt]: 	Training 200/553. train loss: 0.5849,	0.8360 s / batch. (data: 1.19e-02). ETA=12:01:27, max mem: 20.9 GB 
[12/03 11:13:21 visual_prompt]: 	Training 300/553. train loss: 0.6902,	2.4040 s / batch. (data: 1.58e+00). ETA=1 day, 10:30:42, max mem: 20.9 GB 
[12/03 11:15:12 visual_prompt]: 	Training 400/553. train loss: 0.5755,	2.5319 s / batch. (data: 1.70e+00). ETA=1 day, 12:16:40, max mem: 20.9 GB 
[12/03 11:17:01 visual_prompt]: 	Training 500/553. train loss: 1.5788,	0.8306 s / batch. (data: 3.15e-04). ETA=11:52:43, max mem: 20.9 GB 
[12/03 11:17:57 visual_prompt]: Epoch 7 / 100: avg data time: 2.85e-01, avg batch time: 1.1181, average train loss: 1.0505
[12/03 11:19:02 visual_prompt]: Inference (val):avg data time: 5.28e-05, avg batch time: 0.3093, average loss: 0.7697
[12/03 11:19:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.37	
[12/03 11:19:02 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[12/03 11:20:56 visual_prompt]: 	Training 100/553. train loss: 1.1272,	1.1675 s / batch. (data: 3.01e-01). ETA=16:38:44, max mem: 20.9 GB 
[12/03 11:22:50 visual_prompt]: 	Training 200/553. train loss: 0.4748,	0.8300 s / batch. (data: 3.27e-04). ETA=11:48:42, max mem: 20.9 GB 
[12/03 11:24:43 visual_prompt]: 	Training 300/553. train loss: 2.0676,	0.8338 s / batch. (data: 5.79e-03). ETA=11:50:28, max mem: 20.9 GB 
[12/03 11:26:34 visual_prompt]: 	Training 400/553. train loss: 0.6976,	0.8569 s / batch. (data: 8.88e-03). ETA=12:08:48, max mem: 20.9 GB 
[12/03 11:28:27 visual_prompt]: 	Training 500/553. train loss: 2.2300,	1.8483 s / batch. (data: 1.02e+00). ETA=1 day, 2:08:50, max mem: 20.9 GB 
[12/03 11:29:24 visual_prompt]: Epoch 8 / 100: avg data time: 2.92e-01, avg batch time: 1.1243, average train loss: 1.2381
[12/03 11:30:29 visual_prompt]: Inference (val):avg data time: 4.49e-05, avg batch time: 0.3101, average loss: 1.3317
[12/03 11:30:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.58	
[12/03 11:30:29 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[12/03 11:32:25 visual_prompt]: 	Training 100/553. train loss: 0.0375,	0.8383 s / batch. (data: 1.05e-02). ETA=11:49:25, max mem: 20.9 GB 
[12/03 11:34:16 visual_prompt]: 	Training 200/553. train loss: 0.5812,	0.8687 s / batch. (data: 1.76e-02). ETA=12:13:41, max mem: 20.9 GB 
[12/03 11:36:09 visual_prompt]: 	Training 300/553. train loss: 0.9770,	2.2809 s / batch. (data: 1.46e+00). ETA=1 day, 8:02:40, max mem: 20.9 GB 
[12/03 11:38:01 visual_prompt]: 	Training 400/553. train loss: 1.0847,	0.8596 s / batch. (data: 6.52e-03). ETA=12:03:09, max mem: 20.9 GB 
[12/03 11:39:54 visual_prompt]: 	Training 500/553. train loss: 1.1847,	0.8364 s / batch. (data: 3.38e-04). ETA=11:42:12, max mem: 20.9 GB 
[12/03 11:40:50 visual_prompt]: Epoch 9 / 100: avg data time: 2.90e-01, avg batch time: 1.1221, average train loss: 0.9965
[12/03 11:41:55 visual_prompt]: Inference (val):avg data time: 5.01e-05, avg batch time: 0.3084, average loss: 1.3654
[12/03 11:41:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.18	
[12/03 11:41:55 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[12/03 11:43:55 visual_prompt]: 	Training 100/553. train loss: 2.1853,	0.8274 s / batch. (data: 3.32e-04). ETA=11:32:35, max mem: 20.9 GB 
[12/03 11:45:45 visual_prompt]: 	Training 200/553. train loss: 0.7298,	0.8380 s / batch. (data: 3.04e-04). ETA=11:40:01, max mem: 20.9 GB 
[12/03 11:47:36 visual_prompt]: 	Training 300/553. train loss: 0.5715,	0.8446 s / batch. (data: 9.98e-03). ETA=11:44:11, max mem: 20.9 GB 
[12/03 11:49:24 visual_prompt]: 	Training 400/553. train loss: 1.1998,	0.8483 s / batch. (data: 1.56e-02). ETA=11:45:51, max mem: 20.9 GB 
[12/03 11:51:18 visual_prompt]: 	Training 500/553. train loss: 0.5270,	0.8537 s / batch. (data: 9.97e-03). ETA=11:48:54, max mem: 20.9 GB 
[12/03 11:52:15 visual_prompt]: Epoch 10 / 100: avg data time: 2.88e-01, avg batch time: 1.1209, average train loss: 1.3032
[12/03 11:53:20 visual_prompt]: Inference (val):avg data time: 5.16e-05, avg batch time: 0.3100, average loss: 0.7288
[12/03 11:53:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.18	
[12/03 11:53:20 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[12/03 11:55:19 visual_prompt]: 	Training 100/553. train loss: 2.2260,	0.8320 s / batch. (data: 3.08e-04). ETA=11:28:45, max mem: 20.9 GB 
[12/03 11:57:22 visual_prompt]: 	Training 200/553. train loss: 1.4518,	0.8353 s / batch. (data: 1.28e-03). ETA=11:30:04, max mem: 20.9 GB 
[12/03 11:59:51 visual_prompt]: 	Training 300/553. train loss: 0.0190,	0.8279 s / batch. (data: 3.23e-04). ETA=11:22:37, max mem: 20.9 GB 
[12/03 12:02:39 visual_prompt]: 	Training 400/553. train loss: 0.8589,	0.8381 s / batch. (data: 8.49e-04). ETA=11:29:38, max mem: 20.9 GB 
[12/03 12:05:12 visual_prompt]: 	Training 500/553. train loss: 0.7390,	0.8111 s / batch. (data: 4.25e-04). ETA=11:06:00, max mem: 20.9 GB 
[12/03 12:06:33 visual_prompt]: Epoch 11 / 100: avg data time: 6.08e-01, avg batch time: 1.4340, average train loss: 1.0203
[12/03 12:08:10 visual_prompt]: Inference (val):avg data time: 7.77e-05, avg batch time: 0.3056, average loss: 1.1206
[12/03 12:08:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.96	
[12/03 12:08:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[12/03 12:10:14 visual_prompt]: 	Training 100/553. train loss: 0.9176,	1.9148 s / batch. (data: 1.07e+00). ETA=1 day, 2:07:27, max mem: 20.9 GB 
[12/03 12:12:19 visual_prompt]: 	Training 200/553. train loss: 0.5466,	0.8285 s / batch. (data: 3.27e-04). ETA=11:16:51, max mem: 20.9 GB 
[12/03 12:14:34 visual_prompt]: 	Training 300/553. train loss: 0.4498,	0.8600 s / batch. (data: 3.84e-04). ETA=11:41:07, max mem: 20.9 GB 
[12/03 12:16:40 visual_prompt]: 	Training 400/553. train loss: 0.6596,	0.8242 s / batch. (data: 8.22e-04). ETA=11:10:34, max mem: 20.9 GB 
[12/03 12:18:42 visual_prompt]: 	Training 500/553. train loss: 4.8971,	0.8388 s / batch. (data: 3.38e-04). ETA=11:21:02, max mem: 20.9 GB 
[12/03 12:19:49 visual_prompt]: Epoch 12 / 100: avg data time: 4.36e-01, avg batch time: 1.2641, average train loss: 1.0609
[12/03 12:20:55 visual_prompt]: Inference (val):avg data time: 5.62e-05, avg batch time: 0.3099, average loss: 2.9049
[12/03 12:20:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.47	
[12/03 12:20:55 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[12/03 12:22:56 visual_prompt]: 	Training 100/553. train loss: 0.6522,	0.8410 s / batch. (data: 3.24e-04). ETA=11:20:41, max mem: 20.9 GB 
[12/03 12:24:47 visual_prompt]: 	Training 200/553. train loss: 0.4967,	0.8311 s / batch. (data: 4.13e-04). ETA=11:11:16, max mem: 20.9 GB 
[12/03 12:26:42 visual_prompt]: 	Training 300/553. train loss: 0.4334,	2.2816 s / batch. (data: 1.45e+00). ETA=1 day, 6:39:05, max mem: 20.9 GB 
[12/03 12:28:32 visual_prompt]: 	Training 400/553. train loss: 4.0432,	0.8650 s / batch. (data: 2.15e-02). ETA=11:35:46, max mem: 20.9 GB 
[12/03 12:30:25 visual_prompt]: 	Training 500/553. train loss: 1.3895,	0.8400 s / batch. (data: 3.43e-04). ETA=11:14:17, max mem: 20.9 GB 
[12/03 12:31:24 visual_prompt]: Epoch 13 / 100: avg data time: 3.04e-01, avg batch time: 1.1372, average train loss: 1.1305
[12/03 12:32:30 visual_prompt]: Inference (val):avg data time: 5.20e-05, avg batch time: 0.3094, average loss: 0.6267
[12/03 12:32:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 69.49	
[12/03 12:32:30 visual_prompt]: Best epoch 13: best metric: -0.627
[12/03 12:32:30 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[12/03 12:34:28 visual_prompt]: 	Training 100/553. train loss: 0.6649,	0.8166 s / batch. (data: 3.81e-04). ETA=10:53:26, max mem: 20.9 GB 
[12/03 12:36:21 visual_prompt]: 	Training 200/553. train loss: 0.1900,	1.3800 s / batch. (data: 5.15e-01). ETA=18:21:57, max mem: 20.9 GB 
[12/03 12:38:14 visual_prompt]: 	Training 300/553. train loss: 0.4326,	1.0589 s / batch. (data: 2.36e-01). ETA=14:03:47, max mem: 20.9 GB 
[12/03 12:40:05 visual_prompt]: 	Training 400/553. train loss: 0.7925,	0.8163 s / batch. (data: 3.16e-04). ETA=10:49:07, max mem: 20.9 GB 
[12/03 12:41:59 visual_prompt]: 	Training 500/553. train loss: 3.2284,	0.8247 s / batch. (data: 7.74e-03). ETA=10:54:26, max mem: 20.9 GB 
[12/03 12:42:56 visual_prompt]: Epoch 14 / 100: avg data time: 2.98e-01, avg batch time: 1.1325, average train loss: 1.0063
[12/03 12:44:03 visual_prompt]: Inference (val):avg data time: 7.15e-05, avg batch time: 0.3109, average loss: 0.6575
[12/03 12:44:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 71.26	
[12/03 12:44:03 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[12/03 12:46:00 visual_prompt]: 	Training 100/553. train loss: 0.8357,	0.8220 s / batch. (data: 3.12e-04). ETA=10:50:09, max mem: 20.9 GB 
[12/03 12:47:54 visual_prompt]: 	Training 200/553. train loss: 0.2557,	0.8364 s / batch. (data: 3.29e-04). ETA=11:00:12, max mem: 20.9 GB 
[12/03 12:49:50 visual_prompt]: 	Training 300/553. train loss: 1.0142,	0.8440 s / batch. (data: 3.41e-04). ETA=11:04:45, max mem: 20.9 GB 
[12/03 12:51:41 visual_prompt]: 	Training 400/553. train loss: 0.3907,	0.8315 s / batch. (data: 1.18e-02). ETA=10:53:31, max mem: 20.9 GB 
[12/03 12:53:36 visual_prompt]: 	Training 500/553. train loss: 0.4546,	0.8689 s / batch. (data: 8.87e-03). ETA=11:21:29, max mem: 20.9 GB 
[12/03 12:54:37 visual_prompt]: Epoch 15 / 100: avg data time: 3.12e-01, avg batch time: 1.1459, average train loss: 1.1292
[12/03 12:55:42 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.3101, average loss: 2.1255
[12/03 12:55:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.87	
[12/03 12:55:42 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[12/03 12:57:38 visual_prompt]: 	Training 100/553. train loss: 0.4559,	0.8602 s / batch. (data: 3.17e-04). ETA=11:12:27, max mem: 20.9 GB 
[12/03 12:59:32 visual_prompt]: 	Training 200/553. train loss: 0.6217,	0.8204 s / batch. (data: 3.25e-04). ETA=10:39:59, max mem: 20.9 GB 
[12/03 13:01:25 visual_prompt]: 	Training 300/553. train loss: 1.2673,	0.8385 s / batch. (data: 7.90e-03). ETA=10:52:44, max mem: 20.9 GB 
[12/03 13:03:18 visual_prompt]: 	Training 400/553. train loss: 0.4661,	0.8298 s / batch. (data: 1.05e-02). ETA=10:44:33, max mem: 20.9 GB 
[12/03 13:05:11 visual_prompt]: 	Training 500/553. train loss: 0.7612,	1.9201 s / batch. (data: 1.10e+00). ETA=1 day, 0:48:15, max mem: 20.9 GB 
[12/03 13:06:11 visual_prompt]: Epoch 16 / 100: avg data time: 3.03e-01, avg batch time: 1.1370, average train loss: 0.9279
[12/03 13:07:17 visual_prompt]: Inference (val):avg data time: 5.32e-05, avg batch time: 0.3095, average loss: 0.6113
[12/03 13:07:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 71.95	rocauc: 73.55	
[12/03 13:07:17 visual_prompt]: Best epoch 16: best metric: -0.611
[12/03 13:07:17 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[12/03 13:09:13 visual_prompt]: 	Training 100/553. train loss: 0.2198,	0.8340 s / batch. (data: 3.16e-04). ETA=10:44:15, max mem: 20.9 GB 
[12/03 13:11:09 visual_prompt]: 	Training 200/553. train loss: 3.3822,	0.8483 s / batch. (data: 1.56e-02). ETA=10:53:57, max mem: 20.9 GB 
[12/03 13:13:01 visual_prompt]: 	Training 300/553. train loss: 1.1530,	0.8381 s / batch. (data: 3.03e-04). ETA=10:44:41, max mem: 20.9 GB 
[12/03 13:14:54 visual_prompt]: 	Training 400/553. train loss: 0.7215,	1.5107 s / batch. (data: 6.89e-01). ETA=19:19:29, max mem: 20.9 GB 
[12/03 13:16:46 visual_prompt]: 	Training 500/553. train loss: 1.2809,	2.0099 s / batch. (data: 1.17e+00). ETA=1 day, 1:39:19, max mem: 20.9 GB 
[12/03 13:17:45 visual_prompt]: Epoch 17 / 100: avg data time: 3.02e-01, avg batch time: 1.1352, average train loss: 0.9485
[12/03 13:18:52 visual_prompt]: Inference (val):avg data time: 6.44e-05, avg batch time: 0.3117, average loss: 0.7429
[12/03 13:18:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 70.38	
[12/03 13:18:52 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[12/03 13:20:50 visual_prompt]: 	Training 100/553. train loss: 0.4876,	0.8360 s / batch. (data: 3.09e-04). ETA=10:38:08, max mem: 20.9 GB 
[12/03 13:22:46 visual_prompt]: 	Training 200/553. train loss: 0.6285,	0.8454 s / batch. (data: 8.42e-04). ETA=10:43:52, max mem: 20.9 GB 
[12/03 13:24:39 visual_prompt]: 	Training 300/553. train loss: 0.6465,	0.8292 s / batch. (data: 1.05e-02). ETA=10:30:10, max mem: 20.9 GB 
[12/03 13:26:32 visual_prompt]: 	Training 400/553. train loss: 0.9750,	0.8802 s / batch. (data: 1.19e-02). ETA=11:07:30, max mem: 20.9 GB 
[12/03 13:28:26 visual_prompt]: 	Training 500/553. train loss: 1.3419,	0.8345 s / batch. (data: 3.49e-04). ETA=10:31:24, max mem: 20.9 GB 
[12/03 13:29:23 visual_prompt]: Epoch 18 / 100: avg data time: 3.08e-01, avg batch time: 1.1415, average train loss: 1.0641
[12/03 13:30:29 visual_prompt]: Inference (val):avg data time: 3.66e-04, avg batch time: 0.3112, average loss: 0.7910
[12/03 13:30:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 71.49	
[12/03 13:30:29 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[12/03 13:32:27 visual_prompt]: 	Training 100/553. train loss: 1.1723,	1.2034 s / batch. (data: 3.89e-01). ETA=15:07:27, max mem: 20.9 GB 
[12/03 13:34:20 visual_prompt]: 	Training 200/553. train loss: 0.3619,	0.8238 s / batch. (data: 7.95e-03). ETA=10:19:49, max mem: 20.9 GB 
[12/03 13:36:14 visual_prompt]: 	Training 300/553. train loss: 1.8000,	0.8313 s / batch. (data: 5.25e-04). ETA=10:24:04, max mem: 20.9 GB 
[12/03 13:38:08 visual_prompt]: 	Training 400/553. train loss: 0.2757,	0.8376 s / batch. (data: 1.05e-02). ETA=10:27:25, max mem: 20.9 GB 
[12/03 13:39:55 visual_prompt]: 	Training 500/553. train loss: 0.6552,	0.8598 s / batch. (data: 1.24e-03). ETA=10:42:37, max mem: 20.9 GB 
[12/03 13:40:54 visual_prompt]: Epoch 19 / 100: avg data time: 2.96e-01, avg batch time: 1.1297, average train loss: 0.8206
[12/03 13:41:59 visual_prompt]: Inference (val):avg data time: 5.06e-04, avg batch time: 0.3093, average loss: 2.2406
[12/03 13:41:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 71.49	
[12/03 13:41:59 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[12/03 13:43:54 visual_prompt]: 	Training 100/553. train loss: 0.8022,	0.8170 s / batch. (data: 4.43e-04). ETA=10:08:33, max mem: 20.9 GB 
[12/03 13:45:49 visual_prompt]: 	Training 200/553. train loss: 0.1549,	0.8403 s / batch. (data: 1.04e-02). ETA=10:24:30, max mem: 20.9 GB 
[12/03 13:47:42 visual_prompt]: 	Training 300/553. train loss: 1.8837,	0.8318 s / batch. (data: 1.58e-02). ETA=10:16:49, max mem: 20.9 GB 
[12/03 13:49:34 visual_prompt]: 	Training 400/553. train loss: 0.6205,	0.8405 s / batch. (data: 1.20e-02). ETA=10:21:51, max mem: 20.9 GB 
[12/03 13:51:25 visual_prompt]: 	Training 500/553. train loss: 0.5659,	0.8375 s / batch. (data: 1.97e-03). ETA=10:18:16, max mem: 20.9 GB 
[12/03 13:52:26 visual_prompt]: Epoch 20 / 100: avg data time: 2.99e-01, avg batch time: 1.1324, average train loss: 0.9438
[12/03 13:53:31 visual_prompt]: Inference (val):avg data time: 5.23e-05, avg batch time: 0.3097, average loss: 0.6474
[12/03 13:53:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 72.75	
[12/03 13:53:31 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[12/03 13:55:30 visual_prompt]: 	Training 100/553. train loss: 0.2255,	0.8557 s / batch. (data: 1.20e-02). ETA=10:29:28, max mem: 20.9 GB 
[12/03 13:57:20 visual_prompt]: 	Training 200/553. train loss: 0.2152,	0.8164 s / batch. (data: 3.36e-04). ETA=9:59:12, max mem: 20.9 GB 
[12/03 13:59:11 visual_prompt]: 	Training 300/553. train loss: 1.1154,	1.4600 s / batch. (data: 6.22e-01). ETA=17:49:12, max mem: 20.9 GB 
[12/03 14:01:03 visual_prompt]: 	Training 400/553. train loss: 1.1809,	0.8302 s / batch. (data: 3.60e-04). ETA=10:06:34, max mem: 20.9 GB 
[12/03 14:02:58 visual_prompt]: 	Training 500/553. train loss: 0.6918,	0.8674 s / batch. (data: 8.60e-03). ETA=10:32:22, max mem: 20.9 GB 
[12/03 14:03:55 visual_prompt]: Epoch 21 / 100: avg data time: 2.94e-01, avg batch time: 1.1272, average train loss: 0.9820
[12/03 14:05:00 visual_prompt]: Inference (val):avg data time: 5.44e-05, avg batch time: 0.3099, average loss: 0.6259
[12/03 14:05:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.70	rocauc: 72.51	
[12/03 14:05:00 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[12/03 14:06:58 visual_prompt]: 	Training 100/553. train loss: 1.2514,	0.8389 s / batch. (data: 1.20e-02). ETA=10:09:25, max mem: 20.9 GB 
[12/03 14:08:51 visual_prompt]: 	Training 200/553. train loss: 0.4223,	0.8293 s / batch. (data: 3.11e-04). ETA=10:01:05, max mem: 20.9 GB 
[12/03 14:10:42 visual_prompt]: 	Training 300/553. train loss: 0.1473,	0.8327 s / batch. (data: 3.28e-04). ETA=10:02:09, max mem: 20.9 GB 
[12/03 14:12:35 visual_prompt]: 	Training 400/553. train loss: 0.5592,	0.8400 s / batch. (data: 3.25e-04). ETA=10:06:01, max mem: 20.9 GB 
[12/03 14:14:28 visual_prompt]: 	Training 500/553. train loss: 0.7561,	0.8477 s / batch. (data: 1.56e-03). ETA=10:10:10, max mem: 20.9 GB 
[12/03 14:15:28 visual_prompt]: Epoch 22 / 100: avg data time: 3.00e-01, avg batch time: 1.1344, average train loss: 0.8482
[12/03 14:16:33 visual_prompt]: Inference (val):avg data time: 5.43e-05, avg batch time: 0.3135, average loss: 0.6143
[12/03 14:16:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.73	rocauc: 72.60	
[12/03 14:16:33 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[12/03 14:18:33 visual_prompt]: 	Training 100/553. train loss: 1.4942,	0.8454 s / batch. (data: 7.95e-03). ETA=10:06:22, max mem: 20.9 GB 
[12/03 14:20:27 visual_prompt]: 	Training 200/553. train loss: 1.7507,	1.0091 s / batch. (data: 1.81e-01). ETA=12:02:04, max mem: 20.9 GB 
[12/03 14:22:21 visual_prompt]: 	Training 300/553. train loss: 0.6437,	0.8725 s / batch. (data: 1.21e-03). ETA=10:22:51, max mem: 20.9 GB 
[12/03 14:24:12 visual_prompt]: 	Training 400/553. train loss: 0.6500,	0.8303 s / batch. (data: 3.25e-04). ETA=9:51:21, max mem: 20.9 GB 
[12/03 14:26:02 visual_prompt]: 	Training 500/553. train loss: 1.2989,	0.8435 s / batch. (data: 1.47e-03). ETA=9:59:20, max mem: 20.9 GB 
[12/03 14:27:00 visual_prompt]: Epoch 23 / 100: avg data time: 3.01e-01, avg batch time: 1.1344, average train loss: 0.9132
[12/03 14:28:06 visual_prompt]: Inference (val):avg data time: 3.25e-04, avg batch time: 0.3097, average loss: 0.6851
[12/03 14:28:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 71.78	
[12/03 14:28:06 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[12/03 14:30:02 visual_prompt]: 	Training 100/553. train loss: 0.6328,	0.8480 s / batch. (data: 5.51e-03). ETA=10:00:22, max mem: 20.9 GB 
[12/03 14:31:55 visual_prompt]: 	Training 200/553. train loss: 0.9672,	0.8509 s / batch. (data: 8.39e-04). ETA=10:01:00, max mem: 20.9 GB 
[12/03 14:33:49 visual_prompt]: 	Training 300/553. train loss: 0.3380,	1.3419 s / batch. (data: 5.15e-01). ETA=15:45:37, max mem: 20.9 GB 
[12/03 14:35:43 visual_prompt]: 	Training 400/553. train loss: 0.2306,	0.8359 s / batch. (data: 6.55e-04). ETA=9:47:40, max mem: 20.9 GB 
[12/03 14:37:37 visual_prompt]: 	Training 500/553. train loss: 0.4359,	0.8202 s / batch. (data: 3.57e-04). ETA=9:35:13, max mem: 20.9 GB 
[12/03 14:38:38 visual_prompt]: Epoch 24 / 100: avg data time: 3.08e-01, avg batch time: 1.1423, average train loss: 0.9179
[12/03 14:39:44 visual_prompt]: Inference (val):avg data time: 5.00e-05, avg batch time: 0.3091, average loss: 1.2372
[12/03 14:39:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 71.69	
[12/03 14:39:44 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[12/03 14:41:46 visual_prompt]: 	Training 100/553. train loss: 1.0130,	0.8302 s / batch. (data: 7.86e-03). ETA=9:40:09, max mem: 20.9 GB 
[12/03 14:43:35 visual_prompt]: 	Training 200/553. train loss: 0.8323,	0.8200 s / batch. (data: 3.44e-04). ETA=9:31:38, max mem: 20.9 GB 
[12/03 14:45:28 visual_prompt]: 	Training 300/553. train loss: 0.5036,	0.8336 s / batch. (data: 7.98e-03). ETA=9:39:42, max mem: 20.9 GB 
[12/03 14:47:19 visual_prompt]: 	Training 400/553. train loss: 0.7873,	1.6400 s / batch. (data: 8.14e-01). ETA=18:57:50, max mem: 20.9 GB 
[12/03 14:49:12 visual_prompt]: 	Training 500/553. train loss: 0.7543,	2.0500 s / batch. (data: 1.24e+00). ETA=23:38:54, max mem: 20.9 GB 
[12/03 14:50:10 visual_prompt]: Epoch 25 / 100: avg data time: 2.99e-01, avg batch time: 1.1320, average train loss: 0.9234
[12/03 14:51:16 visual_prompt]: Inference (val):avg data time: 6.07e-05, avg batch time: 0.3104, average loss: 1.3822
[12/03 14:51:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 70.73	
[12/03 14:51:16 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[12/03 14:53:12 visual_prompt]: 	Training 100/553. train loss: 0.0206,	0.8458 s / batch. (data: 9.86e-03). ETA=9:43:14, max mem: 20.9 GB 
[12/03 14:55:09 visual_prompt]: 	Training 200/553. train loss: 0.7280,	2.3621 s / batch. (data: 1.55e+00). ETA=1 day, 3:04:56, max mem: 20.9 GB 
[12/03 14:57:05 visual_prompt]: 	Training 300/553. train loss: 1.0639,	0.8404 s / batch. (data: 3.14e-04). ETA=9:36:44, max mem: 20.9 GB 
[12/03 14:58:59 visual_prompt]: 	Training 400/553. train loss: 0.8325,	0.8478 s / batch. (data: 6.56e-04). ETA=9:40:24, max mem: 20.9 GB 
[12/03 15:00:51 visual_prompt]: 	Training 500/553. train loss: 0.1406,	0.8480 s / batch. (data: 7.95e-03). ETA=9:39:06, max mem: 20.9 GB 
[12/03 15:01:49 visual_prompt]: Epoch 26 / 100: avg data time: 3.10e-01, avg batch time: 1.1445, average train loss: 0.8193
[12/03 15:02:58 visual_prompt]: Inference (val):avg data time: 2.59e-04, avg batch time: 0.3133, average loss: 0.6250
[12/03 15:02:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.92	rocauc: 70.22	
[12/03 15:02:58 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[12/03 15:04:58 visual_prompt]: 	Training 100/553. train loss: 0.3142,	0.8388 s / batch. (data: 3.15e-04). ETA=9:30:42, max mem: 20.9 GB 
[12/03 15:06:55 visual_prompt]: 	Training 200/553. train loss: 0.3061,	2.1238 s / batch. (data: 1.27e+00). ETA=1 day, 0:01:26, max mem: 20.9 GB 
[12/03 15:08:51 visual_prompt]: 	Training 300/553. train loss: 0.3831,	0.9194 s / batch. (data: 8.43e-02). ETA=10:22:29, max mem: 20.9 GB 
[12/03 15:10:49 visual_prompt]: 	Training 400/553. train loss: 0.3719,	0.8722 s / batch. (data: 1.56e-02). ETA=9:49:03, max mem: 20.9 GB 
[12/03 15:12:43 visual_prompt]: 	Training 500/553. train loss: 0.5156,	0.8414 s / batch. (data: 3.21e-04). ETA=9:26:50, max mem: 20.9 GB 
[12/03 15:13:40 visual_prompt]: Epoch 27 / 100: avg data time: 3.26e-01, avg batch time: 1.1610, average train loss: 0.7656
[12/03 15:14:48 visual_prompt]: Inference (val):avg data time: 1.94e-04, avg batch time: 0.3099, average loss: 0.9014
[12/03 15:14:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 70.51	
[12/03 15:14:48 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[12/03 15:16:45 visual_prompt]: 	Training 100/553. train loss: 0.4112,	0.8598 s / batch. (data: 1.76e-03). ETA=9:37:01, max mem: 20.9 GB 
[12/03 15:18:40 visual_prompt]: 	Training 200/553. train loss: 0.1149,	0.8400 s / batch. (data: 3.15e-04). ETA=9:22:22, max mem: 20.9 GB 
[12/03 15:20:36 visual_prompt]: 	Training 300/553. train loss: 0.6643,	2.4240 s / batch. (data: 1.60e+00). ETA=1 day, 2:58:48, max mem: 20.9 GB 
[12/03 15:22:29 visual_prompt]: 	Training 400/553. train loss: 0.4057,	0.8519 s / batch. (data: 3.06e-04). ETA=9:27:28, max mem: 20.9 GB 
[12/03 15:24:22 visual_prompt]: 	Training 500/553. train loss: 1.3231,	0.8396 s / batch. (data: 1.20e-02). ETA=9:17:54, max mem: 20.9 GB 
[12/03 15:25:23 visual_prompt]: Epoch 28 / 100: avg data time: 3.13e-01, avg batch time: 1.1484, average train loss: 0.8144
[12/03 15:26:35 visual_prompt]: Inference (val):avg data time: 6.70e-05, avg batch time: 0.3116, average loss: 0.7458
[12/03 15:26:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 73.77	
[12/03 15:26:36 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[12/03 15:28:51 visual_prompt]: 	Training 100/553. train loss: 0.4519,	0.8577 s / batch. (data: 3.74e-03). ETA=9:27:43, max mem: 20.9 GB 
[12/03 15:30:51 visual_prompt]: 	Training 200/553. train loss: 0.8511,	2.3627 s / batch. (data: 1.53e+00). ETA=1 day, 1:59:59, max mem: 20.9 GB 
[12/03 15:32:54 visual_prompt]: 	Training 300/553. train loss: 0.5304,	0.8604 s / batch. (data: 1.04e-02). ETA=9:26:37, max mem: 20.9 GB 
[12/03 15:34:56 visual_prompt]: 	Training 400/553. train loss: 1.2682,	2.3023 s / batch. (data: 1.48e+00). ETA=1 day, 1:12:26, max mem: 20.9 GB 
[12/03 15:37:06 visual_prompt]: 	Training 500/553. train loss: 0.4637,	0.8317 s / batch. (data: 9.70e-04). ETA=9:04:57, max mem: 20.9 GB 
[12/03 15:38:12 visual_prompt]: Epoch 29 / 100: avg data time: 4.28e-01, avg batch time: 1.2585, average train loss: 0.7951
[12/03 15:39:26 visual_prompt]: Inference (val):avg data time: 6.98e-05, avg batch time: 0.3104, average loss: 0.8444
[12/03 15:39:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 71.14	
[12/03 15:39:26 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[12/03 15:41:24 visual_prompt]: 	Training 100/553. train loss: 0.6093,	0.8472 s / batch. (data: 3.33e-04). ETA=9:12:59, max mem: 20.9 GB 
[12/03 15:43:21 visual_prompt]: 	Training 200/553. train loss: 0.7525,	2.0537 s / batch. (data: 1.22e+00). ETA=22:17:04, max mem: 20.9 GB 
[12/03 15:45:16 visual_prompt]: 	Training 300/553. train loss: 0.3053,	2.9605 s / batch. (data: 2.12e+00). ETA=1 day, 8:02:30, max mem: 20.9 GB 
[12/03 15:47:11 visual_prompt]: 	Training 400/553. train loss: 0.9730,	1.5855 s / batch. (data: 7.54e-01). ETA=17:06:55, max mem: 20.9 GB 
[12/03 15:49:04 visual_prompt]: 	Training 500/553. train loss: 0.2157,	1.9479 s / batch. (data: 1.11e+00). ETA=20:58:24, max mem: 20.9 GB 
[12/03 15:50:05 visual_prompt]: Epoch 30 / 100: avg data time: 3.21e-01, avg batch time: 1.1543, average train loss: 0.8042
[12/03 15:51:11 visual_prompt]: Inference (val):avg data time: 4.84e-04, avg batch time: 0.3117, average loss: 0.7949
[12/03 15:51:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 70.94	
[12/03 15:51:11 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[12/03 15:53:13 visual_prompt]: 	Training 100/553. train loss: 0.5912,	0.8338 s / batch. (data: 1.09e-02). ETA=8:56:32, max mem: 20.9 GB 
[12/03 15:55:08 visual_prompt]: 	Training 200/553. train loss: 2.0112,	0.8430 s / batch. (data: 5.44e-03). ETA=9:01:04, max mem: 20.9 GB 
[12/03 15:57:00 visual_prompt]: 	Training 300/553. train loss: 0.9104,	0.8657 s / batch. (data: 5.62e-03). ETA=9:14:12, max mem: 20.9 GB 
[12/03 15:58:53 visual_prompt]: 	Training 400/553. train loss: 0.8093,	1.4157 s / batch. (data: 5.78e-01). ETA=15:03:56, max mem: 20.9 GB 
[12/03 16:00:48 visual_prompt]: 	Training 500/553. train loss: 0.7329,	0.8280 s / batch. (data: 3.39e-04). ETA=8:47:17, max mem: 20.9 GB 
[12/03 16:01:46 visual_prompt]: Epoch 31 / 100: avg data time: 3.13e-01, avg batch time: 1.1479, average train loss: 0.7667
[12/03 16:02:52 visual_prompt]: Inference (val):avg data time: 5.95e-05, avg batch time: 0.3095, average loss: 0.7274
[12/03 16:02:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 72.06	
[12/03 16:02:52 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[12/03 16:04:52 visual_prompt]: 	Training 100/553. train loss: 0.1021,	0.8285 s / batch. (data: 4.25e-04). ETA=8:45:32, max mem: 20.9 GB 
[12/03 16:06:46 visual_prompt]: 	Training 200/553. train loss: 0.1049,	0.8503 s / batch. (data: 1.57e-02). ETA=8:57:54, max mem: 20.9 GB 
[12/03 16:08:45 visual_prompt]: 	Training 300/553. train loss: 0.8564,	0.8357 s / batch. (data: 3.48e-04). ETA=8:47:18, max mem: 20.9 GB 
[12/03 16:10:39 visual_prompt]: 	Training 400/553. train loss: 0.9576,	0.8448 s / batch. (data: 1.10e-02). ETA=8:51:36, max mem: 20.9 GB 
[12/03 16:12:31 visual_prompt]: 	Training 500/553. train loss: 0.8266,	0.8799 s / batch. (data: 1.65e-02). ETA=9:12:14, max mem: 20.9 GB 
[12/03 16:13:28 visual_prompt]: Epoch 32 / 100: avg data time: 3.15e-01, avg batch time: 1.1498, average train loss: 0.6544
[12/03 16:14:36 visual_prompt]: Inference (val):avg data time: 7.22e-05, avg batch time: 0.3079, average loss: 0.7171
[12/03 16:14:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 68.55	
[12/03 16:14:36 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[12/03 16:16:35 visual_prompt]: 	Training 100/553. train loss: 0.9120,	1.5584 s / batch. (data: 7.29e-01). ETA=16:14:05, max mem: 20.9 GB 
[12/03 16:18:31 visual_prompt]: 	Training 200/553. train loss: 0.3511,	2.5430 s / batch. (data: 1.72e+00). ETA=1 day, 2:25:18, max mem: 20.9 GB 
[12/03 16:20:24 visual_prompt]: 	Training 300/553. train loss: 0.5958,	0.8610 s / batch. (data: 1.13e-02). ETA=8:55:19, max mem: 20.9 GB 
[12/03 16:22:17 visual_prompt]: 	Training 400/553. train loss: 0.5923,	0.8265 s / batch. (data: 1.03e-02). ETA=8:32:29, max mem: 20.9 GB 
[12/03 16:24:08 visual_prompt]: 	Training 500/553. train loss: 0.7544,	1.1719 s / batch. (data: 3.41e-01). ETA=12:04:41, max mem: 20.9 GB 
[12/03 16:25:08 visual_prompt]: Epoch 33 / 100: avg data time: 3.08e-01, avg batch time: 1.1424, average train loss: 0.8177
[12/03 16:26:14 visual_prompt]: Inference (val):avg data time: 6.74e-05, avg batch time: 0.3118, average loss: 0.9955
[12/03 16:26:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.85	rocauc: 70.31	
[12/03 16:26:14 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[12/03 16:28:17 visual_prompt]: 	Training 100/553. train loss: 0.7933,	1.6973 s / batch. (data: 8.71e-01). ETA=17:25:17, max mem: 20.9 GB 
[12/03 16:30:08 visual_prompt]: 	Training 200/553. train loss: 0.8934,	1.0967 s / batch. (data: 2.69e-01). ETA=11:13:36, max mem: 20.9 GB 
[12/03 16:32:03 visual_prompt]: 	Training 300/553. train loss: 0.4763,	1.2986 s / batch. (data: 4.77e-01). ETA=13:15:24, max mem: 20.9 GB 
[12/03 16:33:56 visual_prompt]: 	Training 400/553. train loss: 0.9954,	0.8324 s / batch. (data: 4.79e-04). ETA=8:28:29, max mem: 20.9 GB 
[12/03 16:35:52 visual_prompt]: 	Training 500/553. train loss: 0.1685,	2.0552 s / batch. (data: 1.23e+00). ETA=20:52:00, max mem: 20.9 GB 
[12/03 16:36:48 visual_prompt]: Epoch 34 / 100: avg data time: 3.11e-01, avg batch time: 1.1467, average train loss: 0.7271
[12/03 16:37:55 visual_prompt]: Inference (val):avg data time: 7.58e-05, avg batch time: 0.3094, average loss: 0.7090
[12/03 16:37:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 70.85	
[12/03 16:37:55 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[12/03 16:39:56 visual_prompt]: 	Training 100/553. train loss: 2.2133,	0.8445 s / batch. (data: 1.05e-02). ETA=8:32:16, max mem: 20.9 GB 
[12/03 16:41:50 visual_prompt]: 	Training 200/553. train loss: 1.9629,	0.8254 s / batch. (data: 3.83e-04). ETA=8:19:21, max mem: 20.9 GB 
[12/03 16:43:42 visual_prompt]: 	Training 300/553. train loss: 0.1286,	0.8396 s / batch. (data: 1.19e-02). ETA=8:26:32, max mem: 20.9 GB 
[12/03 16:45:35 visual_prompt]: 	Training 400/553. train loss: 0.1532,	1.1681 s / batch. (data: 3.32e-01). ETA=11:42:46, max mem: 20.9 GB 
[12/03 16:47:27 visual_prompt]: 	Training 500/553. train loss: 0.6169,	1.5280 s / batch. (data: 6.88e-01). ETA=15:16:44, max mem: 20.9 GB 
[12/03 16:48:27 visual_prompt]: Epoch 35 / 100: avg data time: 3.08e-01, avg batch time: 1.1425, average train loss: 0.8161
[12/03 16:49:35 visual_prompt]: Inference (val):avg data time: 7.68e-05, avg batch time: 0.3105, average loss: 0.9243
[12/03 16:49:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 69.81	
[12/03 16:49:35 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[12/03 16:51:33 visual_prompt]: 	Training 100/553. train loss: 1.3371,	0.8483 s / batch. (data: 7.64e-03). ETA=8:26:46, max mem: 20.9 GB 
[12/03 16:53:29 visual_prompt]: 	Training 200/553. train loss: 1.1950,	0.9273 s / batch. (data: 5.29e-02). ETA=9:12:27, max mem: 20.9 GB 
[12/03 16:55:27 visual_prompt]: 	Training 300/553. train loss: 0.0691,	0.8341 s / batch. (data: 8.61e-03). ETA=8:15:31, max mem: 20.9 GB 
[12/03 16:57:22 visual_prompt]: 	Training 400/553. train loss: 1.6059,	0.8656 s / batch. (data: 3.40e-04). ETA=8:32:47, max mem: 20.9 GB 
[12/03 16:59:18 visual_prompt]: 	Training 500/553. train loss: 0.3909,	1.5519 s / batch. (data: 7.30e-01). ETA=15:16:46, max mem: 20.9 GB 
[12/03 17:00:15 visual_prompt]: Epoch 36 / 100: avg data time: 3.21e-01, avg batch time: 1.1569, average train loss: 0.6941
[12/03 17:01:23 visual_prompt]: Inference (val):avg data time: 9.10e-05, avg batch time: 0.3118, average loss: 0.6989
[12/03 17:01:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 70.89	
[12/03 17:01:23 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[12/03 17:03:23 visual_prompt]: 	Training 100/553. train loss: 0.7689,	0.8207 s / batch. (data: 3.36e-04). ETA=8:02:43, max mem: 20.9 GB 
[12/03 17:05:17 visual_prompt]: 	Training 200/553. train loss: 0.2522,	0.8166 s / batch. (data: 3.60e-04). ETA=7:58:58, max mem: 20.9 GB 
[12/03 17:07:12 visual_prompt]: 	Training 300/553. train loss: 1.9160,	2.0878 s / batch. (data: 1.25e+00). ETA=20:21:03, max mem: 20.9 GB 
[12/03 17:09:09 visual_prompt]: 	Training 400/553. train loss: 0.7771,	2.0253 s / batch. (data: 1.19e+00). ETA=19:41:08, max mem: 20.9 GB 
[12/03 17:10:59 visual_prompt]: 	Training 500/553. train loss: 1.6037,	1.7362 s / batch. (data: 8.91e-01). ETA=16:49:40, max mem: 20.9 GB 
[12/03 17:11:59 visual_prompt]: Epoch 37 / 100: avg data time: 3.15e-01, avg batch time: 1.1508, average train loss: 0.7876
[12/03 17:13:05 visual_prompt]: Inference (val):avg data time: 6.18e-05, avg batch time: 0.3118, average loss: 0.9509
[12/03 17:13:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 72.86	
[12/03 17:13:05 visual_prompt]: Stopping early.
[12/03 17:13:05 visual_prompt]: Rank of current process: 0. World size: 1
[12/03 17:13:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/03 17:13:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/03 17:13:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/03 17:13:05 visual_prompt]: Training with config:
[12/03 17:13:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.25_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/03 17:13:05 visual_prompt]: Loading training data...
[12/03 17:13:05 visual_prompt]: Constructing mammo-cbis dataset train...
[12/03 17:13:05 visual_prompt]: Loading validation data...
[12/03 17:13:05 visual_prompt]: Constructing mammo-cbis dataset val...
[12/03 17:13:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[12/03 17:13:09 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[12/03 17:13:09 visual_prompt]: tuned percent:0.525
[12/03 17:13:09 visual_prompt]: Device used for model: 0
[12/03 17:13:09 visual_prompt]: Setting up Evaluator...
[12/03 17:13:09 visual_prompt]: Setting up Trainer...
[12/03 17:13:09 visual_prompt]: 	Setting up the optimizer...
[12/03 17:13:09 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/03 17:15:07 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8830 s / batch. (data: 1.48e-03). ETA=13:32:23, max mem: 20.9 GB 
[12/03 17:17:02 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8494 s / batch. (data: 2.07e-02). ETA=13:00:01, max mem: 20.9 GB 
[12/03 17:18:59 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.6499 s / batch. (data: 1.81e+00). ETA=1 day, 16:29:05, max mem: 20.9 GB 
[12/03 17:20:50 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8395 s / batch. (data: 1.20e-02). ETA=12:48:08, max mem: 20.9 GB 
[12/03 17:22:45 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8606 s / batch. (data: 6.81e-03). ETA=13:05:58, max mem: 20.9 GB 
[12/03 17:23:45 visual_prompt]: Epoch 1 / 100: avg data time: 3.15e-01, avg batch time: 1.1498, average train loss: 1.5403
[12/03 17:24:50 visual_prompt]: Inference (val):avg data time: 5.42e-05, avg batch time: 0.3090, average loss: 1.5201
[12/03 17:24:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[12/03 17:24:50 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[12/03 17:26:48 visual_prompt]: 	Training 100/553. train loss: 0.7259,	0.8366 s / batch. (data: 9.25e-04). ETA=12:42:00, max mem: 20.9 GB 
[12/03 17:28:41 visual_prompt]: 	Training 200/553. train loss: 0.2324,	0.8273 s / batch. (data: 9.89e-03). ETA=12:32:05, max mem: 20.9 GB 
[12/03 17:30:37 visual_prompt]: 	Training 300/553. train loss: 0.8152,	1.7077 s / batch. (data: 8.69e-01). ETA=1 day, 1:49:37, max mem: 20.9 GB 
[12/03 17:32:30 visual_prompt]: 	Training 400/553. train loss: 0.9185,	0.8507 s / batch. (data: 3.16e-04). ETA=12:50:33, max mem: 20.9 GB 
[12/03 17:34:26 visual_prompt]: 	Training 500/553. train loss: 0.6950,	0.8430 s / batch. (data: 1.06e-02). ETA=12:42:11, max mem: 20.9 GB 
[12/03 17:35:23 visual_prompt]: Epoch 2 / 100: avg data time: 3.10e-01, avg batch time: 1.1444, average train loss: 0.8104
[12/03 17:36:30 visual_prompt]: Inference (val):avg data time: 6.47e-05, avg batch time: 0.3115, average loss: 0.7343
[12/03 17:36:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.96	
[12/03 17:36:30 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[12/03 17:38:27 visual_prompt]: 	Training 100/553. train loss: 0.7299,	1.0757 s / batch. (data: 2.55e-01). ETA=16:09:49, max mem: 20.9 GB 
[12/03 17:40:22 visual_prompt]: 	Training 200/553. train loss: 0.7123,	1.0103 s / batch. (data: 1.74e-01). ETA=15:09:12, max mem: 20.9 GB 
[12/03 17:42:15 visual_prompt]: 	Training 300/553. train loss: 0.5926,	0.8365 s / batch. (data: 8.59e-03). ETA=12:31:21, max mem: 20.9 GB 
[12/03 17:44:10 visual_prompt]: 	Training 400/553. train loss: 0.6351,	0.8284 s / batch. (data: 3.13e-04). ETA=12:22:42, max mem: 20.9 GB 
[12/03 17:46:06 visual_prompt]: 	Training 500/553. train loss: 0.7292,	1.7750 s / batch. (data: 9.58e-01). ETA=1 day, 2:28:24, max mem: 20.9 GB 
[12/03 17:47:03 visual_prompt]: Epoch 3 / 100: avg data time: 3.11e-01, avg batch time: 1.1445, average train loss: 0.7370
[12/03 17:48:10 visual_prompt]: Inference (val):avg data time: 6.71e-05, avg batch time: 0.3108, average loss: 0.7402
[12/03 17:48:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.22	
[12/03 17:48:10 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[12/03 17:50:12 visual_prompt]: 	Training 100/553. train loss: 0.7316,	0.8160 s / batch. (data: 3.40e-04). ETA=12:08:08, max mem: 20.9 GB 
[12/03 17:52:07 visual_prompt]: 	Training 200/553. train loss: 0.7393,	0.8231 s / batch. (data: 1.01e-03). ETA=12:13:08, max mem: 20.9 GB 
[12/03 17:54:03 visual_prompt]: 	Training 300/553. train loss: 0.5749,	2.0240 s / batch. (data: 1.18e+00). ETA=1 day, 5:59:22, max mem: 20.9 GB 
[12/03 17:55:55 visual_prompt]: 	Training 400/553. train loss: 0.6431,	2.0675 s / batch. (data: 1.23e+00). ETA=1 day, 6:34:33, max mem: 20.9 GB 
[12/03 17:57:53 visual_prompt]: 	Training 500/553. train loss: 0.5387,	4.1605 s / batch. (data: 3.35e+00). ETA=2 days, 13:24:55, max mem: 20.9 GB 
[12/03 17:58:54 visual_prompt]: Epoch 4 / 100: avg data time: 3.28e-01, avg batch time: 1.1633, average train loss: 0.7705
[12/03 18:00:02 visual_prompt]: Inference (val):avg data time: 7.36e-05, avg batch time: 0.3101, average loss: 0.7032
[12/03 18:00:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.59	
[12/03 18:00:02 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[12/03 18:02:00 visual_prompt]: 	Training 100/553. train loss: 0.4886,	0.8266 s / batch. (data: 1.05e-02). ETA=12:09:59, max mem: 20.9 GB 
[12/03 18:03:53 visual_prompt]: 	Training 200/553. train loss: 0.5989,	1.1505 s / batch. (data: 3.18e-01). ETA=16:54:05, max mem: 20.9 GB 
[12/03 18:05:51 visual_prompt]: 	Training 300/553. train loss: 1.5031,	0.8523 s / batch. (data: 1.56e-02). ETA=12:29:51, max mem: 20.9 GB 
[12/03 18:07:45 visual_prompt]: 	Training 400/553. train loss: 1.0950,	0.8360 s / batch. (data: 3.37e-04). ETA=12:14:06, max mem: 20.9 GB 
[12/03 18:09:42 visual_prompt]: 	Training 500/553. train loss: 0.6009,	0.8325 s / batch. (data: 2.78e-04). ETA=12:09:39, max mem: 20.9 GB 
[12/03 18:10:45 visual_prompt]: Epoch 5 / 100: avg data time: 3.28e-01, avg batch time: 1.1624, average train loss: 0.7995
[12/03 18:11:54 visual_prompt]: Inference (val):avg data time: 7.60e-05, avg batch time: 0.3111, average loss: 1.1954
[12/03 18:11:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.96	
[12/03 18:11:54 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[12/03 18:13:54 visual_prompt]: 	Training 100/553. train loss: 0.5873,	0.8438 s / batch. (data: 1.88e-03). ETA=12:17:26, max mem: 20.9 GB 
[12/03 18:15:48 visual_prompt]: 	Training 200/553. train loss: 1.4488,	0.8489 s / batch. (data: 1.05e-02). ETA=12:20:28, max mem: 20.9 GB 
[12/03 18:17:39 visual_prompt]: 	Training 300/553. train loss: 0.5597,	0.8573 s / batch. (data: 1.17e-02). ETA=12:26:19, max mem: 20.9 GB 
[12/03 18:19:37 visual_prompt]: 	Training 400/553. train loss: 0.6484,	0.8360 s / batch. (data: 5.51e-03). ETA=12:06:23, max mem: 20.9 GB 
[12/03 18:21:30 visual_prompt]: 	Training 500/553. train loss: 1.0040,	1.2926 s / batch. (data: 4.56e-01). ETA=18:41:01, max mem: 20.9 GB 
[12/03 18:22:29 visual_prompt]: Epoch 6 / 100: avg data time: 3.15e-01, avg batch time: 1.1489, average train loss: 0.8197
[12/03 18:23:37 visual_prompt]: Inference (val):avg data time: 7.41e-05, avg batch time: 0.3089, average loss: 0.7042
[12/03 18:23:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.52	
[12/03 18:23:37 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[12/03 18:25:35 visual_prompt]: 	Training 100/553. train loss: 0.8747,	0.8439 s / batch. (data: 5.83e-03). ETA=12:09:44, max mem: 20.9 GB 
[12/03 18:27:29 visual_prompt]: 	Training 200/553. train loss: 0.6068,	0.8413 s / batch. (data: 3.48e-04). ETA=12:06:04, max mem: 20.9 GB 
[12/03 18:29:26 visual_prompt]: 	Training 300/553. train loss: 0.5748,	2.4878 s / batch. (data: 1.64e+00). ETA=1 day, 11:42:54, max mem: 20.9 GB 
[12/03 18:31:19 visual_prompt]: 	Training 400/553. train loss: 0.5677,	2.3338 s / batch. (data: 1.51e+00). ETA=1 day, 9:26:21, max mem: 20.9 GB 
[12/03 18:33:11 visual_prompt]: 	Training 500/553. train loss: 0.7092,	0.9815 s / batch. (data: 1.52e-01). ETA=14:02:08, max mem: 20.9 GB 
[12/03 18:34:09 visual_prompt]: Epoch 7 / 100: avg data time: 3.07e-01, avg batch time: 1.1422, average train loss: 0.8428
[12/03 18:35:14 visual_prompt]: Inference (val):avg data time: 5.56e-05, avg batch time: 0.3105, average loss: 0.7045
[12/03 18:35:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.13	
[12/03 18:35:14 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[12/03 18:37:11 visual_prompt]: 	Training 100/553. train loss: 1.0183,	0.8429 s / batch. (data: 3.68e-03). ETA=12:01:04, max mem: 20.9 GB 
[12/03 18:39:06 visual_prompt]: 	Training 200/553. train loss: 0.8070,	0.8241 s / batch. (data: 3.57e-04). ETA=11:43:37, max mem: 20.9 GB 
[12/03 18:41:00 visual_prompt]: 	Training 300/553. train loss: 1.3519,	0.8294 s / batch. (data: 3.21e-04). ETA=11:46:44, max mem: 20.9 GB 
[12/03 18:42:53 visual_prompt]: 	Training 400/553. train loss: 0.7351,	0.8403 s / batch. (data: 5.45e-03). ETA=11:54:41, max mem: 20.9 GB 
[12/03 18:44:44 visual_prompt]: 	Training 500/553. train loss: 1.0141,	2.1579 s / batch. (data: 1.34e+00). ETA=1 day, 6:31:38, max mem: 20.9 GB 
[12/03 18:45:42 visual_prompt]: Epoch 8 / 100: avg data time: 3.03e-01, avg batch time: 1.1361, average train loss: 0.9099
[12/03 18:46:47 visual_prompt]: Inference (val):avg data time: 4.67e-05, avg batch time: 0.3111, average loss: 0.6914
[12/03 18:46:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.99	
[12/03 18:46:47 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[12/03 18:48:45 visual_prompt]: 	Training 100/553. train loss: 0.5348,	0.8322 s / batch. (data: 3.31e-04). ETA=11:44:17, max mem: 20.9 GB 
[12/03 18:50:36 visual_prompt]: 	Training 200/553. train loss: 0.7175,	0.8483 s / batch. (data: 1.05e-02). ETA=11:56:27, max mem: 20.9 GB 
[12/03 18:52:30 visual_prompt]: 	Training 300/553. train loss: 0.5529,	2.2029 s / batch. (data: 1.37e+00). ETA=1 day, 6:56:54, max mem: 20.9 GB 
[12/03 18:54:24 visual_prompt]: 	Training 400/553. train loss: 0.6746,	0.8445 s / batch. (data: 1.24e-02). ETA=11:50:26, max mem: 20.9 GB 
[12/03 18:56:16 visual_prompt]: 	Training 500/553. train loss: 0.6847,	1.2688 s / batch. (data: 4.54e-01). ETA=17:45:15, max mem: 20.9 GB 
[12/03 18:57:13 visual_prompt]: Epoch 9 / 100: avg data time: 2.98e-01, avg batch time: 1.1314, average train loss: 0.8434
[12/03 18:58:18 visual_prompt]: Inference (val):avg data time: 4.96e-05, avg batch time: 0.3083, average loss: 0.7232
[12/03 18:58:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.56	
[12/03 18:58:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[12/03 19:00:17 visual_prompt]: 	Training 100/553. train loss: 1.2551,	0.8442 s / batch. (data: 2.39e-02). ETA=11:46:39, max mem: 20.9 GB 
[12/03 19:02:06 visual_prompt]: 	Training 200/553. train loss: 0.9320,	0.8499 s / batch. (data: 2.66e-02). ETA=11:49:57, max mem: 20.9 GB 
[12/03 19:03:56 visual_prompt]: 	Training 300/553. train loss: 0.5720,	0.8451 s / batch. (data: 9.04e-03). ETA=11:44:33, max mem: 20.9 GB 
[12/03 19:05:45 visual_prompt]: 	Training 400/553. train loss: 0.9727,	0.8798 s / batch. (data: 5.32e-02). ETA=12:12:03, max mem: 20.9 GB 
[12/03 19:07:37 visual_prompt]: 	Training 500/553. train loss: 0.6588,	1.2697 s / batch. (data: 4.55e-01). ETA=17:34:17, max mem: 20.9 GB 
[12/03 19:08:36 visual_prompt]: Epoch 10 / 100: avg data time: 2.84e-01, avg batch time: 1.1172, average train loss: 0.9502
[12/03 19:09:41 visual_prompt]: Inference (val):avg data time: 5.70e-05, avg batch time: 0.3099, average loss: 0.7259
[12/03 19:09:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.58	
[12/03 19:09:41 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[12/03 19:11:40 visual_prompt]: 	Training 100/553. train loss: 1.5573,	0.8261 s / batch. (data: 3.27e-04). ETA=11:23:53, max mem: 20.9 GB 
[12/03 19:13:33 visual_prompt]: 	Training 200/553. train loss: 1.3085,	0.8565 s / batch. (data: 3.45e-04). ETA=11:47:34, max mem: 20.9 GB 
[12/03 19:15:24 visual_prompt]: 	Training 300/553. train loss: 0.1110,	1.2439 s / batch. (data: 4.09e-01). ETA=17:05:35, max mem: 20.9 GB 
[12/03 19:17:16 visual_prompt]: 	Training 400/553. train loss: 0.6141,	0.8566 s / batch. (data: 8.37e-04). ETA=11:44:51, max mem: 20.9 GB 
[12/03 19:19:05 visual_prompt]: 	Training 500/553. train loss: 0.8697,	0.8280 s / batch. (data: 9.02e-03). ETA=11:19:54, max mem: 20.9 GB 
[12/03 19:20:01 visual_prompt]: Epoch 11 / 100: avg data time: 2.89e-01, avg batch time: 1.1217, average train loss: 0.8911
[12/03 19:21:07 visual_prompt]: Inference (val):avg data time: 5.04e-05, avg batch time: 0.3088, average loss: 0.7500
[12/03 19:21:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.31	
[12/03 19:21:07 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[12/03 19:23:05 visual_prompt]: 	Training 100/553. train loss: 0.8803,	0.8231 s / batch. (data: 9.70e-03). ETA=11:13:47, max mem: 20.9 GB 
[12/03 19:24:57 visual_prompt]: 	Training 200/553. train loss: 0.5622,	0.9707 s / batch. (data: 1.16e-01). ETA=13:13:00, max mem: 20.9 GB 
[12/03 19:26:48 visual_prompt]: 	Training 300/553. train loss: 0.6052,	0.8320 s / batch. (data: 3.45e-04). ETA=11:18:18, max mem: 20.9 GB 
[12/03 19:28:39 visual_prompt]: 	Training 400/553. train loss: 0.9251,	0.8371 s / batch. (data: 1.20e-02). ETA=11:21:03, max mem: 20.9 GB 
[12/03 19:30:32 visual_prompt]: 	Training 500/553. train loss: 2.1185,	0.8357 s / batch. (data: 8.13e-04). ETA=11:18:33, max mem: 20.9 GB 
[12/03 19:31:29 visual_prompt]: Epoch 12 / 100: avg data time: 2.94e-01, avg batch time: 1.1258, average train loss: 0.9602
[12/03 19:32:34 visual_prompt]: Inference (val):avg data time: 1.91e-04, avg batch time: 0.3086, average loss: 2.1559
[12/03 19:32:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.15	
[12/03 19:32:34 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[12/03 19:34:33 visual_prompt]: 	Training 100/553. train loss: 0.5731,	0.8560 s / batch. (data: 3.30e-04). ETA=11:32:51, max mem: 20.9 GB 
[12/03 19:36:21 visual_prompt]: 	Training 200/553. train loss: 0.6871,	0.8360 s / batch. (data: 7.97e-03). ETA=11:15:16, max mem: 20.9 GB 
[12/03 19:38:13 visual_prompt]: 	Training 300/553. train loss: 0.7941,	1.8403 s / batch. (data: 1.01e+00). ETA=1 day, 0:43:25, max mem: 20.9 GB 
[12/03 19:40:04 visual_prompt]: 	Training 400/553. train loss: 0.5058,	0.8242 s / batch. (data: 3.75e-04). ETA=11:02:59, max mem: 20.9 GB 
[12/03 19:41:56 visual_prompt]: 	Training 500/553. train loss: 1.0339,	0.8437 s / batch. (data: 9.04e-04). ETA=11:17:16, max mem: 20.9 GB 
[12/03 19:42:55 visual_prompt]: Epoch 13 / 100: avg data time: 2.89e-01, avg batch time: 1.1224, average train loss: 0.9720
[12/03 19:44:00 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.3102, average loss: 0.9952
[12/03 19:44:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.03	
[12/03 19:44:00 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[12/03 19:45:57 visual_prompt]: 	Training 100/553. train loss: 0.6778,	0.8262 s / batch. (data: 1.20e-02). ETA=11:01:06, max mem: 20.9 GB 
[12/03 19:47:49 visual_prompt]: 	Training 200/553. train loss: 0.1189,	1.6802 s / batch. (data: 8.17e-01). ETA=22:21:38, max mem: 20.9 GB 
[12/03 19:49:40 visual_prompt]: 	Training 300/553. train loss: 0.6993,	0.8325 s / batch. (data: 2.94e-04). ETA=11:03:23, max mem: 20.9 GB 
[12/03 19:51:32 visual_prompt]: 	Training 400/553. train loss: 0.5773,	0.8278 s / batch. (data: 1.75e-03). ETA=10:58:13, max mem: 20.9 GB 
[12/03 19:53:24 visual_prompt]: 	Training 500/553. train loss: 1.4140,	0.8225 s / batch. (data: 3.21e-04). ETA=10:52:41, max mem: 20.9 GB 
[12/03 19:54:19 visual_prompt]: Epoch 14 / 100: avg data time: 2.88e-01, avg batch time: 1.1199, average train loss: 0.9793
[12/03 19:55:25 visual_prompt]: Inference (val):avg data time: 5.50e-05, avg batch time: 0.3090, average loss: 0.6933
[12/03 19:55:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 56.67	
[12/03 19:55:25 visual_prompt]: Best epoch 14: best metric: -0.693
[12/03 19:55:25 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[12/03 19:57:22 visual_prompt]: 	Training 100/553. train loss: 0.7365,	0.8290 s / batch. (data: 1.60e-02). ETA=10:55:43, max mem: 20.9 GB 
[12/03 19:59:13 visual_prompt]: 	Training 200/553. train loss: 9.0314,	0.8215 s / batch. (data: 3.10e-04). ETA=10:48:24, max mem: 20.9 GB 
[12/03 20:01:07 visual_prompt]: 	Training 300/553. train loss: 0.8628,	0.8229 s / batch. (data: 3.92e-03). ETA=10:48:07, max mem: 20.9 GB 
[12/03 20:02:55 visual_prompt]: 	Training 400/553. train loss: 0.8945,	0.8474 s / batch. (data: 1.13e-02). ETA=11:06:00, max mem: 20.9 GB 
[12/03 20:04:48 visual_prompt]: 	Training 500/553. train loss: 0.5552,	0.8385 s / batch. (data: 3.46e-04). ETA=10:57:39, max mem: 20.9 GB 
[12/03 20:05:48 visual_prompt]: Epoch 15 / 100: avg data time: 2.95e-01, avg batch time: 1.1262, average train loss: 1.0462
[12/03 20:06:52 visual_prompt]: Inference (val):avg data time: 4.75e-05, avg batch time: 0.3098, average loss: 1.8795
[12/03 20:06:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.07	
[12/03 20:06:52 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[12/03 20:08:48 visual_prompt]: 	Training 100/553. train loss: 0.6909,	0.8401 s / batch. (data: 7.96e-03). ETA=10:56:43, max mem: 20.9 GB 
[12/03 20:10:40 visual_prompt]: 	Training 200/553. train loss: 1.1942,	0.8341 s / batch. (data: 2.99e-04). ETA=10:50:40, max mem: 20.9 GB 
[12/03 20:12:32 visual_prompt]: 	Training 300/553. train loss: 1.1520,	0.8480 s / batch. (data: 8.93e-04). ETA=11:00:04, max mem: 20.9 GB 
[12/03 20:14:23 visual_prompt]: 	Training 400/553. train loss: 1.0239,	0.8280 s / batch. (data: 3.28e-04). ETA=10:43:09, max mem: 20.9 GB 
[12/03 20:16:16 visual_prompt]: 	Training 500/553. train loss: 0.9960,	1.9560 s / batch. (data: 1.12e+00). ETA=1 day, 1:16:03, max mem: 20.9 GB 
[12/03 20:17:14 visual_prompt]: Epoch 16 / 100: avg data time: 2.92e-01, avg batch time: 1.1241, average train loss: 0.9705
[12/03 20:18:19 visual_prompt]: Inference (val):avg data time: 3.11e-04, avg batch time: 0.3090, average loss: 0.6910
[12/03 20:18:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.67	
[12/03 20:18:19 visual_prompt]: Best epoch 16: best metric: -0.691
[12/03 20:18:19 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[12/03 20:20:15 visual_prompt]: 	Training 100/553. train loss: 0.5105,	0.8480 s / batch. (data: 3.39e-04). ETA=10:55:08, max mem: 20.9 GB 
[12/03 20:22:09 visual_prompt]: 	Training 200/553. train loss: 3.3185,	0.8390 s / batch. (data: 1.05e-02). ETA=10:46:44, max mem: 20.9 GB 
[12/03 20:23:59 visual_prompt]: 	Training 300/553. train loss: 1.2886,	0.8521 s / batch. (data: 7.97e-03). ETA=10:55:24, max mem: 20.9 GB 
[12/03 20:25:50 visual_prompt]: 	Training 400/553. train loss: 0.7003,	1.7357 s / batch. (data: 8.89e-01). ETA=22:12:13, max mem: 20.9 GB 
[12/03 20:27:42 visual_prompt]: 	Training 500/553. train loss: 1.3119,	1.7656 s / batch. (data: 9.39e-01). ETA=22:32:13, max mem: 20.9 GB 
[12/03 20:28:41 visual_prompt]: Epoch 17 / 100: avg data time: 2.93e-01, avg batch time: 1.1252, average train loss: 1.1423
[12/03 20:29:47 visual_prompt]: Inference (val):avg data time: 6.14e-05, avg batch time: 0.3096, average loss: 0.8402
[12/03 20:29:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.00	
[12/03 20:29:47 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[12/03 20:31:45 visual_prompt]: 	Training 100/553. train loss: 0.6988,	0.8599 s / batch. (data: 3.57e-04). ETA=10:56:23, max mem: 20.9 GB 
[12/03 20:33:41 visual_prompt]: 	Training 200/553. train loss: 0.9375,	0.8360 s / batch. (data: 3.46e-04). ETA=10:36:43, max mem: 20.9 GB 
[12/03 20:35:31 visual_prompt]: 	Training 300/553. train loss: 0.6529,	0.8229 s / batch. (data: 3.38e-04). ETA=10:25:23, max mem: 20.9 GB 
[12/03 20:37:23 visual_prompt]: 	Training 400/553. train loss: 1.2210,	0.8164 s / batch. (data: 2.98e-04). ETA=10:19:03, max mem: 20.9 GB 
[12/03 20:39:13 visual_prompt]: 	Training 500/553. train loss: 0.7179,	0.8163 s / batch. (data: 3.10e-04). ETA=10:17:38, max mem: 20.9 GB 
[12/03 20:40:10 visual_prompt]: Epoch 18 / 100: avg data time: 2.94e-01, avg batch time: 1.1263, average train loss: 0.9786
[12/03 20:41:15 visual_prompt]: Inference (val):avg data time: 6.10e-05, avg batch time: 0.3091, average loss: 0.6882
[12/03 20:41:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.22	
[12/03 20:41:15 visual_prompt]: Best epoch 18: best metric: -0.688
[12/03 20:41:15 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[12/03 20:43:12 visual_prompt]: 	Training 100/553. train loss: 0.7749,	0.8160 s / batch. (data: 3.72e-04). ETA=10:15:20, max mem: 20.9 GB 
[12/03 20:45:06 visual_prompt]: 	Training 200/553. train loss: 0.6822,	0.8755 s / batch. (data: 4.09e-02). ETA=10:58:46, max mem: 20.9 GB 
[12/03 20:46:57 visual_prompt]: 	Training 300/553. train loss: 2.4590,	1.3552 s / batch. (data: 5.00e-01). ETA=16:57:27, max mem: 20.9 GB 
[12/03 20:48:51 visual_prompt]: 	Training 400/553. train loss: 0.5780,	0.8362 s / batch. (data: 1.00e-03). ETA=10:26:23, max mem: 20.9 GB 
[12/03 20:50:39 visual_prompt]: 	Training 500/553. train loss: 0.5742,	0.8333 s / batch. (data: 3.42e-04). ETA=10:22:48, max mem: 20.9 GB 
[12/03 20:51:39 visual_prompt]: Epoch 19 / 100: avg data time: 2.94e-01, avg batch time: 1.1271, average train loss: 0.9127
[12/03 20:52:45 visual_prompt]: Inference (val):avg data time: 4.86e-04, avg batch time: 0.3126, average loss: 1.4988
[12/03 20:52:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.03	
[12/03 20:52:45 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[12/03 20:54:42 visual_prompt]: 	Training 100/553. train loss: 0.6583,	0.8374 s / batch. (data: 5.47e-03). ETA=10:23:43, max mem: 20.9 GB 
[12/03 20:56:38 visual_prompt]: 	Training 200/553. train loss: 0.5545,	0.8658 s / batch. (data: 5.79e-03). ETA=10:43:27, max mem: 20.9 GB 
[12/03 20:58:31 visual_prompt]: 	Training 300/553. train loss: 0.6616,	0.8280 s / batch. (data: 3.29e-04). ETA=10:14:00, max mem: 20.9 GB 
[12/03 21:00:24 visual_prompt]: 	Training 400/553. train loss: 0.5651,	0.8560 s / batch. (data: 7.95e-03). ETA=10:33:19, max mem: 20.9 GB 
[12/03 21:02:14 visual_prompt]: 	Training 500/553. train loss: 0.8121,	0.8409 s / batch. (data: 1.06e-02). ETA=10:20:47, max mem: 20.9 GB 
[12/03 21:03:13 visual_prompt]: Epoch 20 / 100: avg data time: 3.02e-01, avg batch time: 1.1353, average train loss: 1.0325
[12/03 21:04:20 visual_prompt]: Inference (val):avg data time: 7.07e-05, avg batch time: 0.3106, average loss: 0.7373
[12/03 21:04:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.75	
[12/03 21:04:20 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[12/03 21:06:21 visual_prompt]: 	Training 100/553. train loss: 1.2247,	0.8222 s / batch. (data: 5.53e-03). ETA=10:04:52, max mem: 20.9 GB 
[12/03 21:08:13 visual_prompt]: 	Training 200/553. train loss: 0.9440,	0.8478 s / batch. (data: 6.56e-04). ETA=10:22:19, max mem: 20.9 GB 
[12/03 21:10:07 visual_prompt]: 	Training 300/553. train loss: 2.4146,	1.5440 s / batch. (data: 6.94e-01). ETA=18:50:43, max mem: 20.9 GB 
[12/03 21:11:56 visual_prompt]: 	Training 400/553. train loss: 1.4602,	0.8352 s / batch. (data: 3.91e-04). ETA=10:10:14, max mem: 20.9 GB 
[12/03 21:13:50 visual_prompt]: 	Training 500/553. train loss: 0.6879,	0.8156 s / batch. (data: 5.28e-04). ETA=9:54:36, max mem: 20.9 GB 
[12/03 21:14:47 visual_prompt]: Epoch 21 / 100: avg data time: 3.00e-01, avg batch time: 1.1337, average train loss: 1.0103
[12/03 21:15:53 visual_prompt]: Inference (val):avg data time: 6.05e-05, avg batch time: 0.3110, average loss: 0.6883
[12/03 21:15:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.61	
[12/03 21:15:53 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[12/03 21:17:49 visual_prompt]: 	Training 100/553. train loss: 1.0177,	0.8285 s / batch. (data: 8.38e-03). ETA=10:01:50, max mem: 20.9 GB 
[12/03 21:19:42 visual_prompt]: 	Training 200/553. train loss: 0.5729,	0.8165 s / batch. (data: 3.06e-04). ETA=9:51:46, max mem: 20.9 GB 
[12/03 21:21:32 visual_prompt]: 	Training 300/553. train loss: 0.2426,	0.8320 s / batch. (data: 7.98e-03). ETA=10:01:39, max mem: 20.9 GB 
[12/03 21:23:25 visual_prompt]: 	Training 400/553. train loss: 1.6333,	0.8599 s / batch. (data: 7.90e-03). ETA=10:20:20, max mem: 20.9 GB 
[12/03 21:25:29 visual_prompt]: 	Training 500/553. train loss: 0.5899,	0.8440 s / batch. (data: 7.95e-03). ETA=10:07:30, max mem: 20.9 GB 
[12/03 21:27:17 visual_prompt]: Epoch 22 / 100: avg data time: 4.05e-01, avg batch time: 1.2369, average train loss: 0.9236
[12/03 21:29:14 visual_prompt]: Inference (val):avg data time: 5.39e-05, avg batch time: 0.3068, average loss: 0.7018
[12/03 21:29:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.49	
[12/03 21:29:14 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[12/03 21:32:00 visual_prompt]: 	Training 100/553. train loss: 1.0391,	0.8320 s / batch. (data: 5.46e-03). ETA=9:56:45, max mem: 20.9 GB 
[12/03 21:34:34 visual_prompt]: 	Training 200/553. train loss: 1.2116,	0.8220 s / batch. (data: 3.23e-04). ETA=9:48:11, max mem: 20.9 GB 
[12/03 21:37:21 visual_prompt]: 	Training 300/553. train loss: 0.6818,	0.8431 s / batch. (data: 2.77e-03). ETA=10:01:52, max mem: 20.9 GB 
[12/03 21:39:31 visual_prompt]: 	Training 400/553. train loss: 0.6489,	0.8254 s / batch. (data: 5.67e-04). ETA=9:47:54, max mem: 20.9 GB 
[12/03 21:41:29 visual_prompt]: 	Training 500/553. train loss: 2.0812,	2.3119 s / batch. (data: 1.48e+00). ETA=1 day, 3:22:43, max mem: 20.9 GB 
[12/03 21:42:29 visual_prompt]: Epoch 23 / 100: avg data time: 6.13e-01, avg batch time: 1.4376, average train loss: 0.9907
[12/03 21:43:35 visual_prompt]: Inference (val):avg data time: 5.07e-04, avg batch time: 0.3102, average loss: 0.7896
[12/03 21:43:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.53	
[12/03 21:43:35 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[12/03 21:45:31 visual_prompt]: 	Training 100/553. train loss: 1.0404,	0.8257 s / batch. (data: 3.39e-04). ETA=9:44:38, max mem: 20.9 GB 
[12/03 21:47:24 visual_prompt]: 	Training 200/553. train loss: 0.7835,	0.8556 s / batch. (data: 1.18e-02). ETA=10:04:19, max mem: 20.9 GB 
[12/03 21:49:18 visual_prompt]: 	Training 300/553. train loss: 0.7471,	1.6330 s / batch. (data: 8.06e-01). ETA=19:10:43, max mem: 20.9 GB 
[12/03 21:51:12 visual_prompt]: 	Training 400/553. train loss: 0.8219,	0.8199 s / batch. (data: 3.54e-04). ETA=9:36:22, max mem: 20.9 GB 
[12/03 21:53:07 visual_prompt]: 	Training 500/553. train loss: 0.6914,	0.8233 s / batch. (data: 7.93e-03). ETA=9:37:25, max mem: 20.9 GB 
[12/03 21:54:08 visual_prompt]: Epoch 24 / 100: avg data time: 3.12e-01, avg batch time: 1.1448, average train loss: 0.9448
[12/03 21:55:16 visual_prompt]: Inference (val):avg data time: 7.73e-05, avg batch time: 0.3093, average loss: 1.5532
[12/03 21:55:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.72	
[12/03 21:55:16 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[12/03 21:57:19 visual_prompt]: 	Training 100/553. train loss: 1.3448,	0.8517 s / batch. (data: 1.63e-02). ETA=9:55:09, max mem: 20.9 GB 
[12/03 21:59:10 visual_prompt]: 	Training 200/553. train loss: 0.6383,	1.0840 s / batch. (data: 2.51e-01). ETA=12:35:40, max mem: 20.9 GB 
[12/03 22:01:02 visual_prompt]: 	Training 300/553. train loss: 0.7216,	0.8608 s / batch. (data: 1.59e-03). ETA=9:58:38, max mem: 20.9 GB 
[12/03 22:02:55 visual_prompt]: 	Training 400/553. train loss: 0.6739,	1.8351 s / batch. (data: 9.93e-01). ETA=21:13:12, max mem: 20.9 GB 
[12/03 22:04:50 visual_prompt]: 	Training 500/553. train loss: 0.9483,	2.2640 s / batch. (data: 1.43e+00). ETA=1 day, 2:06:57, max mem: 20.9 GB 
[12/03 22:05:49 visual_prompt]: Epoch 25 / 100: avg data time: 3.12e-01, avg batch time: 1.1451, average train loss: 0.9823
[12/03 22:06:56 visual_prompt]: Inference (val):avg data time: 7.38e-05, avg batch time: 0.3122, average loss: 1.9188
[12/03 22:06:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.34	
[12/03 22:06:56 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[12/03 22:08:55 visual_prompt]: 	Training 100/553. train loss: 0.8600,	0.8766 s / batch. (data: 3.21e-02). ETA=10:04:28, max mem: 20.9 GB 
[12/03 22:10:51 visual_prompt]: 	Training 200/553. train loss: 2.3696,	2.2350 s / batch. (data: 1.40e+00). ETA=1 day, 1:37:30, max mem: 20.9 GB 
[12/03 22:12:47 visual_prompt]: 	Training 300/553. train loss: 0.3658,	0.8162 s / batch. (data: 4.04e-04). ETA=9:20:05, max mem: 20.9 GB 
[12/03 22:14:43 visual_prompt]: 	Training 400/553. train loss: 1.1514,	0.8239 s / batch. (data: 9.91e-04). ETA=9:24:00, max mem: 20.9 GB 
[12/03 22:16:35 visual_prompt]: 	Training 500/553. train loss: 1.1956,	0.8440 s / batch. (data: 3.72e-04). ETA=9:36:21, max mem: 20.9 GB 
[12/03 22:17:34 visual_prompt]: Epoch 26 / 100: avg data time: 3.20e-01, avg batch time: 1.1529, average train loss: 0.9255
[12/03 22:18:42 visual_prompt]: Inference (val):avg data time: 2.77e-04, avg batch time: 0.3113, average loss: 0.6932
[12/03 22:18:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.54	
[12/03 22:18:42 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[12/03 22:20:43 visual_prompt]: 	Training 100/553. train loss: 0.5863,	0.8276 s / batch. (data: 1.05e-03). ETA=9:23:03, max mem: 20.9 GB 
[12/03 22:22:37 visual_prompt]: 	Training 200/553. train loss: 2.8200,	2.0496 s / batch. (data: 1.22e+00). ETA=23:11:03, max mem: 20.9 GB 
[12/03 22:24:31 visual_prompt]: 	Training 300/553. train loss: 0.6432,	0.8381 s / batch. (data: 1.34e-03). ETA=9:27:27, max mem: 20.9 GB 
[12/03 22:26:29 visual_prompt]: 	Training 400/553. train loss: 0.6980,	0.8288 s / batch. (data: 8.36e-04). ETA=9:19:46, max mem: 20.9 GB 
[12/03 22:28:23 visual_prompt]: 	Training 500/553. train loss: 0.6486,	0.8170 s / batch. (data: 4.76e-04). ETA=9:10:23, max mem: 20.9 GB 
[12/03 22:29:19 visual_prompt]: Epoch 27 / 100: avg data time: 3.18e-01, avg batch time: 1.1516, average train loss: 0.9265
[12/03 22:30:24 visual_prompt]: Inference (val):avg data time: 5.52e-05, avg batch time: 0.3109, average loss: 1.0420
[12/03 22:30:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.99	
[12/03 22:30:24 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[12/03 22:32:21 visual_prompt]: 	Training 100/553. train loss: 0.5479,	0.8302 s / batch. (data: 3.11e-04). ETA=9:17:10, max mem: 20.9 GB 
[12/03 22:34:15 visual_prompt]: 	Training 200/553. train loss: 1.8982,	0.8335 s / batch. (data: 3.14e-04). ETA=9:18:01, max mem: 20.9 GB 
[12/03 22:36:12 visual_prompt]: 	Training 300/553. train loss: 1.1819,	2.0023 s / batch. (data: 1.15e+00). ETA=22:17:10, max mem: 20.9 GB 
[12/03 22:38:05 visual_prompt]: 	Training 400/553. train loss: 0.6521,	0.8598 s / batch. (data: 7.84e-03). ETA=9:32:44, max mem: 20.9 GB 
[12/03 22:39:58 visual_prompt]: 	Training 500/553. train loss: 2.6740,	0.8447 s / batch. (data: 7.95e-03). ETA=9:21:17, max mem: 20.9 GB 
[12/03 22:40:57 visual_prompt]: Epoch 28 / 100: avg data time: 3.10e-01, avg batch time: 1.1447, average train loss: 1.0242
[12/03 22:42:04 visual_prompt]: Inference (val):avg data time: 3.05e-04, avg batch time: 0.3108, average loss: 0.6964
[12/03 22:42:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.02	
[12/03 22:42:04 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[12/03 22:44:10 visual_prompt]: 	Training 100/553. train loss: 0.6071,	0.8279 s / batch. (data: 3.52e-04). ETA=9:08:02, max mem: 20.9 GB 
[12/03 22:46:03 visual_prompt]: 	Training 200/553. train loss: 0.7267,	2.4914 s / batch. (data: 1.65e+00). ETA=1 day, 3:25:00, max mem: 20.9 GB 
[12/03 22:47:54 visual_prompt]: 	Training 300/553. train loss: 0.9841,	0.8181 s / batch. (data: 7.36e-04). ETA=8:58:46, max mem: 20.9 GB 
[12/03 22:49:43 visual_prompt]: 	Training 400/553. train loss: 1.3184,	1.8685 s / batch. (data: 1.02e+00). ETA=20:27:26, max mem: 20.9 GB 
[12/03 22:51:35 visual_prompt]: 	Training 500/553. train loss: 1.2788,	0.8467 s / batch. (data: 3.06e-04). ETA=9:14:50, max mem: 20.9 GB 
[12/03 22:52:33 visual_prompt]: Epoch 29 / 100: avg data time: 3.03e-01, avg batch time: 1.1371, average train loss: 1.0274
[12/03 22:53:40 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.3104, average loss: 1.1212
[12/03 22:53:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.40	
[12/03 22:53:40 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[12/03 22:55:36 visual_prompt]: 	Training 100/553. train loss: 0.7903,	0.8280 s / batch. (data: 3.06e-04). ETA=9:00:28, max mem: 20.9 GB 
[12/03 22:57:30 visual_prompt]: 	Training 200/553. train loss: 1.1791,	0.8161 s / batch. (data: 3.63e-04). ETA=8:51:19, max mem: 20.9 GB 
[12/03 22:59:22 visual_prompt]: 	Training 300/553. train loss: 0.3502,	2.2120 s / batch. (data: 1.38e+00). ETA=23:56:25, max mem: 20.9 GB 
[12/03 23:01:18 visual_prompt]: 	Training 400/553. train loss: 0.7023,	1.8730 s / batch. (data: 1.06e+00). ETA=20:13:10, max mem: 20.9 GB 
[12/03 23:03:09 visual_prompt]: 	Training 500/553. train loss: 1.4744,	1.8448 s / batch. (data: 1.01e+00). ETA=19:51:51, max mem: 20.9 GB 
[12/03 23:04:09 visual_prompt]: Epoch 30 / 100: avg data time: 3.03e-01, avg batch time: 1.1366, average train loss: 0.9498
[12/03 23:05:14 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.3108, average loss: 0.7131
[12/03 23:05:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.55	
[12/03 23:05:14 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[12/03 23:07:12 visual_prompt]: 	Training 100/553. train loss: 0.6817,	0.8269 s / batch. (data: 3.16e-04). ETA=8:52:05, max mem: 20.9 GB 
[12/03 23:09:07 visual_prompt]: 	Training 200/553. train loss: 1.2767,	0.8312 s / batch. (data: 3.46e-04). ETA=8:53:30, max mem: 20.9 GB 
[12/03 23:10:59 visual_prompt]: 	Training 300/553. train loss: 1.2913,	0.8382 s / batch. (data: 5.51e-03). ETA=8:56:37, max mem: 20.9 GB 
[12/03 23:12:50 visual_prompt]: 	Training 400/553. train loss: 1.1630,	1.4944 s / batch. (data: 6.70e-01). ETA=15:54:08, max mem: 20.9 GB 
[12/03 23:14:44 visual_prompt]: 	Training 500/553. train loss: 0.5872,	0.8459 s / batch. (data: 5.97e-03). ETA=8:58:41, max mem: 20.9 GB 
[12/03 23:15:41 visual_prompt]: Epoch 31 / 100: avg data time: 3.01e-01, avg batch time: 1.1338, average train loss: 0.9798
[12/03 23:16:47 visual_prompt]: Inference (val):avg data time: 2.35e-04, avg batch time: 0.3097, average loss: 0.7027
[12/03 23:16:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.72	
[12/03 23:16:47 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[12/03 23:18:45 visual_prompt]: 	Training 100/553. train loss: 0.5675,	0.8601 s / batch. (data: 9.91e-04). ETA=9:05:33, max mem: 20.9 GB 
[12/03 23:20:38 visual_prompt]: 	Training 200/553. train loss: 0.6059,	0.8407 s / batch. (data: 5.44e-03). ETA=8:51:52, max mem: 20.9 GB 
[12/03 23:22:34 visual_prompt]: 	Training 300/553. train loss: 0.8385,	0.8360 s / batch. (data: 3.41e-04). ETA=8:47:27, max mem: 20.9 GB 
[12/03 23:24:27 visual_prompt]: 	Training 400/553. train loss: 0.8248,	0.8636 s / batch. (data: 1.33e-03). ETA=9:03:26, max mem: 20.9 GB 
[12/03 23:26:18 visual_prompt]: 	Training 500/553. train loss: 0.8322,	0.8301 s / batch. (data: 6.10e-03). ETA=8:40:58, max mem: 20.9 GB 
[12/03 23:27:16 visual_prompt]: Epoch 32 / 100: avg data time: 3.04e-01, avg batch time: 1.1377, average train loss: 0.8916
[12/03 23:28:21 visual_prompt]: Inference (val):avg data time: 5.13e-05, avg batch time: 0.3111, average loss: 0.9532
[12/03 23:28:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.99	
[12/03 23:28:21 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[12/03 23:30:17 visual_prompt]: 	Training 100/553. train loss: 0.2512,	0.8486 s / batch. (data: 3.16e-04). ETA=8:50:26, max mem: 20.9 GB 
[12/03 23:32:17 visual_prompt]: 	Training 200/553. train loss: 1.1786,	2.0765 s / batch. (data: 1.24e+00). ETA=21:34:28, max mem: 20.9 GB 
[12/03 23:34:09 visual_prompt]: 	Training 300/553. train loss: 0.5806,	0.8280 s / batch. (data: 7.96e-03). ETA=8:34:46, max mem: 20.9 GB 
[12/03 23:36:03 visual_prompt]: 	Training 400/553. train loss: 0.6858,	0.8332 s / batch. (data: 7.96e-03). ETA=8:36:37, max mem: 20.9 GB 
[12/03 23:37:54 visual_prompt]: 	Training 500/553. train loss: 0.5713,	1.4200 s / batch. (data: 5.86e-01). ETA=14:38:06, max mem: 20.9 GB 
[12/03 23:38:52 visual_prompt]: Epoch 33 / 100: avg data time: 3.08e-01, avg batch time: 1.1409, average train loss: 0.9588
[12/03 23:39:58 visual_prompt]: Inference (val):avg data time: 5.59e-05, avg batch time: 0.3105, average loss: 0.6867
[12/03 23:39:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 50.84	
[12/03 23:39:58 visual_prompt]: Best epoch 33: best metric: -0.687
[12/03 23:39:58 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[12/03 23:41:57 visual_prompt]: 	Training 100/553. train loss: 0.7128,	0.8284 s / batch. (data: 1.06e-02). ETA=8:30:10, max mem: 20.9 GB 
[12/03 23:43:47 visual_prompt]: 	Training 200/553. train loss: 0.7944,	1.2459 s / batch. (data: 4.15e-01). ETA=12:45:14, max mem: 20.9 GB 
[12/03 23:45:39 visual_prompt]: 	Training 300/553. train loss: 0.7173,	0.8213 s / batch. (data: 5.44e-04). ETA=8:23:02, max mem: 20.9 GB 
[12/03 23:47:33 visual_prompt]: 	Training 400/553. train loss: 1.2249,	0.8495 s / batch. (data: 1.56e-02). ETA=8:38:54, max mem: 20.9 GB 
[12/03 23:49:27 visual_prompt]: 	Training 500/553. train loss: 0.6148,	1.9815 s / batch. (data: 1.15e+00). ETA=20:07:04, max mem: 20.9 GB 
[12/03 23:50:24 visual_prompt]: Epoch 34 / 100: avg data time: 2.99e-01, avg batch time: 1.1320, average train loss: 0.9494
[12/03 23:51:30 visual_prompt]: Inference (val):avg data time: 5.14e-05, avg batch time: 0.3095, average loss: 0.7455
[12/03 23:51:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.72	
[12/03 23:51:30 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[12/03 23:53:30 visual_prompt]: 	Training 100/553. train loss: 3.5173,	0.8225 s / batch. (data: 6.21e-04). ETA=8:18:58, max mem: 20.9 GB 
[12/03 23:55:24 visual_prompt]: 	Training 200/553. train loss: 1.1597,	0.8583 s / batch. (data: 2.22e-02). ETA=8:39:14, max mem: 20.9 GB 
[12/03 23:57:15 visual_prompt]: 	Training 300/553. train loss: 1.4789,	0.8587 s / batch. (data: 1.59e-02). ETA=8:38:03, max mem: 20.9 GB 
[12/03 23:59:07 visual_prompt]: 	Training 400/553. train loss: 1.1841,	1.1932 s / batch. (data: 3.51e-01). ETA=11:57:52, max mem: 20.9 GB 
[12/04 00:00:59 visual_prompt]: 	Training 500/553. train loss: 0.7028,	1.5001 s / batch. (data: 6.15e-01). ETA=15:00:00, max mem: 20.9 GB 
[12/04 00:01:57 visual_prompt]: Epoch 35 / 100: avg data time: 3.02e-01, avg batch time: 1.1346, average train loss: 0.9412
[12/04 00:03:03 visual_prompt]: Inference (val):avg data time: 2.46e-04, avg batch time: 0.3103, average loss: 1.3627
[12/04 00:03:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.48	
[12/04 00:03:04 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[12/04 00:05:01 visual_prompt]: 	Training 100/553. train loss: 1.5465,	1.3492 s / batch. (data: 5.21e-01). ETA=13:26:03, max mem: 20.9 GB 
[12/04 00:06:58 visual_prompt]: 	Training 200/553. train loss: 2.2709,	1.1878 s / batch. (data: 3.59e-01). ETA=11:47:38, max mem: 20.9 GB 
[12/04 00:08:52 visual_prompt]: 	Training 300/553. train loss: 0.2160,	0.8917 s / batch. (data: 1.23e-03). ETA=8:49:43, max mem: 20.9 GB 
[12/04 00:10:43 visual_prompt]: 	Training 400/553. train loss: 0.7566,	0.8482 s / batch. (data: 7.95e-03). ETA=8:22:29, max mem: 20.9 GB 
[12/04 00:12:38 visual_prompt]: 	Training 500/553. train loss: 0.6187,	1.5359 s / batch. (data: 7.06e-01). ETA=15:07:21, max mem: 20.9 GB 
[12/04 00:13:33 visual_prompt]: Epoch 36 / 100: avg data time: 3.04e-01, avg batch time: 1.1378, average train loss: 0.9664
[12/04 00:14:40 visual_prompt]: Inference (val):avg data time: 5.12e-04, avg batch time: 0.3093, average loss: 1.1024
[12/04 00:14:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.88	
[12/04 00:14:40 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[12/04 00:16:40 visual_prompt]: 	Training 100/553. train loss: 0.6950,	0.8578 s / batch. (data: 9.78e-03). ETA=8:24:34, max mem: 20.9 GB 
[12/04 00:18:34 visual_prompt]: 	Training 200/553. train loss: 0.5668,	0.8315 s / batch. (data: 5.41e-04). ETA=8:07:41, max mem: 20.9 GB 
[12/04 00:20:27 visual_prompt]: 	Training 300/553. train loss: 1.7694,	1.0759 s / batch. (data: 2.36e-01). ETA=10:29:14, max mem: 20.9 GB 
[12/04 00:22:25 visual_prompt]: 	Training 400/553. train loss: 0.5756,	2.7624 s / batch. (data: 1.93e+00). ETA=1 day, 2:51:03, max mem: 20.9 GB 
[12/04 00:24:14 visual_prompt]: 	Training 500/553. train loss: 1.1097,	1.6881 s / batch. (data: 8.43e-01). ETA=16:21:42, max mem: 20.9 GB 
[12/04 00:25:14 visual_prompt]: Epoch 37 / 100: avg data time: 3.14e-01, avg batch time: 1.1472, average train loss: 0.9483
[12/04 00:26:21 visual_prompt]: Inference (val):avg data time: 4.86e-04, avg batch time: 0.3135, average loss: 0.6894
[12/04 00:26:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.03	
[12/04 00:26:21 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[12/04 00:28:17 visual_prompt]: 	Training 100/553. train loss: 0.5736,	1.3457 s / batch. (data: 5.16e-01). ETA=12:59:08, max mem: 20.9 GB 
[12/04 00:30:12 visual_prompt]: 	Training 200/553. train loss: 0.7096,	2.0866 s / batch. (data: 1.27e+00). ETA=20:04:38, max mem: 20.9 GB 
[12/04 00:32:06 visual_prompt]: 	Training 300/553. train loss: 0.5716,	0.8285 s / batch. (data: 3.16e-04). ETA=7:56:56, max mem: 20.9 GB 
[12/04 00:33:58 visual_prompt]: 	Training 400/553. train loss: 0.8491,	0.8182 s / batch. (data: 8.12e-04). ETA=7:49:39, max mem: 20.9 GB 
[12/04 00:35:54 visual_prompt]: 	Training 500/553. train loss: 0.9454,	0.8412 s / batch. (data: 3.47e-04). ETA=8:01:27, max mem: 20.9 GB 
[12/04 00:36:52 visual_prompt]: Epoch 38 / 100: avg data time: 3.08e-01, avg batch time: 1.1407, average train loss: 0.9287
[12/04 00:38:00 visual_prompt]: Inference (val):avg data time: 7.23e-05, avg batch time: 0.3104, average loss: 0.9392
[12/04 00:38:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.91	
[12/04 00:38:00 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[12/04 00:39:58 visual_prompt]: 	Training 100/553. train loss: 0.0992,	0.8601 s / batch. (data: 8.45e-04). ETA=8:10:04, max mem: 20.9 GB 
[12/04 00:41:57 visual_prompt]: 	Training 200/553. train loss: 1.3041,	0.8149 s / batch. (data: 4.24e-04). ETA=7:42:55, max mem: 20.9 GB 
[12/04 00:43:55 visual_prompt]: 	Training 300/553. train loss: 0.6713,	0.8191 s / batch. (data: 5.43e-03). ETA=7:43:57, max mem: 20.9 GB 
[12/04 00:45:45 visual_prompt]: 	Training 400/553. train loss: 0.5644,	0.9646 s / batch. (data: 1.38e-01). ETA=9:04:45, max mem: 20.9 GB 
[12/04 00:47:39 visual_prompt]: 	Training 500/553. train loss: 0.6589,	2.2879 s / batch. (data: 1.47e+00). ETA=21:28:19, max mem: 20.9 GB 
[12/04 00:48:36 visual_prompt]: Epoch 39 / 100: avg data time: 3.17e-01, avg batch time: 1.1502, average train loss: 0.8751
[12/04 00:49:42 visual_prompt]: Inference (val):avg data time: 5.20e-05, avg batch time: 0.3107, average loss: 0.8354
[12/04 00:49:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.32	
[12/04 00:49:42 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[12/04 00:51:42 visual_prompt]: 	Training 100/553. train loss: 1.0123,	0.8400 s / batch. (data: 3.38e-04). ETA=7:50:51, max mem: 20.9 GB 
[12/04 00:53:36 visual_prompt]: 	Training 200/553. train loss: 0.5805,	0.8323 s / batch. (data: 1.31e-03). ETA=7:45:11, max mem: 20.9 GB 
[12/04 00:55:30 visual_prompt]: 	Training 300/553. train loss: 1.3167,	0.8462 s / batch. (data: 1.41e-03). ETA=7:51:30, max mem: 20.9 GB 
[12/04 00:57:25 visual_prompt]: 	Training 400/553. train loss: 1.0364,	0.8428 s / batch. (data: 9.56e-04). ETA=7:48:11, max mem: 20.9 GB 
[12/04 00:59:17 visual_prompt]: 	Training 500/553. train loss: 0.4748,	0.8520 s / batch. (data: 1.20e-03). ETA=7:51:53, max mem: 20.9 GB 
[12/04 01:00:19 visual_prompt]: Epoch 40 / 100: avg data time: 3.17e-01, avg batch time: 1.1514, average train loss: 0.9274
[12/04 01:01:25 visual_prompt]: Inference (val):avg data time: 5.91e-05, avg batch time: 0.3096, average loss: 0.6883
[12/04 01:01:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.65	
[12/04 01:01:25 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[12/04 01:03:30 visual_prompt]: 	Training 100/553. train loss: 0.8150,	0.8536 s / batch. (data: 2.44e-02). ETA=7:50:36, max mem: 20.9 GB 
[12/04 01:05:26 visual_prompt]: 	Training 200/553. train loss: 0.8495,	0.8272 s / batch. (data: 8.21e-04). ETA=7:34:39, max mem: 20.9 GB 
[12/04 01:07:20 visual_prompt]: 	Training 300/553. train loss: 0.7076,	0.8605 s / batch. (data: 9.40e-03). ETA=7:51:33, max mem: 20.9 GB 
[12/04 01:09:13 visual_prompt]: 	Training 400/553. train loss: 0.7069,	0.8198 s / batch. (data: 1.47e-03). ETA=7:27:53, max mem: 20.9 GB 
[12/04 01:11:04 visual_prompt]: 	Training 500/553. train loss: 0.9015,	0.8359 s / batch. (data: 5.46e-03). ETA=7:35:16, max mem: 20.9 GB 
[12/04 01:12:00 visual_prompt]: Epoch 41 / 100: avg data time: 3.16e-01, avg batch time: 1.1494, average train loss: 0.9409
[12/04 01:13:07 visual_prompt]: Inference (val):avg data time: 6.17e-05, avg batch time: 0.3090, average loss: 0.7728
[12/04 01:13:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.71	
[12/04 01:13:07 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[12/04 01:15:04 visual_prompt]: 	Training 100/553. train loss: 1.2659,	0.8163 s / batch. (data: 4.26e-04). ETA=7:22:32, max mem: 20.9 GB 
[12/04 01:16:56 visual_prompt]: 	Training 200/553. train loss: 3.4333,	0.8194 s / batch. (data: 4.52e-04). ETA=7:22:49, max mem: 20.9 GB 
[12/04 01:18:50 visual_prompt]: 	Training 300/553. train loss: 2.6118,	0.8320 s / batch. (data: 3.22e-04). ETA=7:28:16, max mem: 20.9 GB 
[12/04 01:20:43 visual_prompt]: 	Training 400/553. train loss: 0.7986,	0.8269 s / batch. (data: 1.04e-02). ETA=7:24:09, max mem: 20.9 GB 
[12/04 01:22:36 visual_prompt]: 	Training 500/553. train loss: 1.3143,	0.8445 s / batch. (data: 3.47e-04). ETA=7:32:09, max mem: 20.9 GB 
[12/04 01:23:35 visual_prompt]: Epoch 42 / 100: avg data time: 3.02e-01, avg batch time: 1.1360, average train loss: 1.0875
[12/04 01:24:42 visual_prompt]: Inference (val):avg data time: 2.64e-04, avg batch time: 0.3118, average loss: 0.7505
[12/04 01:24:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.63	
[12/04 01:24:42 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[12/04 01:26:43 visual_prompt]: 	Training 100/553. train loss: 1.0132,	0.8396 s / batch. (data: 3.43e-04). ETA=7:27:24, max mem: 20.9 GB 
[12/04 01:28:34 visual_prompt]: 	Training 200/553. train loss: 0.8331,	0.8349 s / batch. (data: 1.19e-03). ETA=7:23:31, max mem: 20.9 GB 
[12/04 01:30:24 visual_prompt]: 	Training 300/553. train loss: 0.3701,	0.8439 s / batch. (data: 1.21e-02). ETA=7:26:53, max mem: 20.9 GB 
[12/04 01:32:17 visual_prompt]: 	Training 400/553. train loss: 0.6267,	0.8511 s / batch. (data: 2.93e-02). ETA=7:29:18, max mem: 20.9 GB 
[12/04 01:34:11 visual_prompt]: 	Training 500/553. train loss: 0.6787,	0.8440 s / batch. (data: 3.65e-04). ETA=7:24:08, max mem: 20.9 GB 
[12/04 01:35:11 visual_prompt]: Epoch 43 / 100: avg data time: 3.03e-01, avg batch time: 1.1370, average train loss: 0.8985
[12/04 01:36:17 visual_prompt]: Inference (val):avg data time: 2.65e-04, avg batch time: 0.3106, average loss: 0.8160
[12/04 01:36:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.85	
[12/04 01:36:17 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[12/04 01:38:15 visual_prompt]: 	Training 100/553. train loss: 0.5654,	0.8366 s / batch. (data: 3.50e-04). ETA=7:18:05, max mem: 20.9 GB 
[12/04 01:40:13 visual_prompt]: 	Training 200/553. train loss: 0.5580,	0.8309 s / batch. (data: 3.13e-04). ETA=7:13:43, max mem: 20.9 GB 
[12/04 01:42:05 visual_prompt]: 	Training 300/553. train loss: 0.9899,	0.8322 s / batch. (data: 3.23e-04). ETA=7:13:01, max mem: 20.9 GB 
[12/04 01:43:56 visual_prompt]: 	Training 400/553. train loss: 0.7103,	0.8291 s / batch. (data: 4.98e-04). ETA=7:10:02, max mem: 20.9 GB 
[12/04 01:45:49 visual_prompt]: 	Training 500/553. train loss: 0.8003,	1.1412 s / batch. (data: 2.91e-01). ETA=9:50:00, max mem: 20.9 GB 
[12/04 01:46:48 visual_prompt]: Epoch 44 / 100: avg data time: 3.08e-01, avg batch time: 1.1410, average train loss: 0.8926
[12/04 01:47:55 visual_prompt]: Inference (val):avg data time: 3.04e-04, avg batch time: 0.3107, average loss: 0.9373
[12/04 01:47:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.62	
[12/04 01:47:55 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[12/04 01:49:54 visual_prompt]: 	Training 100/553. train loss: 0.7105,	0.8240 s / batch. (data: 4.84e-04). ETA=7:03:55, max mem: 20.9 GB 
[12/04 01:51:43 visual_prompt]: 	Training 200/553. train loss: 0.5701,	0.8361 s / batch. (data: 1.37e-03). ETA=7:08:44, max mem: 20.9 GB 
[12/04 01:53:39 visual_prompt]: 	Training 300/553. train loss: 1.1851,	0.8282 s / batch. (data: 7.98e-03). ETA=7:03:19, max mem: 20.9 GB 
[12/04 01:55:30 visual_prompt]: 	Training 400/553. train loss: 0.9968,	0.8280 s / batch. (data: 3.44e-04). ETA=7:01:50, max mem: 20.9 GB 
[12/04 01:57:26 visual_prompt]: 	Training 500/553. train loss: 0.6783,	1.0023 s / batch. (data: 1.71e-01). ETA=8:28:58, max mem: 20.9 GB 
[12/04 01:58:25 visual_prompt]: Epoch 45 / 100: avg data time: 3.05e-01, avg batch time: 1.1390, average train loss: 0.8125
[12/04 01:59:30 visual_prompt]: Inference (val):avg data time: 1.33e-04, avg batch time: 0.3101, average loss: 0.8391
[12/04 01:59:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.03	
[12/04 01:59:31 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[12/04 02:01:29 visual_prompt]: 	Training 100/553. train loss: 0.6955,	1.9845 s / batch. (data: 1.16e+00). ETA=16:42:40, max mem: 20.9 GB 
[12/04 02:03:23 visual_prompt]: 	Training 200/553. train loss: 1.2787,	0.8156 s / batch. (data: 3.36e-04). ETA=6:50:42, max mem: 20.9 GB 
[12/04 02:05:12 visual_prompt]: 	Training 300/553. train loss: 0.9567,	0.8503 s / batch. (data: 1.06e-02). ETA=7:06:45, max mem: 20.9 GB 
[12/04 02:07:05 visual_prompt]: 	Training 400/553. train loss: 0.8321,	0.8317 s / batch. (data: 8.50e-04). ETA=6:56:04, max mem: 20.9 GB 
[12/04 02:08:57 visual_prompt]: 	Training 500/553. train loss: 1.9825,	0.8327 s / batch. (data: 3.20e-04). ETA=6:55:08, max mem: 20.9 GB 
[12/04 02:09:58 visual_prompt]: Epoch 46 / 100: avg data time: 3.00e-01, avg batch time: 1.1333, average train loss: 0.9346
[12/04 02:11:04 visual_prompt]: Inference (val):avg data time: 6.79e-05, avg batch time: 0.3098, average loss: 0.8296
[12/04 02:11:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.99	
[12/04 02:11:04 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[12/04 02:13:05 visual_prompt]: 	Training 100/553. train loss: 1.0557,	0.8349 s / batch. (data: 3.64e-04). ETA=6:54:06, max mem: 20.9 GB 
[12/04 02:14:55 visual_prompt]: 	Training 200/553. train loss: 0.6979,	1.0384 s / batch. (data: 2.18e-01). ETA=8:33:20, max mem: 20.9 GB 
[12/04 02:16:52 visual_prompt]: 	Training 300/553. train loss: 0.7272,	0.8310 s / batch. (data: 7.99e-03). ETA=6:49:27, max mem: 20.9 GB 
[12/04 02:18:44 visual_prompt]: 	Training 400/553. train loss: 0.5820,	0.8476 s / batch. (data: 1.50e-03). ETA=6:56:11, max mem: 20.9 GB 
[12/04 02:20:39 visual_prompt]: 	Training 500/553. train loss: 0.6921,	0.8293 s / batch. (data: 3.35e-04). ETA=6:45:51, max mem: 20.9 GB 
[12/04 02:21:39 visual_prompt]: Epoch 47 / 100: avg data time: 3.13e-01, avg batch time: 1.1474, average train loss: 0.8943
[12/04 02:22:47 visual_prompt]: Inference (val):avg data time: 7.63e-05, avg batch time: 0.3095, average loss: 0.7240
[12/04 02:22:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.86	
[12/04 02:22:47 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[12/04 02:24:47 visual_prompt]: 	Training 100/553. train loss: 1.1651,	0.8182 s / batch. (data: 3.50e-04). ETA=6:38:18, max mem: 20.9 GB 
[12/04 02:26:41 visual_prompt]: 	Training 200/553. train loss: 0.6196,	0.8456 s / batch. (data: 1.05e-02). ETA=6:50:14, max mem: 20.9 GB 
[12/04 02:28:38 visual_prompt]: 	Training 300/553. train loss: 0.9385,	2.3360 s / batch. (data: 1.47e+00). ETA=18:49:23, max mem: 20.9 GB 
[12/04 02:30:26 visual_prompt]: 	Training 400/553. train loss: 0.6318,	0.9799 s / batch. (data: 1.61e-01). ETA=7:52:06, max mem: 20.9 GB 
[12/04 02:32:21 visual_prompt]: 	Training 500/553. train loss: 0.7331,	0.8170 s / batch. (data: 8.15e-04). ETA=6:32:15, max mem: 20.9 GB 
[12/04 02:33:18 visual_prompt]: Epoch 48 / 100: avg data time: 3.08e-01, avg batch time: 1.1416, average train loss: 0.9159
[12/04 02:34:24 visual_prompt]: Inference (val):avg data time: 6.73e-05, avg batch time: 0.3101, average loss: 0.6972
[12/04 02:34:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.35	
[12/04 02:34:24 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[12/04 02:36:21 visual_prompt]: 	Training 100/553. train loss: 0.7134,	0.8480 s / batch. (data: 3.31e-04). ETA=6:45:00, max mem: 20.9 GB 
[12/04 02:38:13 visual_prompt]: 	Training 200/553. train loss: 0.6154,	0.8211 s / batch. (data: 9.98e-04). ETA=6:30:46, max mem: 20.9 GB 
[12/04 02:40:07 visual_prompt]: 	Training 300/553. train loss: 1.7606,	0.8240 s / batch. (data: 4.02e-04). ETA=6:30:47, max mem: 20.9 GB 
[12/04 02:42:02 visual_prompt]: 	Training 400/553. train loss: 0.7138,	0.8311 s / batch. (data: 3.38e-04). ETA=6:32:46, max mem: 20.9 GB 
[12/04 02:43:57 visual_prompt]: 	Training 500/553. train loss: 0.6910,	0.8619 s / batch. (data: 6.17e-03). ETA=6:45:52, max mem: 20.9 GB 
[12/04 02:44:55 visual_prompt]: Epoch 49 / 100: avg data time: 3.08e-01, avg batch time: 1.1405, average train loss: 0.8472
[12/04 02:46:02 visual_prompt]: Inference (val):avg data time: 6.46e-05, avg batch time: 0.3103, average loss: 1.0532
[12/04 02:46:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.85	
[12/04 02:46:02 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[12/04 02:48:02 visual_prompt]: 	Training 100/553. train loss: 1.0088,	0.8158 s / batch. (data: 1.09e-03). ETA=6:22:05, max mem: 20.9 GB 
[12/04 02:49:58 visual_prompt]: 	Training 200/553. train loss: 2.4971,	0.8320 s / batch. (data: 1.20e-02). ETA=6:28:17, max mem: 20.9 GB 
[12/04 02:51:49 visual_prompt]: 	Training 300/553. train loss: 0.8133,	0.8430 s / batch. (data: 7.36e-04). ETA=6:32:03, max mem: 20.9 GB 
[12/04 02:53:43 visual_prompt]: 	Training 400/553. train loss: 1.2818,	0.8481 s / batch. (data: 3.33e-04). ETA=6:32:58, max mem: 20.9 GB 
[12/04 02:55:37 visual_prompt]: 	Training 500/553. train loss: 0.7785,	0.8553 s / batch. (data: 6.13e-03). ETA=6:34:54, max mem: 20.9 GB 
[12/04 02:56:36 visual_prompt]: Epoch 50 / 100: avg data time: 3.13e-01, avg batch time: 1.1477, average train loss: 0.8611
[12/04 02:57:42 visual_prompt]: Inference (val):avg data time: 6.28e-05, avg batch time: 0.3100, average loss: 0.9578
[12/04 02:57:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.35	
[12/04 02:57:42 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[12/04 02:59:43 visual_prompt]: 	Training 100/553. train loss: 0.8875,	1.8646 s / batch. (data: 1.02e+00). ETA=14:16:10, max mem: 20.9 GB 
[12/04 03:01:37 visual_prompt]: 	Training 200/553. train loss: 1.0830,	0.8169 s / batch. (data: 3.26e-04). ETA=6:13:44, max mem: 20.9 GB 
[12/04 03:03:32 visual_prompt]: 	Training 300/553. train loss: 0.5626,	0.8325 s / batch. (data: 3.38e-04). ETA=6:19:29, max mem: 20.9 GB 
[12/04 03:05:26 visual_prompt]: 	Training 400/553. train loss: 1.6538,	2.0600 s / batch. (data: 1.20e+00). ETA=15:35:35, max mem: 20.9 GB 
[12/04 03:07:18 visual_prompt]: 	Training 500/553. train loss: 0.6193,	0.8364 s / batch. (data: 3.04e-04). ETA=6:18:27, max mem: 20.9 GB 
[12/04 03:08:16 visual_prompt]: Epoch 51 / 100: avg data time: 3.11e-01, avg batch time: 1.1449, average train loss: 0.8031
[12/04 03:09:22 visual_prompt]: Inference (val):avg data time: 2.31e-04, avg batch time: 0.3109, average loss: 0.7085
[12/04 03:09:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.92	
[12/04 03:09:22 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[12/04 03:11:27 visual_prompt]: 	Training 100/553. train loss: 0.7189,	0.8507 s / batch. (data: 5.96e-03). ETA=6:22:46, max mem: 20.9 GB 
[12/04 03:13:21 visual_prompt]: 	Training 200/553. train loss: 0.7803,	0.8314 s / batch. (data: 1.43e-03). ETA=6:12:40, max mem: 20.9 GB 
[12/04 03:15:16 visual_prompt]: 	Training 300/553. train loss: 1.0228,	0.8197 s / batch. (data: 8.74e-04). ETA=6:06:06, max mem: 20.9 GB 
[12/04 03:17:12 visual_prompt]: 	Training 400/553. train loss: 0.4413,	0.8480 s / batch. (data: 3.27e-04). ETA=6:17:19, max mem: 20.9 GB 
[12/04 03:19:00 visual_prompt]: 	Training 500/553. train loss: 0.0392,	0.8318 s / batch. (data: 6.37e-04). ETA=6:08:43, max mem: 20.9 GB 
[12/04 03:19:59 visual_prompt]: Epoch 52 / 100: avg data time: 3.17e-01, avg batch time: 1.1508, average train loss: 0.8491
[12/04 03:21:06 visual_prompt]: Inference (val):avg data time: 2.82e-04, avg batch time: 0.3111, average loss: 0.6973
[12/04 03:21:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.58	
[12/04 03:21:06 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[12/04 03:23:04 visual_prompt]: 	Training 100/553. train loss: 0.7535,	0.8322 s / batch. (data: 3.03e-04). ETA=6:06:45, max mem: 20.9 GB 
[12/04 03:24:59 visual_prompt]: 	Training 200/553. train loss: 0.5217,	0.8560 s / batch. (data: 8.73e-04). ETA=6:15:50, max mem: 20.9 GB 
[12/04 03:26:52 visual_prompt]: 	Training 300/553. train loss: 0.7859,	0.8242 s / batch. (data: 3.95e-04). ETA=6:00:31, max mem: 20.9 GB 
[12/04 03:28:47 visual_prompt]: 	Training 400/553. train loss: 0.6277,	0.8480 s / batch. (data: 7.96e-03). ETA=6:09:30, max mem: 20.9 GB 
[12/04 03:30:41 visual_prompt]: 	Training 500/553. train loss: 0.8975,	0.8286 s / batch. (data: 1.19e-02). ETA=5:59:39, max mem: 20.9 GB 
[12/04 03:31:42 visual_prompt]: Epoch 53 / 100: avg data time: 3.15e-01, avg batch time: 1.1492, average train loss: 0.8165
[12/04 03:32:47 visual_prompt]: Inference (val):avg data time: 5.96e-05, avg batch time: 0.3114, average loss: 0.7988
[12/04 03:32:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.26	
[12/04 03:32:47 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[12/04 03:34:49 visual_prompt]: 	Training 100/553. train loss: 0.7951,	0.8394 s / batch. (data: 6.59e-03). ETA=6:02:13, max mem: 20.9 GB 
[12/04 03:36:42 visual_prompt]: 	Training 200/553. train loss: 0.8927,	0.8623 s / batch. (data: 6.23e-04). ETA=6:10:39, max mem: 20.9 GB 
[12/04 03:38:32 visual_prompt]: 	Training 300/553. train loss: 0.5611,	1.4390 s / batch. (data: 6.15e-01). ETA=10:16:08, max mem: 20.9 GB 
[12/04 03:40:24 visual_prompt]: 	Training 400/553. train loss: 1.6045,	0.8240 s / batch. (data: 5.64e-04). ETA=5:51:26, max mem: 20.9 GB 
[12/04 03:42:18 visual_prompt]: 	Training 500/553. train loss: 0.5700,	0.8441 s / batch. (data: 7.40e-03). ETA=5:58:36, max mem: 20.9 GB 
[12/04 03:43:17 visual_prompt]: Epoch 54 / 100: avg data time: 3.05e-01, avg batch time: 1.1382, average train loss: 0.8498
[12/04 03:44:22 visual_prompt]: Inference (val):avg data time: 5.08e-05, avg batch time: 0.3105, average loss: 1.3115
[12/04 03:44:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.84	
[12/04 03:44:22 visual_prompt]: Stopping early.
[12/04 03:44:24 visual_prompt]: Rank of current process: 0. World size: 1
[12/04 03:44:24 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/04 03:44:24 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/04 03:44:24 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/04 03:44:24 visual_prompt]: Training with config:
[12/04 03:44:24 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.25_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/04 03:44:24 visual_prompt]: Loading training data...
[12/04 03:44:24 visual_prompt]: Constructing mammo-cbis dataset train...
[12/04 03:44:24 visual_prompt]: Loading validation data...
[12/04 03:44:24 visual_prompt]: Constructing mammo-cbis dataset val...
[12/04 03:44:24 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[12/04 03:44:27 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[12/04 03:44:27 visual_prompt]: tuned percent:0.525
[12/04 03:44:28 visual_prompt]: Device used for model: 0
[12/04 03:44:28 visual_prompt]: Setting up Evaluator...
[12/04 03:44:28 visual_prompt]: Setting up Trainer...
[12/04 03:44:28 visual_prompt]: 	Setting up the optimizer...
[12/04 03:44:28 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/04 03:46:25 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8185 s / batch. (data: 3.12e-04). ETA=12:33:02, max mem: 20.9 GB 
[12/04 03:48:16 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8588 s / batch. (data: 2.19e-02). ETA=13:08:38, max mem: 20.9 GB 
[12/04 03:50:13 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.7356 s / batch. (data: 1.90e+00). ETA=1 day, 17:47:36, max mem: 20.9 GB 
[12/04 03:52:03 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8926 s / batch. (data: 2.06e-02). ETA=13:36:43, max mem: 20.9 GB 
[12/04 03:53:59 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8472 s / batch. (data: 3.64e-04). ETA=12:53:46, max mem: 20.9 GB 
[12/04 03:54:58 visual_prompt]: Epoch 1 / 100: avg data time: 3.07e-01, avg batch time: 1.1403, average train loss: 1.5403
[12/04 03:56:04 visual_prompt]: Inference (val):avg data time: 6.02e-05, avg batch time: 0.3085, average loss: 1.5201
[12/04 03:56:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[12/04 03:56:04 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[12/04 03:58:02 visual_prompt]: 	Training 100/553. train loss: 0.7428,	0.8614 s / batch. (data: 1.05e-02). ETA=13:04:34, max mem: 20.9 GB 
[12/04 03:59:54 visual_prompt]: 	Training 200/553. train loss: 0.2153,	0.8646 s / batch. (data: 5.48e-03). ETA=13:05:59, max mem: 20.9 GB 
[12/04 04:01:50 visual_prompt]: 	Training 300/553. train loss: 0.9035,	1.4828 s / batch. (data: 6.50e-01). ETA=22:25:36, max mem: 20.9 GB 
[12/04 04:03:40 visual_prompt]: 	Training 400/553. train loss: 1.0799,	0.8480 s / batch. (data: 3.66e-04). ETA=12:48:05, max mem: 20.9 GB 
[12/04 04:05:36 visual_prompt]: 	Training 500/553. train loss: 0.6557,	0.8232 s / batch. (data: 7.96e-03). ETA=12:24:16, max mem: 20.9 GB 
[12/04 04:06:34 visual_prompt]: Epoch 2 / 100: avg data time: 3.06e-01, avg batch time: 1.1385, average train loss: 0.8338
[12/04 04:07:39 visual_prompt]: Inference (val):avg data time: 5.33e-05, avg batch time: 0.3087, average loss: 0.7538
[12/04 04:07:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.77	
[12/04 04:07:39 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[12/04 04:09:35 visual_prompt]: 	Training 100/553. train loss: 0.7588,	0.8283 s / batch. (data: 9.65e-03). ETA=12:26:44, max mem: 20.9 GB 
[12/04 04:11:28 visual_prompt]: 	Training 200/553. train loss: 0.6979,	0.8681 s / batch. (data: 4.36e-02). ETA=13:01:09, max mem: 20.9 GB 
[12/04 04:13:21 visual_prompt]: 	Training 300/553. train loss: 0.6494,	0.8628 s / batch. (data: 1.18e-02). ETA=12:54:58, max mem: 20.9 GB 
[12/04 04:15:15 visual_prompt]: 	Training 400/553. train loss: 0.6664,	0.8381 s / batch. (data: 3.41e-04). ETA=12:31:24, max mem: 20.9 GB 
[12/04 04:17:09 visual_prompt]: 	Training 500/553. train loss: 0.7062,	1.8760 s / batch. (data: 1.03e+00). ETA=1 day, 3:58:51, max mem: 20.9 GB 
[12/04 04:18:07 visual_prompt]: Epoch 3 / 100: avg data time: 3.02e-01, avg batch time: 1.1356, average train loss: 0.7698
[12/04 04:19:13 visual_prompt]: Inference (val):avg data time: 6.58e-05, avg batch time: 0.3115, average loss: 0.7097
[12/04 04:19:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 57.19	
[12/04 04:19:13 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[12/04 04:21:13 visual_prompt]: 	Training 100/553. train loss: 0.7156,	0.8233 s / batch. (data: 5.27e-04). ETA=12:14:38, max mem: 20.9 GB 
[12/04 04:23:06 visual_prompt]: 	Training 200/553. train loss: 1.4199,	0.8440 s / batch. (data: 3.46e-04). ETA=12:31:44, max mem: 20.9 GB 
[12/04 04:24:59 visual_prompt]: 	Training 300/553. train loss: 0.6373,	2.3143 s / batch. (data: 1.47e+00). ETA=1 day, 10:17:26, max mem: 20.9 GB 
[12/04 04:26:47 visual_prompt]: 	Training 400/553. train loss: 0.5697,	1.8040 s / batch. (data: 9.76e-01). ETA=1 day, 2:40:46, max mem: 20.9 GB 
[12/04 04:28:42 visual_prompt]: 	Training 500/553. train loss: 0.7092,	4.2596 s / batch. (data: 3.44e+00). ETA=2 days, 14:52:37, max mem: 20.9 GB 
[12/04 04:29:41 visual_prompt]: Epoch 4 / 100: avg data time: 3.03e-01, avg batch time: 1.1353, average train loss: 0.8536
[12/04 04:30:48 visual_prompt]: Inference (val):avg data time: 1.63e-04, avg batch time: 0.3116, average loss: 0.8408
[12/04 04:30:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.80	
[12/04 04:30:48 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[12/04 04:32:45 visual_prompt]: 	Training 100/553. train loss: 0.4815,	0.8263 s / batch. (data: 5.91e-04). ETA=12:09:42, max mem: 20.9 GB 
[12/04 04:34:39 visual_prompt]: 	Training 200/553. train loss: 0.6136,	1.6322 s / batch. (data: 8.09e-01). ETA=23:58:41, max mem: 20.9 GB 
[12/04 04:36:20 visual_prompt]: 	Training 300/553. train loss: 1.3234,	0.8285 s / batch. (data: 2.94e-04). ETA=12:08:55, max mem: 20.9 GB 
[12/04 04:37:55 visual_prompt]: 	Training 400/553. train loss: 1.1613,	0.8200 s / batch. (data: 2.88e-04). ETA=12:00:05, max mem: 20.9 GB 
[12/04 04:39:31 visual_prompt]: 	Training 500/553. train loss: 0.5823,	0.8199 s / batch. (data: 3.12e-04). ETA=11:58:34, max mem: 20.9 GB 
[12/04 04:40:22 visual_prompt]: Epoch 5 / 100: avg data time: 2.05e-01, avg batch time: 1.0379, average train loss: 0.8950
[12/04 04:41:17 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3100, average loss: 0.7901
[12/04 04:41:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.59	
[12/04 04:41:17 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[12/04 04:43:00 visual_prompt]: 	Training 100/553. train loss: 0.5826,	0.8545 s / batch. (data: 7.61e-04). ETA=12:26:45, max mem: 20.9 GB 
[12/04 04:44:36 visual_prompt]: 	Training 200/553. train loss: 0.6062,	0.8518 s / batch. (data: 3.96e-03). ETA=12:23:00, max mem: 20.9 GB 
[12/04 04:46:11 visual_prompt]: 	Training 300/553. train loss: 0.5727,	0.8194 s / batch. (data: 2.97e-04). ETA=11:53:23, max mem: 20.9 GB 
[12/04 04:47:51 visual_prompt]: 	Training 400/553. train loss: 0.5861,	0.8245 s / batch. (data: 3.50e-04). ETA=11:56:26, max mem: 20.9 GB 
[12/04 04:49:25 visual_prompt]: 	Training 500/553. train loss: 0.8666,	0.8600 s / batch. (data: 3.21e-04). ETA=12:25:50, max mem: 20.9 GB 
[12/04 04:50:15 visual_prompt]: Epoch 6 / 100: avg data time: 1.37e-01, avg batch time: 0.9717, average train loss: 0.7715
[12/04 04:51:10 visual_prompt]: Inference (val):avg data time: 1.17e-04, avg batch time: 0.3121, average loss: 0.6998
[12/04 04:51:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.70	
[12/04 04:51:10 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[12/04 04:52:47 visual_prompt]: 	Training 100/553. train loss: 0.7248,	0.8280 s / batch. (data: 2.95e-04). ETA=11:55:58, max mem: 20.9 GB 
[12/04 04:54:23 visual_prompt]: 	Training 200/553. train loss: 0.5871,	0.8200 s / batch. (data: 2.98e-04). ETA=11:47:40, max mem: 20.9 GB 
[12/04 04:56:01 visual_prompt]: 	Training 300/553. train loss: 0.6997,	1.3143 s / batch. (data: 4.88e-01). ETA=18:52:04, max mem: 20.9 GB 
[12/04 04:57:37 visual_prompt]: 	Training 400/553. train loss: 0.6116,	1.7360 s / batch. (data: 9.04e-01). ETA=1 day, 0:52:25, max mem: 20.9 GB 
[12/04 04:59:11 visual_prompt]: 	Training 500/553. train loss: 1.1290,	0.8451 s / batch. (data: 5.43e-03). ETA=12:05:09, max mem: 20.9 GB 
[12/04 05:00:01 visual_prompt]: Epoch 7 / 100: avg data time: 1.25e-01, avg batch time: 0.9598, average train loss: 0.7805
[12/04 05:00:56 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3087, average loss: 0.7299
[12/04 05:00:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.11	
[12/04 05:00:56 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[12/04 05:02:35 visual_prompt]: 	Training 100/553. train loss: 0.7028,	0.8583 s / batch. (data: 1.43e-02). ETA=12:14:17, max mem: 20.9 GB 
[12/04 05:04:12 visual_prompt]: 	Training 200/553. train loss: 1.2375,	0.8544 s / batch. (data: 1.56e-02). ETA=12:09:31, max mem: 20.9 GB 
[12/04 05:05:49 visual_prompt]: 	Training 300/553. train loss: 0.9843,	0.8564 s / batch. (data: 1.10e-02). ETA=12:09:45, max mem: 20.9 GB 
[12/04 05:07:25 visual_prompt]: 	Training 400/553. train loss: 0.7567,	0.8440 s / batch. (data: 5.44e-03). ETA=11:57:49, max mem: 20.9 GB 
[12/04 05:09:02 visual_prompt]: 	Training 500/553. train loss: 1.0193,	1.2830 s / batch. (data: 4.56e-01). ETA=18:09:02, max mem: 20.9 GB 
[12/04 05:09:53 visual_prompt]: Epoch 8 / 100: avg data time: 1.36e-01, avg batch time: 0.9705, average train loss: 0.8128
[12/04 05:10:48 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3112, average loss: 0.8603
[12/04 05:10:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.03	
[12/04 05:10:48 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[12/04 05:12:29 visual_prompt]: 	Training 100/553. train loss: 0.5982,	0.8440 s / batch. (data: 3.34e-04). ETA=11:54:14, max mem: 20.9 GB 
[12/04 05:14:04 visual_prompt]: 	Training 200/553. train loss: 0.6442,	0.8468 s / batch. (data: 2.06e-02). ETA=11:55:10, max mem: 20.9 GB 
[12/04 05:15:41 visual_prompt]: 	Training 300/553. train loss: 0.6454,	1.6280 s / batch. (data: 7.78e-01). ETA=22:52:16, max mem: 20.9 GB 
[12/04 05:17:19 visual_prompt]: 	Training 400/553. train loss: 0.5966,	0.8840 s / batch. (data: 7.60e-04). ETA=12:23:40, max mem: 20.9 GB 
[12/04 05:18:55 visual_prompt]: 	Training 500/553. train loss: 0.6879,	0.8264 s / batch. (data: 4.10e-04). ETA=11:33:48, max mem: 20.9 GB 
[12/04 05:19:44 visual_prompt]: Epoch 9 / 100: avg data time: 1.35e-01, avg batch time: 0.9702, average train loss: 0.7972
[12/04 05:20:39 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3111, average loss: 1.0446
[12/04 05:20:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.04	
[12/04 05:20:39 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[12/04 05:22:22 visual_prompt]: 	Training 100/553. train loss: 0.9860,	0.8318 s / batch. (data: 8.31e-04). ETA=11:36:17, max mem: 20.9 GB 
[12/04 05:23:55 visual_prompt]: 	Training 200/553. train loss: 0.5646,	0.8303 s / batch. (data: 3.10e-04). ETA=11:33:39, max mem: 20.9 GB 
[12/04 05:25:30 visual_prompt]: 	Training 300/553. train loss: 0.6824,	1.5120 s / batch. (data: 6.79e-01). ETA=21:00:35, max mem: 20.9 GB 
[12/04 05:27:03 visual_prompt]: 	Training 400/553. train loss: 0.9630,	0.8377 s / batch. (data: 3.18e-04). ETA=11:36:59, max mem: 20.9 GB 
[12/04 05:28:40 visual_prompt]: 	Training 500/553. train loss: 1.9114,	0.8478 s / batch. (data: 5.44e-03). ETA=11:44:00, max mem: 20.9 GB 
[12/04 05:29:30 visual_prompt]: Epoch 10 / 100: avg data time: 1.25e-01, avg batch time: 0.9593, average train loss: 0.9018
[12/04 05:30:25 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3099, average loss: 0.8534
[12/04 05:30:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.42	
[12/04 05:30:25 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[12/04 05:32:06 visual_prompt]: 	Training 100/553. train loss: 0.7061,	0.8240 s / batch. (data: 3.05e-04). ETA=11:22:07, max mem: 20.9 GB 
[12/04 05:33:43 visual_prompt]: 	Training 200/553. train loss: 2.2222,	0.8320 s / batch. (data: 3.04e-04). ETA=11:27:22, max mem: 20.9 GB 
[12/04 05:35:18 visual_prompt]: 	Training 300/553. train loss: 0.1973,	1.8759 s / batch. (data: 1.04e+00). ETA=1 day, 1:46:41, max mem: 20.9 GB 
[12/04 05:36:53 visual_prompt]: 	Training 400/553. train loss: 0.5758,	0.8290 s / batch. (data: 3.13e-04). ETA=11:22:05, max mem: 20.9 GB 
[12/04 05:38:27 visual_prompt]: 	Training 500/553. train loss: 0.7289,	0.8480 s / batch. (data: 5.46e-03). ETA=11:36:21, max mem: 20.9 GB 
[12/04 05:39:17 visual_prompt]: Epoch 11 / 100: avg data time: 1.29e-01, avg batch time: 0.9626, average train loss: 0.8837
[12/04 05:40:12 visual_prompt]: Inference (val):avg data time: 5.57e-05, avg batch time: 0.3111, average loss: 0.8879
[12/04 05:40:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.53	
[12/04 05:40:12 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[12/04 05:41:54 visual_prompt]: 	Training 100/553. train loss: 0.7659,	0.8285 s / batch. (data: 3.24e-04). ETA=11:18:13, max mem: 20.9 GB 
[12/04 05:43:31 visual_prompt]: 	Training 200/553. train loss: 0.8116,	1.4600 s / batch. (data: 6.39e-01). ETA=19:52:44, max mem: 20.9 GB 
[12/04 05:45:06 visual_prompt]: 	Training 300/553. train loss: 0.6508,	0.8480 s / batch. (data: 3.24e-04). ETA=11:31:20, max mem: 20.9 GB 
[12/04 05:46:42 visual_prompt]: 	Training 400/553. train loss: 1.0739,	0.8206 s / batch. (data: 4.52e-04). ETA=11:07:39, max mem: 20.9 GB 
[12/04 05:48:18 visual_prompt]: 	Training 500/553. train loss: 1.8426,	0.8280 s / batch. (data: 7.70e-04). ETA=11:12:15, max mem: 20.9 GB 
[12/04 05:49:08 visual_prompt]: Epoch 12 / 100: avg data time: 1.34e-01, avg batch time: 0.9683, average train loss: 0.8246
[12/04 05:50:02 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3111, average loss: 1.5140
[12/04 05:50:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.39	
[12/04 05:50:02 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[12/04 05:51:43 visual_prompt]: 	Training 100/553. train loss: 0.6790,	0.8399 s / batch. (data: 3.16e-04). ETA=11:19:51, max mem: 20.9 GB 
[12/04 05:53:18 visual_prompt]: 	Training 200/553. train loss: 0.7078,	0.8360 s / batch. (data: 7.95e-03). ETA=11:15:15, max mem: 20.9 GB 
[12/04 05:54:55 visual_prompt]: 	Training 300/553. train loss: 0.6904,	1.6000 s / batch. (data: 7.80e-01). ETA=21:29:42, max mem: 20.9 GB 
[12/04 05:56:29 visual_prompt]: 	Training 400/553. train loss: 1.1358,	0.8400 s / batch. (data: 2.82e-04). ETA=11:15:41, max mem: 20.9 GB 
[12/04 05:58:05 visual_prompt]: 	Training 500/553. train loss: 0.7014,	0.8592 s / batch. (data: 2.64e-02). ETA=11:29:44, max mem: 20.9 GB 
[12/04 05:58:55 visual_prompt]: Epoch 13 / 100: avg data time: 1.29e-01, avg batch time: 0.9631, average train loss: 0.8478
[12/04 05:59:49 visual_prompt]: Inference (val):avg data time: 8.95e-05, avg batch time: 0.3094, average loss: 0.7749
[12/04 05:59:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.02	
[12/04 05:59:49 visual_prompt]: Best epoch 13: best metric: -0.775
[12/04 05:59:49 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[12/04 06:01:30 visual_prompt]: 	Training 100/553. train loss: 0.6213,	0.8251 s / batch. (data: 8.72e-03). ETA=11:00:14, max mem: 20.9 GB 
[12/04 06:03:06 visual_prompt]: 	Training 200/553. train loss: 1.1663,	0.8237 s / batch. (data: 5.45e-03). ETA=10:57:44, max mem: 20.9 GB 
[12/04 06:04:42 visual_prompt]: 	Training 300/553. train loss: 0.7462,	0.8642 s / batch. (data: 7.95e-03). ETA=11:28:38, max mem: 20.9 GB 
[12/04 06:06:16 visual_prompt]: 	Training 400/553. train loss: 0.8124,	0.8427 s / batch. (data: 3.14e-04). ETA=11:10:07, max mem: 20.9 GB 
[12/04 06:07:52 visual_prompt]: 	Training 500/553. train loss: 1.1173,	0.8440 s / batch. (data: 2.80e-04). ETA=11:09:43, max mem: 20.9 GB 
[12/04 06:08:41 visual_prompt]: Epoch 14 / 100: avg data time: 1.26e-01, avg batch time: 0.9605, average train loss: 0.7847
[12/04 06:09:35 visual_prompt]: Inference (val):avg data time: 1.53e-04, avg batch time: 0.3098, average loss: 0.6897
[12/04 06:09:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.63	
[12/04 06:09:35 visual_prompt]: Best epoch 14: best metric: -0.690
[12/04 06:09:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[12/04 06:11:15 visual_prompt]: 	Training 100/553. train loss: 0.7982,	0.8320 s / batch. (data: 3.23e-04). ETA=10:58:05, max mem: 20.9 GB 
[12/04 06:12:49 visual_prompt]: 	Training 200/553. train loss: 0.8254,	0.8480 s / batch. (data: 2.63e-04). ETA=11:09:21, max mem: 20.9 GB 
[12/04 06:14:26 visual_prompt]: 	Training 300/553. train loss: 0.6573,	0.8275 s / batch. (data: 2.88e-04). ETA=10:51:43, max mem: 20.9 GB 
[12/04 06:15:59 visual_prompt]: 	Training 400/553. train loss: 0.7014,	1.0225 s / batch. (data: 2.01e-01). ETA=13:23:38, max mem: 20.9 GB 
[12/04 06:17:35 visual_prompt]: 	Training 500/553. train loss: 0.7251,	0.8421 s / batch. (data: 1.05e-02). ETA=11:00:26, max mem: 20.9 GB 
[12/04 06:18:26 visual_prompt]: Epoch 15 / 100: avg data time: 1.25e-01, avg batch time: 0.9593, average train loss: 0.9029
[12/04 06:19:20 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3105, average loss: 0.8379
[12/04 06:19:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.61	
[12/04 06:19:20 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[12/04 06:20:59 visual_prompt]: 	Training 100/553. train loss: 0.5627,	0.8560 s / batch. (data: 1.20e-02). ETA=11:09:09, max mem: 20.9 GB 
[12/04 06:22:35 visual_prompt]: 	Training 200/553. train loss: 1.0237,	0.8480 s / batch. (data: 2.89e-04). ETA=11:01:29, max mem: 20.9 GB 
[12/04 06:24:10 visual_prompt]: 	Training 300/553. train loss: 0.9025,	0.8313 s / batch. (data: 3.10e-04). ETA=10:47:07, max mem: 20.9 GB 
[12/04 06:25:45 visual_prompt]: 	Training 400/553. train loss: 0.6871,	0.8483 s / batch. (data: 1.64e-02). ETA=10:58:54, max mem: 20.9 GB 
[12/04 06:27:20 visual_prompt]: 	Training 500/553. train loss: 0.7218,	1.5249 s / batch. (data: 7.02e-01). ETA=19:41:53, max mem: 20.9 GB 
[12/04 06:28:10 visual_prompt]: Epoch 16 / 100: avg data time: 1.24e-01, avg batch time: 0.9590, average train loss: 0.8325
[12/04 06:29:05 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3090, average loss: 0.6887
[12/04 06:29:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.81	
[12/04 06:29:05 visual_prompt]: Best epoch 16: best metric: -0.689
[12/04 06:29:05 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[12/04 06:30:44 visual_prompt]: 	Training 100/553. train loss: 0.5818,	0.8174 s / batch. (data: 3.64e-04). ETA=10:31:26, max mem: 20.9 GB 
[12/04 06:32:20 visual_prompt]: 	Training 200/553. train loss: 1.9924,	0.8227 s / batch. (data: 3.86e-04). ETA=10:34:09, max mem: 20.9 GB 
[12/04 06:33:55 visual_prompt]: 	Training 300/553. train loss: 1.7095,	0.8249 s / batch. (data: 2.91e-04). ETA=10:34:29, max mem: 20.9 GB 
[12/04 06:35:31 visual_prompt]: 	Training 400/553. train loss: 0.6308,	1.2200 s / batch. (data: 3.60e-01). ETA=15:36:21, max mem: 20.9 GB 
[12/04 06:37:05 visual_prompt]: 	Training 500/553. train loss: 1.0265,	1.0666 s / batch. (data: 2.46e-01). ETA=13:36:50, max mem: 20.9 GB 
[12/04 06:37:56 visual_prompt]: Epoch 17 / 100: avg data time: 1.27e-01, avg batch time: 0.9603, average train loss: 0.9583
[12/04 06:38:51 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3098, average loss: 0.7726
[12/04 06:38:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.10	
[12/04 06:38:51 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[12/04 06:40:30 visual_prompt]: 	Training 100/553. train loss: 0.7899,	0.8541 s / batch. (data: 1.60e-02). ETA=10:51:58, max mem: 20.9 GB 
[12/04 06:42:08 visual_prompt]: 	Training 200/553. train loss: 0.8118,	0.8324 s / batch. (data: 5.42e-03). ETA=10:34:00, max mem: 20.9 GB 
[12/04 06:43:43 visual_prompt]: 	Training 300/553. train loss: 0.6788,	0.8187 s / batch. (data: 2.94e-04). ETA=10:22:09, max mem: 20.9 GB 
[12/04 06:45:19 visual_prompt]: 	Training 400/553. train loss: 0.7317,	0.8360 s / batch. (data: 1.20e-02). ETA=10:33:56, max mem: 20.9 GB 
[12/04 06:46:54 visual_prompt]: 	Training 500/553. train loss: 1.1800,	0.8663 s / batch. (data: 1.56e-02). ETA=10:55:28, max mem: 20.9 GB 
[12/04 06:47:42 visual_prompt]: Epoch 18 / 100: avg data time: 1.28e-01, avg batch time: 0.9617, average train loss: 1.0210
[12/04 06:48:37 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3113, average loss: 0.9951
[12/04 06:48:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.00	
[12/04 06:48:37 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[12/04 06:50:16 visual_prompt]: 	Training 100/553. train loss: 0.5707,	0.8280 s / batch. (data: 3.20e-04). ETA=10:24:24, max mem: 20.9 GB 
[12/04 06:51:53 visual_prompt]: 	Training 200/553. train loss: 0.5834,	0.8297 s / batch. (data: 2.79e-04). ETA=10:24:15, max mem: 20.9 GB 
[12/04 06:53:28 visual_prompt]: 	Training 300/553. train loss: 1.9944,	1.1002 s / batch. (data: 2.66e-01). ETA=13:45:58, max mem: 20.9 GB 
[12/04 06:55:05 visual_prompt]: 	Training 400/553. train loss: 0.6432,	0.8583 s / batch. (data: 5.93e-03). ETA=10:42:56, max mem: 20.9 GB 
[12/04 06:56:37 visual_prompt]: 	Training 500/553. train loss: 0.6219,	0.8332 s / batch. (data: 2.82e-04). ETA=10:22:44, max mem: 20.9 GB 
[12/04 06:57:26 visual_prompt]: Epoch 19 / 100: avg data time: 1.22e-01, avg batch time: 0.9570, average train loss: 0.8275
[12/04 06:58:21 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3095, average loss: 1.2959
[12/04 06:58:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.96	
[12/04 06:58:21 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[12/04 06:59:59 visual_prompt]: 	Training 100/553. train loss: 0.5875,	0.8323 s / batch. (data: 3.10e-04). ETA=10:19:56, max mem: 20.9 GB 
[12/04 07:01:35 visual_prompt]: 	Training 200/553. train loss: 0.5600,	0.8480 s / batch. (data: 5.43e-03). ETA=10:30:15, max mem: 20.9 GB 
[12/04 07:03:11 visual_prompt]: 	Training 300/553. train loss: 0.9484,	0.8560 s / batch. (data: 1.59e-02). ETA=10:34:45, max mem: 20.9 GB 
[12/04 07:04:46 visual_prompt]: 	Training 400/553. train loss: 0.6638,	0.8440 s / batch. (data: 7.95e-03). ETA=10:24:27, max mem: 20.9 GB 
[12/04 07:06:20 visual_prompt]: 	Training 500/553. train loss: 0.7173,	0.8336 s / batch. (data: 9.61e-03). ETA=10:15:24, max mem: 20.9 GB 
[12/04 07:07:11 visual_prompt]: Epoch 20 / 100: avg data time: 1.24e-01, avg batch time: 0.9592, average train loss: 0.8286
[12/04 07:08:06 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3106, average loss: 0.9962
[12/04 07:08:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.31	
[12/04 07:08:06 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[12/04 07:09:47 visual_prompt]: 	Training 100/553. train loss: 0.5715,	0.8280 s / batch. (data: 1.20e-02). ETA=10:09:09, max mem: 20.9 GB 
[12/04 07:11:22 visual_prompt]: 	Training 200/553. train loss: 0.9737,	0.8196 s / batch. (data: 2.89e-04). ETA=10:01:33, max mem: 20.9 GB 
[12/04 07:12:57 visual_prompt]: 	Training 300/553. train loss: 2.0618,	0.8673 s / batch. (data: 3.75e-02). ETA=10:35:09, max mem: 20.9 GB 
[12/04 07:14:30 visual_prompt]: 	Training 400/553. train loss: 1.4197,	0.8293 s / batch. (data: 1.05e-02). ETA=10:05:57, max mem: 20.9 GB 
[12/04 07:16:07 visual_prompt]: 	Training 500/553. train loss: 0.6889,	0.8253 s / batch. (data: 3.26e-04). ETA=10:01:37, max mem: 20.9 GB 
[12/04 07:16:56 visual_prompt]: Epoch 21 / 100: avg data time: 1.26e-01, avg batch time: 0.9589, average train loss: 0.8497
[12/04 07:17:51 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3091, average loss: 0.6998
[12/04 07:17:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.33	
[12/04 07:17:51 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[12/04 07:19:29 visual_prompt]: 	Training 100/553. train loss: 1.6218,	0.8247 s / batch. (data: 2.87e-04). ETA=9:59:05, max mem: 20.9 GB 
[12/04 07:21:05 visual_prompt]: 	Training 200/553. train loss: 0.8761,	0.8379 s / batch. (data: 2.87e-04). ETA=10:07:17, max mem: 20.9 GB 
[12/04 07:22:38 visual_prompt]: 	Training 300/553. train loss: 0.3297,	0.8445 s / batch. (data: 5.44e-03). ETA=10:10:38, max mem: 20.9 GB 
[12/04 07:24:14 visual_prompt]: 	Training 400/553. train loss: 0.7813,	0.8315 s / batch. (data: 2.90e-04). ETA=9:59:55, max mem: 20.9 GB 
[12/04 07:25:49 visual_prompt]: 	Training 500/553. train loss: 0.7249,	0.8410 s / batch. (data: 2.88e-04). ETA=10:05:21, max mem: 20.9 GB 
[12/04 07:26:40 visual_prompt]: Epoch 22 / 100: avg data time: 1.25e-01, avg batch time: 0.9578, average train loss: 0.8943
[12/04 07:27:35 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3105, average loss: 0.7687
[12/04 07:27:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.40	
[12/04 07:27:35 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[12/04 07:29:15 visual_prompt]: 	Training 100/553. train loss: 0.9184,	0.8400 s / batch. (data: 3.60e-04). ETA=10:02:28, max mem: 20.9 GB 
[12/04 07:30:52 visual_prompt]: 	Training 200/553. train loss: 0.7380,	0.8443 s / batch. (data: 1.05e-02). ETA=10:04:10, max mem: 20.9 GB 
[12/04 07:32:29 visual_prompt]: 	Training 300/553. train loss: 0.5879,	0.8209 s / batch. (data: 2.93e-04). ETA=9:46:01, max mem: 20.9 GB 
[12/04 07:34:02 visual_prompt]: 	Training 400/553. train loss: 0.5716,	0.8556 s / batch. (data: 7.62e-04). ETA=10:09:25, max mem: 20.9 GB 
[12/04 07:35:35 visual_prompt]: 	Training 500/553. train loss: 0.2949,	0.8245 s / batch. (data: 3.04e-04). ETA=9:45:49, max mem: 20.9 GB 
[12/04 07:36:25 visual_prompt]: Epoch 23 / 100: avg data time: 1.25e-01, avg batch time: 0.9592, average train loss: 0.8408
[12/04 07:37:20 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3106, average loss: 1.0228
[12/04 07:37:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.63	
[12/04 07:37:20 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[12/04 07:38:57 visual_prompt]: 	Training 100/553. train loss: 1.3352,	0.8524 s / batch. (data: 1.20e-02). ETA=10:03:31, max mem: 20.9 GB 
[12/04 07:40:32 visual_prompt]: 	Training 200/553. train loss: 0.7216,	0.8493 s / batch. (data: 1.55e-02). ETA=9:59:53, max mem: 20.9 GB 
[12/04 07:42:07 visual_prompt]: 	Training 300/553. train loss: 0.7101,	0.8600 s / batch. (data: 3.24e-04). ETA=10:06:01, max mem: 20.9 GB 
[12/04 07:43:43 visual_prompt]: 	Training 400/553. train loss: 0.5791,	0.8200 s / batch. (data: 3.24e-04). ETA=9:36:28, max mem: 20.9 GB 
[12/04 07:45:20 visual_prompt]: 	Training 500/553. train loss: 0.7199,	0.8600 s / batch. (data: 7.96e-03). ETA=10:03:10, max mem: 20.9 GB 
[12/04 07:46:11 visual_prompt]: Epoch 24 / 100: avg data time: 1.26e-01, avg batch time: 0.9598, average train loss: 0.8738
[12/04 07:47:05 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.3107, average loss: 0.7540
[12/04 07:47:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.62	
[12/04 07:47:05 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[12/04 07:48:47 visual_prompt]: 	Training 100/553. train loss: 0.6102,	0.8205 s / batch. (data: 2.77e-04). ETA=9:33:23, max mem: 20.9 GB 
[12/04 07:50:20 visual_prompt]: 	Training 200/553. train loss: 1.1837,	1.0113 s / batch. (data: 1.72e-01). ETA=11:45:01, max mem: 20.9 GB 
[12/04 07:51:54 visual_prompt]: 	Training 300/553. train loss: 0.7502,	1.0689 s / batch. (data: 2.20e-01). ETA=12:23:24, max mem: 20.9 GB 
[12/04 07:53:30 visual_prompt]: 	Training 400/553. train loss: 0.9394,	1.1200 s / batch. (data: 2.83e-01). ETA=12:57:02, max mem: 20.9 GB 
[12/04 07:55:06 visual_prompt]: 	Training 500/553. train loss: 1.1250,	1.1613 s / batch. (data: 3.42e-01). ETA=13:23:45, max mem: 20.9 GB 
[12/04 07:55:56 visual_prompt]: Epoch 25 / 100: avg data time: 1.27e-01, avg batch time: 0.9606, average train loss: 0.8454
[12/04 07:56:51 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3108, average loss: 1.0285
[12/04 07:56:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.67	
[12/04 07:56:51 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[12/04 07:58:30 visual_prompt]: 	Training 100/553. train loss: 0.5638,	0.8342 s / batch. (data: 1.55e-02). ETA=9:35:14, max mem: 20.9 GB 
[12/04 08:00:07 visual_prompt]: 	Training 200/553. train loss: 0.9088,	1.5380 s / batch. (data: 7.22e-01). ETA=17:38:00, max mem: 20.9 GB 
[12/04 08:01:43 visual_prompt]: 	Training 300/553. train loss: 0.9618,	0.8370 s / batch. (data: 7.86e-04). ETA=9:34:24, max mem: 20.9 GB 
[12/04 08:03:18 visual_prompt]: 	Training 400/553. train loss: 1.3372,	0.8320 s / batch. (data: 3.99e-04). ETA=9:29:32, max mem: 20.9 GB 
[12/04 08:04:51 visual_prompt]: 	Training 500/553. train loss: 0.8878,	0.8440 s / batch. (data: 5.45e-03). ETA=9:36:22, max mem: 20.9 GB 
[12/04 08:05:41 visual_prompt]: Epoch 26 / 100: avg data time: 1.24e-01, avg batch time: 0.9594, average train loss: 0.8429
[12/04 08:06:36 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3109, average loss: 0.6883
[12/04 08:06:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.49	
[12/04 08:06:36 visual_prompt]: Best epoch 26: best metric: -0.688
[12/04 08:06:36 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[12/04 08:08:16 visual_prompt]: 	Training 100/553. train loss: 0.6208,	0.8565 s / batch. (data: 1.06e-02). ETA=9:42:44, max mem: 20.9 GB 
[12/04 08:09:51 visual_prompt]: 	Training 200/553. train loss: 0.9432,	1.0480 s / batch. (data: 1.95e-01). ETA=11:51:17, max mem: 20.9 GB 
[12/04 08:11:26 visual_prompt]: 	Training 300/553. train loss: 0.5772,	0.8250 s / batch. (data: 2.90e-04). ETA=9:18:34, max mem: 20.9 GB 
[12/04 08:13:03 visual_prompt]: 	Training 400/553. train loss: 0.7051,	0.8346 s / batch. (data: 9.44e-04). ETA=9:23:41, max mem: 20.9 GB 
[12/04 08:14:39 visual_prompt]: 	Training 500/553. train loss: 0.6068,	0.8320 s / batch. (data: 1.53e-02). ETA=9:20:32, max mem: 20.9 GB 
[12/04 08:15:27 visual_prompt]: Epoch 27 / 100: avg data time: 1.26e-01, avg batch time: 0.9610, average train loss: 0.8669
[12/04 08:16:22 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3109, average loss: 0.7804
[12/04 08:16:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.34	
[12/04 08:16:22 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[12/04 08:18:01 visual_prompt]: 	Training 100/553. train loss: 0.0619,	0.8480 s / batch. (data: 7.97e-03). ETA=9:29:08, max mem: 20.9 GB 
[12/04 08:19:36 visual_prompt]: 	Training 200/553. train loss: 2.5732,	0.8489 s / batch. (data: 1.55e-02). ETA=9:28:18, max mem: 20.9 GB 
[12/04 08:21:12 visual_prompt]: 	Training 300/553. train loss: 1.0202,	1.2487 s / batch. (data: 4.22e-01). ETA=13:53:52, max mem: 20.9 GB 
[12/04 08:22:47 visual_prompt]: 	Training 400/553. train loss: 0.7207,	0.8186 s / batch. (data: 2.91e-04). ETA=9:05:19, max mem: 20.9 GB 
[12/04 08:24:21 visual_prompt]: 	Training 500/553. train loss: 0.9087,	0.8408 s / batch. (data: 2.45e-04). ETA=9:18:43, max mem: 20.9 GB 
[12/04 08:25:12 visual_prompt]: Epoch 28 / 100: avg data time: 1.24e-01, avg batch time: 0.9583, average train loss: 1.0145
[12/04 08:26:06 visual_prompt]: Inference (val):avg data time: 1.72e-04, avg batch time: 0.3092, average loss: 0.7389
[12/04 08:26:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.30	
[12/04 08:26:06 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[12/04 08:27:51 visual_prompt]: 	Training 100/553. train loss: 0.8489,	0.8360 s / batch. (data: 2.69e-04). ETA=9:13:22, max mem: 20.9 GB 
[12/04 08:29:26 visual_prompt]: 	Training 200/553. train loss: 0.8727,	1.5040 s / batch. (data: 6.87e-01). ETA=16:33:01, max mem: 20.9 GB 
[12/04 08:31:00 visual_prompt]: 	Training 300/553. train loss: 0.7270,	0.8328 s / batch. (data: 1.08e-02). ETA=9:08:29, max mem: 20.9 GB 
[12/04 08:32:32 visual_prompt]: 	Training 400/553. train loss: 0.5946,	0.8458 s / batch. (data: 2.62e-02). ETA=9:15:38, max mem: 20.9 GB 
[12/04 08:34:08 visual_prompt]: 	Training 500/553. train loss: 1.1133,	0.8793 s / batch. (data: 2.73e-02). ETA=9:36:09, max mem: 20.9 GB 
[12/04 08:34:57 visual_prompt]: Epoch 29 / 100: avg data time: 1.26e-01, avg batch time: 0.9602, average train loss: 0.9752
[12/04 08:35:52 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3098, average loss: 1.2719
[12/04 08:35:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.75	
[12/04 08:35:52 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[12/04 08:37:30 visual_prompt]: 	Training 100/553. train loss: 0.6954,	0.8360 s / batch. (data: 2.92e-04). ETA=9:05:40, max mem: 20.9 GB 
[12/04 08:39:06 visual_prompt]: 	Training 200/553. train loss: 1.2975,	0.8292 s / batch. (data: 5.40e-03). ETA=8:59:50, max mem: 20.9 GB 
[12/04 08:40:40 visual_prompt]: 	Training 300/553. train loss: 0.2771,	1.8519 s / batch. (data: 9.94e-01). ETA=20:02:36, max mem: 20.9 GB 
[12/04 08:42:17 visual_prompt]: 	Training 400/553. train loss: 1.0932,	0.9747 s / batch. (data: 1.45e-01). ETA=10:31:18, max mem: 20.9 GB 
[12/04 08:43:52 visual_prompt]: 	Training 500/553. train loss: 0.5604,	1.3360 s / batch. (data: 4.87e-01). ETA=14:23:07, max mem: 20.9 GB 
[12/04 08:44:43 visual_prompt]: Epoch 30 / 100: avg data time: 1.26e-01, avg batch time: 0.9602, average train loss: 0.8724
[12/04 08:45:37 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3112, average loss: 0.7306
[12/04 08:45:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.75	
[12/04 08:45:37 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[12/04 08:47:19 visual_prompt]: 	Training 100/553. train loss: 0.6002,	0.8308 s / batch. (data: 5.41e-03). ETA=8:54:38, max mem: 20.9 GB 
[12/04 08:48:56 visual_prompt]: 	Training 200/553. train loss: 0.9437,	0.8592 s / batch. (data: 3.00e-04). ETA=9:11:26, max mem: 20.9 GB 
[12/04 08:50:29 visual_prompt]: 	Training 300/553. train loss: 0.7611,	0.8517 s / batch. (data: 6.45e-03). ETA=9:05:15, max mem: 20.9 GB 
[12/04 08:52:04 visual_prompt]: 	Training 400/553. train loss: 0.5838,	0.9901 s / batch. (data: 1.20e-01). ETA=10:32:10, max mem: 20.9 GB 
[12/04 08:53:39 visual_prompt]: 	Training 500/553. train loss: 0.5712,	0.8480 s / batch. (data: 2.93e-04). ETA=9:00:01, max mem: 20.9 GB 
[12/04 08:54:28 visual_prompt]: Epoch 31 / 100: avg data time: 1.24e-01, avg batch time: 0.9588, average train loss: 0.8127
[12/04 08:55:22 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3119, average loss: 0.7037
[12/04 08:55:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.84	
[12/04 08:55:22 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[12/04 08:57:03 visual_prompt]: 	Training 100/553. train loss: 0.5703,	0.8720 s / batch. (data: 7.31e-04). ETA=9:13:05, max mem: 20.9 GB 
[12/04 08:58:40 visual_prompt]: 	Training 200/553. train loss: 0.5599,	0.8187 s / batch. (data: 2.97e-04). ETA=8:37:55, max mem: 20.9 GB 
[12/04 09:00:18 visual_prompt]: 	Training 300/553. train loss: 0.9876,	0.8440 s / batch. (data: 5.58e-03). ETA=8:52:31, max mem: 20.9 GB 
[12/04 09:01:55 visual_prompt]: 	Training 400/553. train loss: 0.7702,	0.8627 s / batch. (data: 2.00e-04). ETA=9:02:53, max mem: 20.9 GB 
[12/04 09:03:28 visual_prompt]: 	Training 500/553. train loss: 0.7157,	0.8328 s / batch. (data: 5.42e-03). ETA=8:42:39, max mem: 20.9 GB 
[12/04 09:04:17 visual_prompt]: Epoch 32 / 100: avg data time: 1.34e-01, avg batch time: 0.9666, average train loss: 0.8455
[12/04 09:05:12 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3114, average loss: 0.6885
[12/04 09:05:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.80	
[12/04 09:05:12 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[12/04 09:06:50 visual_prompt]: 	Training 100/553. train loss: 1.7185,	0.8256 s / batch. (data: 2.85e-04). ETA=8:36:03, max mem: 20.9 GB 
[12/04 09:08:28 visual_prompt]: 	Training 200/553. train loss: 1.0941,	1.0680 s / batch. (data: 2.45e-01). ETA=11:05:47, max mem: 20.9 GB 
[12/04 09:10:03 visual_prompt]: 	Training 300/553. train loss: 0.8596,	0.8842 s / batch. (data: 2.06e-02). ETA=9:09:45, max mem: 20.9 GB 
[12/04 09:11:40 visual_prompt]: 	Training 400/553. train loss: 0.7348,	0.8200 s / batch. (data: 3.23e-04). ETA=8:28:28, max mem: 20.9 GB 
[12/04 09:13:15 visual_prompt]: 	Training 500/553. train loss: 0.4949,	0.8400 s / batch. (data: 2.99e-04). ETA=8:39:27, max mem: 20.9 GB 
[12/04 09:14:05 visual_prompt]: Epoch 33 / 100: avg data time: 1.30e-01, avg batch time: 0.9637, average train loss: 0.8833
[12/04 09:15:00 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3110, average loss: 1.1352
[12/04 09:15:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.71	
[12/04 09:15:00 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[12/04 09:16:41 visual_prompt]: 	Training 100/553. train loss: 0.9046,	0.8840 s / batch. (data: 5.09e-02). ETA=9:04:24, max mem: 20.9 GB 
[12/04 09:18:16 visual_prompt]: 	Training 200/553. train loss: 0.7317,	0.8520 s / batch. (data: 3.12e-04). ETA=8:43:16, max mem: 20.9 GB 
[12/04 09:19:51 visual_prompt]: 	Training 300/553. train loss: 0.7164,	0.8187 s / batch. (data: 3.56e-04). ETA=8:21:29, max mem: 20.9 GB 
[12/04 09:21:28 visual_prompt]: 	Training 400/553. train loss: 0.9245,	0.8480 s / batch. (data: 2.88e-04). ETA=8:38:00, max mem: 20.9 GB 
[12/04 09:23:04 visual_prompt]: 	Training 500/553. train loss: 0.5839,	1.1160 s / batch. (data: 2.72e-01). ETA=11:19:51, max mem: 20.9 GB 
[12/04 09:23:54 visual_prompt]: Epoch 34 / 100: avg data time: 1.30e-01, avg batch time: 0.9653, average train loss: 0.8411
[12/04 09:24:49 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3114, average loss: 0.7324
[12/04 09:24:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.46	
[12/04 09:24:49 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[12/04 09:26:31 visual_prompt]: 	Training 100/553. train loss: 0.5990,	0.8551 s / batch. (data: 5.16e-03). ETA=8:38:44, max mem: 20.9 GB 
[12/04 09:28:10 visual_prompt]: 	Training 200/553. train loss: 0.6983,	0.8364 s / batch. (data: 3.44e-04). ETA=8:25:59, max mem: 20.9 GB 
[12/04 09:29:46 visual_prompt]: 	Training 300/553. train loss: 0.6906,	0.8312 s / batch. (data: 3.00e-04). ETA=8:21:28, max mem: 20.9 GB 
[12/04 09:31:20 visual_prompt]: 	Training 400/553. train loss: 0.6172,	0.8266 s / batch. (data: 3.22e-04). ETA=8:17:17, max mem: 20.9 GB 
[12/04 09:32:56 visual_prompt]: 	Training 500/553. train loss: 0.7756,	0.8296 s / batch. (data: 3.10e-04). ETA=8:17:45, max mem: 20.9 GB 
[12/04 09:33:47 visual_prompt]: Epoch 35 / 100: avg data time: 1.40e-01, avg batch time: 0.9735, average train loss: 0.8366
[12/04 09:34:42 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3092, average loss: 0.6925
[12/04 09:34:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.68	
[12/04 09:34:42 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[12/04 09:36:23 visual_prompt]: 	Training 100/553. train loss: 0.5637,	0.8247 s / batch. (data: 3.32e-04). ETA=8:12:42, max mem: 20.9 GB 
[12/04 09:38:01 visual_prompt]: 	Training 200/553. train loss: 1.3438,	0.8481 s / batch. (data: 3.07e-04). ETA=8:25:14, max mem: 20.9 GB 
[12/04 09:39:39 visual_prompt]: 	Training 300/553. train loss: 0.1058,	0.8543 s / batch. (data: 2.29e-02). ETA=8:27:32, max mem: 20.9 GB 
[12/04 09:41:15 visual_prompt]: 	Training 400/553. train loss: 1.2804,	0.8209 s / batch. (data: 2.96e-04). ETA=8:06:17, max mem: 20.9 GB 
[12/04 09:42:51 visual_prompt]: 	Training 500/553. train loss: 0.5630,	1.0398 s / batch. (data: 2.00e-01). ETA=10:14:16, max mem: 20.9 GB 
[12/04 09:43:39 visual_prompt]: Epoch 36 / 100: avg data time: 1.38e-01, avg batch time: 0.9715, average train loss: 0.9216
[12/04 09:44:34 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.3100, average loss: 0.7603
[12/04 09:44:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.09	
[12/04 09:44:34 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[12/04 09:46:15 visual_prompt]: 	Training 100/553. train loss: 1.1110,	0.8355 s / batch. (data: 3.63e-04). ETA=8:11:25, max mem: 20.9 GB 
[12/04 09:47:50 visual_prompt]: 	Training 200/553. train loss: 0.7657,	0.8240 s / batch. (data: 2.87e-04). ETA=8:03:19, max mem: 20.9 GB 
[12/04 09:49:26 visual_prompt]: 	Training 300/553. train loss: 0.9095,	1.1814 s / batch. (data: 3.56e-01). ETA=11:30:58, max mem: 20.9 GB 
[12/04 09:51:05 visual_prompt]: 	Training 400/553. train loss: 0.6366,	1.2877 s / batch. (data: 4.70e-01). ETA=12:30:57, max mem: 20.9 GB 
[12/04 09:52:37 visual_prompt]: 	Training 500/553. train loss: 1.0428,	0.9640 s / batch. (data: 1.11e-01). ETA=9:20:36, max mem: 20.9 GB 
[12/04 09:53:29 visual_prompt]: Epoch 37 / 100: avg data time: 1.32e-01, avg batch time: 0.9661, average train loss: 0.8099
[12/04 09:54:23 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3122, average loss: 0.7743
[12/04 09:54:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.74	
[12/04 09:54:23 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[12/04 09:56:02 visual_prompt]: 	Training 100/553. train loss: 0.6800,	0.8394 s / batch. (data: 3.01e-04). ETA=8:05:59, max mem: 20.9 GB 
[12/04 09:57:38 visual_prompt]: 	Training 200/553. train loss: 0.7833,	0.8471 s / batch. (data: 1.06e-02). ETA=8:09:01, max mem: 20.9 GB 
[12/04 09:59:16 visual_prompt]: 	Training 300/553. train loss: 0.5706,	0.8250 s / batch. (data: 4.34e-04). ETA=7:54:54, max mem: 20.9 GB 
[12/04 10:00:50 visual_prompt]: 	Training 400/553. train loss: 0.5608,	0.8279 s / batch. (data: 3.39e-04). ETA=7:55:10, max mem: 20.9 GB 
[12/04 10:02:28 visual_prompt]: 	Training 500/553. train loss: 0.6420,	0.8267 s / batch. (data: 5.69e-03). ETA=7:53:08, max mem: 20.9 GB 
[12/04 10:03:17 visual_prompt]: Epoch 38 / 100: avg data time: 1.30e-01, avg batch time: 0.9640, average train loss: 0.8278
[12/04 10:04:11 visual_prompt]: Inference (val):avg data time: 1.74e-04, avg batch time: 0.3102, average loss: 0.8743
[12/04 10:04:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.12	
[12/04 10:04:11 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[12/04 10:05:50 visual_prompt]: 	Training 100/553. train loss: 0.2804,	0.8366 s / batch. (data: 3.18e-04). ETA=7:56:39, max mem: 20.9 GB 
[12/04 10:07:29 visual_prompt]: 	Training 200/553. train loss: 1.0423,	0.8211 s / batch. (data: 2.86e-04). ETA=7:46:27, max mem: 20.9 GB 
[12/04 10:09:07 visual_prompt]: 	Training 300/553. train loss: 1.9020,	0.8360 s / batch. (data: 7.96e-03). ETA=7:53:32, max mem: 20.9 GB 
[12/04 10:10:40 visual_prompt]: 	Training 400/553. train loss: 0.6270,	1.2977 s / batch. (data: 4.71e-01). ETA=12:12:54, max mem: 20.9 GB 
[12/04 10:12:15 visual_prompt]: 	Training 500/553. train loss: 0.7777,	1.4834 s / batch. (data: 6.38e-01). ETA=13:55:16, max mem: 20.9 GB 
[12/04 10:13:03 visual_prompt]: Epoch 39 / 100: avg data time: 1.27e-01, avg batch time: 0.9609, average train loss: 0.8609
[12/04 10:13:57 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3113, average loss: 0.9256
[12/04 10:13:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.24	
[12/04 10:13:57 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[12/04 10:15:38 visual_prompt]: 	Training 100/553. train loss: 1.0909,	0.8360 s / batch. (data: 3.16e-04). ETA=7:48:36, max mem: 20.9 GB 
[12/04 10:17:13 visual_prompt]: 	Training 200/553. train loss: 1.2362,	0.8347 s / batch. (data: 2.95e-04). ETA=7:46:30, max mem: 20.9 GB 
[12/04 10:18:50 visual_prompt]: 	Training 300/553. train loss: 1.3966,	0.8163 s / batch. (data: 3.02e-04). ETA=7:34:52, max mem: 20.9 GB 
[12/04 10:20:26 visual_prompt]: 	Training 400/553. train loss: 1.1069,	0.8360 s / batch. (data: 7.80e-04). ETA=7:44:25, max mem: 20.9 GB 
[12/04 10:22:00 visual_prompt]: 	Training 500/553. train loss: 0.7662,	0.8352 s / batch. (data: 1.30e-02). ETA=7:42:34, max mem: 20.9 GB 
[12/04 10:22:52 visual_prompt]: Epoch 40 / 100: avg data time: 1.32e-01, avg batch time: 0.9657, average train loss: 0.9662
[12/04 10:23:46 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3089, average loss: 0.7024
[12/04 10:23:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.97	
[12/04 10:23:46 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[12/04 10:25:30 visual_prompt]: 	Training 100/553. train loss: 0.6272,	0.8408 s / batch. (data: 2.92e-04). ETA=7:43:34, max mem: 20.9 GB 
[12/04 10:27:07 visual_prompt]: 	Training 200/553. train loss: 1.1757,	0.8315 s / batch. (data: 7.20e-04). ETA=7:37:01, max mem: 20.9 GB 
[12/04 10:28:42 visual_prompt]: 	Training 300/553. train loss: 0.7744,	0.8389 s / batch. (data: 3.69e-04). ETA=7:39:43, max mem: 20.9 GB 
[12/04 10:30:16 visual_prompt]: 	Training 400/553. train loss: 0.7777,	0.8400 s / batch. (data: 2.86e-04). ETA=7:38:54, max mem: 20.9 GB 
[12/04 10:31:49 visual_prompt]: 	Training 500/553. train loss: 1.4933,	0.8479 s / batch. (data: 4.39e-04). ETA=7:41:50, max mem: 20.9 GB 
[12/04 10:32:37 visual_prompt]: Epoch 41 / 100: avg data time: 1.25e-01, avg batch time: 0.9603, average train loss: 0.8583
[12/04 10:33:31 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3108, average loss: 0.7902
[12/04 10:33:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.68	
[12/04 10:33:31 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[12/04 10:35:11 visual_prompt]: 	Training 100/553. train loss: 0.7258,	0.8360 s / batch. (data: 5.42e-03). ETA=7:33:12, max mem: 20.9 GB 
[12/04 10:36:47 visual_prompt]: 	Training 200/553. train loss: 1.2320,	0.8515 s / batch. (data: 1.56e-02). ETA=7:40:11, max mem: 20.9 GB 
[12/04 10:38:24 visual_prompt]: 	Training 300/553. train loss: 0.9843,	0.8394 s / batch. (data: 2.89e-04). ETA=7:32:16, max mem: 20.9 GB 
[12/04 10:39:59 visual_prompt]: 	Training 400/553. train loss: 0.7143,	0.8507 s / batch. (data: 1.05e-02). ETA=7:36:55, max mem: 20.9 GB 
[12/04 10:41:34 visual_prompt]: 	Training 500/553. train loss: 0.8898,	0.8363 s / batch. (data: 1.05e-02). ETA=7:27:46, max mem: 20.9 GB 
[12/04 10:42:31 visual_prompt]: Epoch 42 / 100: avg data time: 1.41e-01, avg batch time: 0.9751, average train loss: 0.8540
[12/04 10:44:03 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3065, average loss: 0.7190
[12/04 10:44:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.46	
[12/04 10:44:03 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[12/04 10:46:58 visual_prompt]: 	Training 100/553. train loss: 0.8209,	0.8090 s / batch. (data: 2.32e-04). ETA=7:11:05, max mem: 20.9 GB 
[12/04 10:49:52 visual_prompt]: 	Training 200/553. train loss: 0.7926,	0.8472 s / batch. (data: 2.38e-03). ETA=7:30:04, max mem: 20.9 GB 
[12/04 10:52:36 visual_prompt]: 	Training 300/553. train loss: 0.7415,	0.8136 s / batch. (data: 2.16e-04). ETA=7:10:50, max mem: 20.9 GB 
[12/04 10:55:20 visual_prompt]: 	Training 400/553. train loss: 0.8829,	0.8364 s / batch. (data: 3.16e-04). ETA=7:21:31, max mem: 20.9 GB 
[12/04 10:57:52 visual_prompt]: 	Training 500/553. train loss: 0.5658,	0.8110 s / batch. (data: 2.69e-04). ETA=7:06:45, max mem: 20.9 GB 
[12/04 10:59:18 visual_prompt]: Epoch 43 / 100: avg data time: 8.37e-01, avg batch time: 1.6548, average train loss: 0.8125
[12/04 11:00:14 visual_prompt]: Inference (val):avg data time: 2.09e-04, avg batch time: 0.3063, average loss: 0.7209
[12/04 11:00:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.73	
[12/04 11:00:14 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[12/04 11:01:54 visual_prompt]: 	Training 100/553. train loss: 0.5748,	0.8195 s / batch. (data: 3.02e-04). ETA=7:09:09, max mem: 20.9 GB 
[12/04 11:03:32 visual_prompt]: 	Training 200/553. train loss: 0.6390,	0.8523 s / batch. (data: 7.88e-04). ETA=7:24:56, max mem: 20.9 GB 
[12/04 11:05:05 visual_prompt]: 	Training 300/553. train loss: 0.5694,	0.8320 s / batch. (data: 2.79e-04). ETA=7:12:55, max mem: 20.9 GB 
[12/04 11:06:39 visual_prompt]: 	Training 400/553. train loss: 0.7306,	0.8441 s / batch. (data: 3.14e-04). ETA=7:17:48, max mem: 20.9 GB 
[12/04 11:08:16 visual_prompt]: 	Training 500/553. train loss: 0.7828,	0.8363 s / batch. (data: 3.04e-04). ETA=7:12:22, max mem: 20.9 GB 
[12/04 11:09:05 visual_prompt]: Epoch 44 / 100: avg data time: 1.26e-01, avg batch time: 0.9606, average train loss: 0.7939
[12/04 11:10:04 visual_prompt]: Inference (val):avg data time: 2.26e-04, avg batch time: 0.3102, average loss: 0.7000
[12/04 11:10:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.13	
[12/04 11:10:04 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[12/04 11:12:33 visual_prompt]: 	Training 100/553. train loss: 0.7212,	0.8115 s / batch. (data: 3.11e-04). ETA=6:57:30, max mem: 20.9 GB 
[12/04 11:14:35 visual_prompt]: 	Training 200/553. train loss: 0.7743,	4.9367 s / batch. (data: 4.12e+00). ETA=1 day, 18:11:33, max mem: 20.9 GB 
[12/04 11:16:48 visual_prompt]: 	Training 300/553. train loss: 0.9352,	0.8372 s / batch. (data: 3.02e-04). ETA=7:07:56, max mem: 20.9 GB 
[12/04 11:18:50 visual_prompt]: 	Training 400/553. train loss: 0.8498,	0.8405 s / batch. (data: 8.24e-04). ETA=7:08:11, max mem: 20.9 GB 
[12/04 11:20:46 visual_prompt]: 	Training 500/553. train loss: 0.9335,	0.8456 s / batch. (data: 3.00e-04). ETA=7:09:25, max mem: 20.9 GB 
[12/04 11:21:50 visual_prompt]: Epoch 45 / 100: avg data time: 4.54e-01, avg batch time: 1.2775, average train loss: 0.7626
[12/04 11:23:09 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3042, average loss: 1.1311
[12/04 11:23:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.29	
[12/04 11:23:09 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[12/04 11:25:00 visual_prompt]: 	Training 100/553. train loss: 0.9157,	0.8280 s / batch. (data: 3.09e-04). ETA=6:58:20, max mem: 20.9 GB 
[12/04 11:26:37 visual_prompt]: 	Training 200/553. train loss: 1.4457,	0.8181 s / batch. (data: 3.09e-04). ETA=6:51:58, max mem: 20.9 GB 
[12/04 11:28:17 visual_prompt]: 	Training 300/553. train loss: 0.7079,	0.8517 s / batch. (data: 1.17e-02). ETA=7:07:29, max mem: 20.9 GB 
[12/04 11:30:02 visual_prompt]: 	Training 400/553. train loss: 0.9115,	0.8171 s / batch. (data: 2.95e-04). ETA=6:48:46, max mem: 20.9 GB 
[12/04 11:31:42 visual_prompt]: 	Training 500/553. train loss: 1.3124,	0.8209 s / batch. (data: 3.71e-03). ETA=6:49:16, max mem: 20.9 GB 
[12/04 11:32:36 visual_prompt]: Epoch 46 / 100: avg data time: 1.94e-01, avg batch time: 1.0236, average train loss: 1.0036
[12/04 11:33:33 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3089, average loss: 0.6927
[12/04 11:33:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.21	
[12/04 11:33:33 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[12/04 11:35:15 visual_prompt]: 	Training 100/553. train loss: 0.7017,	0.8354 s / batch. (data: 3.46e-04). ETA=6:54:23, max mem: 20.9 GB 
[12/04 11:36:50 visual_prompt]: 	Training 200/553. train loss: 0.7019,	1.0943 s / batch. (data: 2.52e-01). ETA=9:00:59, max mem: 20.9 GB 
[12/04 11:38:31 visual_prompt]: 	Training 300/553. train loss: 0.6901,	0.8255 s / batch. (data: 3.03e-04). ETA=6:46:43, max mem: 20.9 GB 
[12/04 11:40:10 visual_prompt]: 	Training 400/553. train loss: 0.7228,	0.8176 s / batch. (data: 2.90e-04). ETA=6:41:27, max mem: 20.9 GB 
[12/04 11:41:50 visual_prompt]: 	Training 500/553. train loss: 2.2719,	0.8407 s / batch. (data: 4.66e-03). ETA=6:51:24, max mem: 20.9 GB 
[12/04 11:42:51 visual_prompt]: Epoch 47 / 100: avg data time: 1.77e-01, avg batch time: 1.0088, average train loss: 0.8582
[12/04 11:43:52 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3076, average loss: 0.7338
[12/04 11:43:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.27	
[12/04 11:43:52 visual_prompt]: Stopping early.
[12/04 11:43:52 visual_prompt]: Rank of current process: 0. World size: 1
[12/04 11:43:52 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/04 11:43:52 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/04 11:43:52 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/04 11:43:52 visual_prompt]: Training with config:
[12/04 11:43:52 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.25_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/04 11:43:52 visual_prompt]: Loading training data...
[12/04 11:43:52 visual_prompt]: Constructing mammo-cbis dataset train...
[12/04 11:43:52 visual_prompt]: Loading validation data...
[12/04 11:43:52 visual_prompt]: Constructing mammo-cbis dataset val...
[12/04 11:43:52 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[12/04 11:44:36 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[12/04 11:44:36 visual_prompt]: tuned percent:0.525
[12/04 11:44:36 visual_prompt]: Device used for model: 0
[12/04 11:44:36 visual_prompt]: Setting up Evaluator...
[12/04 11:44:36 visual_prompt]: Setting up Trainer...
[12/04 11:44:36 visual_prompt]: 	Setting up the optimizer...
[12/04 11:44:37 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/04 11:46:27 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8415 s / batch. (data: 2.72e-04). ETA=12:54:08, max mem: 20.9 GB 
[12/04 11:48:08 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8195 s / batch. (data: 3.12e-04). ETA=12:32:36, max mem: 20.9 GB 
[12/04 11:50:04 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.1454 s / batch. (data: 1.32e+00). ETA=1 day, 8:46:35, max mem: 20.9 GB 
[12/04 11:51:48 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8440 s / batch. (data: 7.95e-03). ETA=12:52:14, max mem: 20.9 GB 
[12/04 11:53:38 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8191 s / batch. (data: 2.70e-04). ETA=12:28:05, max mem: 20.9 GB 
[12/04 11:54:40 visual_prompt]: Epoch 1 / 100: avg data time: 2.62e-01, avg batch time: 1.0904, average train loss: 1.5403
[12/04 11:55:53 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3073, average loss: 1.5201
[12/04 11:55:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[12/04 11:55:53 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[12/04 11:58:04 visual_prompt]: 	Training 100/553. train loss: 0.7438,	2.3042 s / batch. (data: 1.49e+00). ETA=1 day, 10:58:36, max mem: 20.9 GB 
[12/04 12:00:07 visual_prompt]: 	Training 200/553. train loss: 0.2134,	0.9693 s / batch. (data: 1.36e-01). ETA=14:41:11, max mem: 20.9 GB 
[12/04 12:02:23 visual_prompt]: 	Training 300/553. train loss: 0.9101,	3.2509 s / batch. (data: 2.44e+00). ETA=2 days, 1:10:03, max mem: 20.9 GB 
[12/04 12:05:26 visual_prompt]: 	Training 400/553. train loss: 1.0998,	0.8105 s / batch. (data: 3.21e-04). ETA=12:14:09, max mem: 20.9 GB 
[12/04 12:08:28 visual_prompt]: 	Training 500/553. train loss: 0.6529,	0.8446 s / batch. (data: 1.20e-02). ETA=12:43:34, max mem: 20.9 GB 
[12/04 12:10:02 visual_prompt]: Epoch 2 / 100: avg data time: 7.10e-01, avg batch time: 1.5352, average train loss: 0.8370
[12/04 12:11:35 visual_prompt]: Inference (val):avg data time: 3.78e-04, avg batch time: 0.3116, average loss: 0.7615
[12/04 12:11:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.85	
[12/04 12:11:35 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[12/04 12:13:54 visual_prompt]: 	Training 100/553. train loss: 0.7621,	0.8544 s / batch. (data: 2.23e-02). ETA=12:50:17, max mem: 20.9 GB 
[12/04 12:15:55 visual_prompt]: 	Training 200/553. train loss: 0.7002,	0.8179 s / batch. (data: 9.46e-04). ETA=12:16:00, max mem: 20.9 GB 
[12/04 12:18:02 visual_prompt]: 	Training 300/553. train loss: 0.6370,	0.8324 s / batch. (data: 3.69e-04). ETA=12:27:39, max mem: 20.9 GB 
[12/04 12:20:07 visual_prompt]: 	Training 400/553. train loss: 0.7117,	0.8585 s / batch. (data: 1.27e-03). ETA=12:49:41, max mem: 20.9 GB 
[12/04 12:22:25 visual_prompt]: 	Training 500/553. train loss: 0.7734,	2.3724 s / batch. (data: 1.56e+00). ETA=1 day, 11:23:04, max mem: 20.9 GB 
[12/04 12:23:30 visual_prompt]: Epoch 3 / 100: avg data time: 4.59e-01, avg batch time: 1.2936, average train loss: 0.7755
[12/04 12:24:43 visual_prompt]: Inference (val):avg data time: 1.10e-04, avg batch time: 0.3109, average loss: 0.7067
[12/04 12:24:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 56.78	
[12/04 12:24:43 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[12/04 12:26:51 visual_prompt]: 	Training 100/553. train loss: 0.7445,	0.8640 s / batch. (data: 2.26e-02). ETA=12:51:00, max mem: 20.9 GB 
[12/04 12:28:54 visual_prompt]: 	Training 200/553. train loss: 1.4261,	0.8734 s / batch. (data: 2.05e-03). ETA=12:57:55, max mem: 20.9 GB 
[12/04 12:31:00 visual_prompt]: 	Training 300/553. train loss: 0.6587,	1.5460 s / batch. (data: 7.25e-01). ETA=22:54:23, max mem: 20.9 GB 
[12/04 12:32:57 visual_prompt]: 	Training 400/553. train loss: 0.5731,	2.2741 s / batch. (data: 1.42e+00). ETA=1 day, 9:37:54, max mem: 20.9 GB 
[12/04 12:35:00 visual_prompt]: 	Training 500/553. train loss: 0.6808,	4.7930 s / batch. (data: 3.96e+00). ETA=2 days, 22:45:05, max mem: 20.9 GB 
[12/04 12:36:03 visual_prompt]: Epoch 4 / 100: avg data time: 3.93e-01, avg batch time: 1.2288, average train loss: 0.8678
[12/04 12:37:10 visual_prompt]: Inference (val):avg data time: 7.16e-05, avg batch time: 0.3101, average loss: 0.8937
[12/04 12:37:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.42	
[12/04 12:37:10 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[12/04 12:39:23 visual_prompt]: 	Training 100/553. train loss: 0.5147,	0.8639 s / batch. (data: 2.51e-02). ETA=12:42:54, max mem: 20.9 GB 
[12/04 12:41:20 visual_prompt]: 	Training 200/553. train loss: 0.6129,	1.8123 s / batch. (data: 9.90e-01). ETA=1 day, 2:37:28, max mem: 20.9 GB 
[12/04 12:43:23 visual_prompt]: 	Training 300/553. train loss: 1.5914,	0.8600 s / batch. (data: 8.71e-04). ETA=12:36:36, max mem: 20.9 GB 
[12/04 12:45:19 visual_prompt]: 	Training 400/553. train loss: 1.0384,	0.8515 s / batch. (data: 1.40e-02). ETA=12:27:43, max mem: 20.9 GB 
[12/04 12:47:26 visual_prompt]: 	Training 500/553. train loss: 0.5678,	0.8599 s / batch. (data: 1.18e-02). ETA=12:33:42, max mem: 20.9 GB 
[12/04 12:48:26 visual_prompt]: Epoch 5 / 100: avg data time: 3.84e-01, avg batch time: 1.2222, average train loss: 0.9647
[12/04 12:49:39 visual_prompt]: Inference (val):avg data time: 1.10e-04, avg batch time: 0.3115, average loss: 0.8294
[12/04 12:49:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.77	
[12/04 12:49:39 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[12/04 12:51:51 visual_prompt]: 	Training 100/553. train loss: 0.5381,	0.8574 s / batch. (data: 3.29e-03). ETA=12:29:16, max mem: 20.9 GB 
[12/04 12:53:49 visual_prompt]: 	Training 200/553. train loss: 0.7757,	0.8340 s / batch. (data: 9.45e-03). ETA=12:07:27, max mem: 20.9 GB 
[12/04 12:55:51 visual_prompt]: 	Training 300/553. train loss: 0.5882,	0.8395 s / batch. (data: 1.43e-03). ETA=12:10:48, max mem: 20.9 GB 
[12/04 12:57:53 visual_prompt]: 	Training 400/553. train loss: 0.6270,	0.8646 s / batch. (data: 2.19e-02). ETA=12:31:18, max mem: 20.9 GB 
[12/04 12:59:56 visual_prompt]: 	Training 500/553. train loss: 0.9022,	1.6222 s / batch. (data: 7.71e-01). ETA=23:26:53, max mem: 20.9 GB 
[12/04 13:00:59 visual_prompt]: Epoch 6 / 100: avg data time: 3.90e-01, avg batch time: 1.2282, average train loss: 0.8342
[12/04 13:02:05 visual_prompt]: Inference (val):avg data time: 5.30e-05, avg batch time: 0.3095, average loss: 0.7804
[12/04 13:02:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.93	
[12/04 13:02:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[12/04 13:04:15 visual_prompt]: 	Training 100/553. train loss: 1.0074,	0.8518 s / batch. (data: 1.18e-02). ETA=12:16:35, max mem: 20.9 GB 
[12/04 13:06:12 visual_prompt]: 	Training 200/553. train loss: 0.5370,	0.8349 s / batch. (data: 1.13e-03). ETA=12:00:35, max mem: 20.9 GB 
[12/04 13:08:20 visual_prompt]: 	Training 300/553. train loss: 0.5949,	2.3037 s / batch. (data: 1.45e+00). ETA=1 day, 9:04:22, max mem: 20.9 GB 
[12/04 13:10:21 visual_prompt]: 	Training 400/553. train loss: 0.5927,	2.6059 s / batch. (data: 1.79e+00). ETA=1 day, 13:20:15, max mem: 20.9 GB 
[12/04 13:12:25 visual_prompt]: 	Training 500/553. train loss: 1.4072,	0.8627 s / batch. (data: 1.06e-02). ETA=12:20:14, max mem: 20.9 GB 
[12/04 13:13:29 visual_prompt]: Epoch 7 / 100: avg data time: 4.01e-01, avg batch time: 1.2366, average train loss: 0.8535
[12/04 13:14:39 visual_prompt]: Inference (val):avg data time: 2.71e-04, avg batch time: 0.3115, average loss: 0.7186
[12/04 13:14:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.45	
[12/04 13:14:39 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[12/04 13:16:51 visual_prompt]: 	Training 100/553. train loss: 0.7326,	0.8485 s / batch. (data: 1.19e-02). ETA=12:05:51, max mem: 20.9 GB 
[12/04 13:18:54 visual_prompt]: 	Training 200/553. train loss: 1.6427,	0.8257 s / batch. (data: 1.05e-02). ETA=11:45:01, max mem: 20.9 GB 
[12/04 13:20:59 visual_prompt]: 	Training 300/553. train loss: 0.8773,	0.8599 s / batch. (data: 2.02e-03). ETA=12:12:43, max mem: 20.9 GB 
[12/04 13:22:59 visual_prompt]: 	Training 400/553. train loss: 0.7498,	0.8668 s / batch. (data: 2.04e-03). ETA=12:17:09, max mem: 20.9 GB 
[12/04 13:25:02 visual_prompt]: 	Training 500/553. train loss: 1.2193,	2.2015 s / batch. (data: 1.38e+00). ETA=1 day, 7:08:39, max mem: 20.9 GB 
[12/04 13:26:05 visual_prompt]: Epoch 8 / 100: avg data time: 4.04e-01, avg batch time: 1.2417, average train loss: 0.9269
[12/04 13:27:14 visual_prompt]: Inference (val):avg data time: 3.14e-04, avg batch time: 0.3119, average loss: 1.3096
[12/04 13:27:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.43	
[12/04 13:27:14 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[12/04 13:29:19 visual_prompt]: 	Training 100/553. train loss: 0.3751,	0.8516 s / batch. (data: 4.96e-03). ETA=12:00:41, max mem: 20.9 GB 
[12/04 13:31:17 visual_prompt]: 	Training 200/553. train loss: 0.5990,	0.8471 s / batch. (data: 6.46e-03). ETA=11:55:27, max mem: 20.9 GB 
[12/04 13:33:19 visual_prompt]: 	Training 300/553. train loss: 0.7508,	2.4919 s / batch. (data: 1.67e+00). ETA=1 day, 11:00:27, max mem: 20.9 GB 
[12/04 13:35:21 visual_prompt]: 	Training 400/553. train loss: 0.5719,	0.8494 s / batch. (data: 1.78e-02). ETA=11:54:36, max mem: 20.9 GB 
[12/04 13:37:21 visual_prompt]: 	Training 500/553. train loss: 0.6019,	1.0135 s / batch. (data: 1.73e-01). ETA=14:10:55, max mem: 20.9 GB 
[12/04 13:38:24 visual_prompt]: Epoch 9 / 100: avg data time: 3.72e-01, avg batch time: 1.2101, average train loss: 0.8506
[12/04 13:39:32 visual_prompt]: Inference (val):avg data time: 4.83e-04, avg batch time: 0.3109, average loss: 0.9252
[12/04 13:39:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.27	
[12/04 13:39:32 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[12/04 13:41:48 visual_prompt]: 	Training 100/553. train loss: 1.1945,	0.8290 s / batch. (data: 1.16e-03). ETA=11:33:55, max mem: 20.9 GB 
[12/04 13:43:55 visual_prompt]: 	Training 200/553. train loss: 0.5637,	0.8139 s / batch. (data: 3.56e-04). ETA=11:19:53, max mem: 20.9 GB 
[12/04 13:45:57 visual_prompt]: 	Training 300/553. train loss: 0.6080,	0.8634 s / batch. (data: 1.82e-03). ETA=11:59:50, max mem: 20.9 GB 
[12/04 13:48:00 visual_prompt]: 	Training 400/553. train loss: 0.7187,	1.1480 s / batch. (data: 3.09e-01). ETA=15:55:11, max mem: 20.9 GB 
[12/04 13:50:05 visual_prompt]: 	Training 500/553. train loss: 1.3974,	0.8591 s / batch. (data: 1.03e-02). ETA=11:53:22, max mem: 20.9 GB 
[12/04 13:51:18 visual_prompt]: Epoch 10 / 100: avg data time: 4.41e-01, avg batch time: 1.2758, average train loss: 0.9451
[12/04 13:52:31 visual_prompt]: Inference (val):avg data time: 2.77e-04, avg batch time: 0.3128, average loss: 0.6622
[12/04 13:52:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.60	
[12/04 13:52:31 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[12/04 13:54:38 visual_prompt]: 	Training 100/553. train loss: 1.1209,	0.8486 s / batch. (data: 1.19e-03). ETA=11:42:28, max mem: 20.9 GB 
[12/04 13:56:46 visual_prompt]: 	Training 200/553. train loss: 1.5567,	0.8572 s / batch. (data: 1.10e-02). ETA=11:48:09, max mem: 20.9 GB 
[12/04 13:58:47 visual_prompt]: 	Training 300/553. train loss: 0.1204,	2.2623 s / batch. (data: 1.43e+00). ETA=1 day, 7:05:13, max mem: 20.9 GB 
[12/04 14:00:54 visual_prompt]: 	Training 400/553. train loss: 0.6646,	0.8481 s / batch. (data: 6.17e-03). ETA=11:37:52, max mem: 20.9 GB 
[12/04 14:02:53 visual_prompt]: 	Training 500/553. train loss: 0.7281,	0.8393 s / batch. (data: 1.06e-02). ETA=11:29:14, max mem: 20.9 GB 
[12/04 14:03:55 visual_prompt]: Epoch 11 / 100: avg data time: 4.02e-01, avg batch time: 1.2356, average train loss: 0.9384
[12/04 14:05:08 visual_prompt]: Inference (val):avg data time: 1.25e-04, avg batch time: 0.3119, average loss: 0.6852
[12/04 14:05:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 65.33	
[12/04 14:05:08 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[12/04 14:07:14 visual_prompt]: 	Training 100/553. train loss: 0.7452,	0.8232 s / batch. (data: 5.40e-03). ETA=11:13:51, max mem: 20.9 GB 
[12/04 14:09:18 visual_prompt]: 	Training 200/553. train loss: 0.7320,	1.0982 s / batch. (data: 2.56e-01). ETA=14:57:10, max mem: 20.9 GB 
[12/04 14:11:19 visual_prompt]: 	Training 300/553. train loss: 0.5731,	0.8331 s / batch. (data: 1.05e-02). ETA=11:19:13, max mem: 20.9 GB 
[12/04 14:13:19 visual_prompt]: 	Training 400/553. train loss: 0.6994,	0.8586 s / batch. (data: 6.74e-03). ETA=11:38:33, max mem: 20.9 GB 
[12/04 14:15:21 visual_prompt]: 	Training 500/553. train loss: 2.1286,	0.8315 s / batch. (data: 1.27e-02). ETA=11:15:10, max mem: 20.9 GB 
[12/04 14:16:22 visual_prompt]: Epoch 12 / 100: avg data time: 3.82e-01, avg batch time: 1.2182, average train loss: 0.8608
[12/04 14:17:35 visual_prompt]: Inference (val):avg data time: 1.35e-04, avg batch time: 0.3129, average loss: 1.9649
[12/04 14:17:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.59	
[12/04 14:17:35 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[12/04 14:19:44 visual_prompt]: 	Training 100/553. train loss: 0.7960,	0.8533 s / batch. (data: 1.57e-02). ETA=11:30:38, max mem: 20.9 GB 
[12/04 14:21:37 visual_prompt]: 	Training 200/553. train loss: 0.6433,	0.8316 s / batch. (data: 6.46e-03). ETA=11:11:42, max mem: 20.9 GB 
[12/04 14:23:40 visual_prompt]: 	Training 300/553. train loss: 0.6373,	1.8889 s / batch. (data: 1.04e+00). ETA=1 day, 1:22:32, max mem: 20.9 GB 
[12/04 14:25:37 visual_prompt]: 	Training 400/553. train loss: 2.1056,	0.8249 s / batch. (data: 8.08e-04). ETA=11:03:32, max mem: 20.9 GB 
[12/04 14:27:40 visual_prompt]: 	Training 500/553. train loss: 0.8410,	0.8871 s / batch. (data: 6.17e-03). ETA=11:52:07, max mem: 20.9 GB 
[12/04 14:28:46 visual_prompt]: Epoch 13 / 100: avg data time: 3.75e-01, avg batch time: 1.2124, average train loss: 0.8787
[12/04 14:29:58 visual_prompt]: Inference (val):avg data time: 1.06e-04, avg batch time: 0.3097, average loss: 0.9326
[12/04 14:29:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.42	
[12/04 14:29:58 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[12/04 14:32:01 visual_prompt]: 	Training 100/553. train loss: 0.5916,	0.8482 s / batch. (data: 1.19e-02). ETA=11:18:44, max mem: 20.9 GB 
[12/04 14:34:02 visual_prompt]: 	Training 200/553. train loss: 0.5386,	2.0697 s / batch. (data: 1.24e+00). ETA=1 day, 3:32:40, max mem: 20.9 GB 
[12/04 14:35:59 visual_prompt]: 	Training 300/553. train loss: 0.8136,	0.8520 s / batch. (data: 1.19e-02). ETA=11:18:53, max mem: 20.9 GB 
[12/04 14:38:02 visual_prompt]: 	Training 400/553. train loss: 0.9694,	0.8363 s / batch. (data: 6.31e-04). ETA=11:04:59, max mem: 20.9 GB 
[12/04 14:40:05 visual_prompt]: 	Training 500/553. train loss: 1.3713,	0.8360 s / batch. (data: 4.04e-04). ETA=11:03:23, max mem: 20.9 GB 
[12/04 14:41:01 visual_prompt]: Epoch 14 / 100: avg data time: 3.64e-01, avg batch time: 1.1991, average train loss: 0.8128
[12/04 14:42:14 visual_prompt]: Inference (val):avg data time: 1.13e-04, avg batch time: 0.3133, average loss: 0.6864
[12/04 14:42:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 62.89	
[12/04 14:42:14 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[12/04 14:44:20 visual_prompt]: 	Training 100/553. train loss: 0.6960,	0.8240 s / batch. (data: 3.99e-04). ETA=10:51:44, max mem: 20.9 GB 
[12/04 14:46:17 visual_prompt]: 	Training 200/553. train loss: 1.5204,	0.8423 s / batch. (data: 1.06e-02). ETA=11:04:51, max mem: 20.9 GB 
[12/04 14:48:23 visual_prompt]: 	Training 300/553. train loss: 0.5794,	0.8481 s / batch. (data: 6.07e-04). ETA=11:08:00, max mem: 20.9 GB 
[12/04 14:50:24 visual_prompt]: 	Training 400/553. train loss: 0.4191,	0.8480 s / batch. (data: 1.19e-02). ETA=11:06:29, max mem: 20.9 GB 
[12/04 14:52:32 visual_prompt]: 	Training 500/553. train loss: 0.7216,	0.8361 s / batch. (data: 7.57e-03). ETA=10:55:43, max mem: 20.9 GB 
[12/04 14:53:40 visual_prompt]: Epoch 15 / 100: avg data time: 4.06e-01, avg batch time: 1.2407, average train loss: 0.9148
[12/04 14:54:57 visual_prompt]: Inference (val):avg data time: 7.71e-05, avg batch time: 0.3105, average loss: 1.8960
[12/04 14:54:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.82	
[12/04 14:54:57 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[12/04 14:57:21 visual_prompt]: 	Training 100/553. train loss: 0.5959,	0.8313 s / batch. (data: 2.00e-03). ETA=10:49:50, max mem: 20.9 GB 
[12/04 14:59:42 visual_prompt]: 	Training 200/553. train loss: 1.1388,	0.8400 s / batch. (data: 1.20e-02). ETA=10:55:16, max mem: 20.9 GB 
[12/04 15:01:47 visual_prompt]: 	Training 300/553. train loss: 0.8300,	0.8837 s / batch. (data: 7.38e-03). ETA=11:27:51, max mem: 20.9 GB 
[12/04 15:03:59 visual_prompt]: 	Training 400/553. train loss: 0.4894,	0.8418 s / batch. (data: 3.33e-04). ETA=10:53:50, max mem: 20.9 GB 
[12/04 15:06:06 visual_prompt]: 	Training 500/553. train loss: 1.0545,	2.2844 s / batch. (data: 1.43e+00). ETA=1 day, 5:30:37, max mem: 20.9 GB 
[12/04 15:07:18 visual_prompt]: Epoch 16 / 100: avg data time: 5.07e-01, avg batch time: 1.3393, average train loss: 0.8112
[12/04 15:08:32 visual_prompt]: Inference (val):avg data time: 4.37e-04, avg batch time: 0.3105, average loss: 0.6750
[12/04 15:08:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.97	
[12/04 15:08:32 visual_prompt]: Best epoch 16: best metric: -0.675
[12/04 15:08:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[12/04 15:10:42 visual_prompt]: 	Training 100/553. train loss: 0.3698,	0.8547 s / batch. (data: 2.64e-02). ETA=11:00:17, max mem: 20.9 GB 
[12/04 15:12:49 visual_prompt]: 	Training 200/553. train loss: 1.2701,	0.8381 s / batch. (data: 1.00e-02). ETA=10:46:01, max mem: 20.9 GB 
[12/04 15:14:52 visual_prompt]: 	Training 300/553. train loss: 1.2673,	0.8500 s / batch. (data: 1.47e-03). ETA=10:53:50, max mem: 20.9 GB 
[12/04 15:16:52 visual_prompt]: 	Training 400/553. train loss: 0.6794,	0.8440 s / batch. (data: 1.47e-03). ETA=10:47:47, max mem: 20.9 GB 
[12/04 15:18:52 visual_prompt]: 	Training 500/553. train loss: 0.8860,	2.3421 s / batch. (data: 1.51e+00). ETA=1 day, 5:53:46, max mem: 20.9 GB 
[12/04 15:19:55 visual_prompt]: Epoch 17 / 100: avg data time: 4.00e-01, avg batch time: 1.2360, average train loss: 0.8059
[12/04 15:21:08 visual_prompt]: Inference (val):avg data time: 1.21e-04, avg batch time: 0.3106, average loss: 0.7225
[12/04 15:21:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.79	
[12/04 15:21:08 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[12/04 15:23:13 visual_prompt]: 	Training 100/553. train loss: 0.5705,	0.8194 s / batch. (data: 1.13e-03). ETA=10:25:28, max mem: 20.9 GB 
[12/04 15:25:19 visual_prompt]: 	Training 200/553. train loss: 0.6109,	0.8486 s / batch. (data: 1.14e-02). ETA=10:46:19, max mem: 20.9 GB 
[12/04 15:27:23 visual_prompt]: 	Training 300/553. train loss: 0.3658,	0.8479 s / batch. (data: 1.39e-03). ETA=10:44:24, max mem: 20.9 GB 
[12/04 15:29:22 visual_prompt]: 	Training 400/553. train loss: 0.6038,	0.8650 s / batch. (data: 1.11e-02). ETA=10:55:55, max mem: 20.9 GB 
[12/04 15:31:25 visual_prompt]: 	Training 500/553. train loss: 0.8911,	0.8140 s / batch. (data: 3.88e-04). ETA=10:15:55, max mem: 20.9 GB 
[12/04 15:32:24 visual_prompt]: Epoch 18 / 100: avg data time: 3.86e-01, avg batch time: 1.2217, average train loss: 0.8631
[12/04 15:33:37 visual_prompt]: Inference (val):avg data time: 1.23e-04, avg batch time: 0.3138, average loss: 0.6621
[12/04 15:33:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 64.40	
[12/04 15:33:37 visual_prompt]: Best epoch 18: best metric: -0.662
[12/04 15:33:37 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[12/04 15:35:52 visual_prompt]: 	Training 100/553. train loss: 0.6142,	0.8201 s / batch. (data: 1.17e-03). ETA=10:18:24, max mem: 20.9 GB 
[12/04 15:37:49 visual_prompt]: 	Training 200/553. train loss: 0.4699,	0.8524 s / batch. (data: 7.97e-03). ETA=10:41:21, max mem: 20.9 GB 
[12/04 15:39:52 visual_prompt]: 	Training 300/553. train loss: 1.2461,	1.9198 s / batch. (data: 1.11e+00). ETA=1 day, 0:01:17, max mem: 20.9 GB 
[12/04 15:41:54 visual_prompt]: 	Training 400/553. train loss: 0.6849,	0.8569 s / batch. (data: 1.68e-02). ETA=10:41:56, max mem: 20.9 GB 
[12/04 15:43:53 visual_prompt]: 	Training 500/553. train loss: 0.8544,	0.8679 s / batch. (data: 1.58e-02). ETA=10:48:40, max mem: 20.9 GB 
[12/04 15:44:59 visual_prompt]: Epoch 19 / 100: avg data time: 3.96e-01, avg batch time: 1.2324, average train loss: 0.7991
[12/04 15:46:12 visual_prompt]: Inference (val):avg data time: 1.11e-04, avg batch time: 0.3110, average loss: 1.4556
[12/04 15:46:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.28	
[12/04 15:46:12 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[12/04 15:48:14 visual_prompt]: 	Training 100/553. train loss: 0.6127,	0.8618 s / batch. (data: 1.29e-03). ETA=10:41:54, max mem: 20.9 GB 
[12/04 15:50:21 visual_prompt]: 	Training 200/553. train loss: 0.3464,	0.8308 s / batch. (data: 3.40e-04). ETA=10:17:29, max mem: 20.9 GB 
[12/04 15:52:29 visual_prompt]: 	Training 300/553. train loss: 1.0631,	0.8245 s / batch. (data: 3.24e-04). ETA=10:11:22, max mem: 20.9 GB 
[12/04 15:54:36 visual_prompt]: 	Training 400/553. train loss: 0.6841,	0.8316 s / batch. (data: 1.28e-03). ETA=10:15:15, max mem: 20.9 GB 
[12/04 15:56:38 visual_prompt]: 	Training 500/553. train loss: 2.1441,	0.8244 s / batch. (data: 3.05e-04). ETA=10:08:34, max mem: 20.9 GB 
[12/04 15:57:50 visual_prompt]: Epoch 20 / 100: avg data time: 4.26e-01, avg batch time: 1.2602, average train loss: 0.8370
[12/04 15:59:08 visual_prompt]: Inference (val):avg data time: 1.04e-04, avg batch time: 0.3120, average loss: 0.8575
[12/04 15:59:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 67.19	
[12/04 15:59:08 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[12/04 16:01:17 visual_prompt]: 	Training 100/553. train loss: 0.4125,	0.8333 s / batch. (data: 1.24e-03). ETA=10:12:59, max mem: 20.9 GB 
[12/04 16:03:27 visual_prompt]: 	Training 200/553. train loss: 0.3525,	0.8815 s / batch. (data: 1.60e-02). ETA=10:47:02, max mem: 20.9 GB 
[12/04 16:05:35 visual_prompt]: 	Training 300/553. train loss: 0.7622,	1.7858 s / batch. (data: 9.63e-01). ETA=21:47:49, max mem: 20.9 GB 
[12/04 16:07:32 visual_prompt]: 	Training 400/553. train loss: 1.0077,	0.8617 s / batch. (data: 1.28e-03). ETA=10:29:38, max mem: 20.9 GB 
[12/04 16:09:36 visual_prompt]: 	Training 500/553. train loss: 0.6925,	0.8240 s / batch. (data: 3.96e-04). ETA=10:00:41, max mem: 20.9 GB 
[12/04 16:10:34 visual_prompt]: Epoch 21 / 100: avg data time: 4.06e-01, avg batch time: 1.2404, average train loss: 0.8104
[12/04 16:11:47 visual_prompt]: Inference (val):avg data time: 1.11e-04, avg batch time: 0.3100, average loss: 0.6641
[12/04 16:11:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 65.49	
[12/04 16:11:47 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[12/04 16:13:52 visual_prompt]: 	Training 100/553. train loss: 0.6656,	0.8460 s / batch. (data: 1.00e-02). ETA=10:14:35, max mem: 20.9 GB 
[12/04 16:15:54 visual_prompt]: 	Training 200/553. train loss: 0.7585,	0.8406 s / batch. (data: 5.57e-03). ETA=10:09:14, max mem: 20.9 GB 
[12/04 16:17:52 visual_prompt]: 	Training 300/553. train loss: 0.4793,	0.8385 s / batch. (data: 1.04e-02). ETA=10:06:21, max mem: 20.9 GB 
[12/04 16:20:01 visual_prompt]: 	Training 400/553. train loss: 0.9132,	0.8451 s / batch. (data: 3.54e-04). ETA=10:09:42, max mem: 20.9 GB 
[12/04 16:22:02 visual_prompt]: 	Training 500/553. train loss: 0.7591,	0.8471 s / batch. (data: 1.68e-02). ETA=10:09:41, max mem: 20.9 GB 
[12/04 16:23:10 visual_prompt]: Epoch 22 / 100: avg data time: 3.99e-01, avg batch time: 1.2351, average train loss: 0.7916
[12/04 16:24:22 visual_prompt]: Inference (val):avg data time: 9.89e-05, avg batch time: 0.3097, average loss: 0.7053
[12/04 16:24:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 68.71	
[12/04 16:24:22 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[12/04 16:26:46 visual_prompt]: 	Training 100/553. train loss: 0.9378,	1.6001 s / batch. (data: 7.77e-01). ETA=19:07:37, max mem: 20.9 GB 
[12/04 16:28:59 visual_prompt]: 	Training 200/553. train loss: 0.7419,	1.6259 s / batch. (data: 8.11e-01). ETA=19:23:26, max mem: 20.9 GB 
[12/04 16:31:07 visual_prompt]: 	Training 300/553. train loss: 1.3093,	0.8145 s / batch. (data: 7.58e-04). ETA=9:41:28, max mem: 20.9 GB 
[12/04 16:33:12 visual_prompt]: 	Training 400/553. train loss: 0.5014,	0.8341 s / batch. (data: 9.20e-04). ETA=9:54:04, max mem: 20.9 GB 
[12/04 16:35:15 visual_prompt]: 	Training 500/553. train loss: 0.8242,	0.8684 s / batch. (data: 7.96e-03). ETA=10:17:02, max mem: 20.9 GB 
[12/04 16:36:21 visual_prompt]: Epoch 23 / 100: avg data time: 4.66e-01, avg batch time: 1.2994, average train loss: 0.7951
[12/04 16:37:38 visual_prompt]: Inference (val):avg data time: 9.92e-05, avg batch time: 0.3108, average loss: 0.6610
[12/04 16:37:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 66.63	
[12/04 16:37:38 visual_prompt]: Best epoch 23: best metric: -0.661
[12/04 16:37:38 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[12/04 16:39:51 visual_prompt]: 	Training 100/553. train loss: 0.6541,	0.8328 s / batch. (data: 1.34e-03). ETA=9:49:36, max mem: 20.9 GB 
[12/04 16:41:53 visual_prompt]: 	Training 200/553. train loss: 0.7373,	0.8490 s / batch. (data: 5.52e-03). ETA=9:59:40, max mem: 20.9 GB 
[12/04 16:43:54 visual_prompt]: 	Training 300/553. train loss: 0.5749,	1.8477 s / batch. (data: 1.02e+00). ETA=21:42:01, max mem: 20.9 GB 
[12/04 16:45:57 visual_prompt]: 	Training 400/553. train loss: 0.4404,	0.8411 s / batch. (data: 3.46e-04). ETA=9:51:17, max mem: 20.9 GB 
[12/04 16:48:07 visual_prompt]: 	Training 500/553. train loss: 0.7676,	0.8609 s / batch. (data: 3.35e-02). ETA=10:03:46, max mem: 20.9 GB 
[12/04 16:49:09 visual_prompt]: Epoch 24 / 100: avg data time: 4.17e-01, avg batch time: 1.2504, average train loss: 0.8561
[12/04 16:50:20 visual_prompt]: Inference (val):avg data time: 1.03e-04, avg batch time: 0.3093, average loss: 1.2594
[12/04 16:50:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.59	
[12/04 16:50:20 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[12/04 16:52:29 visual_prompt]: 	Training 100/553. train loss: 0.8078,	0.8372 s / batch. (data: 1.05e-02). ETA=9:45:03, max mem: 20.9 GB 
[12/04 16:54:22 visual_prompt]: 	Training 200/553. train loss: 1.6847,	0.8859 s / batch. (data: 3.83e-02). ETA=10:17:34, max mem: 20.9 GB 
[12/04 16:56:21 visual_prompt]: 	Training 300/553. train loss: 0.6692,	0.8675 s / batch. (data: 4.50e-02). ETA=10:03:19, max mem: 20.9 GB 
[12/04 16:58:21 visual_prompt]: 	Training 400/553. train loss: 1.1286,	2.2131 s / batch. (data: 1.39e+00). ETA=1 day, 1:35:25, max mem: 20.9 GB 
[12/04 17:00:23 visual_prompt]: 	Training 500/553. train loss: 0.8395,	2.1429 s / batch. (data: 1.33e+00). ETA=1 day, 0:43:11, max mem: 20.9 GB 
[12/04 17:01:27 visual_prompt]: Epoch 25 / 100: avg data time: 3.71e-01, avg batch time: 1.2053, average train loss: 0.9025
[12/04 17:02:40 visual_prompt]: Inference (val):avg data time: 9.73e-05, avg batch time: 0.3100, average loss: 1.3245
[12/04 17:02:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 69.97	
[12/04 17:02:40 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[12/04 17:04:45 visual_prompt]: 	Training 100/553. train loss: 0.3550,	0.8437 s / batch. (data: 1.06e-03). ETA=9:41:47, max mem: 20.9 GB 
[12/04 17:06:47 visual_prompt]: 	Training 200/553. train loss: 1.0625,	2.2998 s / batch. (data: 1.45e+00). ETA=1 day, 2:22:04, max mem: 20.9 GB 
[12/04 17:08:49 visual_prompt]: 	Training 300/553. train loss: 0.7684,	0.8512 s / batch. (data: 2.31e-02). ETA=9:44:07, max mem: 20.9 GB 
[12/04 17:10:45 visual_prompt]: 	Training 400/553. train loss: 0.9725,	0.8344 s / batch. (data: 1.16e-03). ETA=9:31:12, max mem: 20.9 GB 
[12/04 17:12:43 visual_prompt]: 	Training 500/553. train loss: 0.6735,	0.8315 s / batch. (data: 1.18e-02). ETA=9:27:51, max mem: 20.9 GB 
[12/04 17:13:46 visual_prompt]: Epoch 26 / 100: avg data time: 3.69e-01, avg batch time: 1.2041, average train loss: 0.8532
[12/04 17:14:53 visual_prompt]: Inference (val):avg data time: 6.64e-05, avg batch time: 0.3109, average loss: 0.7897
[12/04 17:14:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.19	rocauc: 64.02	
[12/04 17:14:53 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[12/04 17:17:01 visual_prompt]: 	Training 100/553. train loss: 0.5590,	0.8485 s / batch. (data: 7.88e-03). ETA=9:37:15, max mem: 20.9 GB 
[12/04 17:19:02 visual_prompt]: 	Training 200/553. train loss: 0.6246,	2.6399 s / batch. (data: 1.80e+00). ETA=1 day, 5:51:41, max mem: 20.9 GB 
[12/04 17:21:00 visual_prompt]: 	Training 300/553. train loss: 0.6184,	0.8360 s / batch. (data: 3.29e-04). ETA=9:26:00, max mem: 20.9 GB 
[12/04 17:23:02 visual_prompt]: 	Training 400/553. train loss: 0.9961,	0.8381 s / batch. (data: 1.08e-03). ETA=9:25:59, max mem: 20.9 GB 
[12/04 17:25:00 visual_prompt]: 	Training 500/553. train loss: 0.6948,	0.8385 s / batch. (data: 5.94e-03). ETA=9:24:54, max mem: 20.9 GB 
[12/04 17:26:01 visual_prompt]: Epoch 27 / 100: avg data time: 3.71e-01, avg batch time: 1.2065, average train loss: 0.7795
[12/04 17:27:13 visual_prompt]: Inference (val):avg data time: 1.05e-04, avg batch time: 0.3112, average loss: 0.6464
[12/04 17:27:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 70.64	
[12/04 17:27:13 visual_prompt]: Best epoch 27: best metric: -0.646
[12/04 17:27:13 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[12/04 17:29:19 visual_prompt]: 	Training 100/553. train loss: 0.4128,	0.8201 s / batch. (data: 1.33e-03). ETA=9:10:22, max mem: 20.9 GB 
[12/04 17:31:20 visual_prompt]: 	Training 200/553. train loss: 0.3812,	0.8304 s / batch. (data: 5.27e-04). ETA=9:15:55, max mem: 20.9 GB 
[12/04 17:33:21 visual_prompt]: 	Training 300/553. train loss: 0.9904,	2.4180 s / batch. (data: 1.57e+00). ETA=1 day, 2:54:45, max mem: 20.9 GB 
[12/04 17:35:18 visual_prompt]: 	Training 400/553. train loss: 0.5543,	0.8443 s / batch. (data: 5.48e-03). ETA=9:22:23, max mem: 20.9 GB 
[12/04 17:37:17 visual_prompt]: 	Training 500/553. train loss: 0.4060,	0.8400 s / batch. (data: 1.33e-03). ETA=9:18:11, max mem: 20.9 GB 
[12/04 17:38:22 visual_prompt]: Epoch 28 / 100: avg data time: 3.72e-01, avg batch time: 1.2088, average train loss: 0.7581
[12/04 17:39:33 visual_prompt]: Inference (val):avg data time: 6.22e-04, avg batch time: 0.3142, average loss: 0.6225
[12/04 17:39:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 70.70	
[12/04 17:39:33 visual_prompt]: Best epoch 28: best metric: -0.622
[12/04 17:39:33 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[12/04 17:41:47 visual_prompt]: 	Training 100/553. train loss: 0.1783,	0.8460 s / batch. (data: 1.05e-02). ETA=9:20:00, max mem: 20.9 GB 
[12/04 17:43:46 visual_prompt]: 	Training 200/553. train loss: 1.4014,	2.7236 s / batch. (data: 1.87e+00). ETA=1 day, 5:58:17, max mem: 20.9 GB 
[12/04 17:45:41 visual_prompt]: 	Training 300/553. train loss: 0.7988,	0.8407 s / batch. (data: 1.06e-02). ETA=9:13:41, max mem: 20.9 GB 
[12/04 17:47:37 visual_prompt]: 	Training 400/553. train loss: 0.6739,	0.8567 s / batch. (data: 1.34e-03). ETA=9:22:46, max mem: 20.9 GB 
[12/04 17:49:49 visual_prompt]: 	Training 500/553. train loss: 0.6385,	2.2012 s / batch. (data: 1.38e+00). ETA=1 day, 0:02:21, max mem: 20.9 GB 
[12/04 17:50:56 visual_prompt]: Epoch 29 / 100: avg data time: 3.98e-01, avg batch time: 1.2337, average train loss: 0.8325
[12/04 17:52:19 visual_prompt]: Inference (val):avg data time: 8.47e-05, avg batch time: 0.3076, average loss: 0.6432
[12/04 17:52:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 71.15	
[12/04 17:52:19 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[12/04 17:54:41 visual_prompt]: 	Training 100/553. train loss: 1.3914,	0.8204 s / batch. (data: 5.98e-03). ETA=8:55:29, max mem: 20.9 GB 
[12/04 17:56:53 visual_prompt]: 	Training 200/553. train loss: 0.8902,	0.8131 s / batch. (data: 3.10e-04). ETA=8:49:21, max mem: 20.9 GB 
[12/04 17:59:02 visual_prompt]: 	Training 300/553. train loss: 0.3225,	2.2933 s / batch. (data: 1.48e+00). ETA=1 day, 0:49:13, max mem: 20.9 GB 
[12/04 18:01:16 visual_prompt]: 	Training 400/553. train loss: 1.4520,	1.7693 s / batch. (data: 9.35e-01). ETA=19:05:59, max mem: 20.9 GB 
[12/04 18:03:19 visual_prompt]: 	Training 500/553. train loss: 0.4983,	1.9939 s / batch. (data: 1.18e+00). ETA=21:28:11, max mem: 20.9 GB 
[12/04 18:04:23 visual_prompt]: Epoch 30 / 100: avg data time: 4.76e-01, avg batch time: 1.3081, average train loss: 0.7754
[12/04 18:05:33 visual_prompt]: Inference (val):avg data time: 9.22e-05, avg batch time: 0.3097, average loss: 0.7292
[12/04 18:05:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 72.53	
[12/04 18:05:33 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[12/04 18:07:39 visual_prompt]: 	Training 100/553. train loss: 0.6565,	0.8771 s / batch. (data: 2.50e-02). ETA=9:24:24, max mem: 20.9 GB 
[12/04 18:09:39 visual_prompt]: 	Training 200/553. train loss: 1.0729,	0.8285 s / batch. (data: 6.42e-03). ETA=8:51:44, max mem: 20.9 GB 
[12/04 18:11:37 visual_prompt]: 	Training 300/553. train loss: 0.7365,	0.8385 s / batch. (data: 1.58e-03). ETA=8:56:46, max mem: 20.9 GB 
[12/04 18:13:36 visual_prompt]: 	Training 400/553. train loss: 0.5884,	1.5750 s / batch. (data: 7.32e-01). ETA=16:45:37, max mem: 20.9 GB 
[12/04 18:15:33 visual_prompt]: 	Training 500/553. train loss: 0.7347,	0.8426 s / batch. (data: 5.46e-03). ETA=8:56:36, max mem: 20.9 GB 
[12/04 18:16:34 visual_prompt]: Epoch 31 / 100: avg data time: 3.58e-01, avg batch time: 1.1945, average train loss: 0.7403
[12/04 18:17:45 visual_prompt]: Inference (val):avg data time: 9.84e-05, avg batch time: 0.3100, average loss: 0.6442
[12/04 18:17:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 68.68	
[12/04 18:17:45 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[12/04 18:19:48 visual_prompt]: 	Training 100/553. train loss: 0.6650,	0.8434 s / batch. (data: 1.06e-02). ETA=8:54:56, max mem: 20.9 GB 
[12/04 18:21:48 visual_prompt]: 	Training 200/553. train loss: 0.4946,	0.8362 s / batch. (data: 1.19e-02). ETA=8:48:57, max mem: 20.9 GB 
[12/04 18:23:52 visual_prompt]: 	Training 300/553. train loss: 0.9610,	0.8400 s / batch. (data: 1.00e-03). ETA=8:49:59, max mem: 20.9 GB 
[12/04 18:25:53 visual_prompt]: 	Training 400/553. train loss: 0.8494,	0.8198 s / batch. (data: 3.60e-04). ETA=8:35:51, max mem: 20.9 GB 
[12/04 18:27:50 visual_prompt]: 	Training 500/553. train loss: 0.9137,	0.8593 s / batch. (data: 1.67e-03). ETA=8:59:17, max mem: 20.9 GB 
[12/04 18:28:52 visual_prompt]: Epoch 32 / 100: avg data time: 3.71e-01, avg batch time: 1.2062, average train loss: 0.7183
[12/04 18:29:59 visual_prompt]: Inference (val):avg data time: 4.63e-05, avg batch time: 0.3098, average loss: 0.6411
[12/04 18:29:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 70.08	
[12/04 18:29:59 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[12/04 18:32:04 visual_prompt]: 	Training 100/553. train loss: 1.2168,	1.6362 s / batch. (data: 7.82e-01). ETA=17:02:42, max mem: 20.9 GB 
[12/04 18:34:07 visual_prompt]: 	Training 200/553. train loss: 0.7286,	2.6241 s / batch. (data: 1.80e+00). ETA=1 day, 3:15:50, max mem: 20.9 GB 
[12/04 18:36:07 visual_prompt]: 	Training 300/553. train loss: 0.5560,	0.8600 s / batch. (data: 4.11e-04). ETA=8:54:40, max mem: 20.9 GB 
[12/04 18:38:06 visual_prompt]: 	Training 400/553. train loss: 0.5274,	0.8778 s / batch. (data: 6.92e-03). ETA=9:04:17, max mem: 20.9 GB 
[12/04 18:40:06 visual_prompt]: 	Training 500/553. train loss: 0.6054,	0.8422 s / batch. (data: 1.06e-02). ETA=8:40:48, max mem: 20.9 GB 
[12/04 18:41:05 visual_prompt]: Epoch 33 / 100: avg data time: 3.68e-01, avg batch time: 1.2043, average train loss: 0.7732
[12/04 18:42:16 visual_prompt]: Inference (val):avg data time: 2.50e-04, avg batch time: 0.3127, average loss: 0.9484
[12/04 18:42:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 69.59	
[12/04 18:42:16 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[12/04 18:44:25 visual_prompt]: 	Training 100/553. train loss: 0.7525,	2.0717 s / batch. (data: 1.24e+00). ETA=21:15:51, max mem: 20.9 GB 
[12/04 18:46:22 visual_prompt]: 	Training 200/553. train loss: 0.9737,	0.8576 s / batch. (data: 6.29e-03). ETA=8:46:42, max mem: 20.9 GB 
[12/04 18:48:18 visual_prompt]: 	Training 300/553. train loss: 0.4960,	0.8915 s / batch. (data: 1.19e-02). ETA=9:06:04, max mem: 20.9 GB 
[12/04 18:50:21 visual_prompt]: 	Training 400/553. train loss: 0.9404,	0.8441 s / batch. (data: 1.12e-03). ETA=8:35:35, max mem: 20.9 GB 
[12/04 18:52:20 visual_prompt]: 	Training 500/553. train loss: 0.5754,	1.8684 s / batch. (data: 1.04e+00). ETA=18:58:13, max mem: 20.9 GB 
[12/04 18:53:23 visual_prompt]: Epoch 34 / 100: avg data time: 3.69e-01, avg batch time: 1.2051, average train loss: 0.7610
[12/04 18:54:36 visual_prompt]: Inference (val):avg data time: 2.88e-04, avg batch time: 0.3129, average loss: 0.6441
[12/04 18:54:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 71.15	
[12/04 18:54:36 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[12/04 18:56:45 visual_prompt]: 	Training 100/553. train loss: 1.4151,	0.8538 s / batch. (data: 1.18e-02). ETA=8:37:57, max mem: 20.9 GB 
[12/04 18:58:43 visual_prompt]: 	Training 200/553. train loss: 1.2026,	0.8362 s / batch. (data: 1.05e-02). ETA=8:25:51, max mem: 20.9 GB 
[12/04 19:00:43 visual_prompt]: 	Training 300/553. train loss: 0.5898,	0.8267 s / batch. (data: 1.15e-03). ETA=8:18:46, max mem: 20.9 GB 
[12/04 19:02:45 visual_prompt]: 	Training 400/553. train loss: 0.3023,	0.8171 s / batch. (data: 3.58e-04). ETA=8:11:37, max mem: 20.9 GB 
[12/04 19:04:48 visual_prompt]: 	Training 500/553. train loss: 0.7569,	0.9869 s / batch. (data: 1.67e-01). ETA=9:52:05, max mem: 20.9 GB 
[12/04 19:05:53 visual_prompt]: Epoch 35 / 100: avg data time: 3.90e-01, avg batch time: 1.2249, average train loss: 0.7742
[12/04 19:07:05 visual_prompt]: Inference (val):avg data time: 5.87e-04, avg batch time: 0.3126, average loss: 0.6447
[12/04 19:07:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.83	
[12/04 19:07:07 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[12/04 19:09:11 visual_prompt]: 	Training 100/553. train loss: 0.9919,	0.8200 s / batch. (data: 4.50e-04). ETA=8:09:53, max mem: 20.9 GB 
[12/04 19:11:11 visual_prompt]: 	Training 200/553. train loss: 1.0478,	0.8387 s / batch. (data: 1.04e-02). ETA=8:19:40, max mem: 20.9 GB 
[12/04 19:13:13 visual_prompt]: 	Training 300/553. train loss: 0.6108,	0.8558 s / batch. (data: 3.15e-02). ETA=8:28:25, max mem: 20.9 GB 
[12/04 19:15:09 visual_prompt]: 	Training 400/553. train loss: 1.0466,	0.8164 s / batch. (data: 3.91e-04). ETA=8:03:37, max mem: 20.9 GB 
[12/04 19:17:11 visual_prompt]: 	Training 500/553. train loss: 0.9504,	2.0636 s / batch. (data: 1.23e+00). ETA=20:19:04, max mem: 20.9 GB 
[12/04 19:18:09 visual_prompt]: Epoch 36 / 100: avg data time: 3.62e-01, avg batch time: 1.1979, average train loss: 0.7729
[12/04 19:19:21 visual_prompt]: Inference (val):avg data time: 3.07e-04, avg batch time: 0.3119, average loss: 0.6560
[12/04 19:19:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 68.97	
[12/04 19:19:21 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[12/04 19:21:23 visual_prompt]: 	Training 100/553. train loss: 0.9606,	0.8241 s / batch. (data: 7.77e-03). ETA=8:04:43, max mem: 20.9 GB 
[12/04 19:23:25 visual_prompt]: 	Training 200/553. train loss: 0.6475,	0.8356 s / batch. (data: 7.70e-03). ETA=8:10:04, max mem: 20.9 GB 
[12/04 19:25:22 visual_prompt]: 	Training 300/553. train loss: 0.8035,	2.2044 s / batch. (data: 1.33e+00). ETA=21:29:17, max mem: 20.9 GB 
[12/04 19:27:26 visual_prompt]: 	Training 400/553. train loss: 1.1164,	2.8058 s / batch. (data: 1.99e+00). ETA=1 day, 3:16:21, max mem: 20.9 GB 
[12/04 19:29:25 visual_prompt]: 	Training 500/553. train loss: 0.7213,	1.9360 s / batch. (data: 1.10e+00). ETA=18:45:52, max mem: 20.9 GB 
[12/04 19:30:29 visual_prompt]: Epoch 37 / 100: avg data time: 3.72e-01, avg batch time: 1.2074, average train loss: 0.7703
[12/04 19:31:39 visual_prompt]: Inference (val):avg data time: 8.17e-05, avg batch time: 0.3103, average loss: 0.7604
[12/04 19:31:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 67.87	
[12/04 19:31:39 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[12/04 19:33:38 visual_prompt]: 	Training 100/553. train loss: 0.5075,	1.4926 s / batch. (data: 6.78e-01). ETA=14:24:13, max mem: 20.9 GB 
[12/04 19:35:42 visual_prompt]: 	Training 200/553. train loss: 0.4885,	1.6888 s / batch. (data: 8.45e-01). ETA=16:14:59, max mem: 20.9 GB 
[12/04 19:37:41 visual_prompt]: 	Training 300/553. train loss: 0.3037,	0.8580 s / batch. (data: 1.02e-02). ETA=8:13:53, max mem: 20.9 GB 
[12/04 19:39:40 visual_prompt]: 	Training 400/553. train loss: 0.4647,	0.8510 s / batch. (data: 1.07e-02). ETA=8:08:28, max mem: 20.9 GB 
[12/04 19:41:43 visual_prompt]: 	Training 500/553. train loss: 0.4255,	0.8475 s / batch. (data: 2.08e-02). ETA=8:05:03, max mem: 20.9 GB 
[12/04 19:42:45 visual_prompt]: Epoch 38 / 100: avg data time: 3.68e-01, avg batch time: 1.2034, average train loss: 0.7358
[12/04 19:43:51 visual_prompt]: Inference (val):avg data time: 6.21e-05, avg batch time: 0.3102, average loss: 0.7029
[12/04 19:43:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 70.23	
[12/04 19:43:51 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[12/04 19:45:55 visual_prompt]: 	Training 100/553. train loss: 0.1664,	0.8244 s / batch. (data: 4.06e-04). ETA=7:49:42, max mem: 20.9 GB 
[12/04 19:47:55 visual_prompt]: 	Training 200/553. train loss: 0.6707,	0.8301 s / batch. (data: 5.27e-04). ETA=7:51:35, max mem: 20.9 GB 
[12/04 19:50:00 visual_prompt]: 	Training 300/553. train loss: 1.0727,	0.8381 s / batch. (data: 1.96e-02). ETA=7:54:42, max mem: 20.9 GB 
[12/04 19:51:58 visual_prompt]: 	Training 400/553. train loss: 0.3855,	0.8440 s / batch. (data: 1.40e-03). ETA=7:56:39, max mem: 20.9 GB 
[12/04 19:53:58 visual_prompt]: 	Training 500/553. train loss: 0.5400,	2.0416 s / batch. (data: 1.21e+00). ETA=19:09:36, max mem: 20.9 GB 
[12/04 19:54:55 visual_prompt]: Epoch 39 / 100: avg data time: 3.65e-01, avg batch time: 1.2006, average train loss: 0.7222
[12/04 19:56:07 visual_prompt]: Inference (val):avg data time: 1.11e-04, avg batch time: 0.3130, average loss: 0.6669
[12/04 19:56:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 69.74	
[12/04 19:56:07 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[12/04 19:58:12 visual_prompt]: 	Training 100/553. train loss: 0.5262,	0.8531 s / batch. (data: 3.01e-02). ETA=7:58:11, max mem: 20.9 GB 
[12/04 20:00:10 visual_prompt]: 	Training 200/553. train loss: 0.7315,	0.8454 s / batch. (data: 6.67e-03). ETA=7:52:27, max mem: 20.9 GB 
[12/04 20:02:13 visual_prompt]: 	Training 300/553. train loss: 0.7615,	0.8311 s / batch. (data: 1.29e-03). ETA=7:43:06, max mem: 20.9 GB 
[12/04 20:04:14 visual_prompt]: 	Training 400/553. train loss: 0.3547,	0.8242 s / batch. (data: 1.26e-03). ETA=7:37:53, max mem: 20.9 GB 
[12/04 20:06:09 visual_prompt]: 	Training 500/553. train loss: 0.2030,	0.8355 s / batch. (data: 3.36e-04). ETA=7:42:45, max mem: 20.9 GB 
[12/04 20:07:14 visual_prompt]: Epoch 40 / 100: avg data time: 3.69e-01, avg batch time: 1.2058, average train loss: 0.7141
[12/04 20:08:25 visual_prompt]: Inference (val):avg data time: 1.16e-04, avg batch time: 0.3104, average loss: 0.6833
[12/04 20:08:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 72.86	
[12/04 20:08:25 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[12/04 20:10:32 visual_prompt]: 	Training 100/553. train loss: 1.6209,	0.8521 s / batch. (data: 3.57e-04). ETA=7:49:46, max mem: 20.9 GB 
[12/04 20:12:36 visual_prompt]: 	Training 200/553. train loss: 2.2665,	0.8704 s / batch. (data: 1.15e-02). ETA=7:58:25, max mem: 20.9 GB 
[12/04 20:14:37 visual_prompt]: 	Training 300/553. train loss: 0.5981,	0.8400 s / batch. (data: 1.41e-03). ETA=7:40:18, max mem: 20.9 GB 
[12/04 20:16:34 visual_prompt]: 	Training 400/553. train loss: 0.8123,	0.8400 s / batch. (data: 3.19e-04). ETA=7:38:53, max mem: 20.9 GB 
[12/04 20:18:30 visual_prompt]: 	Training 500/553. train loss: 0.5780,	0.8413 s / batch. (data: 1.18e-02). ETA=7:38:13, max mem: 20.9 GB 
[12/04 20:19:32 visual_prompt]: Epoch 41 / 100: avg data time: 3.69e-01, avg batch time: 1.2046, average train loss: 0.8587
[12/04 20:20:39 visual_prompt]: Inference (val):avg data time: 6.33e-05, avg batch time: 0.3094, average loss: 0.8731
[12/04 20:20:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 69.81	
[12/04 20:20:39 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[12/04 20:22:42 visual_prompt]: 	Training 100/553. train loss: 1.1274,	0.8542 s / batch. (data: 1.00e-02). ETA=7:43:03, max mem: 20.9 GB 
[12/04 20:24:43 visual_prompt]: 	Training 200/553. train loss: 1.1331,	0.8373 s / batch. (data: 9.10e-04). ETA=7:32:32, max mem: 20.9 GB 
[12/04 20:26:44 visual_prompt]: 	Training 300/553. train loss: 1.8679,	0.8405 s / batch. (data: 3.20e-04). ETA=7:32:52, max mem: 20.9 GB 
[12/04 20:28:40 visual_prompt]: 	Training 400/553. train loss: 0.6600,	0.8479 s / batch. (data: 1.31e-03). ETA=7:35:26, max mem: 20.9 GB 
[12/04 20:30:41 visual_prompt]: 	Training 500/553. train loss: 0.4262,	0.8438 s / batch. (data: 1.52e-03). ETA=7:31:49, max mem: 20.9 GB 
[12/04 20:31:41 visual_prompt]: Epoch 42 / 100: avg data time: 3.60e-01, avg batch time: 1.1970, average train loss: 0.7660
[12/04 20:32:50 visual_prompt]: Inference (val):avg data time: 8.96e-05, avg batch time: 0.3126, average loss: 0.6987
[12/04 20:32:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 72.02	
[12/04 20:32:51 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[12/04 20:34:59 visual_prompt]: 	Training 100/553. train loss: 0.5600,	0.8330 s / batch. (data: 6.35e-03). ETA=7:23:54, max mem: 20.9 GB 
[12/04 20:36:59 visual_prompt]: 	Training 200/553. train loss: 0.8294,	0.8565 s / batch. (data: 2.43e-02). ETA=7:34:58, max mem: 20.9 GB 
[12/04 20:38:54 visual_prompt]: 	Training 300/553. train loss: 0.1853,	0.8595 s / batch. (data: 1.56e-02). ETA=7:35:09, max mem: 20.9 GB 
[12/04 20:40:53 visual_prompt]: 	Training 400/553. train loss: 0.6412,	0.8475 s / batch. (data: 2.18e-02). ETA=7:27:22, max mem: 20.9 GB 
[12/04 20:42:53 visual_prompt]: 	Training 500/553. train loss: 0.4209,	0.8330 s / batch. (data: 1.06e-02). ETA=7:18:21, max mem: 20.9 GB 
[12/04 20:43:58 visual_prompt]: Epoch 43 / 100: avg data time: 3.69e-01, avg batch time: 1.2061, average train loss: 0.7096
[12/04 20:45:09 visual_prompt]: Inference (val):avg data time: 1.12e-04, avg batch time: 0.3117, average loss: 0.8769
[12/04 20:45:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 64.60	
[12/04 20:45:09 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[12/04 20:47:16 visual_prompt]: 	Training 100/553. train loss: 0.5311,	0.8375 s / batch. (data: 8.38e-03). ETA=7:18:36, max mem: 20.9 GB 
[12/04 20:49:18 visual_prompt]: 	Training 200/553. train loss: 0.3089,	0.8133 s / batch. (data: 5.13e-04). ETA=7:04:34, max mem: 20.9 GB 
[12/04 20:51:13 visual_prompt]: 	Training 300/553. train loss: 0.2770,	0.8355 s / batch. (data: 7.81e-03). ETA=7:14:46, max mem: 20.9 GB 
[12/04 20:53:12 visual_prompt]: 	Training 400/553. train loss: 1.2618,	0.8228 s / batch. (data: 3.45e-04). ETA=7:06:46, max mem: 20.9 GB 
[12/04 20:55:11 visual_prompt]: 	Training 500/553. train loss: 0.8570,	0.8479 s / batch. (data: 3.37e-04). ETA=7:18:22, max mem: 20.9 GB 
[12/04 20:56:14 visual_prompt]: Epoch 44 / 100: avg data time: 3.64e-01, avg batch time: 1.2010, average train loss: 0.7247
[12/04 20:57:26 visual_prompt]: Inference (val):avg data time: 3.77e-04, avg batch time: 0.3117, average loss: 0.6698
[12/04 20:57:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.31	
[12/04 20:57:26 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[12/04 20:59:34 visual_prompt]: 	Training 100/553. train loss: 0.9529,	0.8735 s / batch. (data: 2.58e-02). ETA=7:29:22, max mem: 20.9 GB 
[12/04 21:01:26 visual_prompt]: 	Training 200/553. train loss: 0.7645,	0.9353 s / batch. (data: 8.86e-02). ETA=7:59:37, max mem: 20.9 GB 
[12/04 21:03:30 visual_prompt]: 	Training 300/553. train loss: 1.2725,	0.8825 s / batch. (data: 1.08e-02). ETA=7:31:05, max mem: 20.9 GB 
[12/04 21:05:24 visual_prompt]: 	Training 400/553. train loss: 0.4933,	0.8324 s / batch. (data: 3.32e-04). ETA=7:04:04, max mem: 20.9 GB 
[12/04 21:07:29 visual_prompt]: 	Training 500/553. train loss: 0.5581,	0.8457 s / batch. (data: 3.64e-04). ETA=7:09:25, max mem: 20.9 GB 
[12/04 21:08:32 visual_prompt]: Epoch 45 / 100: avg data time: 3.67e-01, avg batch time: 1.2045, average train loss: 0.6976
[12/04 21:09:44 visual_prompt]: Inference (val):avg data time: 1.16e-04, avg batch time: 0.3111, average loss: 0.8126
[12/04 21:09:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 71.99	
[12/04 21:09:44 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[12/04 21:11:51 visual_prompt]: 	Training 100/553. train loss: 0.5444,	3.9280 s / batch. (data: 3.11e+00). ETA=1 day, 9:04:37, max mem: 20.9 GB 
[12/04 21:13:51 visual_prompt]: 	Training 200/553. train loss: 0.4794,	0.8212 s / batch. (data: 1.30e-03). ETA=6:53:31, max mem: 20.9 GB 
[12/04 21:15:48 visual_prompt]: 	Training 300/553. train loss: 0.4994,	0.8247 s / batch. (data: 3.55e-04). ETA=6:53:55, max mem: 20.9 GB 
[12/04 21:17:50 visual_prompt]: 	Training 400/553. train loss: 0.8476,	0.8469 s / batch. (data: 8.51e-04). ETA=7:03:38, max mem: 20.9 GB 
[12/04 21:19:48 visual_prompt]: 	Training 500/553. train loss: 0.7655,	0.8427 s / batch. (data: 1.06e-02). ETA=7:00:08, max mem: 20.9 GB 
[12/04 21:20:53 visual_prompt]: Epoch 46 / 100: avg data time: 3.73e-01, avg batch time: 1.2088, average train loss: 0.7214
[12/04 21:22:05 visual_prompt]: Inference (val):avg data time: 3.12e-04, avg batch time: 0.3100, average loss: 0.6522
[12/04 21:22:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 65.32	
[12/04 21:22:05 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[12/04 21:24:08 visual_prompt]: 	Training 100/553. train loss: 0.7948,	0.9204 s / batch. (data: 8.21e-02). ETA=7:36:33, max mem: 20.9 GB 
[12/04 21:26:06 visual_prompt]: 	Training 200/553. train loss: 0.9605,	2.4526 s / batch. (data: 1.61e+00). ETA=20:12:29, max mem: 20.9 GB 
[12/04 21:28:03 visual_prompt]: 	Training 300/553. train loss: 0.4482,	0.8484 s / batch. (data: 1.20e-02). ETA=6:58:00, max mem: 20.9 GB 
[12/04 21:30:05 visual_prompt]: 	Training 400/553. train loss: 1.2854,	0.8338 s / batch. (data: 1.22e-03). ETA=6:49:24, max mem: 20.9 GB 
[12/04 21:32:05 visual_prompt]: 	Training 500/553. train loss: 1.8538,	0.8766 s / batch. (data: 5.45e-03). ETA=7:08:59, max mem: 20.9 GB 
[12/04 21:33:08 visual_prompt]: Epoch 47 / 100: avg data time: 3.62e-01, avg batch time: 1.1988, average train loss: 0.7697
[12/04 21:34:15 visual_prompt]: Inference (val):avg data time: 4.92e-05, avg batch time: 0.3084, average loss: 0.6317
[12/04 21:34:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 69.44	
[12/04 21:34:15 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[12/04 21:36:21 visual_prompt]: 	Training 100/553. train loss: 0.6654,	0.8403 s / batch. (data: 3.07e-04). ETA=6:49:04, max mem: 20.9 GB 
[12/04 21:38:18 visual_prompt]: 	Training 200/553. train loss: 0.2008,	0.8291 s / batch. (data: 5.46e-03). ETA=6:42:13, max mem: 20.9 GB 
[12/04 21:40:20 visual_prompt]: 	Training 300/553. train loss: 0.5192,	3.0640 s / batch. (data: 2.22e+00). ETA=1 day, 0:41:23, max mem: 20.9 GB 
[12/04 21:42:17 visual_prompt]: 	Training 400/553. train loss: 0.3382,	1.0937 s / batch. (data: 2.73e-01). ETA=8:46:57, max mem: 20.9 GB 
[12/04 21:44:19 visual_prompt]: 	Training 500/553. train loss: 0.7942,	0.8190 s / batch. (data: 3.23e-04). ETA=6:33:14, max mem: 20.9 GB 
[12/04 21:45:18 visual_prompt]: Epoch 48 / 100: avg data time: 3.62e-01, avg batch time: 1.1984, average train loss: 0.6993
[12/04 21:46:26 visual_prompt]: Inference (val):avg data time: 7.33e-05, avg batch time: 0.3105, average loss: 0.8060
[12/04 21:46:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 69.29	
[12/04 21:46:26 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[12/04 21:48:31 visual_prompt]: 	Training 100/553. train loss: 0.3883,	0.8393 s / batch. (data: 1.24e-03). ETA=6:40:51, max mem: 20.9 GB 
[12/04 21:50:26 visual_prompt]: 	Training 200/553. train loss: 0.7765,	0.8351 s / batch. (data: 3.25e-04). ETA=6:37:26, max mem: 20.9 GB 
[12/04 21:52:28 visual_prompt]: 	Training 300/553. train loss: 0.3852,	0.8483 s / batch. (data: 3.11e-04). ETA=6:42:19, max mem: 20.9 GB 
[12/04 21:54:32 visual_prompt]: 	Training 400/553. train loss: 0.5614,	0.8480 s / batch. (data: 1.55e-02). ETA=6:40:46, max mem: 20.9 GB 
[12/04 21:56:32 visual_prompt]: 	Training 500/553. train loss: 0.8951,	0.8343 s / batch. (data: 3.38e-04). ETA=6:32:54, max mem: 20.9 GB 
[12/04 21:57:32 visual_prompt]: Epoch 49 / 100: avg data time: 3.68e-01, avg batch time: 1.2040, average train loss: 0.6802
[12/04 21:58:44 visual_prompt]: Inference (val):avg data time: 1.09e-04, avg batch time: 0.3088, average loss: 1.0383
[12/04 21:58:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 71.66	
[12/04 21:58:44 visual_prompt]: Stopping early.
[12/04 21:58:44 visual_prompt]: Rank of current process: 0. World size: 1
[12/04 21:58:44 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/04 21:58:44 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/04 21:58:44 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/04 21:58:44 visual_prompt]: Training with config:
[12/04 21:58:44 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.25_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/04 21:58:44 visual_prompt]: Loading training data...
[12/04 21:58:44 visual_prompt]: Constructing mammo-cbis dataset train...
[12/04 21:58:44 visual_prompt]: Loading validation data...
[12/04 21:58:44 visual_prompt]: Constructing mammo-cbis dataset val...
[12/04 21:58:44 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[12/04 21:58:48 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[12/04 21:58:48 visual_prompt]: tuned percent:0.525
[12/04 21:58:48 visual_prompt]: Device used for model: 0
[12/04 21:58:48 visual_prompt]: Setting up Evaluator...
[12/04 21:58:48 visual_prompt]: Setting up Trainer...
[12/04 21:58:48 visual_prompt]: 	Setting up the optimizer...
[12/04 21:58:48 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/04 22:00:52 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8357 s / batch. (data: 3.55e-04). ETA=12:48:48, max mem: 20.9 GB 
[12/04 22:02:49 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8179 s / batch. (data: 1.16e-03). ETA=12:31:06, max mem: 20.9 GB 
[12/04 22:04:52 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.9661 s / batch. (data: 2.11e+00). ETA=1 day, 21:18:57, max mem: 20.9 GB 
[12/04 22:06:49 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8341 s / batch. (data: 1.23e-03). ETA=12:43:13, max mem: 20.9 GB 
[12/04 22:08:49 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8536 s / batch. (data: 1.42e-02). ETA=12:59:35, max mem: 20.9 GB 
[12/04 22:09:52 visual_prompt]: Epoch 1 / 100: avg data time: 3.63e-01, avg batch time: 1.1995, average train loss: 1.5403
[12/04 22:11:03 visual_prompt]: Inference (val):avg data time: 2.68e-04, avg batch time: 0.3134, average loss: 1.5201
[12/04 22:11:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[12/04 22:11:03 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[12/04 22:13:03 visual_prompt]: 	Training 100/553. train loss: 0.7436,	0.9237 s / batch. (data: 7.44e-02). ETA=14:01:18, max mem: 20.9 GB 
[12/04 22:15:03 visual_prompt]: 	Training 200/553. train loss: 0.2132,	0.8293 s / batch. (data: 1.06e-02). ETA=12:33:55, max mem: 20.9 GB 
[12/04 22:17:06 visual_prompt]: 	Training 300/553. train loss: 0.9111,	1.7924 s / batch. (data: 9.73e-01). ETA=1 day, 3:06:29, max mem: 20.9 GB 
[12/04 22:19:04 visual_prompt]: 	Training 400/553. train loss: 1.1021,	0.8340 s / batch. (data: 5.44e-03). ETA=12:35:26, max mem: 20.9 GB 
[12/04 22:21:04 visual_prompt]: 	Training 500/553. train loss: 0.6526,	0.8354 s / batch. (data: 3.58e-04). ETA=12:35:20, max mem: 20.9 GB 
[12/04 22:22:06 visual_prompt]: Epoch 2 / 100: avg data time: 3.63e-01, avg batch time: 1.1984, average train loss: 0.8373
[12/04 22:23:14 visual_prompt]: Inference (val):avg data time: 1.87e-04, avg batch time: 0.3101, average loss: 0.7624
[12/04 22:23:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.87	
[12/04 22:23:14 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[12/04 22:25:16 visual_prompt]: 	Training 100/553. train loss: 0.7625,	0.8430 s / batch. (data: 1.44e-02). ETA=12:39:59, max mem: 20.9 GB 
[12/04 22:27:19 visual_prompt]: 	Training 200/553. train loss: 0.7006,	2.1195 s / batch. (data: 1.28e+00). ETA=1 day, 7:47:22, max mem: 20.9 GB 
[12/04 22:29:17 visual_prompt]: 	Training 300/553. train loss: 0.6354,	0.8517 s / batch. (data: 1.18e-02). ETA=12:45:00, max mem: 20.9 GB 
[12/04 22:31:16 visual_prompt]: 	Training 400/553. train loss: 0.7167,	0.8590 s / batch. (data: 2.72e-02). ETA=12:50:09, max mem: 20.9 GB 
[12/04 22:33:18 visual_prompt]: 	Training 500/553. train loss: 0.7742,	2.2500 s / batch. (data: 1.39e+00). ETA=1 day, 9:33:32, max mem: 20.9 GB 
[12/04 22:34:18 visual_prompt]: Epoch 3 / 100: avg data time: 3.64e-01, avg batch time: 1.1995, average train loss: 0.7757
[12/04 22:35:26 visual_prompt]: Inference (val):avg data time: 8.69e-05, avg batch time: 0.3108, average loss: 0.7058
[12/04 22:35:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 56.84	
[12/04 22:35:26 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[12/04 22:37:33 visual_prompt]: 	Training 100/553. train loss: 0.7458,	0.8440 s / batch. (data: 1.17e-02). ETA=12:33:08, max mem: 20.9 GB 
[12/04 22:39:34 visual_prompt]: 	Training 200/553. train loss: 1.4286,	0.8611 s / batch. (data: 1.35e-02). ETA=12:46:58, max mem: 20.9 GB 
[12/04 22:41:32 visual_prompt]: 	Training 300/553. train loss: 0.6624,	1.3675 s / batch. (data: 5.53e-01). ETA=20:15:45, max mem: 20.9 GB 
[12/04 22:43:26 visual_prompt]: 	Training 400/553. train loss: 0.5764,	2.2233 s / batch. (data: 1.40e+00). ETA=1 day, 8:52:50, max mem: 20.9 GB 
[12/04 22:45:28 visual_prompt]: 	Training 500/553. train loss: 0.6758,	4.4360 s / batch. (data: 3.62e+00). ETA=2 days, 17:28:51, max mem: 20.9 GB 
[12/04 22:46:28 visual_prompt]: Epoch 4 / 100: avg data time: 3.59e-01, avg batch time: 1.1966, average train loss: 0.8704
[12/04 22:47:39 visual_prompt]: Inference (val):avg data time: 3.59e-04, avg batch time: 0.3125, average loss: 0.8946
[12/04 22:47:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.39	
[12/04 22:47:39 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[12/04 22:49:42 visual_prompt]: 	Training 100/553. train loss: 0.5171,	0.8549 s / batch. (data: 1.19e-02). ETA=12:35:01, max mem: 20.9 GB 
[12/04 22:51:43 visual_prompt]: 	Training 200/553. train loss: 0.6178,	0.8550 s / batch. (data: 6.46e-03). ETA=12:33:41, max mem: 20.9 GB 
[12/04 22:53:42 visual_prompt]: 	Training 300/553. train loss: 1.6098,	0.8381 s / batch. (data: 9.93e-03). ETA=12:17:19, max mem: 20.9 GB 
[12/04 22:55:41 visual_prompt]: 	Training 400/553. train loss: 1.0288,	0.8366 s / batch. (data: 3.11e-04). ETA=12:14:39, max mem: 20.9 GB 
[12/04 22:57:39 visual_prompt]: 	Training 500/553. train loss: 0.5677,	0.8380 s / batch. (data: 1.36e-02). ETA=12:14:26, max mem: 20.9 GB 
[12/04 22:58:43 visual_prompt]: Epoch 5 / 100: avg data time: 3.65e-01, avg batch time: 1.2004, average train loss: 0.9702
[12/04 22:59:54 visual_prompt]: Inference (val):avg data time: 1.10e-04, avg batch time: 0.3107, average loss: 0.8285
[12/04 22:59:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.83	
[12/04 22:59:54 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[12/04 23:02:02 visual_prompt]: 	Training 100/553. train loss: 0.5388,	0.8288 s / batch. (data: 5.91e-04). ETA=12:04:17, max mem: 20.9 GB 
[12/04 23:04:03 visual_prompt]: 	Training 200/553. train loss: 0.7963,	0.8440 s / batch. (data: 8.85e-03). ETA=12:16:11, max mem: 20.9 GB 
[12/04 23:05:59 visual_prompt]: 	Training 300/553. train loss: 0.6023,	0.8698 s / batch. (data: 1.13e-02). ETA=12:37:12, max mem: 20.9 GB 
[12/04 23:08:08 visual_prompt]: 	Training 400/553. train loss: 0.6314,	0.8171 s / batch. (data: 6.14e-04). ETA=11:49:57, max mem: 20.9 GB 
[12/04 23:10:04 visual_prompt]: 	Training 500/553. train loss: 0.8519,	1.6724 s / batch. (data: 8.59e-01). ETA=1 day, 0:10:21, max mem: 20.9 GB 
[12/04 23:11:06 visual_prompt]: Epoch 6 / 100: avg data time: 3.81e-01, avg batch time: 1.2154, average train loss: 0.8433
[12/04 23:12:19 visual_prompt]: Inference (val):avg data time: 2.88e-04, avg batch time: 0.3126, average loss: 0.7642
[12/04 23:12:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.24	
[12/04 23:12:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[12/04 23:14:23 visual_prompt]: 	Training 100/553. train loss: 0.8887,	0.8262 s / batch. (data: 1.31e-03). ETA=11:54:23, max mem: 20.9 GB 
[12/04 23:16:25 visual_prompt]: 	Training 200/553. train loss: 0.5279,	0.8286 s / batch. (data: 7.96e-03). ETA=11:55:08, max mem: 20.9 GB 
[12/04 23:18:25 visual_prompt]: 	Training 300/553. train loss: 0.6200,	1.5898 s / batch. (data: 7.65e-01). ETA=22:49:23, max mem: 20.9 GB 
[12/04 23:20:27 visual_prompt]: 	Training 400/553. train loss: 0.6064,	2.8997 s / batch. (data: 2.05e+00). ETA=1 day, 17:32:50, max mem: 20.9 GB 
[12/04 23:22:22 visual_prompt]: 	Training 500/553. train loss: 1.4629,	0.8556 s / batch. (data: 1.31e-03). ETA=12:14:05, max mem: 20.9 GB 
[12/04 23:23:24 visual_prompt]: Epoch 7 / 100: avg data time: 3.67e-01, avg batch time: 1.2020, average train loss: 0.8611
[12/04 23:24:34 visual_prompt]: Inference (val):avg data time: 6.19e-04, avg batch time: 0.3125, average loss: 0.7455
[12/04 23:24:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.40	
[12/04 23:24:34 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[12/04 23:26:38 visual_prompt]: 	Training 100/553. train loss: 0.7446,	1.3558 s / batch. (data: 5.32e-01). ETA=19:19:54, max mem: 20.9 GB 
[12/04 23:28:40 visual_prompt]: 	Training 200/553. train loss: 1.5044,	0.8520 s / batch. (data: 1.20e-02). ETA=12:07:26, max mem: 20.9 GB 
[12/04 23:30:37 visual_prompt]: 	Training 300/553. train loss: 0.9343,	0.8717 s / batch. (data: 1.55e-02). ETA=12:22:48, max mem: 20.9 GB 
[12/04 23:32:37 visual_prompt]: 	Training 400/553. train loss: 0.7478,	0.8200 s / batch. (data: 1.15e-03). ETA=11:37:24, max mem: 20.9 GB 
[12/04 23:34:35 visual_prompt]: 	Training 500/553. train loss: 1.2109,	2.2080 s / batch. (data: 1.37e+00). ETA=1 day, 7:14:11, max mem: 20.9 GB 
[12/04 23:35:37 visual_prompt]: Epoch 8 / 100: avg data time: 3.62e-01, avg batch time: 1.1981, average train loss: 0.9507
[12/04 23:36:48 visual_prompt]: Inference (val):avg data time: 1.02e-04, avg batch time: 0.3130, average loss: 1.4041
[12/04 23:36:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.19	
[12/04 23:36:48 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[12/04 23:38:54 visual_prompt]: 	Training 100/553. train loss: 0.2778,	0.8459 s / batch. (data: 2.62e-02). ETA=11:55:50, max mem: 20.9 GB 
[12/04 23:40:52 visual_prompt]: 	Training 200/553. train loss: 0.6187,	0.8440 s / batch. (data: 3.61e-04). ETA=11:52:49, max mem: 20.9 GB 
[12/04 23:42:51 visual_prompt]: 	Training 300/553. train loss: 0.6669,	2.3106 s / batch. (data: 1.48e+00). ETA=1 day, 8:27:41, max mem: 20.9 GB 
[12/04 23:44:51 visual_prompt]: 	Training 400/553. train loss: 0.5471,	0.8440 s / batch. (data: 5.60e-03). ETA=11:50:00, max mem: 20.9 GB 
[12/04 23:46:50 visual_prompt]: 	Training 500/553. train loss: 0.7474,	1.0321 s / batch. (data: 1.95e-01). ETA=14:26:33, max mem: 20.9 GB 
[12/04 23:47:53 visual_prompt]: Epoch 9 / 100: avg data time: 3.66e-01, avg batch time: 1.2013, average train loss: 0.8651
[12/04 23:49:04 visual_prompt]: Inference (val):avg data time: 1.15e-04, avg batch time: 0.3123, average loss: 1.0266
[12/04 23:49:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.45	
[12/04 23:49:04 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[12/04 23:51:13 visual_prompt]: 	Training 100/553. train loss: 1.5081,	0.8304 s / batch. (data: 1.45e-02). ETA=11:35:04, max mem: 20.9 GB 
[12/04 23:53:07 visual_prompt]: 	Training 200/553. train loss: 0.5658,	0.8659 s / batch. (data: 1.56e-02). ETA=12:03:22, max mem: 20.9 GB 
[12/04 23:55:06 visual_prompt]: 	Training 300/553. train loss: 0.6614,	2.1279 s / batch. (data: 1.27e+00). ETA=1 day, 5:34:06, max mem: 20.9 GB 
[12/04 23:57:01 visual_prompt]: 	Training 400/553. train loss: 0.6856,	1.2678 s / batch. (data: 4.42e-01). ETA=17:34:51, max mem: 20.9 GB 
[12/04 23:59:02 visual_prompt]: 	Training 500/553. train loss: 0.7624,	1.7559 s / batch. (data: 9.22e-01). ETA=1 day, 0:18:05, max mem: 20.9 GB 
[12/05 00:00:04 visual_prompt]: Epoch 10 / 100: avg data time: 3.55e-01, avg batch time: 1.1924, average train loss: 0.9914
[12/05 00:01:14 visual_prompt]: Inference (val):avg data time: 3.53e-04, avg batch time: 0.3124, average loss: 0.6945
[12/05 00:01:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.50	
[12/05 00:01:14 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[12/05 00:03:22 visual_prompt]: 	Training 100/553. train loss: 1.4991,	0.8322 s / batch. (data: 5.67e-03). ETA=11:28:54, max mem: 20.9 GB 
[12/05 00:05:20 visual_prompt]: 	Training 200/553. train loss: 2.1116,	0.8447 s / batch. (data: 8.85e-04). ETA=11:37:50, max mem: 20.9 GB 
[12/05 00:07:20 visual_prompt]: 	Training 300/553. train loss: 0.2302,	3.0332 s / batch. (data: 2.21e+00). ETA=1 day, 17:40:52, max mem: 20.9 GB 
[12/05 00:09:14 visual_prompt]: 	Training 400/553. train loss: 0.7068,	0.8161 s / batch. (data: 3.43e-04). ETA=11:11:32, max mem: 20.9 GB 
[12/05 00:11:13 visual_prompt]: 	Training 500/553. train loss: 0.6712,	0.8692 s / batch. (data: 1.54e-02). ETA=11:53:45, max mem: 20.9 GB 
[12/05 00:12:15 visual_prompt]: Epoch 11 / 100: avg data time: 3.58e-01, avg batch time: 1.1937, average train loss: 0.9720
[12/05 00:13:25 visual_prompt]: Inference (val):avg data time: 1.10e-04, avg batch time: 0.3124, average loss: 0.6573
[12/05 00:13:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 63.97	
[12/05 00:13:25 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[12/05 00:15:29 visual_prompt]: 	Training 100/553. train loss: 0.7590,	0.8160 s / batch. (data: 3.80e-04). ETA=11:08:01, max mem: 20.9 GB 
[12/05 00:17:29 visual_prompt]: 	Training 200/553. train loss: 0.7137,	0.8419 s / batch. (data: 5.95e-03). ETA=11:27:47, max mem: 20.9 GB 
[12/05 00:19:25 visual_prompt]: 	Training 300/553. train loss: 0.5980,	0.8480 s / batch. (data: 4.23e-04). ETA=11:31:21, max mem: 20.9 GB 
[12/05 00:21:24 visual_prompt]: 	Training 400/553. train loss: 0.7207,	0.8485 s / batch. (data: 1.19e-02). ETA=11:30:21, max mem: 20.9 GB 
[12/05 00:23:24 visual_prompt]: 	Training 500/553. train loss: 2.8480,	0.8442 s / batch. (data: 4.78e-04). ETA=11:25:28, max mem: 20.9 GB 
[12/05 00:24:24 visual_prompt]: Epoch 12 / 100: avg data time: 3.55e-01, avg batch time: 1.1901, average train loss: 0.8939
[12/05 00:25:33 visual_prompt]: Inference (val):avg data time: 1.00e-04, avg batch time: 0.3109, average loss: 2.2231
[12/05 00:25:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.06	
[12/05 00:25:33 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[12/05 00:27:36 visual_prompt]: 	Training 100/553. train loss: 0.7936,	0.8320 s / batch. (data: 4.03e-04). ETA=11:13:26, max mem: 20.9 GB 
[12/05 00:29:31 visual_prompt]: 	Training 200/553. train loss: 0.6456,	0.8184 s / batch. (data: 3.77e-04). ETA=11:01:03, max mem: 20.9 GB 
[12/05 00:31:31 visual_prompt]: 	Training 300/553. train loss: 0.4188,	2.4320 s / batch. (data: 1.60e+00). ETA=1 day, 8:40:21, max mem: 20.9 GB 
[12/05 00:33:26 visual_prompt]: 	Training 400/553. train loss: 2.7604,	0.8460 s / batch. (data: 1.17e-02). ETA=11:20:29, max mem: 20.9 GB 
[12/05 00:35:26 visual_prompt]: 	Training 500/553. train loss: 1.1969,	0.8578 s / batch. (data: 2.49e-02). ETA=11:28:33, max mem: 20.9 GB 
[12/05 00:36:28 visual_prompt]: Epoch 13 / 100: avg data time: 3.47e-01, avg batch time: 1.1820, average train loss: 0.9678
[12/05 00:37:37 visual_prompt]: Inference (val):avg data time: 1.09e-04, avg batch time: 0.3117, average loss: 1.2001
[12/05 00:37:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.99	
[12/05 00:37:37 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[12/05 00:39:41 visual_prompt]: 	Training 100/553. train loss: 0.5801,	0.8456 s / batch. (data: 1.20e-02). ETA=11:16:39, max mem: 20.9 GB 
[12/05 00:41:37 visual_prompt]: 	Training 200/553. train loss: 1.0630,	1.4135 s / batch. (data: 5.77e-01). ETA=18:48:39, max mem: 20.9 GB 
[12/05 00:43:36 visual_prompt]: 	Training 300/553. train loss: 0.8490,	1.3800 s / batch. (data: 5.29e-01). ETA=18:19:39, max mem: 20.9 GB 
[12/05 00:45:30 visual_prompt]: 	Training 400/553. train loss: 1.0030,	0.8624 s / batch. (data: 5.49e-03). ETA=11:25:48, max mem: 20.9 GB 
[12/05 00:47:28 visual_prompt]: 	Training 500/553. train loss: 1.9111,	0.8355 s / batch. (data: 3.49e-04). ETA=11:03:00, max mem: 20.9 GB 
[12/05 00:48:28 visual_prompt]: Epoch 14 / 100: avg data time: 3.41e-01, avg batch time: 1.1762, average train loss: 0.8577
[12/05 00:49:37 visual_prompt]: Inference (val):avg data time: 8.31e-05, avg batch time: 0.3107, average loss: 0.6717
[12/05 00:49:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 66.40	
[12/05 00:49:37 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[12/05 00:51:40 visual_prompt]: 	Training 100/553. train loss: 0.8441,	1.2706 s / batch. (data: 4.44e-01). ETA=16:44:58, max mem: 20.9 GB 
[12/05 00:53:34 visual_prompt]: 	Training 200/553. train loss: 0.6626,	0.8236 s / batch. (data: 5.51e-03). ETA=10:50:02, max mem: 20.9 GB 
[12/05 00:55:33 visual_prompt]: 	Training 300/553. train loss: 0.4331,	0.8455 s / batch. (data: 3.66e-04). ETA=11:05:56, max mem: 20.9 GB 
[12/05 00:57:28 visual_prompt]: 	Training 400/553. train loss: 0.3525,	1.0332 s / batch. (data: 1.89e-01). ETA=13:32:05, max mem: 20.9 GB 
[12/05 00:59:25 visual_prompt]: 	Training 500/553. train loss: 0.5166,	0.8491 s / batch. (data: 1.20e-02). ETA=11:05:57, max mem: 20.9 GB 
[12/05 01:00:27 visual_prompt]: Epoch 15 / 100: avg data time: 3.41e-01, avg batch time: 1.1765, average train loss: 0.9378
[12/05 01:01:37 visual_prompt]: Inference (val):avg data time: 9.06e-05, avg batch time: 0.3123, average loss: 1.4959
[12/05 01:01:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.26	
[12/05 01:01:37 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[12/05 01:03:38 visual_prompt]: 	Training 100/553. train loss: 0.3294,	0.8262 s / batch. (data: 7.66e-04). ETA=10:45:52, max mem: 20.9 GB 
[12/05 01:05:36 visual_prompt]: 	Training 200/553. train loss: 0.6825,	0.8323 s / batch. (data: 3.54e-04). ETA=10:49:17, max mem: 20.9 GB 
[12/05 01:07:32 visual_prompt]: 	Training 300/553. train loss: 1.0684,	0.8480 s / batch. (data: 1.20e-02). ETA=11:00:05, max mem: 20.9 GB 
[12/05 01:09:29 visual_prompt]: 	Training 400/553. train loss: 0.4496,	0.8293 s / batch. (data: 9.64e-04). ETA=10:44:10, max mem: 20.9 GB 
[12/05 01:11:44 visual_prompt]: 	Training 500/553. train loss: 0.7999,	4.4339 s / batch. (data: 3.62e+00). ETA=2 days, 9:16:38, max mem: 20.9 GB 
[12/05 01:12:55 visual_prompt]: Epoch 16 / 100: avg data time: 3.93e-01, avg batch time: 1.2256, average train loss: 0.8059
[12/05 01:14:08 visual_prompt]: Inference (val):avg data time: 9.79e-05, avg batch time: 0.3105, average loss: 0.6313
[12/05 01:14:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 68.88	
[12/05 01:14:08 visual_prompt]: Best epoch 16: best metric: -0.631
[12/05 01:14:08 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[12/05 01:16:28 visual_prompt]: 	Training 100/553. train loss: 0.2631,	0.8236 s / batch. (data: 1.14e-03). ETA=10:36:14, max mem: 20.9 GB 
[12/05 01:18:39 visual_prompt]: 	Training 200/553. train loss: 1.0558,	0.8399 s / batch. (data: 1.09e-03). ETA=10:47:27, max mem: 20.9 GB 
[12/05 01:20:44 visual_prompt]: 	Training 300/553. train loss: 1.0640,	0.8281 s / batch. (data: 3.23e-04). ETA=10:36:59, max mem: 20.9 GB 
[12/05 01:22:47 visual_prompt]: 	Training 400/553. train loss: 0.6531,	0.8703 s / batch. (data: 1.04e-02). ETA=11:07:57, max mem: 20.9 GB 
[12/05 01:24:53 visual_prompt]: 	Training 500/553. train loss: 1.4022,	2.1665 s / batch. (data: 1.34e+00). ETA=1 day, 3:39:16, max mem: 20.9 GB 
[12/05 01:25:57 visual_prompt]: Epoch 17 / 100: avg data time: 4.49e-01, avg batch time: 1.2815, average train loss: 0.8274
[12/05 01:27:06 visual_prompt]: Inference (val):avg data time: 7.10e-05, avg batch time: 0.3097, average loss: 0.6235
[12/05 01:27:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.11	rocauc: 69.32	
[12/05 01:27:06 visual_prompt]: Best epoch 17: best metric: -0.624
[12/05 01:27:06 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[12/05 01:29:11 visual_prompt]: 	Training 100/553. train loss: 0.4588,	0.8788 s / batch. (data: 1.21e-02). ETA=11:10:49, max mem: 20.9 GB 
[12/05 01:31:13 visual_prompt]: 	Training 200/553. train loss: 0.4853,	0.8293 s / batch. (data: 3.13e-03). ETA=10:31:40, max mem: 20.9 GB 
[12/05 01:33:12 visual_prompt]: 	Training 300/553. train loss: 0.5312,	0.8356 s / batch. (data: 1.12e-03). ETA=10:35:01, max mem: 20.9 GB 
[12/05 01:35:07 visual_prompt]: 	Training 400/553. train loss: 0.7852,	0.8472 s / batch. (data: 7.95e-03). ETA=10:42:27, max mem: 20.9 GB 
[12/05 01:37:04 visual_prompt]: 	Training 500/553. train loss: 1.0336,	0.8755 s / batch. (data: 1.19e-02). ETA=11:02:27, max mem: 20.9 GB 
[12/05 01:38:04 visual_prompt]: Epoch 18 / 100: avg data time: 3.52e-01, avg batch time: 1.1888, average train loss: 0.8607
[12/05 01:39:13 visual_prompt]: Inference (val):avg data time: 4.63e-04, avg batch time: 0.3142, average loss: 0.6609
[12/05 01:39:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 69.26	
[12/05 01:39:13 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[12/05 01:41:16 visual_prompt]: 	Training 100/553. train loss: 0.5239,	0.8404 s / batch. (data: 3.39e-04). ETA=10:33:45, max mem: 20.9 GB 
[12/05 01:43:16 visual_prompt]: 	Training 200/553. train loss: 0.3981,	0.8364 s / batch. (data: 7.94e-03). ETA=10:29:20, max mem: 20.9 GB 
[12/05 01:45:14 visual_prompt]: 	Training 300/553. train loss: 0.9150,	0.8321 s / batch. (data: 3.34e-04). ETA=10:24:42, max mem: 20.9 GB 
[12/05 01:47:14 visual_prompt]: 	Training 400/553. train loss: 0.2576,	0.8527 s / batch. (data: 1.64e-02). ETA=10:38:44, max mem: 20.9 GB 
[12/05 01:49:04 visual_prompt]: 	Training 500/553. train loss: 1.7811,	0.8497 s / batch. (data: 3.24e-04). ETA=10:35:05, max mem: 20.9 GB 
[12/05 01:50:07 visual_prompt]: Epoch 19 / 100: avg data time: 3.46e-01, avg batch time: 1.1816, average train loss: 0.7490
[12/05 01:51:16 visual_prompt]: Inference (val):avg data time: 1.02e-04, avg batch time: 0.3117, average loss: 1.2840
[12/05 01:51:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.53	
[12/05 01:51:16 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[12/05 01:53:14 visual_prompt]: 	Training 100/553. train loss: 0.4561,	0.9809 s / batch. (data: 1.50e-01). ETA=12:10:37, max mem: 20.9 GB 
[12/05 01:55:13 visual_prompt]: 	Training 200/553. train loss: 0.2753,	0.8300 s / batch. (data: 3.07e-04). ETA=10:16:52, max mem: 20.9 GB 
[12/05 01:57:20 visual_prompt]: 	Training 300/553. train loss: 1.0949,	0.8169 s / batch. (data: 2.82e-04). ETA=10:05:47, max mem: 20.9 GB 
[12/05 02:00:07 visual_prompt]: 	Training 400/553. train loss: 0.8389,	0.8435 s / batch. (data: 1.36e-03). ETA=10:24:05, max mem: 20.9 GB 
[12/05 02:02:52 visual_prompt]: 	Training 500/553. train loss: 0.8288,	0.8366 s / batch. (data: 7.96e-03). ETA=10:17:34, max mem: 20.9 GB 
[12/05 02:04:24 visual_prompt]: Epoch 20 / 100: avg data time: 5.92e-01, avg batch time: 1.4240, average train loss: 0.8507
[12/05 02:05:59 visual_prompt]: Inference (val):avg data time: 8.93e-05, avg batch time: 0.3088, average loss: 0.9878
[12/05 02:05:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 67.00	
[12/05 02:05:59 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[12/05 02:08:59 visual_prompt]: 	Training 100/553. train loss: 0.4257,	0.8335 s / batch. (data: 3.12e-04). ETA=10:13:08, max mem: 20.9 GB 
[12/05 02:11:54 visual_prompt]: 	Training 200/553. train loss: 0.3328,	1.8204 s / batch. (data: 9.97e-01). ETA=22:16:09, max mem: 20.9 GB 
[12/05 02:14:27 visual_prompt]: 	Training 300/553. train loss: 0.7221,	0.8339 s / batch. (data: 1.44e-03). ETA=10:10:41, max mem: 20.9 GB 
[12/05 02:16:52 visual_prompt]: 	Training 400/553. train loss: 0.9835,	0.8402 s / batch. (data: 6.24e-03). ETA=10:13:56, max mem: 20.9 GB 
[12/05 02:19:01 visual_prompt]: 	Training 500/553. train loss: 0.6839,	0.8277 s / batch. (data: 3.20e-04). ETA=10:03:24, max mem: 20.9 GB 
[12/05 02:20:19 visual_prompt]: Epoch 21 / 100: avg data time: 7.26e-01, avg batch time: 1.5543, average train loss: 0.8068
[12/05 02:21:27 visual_prompt]: Inference (val):avg data time: 8.67e-05, avg batch time: 0.3103, average loss: 0.6207
[12/05 02:21:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.51	rocauc: 70.98	
[12/05 02:21:27 visual_prompt]: Best epoch 21: best metric: -0.621
[12/05 02:21:27 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[12/05 02:23:32 visual_prompt]: 	Training 100/553. train loss: 0.6275,	0.8541 s / batch. (data: 2.31e-02). ETA=10:20:27, max mem: 20.9 GB 
[12/05 02:25:34 visual_prompt]: 	Training 200/553. train loss: 0.5135,	0.8489 s / batch. (data: 9.64e-03). ETA=10:15:15, max mem: 20.9 GB 
[12/05 02:27:34 visual_prompt]: 	Training 300/553. train loss: 0.2174,	0.8340 s / batch. (data: 1.05e-02). ETA=10:03:06, max mem: 20.9 GB 
[12/05 02:29:31 visual_prompt]: 	Training 400/553. train loss: 0.7639,	0.8560 s / batch. (data: 3.43e-04). ETA=10:17:33, max mem: 20.9 GB 
[12/05 02:31:33 visual_prompt]: 	Training 500/553. train loss: 0.6402,	0.8676 s / batch. (data: 1.15e-03). ETA=10:24:29, max mem: 20.9 GB 
[12/05 02:32:38 visual_prompt]: Epoch 22 / 100: avg data time: 3.75e-01, avg batch time: 1.2135, average train loss: 0.7254
[12/05 02:33:47 visual_prompt]: Inference (val):avg data time: 8.07e-05, avg batch time: 0.3109, average loss: 0.6321
[12/05 02:33:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 71.04	
[12/05 02:33:47 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[12/05 02:35:50 visual_prompt]: 	Training 100/553. train loss: 1.1419,	1.7599 s / batch. (data: 9.26e-01). ETA=21:02:15, max mem: 20.9 GB 
[12/05 02:37:53 visual_prompt]: 	Training 200/553. train loss: 1.1796,	1.5680 s / batch. (data: 7.53e-01). ETA=18:41:58, max mem: 20.9 GB 
[12/05 02:39:56 visual_prompt]: 	Training 300/553. train loss: 1.0276,	0.8574 s / batch. (data: 8.57e-04). ETA=10:12:05, max mem: 20.9 GB 
[12/05 02:41:55 visual_prompt]: 	Training 400/553. train loss: 0.3921,	0.8855 s / batch. (data: 5.99e-03). ETA=10:30:43, max mem: 20.9 GB 
[12/05 02:43:46 visual_prompt]: 	Training 500/553. train loss: 0.7357,	0.8235 s / batch. (data: 2.32e-03). ETA=9:45:10, max mem: 20.9 GB 
[12/05 02:44:50 visual_prompt]: Epoch 23 / 100: avg data time: 3.61e-01, avg batch time: 1.1971, average train loss: 0.7772
[12/05 02:46:01 visual_prompt]: Inference (val):avg data time: 1.06e-04, avg batch time: 0.3122, average loss: 0.6213
[12/05 02:46:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.51	rocauc: 70.51	
[12/05 02:46:01 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[12/05 02:48:01 visual_prompt]: 	Training 100/553. train loss: 0.7550,	0.8450 s / batch. (data: 5.47e-03). ETA=9:58:15, max mem: 20.9 GB 
[12/05 02:49:59 visual_prompt]: 	Training 200/553. train loss: 1.1172,	0.8516 s / batch. (data: 5.46e-03). ETA=10:01:31, max mem: 20.9 GB 
[12/05 02:52:02 visual_prompt]: 	Training 300/553. train loss: 0.2621,	1.7382 s / batch. (data: 9.11e-01). ETA=20:24:51, max mem: 20.9 GB 
[12/05 02:54:01 visual_prompt]: 	Training 400/553. train loss: 0.1795,	0.8460 s / batch. (data: 6.81e-03). ETA=9:54:44, max mem: 20.9 GB 
[12/05 02:56:07 visual_prompt]: 	Training 500/553. train loss: 0.6438,	0.8266 s / batch. (data: 3.30e-04). ETA=9:39:42, max mem: 20.9 GB 
[12/05 02:57:06 visual_prompt]: Epoch 24 / 100: avg data time: 3.64e-01, avg batch time: 1.2011, average train loss: 0.7560
[12/05 02:58:14 visual_prompt]: Inference (val):avg data time: 7.45e-05, avg batch time: 0.3118, average loss: 0.7242
[12/05 02:58:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 68.20	
[12/05 02:58:14 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[12/05 03:00:25 visual_prompt]: 	Training 100/553. train loss: 0.6446,	0.8639 s / batch. (data: 1.35e-03). ETA=10:03:43, max mem: 20.9 GB 
[12/05 03:02:19 visual_prompt]: 	Training 200/553. train loss: 1.5657,	0.9142 s / batch. (data: 1.72e-02). ETA=10:37:20, max mem: 20.9 GB 
[12/05 03:04:19 visual_prompt]: 	Training 300/553. train loss: 0.4855,	1.4280 s / batch. (data: 5.90e-01). ETA=16:33:07, max mem: 20.9 GB 
[12/05 03:06:21 visual_prompt]: 	Training 400/553. train loss: 0.8730,	2.2436 s / batch. (data: 1.42e+00). ETA=1 day, 1:56:36, max mem: 20.9 GB 
[12/05 03:08:23 visual_prompt]: 	Training 500/553. train loss: 0.5101,	2.3071 s / batch. (data: 1.49e+00). ETA=1 day, 2:36:48, max mem: 20.9 GB 
[12/05 03:09:26 visual_prompt]: Epoch 25 / 100: avg data time: 3.75e-01, avg batch time: 1.2130, average train loss: 0.7445
[12/05 03:10:32 visual_prompt]: Inference (val):avg data time: 2.89e-04, avg batch time: 0.3108, average loss: 0.9815
[12/05 03:10:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 67.64	
[12/05 03:10:32 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[12/05 03:12:38 visual_prompt]: 	Training 100/553. train loss: 0.0479,	0.8614 s / batch. (data: 1.21e-03). ETA=9:54:01, max mem: 20.9 GB 
[12/05 03:14:39 visual_prompt]: 	Training 200/553. train loss: 0.5228,	2.2218 s / batch. (data: 1.38e+00). ETA=1 day, 1:28:24, max mem: 20.9 GB 
[12/05 03:16:37 visual_prompt]: 	Training 300/553. train loss: 0.9361,	0.8861 s / batch. (data: 3.03e-02). ETA=10:08:05, max mem: 20.9 GB 
[12/05 03:18:38 visual_prompt]: 	Training 400/553. train loss: 1.1999,	0.8629 s / batch. (data: 1.45e-03). ETA=9:50:42, max mem: 20.9 GB 
[12/05 03:20:53 visual_prompt]: 	Training 500/553. train loss: 0.2359,	0.8316 s / batch. (data: 1.30e-03). ETA=9:27:53, max mem: 20.9 GB 
[12/05 03:22:05 visual_prompt]: Epoch 26 / 100: avg data time: 4.16e-01, avg batch time: 1.2515, average train loss: 0.7377
[12/05 03:23:17 visual_prompt]: Inference (val):avg data time: 8.78e-05, avg batch time: 0.3135, average loss: 0.6744
[12/05 03:23:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 69.70	
[12/05 03:23:17 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[12/05 03:25:32 visual_prompt]: 	Training 100/553. train loss: 0.4973,	0.8639 s / batch. (data: 1.43e-03). ETA=9:47:45, max mem: 20.9 GB 
[12/05 03:27:43 visual_prompt]: 	Training 200/553. train loss: 0.3193,	0.8160 s / batch. (data: 3.33e-04). ETA=9:13:48, max mem: 20.9 GB 
[12/05 03:29:50 visual_prompt]: 	Training 300/553. train loss: 0.8815,	0.8260 s / batch. (data: 3.27e-04). ETA=9:19:14, max mem: 20.9 GB 
[12/05 03:31:56 visual_prompt]: 	Training 400/553. train loss: 0.8618,	0.8686 s / batch. (data: 1.92e-02). ETA=9:46:36, max mem: 20.9 GB 
[12/05 03:34:03 visual_prompt]: 	Training 500/553. train loss: 0.6133,	0.8405 s / batch. (data: 1.30e-03). ETA=9:26:14, max mem: 20.9 GB 
[12/05 03:35:13 visual_prompt]: Epoch 27 / 100: avg data time: 4.61e-01, avg batch time: 1.2949, average train loss: 0.7071
[12/05 03:36:24 visual_prompt]: Inference (val):avg data time: 8.89e-05, avg batch time: 0.3117, average loss: 0.6632
[12/05 03:36:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.71	
[12/05 03:36:24 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[12/05 03:38:25 visual_prompt]: 	Training 100/553. train loss: 0.6293,	0.8478 s / batch. (data: 1.54e-03). ETA=9:29:00, max mem: 20.9 GB 
[12/05 03:40:28 visual_prompt]: 	Training 200/553. train loss: 0.1998,	0.8204 s / batch. (data: 5.47e-03). ETA=9:09:15, max mem: 20.9 GB 
[12/05 03:42:27 visual_prompt]: 	Training 300/553. train loss: 0.8809,	2.6546 s / batch. (data: 1.83e+00). ETA=1 day, 5:32:48, max mem: 20.9 GB 
[12/05 03:44:28 visual_prompt]: 	Training 400/553. train loss: 0.4145,	0.8774 s / batch. (data: 1.28e-03). ETA=9:44:27, max mem: 20.9 GB 
[12/05 03:46:31 visual_prompt]: 	Training 500/553. train loss: 0.4562,	0.8508 s / batch. (data: 1.14e-02). ETA=9:25:19, max mem: 20.9 GB 
[12/05 03:47:35 visual_prompt]: Epoch 28 / 100: avg data time: 3.74e-01, avg batch time: 1.2129, average train loss: 0.6504
[12/05 03:48:48 visual_prompt]: Inference (val):avg data time: 1.13e-04, avg batch time: 0.3124, average loss: 0.6399
[12/05 03:48:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.11	rocauc: 73.17	
[12/05 03:48:48 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[12/05 03:50:56 visual_prompt]: 	Training 100/553. train loss: 0.0428,	0.8340 s / batch. (data: 1.98e-03). ETA=9:12:04, max mem: 20.9 GB 
[12/05 03:52:58 visual_prompt]: 	Training 200/553. train loss: 0.9531,	2.9761 s / batch. (data: 2.13e+00). ETA=1 day, 8:45:00, max mem: 20.9 GB 
[12/05 03:54:56 visual_prompt]: 	Training 300/553. train loss: 0.9154,	0.8398 s / batch. (data: 1.05e-02). ETA=9:13:03, max mem: 20.9 GB 
[12/05 03:56:48 visual_prompt]: 	Training 400/553. train loss: 0.8215,	1.8987 s / batch. (data: 1.07e+00). ETA=20:47:19, max mem: 20.9 GB 
[12/05 03:58:50 visual_prompt]: 	Training 500/553. train loss: 0.3778,	0.8585 s / batch. (data: 1.19e-03). ETA=9:22:33, max mem: 20.9 GB 
[12/05 03:59:54 visual_prompt]: Epoch 29 / 100: avg data time: 3.65e-01, avg batch time: 1.2034, average train loss: 0.7060
[12/05 04:01:07 visual_prompt]: Inference (val):avg data time: 2.99e-04, avg batch time: 0.3127, average loss: 0.6592
[12/05 04:01:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 70.17	
[12/05 04:01:07 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[12/05 04:03:14 visual_prompt]: 	Training 100/553. train loss: 0.9069,	0.8366 s / batch. (data: 1.06e-03). ETA=9:06:03, max mem: 20.9 GB 
[12/05 04:05:09 visual_prompt]: 	Training 200/553. train loss: 0.5660,	0.8597 s / batch. (data: 8.77e-04). ETA=9:19:43, max mem: 20.9 GB 
[12/05 04:07:12 visual_prompt]: 	Training 300/553. train loss: 0.2838,	3.2783 s / batch. (data: 2.45e+00). ETA=1 day, 11:28:54, max mem: 20.9 GB 
[12/05 04:09:11 visual_prompt]: 	Training 400/553. train loss: 1.1661,	1.4487 s / batch. (data: 6.34e-01). ETA=15:38:19, max mem: 20.9 GB 
[12/05 04:11:09 visual_prompt]: 	Training 500/553. train loss: 0.1932,	1.9664 s / batch. (data: 1.13e+00). ETA=21:10:21, max mem: 20.9 GB 
[12/05 04:12:14 visual_prompt]: Epoch 30 / 100: avg data time: 3.68e-01, avg batch time: 1.2054, average train loss: 0.6621
[12/05 04:13:27 visual_prompt]: Inference (val):avg data time: 3.21e-04, avg batch time: 0.3111, average loss: 0.6284
[12/05 04:13:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 71.14	rocauc: 73.43	
[12/05 04:13:27 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[12/05 04:15:35 visual_prompt]: 	Training 100/553. train loss: 1.1883,	0.8840 s / batch. (data: 6.90e-03). ETA=9:28:51, max mem: 20.9 GB 
[12/05 04:17:41 visual_prompt]: 	Training 200/553. train loss: 1.1971,	0.8200 s / batch. (data: 3.82e-04). ETA=8:46:19, max mem: 20.9 GB 
[12/05 04:19:31 visual_prompt]: 	Training 300/553. train loss: 0.9919,	1.2680 s / batch. (data: 4.43e-01). ETA=13:31:43, max mem: 20.9 GB 
[12/05 04:21:33 visual_prompt]: 	Training 400/553. train loss: 0.8116,	0.9166 s / batch. (data: 3.12e-02). ETA=9:45:13, max mem: 20.9 GB 
[12/05 04:23:29 visual_prompt]: 	Training 500/553. train loss: 0.9081,	0.8560 s / batch. (data: 7.95e-03). ETA=9:05:07, max mem: 20.9 GB 
[12/05 04:24:30 visual_prompt]: Epoch 31 / 100: avg data time: 3.62e-01, avg batch time: 1.1986, average train loss: 0.6284
[12/05 04:25:43 visual_prompt]: Inference (val):avg data time: 2.71e-04, avg batch time: 0.3129, average loss: 0.6998
[12/05 04:25:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 70.97	
[12/05 04:25:43 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[12/05 04:27:52 visual_prompt]: 	Training 100/553. train loss: 0.2571,	0.8245 s / batch. (data: 1.06e-02). ETA=8:42:57, max mem: 20.9 GB 
[12/05 04:29:54 visual_prompt]: 	Training 200/553. train loss: 0.3009,	0.8651 s / batch. (data: 1.33e-03). ETA=9:07:16, max mem: 20.9 GB 
[12/05 04:31:58 visual_prompt]: 	Training 300/553. train loss: 1.1221,	0.8143 s / batch. (data: 2.57e-04). ETA=8:33:47, max mem: 20.9 GB 
[12/05 04:33:54 visual_prompt]: 	Training 400/553. train loss: 0.9229,	0.8702 s / batch. (data: 2.21e-02). ETA=9:07:36, max mem: 20.9 GB 
[12/05 04:35:53 visual_prompt]: 	Training 500/553. train loss: 1.2454,	0.8274 s / batch. (data: 1.07e-03). ETA=8:39:18, max mem: 20.9 GB 
[12/05 04:36:54 visual_prompt]: Epoch 32 / 100: avg data time: 3.76e-01, avg batch time: 1.2132, average train loss: 0.5979
[12/05 04:37:59 visual_prompt]: Inference (val):avg data time: 5.36e-05, avg batch time: 0.3099, average loss: 0.8917
[12/05 04:37:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 67.66	
[12/05 04:37:59 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[12/05 04:40:05 visual_prompt]: 	Training 100/553. train loss: 1.0878,	1.0909 s / batch. (data: 2.56e-01). ETA=11:21:52, max mem: 20.9 GB 
[12/05 04:42:11 visual_prompt]: 	Training 200/553. train loss: 0.2799,	2.6840 s / batch. (data: 1.79e+00). ETA=1 day, 3:53:12, max mem: 20.9 GB 
[12/05 04:44:12 visual_prompt]: 	Training 300/553. train loss: 0.7885,	0.8559 s / batch. (data: 1.19e-02). ETA=8:52:07, max mem: 20.9 GB 
[12/05 04:46:12 visual_prompt]: 	Training 400/553. train loss: 0.3498,	0.8410 s / batch. (data: 5.49e-03). ETA=8:41:28, max mem: 20.9 GB 
[12/05 04:48:10 visual_prompt]: 	Training 500/553. train loss: 1.0047,	2.1680 s / batch. (data: 1.35e+00). ETA=22:20:40, max mem: 20.9 GB 
[12/05 04:49:13 visual_prompt]: Epoch 33 / 100: avg data time: 3.78e-01, avg batch time: 1.2171, average train loss: 0.6071
[12/05 04:50:26 visual_prompt]: Inference (val):avg data time: 1.09e-04, avg batch time: 0.3118, average loss: 0.8992
[12/05 04:50:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 69.89	
[12/05 04:50:26 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[12/05 04:52:28 visual_prompt]: 	Training 100/553. train loss: 1.3330,	1.6607 s / batch. (data: 8.15e-01). ETA=17:02:46, max mem: 20.9 GB 
[12/05 04:54:29 visual_prompt]: 	Training 200/553. train loss: 0.5755,	0.8237 s / batch. (data: 9.58e-03). ETA=8:25:53, max mem: 20.9 GB 
[12/05 04:56:30 visual_prompt]: 	Training 300/553. train loss: 0.3913,	1.5238 s / batch. (data: 6.86e-01). ETA=15:33:19, max mem: 20.9 GB 
[12/05 04:58:34 visual_prompt]: 	Training 400/553. train loss: 0.3674,	0.8334 s / batch. (data: 5.90e-03). ETA=8:29:03, max mem: 20.9 GB 
[12/05 05:00:30 visual_prompt]: 	Training 500/553. train loss: 0.1146,	1.6943 s / batch. (data: 8.55e-01). ETA=17:12:08, max mem: 20.9 GB 
[12/05 05:01:31 visual_prompt]: Epoch 34 / 100: avg data time: 3.66e-01, avg batch time: 1.2033, average train loss: 0.5920
[12/05 05:02:45 visual_prompt]: Inference (val):avg data time: 4.85e-04, avg batch time: 0.3116, average loss: 0.7072
[12/05 05:02:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 69.46	
[12/05 05:02:45 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[12/05 05:04:54 visual_prompt]: 	Training 100/553. train loss: 0.6803,	0.8301 s / batch. (data: 3.95e-04). ETA=8:23:33, max mem: 20.9 GB 
[12/05 05:06:52 visual_prompt]: 	Training 200/553. train loss: 1.0449,	1.0548 s / batch. (data: 2.25e-01). ETA=10:38:06, max mem: 20.9 GB 
[12/05 05:08:52 visual_prompt]: 	Training 300/553. train loss: 0.1308,	0.8409 s / batch. (data: 1.06e-02). ETA=8:27:20, max mem: 20.9 GB 
[12/05 05:10:54 visual_prompt]: 	Training 400/553. train loss: 0.2828,	0.8324 s / batch. (data: 3.24e-04). ETA=8:20:49, max mem: 20.9 GB 
[12/05 05:12:58 visual_prompt]: 	Training 500/553. train loss: 0.2773,	1.2359 s / batch. (data: 4.14e-01). ETA=12:21:31, max mem: 20.9 GB 
[12/05 05:13:57 visual_prompt]: Epoch 35 / 100: avg data time: 3.78e-01, avg batch time: 1.2162, average train loss: 0.5914
[12/05 05:15:12 visual_prompt]: Inference (val):avg data time: 2.93e-04, avg batch time: 0.3112, average loss: 0.9828
[12/05 05:15:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 71.55	
[12/05 05:15:12 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[12/05 05:17:20 visual_prompt]: 	Training 100/553. train loss: 0.9047,	0.8600 s / batch. (data: 7.95e-03). ETA=8:33:46, max mem: 20.9 GB 
[12/05 05:19:20 visual_prompt]: 	Training 200/553. train loss: 0.3480,	0.8245 s / batch. (data: 6.53e-03). ETA=8:11:12, max mem: 20.9 GB 
[12/05 05:21:26 visual_prompt]: 	Training 300/553. train loss: 0.2744,	0.8748 s / batch. (data: 6.90e-03). ETA=8:39:41, max mem: 20.9 GB 
[12/05 05:23:30 visual_prompt]: 	Training 400/553. train loss: 0.6387,	0.8716 s / batch. (data: 1.19e-02). ETA=8:36:22, max mem: 20.9 GB 
[12/05 05:25:33 visual_prompt]: 	Training 500/553. train loss: 0.9122,	1.4560 s / batch. (data: 6.14e-01). ETA=14:20:06, max mem: 20.9 GB 
[12/05 05:26:27 visual_prompt]: Epoch 36 / 100: avg data time: 3.83e-01, avg batch time: 1.2211, average train loss: 0.6014
[12/05 05:27:41 visual_prompt]: Inference (val):avg data time: 1.07e-04, avg batch time: 0.3148, average loss: 0.7978
[12/05 05:27:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 71.03	
[12/05 05:27:41 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[12/05 05:29:52 visual_prompt]: 	Training 100/553. train loss: 0.1724,	0.8233 s / batch. (data: 1.46e-03). ETA=8:04:15, max mem: 20.9 GB 
[12/05 05:31:48 visual_prompt]: 	Training 200/553. train loss: 0.3290,	0.8635 s / batch. (data: 1.35e-03). ETA=8:26:29, max mem: 20.9 GB 
[12/05 05:33:53 visual_prompt]: 	Training 300/553. train loss: 0.9562,	2.7620 s / batch. (data: 1.94e+00). ETA=1 day, 2:55:24, max mem: 20.9 GB 
[12/05 05:36:00 visual_prompt]: 	Training 400/553. train loss: 1.2519,	2.8132 s / batch. (data: 1.97e+00). ETA=1 day, 3:20:41, max mem: 20.9 GB 
[12/05 05:38:01 visual_prompt]: 	Training 500/553. train loss: 0.1224,	2.0634 s / batch. (data: 1.21e+00). ETA=19:59:56, max mem: 20.9 GB 
[12/05 05:39:00 visual_prompt]: Epoch 37 / 100: avg data time: 3.89e-01, avg batch time: 1.2270, average train loss: 0.5808
[12/05 05:40:12 visual_prompt]: Inference (val):avg data time: 9.78e-05, avg batch time: 0.3132, average loss: 1.0795
[12/05 05:40:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 71.30	
[12/05 05:40:12 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[12/05 05:42:21 visual_prompt]: 	Training 100/553. train loss: 0.1724,	1.8506 s / batch. (data: 1.02e+00). ETA=17:51:26, max mem: 20.9 GB 
[12/05 05:44:19 visual_prompt]: 	Training 200/553. train loss: 0.1294,	2.1293 s / batch. (data: 1.22e+00). ETA=20:29:17, max mem: 20.9 GB 
[12/05 05:46:25 visual_prompt]: 	Training 300/553. train loss: 0.1390,	0.8595 s / batch. (data: 1.42e-03). ETA=8:14:46, max mem: 20.9 GB 
[12/05 05:48:26 visual_prompt]: 	Training 400/553. train loss: 0.5144,	0.8479 s / batch. (data: 1.23e-03). ETA=8:06:40, max mem: 20.9 GB 
[12/05 05:50:33 visual_prompt]: 	Training 500/553. train loss: 0.4725,	0.8676 s / batch. (data: 1.22e-02). ETA=8:16:32, max mem: 20.9 GB 
[12/05 05:51:30 visual_prompt]: Epoch 38 / 100: avg data time: 3.88e-01, avg batch time: 1.2262, average train loss: 0.5359
[12/05 05:52:40 visual_prompt]: Inference (val):avg data time: 9.05e-05, avg batch time: 0.3112, average loss: 0.7514
[12/05 05:52:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 71.24	
[12/05 05:52:40 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[12/05 05:54:49 visual_prompt]: 	Training 100/553. train loss: 0.3161,	0.8284 s / batch. (data: 3.02e-04). ETA=7:52:01, max mem: 20.9 GB 
[12/05 05:56:51 visual_prompt]: 	Training 200/553. train loss: 1.3277,	0.8482 s / batch. (data: 1.57e-02). ETA=8:01:50, max mem: 20.9 GB 
[12/05 05:58:58 visual_prompt]: 	Training 300/553. train loss: 0.8421,	0.8618 s / batch. (data: 6.94e-03). ETA=8:08:09, max mem: 20.9 GB 
[12/05 06:00:59 visual_prompt]: 	Training 400/553. train loss: 0.5633,	1.6316 s / batch. (data: 8.19e-01). ETA=15:21:29, max mem: 20.9 GB 
[12/05 06:03:04 visual_prompt]: 	Training 500/553. train loss: 0.2587,	3.0604 s / batch. (data: 2.21e+00). ETA=1 day, 4:43:19, max mem: 20.9 GB 
[12/05 06:04:04 visual_prompt]: Epoch 39 / 100: avg data time: 4.00e-01, avg batch time: 1.2375, average train loss: 0.5053
[12/05 06:05:15 visual_prompt]: Inference (val):avg data time: 2.09e-04, avg batch time: 0.3111, average loss: 0.7834
[12/05 06:05:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 69.86	
[12/05 06:05:15 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[12/05 06:07:29 visual_prompt]: 	Training 100/553. train loss: 0.1458,	0.8485 s / batch. (data: 3.07e-04). ETA=7:55:37, max mem: 20.9 GB 
[12/05 06:09:26 visual_prompt]: 	Training 200/553. train loss: 0.3776,	0.8536 s / batch. (data: 7.96e-03). ETA=7:57:02, max mem: 20.9 GB 
[12/05 06:11:30 visual_prompt]: 	Training 300/553. train loss: 0.6596,	0.8205 s / batch. (data: 3.54e-04). ETA=7:37:10, max mem: 20.9 GB 
[12/05 06:13:35 visual_prompt]: 	Training 400/553. train loss: 0.1354,	0.8462 s / batch. (data: 8.87e-04). ETA=7:50:07, max mem: 20.9 GB 
[12/05 06:15:38 visual_prompt]: 	Training 500/553. train loss: 0.1018,	0.8524 s / batch. (data: 7.95e-03). ETA=7:52:08, max mem: 20.9 GB 
[12/05 06:16:41 visual_prompt]: Epoch 40 / 100: avg data time: 4.01e-01, avg batch time: 1.2404, average train loss: 0.4975
[12/05 06:17:49 visual_prompt]: Inference (val):avg data time: 1.41e-04, avg batch time: 0.3134, average loss: 0.8600
[12/05 06:17:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 72.71	
[12/05 06:17:49 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[12/05 06:20:06 visual_prompt]: 	Training 100/553. train loss: 0.5987,	0.8246 s / batch. (data: 3.17e-04). ETA=7:34:38, max mem: 20.9 GB 
[12/05 06:22:05 visual_prompt]: 	Training 200/553. train loss: 1.4101,	0.8245 s / batch. (data: 8.41e-04). ETA=7:33:12, max mem: 20.9 GB 
[12/05 06:24:07 visual_prompt]: 	Training 300/553. train loss: 0.5367,	0.8368 s / batch. (data: 3.16e-04). ETA=7:38:33, max mem: 20.9 GB 
[12/05 06:26:10 visual_prompt]: 	Training 400/553. train loss: 0.2023,	0.8456 s / batch. (data: 1.35e-03). ETA=7:41:59, max mem: 20.9 GB 
[12/05 06:28:11 visual_prompt]: 	Training 500/553. train loss: 0.2501,	0.8365 s / batch. (data: 3.31e-04). ETA=7:35:36, max mem: 20.9 GB 
[12/05 06:29:11 visual_prompt]: Epoch 41 / 100: avg data time: 3.94e-01, avg batch time: 1.2326, average train loss: 0.5044
[12/05 06:30:15 visual_prompt]: Inference (val):avg data time: 5.42e-05, avg batch time: 0.3097, average loss: 0.7487
[12/05 06:30:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 69.26	
[12/05 06:30:15 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[12/05 06:32:24 visual_prompt]: 	Training 100/553. train loss: 0.3799,	0.8310 s / batch. (data: 6.76e-03). ETA=7:30:31, max mem: 20.9 GB 
[12/05 06:34:22 visual_prompt]: 	Training 200/553. train loss: 0.3275,	1.0800 s / batch. (data: 2.43e-01). ETA=9:43:41, max mem: 20.9 GB 
[12/05 06:36:25 visual_prompt]: 	Training 300/553. train loss: 0.2889,	0.8565 s / batch. (data: 7.95e-03). ETA=7:41:27, max mem: 20.9 GB 
[12/05 06:38:28 visual_prompt]: 	Training 400/553. train loss: 0.6291,	0.8352 s / batch. (data: 1.07e-02). ETA=7:28:35, max mem: 20.9 GB 
[12/05 06:40:31 visual_prompt]: 	Training 500/553. train loss: 0.3998,	0.8727 s / batch. (data: 1.72e-03). ETA=7:47:16, max mem: 20.9 GB 
[12/05 06:41:35 visual_prompt]: Epoch 42 / 100: avg data time: 3.90e-01, avg batch time: 1.2289, average train loss: 0.4768
[12/05 06:42:40 visual_prompt]: Inference (val):avg data time: 4.50e-05, avg batch time: 0.3096, average loss: 1.1171
[12/05 06:42:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 63.36	
[12/05 06:42:40 visual_prompt]: Stopping early.
[12/05 06:42:40 visual_prompt]: Rank of current process: 0. World size: 1
[12/05 06:42:40 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/05 06:42:40 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/05 06:42:40 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/05 06:42:40 visual_prompt]: Training with config:
[12/05 06:42:40 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.1_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/05 06:42:40 visual_prompt]: Loading training data...
[12/05 06:42:40 visual_prompt]: Constructing mammo-cbis dataset train...
[12/05 06:42:40 visual_prompt]: Loading validation data...
[12/05 06:42:40 visual_prompt]: Constructing mammo-cbis dataset val...
[12/05 06:42:40 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[12/05 06:42:52 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[12/05 06:42:52 visual_prompt]: tuned percent:0.525
[12/05 06:42:52 visual_prompt]: Device used for model: 0
[12/05 06:42:52 visual_prompt]: Setting up Evaluator...
[12/05 06:42:52 visual_prompt]: Setting up Trainer...
[12/05 06:42:52 visual_prompt]: 	Setting up the optimizer...
[12/05 06:42:52 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/05 06:45:00 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8438 s / batch. (data: 1.88e-03). ETA=12:56:20, max mem: 20.9 GB 
[12/05 06:46:57 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8189 s / batch. (data: 3.20e-04). ETA=12:32:02, max mem: 20.9 GB 
[12/05 06:49:02 visual_prompt]: 	Training 300/553. train loss: 1.3905,	3.6015 s / batch. (data: 2.78e+00). ETA=2 days, 7:01:22, max mem: 20.9 GB 
[12/05 06:51:02 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8681 s / batch. (data: 1.46e-03). ETA=13:14:18, max mem: 20.9 GB 
[12/05 06:53:09 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8552 s / batch. (data: 8.24e-04). ETA=13:01:03, max mem: 20.9 GB 
[12/05 06:54:14 visual_prompt]: Epoch 1 / 100: avg data time: 3.93e-01, avg batch time: 1.2324, average train loss: 1.5403
[12/05 06:55:20 visual_prompt]: Inference (val):avg data time: 5.48e-05, avg batch time: 0.3092, average loss: 1.5201
[12/05 06:55:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[12/05 06:55:20 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[12/05 06:57:26 visual_prompt]: 	Training 100/553. train loss: 0.6431,	2.2723 s / batch. (data: 1.45e+00). ETA=1 day, 10:29:35, max mem: 20.9 GB 
[12/05 06:59:25 visual_prompt]: 	Training 200/553. train loss: 0.2374,	1.1295 s / batch. (data: 2.85e-01). ETA=17:06:51, max mem: 20.9 GB 
[12/05 07:01:27 visual_prompt]: 	Training 300/553. train loss: 0.7816,	2.0534 s / batch. (data: 1.21e+00). ETA=1 day, 7:03:19, max mem: 20.9 GB 
[12/05 07:03:29 visual_prompt]: 	Training 400/553. train loss: 0.9218,	0.8912 s / batch. (data: 7.69e-03). ETA=13:27:16, max mem: 20.9 GB 
[12/05 07:05:35 visual_prompt]: 	Training 500/553. train loss: 0.7228,	0.8496 s / batch. (data: 1.18e-02). ETA=12:48:09, max mem: 20.9 GB 
[12/05 07:06:37 visual_prompt]: Epoch 2 / 100: avg data time: 3.86e-01, avg batch time: 1.2243, average train loss: 0.7701
[12/05 07:07:47 visual_prompt]: Inference (val):avg data time: 8.00e-05, avg batch time: 0.3108, average loss: 0.7302
[12/05 07:07:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.36	
[12/05 07:07:47 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[12/05 07:09:48 visual_prompt]: 	Training 100/553. train loss: 0.7790,	0.8523 s / batch. (data: 3.50e-04). ETA=12:48:23, max mem: 20.9 GB 
[12/05 07:11:53 visual_prompt]: 	Training 200/553. train loss: 0.7828,	1.3238 s / batch. (data: 4.92e-01). ETA=19:51:14, max mem: 20.9 GB 
[12/05 07:13:49 visual_prompt]: 	Training 300/553. train loss: 0.5731,	0.8641 s / batch. (data: 1.00e-03). ETA=12:56:09, max mem: 20.9 GB 
[12/05 07:15:53 visual_prompt]: 	Training 400/553. train loss: 0.6154,	0.8395 s / batch. (data: 1.37e-03). ETA=12:32:39, max mem: 20.9 GB 
[12/05 07:17:58 visual_prompt]: 	Training 500/553. train loss: 0.7019,	1.9239 s / batch. (data: 1.11e+00). ETA=1 day, 4:41:43, max mem: 20.9 GB 
[12/05 07:18:59 visual_prompt]: Epoch 3 / 100: avg data time: 3.79e-01, avg batch time: 1.2158, average train loss: 0.7334
[12/05 07:20:11 visual_prompt]: Inference (val):avg data time: 1.15e-04, avg batch time: 0.3133, average loss: 0.7195
[12/05 07:20:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.31	
[12/05 07:20:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[12/05 07:22:13 visual_prompt]: 	Training 100/553. train loss: 0.7262,	0.8747 s / batch. (data: 5.49e-03). ETA=13:00:31, max mem: 20.9 GB 
[12/05 07:24:15 visual_prompt]: 	Training 200/553. train loss: 0.6107,	0.8635 s / batch. (data: 3.50e-02). ETA=12:49:06, max mem: 20.9 GB 
[12/05 07:26:12 visual_prompt]: 	Training 300/553. train loss: 0.6999,	1.0377 s / batch. (data: 2.23e-01). ETA=15:22:32, max mem: 20.9 GB 
[12/05 07:28:09 visual_prompt]: 	Training 400/553. train loss: 0.7789,	2.4502 s / batch. (data: 1.62e+00). ETA=1 day, 12:14:09, max mem: 20.9 GB 
[12/05 07:30:14 visual_prompt]: 	Training 500/553. train loss: 0.3921,	4.7025 s / batch. (data: 3.89e+00). ETA=2 days, 21:24:55, max mem: 20.9 GB 
[12/05 07:31:17 visual_prompt]: Epoch 4 / 100: avg data time: 3.66e-01, avg batch time: 1.2028, average train loss: 0.7260
[12/05 07:32:29 visual_prompt]: Inference (val):avg data time: 1.09e-04, avg batch time: 0.3137, average loss: 0.7170
[12/05 07:32:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.38	
[12/05 07:32:29 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[12/05 07:34:34 visual_prompt]: 	Training 100/553. train loss: 0.4793,	0.8459 s / batch. (data: 9.78e-03). ETA=12:27:01, max mem: 20.9 GB 
[12/05 07:36:28 visual_prompt]: 	Training 200/553. train loss: 0.5785,	2.0664 s / batch. (data: 1.23e+00). ETA=1 day, 6:21:27, max mem: 20.9 GB 
[12/05 07:38:32 visual_prompt]: 	Training 300/553. train loss: 0.7965,	0.8325 s / batch. (data: 6.00e-03). ETA=12:12:26, max mem: 20.9 GB 
[12/05 07:40:27 visual_prompt]: 	Training 400/553. train loss: 0.5750,	0.8239 s / batch. (data: 5.46e-03). ETA=12:03:32, max mem: 20.9 GB 
[12/05 07:42:28 visual_prompt]: 	Training 500/553. train loss: 0.6880,	0.8595 s / batch. (data: 1.29e-03). ETA=12:33:21, max mem: 20.9 GB 
[12/05 07:43:32 visual_prompt]: Epoch 5 / 100: avg data time: 3.61e-01, avg batch time: 1.1983, average train loss: 0.7161
[12/05 07:44:43 visual_prompt]: Inference (val):avg data time: 1.19e-04, avg batch time: 0.3111, average loss: 0.6887
[12/05 07:44:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.48	
[12/05 07:44:43 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[12/05 07:46:53 visual_prompt]: 	Training 100/553. train loss: 0.7186,	0.8400 s / batch. (data: 1.23e-03). ETA=12:14:04, max mem: 20.9 GB 
[12/05 07:48:54 visual_prompt]: 	Training 200/553. train loss: 0.6947,	0.8505 s / batch. (data: 2.45e-02). ETA=12:21:53, max mem: 20.9 GB 
[12/05 07:50:46 visual_prompt]: 	Training 300/553. train loss: 0.5645,	0.8573 s / batch. (data: 2.19e-02). ETA=12:26:18, max mem: 20.9 GB 
[12/05 07:52:53 visual_prompt]: 	Training 400/553. train loss: 0.6932,	0.8453 s / batch. (data: 9.39e-03). ETA=12:14:28, max mem: 20.9 GB 
[12/05 07:54:47 visual_prompt]: 	Training 500/553. train loss: 0.7095,	1.2960 s / batch. (data: 4.67e-01). ETA=18:43:57, max mem: 20.9 GB 
[12/05 07:55:49 visual_prompt]: Epoch 6 / 100: avg data time: 3.65e-01, avg batch time: 1.2026, average train loss: 0.7280
[12/05 07:57:01 visual_prompt]: Inference (val):avg data time: 5.84e-04, avg batch time: 0.3130, average loss: 0.7389
[12/05 07:57:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.21	
[12/05 07:57:01 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[12/05 07:59:06 visual_prompt]: 	Training 100/553. train loss: 0.4931,	0.8499 s / batch. (data: 7.70e-03). ETA=12:14:57, max mem: 20.9 GB 
[12/05 08:01:09 visual_prompt]: 	Training 200/553. train loss: 0.5623,	0.8306 s / batch. (data: 1.20e-03). ETA=11:56:50, max mem: 20.9 GB 
[12/05 08:03:14 visual_prompt]: 	Training 300/553. train loss: 0.8180,	2.4460 s / batch. (data: 1.63e+00). ETA=1 day, 11:06:51, max mem: 20.9 GB 
[12/05 08:05:10 visual_prompt]: 	Training 400/553. train loss: 0.6170,	3.0058 s / batch. (data: 2.16e+00). ETA=1 day, 19:04:05, max mem: 20.9 GB 
[12/05 08:07:10 visual_prompt]: 	Training 500/553. train loss: 0.9595,	0.8567 s / batch. (data: 1.38e-02). ETA=12:15:06, max mem: 20.9 GB 
[12/05 08:08:10 visual_prompt]: Epoch 7 / 100: avg data time: 3.73e-01, avg batch time: 1.2102, average train loss: 0.7295
[12/05 08:09:16 visual_prompt]: Inference (val):avg data time: 5.54e-05, avg batch time: 0.3099, average loss: 0.8218
[12/05 08:09:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.82	
[12/05 08:09:16 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[12/05 08:11:21 visual_prompt]: 	Training 100/553. train loss: 0.7122,	0.9701 s / batch. (data: 6.68e-03). ETA=13:49:54, max mem: 20.9 GB 
[12/05 08:13:26 visual_prompt]: 	Training 200/553. train loss: 1.2346,	0.8467 s / batch. (data: 7.85e-03). ETA=12:02:58, max mem: 20.9 GB 
[12/05 08:15:29 visual_prompt]: 	Training 300/553. train loss: 0.6719,	0.8601 s / batch. (data: 1.48e-03). ETA=12:12:53, max mem: 20.9 GB 
[12/05 08:17:29 visual_prompt]: 	Training 400/553. train loss: 0.7037,	0.8364 s / batch. (data: 5.49e-03). ETA=11:51:20, max mem: 20.9 GB 
[12/05 08:19:26 visual_prompt]: 	Training 500/553. train loss: 0.9900,	2.4485 s / batch. (data: 1.62e+00). ETA=1 day, 10:38:19, max mem: 20.9 GB 
[12/05 08:20:29 visual_prompt]: Epoch 8 / 100: avg data time: 3.78e-01, avg batch time: 1.2157, average train loss: 0.7613
[12/05 08:21:41 visual_prompt]: Inference (val):avg data time: 9.67e-05, avg batch time: 0.3138, average loss: 0.7099
[12/05 08:21:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.16	
[12/05 08:21:41 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[12/05 08:23:41 visual_prompt]: 	Training 100/553. train loss: 0.4686,	0.8323 s / batch. (data: 1.43e-03). ETA=11:44:19, max mem: 20.9 GB 
[12/05 08:25:43 visual_prompt]: 	Training 200/553. train loss: 0.8585,	0.8443 s / batch. (data: 3.27e-04). ETA=11:53:03, max mem: 20.9 GB 
[12/05 08:27:46 visual_prompt]: 	Training 300/553. train loss: 0.6885,	2.8295 s / batch. (data: 2.01e+00). ETA=1 day, 15:45:04, max mem: 20.9 GB 
[12/05 08:29:49 visual_prompt]: 	Training 400/553. train loss: 0.5860,	0.8538 s / batch. (data: 5.33e-04). ETA=11:58:16, max mem: 20.9 GB 
[12/05 08:31:49 visual_prompt]: 	Training 500/553. train loss: 0.7607,	0.8531 s / batch. (data: 3.61e-04). ETA=11:56:16, max mem: 20.9 GB 
[12/05 08:32:47 visual_prompt]: Epoch 9 / 100: avg data time: 3.67e-01, avg batch time: 1.2040, average train loss: 0.7489
[12/05 08:34:00 visual_prompt]: Inference (val):avg data time: 5.16e-04, avg batch time: 0.3133, average loss: 0.7599
[12/05 08:34:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.02	
[12/05 08:34:00 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[12/05 08:36:11 visual_prompt]: 	Training 100/553. train loss: 0.6948,	0.8648 s / batch. (data: 9.02e-04). ETA=12:03:51, max mem: 20.9 GB 
[12/05 08:38:05 visual_prompt]: 	Training 200/553. train loss: 0.7120,	0.8582 s / batch. (data: 2.58e-02). ETA=11:56:57, max mem: 20.9 GB 
[12/05 08:40:06 visual_prompt]: 	Training 300/553. train loss: 0.6014,	2.2905 s / batch. (data: 1.45e+00). ETA=1 day, 7:49:37, max mem: 20.9 GB 
[12/05 08:42:06 visual_prompt]: 	Training 400/553. train loss: 0.8461,	1.3535 s / batch. (data: 4.83e-01). ETA=18:46:09, max mem: 20.9 GB 
[12/05 08:44:09 visual_prompt]: 	Training 500/553. train loss: 1.3348,	1.2462 s / batch. (data: 4.01e-01). ETA=17:14:50, max mem: 20.9 GB 
[12/05 08:45:13 visual_prompt]: Epoch 10 / 100: avg data time: 3.80e-01, avg batch time: 1.2162, average train loss: 0.7891
[12/05 08:46:17 visual_prompt]: Inference (val):avg data time: 2.54e-04, avg batch time: 0.3102, average loss: 0.9843
[12/05 08:46:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.53	
[12/05 08:46:17 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[12/05 08:48:24 visual_prompt]: 	Training 100/553. train loss: 0.7003,	0.8438 s / batch. (data: 1.14e-03). ETA=11:38:31, max mem: 20.9 GB 
[12/05 08:50:28 visual_prompt]: 	Training 200/553. train loss: 1.8986,	0.8333 s / batch. (data: 9.25e-03). ETA=11:28:26, max mem: 20.9 GB 
[12/05 08:52:25 visual_prompt]: 	Training 300/553. train loss: 0.6453,	3.3294 s / batch. (data: 2.51e+00). ETA=1 day, 21:45:07, max mem: 20.9 GB 
[12/05 08:54:25 visual_prompt]: 	Training 400/553. train loss: 0.5925,	0.8588 s / batch. (data: 1.21e-02). ETA=11:46:41, max mem: 20.9 GB 
[12/05 08:56:29 visual_prompt]: 	Training 500/553. train loss: 0.7808,	0.8398 s / batch. (data: 1.59e-03). ETA=11:29:38, max mem: 20.9 GB 
[12/05 08:57:34 visual_prompt]: Epoch 11 / 100: avg data time: 3.85e-01, avg batch time: 1.2229, average train loss: 0.7997
[12/05 08:58:45 visual_prompt]: Inference (val):avg data time: 9.16e-05, avg batch time: 0.3116, average loss: 0.7660
[12/05 08:58:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.82	
[12/05 08:58:45 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[12/05 09:00:51 visual_prompt]: 	Training 100/553. train loss: 0.8860,	0.9121 s / batch. (data: 7.37e-02). ETA=12:26:38, max mem: 20.9 GB 
[12/05 09:02:56 visual_prompt]: 	Training 200/553. train loss: 0.6502,	0.8307 s / batch. (data: 3.13e-04). ETA=11:18:39, max mem: 20.9 GB 
[12/05 09:04:52 visual_prompt]: 	Training 300/553. train loss: 0.6491,	0.8226 s / batch. (data: 7.22e-04). ETA=11:10:37, max mem: 20.9 GB 
[12/05 09:06:59 visual_prompt]: 	Training 400/553. train loss: 0.7330,	0.8640 s / batch. (data: 1.33e-03). ETA=11:42:58, max mem: 20.9 GB 
[12/05 09:09:04 visual_prompt]: 	Training 500/553. train loss: 1.7519,	0.8573 s / batch. (data: 1.51e-03). ETA=11:36:04, max mem: 20.9 GB 
[12/05 09:10:07 visual_prompt]: Epoch 12 / 100: avg data time: 3.94e-01, avg batch time: 1.2316, average train loss: 0.7785
[12/05 09:11:16 visual_prompt]: Inference (val):avg data time: 8.07e-05, avg batch time: 0.3096, average loss: 1.0841
[12/05 09:11:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.42	
[12/05 09:11:16 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[12/05 09:13:21 visual_prompt]: 	Training 100/553. train loss: 0.7142,	0.8753 s / batch. (data: 1.16e-02). ETA=11:48:26, max mem: 20.9 GB 
[12/05 09:15:19 visual_prompt]: 	Training 200/553. train loss: 0.6996,	0.8503 s / batch. (data: 2.91e-04). ETA=11:26:49, max mem: 20.9 GB 
[12/05 09:17:16 visual_prompt]: 	Training 300/553. train loss: 0.6431,	2.2047 s / batch. (data: 1.38e+00). ETA=1 day, 5:37:07, max mem: 20.9 GB 
[12/05 09:19:18 visual_prompt]: 	Training 400/553. train loss: 1.4196,	0.8675 s / batch. (data: 1.58e-02). ETA=11:37:48, max mem: 20.9 GB 
[12/05 09:21:23 visual_prompt]: 	Training 500/553. train loss: 0.7342,	0.8466 s / batch. (data: 1.40e-03). ETA=11:19:34, max mem: 20.9 GB 
[12/05 09:22:26 visual_prompt]: Epoch 13 / 100: avg data time: 3.73e-01, avg batch time: 1.2101, average train loss: 0.8154
[12/05 09:23:39 visual_prompt]: Inference (val):avg data time: 1.02e-04, avg batch time: 0.3114, average loss: 0.6895
[12/05 09:23:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.09	
[12/05 09:23:39 visual_prompt]: Best epoch 13: best metric: -0.689
[12/05 09:23:39 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[12/05 09:25:39 visual_prompt]: 	Training 100/553. train loss: 0.6347,	0.8364 s / batch. (data: 3.49e-04). ETA=11:09:16, max mem: 20.9 GB 
[12/05 09:27:48 visual_prompt]: 	Training 200/553. train loss: 0.6719,	3.3879 s / batch. (data: 2.53e+00). ETA=1 day, 21:05:18, max mem: 20.9 GB 
[12/05 09:29:49 visual_prompt]: 	Training 300/553. train loss: 0.6946,	1.2097 s / batch. (data: 3.88e-01). ETA=16:03:58, max mem: 20.9 GB 
[12/05 09:31:47 visual_prompt]: 	Training 400/553. train loss: 0.6832,	0.8314 s / batch. (data: 3.53e-04). ETA=11:01:08, max mem: 20.9 GB 
[12/05 09:33:52 visual_prompt]: 	Training 500/553. train loss: 1.4012,	0.8478 s / batch. (data: 9.37e-04). ETA=11:12:42, max mem: 20.9 GB 
[12/05 09:34:55 visual_prompt]: Epoch 14 / 100: avg data time: 3.88e-01, avg batch time: 1.2235, average train loss: 0.7529
[12/05 09:36:12 visual_prompt]: Inference (val):avg data time: 5.64e-04, avg batch time: 0.3120, average loss: 0.7286
[12/05 09:36:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.50	
[12/05 09:36:12 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[12/05 09:38:22 visual_prompt]: 	Training 100/553. train loss: 0.6950,	0.8440 s / batch. (data: 1.06e-02). ETA=11:07:32, max mem: 20.9 GB 
[12/05 09:40:18 visual_prompt]: 	Training 200/553. train loss: 0.8049,	1.6413 s / batch. (data: 8.03e-01). ETA=21:35:29, max mem: 20.9 GB 
[12/05 09:42:24 visual_prompt]: 	Training 300/553. train loss: 0.5667,	0.8684 s / batch. (data: 5.05e-04). ETA=11:23:59, max mem: 20.9 GB 
[12/05 09:44:19 visual_prompt]: 	Training 400/553. train loss: 0.5837,	0.8355 s / batch. (data: 1.33e-03). ETA=10:56:39, max mem: 20.9 GB 
[12/05 09:46:25 visual_prompt]: 	Training 500/553. train loss: 1.0142,	0.8260 s / batch. (data: 3.34e-04). ETA=10:47:50, max mem: 20.9 GB 
[12/05 09:47:31 visual_prompt]: Epoch 15 / 100: avg data time: 3.91e-01, avg batch time: 1.2261, average train loss: 0.7890
[12/05 09:48:45 visual_prompt]: Inference (val):avg data time: 1.15e-04, avg batch time: 0.3138, average loss: 0.8145
[12/05 09:48:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.86	
[12/05 09:48:45 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[12/05 09:50:53 visual_prompt]: 	Training 100/553. train loss: 0.5642,	0.8595 s / batch. (data: 1.29e-03). ETA=11:11:53, max mem: 20.9 GB 
[12/05 09:52:55 visual_prompt]: 	Training 200/553. train loss: 1.4136,	0.8295 s / batch. (data: 1.56e-02). ETA=10:47:06, max mem: 20.9 GB 
[12/05 09:54:53 visual_prompt]: 	Training 300/553. train loss: 0.9059,	0.8515 s / batch. (data: 1.62e-02). ETA=11:02:47, max mem: 20.9 GB 
[12/05 09:56:57 visual_prompt]: 	Training 400/553. train loss: 0.7279,	0.8442 s / batch. (data: 1.37e-02). ETA=10:55:45, max mem: 20.9 GB 
[12/05 09:58:55 visual_prompt]: 	Training 500/553. train loss: 0.9672,	2.0979 s / batch. (data: 1.27e+00). ETA=1 day, 3:06:01, max mem: 20.9 GB 
[12/05 10:00:00 visual_prompt]: Epoch 16 / 100: avg data time: 3.86e-01, avg batch time: 1.2204, average train loss: 0.7948
[12/05 10:01:12 visual_prompt]: Inference (val):avg data time: 2.11e-04, avg batch time: 0.3119, average loss: 0.6926
[12/05 10:01:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.43	
[12/05 10:01:12 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[12/05 10:03:22 visual_prompt]: 	Training 100/553. train loss: 0.5978,	0.8425 s / batch. (data: 9.91e-04). ETA=10:50:51, max mem: 20.9 GB 
[12/05 10:05:25 visual_prompt]: 	Training 200/553. train loss: 0.9430,	0.8944 s / batch. (data: 2.94e-04). ETA=11:29:27, max mem: 20.9 GB 
[12/05 10:07:27 visual_prompt]: 	Training 300/553. train loss: 1.3583,	0.8349 s / batch. (data: 6.93e-03). ETA=10:42:10, max mem: 20.9 GB 
[12/05 10:09:23 visual_prompt]: 	Training 400/553. train loss: 0.8717,	0.8382 s / batch. (data: 4.06e-04). ETA=10:43:22, max mem: 20.9 GB 
[12/05 10:11:23 visual_prompt]: 	Training 500/553. train loss: 0.5718,	2.4637 s / batch. (data: 1.63e+00). ETA=1 day, 7:26:51, max mem: 20.9 GB 
[12/05 10:12:28 visual_prompt]: Epoch 17 / 100: avg data time: 3.87e-01, avg batch time: 1.2221, average train loss: 0.7683
[12/05 10:13:40 visual_prompt]: Inference (val):avg data time: 3.08e-04, avg batch time: 0.3113, average loss: 0.6881
[12/05 10:13:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.96	
[12/05 10:13:40 visual_prompt]: Best epoch 17: best metric: -0.688
[12/05 10:13:40 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[12/05 10:15:40 visual_prompt]: 	Training 100/553. train loss: 0.7365,	0.8390 s / batch. (data: 2.06e-03). ETA=10:40:26, max mem: 20.9 GB 
[12/05 10:17:45 visual_prompt]: 	Training 200/553. train loss: 0.8259,	0.8537 s / batch. (data: 1.17e-02). ETA=10:50:11, max mem: 20.9 GB 
[12/05 10:19:46 visual_prompt]: 	Training 300/553. train loss: 0.5632,	0.8481 s / batch. (data: 5.46e-03). ETA=10:44:30, max mem: 20.9 GB 
[12/05 10:21:47 visual_prompt]: 	Training 400/553. train loss: 0.7363,	0.8555 s / batch. (data: 1.23e-03). ETA=10:48:43, max mem: 20.9 GB 
[12/05 10:23:46 visual_prompt]: 	Training 500/553. train loss: 0.6900,	1.0716 s / batch. (data: 2.39e-01). ETA=13:30:49, max mem: 20.9 GB 
[12/05 10:24:45 visual_prompt]: Epoch 18 / 100: avg data time: 3.65e-01, avg batch time: 1.2019, average train loss: 0.8341
[12/05 10:25:51 visual_prompt]: Inference (val):avg data time: 1.82e-04, avg batch time: 0.3109, average loss: 0.7493
[12/05 10:25:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.58	
[12/05 10:25:51 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[12/05 10:28:02 visual_prompt]: 	Training 100/553. train loss: 0.9425,	2.0306 s / batch. (data: 1.21e+00). ETA=1 day, 1:31:14, max mem: 20.9 GB 
[12/05 10:30:02 visual_prompt]: 	Training 200/553. train loss: 0.7896,	0.8141 s / batch. (data: 3.83e-04). ETA=10:12:34, max mem: 20.9 GB 
[12/05 10:32:04 visual_prompt]: 	Training 300/553. train loss: 1.1253,	0.8644 s / batch. (data: 7.93e-03). ETA=10:48:58, max mem: 20.9 GB 
[12/05 10:34:05 visual_prompt]: 	Training 400/553. train loss: 0.5754,	0.8231 s / batch. (data: 3.46e-04). ETA=10:16:36, max mem: 20.9 GB 
[12/05 10:36:02 visual_prompt]: 	Training 500/553. train loss: 1.1973,	0.8196 s / batch. (data: 1.30e-03). ETA=10:12:34, max mem: 20.9 GB 
[12/05 10:37:05 visual_prompt]: Epoch 19 / 100: avg data time: 3.83e-01, avg batch time: 1.2182, average train loss: 0.7942
[12/05 10:38:16 visual_prompt]: Inference (val):avg data time: 1.02e-04, avg batch time: 0.3127, average loss: 1.0754
[12/05 10:38:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.80	
[12/05 10:38:16 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[12/05 10:40:19 visual_prompt]: 	Training 100/553. train loss: 1.3739,	0.8261 s / batch. (data: 7.94e-03). ETA=10:15:20, max mem: 20.9 GB 
[12/05 10:42:14 visual_prompt]: 	Training 200/553. train loss: 0.5721,	1.6233 s / batch. (data: 7.94e-01). ETA=20:06:27, max mem: 20.9 GB 
[12/05 10:44:15 visual_prompt]: 	Training 300/553. train loss: 0.8490,	0.8525 s / batch. (data: 1.19e-02). ETA=10:32:10, max mem: 20.9 GB 
[12/05 10:46:13 visual_prompt]: 	Training 400/553. train loss: 0.6387,	0.8320 s / batch. (data: 3.58e-04). ETA=10:15:34, max mem: 20.9 GB 
[12/05 10:48:12 visual_prompt]: 	Training 500/553. train loss: 0.9564,	0.8321 s / batch. (data: 1.19e-03). ETA=10:14:14, max mem: 20.9 GB 
[12/05 10:49:19 visual_prompt]: Epoch 20 / 100: avg data time: 3.63e-01, avg batch time: 1.1979, average train loss: 0.8001
[12/05 10:50:32 visual_prompt]: Inference (val):avg data time: 1.56e-04, avg batch time: 0.3123, average loss: 0.7979
[12/05 10:50:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.60	
[12/05 10:50:32 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[12/05 10:52:44 visual_prompt]: 	Training 100/553. train loss: 0.5625,	0.8285 s / batch. (data: 1.17e-03). ETA=10:09:29, max mem: 20.9 GB 
[12/05 10:54:46 visual_prompt]: 	Training 200/553. train loss: 0.6707,	0.8323 s / batch. (data: 3.30e-04). ETA=10:10:53, max mem: 20.9 GB 
[12/05 10:56:59 visual_prompt]: 	Training 300/553. train loss: 1.3704,	1.1424 s / batch. (data: 3.13e-01). ETA=13:56:37, max mem: 20.9 GB 
[12/05 10:59:03 visual_prompt]: 	Training 400/553. train loss: 0.8066,	0.9085 s / batch. (data: 1.28e-02). ETA=11:03:47, max mem: 20.9 GB 
[12/05 11:01:02 visual_prompt]: 	Training 500/553. train loss: 0.6989,	0.8616 s / batch. (data: 7.95e-03). ETA=10:28:04, max mem: 20.9 GB 
[12/05 11:02:01 visual_prompt]: Epoch 21 / 100: avg data time: 4.11e-01, avg batch time: 1.2458, average train loss: 0.7599
[12/05 11:03:14 visual_prompt]: Inference (val):avg data time: 9.89e-05, avg batch time: 0.3118, average loss: 0.7515
[12/05 11:03:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[12/05 11:03:14 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[12/05 11:05:20 visual_prompt]: 	Training 100/553. train loss: 0.6949,	0.8265 s / batch. (data: 1.22e-03). ETA=10:00:25, max mem: 20.9 GB 
[12/05 11:07:21 visual_prompt]: 	Training 200/553. train loss: 0.6059,	0.8715 s / batch. (data: 7.72e-03). ETA=10:31:37, max mem: 20.9 GB 
[12/05 11:09:21 visual_prompt]: 	Training 300/553. train loss: 0.5261,	0.8434 s / batch. (data: 1.49e-03). ETA=10:09:54, max mem: 20.9 GB 
[12/05 11:11:19 visual_prompt]: 	Training 400/553. train loss: 0.8621,	0.8356 s / batch. (data: 1.05e-02). ETA=10:02:48, max mem: 20.9 GB 
[12/05 11:13:16 visual_prompt]: 	Training 500/553. train loss: 0.7520,	0.8355 s / batch. (data: 5.47e-03). ETA=10:01:24, max mem: 20.9 GB 
[12/05 11:14:21 visual_prompt]: Epoch 22 / 100: avg data time: 3.71e-01, avg batch time: 1.2060, average train loss: 0.7927
[12/05 11:15:33 visual_prompt]: Inference (val):avg data time: 1.12e-04, avg batch time: 0.3107, average loss: 0.7243
[12/05 11:15:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.10	
[12/05 11:15:33 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[12/05 11:17:34 visual_prompt]: 	Training 100/553. train loss: 0.8870,	0.8511 s / batch. (data: 1.22e-02). ETA=10:10:26, max mem: 20.9 GB 
[12/05 11:19:37 visual_prompt]: 	Training 200/553. train loss: 0.8144,	1.5345 s / batch. (data: 6.99e-01). ETA=18:18:03, max mem: 20.9 GB 
[12/05 11:21:40 visual_prompt]: 	Training 300/553. train loss: 0.7957,	0.8232 s / batch. (data: 8.50e-04). ETA=9:47:42, max mem: 20.9 GB 
[12/05 11:23:40 visual_prompt]: 	Training 400/553. train loss: 0.5669,	0.8511 s / batch. (data: 5.03e-04). ETA=10:06:09, max mem: 20.9 GB 
[12/05 11:25:38 visual_prompt]: 	Training 500/553. train loss: 1.5836,	0.8858 s / batch. (data: 3.80e-02). ETA=10:29:24, max mem: 20.9 GB 
[12/05 11:26:36 visual_prompt]: Epoch 23 / 100: avg data time: 3.62e-01, avg batch time: 1.1981, average train loss: 0.8282
[12/05 11:27:48 visual_prompt]: Inference (val):avg data time: 2.38e-04, avg batch time: 0.3130, average loss: 0.6939
[12/05 11:27:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.29	
[12/05 11:27:48 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[12/05 11:29:51 visual_prompt]: 	Training 100/553. train loss: 0.8737,	0.8562 s / batch. (data: 1.45e-03). ETA=10:06:14, max mem: 20.9 GB 
[12/05 11:31:45 visual_prompt]: 	Training 200/553. train loss: 0.5736,	0.9006 s / batch. (data: 2.92e-02). ETA=10:36:07, max mem: 20.9 GB 
[12/05 11:33:48 visual_prompt]: 	Training 300/553. train loss: 0.6920,	1.7181 s / batch. (data: 8.75e-01). ETA=20:10:44, max mem: 20.9 GB 
[12/05 11:35:49 visual_prompt]: 	Training 400/553. train loss: 0.5643,	0.8541 s / batch. (data: 6.81e-03). ETA=10:00:26, max mem: 20.9 GB 
[12/05 11:37:54 visual_prompt]: 	Training 500/553. train loss: 0.7896,	0.8734 s / batch. (data: 3.11e-02). ETA=10:12:33, max mem: 20.9 GB 
[12/05 11:38:59 visual_prompt]: Epoch 24 / 100: avg data time: 3.76e-01, avg batch time: 1.2133, average train loss: 0.7836
[12/05 11:40:10 visual_prompt]: Inference (val):avg data time: 1.00e-04, avg batch time: 0.3111, average loss: 0.6891
[12/05 11:40:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.14	
[12/05 11:40:10 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[12/05 11:42:15 visual_prompt]: 	Training 100/553. train loss: 0.7019,	0.9697 s / batch. (data: 1.05e-02). ETA=11:17:38, max mem: 20.9 GB 
[12/05 11:44:13 visual_prompt]: 	Training 200/553. train loss: 1.3208,	0.8431 s / batch. (data: 3.34e-04). ETA=9:47:44, max mem: 20.9 GB 
[12/05 11:46:10 visual_prompt]: 	Training 300/553. train loss: 0.7192,	1.6880 s / batch. (data: 8.74e-01). ETA=19:33:56, max mem: 20.9 GB 
[12/05 11:48:12 visual_prompt]: 	Training 400/553. train loss: 0.7482,	1.7921 s / batch. (data: 9.75e-01). ETA=20:43:21, max mem: 20.9 GB 
[12/05 11:50:16 visual_prompt]: 	Training 500/553. train loss: 0.7579,	2.6479 s / batch. (data: 1.81e+00). ETA=1 day, 6:32:43, max mem: 20.9 GB 
[12/05 11:51:19 visual_prompt]: Epoch 25 / 100: avg data time: 3.72e-01, avg batch time: 1.2085, average train loss: 0.8124
[12/05 11:52:32 visual_prompt]: Inference (val):avg data time: 4.54e-04, avg batch time: 0.3123, average loss: 0.9798
[12/05 11:52:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.11	
[12/05 11:52:32 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[12/05 11:54:39 visual_prompt]: 	Training 100/553. train loss: 0.5660,	0.8419 s / batch. (data: 1.19e-02). ETA=9:40:33, max mem: 20.9 GB 
[12/05 11:56:37 visual_prompt]: 	Training 200/553. train loss: 1.3002,	2.7513 s / batch. (data: 1.91e+00). ETA=1 day, 7:32:39, max mem: 20.9 GB 
[12/05 11:58:39 visual_prompt]: 	Training 300/553. train loss: 1.1651,	0.8619 s / batch. (data: 1.19e-02). ETA=9:51:28, max mem: 20.9 GB 
[12/05 12:00:34 visual_prompt]: 	Training 400/553. train loss: 0.5750,	0.8533 s / batch. (data: 1.56e-02). ETA=9:44:07, max mem: 20.9 GB 
[12/05 12:02:31 visual_prompt]: 	Training 500/553. train loss: 0.6552,	0.8360 s / batch. (data: 3.26e-04). ETA=9:30:55, max mem: 20.9 GB 
[12/05 12:03:32 visual_prompt]: Epoch 26 / 100: avg data time: 3.58e-01, avg batch time: 1.1933, average train loss: 0.8219
[12/05 12:04:43 visual_prompt]: Inference (val):avg data time: 9.19e-05, avg batch time: 0.3102, average loss: 0.7512
[12/05 12:04:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.48	
[12/05 12:04:43 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[12/05 12:06:51 visual_prompt]: 	Training 100/553. train loss: 0.5864,	0.8619 s / batch. (data: 1.25e-02). ETA=9:46:25, max mem: 20.9 GB 
[12/05 12:08:51 visual_prompt]: 	Training 200/553. train loss: 1.0030,	2.4195 s / batch. (data: 1.59e+00). ETA=1 day, 3:22:06, max mem: 20.9 GB 
[12/05 12:10:49 visual_prompt]: 	Training 300/553. train loss: 0.5676,	0.9880 s / batch. (data: 1.47e-01). ETA=11:08:54, max mem: 20.9 GB 
[12/05 12:12:45 visual_prompt]: 	Training 400/553. train loss: 1.0066,	0.8349 s / batch. (data: 1.11e-03). ETA=9:23:50, max mem: 20.9 GB 
[12/05 12:14:48 visual_prompt]: 	Training 500/553. train loss: 0.5624,	0.8705 s / batch. (data: 8.20e-04). ETA=9:46:29, max mem: 20.9 GB 
[12/05 12:15:48 visual_prompt]: Epoch 27 / 100: avg data time: 3.65e-01, avg batch time: 1.2021, average train loss: 0.7858
[12/05 12:16:52 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.3101, average loss: 0.7960
[12/05 12:16:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.18	
[12/05 12:16:52 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[12/05 12:18:58 visual_prompt]: 	Training 100/553. train loss: 0.4251,	0.8675 s / batch. (data: 1.37e-03). ETA=9:42:13, max mem: 20.9 GB 
[12/05 12:20:59 visual_prompt]: 	Training 200/553. train loss: 1.0005,	0.8234 s / batch. (data: 6.24e-04). ETA=9:11:14, max mem: 20.9 GB 
[12/05 12:22:58 visual_prompt]: 	Training 300/553. train loss: 0.8403,	1.5656 s / batch. (data: 7.35e-01). ETA=17:25:30, max mem: 20.9 GB 
[12/05 12:24:57 visual_prompt]: 	Training 400/553. train loss: 0.7023,	0.8419 s / batch. (data: 1.24e-03). ETA=9:20:51, max mem: 20.9 GB 
[12/05 12:26:53 visual_prompt]: 	Training 500/553. train loss: 2.3990,	0.8166 s / batch. (data: 3.06e-04). ETA=9:02:38, max mem: 20.9 GB 
[12/05 12:27:52 visual_prompt]: Epoch 28 / 100: avg data time: 3.58e-01, avg batch time: 1.1927, average train loss: 0.8237
[12/05 12:29:04 visual_prompt]: Inference (val):avg data time: 8.86e-05, avg batch time: 0.3111, average loss: 0.8203
[12/05 12:29:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.88	
[12/05 12:29:04 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[12/05 12:31:17 visual_prompt]: 	Training 100/553. train loss: 0.4185,	0.8420 s / batch. (data: 2.07e-03). ETA=9:17:22, max mem: 20.9 GB 
[12/05 12:33:09 visual_prompt]: 	Training 200/553. train loss: 0.7508,	2.1760 s / batch. (data: 1.34e+00). ETA=23:56:44, max mem: 20.9 GB 
[12/05 12:35:06 visual_prompt]: 	Training 300/553. train loss: 0.6981,	0.8692 s / batch. (data: 1.19e-02). ETA=9:32:25, max mem: 20.9 GB 
[12/05 12:37:03 visual_prompt]: 	Training 400/553. train loss: 0.5628,	0.8163 s / batch. (data: 3.52e-04). ETA=8:56:15, max mem: 20.9 GB 
[12/05 12:39:06 visual_prompt]: 	Training 500/553. train loss: 0.7372,	0.8405 s / batch. (data: 3.40e-04). ETA=9:10:44, max mem: 20.9 GB 
[12/05 12:40:10 visual_prompt]: Epoch 29 / 100: avg data time: 3.68e-01, avg batch time: 1.2042, average train loss: 0.8124
[12/05 12:41:23 visual_prompt]: Inference (val):avg data time: 9.08e-05, avg batch time: 0.3093, average loss: 0.7442
[12/05 12:41:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.34	
[12/05 12:41:23 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[12/05 12:43:24 visual_prompt]: 	Training 100/553. train loss: 0.8218,	0.8351 s / batch. (data: 3.98e-04). ETA=9:05:05, max mem: 20.9 GB 
[12/05 12:45:26 visual_prompt]: 	Training 200/553. train loss: 0.9963,	0.8569 s / batch. (data: 1.38e-03). ETA=9:17:53, max mem: 20.9 GB 
[12/05 12:47:28 visual_prompt]: 	Training 300/553. train loss: 0.3232,	1.9765 s / batch. (data: 1.13e+00). ETA=21:23:31, max mem: 20.9 GB 
[12/05 12:49:28 visual_prompt]: 	Training 400/553. train loss: 0.7676,	1.9080 s / batch. (data: 1.07e+00). ETA=20:35:49, max mem: 20.9 GB 
[12/05 12:51:33 visual_prompt]: 	Training 500/553. train loss: 0.5761,	2.7627 s / batch. (data: 1.95e+00). ETA=1 day, 5:44:48, max mem: 20.9 GB 
[12/05 12:52:37 visual_prompt]: Epoch 30 / 100: avg data time: 3.83e-01, avg batch time: 1.2193, average train loss: 0.7673
[12/05 12:53:50 visual_prompt]: Inference (val):avg data time: 4.84e-04, avg batch time: 0.3106, average loss: 0.7192
[12/05 12:53:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.72	
[12/05 12:53:50 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[12/05 12:56:01 visual_prompt]: 	Training 100/553. train loss: 0.5736,	0.8727 s / batch. (data: 1.21e-02). ETA=9:21:33, max mem: 20.9 GB 
[12/05 12:57:58 visual_prompt]: 	Training 200/553. train loss: 0.7510,	0.8718 s / batch. (data: 2.54e-02). ETA=9:19:33, max mem: 20.9 GB 
[12/05 12:59:58 visual_prompt]: 	Training 300/553. train loss: 0.7081,	0.8400 s / batch. (data: 3.28e-04). ETA=8:57:42, max mem: 20.9 GB 
[12/05 13:01:54 visual_prompt]: 	Training 400/553. train loss: 0.6347,	0.8299 s / batch. (data: 3.60e-04). ETA=8:49:54, max mem: 20.9 GB 
[12/05 13:03:56 visual_prompt]: 	Training 500/553. train loss: 0.5885,	0.8271 s / batch. (data: 9.22e-03). ETA=8:46:42, max mem: 20.9 GB 
[12/05 13:04:58 visual_prompt]: Epoch 31 / 100: avg data time: 3.71e-01, avg batch time: 1.2070, average train loss: 0.7660
[12/05 13:06:10 visual_prompt]: Inference (val):avg data time: 1.07e-04, avg batch time: 0.3124, average loss: 0.6943
[12/05 13:06:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.60	
[12/05 13:06:10 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[12/05 13:08:19 visual_prompt]: 	Training 100/553. train loss: 0.7777,	0.8458 s / batch. (data: 1.40e-03). ETA=8:56:29, max mem: 20.9 GB 
[12/05 13:10:22 visual_prompt]: 	Training 200/553. train loss: 0.5652,	0.8400 s / batch. (data: 9.45e-04). ETA=8:51:23, max mem: 20.9 GB 
[12/05 13:12:21 visual_prompt]: 	Training 300/553. train loss: 0.7572,	0.8257 s / batch. (data: 3.10e-04). ETA=8:40:59, max mem: 20.9 GB 
[12/05 13:14:24 visual_prompt]: 	Training 400/553. train loss: 0.7236,	0.8659 s / batch. (data: 1.05e-02). ETA=9:04:52, max mem: 20.9 GB 
[12/05 13:16:17 visual_prompt]: 	Training 500/553. train loss: 0.7424,	0.8399 s / batch. (data: 1.31e-03). ETA=8:47:08, max mem: 20.9 GB 
[12/05 13:17:19 visual_prompt]: Epoch 32 / 100: avg data time: 3.73e-01, avg batch time: 1.2101, average train loss: 0.7456
[12/05 13:18:33 visual_prompt]: Inference (val):avg data time: 1.08e-04, avg batch time: 0.3114, average loss: 0.6988
[12/05 13:18:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.93	
[12/05 13:18:33 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[12/05 13:20:40 visual_prompt]: 	Training 100/553. train loss: 1.5697,	0.8425 s / batch. (data: 1.56e-02). ETA=8:46:38, max mem: 20.9 GB 
[12/05 13:22:46 visual_prompt]: 	Training 200/553. train loss: 0.8347,	2.9360 s / batch. (data: 2.10e+00). ETA=1 day, 6:30:17, max mem: 20.9 GB 
[12/05 13:24:42 visual_prompt]: 	Training 300/553. train loss: 0.7470,	0.8386 s / batch. (data: 1.05e-02). ETA=8:41:24, max mem: 20.9 GB 
[12/05 13:26:44 visual_prompt]: 	Training 400/553. train loss: 0.7153,	0.8455 s / batch. (data: 1.86e-02). ETA=8:44:15, max mem: 20.9 GB 
[12/05 13:28:47 visual_prompt]: 	Training 500/553. train loss: 1.0168,	2.1023 s / batch. (data: 1.29e+00). ETA=21:40:05, max mem: 20.9 GB 
[12/05 13:29:45 visual_prompt]: Epoch 33 / 100: avg data time: 3.79e-01, avg batch time: 1.2153, average train loss: 0.7885
[12/05 13:30:58 visual_prompt]: Inference (val):avg data time: 1.10e-04, avg batch time: 0.3118, average loss: 0.7460
[12/05 13:30:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.29	
[12/05 13:30:58 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[12/05 13:33:10 visual_prompt]: 	Training 100/553. train loss: 0.7608,	1.5717 s / batch. (data: 7.52e-01). ETA=16:07:57, max mem: 20.9 GB 
[12/05 13:35:12 visual_prompt]: 	Training 200/553. train loss: 0.7069,	0.8576 s / batch. (data: 1.55e-02). ETA=8:46:43, max mem: 20.9 GB 
[12/05 13:37:16 visual_prompt]: 	Training 300/553. train loss: 0.8964,	1.2038 s / batch. (data: 3.44e-01). ETA=12:17:19, max mem: 20.9 GB 
[12/05 13:39:13 visual_prompt]: 	Training 400/553. train loss: 0.5518,	0.8205 s / batch. (data: 1.44e-03). ETA=8:21:13, max mem: 20.9 GB 
[12/05 13:41:16 visual_prompt]: 	Training 500/553. train loss: 0.6424,	1.2395 s / batch. (data: 3.95e-01). ETA=12:35:05, max mem: 20.9 GB 
[12/05 13:42:22 visual_prompt]: Epoch 34 / 100: avg data time: 3.98e-01, avg batch time: 1.2359, average train loss: 0.7997
[12/05 13:43:28 visual_prompt]: Inference (val):avg data time: 1.57e-04, avg batch time: 0.3109, average loss: 0.7222
[12/05 13:43:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.58	
[12/05 13:43:28 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[12/05 13:45:37 visual_prompt]: 	Training 100/553. train loss: 0.5412,	0.8702 s / batch. (data: 1.06e-02). ETA=8:47:54, max mem: 20.9 GB 
[12/05 13:47:45 visual_prompt]: 	Training 200/553. train loss: 0.6431,	1.3626 s / batch. (data: 5.24e-01). ETA=13:44:18, max mem: 20.9 GB 
[12/05 13:49:47 visual_prompt]: 	Training 300/553. train loss: 0.7594,	0.8256 s / batch. (data: 1.17e-03). ETA=8:18:05, max mem: 20.9 GB 
[12/05 13:51:48 visual_prompt]: 	Training 400/553. train loss: 0.6501,	0.8920 s / batch. (data: 7.11e-02). ETA=8:56:39, max mem: 20.9 GB 
[12/05 13:53:45 visual_prompt]: 	Training 500/553. train loss: 0.9243,	1.4559 s / batch. (data: 6.25e-01). ETA=14:33:28, max mem: 20.9 GB 
[12/05 13:54:51 visual_prompt]: Epoch 35 / 100: avg data time: 3.98e-01, avg batch time: 1.2342, average train loss: 0.7666
[12/05 13:56:04 visual_prompt]: Inference (val):avg data time: 3.05e-04, avg batch time: 0.3113, average loss: 0.7028
[12/05 13:56:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.96	
[12/05 13:56:04 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[12/05 13:58:05 visual_prompt]: 	Training 100/553. train loss: 1.4970,	1.5261 s / batch. (data: 6.74e-01). ETA=15:11:42, max mem: 20.9 GB 
[12/05 14:00:09 visual_prompt]: 	Training 200/553. train loss: 0.8278,	0.8355 s / batch. (data: 1.30e-03). ETA=8:17:45, max mem: 20.9 GB 
[12/05 14:02:16 visual_prompt]: 	Training 300/553. train loss: 0.5461,	0.8294 s / batch. (data: 1.56e-02). ETA=8:12:44, max mem: 20.9 GB 
[12/05 14:04:19 visual_prompt]: 	Training 400/553. train loss: 1.0159,	1.8285 s / batch. (data: 1.01e+00). ETA=18:03:12, max mem: 20.9 GB 
[12/05 14:06:19 visual_prompt]: 	Training 500/553. train loss: 1.0994,	0.8150 s / batch. (data: 3.81e-04). ETA=8:01:28, max mem: 20.9 GB 
[12/05 14:07:18 visual_prompt]: Epoch 36 / 100: avg data time: 3.81e-01, avg batch time: 1.2170, average train loss: 0.8029
[12/05 14:08:33 visual_prompt]: Inference (val):avg data time: 1.09e-04, avg batch time: 0.3103, average loss: 0.6978
[12/05 14:08:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.68	
[12/05 14:08:33 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[12/05 14:10:40 visual_prompt]: 	Training 100/553. train loss: 1.1279,	0.8226 s / batch. (data: 5.31e-04). ETA=8:03:49, max mem: 20.9 GB 
[12/05 14:12:43 visual_prompt]: 	Training 200/553. train loss: 1.0006,	0.8563 s / batch. (data: 1.49e-02). ETA=8:22:16, max mem: 20.9 GB 
[12/05 14:14:56 visual_prompt]: 	Training 300/553. train loss: 0.8206,	6.1225 s / batch. (data: 5.26e+00). ETA=2 days, 11:40:52, max mem: 20.9 GB 
[12/05 14:17:06 visual_prompt]: 	Training 400/553. train loss: 0.6847,	3.2826 s / batch. (data: 2.47e+00). ETA=1 day, 7:54:23, max mem: 20.9 GB 
[12/05 14:19:11 visual_prompt]: 	Training 500/553. train loss: 0.8394,	5.5676 s / batch. (data: 4.71e+00). ETA=2 days, 5:57:45, max mem: 20.9 GB 
[12/05 14:20:11 visual_prompt]: Epoch 37 / 100: avg data time: 4.28e-01, avg batch time: 1.2618, average train loss: 0.7996
[12/05 14:21:24 visual_prompt]: Inference (val):avg data time: 9.11e-05, avg batch time: 0.3104, average loss: 0.7307
[12/05 14:21:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.70	
[12/05 14:21:24 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[12/05 14:23:40 visual_prompt]: 	Training 100/553. train loss: 0.6762,	1.7760 s / batch. (data: 9.42e-01). ETA=17:08:15, max mem: 20.9 GB 
[12/05 14:25:38 visual_prompt]: 	Training 200/553. train loss: 0.6929,	2.2661 s / batch. (data: 1.44e+00). ETA=21:48:15, max mem: 20.9 GB 
[12/05 14:27:46 visual_prompt]: 	Training 300/553. train loss: 0.5567,	0.8444 s / batch. (data: 1.19e-02). ETA=8:06:06, max mem: 20.9 GB 
[12/05 14:29:56 visual_prompt]: 	Training 400/553. train loss: 0.6900,	0.8607 s / batch. (data: 6.68e-03). ETA=8:14:02, max mem: 20.9 GB 
[12/05 14:32:05 visual_prompt]: 	Training 500/553. train loss: 0.9169,	0.8500 s / batch. (data: 1.16e-02). ETA=8:06:27, max mem: 20.9 GB 
[12/05 14:33:08 visual_prompt]: Epoch 38 / 100: avg data time: 4.37e-01, avg batch time: 1.2728, average train loss: 0.7744
[12/05 14:34:17 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.3099, average loss: 0.7817
[12/05 14:34:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.66	
[12/05 14:34:17 visual_prompt]: Stopping early.
[12/05 14:34:17 visual_prompt]: Rank of current process: 0. World size: 1
[12/05 14:34:17 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/05 14:34:17 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/05 14:34:17 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/05 14:34:17 visual_prompt]: Training with config:
[12/05 14:34:17 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.1_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/05 14:34:17 visual_prompt]: Loading training data...
[12/05 14:34:17 visual_prompt]: Constructing mammo-cbis dataset train...
[12/05 14:34:17 visual_prompt]: Loading validation data...
[12/05 14:34:17 visual_prompt]: Constructing mammo-cbis dataset val...
[12/05 14:34:17 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[12/05 14:34:21 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[12/05 14:34:21 visual_prompt]: tuned percent:0.525
[12/05 14:34:21 visual_prompt]: Device used for model: 0
[12/05 14:34:21 visual_prompt]: Setting up Evaluator...
[12/05 14:34:21 visual_prompt]: Setting up Trainer...
[12/05 14:34:21 visual_prompt]: 	Setting up the optimizer...
[12/05 14:34:21 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/05 14:36:35 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8633 s / batch. (data: 2.12e-02). ETA=13:14:16, max mem: 20.9 GB 
[12/05 14:38:37 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8614 s / batch. (data: 1.05e-02). ETA=13:11:02, max mem: 20.9 GB 
[12/05 14:40:43 visual_prompt]: 	Training 300/553. train loss: 1.3905,	3.5567 s / batch. (data: 2.73e+00). ETA=2 days, 6:20:21, max mem: 20.9 GB 
[12/05 14:42:45 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8517 s / batch. (data: 1.53e-03). ETA=12:59:20, max mem: 20.9 GB 
[12/05 14:44:50 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8421 s / batch. (data: 3.11e-04). ETA=12:49:08, max mem: 20.9 GB 
[12/05 14:45:54 visual_prompt]: Epoch 1 / 100: avg data time: 4.16e-01, avg batch time: 1.2526, average train loss: 1.5403
[12/05 14:47:07 visual_prompt]: Inference (val):avg data time: 1.01e-04, avg batch time: 0.3114, average loss: 1.5201
[12/05 14:47:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[12/05 14:47:07 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[12/05 14:49:07 visual_prompt]: 	Training 100/553. train loss: 0.6279,	0.8324 s / batch. (data: 8.70e-04). ETA=12:38:08, max mem: 20.9 GB 
[12/05 14:51:09 visual_prompt]: 	Training 200/553. train loss: 0.2577,	0.8322 s / batch. (data: 3.40e-04). ETA=12:36:34, max mem: 20.9 GB 
[12/05 14:53:08 visual_prompt]: 	Training 300/553. train loss: 0.7430,	1.6577 s / batch. (data: 8.16e-01). ETA=1 day, 1:04:18, max mem: 20.9 GB 
[12/05 14:55:09 visual_prompt]: 	Training 400/553. train loss: 0.9585,	0.8461 s / batch. (data: 6.14e-03). ETA=12:46:23, max mem: 20.9 GB 
[12/05 14:57:15 visual_prompt]: 	Training 500/553. train loss: 0.6860,	0.8623 s / batch. (data: 1.20e-02). ETA=12:59:34, max mem: 20.9 GB 
[12/05 14:58:18 visual_prompt]: Epoch 2 / 100: avg data time: 3.76e-01, avg batch time: 1.2123, average train loss: 0.7769
[12/05 14:59:31 visual_prompt]: Inference (val):avg data time: 1.19e-04, avg batch time: 0.3131, average loss: 0.7374
[12/05 14:59:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.00	
[12/05 14:59:31 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[12/05 15:01:38 visual_prompt]: 	Training 100/553. train loss: 0.7862,	0.8280 s / batch. (data: 3.35e-04). ETA=12:26:29, max mem: 20.9 GB 
[12/05 15:03:39 visual_prompt]: 	Training 200/553. train loss: 0.7773,	0.8477 s / batch. (data: 1.05e-02). ETA=12:42:53, max mem: 20.9 GB 
[12/05 15:05:40 visual_prompt]: 	Training 300/553. train loss: 0.7054,	0.8318 s / batch. (data: 1.36e-02). ETA=12:27:06, max mem: 20.9 GB 
[12/05 15:07:37 visual_prompt]: 	Training 400/553. train loss: 0.5651,	0.8394 s / batch. (data: 1.25e-02). ETA=12:32:36, max mem: 20.9 GB 
[12/05 15:09:42 visual_prompt]: 	Training 500/553. train loss: 0.7718,	2.2280 s / batch. (data: 1.40e+00). ETA=1 day, 9:13:50, max mem: 20.9 GB 
[12/05 15:10:44 visual_prompt]: Epoch 3 / 100: avg data time: 3.79e-01, avg batch time: 1.2147, average train loss: 0.7468
[12/05 15:11:57 visual_prompt]: Inference (val):avg data time: 1.14e-04, avg batch time: 0.3116, average loss: 0.7184
[12/05 15:11:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.24	
[12/05 15:11:57 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[12/05 15:14:15 visual_prompt]: 	Training 100/553. train loss: 0.6853,	0.8656 s / batch. (data: 8.96e-04). ETA=12:52:22, max mem: 20.9 GB 
[12/05 15:16:13 visual_prompt]: 	Training 200/553. train loss: 0.8473,	0.8498 s / batch. (data: 1.24e-03). ETA=12:36:53, max mem: 20.9 GB 
[12/05 15:18:18 visual_prompt]: 	Training 300/553. train loss: 0.5468,	2.1062 s / batch. (data: 1.28e+00). ETA=1 day, 7:12:27, max mem: 20.9 GB 
[12/05 15:20:10 visual_prompt]: 	Training 400/553. train loss: 0.6213,	1.6400 s / batch. (data: 8.09e-01). ETA=1 day, 0:15:17, max mem: 20.9 GB 
[12/05 15:22:18 visual_prompt]: 	Training 500/553. train loss: 0.7584,	4.5403 s / batch. (data: 3.72e+00). ETA=2 days, 19:01:17, max mem: 20.9 GB 
[12/05 15:23:28 visual_prompt]: Epoch 4 / 100: avg data time: 4.11e-01, avg batch time: 1.2479, average train loss: 0.7848
[12/05 15:24:44 visual_prompt]: Inference (val):avg data time: 1.07e-04, avg batch time: 0.3125, average loss: 0.6919
[12/05 15:24:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.59	
[12/05 15:24:44 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[12/05 15:26:53 visual_prompt]: 	Training 100/553. train loss: 0.4628,	0.8451 s / batch. (data: 2.05e-02). ETA=12:26:19, max mem: 20.9 GB 
[12/05 15:28:51 visual_prompt]: 	Training 200/553. train loss: 0.8532,	2.1877 s / batch. (data: 1.34e+00). ETA=1 day, 8:08:24, max mem: 20.9 GB 
[12/05 15:31:01 visual_prompt]: 	Training 300/553. train loss: 0.9400,	0.8633 s / batch. (data: 6.71e-03). ETA=12:39:32, max mem: 20.9 GB 
[12/05 15:33:01 visual_prompt]: 	Training 400/553. train loss: 0.7033,	0.8148 s / batch. (data: 3.38e-04). ETA=11:55:28, max mem: 20.9 GB 
[12/05 15:35:05 visual_prompt]: 	Training 500/553. train loss: 0.6027,	0.8395 s / batch. (data: 1.33e-03). ETA=12:15:47, max mem: 20.9 GB 
[12/05 15:36:09 visual_prompt]: Epoch 5 / 100: avg data time: 4.04e-01, avg batch time: 1.2391, average train loss: 0.8043
[12/05 15:37:22 visual_prompt]: Inference (val):avg data time: 9.62e-05, avg batch time: 0.3130, average loss: 0.7580
[12/05 15:37:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.73	
[12/05 15:37:22 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[12/05 15:39:33 visual_prompt]: 	Training 100/553. train loss: 0.5946,	0.8473 s / batch. (data: 3.64e-03). ETA=12:20:25, max mem: 20.9 GB 
[12/05 15:41:34 visual_prompt]: 	Training 200/553. train loss: 0.6098,	0.8627 s / batch. (data: 1.07e-02). ETA=12:32:29, max mem: 20.9 GB 
[12/05 15:43:29 visual_prompt]: 	Training 300/553. train loss: 0.5594,	0.8405 s / batch. (data: 2.99e-04). ETA=12:11:43, max mem: 20.9 GB 
[12/05 15:45:34 visual_prompt]: 	Training 400/553. train loss: 0.5339,	0.8341 s / batch. (data: 3.90e-04). ETA=12:04:48, max mem: 20.9 GB 
[12/05 15:47:08 visual_prompt]: 	Training 500/553. train loss: 0.7692,	0.8426 s / batch. (data: 3.59e-04). ETA=12:10:43, max mem: 20.9 GB 
[12/05 15:48:06 visual_prompt]: Epoch 6 / 100: avg data time: 3.28e-01, avg batch time: 1.1643, average train loss: 0.7513
[12/05 15:49:55 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3058, average loss: 0.6778
[12/05 15:49:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 60.26	
[12/05 15:49:55 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[12/05 15:53:01 visual_prompt]: 	Training 100/553. train loss: 0.5749,	0.8079 s / batch. (data: 4.36e-04). ETA=11:38:35, max mem: 20.9 GB 
[12/05 15:56:54 visual_prompt]: 	Training 200/553. train loss: 0.5493,	3.5252 s / batch. (data: 2.70e+00). ETA=2 days, 2:42:21, max mem: 20.9 GB 
[12/05 15:59:35 visual_prompt]: 	Training 300/553. train loss: 0.7380,	0.9479 s / batch. (data: 1.29e-01). ETA=13:36:29, max mem: 20.9 GB 
[12/05 16:02:11 visual_prompt]: 	Training 400/553. train loss: 0.5772,	4.2653 s / batch. (data: 3.45e+00). ETA=2 days, 13:06:55, max mem: 20.9 GB 
[12/05 16:04:44 visual_prompt]: 	Training 500/553. train loss: 0.9787,	0.8239 s / batch. (data: 9.63e-04). ETA=11:46:58, max mem: 20.9 GB 
[12/05 16:06:35 visual_prompt]: Epoch 7 / 100: avg data time: 9.82e-01, avg batch time: 1.8086, average train loss: 0.7599
[12/05 16:07:50 visual_prompt]: Inference (val):avg data time: 1.03e-04, avg batch time: 0.3108, average loss: 0.8091
[12/05 16:07:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.96	
[12/05 16:07:50 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[12/05 16:10:00 visual_prompt]: 	Training 100/553. train loss: 0.6921,	0.8538 s / batch. (data: 5.48e-03). ETA=12:10:25, max mem: 20.9 GB 
[12/05 16:12:03 visual_prompt]: 	Training 200/553. train loss: 1.2244,	0.8545 s / batch. (data: 5.48e-03). ETA=12:09:35, max mem: 20.9 GB 
[12/05 16:14:05 visual_prompt]: 	Training 300/553. train loss: 0.7496,	0.8432 s / batch. (data: 9.86e-04). ETA=11:58:33, max mem: 20.9 GB 
[12/05 16:16:08 visual_prompt]: 	Training 400/553. train loss: 0.7586,	1.4376 s / batch. (data: 5.92e-01). ETA=20:22:36, max mem: 20.9 GB 
[12/05 16:18:12 visual_prompt]: 	Training 500/553. train loss: 0.8660,	2.6957 s / batch. (data: 1.86e+00). ETA=1 day, 14:08:08, max mem: 20.9 GB 
[12/05 16:19:13 visual_prompt]: Epoch 8 / 100: avg data time: 3.97e-01, avg batch time: 1.2354, average train loss: 0.7670
[12/05 16:20:25 visual_prompt]: Inference (val):avg data time: 9.22e-05, avg batch time: 0.3116, average loss: 0.6925
[12/05 16:20:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.24	
[12/05 16:20:25 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[12/05 16:22:24 visual_prompt]: 	Training 100/553. train loss: 0.4150,	0.8449 s / batch. (data: 2.90e-02). ETA=11:54:58, max mem: 20.9 GB 
[12/05 16:24:19 visual_prompt]: 	Training 200/553. train loss: 0.6221,	0.8320 s / batch. (data: 3.13e-04). ETA=11:42:42, max mem: 20.9 GB 
[12/05 16:26:16 visual_prompt]: 	Training 300/553. train loss: 0.7428,	2.0440 s / batch. (data: 1.21e+00). ETA=1 day, 4:42:57, max mem: 20.9 GB 
[12/05 16:28:12 visual_prompt]: 	Training 400/553. train loss: 0.5888,	0.8572 s / batch. (data: 3.36e-02). ETA=12:01:05, max mem: 20.9 GB 
[12/05 16:30:09 visual_prompt]: 	Training 500/553. train loss: 0.7645,	0.8320 s / batch. (data: 3.52e-04). ETA=11:38:32, max mem: 20.9 GB 
[12/05 16:31:07 visual_prompt]: Epoch 9 / 100: avg data time: 3.27e-01, avg batch time: 1.1609, average train loss: 0.7426
[12/05 16:32:16 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3123, average loss: 0.7558
[12/05 16:32:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.53	
[12/05 16:32:16 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[12/05 16:34:22 visual_prompt]: 	Training 100/553. train loss: 0.6994,	0.8551 s / batch. (data: 5.47e-03). ETA=11:55:43, max mem: 20.9 GB 
[12/05 16:36:15 visual_prompt]: 	Training 200/553. train loss: 0.6694,	0.8558 s / batch. (data: 6.85e-04). ETA=11:54:56, max mem: 20.9 GB 
[12/05 16:38:14 visual_prompt]: 	Training 300/553. train loss: 0.6549,	3.1265 s / batch. (data: 2.30e+00). ETA=1 day, 19:26:37, max mem: 20.9 GB 
[12/05 16:40:07 visual_prompt]: 	Training 400/553. train loss: 0.7412,	1.1252 s / batch. (data: 3.04e-01). ETA=15:36:11, max mem: 20.9 GB 
[12/05 16:42:06 visual_prompt]: 	Training 500/553. train loss: 1.0304,	1.5602 s / batch. (data: 7.46e-01). ETA=21:35:35, max mem: 20.9 GB 
[12/05 16:43:06 visual_prompt]: Epoch 10 / 100: avg data time: 3.40e-01, avg batch time: 1.1743, average train loss: 0.7587
[12/05 16:44:15 visual_prompt]: Inference (val):avg data time: 7.65e-05, avg batch time: 0.3116, average loss: 0.9164
[12/05 16:44:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.02	
[12/05 16:44:15 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[12/05 16:46:22 visual_prompt]: 	Training 100/553. train loss: 0.6765,	0.8644 s / batch. (data: 3.20e-04). ETA=11:55:32, max mem: 20.9 GB 
[12/05 16:48:22 visual_prompt]: 	Training 200/553. train loss: 1.0358,	0.8299 s / batch. (data: 4.30e-04). ETA=11:25:36, max mem: 20.9 GB 
[12/05 16:50:20 visual_prompt]: 	Training 300/553. train loss: 0.6732,	2.9922 s / batch. (data: 2.15e+00). ETA=1 day, 17:07:03, max mem: 20.9 GB 
[12/05 16:52:16 visual_prompt]: 	Training 400/553. train loss: 0.7097,	0.8414 s / batch. (data: 7.73e-04). ETA=11:32:18, max mem: 20.9 GB 
[12/05 16:54:13 visual_prompt]: 	Training 500/553. train loss: 0.6885,	0.8560 s / batch. (data: 1.19e-02). ETA=11:42:54, max mem: 20.9 GB 
[12/05 16:55:13 visual_prompt]: Epoch 11 / 100: avg data time: 3.53e-01, avg batch time: 1.1899, average train loss: 0.7534
[12/05 16:56:20 visual_prompt]: Inference (val):avg data time: 6.88e-05, avg batch time: 0.3098, average loss: 0.6948
[12/05 16:56:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.59	
[12/05 16:56:20 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[12/05 16:58:26 visual_prompt]: 	Training 100/553. train loss: 0.9692,	1.5802 s / batch. (data: 7.42e-01). ETA=21:33:34, max mem: 20.9 GB 
[12/05 17:00:25 visual_prompt]: 	Training 200/553. train loss: 0.5911,	0.8282 s / batch. (data: 3.86e-04). ETA=11:16:34, max mem: 20.9 GB 
[12/05 17:02:20 visual_prompt]: 	Training 300/553. train loss: 0.6782,	0.8333 s / batch. (data: 7.95e-03). ETA=11:19:22, max mem: 20.9 GB 
[12/05 17:04:15 visual_prompt]: 	Training 400/553. train loss: 0.7094,	0.8271 s / batch. (data: 3.57e-04). ETA=11:12:54, max mem: 20.9 GB 
[12/05 17:06:13 visual_prompt]: 	Training 500/553. train loss: 2.0742,	0.8238 s / batch. (data: 7.78e-03). ETA=11:08:54, max mem: 20.9 GB 
[12/05 17:07:12 visual_prompt]: Epoch 12 / 100: avg data time: 3.43e-01, avg batch time: 1.1785, average train loss: 0.7615
[12/05 17:08:19 visual_prompt]: Inference (val):avg data time: 5.26e-05, avg batch time: 0.3109, average loss: 0.8491
[12/05 17:08:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.20	
[12/05 17:08:19 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[12/05 17:10:23 visual_prompt]: 	Training 100/553. train loss: 0.7742,	0.8447 s / batch. (data: 3.29e-04). ETA=11:23:41, max mem: 20.9 GB 
[12/05 17:12:16 visual_prompt]: 	Training 200/553. train loss: 0.6752,	0.8152 s / batch. (data: 3.67e-04). ETA=10:58:26, max mem: 20.9 GB 
[12/05 17:14:19 visual_prompt]: 	Training 300/553. train loss: 0.6694,	2.3520 s / batch. (data: 1.51e+00). ETA=1 day, 7:35:51, max mem: 20.9 GB 
[12/05 17:16:13 visual_prompt]: 	Training 400/553. train loss: 0.9745,	0.8238 s / batch. (data: 3.38e-04). ETA=11:02:38, max mem: 20.9 GB 
[12/05 17:18:12 visual_prompt]: 	Training 500/553. train loss: 0.7368,	0.8255 s / batch. (data: 3.35e-04). ETA=11:02:38, max mem: 20.9 GB 
[12/05 17:19:15 visual_prompt]: Epoch 13 / 100: avg data time: 3.51e-01, avg batch time: 1.1844, average train loss: 0.7640
[12/05 17:20:39 visual_prompt]: Inference (val):avg data time: 6.97e-05, avg batch time: 0.3095, average loss: 0.7014
[12/05 17:20:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.98	
[12/05 17:20:39 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[12/05 17:22:53 visual_prompt]: 	Training 100/553. train loss: 0.6431,	0.8473 s / batch. (data: 1.06e-02). ETA=11:17:57, max mem: 20.9 GB 
[12/05 17:24:59 visual_prompt]: 	Training 200/553. train loss: 0.7708,	2.4636 s / batch. (data: 1.62e+00). ETA=1 day, 8:47:12, max mem: 20.9 GB 
[12/05 17:27:15 visual_prompt]: 	Training 300/553. train loss: 0.7119,	2.2797 s / batch. (data: 1.46e+00). ETA=1 day, 6:16:34, max mem: 20.9 GB 
[12/05 17:29:23 visual_prompt]: 	Training 400/553. train loss: 0.6560,	0.8409 s / batch. (data: 1.47e-02). ETA=11:08:38, max mem: 20.9 GB 
[12/05 17:31:30 visual_prompt]: 	Training 500/553. train loss: 0.9734,	0.8399 s / batch. (data: 5.31e-04). ETA=11:06:30, max mem: 20.9 GB 
[12/05 17:32:39 visual_prompt]: Epoch 14 / 100: avg data time: 4.72e-01, avg batch time: 1.3015, average train loss: 0.7485
[12/05 17:33:51 visual_prompt]: Inference (val):avg data time: 6.80e-05, avg batch time: 0.3095, average loss: 0.6943
[12/05 17:33:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.04	
[12/05 17:33:52 visual_prompt]: Best epoch 14: best metric: -0.694
[12/05 17:33:52 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[12/05 17:36:13 visual_prompt]: 	Training 100/553. train loss: 0.7109,	4.8572 s / batch. (data: 4.01e+00). ETA=2 days, 16:01:52, max mem: 20.9 GB 
[12/05 17:38:22 visual_prompt]: 	Training 200/553. train loss: 0.6378,	1.1279 s / batch. (data: 2.89e-01). ETA=14:50:13, max mem: 20.9 GB 
[12/05 17:40:35 visual_prompt]: 	Training 300/553. train loss: 0.7343,	0.8592 s / batch. (data: 1.61e-02). ETA=11:16:43, max mem: 20.9 GB 
[12/05 17:42:52 visual_prompt]: 	Training 400/553. train loss: 0.5733,	0.8472 s / batch. (data: 3.46e-04). ETA=11:05:50, max mem: 20.9 GB 
[12/05 17:45:02 visual_prompt]: 	Training 500/553. train loss: 0.9717,	0.8291 s / batch. (data: 3.41e-04). ETA=10:50:14, max mem: 20.9 GB 
[12/05 17:46:11 visual_prompt]: Epoch 15 / 100: avg data time: 5.07e-01, avg batch time: 1.3356, average train loss: 0.7359
[12/05 17:47:34 visual_prompt]: Inference (val):avg data time: 7.91e-05, avg batch time: 0.3094, average loss: 0.7089
[12/05 17:47:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.74	
[12/05 17:47:34 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[12/05 17:49:46 visual_prompt]: 	Training 100/553. train loss: 0.5607,	0.8119 s / batch. (data: 3.59e-04). ETA=10:34:42, max mem: 20.9 GB 
[12/05 17:51:48 visual_prompt]: 	Training 200/553. train loss: 0.8435,	0.8678 s / batch. (data: 1.55e-02). ETA=11:16:57, max mem: 20.9 GB 
[12/05 17:53:47 visual_prompt]: 	Training 300/553. train loss: 1.2283,	0.8446 s / batch. (data: 1.18e-03). ETA=10:57:28, max mem: 20.9 GB 
[12/05 17:55:46 visual_prompt]: 	Training 400/553. train loss: 0.6431,	0.8385 s / batch. (data: 1.06e-02). ETA=10:51:17, max mem: 20.9 GB 
[12/05 17:57:39 visual_prompt]: 	Training 500/553. train loss: 0.8496,	1.9982 s / batch. (data: 1.17e+00). ETA=1 day, 1:48:44, max mem: 20.9 GB 
[12/05 17:58:38 visual_prompt]: Epoch 16 / 100: avg data time: 3.71e-01, avg batch time: 1.2016, average train loss: 0.7228
[12/05 17:59:45 visual_prompt]: Inference (val):avg data time: 5.40e-05, avg batch time: 0.3092, average loss: 0.7196
[12/05 17:59:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.11	
[12/05 17:59:45 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[12/05 18:01:42 visual_prompt]: 	Training 100/553. train loss: 0.5634,	0.8333 s / batch. (data: 5.53e-03). ETA=10:43:44, max mem: 20.9 GB 
[12/05 18:03:37 visual_prompt]: 	Training 200/553. train loss: 0.6783,	0.8618 s / batch. (data: 3.38e-02). ETA=11:04:21, max mem: 20.9 GB 
[12/05 18:05:28 visual_prompt]: 	Training 300/553. train loss: 1.0435,	0.8213 s / batch. (data: 3.58e-04). ETA=10:31:45, max mem: 20.9 GB 
[12/05 18:07:20 visual_prompt]: 	Training 400/553. train loss: 0.6753,	0.8320 s / batch. (data: 1.15e-02). ETA=10:38:33, max mem: 20.9 GB 
[12/05 18:09:11 visual_prompt]: 	Training 500/553. train loss: 0.6440,	2.4400 s / batch. (data: 1.60e+00). ETA=1 day, 7:08:44, max mem: 20.9 GB 
[12/05 18:10:11 visual_prompt]: Epoch 17 / 100: avg data time: 2.99e-01, avg batch time: 1.1322, average train loss: 0.7228
[12/05 18:11:16 visual_prompt]: Inference (val):avg data time: 4.83e-05, avg batch time: 0.3113, average loss: 0.7145
[12/05 18:11:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.94	
[12/05 18:11:16 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[12/05 18:13:14 visual_prompt]: 	Training 100/553. train loss: 0.7472,	0.8448 s / batch. (data: 1.68e-02). ETA=10:44:49, max mem: 20.9 GB 
[12/05 18:15:10 visual_prompt]: 	Training 200/553. train loss: 0.7476,	0.8342 s / batch. (data: 1.06e-02). ETA=10:35:23, max mem: 20.9 GB 
[12/05 18:17:02 visual_prompt]: 	Training 300/553. train loss: 0.6563,	0.8246 s / batch. (data: 3.16e-04). ETA=10:26:38, max mem: 20.9 GB 
[12/05 18:18:54 visual_prompt]: 	Training 400/553. train loss: 0.6872,	0.8359 s / batch. (data: 5.32e-04). ETA=10:33:52, max mem: 20.9 GB 
[12/05 18:20:45 visual_prompt]: 	Training 500/553. train loss: 0.7037,	0.8560 s / batch. (data: 1.19e-02). ETA=10:47:40, max mem: 20.9 GB 
[12/05 18:21:42 visual_prompt]: Epoch 18 / 100: avg data time: 2.98e-01, avg batch time: 1.1308, average train loss: 0.7249
[12/05 18:22:48 visual_prompt]: Inference (val):avg data time: 5.79e-05, avg batch time: 0.3093, average loss: 0.7495
[12/05 18:22:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.08	
[12/05 18:22:48 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[12/05 18:24:46 visual_prompt]: 	Training 100/553. train loss: 1.0922,	0.8360 s / batch. (data: 3.56e-04). ETA=10:30:25, max mem: 20.9 GB 
[12/05 18:26:39 visual_prompt]: 	Training 200/553. train loss: 0.7904,	0.8186 s / batch. (data: 4.41e-04). ETA=10:15:56, max mem: 20.9 GB 
[12/05 18:28:31 visual_prompt]: 	Training 300/553. train loss: 1.0872,	1.4345 s / batch. (data: 6.05e-01). ETA=17:56:59, max mem: 20.9 GB 
[12/05 18:30:25 visual_prompt]: 	Training 400/553. train loss: 0.5629,	0.8304 s / batch. (data: 5.81e-04). ETA=10:22:00, max mem: 20.9 GB 
[12/05 18:32:14 visual_prompt]: 	Training 500/553. train loss: 0.7644,	0.8279 s / batch. (data: 6.29e-04). ETA=10:18:49, max mem: 20.9 GB 
[12/05 18:33:13 visual_prompt]: Epoch 19 / 100: avg data time: 2.97e-01, avg batch time: 1.1299, average train loss: 0.7397
[12/05 18:34:19 visual_prompt]: Inference (val):avg data time: 6.09e-05, avg batch time: 0.3125, average loss: 0.9610
[12/05 18:34:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.26	
[12/05 18:34:19 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[12/05 18:36:16 visual_prompt]: 	Training 100/553. train loss: 1.1063,	0.8470 s / batch. (data: 1.27e-02). ETA=10:30:56, max mem: 20.9 GB 
[12/05 18:38:11 visual_prompt]: 	Training 200/553. train loss: 0.5885,	0.8412 s / batch. (data: 3.43e-04). ETA=10:25:10, max mem: 20.9 GB 
[12/05 18:40:06 visual_prompt]: 	Training 300/553. train loss: 0.7721,	0.8205 s / batch. (data: 2.83e-04). ETA=10:08:26, max mem: 20.9 GB 
[12/05 18:41:59 visual_prompt]: 	Training 400/553. train loss: 0.5869,	0.8397 s / batch. (data: 3.09e-04). ETA=10:21:16, max mem: 20.9 GB 
[12/05 18:43:51 visual_prompt]: 	Training 500/553. train loss: 0.7745,	0.8211 s / batch. (data: 5.50e-04). ETA=10:06:10, max mem: 20.9 GB 
[12/05 18:44:52 visual_prompt]: Epoch 20 / 100: avg data time: 3.11e-01, avg batch time: 1.1434, average train loss: 0.7536
[12/05 18:45:59 visual_prompt]: Inference (val):avg data time: 2.73e-04, avg batch time: 0.3099, average loss: 0.7681
[12/05 18:45:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.59	
[12/05 18:45:59 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[12/05 18:48:01 visual_prompt]: 	Training 100/553. train loss: 0.5750,	0.8621 s / batch. (data: 1.39e-02). ETA=10:34:14, max mem: 20.9 GB 
[12/05 18:50:05 visual_prompt]: 	Training 200/553. train loss: 0.6979,	0.8380 s / batch. (data: 5.49e-03). ETA=10:15:05, max mem: 20.9 GB 
[12/05 18:52:01 visual_prompt]: 	Training 300/553. train loss: 0.8871,	1.2989 s / batch. (data: 4.68e-01). ETA=15:51:13, max mem: 20.9 GB 
[12/05 18:53:57 visual_prompt]: 	Training 400/553. train loss: 0.6125,	0.8348 s / batch. (data: 3.45e-04). ETA=10:09:59, max mem: 20.9 GB 
[12/05 18:55:59 visual_prompt]: 	Training 500/553. train loss: 0.7041,	0.8301 s / batch. (data: 3.17e-04). ETA=10:05:10, max mem: 20.9 GB 
[12/05 18:57:00 visual_prompt]: Epoch 21 / 100: avg data time: 3.64e-01, avg batch time: 1.1947, average train loss: 0.7327
[12/05 18:58:08 visual_prompt]: Inference (val):avg data time: 5.82e-05, avg batch time: 0.3106, average loss: 0.8394
[12/05 18:58:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.32	
[12/05 18:58:08 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[12/05 19:00:09 visual_prompt]: 	Training 100/553. train loss: 0.7008,	0.8357 s / batch. (data: 2.07e-02). ETA=10:07:07, max mem: 20.9 GB 
[12/05 19:02:08 visual_prompt]: 	Training 200/553. train loss: 0.6293,	0.8400 s / batch. (data: 3.56e-04). ETA=10:08:48, max mem: 20.9 GB 
[12/05 19:04:01 visual_prompt]: 	Training 300/553. train loss: 0.5109,	0.8440 s / batch. (data: 3.67e-04). ETA=10:10:18, max mem: 20.9 GB 
[12/05 19:05:59 visual_prompt]: 	Training 400/553. train loss: 0.6693,	0.8520 s / batch. (data: 1.19e-02). ETA=10:14:39, max mem: 20.9 GB 
[12/05 19:07:55 visual_prompt]: 	Training 500/553. train loss: 0.7878,	0.8280 s / batch. (data: 7.95e-03). ETA=9:55:58, max mem: 20.9 GB 
[12/05 19:08:58 visual_prompt]: Epoch 22 / 100: avg data time: 3.43e-01, avg batch time: 1.1750, average train loss: 0.7470
[12/05 19:10:10 visual_prompt]: Inference (val):avg data time: 6.53e-05, avg batch time: 0.3098, average loss: 0.8130
[12/05 19:10:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.08	
[12/05 19:10:10 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[12/05 19:12:29 visual_prompt]: 	Training 100/553. train loss: 0.6956,	0.8183 s / batch. (data: 3.59e-04). ETA=9:46:55, max mem: 20.9 GB 
[12/05 19:14:49 visual_prompt]: 	Training 200/553. train loss: 0.6218,	0.8401 s / batch. (data: 3.17e-04). ETA=10:01:10, max mem: 20.9 GB 
[12/05 19:16:59 visual_prompt]: 	Training 300/553. train loss: 0.7497,	0.8415 s / batch. (data: 2.90e-03). ETA=10:00:43, max mem: 20.9 GB 
[12/05 19:19:01 visual_prompt]: 	Training 400/553. train loss: 0.5635,	0.8320 s / batch. (data: 3.40e-04). ETA=9:52:34, max mem: 20.9 GB 
[12/05 19:21:05 visual_prompt]: 	Training 500/553. train loss: 0.9569,	0.8199 s / batch. (data: 9.39e-04). ETA=9:42:35, max mem: 20.9 GB 
[12/05 19:22:04 visual_prompt]: Epoch 23 / 100: avg data time: 4.63e-01, avg batch time: 1.2921, average train loss: 0.7392
[12/05 19:23:14 visual_prompt]: Inference (val):avg data time: 5.84e-05, avg batch time: 0.3108, average loss: 0.7030
[12/05 19:23:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.18	
[12/05 19:23:14 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[12/05 19:25:10 visual_prompt]: 	Training 100/553. train loss: 0.8127,	0.8410 s / batch. (data: 8.37e-03). ETA=9:55:26, max mem: 20.9 GB 
[12/05 19:27:03 visual_prompt]: 	Training 200/553. train loss: 0.6567,	0.8292 s / batch. (data: 6.49e-04). ETA=9:45:41, max mem: 20.9 GB 
[12/05 19:28:59 visual_prompt]: 	Training 300/553. train loss: 0.7255,	1.3936 s / batch. (data: 5.63e-01). ETA=16:22:03, max mem: 20.9 GB 
[12/05 19:30:53 visual_prompt]: 	Training 400/553. train loss: 0.6234,	0.8293 s / batch. (data: 4.78e-04). ETA=9:43:00, max mem: 20.9 GB 
[12/05 19:32:48 visual_prompt]: 	Training 500/553. train loss: 0.8428,	0.8463 s / batch. (data: 8.27e-04). ETA=9:53:31, max mem: 20.9 GB 
[12/05 19:33:48 visual_prompt]: Epoch 24 / 100: avg data time: 3.13e-01, avg batch time: 1.1463, average train loss: 0.7447
[12/05 19:34:55 visual_prompt]: Inference (val):avg data time: 2.72e-04, avg batch time: 0.3095, average loss: 0.6957
[12/05 19:34:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.08	
[12/05 19:34:55 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[12/05 19:36:58 visual_prompt]: 	Training 100/553. train loss: 0.6123,	0.8437 s / batch. (data: 1.17e-02). ETA=9:49:36, max mem: 20.9 GB 
[12/05 19:38:47 visual_prompt]: 	Training 200/553. train loss: 0.7145,	1.1758 s / batch. (data: 3.47e-01). ETA=13:39:39, max mem: 20.9 GB 
[12/05 19:40:46 visual_prompt]: 	Training 300/553. train loss: 0.6933,	0.8390 s / batch. (data: 2.31e-02). ETA=9:43:31, max mem: 20.9 GB 
[12/05 19:42:40 visual_prompt]: 	Training 400/553. train loss: 0.6574,	2.6100 s / batch. (data: 1.80e+00). ETA=1 day, 6:10:49, max mem: 20.9 GB 
[12/05 19:44:35 visual_prompt]: 	Training 500/553. train loss: 0.8415,	1.9296 s / batch. (data: 1.12e+00). ETA=22:15:32, max mem: 20.9 GB 
[12/05 19:45:34 visual_prompt]: Epoch 25 / 100: avg data time: 3.23e-01, avg batch time: 1.1545, average train loss: 0.7556
[12/05 19:46:41 visual_prompt]: Inference (val):avg data time: 5.32e-05, avg batch time: 0.3098, average loss: 0.6886
[12/05 19:46:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.84	
[12/05 19:46:41 visual_prompt]: Best epoch 25: best metric: -0.689
[12/05 19:46:41 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[12/05 19:48:40 visual_prompt]: 	Training 100/553. train loss: 0.6289,	0.8287 s / batch. (data: 3.31e-04). ETA=9:31:29, max mem: 20.9 GB 
[12/05 19:50:36 visual_prompt]: 	Training 200/553. train loss: 0.6732,	2.3692 s / batch. (data: 1.53e+00). ETA=1 day, 3:09:49, max mem: 20.9 GB 
[12/05 19:52:31 visual_prompt]: 	Training 300/553. train loss: 0.6006,	0.8263 s / batch. (data: 5.27e-04). ETA=9:27:04, max mem: 20.9 GB 
[12/05 19:54:24 visual_prompt]: 	Training 400/553. train loss: 0.6134,	0.8252 s / batch. (data: 6.10e-04). ETA=9:24:56, max mem: 20.9 GB 
[12/05 19:56:15 visual_prompt]: 	Training 500/553. train loss: 0.6837,	0.8478 s / batch. (data: 2.07e-02). ETA=9:38:58, max mem: 20.9 GB 
[12/05 19:57:13 visual_prompt]: Epoch 26 / 100: avg data time: 3.10e-01, avg batch time: 1.1426, average train loss: 0.7446
[12/05 19:58:20 visual_prompt]: Inference (val):avg data time: 2.20e-04, avg batch time: 0.3113, average loss: 0.7707
[12/05 19:58:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.01	
[12/05 19:58:20 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[12/05 20:00:20 visual_prompt]: 	Training 100/553. train loss: 0.5759,	0.8357 s / batch. (data: 3.45e-04). ETA=9:28:35, max mem: 20.9 GB 
[12/05 20:02:14 visual_prompt]: 	Training 200/553. train loss: 0.7599,	1.9560 s / batch. (data: 1.12e+00). ETA=22:07:31, max mem: 20.9 GB 
[12/05 20:04:07 visual_prompt]: 	Training 300/553. train loss: 0.6171,	0.9959 s / batch. (data: 1.58e-01). ETA=11:14:15, max mem: 20.9 GB 
[12/05 20:06:02 visual_prompt]: 	Training 400/553. train loss: 0.8538,	0.8160 s / batch. (data: 3.58e-04). ETA=9:11:05, max mem: 20.9 GB 
[12/05 20:07:56 visual_prompt]: 	Training 500/553. train loss: 0.9023,	0.8231 s / batch. (data: 3.67e-04). ETA=9:14:29, max mem: 20.9 GB 
[12/05 20:08:53 visual_prompt]: Epoch 27 / 100: avg data time: 3.11e-01, avg batch time: 1.1438, average train loss: 0.7394
[12/05 20:10:00 visual_prompt]: Inference (val):avg data time: 6.92e-05, avg batch time: 0.3092, average loss: 0.7072
[12/05 20:10:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.06	
[12/05 20:10:00 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[12/05 20:11:58 visual_prompt]: 	Training 100/553. train loss: 0.2810,	0.8200 s / batch. (data: 5.26e-04). ETA=9:10:18, max mem: 20.9 GB 
[12/05 20:13:52 visual_prompt]: 	Training 200/553. train loss: 0.6226,	0.8503 s / batch. (data: 3.25e-04). ETA=9:29:15, max mem: 20.9 GB 
[12/05 20:15:47 visual_prompt]: 	Training 300/553. train loss: 0.6038,	2.0881 s / batch. (data: 1.24e+00). ETA=23:14:27, max mem: 20.9 GB 
[12/05 20:17:39 visual_prompt]: 	Training 400/553. train loss: 0.7277,	0.8300 s / batch. (data: 4.19e-04). ETA=9:12:55, max mem: 20.9 GB 
[12/05 20:19:32 visual_prompt]: 	Training 500/553. train loss: 0.4626,	0.8483 s / batch. (data: 8.78e-04). ETA=9:23:41, max mem: 20.9 GB 
[12/05 20:20:31 visual_prompt]: Epoch 28 / 100: avg data time: 3.09e-01, avg batch time: 1.1418, average train loss: 0.7363
[12/05 20:21:38 visual_prompt]: Inference (val):avg data time: 5.98e-05, avg batch time: 0.3094, average loss: 0.7467
[12/05 20:21:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.67	
[12/05 20:21:38 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[12/05 20:23:43 visual_prompt]: 	Training 100/553. train loss: 0.5933,	0.8467 s / batch. (data: 2.85e-02). ETA=9:20:26, max mem: 20.9 GB 
[12/05 20:25:36 visual_prompt]: 	Training 200/553. train loss: 0.7172,	2.2821 s / batch. (data: 1.46e+00). ETA=1 day, 1:06:47, max mem: 20.9 GB 
[12/05 20:27:27 visual_prompt]: 	Training 300/553. train loss: 0.7275,	0.8701 s / batch. (data: 1.68e-02). ETA=9:33:02, max mem: 20.9 GB 
[12/05 20:29:17 visual_prompt]: 	Training 400/553. train loss: 0.5660,	1.6117 s / batch. (data: 7.70e-01). ETA=17:38:46, max mem: 20.9 GB 
[12/05 20:31:10 visual_prompt]: 	Training 500/553. train loss: 0.7062,	0.8356 s / batch. (data: 1.54e-03). ETA=9:07:33, max mem: 20.9 GB 
[12/05 20:32:09 visual_prompt]: Epoch 29 / 100: avg data time: 3.07e-01, avg batch time: 1.1399, average train loss: 0.7461
[12/05 20:33:16 visual_prompt]: Inference (val):avg data time: 2.25e-04, avg batch time: 0.3086, average loss: 0.6946
[12/05 20:33:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.42	
[12/05 20:33:16 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[12/05 20:35:12 visual_prompt]: 	Training 100/553. train loss: 0.7167,	0.8311 s / batch. (data: 3.75e-04). ETA=9:02:29, max mem: 20.9 GB 
[12/05 20:37:06 visual_prompt]: 	Training 200/553. train loss: 0.8070,	0.8384 s / batch. (data: 1.46e-02). ETA=9:05:51, max mem: 20.9 GB 
[12/05 20:38:59 visual_prompt]: 	Training 300/553. train loss: 0.4083,	1.7399 s / batch. (data: 9.22e-01). ETA=18:49:50, max mem: 20.9 GB 
[12/05 20:40:55 visual_prompt]: 	Training 400/553. train loss: 0.6901,	1.5973 s / batch. (data: 7.83e-01). ETA=17:14:34, max mem: 20.9 GB 
[12/05 20:42:46 visual_prompt]: 	Training 500/553. train loss: 0.6150,	1.9532 s / batch. (data: 1.12e+00). ETA=21:01:50, max mem: 20.9 GB 
[12/05 20:43:47 visual_prompt]: Epoch 30 / 100: avg data time: 3.10e-01, avg batch time: 1.1421, average train loss: 0.7426
[12/05 20:44:54 visual_prompt]: Inference (val):avg data time: 5.97e-05, avg batch time: 0.3092, average loss: 0.6990
[12/05 20:44:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.13	
[12/05 20:44:54 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[12/05 20:46:55 visual_prompt]: 	Training 100/553. train loss: 0.6396,	0.8656 s / batch. (data: 2.07e-02). ETA=9:17:02, max mem: 20.9 GB 
[12/05 20:48:51 visual_prompt]: 	Training 200/553. train loss: 0.7085,	0.8428 s / batch. (data: 1.44e-02). ETA=9:00:54, max mem: 20.9 GB 
[12/05 20:50:41 visual_prompt]: 	Training 300/553. train loss: 0.6918,	0.8366 s / batch. (data: 4.30e-04). ETA=8:55:35, max mem: 20.9 GB 
[12/05 20:52:34 visual_prompt]: 	Training 400/553. train loss: 0.5714,	1.5620 s / batch. (data: 7.36e-01). ETA=16:37:21, max mem: 20.9 GB 
[12/05 20:54:28 visual_prompt]: 	Training 500/553. train loss: 0.8400,	0.8331 s / batch. (data: 1.56e-02). ETA=8:50:32, max mem: 20.9 GB 
[12/05 20:55:26 visual_prompt]: Epoch 31 / 100: avg data time: 3.09e-01, avg batch time: 1.1413, average train loss: 0.7275
[12/05 20:56:32 visual_prompt]: Inference (val):avg data time: 6.07e-05, avg batch time: 0.3102, average loss: 0.7573
[12/05 20:56:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.90	
[12/05 20:56:32 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[12/05 20:58:33 visual_prompt]: 	Training 100/553. train loss: 0.9191,	0.8484 s / batch. (data: 8.08e-03). ETA=8:58:07, max mem: 20.9 GB 
[12/05 21:00:25 visual_prompt]: 	Training 200/553. train loss: 0.6166,	0.8205 s / batch. (data: 5.66e-03). ETA=8:39:04, max mem: 20.9 GB 
[12/05 21:02:27 visual_prompt]: 	Training 300/553. train loss: 0.6997,	0.8444 s / batch. (data: 2.07e-02). ETA=8:52:48, max mem: 20.9 GB 
[12/05 21:04:29 visual_prompt]: 	Training 400/553. train loss: 0.9619,	0.8515 s / batch. (data: 1.36e-03). ETA=8:55:50, max mem: 20.9 GB 
[12/05 21:06:24 visual_prompt]: 	Training 500/553. train loss: 0.7259,	0.8439 s / batch. (data: 5.47e-04). ETA=8:49:40, max mem: 20.9 GB 
[12/05 21:07:22 visual_prompt]: Epoch 32 / 100: avg data time: 3.43e-01, avg batch time: 1.1741, average train loss: 0.7294
[12/05 21:08:30 visual_prompt]: Inference (val):avg data time: 6.80e-05, avg batch time: 0.3124, average loss: 0.7094
[12/05 21:08:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.94	
[12/05 21:08:30 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[12/05 21:10:28 visual_prompt]: 	Training 100/553. train loss: 1.0189,	0.8481 s / batch. (data: 8.01e-03). ETA=8:50:06, max mem: 20.9 GB 
[12/05 21:12:30 visual_prompt]: 	Training 200/553. train loss: 0.5624,	2.6400 s / batch. (data: 1.79e+00). ETA=1 day, 3:25:46, max mem: 20.9 GB 
[12/05 21:14:23 visual_prompt]: 	Training 300/553. train loss: 0.6215,	0.8280 s / batch. (data: 3.45e-04). ETA=8:34:46, max mem: 20.9 GB 
[12/05 21:16:16 visual_prompt]: 	Training 400/553. train loss: 0.7281,	0.8444 s / batch. (data: 2.50e-02). ETA=8:43:34, max mem: 20.9 GB 
[12/05 21:18:10 visual_prompt]: 	Training 500/553. train loss: 0.6636,	0.8704 s / batch. (data: 3.79e-02). ETA=8:58:14, max mem: 20.9 GB 
[12/05 21:19:09 visual_prompt]: Epoch 33 / 100: avg data time: 3.24e-01, avg batch time: 1.1560, average train loss: 0.7357
[12/05 21:20:17 visual_prompt]: Inference (val):avg data time: 7.02e-05, avg batch time: 0.3101, average loss: 0.6899
[12/05 21:20:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.10	
[12/05 21:20:17 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[12/05 21:22:18 visual_prompt]: 	Training 100/553. train loss: 0.7234,	0.9487 s / batch. (data: 8.29e-02). ETA=9:44:17, max mem: 20.9 GB 
[12/05 21:24:08 visual_prompt]: 	Training 200/553. train loss: 0.7554,	0.8360 s / batch. (data: 5.48e-03). ETA=8:33:27, max mem: 20.9 GB 
[12/05 21:26:00 visual_prompt]: 	Training 300/553. train loss: 0.6948,	1.0914 s / batch. (data: 2.51e-01). ETA=11:08:29, max mem: 20.9 GB 
[12/05 21:27:55 visual_prompt]: 	Training 400/553. train loss: 0.6472,	0.8288 s / batch. (data: 6.08e-04). ETA=8:26:16, max mem: 20.9 GB 
[12/05 21:29:49 visual_prompt]: 	Training 500/553. train loss: 0.8990,	1.9838 s / batch. (data: 1.13e+00). ETA=20:08:31, max mem: 20.9 GB 
[12/05 21:30:47 visual_prompt]: Epoch 34 / 100: avg data time: 3.06e-01, avg batch time: 1.1394, average train loss: 0.7285
[12/05 21:31:53 visual_prompt]: Inference (val):avg data time: 5.70e-05, avg batch time: 0.3098, average loss: 0.6929
[12/05 21:31:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.19	rocauc: 61.63	
[12/05 21:31:53 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[12/05 21:33:54 visual_prompt]: 	Training 100/553. train loss: 0.6518,	0.8170 s / batch. (data: 3.26e-04). ETA=8:15:35, max mem: 20.9 GB 
[12/05 21:35:49 visual_prompt]: 	Training 200/553. train loss: 0.5978,	0.8290 s / batch. (data: 3.76e-04). ETA=8:21:30, max mem: 20.9 GB 
[12/05 21:37:40 visual_prompt]: 	Training 300/553. train loss: 0.7145,	0.8675 s / batch. (data: 3.98e-02). ETA=8:43:20, max mem: 20.9 GB 
[12/05 21:39:31 visual_prompt]: 	Training 400/553. train loss: 0.6154,	0.8440 s / batch. (data: 3.67e-04). ETA=8:27:46, max mem: 20.9 GB 
[12/05 21:41:24 visual_prompt]: 	Training 500/553. train loss: 0.8840,	1.2560 s / batch. (data: 4.24e-01). ETA=12:33:33, max mem: 20.9 GB 
[12/05 21:42:24 visual_prompt]: Epoch 35 / 100: avg data time: 3.07e-01, avg batch time: 1.1399, average train loss: 0.7593
[12/05 21:43:29 visual_prompt]: Inference (val):avg data time: 4.88e-05, avg batch time: 0.3098, average loss: 0.6881
[12/05 21:43:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.04	
[12/05 21:43:29 visual_prompt]: Best epoch 35: best metric: -0.688
[12/05 21:43:29 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[12/05 21:45:27 visual_prompt]: 	Training 100/553. train loss: 0.8359,	0.9586 s / batch. (data: 1.31e-01). ETA=9:32:40, max mem: 20.9 GB 
[12/05 21:47:21 visual_prompt]: 	Training 200/553. train loss: 0.8371,	0.8379 s / batch. (data: 1.09e-02). ETA=8:19:11, max mem: 20.9 GB 
[12/05 21:49:16 visual_prompt]: 	Training 300/553. train loss: 0.5996,	0.8725 s / batch. (data: 7.98e-03). ETA=8:38:21, max mem: 20.9 GB 
[12/05 21:51:09 visual_prompt]: 	Training 400/553. train loss: 0.9676,	2.1282 s / batch. (data: 1.31e+00). ETA=21:00:48, max mem: 20.9 GB 
[12/05 21:53:03 visual_prompt]: 	Training 500/553. train loss: 0.8029,	0.9880 s / batch. (data: 1.64e-01). ETA=9:43:39, max mem: 20.9 GB 
[12/05 21:53:58 visual_prompt]: Epoch 36 / 100: avg data time: 3.03e-01, avg batch time: 1.1369, average train loss: 0.7692
[12/05 21:55:04 visual_prompt]: Inference (val):avg data time: 5.99e-05, avg batch time: 0.3122, average loss: 0.6884
[12/05 21:55:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.01	
[12/05 21:55:04 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[12/05 21:57:02 visual_prompt]: 	Training 100/553. train loss: 0.8847,	0.8346 s / batch. (data: 7.13e-03). ETA=8:10:54, max mem: 20.9 GB 
[12/05 21:58:55 visual_prompt]: 	Training 200/553. train loss: 0.8832,	0.8166 s / batch. (data: 5.47e-04). ETA=7:58:56, max mem: 20.9 GB 
[12/05 22:00:49 visual_prompt]: 	Training 300/553. train loss: 0.6878,	2.0270 s / batch. (data: 1.21e+00). ETA=19:45:33, max mem: 20.9 GB 
[12/05 22:02:46 visual_prompt]: 	Training 400/553. train loss: 0.9354,	2.0731 s / batch. (data: 1.25e+00). ETA=20:09:00, max mem: 20.9 GB 
[12/05 22:04:35 visual_prompt]: 	Training 500/553. train loss: 0.7281,	1.5280 s / batch. (data: 6.88e-01). ETA=14:48:35, max mem: 20.9 GB 
[12/05 22:05:35 visual_prompt]: Epoch 37 / 100: avg data time: 3.09e-01, avg batch time: 1.1419, average train loss: 0.7389
[12/05 22:06:41 visual_prompt]: Inference (val):avg data time: 3.21e-04, avg batch time: 0.3117, average loss: 0.6901
[12/05 22:06:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.26	
[12/05 22:06:41 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[12/05 22:08:38 visual_prompt]: 	Training 100/553. train loss: 0.5886,	1.2414 s / batch. (data: 4.14e-01). ETA=11:58:44, max mem: 20.9 GB 
[12/05 22:10:34 visual_prompt]: 	Training 200/553. train loss: 0.6969,	2.0736 s / batch. (data: 1.22e+00). ETA=19:57:08, max mem: 20.9 GB 
[12/05 22:12:29 visual_prompt]: 	Training 300/553. train loss: 0.6772,	0.8305 s / batch. (data: 3.37e-04). ETA=7:58:04, max mem: 20.9 GB 
[12/05 22:14:20 visual_prompt]: 	Training 400/553. train loss: 0.6716,	0.8568 s / batch. (data: 2.89e-02). ETA=8:11:48, max mem: 20.9 GB 
[12/05 22:16:16 visual_prompt]: 	Training 500/553. train loss: 0.8136,	0.8509 s / batch. (data: 1.29e-02). ETA=8:06:59, max mem: 20.9 GB 
[12/05 22:17:14 visual_prompt]: Epoch 38 / 100: avg data time: 3.10e-01, avg batch time: 1.1440, average train loss: 0.7327
[12/05 22:18:20 visual_prompt]: Inference (val):avg data time: 1.98e-04, avg batch time: 0.3097, average loss: 0.7041
[12/05 22:18:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.40	
[12/05 22:18:20 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.07795964517353735
[12/05 22:20:16 visual_prompt]: 	Training 100/553. train loss: 0.9555,	0.8226 s / batch. (data: 5.76e-04). ETA=7:48:41, max mem: 20.9 GB 
[12/05 22:22:14 visual_prompt]: 	Training 200/553. train loss: 1.4361,	0.8335 s / batch. (data: 3.33e-04). ETA=7:53:30, max mem: 20.9 GB 
[12/05 22:24:12 visual_prompt]: 	Training 300/553. train loss: 0.6856,	0.8167 s / batch. (data: 3.71e-04). ETA=7:42:34, max mem: 20.9 GB 
[12/05 22:26:02 visual_prompt]: 	Training 400/553. train loss: 0.6102,	0.8307 s / batch. (data: 1.06e-02). ETA=7:49:09, max mem: 20.9 GB 
[12/05 22:27:55 visual_prompt]: 	Training 500/553. train loss: 0.5632,	2.2772 s / batch. (data: 1.46e+00). ETA=21:22:16, max mem: 20.9 GB 
[12/05 22:28:52 visual_prompt]: Epoch 39 / 100: avg data time: 3.10e-01, avg batch time: 1.1431, average train loss: 0.7370
[12/05 22:29:57 visual_prompt]: Inference (val):avg data time: 5.54e-05, avg batch time: 0.3098, average loss: 0.7066
[12/05 22:29:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.70	
[12/05 22:29:57 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.07649596321166025
[12/05 22:31:56 visual_prompt]: 	Training 100/553. train loss: 0.7542,	0.8379 s / batch. (data: 3.22e-04). ETA=7:49:40, max mem: 20.9 GB 
[12/05 22:33:49 visual_prompt]: 	Training 200/553. train loss: 0.6078,	0.8320 s / batch. (data: 3.24e-04). ETA=7:44:59, max mem: 20.9 GB 
[12/05 22:35:42 visual_prompt]: 	Training 300/553. train loss: 0.6014,	0.8299 s / batch. (data: 5.86e-03). ETA=7:42:26, max mem: 20.9 GB 
[12/05 22:37:37 visual_prompt]: 	Training 400/553. train loss: 0.6166,	0.8353 s / batch. (data: 3.37e-04). ETA=7:44:03, max mem: 20.9 GB 
[12/05 22:39:28 visual_prompt]: 	Training 500/553. train loss: 0.2852,	0.8396 s / batch. (data: 1.29e-03). ETA=7:45:02, max mem: 20.9 GB 
[12/05 22:40:29 visual_prompt]: Epoch 40 / 100: avg data time: 3.08e-01, avg batch time: 1.1417, average train loss: 0.7328
[12/05 22:41:35 visual_prompt]: Inference (val):avg data time: 2.82e-04, avg batch time: 0.3122, average loss: 0.6878
[12/05 22:41:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.89	
[12/05 22:41:35 visual_prompt]: Best epoch 40: best metric: -0.688
[12/05 22:41:35 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.07500000000000001
[12/05 22:43:38 visual_prompt]: 	Training 100/553. train loss: 0.7385,	0.8256 s / batch. (data: 1.20e-02). ETA=7:35:09, max mem: 20.9 GB 
[12/05 22:45:35 visual_prompt]: 	Training 200/553. train loss: 0.6140,	0.8664 s / batch. (data: 1.18e-02). ETA=7:56:14, max mem: 20.9 GB 
[12/05 22:47:27 visual_prompt]: 	Training 300/553. train loss: 0.7005,	0.8402 s / batch. (data: 7.95e-03). ETA=7:40:26, max mem: 20.9 GB 
[12/05 22:49:19 visual_prompt]: 	Training 400/553. train loss: 0.8565,	0.8449 s / batch. (data: 6.60e-03). ETA=7:41:37, max mem: 20.9 GB 
[12/05 22:51:09 visual_prompt]: 	Training 500/553. train loss: 1.2732,	0.8444 s / batch. (data: 3.39e-04). ETA=7:39:53, max mem: 20.9 GB 
[12/05 22:52:05 visual_prompt]: Epoch 41 / 100: avg data time: 3.06e-01, avg batch time: 1.1402, average train loss: 0.7478
[12/05 22:53:11 visual_prompt]: Inference (val):avg data time: 6.05e-05, avg batch time: 0.3097, average loss: 0.7116
[12/05 22:53:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.84	
[12/05 22:53:11 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.07347357813929455
[12/05 22:55:08 visual_prompt]: 	Training 100/553. train loss: 0.7005,	0.8876 s / batch. (data: 1.08e-03). ETA=8:01:11, max mem: 20.9 GB 
[12/05 22:57:02 visual_prompt]: 	Training 200/553. train loss: 0.7370,	0.8220 s / batch. (data: 6.70e-04). ETA=7:24:13, max mem: 20.9 GB 
[12/05 22:58:55 visual_prompt]: 	Training 300/553. train loss: 0.6795,	0.8372 s / batch. (data: 1.56e-02). ETA=7:31:03, max mem: 20.9 GB 
[12/05 23:00:49 visual_prompt]: 	Training 400/553. train loss: 0.6917,	0.8242 s / batch. (data: 3.32e-04). ETA=7:22:42, max mem: 20.9 GB 
[12/05 23:02:41 visual_prompt]: 	Training 500/553. train loss: 1.5781,	0.8406 s / batch. (data: 2.47e-02). ETA=7:30:05, max mem: 20.9 GB 
[12/05 23:03:42 visual_prompt]: Epoch 42 / 100: avg data time: 3.07e-01, avg batch time: 1.1401, average train loss: 0.7293
[12/05 23:04:47 visual_prompt]: Inference (val):avg data time: 6.38e-05, avg batch time: 0.3098, average loss: 0.7266
[12/05 23:04:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.95	
[12/05 23:04:47 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.07191855733945388
[12/05 23:06:47 visual_prompt]: 	Training 100/553. train loss: 0.5882,	0.8567 s / batch. (data: 1.28e-02). ETA=7:36:31, max mem: 20.9 GB 
[12/05 23:08:39 visual_prompt]: 	Training 200/553. train loss: 0.7907,	0.8466 s / batch. (data: 5.92e-03). ETA=7:29:45, max mem: 20.9 GB 
[12/05 23:10:31 visual_prompt]: 	Training 300/553. train loss: 0.9188,	0.8215 s / batch. (data: 5.62e-03). ETA=7:15:00, max mem: 20.9 GB 
[12/05 23:12:22 visual_prompt]: 	Training 400/553. train loss: 0.6374,	0.8456 s / batch. (data: 1.06e-02). ETA=7:26:23, max mem: 20.9 GB 
[12/05 23:14:18 visual_prompt]: 	Training 500/553. train loss: 0.5805,	0.8439 s / batch. (data: 7.85e-03). ETA=7:24:06, max mem: 20.9 GB 
[12/05 23:15:18 visual_prompt]: Epoch 43 / 100: avg data time: 3.07e-01, avg batch time: 1.1400, average train loss: 0.7341
[12/05 23:16:23 visual_prompt]: Inference (val):avg data time: 5.56e-05, avg batch time: 0.3114, average loss: 0.6918
[12/05 23:16:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.26	
[12/05 23:16:23 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.07033683215379002
[12/05 23:18:22 visual_prompt]: 	Training 100/553. train loss: 0.6247,	0.9417 s / batch. (data: 8.25e-02). ETA=8:13:09, max mem: 20.9 GB 
[12/05 23:20:18 visual_prompt]: 	Training 200/553. train loss: 0.6427,	0.8304 s / batch. (data: 3.44e-04). ETA=7:13:28, max mem: 20.9 GB 
[12/05 23:22:10 visual_prompt]: 	Training 300/553. train loss: 0.6587,	0.8425 s / batch. (data: 2.28e-02). ETA=7:18:24, max mem: 20.9 GB 
[12/05 23:24:01 visual_prompt]: 	Training 400/553. train loss: 0.7130,	0.8361 s / batch. (data: 2.50e-04). ETA=7:13:41, max mem: 20.9 GB 
[12/05 23:25:55 visual_prompt]: 	Training 500/553. train loss: 0.8189,	0.8601 s / batch. (data: 3.55e-04). ETA=7:24:41, max mem: 20.9 GB 
[12/05 23:26:55 visual_prompt]: Epoch 44 / 100: avg data time: 3.09e-01, avg batch time: 1.1420, average train loss: 0.7433
[12/05 23:28:01 visual_prompt]: Inference (val):avg data time: 5.57e-05, avg batch time: 0.3090, average loss: 0.7069
[12/05 23:28:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.23	
[12/05 23:28:01 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0687303296707956
[12/05 23:30:01 visual_prompt]: 	Training 100/553. train loss: 0.7058,	0.8384 s / batch. (data: 3.81e-04). ETA=7:11:21, max mem: 20.9 GB 
[12/05 23:31:50 visual_prompt]: 	Training 200/553. train loss: 0.6636,	1.5079 s / batch. (data: 6.73e-01). ETA=12:53:14, max mem: 20.9 GB 
[12/05 23:33:46 visual_prompt]: 	Training 300/553. train loss: 0.7507,	0.8372 s / batch. (data: 3.37e-04). ETA=7:07:54, max mem: 20.9 GB 
[12/05 23:35:36 visual_prompt]: 	Training 400/553. train loss: 0.7370,	0.8213 s / batch. (data: 4.10e-04). ETA=6:58:24, max mem: 20.9 GB 
[12/05 23:37:33 visual_prompt]: 	Training 500/553. train loss: 0.5791,	0.8240 s / batch. (data: 3.87e-04). ETA=6:58:25, max mem: 20.9 GB 
[12/05 23:38:33 visual_prompt]: Epoch 45 / 100: avg data time: 3.10e-01, avg batch time: 1.1426, average train loss: 0.7129
[12/05 23:39:39 visual_prompt]: Inference (val):avg data time: 5.39e-05, avg batch time: 0.3088, average loss: 0.7066
[12/05 23:39:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.59	
[12/05 23:39:39 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.06710100716628345
[12/05 23:41:38 visual_prompt]: 	Training 100/553. train loss: 0.7631,	1.1134 s / batch. (data: 2.66e-01). ETA=9:22:34, max mem: 20.9 GB 
[12/05 23:43:32 visual_prompt]: 	Training 200/553. train loss: 0.7522,	0.8202 s / batch. (data: 3.76e-04). ETA=6:53:01, max mem: 20.9 GB 
[12/05 23:45:23 visual_prompt]: 	Training 300/553. train loss: 0.5610,	0.8488 s / batch. (data: 3.23e-04). ETA=7:06:00, max mem: 20.9 GB 
[12/05 23:47:17 visual_prompt]: 	Training 400/553. train loss: 0.7299,	0.8360 s / batch. (data: 1.11e-03). ETA=6:58:11, max mem: 20.9 GB 
[12/05 23:49:04 visual_prompt]: 	Training 500/553. train loss: 1.0928,	0.8645 s / batch. (data: 7.98e-03). ETA=7:11:02, max mem: 20.9 GB 
[12/05 23:50:07 visual_prompt]: Epoch 46 / 100: avg data time: 3.03e-01, avg batch time: 1.1357, average train loss: 0.7486
[12/05 23:51:13 visual_prompt]: Inference (val):avg data time: 5.53e-05, avg batch time: 0.3089, average loss: 0.7006
[12/05 23:51:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.72	
[12/05 23:51:13 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.06545084971874737
[12/05 23:53:12 visual_prompt]: 	Training 100/553. train loss: 0.6885,	0.8324 s / batch. (data: 4.45e-03). ETA=6:52:52, max mem: 20.9 GB 
[12/05 23:55:01 visual_prompt]: 	Training 200/553. train loss: 0.6934,	0.8512 s / batch. (data: 2.13e-02). ETA=7:00:49, max mem: 20.9 GB 
[12/05 23:56:56 visual_prompt]: 	Training 300/553. train loss: 0.8040,	0.8533 s / batch. (data: 8.34e-04). ETA=7:00:24, max mem: 20.9 GB 
[12/05 23:58:49 visual_prompt]: 	Training 400/553. train loss: 0.6061,	0.8204 s / batch. (data: 3.52e-04). ETA=6:42:51, max mem: 20.9 GB 
[12/06 00:00:41 visual_prompt]: 	Training 500/553. train loss: 0.6369,	0.8450 s / batch. (data: 5.52e-03). ETA=6:53:31, max mem: 20.9 GB 
[12/06 00:01:40 visual_prompt]: Epoch 47 / 100: avg data time: 3.02e-01, avg batch time: 1.1345, average train loss: 0.7348
[12/06 00:02:47 visual_prompt]: Inference (val):avg data time: 5.37e-05, avg batch time: 0.3111, average loss: 0.7785
[12/06 00:02:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.70	
[12/06 00:02:47 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.06378186779084996
[12/06 00:04:45 visual_prompt]: 	Training 100/553. train loss: 0.7700,	0.8300 s / batch. (data: 2.11e-04). ETA=6:44:02, max mem: 20.9 GB 
[12/06 00:06:39 visual_prompt]: 	Training 200/553. train loss: 0.7895,	0.8615 s / batch. (data: 1.13e-02). ETA=6:57:57, max mem: 20.9 GB 
[12/06 00:08:33 visual_prompt]: 	Training 300/553. train loss: 0.7046,	1.5880 s / batch. (data: 7.55e-01). ETA=12:47:45, max mem: 20.9 GB 
[12/06 00:10:22 visual_prompt]: 	Training 400/553. train loss: 0.7237,	1.2641 s / batch. (data: 4.18e-01). ETA=10:09:02, max mem: 20.9 GB 
[12/06 00:12:16 visual_prompt]: 	Training 500/553. train loss: 0.6944,	0.8266 s / batch. (data: 3.59e-04). ETA=6:36:54, max mem: 20.9 GB 
[12/06 00:13:12 visual_prompt]: Epoch 48 / 100: avg data time: 2.98e-01, avg batch time: 1.1302, average train loss: 0.7270
[12/06 00:14:19 visual_prompt]: Inference (val):avg data time: 6.08e-05, avg batch time: 0.3113, average loss: 0.6921
[12/06 00:14:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.02	
[12/06 00:14:19 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.06209609477998338
[12/06 00:16:16 visual_prompt]: 	Training 100/553. train loss: 0.6981,	0.8381 s / batch. (data: 1.06e-02). ETA=6:40:17, max mem: 20.9 GB 
[12/06 00:18:07 visual_prompt]: 	Training 200/553. train loss: 0.5986,	0.8415 s / batch. (data: 1.13e-02). ETA=6:40:29, max mem: 20.9 GB 
[12/06 00:20:01 visual_prompt]: 	Training 300/553. train loss: 0.8896,	0.8279 s / batch. (data: 1.17e-03). ETA=6:32:38, max mem: 20.9 GB 
[12/06 00:21:56 visual_prompt]: 	Training 400/553. train loss: 0.7094,	0.8510 s / batch. (data: 1.61e-02). ETA=6:42:11, max mem: 20.9 GB 
[12/06 00:23:49 visual_prompt]: 	Training 500/553. train loss: 0.5954,	0.8166 s / batch. (data: 3.37e-04). ETA=6:24:32, max mem: 20.9 GB 
[12/06 00:24:48 visual_prompt]: Epoch 49 / 100: avg data time: 3.05e-01, avg batch time: 1.1371, average train loss: 0.7211
[12/06 00:25:53 visual_prompt]: Inference (val):avg data time: 4.94e-05, avg batch time: 0.3105, average loss: 0.7125
[12/06 00:25:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.78	
[12/06 00:25:53 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.06039558454088796
[12/06 00:27:52 visual_prompt]: 	Training 100/553. train loss: 0.8036,	0.8400 s / batch. (data: 1.03e-03). ETA=6:33:26, max mem: 20.9 GB 
[12/06 00:29:47 visual_prompt]: 	Training 200/553. train loss: 0.6952,	0.8243 s / batch. (data: 3.49e-04). ETA=6:24:42, max mem: 20.9 GB 
[12/06 00:31:39 visual_prompt]: 	Training 300/553. train loss: 0.8076,	0.8292 s / batch. (data: 3.48e-04). ETA=6:25:37, max mem: 20.9 GB 
[12/06 00:33:29 visual_prompt]: 	Training 400/553. train loss: 0.6585,	0.8400 s / batch. (data: 3.33e-04). ETA=6:29:14, max mem: 20.9 GB 
[12/06 00:35:23 visual_prompt]: 	Training 500/553. train loss: 0.6939,	0.8405 s / batch. (data: 8.46e-04). ETA=6:28:04, max mem: 20.9 GB 
[12/06 00:36:22 visual_prompt]: Epoch 50 / 100: avg data time: 3.05e-01, avg batch time: 1.1363, average train loss: 0.7186
[12/06 00:37:30 visual_prompt]: Inference (val):avg data time: 5.71e-05, avg batch time: 0.3090, average loss: 0.6883
[12/06 00:37:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.41	
[12/06 00:37:30 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.058682408883346526
[12/06 00:39:30 visual_prompt]: 	Training 100/553. train loss: 0.6971,	1.5358 s / batch. (data: 6.97e-01). ETA=11:45:10, max mem: 20.9 GB 
[12/06 00:41:26 visual_prompt]: 	Training 200/553. train loss: 0.5786,	0.8376 s / batch. (data: 1.06e-02). ETA=6:23:10, max mem: 20.9 GB 
[12/06 00:43:21 visual_prompt]: 	Training 300/553. train loss: 0.7544,	2.1970 s / batch. (data: 1.35e+00). ETA=16:41:29, max mem: 20.9 GB 
[12/06 00:45:15 visual_prompt]: 	Training 400/553. train loss: 0.8711,	1.8599 s / batch. (data: 1.01e+00). ETA=14:04:43, max mem: 20.9 GB 
[12/06 00:47:07 visual_prompt]: 	Training 500/553. train loss: 0.6790,	0.8313 s / batch. (data: 7.11e-04). ETA=6:16:08, max mem: 20.9 GB 
[12/06 00:48:03 visual_prompt]: Epoch 51 / 100: avg data time: 3.13e-01, avg batch time: 1.1446, average train loss: 0.7075
[12/06 00:49:09 visual_prompt]: Inference (val):avg data time: 5.44e-05, avg batch time: 0.3109, average loss: 0.6923
[12/06 00:49:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 59.65	
[12/06 00:49:09 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.05695865504800327
[12/06 00:51:12 visual_prompt]: 	Training 100/553. train loss: 0.6983,	0.8269 s / batch. (data: 8.57e-04). ETA=6:12:05, max mem: 20.9 GB 
[12/06 00:53:03 visual_prompt]: 	Training 200/553. train loss: 0.7498,	0.8510 s / batch. (data: 3.42e-04). ETA=6:21:30, max mem: 20.9 GB 
[12/06 00:54:58 visual_prompt]: 	Training 300/553. train loss: 1.0877,	0.8156 s / batch. (data: 3.08e-04). ETA=6:04:16, max mem: 20.9 GB 
[12/06 00:56:53 visual_prompt]: 	Training 400/553. train loss: 0.2293,	0.8575 s / batch. (data: 5.59e-03). ETA=6:21:33, max mem: 20.9 GB 
[12/06 00:58:40 visual_prompt]: 	Training 500/553. train loss: 0.3295,	0.8303 s / batch. (data: 5.33e-04). ETA=6:08:04, max mem: 20.9 GB 
[12/06 00:59:37 visual_prompt]: Epoch 52 / 100: avg data time: 3.03e-01, avg batch time: 1.1356, average train loss: 0.7140
[12/06 01:00:44 visual_prompt]: Inference (val):avg data time: 3.07e-04, avg batch time: 0.3120, average loss: 0.6886
[12/06 01:00:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.45	
[12/06 01:00:44 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.05522642316338268
[12/06 01:02:41 visual_prompt]: 	Training 100/553. train loss: 0.6933,	0.8307 s / batch. (data: 5.51e-03). ETA=6:06:07, max mem: 20.9 GB 
[12/06 01:04:35 visual_prompt]: 	Training 200/553. train loss: 0.6126,	0.8369 s / batch. (data: 8.50e-04). ETA=6:07:27, max mem: 20.9 GB 
[12/06 01:06:28 visual_prompt]: 	Training 300/553. train loss: 0.7796,	0.8681 s / batch. (data: 5.45e-04). ETA=6:19:41, max mem: 20.9 GB 
[12/06 01:08:24 visual_prompt]: 	Training 400/553. train loss: 0.6196,	1.3280 s / batch. (data: 4.92e-01). ETA=9:38:39, max mem: 20.9 GB 
[12/06 01:10:15 visual_prompt]: 	Training 500/553. train loss: 0.7754,	0.8241 s / batch. (data: 9.11e-03). ETA=5:57:43, max mem: 20.9 GB 
[12/06 01:11:15 visual_prompt]: Epoch 53 / 100: avg data time: 3.08e-01, avg batch time: 1.1408, average train loss: 0.7110
[12/06 01:12:21 visual_prompt]: Inference (val):avg data time: 5.51e-05, avg batch time: 0.3102, average loss: 0.7336
[12/06 01:12:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.94	
[12/06 01:12:21 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.05348782368720626
[12/06 01:14:23 visual_prompt]: 	Training 100/553. train loss: 0.6907,	0.8424 s / batch. (data: 1.87e-02). ETA=6:03:30, max mem: 20.9 GB 
[12/06 01:16:16 visual_prompt]: 	Training 200/553. train loss: 0.8765,	0.8636 s / batch. (data: 1.20e-03). ETA=6:11:13, max mem: 20.9 GB 
[12/06 01:18:07 visual_prompt]: 	Training 300/553. train loss: 0.5391,	1.4497 s / batch. (data: 6.20e-01). ETA=10:20:44, max mem: 20.9 GB 
[12/06 01:20:00 visual_prompt]: 	Training 400/553. train loss: 0.5725,	0.8812 s / batch. (data: 5.45e-03). ETA=6:15:51, max mem: 20.9 GB 
[12/06 01:21:55 visual_prompt]: 	Training 500/553. train loss: 0.7692,	0.8617 s / batch. (data: 2.25e-03). ETA=6:06:05, max mem: 20.9 GB 
[12/06 01:22:53 visual_prompt]: Epoch 54 / 100: avg data time: 3.10e-01, avg batch time: 1.1419, average train loss: 0.7174
[12/06 01:24:00 visual_prompt]: Inference (val):avg data time: 5.89e-05, avg batch time: 0.3097, average loss: 0.7027
[12/06 01:24:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.49	
[12/06 01:24:00 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.05174497483512506
[12/06 01:25:58 visual_prompt]: 	Training 100/553. train loss: 0.5440,	0.8160 s / batch. (data: 4.01e-04). ETA=5:44:35, max mem: 20.9 GB 
[12/06 01:27:49 visual_prompt]: 	Training 200/553. train loss: 0.8228,	0.8436 s / batch. (data: 6.07e-03). ETA=5:54:51, max mem: 20.9 GB 
[12/06 01:29:44 visual_prompt]: 	Training 300/553. train loss: 0.7059,	0.8160 s / batch. (data: 3.50e-04). ETA=5:41:53, max mem: 20.9 GB 
[12/06 01:31:38 visual_prompt]: 	Training 400/553. train loss: 0.7232,	0.8446 s / batch. (data: 1.06e-02). ETA=5:52:26, max mem: 20.9 GB 
[12/06 01:33:29 visual_prompt]: 	Training 500/553. train loss: 0.5736,	0.8204 s / batch. (data: 3.77e-04). ETA=5:40:59, max mem: 20.9 GB 
[12/06 01:34:30 visual_prompt]: Epoch 55 / 100: avg data time: 3.08e-01, avg batch time: 1.1396, average train loss: 0.7140
[12/06 01:35:37 visual_prompt]: Inference (val):avg data time: 7.12e-05, avg batch time: 0.3108, average loss: 0.7568
[12/06 01:35:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.50	
[12/06 01:35:37 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.05
[12/06 01:37:36 visual_prompt]: 	Training 100/553. train loss: 0.5743,	0.8167 s / batch. (data: 3.59e-04). ETA=5:37:21, max mem: 20.9 GB 
[12/06 01:39:27 visual_prompt]: 	Training 200/553. train loss: 0.6604,	0.8456 s / batch. (data: 1.83e-02). ETA=5:47:53, max mem: 20.9 GB 
[12/06 01:41:22 visual_prompt]: 	Training 300/553. train loss: 0.8273,	0.8432 s / batch. (data: 2.19e-03). ETA=5:45:29, max mem: 20.9 GB 
[12/06 01:43:18 visual_prompt]: 	Training 400/553. train loss: 0.6922,	0.8408 s / batch. (data: 8.35e-04). ETA=5:43:07, max mem: 20.9 GB 
[12/06 01:45:09 visual_prompt]: 	Training 500/553. train loss: 0.7116,	2.9771 s / batch. (data: 2.16e+00). ETA=20:09:56, max mem: 20.9 GB 
[12/06 01:46:06 visual_prompt]: Epoch 56 / 100: avg data time: 3.04e-01, avg batch time: 1.1371, average train loss: 0.7259
[12/06 01:47:12 visual_prompt]: Inference (val):avg data time: 1.62e-04, avg batch time: 0.3104, average loss: 0.7514
[12/06 01:47:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.77	
[12/06 01:47:12 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.04825502516487497
[12/06 01:49:14 visual_prompt]: 	Training 100/553. train loss: 0.7456,	0.8601 s / batch. (data: 3.86e-02). ETA=5:47:21, max mem: 20.9 GB 
[12/06 01:51:04 visual_prompt]: 	Training 200/553. train loss: 0.7603,	0.8256 s / batch. (data: 6.09e-03). ETA=5:32:03, max mem: 20.9 GB 
[12/06 01:52:59 visual_prompt]: 	Training 300/553. train loss: 0.6697,	2.6559 s / batch. (data: 1.83e+00). ETA=17:43:46, max mem: 20.9 GB 
[12/06 01:54:49 visual_prompt]: 	Training 400/553. train loss: 0.7668,	0.8218 s / batch. (data: 3.70e-04). ETA=5:27:47, max mem: 20.9 GB 
[12/06 01:56:40 visual_prompt]: 	Training 500/553. train loss: 0.7879,	0.8405 s / batch. (data: 3.05e-04). ETA=5:33:51, max mem: 20.9 GB 
[12/06 01:57:40 visual_prompt]: Epoch 57 / 100: avg data time: 3.02e-01, avg batch time: 1.1348, average train loss: 0.7076
[12/06 01:58:46 visual_prompt]: Inference (val):avg data time: 6.46e-04, avg batch time: 0.3111, average loss: 0.7448
[12/06 01:58:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.10	
[12/06 01:58:46 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.04651217631279374
[12/06 02:00:45 visual_prompt]: 	Training 100/553. train loss: 0.5669,	0.9356 s / batch. (data: 1.09e-01). ETA=6:09:14, max mem: 20.9 GB 
[12/06 02:02:39 visual_prompt]: 	Training 200/553. train loss: 0.7520,	0.8357 s / batch. (data: 1.08e-02). ETA=5:28:25, max mem: 20.9 GB 
[12/06 02:04:34 visual_prompt]: 	Training 300/553. train loss: 0.6367,	0.8240 s / batch. (data: 3.63e-04). ETA=5:22:26, max mem: 20.9 GB 
[12/06 02:06:26 visual_prompt]: 	Training 400/553. train loss: 0.6933,	0.8320 s / batch. (data: 3.63e-04). ETA=5:24:11, max mem: 20.9 GB 
[12/06 02:08:19 visual_prompt]: 	Training 500/553. train loss: 0.4982,	0.8167 s / batch. (data: 3.60e-04). ETA=5:16:51, max mem: 20.9 GB 
[12/06 02:09:16 visual_prompt]: Epoch 58 / 100: avg data time: 3.06e-01, avg batch time: 1.1381, average train loss: 0.7076
[12/06 02:10:22 visual_prompt]: Inference (val):avg data time: 5.20e-05, avg batch time: 0.3117, average loss: 0.6955
[12/06 02:10:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.05	
[12/06 02:10:22 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.04477357683661734
[12/06 02:12:22 visual_prompt]: 	Training 100/553. train loss: 0.6555,	0.8535 s / batch. (data: 1.33e-02). ETA=5:28:58, max mem: 20.9 GB 
[12/06 02:14:16 visual_prompt]: 	Training 200/553. train loss: 0.5385,	0.8226 s / batch. (data: 6.93e-04). ETA=5:15:41, max mem: 20.9 GB 
[12/06 02:16:07 visual_prompt]: 	Training 300/553. train loss: 0.5627,	0.8480 s / batch. (data: 3.84e-04). ETA=5:24:00, max mem: 20.9 GB 
[12/06 02:17:59 visual_prompt]: 	Training 400/553. train loss: 0.7599,	1.1639 s / batch. (data: 3.30e-01). ETA=7:22:47, max mem: 20.9 GB 
[12/06 02:19:54 visual_prompt]: 	Training 500/553. train loss: 0.7786,	0.8288 s / batch. (data: 8.48e-04). ETA=5:13:55, max mem: 20.9 GB 
[12/06 02:20:51 visual_prompt]: Epoch 59 / 100: avg data time: 3.03e-01, avg batch time: 1.1371, average train loss: 0.7090
[12/06 02:21:58 visual_prompt]: Inference (val):avg data time: 6.32e-05, avg batch time: 0.3100, average loss: 0.6876
[12/06 02:21:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 63.90	
[12/06 02:21:58 visual_prompt]: Best epoch 59: best metric: -0.688
[12/06 02:21:58 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.04304134495199674
[12/06 02:23:56 visual_prompt]: 	Training 100/553. train loss: 0.8796,	0.8280 s / batch. (data: 3.96e-04). ETA=5:11:29, max mem: 20.9 GB 
[12/06 02:25:49 visual_prompt]: 	Training 200/553. train loss: 0.7007,	0.8481 s / batch. (data: 9.06e-03). ETA=5:17:38, max mem: 20.9 GB 
[12/06 02:27:40 visual_prompt]: 	Training 300/553. train loss: 0.4380,	2.8798 s / batch. (data: 2.06e+00). ETA=17:53:50, max mem: 20.9 GB 
[12/06 02:29:34 visual_prompt]: 	Training 400/553. train loss: 0.8006,	1.3440 s / batch. (data: 5.24e-01). ETA=8:18:55, max mem: 20.9 GB 
[12/06 02:31:28 visual_prompt]: 	Training 500/553. train loss: 0.6435,	0.8309 s / batch. (data: 1.56e-02). ETA=5:07:03, max mem: 20.9 GB 
[12/06 02:32:26 visual_prompt]: Epoch 60 / 100: avg data time: 3.04e-01, avg batch time: 1.1364, average train loss: 0.7152
[12/06 02:33:32 visual_prompt]: Inference (val):avg data time: 5.82e-05, avg batch time: 0.3099, average loss: 0.6788
[12/06 02:33:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 67.44	
[12/06 02:33:32 visual_prompt]: Best epoch 60: best metric: -0.679
[12/06 02:33:32 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.041317591116653486
[12/06 02:35:31 visual_prompt]: 	Training 100/553. train loss: 0.6703,	0.8160 s / batch. (data: 3.61e-04). ETA=4:59:28, max mem: 20.9 GB 
[12/06 02:37:26 visual_prompt]: 	Training 200/553. train loss: 0.7674,	2.2083 s / batch. (data: 1.39e+00). ETA=13:26:45, max mem: 20.9 GB 
[12/06 02:39:19 visual_prompt]: 	Training 300/553. train loss: 0.8071,	1.7129 s / batch. (data: 8.71e-01). ETA=10:22:55, max mem: 20.9 GB 
[12/06 02:41:08 visual_prompt]: 	Training 400/553. train loss: 0.6790,	0.9432 s / batch. (data: 1.09e-01). ETA=5:41:25, max mem: 20.9 GB 
[12/06 02:43:03 visual_prompt]: 	Training 500/553. train loss: 0.6982,	3.4528 s / batch. (data: 2.62e+00). ETA=20:44:09, max mem: 20.9 GB 
[12/06 02:44:00 visual_prompt]: Epoch 61 / 100: avg data time: 3.02e-01, avg batch time: 1.1340, average train loss: 0.7039
[12/06 02:45:05 visual_prompt]: Inference (val):avg data time: 3.70e-04, avg batch time: 0.3099, average loss: 0.6965
[12/06 02:45:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.82	
[12/06 02:45:05 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.039604415459112044
[12/06 02:47:05 visual_prompt]: 	Training 100/553. train loss: 0.7193,	0.8484 s / batch. (data: 1.89e-03). ETA=5:03:31, max mem: 20.9 GB 
[12/06 02:48:56 visual_prompt]: 	Training 200/553. train loss: 0.7465,	0.8398 s / batch. (data: 6.79e-04). ETA=4:59:04, max mem: 20.9 GB 
[12/06 02:50:47 visual_prompt]: 	Training 300/553. train loss: 0.6063,	0.8320 s / batch. (data: 7.93e-03). ETA=4:54:53, max mem: 20.9 GB 
[12/06 02:52:42 visual_prompt]: 	Training 400/553. train loss: 0.6456,	0.8306 s / batch. (data: 5.74e-03). ETA=4:53:01, max mem: 20.9 GB 
[12/06 02:54:33 visual_prompt]: 	Training 500/553. train loss: 0.6953,	0.8405 s / batch. (data: 3.47e-04). ETA=4:55:06, max mem: 20.9 GB 
[12/06 02:55:33 visual_prompt]: Epoch 62 / 100: avg data time: 3.03e-01, avg batch time: 1.1354, average train loss: 0.7073
[12/06 02:56:39 visual_prompt]: Inference (val):avg data time: 2.79e-04, avg batch time: 0.3110, average loss: 0.6878
[12/06 02:56:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.84	
[12/06 02:56:39 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.037903905220016625
[12/06 02:58:43 visual_prompt]: 	Training 100/553. train loss: 0.6921,	0.8479 s / batch. (data: 1.19e-02). ETA=4:55:33, max mem: 20.9 GB 
[12/06 03:00:40 visual_prompt]: 	Training 200/553. train loss: 0.7003,	0.8499 s / batch. (data: 5.49e-03). ETA=4:54:50, max mem: 20.9 GB 
[12/06 03:02:31 visual_prompt]: 	Training 300/553. train loss: 0.6771,	0.9001 s / batch. (data: 1.21e-02). ETA=5:10:44, max mem: 20.9 GB 
[12/06 03:04:19 visual_prompt]: 	Training 400/553. train loss: 0.6100,	0.8288 s / batch. (data: 3.62e-04). ETA=4:44:43, max mem: 20.9 GB 
[12/06 03:06:10 visual_prompt]: 	Training 500/553. train loss: 0.5255,	0.8480 s / batch. (data: 3.28e-04). ETA=4:49:56, max mem: 20.9 GB 
[12/06 03:07:08 visual_prompt]: Epoch 63 / 100: avg data time: 3.03e-01, avg batch time: 1.1366, average train loss: 0.7010
[12/06 03:08:13 visual_prompt]: Inference (val):avg data time: 5.90e-05, avg batch time: 0.3096, average loss: 0.6911
[12/06 03:08:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.62	
[12/06 03:08:13 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.036218132209150045
[12/06 03:10:14 visual_prompt]: 	Training 100/553. train loss: 0.8437,	0.8564 s / batch. (data: 8.60e-04). ETA=4:50:36, max mem: 20.9 GB 
[12/06 03:12:09 visual_prompt]: 	Training 200/553. train loss: 0.6842,	0.8440 s / batch. (data: 1.05e-02). ETA=4:44:59, max mem: 20.9 GB 
[12/06 03:13:59 visual_prompt]: 	Training 300/553. train loss: 0.5233,	0.8440 s / batch. (data: 3.55e-04). ETA=4:43:35, max mem: 20.9 GB 
[12/06 03:15:51 visual_prompt]: 	Training 400/553. train loss: 0.6871,	1.2326 s / batch. (data: 3.99e-01). ETA=6:52:06, max mem: 20.9 GB 
[12/06 03:17:44 visual_prompt]: 	Training 500/553. train loss: 0.5603,	0.8573 s / batch. (data: 5.43e-03). ETA=4:45:11, max mem: 20.9 GB 
[12/06 03:18:43 visual_prompt]: Epoch 64 / 100: avg data time: 3.04e-01, avg batch time: 1.1374, average train loss: 0.7083
[12/06 03:19:49 visual_prompt]: Inference (val):avg data time: 5.91e-05, avg batch time: 0.3114, average loss: 0.6886
[12/06 03:19:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.79	
[12/06 03:19:49 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.03454915028125263
[12/06 03:21:51 visual_prompt]: 	Training 100/553. train loss: 0.7370,	1.1179 s / batch. (data: 3.04e-01). ETA=6:09:03, max mem: 20.9 GB 
[12/06 03:23:45 visual_prompt]: 	Training 200/553. train loss: 0.9248,	2.0480 s / batch. (data: 1.21e+00). ETA=11:12:41, max mem: 20.9 GB 
[12/06 03:25:36 visual_prompt]: 	Training 300/553. train loss: 0.7521,	2.0277 s / batch. (data: 1.18e+00). ETA=11:02:38, max mem: 20.9 GB 
[12/06 03:27:27 visual_prompt]: 	Training 400/553. train loss: 0.7131,	0.8507 s / batch. (data: 1.19e-02). ETA=4:36:35, max mem: 20.9 GB 
[12/06 03:29:21 visual_prompt]: 	Training 500/553. train loss: 0.7658,	0.8194 s / batch. (data: 3.31e-04). ETA=4:25:03, max mem: 20.9 GB 
[12/06 03:30:17 visual_prompt]: Epoch 65 / 100: avg data time: 3.02e-01, avg batch time: 1.1348, average train loss: 0.7045
[12/06 03:31:24 visual_prompt]: Inference (val):avg data time: 6.58e-05, avg batch time: 0.3113, average loss: 0.6945
[12/06 03:31:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.11	
[12/06 03:31:24 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.03289899283371657
[12/06 03:33:21 visual_prompt]: 	Training 100/553. train loss: 0.6853,	0.8152 s / batch. (data: 6.34e-04). ETA=4:21:36, max mem: 20.9 GB 
[12/06 03:35:15 visual_prompt]: 	Training 200/553. train loss: 0.5670,	2.0293 s / batch. (data: 1.22e+00). ETA=10:47:51, max mem: 20.9 GB 
[12/06 03:37:11 visual_prompt]: 	Training 300/553. train loss: 0.7208,	0.8336 s / batch. (data: 8.29e-04). ETA=4:24:43, max mem: 20.9 GB 
[12/06 03:39:02 visual_prompt]: 	Training 400/553. train loss: 0.5719,	0.8501 s / batch. (data: 5.45e-03). ETA=4:28:33, max mem: 20.9 GB 
[12/06 03:40:53 visual_prompt]: 	Training 500/553. train loss: 0.6654,	0.8471 s / batch. (data: 2.57e-02). ETA=4:26:12, max mem: 20.9 GB 
[12/06 03:41:53 visual_prompt]: Epoch 66 / 100: avg data time: 3.06e-01, avg batch time: 1.1383, average train loss: 0.7026
[12/06 03:43:00 visual_prompt]: Inference (val):avg data time: 6.51e-05, avg batch time: 0.3114, average loss: 0.6973
[12/06 03:43:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.93	
[12/06 03:43:00 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0312696703292044
[12/06 03:45:00 visual_prompt]: 	Training 100/553. train loss: 0.5923,	0.8810 s / batch. (data: 5.08e-02). ETA=4:34:35, max mem: 20.9 GB 
[12/06 03:46:54 visual_prompt]: 	Training 200/553. train loss: 0.4781,	0.8229 s / batch. (data: 5.35e-04). ETA=4:15:08, max mem: 20.9 GB 
[12/06 03:48:44 visual_prompt]: 	Training 300/553. train loss: 0.6979,	0.8371 s / batch. (data: 6.00e-03). ETA=4:18:07, max mem: 20.9 GB 
[12/06 03:50:32 visual_prompt]: 	Training 400/553. train loss: 0.7418,	0.8240 s / batch. (data: 3.39e-04). ETA=4:12:43, max mem: 20.9 GB 
[12/06 03:52:27 visual_prompt]: 	Training 500/553. train loss: 0.6958,	1.1925 s / batch. (data: 3.75e-01). ETA=6:03:44, max mem: 20.9 GB 
[12/06 03:53:28 visual_prompt]: Epoch 67 / 100: avg data time: 3.02e-01, avg batch time: 1.1344, average train loss: 0.6985
[12/06 03:54:34 visual_prompt]: Inference (val):avg data time: 5.88e-05, avg batch time: 0.3105, average loss: 0.6805
[12/06 03:54:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.88	
[12/06 03:54:34 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.02966316784621
[12/06 03:56:32 visual_prompt]: 	Training 100/553. train loss: 0.7141,	0.8295 s / batch. (data: 4.10e-04). ETA=4:10:53, max mem: 20.9 GB 
[12/06 03:58:28 visual_prompt]: 	Training 200/553. train loss: 0.6907,	1.7039 s / batch. (data: 8.76e-01). ETA=8:32:32, max mem: 20.9 GB 
[12/06 04:00:18 visual_prompt]: 	Training 300/553. train loss: 0.6774,	0.8576 s / batch. (data: 3.87e-04). ETA=4:16:32, max mem: 20.9 GB 
[12/06 04:02:09 visual_prompt]: 	Training 400/553. train loss: 0.6584,	0.8729 s / batch. (data: 3.29e-02). ETA=4:19:41, max mem: 20.9 GB 
[12/06 04:04:06 visual_prompt]: 	Training 500/553. train loss: 0.6950,	0.8256 s / batch. (data: 7.58e-04). ETA=4:04:13, max mem: 20.9 GB 
[12/06 04:05:12 visual_prompt]: Epoch 68 / 100: avg data time: 3.20e-01, avg batch time: 1.1524, average train loss: 0.7021
[12/06 04:06:22 visual_prompt]: Inference (val):avg data time: 6.00e-05, avg batch time: 0.3114, average loss: 0.6763
[12/06 04:06:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.34	
[12/06 04:06:22 visual_prompt]: Best epoch 68: best metric: -0.676
[12/06 04:06:22 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.028081442660546126
[12/06 04:08:20 visual_prompt]: 	Training 100/553. train loss: 0.7092,	0.8319 s / batch. (data: 4.91e-04). ETA=4:03:58, max mem: 20.9 GB 
[12/06 04:10:16 visual_prompt]: 	Training 200/553. train loss: 0.7849,	0.8407 s / batch. (data: 7.93e-03). ETA=4:05:08, max mem: 20.9 GB 
[12/06 04:12:09 visual_prompt]: 	Training 300/553. train loss: 0.5754,	0.8180 s / batch. (data: 3.27e-04). ETA=3:57:10, max mem: 20.9 GB 
[12/06 04:14:04 visual_prompt]: 	Training 400/553. train loss: 0.6244,	0.8387 s / batch. (data: 8.93e-04). ETA=4:01:46, max mem: 20.9 GB 
[12/06 04:15:57 visual_prompt]: 	Training 500/553. train loss: 0.9121,	0.8425 s / batch. (data: 7.90e-03). ETA=4:01:27, max mem: 20.9 GB 
[12/06 04:16:56 visual_prompt]: Epoch 69 / 100: avg data time: 3.14e-01, avg batch time: 1.1458, average train loss: 0.6989
[12/06 04:18:03 visual_prompt]: Inference (val):avg data time: 6.29e-05, avg batch time: 0.3105, average loss: 0.6932
[12/06 04:18:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.10	
[12/06 04:18:03 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.026526421860705474
[12/06 04:20:00 visual_prompt]: 	Training 100/553. train loss: 0.6286,	0.8150 s / batch. (data: 3.66e-04). ETA=3:51:30, max mem: 20.9 GB 
[12/06 04:21:55 visual_prompt]: 	Training 200/553. train loss: 0.8597,	1.5440 s / batch. (data: 6.81e-01). ETA=7:16:00, max mem: 20.9 GB 
[12/06 04:23:48 visual_prompt]: 	Training 300/553. train loss: 0.7000,	1.9116 s / batch. (data: 1.04e+00). ETA=8:56:37, max mem: 20.9 GB 
[12/06 04:25:42 visual_prompt]: 	Training 400/553. train loss: 0.6963,	1.4369 s / batch. (data: 6.07e-01). ETA=6:40:57, max mem: 20.9 GB 
[12/06 04:27:33 visual_prompt]: 	Training 500/553. train loss: 0.7620,	0.8282 s / batch. (data: 1.19e-02). ETA=3:49:43, max mem: 20.9 GB 
[12/06 04:28:33 visual_prompt]: Epoch 70 / 100: avg data time: 3.05e-01, avg batch time: 1.1378, average train loss: 0.6949
[12/06 04:29:39 visual_prompt]: Inference (val):avg data time: 6.42e-05, avg batch time: 0.3110, average loss: 0.6806
[12/06 04:29:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 64.64	
[12/06 04:29:39 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.025000000000000012
[12/06 04:31:38 visual_prompt]: 	Training 100/553. train loss: 0.7036,	0.8337 s / batch. (data: 8.17e-04). ETA=3:49:08, max mem: 20.9 GB 
[12/06 04:33:31 visual_prompt]: 	Training 200/553. train loss: 0.6958,	0.8547 s / batch. (data: 9.86e-04). ETA=3:53:27, max mem: 20.9 GB 
[12/06 04:35:26 visual_prompt]: 	Training 300/553. train loss: 0.9983,	0.8384 s / batch. (data: 9.19e-03). ETA=3:47:37, max mem: 20.9 GB 
[12/06 04:37:15 visual_prompt]: 	Training 400/553. train loss: 0.6709,	0.8438 s / batch. (data: 5.85e-03). ETA=3:47:41, max mem: 20.9 GB 
[12/06 04:39:08 visual_prompt]: 	Training 500/553. train loss: 0.7471,	0.8220 s / batch. (data: 5.46e-04). ETA=3:40:26, max mem: 20.9 GB 
[12/06 04:40:07 visual_prompt]: Epoch 71 / 100: avg data time: 3.03e-01, avg batch time: 1.1358, average train loss: 0.6908
[12/06 04:41:14 visual_prompt]: Inference (val):avg data time: 6.84e-05, avg batch time: 0.3096, average loss: 0.6776
[12/06 04:41:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 61.35	
[12/06 04:41:14 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.02350403678833976
[12/06 04:43:15 visual_prompt]: 	Training 100/553. train loss: 0.8574,	1.2770 s / batch. (data: 4.57e-01). ETA=5:39:11, max mem: 20.9 GB 
[12/06 04:45:06 visual_prompt]: 	Training 200/553. train loss: 0.6592,	0.8362 s / batch. (data: 3.28e-04). ETA=3:40:42, max mem: 20.9 GB 
[12/06 04:47:01 visual_prompt]: 	Training 300/553. train loss: 0.7043,	0.8534 s / batch. (data: 1.18e-02). ETA=3:43:49, max mem: 20.9 GB 
[12/06 04:48:54 visual_prompt]: 	Training 400/553. train loss: 0.7173,	2.7403 s / batch. (data: 1.93e+00). ETA=11:54:09, max mem: 20.9 GB 
[12/06 04:50:42 visual_prompt]: 	Training 500/553. train loss: 0.7813,	1.4681 s / batch. (data: 6.24e-01). ETA=6:20:10, max mem: 20.9 GB 
[12/06 04:51:44 visual_prompt]: Epoch 72 / 100: avg data time: 3.05e-01, avg batch time: 1.1389, average train loss: 0.6919
[12/06 04:52:50 visual_prompt]: Inference (val):avg data time: 2.96e-04, avg batch time: 0.3119, average loss: 0.6902
[12/06 04:52:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 56.57	
[12/06 04:52:50 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.022040354826462667
[12/06 04:54:49 visual_prompt]: 	Training 100/553. train loss: 0.9547,	0.8280 s / batch. (data: 6.20e-04). ETA=3:32:17, max mem: 20.9 GB 
[12/06 04:56:46 visual_prompt]: 	Training 200/553. train loss: 0.7600,	0.8336 s / batch. (data: 3.84e-04). ETA=3:32:20, max mem: 20.9 GB 
[12/06 04:58:35 visual_prompt]: 	Training 300/553. train loss: 0.6991,	0.8457 s / batch. (data: 3.48e-04). ETA=3:34:00, max mem: 20.9 GB 
[12/06 05:00:30 visual_prompt]: 	Training 400/553. train loss: 0.6919,	0.8283 s / batch. (data: 3.37e-04). ETA=3:28:13, max mem: 20.9 GB 
[12/06 05:02:23 visual_prompt]: 	Training 500/553. train loss: 0.6324,	0.8289 s / batch. (data: 1.00e-03). ETA=3:27:00, max mem: 20.9 GB 
[12/06 05:03:22 visual_prompt]: Epoch 73 / 100: avg data time: 3.09e-01, avg batch time: 1.1418, average train loss: 0.6958
[12/06 05:04:28 visual_prompt]: Inference (val):avg data time: 6.06e-04, avg batch time: 0.3108, average loss: 0.7087
[12/06 05:04:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.20	
[12/06 05:04:28 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.02061073738537635
[12/06 05:06:30 visual_prompt]: 	Training 100/553. train loss: 0.7448,	0.8463 s / batch. (data: 1.06e-02). ETA=3:29:11, max mem: 20.9 GB 
[12/06 05:08:25 visual_prompt]: 	Training 200/553. train loss: 0.7212,	0.8200 s / batch. (data: 3.28e-04). ETA=3:21:19, max mem: 20.9 GB 
[12/06 05:10:18 visual_prompt]: 	Training 300/553. train loss: 0.6140,	0.8210 s / batch. (data: 1.37e-03). ETA=3:20:11, max mem: 20.9 GB 
[12/06 05:12:10 visual_prompt]: 	Training 400/553. train loss: 0.6774,	1.4021 s / batch. (data: 5.79e-01). ETA=5:39:34, max mem: 20.9 GB 
[12/06 05:14:01 visual_prompt]: 	Training 500/553. train loss: 0.6174,	2.3055 s / batch. (data: 1.48e+00). ETA=9:14:30, max mem: 20.9 GB 
[12/06 05:15:00 visual_prompt]: Epoch 74 / 100: avg data time: 3.11e-01, avg batch time: 1.1427, average train loss: 0.6879
[12/06 05:16:07 visual_prompt]: Inference (val):avg data time: 6.34e-05, avg batch time: 0.3103, average loss: 0.7188
[12/06 05:16:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.02	
[12/06 05:16:07 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.019216926233717086
[12/06 05:18:07 visual_prompt]: 	Training 100/553. train loss: 0.7188,	2.2841 s / batch. (data: 1.47e+00). ETA=9:03:32, max mem: 20.9 GB 
[12/06 05:19:59 visual_prompt]: 	Training 200/553. train loss: 0.8529,	0.8760 s / batch. (data: 3.04e-02). ETA=3:26:59, max mem: 20.9 GB 
[12/06 05:21:51 visual_prompt]: 	Training 300/553. train loss: 0.9304,	0.8634 s / batch. (data: 1.14e-02). ETA=3:22:35, max mem: 20.9 GB 
[12/06 05:23:47 visual_prompt]: 	Training 400/553. train loss: 0.9307,	2.9647 s / batch. (data: 2.15e+00). ETA=11:30:40, max mem: 20.9 GB 
[12/06 05:25:38 visual_prompt]: 	Training 500/553. train loss: 0.6899,	0.8334 s / batch. (data: 1.71e-02). ETA=3:12:46, max mem: 20.9 GB 
[12/06 05:26:37 visual_prompt]: Epoch 75 / 100: avg data time: 3.07e-01, avg batch time: 1.1399, average train loss: 0.6735
[12/06 05:27:44 visual_prompt]: Inference (val):avg data time: 5.43e-05, avg batch time: 0.3122, average loss: 0.6614
[12/06 05:27:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 67.96	
[12/06 05:27:44 visual_prompt]: Best epoch 75: best metric: -0.661
[12/06 05:27:44 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.017860619515673033
[12/06 05:29:46 visual_prompt]: 	Training 100/553. train loss: 0.7565,	0.8639 s / batch. (data: 1.58e-03). ETA=3:17:36, max mem: 20.9 GB 
[12/06 05:31:38 visual_prompt]: 	Training 200/553. train loss: 0.6462,	1.3676 s / batch. (data: 5.25e-01). ETA=5:10:33, max mem: 20.9 GB 
[12/06 05:33:29 visual_prompt]: 	Training 300/553. train loss: 0.6715,	0.8600 s / batch. (data: 3.50e-04). ETA=3:13:51, max mem: 20.9 GB 
[12/06 05:35:20 visual_prompt]: 	Training 400/553. train loss: 0.6525,	0.8264 s / batch. (data: 6.41e-04). ETA=3:04:54, max mem: 20.9 GB 
[12/06 05:37:12 visual_prompt]: 	Training 500/553. train loss: 0.6854,	0.8289 s / batch. (data: 3.88e-04). ETA=3:04:04, max mem: 20.9 GB 
[12/06 05:38:12 visual_prompt]: Epoch 76 / 100: avg data time: 3.04e-01, avg batch time: 1.1362, average train loss: 0.6777
[12/06 05:39:18 visual_prompt]: Inference (val):avg data time: 5.46e-05, avg batch time: 0.3110, average loss: 0.6805
[12/06 05:39:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.30	
[12/06 05:39:18 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.016543469682057107
[12/06 05:41:16 visual_prompt]: 	Training 100/553. train loss: 0.6712,	1.0605 s / batch. (data: 2.15e-01). ETA=3:52:49, max mem: 20.9 GB 
[12/06 05:43:12 visual_prompt]: 	Training 200/553. train loss: 0.7635,	0.8362 s / batch. (data: 4.90e-04). ETA=3:02:10, max mem: 20.9 GB 
[12/06 05:45:02 visual_prompt]: 	Training 300/553. train loss: 0.4972,	0.8444 s / batch. (data: 1.30e-03). ETA=3:02:34, max mem: 20.9 GB 
[12/06 05:46:55 visual_prompt]: 	Training 400/553. train loss: 0.5201,	0.8380 s / batch. (data: 3.35e-04). ETA=2:59:47, max mem: 20.9 GB 
[12/06 05:48:48 visual_prompt]: 	Training 500/553. train loss: 0.5457,	0.8481 s / batch. (data: 1.20e-02). ETA=3:00:32, max mem: 20.9 GB 
[12/06 05:49:47 visual_prompt]: Epoch 77 / 100: avg data time: 3.03e-01, avg batch time: 1.1359, average train loss: 0.6717
[12/06 05:50:52 visual_prompt]: Inference (val):avg data time: 5.29e-05, avg batch time: 0.3129, average loss: 0.6536
[12/06 05:50:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 65.37	
[12/06 05:50:52 visual_prompt]: Best epoch 77: best metric: -0.654
[12/06 05:50:52 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.015267081477050132
[12/06 05:52:51 visual_prompt]: 	Training 100/553. train loss: 0.5959,	0.8514 s / batch. (data: 6.97e-03). ETA=2:59:03, max mem: 20.9 GB 
[12/06 05:54:44 visual_prompt]: 	Training 200/553. train loss: 0.7070,	0.8576 s / batch. (data: 9.70e-03). ETA=2:58:56, max mem: 20.9 GB 
[12/06 05:56:36 visual_prompt]: 	Training 300/553. train loss: 1.0342,	0.8319 s / batch. (data: 5.99e-03). ETA=2:52:11, max mem: 20.9 GB 
[12/06 05:58:29 visual_prompt]: 	Training 400/553. train loss: 0.7616,	0.8321 s / batch. (data: 3.18e-04). ETA=2:50:50, max mem: 20.9 GB 
[12/06 06:00:21 visual_prompt]: 	Training 500/553. train loss: 0.7834,	0.8459 s / batch. (data: 7.97e-03). ETA=2:52:15, max mem: 20.9 GB 
[12/06 06:01:21 visual_prompt]: Epoch 78 / 100: avg data time: 3.03e-01, avg batch time: 1.1361, average train loss: 0.6668
[12/06 06:02:27 visual_prompt]: Inference (val):avg data time: 5.90e-05, avg batch time: 0.3103, average loss: 0.6347
[12/06 06:02:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 67.74	
[12/06 06:02:27 visual_prompt]: Best epoch 78: best metric: -0.635
[12/06 06:02:27 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.014033009983067453
[12/06 06:04:27 visual_prompt]: 	Training 100/553. train loss: 0.6211,	0.8367 s / batch. (data: 7.75e-03). ETA=2:48:15, max mem: 20.9 GB 
[12/06 06:06:19 visual_prompt]: 	Training 200/553. train loss: 0.7305,	0.8260 s / batch. (data: 3.93e-04). ETA=2:44:44, max mem: 20.9 GB 
[12/06 06:08:08 visual_prompt]: 	Training 300/553. train loss: 0.7718,	1.1640 s / batch. (data: 3.33e-01). ETA=3:50:11, max mem: 20.9 GB 
[12/06 06:10:05 visual_prompt]: 	Training 400/553. train loss: 0.4233,	0.8456 s / batch. (data: 9.29e-03). ETA=2:45:49, max mem: 20.9 GB 
[12/06 06:11:59 visual_prompt]: 	Training 500/553. train loss: 0.9876,	0.8358 s / batch. (data: 9.08e-04). ETA=2:42:30, max mem: 20.9 GB 
[12/06 06:12:56 visual_prompt]: Epoch 79 / 100: avg data time: 3.05e-01, avg batch time: 1.1374, average train loss: 0.6712
[12/06 06:14:02 visual_prompt]: Inference (val):avg data time: 2.05e-04, avg batch time: 0.3117, average loss: 0.6835
[12/06 06:14:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 67.01	
[12/06 06:14:02 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.012842758726130284
[12/06 06:16:00 visual_prompt]: 	Training 100/553. train loss: 0.5097,	0.8305 s / batch. (data: 7.88e-03). ETA=2:39:21, max mem: 20.9 GB 
[12/06 06:17:53 visual_prompt]: 	Training 200/553. train loss: 0.4569,	0.8451 s / batch. (data: 7.85e-03). ETA=2:40:45, max mem: 20.9 GB 
[12/06 06:19:45 visual_prompt]: 	Training 300/553. train loss: 0.7191,	1.8309 s / batch. (data: 9.97e-01). ETA=5:45:13, max mem: 20.9 GB 
[12/06 06:21:41 visual_prompt]: 	Training 400/553. train loss: 0.7242,	0.8402 s / batch. (data: 7.95e-03). ETA=2:37:01, max mem: 20.9 GB 
[12/06 06:23:33 visual_prompt]: 	Training 500/553. train loss: 0.6859,	0.8243 s / batch. (data: 7.96e-03). ETA=2:32:40, max mem: 20.9 GB 
[12/06 06:24:32 visual_prompt]: Epoch 80 / 100: avg data time: 3.06e-01, avg batch time: 1.1396, average train loss: 0.6637
[12/06 06:25:39 visual_prompt]: Inference (val):avg data time: 6.48e-05, avg batch time: 0.3104, average loss: 0.6688
[12/06 06:25:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 67.25	
[12/06 06:25:39 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.011697777844051106
[12/06 06:27:38 visual_prompt]: 	Training 100/553. train loss: 0.8274,	0.8552 s / batch. (data: 1.16e-03). ETA=2:36:12, max mem: 20.9 GB 
[12/06 06:29:32 visual_prompt]: 	Training 200/553. train loss: 0.6060,	0.8553 s / batch. (data: 1.10e-02). ETA=2:34:48, max mem: 20.9 GB 
[12/06 06:31:23 visual_prompt]: 	Training 300/553. train loss: 0.3658,	0.8468 s / batch. (data: 6.82e-03). ETA=2:31:51, max mem: 20.9 GB 
[12/06 06:33:17 visual_prompt]: 	Training 400/553. train loss: 0.7089,	1.8191 s / batch. (data: 9.97e-01). ETA=5:23:11, max mem: 20.9 GB 
[12/06 06:35:08 visual_prompt]: 	Training 500/553. train loss: 0.8571,	2.6905 s / batch. (data: 1.85e+00). ETA=7:53:31, max mem: 20.9 GB 
[12/06 06:36:07 visual_prompt]: Epoch 81 / 100: avg data time: 3.02e-01, avg batch time: 1.1351, average train loss: 0.6597
[12/06 06:37:13 visual_prompt]: Inference (val):avg data time: 5.35e-05, avg batch time: 0.3113, average loss: 0.6753
[12/06 06:37:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 66.01	
[12/06 06:37:13 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.010599462319663905
[12/06 06:39:10 visual_prompt]: 	Training 100/553. train loss: 0.4420,	0.8187 s / batch. (data: 8.96e-04). ETA=2:22:00, max mem: 20.9 GB 
[12/06 06:41:06 visual_prompt]: 	Training 200/553. train loss: 0.8722,	1.0150 s / batch. (data: 1.91e-01). ETA=2:54:21, max mem: 20.9 GB 
[12/06 06:42:58 visual_prompt]: 	Training 300/553. train loss: 0.8388,	2.5465 s / batch. (data: 1.73e+00). ETA=7:13:12, max mem: 20.9 GB 
[12/06 06:44:48 visual_prompt]: 	Training 400/553. train loss: 0.4906,	1.9634 s / batch. (data: 1.14e+00). ETA=5:30:43, max mem: 20.9 GB 
[12/06 06:46:43 visual_prompt]: 	Training 500/553. train loss: 0.5209,	0.8374 s / batch. (data: 2.21e-02). ETA=2:19:39, max mem: 20.9 GB 
[12/06 06:47:41 visual_prompt]: Epoch 82 / 100: avg data time: 3.03e-01, avg batch time: 1.1356, average train loss: 0.6531
[12/06 06:48:47 visual_prompt]: Inference (val):avg data time: 2.36e-04, avg batch time: 0.3106, average loss: 0.6501
[12/06 06:48:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 67.07	
[12/06 06:48:47 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.009549150281252633
[12/06 06:50:47 visual_prompt]: 	Training 100/553. train loss: 0.8999,	0.8333 s / batch. (data: 5.47e-03). ETA=2:16:51, max mem: 20.9 GB 
[12/06 06:52:42 visual_prompt]: 	Training 200/553. train loss: 0.5230,	0.8308 s / batch. (data: 3.59e-04). ETA=2:15:03, max mem: 20.9 GB 
[12/06 06:54:34 visual_prompt]: 	Training 300/553. train loss: 0.5984,	0.8347 s / batch. (data: 3.39e-04). ETA=2:14:18, max mem: 20.9 GB 
[12/06 06:56:28 visual_prompt]: 	Training 400/553. train loss: 0.6221,	0.8570 s / batch. (data: 1.06e-02). ETA=2:16:28, max mem: 20.9 GB 
[12/06 06:58:22 visual_prompt]: 	Training 500/553. train loss: 0.7929,	0.8450 s / batch. (data: 6.15e-03). ETA=2:13:08, max mem: 20.9 GB 
[12/06 06:59:19 visual_prompt]: Epoch 83 / 100: avg data time: 3.09e-01, avg batch time: 1.1411, average train loss: 0.6471
[12/06 07:00:25 visual_prompt]: Inference (val):avg data time: 5.53e-05, avg batch time: 0.3107, average loss: 0.6321
[12/06 07:00:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 69.66	
[12/06 07:00:25 visual_prompt]: Best epoch 83: best metric: -0.632
[12/06 07:00:25 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.008548121372247919
[12/06 07:02:23 visual_prompt]: 	Training 100/553. train loss: 0.5415,	0.8587 s / batch. (data: 5.56e-03). ETA=2:13:06, max mem: 20.9 GB 
[12/06 07:04:15 visual_prompt]: 	Training 200/553. train loss: 0.9942,	2.0513 s / batch. (data: 1.23e+00). ETA=5:14:33, max mem: 20.9 GB 
[12/06 07:06:08 visual_prompt]: 	Training 300/553. train loss: 0.7721,	0.8593 s / batch. (data: 2.73e-02). ETA=2:10:20, max mem: 20.9 GB 
[12/06 07:08:03 visual_prompt]: 	Training 400/553. train loss: 0.8016,	0.8448 s / batch. (data: 8.74e-03). ETA=2:06:43, max mem: 20.9 GB 
[12/06 07:09:52 visual_prompt]: 	Training 500/553. train loss: 0.3589,	0.8848 s / batch. (data: 4.57e-02). ETA=2:11:15, max mem: 20.9 GB 
[12/06 07:10:54 visual_prompt]: Epoch 84 / 100: avg data time: 3.05e-01, avg batch time: 1.1381, average train loss: 0.6462
[12/06 07:12:00 visual_prompt]: Inference (val):avg data time: 5.55e-05, avg batch time: 0.3101, average loss: 0.6240
[12/06 07:12:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.07	rocauc: 70.71	
[12/06 07:12:00 visual_prompt]: Best epoch 84: best metric: -0.624
[12/06 07:12:00 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.007597595192178702
[12/06 07:14:01 visual_prompt]: 	Training 100/553. train loss: 0.6973,	3.3273 s / batch. (data: 2.50e+00). ETA=8:05:07, max mem: 20.9 GB 
[12/06 07:15:53 visual_prompt]: 	Training 200/553. train loss: 0.7142,	0.8369 s / batch. (data: 5.44e-03). ETA=2:00:37, max mem: 20.9 GB 
[12/06 07:17:46 visual_prompt]: 	Training 300/553. train loss: 0.5040,	0.8413 s / batch. (data: 1.62e-02). ETA=1:59:51, max mem: 20.9 GB 
[12/06 07:19:38 visual_prompt]: 	Training 400/553. train loss: 0.8871,	0.8552 s / batch. (data: 5.48e-03). ETA=2:00:24, max mem: 20.9 GB 
[12/06 07:21:32 visual_prompt]: 	Training 500/553. train loss: 0.6254,	0.8681 s / batch. (data: 2.07e-02). ETA=2:00:46, max mem: 20.9 GB 
[12/06 07:22:29 visual_prompt]: Epoch 85 / 100: avg data time: 3.04e-01, avg batch time: 1.1383, average train loss: 0.6350
[12/06 07:23:35 visual_prompt]: Inference (val):avg data time: 5.33e-05, avg batch time: 0.3096, average loss: 0.6468
[12/06 07:23:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 69.48	
[12/06 07:23:35 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.006698729810778065
[12/06 07:25:35 visual_prompt]: 	Training 100/553. train loss: 0.7283,	3.9256 s / batch. (data: 3.09e+00). ETA=8:56:10, max mem: 20.9 GB 
[12/06 07:27:26 visual_prompt]: 	Training 200/553. train loss: 0.6758,	0.8373 s / batch. (data: 3.60e-04). ETA=1:52:57, max mem: 20.9 GB 
[12/06 07:29:17 visual_prompt]: 	Training 300/553. train loss: 0.7184,	0.8337 s / batch. (data: 1.30e-02). ETA=1:51:05, max mem: 20.9 GB 
[12/06 07:31:13 visual_prompt]: 	Training 400/553. train loss: 0.6720,	0.8600 s / batch. (data: 3.97e-04). ETA=1:53:09, max mem: 20.9 GB 
[12/06 07:33:07 visual_prompt]: 	Training 500/553. train loss: 0.4566,	0.8414 s / batch. (data: 2.09e-02). ETA=1:49:18, max mem: 20.9 GB 
[12/06 07:34:08 visual_prompt]: Epoch 86 / 100: avg data time: 3.11e-01, avg batch time: 1.1440, average train loss: 0.6298
[12/06 07:35:14 visual_prompt]: Inference (val):avg data time: 4.74e-05, avg batch time: 0.3111, average loss: 0.6228
[12/06 07:35:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 70.15	
[12/06 07:35:14 visual_prompt]: Best epoch 86: best metric: -0.623
[12/06 07:35:14 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.005852620357053651
[12/06 07:37:15 visual_prompt]: 	Training 100/553. train loss: 0.5607,	0.8440 s / batch. (data: 7.97e-03). ETA=1:47:29, max mem: 20.9 GB 
[12/06 07:39:10 visual_prompt]: 	Training 200/553. train loss: 0.3773,	0.8547 s / batch. (data: 1.47e-02). ETA=1:47:26, max mem: 20.9 GB 
[12/06 07:41:03 visual_prompt]: 	Training 300/553. train loss: 0.7283,	2.0715 s / batch. (data: 1.24e+00). ETA=4:16:55, max mem: 20.9 GB 
[12/06 07:42:54 visual_prompt]: 	Training 400/553. train loss: 0.6299,	0.8360 s / batch. (data: 3.32e-04). ETA=1:42:18, max mem: 20.9 GB 
[12/06 07:44:47 visual_prompt]: 	Training 500/553. train loss: 0.8559,	0.8365 s / batch. (data: 3.34e-04). ETA=1:40:57, max mem: 20.9 GB 
[12/06 07:45:45 visual_prompt]: Epoch 87 / 100: avg data time: 3.09e-01, avg batch time: 1.1413, average train loss: 0.6290
[12/06 07:46:51 visual_prompt]: Inference (val):avg data time: 1.60e-04, avg batch time: 0.3104, average loss: 0.6814
[12/06 07:46:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 70.24	
[12/06 07:46:51 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00506029768504166
[12/06 07:48:48 visual_prompt]: 	Training 100/553. train loss: 0.2185,	0.8249 s / batch. (data: 6.84e-04). ETA=1:37:28, max mem: 20.9 GB 
[12/06 07:50:41 visual_prompt]: 	Training 200/553. train loss: 0.3917,	0.8184 s / batch. (data: 6.50e-04). ETA=1:35:20, max mem: 20.9 GB 
[12/06 07:52:37 visual_prompt]: 	Training 300/553. train loss: 0.2680,	0.8557 s / batch. (data: 1.99e-03). ETA=1:38:14, max mem: 20.9 GB 
[12/06 07:54:34 visual_prompt]: 	Training 400/553. train loss: 0.6254,	2.9481 s / batch. (data: 2.14e+00). ETA=5:33:34, max mem: 20.9 GB 
[12/06 07:56:22 visual_prompt]: 	Training 500/553. train loss: 0.9874,	2.0796 s / batch. (data: 1.26e+00). ETA=3:51:50, max mem: 20.9 GB 
[12/06 07:57:21 visual_prompt]: Epoch 88 / 100: avg data time: 3.06e-01, avg batch time: 1.1387, average train loss: 0.6257
[12/06 07:58:26 visual_prompt]: Inference (val):avg data time: 5.60e-05, avg batch time: 0.3103, average loss: 0.6612
[12/06 07:58:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 71.78	
[12/06 07:58:26 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.004322727117869951
[12/06 08:00:26 visual_prompt]: 	Training 100/553. train loss: 0.6868,	0.8240 s / batch. (data: 3.65e-04). ETA=1:29:45, max mem: 20.9 GB 
[12/06 08:02:18 visual_prompt]: 	Training 200/553. train loss: 0.6921,	0.8371 s / batch. (data: 1.32e-03). ETA=1:29:47, max mem: 20.9 GB 
[12/06 08:04:13 visual_prompt]: 	Training 300/553. train loss: 0.6138,	0.8301 s / batch. (data: 4.30e-04). ETA=1:27:39, max mem: 20.9 GB 
[12/06 08:06:07 visual_prompt]: 	Training 400/553. train loss: 0.7889,	0.8170 s / batch. (data: 7.43e-04). ETA=1:24:54, max mem: 20.9 GB 
[12/06 08:08:00 visual_prompt]: 	Training 500/553. train loss: 0.6582,	0.8528 s / batch. (data: 1.06e-03). ETA=1:27:13, max mem: 20.9 GB 
[12/06 08:08:58 visual_prompt]: Epoch 89 / 100: avg data time: 3.09e-01, avg batch time: 1.1424, average train loss: 0.6135
[12/06 08:10:04 visual_prompt]: Inference (val):avg data time: 6.99e-05, avg batch time: 0.3124, average loss: 0.6239
[12/06 08:10:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 71.79	
[12/06 08:10:04 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0036408072716606343
[12/06 08:12:05 visual_prompt]: 	Training 100/553. train loss: 0.7990,	0.8217 s / batch. (data: 1.11e-03). ETA=1:21:56, max mem: 20.9 GB 
[12/06 08:13:56 visual_prompt]: 	Training 200/553. train loss: 0.5489,	1.1194 s / batch. (data: 3.03e-01). ETA=1:49:45, max mem: 20.9 GB 
[12/06 08:15:49 visual_prompt]: 	Training 300/553. train loss: 1.1753,	0.8280 s / batch. (data: 4.16e-04). ETA=1:19:48, max mem: 20.9 GB 
[12/06 08:17:42 visual_prompt]: 	Training 400/553. train loss: 0.9282,	1.2236 s / batch. (data: 4.11e-01). ETA=1:55:53, max mem: 20.9 GB 
[12/06 08:19:36 visual_prompt]: 	Training 500/553. train loss: 0.8833,	0.8331 s / batch. (data: 1.03e-02). ETA=1:17:31, max mem: 20.9 GB 
[12/06 08:20:33 visual_prompt]: Epoch 90 / 100: avg data time: 3.03e-01, avg batch time: 1.1360, average train loss: 0.6146
[12/06 08:21:38 visual_prompt]: Inference (val):avg data time: 6.22e-05, avg batch time: 0.3104, average loss: 0.6210
[12/06 08:21:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 71.79	
[12/06 08:21:38 visual_prompt]: Best epoch 90: best metric: -0.621
[12/06 08:21:38 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0030153689607045845
[12/06 08:23:37 visual_prompt]: 	Training 100/553. train loss: 0.7936,	0.8403 s / batch. (data: 3.37e-04). ETA=1:16:02, max mem: 20.9 GB 
[12/06 08:25:33 visual_prompt]: 	Training 200/553. train loss: 0.6832,	0.8653 s / batch. (data: 6.47e-03). ETA=1:16:51, max mem: 20.9 GB 
[12/06 08:27:27 visual_prompt]: 	Training 300/553. train loss: 0.9989,	0.8448 s / batch. (data: 5.94e-03). ETA=1:13:38, max mem: 20.9 GB 
[12/06 08:29:23 visual_prompt]: 	Training 400/553. train loss: 0.4355,	2.2919 s / batch. (data: 1.46e+00). ETA=3:15:57, max mem: 20.9 GB 
[12/06 08:31:11 visual_prompt]: 	Training 500/553. train loss: 0.7587,	1.0880 s / batch. (data: 2.52e-01). ETA=1:31:12, max mem: 20.9 GB 
[12/06 08:32:09 visual_prompt]: Epoch 91 / 100: avg data time: 3.07e-01, avg batch time: 1.1400, average train loss: 0.6012
[12/06 08:33:15 visual_prompt]: Inference (val):avg data time: 5.67e-05, avg batch time: 0.3101, average loss: 0.6313
[12/06 08:33:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 71.06	
[12/06 08:33:15 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0024471741852423235
[12/06 08:35:16 visual_prompt]: 	Training 100/553. train loss: 0.4560,	2.0682 s / batch. (data: 1.20e+00). ETA=2:48:06, max mem: 20.9 GB 
[12/06 08:37:09 visual_prompt]: 	Training 200/553. train loss: 0.7297,	0.8317 s / batch. (data: 9.27e-04). ETA=1:06:13, max mem: 20.9 GB 
[12/06 08:39:01 visual_prompt]: 	Training 300/553. train loss: 0.7840,	0.8309 s / batch. (data: 3.39e-04). ETA=1:04:45, max mem: 20.9 GB 
[12/06 08:40:57 visual_prompt]: 	Training 400/553. train loss: 0.6266,	0.8360 s / batch. (data: 3.71e-04). ETA=1:03:46, max mem: 20.9 GB 
[12/06 08:42:49 visual_prompt]: 	Training 500/553. train loss: 0.4842,	0.8314 s / batch. (data: 3.47e-04). ETA=1:02:02, max mem: 20.9 GB 
[12/06 08:43:48 visual_prompt]: Epoch 92 / 100: avg data time: 3.12e-01, avg batch time: 1.1442, average train loss: 0.5935
[12/06 08:44:54 visual_prompt]: Inference (val):avg data time: 5.86e-05, avg batch time: 0.3102, average loss: 0.6178
[12/06 08:44:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 71.51	
[12/06 08:44:54 visual_prompt]: Best epoch 92: best metric: -0.618
[12/06 08:44:54 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0019369152030840554
[12/06 08:46:52 visual_prompt]: 	Training 100/553. train loss: 0.3047,	1.2454 s / batch. (data: 4.13e-01). ETA=1:29:45, max mem: 20.9 GB 
[12/06 08:48:50 visual_prompt]: 	Training 200/553. train loss: 1.2322,	1.9099 s / batch. (data: 1.07e+00). ETA=2:14:27, max mem: 20.9 GB 
[12/06 08:50:42 visual_prompt]: 	Training 300/553. train loss: 0.7844,	0.8358 s / batch. (data: 6.09e-03). ETA=0:57:26, max mem: 20.9 GB 
[12/06 08:52:35 visual_prompt]: 	Training 400/553. train loss: 0.5015,	1.5443 s / batch. (data: 7.07e-01). ETA=1:43:34, max mem: 20.9 GB 
[12/06 08:54:32 visual_prompt]: 	Training 500/553. train loss: 0.6334,	0.8288 s / batch. (data: 3.57e-04). ETA=0:54:12, max mem: 20.9 GB 
[12/06 08:55:28 visual_prompt]: Epoch 93 / 100: avg data time: 3.13e-01, avg batch time: 1.1456, average train loss: 0.5838
[12/06 08:56:35 visual_prompt]: Inference (val):avg data time: 2.37e-04, avg batch time: 0.3097, average loss: 0.6257
[12/06 08:56:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 72.81	
[12/06 08:56:35 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0014852136862001765
[12/06 08:58:34 visual_prompt]: 	Training 100/553. train loss: 0.4855,	1.2717 s / batch. (data: 4.49e-01). ETA=1:19:55, max mem: 20.9 GB 
[12/06 09:00:26 visual_prompt]: 	Training 200/553. train loss: 0.6780,	1.9962 s / batch. (data: 1.16e+00). ETA=2:02:08, max mem: 20.9 GB 
[12/06 09:02:22 visual_prompt]: 	Training 300/553. train loss: 0.7760,	0.8388 s / batch. (data: 1.06e-02). ETA=0:49:55, max mem: 20.9 GB 
[12/06 09:04:13 visual_prompt]: 	Training 400/553. train loss: 0.8029,	1.1205 s / batch. (data: 2.76e-01). ETA=1:04:49, max mem: 20.9 GB 
[12/06 09:06:05 visual_prompt]: 	Training 500/553. train loss: 0.3174,	1.1203 s / batch. (data: 2.73e-01). ETA=1:02:56, max mem: 20.9 GB 
[12/06 09:07:07 visual_prompt]: Epoch 94 / 100: avg data time: 3.09e-01, avg batch time: 1.1423, average train loss: 0.5703
[12/06 09:08:13 visual_prompt]: Inference (val):avg data time: 3.81e-04, avg batch time: 0.3105, average loss: 0.6862
[12/06 09:08:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 71.24	
[12/06 09:08:13 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0010926199633097156
[12/06 09:10:11 visual_prompt]: 	Training 100/553. train loss: 0.6754,	0.8527 s / batch. (data: 3.47e-04). ETA=0:45:43, max mem: 20.9 GB 
[12/06 09:12:08 visual_prompt]: 	Training 200/553. train loss: 0.3623,	1.3236 s / batch. (data: 5.05e-01). ETA=1:08:47, max mem: 20.9 GB 
[12/06 09:13:59 visual_prompt]: 	Training 300/553. train loss: 0.5145,	1.9959 s / batch. (data: 1.17e+00). ETA=1:40:23, max mem: 20.9 GB 
[12/06 09:15:52 visual_prompt]: 	Training 400/553. train loss: 0.6069,	2.6720 s / batch. (data: 1.84e+00). ETA=2:09:56, max mem: 20.9 GB 
[12/06 09:17:45 visual_prompt]: 	Training 500/553. train loss: 0.6058,	0.8520 s / batch. (data: 5.31e-04). ETA=0:40:00, max mem: 20.9 GB 
[12/06 09:18:43 visual_prompt]: Epoch 95 / 100: avg data time: 3.05e-01, avg batch time: 1.1378, average train loss: 0.5738
[12/06 09:19:49 visual_prompt]: Inference (val):avg data time: 5.80e-05, avg batch time: 0.3115, average loss: 0.6335
[12/06 09:19:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 72.69	
[12/06 09:19:49 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0007596123493895991
[12/06 09:21:51 visual_prompt]: 	Training 100/553. train loss: 0.6865,	0.8197 s / batch. (data: 4.43e-04). ETA=0:36:24, max mem: 20.9 GB 
[12/06 09:23:46 visual_prompt]: 	Training 200/553. train loss: 0.3333,	0.8161 s / batch. (data: 3.46e-04). ETA=0:34:53, max mem: 20.9 GB 
[12/06 09:25:37 visual_prompt]: 	Training 300/553. train loss: 0.4621,	1.3595 s / batch. (data: 5.19e-01). ETA=0:55:51, max mem: 20.9 GB 
[12/06 09:27:30 visual_prompt]: 	Training 400/553. train loss: 0.4578,	0.9098 s / batch. (data: 8.02e-02). ETA=0:35:51, max mem: 20.9 GB 
[12/06 09:29:24 visual_prompt]: 	Training 500/553. train loss: 0.6929,	0.8560 s / batch. (data: 3.32e-04). ETA=0:32:18, max mem: 20.9 GB 
[12/06 09:30:22 visual_prompt]: Epoch 96 / 100: avg data time: 3.10e-01, avg batch time: 1.1429, average train loss: 0.5517
[12/06 09:31:29 visual_prompt]: Inference (val):avg data time: 7.04e-05, avg batch time: 0.3107, average loss: 0.6301
[12/06 09:31:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 73.33	
[12/06 09:31:29 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00048659656292148194
[12/06 09:33:26 visual_prompt]: 	Training 100/553. train loss: 0.4558,	0.8561 s / batch. (data: 1.51e-02). ETA=0:30:08, max mem: 20.9 GB 
[12/06 09:35:21 visual_prompt]: 	Training 200/553. train loss: 0.3076,	0.8386 s / batch. (data: 2.25e-02). ETA=0:28:07, max mem: 20.9 GB 
[12/06 09:37:15 visual_prompt]: 	Training 300/553. train loss: 0.3967,	0.8252 s / batch. (data: 3.17e-04). ETA=0:26:17, max mem: 20.9 GB 
[12/06 09:39:09 visual_prompt]: 	Training 400/553. train loss: 0.4857,	0.8627 s / batch. (data: 3.32e-04). ETA=0:26:03, max mem: 20.9 GB 
[12/06 09:40:59 visual_prompt]: 	Training 500/553. train loss: 0.9521,	0.8389 s / batch. (data: 1.11e-02). ETA=0:23:56, max mem: 20.9 GB 
[12/06 09:42:00 visual_prompt]: Epoch 97 / 100: avg data time: 3.08e-01, avg batch time: 1.1409, average train loss: 0.5425
[12/06 09:43:05 visual_prompt]: Inference (val):avg data time: 5.85e-05, avg batch time: 0.3105, average loss: 0.6247
[12/06 09:43:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 73.71	
[12/06 09:43:05 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0002739052315863355
[12/06 09:45:08 visual_prompt]: 	Training 100/553. train loss: 0.3889,	0.8358 s / batch. (data: 5.64e-03). ETA=0:21:43, max mem: 20.9 GB 
[12/06 09:46:59 visual_prompt]: 	Training 200/553. train loss: 0.5332,	0.8490 s / batch. (data: 5.53e-03). ETA=0:20:38, max mem: 20.9 GB 
[12/06 09:48:53 visual_prompt]: 	Training 300/553. train loss: 0.2985,	2.8261 s / batch. (data: 2.00e+00). ETA=1:04:00, max mem: 20.9 GB 
[12/06 09:50:45 visual_prompt]: 	Training 400/553. train loss: 0.3763,	1.8190 s / batch. (data: 9.85e-01). ETA=0:38:10, max mem: 20.9 GB 
[12/06 09:52:38 visual_prompt]: 	Training 500/553. train loss: 0.6941,	0.8269 s / batch. (data: 4.23e-04). ETA=0:15:58, max mem: 20.9 GB 
[12/06 09:53:37 visual_prompt]: Epoch 98 / 100: avg data time: 3.08e-01, avg batch time: 1.1415, average train loss: 0.5337
[12/06 09:54:42 visual_prompt]: Inference (val):avg data time: 5.90e-05, avg batch time: 0.3099, average loss: 0.6820
[12/06 09:54:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 73.05	
[12/06 09:54:42 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.00012179748700879012
[12/06 09:56:40 visual_prompt]: 	Training 100/553. train loss: 0.4958,	0.8365 s / batch. (data: 7.95e-03). ETA=0:14:01, max mem: 20.9 GB 
[12/06 09:58:35 visual_prompt]: 	Training 200/553. train loss: 0.3458,	0.8315 s / batch. (data: 1.13e-02). ETA=0:12:33, max mem: 20.9 GB 
[12/06 10:00:32 visual_prompt]: 	Training 300/553. train loss: 0.9563,	2.1117 s / batch. (data: 1.29e+00). ETA=0:28:22, max mem: 20.9 GB 
[12/06 10:02:21 visual_prompt]: 	Training 400/553. train loss: 0.4440,	0.8525 s / batch. (data: 3.41e-04). ETA=0:10:01, max mem: 20.9 GB 
[12/06 10:04:14 visual_prompt]: 	Training 500/553. train loss: 0.4811,	1.4254 s / batch. (data: 5.82e-01). ETA=0:14:23, max mem: 20.9 GB 
[12/06 10:05:14 visual_prompt]: Epoch 99 / 100: avg data time: 3.10e-01, avg batch time: 1.1426, average train loss: 0.5168
[12/06 10:06:20 visual_prompt]: Inference (val):avg data time: 2.32e-04, avg batch time: 0.3133, average loss: 0.6611
[12/06 10:06:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 73.03	
[12/06 10:06:20 visual_prompt]: Training 100 / 100 epoch, with learning rate 3.0458649045211895e-05
[12/06 10:08:22 visual_prompt]: 	Training 100/553. train loss: 0.5106,	1.7704 s / batch. (data: 9.36e-01). ETA=0:13:22, max mem: 20.9 GB 
[12/06 10:10:13 visual_prompt]: 	Training 200/553. train loss: 0.2913,	1.5041 s / batch. (data: 6.75e-01). ETA=0:08:50, max mem: 20.9 GB 
[12/06 10:12:07 visual_prompt]: 	Training 300/553. train loss: 0.2822,	0.8356 s / batch. (data: 3.81e-04). ETA=0:03:31, max mem: 20.9 GB 
[12/06 10:14:00 visual_prompt]: 	Training 400/553. train loss: 0.5567,	0.8480 s / batch. (data: 5.46e-03). ETA=0:02:09, max mem: 20.9 GB 
[12/06 10:15:51 visual_prompt]: 	Training 500/553. train loss: 0.2284,	0.8451 s / batch. (data: 6.68e-03). ETA=0:00:44, max mem: 20.9 GB 
[12/06 10:16:49 visual_prompt]: Epoch 100 / 100: avg data time: 3.04e-01, avg batch time: 1.1372, average train loss: 0.5202
[12/06 10:17:54 visual_prompt]: Inference (val):avg data time: 5.53e-05, avg batch time: 0.3114, average loss: 0.6902
[12/06 10:17:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 72.58	
[12/06 10:17:54 visual_prompt]: Rank of current process: 0. World size: 1
[12/06 10:17:54 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/06 10:17:54 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[12/06 10:17:54 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[12/06 10:17:54 visual_prompt]: Training with config:
[12/06 10:17:54 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.1_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[12/06 10:17:54 visual_prompt]: Loading training data...
[12/06 10:17:54 visual_prompt]: Constructing mammo-cbis dataset train...
[12/06 10:17:54 visual_prompt]: Loading validation data...
[12/06 10:17:54 visual_prompt]: Constructing mammo-cbis dataset val...
[12/06 10:17:54 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[12/06 10:17:58 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[12/06 10:17:58 visual_prompt]: tuned percent:0.525
[12/06 10:17:58 visual_prompt]: Device used for model: 0
[12/06 10:17:58 visual_prompt]: Setting up Evaluator...
[12/06 10:17:58 visual_prompt]: Setting up Trainer...
[12/06 10:17:58 visual_prompt]: 	Setting up the optimizer...
[12/06 10:17:58 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[12/06 10:19:56 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8159 s / batch. (data: 1.18e-03). ETA=12:30:39, max mem: 20.9 GB 
[12/06 10:21:47 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8281 s / batch. (data: 1.13e-03). ETA=12:40:26, max mem: 20.9 GB 
[12/06 10:23:43 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.5460 s / batch. (data: 1.72e+00). ETA=1 day, 14:53:52, max mem: 20.9 GB 
[12/06 10:25:35 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8439 s / batch. (data: 4.30e-04). ETA=12:52:09, max mem: 20.9 GB 
[12/06 10:27:31 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8440 s / batch. (data: 5.47e-03). ETA=12:50:51, max mem: 20.9 GB 
[12/06 10:28:31 visual_prompt]: Epoch 1 / 100: avg data time: 3.13e-01, avg batch time: 1.1451, average train loss: 1.5403
[12/06 10:29:37 visual_prompt]: Inference (val):avg data time: 5.58e-05, avg batch time: 0.3133, average loss: 1.5201
[12/06 10:29:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[12/06 10:29:37 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[12/06 10:31:35 visual_prompt]: 	Training 100/553. train loss: 0.6265,	0.8356 s / batch. (data: 5.87e-03). ETA=12:41:04, max mem: 20.9 GB 
[12/06 10:33:27 visual_prompt]: 	Training 200/553. train loss: 0.2616,	0.8268 s / batch. (data: 3.34e-04). ETA=12:31:37, max mem: 20.9 GB 
[12/06 10:35:23 visual_prompt]: 	Training 300/553. train loss: 0.7625,	1.4515 s / batch. (data: 6.21e-01). ETA=21:57:07, max mem: 20.9 GB 
[12/06 10:37:15 visual_prompt]: 	Training 400/553. train loss: 0.9386,	0.8542 s / batch. (data: 3.89e-04). ETA=12:53:40, max mem: 20.9 GB 
[12/06 10:39:09 visual_prompt]: 	Training 500/553. train loss: 0.6741,	0.8400 s / batch. (data: 1.20e-02). ETA=12:39:27, max mem: 20.9 GB 
[12/06 10:40:07 visual_prompt]: Epoch 2 / 100: avg data time: 3.07e-01, avg batch time: 1.1391, average train loss: 0.7773
[12/06 10:41:12 visual_prompt]: Inference (val):avg data time: 5.78e-05, avg batch time: 0.3110, average loss: 0.7351
[12/06 10:41:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.42	
[12/06 10:41:12 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[12/06 10:43:09 visual_prompt]: 	Training 100/553. train loss: 0.8010,	0.8237 s / batch. (data: 1.09e-03). ETA=12:22:34, max mem: 20.9 GB 
[12/06 10:45:04 visual_prompt]: 	Training 200/553. train loss: 0.7660,	1.4997 s / batch. (data: 6.27e-01). ETA=22:29:33, max mem: 20.9 GB 
[12/06 10:46:55 visual_prompt]: 	Training 300/553. train loss: 0.5735,	0.8282 s / batch. (data: 5.47e-03). ETA=12:23:56, max mem: 20.9 GB 
[12/06 10:48:50 visual_prompt]: 	Training 400/553. train loss: 1.6354,	0.8238 s / batch. (data: 6.36e-04). ETA=12:18:36, max mem: 20.9 GB 
[12/06 10:50:46 visual_prompt]: 	Training 500/553. train loss: 0.8072,	1.9881 s / batch. (data: 1.15e+00). ETA=1 day, 5:39:08, max mem: 20.9 GB 
[12/06 10:51:44 visual_prompt]: Epoch 3 / 100: avg data time: 3.11e-01, avg batch time: 1.1426, average train loss: 0.7552
[12/06 10:52:51 visual_prompt]: Inference (val):avg data time: 5.72e-05, avg batch time: 0.3103, average loss: 0.7268
[12/06 10:52:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.31	rocauc: 58.34	
[12/06 10:52:51 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[12/06 10:54:53 visual_prompt]: 	Training 100/553. train loss: 0.6764,	0.8440 s / batch. (data: 4.48e-04). ETA=12:33:10, max mem: 20.9 GB 
[12/06 10:56:47 visual_prompt]: 	Training 200/553. train loss: 0.4517,	0.8288 s / batch. (data: 1.01e-03). ETA=12:18:11, max mem: 20.9 GB 
[12/06 10:58:43 visual_prompt]: 	Training 300/553. train loss: 0.6148,	2.1550 s / batch. (data: 1.32e+00). ETA=1 day, 7:55:51, max mem: 20.9 GB 
[12/06 11:00:31 visual_prompt]: 	Training 400/553. train loss: 0.5692,	2.0043 s / batch. (data: 1.15e+00). ETA=1 day, 5:38:32, max mem: 20.9 GB 
[12/06 11:02:30 visual_prompt]: 	Training 500/553. train loss: 0.9406,	4.4811 s / batch. (data: 3.66e+00). ETA=2 days, 18:08:49, max mem: 20.9 GB 
[12/06 11:03:29 visual_prompt]: Epoch 4 / 100: avg data time: 3.23e-01, avg batch time: 1.1533, average train loss: 0.8193
[12/06 11:04:36 visual_prompt]: Inference (val):avg data time: 5.46e-05, avg batch time: 0.3103, average loss: 0.7209
[12/06 11:04:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.16	
[12/06 11:04:36 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[12/06 11:06:32 visual_prompt]: 	Training 100/553. train loss: 0.4178,	0.8480 s / batch. (data: 6.14e-03). ETA=12:28:51, max mem: 20.9 GB 
[12/06 11:08:27 visual_prompt]: 	Training 200/553. train loss: 0.7487,	1.7739 s / batch. (data: 9.57e-01). ETA=1 day, 2:03:38, max mem: 20.9 GB 
[12/06 11:10:21 visual_prompt]: 	Training 300/553. train loss: 0.9754,	0.8441 s / batch. (data: 3.40e-04). ETA=12:22:38, max mem: 20.9 GB 
[12/06 11:12:12 visual_prompt]: 	Training 400/553. train loss: 0.9884,	0.8250 s / batch. (data: 5.46e-03). ETA=12:04:26, max mem: 20.9 GB 
[12/06 11:14:08 visual_prompt]: 	Training 500/553. train loss: 0.5832,	0.8307 s / batch. (data: 3.04e-04). ETA=12:08:04, max mem: 20.9 GB 
[12/06 11:15:07 visual_prompt]: Epoch 5 / 100: avg data time: 3.10e-01, avg batch time: 1.1418, average train loss: 0.8314
[12/06 11:16:15 visual_prompt]: Inference (val):avg data time: 6.12e-05, avg batch time: 0.3100, average loss: 0.6827
[12/06 11:16:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.66	
[12/06 11:16:15 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[12/06 11:18:17 visual_prompt]: 	Training 100/553. train loss: 0.6242,	0.8332 s / batch. (data: 9.26e-04). ETA=12:08:11, max mem: 20.9 GB 
[12/06 11:20:11 visual_prompt]: 	Training 200/553. train loss: 0.5877,	0.8535 s / batch. (data: 5.47e-03). ETA=12:24:28, max mem: 20.9 GB 
[12/06 11:22:05 visual_prompt]: 	Training 300/553. train loss: 0.5429,	0.8251 s / batch. (data: 5.45e-03). ETA=11:58:20, max mem: 20.9 GB 
[12/06 11:24:04 visual_prompt]: 	Training 400/553. train loss: 0.5565,	0.8358 s / batch. (data: 9.23e-04). ETA=12:06:12, max mem: 20.9 GB 
[12/06 11:25:56 visual_prompt]: 	Training 500/553. train loss: 0.8099,	1.1291 s / batch. (data: 2.88e-01). ETA=16:19:14, max mem: 20.9 GB 
[12/06 11:26:56 visual_prompt]: Epoch 6 / 100: avg data time: 3.27e-01, avg batch time: 1.1585, average train loss: 0.7602
[12/06 11:28:03 visual_prompt]: Inference (val):avg data time: 6.57e-05, avg batch time: 0.3120, average loss: 0.6798
[12/06 11:28:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.56	
[12/06 11:28:03 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[12/06 11:30:00 visual_prompt]: 	Training 100/553. train loss: 0.6582,	0.8524 s / batch. (data: 3.25e-04). ETA=12:17:01, max mem: 20.9 GB 
[12/06 11:31:54 visual_prompt]: 	Training 200/553. train loss: 0.5005,	0.8352 s / batch. (data: 3.78e-04). ETA=12:00:50, max mem: 20.9 GB 
[12/06 11:33:51 visual_prompt]: 	Training 300/553. train loss: 0.7510,	2.4709 s / batch. (data: 1.65e+00). ETA=1 day, 11:28:22, max mem: 20.9 GB 
[12/06 11:35:45 visual_prompt]: 	Training 400/553. train loss: 0.6171,	2.4800 s / batch. (data: 1.65e+00). ETA=1 day, 11:32:02, max mem: 20.9 GB 
[12/06 11:37:36 visual_prompt]: 	Training 500/553. train loss: 1.1128,	0.8596 s / batch. (data: 1.19e-02). ETA=12:17:33, max mem: 20.9 GB 
[12/06 11:38:35 visual_prompt]: Epoch 7 / 100: avg data time: 3.10e-01, avg batch time: 1.1420, average train loss: 0.7771
[12/06 11:39:41 visual_prompt]: Inference (val):avg data time: 5.54e-05, avg batch time: 0.3087, average loss: 0.7506
[12/06 11:39:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.39	
[12/06 11:39:41 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[12/06 11:41:37 visual_prompt]: 	Training 100/553. train loss: 0.7150,	1.0627 s / batch. (data: 2.49e-01). ETA=15:09:05, max mem: 20.9 GB 
[12/06 11:43:32 visual_prompt]: 	Training 200/553. train loss: 1.3153,	0.8760 s / batch. (data: 1.20e-02). ETA=12:27:56, max mem: 20.9 GB 
[12/06 11:45:26 visual_prompt]: 	Training 300/553. train loss: 0.8282,	0.8428 s / batch. (data: 3.41e-04). ETA=11:58:10, max mem: 20.9 GB 
[12/06 11:47:19 visual_prompt]: 	Training 400/553. train loss: 0.6084,	0.8157 s / batch. (data: 4.51e-04). ETA=11:33:44, max mem: 20.9 GB 
[12/06 11:49:15 visual_prompt]: 	Training 500/553. train loss: 0.9699,	2.2040 s / batch. (data: 1.39e+00). ETA=1 day, 7:10:47, max mem: 20.9 GB 
[12/06 11:50:13 visual_prompt]: Epoch 8 / 100: avg data time: 3.10e-01, avg batch time: 1.1426, average train loss: 0.7740
[12/06 11:51:20 visual_prompt]: Inference (val):avg data time: 7.71e-05, avg batch time: 0.3106, average loss: 0.8966
[12/06 11:51:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.31	
[12/06 11:51:20 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[12/06 11:53:18 visual_prompt]: 	Training 100/553. train loss: 0.6805,	0.8371 s / batch. (data: 6.76e-03). ETA=11:48:26, max mem: 20.9 GB 
[12/06 11:55:11 visual_prompt]: 	Training 200/553. train loss: 0.6354,	0.8200 s / batch. (data: 5.51e-03). ETA=11:32:36, max mem: 20.9 GB 
[12/06 11:57:05 visual_prompt]: 	Training 300/553. train loss: 0.6115,	2.3381 s / batch. (data: 1.53e+00). ETA=1 day, 8:50:52, max mem: 20.9 GB 
[12/06 11:59:00 visual_prompt]: 	Training 400/553. train loss: 0.5351,	0.8426 s / batch. (data: 3.37e-04). ETA=11:48:48, max mem: 20.9 GB 
[12/06 12:00:53 visual_prompt]: 	Training 500/553. train loss: 0.9439,	1.3819 s / batch. (data: 5.40e-01). ETA=19:20:12, max mem: 20.9 GB 
[12/06 12:01:51 visual_prompt]: Epoch 9 / 100: avg data time: 3.08e-01, avg batch time: 1.1402, average train loss: 0.7596
[12/06 12:02:57 visual_prompt]: Inference (val):avg data time: 2.02e-04, avg batch time: 0.3108, average loss: 0.7185
[12/06 12:02:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 64.10	
[12/06 12:02:57 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[12/06 12:04:58 visual_prompt]: 	Training 100/553. train loss: 0.6646,	0.8399 s / batch. (data: 7.89e-03). ETA=11:43:03, max mem: 20.9 GB 
[12/06 12:06:50 visual_prompt]: 	Training 200/553. train loss: 0.5143,	0.8229 s / batch. (data: 3.42e-04). ETA=11:27:25, max mem: 20.9 GB 
[12/06 12:08:43 visual_prompt]: 	Training 300/553. train loss: 0.4989,	0.8515 s / batch. (data: 1.16e-02). ETA=11:49:53, max mem: 20.9 GB 
[12/06 12:10:35 visual_prompt]: 	Training 400/553. train loss: 0.6560,	1.1851 s / batch. (data: 3.05e-01). ETA=16:26:05, max mem: 20.9 GB 
[12/06 12:12:29 visual_prompt]: 	Training 500/553. train loss: 1.9120,	1.1842 s / batch. (data: 3.54e-01). ETA=16:23:19, max mem: 20.9 GB 
[12/06 12:13:30 visual_prompt]: Epoch 10 / 100: avg data time: 3.12e-01, avg batch time: 1.1443, average train loss: 0.7746
[12/06 12:14:36 visual_prompt]: Inference (val):avg data time: 6.32e-05, avg batch time: 0.3122, average loss: 1.0198
[12/06 12:14:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.63	
[12/06 12:14:36 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[12/06 12:16:36 visual_prompt]: 	Training 100/553. train loss: 0.5947,	0.8562 s / batch. (data: 4.53e-04). ETA=11:48:46, max mem: 20.9 GB 
[12/06 12:18:32 visual_prompt]: 	Training 200/553. train loss: 1.8940,	0.8285 s / batch. (data: 3.09e-04). ETA=11:24:27, max mem: 20.9 GB 
[12/06 12:20:25 visual_prompt]: 	Training 300/553. train loss: 0.7495,	2.5036 s / batch. (data: 1.68e+00). ETA=1 day, 10:24:13, max mem: 20.9 GB 
[12/06 12:22:16 visual_prompt]: 	Training 400/553. train loss: 0.7169,	0.8559 s / batch. (data: 3.34e-04). ETA=11:44:16, max mem: 20.9 GB 
[12/06 12:24:07 visual_prompt]: 	Training 500/553. train loss: 0.6069,	0.8249 s / batch. (data: 7.96e-03). ETA=11:17:20, max mem: 20.9 GB 
[12/06 12:25:05 visual_prompt]: Epoch 11 / 100: avg data time: 3.04e-01, avg batch time: 1.1368, average train loss: 0.7998
[12/06 12:26:12 visual_prompt]: Inference (val):avg data time: 6.17e-05, avg batch time: 0.3093, average loss: 0.7757
[12/06 12:26:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 64.18	
[12/06 12:26:12 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[12/06 12:28:11 visual_prompt]: 	Training 100/553. train loss: 0.7897,	1.1474 s / batch. (data: 3.24e-01). ETA=15:39:14, max mem: 20.9 GB 
[12/06 12:30:05 visual_prompt]: 	Training 200/553. train loss: 0.6715,	0.8428 s / batch. (data: 6.62e-04). ETA=11:28:32, max mem: 20.9 GB 
[12/06 12:31:57 visual_prompt]: 	Training 300/553. train loss: 0.6489,	0.8439 s / batch. (data: 5.83e-03). ETA=11:28:02, max mem: 20.9 GB 
[12/06 12:33:51 visual_prompt]: 	Training 400/553. train loss: 1.0776,	0.8248 s / batch. (data: 3.94e-04). ETA=11:11:03, max mem: 20.9 GB 
[12/06 12:35:46 visual_prompt]: 	Training 500/553. train loss: 1.2660,	0.8288 s / batch. (data: 3.23e-04). ETA=11:12:54, max mem: 20.9 GB 
[12/06 12:36:44 visual_prompt]: Epoch 12 / 100: avg data time: 3.10e-01, avg batch time: 1.1426, average train loss: 0.7510
[12/06 12:37:51 visual_prompt]: Inference (val):avg data time: 6.39e-05, avg batch time: 0.3086, average loss: 1.2077
[12/06 12:37:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.69	
[12/06 12:37:51 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[12/06 12:39:51 visual_prompt]: 	Training 100/553. train loss: 0.7846,	0.8381 s / batch. (data: 3.47e-04). ETA=11:18:19, max mem: 20.9 GB 
[12/06 12:41:41 visual_prompt]: 	Training 200/553. train loss: 0.6277,	0.8600 s / batch. (data: 1.19e-02). ETA=11:34:39, max mem: 20.9 GB 
[12/06 12:43:35 visual_prompt]: 	Training 300/553. train loss: 0.7024,	1.9146 s / batch. (data: 1.07e+00). ETA=1 day, 1:43:15, max mem: 20.9 GB 
[12/06 12:45:27 visual_prompt]: 	Training 400/553. train loss: 0.9833,	0.8184 s / batch. (data: 4.73e-04). ETA=10:58:17, max mem: 20.9 GB 
[12/06 12:47:21 visual_prompt]: 	Training 500/553. train loss: 0.6022,	0.8584 s / batch. (data: 7.40e-03). ETA=11:29:05, max mem: 20.9 GB 
[12/06 12:48:20 visual_prompt]: Epoch 13 / 100: avg data time: 3.06e-01, avg batch time: 1.1380, average train loss: 0.7644
[12/06 12:49:26 visual_prompt]: Inference (val):avg data time: 5.77e-05, avg batch time: 0.3098, average loss: 0.6954
[12/06 12:49:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 63.55	
[12/06 12:49:26 visual_prompt]: Best epoch 13: best metric: -0.695
[12/06 12:49:26 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[12/06 12:51:25 visual_prompt]: 	Training 100/553. train loss: 0.7094,	0.8502 s / batch. (data: 1.19e-02). ETA=11:20:18, max mem: 20.9 GB 
[12/06 12:53:17 visual_prompt]: 	Training 200/553. train loss: 0.5219,	1.0638 s / batch. (data: 2.29e-01). ETA=14:09:26, max mem: 20.9 GB 
[12/06 12:55:11 visual_prompt]: 	Training 300/553. train loss: 0.7927,	1.1840 s / batch. (data: 3.57e-01). ETA=15:43:28, max mem: 20.9 GB 
[12/06 12:57:04 visual_prompt]: 	Training 400/553. train loss: 0.8142,	0.8152 s / batch. (data: 3.92e-04). ETA=10:48:13, max mem: 20.9 GB 
[12/06 12:58:58 visual_prompt]: 	Training 500/553. train loss: 0.9324,	0.8438 s / batch. (data: 1.19e-02). ETA=11:09:35, max mem: 20.9 GB 
[12/06 12:59:54 visual_prompt]: Epoch 14 / 100: avg data time: 3.05e-01, avg batch time: 1.1362, average train loss: 0.7312
[12/06 13:01:01 visual_prompt]: Inference (val):avg data time: 6.57e-05, avg batch time: 0.3110, average loss: 0.7078
[12/06 13:01:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 67.65	
[12/06 13:01:01 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[12/06 13:02:58 visual_prompt]: 	Training 100/553. train loss: 0.8414,	0.8361 s / batch. (data: 3.28e-04). ETA=11:01:20, max mem: 20.9 GB 
[12/06 13:04:49 visual_prompt]: 	Training 200/553. train loss: 1.0052,	0.8321 s / batch. (data: 3.33e-04). ETA=10:56:44, max mem: 20.9 GB 
[12/06 13:06:44 visual_prompt]: 	Training 300/553. train loss: 0.4177,	0.8551 s / batch. (data: 3.56e-04). ETA=11:13:32, max mem: 20.9 GB 
[12/06 13:08:34 visual_prompt]: 	Training 400/553. train loss: 0.3850,	0.8300 s / batch. (data: 3.66e-04). ETA=10:52:23, max mem: 20.9 GB 
[12/06 13:10:28 visual_prompt]: 	Training 500/553. train loss: 0.8735,	0.8421 s / batch. (data: 5.48e-03). ETA=11:00:27, max mem: 20.9 GB 
[12/06 13:11:27 visual_prompt]: Epoch 15 / 100: avg data time: 3.00e-01, avg batch time: 1.1326, average train loss: 0.7632
[12/06 13:12:33 visual_prompt]: Inference (val):avg data time: 5.96e-05, avg batch time: 0.3123, average loss: 0.8663
[12/06 13:12:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.07	
[12/06 13:12:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[12/06 13:14:29 visual_prompt]: 	Training 100/553. train loss: 0.4721,	0.8331 s / batch. (data: 5.78e-03). ETA=10:51:16, max mem: 20.9 GB 
[12/06 13:16:22 visual_prompt]: 	Training 200/553. train loss: 1.0872,	0.8364 s / batch. (data: 1.35e-02). ETA=10:52:28, max mem: 20.9 GB 
[12/06 13:18:16 visual_prompt]: 	Training 300/553. train loss: 1.1451,	0.8580 s / batch. (data: 6.30e-03). ETA=11:07:53, max mem: 20.9 GB 
[12/06 13:20:08 visual_prompt]: 	Training 400/553. train loss: 0.3235,	0.8526 s / batch. (data: 1.68e-02). ETA=11:02:15, max mem: 20.9 GB 
[12/06 13:22:00 visual_prompt]: 	Training 500/553. train loss: 1.2523,	2.1025 s / batch. (data: 1.28e+00). ETA=1 day, 3:09:36, max mem: 20.9 GB 
[12/06 13:23:00 visual_prompt]: Epoch 16 / 100: avg data time: 3.02e-01, avg batch time: 1.1329, average train loss: 0.7254
[12/06 13:24:05 visual_prompt]: Inference (val):avg data time: 4.95e-05, avg batch time: 0.3100, average loss: 0.6363
[12/06 13:24:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 69.57	
[12/06 13:24:05 visual_prompt]: Best epoch 16: best metric: -0.636
[12/06 13:24:05 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[12/06 13:26:02 visual_prompt]: 	Training 100/553. train loss: 0.3604,	0.8414 s / batch. (data: 9.68e-04). ETA=10:50:00, max mem: 20.9 GB 
[12/06 13:27:56 visual_prompt]: 	Training 200/553. train loss: 0.9815,	0.8421 s / batch. (data: 4.03e-04). ETA=10:49:06, max mem: 20.9 GB 
[12/06 13:29:49 visual_prompt]: 	Training 300/553. train loss: 1.2304,	0.8153 s / batch. (data: 4.89e-04). ETA=10:27:05, max mem: 20.9 GB 
[12/06 13:31:41 visual_prompt]: 	Training 400/553. train loss: 0.6718,	1.0835 s / batch. (data: 2.49e-01). ETA=13:51:37, max mem: 20.9 GB 
[12/06 13:33:35 visual_prompt]: 	Training 500/553. train loss: 0.5687,	2.5457 s / batch. (data: 1.72e+00). ETA=1 day, 8:29:40, max mem: 20.9 GB 
[12/06 13:34:35 visual_prompt]: Epoch 17 / 100: avg data time: 3.08e-01, avg batch time: 1.1385, average train loss: 0.6980
[12/06 13:35:40 visual_prompt]: Inference (val):avg data time: 5.20e-05, avg batch time: 0.3098, average loss: 0.7770
[12/06 13:35:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.62	
[12/06 13:35:40 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[12/06 13:37:38 visual_prompt]: 	Training 100/553. train loss: 0.5526,	0.8301 s / batch. (data: 1.23e-03). ETA=10:33:38, max mem: 20.9 GB 
[12/06 13:39:35 visual_prompt]: 	Training 200/553. train loss: 0.6147,	0.8756 s / batch. (data: 2.59e-02). ETA=11:06:53, max mem: 20.9 GB 
[12/06 13:41:27 visual_prompt]: 	Training 300/553. train loss: 0.4375,	0.8306 s / batch. (data: 1.16e-02). ETA=10:31:13, max mem: 20.9 GB 
[12/06 13:43:20 visual_prompt]: 	Training 400/553. train loss: 0.7208,	0.8548 s / batch. (data: 1.96e-02). ETA=10:48:14, max mem: 20.9 GB 
[12/06 13:45:13 visual_prompt]: 	Training 500/553. train loss: 0.6741,	0.8320 s / batch. (data: 3.58e-04). ETA=10:29:31, max mem: 20.9 GB 
[12/06 13:46:10 visual_prompt]: Epoch 18 / 100: avg data time: 3.07e-01, avg batch time: 1.1391, average train loss: 0.7153
[12/06 13:47:17 visual_prompt]: Inference (val):avg data time: 6.01e-05, avg batch time: 0.3129, average loss: 0.6302
[12/06 13:47:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 70.32	
[12/06 13:47:17 visual_prompt]: Best epoch 18: best metric: -0.630
[12/06 13:47:17 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[12/06 13:49:15 visual_prompt]: 	Training 100/553. train loss: 1.2634,	0.8597 s / batch. (data: 1.22e-02). ETA=10:48:17, max mem: 20.9 GB 
[12/06 13:51:08 visual_prompt]: 	Training 200/553. train loss: 0.5345,	0.8320 s / batch. (data: 5.35e-04). ETA=10:25:59, max mem: 20.9 GB 
[12/06 13:53:01 visual_prompt]: 	Training 300/553. train loss: 0.4904,	0.8440 s / batch. (data: 2.50e-02). ETA=10:33:38, max mem: 20.9 GB 
[12/06 13:54:56 visual_prompt]: 	Training 400/553. train loss: 0.3932,	0.8157 s / batch. (data: 3.34e-04). ETA=10:11:01, max mem: 20.9 GB 
[12/06 13:56:44 visual_prompt]: 	Training 500/553. train loss: 0.7870,	0.8664 s / batch. (data: 3.74e-02). ETA=10:47:35, max mem: 20.9 GB 
[12/06 13:57:44 visual_prompt]: Epoch 19 / 100: avg data time: 3.02e-01, avg batch time: 1.1338, average train loss: 0.7050
[12/06 13:58:50 visual_prompt]: Inference (val):avg data time: 6.62e-05, avg batch time: 0.3109, average loss: 0.7530
[12/06 13:58:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.12	
[12/06 13:58:50 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[12/06 14:00:46 visual_prompt]: 	Training 100/553. train loss: 1.6583,	0.8280 s / batch. (data: 3.55e-04). ETA=10:16:44, max mem: 20.9 GB 
[12/06 14:02:42 visual_prompt]: 	Training 200/553. train loss: 0.3161,	0.8287 s / batch. (data: 2.18e-03). ETA=10:15:53, max mem: 20.9 GB 
[12/06 14:04:41 visual_prompt]: 	Training 300/553. train loss: 0.7349,	0.8159 s / batch. (data: 5.97e-04). ETA=10:05:03, max mem: 20.9 GB 
[12/06 14:06:34 visual_prompt]: 	Training 400/553. train loss: 0.6471,	0.8377 s / batch. (data: 1.08e-02). ETA=10:19:46, max mem: 20.9 GB 
[12/06 14:08:53 visual_prompt]: 	Training 500/553. train loss: 1.3095,	0.8126 s / batch. (data: 5.43e-04). ETA=9:59:53, max mem: 20.9 GB 
[12/06 14:10:13 visual_prompt]: Epoch 20 / 100: avg data time: 4.05e-01, avg batch time: 1.2339, average train loss: 0.7344
[12/06 14:11:41 visual_prompt]: Inference (val):avg data time: 4.69e-05, avg batch time: 0.3072, average loss: 0.6733
[12/06 14:11:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 69.66	
[12/06 14:11:41 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[12/06 14:13:53 visual_prompt]: 	Training 100/553. train loss: 0.4429,	0.8200 s / batch. (data: 3.45e-04). ETA=10:03:15, max mem: 20.9 GB 
[12/06 14:16:03 visual_prompt]: 	Training 200/553. train loss: 0.3585,	1.9954 s / batch. (data: 1.15e+00). ETA=1 day, 0:24:38, max mem: 20.9 GB 
[12/06 14:18:27 visual_prompt]: 	Training 300/553. train loss: 1.0149,	5.8596 s / batch. (data: 5.04e+00). ETA=2 days, 23:31:10, max mem: 20.9 GB 
[12/06 14:20:46 visual_prompt]: 	Training 400/553. train loss: 0.5129,	0.8399 s / batch. (data: 5.75e-03). ETA=10:13:43, max mem: 20.9 GB 
[12/06 14:23:04 visual_prompt]: 	Training 500/553. train loss: 0.6940,	0.8441 s / batch. (data: 3.25e-04). ETA=10:15:22, max mem: 20.9 GB 
[12/06 14:24:08 visual_prompt]: Epoch 21 / 100: avg data time: 5.26e-01, avg batch time: 1.3499, average train loss: 0.6990
[12/06 14:25:16 visual_prompt]: Inference (val):avg data time: 6.05e-05, avg batch time: 0.3104, average loss: 0.6145
[12/06 14:25:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 73.59	
[12/06 14:25:16 visual_prompt]: Best epoch 21: best metric: -0.614
[12/06 14:25:16 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[12/06 14:27:15 visual_prompt]: 	Training 100/553. train loss: 0.7278,	0.8404 s / batch. (data: 3.51e-04). ETA=10:10:30, max mem: 20.9 GB 
[12/06 14:29:08 visual_prompt]: 	Training 200/553. train loss: 0.4200,	0.8359 s / batch. (data: 3.36e-04). ETA=10:05:48, max mem: 20.9 GB 
[12/06 14:30:59 visual_prompt]: 	Training 300/553. train loss: 0.2538,	0.8664 s / batch. (data: 2.05e-02). ETA=10:26:28, max mem: 20.9 GB 
[12/06 14:32:54 visual_prompt]: 	Training 400/553. train loss: 0.7925,	0.8289 s / batch. (data: 3.39e-04). ETA=9:58:01, max mem: 20.9 GB 
[12/06 14:34:48 visual_prompt]: 	Training 500/553. train loss: 1.0197,	0.8651 s / batch. (data: 7.27e-03). ETA=10:22:40, max mem: 20.9 GB 
[12/06 14:35:50 visual_prompt]: Epoch 22 / 100: avg data time: 3.13e-01, avg batch time: 1.1464, average train loss: 0.7016
[12/06 14:36:56 visual_prompt]: Inference (val):avg data time: 6.18e-04, avg batch time: 0.3099, average loss: 0.6464
[12/06 14:36:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 73.45	
[12/06 14:36:56 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[12/06 14:38:56 visual_prompt]: 	Training 100/553. train loss: 0.8485,	0.8406 s / batch. (data: 1.21e-02). ETA=10:02:52, max mem: 20.9 GB 
[12/06 14:40:52 visual_prompt]: 	Training 200/553. train loss: 0.6734,	1.5722 s / batch. (data: 7.22e-01). ETA=18:45:00, max mem: 20.9 GB 
[12/06 14:42:47 visual_prompt]: 	Training 300/553. train loss: 1.5267,	0.8393 s / batch. (data: 1.16e-03). ETA=9:59:08, max mem: 20.9 GB 
[12/06 14:44:38 visual_prompt]: 	Training 400/553. train loss: 0.5009,	0.8414 s / batch. (data: 5.23e-04). ETA=9:59:14, max mem: 20.9 GB 
[12/06 14:46:30 visual_prompt]: 	Training 500/553. train loss: 0.5974,	0.8411 s / batch. (data: 1.06e-02). ETA=9:57:38, max mem: 20.9 GB 
[12/06 14:47:29 visual_prompt]: Epoch 23 / 100: avg data time: 3.12e-01, avg batch time: 1.1448, average train loss: 0.6758
[12/06 14:48:36 visual_prompt]: Inference (val):avg data time: 6.83e-05, avg batch time: 0.3130, average loss: 0.6746
[12/06 14:48:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 72.25	
[12/06 14:48:36 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[12/06 14:50:38 visual_prompt]: 	Training 100/553. train loss: 0.9864,	0.8678 s / batch. (data: 7.46e-04). ETA=10:14:25, max mem: 20.9 GB 
[12/06 14:52:34 visual_prompt]: 	Training 200/553. train loss: 0.8586,	0.8251 s / batch. (data: 6.36e-03). ETA=9:42:46, max mem: 20.9 GB 
[12/06 14:54:28 visual_prompt]: 	Training 300/553. train loss: 0.6382,	1.3784 s / batch. (data: 5.35e-01). ETA=16:11:19, max mem: 20.9 GB 
[12/06 14:56:23 visual_prompt]: 	Training 400/553. train loss: 0.3481,	0.8560 s / batch. (data: 3.34e-04). ETA=10:01:46, max mem: 20.9 GB 
[12/06 14:58:19 visual_prompt]: 	Training 500/553. train loss: 0.5297,	0.8292 s / batch. (data: 3.66e-04). ETA=9:41:32, max mem: 20.9 GB 
[12/06 14:59:19 visual_prompt]: Epoch 24 / 100: avg data time: 3.29e-01, avg batch time: 1.1617, average train loss: 0.7003
[12/06 15:00:26 visual_prompt]: Inference (val):avg data time: 7.22e-05, avg batch time: 0.3108, average loss: 0.6441
[12/06 15:00:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 69.99	
[12/06 15:00:26 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[12/06 15:02:30 visual_prompt]: 	Training 100/553. train loss: 0.6753,	0.8387 s / batch. (data: 3.85e-04). ETA=9:46:04, max mem: 20.9 GB 
[12/06 15:04:21 visual_prompt]: 	Training 200/553. train loss: 1.3931,	0.8301 s / batch. (data: 1.20e-02). ETA=9:38:42, max mem: 20.9 GB 
[12/06 15:06:14 visual_prompt]: 	Training 300/553. train loss: 0.5303,	1.7529 s / batch. (data: 9.24e-01). ETA=20:19:03, max mem: 20.9 GB 
[12/06 15:08:09 visual_prompt]: 	Training 400/553. train loss: 0.7206,	1.0955 s / batch. (data: 2.67e-01). ETA=12:40:03, max mem: 20.9 GB 
[12/06 15:10:15 visual_prompt]: 	Training 500/553. train loss: 0.7247,	2.0122 s / batch. (data: 1.18e+00). ETA=23:12:44, max mem: 20.9 GB 
[12/06 15:11:14 visual_prompt]: Epoch 25 / 100: avg data time: 3.40e-01, avg batch time: 1.1714, average train loss: 0.6811
[12/06 15:12:21 visual_prompt]: Inference (val):avg data time: 6.44e-05, avg batch time: 0.3102, average loss: 0.6921
[12/06 15:12:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 71.80	
[12/06 15:12:21 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[12/06 15:14:22 visual_prompt]: 	Training 100/553. train loss: 0.2271,	0.8566 s / batch. (data: 1.20e-02). ETA=9:50:40, max mem: 20.9 GB 
[12/06 15:16:17 visual_prompt]: 	Training 200/553. train loss: 0.6226,	2.0299 s / batch. (data: 1.20e+00). ETA=23:16:22, max mem: 20.9 GB 
[12/06 15:18:13 visual_prompt]: 	Training 300/553. train loss: 0.7106,	0.8335 s / batch. (data: 5.47e-03). ETA=9:31:58, max mem: 20.9 GB 
[12/06 15:20:06 visual_prompt]: 	Training 400/553. train loss: 0.5168,	0.8204 s / batch. (data: 3.35e-04). ETA=9:21:39, max mem: 20.9 GB 
[12/06 15:21:58 visual_prompt]: 	Training 500/553. train loss: 0.4132,	0.8244 s / batch. (data: 5.52e-03). ETA=9:22:59, max mem: 20.9 GB 
[12/06 15:22:57 visual_prompt]: Epoch 26 / 100: avg data time: 3.17e-01, avg batch time: 1.1497, average train loss: 0.6779
[12/06 15:24:04 visual_prompt]: Inference (val):avg data time: 5.36e-05, avg batch time: 0.3104, average loss: 0.6556
[12/06 15:24:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 71.91	
[12/06 15:24:04 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[12/06 15:26:04 visual_prompt]: 	Training 100/553. train loss: 0.6711,	0.8435 s / batch. (data: 2.22e-02). ETA=9:33:52, max mem: 20.9 GB 
[12/06 15:27:59 visual_prompt]: 	Training 200/553. train loss: 0.5084,	2.4257 s / batch. (data: 1.59e+00). ETA=1 day, 3:26:18, max mem: 20.9 GB 
[12/06 15:29:52 visual_prompt]: 	Training 300/553. train loss: 0.6906,	1.2108 s / batch. (data: 3.80e-01). ETA=13:39:47, max mem: 20.9 GB 
[12/06 15:31:49 visual_prompt]: 	Training 400/553. train loss: 0.8445,	0.8321 s / batch. (data: 3.32e-04). ETA=9:21:58, max mem: 20.9 GB 
[12/06 15:33:44 visual_prompt]: 	Training 500/553. train loss: 0.7647,	0.8520 s / batch. (data: 8.04e-04). ETA=9:33:58, max mem: 20.9 GB 
[12/06 15:34:41 visual_prompt]: Epoch 27 / 100: avg data time: 3.19e-01, avg batch time: 1.1520, average train loss: 0.6682
[12/06 15:35:47 visual_prompt]: Inference (val):avg data time: 5.12e-05, avg batch time: 0.3108, average loss: 0.7004
[12/06 15:35:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 75.26	
[12/06 15:35:47 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[12/06 15:37:46 visual_prompt]: 	Training 100/553. train loss: 0.1672,	0.8725 s / batch. (data: 8.41e-03). ETA=9:45:33, max mem: 20.9 GB 
[12/06 15:39:42 visual_prompt]: 	Training 200/553. train loss: 0.3424,	0.8237 s / batch. (data: 9.86e-04). ETA=9:11:25, max mem: 20.9 GB 
[12/06 15:41:37 visual_prompt]: 	Training 300/553. train loss: 0.6658,	2.2736 s / batch. (data: 1.46e+00). ETA=1 day, 1:18:21, max mem: 20.9 GB 
[12/06 15:43:28 visual_prompt]: 	Training 400/553. train loss: 0.5723,	0.8444 s / batch. (data: 7.94e-03). ETA=9:22:31, max mem: 20.9 GB 
[12/06 15:45:21 visual_prompt]: 	Training 500/553. train loss: 0.4167,	0.8430 s / batch. (data: 1.93e-02). ETA=9:20:10, max mem: 20.9 GB 
[12/06 15:46:22 visual_prompt]: Epoch 28 / 100: avg data time: 3.14e-01, avg batch time: 1.1484, average train loss: 0.6667
[12/06 15:47:29 visual_prompt]: Inference (val):avg data time: 2.92e-04, avg batch time: 0.3094, average loss: 0.7336
[12/06 15:47:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 72.38	
[12/06 15:47:29 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[12/06 15:49:35 visual_prompt]: 	Training 100/553. train loss: 0.4591,	0.8521 s / batch. (data: 5.45e-03). ETA=9:24:00, max mem: 20.9 GB 
[12/06 15:51:30 visual_prompt]: 	Training 200/553. train loss: 0.5330,	2.0350 s / batch. (data: 1.21e+00). ETA=22:23:38, max mem: 20.9 GB 
[12/06 15:53:21 visual_prompt]: 	Training 300/553. train loss: 0.6950,	0.8459 s / batch. (data: 3.38e-04). ETA=9:17:07, max mem: 20.9 GB 
[12/06 15:55:12 visual_prompt]: 	Training 400/553. train loss: 0.5692,	1.8073 s / batch. (data: 9.84e-01). ETA=19:47:14, max mem: 20.9 GB 
[12/06 15:57:06 visual_prompt]: 	Training 500/553. train loss: 0.5814,	0.9003 s / batch. (data: 1.29e-02). ETA=9:49:58, max mem: 20.9 GB 
[12/06 15:58:06 visual_prompt]: Epoch 29 / 100: avg data time: 3.18e-01, avg batch time: 1.1514, average train loss: 0.6651
[12/06 15:59:12 visual_prompt]: Inference (val):avg data time: 5.97e-05, avg batch time: 0.3102, average loss: 0.6405
[12/06 15:59:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 72.70	
[12/06 15:59:12 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[12/06 16:01:10 visual_prompt]: 	Training 100/553. train loss: 0.7716,	0.8288 s / batch. (data: 3.54e-04). ETA=9:00:57, max mem: 20.9 GB 
[12/06 16:03:07 visual_prompt]: 	Training 200/553. train loss: 0.6805,	0.8522 s / batch. (data: 3.62e-04). ETA=9:14:49, max mem: 20.9 GB 
[12/06 16:05:00 visual_prompt]: 	Training 300/553. train loss: 0.1207,	1.4081 s / batch. (data: 5.95e-01). ETA=15:14:24, max mem: 20.9 GB 
[12/06 16:06:56 visual_prompt]: 	Training 400/553. train loss: 0.9391,	1.7660 s / batch. (data: 9.28e-01). ETA=19:03:51, max mem: 20.9 GB 
[12/06 16:08:50 visual_prompt]: 	Training 500/553. train loss: 0.3889,	2.1024 s / batch. (data: 1.27e+00). ETA=22:38:16, max mem: 20.9 GB 
[12/06 16:09:51 visual_prompt]: Epoch 30 / 100: avg data time: 3.21e-01, avg batch time: 1.1550, average train loss: 0.6630
[12/06 16:10:57 visual_prompt]: Inference (val):avg data time: 2.92e-04, avg batch time: 0.3127, average loss: 0.6492
[12/06 16:10:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 74.81	
[12/06 16:10:57 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[12/06 16:12:58 visual_prompt]: 	Training 100/553. train loss: 0.6291,	0.8718 s / batch. (data: 3.22e-03). ETA=9:20:59, max mem: 20.9 GB 
[12/06 16:14:54 visual_prompt]: 	Training 200/553. train loss: 0.7452,	0.8343 s / batch. (data: 7.03e-03). ETA=8:55:28, max mem: 20.9 GB 
[12/06 16:16:47 visual_prompt]: 	Training 300/553. train loss: 0.8310,	0.8198 s / batch. (data: 7.18e-04). ETA=8:44:49, max mem: 20.9 GB 
[12/06 16:18:41 visual_prompt]: 	Training 400/553. train loss: 0.4101,	0.8661 s / batch. (data: 2.24e-02). ETA=9:13:01, max mem: 20.9 GB 
[12/06 16:20:36 visual_prompt]: 	Training 500/553. train loss: 0.6200,	0.8384 s / batch. (data: 1.05e-02). ETA=8:53:54, max mem: 20.9 GB 
[12/06 16:21:34 visual_prompt]: Epoch 31 / 100: avg data time: 3.19e-01, avg batch time: 1.1510, average train loss: 0.6558
[12/06 16:22:41 visual_prompt]: Inference (val):avg data time: 6.02e-05, avg batch time: 0.3102, average loss: 0.7192
[12/06 16:22:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.56	
[12/06 16:22:41 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[12/06 16:24:43 visual_prompt]: 	Training 100/553. train loss: 0.6730,	0.8359 s / batch. (data: 8.19e-04). ETA=8:50:11, max mem: 20.9 GB 
[12/06 16:26:36 visual_prompt]: 	Training 200/553. train loss: 0.1892,	0.8208 s / batch. (data: 3.69e-04). ETA=8:39:16, max mem: 20.9 GB 
[12/06 16:28:35 visual_prompt]: 	Training 300/553. train loss: 1.0313,	0.8291 s / batch. (data: 3.36e-04). ETA=8:43:07, max mem: 20.9 GB 
[12/06 16:30:29 visual_prompt]: 	Training 400/553. train loss: 1.2480,	0.8464 s / batch. (data: 1.16e-02). ETA=8:52:36, max mem: 20.9 GB 
[12/06 16:32:20 visual_prompt]: 	Training 500/553. train loss: 1.0696,	0.8333 s / batch. (data: 8.49e-04). ETA=8:43:00, max mem: 20.9 GB 
[12/06 16:33:17 visual_prompt]: Epoch 32 / 100: avg data time: 3.17e-01, avg batch time: 1.1490, average train loss: 0.6372
[12/06 16:34:24 visual_prompt]: Inference (val):avg data time: 7.36e-05, avg batch time: 0.3102, average loss: 0.7771
[12/06 16:34:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 73.26	
[12/06 16:34:24 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[12/06 16:36:23 visual_prompt]: 	Training 100/553. train loss: 1.4604,	0.8600 s / batch. (data: 1.19e-02). ETA=8:57:33, max mem: 20.9 GB 
[12/06 16:38:18 visual_prompt]: 	Training 200/553. train loss: 0.2550,	2.1967 s / batch. (data: 1.38e+00). ETA=22:49:24, max mem: 20.9 GB 
[12/06 16:40:13 visual_prompt]: 	Training 300/553. train loss: 0.4907,	0.8453 s / batch. (data: 2.97e-04). ETA=8:45:34, max mem: 20.9 GB 
[12/06 16:42:08 visual_prompt]: 	Training 400/553. train loss: 0.4753,	0.8280 s / batch. (data: 3.54e-04). ETA=8:33:24, max mem: 20.9 GB 
[12/06 16:44:00 visual_prompt]: 	Training 500/553. train loss: 0.5817,	1.0468 s / batch. (data: 2.17e-01). ETA=10:47:20, max mem: 20.9 GB 
[12/06 16:44:59 visual_prompt]: Epoch 33 / 100: avg data time: 3.16e-01, avg batch time: 1.1494, average train loss: 0.6314
[12/06 16:46:06 visual_prompt]: Inference (val):avg data time: 6.39e-05, avg batch time: 0.3121, average loss: 0.6186
[12/06 16:46:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.11	rocauc: 72.70	
[12/06 16:46:06 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[12/06 16:48:08 visual_prompt]: 	Training 100/553. train loss: 0.8224,	1.6720 s / batch. (data: 8.55e-01). ETA=17:09:42, max mem: 20.9 GB 
[12/06 16:50:00 visual_prompt]: 	Training 200/553. train loss: 1.1634,	0.8293 s / batch. (data: 6.60e-04). ETA=8:29:20, max mem: 20.9 GB 
[12/06 16:51:54 visual_prompt]: 	Training 300/553. train loss: 0.5540,	0.9040 s / batch. (data: 4.73e-02). ETA=9:13:42, max mem: 20.9 GB 
[12/06 16:53:49 visual_prompt]: 	Training 400/553. train loss: 0.4083,	0.8401 s / batch. (data: 2.07e-02). ETA=8:33:12, max mem: 20.9 GB 
[12/06 16:55:45 visual_prompt]: 	Training 500/553. train loss: 0.2077,	2.2156 s / batch. (data: 1.38e+00). ETA=22:29:40, max mem: 20.9 GB 
[12/06 16:56:44 visual_prompt]: Epoch 34 / 100: avg data time: 3.19e-01, avg batch time: 1.1526, average train loss: 0.6117
[12/06 16:57:51 visual_prompt]: Inference (val):avg data time: 2.70e-04, avg batch time: 0.3127, average loss: 0.6200
[12/06 16:57:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.73	rocauc: 75.07	
[12/06 16:57:51 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[12/06 16:59:52 visual_prompt]: 	Training 100/553. train loss: 0.8216,	0.8319 s / batch. (data: 3.30e-04). ETA=8:24:40, max mem: 20.9 GB 
[12/06 17:01:47 visual_prompt]: 	Training 200/553. train loss: 0.8303,	0.8565 s / batch. (data: 3.48e-04). ETA=8:38:08, max mem: 20.9 GB 
[12/06 17:03:41 visual_prompt]: 	Training 300/553. train loss: 0.4385,	0.8200 s / batch. (data: 3.41e-04). ETA=8:14:40, max mem: 20.9 GB 
[12/06 17:05:33 visual_prompt]: 	Training 400/553. train loss: 0.3192,	0.9507 s / batch. (data: 9.93e-02). ETA=9:31:59, max mem: 20.9 GB 
[12/06 17:07:27 visual_prompt]: 	Training 500/553. train loss: 1.1367,	1.5127 s / batch. (data: 6.98e-01). ETA=15:07:34, max mem: 20.9 GB 
[12/06 17:08:27 visual_prompt]: Epoch 35 / 100: avg data time: 3.16e-01, avg batch time: 1.1492, average train loss: 0.6135
[12/06 17:09:33 visual_prompt]: Inference (val):avg data time: 9.59e-04, avg batch time: 0.3121, average loss: 0.6024
[12/06 17:09:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 69.92	rocauc: 72.87	
[12/06 17:09:33 visual_prompt]: Best epoch 35: best metric: -0.602
[12/06 17:09:33 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[12/06 17:11:32 visual_prompt]: 	Training 100/553. train loss: 1.2477,	0.8599 s / batch. (data: 3.09e-02). ETA=8:33:42, max mem: 20.9 GB 

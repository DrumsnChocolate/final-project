/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/23 00:59:52 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 00:59:54 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 00:59:54 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/23 00:59:54 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 00:59:54 visual_prompt]: Training with config:
[11/23 00:59:54 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/23 00:59:54 visual_prompt]: Loading training data...
[11/23 00:59:54 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 00:59:54 visual_prompt]: Loading validation data...
[11/23 00:59:54 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 00:59:54 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 01:00:03 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 01:00:03 visual_prompt]: tuned percent:0.525
[11/23 01:00:04 visual_prompt]: Device used for model: 0
[11/23 01:00:04 visual_prompt]: Setting up Evaluator...
[11/23 01:00:04 visual_prompt]: Setting up Trainer...
[11/23 01:00:04 visual_prompt]: 	Setting up the optimizer...
[11/23 01:00:04 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 01:01:49 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8283 s / batch. (data: 4.84e-04). ETA=12:42:03, max mem: 20.9 GB 
[11/23 01:03:27 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8360 s / batch. (data: 1.03e-02). ETA=12:47:41, max mem: 20.9 GB 
[11/23 01:05:07 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3200 s / batch. (data: 4.93e-01). ETA=20:09:59, max mem: 20.9 GB 
[11/23 01:06:42 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8357 s / batch. (data: 5.41e-03). ETA=12:44:38, max mem: 20.9 GB 
[11/23 01:08:21 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8281 s / batch. (data: 5.39e-03). ETA=12:36:18, max mem: 20.9 GB 
[11/23 01:09:12 visual_prompt]: Epoch 1 / 100: avg data time: 1.60e-01, avg batch time: 0.9913, average train loss: 1.5403
[11/23 01:10:06 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3115, average loss: 1.5201
[11/23 01:10:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 01:10:06 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/23 01:11:46 visual_prompt]: 	Training 100/553. train loss: 11.0660,	1.0625 s / batch. (data: 2.42e-01). ETA=16:07:40, max mem: 20.9 GB 
[11/23 01:13:21 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.9680 s / batch. (data: 1.37e-01). ETA=14:40:01, max mem: 20.9 GB 
[11/23 01:15:00 visual_prompt]: 	Training 300/553. train loss: 16.4932,	0.9888 s / batch. (data: 1.70e-01). ETA=14:57:17, max mem: 20.9 GB 
[11/23 01:16:35 visual_prompt]: 	Training 400/553. train loss: 6.3848,	0.8312 s / batch. (data: 2.98e-04). ETA=12:32:54, max mem: 20.9 GB 
[11/23 01:18:13 visual_prompt]: 	Training 500/553. train loss: 1.8677,	0.8368 s / batch. (data: 3.21e-04). ETA=12:36:35, max mem: 20.9 GB 
[11/23 01:19:02 visual_prompt]: Epoch 2 / 100: avg data time: 1.36e-01, avg batch time: 0.9680, average train loss: 14.2649
[11/23 01:19:56 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3113, average loss: 67.7520
[11/23 01:19:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.35	
[11/23 01:19:56 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/23 01:21:35 visual_prompt]: 	Training 100/553. train loss: 36.6844,	0.8450 s / batch. (data: 1.08e-02). ETA=12:41:50, max mem: 20.9 GB 
[11/23 01:23:12 visual_prompt]: 	Training 200/553. train loss: 39.5236,	0.8348 s / batch. (data: 2.93e-04). ETA=12:31:12, max mem: 20.9 GB 
[11/23 01:24:47 visual_prompt]: 	Training 300/553. train loss: 87.4285,	0.8440 s / batch. (data: 3.04e-04). ETA=12:38:05, max mem: 20.9 GB 
[11/23 01:26:23 visual_prompt]: 	Training 400/553. train loss: 14.2160,	0.8271 s / batch. (data: 3.11e-04). ETA=12:21:34, max mem: 20.9 GB 
[11/23 01:28:01 visual_prompt]: 	Training 500/553. train loss: 37.0757,	1.1080 s / batch. (data: 2.78e-01). ETA=16:31:31, max mem: 20.9 GB 
[11/23 01:28:50 visual_prompt]: Epoch 3 / 100: avg data time: 1.35e-01, avg batch time: 0.9646, average train loss: 39.4319
[11/23 01:29:44 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3084, average loss: 28.8792
[11/23 01:29:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.40	
[11/23 01:29:44 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/23 01:31:27 visual_prompt]: 	Training 100/553. train loss: 151.6634,	0.8734 s / batch. (data: 2.54e-02). ETA=12:59:23, max mem: 20.9 GB 
[11/23 01:33:07 visual_prompt]: 	Training 200/553. train loss: 0.4510,	0.8262 s / batch. (data: 3.11e-04). ETA=12:15:53, max mem: 20.9 GB 
[11/23 01:34:45 visual_prompt]: 	Training 300/553. train loss: 7.5754,	1.2880 s / batch. (data: 4.54e-01). ETA=19:05:01, max mem: 20.9 GB 
[11/23 01:36:18 visual_prompt]: 	Training 400/553. train loss: 67.7845,	1.2680 s / batch. (data: 4.30e-01). ETA=18:45:10, max mem: 20.9 GB 
[11/23 01:37:56 visual_prompt]: 	Training 500/553. train loss: 0.0000,	2.9759 s / batch. (data: 2.17e+00). ETA=1 day, 19:55:39, max mem: 20.9 GB 
[11/23 01:38:49 visual_prompt]: Epoch 4 / 100: avg data time: 1.56e-01, avg batch time: 0.9841, average train loss: 53.2946
[11/23 01:39:44 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3112, average loss: 34.9306
[11/23 01:39:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.06	
[11/23 01:39:44 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/23 01:41:23 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8280 s / batch. (data: 2.89e-04). ETA=12:11:14, max mem: 20.9 GB 
[11/23 01:43:00 visual_prompt]: 	Training 200/553. train loss: 35.7910,	1.1909 s / batch. (data: 3.61e-01). ETA=17:29:45, max mem: 20.9 GB 
[11/23 01:44:37 visual_prompt]: 	Training 300/553. train loss: 13.4560,	0.8471 s / batch. (data: 3.27e-04). ETA=12:25:18, max mem: 20.9 GB 
[11/23 01:46:14 visual_prompt]: 	Training 400/553. train loss: 74.0575,	0.8314 s / batch. (data: 5.42e-03). ETA=12:10:05, max mem: 20.9 GB 
[11/23 01:47:51 visual_prompt]: 	Training 500/553. train loss: 51.1401,	0.8414 s / batch. (data: 9.33e-03). ETA=12:17:25, max mem: 20.9 GB 
[11/23 01:48:42 visual_prompt]: Epoch 5 / 100: avg data time: 1.45e-01, avg batch time: 0.9730, average train loss: 70.3916
[11/23 01:49:36 visual_prompt]: Inference (val):avg data time: 2.77e-04, avg batch time: 0.3115, average loss: 43.8182
[11/23 01:49:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.23	
[11/23 01:49:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/23 01:51:18 visual_prompt]: 	Training 100/553. train loss: 363.2985,	0.8453 s / batch. (data: 1.60e-02). ETA=12:18:42, max mem: 20.9 GB 
[11/23 01:52:53 visual_prompt]: 	Training 200/553. train loss: 621.9333,	0.8432 s / batch. (data: 1.12e-02). ETA=12:15:28, max mem: 20.9 GB 
[11/23 01:54:27 visual_prompt]: 	Training 300/553. train loss: 82.2972,	0.8720 s / batch. (data: 7.97e-03). ETA=12:39:09, max mem: 20.9 GB 
[11/23 01:56:07 visual_prompt]: 	Training 400/553. train loss: 33.9150,	0.8163 s / batch. (data: 5.40e-03). ETA=11:49:17, max mem: 20.9 GB 
[11/23 01:57:42 visual_prompt]: 	Training 500/553. train loss: 96.2137,	0.8199 s / batch. (data: 3.40e-04). ETA=11:51:02, max mem: 20.9 GB 
[11/23 01:58:31 visual_prompt]: Epoch 6 / 100: avg data time: 1.40e-01, avg batch time: 0.9670, average train loss: 103.8233
[11/23 01:59:26 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3103, average loss: 91.2159
[11/23 01:59:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.52	
[11/23 01:59:26 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/23 02:01:04 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8017 s / batch. (data: 3.08e-04). ETA=11:33:14, max mem: 20.9 GB 
[11/23 02:02:40 visual_prompt]: 	Training 200/553. train loss: 64.2342,	0.8290 s / batch. (data: 7.95e-03). ETA=11:55:25, max mem: 20.9 GB 
[11/23 02:04:18 visual_prompt]: 	Training 300/553. train loss: 5.0139,	1.4510 s / batch. (data: 6.43e-01). ETA=20:49:52, max mem: 20.9 GB 
[11/23 02:05:55 visual_prompt]: 	Training 400/553. train loss: 52.8119,	1.5314 s / batch. (data: 7.18e-01). ETA=21:56:34, max mem: 20.9 GB 
[11/23 02:07:31 visual_prompt]: 	Training 500/553. train loss: 68.9438,	0.8252 s / batch. (data: 5.41e-03). ETA=11:48:04, max mem: 20.9 GB 
[11/23 02:08:20 visual_prompt]: Epoch 7 / 100: avg data time: 1.39e-01, avg batch time: 0.9660, average train loss: 96.4033
[11/23 02:09:16 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3095, average loss: 174.5468
[11/23 02:09:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.70	
[11/23 02:09:16 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/23 02:10:54 visual_prompt]: 	Training 100/553. train loss: 53.3281,	0.8160 s / batch. (data: 3.60e-04). ETA=11:38:06, max mem: 20.9 GB 
[11/23 02:12:33 visual_prompt]: 	Training 200/553. train loss: 583.0183,	0.8236 s / batch. (data: 5.44e-03). ETA=11:43:11, max mem: 20.9 GB 
[11/23 02:14:09 visual_prompt]: 	Training 300/553. train loss: 62.8790,	0.8097 s / batch. (data: 2.91e-04). ETA=11:29:57, max mem: 20.9 GB 
[11/23 02:15:46 visual_prompt]: 	Training 400/553. train loss: 97.0186,	0.8560 s / batch. (data: 3.30e-04). ETA=12:08:00, max mem: 20.9 GB 
[11/23 02:17:22 visual_prompt]: 	Training 500/553. train loss: 524.1109,	1.1880 s / batch. (data: 3.43e-01). ETA=16:48:23, max mem: 20.9 GB 
[11/23 02:18:13 visual_prompt]: Epoch 8 / 100: avg data time: 1.44e-01, avg batch time: 0.9707, average train loss: 112.1865
[11/23 02:19:07 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3101, average loss: 16.1529
[11/23 02:19:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.73	
[11/23 02:19:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/23 02:20:47 visual_prompt]: 	Training 100/553. train loss: 0.3753,	0.8180 s / batch. (data: 3.07e-04). ETA=11:32:12, max mem: 20.9 GB 
[11/23 02:22:22 visual_prompt]: 	Training 200/553. train loss: 171.0539,	0.8760 s / batch. (data: 1.42e-02). ETA=12:19:52, max mem: 20.9 GB 
[11/23 02:23:59 visual_prompt]: 	Training 300/553. train loss: 55.7412,	1.5533 s / batch. (data: 7.16e-01). ETA=21:49:19, max mem: 20.9 GB 
[11/23 02:25:36 visual_prompt]: 	Training 400/553. train loss: 35.6600,	0.8204 s / batch. (data: 7.76e-04). ETA=11:30:08, max mem: 20.9 GB 
[11/23 02:27:12 visual_prompt]: 	Training 500/553. train loss: 203.6946,	0.8239 s / batch. (data: 1.05e-02). ETA=11:31:47, max mem: 20.9 GB 
[11/23 02:28:02 visual_prompt]: Epoch 9 / 100: avg data time: 1.39e-01, avg batch time: 0.9661, average train loss: 156.6385
[11/23 02:28:57 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3100, average loss: 34.7765
[11/23 02:28:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.50	
[11/23 02:28:57 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/23 02:30:39 visual_prompt]: 	Training 100/553. train loss: 35.8805,	0.8153 s / batch. (data: 3.02e-04). ETA=11:22:24, max mem: 20.9 GB 
[11/23 02:32:13 visual_prompt]: 	Training 200/553. train loss: 283.7655,	0.8280 s / batch. (data: 3.05e-04). ETA=11:31:41, max mem: 20.9 GB 
[11/23 02:33:49 visual_prompt]: 	Training 300/553. train loss: 90.8150,	0.8406 s / batch. (data: 2.84e-04). ETA=11:40:49, max mem: 20.9 GB 
[11/23 02:35:23 visual_prompt]: 	Training 400/553. train loss: 170.0476,	0.8203 s / batch. (data: 3.14e-04). ETA=11:22:32, max mem: 20.9 GB 
[11/23 02:37:00 visual_prompt]: 	Training 500/553. train loss: 16.0459,	0.8625 s / batch. (data: 2.24e-02). ETA=11:56:10, max mem: 20.9 GB 
[11/23 02:37:50 visual_prompt]: Epoch 10 / 100: avg data time: 1.38e-01, avg batch time: 0.9647, average train loss: 156.2801
[11/23 02:38:45 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3123, average loss: 7.4814
[11/23 02:38:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 57.60	
[11/23 02:38:45 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/23 02:40:27 visual_prompt]: 	Training 100/553. train loss: 436.6214,	0.8185 s / batch. (data: 5.40e-03). ETA=11:17:32, max mem: 20.9 GB 
[11/23 02:42:04 visual_prompt]: 	Training 200/553. train loss: 283.7959,	0.8403 s / batch. (data: 5.41e-03). ETA=11:34:12, max mem: 20.9 GB 
[11/23 02:43:39 visual_prompt]: 	Training 300/553. train loss: 7.6936,	2.0778 s / batch. (data: 1.24e+00). ETA=1 day, 4:33:06, max mem: 20.9 GB 
[11/23 02:45:14 visual_prompt]: 	Training 400/553. train loss: 61.5182,	0.8234 s / batch. (data: 5.39e-03). ETA=11:17:32, max mem: 20.9 GB 
[11/23 02:46:48 visual_prompt]: 	Training 500/553. train loss: 224.7789,	0.8630 s / batch. (data: 2.29e-02). ETA=11:48:38, max mem: 20.9 GB 
[11/23 02:47:38 visual_prompt]: Epoch 11 / 100: avg data time: 1.38e-01, avg batch time: 0.9644, average train loss: 171.8130
[11/23 02:48:33 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3100, average loss: 178.9753
[11/23 02:48:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.97	
[11/23 02:48:33 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/23 02:50:14 visual_prompt]: 	Training 100/553. train loss: 185.6118,	0.8185 s / batch. (data: 3.01e-04). ETA=11:10:01, max mem: 20.9 GB 
[11/23 02:51:51 visual_prompt]: 	Training 200/553. train loss: 41.3509,	1.8374 s / batch. (data: 1.01e+00). ETA=1 day, 1:01:02, max mem: 20.9 GB 
[11/23 02:53:25 visual_prompt]: 	Training 300/553. train loss: 61.1303,	0.8229 s / batch. (data: 3.02e-04). ETA=11:10:51, max mem: 20.9 GB 
[11/23 02:55:02 visual_prompt]: 	Training 400/553. train loss: 41.7388,	0.8440 s / batch. (data: 2.75e-04). ETA=11:26:41, max mem: 20.9 GB 
[11/23 02:56:37 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8520 s / batch. (data: 7.94e-04). ETA=11:31:46, max mem: 20.9 GB 
[11/23 02:57:27 visual_prompt]: Epoch 12 / 100: avg data time: 1.39e-01, avg batch time: 0.9653, average train loss: 194.8883
[11/23 02:58:21 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3100, average loss: 327.9139
[11/23 02:58:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.36	
[11/23 02:58:21 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/23 03:00:03 visual_prompt]: 	Training 100/553. train loss: 219.3569,	0.8553 s / batch. (data: 7.97e-03). ETA=11:32:18, max mem: 20.9 GB 
[11/23 03:01:35 visual_prompt]: 	Training 200/553. train loss: 397.0056,	0.8262 s / batch. (data: 1.05e-02). ETA=11:07:21, max mem: 20.9 GB 
[11/23 03:03:12 visual_prompt]: 	Training 300/553. train loss: 206.8777,	1.5963 s / batch. (data: 7.89e-01). ETA=21:26:41, max mem: 20.9 GB 
[11/23 03:04:47 visual_prompt]: 	Training 400/553. train loss: 107.5091,	0.8428 s / batch. (data: 7.96e-03). ETA=11:17:55, max mem: 20.9 GB 
[11/23 03:06:24 visual_prompt]: 	Training 500/553. train loss: 72.6317,	0.8250 s / batch. (data: 2.92e-04). ETA=11:02:16, max mem: 20.9 GB 
[11/23 03:07:14 visual_prompt]: Epoch 13 / 100: avg data time: 1.38e-01, avg batch time: 0.9638, average train loss: 154.9497
[11/23 03:08:09 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3093, average loss: 150.0563
[11/23 03:08:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.86	
[11/23 03:08:09 visual_prompt]: Best epoch 13: best metric: -150.056
[11/23 03:08:09 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/23 03:09:50 visual_prompt]: 	Training 100/553. train loss: 67.0006,	0.8320 s / batch. (data: 3.97e-04). ETA=11:05:43, max mem: 20.9 GB 
[11/23 03:11:26 visual_prompt]: 	Training 200/553. train loss: 235.3682,	0.8702 s / batch. (data: 5.04e-02). ETA=11:34:51, max mem: 20.9 GB 
[11/23 03:13:02 visual_prompt]: 	Training 300/553. train loss: 105.3834,	0.8274 s / batch. (data: 3.04e-04). ETA=10:59:20, max mem: 20.9 GB 
[11/23 03:14:37 visual_prompt]: 	Training 400/553. train loss: 222.6599,	0.8144 s / batch. (data: 7.95e-03). ETA=10:47:34, max mem: 20.9 GB 
[11/23 03:16:13 visual_prompt]: 	Training 500/553. train loss: 442.3081,	0.8160 s / batch. (data: 2.92e-04). ETA=10:47:31, max mem: 20.9 GB 
[11/23 03:17:03 visual_prompt]: Epoch 14 / 100: avg data time: 1.39e-01, avg batch time: 0.9656, average train loss: 190.7561
[11/23 03:17:58 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3107, average loss: 204.7610
[11/23 03:17:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.46	
[11/23 03:17:58 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/23 03:19:37 visual_prompt]: 	Training 100/553. train loss: 86.5963,	0.8280 s / batch. (data: 1.20e-02). ETA=10:54:54, max mem: 20.9 GB 
[11/23 03:21:11 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8440 s / batch. (data: 3.13e-04). ETA=11:06:11, max mem: 20.9 GB 
[11/23 03:22:50 visual_prompt]: 	Training 300/553. train loss: 107.6311,	0.8480 s / batch. (data: 3.08e-04). ETA=11:07:53, max mem: 20.9 GB 
[11/23 03:24:23 visual_prompt]: 	Training 400/553. train loss: 29.8066,	1.0569 s / batch. (data: 2.46e-01). ETA=13:50:40, max mem: 20.9 GB 
[11/23 03:26:00 visual_prompt]: 	Training 500/553. train loss: 232.2905,	0.8430 s / batch. (data: 1.72e-02). ETA=11:01:07, max mem: 20.9 GB 
[11/23 03:26:51 visual_prompt]: Epoch 15 / 100: avg data time: 1.37e-01, avg batch time: 0.9638, average train loss: 186.5540
[11/23 03:27:45 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3101, average loss: 120.4946
[11/23 03:27:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.99	
[11/23 03:27:45 visual_prompt]: Best epoch 15: best metric: -120.495
[11/23 03:27:45 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/23 03:29:25 visual_prompt]: 	Training 100/553. train loss: 43.8713,	0.8320 s / batch. (data: 7.95e-03). ETA=10:50:24, max mem: 20.9 GB 
[11/23 03:31:00 visual_prompt]: 	Training 200/553. train loss: 317.8667,	0.8313 s / batch. (data: 4.16e-04). ETA=10:48:29, max mem: 20.9 GB 
[11/23 03:32:37 visual_prompt]: 	Training 300/553. train loss: 10.5811,	0.8400 s / batch. (data: 8.51e-03). ETA=10:53:50, max mem: 20.9 GB 
[11/23 03:34:13 visual_prompt]: 	Training 400/553. train loss: 195.0448,	0.8492 s / batch. (data: 1.10e-02). ETA=10:59:39, max mem: 20.9 GB 
[11/23 03:35:48 visual_prompt]: 	Training 500/553. train loss: 188.4037,	1.0202 s / batch. (data: 2.03e-01). ETA=13:10:45, max mem: 20.9 GB 
[11/23 03:36:38 visual_prompt]: Epoch 16 / 100: avg data time: 1.37e-01, avg batch time: 0.9639, average train loss: 190.2302
[11/23 03:37:33 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3090, average loss: 2.1358
[11/23 03:37:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 52.24	
[11/23 03:37:33 visual_prompt]: Best epoch 16: best metric: -2.136
[11/23 03:37:33 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/23 03:39:12 visual_prompt]: 	Training 100/553. train loss: 243.7455,	0.8362 s / batch. (data: 1.59e-02). ETA=10:46:00, max mem: 20.9 GB 
[11/23 03:40:49 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8332 s / batch. (data: 1.55e-02). ETA=10:42:18, max mem: 20.9 GB 
[11/23 03:42:25 visual_prompt]: 	Training 300/553. train loss: 380.4587,	0.8320 s / batch. (data: 7.96e-03). ETA=10:39:57, max mem: 20.9 GB 
[11/23 03:44:01 visual_prompt]: 	Training 400/553. train loss: 251.5679,	1.0081 s / batch. (data: 2.01e-01). ETA=12:53:44, max mem: 20.9 GB 
[11/23 03:45:36 visual_prompt]: 	Training 500/553. train loss: 26.9485,	1.5014 s / batch. (data: 6.60e-01). ETA=19:09:52, max mem: 20.9 GB 
[11/23 03:46:27 visual_prompt]: Epoch 17 / 100: avg data time: 1.39e-01, avg batch time: 0.9661, average train loss: 160.8307
[11/23 03:47:22 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3098, average loss: 67.9313
[11/23 03:47:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.46	
[11/23 03:47:22 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/23 03:49:02 visual_prompt]: 	Training 100/553. train loss: 337.4742,	0.8320 s / batch. (data: 1.20e-02). ETA=10:35:06, max mem: 20.9 GB 
[11/23 03:50:40 visual_prompt]: 	Training 200/553. train loss: 31.8491,	0.8679 s / batch. (data: 1.09e-02). ETA=11:00:59, max mem: 20.9 GB 
[11/23 03:52:16 visual_prompt]: 	Training 300/553. train loss: 227.8670,	0.8400 s / batch. (data: 2.77e-04). ETA=10:38:23, max mem: 20.9 GB 
[11/23 03:53:51 visual_prompt]: 	Training 400/553. train loss: 127.6485,	0.8506 s / batch. (data: 2.87e-04). ETA=10:45:00, max mem: 20.9 GB 
[11/23 03:55:26 visual_prompt]: 	Training 500/553. train loss: 38.8039,	0.8467 s / batch. (data: 1.47e-02). ETA=10:40:40, max mem: 20.9 GB 
[11/23 03:56:15 visual_prompt]: Epoch 18 / 100: avg data time: 1.38e-01, avg batch time: 0.9644, average train loss: 172.3706
[11/23 03:57:10 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3091, average loss: 23.0419
[11/23 03:57:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.30	
[11/23 03:57:10 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/23 03:58:50 visual_prompt]: 	Training 100/553. train loss: 30.3725,	0.9437 s / batch. (data: 1.21e-01). ETA=11:51:40, max mem: 20.9 GB 
[11/23 04:00:26 visual_prompt]: 	Training 200/553. train loss: 76.1996,	0.8400 s / batch. (data: 3.14e-04). ETA=10:32:02, max mem: 20.9 GB 
[11/23 04:02:02 visual_prompt]: 	Training 300/553. train loss: 526.4801,	0.8402 s / batch. (data: 3.33e-04). ETA=10:30:47, max mem: 20.9 GB 
[11/23 04:03:39 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8243 s / batch. (data: 2.93e-04). ETA=10:17:27, max mem: 20.9 GB 
[11/23 04:05:11 visual_prompt]: 	Training 500/553. train loss: 9.3277,	0.8489 s / batch. (data: 1.56e-02). ETA=10:34:29, max mem: 20.9 GB 
[11/23 04:06:01 visual_prompt]: Epoch 19 / 100: avg data time: 1.33e-01, avg batch time: 0.9604, average train loss: 195.7430
[11/23 04:06:56 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3091, average loss: 76.6227
[11/23 04:06:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.08	
[11/23 04:06:56 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/23 04:08:34 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8223 s / batch. (data: 3.19e-04). ETA=10:12:29, max mem: 20.9 GB 
[11/23 04:10:11 visual_prompt]: 	Training 200/553. train loss: 80.9129,	0.8185 s / batch. (data: 3.20e-04). ETA=10:08:20, max mem: 20.9 GB 
[11/23 04:11:48 visual_prompt]: 	Training 300/553. train loss: 117.1381,	0.8146 s / batch. (data: 3.58e-04). ETA=10:04:03, max mem: 20.9 GB 
[11/23 04:13:23 visual_prompt]: 	Training 400/553. train loss: 17.3723,	0.8320 s / batch. (data: 2.88e-04). ETA=10:15:32, max mem: 20.9 GB 
[11/23 04:14:59 visual_prompt]: 	Training 500/553. train loss: 100.9234,	0.8280 s / batch. (data: 3.02e-04). ETA=10:11:14, max mem: 20.9 GB 
[11/23 04:15:50 visual_prompt]: Epoch 20 / 100: avg data time: 1.39e-01, avg batch time: 0.9662, average train loss: 160.8579
[11/23 04:16:45 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3113, average loss: 58.2357
[11/23 04:16:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.21	
[11/23 04:16:45 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/23 04:18:28 visual_prompt]: 	Training 100/553. train loss: 113.3547,	0.8370 s / batch. (data: 5.87e-03). ETA=10:15:46, max mem: 20.9 GB 
[11/23 04:20:02 visual_prompt]: 	Training 200/553. train loss: 94.7154,	0.8474 s / batch. (data: 1.60e-02). ETA=10:21:58, max mem: 20.9 GB 
[11/23 04:21:37 visual_prompt]: 	Training 300/553. train loss: 420.3132,	0.9520 s / batch. (data: 1.19e-01). ETA=11:37:10, max mem: 20.9 GB 
[11/23 04:23:13 visual_prompt]: 	Training 400/553. train loss: 258.5422,	0.8358 s / batch. (data: 2.95e-04). ETA=10:10:42, max mem: 20.9 GB 
[11/23 04:24:50 visual_prompt]: 	Training 500/553. train loss: 24.6469,	0.8160 s / batch. (data: 3.05e-04). ETA=9:54:51, max mem: 20.9 GB 
[11/23 04:25:40 visual_prompt]: Epoch 21 / 100: avg data time: 1.40e-01, avg batch time: 0.9662, average train loss: 152.4561
[11/23 04:26:34 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3099, average loss: 8.9101
[11/23 04:26:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.52	
[11/23 04:26:34 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/23 04:28:13 visual_prompt]: 	Training 100/553. train loss: 76.3283,	0.8199 s / batch. (data: 5.41e-03). ETA=9:55:35, max mem: 20.9 GB 
[11/23 04:29:49 visual_prompt]: 	Training 200/553. train loss: 141.7162,	0.8280 s / batch. (data: 3.04e-04). ETA=10:00:07, max mem: 20.9 GB 
[11/23 04:31:23 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8352 s / batch. (data: 5.41e-03). ETA=10:03:55, max mem: 20.9 GB 
[11/23 04:33:00 visual_prompt]: 	Training 400/553. train loss: 18.4423,	0.8183 s / batch. (data: 4.52e-04). ETA=9:50:20, max mem: 20.9 GB 
[11/23 04:34:37 visual_prompt]: 	Training 500/553. train loss: 34.7550,	0.8172 s / batch. (data: 3.08e-04). ETA=9:48:12, max mem: 20.9 GB 
[11/23 04:35:28 visual_prompt]: Epoch 22 / 100: avg data time: 1.39e-01, avg batch time: 0.9650, average train loss: 170.8602
[11/23 04:36:22 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3095, average loss: 246.9585
[11/23 04:36:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.10	
[11/23 04:36:22 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/23 04:38:03 visual_prompt]: 	Training 100/553. train loss: 186.1240,	0.8288 s / batch. (data: 1.06e-02). ETA=9:54:27, max mem: 20.9 GB 
[11/23 04:39:40 visual_prompt]: 	Training 200/553. train loss: 272.6903,	0.8440 s / batch. (data: 7.96e-03). ETA=10:03:55, max mem: 20.9 GB 
[11/23 04:41:18 visual_prompt]: 	Training 300/553. train loss: 47.7774,	0.8322 s / batch. (data: 3.19e-04). ETA=9:54:04, max mem: 20.9 GB 
[11/23 04:42:52 visual_prompt]: 	Training 400/553. train loss: 123.1810,	0.8418 s / batch. (data: 5.46e-03). ETA=9:59:31, max mem: 20.9 GB 
[11/23 04:44:26 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8040 s / batch. (data: 3.41e-04). ETA=9:31:16, max mem: 20.9 GB 
[11/23 04:45:16 visual_prompt]: Epoch 23 / 100: avg data time: 1.38e-01, avg batch time: 0.9647, average train loss: 160.6898
[11/23 04:46:10 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.3114, average loss: 47.8060
[11/23 04:46:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.12	
[11/23 04:46:11 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/23 04:47:47 visual_prompt]: 	Training 100/553. train loss: 14.8031,	0.8240 s / batch. (data: 3.08e-04). ETA=9:43:23, max mem: 20.9 GB 
[11/23 04:49:23 visual_prompt]: 	Training 200/553. train loss: 150.9551,	0.8214 s / batch. (data: 2.86e-04). ETA=9:40:10, max mem: 20.9 GB 
[11/23 04:50:59 visual_prompt]: 	Training 300/553. train loss: 100.0307,	0.8772 s / batch. (data: 3.87e-02). ETA=10:18:07, max mem: 20.9 GB 
[11/23 04:52:36 visual_prompt]: 	Training 400/553. train loss: 40.2626,	0.8398 s / batch. (data: 3.00e-04). ETA=9:50:22, max mem: 20.9 GB 
[11/23 04:54:14 visual_prompt]: 	Training 500/553. train loss: 169.1389,	0.8155 s / batch. (data: 3.23e-04). ETA=9:31:57, max mem: 20.9 GB 
[11/23 04:55:04 visual_prompt]: Epoch 24 / 100: avg data time: 1.38e-01, avg batch time: 0.9650, average train loss: 156.1355
[11/23 04:55:59 visual_prompt]: Inference (val):avg data time: 2.58e-04, avg batch time: 0.3127, average loss: 125.1586
[11/23 04:55:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.33	
[11/23 04:55:59 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/23 04:57:41 visual_prompt]: 	Training 100/553. train loss: 52.7218,	0.8280 s / batch. (data: 3.60e-04). ETA=9:38:37, max mem: 20.9 GB 
[11/23 04:59:14 visual_prompt]: 	Training 200/553. train loss: 223.0438,	0.8561 s / batch. (data: 2.10e-02). ETA=9:56:49, max mem: 20.9 GB 
[11/23 05:00:50 visual_prompt]: 	Training 300/553. train loss: 104.7876,	0.8486 s / batch. (data: 2.06e-02). ETA=9:50:08, max mem: 20.9 GB 
[11/23 05:02:25 visual_prompt]: 	Training 400/553. train loss: 683.1397,	0.8358 s / batch. (data: 3.37e-04). ETA=9:39:53, max mem: 20.9 GB 
[11/23 05:04:01 visual_prompt]: 	Training 500/553. train loss: 55.3867,	1.0479 s / batch. (data: 2.10e-01). ETA=12:05:15, max mem: 20.9 GB 
[11/23 05:04:52 visual_prompt]: Epoch 25 / 100: avg data time: 1.36e-01, avg batch time: 0.9633, average train loss: 164.1294
[11/23 05:05:46 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3097, average loss: 7.3253
[11/23 05:05:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.97	
[11/23 05:05:46 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/23 05:07:26 visual_prompt]: 	Training 100/553. train loss: 129.7458,	0.8169 s / batch. (data: 2.86e-04). ETA=9:23:18, max mem: 20.9 GB 
[11/23 05:09:03 visual_prompt]: 	Training 200/553. train loss: 1.3798,	1.5436 s / batch. (data: 7.27e-01). ETA=17:41:51, max mem: 20.9 GB 
[11/23 05:10:41 visual_prompt]: 	Training 300/553. train loss: 7.9059,	0.8493 s / batch. (data: 1.09e-02). ETA=9:42:48, max mem: 20.9 GB 
[11/23 05:12:15 visual_prompt]: 	Training 400/553. train loss: 55.3619,	0.8357 s / batch. (data: 3.25e-04). ETA=9:32:05, max mem: 20.9 GB 
[11/23 05:13:50 visual_prompt]: 	Training 500/553. train loss: 29.2409,	0.8405 s / batch. (data: 2.91e-04). ETA=9:33:58, max mem: 20.9 GB 
[11/23 05:14:40 visual_prompt]: Epoch 26 / 100: avg data time: 1.37e-01, avg batch time: 0.9639, average train loss: 145.6865
[11/23 05:15:34 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3104, average loss: 113.1753
[11/23 05:15:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.51	
[11/23 05:15:34 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/23 05:17:15 visual_prompt]: 	Training 100/553. train loss: 7.8318,	0.8306 s / batch. (data: 3.73e-04). ETA=9:25:07, max mem: 20.9 GB 
[11/23 05:18:50 visual_prompt]: 	Training 200/553. train loss: 111.5258,	1.1822 s / batch. (data: 3.57e-01). ETA=13:22:20, max mem: 20.9 GB 
[11/23 05:20:26 visual_prompt]: 	Training 300/553. train loss: 89.1500,	0.8245 s / batch. (data: 7.96e-03). ETA=9:18:11, max mem: 20.9 GB 
[11/23 05:22:03 visual_prompt]: 	Training 400/553. train loss: 120.9062,	0.8202 s / batch. (data: 2.98e-04). ETA=9:13:55, max mem: 20.9 GB 
[11/23 05:23:39 visual_prompt]: 	Training 500/553. train loss: 298.4228,	0.8364 s / batch. (data: 5.41e-03). ETA=9:23:30, max mem: 20.9 GB 
[11/23 05:24:27 visual_prompt]: Epoch 27 / 100: avg data time: 1.37e-01, avg batch time: 0.9640, average train loss: 150.7523
[11/23 05:25:22 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3107, average loss: 162.6716
[11/23 05:25:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.07	
[11/23 05:25:22 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/23 05:27:01 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8078 s / batch. (data: 3.58e-04). ETA=9:02:10, max mem: 20.9 GB 
[11/23 05:28:37 visual_prompt]: 	Training 200/553. train loss: 48.0571,	0.8320 s / batch. (data: 1.20e-02). ETA=9:17:00, max mem: 20.9 GB 
[11/23 05:30:14 visual_prompt]: 	Training 300/553. train loss: 1.7146,	0.8595 s / batch. (data: 1.32e-02). ETA=9:33:59, max mem: 20.9 GB 
[11/23 05:31:49 visual_prompt]: 	Training 400/553. train loss: 7.2133,	0.8310 s / batch. (data: 2.95e-04). ETA=9:13:35, max mem: 20.9 GB 
[11/23 05:33:23 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8197 s / batch. (data: 5.42e-03). ETA=9:04:39, max mem: 20.9 GB 
[11/23 05:34:15 visual_prompt]: Epoch 28 / 100: avg data time: 1.37e-01, avg batch time: 0.9638, average train loss: 158.9682
[11/23 05:35:09 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3117, average loss: 344.8593
[11/23 05:35:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.29	
[11/23 05:35:10 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/23 05:36:55 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8116 s / batch. (data: 3.05e-04). ETA=8:57:13, max mem: 20.9 GB 
[11/23 05:38:30 visual_prompt]: 	Training 200/553. train loss: 63.6507,	1.5899 s / batch. (data: 7.82e-01). ETA=17:29:45, max mem: 20.9 GB 
[11/23 05:40:03 visual_prompt]: 	Training 300/553. train loss: 61.6685,	0.8134 s / batch. (data: 3.02e-04). ETA=8:55:43, max mem: 20.9 GB 
[11/23 05:41:35 visual_prompt]: 	Training 400/553. train loss: 166.5122,	1.2133 s / batch. (data: 3.54e-01). ETA=13:17:04, max mem: 20.9 GB 
[11/23 05:43:11 visual_prompt]: 	Training 500/553. train loss: 145.6787,	0.8321 s / batch. (data: 1.20e-02). ETA=9:05:13, max mem: 20.9 GB 
[11/23 05:44:01 visual_prompt]: Epoch 29 / 100: avg data time: 1.35e-01, avg batch time: 0.9618, average train loss: 138.1748
[11/23 05:44:56 visual_prompt]: Inference (val):avg data time: 3.35e-04, avg batch time: 0.3099, average loss: 183.6311
[11/23 05:44:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.50	
[11/23 05:44:56 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/23 05:46:35 visual_prompt]: 	Training 100/553. train loss: 19.7214,	0.8389 s / batch. (data: 2.82e-04). ETA=9:07:32, max mem: 20.9 GB 
[11/23 05:48:12 visual_prompt]: 	Training 200/553. train loss: 509.7855,	0.8820 s / batch. (data: 5.41e-03). ETA=9:34:14, max mem: 20.9 GB 
[11/23 05:49:46 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8560 s / batch. (data: 7.96e-03). ETA=9:15:52, max mem: 20.9 GB 
[11/23 05:51:23 visual_prompt]: 	Training 400/553. train loss: 28.2085,	1.0739 s / batch. (data: 2.40e-01). ETA=11:35:35, max mem: 20.9 GB 
[11/23 05:52:58 visual_prompt]: 	Training 500/553. train loss: 231.7779,	1.3057 s / batch. (data: 4.77e-01). ETA=14:03:32, max mem: 20.9 GB 
[11/23 05:53:50 visual_prompt]: Epoch 30 / 100: avg data time: 1.39e-01, avg batch time: 0.9649, average train loss: 124.2348
[11/23 05:54:45 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3102, average loss: 68.2561
[11/23 05:54:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.27	
[11/23 05:54:45 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[11/23 05:56:26 visual_prompt]: 	Training 100/553. train loss: 158.6663,	0.8440 s / batch. (data: 3.04e-04). ETA=9:03:08, max mem: 20.9 GB 
[11/23 05:58:04 visual_prompt]: 	Training 200/553. train loss: 74.8282,	0.8235 s / batch. (data: 2.71e-04). ETA=8:48:32, max mem: 20.9 GB 
[11/23 05:59:37 visual_prompt]: 	Training 300/553. train loss: 216.2123,	0.8234 s / batch. (data: 2.82e-04). ETA=8:47:07, max mem: 20.9 GB 
[11/23 06:01:12 visual_prompt]: 	Training 400/553. train loss: 41.8518,	1.0444 s / batch. (data: 2.27e-01). ETA=11:06:50, max mem: 20.9 GB 
[11/23 06:02:47 visual_prompt]: 	Training 500/553. train loss: 106.7536,	0.8120 s / batch. (data: 3.04e-04). ETA=8:37:06, max mem: 20.9 GB 
[11/23 06:03:36 visual_prompt]: Epoch 31 / 100: avg data time: 1.34e-01, avg batch time: 0.9611, average train loss: 147.2976
[11/23 06:04:31 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3102, average loss: 26.1778
[11/23 06:04:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.19	
[11/23 06:04:31 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[11/23 06:06:12 visual_prompt]: 	Training 100/553. train loss: 12.1126,	0.8400 s / batch. (data: 3.07e-04). ETA=8:52:48, max mem: 20.9 GB 
[11/23 06:07:47 visual_prompt]: 	Training 200/553. train loss: 213.4811,	0.8231 s / batch. (data: 7.57e-04). ETA=8:40:40, max mem: 20.9 GB 
[11/23 06:09:26 visual_prompt]: 	Training 300/553. train loss: 59.8548,	0.8320 s / batch. (data: 3.06e-04). ETA=8:44:56, max mem: 20.9 GB 
[11/23 06:11:02 visual_prompt]: 	Training 400/553. train loss: 17.2170,	0.8280 s / batch. (data: 2.97e-04). ETA=8:41:02, max mem: 20.9 GB 
[11/23 06:12:34 visual_prompt]: 	Training 500/553. train loss: 19.9192,	0.8292 s / batch. (data: 5.40e-03). ETA=8:40:25, max mem: 20.9 GB 
[11/23 06:13:22 visual_prompt]: Epoch 32 / 100: avg data time: 1.35e-01, avg batch time: 0.9601, average train loss: 150.7823
[11/23 06:14:16 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3091, average loss: 6.9319
[11/23 06:14:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.54	
[11/23 06:14:16 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[11/23 06:15:54 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8160 s / batch. (data: 2.95e-04). ETA=8:30:03, max mem: 20.9 GB 
[11/23 06:17:30 visual_prompt]: 	Training 200/553. train loss: 62.5337,	0.9015 s / batch. (data: 9.45e-02). ETA=9:21:59, max mem: 20.9 GB 
[11/23 06:19:05 visual_prompt]: 	Training 300/553. train loss: 8.7674,	0.8188 s / batch. (data: 2.74e-04). ETA=8:29:04, max mem: 20.9 GB 
[11/23 06:20:40 visual_prompt]: 	Training 400/553. train loss: 132.0480,	0.8239 s / batch. (data: 1.05e-02). ETA=8:30:52, max mem: 20.9 GB 
[11/23 06:22:15 visual_prompt]: 	Training 500/553. train loss: 26.6511,	0.8478 s / batch. (data: 1.05e-02). ETA=8:44:16, max mem: 20.9 GB 
[11/23 06:23:04 visual_prompt]: Epoch 33 / 100: avg data time: 1.27e-01, avg batch time: 0.9534, average train loss: 159.6651
[11/23 06:23:58 visual_prompt]: Inference (val):avg data time: 4.97e-04, avg batch time: 0.3116, average loss: 174.1229
[11/23 06:23:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.58	
[11/23 06:23:58 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[11/23 06:25:38 visual_prompt]: 	Training 100/553. train loss: 52.6122,	0.8160 s / batch. (data: 3.28e-04). ETA=8:22:31, max mem: 20.9 GB 
[11/23 06:27:10 visual_prompt]: 	Training 200/553. train loss: 68.4719,	0.8242 s / batch. (data: 1.23e-02). ETA=8:26:11, max mem: 20.9 GB 
[11/23 06:28:46 visual_prompt]: 	Training 300/553. train loss: 172.0752,	0.8107 s / batch. (data: 3.23e-04). ETA=8:16:34, max mem: 20.9 GB 
[11/23 06:30:22 visual_prompt]: 	Training 400/553. train loss: 610.7329,	0.8400 s / batch. (data: 2.80e-04). ETA=8:33:07, max mem: 20.9 GB 
[11/23 06:31:57 visual_prompt]: 	Training 500/553. train loss: 380.0856,	1.4240 s / batch. (data: 5.59e-01). ETA=14:27:27, max mem: 20.9 GB 
[11/23 06:32:47 visual_prompt]: Epoch 34 / 100: avg data time: 1.29e-01, avg batch time: 0.9563, average train loss: 164.0386
[11/23 06:33:41 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3112, average loss: 78.3900
[11/23 06:33:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 55.08	
[11/23 06:33:41 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[11/23 06:35:23 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8099 s / batch. (data: 8.33e-03). ETA=8:11:17, max mem: 20.9 GB 
[11/23 06:37:00 visual_prompt]: 	Training 200/553. train loss: 61.8306,	0.8200 s / batch. (data: 3.26e-04). ETA=8:16:03, max mem: 20.9 GB 
[11/23 06:38:34 visual_prompt]: 	Training 300/553. train loss: 18.9457,	0.8345 s / batch. (data: 2.90e-04). ETA=8:23:26, max mem: 20.9 GB 
[11/23 06:40:09 visual_prompt]: 	Training 400/553. train loss: 100.3453,	0.8320 s / batch. (data: 3.14e-04). ETA=8:20:33, max mem: 20.9 GB 
[11/23 06:41:44 visual_prompt]: 	Training 500/553. train loss: 194.8438,	0.8522 s / batch. (data: 8.18e-03). ETA=8:31:17, max mem: 20.9 GB 
[11/23 06:42:35 visual_prompt]: Epoch 35 / 100: avg data time: 1.39e-01, avg batch time: 0.9656, average train loss: 160.5242
[11/23 06:43:30 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.3113, average loss: 12.3208
[11/23 06:43:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.55	
[11/23 06:43:30 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[11/23 06:45:09 visual_prompt]: 	Training 100/553. train loss: 15.6176,	0.8274 s / batch. (data: 3.09e-04). ETA=8:14:19, max mem: 20.9 GB 
[11/23 06:46:46 visual_prompt]: 	Training 200/553. train loss: 491.0982,	0.8246 s / batch. (data: 3.32e-04). ETA=8:11:16, max mem: 20.9 GB 
[11/23 06:48:23 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8522 s / batch. (data: 2.42e-02). ETA=8:26:17, max mem: 20.9 GB 
[11/23 06:49:59 visual_prompt]: 	Training 400/553. train loss: 8.7644,	0.8205 s / batch. (data: 2.79e-04). ETA=8:06:03, max mem: 20.9 GB 
[11/23 06:51:35 visual_prompt]: 	Training 500/553. train loss: 37.5856,	0.8723 s / batch. (data: 5.38e-02). ETA=8:35:18, max mem: 20.9 GB 
[11/23 06:52:22 visual_prompt]: Epoch 36 / 100: avg data time: 1.36e-01, avg batch time: 0.9628, average train loss: 152.7814
[11/23 06:53:17 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3111, average loss: 30.2378
[11/23 06:53:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.42	
[11/23 06:53:17 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[11/23 06:54:57 visual_prompt]: 	Training 100/553. train loss: 75.8718,	0.8189 s / batch. (data: 2.94e-04). ETA=8:01:41, max mem: 20.9 GB 
[11/23 06:56:32 visual_prompt]: 	Training 200/553. train loss: 103.2251,	0.8370 s / batch. (data: 2.78e-04). ETA=8:10:54, max mem: 20.9 GB 
[11/23 06:58:08 visual_prompt]: 	Training 300/553. train loss: 166.5088,	1.1331 s / batch. (data: 3.16e-01). ETA=11:02:43, max mem: 20.9 GB 
[11/23 06:59:47 visual_prompt]: 	Training 400/553. train loss: 6.2100,	1.8117 s / batch. (data: 9.85e-01). ETA=17:36:33, max mem: 20.9 GB 
[11/23 07:01:19 visual_prompt]: 	Training 500/553. train loss: 125.8673,	0.8356 s / batch. (data: 6.47e-03). ETA=8:05:54, max mem: 20.9 GB 
[11/23 07:02:11 visual_prompt]: Epoch 37 / 100: avg data time: 1.38e-01, avg batch time: 0.9651, average train loss: 124.8053
[11/23 07:03:05 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3097, average loss: 122.9183
[11/23 07:03:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.28	
[11/23 07:03:05 visual_prompt]: Stopping early.
[11/23 07:03:05 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 07:03:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 07:03:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/23 07:03:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 07:03:05 visual_prompt]: Training with config:
[11/23 07:03:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/23 07:03:05 visual_prompt]: Loading training data...
[11/23 07:03:05 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 07:03:05 visual_prompt]: Loading validation data...
[11/23 07:03:05 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 07:03:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 07:03:10 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 07:03:10 visual_prompt]: tuned percent:0.525
[11/23 07:03:10 visual_prompt]: Device used for model: 0
[11/23 07:03:10 visual_prompt]: Setting up Evaluator...
[11/23 07:03:10 visual_prompt]: Setting up Trainer...
[11/23 07:03:10 visual_prompt]: 	Setting up the optimizer...
[11/23 07:03:10 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 07:04:50 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8359 s / batch. (data: 4.60e-04). ETA=12:49:03, max mem: 20.9 GB 
[11/23 07:06:24 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8760 s / batch. (data: 2.86e-04). ETA=13:24:28, max mem: 20.9 GB 
[11/23 07:08:02 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8550 s / batch. (data: 3.69e-02). ETA=13:03:42, max mem: 20.9 GB 
[11/23 07:09:37 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8254 s / batch. (data: 3.05e-04). ETA=12:35:14, max mem: 20.9 GB 
[11/23 07:11:15 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8720 s / batch. (data: 1.19e-02). ETA=13:16:28, max mem: 20.9 GB 
[11/23 07:12:06 visual_prompt]: Epoch 1 / 100: avg data time: 1.34e-01, avg batch time: 0.9680, average train loss: 1.5403
[11/23 07:13:00 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3116, average loss: 1.5201
[11/23 07:13:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 07:13:00 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/23 07:14:39 visual_prompt]: 	Training 100/553. train loss: 10.6743,	0.8360 s / batch. (data: 3.24e-04). ETA=12:41:24, max mem: 20.9 GB 
[11/23 07:16:15 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8272 s / batch. (data: 1.12e-02). ETA=12:32:03, max mem: 20.9 GB 
[11/23 07:17:53 visual_prompt]: 	Training 300/553. train loss: 0.8119,	1.0164 s / batch. (data: 1.79e-01). ETA=15:22:22, max mem: 20.9 GB 
[11/23 07:19:28 visual_prompt]: 	Training 400/553. train loss: 11.9164,	0.8440 s / batch. (data: 7.97e-03). ETA=12:44:28, max mem: 20.9 GB 
[11/23 07:21:05 visual_prompt]: 	Training 500/553. train loss: 8.2818,	0.8200 s / batch. (data: 2.98e-04). ETA=12:21:23, max mem: 20.9 GB 
[11/23 07:21:55 visual_prompt]: Epoch 2 / 100: avg data time: 1.34e-01, avg batch time: 0.9662, average train loss: 15.2291
[11/23 07:22:49 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3093, average loss: 15.8872
[11/23 07:22:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.27	
[11/23 07:22:49 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/23 07:24:28 visual_prompt]: 	Training 100/553. train loss: 37.7481,	0.8203 s / batch. (data: 5.41e-03). ETA=12:19:31, max mem: 20.9 GB 
[11/23 07:26:04 visual_prompt]: 	Training 200/553. train loss: 27.9482,	1.0840 s / batch. (data: 2.41e-01). ETA=16:15:31, max mem: 20.9 GB 
[11/23 07:27:40 visual_prompt]: 	Training 300/553. train loss: 10.3654,	0.8507 s / batch. (data: 2.95e-04). ETA=12:44:08, max mem: 20.9 GB 
[11/23 07:29:16 visual_prompt]: 	Training 400/553. train loss: 96.2913,	0.8327 s / batch. (data: 7.96e-03). ETA=12:26:33, max mem: 20.9 GB 
[11/23 07:30:53 visual_prompt]: 	Training 500/553. train loss: 67.0479,	1.0800 s / batch. (data: 2.54e-01). ETA=16:06:30, max mem: 20.9 GB 
[11/23 07:31:42 visual_prompt]: Epoch 3 / 100: avg data time: 1.33e-01, avg batch time: 0.9638, average train loss: 40.2478
[11/23 07:32:37 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3103, average loss: 14.2773
[11/23 07:32:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.73	
[11/23 07:32:37 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/23 07:34:18 visual_prompt]: 	Training 100/553. train loss: 16.8942,	0.8600 s / batch. (data: 3.05e-04). ETA=12:47:25, max mem: 20.9 GB 
[11/23 07:35:54 visual_prompt]: 	Training 200/553. train loss: 4.3071,	0.8400 s / batch. (data: 1.19e-02). ETA=12:28:09, max mem: 20.9 GB 
[11/23 07:37:31 visual_prompt]: 	Training 300/553. train loss: 86.3309,	1.1198 s / batch. (data: 3.01e-01). ETA=16:35:33, max mem: 20.9 GB 
[11/23 07:39:03 visual_prompt]: 	Training 400/553. train loss: 79.9603,	0.9092 s / batch. (data: 9.56e-02). ETA=13:26:45, max mem: 20.9 GB 
[11/23 07:40:41 visual_prompt]: 	Training 500/553. train loss: 0.0007,	3.3659 s / batch. (data: 2.55e+00). ETA=2 days, 1:41:07, max mem: 20.9 GB 
[11/23 07:41:32 visual_prompt]: Epoch 4 / 100: avg data time: 1.39e-01, avg batch time: 0.9677, average train loss: 63.0865
[11/23 07:42:27 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3106, average loss: 14.3394
[11/23 07:42:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.73	
[11/23 07:42:27 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/23 07:44:04 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8360 s / batch. (data: 3.08e-04). ETA=12:18:18, max mem: 20.9 GB 
[11/23 07:45:41 visual_prompt]: 	Training 200/553. train loss: 38.8275,	1.0153 s / batch. (data: 1.92e-01). ETA=14:54:58, max mem: 20.9 GB 
[11/23 07:47:18 visual_prompt]: 	Training 300/553. train loss: 151.9556,	0.8240 s / batch. (data: 5.37e-03). ETA=12:04:57, max mem: 20.9 GB 
[11/23 07:48:53 visual_prompt]: 	Training 400/553. train loss: 228.9831,	0.8320 s / batch. (data: 2.81e-04). ETA=12:10:35, max mem: 20.9 GB 
[11/23 07:50:30 visual_prompt]: 	Training 500/553. train loss: 162.6530,	0.8236 s / batch. (data: 7.96e-03). ETA=12:01:50, max mem: 20.9 GB 
[11/23 07:51:21 visual_prompt]: Epoch 5 / 100: avg data time: 1.36e-01, avg batch time: 0.9655, average train loss: 59.6810
[11/23 07:52:15 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3104, average loss: 53.9044
[11/23 07:52:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.30	
[11/23 07:52:15 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/23 07:53:57 visual_prompt]: 	Training 100/553. train loss: 39.3588,	0.8400 s / batch. (data: 7.68e-04). ETA=12:14:06, max mem: 20.9 GB 
[11/23 07:55:32 visual_prompt]: 	Training 200/553. train loss: 64.8807,	0.8320 s / batch. (data: 2.81e-04). ETA=12:05:43, max mem: 20.9 GB 
[11/23 07:57:07 visual_prompt]: 	Training 300/553. train loss: 19.3553,	0.8280 s / batch. (data: 3.62e-04). ETA=12:00:48, max mem: 20.9 GB 
[11/23 07:58:46 visual_prompt]: 	Training 400/553. train loss: 41.5402,	0.8278 s / batch. (data: 5.42e-03). ETA=11:59:19, max mem: 20.9 GB 
[11/23 08:00:21 visual_prompt]: 	Training 500/553. train loss: 67.3078,	0.8112 s / batch. (data: 3.15e-04). ETA=11:43:30, max mem: 20.9 GB 
[11/23 08:01:11 visual_prompt]: Epoch 6 / 100: avg data time: 1.39e-01, avg batch time: 0.9681, average train loss: 76.6691
[11/23 08:02:05 visual_prompt]: Inference (val):avg data time: 3.14e-04, avg batch time: 0.3110, average loss: 44.6269
[11/23 08:02:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.56	
[11/23 08:02:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/23 08:03:43 visual_prompt]: 	Training 100/553. train loss: 801.0915,	0.8322 s / batch. (data: 8.11e-03). ETA=11:59:36, max mem: 20.9 GB 
[11/23 08:05:20 visual_prompt]: 	Training 200/553. train loss: 8.7732,	0.8243 s / batch. (data: 1.47e-03). ETA=11:51:21, max mem: 20.9 GB 
[11/23 08:06:59 visual_prompt]: 	Training 300/553. train loss: 77.1621,	1.7385 s / batch. (data: 9.26e-01). ETA=1 day, 0:57:27, max mem: 20.9 GB 
[11/23 08:08:35 visual_prompt]: 	Training 400/553. train loss: 87.9645,	1.7440 s / batch. (data: 9.25e-01). ETA=1 day, 0:59:18, max mem: 20.9 GB 
[11/23 08:10:10 visual_prompt]: 	Training 500/553. train loss: 22.3223,	0.8598 s / batch. (data: 7.81e-03). ETA=12:17:46, max mem: 20.9 GB 
[11/23 08:10:58 visual_prompt]: Epoch 7 / 100: avg data time: 1.36e-01, avg batch time: 0.9632, average train loss: 118.3119
[11/23 08:11:53 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3103, average loss: 200.2819
[11/23 08:11:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.85	
[11/23 08:11:53 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/23 08:13:30 visual_prompt]: 	Training 100/553. train loss: 38.9741,	0.8318 s / batch. (data: 3.09e-04). ETA=11:51:36, max mem: 20.9 GB 
[11/23 08:15:08 visual_prompt]: 	Training 200/553. train loss: 10.0451,	0.8238 s / batch. (data: 3.38e-04). ETA=11:43:22, max mem: 20.9 GB 
[11/23 08:16:44 visual_prompt]: 	Training 300/553. train loss: 94.3741,	0.8314 s / batch. (data: 2.87e-04). ETA=11:48:28, max mem: 20.9 GB 
[11/23 08:18:20 visual_prompt]: 	Training 400/553. train loss: 1.2774,	0.8280 s / batch. (data: 3.02e-04). ETA=11:44:11, max mem: 20.9 GB 
[11/23 08:19:56 visual_prompt]: 	Training 500/553. train loss: 130.4558,	1.3377 s / batch. (data: 5.12e-01). ETA=18:55:28, max mem: 20.9 GB 
[11/23 08:20:46 visual_prompt]: Epoch 8 / 100: avg data time: 1.36e-01, avg batch time: 0.9638, average train loss: 105.7661
[11/23 08:21:40 visual_prompt]: Inference (val):avg data time: 3.70e-04, avg batch time: 0.3112, average loss: 36.9941
[11/23 08:21:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.93	
[11/23 08:21:40 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/23 08:23:18 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8323 s / batch. (data: 7.41e-04). ETA=11:44:18, max mem: 20.9 GB 
[11/23 08:24:54 visual_prompt]: 	Training 200/553. train loss: 0.3531,	0.8472 s / batch. (data: 1.11e-02). ETA=11:55:32, max mem: 20.9 GB 
[11/23 08:26:30 visual_prompt]: 	Training 300/553. train loss: 52.0310,	1.3080 s / batch. (data: 4.94e-01). ETA=18:22:32, max mem: 20.9 GB 
[11/23 08:28:07 visual_prompt]: 	Training 400/553. train loss: 45.0630,	0.8360 s / batch. (data: 1.20e-02). ETA=11:43:18, max mem: 20.9 GB 
[11/23 08:29:44 visual_prompt]: 	Training 500/553. train loss: 277.6619,	0.8510 s / batch. (data: 1.10e-02). ETA=11:54:30, max mem: 20.9 GB 
[11/23 08:30:33 visual_prompt]: Epoch 9 / 100: avg data time: 1.38e-01, avg batch time: 0.9644, average train loss: 126.2693
[11/23 08:31:28 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.3125, average loss: 152.5892
[11/23 08:31:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.18	
[11/23 08:31:28 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/23 08:33:09 visual_prompt]: 	Training 100/553. train loss: 187.3826,	0.8200 s / batch. (data: 4.54e-04). ETA=11:26:21, max mem: 20.9 GB 
[11/23 08:34:44 visual_prompt]: 	Training 200/553. train loss: 37.7419,	0.8080 s / batch. (data: 3.13e-04). ETA=11:14:58, max mem: 20.9 GB 
[11/23 08:36:19 visual_prompt]: 	Training 300/553. train loss: 90.5083,	0.8266 s / batch. (data: 3.11e-04). ETA=11:29:10, max mem: 20.9 GB 
[11/23 08:37:53 visual_prompt]: 	Training 400/553. train loss: 265.5869,	0.8280 s / batch. (data: 2.88e-04). ETA=11:28:56, max mem: 20.9 GB 
[11/23 08:39:30 visual_prompt]: 	Training 500/553. train loss: 52.3765,	0.8399 s / batch. (data: 1.05e-02). ETA=11:37:27, max mem: 20.9 GB 
[11/23 08:40:21 visual_prompt]: Epoch 10 / 100: avg data time: 1.37e-01, avg batch time: 0.9642, average train loss: 146.8459
[11/23 08:41:16 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3100, average loss: 202.4706
[11/23 08:41:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.96	
[11/23 08:41:16 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/23 08:42:57 visual_prompt]: 	Training 100/553. train loss: 170.1281,	0.8349 s / batch. (data: 3.01e-04). ETA=11:31:10, max mem: 20.9 GB 
[11/23 08:44:34 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8176 s / batch. (data: 5.40e-03). ETA=11:15:30, max mem: 20.9 GB 
[11/23 08:46:10 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.0155 s / batch. (data: 1.16e+00). ETA=1 day, 3:41:47, max mem: 20.9 GB 
[11/23 08:47:44 visual_prompt]: 	Training 400/553. train loss: 144.2417,	0.8201 s / batch. (data: 2.87e-04). ETA=11:14:48, max mem: 20.9 GB 
[11/23 08:49:19 visual_prompt]: 	Training 500/553. train loss: 227.5852,	0.8320 s / batch. (data: 2.77e-04). ETA=11:23:14, max mem: 20.9 GB 
[11/23 08:50:08 visual_prompt]: Epoch 11 / 100: avg data time: 1.36e-01, avg batch time: 0.9631, average train loss: 167.5750
[11/23 08:51:03 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3099, average loss: 24.0446
[11/23 08:51:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.40	
[11/23 08:51:03 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/23 08:52:44 visual_prompt]: 	Training 100/553. train loss: 120.9846,	0.8224 s / batch. (data: 2.87e-04). ETA=11:13:13, max mem: 20.9 GB 
[11/23 08:54:21 visual_prompt]: 	Training 200/553. train loss: 51.3779,	1.1440 s / batch. (data: 3.22e-01). ETA=15:34:34, max mem: 20.9 GB 
[11/23 08:55:55 visual_prompt]: 	Training 300/553. train loss: 18.5274,	0.8640 s / batch. (data: 7.95e-03). ETA=11:44:23, max mem: 20.9 GB 
[11/23 08:57:32 visual_prompt]: 	Training 400/553. train loss: 151.8916,	0.8280 s / batch. (data: 3.12e-04). ETA=11:13:39, max mem: 20.9 GB 
[11/23 08:59:07 visual_prompt]: 	Training 500/553. train loss: 212.2830,	0.8394 s / batch. (data: 5.41e-03). ETA=11:21:34, max mem: 20.9 GB 
[11/23 08:59:57 visual_prompt]: Epoch 12 / 100: avg data time: 1.37e-01, avg batch time: 0.9653, average train loss: 174.7716
[11/23 09:00:51 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3115, average loss: 324.7250
[11/23 09:00:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.69	
[11/23 09:00:51 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/23 09:02:33 visual_prompt]: 	Training 100/553. train loss: 46.6780,	0.8428 s / batch. (data: 7.99e-03). ETA=11:22:08, max mem: 20.9 GB 
[11/23 09:04:06 visual_prompt]: 	Training 200/553. train loss: 108.3145,	0.8105 s / batch. (data: 2.99e-04). ETA=10:54:38, max mem: 20.9 GB 
[11/23 09:05:43 visual_prompt]: 	Training 300/553. train loss: 56.2402,	1.5774 s / batch. (data: 7.48e-01). ETA=21:11:31, max mem: 20.9 GB 
[11/23 09:07:17 visual_prompt]: 	Training 400/553. train loss: 401.1780,	0.8328 s / batch. (data: 3.50e-04). ETA=11:09:54, max mem: 20.9 GB 
[11/23 09:08:54 visual_prompt]: 	Training 500/553. train loss: 278.3896,	0.8426 s / batch. (data: 2.69e-04). ETA=11:16:24, max mem: 20.9 GB 
[11/23 09:09:45 visual_prompt]: Epoch 13 / 100: avg data time: 1.38e-01, avg batch time: 0.9639, average train loss: 174.0920
[11/23 09:10:39 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3109, average loss: 44.2031
[11/23 09:10:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 50.94	
[11/23 09:10:39 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/23 09:12:20 visual_prompt]: 	Training 100/553. train loss: 1101.5603,	0.8292 s / batch. (data: 2.91e-04). ETA=11:03:31, max mem: 20.9 GB 
[11/23 09:13:55 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8252 s / batch. (data: 1.29e-02). ETA=10:58:56, max mem: 20.9 GB 
[11/23 09:15:32 visual_prompt]: 	Training 300/553. train loss: 54.3812,	0.8160 s / batch. (data: 2.83e-04). ETA=10:50:15, max mem: 20.9 GB 
[11/23 09:17:07 visual_prompt]: 	Training 400/553. train loss: 266.3464,	0.8360 s / batch. (data: 3.16e-04). ETA=11:04:45, max mem: 20.9 GB 
[11/23 09:18:43 visual_prompt]: 	Training 500/553. train loss: 163.6314,	0.8150 s / batch. (data: 3.02e-04). ETA=10:46:42, max mem: 20.9 GB 
[11/23 09:19:33 visual_prompt]: Epoch 14 / 100: avg data time: 1.39e-01, avg batch time: 0.9654, average train loss: 158.8360
[11/23 09:20:28 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3096, average loss: 98.8413
[11/23 09:20:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.20	
[11/23 09:20:28 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/23 09:22:07 visual_prompt]: 	Training 100/553. train loss: 106.0826,	0.8440 s / batch. (data: 3.07e-04). ETA=11:07:35, max mem: 20.9 GB 
[11/23 09:23:42 visual_prompt]: 	Training 200/553. train loss: 797.1801,	0.8247 s / batch. (data: 2.95e-04). ETA=10:50:55, max mem: 20.9 GB 
[11/23 09:25:20 visual_prompt]: 	Training 300/553. train loss: 390.2479,	0.8745 s / batch. (data: 1.03e-02). ETA=11:28:48, max mem: 20.9 GB 
[11/23 09:26:54 visual_prompt]: 	Training 400/553. train loss: 66.4679,	1.0240 s / batch. (data: 1.89e-01). ETA=13:24:48, max mem: 20.9 GB 
[11/23 09:28:30 visual_prompt]: 	Training 500/553. train loss: 56.4750,	0.8400 s / batch. (data: 2.97e-04). ETA=10:58:49, max mem: 20.9 GB 
[11/23 09:29:22 visual_prompt]: Epoch 15 / 100: avg data time: 1.38e-01, avg batch time: 0.9653, average train loss: 165.2661
[11/23 09:30:16 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3098, average loss: 518.4259
[11/23 09:30:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.15	
[11/23 09:30:16 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/23 09:31:55 visual_prompt]: 	Training 100/553. train loss: 103.7124,	0.8360 s / batch. (data: 3.00e-04). ETA=10:53:33, max mem: 20.9 GB 
[11/23 09:33:31 visual_prompt]: 	Training 200/553. train loss: 263.4484,	0.8521 s / batch. (data: 8.94e-03). ETA=11:04:41, max mem: 20.9 GB 
[11/23 09:35:08 visual_prompt]: 	Training 300/553. train loss: 192.0815,	0.8283 s / batch. (data: 1.01e-03). ETA=10:44:44, max mem: 20.9 GB 
[11/23 09:36:43 visual_prompt]: 	Training 400/553. train loss: 154.5278,	0.8498 s / batch. (data: 2.76e-04). ETA=11:00:02, max mem: 20.9 GB 
[11/23 09:38:18 visual_prompt]: 	Training 500/553. train loss: 77.0727,	1.0193 s / batch. (data: 2.01e-01). ETA=13:10:01, max mem: 20.9 GB 
[11/23 09:39:09 visual_prompt]: Epoch 16 / 100: avg data time: 1.37e-01, avg batch time: 0.9633, average train loss: 160.8979
[11/23 09:40:04 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3099, average loss: 42.0164
[11/23 09:40:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 51.17	
[11/23 09:40:04 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/23 09:41:43 visual_prompt]: 	Training 100/553. train loss: 53.9767,	0.8560 s / batch. (data: 7.97e-03). ETA=11:01:17, max mem: 20.9 GB 
[11/23 09:43:20 visual_prompt]: 	Training 200/553. train loss: 253.7905,	0.8360 s / batch. (data: 7.96e-03). ETA=10:44:26, max mem: 20.9 GB 
[11/23 09:44:56 visual_prompt]: 	Training 300/553. train loss: 176.7908,	0.8551 s / batch. (data: 2.96e-04). ETA=10:57:43, max mem: 20.9 GB 
[11/23 09:46:31 visual_prompt]: 	Training 400/553. train loss: 53.5881,	0.8242 s / batch. (data: 1.66e-02). ETA=10:32:36, max mem: 20.9 GB 
[11/23 09:48:06 visual_prompt]: 	Training 500/553. train loss: 101.5773,	1.0971 s / batch. (data: 2.74e-01). ETA=14:00:15, max mem: 20.9 GB 
[11/23 09:48:58 visual_prompt]: Epoch 17 / 100: avg data time: 1.38e-01, avg batch time: 0.9657, average train loss: 177.7189
[11/23 09:49:53 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3111, average loss: 180.0793
[11/23 09:49:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.05	
[11/23 09:49:53 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/23 09:51:33 visual_prompt]: 	Training 100/553. train loss: 42.3787,	0.8400 s / batch. (data: 2.93e-04). ETA=10:41:10, max mem: 20.9 GB 
[11/23 09:53:11 visual_prompt]: 	Training 200/553. train loss: 41.7151,	0.8360 s / batch. (data: 2.93e-04). ETA=10:36:44, max mem: 20.9 GB 
[11/23 09:54:47 visual_prompt]: 	Training 300/553. train loss: 76.5457,	0.8157 s / batch. (data: 3.09e-04). ETA=10:19:55, max mem: 20.9 GB 
[11/23 09:56:22 visual_prompt]: 	Training 400/553. train loss: 110.9185,	0.8106 s / batch. (data: 2.90e-04). ETA=10:14:40, max mem: 20.9 GB 
[11/23 09:57:57 visual_prompt]: 	Training 500/553. train loss: 114.4731,	0.8108 s / batch. (data: 2.75e-04). ETA=10:13:29, max mem: 20.9 GB 
[11/23 09:58:46 visual_prompt]: Epoch 18 / 100: avg data time: 1.37e-01, avg batch time: 0.9647, average train loss: 154.6931
[11/23 09:59:41 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3091, average loss: 218.2068
[11/23 09:59:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.01	
[11/23 09:59:41 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/23 10:01:22 visual_prompt]: 	Training 100/553. train loss: 100.4358,	0.8178 s / batch. (data: 3.82e-04). ETA=10:16:42, max mem: 20.9 GB 
[11/23 10:02:59 visual_prompt]: 	Training 200/553. train loss: 326.8077,	0.8345 s / batch. (data: 2.97e-04). ETA=10:27:54, max mem: 20.9 GB 
[11/23 10:04:37 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8289 s / batch. (data: 5.08e-04). ETA=10:22:18, max mem: 20.9 GB 
[11/23 10:06:16 visual_prompt]: 	Training 400/553. train loss: 211.2019,	0.8161 s / batch. (data: 8.17e-04). ETA=10:11:22, max mem: 20.9 GB 
[11/23 10:07:48 visual_prompt]: 	Training 500/553. train loss: 568.7255,	0.8237 s / batch. (data: 3.23e-04). ETA=10:15:38, max mem: 20.9 GB 
[11/23 10:08:39 visual_prompt]: Epoch 19 / 100: avg data time: 1.46e-01, avg batch time: 0.9729, average train loss: 181.0746
[11/23 10:09:34 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3104, average loss: 410.1010
[11/23 10:09:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 36.76	
[11/23 10:09:34 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/23 10:11:12 visual_prompt]: 	Training 100/553. train loss: 10.1556,	0.8734 s / batch. (data: 2.14e-02). ETA=10:50:33, max mem: 20.9 GB 
[11/23 10:12:50 visual_prompt]: 	Training 200/553. train loss: 81.5022,	0.8815 s / batch. (data: 2.54e-02). ETA=10:55:06, max mem: 20.9 GB 
[11/23 10:14:26 visual_prompt]: 	Training 300/553. train loss: 175.6327,	0.8247 s / batch. (data: 5.40e-03). ETA=10:11:33, max mem: 20.9 GB 
[11/23 10:16:02 visual_prompt]: 	Training 400/553. train loss: 416.4282,	0.8118 s / batch. (data: 2.97e-04). ETA=10:00:39, max mem: 20.9 GB 
[11/23 10:17:37 visual_prompt]: 	Training 500/553. train loss: 294.9010,	0.8480 s / batch. (data: 2.97e-04). ETA=10:25:59, max mem: 20.9 GB 
[11/23 10:18:29 visual_prompt]: Epoch 20 / 100: avg data time: 1.40e-01, avg batch time: 0.9663, average train loss: 159.3108
[11/23 10:19:24 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3124, average loss: 33.0233
[11/23 10:19:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.73	
[11/23 10:19:24 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/23 10:21:05 visual_prompt]: 	Training 100/553. train loss: 51.4833,	0.8240 s / batch. (data: 3.01e-04). ETA=10:06:11, max mem: 20.9 GB 
[11/23 10:22:41 visual_prompt]: 	Training 200/553. train loss: 244.3890,	0.8336 s / batch. (data: 5.39e-03). ETA=10:11:51, max mem: 20.9 GB 
[11/23 10:24:18 visual_prompt]: 	Training 300/553. train loss: 32.4769,	0.9379 s / batch. (data: 1.31e-01). ETA=11:26:51, max mem: 20.9 GB 
[11/23 10:25:54 visual_prompt]: 	Training 400/553. train loss: 14.3152,	0.8360 s / batch. (data: 3.15e-04). ETA=10:10:51, max mem: 20.9 GB 
[11/23 10:27:34 visual_prompt]: 	Training 500/553. train loss: 38.1298,	0.8505 s / batch. (data: 2.23e-02). ETA=10:20:00, max mem: 20.9 GB 
[11/23 10:28:24 visual_prompt]: Epoch 21 / 100: avg data time: 1.50e-01, avg batch time: 0.9763, average train loss: 169.9309
[11/23 10:29:19 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3104, average loss: 123.6120
[11/23 10:29:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.34	
[11/23 10:29:19 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/23 10:30:57 visual_prompt]: 	Training 100/553. train loss: 72.6496,	0.8360 s / batch. (data: 1.05e-02). ETA=10:07:17, max mem: 20.9 GB 
[11/23 10:32:34 visual_prompt]: 	Training 200/553. train loss: 85.5240,	0.8120 s / batch. (data: 2.92e-04). ETA=9:48:31, max mem: 20.9 GB 
[11/23 10:34:08 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8309 s / batch. (data: 3.07e-04). ETA=10:00:48, max mem: 20.9 GB 
[11/23 10:35:46 visual_prompt]: 	Training 400/553. train loss: 131.4046,	0.8580 s / batch. (data: 2.21e-02). ETA=10:18:58, max mem: 20.9 GB 
[11/23 10:37:22 visual_prompt]: 	Training 500/553. train loss: 29.0696,	0.8360 s / batch. (data: 3.18e-04). ETA=10:01:44, max mem: 20.9 GB 
[11/23 10:38:14 visual_prompt]: Epoch 22 / 100: avg data time: 1.41e-01, avg batch time: 0.9675, average train loss: 160.6525
[11/23 10:39:09 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3104, average loss: 23.9632
[11/23 10:39:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.20	
[11/23 10:39:09 visual_prompt]: Best epoch 22: best metric: -23.963
[11/23 10:39:09 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/23 10:40:51 visual_prompt]: 	Training 100/553. train loss: 80.6201,	0.8450 s / batch. (data: 8.98e-03). ETA=10:06:04, max mem: 20.9 GB 
[11/23 10:42:29 visual_prompt]: 	Training 200/553. train loss: 155.8318,	0.8209 s / batch. (data: 3.15e-04). ETA=9:47:23, max mem: 20.9 GB 
[11/23 10:44:07 visual_prompt]: 	Training 300/553. train loss: 79.0297,	0.8285 s / batch. (data: 1.05e-02). ETA=9:51:29, max mem: 20.9 GB 
[11/23 10:45:41 visual_prompt]: 	Training 400/553. train loss: 192.9761,	0.8480 s / batch. (data: 7.61e-04). ETA=10:03:57, max mem: 20.9 GB 
[11/23 10:47:15 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8155 s / batch. (data: 3.26e-04). ETA=9:39:29, max mem: 20.9 GB 
[11/23 10:48:06 visual_prompt]: Epoch 23 / 100: avg data time: 1.43e-01, avg batch time: 0.9701, average train loss: 154.6760
[11/23 10:49:01 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3099, average loss: 340.3469
[11/23 10:49:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.29	
[11/23 10:49:01 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/23 10:50:39 visual_prompt]: 	Training 100/553. train loss: 2.7256,	0.8897 s / batch. (data: 6.53e-02). ETA=10:29:55, max mem: 20.9 GB 
[11/23 10:52:14 visual_prompt]: 	Training 200/553. train loss: 166.2729,	0.8520 s / batch. (data: 1.19e-02). ETA=10:01:46, max mem: 20.9 GB 
[11/23 10:53:50 visual_prompt]: 	Training 300/553. train loss: 232.3090,	0.8653 s / batch. (data: 4.81e-02). ETA=10:09:46, max mem: 20.9 GB 
[11/23 10:55:28 visual_prompt]: 	Training 400/553. train loss: 35.5179,	0.8438 s / batch. (data: 4.43e-04). ETA=9:53:13, max mem: 20.9 GB 
[11/23 10:57:06 visual_prompt]: 	Training 500/553. train loss: 125.8313,	0.8100 s / batch. (data: 3.50e-04). ETA=9:28:06, max mem: 20.9 GB 
[11/23 10:57:57 visual_prompt]: Epoch 24 / 100: avg data time: 1.43e-01, avg batch time: 0.9697, average train loss: 161.9279
[11/23 10:58:52 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3096, average loss: 42.9661
[11/23 10:58:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.48	
[11/23 10:58:52 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/23 11:00:36 visual_prompt]: 	Training 100/553. train loss: 334.7375,	0.8480 s / batch. (data: 3.05e-04). ETA=9:52:35, max mem: 20.9 GB 
[11/23 11:02:10 visual_prompt]: 	Training 200/553. train loss: 7.2485,	0.8811 s / batch. (data: 3.32e-02). ETA=10:14:16, max mem: 20.9 GB 
[11/23 11:03:45 visual_prompt]: 	Training 300/553. train loss: 403.8047,	0.9168 s / batch. (data: 8.33e-02). ETA=10:37:37, max mem: 20.9 GB 
[11/23 11:05:22 visual_prompt]: 	Training 400/553. train loss: 52.0730,	1.1196 s / batch. (data: 3.10e-01). ETA=12:56:46, max mem: 20.9 GB 
[11/23 11:07:00 visual_prompt]: 	Training 500/553. train loss: 298.8242,	1.3680 s / batch. (data: 5.29e-01). ETA=15:46:49, max mem: 20.9 GB 
[11/23 11:07:50 visual_prompt]: Epoch 25 / 100: avg data time: 1.45e-01, avg batch time: 0.9722, average train loss: 149.2566
[11/23 11:08:45 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3089, average loss: 127.0611
[11/23 11:08:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.03	
[11/23 11:08:45 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/23 11:10:26 visual_prompt]: 	Training 100/553. train loss: 256.6980,	0.8640 s / batch. (data: 7.97e-03). ETA=9:55:47, max mem: 20.9 GB 
[11/23 11:12:04 visual_prompt]: 	Training 200/553. train loss: 16.0175,	1.7610 s / batch. (data: 9.50e-01). ETA=20:11:27, max mem: 20.9 GB 
[11/23 11:13:42 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8026 s / batch. (data: 5.95e-04). ETA=9:10:48, max mem: 20.9 GB 
[11/23 11:15:18 visual_prompt]: 	Training 400/553. train loss: 180.0629,	0.8143 s / batch. (data: 3.00e-04). ETA=9:17:27, max mem: 20.9 GB 
[11/23 11:16:53 visual_prompt]: 	Training 500/553. train loss: 32.3860,	0.8161 s / batch. (data: 8.47e-03). ETA=9:17:18, max mem: 20.9 GB 
[11/23 11:17:42 visual_prompt]: Epoch 26 / 100: avg data time: 1.44e-01, avg batch time: 0.9704, average train loss: 159.3604
[11/23 11:18:37 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3099, average loss: 10.0895
[11/23 11:18:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 55.72	
[11/23 11:18:37 visual_prompt]: Best epoch 26: best metric: -10.089
[11/23 11:18:37 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/23 11:20:18 visual_prompt]: 	Training 100/553. train loss: 340.0575,	0.8476 s / batch. (data: 5.41e-03). ETA=9:36:41, max mem: 20.9 GB 
[11/23 11:21:54 visual_prompt]: 	Training 200/553. train loss: 360.8433,	1.0433 s / batch. (data: 2.09e-01). ETA=11:48:03, max mem: 20.9 GB 
[11/23 11:23:31 visual_prompt]: 	Training 300/553. train loss: 7.3428,	0.8450 s / batch. (data: 9.02e-03). ETA=9:32:06, max mem: 20.9 GB 
[11/23 11:25:08 visual_prompt]: 	Training 400/553. train loss: 24.9038,	0.8400 s / batch. (data: 7.50e-04). ETA=9:27:18, max mem: 20.9 GB 
[11/23 11:26:45 visual_prompt]: 	Training 500/553. train loss: 30.3918,	0.8326 s / batch. (data: 8.05e-04). ETA=9:20:55, max mem: 20.9 GB 
[11/23 11:27:34 visual_prompt]: Epoch 27 / 100: avg data time: 1.43e-01, avg batch time: 0.9706, average train loss: 136.9341
[11/23 11:28:29 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3112, average loss: 98.2299
[11/23 11:28:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.05	
[11/23 11:28:29 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/23 11:30:08 visual_prompt]: 	Training 100/553. train loss: 128.2742,	0.8336 s / batch. (data: 2.82e-04). ETA=9:19:28, max mem: 20.9 GB 
[11/23 11:31:44 visual_prompt]: 	Training 200/553. train loss: 186.6500,	0.8425 s / batch. (data: 3.09e-04). ETA=9:24:02, max mem: 20.9 GB 
[11/23 11:33:21 visual_prompt]: 	Training 300/553. train loss: 20.4521,	1.3012 s / batch. (data: 4.52e-01). ETA=14:28:58, max mem: 20.9 GB 
[11/23 11:34:56 visual_prompt]: 	Training 400/553. train loss: 116.6639,	0.8550 s / batch. (data: 5.89e-03). ETA=9:29:32, max mem: 20.9 GB 
[11/23 11:36:32 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8280 s / batch. (data: 2.84e-04). ETA=9:10:11, max mem: 20.9 GB 
[11/23 11:37:23 visual_prompt]: Epoch 28 / 100: avg data time: 1.39e-01, avg batch time: 0.9656, average train loss: 154.5328
[11/23 11:38:17 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3098, average loss: 47.1506
[11/23 11:38:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.10	
[11/23 11:38:17 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/23 11:40:03 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8361 s / batch. (data: 9.00e-03). ETA=9:13:25, max mem: 20.9 GB 
[11/23 11:41:39 visual_prompt]: 	Training 200/553. train loss: 33.9879,	1.5241 s / batch. (data: 6.96e-01). ETA=16:46:19, max mem: 20.9 GB 
[11/23 11:43:14 visual_prompt]: 	Training 300/553. train loss: 181.5701,	0.8360 s / batch. (data: 7.65e-04). ETA=9:10:35, max mem: 20.9 GB 
[11/23 11:44:48 visual_prompt]: 	Training 400/553. train loss: 188.7184,	1.2192 s / batch. (data: 4.02e-01). ETA=13:20:56, max mem: 20.9 GB 
[11/23 11:46:25 visual_prompt]: 	Training 500/553. train loss: 131.5021,	0.8440 s / batch. (data: 1.05e-02). ETA=9:13:01, max mem: 20.9 GB 
[11/23 11:47:15 visual_prompt]: Epoch 29 / 100: avg data time: 1.44e-01, avg batch time: 0.9717, average train loss: 165.1326
[11/23 11:48:09 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3103, average loss: 137.0830
[11/23 11:48:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.47	
[11/23 11:48:09 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/23 11:49:48 visual_prompt]: 	Training 100/553. train loss: 137.3621,	0.8389 s / batch. (data: 5.41e-03). ETA=9:07:34, max mem: 20.9 GB 
[11/23 11:51:26 visual_prompt]: 	Training 200/553. train loss: 293.8915,	0.8185 s / batch. (data: 3.05e-04). ETA=8:52:53, max mem: 20.9 GB 
[11/23 11:53:01 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.2413 s / batch. (data: 3.92e-01). ETA=13:26:05, max mem: 20.9 GB 
[11/23 11:54:39 visual_prompt]: 	Training 400/553. train loss: 48.6005,	1.0164 s / batch. (data: 2.00e-01). ETA=10:58:18, max mem: 20.9 GB 
[11/23 11:56:13 visual_prompt]: 	Training 500/553. train loss: 15.0019,	1.2680 s / batch. (data: 4.36e-01). ETA=13:39:11, max mem: 20.9 GB 
[11/23 11:57:06 visual_prompt]: Epoch 30 / 100: avg data time: 1.43e-01, avg batch time: 0.9695, average train loss: 152.4408
[11/23 11:58:01 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3100, average loss: 126.2633
[11/23 11:58:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.01	
[11/23 11:58:01 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[11/23 11:59:43 visual_prompt]: 	Training 100/553. train loss: 115.0993,	0.8401 s / batch. (data: 3.34e-03). ETA=9:00:34, max mem: 20.9 GB 
[11/23 12:01:22 visual_prompt]: 	Training 200/553. train loss: 250.0156,	0.8646 s / batch. (data: 5.41e-03). ETA=9:14:54, max mem: 20.9 GB 
[11/23 12:02:56 visual_prompt]: 	Training 300/553. train loss: 112.1726,	0.8251 s / batch. (data: 3.19e-04). ETA=8:48:13, max mem: 20.9 GB 
[11/23 12:04:32 visual_prompt]: 	Training 400/553. train loss: 204.9033,	1.1838 s / batch. (data: 3.56e-01). ETA=12:35:51, max mem: 20.9 GB 
[11/23 12:06:10 visual_prompt]: 	Training 500/553. train loss: 57.7240,	0.8295 s / batch. (data: 5.41e-03). ETA=8:48:14, max mem: 20.9 GB 
[11/23 12:07:00 visual_prompt]: Epoch 31 / 100: avg data time: 1.49e-01, avg batch time: 0.9751, average train loss: 147.3782
[11/23 12:07:56 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3076, average loss: 66.5390
[11/23 12:07:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.88	
[11/23 12:07:56 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[11/23 12:09:38 visual_prompt]: 	Training 100/553. train loss: 63.9537,	0.8545 s / batch. (data: 1.10e-02). ETA=9:02:00, max mem: 20.9 GB 
[11/23 12:11:14 visual_prompt]: 	Training 200/553. train loss: 80.4546,	0.8188 s / batch. (data: 7.71e-04). ETA=8:37:57, max mem: 20.9 GB 
[11/23 12:12:53 visual_prompt]: 	Training 300/553. train loss: 192.4452,	0.8304 s / batch. (data: 2.06e-02). ETA=8:43:54, max mem: 20.9 GB 
[11/23 12:14:30 visual_prompt]: 	Training 400/553. train loss: 4.1298,	0.8419 s / batch. (data: 3.16e-04). ETA=8:49:47, max mem: 20.9 GB 
[11/23 12:16:04 visual_prompt]: 	Training 500/553. train loss: 370.3149,	0.8362 s / batch. (data: 7.30e-03). ETA=8:44:49, max mem: 20.9 GB 
[11/23 12:16:52 visual_prompt]: Epoch 32 / 100: avg data time: 1.43e-01, avg batch time: 0.9703, average train loss: 144.6342
[11/23 12:17:47 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3096, average loss: 325.2564
[11/23 12:17:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.09	
[11/23 12:17:47 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[11/23 12:19:27 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8200 s / batch. (data: 2.96e-04). ETA=8:32:32, max mem: 20.9 GB 
[11/23 12:21:06 visual_prompt]: 	Training 200/553. train loss: 107.0186,	0.8549 s / batch. (data: 2.17e-02). ETA=8:52:57, max mem: 20.9 GB 
[11/23 12:22:44 visual_prompt]: 	Training 300/553. train loss: 104.5602,	0.8407 s / batch. (data: 7.99e-03). ETA=8:42:40, max mem: 20.9 GB 
[11/23 12:24:22 visual_prompt]: 	Training 400/553. train loss: 18.8342,	0.8509 s / batch. (data: 2.06e-02). ETA=8:47:37, max mem: 20.9 GB 
[11/23 12:25:59 visual_prompt]: 	Training 500/553. train loss: 92.1204,	0.8280 s / batch. (data: 4.79e-04). ETA=8:32:00, max mem: 20.9 GB 
[11/23 12:26:49 visual_prompt]: Epoch 33 / 100: avg data time: 1.53e-01, avg batch time: 0.9797, average train loss: 163.7719
[11/23 12:27:45 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3101, average loss: 33.5294
[11/23 12:27:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.21	
[11/23 12:27:45 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[11/23 12:29:28 visual_prompt]: 	Training 100/553. train loss: 100.4352,	0.8200 s / batch. (data: 8.00e-03). ETA=8:24:58, max mem: 20.9 GB 
[11/23 12:31:03 visual_prompt]: 	Training 200/553. train loss: 168.7859,	0.8317 s / batch. (data: 3.23e-04). ETA=8:30:49, max mem: 20.9 GB 
[11/23 12:32:39 visual_prompt]: 	Training 300/553. train loss: 56.7570,	0.8316 s / batch. (data: 5.43e-03). ETA=8:29:23, max mem: 20.9 GB 
[11/23 12:34:16 visual_prompt]: 	Training 400/553. train loss: 172.6722,	0.8362 s / batch. (data: 3.00e-04). ETA=8:30:47, max mem: 20.9 GB 
[11/23 12:35:53 visual_prompt]: 	Training 500/553. train loss: 72.8756,	1.2713 s / batch. (data: 4.40e-01). ETA=12:54:26, max mem: 20.9 GB 
[11/23 12:36:44 visual_prompt]: Epoch 34 / 100: avg data time: 1.47e-01, avg batch time: 0.9744, average train loss: 139.0871
[11/23 12:37:39 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3123, average loss: 100.6480
[11/23 12:37:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.71	
[11/23 12:37:39 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[11/23 12:39:22 visual_prompt]: 	Training 100/553. train loss: 222.7979,	0.8320 s / batch. (data: 3.02e-04). ETA=8:24:43, max mem: 20.9 GB 
[11/23 12:41:01 visual_prompt]: 	Training 200/553. train loss: 64.9297,	0.8080 s / batch. (data: 4.02e-04). ETA=8:08:48, max mem: 20.9 GB 
[11/23 12:42:36 visual_prompt]: 	Training 300/553. train loss: 56.2827,	0.8520 s / batch. (data: 7.96e-03). ETA=8:34:01, max mem: 20.9 GB 
[11/23 12:44:12 visual_prompt]: 	Training 400/553. train loss: 203.8705,	0.8060 s / batch. (data: 3.65e-04). ETA=8:04:53, max mem: 20.9 GB 
[11/23 12:45:47 visual_prompt]: 	Training 500/553. train loss: 13.9455,	0.9844 s / batch. (data: 1.54e-01). ETA=9:50:37, max mem: 20.9 GB 
[11/23 12:46:39 visual_prompt]: Epoch 35 / 100: avg data time: 1.49e-01, avg batch time: 0.9762, average train loss: 121.9376
[11/23 12:47:35 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3105, average loss: 78.2520
[11/23 12:47:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.94	
[11/23 12:47:35 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[11/23 12:49:15 visual_prompt]: 	Training 100/553. train loss: 188.7446,	0.8159 s / batch. (data: 3.16e-04). ETA=8:07:24, max mem: 20.9 GB 
[11/23 12:50:53 visual_prompt]: 	Training 200/553. train loss: 150.9630,	0.8244 s / batch. (data: 5.41e-03). ETA=8:11:07, max mem: 20.9 GB 
[11/23 12:52:31 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8245 s / batch. (data: 2.85e-04). ETA=8:09:49, max mem: 20.9 GB 
[11/23 12:54:07 visual_prompt]: 	Training 400/553. train loss: 8.6037,	0.8156 s / batch. (data: 3.09e-04). ETA=8:03:09, max mem: 20.9 GB 
[11/23 12:55:45 visual_prompt]: 	Training 500/553. train loss: 187.1887,	0.9689 s / batch. (data: 1.53e-01). ETA=9:32:23, max mem: 20.9 GB 
[11/23 12:56:33 visual_prompt]: Epoch 36 / 100: avg data time: 1.46e-01, avg batch time: 0.9728, average train loss: 147.0550
[11/23 12:57:28 visual_prompt]: Inference (val):avg data time: 2.77e-04, avg batch time: 0.3132, average loss: 404.8266
[11/23 12:57:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.33	
[11/23 12:57:28 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[11/23 12:59:09 visual_prompt]: 	Training 100/553. train loss: 3.6021,	0.8320 s / batch. (data: 3.07e-04). ETA=8:09:22, max mem: 20.9 GB 
[11/23 13:00:46 visual_prompt]: 	Training 200/553. train loss: 141.7495,	0.8350 s / batch. (data: 5.49e-03). ETA=8:09:45, max mem: 20.9 GB 
[11/23 13:02:24 visual_prompt]: 	Training 300/553. train loss: 111.5356,	1.4640 s / batch. (data: 6.42e-01). ETA=14:16:15, max mem: 20.9 GB 
[11/23 13:04:02 visual_prompt]: 	Training 400/553. train loss: 268.0103,	1.7827 s / batch. (data: 9.39e-01). ETA=17:19:41, max mem: 20.9 GB 
[11/23 13:05:35 visual_prompt]: 	Training 500/553. train loss: 109.1986,	0.8428 s / batch. (data: 1.05e-02). ETA=8:10:06, max mem: 20.9 GB 
[11/23 13:06:27 visual_prompt]: Epoch 37 / 100: avg data time: 1.48e-01, avg batch time: 0.9746, average train loss: 144.4816
[11/23 13:07:23 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3099, average loss: 276.0451
[11/23 13:07:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.37	
[11/23 13:07:23 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[11/23 13:09:03 visual_prompt]: 	Training 100/553. train loss: 46.1960,	0.8078 s / batch. (data: 3.13e-04). ETA=7:47:42, max mem: 20.9 GB 
[11/23 13:10:40 visual_prompt]: 	Training 200/553. train loss: 26.8297,	0.8333 s / batch. (data: 5.45e-03). ETA=8:01:03, max mem: 20.9 GB 
[11/23 13:12:18 visual_prompt]: 	Training 300/553. train loss: 63.9591,	0.8057 s / batch. (data: 2.83e-04). ETA=7:43:49, max mem: 20.9 GB 
[11/23 13:13:54 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8280 s / batch. (data: 7.98e-03). ETA=7:55:15, max mem: 20.9 GB 
[11/23 13:15:34 visual_prompt]: 	Training 500/553. train loss: 132.2172,	0.8267 s / batch. (data: 3.31e-04). ETA=7:53:06, max mem: 20.9 GB 
[11/23 13:16:24 visual_prompt]: Epoch 38 / 100: avg data time: 1.50e-01, avg batch time: 0.9774, average train loss: 141.5284
[11/23 13:17:19 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3088, average loss: 468.5666
[11/23 13:17:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.83	
[11/23 13:17:19 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[11/23 13:18:58 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8083 s / batch. (data: 5.47e-03). ETA=7:40:31, max mem: 20.9 GB 
[11/23 13:20:38 visual_prompt]: 	Training 200/553. train loss: 318.9404,	0.8449 s / batch. (data: 2.53e-02). ETA=8:00:00, max mem: 20.9 GB 
[11/23 13:22:18 visual_prompt]: 	Training 300/553. train loss: 309.1207,	0.8453 s / batch. (data: 5.47e-03). ETA=7:58:47, max mem: 20.9 GB 
[11/23 13:23:52 visual_prompt]: 	Training 400/553. train loss: 135.6061,	0.8562 s / batch. (data: 3.52e-04). ETA=8:03:33, max mem: 20.9 GB 
[11/23 13:25:28 visual_prompt]: 	Training 500/553. train loss: 73.8458,	1.4671 s / batch. (data: 6.61e-01). ETA=13:46:08, max mem: 20.9 GB 
[11/23 13:26:17 visual_prompt]: Epoch 39 / 100: avg data time: 1.44e-01, avg batch time: 0.9717, average train loss: 140.0020
[11/23 13:27:12 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3111, average loss: 137.8621
[11/23 13:27:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.12	
[11/23 13:27:12 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[11/23 13:28:53 visual_prompt]: 	Training 100/553. train loss: 257.4938,	0.8280 s / batch. (data: 3.13e-04). ETA=7:44:08, max mem: 20.9 GB 
[11/23 13:30:29 visual_prompt]: 	Training 200/553. train loss: 12.0638,	0.8280 s / batch. (data: 3.04e-04). ETA=7:42:45, max mem: 20.9 GB 
[11/23 13:32:07 visual_prompt]: 	Training 300/553. train loss: 318.8055,	0.8520 s / batch. (data: 2.95e-04). ETA=7:54:45, max mem: 20.9 GB 
[11/23 13:33:44 visual_prompt]: 	Training 400/553. train loss: 26.2209,	0.8455 s / batch. (data: 1.10e-02). ETA=7:49:42, max mem: 20.9 GB 
[11/23 13:35:20 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8370 s / batch. (data: 2.50e-04). ETA=7:43:37, max mem: 20.9 GB 
[11/23 13:36:12 visual_prompt]: Epoch 40 / 100: avg data time: 1.50e-01, avg batch time: 0.9769, average train loss: 100.7834
[11/23 13:37:07 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3101, average loss: 243.3437
[11/23 13:37:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.03	
[11/23 13:37:07 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[11/23 13:38:51 visual_prompt]: 	Training 100/553. train loss: 50.4379,	0.8120 s / batch. (data: 2.89e-04). ETA=7:27:41, max mem: 20.9 GB 
[11/23 13:40:29 visual_prompt]: 	Training 200/553. train loss: 207.1020,	0.8214 s / batch. (data: 3.70e-04). ETA=7:31:28, max mem: 20.9 GB 
[11/23 13:42:05 visual_prompt]: 	Training 300/553. train loss: 78.5583,	0.8427 s / batch. (data: 5.42e-03). ETA=7:41:48, max mem: 20.9 GB 
[11/23 13:43:39 visual_prompt]: 	Training 400/553. train loss: 188.9894,	0.8272 s / batch. (data: 6.31e-04). ETA=7:31:54, max mem: 20.9 GB 
[11/23 13:45:14 visual_prompt]: 	Training 500/553. train loss: 154.2863,	0.8378 s / batch. (data: 2.94e-04). ETA=7:36:17, max mem: 20.9 GB 
[11/23 13:46:02 visual_prompt]: Epoch 41 / 100: avg data time: 1.41e-01, avg batch time: 0.9673, average train loss: 131.5228
[11/23 13:46:56 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3100, average loss: 192.0727
[11/23 13:46:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.73	
[11/23 13:46:56 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[11/23 13:48:35 visual_prompt]: 	Training 100/553. train loss: 341.8537,	0.8215 s / batch. (data: 3.03e-04). ETA=7:25:20, max mem: 20.9 GB 
[11/23 13:50:11 visual_prompt]: 	Training 200/553. train loss: 142.7461,	0.8282 s / batch. (data: 1.64e-02). ETA=7:27:36, max mem: 20.9 GB 
[11/23 13:51:48 visual_prompt]: 	Training 300/553. train loss: 368.0974,	0.8498 s / batch. (data: 5.42e-03). ETA=7:37:51, max mem: 20.9 GB 
[11/23 13:53:25 visual_prompt]: 	Training 400/553. train loss: 129.9272,	0.8206 s / batch. (data: 3.28e-04). ETA=7:20:46, max mem: 20.9 GB 
[11/23 13:55:00 visual_prompt]: 	Training 500/553. train loss: 389.5491,	0.8440 s / batch. (data: 2.98e-04). ETA=7:31:54, max mem: 20.9 GB 
[11/23 13:55:52 visual_prompt]: Epoch 42 / 100: avg data time: 1.40e-01, avg batch time: 0.9677, average train loss: 140.2088
[11/23 13:56:46 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3107, average loss: 221.4530
[11/23 13:56:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.79	
[11/23 13:56:46 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[11/23 13:58:28 visual_prompt]: 	Training 100/553. train loss: 251.0161,	0.8142 s / batch. (data: 2.97e-04). ETA=7:13:51, max mem: 20.9 GB 
[11/23 14:00:03 visual_prompt]: 	Training 200/553. train loss: 186.0584,	0.8188 s / batch. (data: 3.09e-04). ETA=7:14:59, max mem: 20.9 GB 
[11/23 14:01:38 visual_prompt]: 	Training 300/553. train loss: 259.0633,	0.8360 s / batch. (data: 2.97e-04). ETA=7:22:43, max mem: 20.9 GB 
[11/23 14:03:13 visual_prompt]: 	Training 400/553. train loss: 175.8335,	0.8240 s / batch. (data: 7.95e-03). ETA=7:14:58, max mem: 20.9 GB 
[11/23 14:04:50 visual_prompt]: 	Training 500/553. train loss: 142.0669,	0.8320 s / batch. (data: 3.19e-04). ETA=7:17:50, max mem: 20.9 GB 
[11/23 14:05:43 visual_prompt]: Epoch 43 / 100: avg data time: 1.42e-01, avg batch time: 0.9694, average train loss: 131.8388
[11/23 14:06:37 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3106, average loss: 335.8352
[11/23 14:06:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.73	
[11/23 14:06:37 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[11/23 14:08:18 visual_prompt]: 	Training 100/553. train loss: 98.0789,	1.0587 s / batch. (data: 2.54e-01). ETA=9:14:24, max mem: 20.9 GB 
[11/23 14:09:56 visual_prompt]: 	Training 200/553. train loss: 27.2970,	0.8232 s / batch. (data: 3.06e-04). ETA=7:09:42, max mem: 20.9 GB 
[11/23 14:11:31 visual_prompt]: 	Training 300/553. train loss: 110.8865,	0.8089 s / batch. (data: 2.99e-04). ETA=7:00:53, max mem: 20.9 GB 
[11/23 14:13:06 visual_prompt]: 	Training 400/553. train loss: 64.9772,	0.8288 s / batch. (data: 3.10e-04). ETA=7:09:53, max mem: 20.9 GB 
[11/23 14:14:43 visual_prompt]: 	Training 500/553. train loss: 48.6969,	0.8209 s / batch. (data: 3.21e-04). ETA=7:04:26, max mem: 20.9 GB 
[11/23 14:15:34 visual_prompt]: Epoch 44 / 100: avg data time: 1.44e-01, avg batch time: 0.9710, average train loss: 119.7253
[11/23 14:16:29 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3111, average loss: 10.1329
[11/23 14:16:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.18	
[11/23 14:16:29 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[11/23 14:18:11 visual_prompt]: 	Training 100/553. train loss: 13.1764,	0.8440 s / batch. (data: 3.06e-04). ETA=7:14:11, max mem: 20.9 GB 
[11/23 14:19:44 visual_prompt]: 	Training 200/553. train loss: 51.7602,	0.8280 s / batch. (data: 3.27e-04). ETA=7:04:35, max mem: 20.9 GB 
[11/23 14:21:22 visual_prompt]: 	Training 300/553. train loss: 23.4284,	0.8360 s / batch. (data: 3.02e-04). ETA=7:07:18, max mem: 20.9 GB 
[11/23 14:22:57 visual_prompt]: 	Training 400/553. train loss: 156.7916,	0.8165 s / batch. (data: 5.45e-03). ETA=6:55:58, max mem: 20.9 GB 
[11/23 14:24:36 visual_prompt]: 	Training 500/553. train loss: 70.1288,	0.8199 s / batch. (data: 3.80e-03). ETA=6:56:21, max mem: 20.9 GB 
[11/23 14:25:26 visual_prompt]: Epoch 45 / 100: avg data time: 1.44e-01, avg batch time: 0.9703, average train loss: 123.9202
[11/23 14:26:21 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3115, average loss: 114.6815
[11/23 14:26:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.27	
[11/23 14:26:21 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[11/23 14:28:02 visual_prompt]: 	Training 100/553. train loss: 184.4694,	1.0880 s / batch. (data: 2.69e-01). ETA=9:09:42, max mem: 20.9 GB 
[11/23 14:29:40 visual_prompt]: 	Training 200/553. train loss: 13.2624,	0.8560 s / batch. (data: 3.15e-04). ETA=7:11:03, max mem: 20.9 GB 
[11/23 14:31:14 visual_prompt]: 	Training 300/553. train loss: 361.9542,	0.8398 s / batch. (data: 3.27e-04). ETA=7:01:32, max mem: 20.9 GB 
[11/23 14:32:51 visual_prompt]: 	Training 400/553. train loss: 314.4479,	0.8103 s / batch. (data: 2.92e-04). ETA=6:45:20, max mem: 20.9 GB 
[11/23 14:34:26 visual_prompt]: 	Training 500/553. train loss: 120.6618,	0.8245 s / batch. (data: 2.74e-04). ETA=6:51:04, max mem: 20.9 GB 
[11/23 14:35:18 visual_prompt]: Epoch 46 / 100: avg data time: 1.42e-01, avg batch time: 0.9702, average train loss: 123.0109
[11/23 14:36:12 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3108, average loss: 189.0893
[11/23 14:36:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.63	
[11/23 14:36:12 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[11/23 14:37:54 visual_prompt]: 	Training 100/553. train loss: 34.3043,	0.8358 s / batch. (data: 2.80e-04). ETA=6:54:36, max mem: 20.9 GB 
[11/23 14:39:27 visual_prompt]: 	Training 200/553. train loss: 115.0741,	0.9440 s / batch. (data: 1.03e-01). ETA=7:46:41, max mem: 20.9 GB 
[11/23 14:41:05 visual_prompt]: 	Training 300/553. train loss: 101.0320,	0.8230 s / batch. (data: 2.84e-04). ETA=6:45:30, max mem: 20.9 GB 
[11/23 14:42:41 visual_prompt]: 	Training 400/553. train loss: 10.7620,	0.8240 s / batch. (data: 2.97e-04). ETA=6:44:36, max mem: 20.9 GB 
[11/23 14:44:17 visual_prompt]: 	Training 500/553. train loss: 19.4447,	0.8718 s / batch. (data: 3.05e-04). ETA=7:06:38, max mem: 20.9 GB 
[11/23 14:45:09 visual_prompt]: Epoch 47 / 100: avg data time: 1.43e-01, avg batch time: 0.9695, average train loss: 123.5316
[11/23 14:46:04 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3091, average loss: 139.5126
[11/23 14:46:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.60	
[11/23 14:46:04 visual_prompt]: Stopping early.
[11/23 14:46:06 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 14:46:06 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 14:46:06 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/23 14:46:06 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 14:46:06 visual_prompt]: Training with config:
[11/23 14:46:06 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/23 14:46:06 visual_prompt]: Loading training data...
[11/23 14:46:06 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 14:46:06 visual_prompt]: Loading validation data...
[11/23 14:46:06 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 14:46:07 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 14:46:16 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 14:46:16 visual_prompt]: tuned percent:0.525
[11/23 14:46:16 visual_prompt]: Device used for model: 0
[11/23 14:46:16 visual_prompt]: Setting up Evaluator...
[11/23 14:46:16 visual_prompt]: Setting up Trainer...
[11/23 14:46:16 visual_prompt]: 	Setting up the optimizer...
[11/23 14:46:16 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 14:47:59 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8197 s / batch. (data: 3.05e-04). ETA=12:34:05, max mem: 20.9 GB 
[11/23 14:49:34 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8320 s / batch. (data: 3.09e-04). ETA=12:44:03, max mem: 20.9 GB 
[11/23 14:51:13 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8600 s / batch. (data: 1.60e-02). ETA=13:08:19, max mem: 20.9 GB 
[11/23 14:52:49 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8311 s / batch. (data: 3.32e-04). ETA=12:40:29, max mem: 20.9 GB 
[11/23 14:54:28 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8480 s / batch. (data: 8.02e-03). ETA=12:54:31, max mem: 20.9 GB 
[11/23 14:55:19 visual_prompt]: Epoch 1 / 100: avg data time: 1.49e-01, avg batch time: 0.9820, average train loss: 1.5403
[11/23 14:56:15 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3105, average loss: 1.5201
[11/23 14:56:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 14:56:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/23 14:57:55 visual_prompt]: 	Training 100/553. train loss: 25.0325,	0.8695 s / batch. (data: 5.46e-03). ETA=13:11:57, max mem: 20.9 GB 
[11/23 14:59:31 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8240 s / batch. (data: 3.09e-04). ETA=12:29:07, max mem: 20.9 GB 
[11/23 15:01:10 visual_prompt]: 	Training 300/553. train loss: 7.4490,	1.0075 s / batch. (data: 1.76e-01). ETA=15:14:17, max mem: 20.9 GB 
[11/23 15:02:45 visual_prompt]: 	Training 400/553. train loss: 29.1996,	0.8729 s / batch. (data: 9.92e-03). ETA=13:10:38, max mem: 20.9 GB 
[11/23 15:04:23 visual_prompt]: 	Training 500/553. train loss: 80.6536,	0.8195 s / batch. (data: 3.38e-04). ETA=12:20:55, max mem: 20.9 GB 
[11/23 15:05:13 visual_prompt]: Epoch 2 / 100: avg data time: 1.42e-01, avg batch time: 0.9730, average train loss: 26.5332
[11/23 15:06:08 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3113, average loss: 67.9892
[11/23 15:06:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.61	
[11/23 15:06:08 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/23 15:07:47 visual_prompt]: 	Training 100/553. train loss: 60.1347,	0.8237 s / batch. (data: 3.08e-04). ETA=12:22:38, max mem: 20.9 GB 
[11/23 15:09:25 visual_prompt]: 	Training 200/553. train loss: 29.4592,	0.8372 s / batch. (data: 5.58e-03). ETA=12:33:22, max mem: 20.9 GB 
[11/23 15:11:01 visual_prompt]: 	Training 300/553. train loss: 40.9004,	0.8240 s / batch. (data: 3.09e-04). ETA=12:20:08, max mem: 20.9 GB 
[11/23 15:12:38 visual_prompt]: 	Training 400/553. train loss: 14.7090,	0.8214 s / batch. (data: 3.13e-04). ETA=12:16:27, max mem: 20.9 GB 
[11/23 15:14:17 visual_prompt]: 	Training 500/553. train loss: 23.5847,	1.1563 s / batch. (data: 3.25e-01). ETA=17:14:45, max mem: 20.9 GB 
[11/23 15:15:07 visual_prompt]: Epoch 3 / 100: avg data time: 1.44e-01, avg batch time: 0.9737, average train loss: 43.5661
[11/23 15:16:03 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3113, average loss: 40.6659
[11/23 15:16:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.00	
[11/23 15:16:03 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/23 15:17:45 visual_prompt]: 	Training 100/553. train loss: 39.0314,	0.8491 s / batch. (data: 1.05e-02). ETA=12:37:43, max mem: 20.9 GB 
[11/23 15:19:24 visual_prompt]: 	Training 200/553. train loss: 61.0410,	0.8610 s / batch. (data: 5.41e-03). ETA=12:46:53, max mem: 20.9 GB 
[11/23 15:21:01 visual_prompt]: 	Training 300/553. train loss: 14.5468,	1.2437 s / batch. (data: 4.18e-01). ETA=18:25:40, max mem: 20.9 GB 
[11/23 15:22:33 visual_prompt]: 	Training 400/553. train loss: 25.0609,	0.9840 s / batch. (data: 1.29e-01). ETA=14:33:07, max mem: 20.9 GB 
[11/23 15:24:12 visual_prompt]: 	Training 500/553. train loss: 145.8390,	3.1000 s / batch. (data: 2.27e+00). ETA=1 day, 21:45:35, max mem: 20.9 GB 
[11/23 15:25:04 visual_prompt]: Epoch 4 / 100: avg data time: 1.48e-01, avg batch time: 0.9782, average train loss: 41.9266
[11/23 15:25:59 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3087, average loss: 32.3341
[11/23 15:25:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.60	
[11/23 15:25:59 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/23 15:27:38 visual_prompt]: 	Training 100/553. train loss: 386.8663,	0.8320 s / batch. (data: 2.90e-04). ETA=12:14:47, max mem: 20.9 GB 
[11/23 15:29:14 visual_prompt]: 	Training 200/553. train loss: 53.7453,	0.9897 s / batch. (data: 1.80e-01). ETA=14:32:23, max mem: 20.9 GB 
[11/23 15:30:52 visual_prompt]: 	Training 300/553. train loss: 214.9673,	0.8513 s / batch. (data: 5.88e-03). ETA=12:28:59, max mem: 20.9 GB 
[11/23 15:32:27 visual_prompt]: 	Training 400/553. train loss: 27.8766,	0.8295 s / batch. (data: 2.89e-04). ETA=12:08:25, max mem: 20.9 GB 
[11/23 15:34:04 visual_prompt]: 	Training 500/553. train loss: 19.4011,	0.8560 s / batch. (data: 2.78e-04). ETA=12:30:15, max mem: 20.9 GB 
[11/23 15:34:55 visual_prompt]: Epoch 5 / 100: avg data time: 1.43e-01, avg batch time: 0.9703, average train loss: 85.3709
[11/23 15:35:50 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3086, average loss: 19.8969
[11/23 15:35:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.41	
[11/23 15:35:50 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/23 15:37:32 visual_prompt]: 	Training 100/553. train loss: 88.2537,	0.8520 s / batch. (data: 3.60e-02). ETA=12:24:33, max mem: 20.9 GB 
[11/23 15:39:08 visual_prompt]: 	Training 200/553. train loss: 21.5776,	0.8176 s / batch. (data: 3.02e-04). ETA=11:53:06, max mem: 20.9 GB 
[11/23 15:40:43 visual_prompt]: 	Training 300/553. train loss: 217.9171,	0.8328 s / batch. (data: 7.96e-03). ETA=12:05:00, max mem: 20.9 GB 
[11/23 15:42:24 visual_prompt]: 	Training 400/553. train loss: 6.9724,	0.8320 s / batch. (data: 3.17e-04). ETA=12:02:57, max mem: 20.9 GB 
[11/23 15:43:59 visual_prompt]: 	Training 500/553. train loss: 1.2620,	0.8200 s / batch. (data: 3.40e-04). ETA=11:51:07, max mem: 20.9 GB 
[11/23 15:44:49 visual_prompt]: Epoch 6 / 100: avg data time: 1.47e-01, avg batch time: 0.9738, average train loss: 87.7367
[11/23 15:45:44 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3097, average loss: 26.4101
[11/23 15:45:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.30	
[11/23 15:45:44 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/23 15:47:23 visual_prompt]: 	Training 100/553. train loss: 50.2073,	0.8355 s / batch. (data: 7.95e-03). ETA=12:02:25, max mem: 20.9 GB 
[11/23 15:49:00 visual_prompt]: 	Training 200/553. train loss: 18.5600,	0.8600 s / batch. (data: 5.43e-03). ETA=12:22:11, max mem: 20.9 GB 
[11/23 15:50:40 visual_prompt]: 	Training 300/553. train loss: 4.7670,	1.5351 s / batch. (data: 7.14e-01). ETA=22:02:17, max mem: 20.9 GB 
[11/23 15:52:15 visual_prompt]: 	Training 400/553. train loss: 29.0680,	0.8280 s / batch. (data: 3.92e-04). ETA=11:51:50, max mem: 20.9 GB 
[11/23 15:53:52 visual_prompt]: 	Training 500/553. train loss: 176.5945,	0.8221 s / batch. (data: 4.30e-04). ETA=11:45:21, max mem: 20.9 GB 
[11/23 15:54:41 visual_prompt]: Epoch 7 / 100: avg data time: 1.42e-01, avg batch time: 0.9697, average train loss: 99.9768
[11/23 15:55:36 visual_prompt]: Inference (val):avg data time: 8.39e-05, avg batch time: 0.3093, average loss: 111.2492
[11/23 15:55:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.62	
[11/23 15:55:36 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/23 15:57:15 visual_prompt]: 	Training 100/553. train loss: 282.0644,	0.8465 s / batch. (data: 2.27e-02). ETA=12:04:09, max mem: 20.9 GB 
[11/23 15:58:54 visual_prompt]: 	Training 200/553. train loss: 32.4692,	0.8439 s / batch. (data: 7.50e-03). ETA=12:00:29, max mem: 20.9 GB 
[11/23 16:00:32 visual_prompt]: 	Training 300/553. train loss: 50.4068,	0.8396 s / batch. (data: 9.61e-03). ETA=11:55:27, max mem: 20.9 GB 
[11/23 16:02:09 visual_prompt]: 	Training 400/553. train loss: 234.5956,	0.8202 s / batch. (data: 3.29e-04). ETA=11:37:36, max mem: 20.9 GB 
[11/23 16:03:46 visual_prompt]: 	Training 500/553. train loss: 329.2100,	1.4310 s / batch. (data: 6.04e-01). ETA=20:14:38, max mem: 20.9 GB 
[11/23 16:04:37 visual_prompt]: Epoch 8 / 100: avg data time: 1.50e-01, avg batch time: 0.9770, average train loss: 113.6568
[11/23 16:05:32 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3104, average loss: 4.6736
[11/23 16:05:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.02	
[11/23 16:05:32 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/23 16:07:12 visual_prompt]: 	Training 100/553. train loss: 38.1540,	0.8520 s / batch. (data: 1.59e-02). ETA=12:00:59, max mem: 20.9 GB 
[11/23 16:08:46 visual_prompt]: 	Training 200/553. train loss: 2.7190,	0.8360 s / batch. (data: 2.98e-04). ETA=11:46:04, max mem: 20.9 GB 
[11/23 16:10:23 visual_prompt]: 	Training 300/553. train loss: 53.2733,	1.4560 s / batch. (data: 6.37e-01). ETA=20:27:18, max mem: 20.9 GB 
[11/23 16:12:00 visual_prompt]: 	Training 400/553. train loss: 240.5974,	0.8398 s / batch. (data: 7.83e-04). ETA=11:46:30, max mem: 20.9 GB 
[11/23 16:13:36 visual_prompt]: 	Training 500/553. train loss: 59.7817,	0.8486 s / batch. (data: 3.48e-02). ETA=11:52:29, max mem: 20.9 GB 
[11/23 16:14:25 visual_prompt]: Epoch 9 / 100: avg data time: 1.37e-01, avg batch time: 0.9642, average train loss: 133.1418
[11/23 16:15:20 visual_prompt]: Inference (val):avg data time: 1.70e-04, avg batch time: 0.3091, average loss: 184.4700
[11/23 16:15:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.80	
[11/23 16:15:20 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/23 16:17:02 visual_prompt]: 	Training 100/553. train loss: 277.8878,	0.8240 s / batch. (data: 2.87e-04). ETA=11:29:44, max mem: 20.9 GB 
[11/23 16:18:36 visual_prompt]: 	Training 200/553. train loss: 8.4880,	0.8451 s / batch. (data: 5.43e-03). ETA=11:45:58, max mem: 20.9 GB 
[11/23 16:20:11 visual_prompt]: 	Training 300/553. train loss: 49.5425,	1.2960 s / batch. (data: 4.56e-01). ETA=18:00:29, max mem: 20.9 GB 
[11/23 16:21:45 visual_prompt]: 	Training 400/553. train loss: 7.6892,	0.8477 s / batch. (data: 3.14e-04). ETA=11:45:19, max mem: 20.9 GB 
[11/23 16:23:22 visual_prompt]: 	Training 500/553. train loss: 26.2307,	0.8073 s / batch. (data: 3.48e-04). ETA=11:10:20, max mem: 20.9 GB 
[11/23 16:24:12 visual_prompt]: Epoch 10 / 100: avg data time: 1.35e-01, avg batch time: 0.9619, average train loss: 164.3878
[11/23 16:25:07 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3103, average loss: 31.7203
[11/23 16:25:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.98	
[11/23 16:25:07 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/23 16:26:51 visual_prompt]: 	Training 100/553. train loss: 9.9833,	0.8262 s / batch. (data: 1.02e-02). ETA=11:23:58, max mem: 20.9 GB 
[11/23 16:28:30 visual_prompt]: 	Training 200/553. train loss: 469.3429,	0.8405 s / batch. (data: 2.33e-02). ETA=11:34:23, max mem: 20.9 GB 
[11/23 16:30:06 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.9720 s / batch. (data: 1.14e+00). ETA=1 day, 3:05:54, max mem: 20.9 GB 
[11/23 16:31:40 visual_prompt]: 	Training 400/553. train loss: 100.8531,	0.8329 s / batch. (data: 7.82e-04). ETA=11:25:20, max mem: 20.9 GB 
[11/23 16:33:14 visual_prompt]: 	Training 500/553. train loss: 263.9169,	0.9840 s / batch. (data: 1.57e-01). ETA=13:28:01, max mem: 20.9 GB 
[11/23 16:34:06 visual_prompt]: Epoch 11 / 100: avg data time: 1.47e-01, avg batch time: 0.9734, average train loss: 146.0352
[11/23 16:35:01 visual_prompt]: Inference (val):avg data time: 1.68e-04, avg batch time: 0.3095, average loss: 132.6761
[11/23 16:35:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.21	
[11/23 16:35:01 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/23 16:36:43 visual_prompt]: 	Training 100/553. train loss: 60.8919,	0.8410 s / batch. (data: 1.05e-02). ETA=11:28:29, max mem: 20.9 GB 
[11/23 16:38:20 visual_prompt]: 	Training 200/553. train loss: 56.2458,	1.0464 s / batch. (data: 2.18e-01). ETA=14:14:51, max mem: 20.9 GB 
[11/23 16:39:57 visual_prompt]: 	Training 300/553. train loss: 37.1902,	0.8240 s / batch. (data: 1.20e-02). ETA=11:11:47, max mem: 20.9 GB 
[11/23 16:41:34 visual_prompt]: 	Training 400/553. train loss: 9.3180,	0.8586 s / batch. (data: 5.25e-04). ETA=11:38:32, max mem: 20.9 GB 
[11/23 16:43:11 visual_prompt]: 	Training 500/553. train loss: 13.2477,	0.8277 s / batch. (data: 7.72e-04). ETA=11:12:02, max mem: 20.9 GB 
[11/23 16:44:00 visual_prompt]: Epoch 12 / 100: avg data time: 1.49e-01, avg batch time: 0.9749, average train loss: 172.4881
[11/23 16:44:56 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3108, average loss: 32.2117
[11/23 16:44:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.13	
[11/23 16:44:56 visual_prompt]: Best epoch 12: best metric: -32.212
[11/23 16:44:56 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/23 16:46:38 visual_prompt]: 	Training 100/553. train loss: 126.3914,	0.8035 s / batch. (data: 3.49e-04). ETA=10:50:21, max mem: 20.9 GB 
[11/23 16:48:11 visual_prompt]: 	Training 200/553. train loss: 6.3517,	0.8309 s / batch. (data: 1.05e-02). ETA=11:11:09, max mem: 20.9 GB 
[11/23 16:49:48 visual_prompt]: 	Training 300/553. train loss: 238.2179,	1.6805 s / batch. (data: 8.76e-01). ETA=22:34:37, max mem: 20.9 GB 
[11/23 16:51:22 visual_prompt]: 	Training 400/553. train loss: 48.1541,	0.8482 s / batch. (data: 1.63e-02). ETA=11:22:15, max mem: 20.9 GB 
[11/23 16:52:59 visual_prompt]: 	Training 500/553. train loss: 359.6395,	0.8637 s / batch. (data: 2.96e-04). ETA=11:33:18, max mem: 20.9 GB 
[11/23 16:53:49 visual_prompt]: Epoch 13 / 100: avg data time: 1.37e-01, avg batch time: 0.9645, average train loss: 158.3108
[11/23 16:54:43 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3115, average loss: 64.0487
[11/23 16:54:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.74	
[11/23 16:54:43 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/23 16:56:24 visual_prompt]: 	Training 100/553. train loss: 354.7159,	0.8390 s / batch. (data: 2.99e-04). ETA=11:11:22, max mem: 20.9 GB 
[11/23 16:57:59 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8444 s / batch. (data: 3.19e-02). ETA=11:14:17, max mem: 20.9 GB 
[11/23 16:59:34 visual_prompt]: 	Training 300/553. train loss: 281.6381,	0.8199 s / batch. (data: 2.93e-04). ETA=10:53:22, max mem: 20.9 GB 
[11/23 17:01:09 visual_prompt]: 	Training 400/553. train loss: 143.7888,	0.8214 s / batch. (data: 2.98e-04). ETA=10:53:09, max mem: 20.9 GB 
[11/23 17:02:44 visual_prompt]: 	Training 500/553. train loss: 306.5684,	0.8270 s / batch. (data: 5.42e-03). ETA=10:56:12, max mem: 20.9 GB 
[11/23 17:03:33 visual_prompt]: Epoch 14 / 100: avg data time: 1.31e-01, avg batch time: 0.9569, average train loss: 151.1336
[11/23 17:04:27 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3093, average loss: 183.7384
[11/23 17:04:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.66	
[11/23 17:04:27 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/23 17:06:06 visual_prompt]: 	Training 100/553. train loss: 105.0922,	1.5085 s / batch. (data: 7.01e-01). ETA=19:53:10, max mem: 20.9 GB 
[11/23 17:07:40 visual_prompt]: 	Training 200/553. train loss: 1187.5264,	0.8240 s / batch. (data: 2.99e-04). ETA=10:50:22, max mem: 20.9 GB 
[11/23 17:09:18 visual_prompt]: 	Training 300/553. train loss: 32.1880,	0.8280 s / batch. (data: 3.06e-04). ETA=10:52:08, max mem: 20.9 GB 
[11/23 17:10:52 visual_prompt]: 	Training 400/553. train loss: 28.2467,	0.8960 s / batch. (data: 7.09e-02). ETA=11:44:12, max mem: 20.9 GB 
[11/23 17:12:29 visual_prompt]: 	Training 500/553. train loss: 68.7989,	0.8400 s / batch. (data: 7.96e-03). ETA=10:58:49, max mem: 20.9 GB 
[11/23 17:13:20 visual_prompt]: Epoch 15 / 100: avg data time: 1.37e-01, avg batch time: 0.9640, average train loss: 206.1229
[11/23 17:14:15 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3110, average loss: 118.2958
[11/23 17:14:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.74	
[11/23 17:14:15 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/23 17:15:54 visual_prompt]: 	Training 100/553. train loss: 202.4007,	0.8265 s / batch. (data: 3.03e-04). ETA=10:46:09, max mem: 20.9 GB 
[11/23 17:17:31 visual_prompt]: 	Training 200/553. train loss: 48.7037,	0.8216 s / batch. (data: 9.58e-03). ETA=10:40:55, max mem: 20.9 GB 
[11/23 17:19:07 visual_prompt]: 	Training 300/553. train loss: 37.0103,	0.8280 s / batch. (data: 3.88e-04). ETA=10:44:30, max mem: 20.9 GB 
[11/23 17:20:44 visual_prompt]: 	Training 400/553. train loss: 27.4519,	0.8427 s / batch. (data: 2.49e-02). ETA=10:54:35, max mem: 20.9 GB 
[11/23 17:22:19 visual_prompt]: 	Training 500/553. train loss: 121.7329,	1.4128 s / batch. (data: 5.89e-01). ETA=18:15:03, max mem: 20.9 GB 
[11/23 17:23:10 visual_prompt]: Epoch 16 / 100: avg data time: 1.41e-01, avg batch time: 0.9682, average train loss: 164.7299
[11/23 17:24:06 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3107, average loss: 117.7863
[11/23 17:24:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.32	
[11/23 17:24:06 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/23 17:25:45 visual_prompt]: 	Training 100/553. train loss: 58.3650,	0.8355 s / batch. (data: 1.15e-02). ETA=10:45:25, max mem: 20.9 GB 
[11/23 17:27:23 visual_prompt]: 	Training 200/553. train loss: 227.1207,	0.8368 s / batch. (data: 1.05e-02). ETA=10:45:04, max mem: 20.9 GB 
[11/23 17:28:58 visual_prompt]: 	Training 300/553. train loss: 46.0636,	0.8332 s / batch. (data: 1.49e-02). ETA=10:40:53, max mem: 20.9 GB 
[11/23 17:30:34 visual_prompt]: 	Training 400/553. train loss: 114.2639,	1.1240 s / batch. (data: 2.83e-01). ETA=14:22:42, max mem: 20.9 GB 
[11/23 17:32:10 visual_prompt]: 	Training 500/553. train loss: 87.6181,	1.5366 s / batch. (data: 7.22e-01). ETA=19:36:47, max mem: 20.9 GB 
[11/23 17:33:01 visual_prompt]: Epoch 17 / 100: avg data time: 1.42e-01, avg batch time: 0.9687, average train loss: 168.7049
[11/23 17:33:56 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3085, average loss: 22.6526
[11/23 17:33:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.86	
[11/23 17:33:56 visual_prompt]: Best epoch 17: best metric: -22.653
[11/23 17:33:56 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/23 17:35:37 visual_prompt]: 	Training 100/553. train loss: 131.4120,	0.8284 s / batch. (data: 2.88e-04). ETA=10:32:18, max mem: 20.9 GB 
[11/23 17:37:16 visual_prompt]: 	Training 200/553. train loss: 82.1683,	0.8312 s / batch. (data: 8.80e-04). ETA=10:33:05, max mem: 20.9 GB 
[11/23 17:38:52 visual_prompt]: 	Training 300/553. train loss: 208.3451,	0.8318 s / batch. (data: 3.34e-04). ETA=10:32:10, max mem: 20.9 GB 
[11/23 17:40:29 visual_prompt]: 	Training 400/553. train loss: 118.2526,	0.8385 s / batch. (data: 3.11e-04). ETA=10:35:52, max mem: 20.9 GB 
[11/23 17:42:04 visual_prompt]: 	Training 500/553. train loss: 105.4762,	0.8318 s / batch. (data: 3.11e-04). ETA=10:29:23, max mem: 20.9 GB 
[11/23 17:42:53 visual_prompt]: Epoch 18 / 100: avg data time: 1.44e-01, avg batch time: 0.9706, average train loss: 173.0700
[11/23 17:43:50 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3087, average loss: 234.8130
[11/23 17:43:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.62	
[11/23 17:43:50 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/23 17:45:31 visual_prompt]: 	Training 100/553. train loss: 102.9204,	0.8280 s / batch. (data: 3.08e-04). ETA=10:24:24, max mem: 20.9 GB 
[11/23 17:47:08 visual_prompt]: 	Training 200/553. train loss: 134.7831,	0.8234 s / batch. (data: 2.90e-04). ETA=10:19:35, max mem: 20.9 GB 
[11/23 17:48:44 visual_prompt]: 	Training 300/553. train loss: 185.0444,	0.8400 s / batch. (data: 2.94e-04). ETA=10:30:39, max mem: 20.9 GB 
[11/23 17:50:21 visual_prompt]: 	Training 400/553. train loss: 27.8588,	0.8058 s / batch. (data: 2.96e-04). ETA=10:03:38, max mem: 20.9 GB 
[11/23 17:51:54 visual_prompt]: 	Training 500/553. train loss: 32.7093,	0.8324 s / batch. (data: 5.39e-03). ETA=10:22:07, max mem: 20.9 GB 
[11/23 17:52:44 visual_prompt]: Epoch 19 / 100: avg data time: 1.38e-01, avg batch time: 0.9640, average train loss: 148.0561
[11/23 17:53:38 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.3109, average loss: 392.9309
[11/23 17:53:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.46	
[11/23 17:53:38 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/23 17:55:17 visual_prompt]: 	Training 100/553. train loss: 4.1941,	0.8654 s / batch. (data: 4.93e-02). ETA=10:44:39, max mem: 20.9 GB 
[11/23 17:56:55 visual_prompt]: 	Training 200/553. train loss: 0.8545,	0.8365 s / batch. (data: 2.95e-04). ETA=10:21:41, max mem: 20.9 GB 
[11/23 17:58:31 visual_prompt]: 	Training 300/553. train loss: 68.5057,	0.8440 s / batch. (data: 2.84e-04). ETA=10:25:52, max mem: 20.9 GB 
[11/23 18:00:06 visual_prompt]: 	Training 400/553. train loss: 805.9830,	0.8413 s / batch. (data: 5.41e-03). ETA=10:22:27, max mem: 20.9 GB 
[11/23 18:01:42 visual_prompt]: 	Training 500/553. train loss: 203.7889,	0.8400 s / batch. (data: 2.86e-04). ETA=10:20:06, max mem: 20.9 GB 
[11/23 18:02:33 visual_prompt]: Epoch 20 / 100: avg data time: 1.40e-01, avg batch time: 0.9672, average train loss: 131.3989
[11/23 18:03:28 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3087, average loss: 165.6595
[11/23 18:03:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.91	
[11/23 18:03:28 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/23 18:05:11 visual_prompt]: 	Training 100/553. train loss: 19.3996,	0.8095 s / batch. (data: 3.32e-04). ETA=9:55:31, max mem: 20.9 GB 
[11/23 18:06:46 visual_prompt]: 	Training 200/553. train loss: 379.2604,	0.8194 s / batch. (data: 5.35e-03). ETA=10:01:28, max mem: 20.9 GB 
[11/23 18:08:21 visual_prompt]: 	Training 300/553. train loss: 641.3419,	0.9480 s / batch. (data: 1.17e-01). ETA=11:34:16, max mem: 20.9 GB 
[11/23 18:09:57 visual_prompt]: 	Training 400/553. train loss: 61.0983,	0.8372 s / batch. (data: 2.81e-04). ETA=10:11:40, max mem: 20.9 GB 
[11/23 18:11:34 visual_prompt]: 	Training 500/553. train loss: 171.5970,	0.8280 s / batch. (data: 2.96e-04). ETA=10:03:38, max mem: 20.9 GB 
[11/23 18:12:23 visual_prompt]: Epoch 21 / 100: avg data time: 1.40e-01, avg batch time: 0.9670, average train loss: 151.6709
[11/23 18:13:18 visual_prompt]: Inference (val):avg data time: 5.84e-04, avg batch time: 0.3112, average loss: 111.4839
[11/23 18:13:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.89	
[11/23 18:13:18 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/23 18:14:58 visual_prompt]: 	Training 100/553. train loss: 122.2994,	0.8274 s / batch. (data: 7.57e-03). ETA=10:01:02, max mem: 20.9 GB 
[11/23 18:16:34 visual_prompt]: 	Training 200/553. train loss: 80.7615,	0.8360 s / batch. (data: 2.82e-04). ETA=10:05:54, max mem: 20.9 GB 
[11/23 18:18:08 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8320 s / batch. (data: 3.07e-04). ETA=10:01:38, max mem: 20.9 GB 
[11/23 18:19:46 visual_prompt]: 	Training 400/553. train loss: 179.3458,	0.8557 s / batch. (data: 7.69e-03). ETA=10:17:21, max mem: 20.9 GB 
[11/23 18:21:23 visual_prompt]: 	Training 500/553. train loss: 114.1174,	0.8059 s / batch. (data: 3.11e-04). ETA=9:40:02, max mem: 20.9 GB 
[11/23 18:22:15 visual_prompt]: Epoch 22 / 100: avg data time: 1.43e-01, avg batch time: 0.9702, average train loss: 152.8411
[11/23 18:23:10 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3101, average loss: 75.6044
[11/23 18:23:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.51	
[11/23 18:23:10 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/23 18:24:53 visual_prompt]: 	Training 100/553. train loss: 238.0629,	0.8140 s / batch. (data: 3.19e-04). ETA=9:43:50, max mem: 20.9 GB 
[11/23 18:26:29 visual_prompt]: 	Training 200/553. train loss: 108.5072,	0.8360 s / batch. (data: 1.60e-02). ETA=9:58:13, max mem: 20.9 GB 
[11/23 18:28:08 visual_prompt]: 	Training 300/553. train loss: 15.3152,	0.8535 s / batch. (data: 7.83e-04). ETA=10:09:16, max mem: 20.9 GB 
[11/23 18:29:42 visual_prompt]: 	Training 400/553. train loss: 370.6310,	0.8455 s / batch. (data: 2.86e-04). ETA=10:02:09, max mem: 20.9 GB 
[11/23 18:31:16 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8240 s / batch. (data: 2.86e-04). ETA=9:45:30, max mem: 20.9 GB 
[11/23 18:32:07 visual_prompt]: Epoch 23 / 100: avg data time: 1.44e-01, avg batch time: 0.9707, average train loss: 164.8058
[11/23 18:33:02 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3092, average loss: 19.6481
[11/23 18:33:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.39	
[11/23 18:33:02 visual_prompt]: Best epoch 23: best metric: -19.648
[11/23 18:33:02 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/23 18:34:39 visual_prompt]: 	Training 100/553. train loss: 220.7708,	0.8240 s / batch. (data: 7.95e-03). ETA=9:43:24, max mem: 20.9 GB 
[11/23 18:36:15 visual_prompt]: 	Training 200/553. train loss: 57.7030,	0.8497 s / batch. (data: 1.20e-02). ETA=10:00:11, max mem: 20.9 GB 
[11/23 18:37:52 visual_prompt]: 	Training 300/553. train loss: 165.2409,	0.8177 s / batch. (data: 8.45e-03). ETA=9:36:14, max mem: 20.9 GB 
[11/23 18:39:30 visual_prompt]: 	Training 400/553. train loss: 54.8669,	0.8423 s / batch. (data: 1.03e-02). ETA=9:52:09, max mem: 20.9 GB 
[11/23 18:41:09 visual_prompt]: 	Training 500/553. train loss: 233.0795,	0.8370 s / batch. (data: 5.41e-03). ETA=9:47:01, max mem: 20.9 GB 
[11/23 18:42:00 visual_prompt]: Epoch 24 / 100: avg data time: 1.46e-01, avg batch time: 0.9728, average train loss: 146.3013
[11/23 18:42:55 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.3095, average loss: 171.0182
[11/23 18:42:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.56	
[11/23 18:42:55 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/23 18:44:40 visual_prompt]: 	Training 100/553. train loss: 15.2874,	0.8351 s / batch. (data: 3.01e-04). ETA=9:43:32, max mem: 20.9 GB 
[11/23 18:46:14 visual_prompt]: 	Training 200/553. train loss: 186.4530,	0.8361 s / batch. (data: 1.42e-02). ETA=9:42:52, max mem: 20.9 GB 
[11/23 18:47:50 visual_prompt]: 	Training 300/553. train loss: 25.8697,	0.8397 s / batch. (data: 3.17e-04). ETA=9:43:58, max mem: 20.9 GB 
[11/23 18:49:27 visual_prompt]: 	Training 400/553. train loss: 33.5844,	1.1930 s / batch. (data: 3.68e-01). ETA=13:47:42, max mem: 20.9 GB 
[11/23 18:51:04 visual_prompt]: 	Training 500/553. train loss: 120.7112,	1.4435 s / batch. (data: 6.15e-01). ETA=16:39:04, max mem: 20.9 GB 
[11/23 18:51:55 visual_prompt]: Epoch 25 / 100: avg data time: 1.50e-01, avg batch time: 0.9761, average train loss: 152.1037
[11/23 18:52:50 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3102, average loss: 209.1437
[11/23 18:52:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.07	
[11/23 18:52:50 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/23 18:54:32 visual_prompt]: 	Training 100/553. train loss: 83.1610,	0.8267 s / batch. (data: 9.58e-03). ETA=9:30:05, max mem: 20.9 GB 
[11/23 18:56:10 visual_prompt]: 	Training 200/553. train loss: 598.5208,	1.5891 s / batch. (data: 7.75e-01). ETA=18:13:08, max mem: 20.9 GB 
[11/23 18:57:48 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8154 s / batch. (data: 8.06e-04). ETA=9:19:32, max mem: 20.9 GB 
[11/23 18:59:24 visual_prompt]: 	Training 400/553. train loss: 457.4002,	0.8248 s / batch. (data: 4.28e-04). ETA=9:24:39, max mem: 20.9 GB 
[11/23 19:01:00 visual_prompt]: 	Training 500/553. train loss: 34.4658,	0.8480 s / batch. (data: 8.08e-04). ETA=9:39:06, max mem: 20.9 GB 
[11/23 19:01:50 visual_prompt]: Epoch 26 / 100: avg data time: 1.49e-01, avg batch time: 0.9759, average train loss: 149.0037
[11/23 19:02:46 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3097, average loss: 44.7650
[11/23 19:02:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.99	
[11/23 19:02:46 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/23 19:04:28 visual_prompt]: 	Training 100/553. train loss: 14.7403,	0.8605 s / batch. (data: 4.68e-04). ETA=9:45:25, max mem: 20.9 GB 
[11/23 19:06:04 visual_prompt]: 	Training 200/553. train loss: 319.8436,	1.0278 s / batch. (data: 1.95e-01). ETA=11:37:33, max mem: 20.9 GB 
[11/23 19:07:41 visual_prompt]: 	Training 300/553. train loss: 199.6077,	0.8360 s / batch. (data: 7.95e-03). ETA=9:25:59, max mem: 20.9 GB 
[11/23 19:09:20 visual_prompt]: 	Training 400/553. train loss: 0.2961,	0.8590 s / batch. (data: 5.91e-03). ETA=9:40:07, max mem: 20.9 GB 
[11/23 19:10:57 visual_prompt]: 	Training 500/553. train loss: 63.2854,	0.8280 s / batch. (data: 1.20e-02). ETA=9:17:48, max mem: 20.9 GB 
[11/23 19:11:46 visual_prompt]: Epoch 27 / 100: avg data time: 1.49e-01, avg batch time: 0.9762, average train loss: 175.4549
[11/23 19:12:41 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3088, average loss: 296.2717
[11/23 19:12:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 34.15	
[11/23 19:12:41 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/23 19:14:21 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8129 s / batch. (data: 3.05e-04). ETA=9:05:34, max mem: 20.9 GB 
[11/23 19:15:59 visual_prompt]: 	Training 200/553. train loss: 147.5573,	0.8271 s / batch. (data: 3.12e-04). ETA=9:13:45, max mem: 20.9 GB 
[11/23 19:17:37 visual_prompt]: 	Training 300/553. train loss: 39.2931,	1.2597 s / batch. (data: 4.55e-01). ETA=14:01:15, max mem: 20.9 GB 
[11/23 19:19:11 visual_prompt]: 	Training 400/553. train loss: 13.6634,	0.8292 s / batch. (data: 3.11e-04). ETA=9:12:20, max mem: 20.9 GB 
[11/23 19:20:46 visual_prompt]: 	Training 500/553. train loss: 440.2202,	0.8240 s / batch. (data: 5.42e-03). ETA=9:07:33, max mem: 20.9 GB 
[11/23 19:21:36 visual_prompt]: Epoch 28 / 100: avg data time: 1.41e-01, avg batch time: 0.9678, average train loss: 158.8234
[11/23 19:22:31 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3120, average loss: 254.6606
[11/23 19:22:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.74	
[11/23 19:22:31 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/23 19:24:17 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8250 s / batch. (data: 2.91e-04). ETA=9:06:04, max mem: 20.9 GB 
[11/23 19:25:53 visual_prompt]: 	Training 200/553. train loss: 346.1683,	1.6812 s / batch. (data: 8.65e-01). ETA=18:30:01, max mem: 20.9 GB 
[11/23 19:27:26 visual_prompt]: 	Training 300/553. train loss: 34.8636,	0.8499 s / batch. (data: 6.87e-04). ETA=9:19:44, max mem: 20.9 GB 
[11/23 19:28:59 visual_prompt]: 	Training 400/553. train loss: 364.2102,	1.2079 s / batch. (data: 3.83e-01). ETA=13:13:30, max mem: 20.9 GB 
[11/23 19:30:35 visual_prompt]: 	Training 500/553. train loss: 46.9617,	0.8280 s / batch. (data: 3.15e-04). ETA=9:02:34, max mem: 20.9 GB 
[11/23 19:31:25 visual_prompt]: Epoch 29 / 100: avg data time: 1.38e-01, avg batch time: 0.9652, average train loss: 139.2024
[11/23 19:32:20 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3102, average loss: 19.5667
[11/23 19:32:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.65	
[11/23 19:32:20 visual_prompt]: Best epoch 29: best metric: -19.567
[11/23 19:32:20 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/23 19:33:58 visual_prompt]: 	Training 100/553. train loss: 154.6503,	0.8600 s / batch. (data: 7.96e-03). ETA=9:21:19, max mem: 20.9 GB 
[11/23 19:35:36 visual_prompt]: 	Training 200/553. train loss: 16.3775,	0.8250 s / batch. (data: 3.19e-03). ETA=8:57:07, max mem: 20.9 GB 
[11/23 19:37:10 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.3640 s / batch. (data: 5.31e-01). ETA=14:45:44, max mem: 20.9 GB 
[11/23 19:38:49 visual_prompt]: 	Training 400/553. train loss: 98.6266,	1.0874 s / batch. (data: 2.71e-01). ETA=11:44:18, max mem: 20.9 GB 
[11/23 19:40:25 visual_prompt]: 	Training 500/553. train loss: 48.5852,	1.3840 s / batch. (data: 5.34e-01). ETA=14:54:07, max mem: 20.9 GB 
[11/23 19:41:17 visual_prompt]: Epoch 30 / 100: avg data time: 1.44e-01, avg batch time: 0.9716, average train loss: 140.6919
[11/23 19:42:13 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3088, average loss: 85.6259
[11/23 19:42:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.40	
[11/23 19:42:13 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[11/23 19:43:54 visual_prompt]: 	Training 100/553. train loss: 122.3913,	0.8320 s / batch. (data: 3.37e-04). ETA=8:55:23, max mem: 20.9 GB 
[11/23 19:45:33 visual_prompt]: 	Training 200/553. train loss: 31.0906,	0.8412 s / batch. (data: 2.78e-04). ETA=8:59:55, max mem: 20.9 GB 
[11/23 19:47:06 visual_prompt]: 	Training 300/553. train loss: 229.8587,	0.8280 s / batch. (data: 3.21e-04). ETA=8:50:02, max mem: 20.9 GB 
[11/23 19:48:41 visual_prompt]: 	Training 400/553. train loss: 157.4476,	1.0440 s / batch. (data: 2.16e-01). ETA=11:06:35, max mem: 20.9 GB 
[11/23 19:50:17 visual_prompt]: 	Training 500/553. train loss: 97.8932,	0.8173 s / batch. (data: 3.13e-04). ETA=8:40:29, max mem: 20.9 GB 
[11/23 19:51:06 visual_prompt]: Epoch 31 / 100: avg data time: 1.37e-01, avg batch time: 0.9642, average train loss: 142.1276
[11/23 19:52:01 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3103, average loss: 81.8708
[11/23 19:52:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.45	
[11/23 19:52:01 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[11/23 19:53:42 visual_prompt]: 	Training 100/553. train loss: 165.4758,	0.8480 s / batch. (data: 7.57e-04). ETA=8:57:52, max mem: 20.9 GB 
[11/23 19:55:17 visual_prompt]: 	Training 200/553. train loss: 15.5328,	0.8300 s / batch. (data: 7.50e-04). ETA=8:45:05, max mem: 20.9 GB 
[11/23 19:56:56 visual_prompt]: 	Training 300/553. train loss: 174.6999,	0.8390 s / batch. (data: 1.05e-02). ETA=8:49:23, max mem: 20.9 GB 
[11/23 19:58:32 visual_prompt]: 	Training 400/553. train loss: 54.7532,	0.8320 s / batch. (data: 3.01e-04). ETA=8:43:33, max mem: 20.9 GB 
[11/23 20:00:05 visual_prompt]: 	Training 500/553. train loss: 37.7764,	0.8355 s / batch. (data: 2.93e-04). ETA=8:44:21, max mem: 20.9 GB 
[11/23 20:00:54 visual_prompt]: Epoch 32 / 100: avg data time: 1.36e-01, avg batch time: 0.9638, average train loss: 139.3340
[11/23 20:01:49 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3107, average loss: 19.7126
[11/23 20:01:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.32	
[11/23 20:01:49 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[11/23 20:03:29 visual_prompt]: 	Training 100/553. train loss: 4.8962,	0.8520 s / batch. (data: 3.28e-04). ETA=8:52:33, max mem: 20.9 GB 
[11/23 20:05:07 visual_prompt]: 	Training 200/553. train loss: 18.0701,	0.9720 s / batch. (data: 1.35e-01). ETA=10:05:56, max mem: 20.9 GB 
[11/23 20:06:44 visual_prompt]: 	Training 300/553. train loss: 210.4590,	0.8455 s / batch. (data: 2.45e-04). ETA=8:45:41, max mem: 20.9 GB 
[11/23 20:08:22 visual_prompt]: 	Training 400/553. train loss: 330.1152,	0.8283 s / batch. (data: 3.32e-04). ETA=8:33:37, max mem: 20.9 GB 
[11/23 20:09:58 visual_prompt]: 	Training 500/553. train loss: 507.6531,	0.8245 s / batch. (data: 1.05e-02). ETA=8:29:50, max mem: 20.9 GB 
[11/23 20:10:48 visual_prompt]: Epoch 33 / 100: avg data time: 1.45e-01, avg batch time: 0.9733, average train loss: 143.6312
[11/23 20:11:43 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3093, average loss: 127.1067
[11/23 20:11:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.03	
[11/23 20:11:43 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[11/23 20:13:25 visual_prompt]: 	Training 100/553. train loss: 19.8462,	0.8309 s / batch. (data: 3.31e-04). ETA=8:31:41, max mem: 20.9 GB 
[11/23 20:15:01 visual_prompt]: 	Training 200/553. train loss: 18.9643,	0.8496 s / batch. (data: 5.43e-03). ETA=8:41:48, max mem: 20.9 GB 
[11/23 20:16:37 visual_prompt]: 	Training 300/553. train loss: 309.5136,	0.8383 s / batch. (data: 5.41e-03). ETA=8:33:28, max mem: 20.9 GB 
[11/23 20:18:15 visual_prompt]: 	Training 400/553. train loss: 8.1546,	0.8329 s / batch. (data: 3.45e-04). ETA=8:28:47, max mem: 20.9 GB 
[11/23 20:19:53 visual_prompt]: 	Training 500/553. train loss: 112.1696,	1.4725 s / batch. (data: 6.44e-01). ETA=14:57:01, max mem: 20.9 GB 
[11/23 20:20:43 visual_prompt]: Epoch 34 / 100: avg data time: 1.50e-01, avg batch time: 0.9760, average train loss: 149.7802
[11/23 20:21:38 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3113, average loss: 111.8793
[11/23 20:21:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.49	
[11/23 20:21:38 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[11/23 20:23:21 visual_prompt]: 	Training 100/553. train loss: 103.1113,	0.8429 s / batch. (data: 1.23e-02). ETA=8:31:18, max mem: 20.9 GB 
[11/23 20:25:00 visual_prompt]: 	Training 200/553. train loss: 27.2771,	0.8501 s / batch. (data: 3.76e-04). ETA=8:34:15, max mem: 20.9 GB 
[11/23 20:26:35 visual_prompt]: 	Training 300/553. train loss: 37.5424,	0.8615 s / batch. (data: 2.57e-02). ETA=8:39:46, max mem: 20.9 GB 
[11/23 20:28:11 visual_prompt]: 	Training 400/553. train loss: 14.9664,	0.8360 s / batch. (data: 2.95e-04). ETA=8:22:58, max mem: 20.9 GB 
[11/23 20:29:47 visual_prompt]: 	Training 500/553. train loss: 55.5980,	0.9902 s / batch. (data: 1.81e-01). ETA=9:54:05, max mem: 20.9 GB 
[11/23 20:30:38 visual_prompt]: Epoch 35 / 100: avg data time: 1.49e-01, avg batch time: 0.9763, average train loss: 149.4510
[11/23 20:31:34 visual_prompt]: Inference (val):avg data time: 2.84e-04, avg batch time: 0.3104, average loss: 265.0159
[11/23 20:31:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.16	
[11/23 20:31:34 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[11/23 20:33:14 visual_prompt]: 	Training 100/553. train loss: 115.7175,	0.8139 s / batch. (data: 1.06e-02). ETA=8:06:15, max mem: 20.9 GB 
[11/23 20:34:52 visual_prompt]: 	Training 200/553. train loss: 506.0617,	0.8400 s / batch. (data: 1.20e-02). ETA=8:20:24, max mem: 20.9 GB 
[11/23 20:36:31 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8169 s / batch. (data: 5.44e-03). ETA=8:05:18, max mem: 20.9 GB 
[11/23 20:38:07 visual_prompt]: 	Training 400/553. train loss: 23.3570,	0.8342 s / batch. (data: 3.04e-04). ETA=8:14:10, max mem: 20.9 GB 
[11/23 20:39:45 visual_prompt]: 	Training 500/553. train loss: 290.1146,	0.8419 s / batch. (data: 3.08e-04). ETA=8:17:21, max mem: 20.9 GB 
[11/23 20:40:33 visual_prompt]: Epoch 36 / 100: avg data time: 1.48e-01, avg batch time: 0.9748, average train loss: 140.2568
[11/23 20:41:28 visual_prompt]: Inference (val):avg data time: 2.28e-04, avg batch time: 0.3106, average loss: 111.5739
[11/23 20:41:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.40	
[11/23 20:41:28 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[11/23 20:43:09 visual_prompt]: 	Training 100/553. train loss: 141.5817,	0.8134 s / batch. (data: 3.13e-04). ETA=7:58:25, max mem: 20.9 GB 
[11/23 20:44:47 visual_prompt]: 	Training 200/553. train loss: 168.3323,	0.8455 s / batch. (data: 3.02e-04). ETA=8:15:55, max mem: 20.9 GB 
[11/23 20:46:24 visual_prompt]: 	Training 300/553. train loss: 281.6743,	1.2520 s / batch. (data: 4.01e-01). ETA=12:12:15, max mem: 20.9 GB 
[11/23 20:48:04 visual_prompt]: 	Training 400/553. train loss: 3.9071,	1.8665 s / batch. (data: 1.04e+00). ETA=18:08:32, max mem: 20.9 GB 
[11/23 20:49:37 visual_prompt]: 	Training 500/553. train loss: 34.7462,	0.9944 s / batch. (data: 1.66e-01). ETA=9:38:15, max mem: 20.9 GB 
[11/23 20:50:30 visual_prompt]: Epoch 37 / 100: avg data time: 1.52e-01, avg batch time: 0.9784, average train loss: 140.7057
[11/23 20:51:25 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3116, average loss: 52.1396
[11/23 20:51:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.22	
[11/23 20:51:25 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[11/23 20:53:04 visual_prompt]: 	Training 100/553. train loss: 6.9585,	0.8279 s / batch. (data: 5.44e-03). ETA=7:59:21, max mem: 20.9 GB 
[11/23 20:54:42 visual_prompt]: 	Training 200/553. train loss: 107.9177,	1.1703 s / batch. (data: 3.26e-01). ETA=11:15:37, max mem: 20.9 GB 
[11/23 20:56:20 visual_prompt]: 	Training 300/553. train loss: 33.5977,	0.8240 s / batch. (data: 3.09e-04). ETA=7:54:20, max mem: 20.9 GB 
[11/23 20:57:55 visual_prompt]: 	Training 400/553. train loss: 126.3971,	0.8210 s / batch. (data: 3.11e-04). ETA=7:51:13, max mem: 20.9 GB 
[11/23 20:59:34 visual_prompt]: 	Training 500/553. train loss: 5.5677,	0.8318 s / batch. (data: 3.27e-04). ETA=7:56:03, max mem: 20.9 GB 
[11/23 21:00:24 visual_prompt]: Epoch 38 / 100: avg data time: 1.47e-01, avg batch time: 0.9737, average train loss: 140.6521
[11/23 21:01:19 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3084, average loss: 276.6873
[11/23 21:01:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.70	
[11/23 21:01:19 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[11/23 21:02:58 visual_prompt]: 	Training 100/553. train loss: 0.0002,	0.8264 s / batch. (data: 9.41e-03). ETA=7:50:52, max mem: 20.9 GB 
[11/23 21:04:39 visual_prompt]: 	Training 200/553. train loss: 413.8148,	0.8664 s / batch. (data: 3.84e-02). ETA=8:12:13, max mem: 20.9 GB 
[11/23 21:06:19 visual_prompt]: 	Training 300/553. train loss: 124.1934,	0.8440 s / batch. (data: 7.96e-03). ETA=7:58:04, max mem: 20.9 GB 
[11/23 21:07:54 visual_prompt]: 	Training 400/553. train loss: 115.5211,	1.2268 s / batch. (data: 4.21e-01). ETA=11:32:51, max mem: 20.9 GB 
[11/23 21:09:30 visual_prompt]: 	Training 500/553. train loss: 40.5285,	1.7160 s / batch. (data: 8.60e-01). ETA=16:06:16, max mem: 20.9 GB 
[11/23 21:10:19 visual_prompt]: Epoch 39 / 100: avg data time: 1.49e-01, avg batch time: 0.9764, average train loss: 108.4750
[11/23 21:11:14 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3099, average loss: 209.0526
[11/23 21:11:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.69	
[11/23 21:11:14 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[11/23 21:12:56 visual_prompt]: 	Training 100/553. train loss: 375.7561,	0.8280 s / batch. (data: 2.93e-04). ETA=7:44:07, max mem: 20.9 GB 
[11/23 21:14:32 visual_prompt]: 	Training 200/553. train loss: 3.0492,	0.8440 s / batch. (data: 7.95e-03). ETA=7:51:41, max mem: 20.9 GB 
[11/23 21:16:11 visual_prompt]: 	Training 300/553. train loss: 100.3886,	0.8321 s / batch. (data: 7.87e-04). ETA=7:43:40, max mem: 20.9 GB 
[11/23 21:17:48 visual_prompt]: 	Training 400/553. train loss: 117.8543,	0.8199 s / batch. (data: 1.05e-02). ETA=7:35:28, max mem: 20.9 GB 
[11/23 21:19:24 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8120 s / batch. (data: 3.15e-04). ETA=7:29:46, max mem: 20.9 GB 
[11/23 21:20:16 visual_prompt]: Epoch 40 / 100: avg data time: 1.54e-01, avg batch time: 0.9799, average train loss: 132.2321
[11/23 21:21:12 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3105, average loss: 34.4287
[11/23 21:21:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.58	
[11/23 21:21:12 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[11/23 21:22:57 visual_prompt]: 	Training 100/553. train loss: 74.8127,	0.8402 s / batch. (data: 3.11e-04). ETA=7:43:12, max mem: 20.9 GB 
[11/23 21:24:36 visual_prompt]: 	Training 200/553. train loss: 32.2467,	0.8094 s / batch. (data: 7.10e-04). ETA=7:24:53, max mem: 20.9 GB 
[11/23 21:26:12 visual_prompt]: 	Training 300/553. train loss: 170.9885,	0.8296 s / batch. (data: 4.06e-04). ETA=7:34:35, max mem: 20.9 GB 
[11/23 21:27:48 visual_prompt]: 	Training 400/553. train loss: 106.4628,	0.8228 s / batch. (data: 2.68e-04). ETA=7:29:30, max mem: 20.9 GB 
[11/23 21:29:23 visual_prompt]: 	Training 500/553. train loss: 99.3491,	0.8160 s / batch. (data: 3.04e-04). ETA=7:24:26, max mem: 20.9 GB 
[11/23 21:30:11 visual_prompt]: Epoch 41 / 100: avg data time: 1.47e-01, avg batch time: 0.9744, average train loss: 141.9813
[11/23 21:31:07 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3122, average loss: 86.1727
[11/23 21:31:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.03	
[11/23 21:31:07 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[11/23 21:32:46 visual_prompt]: 	Training 100/553. train loss: 261.2810,	0.8201 s / batch. (data: 2.97e-04). ETA=7:24:35, max mem: 20.9 GB 
[11/23 21:34:23 visual_prompt]: 	Training 200/553. train loss: 1197.9874,	0.8474 s / batch. (data: 3.14e-04). ETA=7:37:59, max mem: 20.9 GB 
[11/23 21:36:01 visual_prompt]: 	Training 300/553. train loss: 27.7327,	0.8440 s / batch. (data: 3.12e-04). ETA=7:34:44, max mem: 20.9 GB 
[11/23 21:37:38 visual_prompt]: 	Training 400/553. train loss: 58.5433,	0.8551 s / batch. (data: 2.98e-04). ETA=7:39:16, max mem: 20.9 GB 
[11/23 21:39:14 visual_prompt]: 	Training 500/553. train loss: 0.5195,	0.8318 s / batch. (data: 1.41e-02). ETA=7:25:22, max mem: 20.9 GB 
[11/23 21:40:06 visual_prompt]: Epoch 42 / 100: avg data time: 1.48e-01, avg batch time: 0.9751, average train loss: 173.6028
[11/23 21:41:01 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3108, average loss: 80.3345
[11/23 21:41:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.94	
[11/23 21:41:01 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[11/23 21:42:44 visual_prompt]: 	Training 100/553. train loss: 210.0464,	0.8290 s / batch. (data: 2.96e-04). ETA=7:21:46, max mem: 20.9 GB 
[11/23 21:44:20 visual_prompt]: 	Training 200/553. train loss: 132.1355,	0.8341 s / batch. (data: 6.02e-03). ETA=7:23:04, max mem: 20.9 GB 
[11/23 21:45:56 visual_prompt]: 	Training 300/553. train loss: 264.3711,	0.8440 s / batch. (data: 3.00e-04). ETA=7:26:57, max mem: 20.9 GB 
[11/23 21:47:31 visual_prompt]: 	Training 400/553. train loss: 103.4724,	0.8395 s / batch. (data: 5.41e-03). ETA=7:23:09, max mem: 20.9 GB 
[11/23 21:49:10 visual_prompt]: 	Training 500/553. train loss: 181.9322,	0.8476 s / batch. (data: 1.05e-02). ETA=7:26:02, max mem: 20.9 GB 
[11/23 21:50:01 visual_prompt]: Epoch 43 / 100: avg data time: 1.50e-01, avg batch time: 0.9764, average train loss: 111.9246
[11/23 21:50:57 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3088, average loss: 79.2636
[11/23 21:50:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.53	
[11/23 21:50:57 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[11/23 21:52:38 visual_prompt]: 	Training 100/553. train loss: 351.5696,	0.8258 s / batch. (data: 2.19e-02). ETA=7:12:27, max mem: 20.9 GB 
[11/23 21:54:16 visual_prompt]: 	Training 200/553. train loss: 40.4058,	0.8454 s / batch. (data: 2.14e-02). ETA=7:21:18, max mem: 20.9 GB 
[11/23 21:55:51 visual_prompt]: 	Training 300/553. train loss: 184.5063,	0.8353 s / batch. (data: 1.13e-02). ETA=7:14:40, max mem: 20.9 GB 
[11/23 21:57:26 visual_prompt]: 	Training 400/553. train loss: 34.6635,	0.8199 s / batch. (data: 2.98e-04). ETA=7:05:17, max mem: 20.9 GB 
[11/23 21:59:02 visual_prompt]: 	Training 500/553. train loss: 3.6811,	0.8277 s / batch. (data: 4.41e-04). ETA=7:07:57, max mem: 20.9 GB 
[11/23 21:59:52 visual_prompt]: Epoch 44 / 100: avg data time: 1.41e-01, avg batch time: 0.9678, average train loss: 126.6946
[11/23 22:00:47 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3107, average loss: 172.6289
[11/23 22:00:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 55.42	
[11/23 22:00:47 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[11/23 22:02:28 visual_prompt]: 	Training 100/553. train loss: 4.7575,	0.8145 s / batch. (data: 3.12e-04). ETA=6:59:02, max mem: 20.9 GB 
[11/23 22:04:00 visual_prompt]: 	Training 200/553. train loss: 76.7605,	0.8115 s / batch. (data: 5.45e-03). ETA=6:56:08, max mem: 20.9 GB 
[11/23 22:05:38 visual_prompt]: 	Training 300/553. train loss: 517.3029,	0.8550 s / batch. (data: 1.89e-02). ETA=7:17:00, max mem: 20.9 GB 
[11/23 22:07:11 visual_prompt]: 	Training 400/553. train loss: 17.2243,	0.8360 s / batch. (data: 3.34e-04). ETA=7:05:53, max mem: 20.9 GB 
[11/23 22:08:50 visual_prompt]: 	Training 500/553. train loss: 47.1717,	0.8207 s / batch. (data: 1.05e-02). ETA=6:56:45, max mem: 20.9 GB 
[11/23 22:09:41 visual_prompt]: Epoch 45 / 100: avg data time: 1.37e-01, avg batch time: 0.9654, average train loss: 104.8145
[11/23 22:10:36 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3095, average loss: 39.0389
[11/23 22:10:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.39	
[11/23 22:10:36 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[11/23 22:12:17 visual_prompt]: 	Training 100/553. train loss: 97.2375,	0.8593 s / batch. (data: 2.45e-02). ETA=7:14:09, max mem: 20.9 GB 
[11/23 22:13:55 visual_prompt]: 	Training 200/553. train loss: 1137.4886,	0.8599 s / batch. (data: 1.10e-02). ETA=7:13:00, max mem: 20.9 GB 
[11/23 22:15:31 visual_prompt]: 	Training 300/553. train loss: 211.1017,	0.8169 s / batch. (data: 5.45e-03). ETA=6:50:01, max mem: 20.9 GB 
[11/23 22:17:09 visual_prompt]: 	Training 400/553. train loss: 21.3189,	0.8335 s / batch. (data: 7.81e-04). ETA=6:56:57, max mem: 20.9 GB 
[11/23 22:18:43 visual_prompt]: 	Training 500/553. train loss: 243.4124,	0.8400 s / batch. (data: 3.19e-04). ETA=6:58:48, max mem: 20.9 GB 
[11/23 22:19:35 visual_prompt]: Epoch 46 / 100: avg data time: 1.48e-01, avg batch time: 0.9749, average train loss: 112.3867
[11/23 22:20:30 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.3118, average loss: 77.4649
[11/23 22:20:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.69	
[11/23 22:20:30 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[11/23 22:22:12 visual_prompt]: 	Training 100/553. train loss: 54.7013,	0.8240 s / batch. (data: 2.95e-04). ETA=6:48:44, max mem: 20.9 GB 
[11/23 22:23:45 visual_prompt]: 	Training 200/553. train loss: 237.6187,	1.2520 s / batch. (data: 4.08e-01). ETA=10:18:56, max mem: 20.9 GB 
[11/23 22:25:21 visual_prompt]: 	Training 300/553. train loss: 5.9041,	0.8520 s / batch. (data: 1.41e-02). ETA=6:59:46, max mem: 20.9 GB 
[11/23 22:26:57 visual_prompt]: 	Training 400/553. train loss: 57.4155,	0.8433 s / batch. (data: 8.29e-04). ETA=6:54:05, max mem: 20.9 GB 
[11/23 22:28:31 visual_prompt]: 	Training 500/553. train loss: 181.2313,	0.8200 s / batch. (data: 2.89e-04). ETA=6:41:17, max mem: 20.9 GB 
[11/23 22:29:22 visual_prompt]: Epoch 47 / 100: avg data time: 1.34e-01, avg batch time: 0.9608, average train loss: 102.7517
[11/23 22:30:16 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3102, average loss: 113.9753
[11/23 22:30:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.59	
[11/23 22:30:16 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[11/23 22:31:56 visual_prompt]: 	Training 100/553. train loss: 101.8445,	0.8341 s / batch. (data: 2.06e-02). ETA=6:46:03, max mem: 20.9 GB 
[11/23 22:33:32 visual_prompt]: 	Training 200/553. train loss: 83.2641,	0.8548 s / batch. (data: 1.05e-02). ETA=6:54:41, max mem: 20.9 GB 
[11/23 22:35:09 visual_prompt]: 	Training 300/553. train loss: 197.4628,	1.5880 s / batch. (data: 7.32e-01). ETA=12:47:45, max mem: 20.9 GB 
[11/23 22:36:42 visual_prompt]: 	Training 400/553. train loss: 575.2391,	0.8368 s / batch. (data: 7.96e-03). ETA=6:43:10, max mem: 20.9 GB 
[11/23 22:38:18 visual_prompt]: 	Training 500/553. train loss: 41.3837,	0.8271 s / batch. (data: 1.60e-02). ETA=6:37:07, max mem: 20.9 GB 
[11/23 22:39:08 visual_prompt]: Epoch 48 / 100: avg data time: 1.36e-01, avg batch time: 0.9627, average train loss: 124.2199
[11/23 22:40:03 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3079, average loss: 174.4985
[11/23 22:40:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.39	
[11/23 22:40:03 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[11/23 22:41:43 visual_prompt]: 	Training 100/553. train loss: 109.8831,	0.8359 s / batch. (data: 1.50e-02). ETA=6:39:12, max mem: 20.9 GB 
[11/23 22:43:18 visual_prompt]: 	Training 200/553. train loss: 28.6203,	0.8425 s / batch. (data: 2.85e-04). ETA=6:40:58, max mem: 20.9 GB 
[11/23 22:44:55 visual_prompt]: 	Training 300/553. train loss: 182.3253,	0.8320 s / batch. (data: 2.77e-04). ETA=6:34:35, max mem: 20.9 GB 
[11/23 22:46:33 visual_prompt]: 	Training 400/553. train loss: 143.3421,	0.8216 s / batch. (data: 7.87e-04). ETA=6:28:17, max mem: 20.9 GB 
[11/23 22:48:09 visual_prompt]: 	Training 500/553. train loss: 57.9728,	0.8281 s / batch. (data: 2.72e-04). ETA=6:29:58, max mem: 20.9 GB 
[11/23 22:48:59 visual_prompt]: Epoch 49 / 100: avg data time: 1.42e-01, avg batch time: 0.9690, average train loss: 106.7669
[11/23 22:49:54 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3094, average loss: 70.2803
[11/23 22:49:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.07	
[11/23 22:49:54 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[11/23 22:51:35 visual_prompt]: 	Training 100/553. train loss: 135.8259,	0.8240 s / batch. (data: 3.04e-04). ETA=6:25:56, max mem: 20.9 GB 
[11/23 22:53:12 visual_prompt]: 	Training 200/553. train loss: 353.7992,	0.8201 s / batch. (data: 3.02e-04). ETA=6:22:44, max mem: 20.9 GB 
[11/23 22:54:47 visual_prompt]: 	Training 300/553. train loss: 450.7252,	0.8440 s / batch. (data: 7.66e-04). ETA=6:32:29, max mem: 20.9 GB 
[11/23 22:56:21 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8441 s / batch. (data: 7.98e-03). ETA=6:31:07, max mem: 20.9 GB 
[11/23 22:57:58 visual_prompt]: 	Training 500/553. train loss: 85.6277,	0.8320 s / batch. (data: 3.04e-04). ETA=6:24:08, max mem: 20.9 GB 
[11/23 22:58:48 visual_prompt]: Epoch 50 / 100: avg data time: 1.39e-01, avg batch time: 0.9658, average train loss: 127.9936
[11/23 22:59:43 visual_prompt]: Inference (val):avg data time: 7.06e-05, avg batch time: 0.3088, average loss: 134.5663
[11/23 22:59:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.63	
[11/23 22:59:43 visual_prompt]: Stopping early.
[11/23 22:59:43 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 22:59:43 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 22:59:43 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/23 22:59:43 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 22:59:43 visual_prompt]: Training with config:
[11/23 22:59:43 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/23 22:59:43 visual_prompt]: Loading training data...
[11/23 22:59:43 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 22:59:43 visual_prompt]: Loading validation data...
[11/23 22:59:43 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 22:59:43 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 22:59:46 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 22:59:46 visual_prompt]: tuned percent:0.525
[11/23 22:59:46 visual_prompt]: Device used for model: 0
[11/23 22:59:46 visual_prompt]: Setting up Evaluator...
[11/23 22:59:46 visual_prompt]: Setting up Trainer...
[11/23 22:59:46 visual_prompt]: 	Setting up the optimizer...
[11/23 22:59:46 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 23:01:26 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8376 s / batch. (data: 7.95e-03). ETA=12:50:33, max mem: 20.9 GB 
[11/23 23:03:01 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8240 s / batch. (data: 2.79e-04). ETA=12:36:40, max mem: 20.9 GB 
[11/23 23:04:40 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8400 s / batch. (data: 3.14e-04). ETA=12:50:00, max mem: 20.9 GB 
[11/23 23:06:15 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8440 s / batch. (data: 7.95e-03). ETA=12:52:15, max mem: 20.9 GB 
[11/23 23:07:53 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8189 s / batch. (data: 2.92e-04). ETA=12:27:54, max mem: 20.9 GB 
[11/23 23:08:44 visual_prompt]: Epoch 1 / 100: avg data time: 1.39e-01, avg batch time: 0.9718, average train loss: 1.5403
[11/23 23:09:38 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3110, average loss: 1.5201
[11/23 23:09:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 23:09:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/23 23:11:18 visual_prompt]: 	Training 100/553. train loss: 32.7968,	0.8680 s / batch. (data: 3.37e-04). ETA=13:10:33, max mem: 20.9 GB 
[11/23 23:12:54 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8461 s / batch. (data: 3.37e-04). ETA=12:49:13, max mem: 20.9 GB 
[11/23 23:14:31 visual_prompt]: 	Training 300/553. train loss: 2.9277,	0.8480 s / batch. (data: 1.53e-02). ETA=12:49:31, max mem: 20.9 GB 
[11/23 23:16:07 visual_prompt]: 	Training 400/553. train loss: 36.1745,	0.8440 s / batch. (data: 3.03e-04). ETA=12:44:28, max mem: 20.9 GB 
[11/23 23:17:45 visual_prompt]: 	Training 500/553. train loss: 13.4565,	0.8320 s / batch. (data: 3.20e-04). ETA=12:32:12, max mem: 20.9 GB 
[11/23 23:18:34 visual_prompt]: Epoch 2 / 100: avg data time: 1.37e-01, avg batch time: 0.9688, average train loss: 22.7402
[11/23 23:19:29 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3088, average loss: 21.8746
[11/23 23:19:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.07	
[11/23 23:19:29 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/23 23:21:08 visual_prompt]: 	Training 100/553. train loss: 16.2583,	0.8356 s / batch. (data: 2.98e-04). ETA=12:33:19, max mem: 20.9 GB 
[11/23 23:22:45 visual_prompt]: 	Training 200/553. train loss: 23.2656,	0.8424 s / batch. (data: 1.03e-02). ETA=12:38:04, max mem: 20.9 GB 
[11/23 23:24:21 visual_prompt]: 	Training 300/553. train loss: 11.3496,	0.8572 s / batch. (data: 8.69e-03). ETA=12:49:56, max mem: 20.9 GB 
[11/23 23:26:00 visual_prompt]: 	Training 400/553. train loss: 189.8524,	0.8321 s / batch. (data: 3.10e-04). ETA=12:26:00, max mem: 20.9 GB 
[11/23 23:27:38 visual_prompt]: 	Training 500/553. train loss: 22.7126,	1.1000 s / batch. (data: 2.67e-01). ETA=16:24:22, max mem: 20.9 GB 
[11/23 23:28:28 visual_prompt]: Epoch 3 / 100: avg data time: 1.44e-01, avg batch time: 0.9751, average train loss: 34.7822
[11/23 23:29:24 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3105, average loss: 19.8792
[11/23 23:29:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.07	
[11/23 23:29:24 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/23 23:31:07 visual_prompt]: 	Training 100/553. train loss: 6.3957,	0.8255 s / batch. (data: 1.05e-02). ETA=12:16:39, max mem: 20.9 GB 
[11/23 23:32:44 visual_prompt]: 	Training 200/553. train loss: 52.6492,	0.8303 s / batch. (data: 3.09e-04). ETA=12:19:34, max mem: 20.9 GB 
[11/23 23:34:22 visual_prompt]: 	Training 300/553. train loss: 10.5361,	1.1512 s / batch. (data: 3.01e-01). ETA=17:03:27, max mem: 20.9 GB 
[11/23 23:35:55 visual_prompt]: 	Training 400/553. train loss: 52.3675,	0.8357 s / batch. (data: 1.05e-02). ETA=12:21:34, max mem: 20.9 GB 
[11/23 23:37:34 visual_prompt]: 	Training 500/553. train loss: 40.5577,	3.3369 s / batch. (data: 2.51e+00). ETA=2 days, 1:15:27, max mem: 20.9 GB 
[11/23 23:38:26 visual_prompt]: Epoch 4 / 100: avg data time: 1.52e-01, avg batch time: 0.9804, average train loss: 63.2746
[11/23 23:39:22 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3105, average loss: 170.5102
[11/23 23:39:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.57	
[11/23 23:39:22 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/23 23:41:02 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8329 s / batch. (data: 2.86e-04). ETA=12:15:32, max mem: 20.9 GB 
[11/23 23:42:39 visual_prompt]: 	Training 200/553. train loss: 3.0704,	1.1760 s / batch. (data: 3.45e-01). ETA=17:16:37, max mem: 20.9 GB 
[11/23 23:44:17 visual_prompt]: 	Training 300/553. train loss: 18.1324,	0.8480 s / batch. (data: 4.86e-04). ETA=12:26:02, max mem: 20.9 GB 
[11/23 23:45:54 visual_prompt]: 	Training 400/553. train loss: 79.8581,	0.8476 s / batch. (data: 3.12e-04). ETA=12:24:19, max mem: 20.9 GB 
[11/23 23:47:32 visual_prompt]: 	Training 500/553. train loss: 23.4526,	0.8394 s / batch. (data: 3.03e-04). ETA=12:15:43, max mem: 20.9 GB 
[11/23 23:48:23 visual_prompt]: Epoch 5 / 100: avg data time: 1.52e-01, avg batch time: 0.9794, average train loss: 79.2135
[11/23 23:49:19 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3106, average loss: 143.7809
[11/23 23:49:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.37	
[11/23 23:49:19 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/23 23:51:02 visual_prompt]: 	Training 100/553. train loss: 6.2837,	0.8231 s / batch. (data: 4.71e-04). ETA=11:59:17, max mem: 20.9 GB 
[11/23 23:52:38 visual_prompt]: 	Training 200/553. train loss: 118.3204,	0.8612 s / batch. (data: 1.56e-02). ETA=12:31:10, max mem: 20.9 GB 
[11/23 23:54:13 visual_prompt]: 	Training 300/553. train loss: 54.3363,	0.8231 s / batch. (data: 3.09e-04). ETA=11:56:36, max mem: 20.9 GB 
[11/23 23:55:54 visual_prompt]: 	Training 400/553. train loss: 99.6455,	0.8213 s / batch. (data: 3.24e-04). ETA=11:53:38, max mem: 20.9 GB 
[11/23 23:57:30 visual_prompt]: 	Training 500/553. train loss: 30.0783,	0.8328 s / batch. (data: 3.50e-04). ETA=12:02:14, max mem: 20.9 GB 
[11/23 23:58:20 visual_prompt]: Epoch 6 / 100: avg data time: 1.49e-01, avg batch time: 0.9784, average train loss: 66.1335
[11/23 23:59:15 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3099, average loss: 99.1012
[11/23 23:59:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.41	
[11/23 23:59:15 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/24 00:00:55 visual_prompt]: 	Training 100/553. train loss: 0.2706,	0.8400 s / batch. (data: 3.08e-04). ETA=12:06:20, max mem: 20.9 GB 
[11/24 00:02:32 visual_prompt]: 	Training 200/553. train loss: 25.0724,	0.8355 s / batch. (data: 1.15e-02). ETA=12:01:05, max mem: 20.9 GB 
[11/24 00:04:13 visual_prompt]: 	Training 300/553. train loss: 10.4053,	1.7130 s / batch. (data: 8.95e-01). ETA=1 day, 0:35:31, max mem: 20.9 GB 
[11/24 00:05:49 visual_prompt]: 	Training 400/553. train loss: 14.4586,	1.7024 s / batch. (data: 8.82e-01). ETA=1 day, 0:23:35, max mem: 20.9 GB 
[11/24 00:07:25 visual_prompt]: 	Training 500/553. train loss: 24.6965,	0.8361 s / batch. (data: 3.05e-04). ETA=11:57:22, max mem: 20.9 GB 
[11/24 00:08:14 visual_prompt]: Epoch 7 / 100: avg data time: 1.47e-01, avg batch time: 0.9743, average train loss: 73.2909
[11/24 00:09:10 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3100, average loss: 76.7275
[11/24 00:09:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.65	
[11/24 00:09:10 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/24 00:10:48 visual_prompt]: 	Training 100/553. train loss: 15.1555,	0.8243 s / batch. (data: 3.10e-04). ETA=11:45:08, max mem: 20.9 GB 
[11/24 00:12:27 visual_prompt]: 	Training 200/553. train loss: 95.7314,	0.9227 s / batch. (data: 1.00e-01). ETA=13:07:51, max mem: 20.9 GB 
[11/24 00:14:05 visual_prompt]: 	Training 300/553. train loss: 156.5140,	0.8240 s / batch. (data: 3.06e-04). ETA=11:42:10, max mem: 20.9 GB 
[11/24 00:15:42 visual_prompt]: 	Training 400/553. train loss: 166.6465,	0.8587 s / batch. (data: 3.08e-02). ETA=12:10:20, max mem: 20.9 GB 
[11/24 00:17:19 visual_prompt]: 	Training 500/553. train loss: 285.2204,	1.2146 s / batch. (data: 3.85e-01). ETA=17:10:57, max mem: 20.9 GB 
[11/24 00:18:11 visual_prompt]: Epoch 8 / 100: avg data time: 1.52e-01, avg batch time: 0.9780, average train loss: 99.5197
[11/24 00:19:06 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3104, average loss: 35.6332
[11/24 00:19:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.89	
[11/24 00:19:06 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/24 00:20:47 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8252 s / batch. (data: 1.07e-02). ETA=11:38:22, max mem: 20.9 GB 
[11/24 00:22:23 visual_prompt]: 	Training 200/553. train loss: 43.8888,	0.8192 s / batch. (data: 3.11e-04). ETA=11:31:53, max mem: 20.9 GB 
[11/24 00:24:01 visual_prompt]: 	Training 300/553. train loss: 137.8746,	1.5866 s / batch. (data: 7.75e-01). ETA=22:17:24, max mem: 20.9 GB 
[11/24 00:25:39 visual_prompt]: 	Training 400/553. train loss: 35.5964,	0.8125 s / batch. (data: 5.41e-03). ETA=11:23:34, max mem: 20.9 GB 
[11/24 00:27:17 visual_prompt]: 	Training 500/553. train loss: 182.5312,	1.0834 s / batch. (data: 2.37e-01). ETA=15:09:35, max mem: 20.9 GB 
[11/24 00:28:07 visual_prompt]: Epoch 9 / 100: avg data time: 1.52e-01, avg batch time: 0.9786, average train loss: 78.0807
[11/24 00:29:03 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3101, average loss: 328.8175
[11/24 00:29:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.86	
[11/24 00:29:03 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/24 00:30:47 visual_prompt]: 	Training 100/553. train loss: 26.8022,	0.8373 s / batch. (data: 3.11e-04). ETA=11:40:49, max mem: 20.9 GB 
[11/24 00:32:22 visual_prompt]: 	Training 200/553. train loss: 121.5240,	0.8113 s / batch. (data: 5.42e-03). ETA=11:17:45, max mem: 20.9 GB 
[11/24 00:33:59 visual_prompt]: 	Training 300/553. train loss: 160.5820,	1.9756 s / batch. (data: 1.17e+00). ETA=1 day, 3:27:05, max mem: 20.9 GB 
[11/24 00:35:34 visual_prompt]: 	Training 400/553. train loss: 168.6008,	0.8160 s / batch. (data: 3.39e-04). ETA=11:18:57, max mem: 20.9 GB 
[11/24 00:37:10 visual_prompt]: 	Training 500/553. train loss: 579.5065,	0.8480 s / batch. (data: 7.95e-03). ETA=11:44:09, max mem: 20.9 GB 
[11/24 00:38:00 visual_prompt]: Epoch 10 / 100: avg data time: 1.45e-01, avg batch time: 0.9713, average train loss: 138.4034
[11/24 00:38:55 visual_prompt]: Inference (val):avg data time: 3.77e-04, avg batch time: 0.3098, average loss: 7.5994
[11/24 00:38:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.49	
[11/24 00:38:55 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/24 00:40:38 visual_prompt]: 	Training 100/553. train loss: 88.7464,	0.8233 s / batch. (data: 1.59e-02). ETA=11:21:30, max mem: 20.9 GB 
[11/24 00:42:16 visual_prompt]: 	Training 200/553. train loss: 85.2204,	0.8268 s / batch. (data: 1.05e-02). ETA=11:23:06, max mem: 20.9 GB 
[11/24 00:43:52 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.9236 s / batch. (data: 1.12e+00). ETA=1 day, 2:26:00, max mem: 20.9 GB 
[11/24 00:45:28 visual_prompt]: 	Training 400/553. train loss: 99.9953,	0.8139 s / batch. (data: 5.42e-03). ETA=11:09:42, max mem: 20.9 GB 
[11/24 00:47:04 visual_prompt]: 	Training 500/553. train loss: 108.4333,	0.8280 s / batch. (data: 3.04e-04). ETA=11:19:54, max mem: 20.9 GB 
[11/24 00:47:54 visual_prompt]: Epoch 11 / 100: avg data time: 1.48e-01, avg batch time: 0.9743, average train loss: 111.4261
[11/24 00:48:49 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3092, average loss: 323.4079
[11/24 00:48:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.19	
[11/24 00:48:49 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/24 00:50:32 visual_prompt]: 	Training 100/553. train loss: 40.6332,	0.8920 s / batch. (data: 6.66e-02). ETA=12:10:12, max mem: 20.9 GB 
[11/24 00:52:10 visual_prompt]: 	Training 200/553. train loss: 21.3151,	0.8656 s / batch. (data: 2.57e-02). ETA=11:47:09, max mem: 20.9 GB 
[11/24 00:53:45 visual_prompt]: 	Training 300/553. train loss: 58.6921,	0.8403 s / batch. (data: 7.96e-03). ETA=11:25:05, max mem: 20.9 GB 
[11/24 00:55:23 visual_prompt]: 	Training 400/553. train loss: 200.1610,	0.8320 s / batch. (data: 3.11e-04). ETA=11:16:56, max mem: 20.9 GB 
[11/24 00:57:00 visual_prompt]: 	Training 500/553. train loss: 466.1448,	0.8400 s / batch. (data: 3.06e-04). ETA=11:22:01, max mem: 20.9 GB 
[11/24 00:57:50 visual_prompt]: Epoch 12 / 100: avg data time: 1.51e-01, avg batch time: 0.9772, average train loss: 115.6275
[11/24 00:58:45 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3101, average loss: 121.8249
[11/24 00:58:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.01	
[11/24 00:58:45 visual_prompt]: Best epoch 12: best metric: -121.825
[11/24 00:58:45 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/24 01:00:28 visual_prompt]: 	Training 100/553. train loss: 104.7513,	0.8360 s / batch. (data: 7.98e-03). ETA=11:16:41, max mem: 20.9 GB 
[11/24 01:02:02 visual_prompt]: 	Training 200/553. train loss: 6.9765,	0.8440 s / batch. (data: 3.13e-04). ETA=11:21:43, max mem: 20.9 GB 
[11/24 01:03:40 visual_prompt]: 	Training 300/553. train loss: 98.9173,	1.6829 s / batch. (data: 8.44e-01). ETA=22:36:31, max mem: 20.9 GB 
[11/24 01:05:16 visual_prompt]: 	Training 400/553. train loss: 208.4926,	0.8240 s / batch. (data: 5.45e-03). ETA=11:02:48, max mem: 20.9 GB 
[11/24 01:06:54 visual_prompt]: 	Training 500/553. train loss: 157.6126,	0.8676 s / batch. (data: 2.48e-04). ETA=11:36:27, max mem: 20.9 GB 
[11/24 01:07:45 visual_prompt]: Epoch 13 / 100: avg data time: 1.49e-01, avg batch time: 0.9753, average train loss: 87.1781
[11/24 01:08:40 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3085, average loss: 192.0014
[11/24 01:08:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.44	
[11/24 01:08:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/24 01:10:22 visual_prompt]: 	Training 100/553. train loss: 31.8856,	0.8596 s / batch. (data: 2.20e-02). ETA=11:27:52, max mem: 20.9 GB 
[11/24 01:11:59 visual_prompt]: 	Training 200/553. train loss: 20.6185,	0.8472 s / batch. (data: 1.48e-02). ETA=11:16:28, max mem: 20.9 GB 
[11/24 01:13:36 visual_prompt]: 	Training 300/553. train loss: 50.9220,	0.8160 s / batch. (data: 3.12e-04). ETA=10:50:14, max mem: 20.9 GB 
[11/24 01:15:13 visual_prompt]: 	Training 400/553. train loss: 113.5434,	0.8352 s / batch. (data: 2.96e-04). ETA=11:04:08, max mem: 20.9 GB 
[11/24 01:16:50 visual_prompt]: 	Training 500/553. train loss: 16.4016,	0.8453 s / batch. (data: 7.95e-03). ETA=11:10:44, max mem: 20.9 GB 
[11/24 01:17:39 visual_prompt]: Epoch 14 / 100: avg data time: 1.48e-01, avg batch time: 0.9749, average train loss: 95.4231
[11/24 01:18:35 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3095, average loss: 35.9889
[11/24 01:18:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.24	
[11/24 01:18:35 visual_prompt]: Best epoch 14: best metric: -35.989
[11/24 01:18:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/24 01:20:16 visual_prompt]: 	Training 100/553. train loss: 77.4675,	0.9576 s / batch. (data: 1.37e-01). ETA=12:37:24, max mem: 20.9 GB 
[11/24 01:21:52 visual_prompt]: 	Training 200/553. train loss: 24.2004,	0.8320 s / batch. (data: 2.93e-04). ETA=10:56:41, max mem: 20.9 GB 
[11/24 01:23:30 visual_prompt]: 	Training 300/553. train loss: 144.2614,	0.8207 s / batch. (data: 8.56e-03). ETA=10:46:22, max mem: 20.9 GB 
[11/24 01:25:05 visual_prompt]: 	Training 400/553. train loss: 97.0569,	1.1560 s / batch. (data: 3.27e-01). ETA=15:08:36, max mem: 20.9 GB 
[11/24 01:26:43 visual_prompt]: 	Training 500/553. train loss: 26.6353,	0.8756 s / batch. (data: 2.76e-02). ETA=11:26:43, max mem: 20.9 GB 
[11/24 01:27:34 visual_prompt]: Epoch 15 / 100: avg data time: 1.48e-01, avg batch time: 0.9761, average train loss: 137.0193
[11/24 01:28:30 visual_prompt]: Inference (val):avg data time: 1.76e-04, avg batch time: 0.3098, average loss: 318.4160
[11/24 01:28:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.17	
[11/24 01:28:30 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/24 01:30:10 visual_prompt]: 	Training 100/553. train loss: 188.9208,	0.8320 s / batch. (data: 3.03e-04). ETA=10:50:25, max mem: 20.9 GB 
[11/24 01:31:47 visual_prompt]: 	Training 200/553. train loss: 44.1214,	0.8148 s / batch. (data: 3.05e-04). ETA=10:35:38, max mem: 20.9 GB 
[11/24 01:33:25 visual_prompt]: 	Training 300/553. train loss: 83.9578,	0.8403 s / batch. (data: 2.06e-02). ETA=10:54:06, max mem: 20.9 GB 
[11/24 01:35:02 visual_prompt]: 	Training 400/553. train loss: 95.3845,	0.8228 s / batch. (data: 5.41e-03). ETA=10:39:04, max mem: 20.9 GB 
[11/24 01:36:39 visual_prompt]: 	Training 500/553. train loss: 26.6261,	1.6370 s / batch. (data: 8.13e-01). ETA=21:08:48, max mem: 20.9 GB 
[11/24 01:37:30 visual_prompt]: Epoch 16 / 100: avg data time: 1.50e-01, avg batch time: 0.9767, average train loss: 90.8547
[11/24 01:38:26 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3103, average loss: 12.5311
[11/24 01:38:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[11/24 01:38:26 visual_prompt]: Best epoch 16: best metric: -12.531
[11/24 01:38:26 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/24 01:40:06 visual_prompt]: 	Training 100/553. train loss: 123.2160,	0.8200 s / batch. (data: 3.33e-04). ETA=10:33:27, max mem: 20.9 GB 
[11/24 01:41:44 visual_prompt]: 	Training 200/553. train loss: 49.1753,	0.8403 s / batch. (data: 5.20e-03). ETA=10:47:44, max mem: 20.9 GB 
[11/24 01:43:20 visual_prompt]: 	Training 300/553. train loss: 124.8512,	0.8760 s / batch. (data: 2.73e-02). ETA=11:13:49, max mem: 20.9 GB 
[11/24 01:44:57 visual_prompt]: 	Training 400/553. train loss: 30.3349,	1.0120 s / batch. (data: 1.61e-01). ETA=12:56:44, max mem: 20.9 GB 
[11/24 01:46:34 visual_prompt]: 	Training 500/553. train loss: 288.6173,	1.6240 s / batch. (data: 8.14e-01). ETA=20:43:46, max mem: 20.9 GB 
[11/24 01:47:26 visual_prompt]: Epoch 17 / 100: avg data time: 1.49e-01, avg batch time: 0.9767, average train loss: 94.9273
[11/24 01:48:21 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3098, average loss: 44.3459
[11/24 01:48:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.74	
[11/24 01:48:21 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/24 01:50:02 visual_prompt]: 	Training 100/553. train loss: 103.0587,	0.8204 s / batch. (data: 8.88e-03). ETA=10:26:14, max mem: 20.9 GB 
[11/24 01:51:42 visual_prompt]: 	Training 200/553. train loss: 12.4207,	0.8680 s / batch. (data: 3.02e-04). ETA=11:01:06, max mem: 20.9 GB 
[11/24 01:53:19 visual_prompt]: 	Training 300/553. train loss: 114.4238,	0.8515 s / batch. (data: 8.18e-03). ETA=10:47:08, max mem: 20.9 GB 
[11/24 01:54:56 visual_prompt]: 	Training 400/553. train loss: 77.0176,	0.8284 s / batch. (data: 1.57e-02). ETA=10:28:12, max mem: 20.9 GB 
[11/24 01:56:31 visual_prompt]: 	Training 500/553. train loss: 48.2409,	0.8320 s / batch. (data: 5.42e-03). ETA=10:29:31, max mem: 20.9 GB 
[11/24 01:57:20 visual_prompt]: Epoch 18 / 100: avg data time: 1.47e-01, avg batch time: 0.9741, average train loss: 95.3656
[11/24 01:58:14 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3089, average loss: 165.1938
[11/24 01:58:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.67	
[11/24 01:58:14 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/24 01:59:55 visual_prompt]: 	Training 100/553. train loss: 37.5920,	0.8359 s / batch. (data: 2.12e-02). ETA=10:30:22, max mem: 20.9 GB 
[11/24 02:01:34 visual_prompt]: 	Training 200/553. train loss: 78.9635,	0.8444 s / batch. (data: 5.86e-03). ETA=10:35:22, max mem: 20.9 GB 
[11/24 02:03:12 visual_prompt]: 	Training 300/553. train loss: 444.9015,	0.8375 s / batch. (data: 3.24e-04). ETA=10:28:44, max mem: 20.9 GB 
[11/24 02:04:51 visual_prompt]: 	Training 400/553. train loss: 80.1770,	0.8430 s / batch. (data: 1.09e-02). ETA=10:31:30, max mem: 20.9 GB 
[11/24 02:06:25 visual_prompt]: 	Training 500/553. train loss: 267.2652,	0.8160 s / batch. (data: 2.92e-04). ETA=10:09:54, max mem: 20.9 GB 
[11/24 02:07:16 visual_prompt]: Epoch 19 / 100: avg data time: 1.53e-01, avg batch time: 0.9791, average train loss: 111.7685
[11/24 02:08:11 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3092, average loss: 261.0260
[11/24 02:08:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.96	
[11/24 02:08:11 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/24 02:09:51 visual_prompt]: 	Training 100/553. train loss: 41.4846,	0.8160 s / batch. (data: 3.10e-04). ETA=10:07:49, max mem: 20.9 GB 
[11/24 02:11:29 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8149 s / batch. (data: 3.13e-04). ETA=10:05:38, max mem: 20.9 GB 
[11/24 02:13:06 visual_prompt]: 	Training 300/553. train loss: 32.8508,	0.8146 s / batch. (data: 3.25e-04). ETA=10:04:02, max mem: 20.9 GB 
[11/24 02:14:42 visual_prompt]: 	Training 400/553. train loss: 91.8494,	0.8263 s / batch. (data: 1.05e-02). ETA=10:11:23, max mem: 20.9 GB 
[11/24 02:16:19 visual_prompt]: 	Training 500/553. train loss: 37.3914,	0.8559 s / batch. (data: 3.08e-04). ETA=10:31:52, max mem: 20.9 GB 
[11/24 02:17:11 visual_prompt]: Epoch 20 / 100: avg data time: 1.50e-01, avg batch time: 0.9760, average train loss: 83.9298
[11/24 02:18:05 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3090, average loss: 56.4203
[11/24 02:18:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.07	
[11/24 02:18:05 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/24 02:19:48 visual_prompt]: 	Training 100/553. train loss: 44.5488,	0.8120 s / batch. (data: 3.19e-04). ETA=9:57:21, max mem: 20.9 GB 
[11/24 02:21:24 visual_prompt]: 	Training 200/553. train loss: 0.0001,	0.8401 s / batch. (data: 2.41e-04). ETA=10:16:35, max mem: 20.9 GB 
[11/24 02:23:00 visual_prompt]: 	Training 300/553. train loss: 287.9181,	0.9748 s / batch. (data: 1.47e-01). ETA=11:53:53, max mem: 20.9 GB 
[11/24 02:24:35 visual_prompt]: 	Training 400/553. train loss: 398.9373,	0.8320 s / batch. (data: 5.41e-03). ETA=10:07:55, max mem: 20.9 GB 
[11/24 02:26:14 visual_prompt]: 	Training 500/553. train loss: 99.6390,	0.8245 s / batch. (data: 3.11e-04). ETA=10:01:01, max mem: 20.9 GB 
[11/24 02:27:03 visual_prompt]: Epoch 21 / 100: avg data time: 1.45e-01, avg batch time: 0.9725, average train loss: 98.6770
[11/24 02:27:58 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3080, average loss: 27.0083
[11/24 02:27:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.06	
[11/24 02:27:58 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/24 02:29:38 visual_prompt]: 	Training 100/553. train loss: 215.8323,	0.8516 s / batch. (data: 1.05e-02). ETA=10:18:37, max mem: 20.9 GB 
[11/24 02:31:15 visual_prompt]: 	Training 200/553. train loss: 27.9844,	0.8463 s / batch. (data: 5.42e-03). ETA=10:13:22, max mem: 20.9 GB 
[11/24 02:32:49 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8014 s / batch. (data: 3.05e-04). ETA=9:39:28, max mem: 20.9 GB 
[11/24 02:34:28 visual_prompt]: 	Training 400/553. train loss: 2.6714,	0.8532 s / batch. (data: 2.57e-02). ETA=10:15:34, max mem: 20.9 GB 
[11/24 02:36:05 visual_prompt]: 	Training 500/553. train loss: 100.9906,	0.8103 s / batch. (data: 3.03e-04). ETA=9:43:13, max mem: 20.9 GB 
[11/24 02:36:56 visual_prompt]: Epoch 22 / 100: avg data time: 1.45e-01, avg batch time: 0.9717, average train loss: 81.1880
[11/24 02:37:51 visual_prompt]: Inference (val):avg data time: 4.03e-04, avg batch time: 0.3094, average loss: 61.1178
[11/24 02:37:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.34	
[11/24 02:37:51 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/24 02:39:33 visual_prompt]: 	Training 100/553. train loss: 140.7388,	0.9760 s / batch. (data: 1.40e-01). ETA=11:40:01, max mem: 20.9 GB 
[11/24 02:41:11 visual_prompt]: 	Training 200/553. train loss: 43.6858,	0.8442 s / batch. (data: 5.43e-03). ETA=10:04:04, max mem: 20.9 GB 
[11/24 02:42:49 visual_prompt]: 	Training 300/553. train loss: 9.4103,	0.8437 s / batch. (data: 3.03e-04). ETA=10:02:19, max mem: 20.9 GB 
[11/24 02:44:25 visual_prompt]: 	Training 400/553. train loss: 2.1598,	0.8480 s / batch. (data: 8.18e-04). ETA=10:03:58, max mem: 20.9 GB 
[11/24 02:45:59 visual_prompt]: 	Training 500/553. train loss: 317.9267,	0.8230 s / batch. (data: 5.40e-03). ETA=9:44:48, max mem: 20.9 GB 
[11/24 02:46:50 visual_prompt]: Epoch 23 / 100: avg data time: 1.48e-01, avg batch time: 0.9747, average train loss: 82.0441
[11/24 02:47:45 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3096, average loss: 39.6244
[11/24 02:47:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.19	
[11/24 02:47:45 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/24 02:49:24 visual_prompt]: 	Training 100/553. train loss: 20.0959,	0.8610 s / batch. (data: 3.72e-02). ETA=10:09:37, max mem: 20.9 GB 
[11/24 02:51:00 visual_prompt]: 	Training 200/553. train loss: 130.3906,	0.8440 s / batch. (data: 1.20e-02). ETA=9:56:08, max mem: 20.9 GB 
[11/24 02:52:38 visual_prompt]: 	Training 300/553. train loss: 8.9930,	0.8640 s / batch. (data: 3.15e-02). ETA=10:08:52, max mem: 20.9 GB 
[11/24 02:54:15 visual_prompt]: 	Training 400/553. train loss: 39.7635,	0.8260 s / batch. (data: 2.25e-04). ETA=9:40:39, max mem: 20.9 GB 
[11/24 02:55:53 visual_prompt]: 	Training 500/553. train loss: 108.8881,	0.8414 s / batch. (data: 5.47e-03). ETA=9:50:05, max mem: 20.9 GB 
[11/24 02:56:44 visual_prompt]: Epoch 24 / 100: avg data time: 1.47e-01, avg batch time: 0.9741, average train loss: 81.1871
[11/24 02:57:39 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3106, average loss: 28.5256
[11/24 02:57:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 64.54	
[11/24 02:57:39 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/24 02:59:23 visual_prompt]: 	Training 100/553. train loss: 91.4839,	0.8424 s / batch. (data: 7.93e-03). ETA=9:48:40, max mem: 20.9 GB 
[11/24 03:00:57 visual_prompt]: 	Training 200/553. train loss: 77.1709,	0.8350 s / batch. (data: 3.78e-04). ETA=9:42:07, max mem: 20.9 GB 
[11/24 03:02:33 visual_prompt]: 	Training 300/553. train loss: 43.7938,	0.8240 s / batch. (data: 7.47e-03). ETA=9:33:03, max mem: 20.9 GB 
[11/24 03:04:09 visual_prompt]: 	Training 400/553. train loss: 102.3129,	1.1560 s / batch. (data: 3.12e-01). ETA=13:22:02, max mem: 20.9 GB 
[11/24 03:05:46 visual_prompt]: 	Training 500/553. train loss: 86.9998,	1.3636 s / batch. (data: 5.46e-01). ETA=15:43:47, max mem: 20.9 GB 
[11/24 03:06:37 visual_prompt]: Epoch 25 / 100: avg data time: 1.45e-01, avg batch time: 0.9716, average train loss: 77.0688
[11/24 03:07:32 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3130, average loss: 23.5282
[11/24 03:07:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 67.86	
[11/24 03:07:32 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/24 03:09:12 visual_prompt]: 	Training 100/553. train loss: 27.9279,	0.8307 s / batch. (data: 5.39e-03). ETA=9:32:52, max mem: 20.9 GB 
[11/24 03:10:50 visual_prompt]: 	Training 200/553. train loss: 114.5106,	1.6120 s / batch. (data: 7.76e-01). ETA=18:28:54, max mem: 20.9 GB 
[11/24 03:12:27 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8439 s / batch. (data: 7.85e-04). ETA=9:39:07, max mem: 20.9 GB 
[11/24 03:14:02 visual_prompt]: 	Training 400/553. train loss: 5.7127,	0.8360 s / batch. (data: 2.93e-04). ETA=9:32:18, max mem: 20.9 GB 
[11/24 03:15:37 visual_prompt]: 	Training 500/553. train loss: 82.9827,	0.8466 s / batch. (data: 2.26e-02). ETA=9:38:10, max mem: 20.9 GB 
[11/24 03:16:27 visual_prompt]: Epoch 26 / 100: avg data time: 1.41e-01, avg batch time: 0.9679, average train loss: 83.6908
[11/24 03:17:22 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3099, average loss: 96.7490
[11/24 03:17:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.30	
[11/24 03:17:22 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/24 03:19:03 visual_prompt]: 	Training 100/553. train loss: 25.1682,	0.8239 s / batch. (data: 3.14e-04). ETA=9:20:34, max mem: 20.9 GB 
[11/24 03:20:40 visual_prompt]: 	Training 200/553. train loss: 37.1963,	0.9392 s / batch. (data: 1.13e-01). ETA=10:37:28, max mem: 20.9 GB 
[11/24 03:22:17 visual_prompt]: 	Training 300/553. train loss: 144.5471,	0.8572 s / batch. (data: 2.13e-02). ETA=9:40:21, max mem: 20.9 GB 
[11/24 03:23:55 visual_prompt]: 	Training 400/553. train loss: 49.3821,	0.8585 s / batch. (data: 5.93e-03). ETA=9:39:46, max mem: 20.9 GB 
[11/24 03:25:33 visual_prompt]: 	Training 500/553. train loss: 55.3535,	0.8386 s / batch. (data: 3.29e-04). ETA=9:24:59, max mem: 20.9 GB 
[11/24 03:26:22 visual_prompt]: Epoch 27 / 100: avg data time: 1.51e-01, avg batch time: 0.9765, average train loss: 103.2925
[11/24 03:27:17 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3103, average loss: 34.6598
[11/24 03:27:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 69.30	
[11/24 03:27:17 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/24 03:28:57 visual_prompt]: 	Training 100/553. train loss: 13.8401,	0.8520 s / batch. (data: 1.60e-02). ETA=9:31:48, max mem: 20.9 GB 
[11/24 03:30:34 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8622 s / batch. (data: 3.10e-04). ETA=9:37:13, max mem: 20.9 GB 
[11/24 03:32:12 visual_prompt]: 	Training 300/553. train loss: 211.2960,	1.4037 s / batch. (data: 5.88e-01). ETA=15:37:24, max mem: 20.9 GB 
[11/24 03:33:48 visual_prompt]: 	Training 400/553. train loss: 51.1757,	0.8140 s / batch. (data: 3.02e-04). ETA=9:02:16, max mem: 20.9 GB 
[11/24 03:35:23 visual_prompt]: 	Training 500/553. train loss: 0.0495,	0.8358 s / batch. (data: 5.42e-03). ETA=9:15:22, max mem: 20.9 GB 
[11/24 03:36:15 visual_prompt]: Epoch 28 / 100: avg data time: 1.46e-01, avg batch time: 0.9719, average train loss: 84.6978
[11/24 03:37:10 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3106, average loss: 36.3148
[11/24 03:37:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 68.10	
[11/24 03:37:10 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/24 03:38:57 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8073 s / batch. (data: 2.50e-04). ETA=8:54:21, max mem: 20.9 GB 
[11/24 03:40:32 visual_prompt]: 	Training 200/553. train loss: 3.2430,	1.7440 s / batch. (data: 9.13e-01). ETA=19:11:30, max mem: 20.9 GB 
[11/24 03:42:07 visual_prompt]: 	Training 300/553. train loss: 189.5812,	0.8320 s / batch. (data: 8.95e-04). ETA=9:07:58, max mem: 20.9 GB 
[11/24 03:43:40 visual_prompt]: 	Training 400/553. train loss: 149.5882,	1.4085 s / batch. (data: 5.66e-01). ETA=15:25:18, max mem: 20.9 GB 
[11/24 03:45:17 visual_prompt]: 	Training 500/553. train loss: 33.2032,	0.8486 s / batch. (data: 1.56e-02). ETA=9:16:03, max mem: 20.9 GB 
[11/24 03:46:08 visual_prompt]: Epoch 29 / 100: avg data time: 1.45e-01, avg batch time: 0.9720, average train loss: 90.5373
[11/24 03:47:02 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3113, average loss: 19.8268
[11/24 03:47:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 67.25	
[11/24 03:47:02 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/24 03:48:41 visual_prompt]: 	Training 100/553. train loss: 47.6176,	0.8345 s / batch. (data: 2.98e-04). ETA=9:04:43, max mem: 20.9 GB 
[11/24 03:50:19 visual_prompt]: 	Training 200/553. train loss: 65.9663,	0.8195 s / batch. (data: 3.24e-04). ETA=8:53:32, max mem: 20.9 GB 
[11/24 03:51:55 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8160 s / batch. (data: 3.16e-04). ETA=8:49:53, max mem: 20.9 GB 
[11/24 03:53:34 visual_prompt]: 	Training 400/553. train loss: 375.7125,	0.8480 s / batch. (data: 3.99e-04). ETA=9:09:15, max mem: 20.9 GB 
[11/24 03:55:09 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.4760 s / batch. (data: 6.55e-01). ETA=15:53:34, max mem: 20.9 GB 
[11/24 03:56:01 visual_prompt]: Epoch 30 / 100: avg data time: 1.46e-01, avg batch time: 0.9733, average train loss: 86.8718
[11/24 03:56:56 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3085, average loss: 26.1372
[11/24 03:56:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 68.86	
[11/24 03:56:56 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[11/24 03:58:38 visual_prompt]: 	Training 100/553. train loss: 79.1386,	0.8080 s / batch. (data: 3.07e-04). ETA=8:39:56, max mem: 20.9 GB 
[11/24 04:00:17 visual_prompt]: 	Training 200/553. train loss: 163.1584,	0.8160 s / batch. (data: 5.42e-03). ETA=8:43:44, max mem: 20.9 GB 
[11/24 04:01:52 visual_prompt]: 	Training 300/553. train loss: 41.7450,	0.8280 s / batch. (data: 3.73e-04). ETA=8:50:03, max mem: 20.9 GB 
[11/24 04:03:28 visual_prompt]: 	Training 400/553. train loss: 0.0001,	1.0828 s / batch. (data: 2.50e-01). ETA=11:31:23, max mem: 20.9 GB 
[11/24 04:05:04 visual_prompt]: 	Training 500/553. train loss: 55.1165,	0.8457 s / batch. (data: 1.05e-02). ETA=8:58:32, max mem: 20.9 GB 
[11/24 04:05:54 visual_prompt]: Epoch 31 / 100: avg data time: 1.46e-01, avg batch time: 0.9722, average train loss: 85.4424
[11/24 04:06:49 visual_prompt]: Inference (val):avg data time: 2.22e-04, avg batch time: 0.3108, average loss: 91.3646
[11/24 04:06:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.24	
[11/24 04:06:49 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[11/24 04:08:31 visual_prompt]: 	Training 100/553. train loss: 45.1156,	0.8760 s / batch. (data: 7.85e-04). ETA=9:15:36, max mem: 20.9 GB 
[11/24 04:10:07 visual_prompt]: 	Training 200/553. train loss: 67.7762,	0.8225 s / batch. (data: 7.14e-04). ETA=8:40:17, max mem: 20.9 GB 
[11/24 04:11:47 visual_prompt]: 	Training 300/553. train loss: 129.0591,	0.8166 s / batch. (data: 3.13e-04). ETA=8:35:15, max mem: 20.9 GB 
[11/24 04:13:24 visual_prompt]: 	Training 400/553. train loss: 68.5516,	0.8600 s / batch. (data: 7.96e-03). ETA=9:01:09, max mem: 20.9 GB 
[11/24 04:14:58 visual_prompt]: 	Training 500/553. train loss: 40.9556,	0.8360 s / batch. (data: 3.04e-04). ETA=8:44:41, max mem: 20.9 GB 
[11/24 04:15:47 visual_prompt]: Epoch 32 / 100: avg data time: 1.47e-01, avg batch time: 0.9730, average train loss: 70.2997
[11/24 04:16:42 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3103, average loss: 17.1912
[11/24 04:16:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 70.16	
[11/24 04:16:42 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[11/24 04:18:22 visual_prompt]: 	Training 100/553. train loss: 31.4517,	1.0013 s / batch. (data: 1.84e-01). ETA=10:25:51, max mem: 20.9 GB 
[11/24 04:20:00 visual_prompt]: 	Training 200/553. train loss: 305.1340,	1.4540 s / batch. (data: 6.22e-01). ETA=15:06:26, max mem: 20.9 GB 
[11/24 04:21:36 visual_prompt]: 	Training 300/553. train loss: 6.9267,	0.8360 s / batch. (data: 1.20e-02). ETA=8:39:46, max mem: 20.9 GB 
[11/24 04:23:13 visual_prompt]: 	Training 400/553. train loss: 30.0530,	0.8266 s / batch. (data: 5.41e-03). ETA=8:32:31, max mem: 20.9 GB 
[11/24 04:24:49 visual_prompt]: 	Training 500/553. train loss: 43.6602,	0.8400 s / batch. (data: 3.11e-04). ETA=8:39:26, max mem: 20.9 GB 
[11/24 04:25:39 visual_prompt]: Epoch 33 / 100: avg data time: 1.43e-01, avg batch time: 0.9695, average train loss: 92.9202
[11/24 04:26:34 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3120, average loss: 27.9399
[11/24 04:26:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 67.38	
[11/24 04:26:34 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[11/24 04:28:15 visual_prompt]: 	Training 100/553. train loss: 25.6993,	0.8640 s / batch. (data: 4.61e-02). ETA=8:52:04, max mem: 20.9 GB 
[11/24 04:29:51 visual_prompt]: 	Training 200/553. train loss: 53.0974,	0.8200 s / batch. (data: 3.11e-04). ETA=8:23:36, max mem: 20.9 GB 
[11/24 04:31:26 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8222 s / batch. (data: 3.40e-04). ETA=8:23:35, max mem: 20.9 GB 
[11/24 04:33:03 visual_prompt]: 	Training 400/553. train loss: 113.9986,	0.8238 s / batch. (data: 2.83e-04). ETA=8:23:12, max mem: 20.9 GB 
[11/24 04:34:40 visual_prompt]: 	Training 500/553. train loss: 12.1594,	1.3765 s / batch. (data: 5.64e-01). ETA=13:58:31, max mem: 20.9 GB 
[11/24 04:35:30 visual_prompt]: Epoch 34 / 100: avg data time: 1.42e-01, avg batch time: 0.9692, average train loss: 65.4234
[11/24 04:36:25 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3107, average loss: 40.9116
[11/24 04:36:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 67.02	
[11/24 04:36:25 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[11/24 04:38:07 visual_prompt]: 	Training 100/553. train loss: 262.2329,	0.8200 s / batch. (data: 3.42e-04). ETA=8:17:26, max mem: 20.9 GB 
[11/24 04:39:45 visual_prompt]: 	Training 200/553. train loss: 113.3687,	0.8222 s / batch. (data: 5.43e-03). ETA=8:17:25, max mem: 20.9 GB 
[11/24 04:41:20 visual_prompt]: 	Training 300/553. train loss: 57.4261,	0.8104 s / batch. (data: 3.05e-04). ETA=8:08:54, max mem: 20.9 GB 
[11/24 04:42:55 visual_prompt]: 	Training 400/553. train loss: 136.0901,	0.8480 s / batch. (data: 1.22e-02). ETA=8:30:11, max mem: 20.9 GB 
[11/24 04:44:33 visual_prompt]: 	Training 500/553. train loss: 66.5506,	1.1291 s / batch. (data: 2.92e-01). ETA=11:17:26, max mem: 20.9 GB 
[11/24 04:45:24 visual_prompt]: Epoch 35 / 100: avg data time: 1.47e-01, avg batch time: 0.9736, average train loss: 97.3900
[11/24 04:46:19 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3109, average loss: 19.0127
[11/24 04:46:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 66.08	
[11/24 04:46:19 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[11/24 04:47:59 visual_prompt]: 	Training 100/553. train loss: 235.7105,	0.8200 s / batch. (data: 7.95e-03). ETA=8:09:53, max mem: 20.9 GB 
[11/24 04:49:36 visual_prompt]: 	Training 200/553. train loss: 331.8435,	0.8336 s / batch. (data: 2.90e-04). ETA=8:16:37, max mem: 20.9 GB 
[11/24 04:51:15 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8228 s / batch. (data: 7.94e-03). ETA=8:08:48, max mem: 20.9 GB 
[11/24 04:52:51 visual_prompt]: 	Training 400/553. train loss: 48.5739,	0.8320 s / batch. (data: 3.08e-04). ETA=8:12:52, max mem: 20.9 GB 
[11/24 04:54:28 visual_prompt]: 	Training 500/553. train loss: 54.7954,	1.1241 s / batch. (data: 3.20e-01). ETA=11:04:05, max mem: 20.9 GB 
[11/24 04:55:16 visual_prompt]: Epoch 36 / 100: avg data time: 1.45e-01, avg batch time: 0.9715, average train loss: 85.6177
[11/24 04:56:12 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3093, average loss: 24.8498
[11/24 04:56:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 66.00	
[11/24 04:56:12 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[11/24 04:57:53 visual_prompt]: 	Training 100/553. train loss: 132.9640,	0.8501 s / batch. (data: 2.61e-02). ETA=8:20:01, max mem: 20.9 GB 
[11/24 04:59:28 visual_prompt]: 	Training 200/553. train loss: 27.2551,	0.8520 s / batch. (data: 3.22e-04). ETA=8:19:43, max mem: 20.9 GB 
[11/24 05:01:06 visual_prompt]: 	Training 300/553. train loss: 226.2066,	1.2284 s / batch. (data: 3.87e-01). ETA=11:58:26, max mem: 20.9 GB 
[11/24 05:02:44 visual_prompt]: 	Training 400/553. train loss: 116.5245,	1.6559 s / batch. (data: 8.31e-01). ETA=16:05:44, max mem: 20.9 GB 
[11/24 05:04:17 visual_prompt]: 	Training 500/553. train loss: 65.7784,	0.8280 s / batch. (data: 7.96e-03). ETA=8:01:30, max mem: 20.9 GB 
[11/24 05:05:09 visual_prompt]: Epoch 37 / 100: avg data time: 1.45e-01, avg batch time: 0.9716, average train loss: 57.7109
[11/24 05:06:04 visual_prompt]: Inference (val):avg data time: 1.71e-04, avg batch time: 0.3095, average loss: 64.5826
[11/24 05:06:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 70.45	
[11/24 05:06:04 visual_prompt]: Stopping early.
[11/24 05:06:04 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 05:06:04 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 05:06:04 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/24 05:06:04 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 05:06:04 visual_prompt]: Training with config:
[11/24 05:06:04 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/24 05:06:04 visual_prompt]: Loading training data...
[11/24 05:06:04 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 05:06:04 visual_prompt]: Loading validation data...
[11/24 05:06:04 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 05:06:04 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 05:06:07 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 05:06:07 visual_prompt]: tuned percent:0.525
[11/24 05:06:07 visual_prompt]: Device used for model: 0
[11/24 05:06:07 visual_prompt]: Setting up Evaluator...
[11/24 05:06:07 visual_prompt]: Setting up Trainer...
[11/24 05:06:07 visual_prompt]: 	Setting up the optimizer...
[11/24 05:06:07 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 05:07:47 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8376 s / batch. (data: 1.42e-02). ETA=12:50:35, max mem: 20.9 GB 
[11/24 05:09:23 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8200 s / batch. (data: 2.98e-04). ETA=12:33:01, max mem: 20.9 GB 
[11/24 05:11:02 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.8332 s / batch. (data: 1.00e+00). ETA=1 day, 4:00:23, max mem: 20.9 GB 
[11/24 05:12:37 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8562 s / batch. (data: 1.21e-02). ETA=13:03:24, max mem: 20.9 GB 
[11/24 05:14:16 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8341 s / batch. (data: 1.38e-02). ETA=12:41:50, max mem: 20.9 GB 
[11/24 05:15:08 visual_prompt]: Epoch 1 / 100: avg data time: 1.44e-01, avg batch time: 0.9773, average train loss: 1.5403
[11/24 05:16:03 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3108, average loss: 1.5201
[11/24 05:16:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 05:16:03 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/24 05:17:43 visual_prompt]: 	Training 100/553. train loss: 12.5197,	0.8686 s / batch. (data: 2.95e-04). ETA=13:11:03, max mem: 20.9 GB 
[11/24 05:19:20 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8320 s / batch. (data: 3.52e-04). ETA=12:36:22, max mem: 20.9 GB 
[11/24 05:20:58 visual_prompt]: 	Training 300/553. train loss: 11.5898,	0.9584 s / batch. (data: 1.16e-01). ETA=14:29:39, max mem: 20.9 GB 
[11/24 05:22:35 visual_prompt]: 	Training 400/553. train loss: 19.1761,	0.8450 s / batch. (data: 5.41e-03). ETA=12:45:24, max mem: 20.9 GB 
[11/24 05:24:13 visual_prompt]: 	Training 500/553. train loss: 3.7540,	0.8440 s / batch. (data: 3.11e-04). ETA=12:43:03, max mem: 20.9 GB 
[11/24 05:25:03 visual_prompt]: Epoch 2 / 100: avg data time: 1.42e-01, avg batch time: 0.9755, average train loss: 8.2145
[11/24 05:25:58 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3105, average loss: 5.2982
[11/24 05:25:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.54	
[11/24 05:25:58 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/24 05:27:38 visual_prompt]: 	Training 100/553. train loss: 43.9622,	0.8287 s / batch. (data: 3.18e-04). ETA=12:27:05, max mem: 20.9 GB 
[11/24 05:29:16 visual_prompt]: 	Training 200/553. train loss: 15.4211,	1.8882 s / batch. (data: 1.07e+00). ETA=1 day, 4:19:09, max mem: 20.9 GB 
[11/24 05:30:51 visual_prompt]: 	Training 300/553. train loss: 16.5867,	0.8414 s / batch. (data: 3.03e-04). ETA=12:35:48, max mem: 20.9 GB 
[11/24 05:32:28 visual_prompt]: 	Training 400/553. train loss: 14.2569,	0.8602 s / batch. (data: 1.22e-02). ETA=12:51:13, max mem: 20.9 GB 
[11/24 05:34:06 visual_prompt]: 	Training 500/553. train loss: 12.0256,	1.0881 s / batch. (data: 2.25e-01). ETA=16:13:43, max mem: 20.9 GB 
[11/24 05:34:55 visual_prompt]: Epoch 3 / 100: avg data time: 1.38e-01, avg batch time: 0.9709, average train loss: 19.3897
[11/24 05:35:50 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.3106, average loss: 39.4502
[11/24 05:35:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.94	
[11/24 05:35:50 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/24 05:37:31 visual_prompt]: 	Training 100/553. train loss: 53.0640,	0.8360 s / batch. (data: 3.14e-04). ETA=12:26:01, max mem: 20.9 GB 
[11/24 05:39:08 visual_prompt]: 	Training 200/553. train loss: 50.8620,	0.8200 s / batch. (data: 3.12e-04). ETA=12:10:23, max mem: 20.9 GB 
[11/24 05:40:46 visual_prompt]: 	Training 300/553. train loss: 58.1711,	1.2530 s / batch. (data: 4.48e-01). ETA=18:33:54, max mem: 20.9 GB 
[11/24 05:42:18 visual_prompt]: 	Training 400/553. train loss: 24.9885,	1.5240 s / batch. (data: 6.88e-01). ETA=22:32:20, max mem: 20.9 GB 
[11/24 05:43:57 visual_prompt]: 	Training 500/553. train loss: 0.0108,	3.3765 s / batch. (data: 2.55e+00). ETA=2 days, 1:50:28, max mem: 20.9 GB 
[11/24 05:44:49 visual_prompt]: Epoch 4 / 100: avg data time: 1.44e-01, avg batch time: 0.9751, average train loss: 24.8907
[11/24 05:45:45 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3098, average loss: 20.4354
[11/24 05:45:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.85	
[11/24 05:45:45 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/24 05:47:24 visual_prompt]: 	Training 100/553. train loss: 32.3181,	0.8238 s / batch. (data: 7.95e-03). ETA=12:07:31, max mem: 20.9 GB 
[11/24 05:49:01 visual_prompt]: 	Training 200/553. train loss: 23.3870,	1.2166 s / batch. (data: 3.69e-01). ETA=17:52:22, max mem: 20.9 GB 
[11/24 05:50:39 visual_prompt]: 	Training 300/553. train loss: 34.9429,	0.8534 s / batch. (data: 7.96e-03). ETA=12:30:47, max mem: 20.9 GB 
[11/24 05:52:16 visual_prompt]: 	Training 400/553. train loss: 5.3576,	0.8396 s / batch. (data: 7.96e-03). ETA=12:17:15, max mem: 20.9 GB 
[11/24 05:53:53 visual_prompt]: 	Training 500/553. train loss: 38.4655,	0.8400 s / batch. (data: 3.10e-04). ETA=12:16:13, max mem: 20.9 GB 
[11/24 05:54:45 visual_prompt]: Epoch 5 / 100: avg data time: 1.46e-01, avg batch time: 0.9768, average train loss: 35.4545
[11/24 05:55:40 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3109, average loss: 52.2161
[11/24 05:55:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.09	
[11/24 05:55:40 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/24 05:57:23 visual_prompt]: 	Training 100/553. train loss: 22.3957,	0.8411 s / batch. (data: 3.07e-04). ETA=12:15:04, max mem: 20.9 GB 
[11/24 05:58:58 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8246 s / batch. (data: 4.57e-04). ETA=11:59:13, max mem: 20.9 GB 
[11/24 06:00:34 visual_prompt]: 	Training 300/553. train loss: 100.9606,	0.8320 s / batch. (data: 5.41e-03). ETA=12:04:19, max mem: 20.9 GB 
[11/24 06:02:15 visual_prompt]: 	Training 400/553. train loss: 36.3876,	0.8240 s / batch. (data: 3.78e-04). ETA=11:55:58, max mem: 20.9 GB 
[11/24 06:03:50 visual_prompt]: 	Training 500/553. train loss: 7.5757,	0.8475 s / batch. (data: 3.41e-04). ETA=12:14:58, max mem: 20.9 GB 
[11/24 06:04:40 visual_prompt]: Epoch 6 / 100: avg data time: 1.46e-01, avg batch time: 0.9759, average train loss: 46.4969
[11/24 06:05:35 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3103, average loss: 2.9002
[11/24 06:05:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 42.29	
[11/24 06:05:35 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/24 06:07:14 visual_prompt]: 	Training 100/553. train loss: 79.7656,	0.8560 s / batch. (data: 3.59e-04). ETA=12:20:10, max mem: 20.9 GB 
[11/24 06:08:51 visual_prompt]: 	Training 200/553. train loss: 23.5082,	0.8090 s / batch. (data: 3.13e-04). ETA=11:38:11, max mem: 20.9 GB 
[11/24 06:10:31 visual_prompt]: 	Training 300/553. train loss: 89.3683,	1.5586 s / batch. (data: 7.26e-01). ETA=22:22:31, max mem: 20.9 GB 
[11/24 06:12:08 visual_prompt]: 	Training 400/553. train loss: 9.0612,	1.9200 s / batch. (data: 1.07e+00). ETA=1 day, 3:30:36, max mem: 20.9 GB 
[11/24 06:13:44 visual_prompt]: 	Training 500/553. train loss: 2.2699,	0.8589 s / batch. (data: 3.06e-04). ETA=12:16:59, max mem: 20.9 GB 
[11/24 06:14:34 visual_prompt]: Epoch 7 / 100: avg data time: 1.44e-01, avg batch time: 0.9738, average train loss: 53.5550
[11/24 06:15:29 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3102, average loss: 85.8453
[11/24 06:15:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.41	
[11/24 06:15:29 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/24 06:17:09 visual_prompt]: 	Training 100/553. train loss: 54.0753,	0.8480 s / batch. (data: 5.45e-03). ETA=12:05:26, max mem: 20.9 GB 
[11/24 06:18:47 visual_prompt]: 	Training 200/553. train loss: 8.1587,	1.1515 s / batch. (data: 3.07e-01). ETA=16:23:10, max mem: 20.9 GB 
[11/24 06:20:24 visual_prompt]: 	Training 300/553. train loss: 10.8678,	0.8234 s / batch. (data: 2.59e-04). ETA=11:41:39, max mem: 20.9 GB 
[11/24 06:22:02 visual_prompt]: 	Training 400/553. train loss: 60.3140,	0.8227 s / batch. (data: 3.24e-04). ETA=11:39:39, max mem: 20.9 GB 
[11/24 06:23:39 visual_prompt]: 	Training 500/553. train loss: 200.6376,	1.1982 s / batch. (data: 3.57e-01). ETA=16:57:04, max mem: 20.9 GB 
[11/24 06:24:30 visual_prompt]: Epoch 8 / 100: avg data time: 1.50e-01, avg batch time: 0.9778, average train loss: 58.2043
[11/24 06:25:25 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3098, average loss: 165.8387
[11/24 06:25:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.33	
[11/24 06:25:25 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/24 06:27:06 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8160 s / batch. (data: 3.02e-04). ETA=11:30:33, max mem: 20.9 GB 
[11/24 06:28:42 visual_prompt]: 	Training 200/553. train loss: 10.1231,	0.8296 s / batch. (data: 5.43e-03). ETA=11:40:42, max mem: 20.9 GB 
[11/24 06:30:19 visual_prompt]: 	Training 300/553. train loss: 65.8969,	1.3880 s / batch. (data: 5.68e-01). ETA=19:29:58, max mem: 20.9 GB 
[11/24 06:31:58 visual_prompt]: 	Training 400/553. train loss: 66.5996,	0.8410 s / batch. (data: 1.05e-02). ETA=11:47:30, max mem: 20.9 GB 
[11/24 06:33:35 visual_prompt]: 	Training 500/553. train loss: 21.8308,	0.8334 s / batch. (data: 2.39e-02). ETA=11:39:45, max mem: 20.9 GB 
[11/24 06:34:25 visual_prompt]: Epoch 9 / 100: avg data time: 1.47e-01, avg batch time: 0.9748, average train loss: 71.2444
[11/24 06:35:20 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3104, average loss: 110.4002
[11/24 06:35:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.95	
[11/24 06:35:20 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/24 06:37:03 visual_prompt]: 	Training 100/553. train loss: 133.3066,	0.8160 s / batch. (data: 7.97e-03). ETA=11:22:59, max mem: 20.9 GB 
[11/24 06:38:39 visual_prompt]: 	Training 200/553. train loss: 84.7284,	0.8456 s / batch. (data: 2.06e-02). ETA=11:46:24, max mem: 20.9 GB 
[11/24 06:40:15 visual_prompt]: 	Training 300/553. train loss: 576.9364,	1.3360 s / batch. (data: 5.09e-01). ETA=18:33:50, max mem: 20.9 GB 
[11/24 06:41:49 visual_prompt]: 	Training 400/553. train loss: 51.9027,	0.8378 s / batch. (data: 3.40e-04). ETA=11:37:05, max mem: 20.9 GB 
[11/24 06:43:28 visual_prompt]: 	Training 500/553. train loss: 23.9710,	0.8371 s / batch. (data: 3.11e-04). ETA=11:35:07, max mem: 20.9 GB 
[11/24 06:44:18 visual_prompt]: Epoch 10 / 100: avg data time: 1.45e-01, avg batch time: 0.9733, average train loss: 87.3272
[11/24 06:45:14 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3098, average loss: 112.9292
[11/24 06:45:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.07	
[11/24 06:45:14 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/24 06:46:57 visual_prompt]: 	Training 100/553. train loss: 114.9638,	0.8781 s / batch. (data: 2.60e-02). ETA=12:06:54, max mem: 20.9 GB 
[11/24 06:48:35 visual_prompt]: 	Training 200/553. train loss: 88.0898,	0.8399 s / batch. (data: 3.51e-04). ETA=11:33:55, max mem: 20.9 GB 
[11/24 06:50:10 visual_prompt]: 	Training 300/553. train loss: 14.5853,	1.8793 s / batch. (data: 1.06e+00). ETA=1 day, 1:49:29, max mem: 20.9 GB 
[11/24 06:51:45 visual_prompt]: 	Training 400/553. train loss: 141.8358,	0.8181 s / batch. (data: 3.12e-04). ETA=11:13:08, max mem: 20.9 GB 
[11/24 06:53:20 visual_prompt]: 	Training 500/553. train loss: 142.3403,	0.8581 s / batch. (data: 1.00e-02). ETA=11:44:36, max mem: 20.9 GB 
[11/24 06:54:10 visual_prompt]: Epoch 11 / 100: avg data time: 1.41e-01, avg batch time: 0.9698, average train loss: 88.3990
[11/24 06:55:06 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3101, average loss: 112.8124
[11/24 06:55:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.28	
[11/24 06:55:06 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/24 06:56:48 visual_prompt]: 	Training 100/553. train loss: 19.0796,	0.8312 s / batch. (data: 3.29e-04). ETA=11:20:23, max mem: 20.9 GB 
[11/24 06:58:26 visual_prompt]: 	Training 200/553. train loss: 83.0412,	0.8410 s / batch. (data: 1.55e-02). ETA=11:27:05, max mem: 20.9 GB 
[11/24 07:00:01 visual_prompt]: 	Training 300/553. train loss: 119.7655,	0.8320 s / batch. (data: 3.04e-04). ETA=11:18:19, max mem: 20.9 GB 
[11/24 07:01:38 visual_prompt]: 	Training 400/553. train loss: 22.7943,	0.8178 s / batch. (data: 3.25e-04). ETA=11:05:24, max mem: 20.9 GB 
[11/24 07:03:15 visual_prompt]: 	Training 500/553. train loss: 38.2310,	0.8307 s / batch. (data: 7.84e-04). ETA=11:14:31, max mem: 20.9 GB 
[11/24 07:04:05 visual_prompt]: Epoch 12 / 100: avg data time: 1.47e-01, avg batch time: 0.9749, average train loss: 85.2541
[11/24 07:05:00 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3093, average loss: 152.8936
[11/24 07:05:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.11	
[11/24 07:05:00 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/24 07:06:42 visual_prompt]: 	Training 100/553. train loss: 23.9103,	0.8564 s / batch. (data: 5.43e-03). ETA=11:33:10, max mem: 20.9 GB 
[11/24 07:08:17 visual_prompt]: 	Training 200/553. train loss: 49.2319,	0.8600 s / batch. (data: 3.20e-04). ETA=11:34:39, max mem: 20.9 GB 
[11/24 07:09:54 visual_prompt]: 	Training 300/553. train loss: 28.9799,	1.6680 s / batch. (data: 8.44e-01). ETA=22:24:30, max mem: 20.9 GB 
[11/24 07:11:30 visual_prompt]: 	Training 400/553. train loss: 96.5915,	0.8374 s / batch. (data: 3.06e-04). ETA=11:13:37, max mem: 20.9 GB 
[11/24 07:13:08 visual_prompt]: 	Training 500/553. train loss: 92.0966,	0.8245 s / batch. (data: 2.90e-04). ETA=11:01:50, max mem: 20.9 GB 
[11/24 07:13:58 visual_prompt]: Epoch 13 / 100: avg data time: 1.45e-01, avg batch time: 0.9729, average train loss: 92.7903
[11/24 07:14:54 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3113, average loss: 21.4387
[11/24 07:14:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.53	
[11/24 07:14:54 visual_prompt]: Best epoch 13: best metric: -21.439
[11/24 07:14:54 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/24 07:16:36 visual_prompt]: 	Training 100/553. train loss: 13.0888,	0.8400 s / batch. (data: 7.94e-03). ETA=11:12:08, max mem: 20.9 GB 
[11/24 07:18:12 visual_prompt]: 	Training 200/553. train loss: 48.8425,	0.9236 s / batch. (data: 1.05e-01). ETA=12:17:32, max mem: 20.9 GB 
[11/24 07:19:49 visual_prompt]: 	Training 300/553. train loss: 38.0073,	0.8360 s / batch. (data: 3.15e-04). ETA=11:06:10, max mem: 20.9 GB 
[11/24 07:21:25 visual_prompt]: 	Training 400/553. train loss: 27.0583,	0.8216 s / batch. (data: 3.28e-04). ETA=10:53:18, max mem: 20.9 GB 
[11/24 07:23:03 visual_prompt]: 	Training 500/553. train loss: 136.6361,	0.8301 s / batch. (data: 2.83e-04). ETA=10:58:40, max mem: 20.9 GB 
[11/24 07:23:52 visual_prompt]: Epoch 14 / 100: avg data time: 1.45e-01, avg batch time: 0.9729, average train loss: 85.7605
[11/24 07:24:47 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3109, average loss: 75.6512
[11/24 07:24:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.40	
[11/24 07:24:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/24 07:26:28 visual_prompt]: 	Training 100/553. train loss: 33.4414,	0.8261 s / batch. (data: 3.04e-04). ETA=10:53:27, max mem: 20.9 GB 
[11/24 07:28:04 visual_prompt]: 	Training 200/553. train loss: 190.2530,	0.8404 s / batch. (data: 3.14e-04). ETA=11:03:17, max mem: 20.9 GB 
[11/24 07:29:43 visual_prompt]: 	Training 300/553. train loss: 39.7126,	0.8272 s / batch. (data: 8.36e-04). ETA=10:51:34, max mem: 20.9 GB 
[11/24 07:31:17 visual_prompt]: 	Training 400/553. train loss: 36.3495,	0.8400 s / batch. (data: 3.07e-04). ETA=11:00:12, max mem: 20.9 GB 
[11/24 07:32:55 visual_prompt]: 	Training 500/553. train loss: 174.3483,	0.8317 s / batch. (data: 2.98e-04). ETA=10:52:18, max mem: 20.9 GB 
[11/24 07:33:46 visual_prompt]: Epoch 15 / 100: avg data time: 1.47e-01, avg batch time: 0.9745, average train loss: 81.3469
[11/24 07:34:42 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3102, average loss: 42.3722
[11/24 07:34:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.24	
[11/24 07:34:42 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/24 07:36:21 visual_prompt]: 	Training 100/553. train loss: 59.0136,	0.8680 s / batch. (data: 1.20e-02). ETA=11:18:33, max mem: 20.9 GB 
[11/24 07:37:57 visual_prompt]: 	Training 200/553. train loss: 124.4367,	0.8212 s / batch. (data: 2.98e-04). ETA=10:40:38, max mem: 20.9 GB 
[11/24 07:39:35 visual_prompt]: 	Training 300/553. train loss: 10.7524,	0.8360 s / batch. (data: 2.91e-04). ETA=10:50:45, max mem: 20.9 GB 
[11/24 07:41:12 visual_prompt]: 	Training 400/553. train loss: 123.1823,	0.8254 s / batch. (data: 7.92e-04). ETA=10:41:06, max mem: 20.9 GB 
[11/24 07:42:48 visual_prompt]: 	Training 500/553. train loss: 230.5610,	1.1564 s / batch. (data: 3.38e-01). ETA=14:56:20, max mem: 20.9 GB 
[11/24 07:43:40 visual_prompt]: Epoch 16 / 100: avg data time: 1.45e-01, avg batch time: 0.9730, average train loss: 92.4808
[11/24 07:44:35 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3094, average loss: 6.6274
[11/24 07:44:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.14	
[11/24 07:44:35 visual_prompt]: Best epoch 16: best metric: -6.627
[11/24 07:44:35 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/24 07:46:16 visual_prompt]: 	Training 100/553. train loss: 2.6529,	0.8611 s / batch. (data: 2.82e-04). ETA=11:05:11, max mem: 20.9 GB 
[11/24 07:47:54 visual_prompt]: 	Training 200/553. train loss: 152.5670,	0.8209 s / batch. (data: 3.28e-04). ETA=10:32:49, max mem: 20.9 GB 
[11/24 07:49:31 visual_prompt]: 	Training 300/553. train loss: 253.1240,	0.8460 s / batch. (data: 5.41e-03). ETA=10:50:46, max mem: 20.9 GB 
[11/24 07:51:07 visual_prompt]: 	Training 400/553. train loss: 36.0315,	1.1520 s / batch. (data: 3.12e-01). ETA=14:44:10, max mem: 20.9 GB 
[11/24 07:52:43 visual_prompt]: 	Training 500/553. train loss: 14.2437,	1.5555 s / batch. (data: 7.38e-01). ETA=19:51:19, max mem: 20.9 GB 
[11/24 07:53:35 visual_prompt]: Epoch 17 / 100: avg data time: 1.47e-01, avg batch time: 0.9751, average train loss: 97.3880
[11/24 07:54:30 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3090, average loss: 34.5270
[11/24 07:54:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 37.29	
[11/24 07:54:30 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/24 07:56:11 visual_prompt]: 	Training 100/553. train loss: 21.5394,	0.8237 s / batch. (data: 7.94e-03). ETA=10:28:46, max mem: 20.9 GB 
[11/24 07:57:50 visual_prompt]: 	Training 200/553. train loss: 102.2191,	0.8576 s / batch. (data: 7.90e-04). ETA=10:53:12, max mem: 20.9 GB 
[11/24 07:59:27 visual_prompt]: 	Training 300/553. train loss: 108.1322,	0.8069 s / batch. (data: 2.98e-04). ETA=10:13:15, max mem: 20.9 GB 
[11/24 08:01:04 visual_prompt]: 	Training 400/553. train loss: 52.3275,	0.8320 s / batch. (data: 2.79e-04). ETA=10:30:56, max mem: 20.9 GB 
[11/24 08:02:39 visual_prompt]: 	Training 500/553. train loss: 47.7289,	0.8953 s / batch. (data: 6.32e-02). ETA=11:17:27, max mem: 20.9 GB 
[11/24 08:03:28 visual_prompt]: Epoch 18 / 100: avg data time: 1.46e-01, avg batch time: 0.9737, average train loss: 86.3528
[11/24 08:04:23 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3093, average loss: 2.3279
[11/24 08:04:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 40.65	rocauc: 37.02	
[11/24 08:04:23 visual_prompt]: Best epoch 18: best metric: -2.328
[11/24 08:04:23 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/24 08:06:03 visual_prompt]: 	Training 100/553. train loss: 69.7756,	1.1422 s / batch. (data: 2.99e-01). ETA=14:21:20, max mem: 20.9 GB 
[11/24 08:07:41 visual_prompt]: 	Training 200/553. train loss: 22.1144,	0.8176 s / batch. (data: 3.02e-04). ETA=10:15:11, max mem: 20.9 GB 
[11/24 08:09:18 visual_prompt]: 	Training 300/553. train loss: 408.8005,	0.8640 s / batch. (data: 3.31e-04). ETA=10:48:39, max mem: 20.9 GB 
[11/24 08:10:55 visual_prompt]: 	Training 400/553. train loss: 72.7405,	0.8441 s / batch. (data: 1.14e-03). ETA=10:32:19, max mem: 20.9 GB 
[11/24 08:12:28 visual_prompt]: 	Training 500/553. train loss: 109.6554,	0.8200 s / batch. (data: 2.99e-04). ETA=10:12:53, max mem: 20.9 GB 
[11/24 08:13:19 visual_prompt]: Epoch 19 / 100: avg data time: 1.40e-01, avg batch time: 0.9685, average train loss: 89.7050
[11/24 08:14:14 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3114, average loss: 100.0859
[11/24 08:14:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.88	
[11/24 08:14:14 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/24 08:15:53 visual_prompt]: 	Training 100/553. train loss: 79.4471,	0.8440 s / batch. (data: 3.24e-04). ETA=10:28:40, max mem: 20.9 GB 
[11/24 08:17:30 visual_prompt]: 	Training 200/553. train loss: 157.0300,	0.8349 s / batch. (data: 3.24e-04). ETA=10:20:30, max mem: 20.9 GB 
[11/24 08:19:07 visual_prompt]: 	Training 300/553. train loss: 8.1106,	0.8198 s / batch. (data: 3.73e-04). ETA=10:07:54, max mem: 20.9 GB 
[11/24 08:20:44 visual_prompt]: 	Training 400/553. train loss: 2.5973,	0.8760 s / batch. (data: 1.19e-02). ETA=10:48:07, max mem: 20.9 GB 
[11/24 08:22:20 visual_prompt]: 	Training 500/553. train loss: 169.9998,	0.8213 s / batch. (data: 1.05e-02). ETA=10:06:16, max mem: 20.9 GB 
[11/24 08:23:11 visual_prompt]: Epoch 20 / 100: avg data time: 1.44e-01, avg batch time: 0.9718, average train loss: 91.2562
[11/24 08:24:07 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3095, average loss: 90.9191
[11/24 08:24:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.28	
[11/24 08:24:07 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/24 08:25:50 visual_prompt]: 	Training 100/553. train loss: 4.2993,	0.8400 s / batch. (data: 3.23e-04). ETA=10:17:58, max mem: 20.9 GB 
[11/24 08:27:26 visual_prompt]: 	Training 200/553. train loss: 36.2410,	0.8204 s / batch. (data: 3.05e-04). ETA=10:02:12, max mem: 20.9 GB 
[11/24 08:29:02 visual_prompt]: 	Training 300/553. train loss: 135.9170,	0.8144 s / batch. (data: 3.28e-04). ETA=9:56:25, max mem: 20.9 GB 
[11/24 08:30:38 visual_prompt]: 	Training 400/553. train loss: 9.0030,	0.8359 s / batch. (data: 2.90e-04). ETA=10:10:46, max mem: 20.9 GB 
[11/24 08:32:17 visual_prompt]: 	Training 500/553. train loss: 113.5523,	0.8349 s / batch. (data: 9.27e-03). ETA=10:08:36, max mem: 20.9 GB 
[11/24 08:33:06 visual_prompt]: Epoch 21 / 100: avg data time: 1.48e-01, avg batch time: 0.9754, average train loss: 95.3262
[11/24 08:34:02 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3095, average loss: 4.7616
[11/24 08:34:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 52.42	
[11/24 08:34:02 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/24 08:35:42 visual_prompt]: 	Training 100/553. train loss: 50.1048,	0.8127 s / batch. (data: 2.40e-04). ETA=9:50:23, max mem: 20.9 GB 
[11/24 08:37:19 visual_prompt]: 	Training 200/553. train loss: 62.6233,	0.8446 s / batch. (data: 3.00e-04). ETA=10:12:07, max mem: 20.9 GB 
[11/24 08:38:53 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8280 s / batch. (data: 5.42e-03). ETA=9:58:43, max mem: 20.9 GB 
[11/24 08:40:31 visual_prompt]: 	Training 400/553. train loss: 54.8575,	0.8526 s / batch. (data: 7.95e-03). ETA=10:15:05, max mem: 20.9 GB 
[11/24 08:42:08 visual_prompt]: 	Training 500/553. train loss: 48.8533,	0.8189 s / batch. (data: 3.08e-04). ETA=9:49:24, max mem: 20.9 GB 
[11/24 08:42:59 visual_prompt]: Epoch 22 / 100: avg data time: 1.43e-01, avg batch time: 0.9714, average train loss: 75.0675
[11/24 08:43:54 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3097, average loss: 47.6330
[11/24 08:43:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.47	
[11/24 08:43:54 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/24 08:45:36 visual_prompt]: 	Training 100/553. train loss: 15.8041,	0.8267 s / batch. (data: 3.28e-04). ETA=9:52:54, max mem: 20.9 GB 
[11/24 08:47:13 visual_prompt]: 	Training 200/553. train loss: 60.4298,	0.8189 s / batch. (data: 3.02e-04). ETA=9:45:59, max mem: 20.9 GB 
[11/24 08:48:50 visual_prompt]: 	Training 300/553. train loss: 9.5525,	0.8284 s / batch. (data: 3.04e-04). ETA=9:51:23, max mem: 20.9 GB 
[11/24 08:50:24 visual_prompt]: 	Training 400/553. train loss: 76.4443,	0.8643 s / batch. (data: 8.51e-04). ETA=10:15:33, max mem: 20.9 GB 
[11/24 08:52:00 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8436 s / batch. (data: 2.05e-02). ETA=9:59:27, max mem: 20.9 GB 
[11/24 08:52:50 visual_prompt]: Epoch 23 / 100: avg data time: 1.42e-01, avg batch time: 0.9687, average train loss: 81.9495
[11/24 08:53:45 visual_prompt]: Inference (val):avg data time: 2.20e-04, avg batch time: 0.3103, average loss: 119.5339
[11/24 08:53:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.64	
[11/24 08:53:45 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/24 08:55:23 visual_prompt]: 	Training 100/553. train loss: 16.3050,	0.8173 s / batch. (data: 3.19e-04). ETA=9:38:40, max mem: 20.9 GB 
[11/24 08:56:59 visual_prompt]: 	Training 200/553. train loss: 61.7671,	0.8160 s / batch. (data: 2.96e-04). ETA=9:36:23, max mem: 20.9 GB 
[11/24 08:58:35 visual_prompt]: 	Training 300/553. train loss: 61.0474,	0.8760 s / batch. (data: 2.85e-02). ETA=10:17:18, max mem: 20.9 GB 
[11/24 09:00:12 visual_prompt]: 	Training 400/553. train loss: 97.8063,	0.8236 s / batch. (data: 2.82e-04). ETA=9:39:00, max mem: 20.9 GB 
[11/24 09:01:49 visual_prompt]: 	Training 500/553. train loss: 311.2027,	0.8255 s / batch. (data: 1.05e-02). ETA=9:38:57, max mem: 20.9 GB 
[11/24 09:02:40 visual_prompt]: Epoch 24 / 100: avg data time: 1.40e-01, avg batch time: 0.9671, average train loss: 82.1351
[11/24 09:03:35 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3109, average loss: 112.9347
[11/24 09:03:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.77	
[11/24 09:03:35 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/24 09:05:18 visual_prompt]: 	Training 100/553. train loss: 107.3279,	0.8401 s / batch. (data: 7.96e-03). ETA=9:47:02, max mem: 20.9 GB 
[11/24 09:06:51 visual_prompt]: 	Training 200/553. train loss: 86.5670,	0.8371 s / batch. (data: 3.43e-04). ETA=9:43:33, max mem: 20.9 GB 
[11/24 09:08:27 visual_prompt]: 	Training 300/553. train loss: 123.3767,	0.8480 s / batch. (data: 3.66e-04). ETA=9:49:45, max mem: 20.9 GB 
[11/24 09:10:03 visual_prompt]: 	Training 400/553. train loss: 207.6805,	1.1960 s / batch. (data: 3.60e-01). ETA=13:49:46, max mem: 20.9 GB 
[11/24 09:11:40 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.3360 s / batch. (data: 5.19e-01). ETA=15:24:39, max mem: 20.9 GB 
[11/24 09:12:31 visual_prompt]: Epoch 25 / 100: avg data time: 1.40e-01, avg batch time: 0.9692, average train loss: 88.6766
[11/24 09:13:26 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3104, average loss: 65.0240
[11/24 09:13:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.04	
[11/24 09:13:26 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/24 09:15:07 visual_prompt]: 	Training 100/553. train loss: 30.0034,	0.8295 s / batch. (data: 3.19e-04). ETA=9:32:00, max mem: 20.9 GB 
[11/24 09:16:45 visual_prompt]: 	Training 200/553. train loss: 67.1940,	1.6600 s / batch. (data: 8.45e-01). ETA=19:01:57, max mem: 20.9 GB 
[11/24 09:18:23 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8220 s / batch. (data: 3.23e-04). ETA=9:24:05, max mem: 20.9 GB 
[11/24 09:19:58 visual_prompt]: 	Training 400/553. train loss: 92.3177,	0.8160 s / batch. (data: 3.00e-04). ETA=9:18:37, max mem: 20.9 GB 
[11/24 09:21:34 visual_prompt]: 	Training 500/553. train loss: 38.7327,	0.8200 s / batch. (data: 3.11e-04). ETA=9:19:59, max mem: 20.9 GB 
[11/24 09:22:25 visual_prompt]: Epoch 26 / 100: avg data time: 1.45e-01, avg batch time: 0.9735, average train loss: 69.7019
[11/24 09:23:20 visual_prompt]: Inference (val):avg data time: 2.03e-04, avg batch time: 0.3113, average loss: 14.4951
[11/24 09:23:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.32	
[11/24 09:23:20 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[11/24 09:25:02 visual_prompt]: 	Training 100/553. train loss: 219.0955,	0.8274 s / batch. (data: 5.39e-03). ETA=9:22:54, max mem: 20.9 GB 
[11/24 09:26:38 visual_prompt]: 	Training 200/553. train loss: 145.9541,	0.8440 s / batch. (data: 7.95e-03). ETA=9:32:50, max mem: 20.9 GB 
[11/24 09:28:15 visual_prompt]: 	Training 300/553. train loss: 24.4080,	0.8360 s / batch. (data: 3.02e-04). ETA=9:25:59, max mem: 20.9 GB 
[11/24 09:29:52 visual_prompt]: 	Training 400/553. train loss: 44.4365,	0.8349 s / batch. (data: 1.10e-02). ETA=9:23:50, max mem: 20.9 GB 
[11/24 09:31:29 visual_prompt]: 	Training 500/553. train loss: 161.3302,	0.8360 s / batch. (data: 8.18e-04). ETA=9:23:11, max mem: 20.9 GB 
[11/24 09:32:18 visual_prompt]: Epoch 27 / 100: avg data time: 1.44e-01, avg batch time: 0.9726, average train loss: 74.8556
[11/24 09:33:13 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3109, average loss: 34.8650
[11/24 09:33:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.19	
[11/24 09:33:13 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[11/24 09:34:52 visual_prompt]: 	Training 100/553. train loss: 86.5541,	0.8375 s / batch. (data: 1.05e-02). ETA=9:22:03, max mem: 20.9 GB 
[11/24 09:36:29 visual_prompt]: 	Training 200/553. train loss: 22.8274,	0.8327 s / batch. (data: 2.99e-04). ETA=9:17:27, max mem: 20.9 GB 
[11/24 09:38:06 visual_prompt]: 	Training 300/553. train loss: 59.2656,	1.3952 s / batch. (data: 5.63e-01). ETA=15:31:45, max mem: 20.9 GB 
[11/24 09:39:42 visual_prompt]: 	Training 400/553. train loss: 150.4455,	0.8228 s / batch. (data: 4.32e-04). ETA=9:08:05, max mem: 20.9 GB 
[11/24 09:41:18 visual_prompt]: 	Training 500/553. train loss: 147.9897,	0.8312 s / batch. (data: 3.30e-04). ETA=9:12:18, max mem: 20.9 GB 
[11/24 09:42:08 visual_prompt]: Epoch 28 / 100: avg data time: 1.39e-01, avg batch time: 0.9666, average train loss: 69.9625
[11/24 09:43:03 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3112, average loss: 70.9363
[11/24 09:43:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.18	
[11/24 09:43:03 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[11/24 09:44:50 visual_prompt]: 	Training 100/553. train loss: 76.6778,	0.8680 s / batch. (data: 2.92e-04). ETA=9:34:33, max mem: 20.9 GB 
[11/24 09:46:26 visual_prompt]: 	Training 200/553. train loss: 29.0995,	1.6255 s / batch. (data: 7.98e-01). ETA=17:53:16, max mem: 20.9 GB 
[11/24 09:48:00 visual_prompt]: 	Training 300/553. train loss: 55.0209,	0.8240 s / batch. (data: 2.91e-04). ETA=9:02:41, max mem: 20.9 GB 
[11/24 09:49:34 visual_prompt]: 	Training 400/553. train loss: 17.1176,	0.8736 s / batch. (data: 4.07e-02). ETA=9:33:51, max mem: 20.9 GB 
[11/24 09:51:11 visual_prompt]: 	Training 500/553. train loss: 68.7080,	0.8548 s / batch. (data: 1.56e-02). ETA=9:20:05, max mem: 20.9 GB 
[11/24 09:52:02 visual_prompt]: Epoch 29 / 100: avg data time: 1.45e-01, avg batch time: 0.9742, average train loss: 77.5595
[11/24 09:52:57 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3110, average loss: 139.9693
[11/24 09:52:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.88	
[11/24 09:52:57 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[11/24 09:54:37 visual_prompt]: 	Training 100/553. train loss: 95.7120,	0.8400 s / batch. (data: 2.87e-04). ETA=9:08:16, max mem: 20.9 GB 
[11/24 09:56:14 visual_prompt]: 	Training 200/553. train loss: 103.8336,	0.8145 s / batch. (data: 3.81e-04). ETA=8:50:17, max mem: 20.9 GB 
[11/24 09:57:49 visual_prompt]: 	Training 300/553. train loss: 66.8459,	0.8240 s / batch. (data: 3.09e-04). ETA=8:55:05, max mem: 20.9 GB 
[11/24 09:59:27 visual_prompt]: 	Training 400/553. train loss: 17.2390,	1.0920 s / batch. (data: 2.52e-01). ETA=11:47:18, max mem: 20.9 GB 
[11/24 10:01:02 visual_prompt]: 	Training 500/553. train loss: 25.4649,	1.2762 s / batch. (data: 4.59e-01). ETA=13:44:30, max mem: 20.9 GB 
[11/24 10:01:55 visual_prompt]: Epoch 30 / 100: avg data time: 1.43e-01, avg batch time: 0.9715, average train loss: 77.3183
[11/24 10:02:50 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3103, average loss: 59.4566
[11/24 10:02:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.43	
[11/24 10:02:50 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[11/24 10:04:32 visual_prompt]: 	Training 100/553. train loss: 11.5690,	0.8276 s / batch. (data: 1.20e-02). ETA=8:52:35, max mem: 20.9 GB 
[11/24 10:06:11 visual_prompt]: 	Training 200/553. train loss: 33.0901,	0.8279 s / batch. (data: 3.24e-04). ETA=8:51:20, max mem: 20.9 GB 
[11/24 10:07:45 visual_prompt]: 	Training 300/553. train loss: 22.6195,	0.8514 s / batch. (data: 1.15e-02). ETA=9:05:03, max mem: 20.9 GB 
[11/24 10:09:20 visual_prompt]: 	Training 400/553. train loss: 42.0631,	1.4342 s / batch. (data: 5.99e-01). ETA=15:15:45, max mem: 20.9 GB 
[11/24 10:10:56 visual_prompt]: 	Training 500/553. train loss: 14.5550,	0.8277 s / batch. (data: 2.82e-04). ETA=8:47:05, max mem: 20.9 GB 
[11/24 10:11:46 visual_prompt]: Epoch 31 / 100: avg data time: 1.42e-01, avg batch time: 0.9703, average train loss: 73.1653
[11/24 10:12:42 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3098, average loss: 39.5140
[11/24 10:12:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.60	
[11/24 10:12:42 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[11/24 10:14:24 visual_prompt]: 	Training 100/553. train loss: 94.3087,	0.8575 s / batch. (data: 1.05e-02). ETA=9:03:54, max mem: 20.9 GB 
[11/24 10:16:00 visual_prompt]: 	Training 200/553. train loss: 99.7104,	0.8522 s / batch. (data: 3.22e-02). ETA=8:59:08, max mem: 20.9 GB 
[11/24 10:17:39 visual_prompt]: 	Training 300/553. train loss: 35.2079,	1.0040 s / batch. (data: 1.61e-01). ETA=10:33:28, max mem: 20.9 GB 
[11/24 10:19:17 visual_prompt]: 	Training 400/553. train loss: 178.5225,	0.8368 s / batch. (data: 3.03e-04). ETA=8:46:34, max mem: 20.9 GB 
[11/24 10:20:52 visual_prompt]: 	Training 500/553. train loss: 31.5324,	0.8320 s / batch. (data: 3.04e-04). ETA=8:42:10, max mem: 20.9 GB 
[11/24 10:21:41 visual_prompt]: Epoch 32 / 100: avg data time: 1.46e-01, avg batch time: 0.9743, average train loss: 80.8004
[11/24 10:22:36 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3110, average loss: 143.6310
[11/24 10:22:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.18	
[11/24 10:22:36 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[11/24 10:24:15 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8185 s / batch. (data: 3.30e-04). ETA=8:31:37, max mem: 20.9 GB 
[11/24 10:25:54 visual_prompt]: 	Training 200/553. train loss: 96.3196,	1.1403 s / batch. (data: 2.91e-01). ETA=11:50:51, max mem: 20.9 GB 
[11/24 10:27:30 visual_prompt]: 	Training 300/553. train loss: 72.6527,	0.8280 s / batch. (data: 3.05e-04). ETA=8:34:47, max mem: 20.9 GB 
[11/24 10:29:07 visual_prompt]: 	Training 400/553. train loss: 83.9825,	0.8560 s / batch. (data: 1.20e-02). ETA=8:50:46, max mem: 20.9 GB 
[11/24 10:30:43 visual_prompt]: 	Training 500/553. train loss: 125.3319,	0.8257 s / batch. (data: 3.12e-04). ETA=8:30:36, max mem: 20.9 GB 
[11/24 10:31:33 visual_prompt]: Epoch 33 / 100: avg data time: 1.43e-01, avg batch time: 0.9709, average train loss: 79.4853
[11/24 10:32:28 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3098, average loss: 10.7333
[11/24 10:32:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.54	
[11/24 10:32:28 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[11/24 10:34:08 visual_prompt]: 	Training 100/553. train loss: 30.8330,	0.8789 s / batch. (data: 4.43e-02). ETA=9:01:16, max mem: 20.9 GB 
[11/24 10:35:43 visual_prompt]: 	Training 200/553. train loss: 10.0184,	0.8190 s / batch. (data: 3.24e-04). ETA=8:23:00, max mem: 20.9 GB 
[11/24 10:37:19 visual_prompt]: 	Training 300/553. train loss: 218.5066,	0.8370 s / batch. (data: 3.25e-04). ETA=8:32:41, max mem: 20.9 GB 
[11/24 10:38:57 visual_prompt]: 	Training 400/553. train loss: 36.7482,	0.8202 s / batch. (data: 2.83e-04). ETA=8:21:00, max mem: 20.9 GB 
[11/24 10:40:34 visual_prompt]: 	Training 500/553. train loss: 12.9240,	1.3680 s / batch. (data: 5.36e-01). ETA=13:53:21, max mem: 20.9 GB 
[11/24 10:41:24 visual_prompt]: Epoch 34 / 100: avg data time: 1.41e-01, avg batch time: 0.9691, average train loss: 77.0760
[11/24 10:42:19 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3111, average loss: 128.6205
[11/24 10:42:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.07	
[11/24 10:42:19 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[11/24 10:44:02 visual_prompt]: 	Training 100/553. train loss: 85.9022,	0.8365 s / batch. (data: 5.42e-03). ETA=8:27:27, max mem: 20.9 GB 
[11/24 10:45:40 visual_prompt]: 	Training 200/553. train loss: 1.6506,	0.8400 s / batch. (data: 3.15e-04). ETA=8:28:08, max mem: 20.9 GB 
[11/24 10:47:15 visual_prompt]: 	Training 300/553. train loss: 9.8607,	0.8360 s / batch. (data: 3.45e-04). ETA=8:24:20, max mem: 20.9 GB 
[11/24 10:48:50 visual_prompt]: 	Training 400/553. train loss: 9.2928,	0.8574 s / batch. (data: 1.52e-02). ETA=8:35:51, max mem: 20.9 GB 
[11/24 10:50:27 visual_prompt]: 	Training 500/553. train loss: 38.0204,	0.9739 s / batch. (data: 1.58e-01). ETA=9:44:17, max mem: 20.9 GB 
[11/24 10:51:18 visual_prompt]: Epoch 35 / 100: avg data time: 1.45e-01, avg batch time: 0.9736, average train loss: 76.9145
[11/24 10:52:13 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3088, average loss: 226.2540
[11/24 10:52:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.64	
[11/24 10:52:13 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[11/24 10:53:53 visual_prompt]: 	Training 100/553. train loss: 204.2154,	0.8280 s / batch. (data: 3.40e-04). ETA=8:14:38, max mem: 20.9 GB 
[11/24 10:55:30 visual_prompt]: 	Training 200/553. train loss: 130.6249,	0.8280 s / batch. (data: 3.37e-04). ETA=8:13:16, max mem: 20.9 GB 
[11/24 10:57:08 visual_prompt]: 	Training 300/553. train loss: 73.9381,	0.8372 s / batch. (data: 1.05e-02). ETA=8:17:21, max mem: 20.9 GB 
[11/24 10:58:44 visual_prompt]: 	Training 400/553. train loss: 78.8268,	0.8417 s / batch. (data: 2.40e-02). ETA=8:18:36, max mem: 20.9 GB 
[11/24 11:00:21 visual_prompt]: 	Training 500/553. train loss: 132.2474,	1.1958 s / batch. (data: 3.64e-01). ETA=11:46:24, max mem: 20.9 GB 
[11/24 11:01:09 visual_prompt]: Epoch 36 / 100: avg data time: 1.42e-01, avg batch time: 0.9693, average train loss: 84.0513
[11/24 11:02:05 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3100, average loss: 107.7796
[11/24 11:02:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.74	
[11/24 11:02:05 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[11/24 11:03:45 visual_prompt]: 	Training 100/553. train loss: 194.6217,	0.8356 s / batch. (data: 3.26e-04). ETA=8:11:31, max mem: 20.9 GB 
[11/24 11:05:20 visual_prompt]: 	Training 200/553. train loss: 76.6887,	0.8320 s / batch. (data: 3.09e-04). ETA=8:07:59, max mem: 20.9 GB 
[11/24 11:06:57 visual_prompt]: 	Training 300/553. train loss: 47.6810,	1.1606 s / batch. (data: 3.30e-01). ETA=11:18:49, max mem: 20.9 GB 
[11/24 11:08:35 visual_prompt]: 	Training 400/553. train loss: 23.0807,	1.4307 s / batch. (data: 6.26e-01). ETA=13:54:22, max mem: 20.9 GB 
[11/24 11:10:07 visual_prompt]: 	Training 500/553. train loss: 54.7606,	0.8320 s / batch. (data: 7.96e-03). ETA=8:03:50, max mem: 20.9 GB 
[11/24 11:10:59 visual_prompt]: Epoch 37 / 100: avg data time: 1.37e-01, avg batch time: 0.9661, average train loss: 63.5501
[11/24 11:11:54 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3106, average loss: 28.5747
[11/24 11:11:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.05	
[11/24 11:11:54 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[11/24 11:13:32 visual_prompt]: 	Training 100/553. train loss: 72.7354,	0.8160 s / batch. (data: 3.23e-04). ETA=7:52:26, max mem: 20.9 GB 
[11/24 11:15:09 visual_prompt]: 	Training 200/553. train loss: 29.6928,	0.8535 s / batch. (data: 5.43e-03). ETA=8:12:45, max mem: 20.9 GB 
[11/24 11:16:46 visual_prompt]: 	Training 300/553. train loss: 144.9714,	0.8412 s / batch. (data: 1.05e-02). ETA=8:04:12, max mem: 20.9 GB 
[11/24 11:18:20 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8199 s / batch. (data: 4.54e-04). ETA=7:50:37, max mem: 20.9 GB 
[11/24 11:19:58 visual_prompt]: 	Training 500/553. train loss: 5.5101,	0.8400 s / batch. (data: 2.96e-04). ETA=8:00:44, max mem: 20.9 GB 
[11/24 11:20:47 visual_prompt]: Epoch 38 / 100: avg data time: 1.36e-01, avg batch time: 0.9649, average train loss: 68.1992
[11/24 11:21:42 visual_prompt]: Inference (val):avg data time: 2.77e-04, avg batch time: 0.3105, average loss: 51.7917
[11/24 11:21:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.04	
[11/24 11:21:42 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[11/24 11:23:20 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8310 s / batch. (data: 6.91e-03). ETA=7:53:28, max mem: 20.9 GB 
[11/24 11:25:03 visual_prompt]: 	Training 200/553. train loss: 141.9730,	0.8333 s / batch. (data: 7.23e-04). ETA=7:53:23, max mem: 20.9 GB 
[11/24 11:26:50 visual_prompt]: 	Training 300/553. train loss: 44.2764,	0.8355 s / batch. (data: 4.28e-04). ETA=7:53:15, max mem: 20.9 GB 
[11/24 11:28:32 visual_prompt]: 	Training 400/553. train loss: 30.7900,	0.8397 s / batch. (data: 3.31e-04). ETA=7:54:14, max mem: 20.9 GB 
[11/24 11:30:15 visual_prompt]: 	Training 500/553. train loss: 35.6469,	1.8769 s / batch. (data: 1.06e+00). ETA=17:36:52, max mem: 20.9 GB 
[11/24 11:31:06 visual_prompt]: Epoch 39 / 100: avg data time: 1.92e-01, avg batch time: 1.0197, average train loss: 62.3315
[11/24 11:32:05 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3117, average loss: 91.6431
[11/24 11:32:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.29	
[11/24 11:32:05 visual_prompt]: Stopping early.
[11/24 11:32:05 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 11:32:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 11:32:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/24 11:32:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 11:32:05 visual_prompt]: Training with config:
[11/24 11:32:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/24 11:32:05 visual_prompt]: Loading training data...
[11/24 11:32:05 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 11:32:05 visual_prompt]: Loading validation data...
[11/24 11:32:05 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 11:32:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 11:32:11 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 11:32:11 visual_prompt]: tuned percent:0.525
[11/24 11:32:11 visual_prompt]: Device used for model: 0
[11/24 11:32:11 visual_prompt]: Setting up Evaluator...
[11/24 11:32:11 visual_prompt]: Setting up Trainer...
[11/24 11:32:11 visual_prompt]: 	Setting up the optimizer...
[11/24 11:32:11 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 11:33:58 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8383 s / batch. (data: 2.23e-02). ETA=12:51:16, max mem: 20.9 GB 
[11/24 11:35:39 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8408 s / batch. (data: 1.09e-02). ETA=12:52:08, max mem: 20.9 GB 
[11/24 11:37:24 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.0796 s / batch. (data: 2.38e-01). ETA=16:29:38, max mem: 20.9 GB 
[11/24 11:39:04 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8360 s / batch. (data: 5.40e-03). ETA=12:44:56, max mem: 20.9 GB 
[11/24 11:40:47 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8320 s / batch. (data: 3.31e-04). ETA=12:39:54, max mem: 20.9 GB 
[11/24 11:41:40 visual_prompt]: Epoch 1 / 100: avg data time: 1.98e-01, avg batch time: 1.0292, average train loss: 1.5403
[11/24 11:42:38 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3107, average loss: 1.5201
[11/24 11:42:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 11:42:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/24 11:44:23 visual_prompt]: 	Training 100/553. train loss: 11.4707,	0.9320 s / batch. (data: 1.13e-01). ETA=14:08:50, max mem: 20.9 GB 
[11/24 11:46:04 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8280 s / batch. (data: 3.38e-04). ETA=12:32:44, max mem: 20.9 GB 
[11/24 11:47:47 visual_prompt]: 	Training 300/553. train loss: 5.9929,	0.9837 s / batch. (data: 1.63e-01). ETA=14:52:41, max mem: 20.9 GB 
[11/24 11:49:26 visual_prompt]: 	Training 400/553. train loss: 7.5676,	0.8409 s / batch. (data: 2.97e-04). ETA=12:41:38, max mem: 20.9 GB 
[11/24 11:51:08 visual_prompt]: 	Training 500/553. train loss: 1.9307,	0.8365 s / batch. (data: 3.02e-04). ETA=12:36:16, max mem: 20.9 GB 
[11/24 11:52:01 visual_prompt]: Epoch 2 / 100: avg data time: 1.85e-01, avg batch time: 1.0175, average train loss: 9.8064
[11/24 11:53:01 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3090, average loss: 21.4495
[11/24 11:53:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.40	
[11/24 11:53:01 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/24 11:54:47 visual_prompt]: 	Training 100/553. train loss: 15.6780,	0.8320 s / batch. (data: 4.83e-04). ETA=12:30:03, max mem: 20.9 GB 
[11/24 11:56:31 visual_prompt]: 	Training 200/553. train loss: 6.2633,	0.8327 s / batch. (data: 8.63e-03). ETA=12:29:20, max mem: 20.9 GB 
[11/24 11:58:13 visual_prompt]: 	Training 300/553. train loss: 8.7254,	0.8562 s / batch. (data: 2.34e-02). ETA=12:49:04, max mem: 20.9 GB 
[11/24 11:59:57 visual_prompt]: 	Training 400/553. train loss: 15.1111,	0.8400 s / batch. (data: 7.95e-03). ETA=12:33:07, max mem: 20.9 GB 
[11/24 12:01:41 visual_prompt]: 	Training 500/553. train loss: 71.0609,	1.3680 s / batch. (data: 5.36e-01). ETA=20:24:14, max mem: 20.9 GB 
[11/24 12:02:34 visual_prompt]: Epoch 3 / 100: avg data time: 2.03e-01, avg batch time: 1.0353, average train loss: 14.2301
[11/24 12:03:32 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3095, average loss: 21.8537
[11/24 12:03:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.46	
[11/24 12:03:32 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/24 12:05:18 visual_prompt]: 	Training 100/553. train loss: 9.9626,	0.8411 s / batch. (data: 3.33e-04). ETA=12:30:33, max mem: 20.9 GB 
[11/24 12:07:00 visual_prompt]: 	Training 200/553. train loss: 59.3786,	0.8251 s / batch. (data: 7.95e-03). ETA=12:14:54, max mem: 20.9 GB 
[11/24 12:08:41 visual_prompt]: 	Training 300/553. train loss: 4.1254,	1.5440 s / batch. (data: 7.13e-01). ETA=22:52:38, max mem: 20.9 GB 
[11/24 12:10:19 visual_prompt]: 	Training 400/553. train loss: 6.7726,	1.3200 s / batch. (data: 4.73e-01). ETA=19:31:15, max mem: 20.9 GB 
[11/24 12:12:02 visual_prompt]: 	Training 500/553. train loss: 49.7587,	3.6256 s / batch. (data: 2.80e+00). ETA=2 days, 5:31:08, max mem: 20.9 GB 
[11/24 12:12:55 visual_prompt]: Epoch 4 / 100: avg data time: 1.89e-01, avg batch time: 1.0194, average train loss: 26.8838
[11/24 12:13:53 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3105, average loss: 25.4921
[11/24 12:13:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.11	
[11/24 12:13:53 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/24 12:15:38 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8269 s / batch. (data: 3.10e-04). ETA=12:10:15, max mem: 20.9 GB 
[11/24 12:17:21 visual_prompt]: 	Training 200/553. train loss: 14.0414,	1.3300 s / batch. (data: 5.16e-01). ETA=19:32:23, max mem: 20.9 GB 
[11/24 12:19:05 visual_prompt]: 	Training 300/553. train loss: 54.3054,	0.8272 s / batch. (data: 5.50e-03). ETA=12:07:46, max mem: 20.9 GB 
[11/24 12:20:48 visual_prompt]: 	Training 400/553. train loss: 31.3902,	0.8487 s / batch. (data: 1.64e-02). ETA=12:25:18, max mem: 20.9 GB 
[11/24 12:22:31 visual_prompt]: 	Training 500/553. train loss: 138.4451,	0.8440 s / batch. (data: 3.20e-04). ETA=12:19:43, max mem: 20.9 GB 
[11/24 12:23:26 visual_prompt]: Epoch 5 / 100: avg data time: 2.05e-01, avg batch time: 1.0346, average train loss: 29.4678
[11/24 12:24:25 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3080, average loss: 44.2434
[11/24 12:24:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.93	
[11/24 12:24:25 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/24 12:26:13 visual_prompt]: 	Training 100/553. train loss: 152.0571,	0.8313 s / batch. (data: 3.33e-04). ETA=12:06:31, max mem: 20.9 GB 
[11/24 12:27:53 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8193 s / batch. (data: 1.55e-02). ETA=11:54:38, max mem: 20.9 GB 
[11/24 12:29:32 visual_prompt]: 	Training 300/553. train loss: 257.1109,	0.8200 s / batch. (data: 3.00e-04). ETA=11:53:52, max mem: 20.9 GB 
[11/24 12:31:16 visual_prompt]: 	Training 400/553. train loss: 63.9711,	0.8399 s / batch. (data: 1.21e-02). ETA=12:09:49, max mem: 20.9 GB 
[11/24 12:32:54 visual_prompt]: 	Training 500/553. train loss: 22.8794,	0.8243 s / batch. (data: 3.47e-04). ETA=11:54:52, max mem: 20.9 GB 
[11/24 12:33:46 visual_prompt]: Epoch 6 / 100: avg data time: 1.86e-01, avg batch time: 1.0158, average train loss: 43.9060
[11/24 12:34:44 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3108, average loss: 43.3593
[11/24 12:34:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.47	
[11/24 12:34:44 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/24 12:36:26 visual_prompt]: 	Training 100/553. train loss: 202.3694,	0.8312 s / batch. (data: 7.97e-03). ETA=11:58:42, max mem: 20.9 GB 
[11/24 12:38:06 visual_prompt]: 	Training 200/553. train loss: 34.1242,	0.9265 s / batch. (data: 9.73e-02). ETA=13:19:37, max mem: 20.9 GB 
[11/24 12:39:50 visual_prompt]: 	Training 300/553. train loss: 39.2158,	1.9452 s / batch. (data: 1.13e+00). ETA=1 day, 3:55:32, max mem: 20.9 GB 
[11/24 12:41:30 visual_prompt]: 	Training 400/553. train loss: 74.3282,	1.7071 s / batch. (data: 8.86e-01). ETA=1 day, 0:27:36, max mem: 20.9 GB 
[11/24 12:43:07 visual_prompt]: 	Training 500/553. train loss: 79.8908,	0.8571 s / batch. (data: 4.46e-02). ETA=12:15:24, max mem: 20.9 GB 
[11/24 12:43:59 visual_prompt]: Epoch 7 / 100: avg data time: 1.75e-01, avg batch time: 1.0044, average train loss: 55.4997
[11/24 12:44:56 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3094, average loss: 23.6999
[11/24 12:44:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.52	
[11/24 12:44:56 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/24 12:46:38 visual_prompt]: 	Training 100/553. train loss: 2.0181,	0.8216 s / batch. (data: 3.37e-04). ETA=11:42:53, max mem: 20.9 GB 
[11/24 12:48:18 visual_prompt]: 	Training 200/553. train loss: 61.1637,	0.8506 s / batch. (data: 2.61e-02). ETA=12:06:16, max mem: 20.9 GB 
[11/24 12:50:03 visual_prompt]: 	Training 300/553. train loss: 145.5445,	0.8335 s / batch. (data: 8.34e-04). ETA=11:50:18, max mem: 20.9 GB 
[11/24 12:51:46 visual_prompt]: 	Training 400/553. train loss: 41.8939,	0.8172 s / batch. (data: 3.20e-04). ETA=11:35:01, max mem: 20.9 GB 
[11/24 12:53:29 visual_prompt]: 	Training 500/553. train loss: 21.5642,	1.4273 s / batch. (data: 6.06e-01). ETA=20:11:33, max mem: 20.9 GB 
[11/24 12:54:23 visual_prompt]: Epoch 8 / 100: avg data time: 1.97e-01, avg batch time: 1.0244, average train loss: 56.5643
[11/24 12:55:22 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3089, average loss: 57.6910
[11/24 12:55:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.04	
[11/24 12:55:22 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/24 12:57:08 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8520 s / batch. (data: 7.95e-03). ETA=12:01:01, max mem: 20.9 GB 
[11/24 12:58:50 visual_prompt]: 	Training 200/553. train loss: 785.9756,	0.8226 s / batch. (data: 1.00e-02). ETA=11:34:46, max mem: 20.9 GB 
[11/24 13:00:34 visual_prompt]: 	Training 300/553. train loss: 56.0445,	1.7175 s / batch. (data: 8.96e-01). ETA=1 day, 0:07:44, max mem: 20.9 GB 
[11/24 13:02:17 visual_prompt]: 	Training 400/553. train loss: 135.3336,	0.8440 s / batch. (data: 3.10e-04). ETA=11:50:01, max mem: 20.9 GB 
[11/24 13:04:01 visual_prompt]: 	Training 500/553. train loss: 26.5560,	0.9887 s / batch. (data: 1.74e-01). ETA=13:50:05, max mem: 20.9 GB 
[11/24 13:04:54 visual_prompt]: Epoch 9 / 100: avg data time: 2.07e-01, avg batch time: 1.0337, average train loss: 68.2584
[11/24 13:05:52 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3097, average loss: 19.6656
[11/24 13:05:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.07	
[11/24 13:05:52 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/24 13:07:41 visual_prompt]: 	Training 100/553. train loss: 115.1190,	0.8292 s / batch. (data: 7.96e-03). ETA=11:34:05, max mem: 20.9 GB 
[11/24 13:09:21 visual_prompt]: 	Training 200/553. train loss: 64.3213,	0.8309 s / batch. (data: 1.07e-02). ETA=11:34:07, max mem: 20.9 GB 
[11/24 13:10:59 visual_prompt]: 	Training 300/553. train loss: 6.0597,	1.1005 s / batch. (data: 2.64e-01). ETA=15:17:32, max mem: 20.9 GB 
[11/24 13:12:36 visual_prompt]: 	Training 400/553. train loss: 90.0477,	0.8440 s / batch. (data: 3.33e-04). ETA=11:42:13, max mem: 20.9 GB 
[11/24 13:14:17 visual_prompt]: 	Training 500/553. train loss: 8.8133,	0.8348 s / batch. (data: 3.19e-04). ETA=11:33:13, max mem: 20.9 GB 
[11/24 13:15:08 visual_prompt]: Epoch 10 / 100: avg data time: 1.78e-01, avg batch time: 1.0050, average train loss: 76.3979
[11/24 13:16:04 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3114, average loss: 44.8438
[11/24 13:16:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.15	
[11/24 13:16:04 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/24 13:17:49 visual_prompt]: 	Training 100/553. train loss: 196.5261,	0.8491 s / batch. (data: 2.10e-02). ETA=11:42:53, max mem: 20.9 GB 
[11/24 13:19:30 visual_prompt]: 	Training 200/553. train loss: 278.2780,	0.8360 s / batch. (data: 3.23e-04). ETA=11:30:40, max mem: 20.9 GB 
[11/24 13:21:08 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.1126 s / batch. (data: 1.30e+00). ETA=1 day, 5:01:48, max mem: 20.9 GB 
[11/24 13:22:45 visual_prompt]: 	Training 400/553. train loss: 36.4821,	0.8497 s / batch. (data: 1.56e-02). ETA=11:39:09, max mem: 20.9 GB 
[11/24 13:24:23 visual_prompt]: 	Training 500/553. train loss: 162.8521,	0.8360 s / batch. (data: 7.96e-03). ETA=11:26:29, max mem: 20.9 GB 
[11/24 13:25:13 visual_prompt]: Epoch 11 / 100: avg data time: 1.65e-01, avg batch time: 0.9928, average train loss: 83.0005
[11/24 13:26:10 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3134, average loss: 18.7106
[11/24 13:26:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.66	
[11/24 13:26:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/24 13:27:54 visual_prompt]: 	Training 100/553. train loss: 126.5586,	0.9720 s / batch. (data: 1.40e-01). ETA=13:15:41, max mem: 20.9 GB 
[11/24 13:29:34 visual_prompt]: 	Training 200/553. train loss: 152.5565,	0.9224 s / batch. (data: 1.03e-01). ETA=12:33:31, max mem: 20.9 GB 
[11/24 13:31:11 visual_prompt]: 	Training 300/553. train loss: 21.7700,	0.8204 s / batch. (data: 1.20e-02). ETA=11:08:52, max mem: 20.9 GB 
[11/24 13:32:51 visual_prompt]: 	Training 400/553. train loss: 93.6778,	0.8357 s / batch. (data: 3.21e-04). ETA=11:19:54, max mem: 20.9 GB 
[11/24 13:34:29 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8315 s / batch. (data: 2.20e-02). ETA=11:15:07, max mem: 20.9 GB 
[11/24 13:35:20 visual_prompt]: Epoch 12 / 100: avg data time: 1.68e-01, avg batch time: 0.9951, average train loss: 86.6268
[11/24 13:36:16 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3100, average loss: 228.3232
[11/24 13:36:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.58	
[11/24 13:36:16 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/24 13:38:01 visual_prompt]: 	Training 100/553. train loss: 23.4021,	0.8143 s / batch. (data: 5.44e-03). ETA=10:59:06, max mem: 20.9 GB 
[11/24 13:39:36 visual_prompt]: 	Training 200/553. train loss: 55.1706,	0.8155 s / batch. (data: 3.10e-04). ETA=10:58:40, max mem: 20.9 GB 
[11/24 13:41:16 visual_prompt]: 	Training 300/553. train loss: 16.4816,	1.9027 s / batch. (data: 1.07e+00). ETA=1 day, 1:33:39, max mem: 20.9 GB 
[11/24 13:42:53 visual_prompt]: 	Training 400/553. train loss: 7.3962,	0.8177 s / batch. (data: 3.42e-04). ETA=10:57:46, max mem: 20.9 GB 
[11/24 13:44:34 visual_prompt]: 	Training 500/553. train loss: 32.3706,	0.8307 s / batch. (data: 2.94e-04). ETA=11:06:47, max mem: 20.9 GB 
[11/24 13:45:25 visual_prompt]: Epoch 13 / 100: avg data time: 1.65e-01, avg batch time: 0.9926, average train loss: 89.5001
[11/24 13:46:22 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3102, average loss: 41.0821
[11/24 13:46:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.43	
[11/24 13:46:22 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/24 13:48:06 visual_prompt]: 	Training 100/553. train loss: 79.9684,	0.8227 s / batch. (data: 2.94e-04). ETA=10:58:20, max mem: 20.9 GB 
[11/24 13:49:45 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8340 s / batch. (data: 2.89e-04). ETA=11:06:00, max mem: 20.9 GB 
[11/24 13:51:23 visual_prompt]: 	Training 300/553. train loss: 81.4320,	0.8880 s / batch. (data: 6.00e-02). ETA=11:47:35, max mem: 20.9 GB 
[11/24 13:53:05 visual_prompt]: 	Training 400/553. train loss: 23.2426,	0.8596 s / batch. (data: 2.36e-02). ETA=11:23:34, max mem: 20.9 GB 
[11/24 13:54:49 visual_prompt]: 	Training 500/553. train loss: 128.8445,	0.8400 s / batch. (data: 3.28e-04). ETA=11:06:33, max mem: 20.9 GB 
[11/24 13:55:42 visual_prompt]: Epoch 14 / 100: avg data time: 1.86e-01, avg batch time: 1.0130, average train loss: 94.1733
[11/24 13:56:41 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.3097, average loss: 72.9262
[11/24 13:56:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.32	
[11/24 13:56:41 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/24 13:58:27 visual_prompt]: 	Training 100/553. train loss: 107.7245,	0.9360 s / batch. (data: 1.07e-01). ETA=12:20:18, max mem: 20.9 GB 
[11/24 14:00:09 visual_prompt]: 	Training 200/553. train loss: 338.1583,	0.8353 s / batch. (data: 3.17e-04). ETA=10:59:16, max mem: 20.9 GB 
[11/24 14:01:54 visual_prompt]: 	Training 300/553. train loss: 10.7235,	0.8260 s / batch. (data: 3.26e-04). ETA=10:50:35, max mem: 20.9 GB 
[11/24 14:03:34 visual_prompt]: 	Training 400/553. train loss: 204.6113,	1.2204 s / batch. (data: 3.96e-01). ETA=15:59:10, max mem: 20.9 GB 
[11/24 14:05:17 visual_prompt]: 	Training 500/553. train loss: 120.6582,	0.8480 s / batch. (data: 2.61e-04). ETA=11:05:06, max mem: 20.9 GB 
[11/24 14:06:12 visual_prompt]: Epoch 15 / 100: avg data time: 2.05e-01, avg batch time: 1.0321, average train loss: 93.8931
[11/24 14:07:10 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3093, average loss: 5.1922
[11/24 14:07:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.96	
[11/24 14:07:10 visual_prompt]: Best epoch 15: best metric: -5.192
[11/24 14:07:10 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/24 14:08:55 visual_prompt]: 	Training 100/553. train loss: 40.4202,	0.8384 s / batch. (data: 3.19e-04). ETA=10:55:26, max mem: 20.9 GB 
[11/24 14:10:39 visual_prompt]: 	Training 200/553. train loss: 140.7945,	0.8160 s / batch. (data: 3.07e-04). ETA=10:36:33, max mem: 20.9 GB 
[11/24 14:12:20 visual_prompt]: 	Training 300/553. train loss: 13.6437,	0.8214 s / batch. (data: 2.88e-04). ETA=10:39:23, max mem: 20.9 GB 
[11/24 14:13:59 visual_prompt]: 	Training 400/553. train loss: 64.7440,	0.8340 s / batch. (data: 8.13e-04). ETA=10:47:48, max mem: 20.9 GB 
[11/24 14:15:37 visual_prompt]: 	Training 500/553. train loss: 203.5595,	1.0000 s / batch. (data: 1.67e-01). ETA=12:55:04, max mem: 20.9 GB 
[11/24 14:16:29 visual_prompt]: Epoch 16 / 100: avg data time: 1.83e-01, avg batch time: 1.0106, average train loss: 78.6311
[11/24 14:17:26 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3095, average loss: 4.1551
[11/24 14:17:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.29	
[11/24 14:17:26 visual_prompt]: Best epoch 16: best metric: -4.155
[11/24 14:17:26 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/24 14:19:08 visual_prompt]: 	Training 100/553. train loss: 3.5588,	0.8423 s / batch. (data: 7.94e-03). ETA=10:50:41, max mem: 20.9 GB 
[11/24 14:20:48 visual_prompt]: 	Training 200/553. train loss: 182.7799,	0.8150 s / batch. (data: 2.92e-04). ETA=10:28:17, max mem: 20.9 GB 
[11/24 14:22:26 visual_prompt]: 	Training 300/553. train loss: 111.7145,	0.8200 s / batch. (data: 3.12e-04). ETA=10:30:45, max mem: 20.9 GB 
[11/24 14:24:04 visual_prompt]: 	Training 400/553. train loss: 320.7078,	1.1328 s / batch. (data: 3.17e-01). ETA=14:29:25, max mem: 20.9 GB 
[11/24 14:25:43 visual_prompt]: 	Training 500/553. train loss: 124.6979,	1.6880 s / batch. (data: 8.62e-01). ETA=21:32:45, max mem: 20.9 GB 
[11/24 14:26:35 visual_prompt]: Epoch 17 / 100: avg data time: 1.66e-01, avg batch time: 0.9930, average train loss: 80.3591
[11/24 14:27:32 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3097, average loss: 219.9071
[11/24 14:27:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.28	
[11/24 14:27:32 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/24 14:29:15 visual_prompt]: 	Training 100/553. train loss: 65.2004,	0.8400 s / batch. (data: 7.57e-04). ETA=10:41:11, max mem: 20.9 GB 
[11/24 14:30:56 visual_prompt]: 	Training 200/553. train loss: 30.3554,	0.8466 s / batch. (data: 7.54e-04). ETA=10:44:49, max mem: 20.9 GB 
[11/24 14:32:35 visual_prompt]: 	Training 300/553. train loss: 118.4125,	0.8440 s / batch. (data: 2.95e-04). ETA=10:41:24, max mem: 20.9 GB 
[11/24 14:34:13 visual_prompt]: 	Training 400/553. train loss: 23.7052,	0.8501 s / batch. (data: 1.20e-02). ETA=10:44:37, max mem: 20.9 GB 
[11/24 14:35:52 visual_prompt]: 	Training 500/553. train loss: 84.4155,	0.8208 s / batch. (data: 5.46e-03). ETA=10:21:02, max mem: 20.9 GB 
[11/24 14:36:42 visual_prompt]: Epoch 18 / 100: avg data time: 1.67e-01, avg batch time: 0.9949, average train loss: 81.1541
[11/24 14:37:39 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3108, average loss: 55.5910
[11/24 14:37:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.31	
[11/24 14:37:39 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/24 14:39:21 visual_prompt]: 	Training 100/553. train loss: 33.5354,	1.0880 s / batch. (data: 2.58e-01). ETA=13:40:27, max mem: 20.9 GB 
[11/24 14:41:01 visual_prompt]: 	Training 200/553. train loss: 48.6493,	0.8271 s / batch. (data: 3.13e-04). ETA=10:22:18, max mem: 20.9 GB 
[11/24 14:42:39 visual_prompt]: 	Training 300/553. train loss: 153.9044,	0.8400 s / batch. (data: 3.19e-04). ETA=10:30:36, max mem: 20.9 GB 
[11/24 14:44:19 visual_prompt]: 	Training 400/553. train loss: 50.0509,	0.8192 s / batch. (data: 7.99e-04). ETA=10:13:39, max mem: 20.9 GB 
[11/24 14:45:55 visual_prompt]: 	Training 500/553. train loss: 2.3170,	0.8554 s / batch. (data: 5.44e-03). ETA=10:39:21, max mem: 20.9 GB 
[11/24 14:46:46 visual_prompt]: Epoch 19 / 100: avg data time: 1.61e-01, avg batch time: 0.9897, average train loss: 72.0063
[11/24 14:47:42 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3105, average loss: 17.9899
[11/24 14:47:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.09	
[11/24 14:47:42 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/24 14:49:24 visual_prompt]: 	Training 100/553. train loss: 5.2882,	0.8560 s / batch. (data: 3.16e-04). ETA=10:37:36, max mem: 20.9 GB 
[11/24 14:51:03 visual_prompt]: 	Training 200/553. train loss: 18.6954,	0.8440 s / batch. (data: 3.27e-04). ETA=10:27:16, max mem: 20.9 GB 
[11/24 14:52:43 visual_prompt]: 	Training 300/553. train loss: 22.6528,	0.8201 s / batch. (data: 3.34e-04). ETA=10:08:08, max mem: 20.9 GB 
[11/24 14:54:25 visual_prompt]: 	Training 400/553. train loss: 22.2894,	0.8383 s / batch. (data: 7.95e-03). ETA=10:20:12, max mem: 20.9 GB 
[11/24 14:56:08 visual_prompt]: 	Training 500/553. train loss: 41.9790,	0.8473 s / batch. (data: 1.19e-02). ETA=10:25:27, max mem: 20.9 GB 
[11/24 14:57:03 visual_prompt]: Epoch 20 / 100: avg data time: 1.86e-01, avg batch time: 1.0135, average train loss: 79.3548
[11/24 14:58:02 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3102, average loss: 51.9980
[11/24 14:58:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.62	
[11/24 14:58:02 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/24 14:59:51 visual_prompt]: 	Training 100/553. train loss: 77.1954,	0.8171 s / batch. (data: 3.06e-04). ETA=10:01:07, max mem: 20.9 GB 
[11/24 15:01:33 visual_prompt]: 	Training 200/553. train loss: 90.5409,	0.8169 s / batch. (data: 3.54e-04). ETA=9:59:35, max mem: 20.9 GB 
[11/24 15:03:14 visual_prompt]: 	Training 300/553. train loss: 120.1923,	1.0324 s / batch. (data: 2.04e-01). ETA=12:36:01, max mem: 20.9 GB 
[11/24 15:04:56 visual_prompt]: 	Training 400/553. train loss: 23.0740,	0.8216 s / batch. (data: 3.48e-04). ETA=10:00:17, max mem: 20.9 GB 
[11/24 15:06:42 visual_prompt]: 	Training 500/553. train loss: 62.0444,	0.8487 s / batch. (data: 8.65e-03). ETA=10:18:41, max mem: 20.9 GB 
[11/24 15:07:36 visual_prompt]: Epoch 21 / 100: avg data time: 2.11e-01, avg batch time: 1.0380, average train loss: 80.6375
[11/24 15:08:33 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3112, average loss: 5.7173
[11/24 15:08:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.95	
[11/24 15:08:33 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/24 15:10:21 visual_prompt]: 	Training 100/553. train loss: 45.5380,	0.8200 s / batch. (data: 3.20e-04). ETA=9:55:41, max mem: 20.9 GB 
[11/24 15:12:06 visual_prompt]: 	Training 200/553. train loss: 14.3998,	0.8400 s / batch. (data: 7.94e-03). ETA=10:08:47, max mem: 20.9 GB 
[11/24 15:13:47 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8280 s / batch. (data: 7.95e-03). ETA=9:58:43, max mem: 20.9 GB 
[11/24 15:15:33 visual_prompt]: 	Training 400/553. train loss: 41.4337,	0.8358 s / batch. (data: 7.05e-04). ETA=10:03:00, max mem: 20.9 GB 
[11/24 15:17:15 visual_prompt]: 	Training 500/553. train loss: 41.4893,	0.8080 s / batch. (data: 3.22e-04). ETA=9:41:34, max mem: 20.9 GB 
[11/24 15:18:11 visual_prompt]: Epoch 22 / 100: avg data time: 2.16e-01, avg batch time: 1.0448, average train loss: 82.7818
[11/24 15:19:11 visual_prompt]: Inference (val):avg data time: 4.44e-05, avg batch time: 0.3112, average loss: 30.7563
[11/24 15:19:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.44	
[11/24 15:19:11 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/24 15:20:56 visual_prompt]: 	Training 100/553. train loss: 123.4828,	0.8200 s / batch. (data: 5.43e-03). ETA=9:48:09, max mem: 20.9 GB 
[11/24 15:22:33 visual_prompt]: 	Training 200/553. train loss: 15.2011,	0.8359 s / batch. (data: 3.05e-04). ETA=9:58:09, max mem: 20.9 GB 
[11/24 15:24:11 visual_prompt]: 	Training 300/553. train loss: 234.7354,	0.8138 s / batch. (data: 2.94e-04). ETA=9:40:57, max mem: 20.9 GB 
[11/24 15:25:52 visual_prompt]: 	Training 400/553. train loss: 102.2313,	0.8201 s / batch. (data: 3.18e-04). ETA=9:44:06, max mem: 20.9 GB 
[11/24 15:27:34 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8200 s / batch. (data: 3.86e-04). ETA=9:42:38, max mem: 20.9 GB 
[11/24 15:28:27 visual_prompt]: Epoch 23 / 100: avg data time: 1.78e-01, avg batch time: 1.0049, average train loss: 75.8306
[11/24 15:29:24 visual_prompt]: Inference (val):avg data time: 4.20e-04, avg batch time: 0.3094, average loss: 146.7328
[11/24 15:29:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.00	
[11/24 15:29:24 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/24 15:31:10 visual_prompt]: 	Training 100/553. train loss: 80.0707,	0.8202 s / batch. (data: 1.18e-02). ETA=9:40:44, max mem: 20.9 GB 
[11/24 15:32:52 visual_prompt]: 	Training 200/553. train loss: 151.4542,	0.8338 s / batch. (data: 1.73e-02). ETA=9:48:56, max mem: 20.9 GB 
[11/24 15:34:29 visual_prompt]: 	Training 300/553. train loss: 10.0722,	0.8580 s / batch. (data: 3.64e-04). ETA=10:04:35, max mem: 20.9 GB 
[11/24 15:36:05 visual_prompt]: 	Training 400/553. train loss: 18.1664,	0.8456 s / batch. (data: 2.81e-02). ETA=9:54:28, max mem: 20.9 GB 
[11/24 15:37:42 visual_prompt]: 	Training 500/553. train loss: 34.1217,	0.8524 s / batch. (data: 1.02e-03). ETA=9:57:50, max mem: 20.9 GB 
[11/24 15:38:32 visual_prompt]: Epoch 24 / 100: avg data time: 1.64e-01, avg batch time: 0.9912, average train loss: 79.6634
[11/24 15:39:29 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3118, average loss: 23.4213
[11/24 15:39:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.63	
[11/24 15:39:29 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/24 15:41:21 visual_prompt]: 	Training 100/553. train loss: 141.2382,	0.8578 s / batch. (data: 1.06e-02). ETA=9:59:24, max mem: 20.9 GB 
[11/24 15:43:01 visual_prompt]: 	Training 200/553. train loss: 37.6976,	0.9555 s / batch. (data: 1.28e-01). ETA=11:06:07, max mem: 20.9 GB 
[11/24 15:44:45 visual_prompt]: 	Training 300/553. train loss: 216.4294,	1.0473 s / batch. (data: 2.38e-01). ETA=12:08:20, max mem: 20.9 GB 
[11/24 15:46:30 visual_prompt]: 	Training 400/553. train loss: 105.7682,	1.3440 s / batch. (data: 5.16e-01). ETA=15:32:28, max mem: 20.9 GB 
[11/24 15:48:14 visual_prompt]: 	Training 500/553. train loss: 40.7476,	1.7089 s / batch. (data: 8.99e-01). ETA=19:42:45, max mem: 20.9 GB 
[11/24 15:49:08 visual_prompt]: Epoch 25 / 100: avg data time: 2.19e-01, avg batch time: 1.0462, average train loss: 73.2479
[11/24 15:50:08 visual_prompt]: Inference (val):avg data time: 4.80e-05, avg batch time: 0.3075, average loss: 121.4401
[11/24 15:50:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.64	
[11/24 15:50:08 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/24 15:51:49 visual_prompt]: 	Training 100/553. train loss: 232.7358,	0.8288 s / batch. (data: 3.08e-04). ETA=9:31:33, max mem: 20.9 GB 
[11/24 15:53:35 visual_prompt]: 	Training 200/553. train loss: 22.3119,	2.0153 s / batch. (data: 1.21e+00). ETA=23:06:22, max mem: 20.9 GB 
[11/24 15:55:21 visual_prompt]: 	Training 300/553. train loss: 224.7032,	0.8743 s / batch. (data: 8.64e-04). ETA=9:59:59, max mem: 20.9 GB 
[11/24 15:56:57 visual_prompt]: 	Training 400/553. train loss: 11.7838,	0.8574 s / batch. (data: 2.14e-02). ETA=9:46:57, max mem: 20.9 GB 
[11/24 15:58:31 visual_prompt]: 	Training 500/553. train loss: 139.0402,	0.8312 s / batch. (data: 1.05e-02). ETA=9:27:40, max mem: 20.9 GB 
[11/24 15:59:21 visual_prompt]: Epoch 26 / 100: avg data time: 1.71e-01, avg batch time: 0.9988, average train loss: 90.0629
[11/24 16:00:15 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.3108, average loss: 45.1047
[11/24 16:00:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.26	
[11/24 16:00:15 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[11/24 16:01:55 visual_prompt]: 	Training 100/553. train loss: 22.0259,	0.8437 s / batch. (data: 2.86e-04). ETA=9:34:01, max mem: 20.9 GB 
[11/24 16:03:29 visual_prompt]: 	Training 200/553. train loss: 166.5758,	0.8480 s / batch. (data: 1.19e-02). ETA=9:35:32, max mem: 20.9 GB 
[11/24 16:05:04 visual_prompt]: 	Training 300/553. train loss: 47.4541,	0.8473 s / batch. (data: 5.41e-03). ETA=9:33:37, max mem: 20.9 GB 
[11/24 16:06:39 visual_prompt]: 	Training 400/553. train loss: 40.6689,	0.8306 s / batch. (data: 5.40e-03). ETA=9:20:58, max mem: 20.9 GB 
[11/24 16:08:16 visual_prompt]: 	Training 500/553. train loss: 119.7567,	0.8410 s / batch. (data: 1.60e-02). ETA=9:26:34, max mem: 20.9 GB 
[11/24 16:09:04 visual_prompt]: Epoch 27 / 100: avg data time: 1.27e-01, avg batch time: 0.9566, average train loss: 77.0611
[11/24 16:09:58 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3106, average loss: 40.9811
[11/24 16:09:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.89	
[11/24 16:09:58 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[11/24 16:11:36 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8266 s / batch. (data: 5.51e-03). ETA=9:14:45, max mem: 20.9 GB 
[11/24 16:13:13 visual_prompt]: 	Training 200/553. train loss: 54.0814,	0.8321 s / batch. (data: 6.55e-03). ETA=9:17:02, max mem: 20.9 GB 
[11/24 16:14:50 visual_prompt]: 	Training 300/553. train loss: 81.7058,	1.4800 s / batch. (data: 6.55e-01). ETA=16:28:21, max mem: 20.9 GB 
[11/24 16:16:28 visual_prompt]: 	Training 400/553. train loss: 65.5140,	0.8275 s / batch. (data: 3.08e-04). ETA=9:11:14, max mem: 20.9 GB 
[11/24 16:18:12 visual_prompt]: 	Training 500/553. train loss: 182.4296,	0.8356 s / batch. (data: 5.45e-03). ETA=9:15:12, max mem: 20.9 GB 
[11/24 16:19:07 visual_prompt]: Epoch 28 / 100: avg data time: 1.64e-01, avg batch time: 0.9920, average train loss: 82.3342
[11/24 16:20:03 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3093, average loss: 18.1313
[11/24 16:20:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.82	
[11/24 16:20:03 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[11/24 16:21:50 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8316 s / batch. (data: 6.93e-04). ETA=9:10:29, max mem: 20.9 GB 
[11/24 16:23:26 visual_prompt]: 	Training 200/553. train loss: 130.6215,	1.6437 s / batch. (data: 8.31e-01). ETA=18:05:17, max mem: 20.9 GB 
[11/24 16:25:00 visual_prompt]: 	Training 300/553. train loss: 62.4456,	0.8432 s / batch. (data: 5.91e-03). ETA=9:15:20, max mem: 20.9 GB 
[11/24 16:26:37 visual_prompt]: 	Training 400/553. train loss: 62.6526,	0.8200 s / batch. (data: 3.60e-04). ETA=8:58:41, max mem: 20.9 GB 
[11/24 16:28:22 visual_prompt]: 	Training 500/553. train loss: 33.2094,	0.8322 s / batch. (data: 3.59e-04). ETA=9:05:17, max mem: 20.9 GB 
[11/24 16:29:17 visual_prompt]: Epoch 29 / 100: avg data time: 1.74e-01, avg batch time: 1.0015, average train loss: 68.3521
[11/24 16:30:14 visual_prompt]: Inference (val):avg data time: 2.05e-04, avg batch time: 0.3111, average loss: 34.2258
[11/24 16:30:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.92	
[11/24 16:30:14 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[11/24 16:31:52 visual_prompt]: 	Training 100/553. train loss: 201.8579,	0.8160 s / batch. (data: 2.95e-04). ETA=8:52:37, max mem: 20.9 GB 
[11/24 16:33:38 visual_prompt]: 	Training 200/553. train loss: 57.9761,	0.8259 s / batch. (data: 3.63e-04). ETA=8:57:42, max mem: 20.9 GB 
[11/24 16:35:22 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.5851 s / batch. (data: 7.57e-01). ETA=17:09:19, max mem: 20.9 GB 
[11/24 16:37:00 visual_prompt]: 	Training 400/553. train loss: 193.9628,	1.0579 s / batch. (data: 2.45e-01). ETA=11:25:11, max mem: 20.9 GB 
[11/24 16:38:36 visual_prompt]: 	Training 500/553. train loss: 112.9856,	1.3351 s / batch. (data: 5.01e-01). ETA=14:22:30, max mem: 20.9 GB 
[11/24 16:39:28 visual_prompt]: Epoch 30 / 100: avg data time: 1.74e-01, avg batch time: 1.0015, average train loss: 75.3280
[11/24 16:40:23 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3107, average loss: 84.2754
[11/24 16:40:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.72	
[11/24 16:40:23 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[11/24 16:42:04 visual_prompt]: 	Training 100/553. train loss: 85.2338,	0.8084 s / batch. (data: 5.43e-03). ETA=8:40:12, max mem: 20.9 GB 
[11/24 16:43:42 visual_prompt]: 	Training 200/553. train loss: 79.2875,	0.8501 s / batch. (data: 2.96e-02). ETA=9:05:38, max mem: 20.9 GB 
[11/24 16:45:15 visual_prompt]: 	Training 300/553. train loss: 85.9727,	0.8360 s / batch. (data: 3.05e-04). ETA=8:55:11, max mem: 20.9 GB 
[11/24 16:46:50 visual_prompt]: 	Training 400/553. train loss: 94.8413,	1.0199 s / batch. (data: 1.79e-01). ETA=10:51:14, max mem: 20.9 GB 
[11/24 16:48:26 visual_prompt]: 	Training 500/553. train loss: 3.8826,	0.8369 s / batch. (data: 2.04e-02). ETA=8:52:57, max mem: 20.9 GB 
[11/24 16:49:16 visual_prompt]: Epoch 31 / 100: avg data time: 1.35e-01, avg batch time: 0.9631, average train loss: 77.4775
[11/24 16:50:10 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3102, average loss: 164.9876
[11/24 16:50:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.85	
[11/24 16:50:10 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[11/24 16:51:50 visual_prompt]: 	Training 100/553. train loss: 33.6230,	0.8162 s / batch. (data: 5.51e-03). ETA=8:37:41, max mem: 20.9 GB 
[11/24 16:53:25 visual_prompt]: 	Training 200/553. train loss: 584.8047,	0.8441 s / batch. (data: 5.45e-03). ETA=8:54:01, max mem: 20.9 GB 
[11/24 16:55:05 visual_prompt]: 	Training 300/553. train loss: 94.3624,	0.8496 s / batch. (data: 1.56e-02). ETA=8:56:01, max mem: 20.9 GB 
[11/24 16:56:43 visual_prompt]: 	Training 400/553. train loss: 93.1784,	0.8400 s / batch. (data: 7.96e-03). ETA=8:48:36, max mem: 20.9 GB 
[11/24 16:58:18 visual_prompt]: 	Training 500/553. train loss: 66.1400,	0.8224 s / batch. (data: 8.53e-03). ETA=8:36:09, max mem: 20.9 GB 
[11/24 16:59:06 visual_prompt]: Epoch 32 / 100: avg data time: 1.41e-01, avg batch time: 0.9684, average train loss: 83.4985
[11/24 17:00:00 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.3101, average loss: 234.1134
[11/24 17:00:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.03	
[11/24 17:00:00 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[11/24 17:01:38 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8400 s / batch. (data: 3.04e-04). ETA=8:45:02, max mem: 20.9 GB 
[11/24 17:03:15 visual_prompt]: 	Training 200/553. train loss: 216.9189,	0.8465 s / batch. (data: 2.97e-04). ETA=8:47:41, max mem: 20.9 GB 
[11/24 17:04:49 visual_prompt]: 	Training 300/553. train loss: 64.6718,	0.8560 s / batch. (data: 2.88e-04). ETA=8:52:12, max mem: 20.9 GB 
[11/24 17:06:27 visual_prompt]: 	Training 400/553. train loss: 8.0882,	0.8249 s / batch. (data: 3.01e-04). ETA=8:31:29, max mem: 20.9 GB 
[11/24 17:08:03 visual_prompt]: 	Training 500/553. train loss: 4.8989,	0.8240 s / batch. (data: 3.23e-04). ETA=8:29:33, max mem: 20.9 GB 
[11/24 17:08:53 visual_prompt]: Epoch 33 / 100: avg data time: 1.34e-01, avg batch time: 0.9634, average train loss: 67.1823
[11/24 17:09:48 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3110, average loss: 66.9563
[11/24 17:09:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.69	
[11/24 17:09:48 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[11/24 17:11:30 visual_prompt]: 	Training 100/553. train loss: 55.6792,	0.8190 s / batch. (data: 1.05e-02). ETA=8:24:21, max mem: 20.9 GB 
[11/24 17:13:05 visual_prompt]: 	Training 200/553. train loss: 100.1291,	0.8240 s / batch. (data: 1.19e-02). ETA=8:26:05, max mem: 20.9 GB 
[11/24 17:14:41 visual_prompt]: 	Training 300/553. train loss: 10.0560,	0.8242 s / batch. (data: 3.58e-04). ETA=8:24:49, max mem: 20.9 GB 
[11/24 17:16:18 visual_prompt]: 	Training 400/553. train loss: 19.5371,	0.8440 s / batch. (data: 2.73e-04). ETA=8:35:33, max mem: 20.9 GB 
[11/24 17:17:55 visual_prompt]: 	Training 500/553. train loss: 9.3318,	1.3760 s / batch. (data: 5.35e-01). ETA=13:58:15, max mem: 20.9 GB 
[11/24 17:18:45 visual_prompt]: Epoch 34 / 100: avg data time: 1.42e-01, avg batch time: 0.9711, average train loss: 75.3529
[11/24 17:19:40 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3114, average loss: 29.7108
[11/24 17:19:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.47	
[11/24 17:19:40 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[11/24 17:21:22 visual_prompt]: 	Training 100/553. train loss: 29.5550,	0.8239 s / batch. (data: 5.60e-03). ETA=8:19:47, max mem: 20.9 GB 
[11/24 17:22:59 visual_prompt]: 	Training 200/553. train loss: 86.2786,	0.8206 s / batch. (data: 3.10e-04). ETA=8:16:26, max mem: 20.9 GB 
[11/24 17:24:35 visual_prompt]: 	Training 300/553. train loss: 75.0879,	0.8247 s / batch. (data: 3.33e-04). ETA=8:17:32, max mem: 20.9 GB 
[11/24 17:26:10 visual_prompt]: 	Training 400/553. train loss: 96.4444,	0.8121 s / batch. (data: 5.42e-03). ETA=8:08:34, max mem: 20.9 GB 
[11/24 17:27:45 visual_prompt]: 	Training 500/553. train loss: 102.2573,	0.9397 s / batch. (data: 1.11e-01). ETA=9:23:48, max mem: 20.9 GB 
[11/24 17:28:37 visual_prompt]: Epoch 35 / 100: avg data time: 1.43e-01, avg batch time: 0.9709, average train loss: 72.9441
[11/24 17:29:32 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3099, average loss: 154.1847
[11/24 17:29:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.02	
[11/24 17:29:32 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[11/24 17:31:12 visual_prompt]: 	Training 100/553. train loss: 229.5056,	0.8640 s / batch. (data: 7.95e-03). ETA=8:36:10, max mem: 20.9 GB 
[11/24 17:32:49 visual_prompt]: 	Training 200/553. train loss: 52.7191,	0.8268 s / batch. (data: 7.96e-03). ETA=8:12:34, max mem: 20.9 GB 
[11/24 17:34:28 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8065 s / batch. (data: 2.92e-04). ETA=7:59:08, max mem: 20.9 GB 
[11/24 17:36:04 visual_prompt]: 	Training 400/553. train loss: 8.9776,	0.8360 s / batch. (data: 3.75e-04). ETA=8:15:15, max mem: 20.9 GB 
[11/24 17:37:42 visual_prompt]: 	Training 500/553. train loss: 9.7776,	0.9565 s / batch. (data: 1.31e-01). ETA=9:25:04, max mem: 20.9 GB 
[11/24 17:38:30 visual_prompt]: Epoch 36 / 100: avg data time: 1.44e-01, avg batch time: 0.9716, average train loss: 79.3547
[11/24 17:39:25 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3098, average loss: 2.7531
[11/24 17:39:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.56	rocauc: 51.42	
[11/24 17:39:25 visual_prompt]: Best epoch 36: best metric: -2.753
[11/24 17:39:25 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[11/24 17:41:05 visual_prompt]: 	Training 100/553. train loss: 2.4550,	0.8803 s / batch. (data: 2.43e-02). ETA=8:37:46, max mem: 20.9 GB 
[11/24 17:42:41 visual_prompt]: 	Training 200/553. train loss: 2.7339,	0.8524 s / batch. (data: 2.93e-04). ETA=8:19:59, max mem: 20.9 GB 
[11/24 17:44:17 visual_prompt]: 	Training 300/553. train loss: 5.9133,	1.1696 s / batch. (data: 3.42e-01). ETA=11:24:04, max mem: 20.9 GB 
[11/24 17:45:55 visual_prompt]: 	Training 400/553. train loss: 22.4601,	1.8960 s / batch. (data: 1.06e+00). ETA=18:25:44, max mem: 20.9 GB 
[11/24 17:47:28 visual_prompt]: 	Training 500/553. train loss: 3.0500,	1.0995 s / batch. (data: 2.63e-01). ETA=10:39:24, max mem: 20.9 GB 
[11/24 17:48:20 visual_prompt]: Epoch 37 / 100: avg data time: 1.40e-01, avg batch time: 0.9675, average train loss: 75.5481
[11/24 17:49:14 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3112, average loss: 55.5520
[11/24 17:49:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.53	
[11/24 17:49:14 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[11/24 17:50:52 visual_prompt]: 	Training 100/553. train loss: 3.5900,	0.8520 s / batch. (data: 2.50e-02). ETA=8:13:18, max mem: 20.9 GB 
[11/24 17:52:29 visual_prompt]: 	Training 200/553. train loss: 70.7483,	1.2559 s / batch. (data: 4.39e-01). ETA=12:05:04, max mem: 20.9 GB 
[11/24 17:54:06 visual_prompt]: 	Training 300/553. train loss: 19.4833,	0.8208 s / batch. (data: 1.04e-02). ETA=7:52:28, max mem: 20.9 GB 
[11/24 17:55:40 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8274 s / batch. (data: 4.38e-04). ETA=7:54:54, max mem: 20.9 GB 
[11/24 17:57:18 visual_prompt]: 	Training 500/553. train loss: 130.8910,	0.8293 s / batch. (data: 1.05e-02). ETA=7:54:36, max mem: 20.9 GB 
[11/24 17:58:06 visual_prompt]: Epoch 38 / 100: avg data time: 1.32e-01, avg batch time: 0.9618, average train loss: 64.8333
[11/24 17:59:01 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3098, average loss: 91.3237
[11/24 17:59:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.00	
[11/24 17:59:01 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[11/24 18:00:40 visual_prompt]: 	Training 100/553. train loss: 112.4012,	0.8240 s / batch. (data: 2.88e-04). ETA=7:49:28, max mem: 20.9 GB 
[11/24 18:02:21 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8198 s / batch. (data: 2.53e-04). ETA=7:45:42, max mem: 20.9 GB 
[11/24 18:04:01 visual_prompt]: 	Training 300/553. train loss: 64.3278,	0.8310 s / batch. (data: 2.94e-04). ETA=7:50:43, max mem: 20.9 GB 
[11/24 18:05:35 visual_prompt]: 	Training 400/553. train loss: 135.3309,	0.8249 s / batch. (data: 1.40e-03). ETA=7:45:51, max mem: 20.9 GB 
[11/24 18:07:13 visual_prompt]: 	Training 500/553. train loss: 7.7295,	1.5228 s / batch. (data: 6.88e-01). ETA=14:17:29, max mem: 20.9 GB 
[11/24 18:08:03 visual_prompt]: Epoch 39 / 100: avg data time: 1.50e-01, avg batch time: 0.9790, average train loss: 61.6084
[11/24 18:08:58 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3099, average loss: 42.6650
[11/24 18:08:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.31	
[11/24 18:08:58 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[11/24 18:10:40 visual_prompt]: 	Training 100/553. train loss: 63.2694,	0.8165 s / batch. (data: 2.89e-04). ETA=7:37:40, max mem: 20.9 GB 
[11/24 18:12:15 visual_prompt]: 	Training 200/553. train loss: 65.0934,	0.8275 s / batch. (data: 3.04e-04). ETA=7:42:28, max mem: 20.9 GB 
[11/24 18:13:53 visual_prompt]: 	Training 300/553. train loss: 29.2789,	0.8328 s / batch. (data: 5.88e-03). ETA=7:44:04, max mem: 20.9 GB 
[11/24 18:15:30 visual_prompt]: 	Training 400/553. train loss: 25.7974,	0.8362 s / batch. (data: 2.99e-04). ETA=7:44:33, max mem: 20.9 GB 
[11/24 18:17:06 visual_prompt]: 	Training 500/553. train loss: 54.5019,	0.8338 s / batch. (data: 1.59e-02). ETA=7:41:50, max mem: 20.9 GB 
[11/24 18:17:59 visual_prompt]: Epoch 40 / 100: avg data time: 1.48e-01, avg batch time: 0.9775, average train loss: 62.1924
[11/24 18:18:54 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3103, average loss: 38.1881
[11/24 18:18:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.34	
[11/24 18:18:54 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[11/24 18:20:38 visual_prompt]: 	Training 100/553. train loss: 21.5577,	0.8240 s / batch. (data: 8.03e-03). ETA=7:34:19, max mem: 20.9 GB 
[11/24 18:22:16 visual_prompt]: 	Training 200/553. train loss: 70.5236,	0.8400 s / batch. (data: 7.83e-04). ETA=7:41:43, max mem: 20.9 GB 
[11/24 18:23:51 visual_prompt]: 	Training 300/553. train loss: 87.8134,	0.8283 s / batch. (data: 5.42e-03). ETA=7:33:53, max mem: 20.9 GB 
[11/24 18:25:26 visual_prompt]: 	Training 400/553. train loss: 35.0689,	0.8373 s / batch. (data: 2.97e-04). ETA=7:37:26, max mem: 20.9 GB 
[11/24 18:27:00 visual_prompt]: 	Training 500/553. train loss: 11.2892,	0.8255 s / batch. (data: 2.67e-04). ETA=7:29:35, max mem: 20.9 GB 
[11/24 18:27:48 visual_prompt]: Epoch 41 / 100: avg data time: 1.37e-01, avg batch time: 0.9654, average train loss: 69.5526
[11/24 18:28:42 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3121, average loss: 61.6427
[11/24 18:28:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.02	
[11/24 18:28:42 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[11/24 18:30:19 visual_prompt]: 	Training 100/553. train loss: 53.7447,	0.8247 s / batch. (data: 1.55e-02). ETA=7:27:03, max mem: 20.9 GB 
[11/24 18:31:55 visual_prompt]: 	Training 200/553. train loss: 100.2336,	0.8587 s / batch. (data: 2.26e-02). ETA=7:44:04, max mem: 20.9 GB 
[11/24 18:33:29 visual_prompt]: 	Training 300/553. train loss: 77.9023,	0.8480 s / batch. (data: 2.88e-04). ETA=7:36:53, max mem: 20.9 GB 
[11/24 18:35:05 visual_prompt]: 	Training 400/553. train loss: 73.5132,	0.8158 s / batch. (data: 2.88e-04). ETA=7:18:10, max mem: 20.9 GB 
[11/24 18:36:39 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8039 s / batch. (data: 4.28e-04). ETA=7:10:28, max mem: 20.9 GB 
[11/24 18:37:29 visual_prompt]: Epoch 42 / 100: avg data time: 1.25e-01, avg batch time: 0.9534, average train loss: 63.3192
[11/24 18:38:23 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3095, average loss: 107.2634
[11/24 18:38:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.22	
[11/24 18:38:23 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[11/24 18:40:04 visual_prompt]: 	Training 100/553. train loss: 24.1858,	0.8242 s / batch. (data: 7.96e-03). ETA=7:19:13, max mem: 20.9 GB 
[11/24 18:41:38 visual_prompt]: 	Training 200/553. train loss: 55.2119,	0.8325 s / batch. (data: 3.19e-04). ETA=7:22:13, max mem: 20.9 GB 
[11/24 18:43:11 visual_prompt]: 	Training 300/553. train loss: 117.6169,	0.8396 s / batch. (data: 7.96e-03). ETA=7:24:38, max mem: 20.9 GB 
[11/24 18:44:44 visual_prompt]: 	Training 400/553. train loss: 16.0340,	0.8400 s / batch. (data: 3.25e-04). ETA=7:23:26, max mem: 20.9 GB 
[11/24 18:46:21 visual_prompt]: 	Training 500/553. train loss: 111.6719,	0.8321 s / batch. (data: 5.42e-03). ETA=7:17:52, max mem: 20.9 GB 
[11/24 18:47:12 visual_prompt]: Epoch 43 / 100: avg data time: 1.27e-01, avg batch time: 0.9562, average train loss: 55.8341
[11/24 18:48:06 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3087, average loss: 21.4282
[11/24 18:48:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.69	
[11/24 18:48:06 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[11/24 18:49:45 visual_prompt]: 	Training 100/553. train loss: 34.7514,	0.9783 s / batch. (data: 1.71e-01). ETA=8:32:19, max mem: 20.9 GB 
[11/24 18:51:22 visual_prompt]: 	Training 200/553. train loss: 6.3591,	0.8340 s / batch. (data: 6.94e-03). ETA=7:15:21, max mem: 20.9 GB 
[11/24 18:52:55 visual_prompt]: 	Training 300/553. train loss: 30.0497,	0.8243 s / batch. (data: 3.49e-04). ETA=7:08:56, max mem: 20.9 GB 
[11/24 18:54:29 visual_prompt]: 	Training 400/553. train loss: 16.9997,	0.8383 s / batch. (data: 1.20e-02). ETA=7:14:47, max mem: 20.9 GB 
[11/24 18:56:04 visual_prompt]: 	Training 500/553. train loss: 52.5224,	0.8520 s / batch. (data: 7.97e-03). ETA=7:20:30, max mem: 20.9 GB 
[11/24 18:56:54 visual_prompt]: Epoch 44 / 100: avg data time: 1.25e-01, avg batch time: 0.9540, average train loss: 64.4375
[11/24 18:57:48 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3103, average loss: 77.4245
[11/24 18:57:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.14	
[11/24 18:57:48 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[11/24 18:59:28 visual_prompt]: 	Training 100/553. train loss: 105.2987,	0.8176 s / batch. (data: 8.67e-03). ETA=7:00:39, max mem: 20.9 GB 
[11/24 19:01:00 visual_prompt]: 	Training 200/553. train loss: 50.7398,	0.8352 s / batch. (data: 5.41e-03). ETA=7:08:16, max mem: 20.9 GB 
[11/24 19:02:36 visual_prompt]: 	Training 300/553. train loss: 96.7920,	0.8114 s / batch. (data: 3.25e-04). ETA=6:54:43, max mem: 20.9 GB 
[11/24 19:04:10 visual_prompt]: 	Training 400/553. train loss: 26.5873,	0.8449 s / batch. (data: 1.99e-02). ETA=7:10:26, max mem: 20.9 GB 
[11/24 19:05:47 visual_prompt]: 	Training 500/553. train loss: 0.0645,	0.8204 s / batch. (data: 3.18e-04). ETA=6:56:35, max mem: 20.9 GB 
[11/24 19:06:36 visual_prompt]: Epoch 45 / 100: avg data time: 1.25e-01, avg batch time: 0.9545, average train loss: 59.2357
[11/24 19:07:30 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3096, average loss: 1.0666
[11/24 19:07:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.47	
[11/24 19:07:30 visual_prompt]: Best epoch 45: best metric: -1.067
[11/24 19:07:30 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[11/24 19:09:10 visual_prompt]: 	Training 100/553. train loss: 80.2868,	1.3092 s / batch. (data: 4.50e-01). ETA=11:01:29, max mem: 20.9 GB 
[11/24 19:10:46 visual_prompt]: 	Training 200/553. train loss: 23.6908,	0.8598 s / batch. (data: 2.99e-04). ETA=7:13:00, max mem: 20.9 GB 
[11/24 19:12:20 visual_prompt]: 	Training 300/553. train loss: 64.1164,	0.8267 s / batch. (data: 7.96e-03). ETA=6:54:55, max mem: 20.9 GB 
[11/24 19:13:55 visual_prompt]: 	Training 400/553. train loss: 27.6399,	0.8443 s / batch. (data: 5.40e-03). ETA=7:02:20, max mem: 20.9 GB 
[11/24 19:15:28 visual_prompt]: 	Training 500/553. train loss: 203.7790,	0.8240 s / batch. (data: 2.99e-04). ETA=6:50:49, max mem: 20.9 GB 
[11/24 19:16:19 visual_prompt]: Epoch 46 / 100: avg data time: 1.29e-01, avg batch time: 0.9567, average train loss: 58.7898
[11/24 19:17:13 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3101, average loss: 58.1680
[11/24 19:17:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.47	
[11/24 19:17:13 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[11/24 19:18:54 visual_prompt]: 	Training 100/553. train loss: 102.3112,	0.8310 s / batch. (data: 1.04e-02). ETA=6:52:11, max mem: 20.9 GB 
[11/24 19:20:25 visual_prompt]: 	Training 200/553. train loss: 39.0308,	1.3164 s / batch. (data: 4.86e-01). ETA=10:50:46, max mem: 20.9 GB 
[11/24 19:22:00 visual_prompt]: 	Training 300/553. train loss: 28.9458,	0.8280 s / batch. (data: 2.97e-04). ETA=6:47:58, max mem: 20.9 GB 
[11/24 19:23:36 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8417 s / batch. (data: 1.05e-02). ETA=6:53:18, max mem: 20.9 GB 
[11/24 19:25:10 visual_prompt]: 	Training 500/553. train loss: 123.2391,	0.8194 s / batch. (data: 2.95e-04). ETA=6:41:00, max mem: 20.9 GB 
[11/24 19:26:01 visual_prompt]: Epoch 47 / 100: avg data time: 1.25e-01, avg batch time: 0.9539, average train loss: 56.4288
[11/24 19:26:55 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3105, average loss: 65.2392
[11/24 19:26:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.92	
[11/24 19:26:55 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[11/24 19:28:34 visual_prompt]: 	Training 100/553. train loss: 27.8295,	0.8377 s / batch. (data: 2.91e-04). ETA=6:47:49, max mem: 20.9 GB 
[11/24 19:30:10 visual_prompt]: 	Training 200/553. train loss: 6.9854,	0.8492 s / batch. (data: 3.29e-02). ETA=6:51:59, max mem: 20.9 GB 
[11/24 19:31:46 visual_prompt]: 	Training 300/553. train loss: 101.3854,	1.6680 s / batch. (data: 8.43e-01). ETA=13:26:25, max mem: 20.9 GB 
[11/24 19:33:18 visual_prompt]: 	Training 400/553. train loss: 37.9694,	0.8322 s / batch. (data: 3.24e-04). ETA=6:40:59, max mem: 20.9 GB 
[11/24 19:34:54 visual_prompt]: 	Training 500/553. train loss: 114.5325,	0.8259 s / batch. (data: 1.21e-02). ETA=6:36:32, max mem: 20.9 GB 
[11/24 19:35:43 visual_prompt]: Epoch 48 / 100: avg data time: 1.25e-01, avg batch time: 0.9541, average train loss: 55.6506
[11/24 19:36:36 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3102, average loss: 94.5673
[11/24 19:36:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.57	
[11/24 19:36:36 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[11/24 19:38:15 visual_prompt]: 	Training 100/553. train loss: 48.2335,	0.8225 s / batch. (data: 8.09e-04). ETA=6:32:49, max mem: 20.9 GB 
[11/24 19:39:49 visual_prompt]: 	Training 200/553. train loss: 8.1025,	0.8559 s / batch. (data: 2.07e-02). ETA=6:47:20, max mem: 20.9 GB 
[11/24 19:41:24 visual_prompt]: 	Training 300/553. train loss: 48.0971,	0.8520 s / batch. (data: 2.93e-04). ETA=6:44:04, max mem: 20.9 GB 
[11/24 19:43:02 visual_prompt]: 	Training 400/553. train loss: 18.1919,	0.8519 s / batch. (data: 5.86e-03). ETA=6:42:37, max mem: 20.9 GB 
[11/24 19:44:38 visual_prompt]: 	Training 500/553. train loss: 20.7298,	0.8208 s / batch. (data: 8.08e-04). ETA=6:26:33, max mem: 20.9 GB 
[11/24 19:45:29 visual_prompt]: Epoch 49 / 100: avg data time: 1.33e-01, avg batch time: 0.9619, average train loss: 51.4502
[11/24 19:46:23 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3097, average loss: 7.3284
[11/24 19:46:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.43	
[11/24 19:46:23 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[11/24 19:48:03 visual_prompt]: 	Training 100/553. train loss: 19.3923,	0.8357 s / batch. (data: 9.22e-03). ETA=6:31:24, max mem: 20.9 GB 
[11/24 19:49:38 visual_prompt]: 	Training 200/553. train loss: 46.2160,	0.8528 s / batch. (data: 1.27e-02). ETA=6:38:00, max mem: 20.9 GB 
[11/24 19:51:12 visual_prompt]: 	Training 300/553. train loss: 103.4884,	0.8190 s / batch. (data: 7.11e-04). ETA=6:20:51, max mem: 20.9 GB 
[11/24 19:52:46 visual_prompt]: 	Training 400/553. train loss: 9.5271,	0.8640 s / batch. (data: 2.99e-04). ETA=6:40:21, max mem: 20.9 GB 
[11/24 19:54:21 visual_prompt]: 	Training 500/553. train loss: 27.6025,	0.8441 s / batch. (data: 2.91e-04). ETA=6:29:44, max mem: 20.9 GB 
[11/24 19:55:11 visual_prompt]: Epoch 50 / 100: avg data time: 1.25e-01, avg batch time: 0.9543, average train loss: 48.2504
[11/24 19:56:05 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3109, average loss: 77.0873
[11/24 19:56:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.27	
[11/24 19:56:05 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[11/24 19:57:44 visual_prompt]: 	Training 100/553. train loss: 23.4318,	1.0800 s / batch. (data: 2.27e-01). ETA=8:15:53, max mem: 20.9 GB 
[11/24 19:59:19 visual_prompt]: 	Training 200/553. train loss: 20.1287,	0.8489 s / batch. (data: 1.55e-02). ETA=6:28:23, max mem: 20.9 GB 
[11/24 20:00:55 visual_prompt]: 	Training 300/553. train loss: 22.4726,	0.8424 s / batch. (data: 7.96e-03). ETA=6:23:58, max mem: 20.9 GB 
[11/24 20:02:30 visual_prompt]: 	Training 400/553. train loss: 32.5742,	1.3118 s / batch. (data: 4.89e-01). ETA=9:55:45, max mem: 20.9 GB 
[11/24 20:04:05 visual_prompt]: 	Training 500/553. train loss: 2.4241,	0.8476 s / batch. (data: 2.55e-04). ETA=6:23:33, max mem: 20.9 GB 
[11/24 20:04:53 visual_prompt]: Epoch 51 / 100: avg data time: 1.25e-01, avg batch time: 0.9552, average train loss: 43.6193
[11/24 20:05:47 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3102, average loss: 26.9971
[11/24 20:05:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.01	
[11/24 20:05:47 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[11/24 20:07:30 visual_prompt]: 	Training 100/553. train loss: 113.3093,	0.8481 s / batch. (data: 7.84e-04). ETA=6:21:34, max mem: 20.9 GB 
[11/24 20:09:04 visual_prompt]: 	Training 200/553. train loss: 1.6175,	0.8508 s / batch. (data: 6.74e-03). ETA=6:21:23, max mem: 20.9 GB 
[11/24 20:10:39 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8357 s / batch. (data: 2.84e-04). ETA=6:13:13, max mem: 20.9 GB 
[11/24 20:12:16 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8281 s / batch. (data: 2.42e-04). ETA=6:08:27, max mem: 20.9 GB 
[11/24 20:13:47 visual_prompt]: 	Training 500/553. train loss: 30.4923,	0.8349 s / batch. (data: 1.60e-02). ETA=6:10:04, max mem: 20.9 GB 
[11/24 20:14:35 visual_prompt]: Epoch 52 / 100: avg data time: 1.26e-01, avg batch time: 0.9550, average train loss: 49.8677
[11/24 20:15:29 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3120, average loss: 289.2762
[11/24 20:15:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.61	
[11/24 20:15:29 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[11/24 20:17:08 visual_prompt]: 	Training 100/553. train loss: 97.3026,	0.8266 s / batch. (data: 2.88e-04). ETA=6:04:19, max mem: 20.9 GB 
[11/24 20:18:43 visual_prompt]: 	Training 200/553. train loss: 114.2686,	0.8440 s / batch. (data: 7.55e-04). ETA=6:10:35, max mem: 20.9 GB 
[11/24 20:20:18 visual_prompt]: 	Training 300/553. train loss: 64.3623,	0.8197 s / batch. (data: 7.53e-04). ETA=5:58:32, max mem: 20.9 GB 
[11/24 20:21:54 visual_prompt]: 	Training 400/553. train loss: 14.2451,	0.8280 s / batch. (data: 2.93e-04). ETA=6:00:47, max mem: 20.9 GB 
[11/24 20:23:28 visual_prompt]: 	Training 500/553. train loss: 49.2907,	0.8210 s / batch. (data: 2.83e-04). ETA=5:56:22, max mem: 20.9 GB 
[11/24 20:24:19 visual_prompt]: Epoch 53 / 100: avg data time: 1.28e-01, avg batch time: 0.9574, average train loss: 45.0775
[11/24 20:25:13 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3102, average loss: 70.0438
[11/24 20:25:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.39	
[11/24 20:25:13 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[11/24 20:26:53 visual_prompt]: 	Training 100/553. train loss: 3.6098,	0.8440 s / batch. (data: 2.99e-04). ETA=6:04:12, max mem: 20.9 GB 
[11/24 20:28:29 visual_prompt]: 	Training 200/553. train loss: 99.3682,	0.8205 s / batch. (data: 2.91e-04). ETA=5:52:40, max mem: 20.9 GB 
[11/24 20:30:04 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8120 s / batch. (data: 2.92e-04). ETA=5:47:41, max mem: 20.9 GB 
[11/24 20:31:39 visual_prompt]: 	Training 400/553. train loss: 204.8930,	0.8588 s / batch. (data: 6.74e-03). ETA=6:06:17, max mem: 20.9 GB 
[11/24 20:33:15 visual_prompt]: 	Training 500/553. train loss: 23.8333,	0.8258 s / batch. (data: 7.71e-04). ETA=5:50:50, max mem: 20.9 GB 
[11/24 20:34:05 visual_prompt]: Epoch 54 / 100: avg data time: 1.34e-01, avg batch time: 0.9626, average train loss: 49.6782
[11/24 20:34:59 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3108, average loss: 161.1635
[11/24 20:34:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.30	
[11/24 20:34:59 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[11/24 20:36:37 visual_prompt]: 	Training 100/553. train loss: 3.1165,	0.8860 s / batch. (data: 5.85e-02). ETA=6:14:08, max mem: 20.9 GB 
[11/24 20:38:12 visual_prompt]: 	Training 200/553. train loss: 58.2722,	0.8345 s / batch. (data: 5.44e-03). ETA=5:51:00, max mem: 20.9 GB 
[11/24 20:39:46 visual_prompt]: 	Training 300/553. train loss: 69.6328,	0.8280 s / batch. (data: 3.08e-04). ETA=5:46:54, max mem: 20.9 GB 
[11/24 20:41:21 visual_prompt]: 	Training 400/553. train loss: 46.1825,	0.8110 s / batch. (data: 3.13e-04). ETA=5:38:25, max mem: 20.9 GB 
[11/24 20:42:56 visual_prompt]: 	Training 500/553. train loss: 38.0969,	0.8200 s / batch. (data: 2.92e-04). ETA=5:40:49, max mem: 20.9 GB 
[11/24 20:43:46 visual_prompt]: Epoch 55 / 100: avg data time: 1.23e-01, avg batch time: 0.9529, average train loss: 44.1909
[11/24 20:44:41 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3091, average loss: 23.5711
[11/24 20:44:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.92	
[11/24 20:44:41 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[11/24 20:46:21 visual_prompt]: 	Training 100/553. train loss: 30.3800,	0.8190 s / batch. (data: 2.95e-04). ETA=5:38:19, max mem: 20.9 GB 
[11/24 20:47:56 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8292 s / batch. (data: 2.50e-04). ETA=5:41:08, max mem: 20.9 GB 
[11/24 20:49:33 visual_prompt]: 	Training 300/553. train loss: 8.7538,	0.8343 s / batch. (data: 7.46e-04). ETA=5:41:50, max mem: 20.9 GB 
[11/24 20:51:09 visual_prompt]: 	Training 400/553. train loss: 65.1911,	0.8532 s / batch. (data: 1.55e-02). ETA=5:48:10, max mem: 20.9 GB 
[11/24 20:52:45 visual_prompt]: 	Training 500/553. train loss: 43.9096,	2.0800 s / batch. (data: 1.26e+00). ETA=14:05:21, max mem: 20.9 GB 
[11/24 20:53:33 visual_prompt]: Epoch 56 / 100: avg data time: 1.32e-01, avg batch time: 0.9625, average train loss: 35.5305
[11/24 20:54:27 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3110, average loss: 11.8125
[11/24 20:54:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.58	
[11/24 20:54:27 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[11/24 20:56:09 visual_prompt]: 	Training 100/553. train loss: 25.2461,	0.8516 s / batch. (data: 8.69e-04). ETA=5:43:56, max mem: 20.9 GB 
[11/24 20:57:43 visual_prompt]: 	Training 200/553. train loss: 53.7476,	0.8458 s / batch. (data: 5.89e-03). ETA=5:40:10, max mem: 20.9 GB 
[11/24 20:59:17 visual_prompt]: 	Training 300/553. train loss: 718.4894,	0.8739 s / batch. (data: 1.55e-02). ETA=5:50:02, max mem: 20.9 GB 
[11/24 21:00:51 visual_prompt]: 	Training 400/553. train loss: 99.0629,	0.8131 s / batch. (data: 2.78e-04). ETA=5:24:19, max mem: 20.9 GB 
[11/24 21:02:24 visual_prompt]: 	Training 500/553. train loss: 34.1063,	0.8309 s / batch. (data: 2.52e-04). ETA=5:30:02, max mem: 20.9 GB 
[11/24 21:03:15 visual_prompt]: Epoch 57 / 100: avg data time: 1.26e-01, avg batch time: 0.9552, average train loss: 42.7073
[11/24 21:04:10 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3091, average loss: 34.4639
[11/24 21:04:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.28	
[11/24 21:04:10 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[11/24 21:05:49 visual_prompt]: 	Training 100/553. train loss: 19.3993,	1.0320 s / batch. (data: 1.78e-01). ETA=6:47:16, max mem: 20.9 GB 
[11/24 21:07:25 visual_prompt]: 	Training 200/553. train loss: 6.9843,	0.8480 s / batch. (data: 7.95e-03). ETA=5:33:15, max mem: 20.9 GB 
[11/24 21:09:04 visual_prompt]: 	Training 300/553. train loss: 7.0246,	0.8283 s / batch. (data: 5.40e-03). ETA=5:24:07, max mem: 20.9 GB 
[11/24 21:10:39 visual_prompt]: 	Training 400/553. train loss: 31.5897,	0.8400 s / batch. (data: 7.89e-04). ETA=5:27:18, max mem: 20.9 GB 
[11/24 21:12:14 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8200 s / batch. (data: 8.63e-04). ETA=5:18:08, max mem: 20.9 GB 
[11/24 21:13:03 visual_prompt]: Epoch 58 / 100: avg data time: 1.34e-01, avg batch time: 0.9646, average train loss: 37.0176
[11/24 21:13:58 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3108, average loss: 6.4688
[11/24 21:13:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.49	
[11/24 21:13:58 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[11/24 21:15:39 visual_prompt]: 	Training 100/553. train loss: 117.1536,	0.8151 s / batch. (data: 2.50e-04). ETA=5:14:09, max mem: 20.9 GB 
[11/24 21:17:15 visual_prompt]: 	Training 200/553. train loss: 103.0690,	0.8395 s / batch. (data: 3.31e-04). ETA=5:22:11, max mem: 20.9 GB 
[11/24 21:18:50 visual_prompt]: 	Training 300/553. train loss: 68.1451,	0.8239 s / batch. (data: 5.39e-03). ETA=5:14:47, max mem: 20.9 GB 
[11/24 21:20:25 visual_prompt]: 	Training 400/553. train loss: 17.8920,	0.8155 s / batch. (data: 3.15e-04). ETA=5:10:14, max mem: 20.9 GB 
[11/24 21:22:03 visual_prompt]: 	Training 500/553. train loss: 49.7379,	0.8310 s / batch. (data: 7.55e-04). ETA=5:14:44, max mem: 20.9 GB 
[11/24 21:22:51 visual_prompt]: Epoch 59 / 100: avg data time: 1.33e-01, avg batch time: 0.9641, average train loss: 41.9175
[11/24 21:23:45 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3101, average loss: 22.8153
[11/24 21:23:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.15	
[11/24 21:23:45 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[11/24 21:25:26 visual_prompt]: 	Training 100/553. train loss: 13.0365,	0.8400 s / batch. (data: 2.96e-04). ETA=5:16:00, max mem: 20.9 GB 
[11/24 21:27:02 visual_prompt]: 	Training 200/553. train loss: 14.9202,	0.8500 s / batch. (data: 9.99e-03). ETA=5:18:23, max mem: 20.9 GB 
[11/24 21:28:35 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.9320 s / batch. (data: 1.16e-01). ETA=5:47:31, max mem: 20.9 GB 
[11/24 21:30:13 visual_prompt]: 	Training 400/553. train loss: 25.1963,	1.0636 s / batch. (data: 2.03e-01). ETA=6:34:48, max mem: 20.9 GB 
[11/24 21:31:49 visual_prompt]: 	Training 500/553. train loss: 27.4358,	0.8395 s / batch. (data: 3.01e-04). ETA=5:10:13, max mem: 20.9 GB 
[11/24 21:32:40 visual_prompt]: Epoch 60 / 100: avg data time: 1.35e-01, avg batch time: 0.9655, average train loss: 36.9652
[11/24 21:33:34 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3103, average loss: 61.2489
[11/24 21:33:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.20	
[11/24 21:33:34 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[11/24 21:35:15 visual_prompt]: 	Training 100/553. train loss: 16.1770,	0.8257 s / batch. (data: 3.17e-04). ETA=5:03:01, max mem: 20.9 GB 
[11/24 21:36:52 visual_prompt]: 	Training 200/553. train loss: 13.1755,	1.7000 s / batch. (data: 8.68e-01). ETA=10:21:03, max mem: 20.9 GB 
[11/24 21:38:28 visual_prompt]: 	Training 300/553. train loss: 22.3110,	1.3053 s / batch. (data: 4.99e-01). ETA=7:54:42, max mem: 20.9 GB 
[11/24 21:40:01 visual_prompt]: 	Training 400/553. train loss: 9.8325,	0.8320 s / batch. (data: 5.70e-04). ETA=5:01:09, max mem: 20.9 GB 
[11/24 21:41:38 visual_prompt]: 	Training 500/553. train loss: 47.2574,	2.4073 s / batch. (data: 1.60e+00). ETA=14:27:26, max mem: 20.9 GB 
[11/24 21:42:26 visual_prompt]: Epoch 61 / 100: avg data time: 1.31e-01, avg batch time: 0.9619, average train loss: 31.4750
[11/24 21:43:21 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3104, average loss: 10.8324
[11/24 21:43:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.75	
[11/24 21:43:21 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[11/24 21:45:01 visual_prompt]: 	Training 100/553. train loss: 6.1677,	0.8440 s / batch. (data: 3.05e-04). ETA=5:01:57, max mem: 20.9 GB 
[11/24 21:46:37 visual_prompt]: 	Training 200/553. train loss: 87.6317,	0.8324 s / batch. (data: 2.84e-04). ETA=4:56:25, max mem: 20.9 GB 
[11/24 21:48:11 visual_prompt]: 	Training 300/553. train loss: 35.6326,	0.8484 s / batch. (data: 8.37e-03). ETA=5:00:43, max mem: 20.9 GB 
[11/24 21:49:47 visual_prompt]: 	Training 400/553. train loss: 1.8927,	0.8211 s / batch. (data: 2.80e-04). ETA=4:49:40, max mem: 20.9 GB 
[11/24 21:51:22 visual_prompt]: 	Training 500/553. train loss: 44.4709,	0.8240 s / batch. (data: 3.00e-04). ETA=4:49:19, max mem: 20.9 GB 
[11/24 21:52:14 visual_prompt]: Epoch 62 / 100: avg data time: 1.33e-01, avg batch time: 0.9632, average train loss: 34.4736
[11/24 21:53:08 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3099, average loss: 37.4871
[11/24 21:53:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.07	
[11/24 21:53:08 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[11/24 21:54:51 visual_prompt]: 	Training 100/553. train loss: 11.6051,	0.8335 s / batch. (data: 2.77e-04). ETA=4:50:31, max mem: 20.9 GB 
[11/24 21:56:30 visual_prompt]: 	Training 200/553. train loss: 23.2807,	0.8239 s / batch. (data: 3.80e-04). ETA=4:45:48, max mem: 20.9 GB 
[11/24 21:58:05 visual_prompt]: 	Training 300/553. train loss: 17.6828,	0.8557 s / batch. (data: 1.16e-02). ETA=4:55:24, max mem: 20.9 GB 
[11/24 21:59:37 visual_prompt]: 	Training 400/553. train loss: 5.7767,	0.8177 s / batch. (data: 2.82e-04). ETA=4:40:57, max mem: 20.9 GB 
[11/24 22:01:11 visual_prompt]: 	Training 500/553. train loss: 19.2352,	0.8400 s / batch. (data: 2.92e-04). ETA=4:47:12, max mem: 20.9 GB 
[11/24 22:02:00 visual_prompt]: Epoch 63 / 100: avg data time: 1.30e-01, avg batch time: 0.9616, average train loss: 30.2110
[11/24 22:02:55 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3106, average loss: 11.3012
[11/24 22:02:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.90	
[11/24 22:02:55 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[11/24 22:04:37 visual_prompt]: 	Training 100/553. train loss: 34.9543,	0.8403 s / batch. (data: 1.10e-02). ETA=4:45:09, max mem: 20.9 GB 
[11/24 22:06:14 visual_prompt]: 	Training 200/553. train loss: 13.7656,	0.8179 s / batch. (data: 2.98e-04). ETA=4:36:10, max mem: 20.9 GB 
[11/24 22:07:47 visual_prompt]: 	Training 300/553. train loss: 339.8531,	0.8360 s / batch. (data: 2.83e-04). ETA=4:40:54, max mem: 20.9 GB 
[11/24 22:09:23 visual_prompt]: 	Training 400/553. train loss: 15.6826,	0.8480 s / batch. (data: 1.20e-02). ETA=4:43:32, max mem: 20.9 GB 
[11/24 22:10:59 visual_prompt]: 	Training 500/553. train loss: 1.9167,	0.8679 s / batch. (data: 1.20e-02). ETA=4:48:44, max mem: 20.9 GB 
[11/24 22:11:48 visual_prompt]: Epoch 64 / 100: avg data time: 1.33e-01, avg batch time: 0.9648, average train loss: 33.4037
[11/24 22:12:43 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3108, average loss: 21.2460
[11/24 22:12:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.50	
[11/24 22:12:43 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[11/24 22:14:27 visual_prompt]: 	Training 100/553. train loss: 17.4793,	0.8426 s / batch. (data: 1.56e-02). ETA=4:38:10, max mem: 20.9 GB 
[11/24 22:16:04 visual_prompt]: 	Training 200/553. train loss: 8.7240,	1.1744 s / batch. (data: 3.44e-01). ETA=6:25:46, max mem: 20.9 GB 
[11/24 22:17:37 visual_prompt]: 	Training 300/553. train loss: 23.7983,	0.9200 s / batch. (data: 9.11e-02). ETA=5:00:39, max mem: 20.9 GB 
[11/24 22:19:13 visual_prompt]: 	Training 400/553. train loss: 3.0781,	0.8520 s / batch. (data: 7.88e-04). ETA=4:37:00, max mem: 20.9 GB 
[11/24 22:20:48 visual_prompt]: 	Training 500/553. train loss: 0.3122,	0.8440 s / batch. (data: 7.95e-03). ETA=4:33:00, max mem: 20.9 GB 
[11/24 22:21:36 visual_prompt]: Epoch 65 / 100: avg data time: 1.33e-01, avg batch time: 0.9638, average train loss: 28.8003
[11/24 22:22:31 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3104, average loss: 39.8656
[11/24 22:22:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.74	
[11/24 22:22:31 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[11/24 22:24:10 visual_prompt]: 	Training 100/553. train loss: 17.0419,	0.8208 s / batch. (data: 2.81e-04). ETA=4:23:24, max mem: 20.9 GB 
[11/24 22:25:46 visual_prompt]: 	Training 200/553. train loss: 10.1598,	1.4042 s / batch. (data: 5.70e-01). ETA=7:28:17, max mem: 20.9 GB 
[11/24 22:27:24 visual_prompt]: 	Training 300/553. train loss: 11.5037,	0.8440 s / batch. (data: 3.08e-04). ETA=4:28:02, max mem: 20.9 GB 
[11/24 22:28:58 visual_prompt]: 	Training 400/553. train loss: 10.3658,	0.8407 s / batch. (data: 1.09e-02). ETA=4:25:35, max mem: 20.9 GB 
[11/24 22:30:33 visual_prompt]: 	Training 500/553. train loss: 13.5958,	0.8359 s / batch. (data: 3.20e-04). ETA=4:22:41, max mem: 20.9 GB 
[11/24 22:31:25 visual_prompt]: Epoch 66 / 100: avg data time: 1.34e-01, avg batch time: 0.9659, average train loss: 27.3825
[11/24 22:32:20 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3098, average loss: 69.7469
[11/24 22:32:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.67	
[11/24 22:32:20 visual_prompt]: Stopping early.
[11/24 22:32:20 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 22:32:20 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 22:32:20 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/24 22:32:20 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 22:32:20 visual_prompt]: Training with config:
[11/24 22:32:20 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/24 22:32:20 visual_prompt]: Loading training data...
[11/24 22:32:20 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 22:32:20 visual_prompt]: Loading validation data...
[11/24 22:32:20 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 22:32:20 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 22:32:22 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 22:32:22 visual_prompt]: tuned percent:0.525
[11/24 22:32:22 visual_prompt]: Device used for model: 0
[11/24 22:32:22 visual_prompt]: Setting up Evaluator...
[11/24 22:32:22 visual_prompt]: Setting up Trainer...
[11/24 22:32:22 visual_prompt]: 	Setting up the optimizer...
[11/24 22:32:22 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 22:34:02 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8310 s / batch. (data: 2.82e-04). ETA=12:44:33, max mem: 20.9 GB 
[11/24 22:35:37 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8360 s / batch. (data: 2.78e-04). ETA=12:47:42, max mem: 20.9 GB 
[11/24 22:37:15 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8245 s / batch. (data: 5.45e-03). ETA=12:35:45, max mem: 20.9 GB 
[11/24 22:38:49 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8350 s / batch. (data: 2.93e-04). ETA=12:44:03, max mem: 20.9 GB 
[11/24 22:40:27 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8544 s / batch. (data: 1.22e-02). ETA=13:00:22, max mem: 20.9 GB 
[11/24 22:41:18 visual_prompt]: Epoch 1 / 100: avg data time: 1.34e-01, avg batch time: 0.9682, average train loss: 1.5403
[11/24 22:42:13 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3104, average loss: 1.5201
[11/24 22:42:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 22:42:13 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/24 22:43:52 visual_prompt]: 	Training 100/553. train loss: 9.3182,	0.8544 s / batch. (data: 3.77e-02). ETA=12:58:10, max mem: 20.9 GB 
[11/24 22:45:28 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8880 s / batch. (data: 5.55e-02). ETA=13:27:18, max mem: 20.9 GB 
[11/24 22:47:06 visual_prompt]: 	Training 300/553. train loss: 5.1931,	1.1080 s / batch. (data: 2.68e-01). ETA=16:45:26, max mem: 20.9 GB 
[11/24 22:48:41 visual_prompt]: 	Training 400/553. train loss: 0.6944,	0.8576 s / batch. (data: 2.06e-02). ETA=12:56:47, max mem: 20.9 GB 
[11/24 22:50:18 visual_prompt]: 	Training 500/553. train loss: 0.5637,	0.8360 s / batch. (data: 2.99e-04). ETA=12:35:51, max mem: 20.9 GB 
[11/24 22:51:08 visual_prompt]: Epoch 2 / 100: avg data time: 1.34e-01, avg batch time: 0.9674, average train loss: 9.8902
[11/24 22:52:03 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3098, average loss: 11.6990
[11/24 22:52:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.64	
[11/24 22:52:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/24 22:53:41 visual_prompt]: 	Training 100/553. train loss: 20.6916,	0.8400 s / batch. (data: 1.19e-02). ETA=12:37:18, max mem: 20.9 GB 
[11/24 22:55:18 visual_prompt]: 	Training 200/553. train loss: 7.1333,	1.2292 s / batch. (data: 4.01e-01). ETA=18:26:07, max mem: 20.9 GB 
[11/24 22:56:53 visual_prompt]: 	Training 300/553. train loss: 9.6720,	0.8884 s / batch. (data: 2.85e-02). ETA=13:18:00, max mem: 20.9 GB 
[11/24 22:58:30 visual_prompt]: 	Training 400/553. train loss: 15.5484,	0.8184 s / batch. (data: 3.17e-04). ETA=12:13:43, max mem: 20.9 GB 
[11/24 23:00:07 visual_prompt]: 	Training 500/553. train loss: 4.5820,	1.1860 s / batch. (data: 3.63e-01). ETA=17:41:18, max mem: 20.9 GB 
[11/24 23:00:56 visual_prompt]: Epoch 3 / 100: avg data time: 1.32e-01, avg batch time: 0.9640, average train loss: 12.6822
[11/24 23:01:50 visual_prompt]: Inference (val):avg data time: 2.10e-04, avg batch time: 0.3115, average loss: 9.8754
[11/24 23:01:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.28	
[11/24 23:01:50 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/24 23:03:32 visual_prompt]: 	Training 100/553. train loss: 14.6318,	0.8300 s / batch. (data: 5.47e-03). ETA=12:20:39, max mem: 20.9 GB 
[11/24 23:05:17 visual_prompt]: 	Training 200/553. train loss: 14.6047,	0.8358 s / batch. (data: 7.88e-03). ETA=12:24:28, max mem: 20.9 GB 
[11/24 23:07:03 visual_prompt]: 	Training 300/553. train loss: 12.7162,	1.5192 s / batch. (data: 6.87e-01). ETA=22:30:35, max mem: 20.9 GB 
[11/24 23:08:40 visual_prompt]: 	Training 400/553. train loss: 4.5224,	1.4685 s / batch. (data: 6.34e-01). ETA=21:43:04, max mem: 20.9 GB 
[11/24 23:10:27 visual_prompt]: 	Training 500/553. train loss: 0.0000,	3.7050 s / batch. (data: 2.88e+00). ETA=2 days, 6:41:29, max mem: 20.9 GB 
[11/24 23:11:22 visual_prompt]: Epoch 4 / 100: avg data time: 2.01e-01, avg batch time: 1.0343, average train loss: 17.3637
[11/24 23:12:17 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3096, average loss: 93.1347
[11/24 23:12:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.46	
[11/24 23:12:17 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/24 23:13:55 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8840 s / batch. (data: 5.40e-03). ETA=13:00:43, max mem: 20.9 GB 
[11/24 23:15:30 visual_prompt]: 	Training 200/553. train loss: 6.3354,	1.0275 s / batch. (data: 2.11e-01). ETA=15:05:43, max mem: 20.9 GB 
[11/24 23:17:07 visual_prompt]: 	Training 300/553. train loss: 36.2018,	0.8395 s / batch. (data: 2.76e-04). ETA=12:18:33, max mem: 20.9 GB 
[11/24 23:18:42 visual_prompt]: 	Training 400/553. train loss: 41.4358,	0.8615 s / batch. (data: 2.93e-04). ETA=12:36:30, max mem: 20.9 GB 
[11/24 23:20:17 visual_prompt]: 	Training 500/553. train loss: 32.2609,	0.8230 s / batch. (data: 2.94e-04). ETA=12:01:19, max mem: 20.9 GB 
[11/24 23:21:07 visual_prompt]: Epoch 5 / 100: avg data time: 1.28e-01, avg batch time: 0.9595, average train loss: 27.6257
[11/24 23:22:02 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3089, average loss: 89.6345
[11/24 23:22:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.05	
[11/24 23:22:02 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/24 23:23:42 visual_prompt]: 	Training 100/553. train loss: 9.4687,	0.8400 s / batch. (data: 8.05e-04). ETA=12:14:05, max mem: 20.9 GB 
[11/24 23:25:17 visual_prompt]: 	Training 200/553. train loss: 8.8481,	0.8600 s / batch. (data: 3.07e-04). ETA=12:30:07, max mem: 20.9 GB 
[11/24 23:26:57 visual_prompt]: 	Training 300/553. train loss: 5.5799,	0.8403 s / batch. (data: 3.45e-04). ETA=12:11:31, max mem: 20.9 GB 
[11/24 23:28:48 visual_prompt]: 	Training 400/553. train loss: 35.0367,	0.8359 s / batch. (data: 1.20e-02). ETA=12:06:20, max mem: 20.9 GB 
[11/24 23:30:28 visual_prompt]: 	Training 500/553. train loss: 27.2777,	0.8973 s / batch. (data: 5.63e-02). ETA=12:58:10, max mem: 20.9 GB 
[11/24 23:31:20 visual_prompt]: Epoch 6 / 100: avg data time: 1.80e-01, avg batch time: 1.0100, average train loss: 34.5054
[11/24 23:32:21 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3102, average loss: 11.0577
[11/24 23:32:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.58	
[11/24 23:32:21 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/24 23:34:01 visual_prompt]: 	Training 100/553. train loss: 0.0337,	0.8360 s / batch. (data: 3.51e-04). ETA=12:02:52, max mem: 20.9 GB 
[11/24 23:35:38 visual_prompt]: 	Training 200/553. train loss: 22.7581,	0.8384 s / batch. (data: 1.19e-02). ETA=12:03:32, max mem: 20.9 GB 
[11/24 23:37:19 visual_prompt]: 	Training 300/553. train loss: 3.9200,	1.7360 s / batch. (data: 9.02e-01). ETA=1 day, 0:55:20, max mem: 20.9 GB 
[11/24 23:38:57 visual_prompt]: 	Training 400/553. train loss: 17.9251,	1.8250 s / batch. (data: 9.88e-01). ETA=1 day, 2:08:54, max mem: 20.9 GB 
[11/24 23:40:32 visual_prompt]: 	Training 500/553. train loss: 135.0038,	0.8239 s / batch. (data: 5.47e-03). ETA=11:46:53, max mem: 20.9 GB 
[11/24 23:41:22 visual_prompt]: Epoch 7 / 100: avg data time: 1.49e-01, avg batch time: 0.9787, average train loss: 43.3462
[11/24 23:42:18 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3096, average loss: 30.7992
[11/24 23:42:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.91	
[11/24 23:42:18 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/24 23:43:58 visual_prompt]: 	Training 100/553. train loss: 92.5026,	0.8230 s / batch. (data: 3.16e-04). ETA=11:44:01, max mem: 20.9 GB 
[11/24 23:45:37 visual_prompt]: 	Training 200/553. train loss: 133.4921,	0.8440 s / batch. (data: 3.17e-04). ETA=12:00:37, max mem: 20.9 GB 
[11/24 23:47:14 visual_prompt]: 	Training 300/553. train loss: 234.0344,	0.8200 s / batch. (data: 3.11e-04). ETA=11:38:46, max mem: 20.9 GB 
[11/24 23:48:52 visual_prompt]: 	Training 400/553. train loss: 94.3837,	0.8878 s / batch. (data: 5.83e-02). ETA=12:35:03, max mem: 20.9 GB 
[11/24 23:50:30 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.4760 s / batch. (data: 6.49e-01). ETA=20:52:52, max mem: 20.9 GB 
[11/24 23:51:21 visual_prompt]: Epoch 8 / 100: avg data time: 1.52e-01, avg batch time: 0.9819, average train loss: 73.7924
[11/24 23:52:17 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3101, average loss: 203.1180
[11/24 23:52:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.60	
[11/24 23:52:17 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/24 23:53:58 visual_prompt]: 	Training 100/553. train loss: 89.9425,	0.8280 s / batch. (data: 3.10e-04). ETA=11:40:42, max mem: 20.9 GB 
[11/24 23:55:35 visual_prompt]: 	Training 200/553. train loss: 24.2411,	0.8180 s / batch. (data: 2.91e-04). ETA=11:30:54, max mem: 20.9 GB 
[11/24 23:57:16 visual_prompt]: 	Training 300/553. train loss: 93.5788,	1.9918 s / batch. (data: 1.19e+00). ETA=1 day, 3:58:54, max mem: 20.9 GB 
[11/24 23:59:03 visual_prompt]: 	Training 400/553. train loss: 12.0334,	0.8588 s / batch. (data: 7.96e-03). ETA=12:02:30, max mem: 20.9 GB 
[11/25 00:00:47 visual_prompt]: 	Training 500/553. train loss: 3.0766,	0.9080 s / batch. (data: 5.31e-02). ETA=12:42:19, max mem: 20.9 GB 
[11/25 00:01:38 visual_prompt]: Epoch 9 / 100: avg data time: 1.84e-01, avg batch time: 1.0144, average train loss: 48.2814
[11/25 00:02:34 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3098, average loss: 42.4104
[11/25 00:02:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.11	
[11/25 00:02:34 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/25 00:04:18 visual_prompt]: 	Training 100/553. train loss: 184.7925,	0.8211 s / batch. (data: 7.34e-04). ETA=11:27:18, max mem: 20.9 GB 
[11/25 00:05:54 visual_prompt]: 	Training 200/553. train loss: 3.7531,	0.8178 s / batch. (data: 3.35e-04). ETA=11:23:12, max mem: 20.9 GB 
[11/25 00:07:31 visual_prompt]: 	Training 300/553. train loss: 19.7834,	2.7983 s / batch. (data: 1.97e+00). ETA=1 day, 14:53:01, max mem: 20.9 GB 
[11/25 00:09:07 visual_prompt]: 	Training 400/553. train loss: 71.5181,	0.8381 s / batch. (data: 8.59e-03). ETA=11:37:17, max mem: 20.9 GB 
[11/25 00:10:48 visual_prompt]: 	Training 500/553. train loss: 93.8140,	0.9413 s / batch. (data: 1.34e-01). ETA=13:01:39, max mem: 20.9 GB 
[11/25 00:11:44 visual_prompt]: Epoch 10 / 100: avg data time: 1.68e-01, avg batch time: 0.9954, average train loss: 82.9872
[11/25 00:12:45 visual_prompt]: Inference (val):avg data time: 9.32e-05, avg batch time: 0.3090, average loss: 48.1733
[11/25 00:12:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.14	
[11/25 00:12:45 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/25 00:14:34 visual_prompt]: 	Training 100/553. train loss: 151.3405,	0.8431 s / batch. (data: 5.53e-03). ETA=11:37:58, max mem: 20.9 GB 
[11/25 00:16:22 visual_prompt]: 	Training 200/553. train loss: 99.0040,	0.8361 s / batch. (data: 3.16e-04). ETA=11:30:47, max mem: 20.9 GB 
[11/25 00:18:07 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.2653 s / batch. (data: 1.44e+00). ETA=1 day, 7:07:46, max mem: 20.9 GB 
[11/25 00:19:42 visual_prompt]: 	Training 400/553. train loss: 181.2127,	0.8680 s / batch. (data: 6.81e-04). ETA=11:54:13, max mem: 20.9 GB 
[11/25 00:21:18 visual_prompt]: 	Training 500/553. train loss: 98.6965,	0.8289 s / batch. (data: 8.59e-03). ETA=11:20:38, max mem: 20.9 GB 
[11/25 00:22:09 visual_prompt]: Epoch 11 / 100: avg data time: 1.92e-01, avg batch time: 1.0189, average train loss: 72.5064
[11/25 00:23:05 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3100, average loss: 107.5733
[11/25 00:23:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.46	
[11/25 00:23:05 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/25 00:24:47 visual_prompt]: 	Training 100/553. train loss: 54.0880,	0.8091 s / batch. (data: 2.94e-04). ETA=11:02:19, max mem: 20.9 GB 
[11/25 00:26:25 visual_prompt]: 	Training 200/553. train loss: 48.0289,	0.8209 s / batch. (data: 3.23e-04). ETA=11:10:37, max mem: 20.9 GB 
[11/25 00:28:01 visual_prompt]: 	Training 300/553. train loss: 37.7816,	0.8400 s / batch. (data: 2.94e-04). ETA=11:24:51, max mem: 20.9 GB 
[11/25 00:29:39 visual_prompt]: 	Training 400/553. train loss: 92.9019,	0.8400 s / batch. (data: 3.24e-04). ETA=11:23:24, max mem: 20.9 GB 
[11/25 00:31:17 visual_prompt]: 	Training 500/553. train loss: 41.5009,	0.8400 s / batch. (data: 2.05e-02). ETA=11:22:01, max mem: 20.9 GB 
[11/25 00:32:06 visual_prompt]: Epoch 12 / 100: avg data time: 1.52e-01, avg batch time: 0.9798, average train loss: 69.8166
[11/25 00:33:02 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3094, average loss: 98.0493
[11/25 00:33:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.43	
[11/25 00:33:02 visual_prompt]: Best epoch 12: best metric: -98.049
[11/25 00:33:02 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/25 00:34:45 visual_prompt]: 	Training 100/553. train loss: 73.2958,	0.8271 s / batch. (data: 5.42e-03). ETA=11:09:28, max mem: 20.9 GB 
[11/25 00:36:28 visual_prompt]: 	Training 200/553. train loss: 69.2323,	0.8216 s / batch. (data: 3.21e-04). ETA=11:03:38, max mem: 20.9 GB 
[11/25 00:38:14 visual_prompt]: 	Training 300/553. train loss: 30.0433,	2.0219 s / batch. (data: 1.18e+00). ETA=1 day, 3:09:45, max mem: 20.9 GB 
[11/25 00:39:58 visual_prompt]: 	Training 400/553. train loss: 241.2708,	0.8412 s / batch. (data: 1.06e-02). ETA=11:16:39, max mem: 20.9 GB 
[11/25 00:41:40 visual_prompt]: 	Training 500/553. train loss: 34.8105,	0.8385 s / batch. (data: 2.92e-04). ETA=11:13:05, max mem: 20.9 GB 
[11/25 00:42:31 visual_prompt]: Epoch 13 / 100: avg data time: 2.01e-01, avg batch time: 1.0296, average train loss: 97.3977
[11/25 00:43:28 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3110, average loss: 54.5825
[11/25 00:43:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.08	
[11/25 00:43:28 visual_prompt]: Best epoch 13: best metric: -54.582
[11/25 00:43:28 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/25 00:45:10 visual_prompt]: 	Training 100/553. train loss: 58.1163,	0.8293 s / batch. (data: 2.80e-04). ETA=11:03:33, max mem: 20.9 GB 
[11/25 00:46:47 visual_prompt]: 	Training 200/553. train loss: 0.0389,	0.8480 s / batch. (data: 1.20e-02). ETA=11:17:07, max mem: 20.9 GB 
[11/25 00:48:24 visual_prompt]: 	Training 300/553. train loss: 34.0453,	0.8400 s / batch. (data: 3.24e-04). ETA=11:09:21, max mem: 20.9 GB 
[11/25 00:50:00 visual_prompt]: 	Training 400/553. train loss: 20.2472,	0.8320 s / batch. (data: 2.98e-04). ETA=11:01:34, max mem: 20.9 GB 
[11/25 00:51:37 visual_prompt]: 	Training 500/553. train loss: 164.4967,	0.8110 s / batch. (data: 3.05e-04). ETA=10:43:31, max mem: 20.9 GB 
[11/25 00:52:27 visual_prompt]: Epoch 14 / 100: avg data time: 1.46e-01, avg batch time: 0.9753, average train loss: 61.9254
[11/25 00:53:23 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3097, average loss: 27.4170
[11/25 00:53:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.04	
[11/25 00:53:23 visual_prompt]: Best epoch 14: best metric: -27.417
[11/25 00:53:23 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/25 00:55:04 visual_prompt]: 	Training 100/553. train loss: 110.0935,	0.8275 s / batch. (data: 3.51e-04). ETA=10:54:29, max mem: 20.9 GB 
[11/25 00:56:40 visual_prompt]: 	Training 200/553. train loss: 372.1039,	0.8188 s / batch. (data: 2.96e-04). ETA=10:46:17, max mem: 20.9 GB 
[11/25 00:58:20 visual_prompt]: 	Training 300/553. train loss: 5.5879,	0.8309 s / batch. (data: 7.76e-04). ETA=10:54:27, max mem: 20.9 GB 
[11/25 00:59:55 visual_prompt]: 	Training 400/553. train loss: 0.3316,	1.0480 s / batch. (data: 2.08e-01). ETA=13:43:41, max mem: 20.9 GB 
[11/25 01:01:34 visual_prompt]: 	Training 500/553. train loss: 112.7657,	0.8366 s / batch. (data: 8.50e-03). ETA=10:56:07, max mem: 20.9 GB 
[11/25 01:02:25 visual_prompt]: Epoch 15 / 100: avg data time: 1.50e-01, avg batch time: 0.9797, average train loss: 87.3216
[11/25 01:03:20 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3092, average loss: 210.5192
[11/25 01:03:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.00	
[11/25 01:03:20 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/25 01:05:00 visual_prompt]: 	Training 100/553. train loss: 16.6388,	0.8286 s / batch. (data: 2.89e-04). ETA=10:47:45, max mem: 20.9 GB 
[11/25 01:06:38 visual_prompt]: 	Training 200/553. train loss: 101.8530,	0.8399 s / batch. (data: 1.55e-02). ETA=10:55:13, max mem: 20.9 GB 
[11/25 01:08:15 visual_prompt]: 	Training 300/553. train loss: 149.5668,	0.8168 s / batch. (data: 2.79e-04). ETA=10:35:50, max mem: 20.9 GB 
[11/25 01:09:53 visual_prompt]: 	Training 400/553. train loss: 25.5012,	0.8551 s / batch. (data: 1.11e-02). ETA=11:04:09, max mem: 20.9 GB 
[11/25 01:11:29 visual_prompt]: 	Training 500/553. train loss: 156.1897,	1.0906 s / batch. (data: 2.60e-01). ETA=14:05:19, max mem: 20.9 GB 
[11/25 01:12:21 visual_prompt]: Epoch 16 / 100: avg data time: 1.50e-01, avg batch time: 0.9771, average train loss: 82.4612
[11/25 01:13:16 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3090, average loss: 25.8207
[11/25 01:13:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.73	
[11/25 01:13:16 visual_prompt]: Best epoch 16: best metric: -25.821
[11/25 01:13:16 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/25 01:14:57 visual_prompt]: 	Training 100/553. train loss: 79.3474,	0.8360 s / batch. (data: 2.90e-04). ETA=10:45:50, max mem: 20.9 GB 
[11/25 01:16:36 visual_prompt]: 	Training 200/553. train loss: 313.7220,	0.8200 s / batch. (data: 2.96e-04). ETA=10:32:05, max mem: 20.9 GB 
[11/25 01:18:13 visual_prompt]: 	Training 300/553. train loss: 178.4385,	0.8456 s / batch. (data: 1.55e-02). ETA=10:50:25, max mem: 20.9 GB 
[11/25 01:19:49 visual_prompt]: 	Training 400/553. train loss: 55.0876,	1.0850 s / batch. (data: 2.69e-01). ETA=13:52:47, max mem: 20.9 GB 
[11/25 01:21:26 visual_prompt]: 	Training 500/553. train loss: 61.1381,	1.4730 s / batch. (data: 6.63e-01). ETA=18:48:07, max mem: 20.9 GB 
[11/25 01:22:18 visual_prompt]: Epoch 17 / 100: avg data time: 1.51e-01, avg batch time: 0.9797, average train loss: 96.4472
[11/25 01:23:14 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.3114, average loss: 52.4195
[11/25 01:23:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.80	
[11/25 01:23:14 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/25 01:24:55 visual_prompt]: 	Training 100/553. train loss: 179.4604,	0.8357 s / batch. (data: 5.40e-03). ETA=10:37:53, max mem: 20.9 GB 
[11/25 01:26:35 visual_prompt]: 	Training 200/553. train loss: 6.3386,	0.8612 s / batch. (data: 2.52e-02). ETA=10:55:54, max mem: 20.9 GB 
[11/25 01:28:12 visual_prompt]: 	Training 300/553. train loss: 5.8222,	0.8599 s / batch. (data: 1.05e-02). ETA=10:53:32, max mem: 20.9 GB 
[11/25 01:29:49 visual_prompt]: 	Training 400/553. train loss: 27.2760,	0.8252 s / batch. (data: 9.24e-03). ETA=10:25:44, max mem: 20.9 GB 
[11/25 01:31:26 visual_prompt]: 	Training 500/553. train loss: 103.9776,	0.8320 s / batch. (data: 3.14e-04). ETA=10:29:32, max mem: 20.9 GB 
[11/25 01:32:16 visual_prompt]: Epoch 18 / 100: avg data time: 1.51e-01, avg batch time: 0.9790, average train loss: 108.9397
[11/25 01:33:11 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3107, average loss: 174.6840
[11/25 01:33:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.30	
[11/25 01:33:11 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/25 01:34:52 visual_prompt]: 	Training 100/553. train loss: 62.0611,	1.1855 s / batch. (data: 3.82e-01). ETA=14:54:00, max mem: 20.9 GB 
[11/25 01:36:30 visual_prompt]: 	Training 200/553. train loss: 12.6486,	0.8638 s / batch. (data: 2.94e-04). ETA=10:49:56, max mem: 20.9 GB 
[11/25 01:38:07 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8025 s / batch. (data: 2.87e-04). ETA=10:02:27, max mem: 20.9 GB 
[11/25 01:39:46 visual_prompt]: 	Training 400/553. train loss: 51.9433,	0.8201 s / batch. (data: 2.71e-04). ETA=10:14:18, max mem: 20.9 GB 
[11/25 01:41:19 visual_prompt]: 	Training 500/553. train loss: 25.7980,	0.8160 s / batch. (data: 3.19e-04). ETA=10:09:53, max mem: 20.9 GB 
[11/25 01:42:10 visual_prompt]: Epoch 19 / 100: avg data time: 1.45e-01, avg batch time: 0.9744, average train loss: 68.3429
[11/25 01:43:05 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3119, average loss: 5.8235
[11/25 01:43:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.74	
[11/25 01:43:05 visual_prompt]: Best epoch 19: best metric: -5.824
[11/25 01:43:05 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/25 01:44:45 visual_prompt]: 	Training 100/553. train loss: 9.4218,	0.8413 s / batch. (data: 3.76e-03). ETA=10:26:39, max mem: 20.9 GB 
[11/25 01:46:24 visual_prompt]: 	Training 200/553. train loss: 10.9273,	0.8600 s / batch. (data: 2.79e-02). ETA=10:39:08, max mem: 20.9 GB 
[11/25 01:48:02 visual_prompt]: 	Training 300/553. train loss: 76.0858,	0.8728 s / batch. (data: 1.10e-02). ETA=10:47:11, max mem: 20.9 GB 
[11/25 01:49:39 visual_prompt]: 	Training 400/553. train loss: 187.2953,	0.8280 s / batch. (data: 1.20e-02). ETA=10:12:37, max mem: 20.9 GB 
[11/25 01:51:15 visual_prompt]: 	Training 500/553. train loss: 45.7235,	0.8369 s / batch. (data: 4.81e-03). ETA=10:17:46, max mem: 20.9 GB 
[11/25 01:52:07 visual_prompt]: Epoch 20 / 100: avg data time: 1.51e-01, avg batch time: 0.9794, average train loss: 91.9706
[11/25 01:53:03 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3102, average loss: 98.4013
[11/25 01:53:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.25	
[11/25 01:53:03 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/25 01:54:47 visual_prompt]: 	Training 100/553. train loss: 251.7324,	0.8718 s / batch. (data: 5.01e-02). ETA=10:41:20, max mem: 20.9 GB 
[11/25 01:56:23 visual_prompt]: 	Training 200/553. train loss: 142.9680,	0.8480 s / batch. (data: 2.86e-04). ETA=10:22:26, max mem: 20.9 GB 
[11/25 01:58:00 visual_prompt]: 	Training 300/553. train loss: 511.1374,	1.1480 s / batch. (data: 3.23e-01). ETA=14:00:43, max mem: 20.9 GB 
[11/25 01:59:36 visual_prompt]: 	Training 400/553. train loss: 197.3434,	0.8355 s / batch. (data: 5.43e-03). ETA=10:10:29, max mem: 20.9 GB 
[11/25 02:01:15 visual_prompt]: 	Training 500/553. train loss: 40.3862,	0.8520 s / batch. (data: 3.00e-04). ETA=10:21:06, max mem: 20.9 GB 
[11/25 02:02:05 visual_prompt]: Epoch 21 / 100: avg data time: 1.52e-01, avg batch time: 0.9807, average train loss: 107.1739
[11/25 02:03:01 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3097, average loss: 11.7291
[11/25 02:03:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.31	
[11/25 02:03:01 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/25 02:04:41 visual_prompt]: 	Training 100/553. train loss: 129.5088,	0.8357 s / batch. (data: 2.86e-04). ETA=10:07:07, max mem: 20.9 GB 
[11/25 02:06:18 visual_prompt]: 	Training 200/553. train loss: 8.2347,	0.8401 s / batch. (data: 3.02e-04). ETA=10:08:54, max mem: 20.9 GB 
[11/25 02:07:54 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8240 s / batch. (data: 7.94e-03). ETA=9:55:49, max mem: 20.9 GB 
[11/25 02:09:32 visual_prompt]: 	Training 400/553. train loss: 119.1904,	0.8760 s / batch. (data: 1.20e-02). ETA=10:31:59, max mem: 20.9 GB 
[11/25 02:11:10 visual_prompt]: 	Training 500/553. train loss: 41.2609,	0.8228 s / batch. (data: 2.99e-04). ETA=9:52:15, max mem: 20.9 GB 
[11/25 02:12:02 visual_prompt]: Epoch 22 / 100: avg data time: 1.49e-01, avg batch time: 0.9780, average train loss: 77.9296
[11/25 02:12:57 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3097, average loss: 18.1174
[11/25 02:12:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.81	
[11/25 02:12:57 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/25 02:14:40 visual_prompt]: 	Training 100/553. train loss: 18.7215,	0.8172 s / batch. (data: 3.32e-04). ETA=9:46:08, max mem: 20.9 GB 
[11/25 02:16:18 visual_prompt]: 	Training 200/553. train loss: 2.1599,	0.8417 s / batch. (data: 5.42e-03). ETA=10:02:16, max mem: 20.9 GB 
[11/25 02:17:57 visual_prompt]: 	Training 300/553. train loss: 92.5511,	0.8405 s / batch. (data: 5.42e-03). ETA=9:59:59, max mem: 20.9 GB 
[11/25 02:19:33 visual_prompt]: 	Training 400/553. train loss: 17.3159,	0.8480 s / batch. (data: 7.90e-04). ETA=10:04:00, max mem: 20.9 GB 
[11/25 02:21:08 visual_prompt]: 	Training 500/553. train loss: 85.7274,	0.8555 s / batch. (data: 1.05e-02). ETA=10:07:55, max mem: 20.9 GB 
[11/25 02:21:59 visual_prompt]: Epoch 23 / 100: avg data time: 1.51e-01, avg batch time: 0.9798, average train loss: 68.7983
[11/25 02:22:56 visual_prompt]: Inference (val):avg data time: 6.06e-05, avg batch time: 0.3223, average loss: 93.9192
[11/25 02:22:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.86	
[11/25 02:22:56 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/25 02:24:34 visual_prompt]: 	Training 100/553. train loss: 121.3363,	0.8317 s / batch. (data: 5.43e-03). ETA=9:48:51, max mem: 20.9 GB 
[11/25 02:26:10 visual_prompt]: 	Training 200/553. train loss: 30.4765,	0.8512 s / batch. (data: 1.11e-02). ETA=10:01:14, max mem: 20.9 GB 
[11/25 02:27:49 visual_prompt]: 	Training 300/553. train loss: 30.0802,	1.0782 s / batch. (data: 2.37e-01). ETA=12:39:47, max mem: 20.9 GB 
[11/25 02:29:27 visual_prompt]: 	Training 400/553. train loss: 24.1946,	0.8464 s / batch. (data: 1.04e-02). ETA=9:55:03, max mem: 20.9 GB 
[11/25 02:31:06 visual_prompt]: 	Training 500/553. train loss: 155.4979,	0.8608 s / batch. (data: 1.07e-02). ETA=10:03:44, max mem: 20.9 GB 
[11/25 02:31:57 visual_prompt]: Epoch 24 / 100: avg data time: 1.50e-01, avg batch time: 0.9786, average train loss: 77.4350
[11/25 02:32:53 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3083, average loss: 59.1853
[11/25 02:32:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.16	
[11/25 02:32:53 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/25 02:34:36 visual_prompt]: 	Training 100/553. train loss: 197.0322,	0.8277 s / batch. (data: 3.00e-04). ETA=9:38:25, max mem: 20.9 GB 
[11/25 02:36:11 visual_prompt]: 	Training 200/553. train loss: 35.0548,	0.8100 s / batch. (data: 3.25e-04). ETA=9:24:39, max mem: 20.9 GB 
[11/25 02:37:47 visual_prompt]: 	Training 300/553. train loss: 140.3271,	1.0640 s / batch. (data: 2.49e-01). ETA=12:19:58, max mem: 20.9 GB 
[11/25 02:39:25 visual_prompt]: 	Training 400/553. train loss: 81.6939,	1.2800 s / batch. (data: 4.32e-01). ETA=14:48:04, max mem: 20.9 GB 
[11/25 02:41:03 visual_prompt]: 	Training 500/553. train loss: 61.1020,	1.3745 s / batch. (data: 5.65e-01). ETA=15:51:18, max mem: 20.9 GB 
[11/25 02:41:54 visual_prompt]: Epoch 25 / 100: avg data time: 1.50e-01, avg batch time: 0.9784, average train loss: 73.0670
[11/25 02:42:49 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3115, average loss: 142.0155
[11/25 02:42:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.36	
[11/25 02:42:49 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/25 02:44:31 visual_prompt]: 	Training 100/553. train loss: 29.4657,	0.8319 s / batch. (data: 1.19e-02). ETA=9:33:38, max mem: 20.9 GB 
[11/25 02:46:10 visual_prompt]: 	Training 200/553. train loss: 385.2049,	1.6146 s / batch. (data: 7.88e-01). ETA=18:30:40, max mem: 20.9 GB 
[11/25 02:47:49 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8280 s / batch. (data: 1.05e-02). ETA=9:28:12, max mem: 20.9 GB 
[11/25 02:49:25 visual_prompt]: 	Training 400/553. train loss: 54.7317,	0.8160 s / batch. (data: 3.02e-04). ETA=9:18:36, max mem: 20.9 GB 
[11/25 02:51:01 visual_prompt]: 	Training 500/553. train loss: 57.9050,	0.8316 s / batch. (data: 2.93e-04). ETA=9:27:55, max mem: 20.9 GB 
[11/25 02:51:52 visual_prompt]: Epoch 26 / 100: avg data time: 1.51e-01, avg batch time: 0.9808, average train loss: 73.4555
[11/25 02:52:47 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3107, average loss: 80.7753
[11/25 02:52:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.97	
[11/25 02:52:47 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[11/25 02:54:29 visual_prompt]: 	Training 100/553. train loss: 146.1667,	0.8540 s / batch. (data: 3.04e-04). ETA=9:41:02, max mem: 20.9 GB 
[11/25 02:56:06 visual_prompt]: 	Training 200/553. train loss: 171.7426,	0.9796 s / batch. (data: 1.49e-01). ETA=11:04:51, max mem: 20.9 GB 
[11/25 02:57:44 visual_prompt]: 	Training 300/553. train loss: 71.7340,	0.8067 s / batch. (data: 3.20e-04). ETA=9:06:11, max mem: 20.9 GB 
[11/25 02:59:22 visual_prompt]: 	Training 400/553. train loss: 35.2200,	0.8520 s / batch. (data: 7.14e-04). ETA=9:35:24, max mem: 20.9 GB 
[11/25 03:01:00 visual_prompt]: 	Training 500/553. train loss: 3.3718,	0.8440 s / batch. (data: 7.91e-04). ETA=9:28:34, max mem: 20.9 GB 
[11/25 03:01:49 visual_prompt]: Epoch 27 / 100: avg data time: 1.51e-01, avg batch time: 0.9803, average train loss: 64.3172
[11/25 03:02:45 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3100, average loss: 175.3402
[11/25 03:02:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.95	
[11/25 03:02:45 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[11/25 03:04:25 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8110 s / batch. (data: 3.24e-04). ETA=9:04:19, max mem: 20.9 GB 
[11/25 03:06:03 visual_prompt]: 	Training 200/553. train loss: 155.6715,	0.8342 s / batch. (data: 3.14e-04). ETA=9:18:28, max mem: 20.9 GB 
[11/25 03:07:41 visual_prompt]: 	Training 300/553. train loss: 14.1574,	1.2360 s / batch. (data: 4.08e-01). ETA=13:45:25, max mem: 20.9 GB 
[11/25 03:09:17 visual_prompt]: 	Training 400/553. train loss: 171.3097,	0.8480 s / batch. (data: 7.98e-03). ETA=9:24:53, max mem: 20.9 GB 
[11/25 03:10:54 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8244 s / batch. (data: 7.95e-03). ETA=9:07:48, max mem: 20.9 GB 
[11/25 03:11:45 visual_prompt]: Epoch 28 / 100: avg data time: 1.48e-01, avg batch time: 0.9769, average train loss: 73.8999
[11/25 03:12:41 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3087, average loss: 56.4914
[11/25 03:12:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.33	
[11/25 03:12:41 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[11/25 03:14:28 visual_prompt]: 	Training 100/553. train loss: 34.7881,	0.8520 s / batch. (data: 7.98e-03). ETA=9:23:58, max mem: 20.9 GB 
[11/25 03:16:04 visual_prompt]: 	Training 200/553. train loss: 0.0895,	1.6840 s / batch. (data: 8.40e-01). ETA=18:31:53, max mem: 20.9 GB 
[11/25 03:17:40 visual_prompt]: 	Training 300/553. train loss: 121.7846,	0.8360 s / batch. (data: 2.90e-04). ETA=9:10:35, max mem: 20.9 GB 
[11/25 03:19:15 visual_prompt]: 	Training 400/553. train loss: 119.9498,	1.1462 s / batch. (data: 3.41e-01). ETA=12:32:58, max mem: 20.9 GB 
[11/25 03:20:52 visual_prompt]: 	Training 500/553. train loss: 61.3509,	0.8280 s / batch. (data: 4.15e-04). ETA=9:02:33, max mem: 20.9 GB 
[11/25 03:21:43 visual_prompt]: Epoch 29 / 100: avg data time: 1.51e-01, avg batch time: 0.9796, average train loss: 84.4679
[11/25 03:22:39 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3095, average loss: 26.3829
[11/25 03:22:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.01	
[11/25 03:22:39 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[11/25 03:24:19 visual_prompt]: 	Training 100/553. train loss: 59.7197,	0.8253 s / batch. (data: 3.11e-04). ETA=8:58:42, max mem: 20.9 GB 
[11/25 03:25:57 visual_prompt]: 	Training 200/553. train loss: 133.5192,	0.8148 s / batch. (data: 3.03e-04). ETA=8:50:28, max mem: 20.9 GB 
[11/25 03:27:32 visual_prompt]: 	Training 300/553. train loss: 78.8985,	1.1259 s / batch. (data: 2.97e-01). ETA=12:11:09, max mem: 20.9 GB 
[11/25 03:29:11 visual_prompt]: 	Training 400/553. train loss: 3.7242,	0.9002 s / batch. (data: 8.14e-02). ETA=9:43:06, max mem: 20.9 GB 
[11/25 03:30:48 visual_prompt]: 	Training 500/553. train loss: 44.4254,	1.3791 s / batch. (data: 5.48e-01). ETA=14:50:56, max mem: 20.9 GB 
[11/25 03:31:41 visual_prompt]: Epoch 30 / 100: avg data time: 1.51e-01, avg batch time: 0.9800, average train loss: 76.9443
[11/25 03:32:37 visual_prompt]: Inference (val):avg data time: 1.82e-04, avg batch time: 0.3144, average loss: 15.7663
[11/25 03:32:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.12	
[11/25 03:32:37 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[11/25 03:34:19 visual_prompt]: 	Training 100/553. train loss: 12.9275,	0.8377 s / batch. (data: 3.24e-04). ETA=8:59:05, max mem: 20.9 GB 
[11/25 03:35:58 visual_prompt]: 	Training 200/553. train loss: 239.9660,	0.8320 s / batch. (data: 5.46e-03). ETA=8:53:59, max mem: 20.9 GB 
[11/25 03:37:33 visual_prompt]: 	Training 300/553. train loss: 74.7414,	0.8400 s / batch. (data: 3.08e-04). ETA=8:57:44, max mem: 20.9 GB 
[11/25 03:39:10 visual_prompt]: 	Training 400/553. train loss: 25.2471,	1.1454 s / batch. (data: 3.20e-01). ETA=12:11:21, max mem: 20.9 GB 
[11/25 03:40:48 visual_prompt]: 	Training 500/553. train loss: 30.2537,	0.8677 s / batch. (data: 5.42e-03). ETA=9:12:33, max mem: 20.9 GB 
[11/25 03:41:38 visual_prompt]: Epoch 31 / 100: avg data time: 1.49e-01, avg batch time: 0.9781, average train loss: 63.6474
[11/25 03:42:33 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3098, average loss: 18.8359
[11/25 03:42:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.38	
[11/25 03:42:33 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[11/25 03:44:16 visual_prompt]: 	Training 100/553. train loss: 15.2904,	0.8632 s / batch. (data: 2.88e-04). ETA=9:07:29, max mem: 20.9 GB 
[11/25 03:45:52 visual_prompt]: 	Training 200/553. train loss: 58.0532,	0.8350 s / batch. (data: 3.05e-04). ETA=8:48:13, max mem: 20.9 GB 
[11/25 03:47:33 visual_prompt]: 	Training 300/553. train loss: 40.2767,	0.8141 s / batch. (data: 2.93e-04). ETA=8:33:38, max mem: 20.9 GB 
[11/25 03:49:11 visual_prompt]: 	Training 400/553. train loss: 32.6062,	0.8473 s / batch. (data: 1.84e-02). ETA=8:53:12, max mem: 20.9 GB 
[11/25 03:50:46 visual_prompt]: 	Training 500/553. train loss: 2.6893,	0.8310 s / batch. (data: 2.79e-04). ETA=8:41:33, max mem: 20.9 GB 
[11/25 03:51:36 visual_prompt]: Epoch 32 / 100: avg data time: 1.52e-01, avg batch time: 0.9807, average train loss: 68.8120
[11/25 03:52:31 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3100, average loss: 18.6799
[11/25 03:52:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.72	
[11/25 03:52:31 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[11/25 03:54:11 visual_prompt]: 	Training 100/553. train loss: 0.1960,	0.8166 s / batch. (data: 2.90e-04). ETA=8:30:24, max mem: 20.9 GB 
[11/25 03:55:50 visual_prompt]: 	Training 200/553. train loss: 67.9366,	1.1207 s / batch. (data: 3.07e-01). ETA=11:38:37, max mem: 20.9 GB 
[11/25 03:57:26 visual_prompt]: 	Training 300/553. train loss: 51.7071,	0.8387 s / batch. (data: 3.30e-04). ETA=8:41:25, max mem: 20.9 GB 
[11/25 03:59:05 visual_prompt]: 	Training 400/553. train loss: 24.1078,	0.8341 s / batch. (data: 3.00e-04). ETA=8:37:10, max mem: 20.9 GB 
[11/25 04:00:42 visual_prompt]: 	Training 500/553. train loss: 8.8907,	0.8441 s / batch. (data: 5.43e-03). ETA=8:41:58, max mem: 20.9 GB 
[11/25 04:01:32 visual_prompt]: Epoch 33 / 100: avg data time: 1.48e-01, avg batch time: 0.9770, average train loss: 60.6009
[11/25 04:02:27 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3113, average loss: 30.7730
[11/25 04:02:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.16	
[11/25 04:02:27 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[11/25 04:04:09 visual_prompt]: 	Training 100/553. train loss: 75.7294,	0.8440 s / batch. (data: 3.17e-04). ETA=8:39:45, max mem: 20.9 GB 
[11/25 04:05:45 visual_prompt]: 	Training 200/553. train loss: 66.3294,	0.8324 s / batch. (data: 3.25e-04). ETA=8:31:15, max mem: 20.9 GB 
[11/25 04:07:21 visual_prompt]: 	Training 300/553. train loss: 104.9058,	0.8160 s / batch. (data: 3.26e-04). ETA=8:19:47, max mem: 20.9 GB 
[11/25 04:09:00 visual_prompt]: 	Training 400/553. train loss: 131.9211,	0.8353 s / batch. (data: 2.90e-04). ETA=8:30:13, max mem: 20.9 GB 
[11/25 04:10:38 visual_prompt]: 	Training 500/553. train loss: 32.8797,	1.3960 s / batch. (data: 5.41e-01). ETA=14:10:23, max mem: 20.9 GB 
[11/25 04:11:28 visual_prompt]: Epoch 34 / 100: avg data time: 1.49e-01, avg batch time: 0.9785, average train loss: 70.1690
[11/25 04:12:24 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3109, average loss: 99.1878
[11/25 04:12:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.86	
[11/25 04:12:24 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[11/25 04:14:07 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8080 s / batch. (data: 3.22e-04). ETA=8:10:10, max mem: 20.9 GB 
[11/25 04:15:46 visual_prompt]: 	Training 200/553. train loss: 13.2344,	0.8527 s / batch. (data: 1.05e-02). ETA=8:35:49, max mem: 20.9 GB 
[11/25 04:17:21 visual_prompt]: 	Training 300/553. train loss: 16.2176,	0.8677 s / batch. (data: 2.78e-02). ETA=8:43:27, max mem: 20.9 GB 
[11/25 04:18:58 visual_prompt]: 	Training 400/553. train loss: 85.7057,	0.8109 s / batch. (data: 5.43e-03). ETA=8:07:51, max mem: 20.9 GB 
[11/25 04:20:34 visual_prompt]: 	Training 500/553. train loss: 59.5886,	0.8318 s / batch. (data: 2.88e-04). ETA=8:19:01, max mem: 20.9 GB 
[11/25 04:21:26 visual_prompt]: Epoch 35 / 100: avg data time: 1.52e-01, avg batch time: 0.9793, average train loss: 69.4832
[11/25 04:22:21 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3106, average loss: 222.5221
[11/25 04:22:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.66	
[11/25 04:22:21 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[11/25 04:24:02 visual_prompt]: 	Training 100/553. train loss: 14.2917,	0.8440 s / batch. (data: 3.20e-04). ETA=8:24:13, max mem: 20.9 GB 
[11/25 04:25:41 visual_prompt]: 	Training 200/553. train loss: 213.9611,	0.8343 s / batch. (data: 2.87e-04). ETA=8:17:03, max mem: 20.9 GB 
[11/25 04:27:20 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8192 s / batch. (data: 5.38e-03). ETA=8:06:42, max mem: 20.9 GB 
[11/25 04:28:56 visual_prompt]: 	Training 400/553. train loss: 34.5564,	0.8342 s / batch. (data: 3.18e-04). ETA=8:14:11, max mem: 20.9 GB 
[11/25 04:30:34 visual_prompt]: 	Training 500/553. train loss: 32.5290,	1.2720 s / batch. (data: 4.50e-01). ETA=12:31:25, max mem: 20.9 GB 
[11/25 04:31:23 visual_prompt]: Epoch 36 / 100: avg data time: 1.48e-01, avg batch time: 0.9783, average train loss: 66.5310
[11/25 04:32:18 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.3116, average loss: 187.9097
[11/25 04:32:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.82	
[11/25 04:32:18 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[11/25 04:33:59 visual_prompt]: 	Training 100/553. train loss: 40.9709,	0.8240 s / batch. (data: 3.26e-04). ETA=8:04:40, max mem: 20.9 GB 
[11/25 04:35:37 visual_prompt]: 	Training 200/553. train loss: 21.7606,	0.8258 s / batch. (data: 3.17e-04). ETA=8:04:21, max mem: 20.9 GB 
[11/25 04:37:15 visual_prompt]: 	Training 300/553. train loss: 232.1864,	1.2514 s / batch. (data: 4.23e-01). ETA=12:11:54, max mem: 20.9 GB 
[11/25 04:38:55 visual_prompt]: 	Training 400/553. train loss: 20.2850,	1.8880 s / batch. (data: 1.07e+00). ETA=18:21:05, max mem: 20.9 GB 
[11/25 04:40:28 visual_prompt]: 	Training 500/553. train loss: 67.3315,	0.9748 s / batch. (data: 1.50e-01). ETA=9:26:51, max mem: 20.9 GB 
[11/25 04:41:21 visual_prompt]: Epoch 37 / 100: avg data time: 1.52e-01, avg batch time: 0.9820, average train loss: 64.9782
[11/25 04:42:17 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3113, average loss: 151.3353
[11/25 04:42:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.82	
[11/25 04:42:17 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[11/25 04:43:57 visual_prompt]: 	Training 100/553. train loss: 80.2315,	1.0390 s / batch. (data: 1.98e-01). ETA=10:01:35, max mem: 20.9 GB 
[11/25 04:45:35 visual_prompt]: 	Training 200/553. train loss: 64.3392,	1.2440 s / batch. (data: 4.05e-01). ETA=11:58:10, max mem: 20.9 GB 
[11/25 04:47:14 visual_prompt]: 	Training 300/553. train loss: 35.4241,	0.8118 s / batch. (data: 3.01e-04). ETA=7:47:20, max mem: 20.9 GB 
[11/25 04:48:49 visual_prompt]: 	Training 400/553. train loss: 55.5618,	0.8280 s / batch. (data: 2.98e-04). ETA=7:55:15, max mem: 20.9 GB 
[11/25 04:50:29 visual_prompt]: 	Training 500/553. train loss: 100.8810,	0.8560 s / batch. (data: 2.94e-04). ETA=8:09:54, max mem: 20.9 GB 
[11/25 04:51:18 visual_prompt]: Epoch 38 / 100: avg data time: 1.50e-01, avg batch time: 0.9786, average train loss: 76.1756
[11/25 04:52:14 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3112, average loss: 0.7080
[11/25 04:52:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.61	
[11/25 04:52:14 visual_prompt]: Best epoch 38: best metric: -0.708
[11/25 04:52:14 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[11/25 04:53:54 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8355 s / batch. (data: 7.97e-03). ETA=7:56:00, max mem: 20.9 GB 
[11/25 04:55:36 visual_prompt]: 	Training 200/553. train loss: 257.0297,	0.8480 s / batch. (data: 2.94e-04). ETA=8:01:44, max mem: 20.9 GB 
[11/25 04:57:15 visual_prompt]: 	Training 300/553. train loss: 128.9853,	0.8275 s / batch. (data: 3.19e-04). ETA=7:48:43, max mem: 20.9 GB 
[11/25 04:58:51 visual_prompt]: 	Training 400/553. train loss: 10.1288,	0.8875 s / batch. (data: 5.99e-02). ETA=8:21:13, max mem: 20.9 GB 
[11/25 05:00:29 visual_prompt]: 	Training 500/553. train loss: 70.0008,	1.6995 s / batch. (data: 8.86e-01). ETA=15:57:00, max mem: 20.9 GB 
[11/25 05:01:18 visual_prompt]: Epoch 39 / 100: avg data time: 1.53e-01, avg batch time: 0.9830, average train loss: 62.4884
[11/25 05:02:12 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.3109, average loss: 80.0580
[11/25 05:02:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.21	
[11/25 05:02:12 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[11/25 05:03:55 visual_prompt]: 	Training 100/553. train loss: 84.9651,	0.8151 s / batch. (data: 3.09e-04). ETA=7:36:53, max mem: 20.9 GB 
[11/25 05:05:31 visual_prompt]: 	Training 200/553. train loss: 46.7133,	0.8320 s / batch. (data: 3.05e-04). ETA=7:44:59, max mem: 20.9 GB 
[11/25 05:07:10 visual_prompt]: 	Training 300/553. train loss: 10.3414,	0.8581 s / batch. (data: 3.41e-04). ETA=7:58:08, max mem: 20.9 GB 
[11/25 05:08:48 visual_prompt]: 	Training 400/553. train loss: 82.0898,	0.8199 s / batch. (data: 7.85e-04). ETA=7:35:29, max mem: 20.9 GB 
[11/25 05:10:24 visual_prompt]: 	Training 500/553. train loss: 97.8887,	0.8320 s / batch. (data: 3.01e-04). ETA=7:40:49, max mem: 20.9 GB 
[11/25 05:11:17 visual_prompt]: Epoch 40 / 100: avg data time: 1.56e-01, avg batch time: 0.9842, average train loss: 64.2023
[11/25 05:12:12 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3114, average loss: 40.3789
[11/25 05:12:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.03	
[11/25 05:12:12 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[11/25 05:13:58 visual_prompt]: 	Training 100/553. train loss: 16.3361,	0.8280 s / batch. (data: 2.98e-04). ETA=7:36:29, max mem: 20.9 GB 
[11/25 05:15:37 visual_prompt]: 	Training 200/553. train loss: 4.5002,	0.8480 s / batch. (data: 7.90e-04). ETA=7:46:06, max mem: 20.9 GB 
[11/25 05:17:14 visual_prompt]: 	Training 300/553. train loss: 37.4190,	0.8339 s / batch. (data: 8.02e-03). ETA=7:36:58, max mem: 20.9 GB 
[11/25 05:18:50 visual_prompt]: 	Training 400/553. train loss: 63.4580,	0.8290 s / batch. (data: 7.42e-03). ETA=7:32:53, max mem: 20.9 GB 
[11/25 05:20:25 visual_prompt]: 	Training 500/553. train loss: 1.0989,	0.8245 s / batch. (data: 5.44e-03). ETA=7:29:04, max mem: 20.9 GB 
[11/25 05:21:14 visual_prompt]: Epoch 41 / 100: avg data time: 1.49e-01, avg batch time: 0.9793, average train loss: 67.2143
[11/25 05:22:09 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3110, average loss: 434.8864
[11/25 05:22:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.06	
[11/25 05:22:09 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[11/25 05:23:48 visual_prompt]: 	Training 100/553. train loss: 57.4771,	0.8313 s / batch. (data: 7.96e-03). ETA=7:30:40, max mem: 20.9 GB 
[11/25 05:25:24 visual_prompt]: 	Training 200/553. train loss: 5.7742,	0.8569 s / batch. (data: 6.02e-03). ETA=7:43:07, max mem: 20.9 GB 
[11/25 05:27:00 visual_prompt]: 	Training 300/553. train loss: 12.9850,	0.8212 s / batch. (data: 2.88e-04). ETA=7:22:26, max mem: 20.9 GB 
[11/25 05:28:36 visual_prompt]: 	Training 400/553. train loss: 11.0050,	0.8304 s / batch. (data: 3.16e-04). ETA=7:26:01, max mem: 20.9 GB 
[11/25 05:30:11 visual_prompt]: 	Training 500/553. train loss: 24.0132,	0.8437 s / batch. (data: 5.38e-03). ETA=7:31:44, max mem: 20.9 GB 
[11/25 05:31:01 visual_prompt]: Epoch 42 / 100: avg data time: 1.32e-01, avg batch time: 0.9618, average train loss: 50.8760
[11/25 05:31:56 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3098, average loss: 65.7702
[11/25 05:31:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.96	
[11/25 05:31:56 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[11/25 05:33:39 visual_prompt]: 	Training 100/553. train loss: 5.4537,	0.8372 s / batch. (data: 1.55e-02). ETA=7:26:09, max mem: 20.9 GB 
[11/25 05:35:16 visual_prompt]: 	Training 200/553. train loss: 30.4806,	0.8223 s / batch. (data: 3.26e-04). ETA=7:16:50, max mem: 20.9 GB 
[11/25 05:36:52 visual_prompt]: 	Training 300/553. train loss: 13.4713,	0.8178 s / batch. (data: 3.09e-04). ETA=7:13:05, max mem: 20.9 GB 
[11/25 05:38:27 visual_prompt]: 	Training 400/553. train loss: 116.0019,	0.8449 s / batch. (data: 1.05e-02). ETA=7:26:01, max mem: 20.9 GB 
[11/25 05:40:07 visual_prompt]: 	Training 500/553. train loss: 5.1182,	0.8480 s / batch. (data: 5.42e-03). ETA=7:26:14, max mem: 20.9 GB 
[11/25 05:40:59 visual_prompt]: Epoch 43 / 100: avg data time: 1.52e-01, avg batch time: 0.9809, average train loss: 61.3016
[11/25 05:41:55 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3106, average loss: 37.1228
[11/25 05:41:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.51	
[11/25 05:41:55 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[11/25 05:43:36 visual_prompt]: 	Training 100/553. train loss: 17.0520,	0.8455 s / batch. (data: 1.05e-02). ETA=7:22:46, max mem: 20.9 GB 
[11/25 05:45:16 visual_prompt]: 	Training 200/553. train loss: 41.8210,	0.8240 s / batch. (data: 2.83e-04). ETA=7:10:08, max mem: 20.9 GB 
[11/25 05:46:51 visual_prompt]: 	Training 300/553. train loss: 33.0272,	0.8246 s / batch. (data: 1.76e-03). ETA=7:09:04, max mem: 20.9 GB 
[11/25 05:48:28 visual_prompt]: 	Training 400/553. train loss: 18.8421,	0.8360 s / batch. (data: 5.44e-03). ETA=7:13:36, max mem: 20.9 GB 
[11/25 05:50:06 visual_prompt]: 	Training 500/553. train loss: 4.9568,	0.8176 s / batch. (data: 2.97e-04). ETA=7:02:43, max mem: 20.9 GB 
[11/25 05:50:57 visual_prompt]: Epoch 44 / 100: avg data time: 1.50e-01, avg batch time: 0.9803, average train loss: 59.3901
[11/25 05:51:52 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3105, average loss: 35.7947
[11/25 05:51:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.83	
[11/25 05:51:52 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[11/25 05:53:35 visual_prompt]: 	Training 100/553. train loss: 55.2928,	0.8320 s / batch. (data: 8.02e-03). ETA=7:08:02, max mem: 20.9 GB 
[11/25 05:55:08 visual_prompt]: 	Training 200/553. train loss: 7.9507,	0.9375 s / batch. (data: 1.02e-01). ETA=8:00:44, max mem: 20.9 GB 
[11/25 05:56:47 visual_prompt]: 	Training 300/553. train loss: 97.6082,	0.8393 s / batch. (data: 1.00e-02). ETA=7:09:00, max mem: 20.9 GB 
[11/25 05:58:22 visual_prompt]: 	Training 400/553. train loss: 46.9183,	0.8359 s / batch. (data: 1.20e-02). ETA=7:05:51, max mem: 20.9 GB 
[11/25 06:00:03 visual_prompt]: 	Training 500/553. train loss: 2.4295,	0.8400 s / batch. (data: 3.32e-04). ETA=7:06:32, max mem: 20.9 GB 
[11/25 06:00:53 visual_prompt]: Epoch 45 / 100: avg data time: 1.47e-01, avg batch time: 0.9786, average train loss: 41.2939
[11/25 06:01:49 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3109, average loss: 51.0713
[11/25 06:01:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.49	
[11/25 06:01:49 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[11/25 06:03:31 visual_prompt]: 	Training 100/553. train loss: 108.2759,	1.2513 s / batch. (data: 4.34e-01). ETA=10:32:12, max mem: 20.9 GB 
[11/25 06:05:10 visual_prompt]: 	Training 200/553. train loss: 6.5789,	0.8421 s / batch. (data: 3.06e-04). ETA=7:04:02, max mem: 20.9 GB 
[11/25 06:06:46 visual_prompt]: 	Training 300/553. train loss: 10.2063,	0.8275 s / batch. (data: 4.69e-04). ETA=6:55:19, max mem: 20.9 GB 
[11/25 06:08:24 visual_prompt]: 	Training 400/553. train loss: 48.9788,	0.8449 s / batch. (data: 7.59e-04). ETA=7:02:41, max mem: 20.9 GB 
[11/25 06:09:57 visual_prompt]: 	Training 500/553. train loss: 79.6497,	0.8281 s / batch. (data: 2.82e-04). ETA=6:52:51, max mem: 20.9 GB 
[11/25 06:10:51 visual_prompt]: Epoch 46 / 100: avg data time: 1.50e-01, avg batch time: 0.9797, average train loss: 55.6199
[11/25 06:11:46 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3080, average loss: 56.1374
[11/25 06:11:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.27	
[11/25 06:11:46 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[11/25 06:13:29 visual_prompt]: 	Training 100/553. train loss: 14.5243,	0.8400 s / batch. (data: 3.07e-04). ETA=6:56:39, max mem: 20.9 GB 
[11/25 06:15:03 visual_prompt]: 	Training 200/553. train loss: 41.7565,	1.2840 s / batch. (data: 4.45e-01). ETA=10:34:45, max mem: 20.9 GB 
[11/25 06:16:41 visual_prompt]: 	Training 300/553. train loss: 50.3816,	0.8200 s / batch. (data: 2.76e-04). ETA=6:44:01, max mem: 20.9 GB 
[11/25 06:18:18 visual_prompt]: 	Training 400/553. train loss: 16.0877,	0.8285 s / batch. (data: 7.17e-04). ETA=6:46:48, max mem: 20.9 GB 
[11/25 06:19:54 visual_prompt]: 	Training 500/553. train loss: 21.9337,	0.8237 s / batch. (data: 1.05e-02). ETA=6:43:04, max mem: 20.9 GB 
[11/25 06:20:47 visual_prompt]: Epoch 47 / 100: avg data time: 1.48e-01, avg batch time: 0.9773, average train loss: 74.1330
[11/25 06:21:42 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3097, average loss: 14.5108
[11/25 06:21:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.96	
[11/25 06:21:42 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[11/25 06:23:23 visual_prompt]: 	Training 100/553. train loss: 70.8380,	0.8154 s / batch. (data: 3.12e-04). ETA=6:36:57, max mem: 20.9 GB 
[11/25 06:25:01 visual_prompt]: 	Training 200/553. train loss: 3.5125,	0.8242 s / batch. (data: 7.56e-03). ETA=6:39:52, max mem: 20.9 GB 
[11/25 06:26:40 visual_prompt]: 	Training 300/553. train loss: 87.6523,	1.4556 s / batch. (data: 6.41e-01). ETA=11:43:45, max mem: 20.9 GB 
[11/25 06:28:14 visual_prompt]: 	Training 400/553. train loss: 48.0421,	0.8667 s / batch. (data: 3.23e-04). ETA=6:57:34, max mem: 20.9 GB 
[11/25 06:29:52 visual_prompt]: 	Training 500/553. train loss: 19.7940,	0.8411 s / batch. (data: 5.42e-03). ETA=6:43:51, max mem: 20.9 GB 
[11/25 06:30:42 visual_prompt]: Epoch 48 / 100: avg data time: 1.48e-01, avg batch time: 0.9764, average train loss: 60.5188
[11/25 06:31:38 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3105, average loss: 57.0932
[11/25 06:31:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.17	
[11/25 06:31:38 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[11/25 06:33:18 visual_prompt]: 	Training 100/553. train loss: 91.9706,	0.8254 s / batch. (data: 3.26e-04). ETA=6:34:13, max mem: 20.9 GB 
[11/25 06:34:55 visual_prompt]: 	Training 200/553. train loss: 91.7134,	0.8158 s / batch. (data: 2.86e-04). ETA=6:28:15, max mem: 20.9 GB 
[11/25 06:36:33 visual_prompt]: 	Training 300/553. train loss: 12.2309,	0.8252 s / batch. (data: 3.37e-04). ETA=6:31:23, max mem: 20.9 GB 
[11/25 06:38:12 visual_prompt]: 	Training 400/553. train loss: 2.3768,	0.8267 s / batch. (data: 9.33e-03). ETA=6:30:42, max mem: 20.9 GB 
[11/25 06:39:49 visual_prompt]: 	Training 500/553. train loss: 8.9641,	0.8510 s / batch. (data: 7.15e-04). ETA=6:40:45, max mem: 20.9 GB 
[11/25 06:40:41 visual_prompt]: Epoch 49 / 100: avg data time: 1.52e-01, avg batch time: 0.9818, average train loss: 44.0392
[11/25 06:41:36 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3101, average loss: 24.5478
[11/25 06:41:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.05	
[11/25 06:41:36 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[11/25 06:43:19 visual_prompt]: 	Training 100/553. train loss: 139.8577,	0.8400 s / batch. (data: 8.09e-04). ETA=6:33:26, max mem: 20.9 GB 
[11/25 06:44:57 visual_prompt]: 	Training 200/553. train loss: 227.0549,	0.8120 s / batch. (data: 2.72e-04). ETA=6:18:59, max mem: 20.9 GB 
[11/25 06:46:33 visual_prompt]: 	Training 300/553. train loss: 31.9506,	0.8280 s / batch. (data: 3.09e-04). ETA=6:25:03, max mem: 20.9 GB 
[11/25 06:48:09 visual_prompt]: 	Training 400/553. train loss: 15.5226,	0.8388 s / batch. (data: 3.18e-04). ETA=6:28:40, max mem: 20.9 GB 
[11/25 06:49:47 visual_prompt]: 	Training 500/553. train loss: 82.9442,	0.8456 s / batch. (data: 2.20e-02). ETA=6:30:24, max mem: 20.9 GB 
[11/25 06:50:37 visual_prompt]: Epoch 50 / 100: avg data time: 1.48e-01, avg batch time: 0.9774, average train loss: 47.5957
[11/25 06:51:33 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3101, average loss: 36.8941
[11/25 06:51:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.32	
[11/25 06:51:33 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[11/25 06:53:14 visual_prompt]: 	Training 100/553. train loss: 63.9178,	1.0560 s / batch. (data: 2.44e-01). ETA=8:04:52, max mem: 20.9 GB 
[11/25 06:54:52 visual_prompt]: 	Training 200/553. train loss: 154.4764,	0.8469 s / batch. (data: 3.08e-04). ETA=6:27:28, max mem: 20.9 GB 
[11/25 06:56:30 visual_prompt]: 	Training 300/553. train loss: 17.4423,	1.3886 s / batch. (data: 5.49e-01). ETA=10:32:59, max mem: 20.9 GB 
[11/25 06:58:07 visual_prompt]: 	Training 400/553. train loss: 223.1041,	1.3241 s / batch. (data: 4.86e-01). ETA=10:01:22, max mem: 20.9 GB 
[11/25 06:59:44 visual_prompt]: 	Training 500/553. train loss: 73.4741,	0.8293 s / batch. (data: 7.80e-04). ETA=6:15:16, max mem: 20.9 GB 
[11/25 07:00:33 visual_prompt]: Epoch 51 / 100: avg data time: 1.47e-01, avg batch time: 0.9778, average train loss: 41.7550
[11/25 07:01:29 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3094, average loss: 28.3620
[11/25 07:01:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.72	
[11/25 07:01:29 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[11/25 07:03:12 visual_prompt]: 	Training 100/553. train loss: 2.7499,	0.8485 s / batch. (data: 5.93e-03). ETA=6:21:48, max mem: 20.9 GB 
[11/25 07:04:48 visual_prompt]: 	Training 200/553. train loss: 58.9792,	0.8320 s / batch. (data: 3.19e-04). ETA=6:12:58, max mem: 20.9 GB 
[11/25 07:06:26 visual_prompt]: 	Training 300/553. train loss: 136.8985,	0.8241 s / batch. (data: 3.09e-04). ETA=6:08:03, max mem: 20.9 GB 
[11/25 07:08:05 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8200 s / batch. (data: 3.00e-04). ETA=6:04:51, max mem: 20.9 GB 
[11/25 07:09:38 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8376 s / batch. (data: 9.55e-03). ETA=6:11:17, max mem: 20.9 GB 
[11/25 07:10:28 visual_prompt]: Epoch 52 / 100: avg data time: 1.44e-01, avg batch time: 0.9743, average train loss: 42.6473
[11/25 07:11:23 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3108, average loss: 151.0794
[11/25 07:11:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.23	
[11/25 07:11:23 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[11/25 07:13:02 visual_prompt]: 	Training 100/553. train loss: 120.2621,	0.8251 s / batch. (data: 2.99e-04). ETA=6:03:40, max mem: 20.9 GB 
[11/25 07:14:38 visual_prompt]: 	Training 200/553. train loss: 60.7727,	0.8493 s / batch. (data: 2.13e-02). ETA=6:12:55, max mem: 20.9 GB 
[11/25 07:16:14 visual_prompt]: 	Training 300/553. train loss: 6.7630,	0.8485 s / batch. (data: 7.12e-04). ETA=6:11:08, max mem: 20.9 GB 
[11/25 07:17:51 visual_prompt]: 	Training 400/553. train loss: 51.5285,	0.8480 s / batch. (data: 7.95e-03). ETA=6:09:30, max mem: 20.9 GB 
[11/25 07:19:26 visual_prompt]: 	Training 500/553. train loss: 53.9246,	0.8221 s / batch. (data: 1.10e-02). ETA=5:56:51, max mem: 20.9 GB 
[11/25 07:20:17 visual_prompt]: Epoch 53 / 100: avg data time: 1.35e-01, avg batch time: 0.9657, average train loss: 38.3206
[11/25 07:21:12 visual_prompt]: Inference (val):avg data time: 2.04e-04, avg batch time: 0.3108, average loss: 9.8799
[11/25 07:21:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.63	
[11/25 07:21:12 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[11/25 07:22:55 visual_prompt]: 	Training 100/553. train loss: 23.6152,	0.8305 s / batch. (data: 3.14e-04). ETA=5:58:22, max mem: 20.9 GB 
[11/25 07:24:31 visual_prompt]: 	Training 200/553. train loss: 54.6479,	0.8320 s / batch. (data: 2.98e-04). ETA=5:57:38, max mem: 20.9 GB 
[11/25 07:26:06 visual_prompt]: 	Training 300/553. train loss: 4.9590,	0.8335 s / batch. (data: 3.23e-04). ETA=5:56:52, max mem: 20.9 GB 
[11/25 07:27:41 visual_prompt]: 	Training 400/553. train loss: 145.9541,	0.8380 s / batch. (data: 2.25e-02). ETA=5:57:26, max mem: 20.9 GB 
[11/25 07:29:17 visual_prompt]: 	Training 500/553. train loss: 40.1548,	0.8280 s / batch. (data: 3.30e-04). ETA=5:51:46, max mem: 20.9 GB 
[11/25 07:30:07 visual_prompt]: Epoch 54 / 100: avg data time: 1.39e-01, avg batch time: 0.9684, average train loss: 39.6460
[11/25 07:31:02 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3105, average loss: 24.6680
[11/25 07:31:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.68	
[11/25 07:31:02 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[11/25 07:32:41 visual_prompt]: 	Training 100/553. train loss: 29.5495,	0.8307 s / batch. (data: 3.25e-04). ETA=5:50:47, max mem: 20.9 GB 
[11/25 07:34:16 visual_prompt]: 	Training 200/553. train loss: 8.5054,	0.8200 s / batch. (data: 3.08e-04). ETA=5:44:55, max mem: 20.9 GB 
[11/25 07:35:52 visual_prompt]: 	Training 300/553. train loss: 55.7648,	0.8232 s / batch. (data: 2.85e-04). ETA=5:44:53, max mem: 20.9 GB 
[11/25 07:37:27 visual_prompt]: 	Training 400/553. train loss: 34.0442,	1.0999 s / batch. (data: 2.61e-01). ETA=7:39:00, max mem: 20.9 GB 
[11/25 07:39:02 visual_prompt]: 	Training 500/553. train loss: 2.2429,	0.8381 s / batch. (data: 1.19e-02). ETA=5:48:19, max mem: 20.9 GB 
[11/25 07:39:53 visual_prompt]: Epoch 55 / 100: avg data time: 1.31e-01, avg batch time: 0.9614, average train loss: 37.3406
[11/25 07:40:48 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3096, average loss: 72.3726
[11/25 07:40:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.51	
[11/25 07:40:48 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[11/25 07:42:29 visual_prompt]: 	Training 100/553. train loss: 30.2240,	0.8324 s / batch. (data: 7.97e-03). ETA=5:43:52, max mem: 20.9 GB 
[11/25 07:44:04 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8360 s / batch. (data: 3.10e-04). ETA=5:43:56, max mem: 20.9 GB 
[11/25 07:45:41 visual_prompt]: 	Training 300/553. train loss: 14.4014,	0.8464 s / batch. (data: 3.01e-04). ETA=5:46:49, max mem: 20.9 GB 
[11/25 07:47:18 visual_prompt]: 	Training 400/553. train loss: 55.4842,	0.8513 s / batch. (data: 1.10e-02). ETA=5:47:25, max mem: 20.9 GB 
[11/25 07:48:53 visual_prompt]: 	Training 500/553. train loss: 5.5186,	1.9546 s / batch. (data: 1.13e+00). ETA=13:14:22, max mem: 20.9 GB 
[11/25 07:49:42 visual_prompt]: Epoch 56 / 100: avg data time: 1.36e-01, avg batch time: 0.9652, average train loss: 43.4949
[11/25 07:50:37 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3096, average loss: 36.1415
[11/25 07:50:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.49	
[11/25 07:50:37 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[11/25 07:52:20 visual_prompt]: 	Training 100/553. train loss: 11.7745,	0.8309 s / batch. (data: 7.50e-04). ETA=5:35:33, max mem: 20.9 GB 
[11/25 07:53:55 visual_prompt]: 	Training 200/553. train loss: 20.4587,	0.8366 s / batch. (data: 1.07e-02). ETA=5:36:27, max mem: 20.9 GB 
[11/25 07:55:29 visual_prompt]: 	Training 300/553. train loss: 52.7563,	0.8485 s / batch. (data: 3.86e-04). ETA=5:39:50, max mem: 20.9 GB 
[11/25 07:57:04 visual_prompt]: 	Training 400/553. train loss: 107.6301,	0.8450 s / batch. (data: 2.99e-04). ETA=5:37:01, max mem: 20.9 GB 
[11/25 07:58:37 visual_prompt]: 	Training 500/553. train loss: 26.3934,	0.8389 s / batch. (data: 3.24e-04). ETA=5:33:11, max mem: 20.9 GB 
[11/25 07:59:28 visual_prompt]: Epoch 57 / 100: avg data time: 1.30e-01, avg batch time: 0.9608, average train loss: 33.1353
[11/25 08:00:22 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3111, average loss: 52.2059
[11/25 08:00:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.20	
[11/25 08:00:22 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[11/25 08:02:02 visual_prompt]: 	Training 100/553. train loss: 8.4880,	1.0822 s / batch. (data: 2.59e-01). ETA=7:07:05, max mem: 20.9 GB 
[11/25 08:03:38 visual_prompt]: 	Training 200/553. train loss: 121.7783,	0.8227 s / batch. (data: 2.90e-04). ETA=5:23:18, max mem: 20.9 GB 
[11/25 08:05:17 visual_prompt]: 	Training 300/553. train loss: 33.8112,	0.8566 s / batch. (data: 1.59e-02). ETA=5:35:11, max mem: 20.9 GB 
[11/25 08:06:52 visual_prompt]: 	Training 400/553. train loss: 18.9799,	1.3534 s / batch. (data: 5.34e-01). ETA=8:47:20, max mem: 20.9 GB 
[11/25 08:08:26 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8160 s / batch. (data: 2.86e-04). ETA=5:16:35, max mem: 20.9 GB 
[11/25 08:09:16 visual_prompt]: Epoch 58 / 100: avg data time: 1.34e-01, avg batch time: 0.9649, average train loss: 31.1715
[11/25 08:10:11 visual_prompt]: Inference (val):avg data time: 4.04e-04, avg batch time: 0.3097, average loss: 139.2448
[11/25 08:10:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.89	
[11/25 08:10:11 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[11/25 08:11:52 visual_prompt]: 	Training 100/553. train loss: 32.4072,	0.8381 s / batch. (data: 1.05e-02). ETA=5:23:02, max mem: 20.9 GB 
[11/25 08:13:29 visual_prompt]: 	Training 200/553. train loss: 0.0502,	0.8319 s / batch. (data: 5.42e-03). ETA=5:19:16, max mem: 20.9 GB 
[11/25 08:15:04 visual_prompt]: 	Training 300/553. train loss: 124.5973,	0.8386 s / batch. (data: 3.02e-04). ETA=5:20:26, max mem: 20.9 GB 
[11/25 08:16:39 visual_prompt]: 	Training 400/553. train loss: 20.7312,	0.8277 s / batch. (data: 3.37e-04). ETA=5:14:53, max mem: 20.9 GB 
[11/25 08:18:18 visual_prompt]: 	Training 500/553. train loss: 31.7928,	0.8331 s / batch. (data: 8.12e-04). ETA=5:15:32, max mem: 20.9 GB 
[11/25 08:19:06 visual_prompt]: Epoch 59 / 100: avg data time: 1.37e-01, avg batch time: 0.9676, average train loss: 33.6481
[11/25 08:20:01 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3097, average loss: 35.2008
[11/25 08:20:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.83	
[11/25 08:20:01 visual_prompt]: Stopping early.
[11/25 08:20:01 visual_prompt]: Rank of current process: 0. World size: 1
[11/25 08:20:01 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/25 08:20:01 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/25 08:20:01 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/25 08:20:01 visual_prompt]: Training with config:
[11/25 08:20:01 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/25 08:20:01 visual_prompt]: Loading training data...
[11/25 08:20:01 visual_prompt]: Constructing mammo-cbis dataset train...
[11/25 08:20:01 visual_prompt]: Loading validation data...
[11/25 08:20:01 visual_prompt]: Constructing mammo-cbis dataset val...
[11/25 08:20:01 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/25 08:20:04 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/25 08:20:04 visual_prompt]: tuned percent:0.525
[11/25 08:20:04 visual_prompt]: Device used for model: 0
[11/25 08:20:04 visual_prompt]: Setting up Evaluator...
[11/25 08:20:04 visual_prompt]: Setting up Trainer...
[11/25 08:20:04 visual_prompt]: 	Setting up the optimizer...
[11/25 08:20:04 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/25 08:21:44 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8172 s / batch. (data: 2.84e-04). ETA=12:31:47, max mem: 20.9 GB 
[11/25 08:23:18 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8370 s / batch. (data: 2.85e-04). ETA=12:48:37, max mem: 20.9 GB 
[11/25 08:24:57 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8529 s / batch. (data: 5.46e-03). ETA=13:01:47, max mem: 20.9 GB 
[11/25 08:26:31 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8178 s / batch. (data: 2.92e-04). ETA=12:28:14, max mem: 20.9 GB 
[11/25 08:28:08 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8265 s / batch. (data: 7.98e-03). ETA=12:34:54, max mem: 20.9 GB 
[11/25 08:28:59 visual_prompt]: Epoch 1 / 100: avg data time: 1.34e-01, avg batch time: 0.9674, average train loss: 1.5403
[11/25 08:29:53 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3093, average loss: 1.5201
[11/25 08:29:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/25 08:29:53 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/25 08:31:32 visual_prompt]: 	Training 100/553. train loss: 7.2424,	0.8342 s / batch. (data: 4.40e-04). ETA=12:39:44, max mem: 20.9 GB 
[11/25 08:33:07 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.1719 s / batch. (data: 3.32e-01). ETA=17:45:22, max mem: 20.9 GB 
[11/25 08:34:47 visual_prompt]: 	Training 300/553. train loss: 4.3327,	1.2893 s / batch. (data: 4.60e-01). ETA=19:30:00, max mem: 20.9 GB 
[11/25 08:36:47 visual_prompt]: 	Training 400/553. train loss: 1.6245,	0.8534 s / batch. (data: 3.43e-04). ETA=12:52:57, max mem: 20.9 GB 
[11/25 08:38:31 visual_prompt]: 	Training 500/553. train loss: 1.6753,	0.8265 s / batch. (data: 3.21e-04). ETA=12:27:13, max mem: 20.9 GB 
[11/25 08:39:20 visual_prompt]: Epoch 2 / 100: avg data time: 1.94e-01, avg batch time: 1.0260, average train loss: 12.4687
[11/25 08:40:16 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3098, average loss: 17.9150
[11/25 08:40:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.10	
[11/25 08:40:16 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/25 08:41:56 visual_prompt]: 	Training 100/553. train loss: 28.1278,	0.8388 s / batch. (data: 2.99e-04). ETA=12:36:12, max mem: 20.9 GB 
[11/25 08:43:33 visual_prompt]: 	Training 200/553. train loss: 10.4513,	0.8280 s / batch. (data: 3.15e-04). ETA=12:25:07, max mem: 20.9 GB 
[11/25 08:45:08 visual_prompt]: 	Training 300/553. train loss: 2.1308,	0.8241 s / batch. (data: 2.87e-04). ETA=12:20:11, max mem: 20.9 GB 
[11/25 08:46:46 visual_prompt]: 	Training 400/553. train loss: 64.6666,	0.9209 s / batch. (data: 8.89e-02). ETA=13:45:40, max mem: 20.9 GB 
[11/25 08:48:23 visual_prompt]: 	Training 500/553. train loss: 6.3802,	1.0603 s / batch. (data: 2.25e-01). ETA=15:48:52, max mem: 20.9 GB 
[11/25 08:49:13 visual_prompt]: Epoch 3 / 100: avg data time: 1.38e-01, avg batch time: 0.9710, average train loss: 13.6450
[11/25 08:50:07 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3102, average loss: 12.2957
[11/25 08:50:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.88	
[11/25 08:50:07 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/25 08:51:48 visual_prompt]: 	Training 100/553. train loss: 14.1667,	0.8541 s / batch. (data: 1.40e-02). ETA=12:42:07, max mem: 20.9 GB 
[11/25 08:53:24 visual_prompt]: 	Training 200/553. train loss: 13.1459,	0.8363 s / batch. (data: 3.20e-04). ETA=12:24:50, max mem: 20.9 GB 
[11/25 08:55:00 visual_prompt]: 	Training 300/553. train loss: 5.1815,	1.3978 s / batch. (data: 5.65e-01). ETA=20:42:38, max mem: 20.9 GB 
[11/25 08:56:32 visual_prompt]: 	Training 400/553. train loss: 0.4137,	1.2166 s / batch. (data: 3.82e-01). ETA=17:59:30, max mem: 20.9 GB 
[11/25 08:58:10 visual_prompt]: 	Training 500/553. train loss: 0.0000,	3.3040 s / batch. (data: 2.47e+00). ETA=2 days, 0:46:17, max mem: 20.9 GB 
[11/25 08:59:01 visual_prompt]: Epoch 4 / 100: avg data time: 1.32e-01, avg batch time: 0.9658, average train loss: 18.2450
[11/25 08:59:56 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3109, average loss: 5.3036
[11/25 08:59:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[11/25 08:59:56 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/25 09:01:34 visual_prompt]: 	Training 100/553. train loss: 83.4474,	0.8320 s / batch. (data: 7.96e-03). ETA=12:14:43, max mem: 20.9 GB 
[11/25 09:03:11 visual_prompt]: 	Training 200/553. train loss: 0.9112,	1.1480 s / batch. (data: 3.29e-01). ETA=16:51:54, max mem: 20.9 GB 
[11/25 09:04:48 visual_prompt]: 	Training 300/553. train loss: 73.5406,	0.8400 s / batch. (data: 1.61e-02). ETA=12:19:02, max mem: 20.9 GB 
[11/25 09:06:23 visual_prompt]: 	Training 400/553. train loss: 2.6459,	0.8328 s / batch. (data: 5.43e-03). ETA=12:11:17, max mem: 20.9 GB 
[11/25 09:07:59 visual_prompt]: 	Training 500/553. train loss: 22.0819,	0.8332 s / batch. (data: 1.20e-02). ETA=12:10:16, max mem: 20.9 GB 
[11/25 09:08:50 visual_prompt]: Epoch 5 / 100: avg data time: 1.34e-01, avg batch time: 0.9659, average train loss: 25.7894
[11/25 09:09:45 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3098, average loss: 3.6946
[11/25 09:09:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.25	
[11/25 09:09:45 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/25 09:11:25 visual_prompt]: 	Training 100/553. train loss: 24.2818,	0.8180 s / batch. (data: 1.04e-02). ETA=11:54:53, max mem: 20.9 GB 
[11/25 09:13:01 visual_prompt]: 	Training 200/553. train loss: 99.6738,	0.8603 s / batch. (data: 2.42e-02). ETA=12:30:22, max mem: 20.9 GB 
[11/25 09:14:35 visual_prompt]: 	Training 300/553. train loss: 8.5172,	0.8325 s / batch. (data: 3.23e-04). ETA=12:04:45, max mem: 20.9 GB 
[11/25 09:16:15 visual_prompt]: 	Training 400/553. train loss: 18.0969,	0.8427 s / batch. (data: 1.08e-02). ETA=12:12:12, max mem: 20.9 GB 
[11/25 09:17:49 visual_prompt]: 	Training 500/553. train loss: 11.3561,	0.8363 s / batch. (data: 5.48e-03). ETA=12:05:14, max mem: 20.9 GB 
[11/25 09:18:39 visual_prompt]: Epoch 6 / 100: avg data time: 1.33e-01, avg batch time: 0.9655, average train loss: 23.5509
[11/25 09:19:33 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3090, average loss: 6.1940
[11/25 09:19:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.42	
[11/25 09:19:33 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/25 09:21:11 visual_prompt]: 	Training 100/553. train loss: 71.1854,	0.8291 s / batch. (data: 3.13e-04). ETA=11:56:54, max mem: 20.9 GB 
[11/25 09:22:48 visual_prompt]: 	Training 200/553. train loss: 5.1444,	0.8320 s / batch. (data: 2.99e-04). ETA=11:58:02, max mem: 20.9 GB 
[11/25 09:24:26 visual_prompt]: 	Training 300/553. train loss: 18.4046,	1.0480 s / batch. (data: 2.01e-01). ETA=15:02:42, max mem: 20.9 GB 
[11/25 09:26:03 visual_prompt]: 	Training 400/553. train loss: 13.8397,	1.8186 s / batch. (data: 9.93e-01). ETA=1 day, 2:03:26, max mem: 20.9 GB 
[11/25 09:27:38 visual_prompt]: 	Training 500/553. train loss: 68.1425,	0.8400 s / batch. (data: 3.23e-04). ETA=12:00:44, max mem: 20.9 GB 
[11/25 09:28:27 visual_prompt]: Epoch 7 / 100: avg data time: 1.32e-01, avg batch time: 0.9642, average train loss: 24.9745
[11/25 09:29:21 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3086, average loss: 6.3741
[11/25 09:29:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.37	
[11/25 09:29:21 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/25 09:31:00 visual_prompt]: 	Training 100/553. train loss: 69.9379,	0.8120 s / batch. (data: 4.67e-04). ETA=11:34:37, max mem: 20.9 GB 
[11/25 09:32:37 visual_prompt]: 	Training 200/553. train loss: 120.0211,	0.8272 s / batch. (data: 5.42e-03). ETA=11:46:14, max mem: 20.9 GB 
[11/25 09:34:13 visual_prompt]: 	Training 300/553. train loss: 14.7993,	0.8490 s / batch. (data: 1.05e-02). ETA=12:03:29, max mem: 20.9 GB 
[11/25 09:35:48 visual_prompt]: 	Training 400/553. train loss: 60.3261,	0.8539 s / batch. (data: 5.42e-03). ETA=12:06:15, max mem: 20.9 GB 
[11/25 09:37:25 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.2520 s / batch. (data: 4.11e-01). ETA=17:42:43, max mem: 20.9 GB 
[11/25 09:38:15 visual_prompt]: Epoch 8 / 100: avg data time: 1.35e-01, avg batch time: 0.9653, average train loss: 35.4590
[11/25 09:39:10 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3089, average loss: 29.6084
[11/25 09:39:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.89	
[11/25 09:39:10 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/25 09:40:50 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8162 s / batch. (data: 5.43e-03). ETA=11:30:45, max mem: 20.9 GB 
[11/25 09:42:25 visual_prompt]: 	Training 200/553. train loss: 17.5484,	0.8406 s / batch. (data: 1.60e-02). ETA=11:49:57, max mem: 20.9 GB 
[11/25 09:44:02 visual_prompt]: 	Training 300/553. train loss: 11.5687,	1.4747 s / batch. (data: 6.42e-01). ETA=20:43:06, max mem: 20.9 GB 
[11/25 09:45:38 visual_prompt]: 	Training 400/553. train loss: 20.6134,	0.8507 s / batch. (data: 1.47e-02). ETA=11:55:39, max mem: 20.9 GB 
[11/25 09:47:15 visual_prompt]: 	Training 500/553. train loss: 41.5786,	0.8611 s / batch. (data: 2.11e-02). ETA=12:02:58, max mem: 20.9 GB 
[11/25 09:48:05 visual_prompt]: Epoch 9 / 100: avg data time: 1.34e-01, avg batch time: 0.9666, average train loss: 28.5791
[11/25 09:48:59 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3117, average loss: 23.8281
[11/25 09:49:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.98	
[11/25 09:49:00 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/25 09:50:42 visual_prompt]: 	Training 100/553. train loss: 82.2706,	0.8310 s / batch. (data: 1.78e-02). ETA=11:35:37, max mem: 20.9 GB 
[11/25 09:52:16 visual_prompt]: 	Training 200/553. train loss: 1.1150,	0.8320 s / batch. (data: 2.96e-04). ETA=11:35:00, max mem: 20.9 GB 
[11/25 09:53:52 visual_prompt]: 	Training 300/553. train loss: 30.2138,	0.8625 s / batch. (data: 1.19e-02). ETA=11:59:04, max mem: 20.9 GB 
[11/25 09:55:25 visual_prompt]: 	Training 400/553. train loss: 40.2294,	0.8273 s / batch. (data: 3.09e-04). ETA=11:28:23, max mem: 20.9 GB 
[11/25 09:57:03 visual_prompt]: 	Training 500/553. train loss: 7.8690,	0.8440 s / batch. (data: 2.52e-04). ETA=11:40:50, max mem: 20.9 GB 
[11/25 09:57:53 visual_prompt]: Epoch 10 / 100: avg data time: 1.34e-01, avg batch time: 0.9638, average train loss: 33.4559
[11/25 09:58:47 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3094, average loss: 1.7496
[11/25 09:58:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.90	
[11/25 09:58:47 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/25 10:00:29 visual_prompt]: 	Training 100/553. train loss: 52.0340,	0.8480 s / batch. (data: 4.40e-04). ETA=11:41:58, max mem: 20.9 GB 
[11/25 10:02:06 visual_prompt]: 	Training 200/553. train loss: 52.9471,	0.8197 s / batch. (data: 2.92e-04). ETA=11:17:11, max mem: 20.9 GB 
[11/25 10:03:41 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.9960 s / batch. (data: 1.15e+00). ETA=1 day, 3:25:43, max mem: 20.9 GB 
[11/25 10:05:16 visual_prompt]: 	Training 400/553. train loss: 18.9793,	0.8292 s / batch. (data: 5.37e-03). ETA=11:22:17, max mem: 20.9 GB 
[11/25 10:06:51 visual_prompt]: 	Training 500/553. train loss: 18.9895,	0.8440 s / batch. (data: 1.19e-02). ETA=11:33:03, max mem: 20.9 GB 
[11/25 10:07:41 visual_prompt]: Epoch 11 / 100: avg data time: 1.35e-01, avg batch time: 0.9655, average train loss: 33.8645
[11/25 10:08:35 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3095, average loss: 56.5795
[11/25 10:08:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.93	
[11/25 10:08:35 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/25 10:10:16 visual_prompt]: 	Training 100/553. train loss: 4.4777,	0.8440 s / batch. (data: 7.94e-03). ETA=11:30:54, max mem: 20.9 GB 
[11/25 10:11:53 visual_prompt]: 	Training 200/553. train loss: 12.2439,	0.8480 s / batch. (data: 7.36e-04). ETA=11:32:46, max mem: 20.9 GB 
[11/25 10:13:28 visual_prompt]: 	Training 300/553. train loss: 6.2091,	0.8240 s / batch. (data: 2.90e-04). ETA=11:11:47, max mem: 20.9 GB 
[11/25 10:15:04 visual_prompt]: 	Training 400/553. train loss: 36.6586,	0.8440 s / batch. (data: 3.30e-04). ETA=11:26:40, max mem: 20.9 GB 
[11/25 10:16:41 visual_prompt]: 	Training 500/553. train loss: 194.9420,	0.8360 s / batch. (data: 2.98e-04). ETA=11:18:48, max mem: 20.9 GB 
[11/25 10:17:30 visual_prompt]: Epoch 12 / 100: avg data time: 1.37e-01, avg batch time: 0.9669, average train loss: 38.2417
[11/25 10:18:25 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3102, average loss: 78.2413
[11/25 10:18:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.06	
[11/25 10:18:25 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/25 10:20:06 visual_prompt]: 	Training 100/553. train loss: 25.1437,	0.8174 s / batch. (data: 1.19e-02). ETA=11:01:37, max mem: 20.9 GB 
[11/25 10:21:40 visual_prompt]: 	Training 200/553. train loss: 12.9302,	0.8477 s / batch. (data: 1.16e-02). ETA=11:24:42, max mem: 20.9 GB 
[11/25 10:23:16 visual_prompt]: 	Training 300/553. train loss: 32.4434,	1.4171 s / batch. (data: 6.10e-01). ETA=19:02:15, max mem: 20.9 GB 
[11/25 10:24:51 visual_prompt]: 	Training 400/553. train loss: 112.1674,	0.8395 s / batch. (data: 1.05e-02). ETA=11:15:18, max mem: 20.9 GB 
[11/25 10:26:28 visual_prompt]: 	Training 500/553. train loss: 73.3396,	0.8195 s / batch. (data: 2.95e-04). ETA=10:57:51, max mem: 20.9 GB 
[11/25 10:27:18 visual_prompt]: Epoch 13 / 100: avg data time: 1.34e-01, avg batch time: 0.9636, average train loss: 42.1476
[11/25 10:28:12 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3102, average loss: 14.5400
[11/25 10:28:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 61.97	
[11/25 10:28:12 visual_prompt]: Best epoch 13: best metric: -14.540
[11/25 10:28:12 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/25 10:29:53 visual_prompt]: 	Training 100/553. train loss: 32.7932,	0.8440 s / batch. (data: 2.89e-04). ETA=11:15:19, max mem: 20.9 GB 
[11/25 10:31:28 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8405 s / batch. (data: 1.05e-02). ETA=11:11:10, max mem: 20.9 GB 
[11/25 10:33:03 visual_prompt]: 	Training 300/553. train loss: 16.0893,	0.8374 s / batch. (data: 3.22e-04). ETA=11:07:17, max mem: 20.9 GB 
[11/25 10:34:38 visual_prompt]: 	Training 400/553. train loss: 9.4391,	0.8440 s / batch. (data: 2.69e-04). ETA=11:11:06, max mem: 20.9 GB 
[11/25 10:36:13 visual_prompt]: 	Training 500/553. train loss: 11.7443,	0.8572 s / batch. (data: 1.55e-02). ETA=11:20:14, max mem: 20.9 GB 
[11/25 10:37:03 visual_prompt]: Epoch 14 / 100: avg data time: 1.26e-01, avg batch time: 0.9592, average train loss: 26.6456
[11/25 10:37:57 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3094, average loss: 7.4371
[11/25 10:37:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.78	
[11/25 10:37:57 visual_prompt]: Best epoch 14: best metric: -7.437
[11/25 10:37:57 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/25 10:39:37 visual_prompt]: 	Training 100/553. train loss: 49.4517,	0.8454 s / batch. (data: 7.66e-03). ETA=11:08:40, max mem: 20.9 GB 
[11/25 10:41:10 visual_prompt]: 	Training 200/553. train loss: 73.2785,	0.8427 s / batch. (data: 6.64e-03). ETA=11:05:08, max mem: 20.9 GB 
[11/25 10:42:47 visual_prompt]: 	Training 300/553. train loss: 45.8299,	0.8279 s / batch. (data: 3.34e-04). ETA=10:52:06, max mem: 20.9 GB 
[11/25 10:44:20 visual_prompt]: 	Training 400/553. train loss: 3.9994,	0.9200 s / batch. (data: 5.37e-02). ETA=12:03:05, max mem: 20.9 GB 
[11/25 10:45:58 visual_prompt]: 	Training 500/553. train loss: 12.0135,	0.8312 s / batch. (data: 5.58e-03). ETA=10:51:54, max mem: 20.9 GB 
[11/25 10:46:48 visual_prompt]: Epoch 15 / 100: avg data time: 1.30e-01, avg batch time: 0.9605, average train loss: 42.6542
[11/25 10:47:43 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3094, average loss: 119.1599
[11/25 10:47:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.93	
[11/25 10:47:43 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/25 10:49:22 visual_prompt]: 	Training 100/553. train loss: 28.5121,	0.8350 s / batch. (data: 2.91e-04). ETA=10:52:47, max mem: 20.9 GB 
[11/25 10:50:57 visual_prompt]: 	Training 200/553. train loss: 4.4193,	0.8179 s / batch. (data: 3.02e-04). ETA=10:37:59, max mem: 20.9 GB 
[11/25 10:52:33 visual_prompt]: 	Training 300/553. train loss: 18.1549,	0.8558 s / batch. (data: 6.50e-04). ETA=11:06:08, max mem: 20.9 GB 
[11/25 10:54:08 visual_prompt]: 	Training 400/553. train loss: 8.0040,	0.8319 s / batch. (data: 1.56e-02). ETA=10:46:08, max mem: 20.9 GB 
[11/25 10:55:42 visual_prompt]: 	Training 500/553. train loss: 2.9585,	1.2879 s / batch. (data: 4.60e-01). ETA=16:38:14, max mem: 20.9 GB 
[11/25 10:56:33 visual_prompt]: Epoch 16 / 100: avg data time: 1.28e-01, avg batch time: 0.9588, average train loss: 32.4045
[11/25 10:57:27 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3116, average loss: 11.1458
[11/25 10:57:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.12	
[11/25 10:57:27 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/25 10:59:06 visual_prompt]: 	Training 100/553. train loss: 0.0228,	0.8432 s / batch. (data: 1.05e-02). ETA=10:51:22, max mem: 20.9 GB 
[11/25 11:00:43 visual_prompt]: 	Training 200/553. train loss: 133.8049,	0.8357 s / batch. (data: 5.34e-03). ETA=10:44:10, max mem: 20.9 GB 
[11/25 11:02:18 visual_prompt]: 	Training 300/553. train loss: 80.5348,	0.8480 s / batch. (data: 2.58e-04). ETA=10:52:19, max mem: 20.9 GB 
[11/25 11:03:54 visual_prompt]: 	Training 400/553. train loss: 0.2826,	1.0920 s / batch. (data: 2.54e-01). ETA=13:58:09, max mem: 20.9 GB 
[11/25 11:05:30 visual_prompt]: 	Training 500/553. train loss: 58.0921,	1.4932 s / batch. (data: 6.83e-01). ETA=19:03:33, max mem: 20.9 GB 
[11/25 11:06:21 visual_prompt]: Epoch 17 / 100: avg data time: 1.34e-01, avg batch time: 0.9651, average train loss: 34.7385
[11/25 11:07:16 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3113, average loss: 13.7672
[11/25 11:07:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 65.09	
[11/25 11:07:16 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/25 11:08:55 visual_prompt]: 	Training 100/553. train loss: 1.1119,	0.8281 s / batch. (data: 3.03e-04). ETA=10:32:05, max mem: 20.9 GB 
[11/25 11:10:34 visual_prompt]: 	Training 200/553. train loss: 7.7370,	0.8215 s / batch. (data: 2.47e-04). ETA=10:25:43, max mem: 20.9 GB 
[11/25 11:12:10 visual_prompt]: 	Training 300/553. train loss: 15.1899,	0.8200 s / batch. (data: 3.06e-04). ETA=10:23:09, max mem: 20.9 GB 
[11/25 11:13:46 visual_prompt]: 	Training 400/553. train loss: 19.8836,	0.8360 s / batch. (data: 4.75e-04). ETA=10:33:55, max mem: 20.9 GB 
[11/25 11:15:21 visual_prompt]: 	Training 500/553. train loss: 7.3998,	0.8402 s / batch. (data: 3.43e-04). ETA=10:35:42, max mem: 20.9 GB 
[11/25 11:16:11 visual_prompt]: Epoch 18 / 100: avg data time: 1.36e-01, avg batch time: 0.9665, average train loss: 38.3788
[11/25 11:17:05 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3094, average loss: 55.3526
[11/25 11:17:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.85	
[11/25 11:17:05 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/25 11:18:45 visual_prompt]: 	Training 100/553. train loss: 26.4222,	1.0070 s / batch. (data: 1.89e-01). ETA=12:39:21, max mem: 20.9 GB 
[11/25 11:20:21 visual_prompt]: 	Training 200/553. train loss: 3.7860,	0.8320 s / batch. (data: 5.41e-03). ETA=10:26:01, max mem: 20.9 GB 
[11/25 11:21:58 visual_prompt]: 	Training 300/553. train loss: 22.6057,	0.8213 s / batch. (data: 3.17e-04). ETA=10:16:36, max mem: 20.9 GB 
[11/25 11:23:35 visual_prompt]: 	Training 400/553. train loss: 12.2948,	0.8320 s / batch. (data: 7.65e-04). ETA=10:23:15, max mem: 20.9 GB 
[11/25 11:25:07 visual_prompt]: 	Training 500/553. train loss: 7.6475,	0.8503 s / batch. (data: 1.03e-02). ETA=10:35:32, max mem: 20.9 GB 
[11/25 11:25:57 visual_prompt]: Epoch 19 / 100: avg data time: 1.30e-01, avg batch time: 0.9619, average train loss: 27.2900
[11/25 11:26:52 visual_prompt]: Inference (val):avg data time: 3.21e-04, avg batch time: 0.3105, average loss: 93.1748
[11/25 11:26:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.80	
[11/25 11:26:52 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/25 11:28:30 visual_prompt]: 	Training 100/553. train loss: 39.9212,	0.8435 s / batch. (data: 2.29e-02). ETA=10:28:20, max mem: 20.9 GB 
[11/25 11:30:07 visual_prompt]: 	Training 200/553. train loss: 2.5359,	0.9015 s / batch. (data: 6.98e-02). ETA=11:10:02, max mem: 20.9 GB 
[11/25 11:31:43 visual_prompt]: 	Training 300/553. train loss: 85.2379,	0.8214 s / batch. (data: 2.98e-04). ETA=10:09:08, max mem: 20.9 GB 
[11/25 11:33:20 visual_prompt]: 	Training 400/553. train loss: 90.9195,	0.8299 s / batch. (data: 1.20e-02). ETA=10:14:00, max mem: 20.9 GB 
[11/25 11:34:55 visual_prompt]: 	Training 500/553. train loss: 29.1014,	0.8398 s / batch. (data: 1.05e-02). ETA=10:19:58, max mem: 20.9 GB 
[11/25 11:35:46 visual_prompt]: Epoch 20 / 100: avg data time: 1.35e-01, avg batch time: 0.9662, average train loss: 36.6560
[11/25 11:36:41 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3093, average loss: 14.7902
[11/25 11:36:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.40	
[11/25 11:36:41 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/25 11:38:22 visual_prompt]: 	Training 100/553. train loss: 19.7591,	0.8478 s / batch. (data: 2.46e-02). ETA=10:23:41, max mem: 20.9 GB 
[11/25 11:39:57 visual_prompt]: 	Training 200/553. train loss: 0.0096,	0.8175 s / batch. (data: 2.92e-04). ETA=10:00:01, max mem: 20.9 GB 
[11/25 11:41:32 visual_prompt]: 	Training 300/553. train loss: 136.8840,	1.0160 s / batch. (data: 1.67e-01). ETA=12:24:02, max mem: 20.9 GB 
[11/25 11:43:06 visual_prompt]: 	Training 400/553. train loss: 25.3831,	0.8394 s / batch. (data: 2.91e-04). ETA=10:13:20, max mem: 20.9 GB 
[11/25 11:44:44 visual_prompt]: 	Training 500/553. train loss: 31.8285,	0.8835 s / batch. (data: 1.55e-02). ETA=10:44:05, max mem: 20.9 GB 
[11/25 11:45:33 visual_prompt]: Epoch 21 / 100: avg data time: 1.30e-01, avg batch time: 0.9613, average train loss: 28.5649
[11/25 11:46:27 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3107, average loss: 14.4644
[11/25 11:46:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 67.94	
[11/25 11:46:27 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/25 11:48:05 visual_prompt]: 	Training 100/553. train loss: 26.2604,	0.8240 s / batch. (data: 2.94e-04). ETA=9:58:36, max mem: 20.9 GB 
[11/25 11:49:40 visual_prompt]: 	Training 200/553. train loss: 38.4190,	0.8320 s / batch. (data: 3.37e-04). ETA=10:02:59, max mem: 20.9 GB 
[11/25 11:51:13 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8397 s / batch. (data: 3.79e-04). ETA=10:07:10, max mem: 20.9 GB 
[11/25 11:52:49 visual_prompt]: 	Training 400/553. train loss: 8.0782,	0.8324 s / batch. (data: 2.91e-04). ETA=10:00:30, max mem: 20.9 GB 
[11/25 11:54:24 visual_prompt]: 	Training 500/553. train loss: 24.0354,	0.8240 s / batch. (data: 3.18e-04). ETA=9:53:06, max mem: 20.9 GB 
[11/25 11:55:15 visual_prompt]: Epoch 22 / 100: avg data time: 1.24e-01, avg batch time: 0.9550, average train loss: 31.6379
[11/25 11:56:09 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3125, average loss: 22.8661
[11/25 11:56:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.05	
[11/25 11:56:09 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/25 11:57:49 visual_prompt]: 	Training 100/553. train loss: 69.6765,	0.8160 s / batch. (data: 2.83e-04). ETA=9:45:15, max mem: 20.9 GB 
[11/25 11:59:24 visual_prompt]: 	Training 200/553. train loss: 26.6767,	0.8280 s / batch. (data: 2.96e-04). ETA=9:52:30, max mem: 20.9 GB 
[11/25 12:01:00 visual_prompt]: 	Training 300/553. train loss: 4.3607,	0.8600 s / batch. (data: 1.60e-02). ETA=10:13:56, max mem: 20.9 GB 
[11/25 12:02:33 visual_prompt]: 	Training 400/553. train loss: 16.4394,	0.8196 s / batch. (data: 1.06e-02). ETA=9:43:44, max mem: 20.9 GB 
[11/25 12:04:06 visual_prompt]: 	Training 500/553. train loss: 0.2407,	0.8520 s / batch. (data: 2.93e-04). ETA=10:05:23, max mem: 20.9 GB 
[11/25 12:04:56 visual_prompt]: Epoch 23 / 100: avg data time: 1.22e-01, avg batch time: 0.9530, average train loss: 26.8034
[11/25 12:05:50 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3104, average loss: 13.1462
[11/25 12:05:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.32	
[11/25 12:05:50 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/25 12:07:26 visual_prompt]: 	Training 100/553. train loss: 6.5169,	0.8273 s / batch. (data: 2.92e-04). ETA=9:45:43, max mem: 20.9 GB 
[11/25 12:09:00 visual_prompt]: 	Training 200/553. train loss: 11.6133,	0.8445 s / batch. (data: 1.20e-02). ETA=9:56:31, max mem: 20.9 GB 
[11/25 12:10:36 visual_prompt]: 	Training 300/553. train loss: 7.3411,	0.8187 s / batch. (data: 2.75e-04). ETA=9:36:55, max mem: 20.9 GB 
[11/25 12:12:11 visual_prompt]: 	Training 400/553. train loss: 23.0182,	0.8293 s / batch. (data: 7.95e-03). ETA=9:43:00, max mem: 20.9 GB 
[11/25 12:13:48 visual_prompt]: 	Training 500/553. train loss: 8.7479,	0.8556 s / batch. (data: 3.12e-04). ETA=10:00:02, max mem: 20.9 GB 
[11/25 12:14:38 visual_prompt]: Epoch 24 / 100: avg data time: 1.24e-01, avg batch time: 0.9545, average train loss: 27.4048
[11/25 12:15:32 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3096, average loss: 12.1995
[11/25 12:15:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 65.38	
[11/25 12:15:32 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/25 12:17:13 visual_prompt]: 	Training 100/553. train loss: 59.5628,	0.8256 s / batch. (data: 4.17e-04). ETA=9:36:56, max mem: 20.9 GB 
[11/25 12:18:45 visual_prompt]: 	Training 200/553. train loss: 14.7123,	1.0120 s / batch. (data: 1.78e-01). ETA=11:45:29, max mem: 20.9 GB 
[11/25 12:20:20 visual_prompt]: 	Training 300/553. train loss: 24.4865,	0.8473 s / batch. (data: 3.53e-04). ETA=9:49:15, max mem: 20.9 GB 
[11/25 12:21:54 visual_prompt]: 	Training 400/553. train loss: 0.6782,	1.0400 s / batch. (data: 2.00e-01). ETA=12:01:32, max mem: 20.9 GB 
[11/25 12:23:29 visual_prompt]: 	Training 500/553. train loss: 9.5160,	1.2562 s / batch. (data: 4.21e-01). ETA=14:29:29, max mem: 20.9 GB 
[11/25 12:24:19 visual_prompt]: Epoch 25 / 100: avg data time: 1.23e-01, avg batch time: 0.9544, average train loss: 27.0481
[11/25 12:25:13 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3124, average loss: 73.2002
[11/25 12:25:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.01	
[11/25 12:25:13 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/25 12:26:52 visual_prompt]: 	Training 100/553. train loss: 6.2010,	0.8280 s / batch. (data: 2.86e-04). ETA=9:30:58, max mem: 20.9 GB 
[11/25 12:28:27 visual_prompt]: 	Training 200/553. train loss: 49.9791,	1.4704 s / batch. (data: 6.41e-01). ETA=16:51:32, max mem: 20.9 GB 
[11/25 12:30:04 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8578 s / batch. (data: 3.37e-02). ETA=9:48:41, max mem: 20.9 GB 
[11/25 12:31:37 visual_prompt]: 	Training 400/553. train loss: 3.7007,	0.8307 s / batch. (data: 1.20e-02). ETA=9:28:40, max mem: 20.9 GB 
[11/25 12:33:11 visual_prompt]: 	Training 500/553. train loss: 2.9932,	0.8433 s / batch. (data: 1.13e-02). ETA=9:35:56, max mem: 20.9 GB 
[11/25 12:34:00 visual_prompt]: Epoch 26 / 100: avg data time: 1.21e-01, avg batch time: 0.9523, average train loss: 27.0992
[11/25 12:34:54 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3110, average loss: 12.7327
[11/25 12:34:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 70.10	
[11/25 12:34:54 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[11/25 12:36:33 visual_prompt]: 	Training 100/553. train loss: 9.6838,	0.8320 s / batch. (data: 2.90e-04). ETA=9:26:03, max mem: 20.9 GB 
[11/25 12:38:07 visual_prompt]: 	Training 200/553. train loss: 61.5466,	0.8266 s / batch. (data: 1.19e-02). ETA=9:21:00, max mem: 20.9 GB 
[11/25 12:39:43 visual_prompt]: 	Training 300/553. train loss: 41.0477,	0.8080 s / batch. (data: 3.51e-04). ETA=9:07:02, max mem: 20.9 GB 
[11/25 12:41:19 visual_prompt]: 	Training 400/553. train loss: 66.3537,	0.8111 s / batch. (data: 3.17e-04). ETA=9:07:46, max mem: 20.9 GB 
[11/25 12:42:54 visual_prompt]: 	Training 500/553. train loss: 35.7847,	0.8262 s / batch. (data: 2.91e-04). ETA=9:16:37, max mem: 20.9 GB 
[11/25 12:43:42 visual_prompt]: Epoch 27 / 100: avg data time: 1.24e-01, avg batch time: 0.9549, average train loss: 27.1342
[11/25 12:44:36 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3113, average loss: 86.1610
[11/25 12:44:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 68.93	
[11/25 12:44:36 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[11/25 12:46:31 visual_prompt]: 	Training 100/553. train loss: 76.4337,	0.8107 s / batch. (data: 3.20e-04). ETA=9:04:05, max mem: 20.9 GB 
[11/25 12:48:12 visual_prompt]: 	Training 200/553. train loss: 29.6996,	0.8480 s / batch. (data: 3.24e-04). ETA=9:27:43, max mem: 20.9 GB 
[11/25 12:49:51 visual_prompt]: 	Training 300/553. train loss: 10.3368,	1.4046 s / batch. (data: 5.65e-01). ETA=15:38:00, max mem: 20.9 GB 
[11/25 12:51:29 visual_prompt]: 	Training 400/553. train loss: 18.3533,	0.8376 s / batch. (data: 5.42e-03). ETA=9:17:59, max mem: 20.9 GB 
[11/25 12:53:07 visual_prompt]: 	Training 500/553. train loss: 108.3515,	0.8291 s / batch. (data: 1.19e-02). ETA=9:10:54, max mem: 20.9 GB 
[11/25 12:54:00 visual_prompt]: Epoch 28 / 100: avg data time: 1.91e-01, avg batch time: 1.0199, average train loss: 27.1525
[11/25 12:54:58 visual_prompt]: Inference (val):avg data time: 1.68e-04, avg batch time: 0.3096, average loss: 49.8572
[11/25 12:54:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 69.52	
[11/25 12:54:58 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[11/25 12:56:45 visual_prompt]: 	Training 100/553. train loss: 23.9725,	0.8368 s / batch. (data: 5.42e-03). ETA=9:13:54, max mem: 20.9 GB 
[11/25 12:58:22 visual_prompt]: 	Training 200/553. train loss: 3.4627,	1.7724 s / batch. (data: 9.47e-01). ETA=19:30:15, max mem: 20.9 GB 
[11/25 12:59:58 visual_prompt]: 	Training 300/553. train loss: 2.1530,	0.8431 s / batch. (data: 8.05e-04). ETA=9:15:14, max mem: 20.9 GB 
[11/25 13:01:32 visual_prompt]: 	Training 400/553. train loss: 68.9398,	1.3956 s / batch. (data: 5.90e-01). ETA=15:16:50, max mem: 20.9 GB 
[11/25 13:03:10 visual_prompt]: 	Training 500/553. train loss: 11.5358,	0.8281 s / batch. (data: 3.41e-04). ETA=9:02:38, max mem: 20.9 GB 
[11/25 13:04:00 visual_prompt]: Epoch 29 / 100: avg data time: 1.51e-01, avg batch time: 0.9815, average train loss: 33.5493
[11/25 13:04:56 visual_prompt]: Inference (val):avg data time: 2.21e-04, avg batch time: 0.3112, average loss: 43.7048
[11/25 13:04:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.65	
[11/25 13:04:56 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[11/25 13:06:37 visual_prompt]: 	Training 100/553. train loss: 17.2615,	0.8480 s / batch. (data: 5.45e-03). ETA=9:13:30, max mem: 20.9 GB 
[11/25 13:08:15 visual_prompt]: 	Training 200/553. train loss: 0.0042,	0.8320 s / batch. (data: 3.08e-04). ETA=9:01:40, max mem: 20.9 GB 
[11/25 13:09:50 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.1440 s / batch. (data: 3.00e-01). ETA=12:22:53, max mem: 20.9 GB 
[11/25 13:11:29 visual_prompt]: 	Training 400/553. train loss: 9.7025,	1.3470 s / batch. (data: 5.10e-01). ETA=14:32:27, max mem: 20.9 GB 
[11/25 13:13:07 visual_prompt]: 	Training 500/553. train loss: 17.4095,	1.1360 s / batch. (data: 3.09e-01). ETA=12:13:55, max mem: 20.9 GB 
[11/25 13:14:01 visual_prompt]: Epoch 30 / 100: avg data time: 1.53e-01, avg batch time: 0.9843, average train loss: 20.2606
[11/25 13:14:57 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3092, average loss: 9.6827
[11/25 13:14:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 69.78	
[11/25 13:14:57 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[11/25 13:16:40 visual_prompt]: 	Training 100/553. train loss: 0.8287,	0.8242 s / batch. (data: 1.20e-02). ETA=8:50:24, max mem: 20.9 GB 
[11/25 13:18:19 visual_prompt]: 	Training 200/553. train loss: 5.9134,	0.8400 s / batch. (data: 3.21e-04). ETA=8:59:08, max mem: 20.9 GB 
[11/25 13:19:53 visual_prompt]: 	Training 300/553. train loss: 25.1979,	0.8320 s / batch. (data: 1.20e-02). ETA=8:52:37, max mem: 20.9 GB 
[11/25 13:21:30 visual_prompt]: 	Training 400/553. train loss: 15.6313,	1.2052 s / batch. (data: 3.69e-01). ETA=12:49:31, max mem: 20.9 GB 
[11/25 13:23:08 visual_prompt]: 	Training 500/553. train loss: 9.3144,	0.8278 s / batch. (data: 2.64e-04). ETA=8:47:12, max mem: 20.9 GB 
[11/25 13:23:57 visual_prompt]: Epoch 31 / 100: avg data time: 1.46e-01, avg batch time: 0.9778, average train loss: 27.2723
[11/25 13:24:53 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3109, average loss: 9.3283
[11/25 13:24:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 70.34	
[11/25 13:24:53 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[11/25 13:26:35 visual_prompt]: 	Training 100/553. train loss: 0.0084,	0.8454 s / batch. (data: 1.42e-02). ETA=8:56:12, max mem: 20.9 GB 
[11/25 13:28:12 visual_prompt]: 	Training 200/553. train loss: 2.5889,	0.8528 s / batch. (data: 1.09e-02). ETA=8:59:29, max mem: 20.9 GB 
[11/25 13:29:51 visual_prompt]: 	Training 300/553. train loss: 21.6072,	0.8435 s / batch. (data: 1.16e-02). ETA=8:52:13, max mem: 20.9 GB 
[11/25 13:31:30 visual_prompt]: 	Training 400/553. train loss: 57.6236,	0.8315 s / batch. (data: 2.99e-04). ETA=8:43:13, max mem: 20.9 GB 
[11/25 13:33:05 visual_prompt]: 	Training 500/553. train loss: 11.1369,	0.8320 s / batch. (data: 3.04e-04). ETA=8:42:11, max mem: 20.9 GB 
[11/25 13:33:54 visual_prompt]: Epoch 32 / 100: avg data time: 1.45e-01, avg batch time: 0.9769, average train loss: 20.3501
[11/25 13:34:49 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3114, average loss: 23.9036
[11/25 13:34:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 70.46	
[11/25 13:34:49 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[11/25 13:36:29 visual_prompt]: 	Training 100/553. train loss: 92.1395,	0.8297 s / batch. (data: 1.20e-02). ETA=8:38:38, max mem: 20.9 GB 
[11/25 13:38:08 visual_prompt]: 	Training 200/553. train loss: 27.5268,	1.1500 s / batch. (data: 3.33e-01). ETA=11:56:55, max mem: 20.9 GB 
[11/25 13:39:44 visual_prompt]: 	Training 300/553. train loss: 9.8688,	0.8664 s / batch. (data: 2.84e-04). ETA=8:58:38, max mem: 20.9 GB 
[11/25 13:41:22 visual_prompt]: 	Training 400/553. train loss: 1.8637,	0.8314 s / batch. (data: 5.42e-03). ETA=8:35:31, max mem: 20.9 GB 
[11/25 13:42:59 visual_prompt]: 	Training 500/553. train loss: 20.8694,	0.8231 s / batch. (data: 3.23e-04). ETA=8:28:59, max mem: 20.9 GB 
[11/25 13:43:50 visual_prompt]: Epoch 33 / 100: avg data time: 1.48e-01, avg batch time: 0.9780, average train loss: 33.1611
[11/25 13:44:45 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3103, average loss: 10.2849
[11/25 13:44:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 70.90	
[11/25 13:44:45 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[11/25 13:46:27 visual_prompt]: 	Training 100/553. train loss: 16.4824,	0.8269 s / batch. (data: 3.46e-04). ETA=8:29:15, max mem: 20.9 GB 
[11/25 13:48:02 visual_prompt]: 	Training 200/553. train loss: 57.6707,	0.8238 s / batch. (data: 1.20e-02). ETA=8:25:56, max mem: 20.9 GB 
[11/25 13:49:39 visual_prompt]: 	Training 300/553. train loss: 1.1536,	0.8714 s / batch. (data: 3.31e-04). ETA=8:53:44, max mem: 20.9 GB 
[11/25 13:51:17 visual_prompt]: 	Training 400/553. train loss: 7.4392,	0.8293 s / batch. (data: 3.01e-04). ETA=8:26:35, max mem: 20.9 GB 
[11/25 13:52:54 visual_prompt]: 	Training 500/553. train loss: 8.9543,	1.4220 s / batch. (data: 5.90e-01). ETA=14:26:13, max mem: 20.9 GB 
[11/25 13:53:44 visual_prompt]: Epoch 34 / 100: avg data time: 1.42e-01, avg batch time: 0.9732, average train loss: 27.6327
[11/25 13:54:39 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3113, average loss: 16.1818
[11/25 13:54:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 71.22	
[11/25 13:54:39 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[11/25 13:56:22 visual_prompt]: 	Training 100/553. train loss: 42.2535,	0.8480 s / batch. (data: 3.46e-04). ETA=8:34:25, max mem: 20.9 GB 
[11/25 13:58:00 visual_prompt]: 	Training 200/553. train loss: 33.6156,	0.8404 s / batch. (data: 3.17e-04). ETA=8:28:26, max mem: 20.9 GB 
[11/25 13:59:36 visual_prompt]: 	Training 300/553. train loss: 25.7727,	0.8210 s / batch. (data: 3.18e-04). ETA=8:15:17, max mem: 20.9 GB 
[11/25 14:01:12 visual_prompt]: 	Training 400/553. train loss: 29.7589,	0.8200 s / batch. (data: 3.29e-04). ETA=8:13:19, max mem: 20.9 GB 
[11/25 14:02:48 visual_prompt]: 	Training 500/553. train loss: 11.0623,	1.0943 s / batch. (data: 2.62e-01). ETA=10:56:32, max mem: 20.9 GB 
[11/25 14:03:39 visual_prompt]: Epoch 35 / 100: avg data time: 1.45e-01, avg batch time: 0.9759, average train loss: 22.2768
[11/25 14:04:34 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3105, average loss: 47.3933
[11/25 14:04:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 70.17	
[11/25 14:04:34 visual_prompt]: Stopping early.
[11/25 14:04:35 visual_prompt]: Rank of current process: 0. World size: 1
[11/25 14:04:35 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/25 14:04:35 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/25 14:04:35 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/25 14:04:35 visual_prompt]: Training with config:
[11/25 14:04:35 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/25 14:04:35 visual_prompt]: Loading training data...
[11/25 14:04:35 visual_prompt]: Constructing mammo-cbis dataset train...
[11/25 14:04:35 visual_prompt]: Loading validation data...
[11/25 14:04:35 visual_prompt]: Constructing mammo-cbis dataset val...
[11/25 14:04:35 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/25 14:04:40 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/25 14:04:40 visual_prompt]: tuned percent:0.525
[11/25 14:04:40 visual_prompt]: Device used for model: 0
[11/25 14:04:40 visual_prompt]: Setting up Evaluator...
[11/25 14:04:40 visual_prompt]: Setting up Trainer...
[11/25 14:04:40 visual_prompt]: 	Setting up the optimizer...
[11/25 14:04:40 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/25 14:06:21 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8266 s / batch. (data: 1.05e-02). ETA=12:40:25, max mem: 20.9 GB 
[11/25 14:07:56 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8440 s / batch. (data: 1.20e-02). ETA=12:55:05, max mem: 20.9 GB 
[11/25 14:09:36 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.5474 s / batch. (data: 6.86e-01). ETA=23:38:25, max mem: 20.9 GB 
[11/25 14:11:11 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8320 s / batch. (data: 3.10e-04). ETA=12:41:14, max mem: 20.9 GB 
[11/25 14:12:49 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8475 s / batch. (data: 7.20e-04). ETA=12:54:01, max mem: 20.9 GB 
[11/25 14:13:41 visual_prompt]: Epoch 1 / 100: avg data time: 1.43e-01, avg batch time: 0.9769, average train loss: 1.5403
[11/25 14:14:36 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3096, average loss: 1.5201
[11/25 14:14:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/25 14:14:36 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/25 14:16:18 visual_prompt]: 	Training 100/553. train loss: 2.9190,	0.8339 s / batch. (data: 3.28e-04). ETA=12:39:31, max mem: 20.9 GB 
[11/25 14:17:54 visual_prompt]: 	Training 200/553. train loss: 0.0037,	1.1952 s / batch. (data: 3.67e-01). ETA=18:06:33, max mem: 20.9 GB 
[11/25 14:19:32 visual_prompt]: 	Training 300/553. train loss: 7.8096,	0.8973 s / batch. (data: 7.27e-02). ETA=13:34:16, max mem: 20.9 GB 
[11/25 14:21:08 visual_prompt]: 	Training 400/553. train loss: 0.6231,	0.8504 s / batch. (data: 3.03e-04). ETA=12:50:16, max mem: 20.9 GB 
[11/25 14:22:46 visual_prompt]: 	Training 500/553. train loss: 3.2929,	0.8207 s / batch. (data: 3.13e-04). ETA=12:22:00, max mem: 20.9 GB 
[11/25 14:23:35 visual_prompt]: Epoch 2 / 100: avg data time: 1.40e-01, avg batch time: 0.9742, average train loss: 3.4209
[11/25 14:24:30 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3121, average loss: 19.1750
[11/25 14:24:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.67	
[11/25 14:24:30 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/25 14:26:10 visual_prompt]: 	Training 100/553. train loss: 0.7196,	0.8360 s / batch. (data: 1.20e-02). ETA=12:33:42, max mem: 20.9 GB 
[11/25 14:27:48 visual_prompt]: 	Training 200/553. train loss: 1.8945,	1.0363 s / batch. (data: 2.08e-01). ETA=15:32:34, max mem: 20.9 GB 
[11/25 14:29:24 visual_prompt]: 	Training 300/553. train loss: 4.1691,	0.8423 s / batch. (data: 3.13e-04). ETA=12:36:32, max mem: 20.9 GB 
[11/25 14:31:02 visual_prompt]: 	Training 400/553. train loss: 55.2133,	0.8321 s / batch. (data: 3.08e-04). ETA=12:25:59, max mem: 20.9 GB 
[11/25 14:32:40 visual_prompt]: 	Training 500/553. train loss: 7.2534,	1.2394 s / batch. (data: 4.21e-01). ETA=18:29:08, max mem: 20.9 GB 
[11/25 14:33:30 visual_prompt]: Epoch 3 / 100: avg data time: 1.43e-01, avg batch time: 0.9762, average train loss: 6.8817
[11/25 14:34:26 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3094, average loss: 4.1820
[11/25 14:34:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.68	
[11/25 14:34:26 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/25 14:36:08 visual_prompt]: 	Training 100/553. train loss: 20.7751,	0.8260 s / batch. (data: 2.44e-04). ETA=12:17:02, max mem: 20.9 GB 
[11/25 14:37:46 visual_prompt]: 	Training 200/553. train loss: 2.4443,	0.8284 s / batch. (data: 5.44e-03). ETA=12:17:51, max mem: 20.9 GB 
[11/25 14:39:23 visual_prompt]: 	Training 300/553. train loss: 2.6300,	1.2156 s / batch. (data: 3.86e-01). ETA=18:00:41, max mem: 20.9 GB 
[11/25 14:40:55 visual_prompt]: 	Training 400/553. train loss: 5.1956,	1.3393 s / batch. (data: 5.14e-01). ETA=19:48:25, max mem: 20.9 GB 
[11/25 14:42:34 visual_prompt]: 	Training 500/553. train loss: 49.1465,	3.3321 s / batch. (data: 2.51e+00). ETA=2 days, 1:11:13, max mem: 20.9 GB 
[11/25 14:43:26 visual_prompt]: Epoch 4 / 100: avg data time: 1.43e-01, avg batch time: 0.9770, average train loss: 9.3442
[11/25 14:44:21 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3112, average loss: 2.7230
[11/25 14:44:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.14	
[11/25 14:44:21 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/25 14:46:01 visual_prompt]: 	Training 100/553. train loss: 0.3351,	0.8709 s / batch. (data: 1.49e-02). ETA=12:49:08, max mem: 20.9 GB 
[11/25 14:47:37 visual_prompt]: 	Training 200/553. train loss: 12.2037,	1.1255 s / batch. (data: 2.80e-01). ETA=16:32:07, max mem: 20.9 GB 
[11/25 14:49:15 visual_prompt]: 	Training 300/553. train loss: 47.6977,	0.8240 s / batch. (data: 4.23e-04). ETA=12:04:56, max mem: 20.9 GB 
[11/25 14:50:51 visual_prompt]: 	Training 400/553. train loss: 16.4583,	0.8429 s / batch. (data: 1.04e-02). ETA=12:20:11, max mem: 20.9 GB 
[11/25 14:52:29 visual_prompt]: 	Training 500/553. train loss: 4.7859,	0.8471 s / batch. (data: 3.06e-04). ETA=12:22:28, max mem: 20.9 GB 
[11/25 14:53:20 visual_prompt]: Epoch 5 / 100: avg data time: 1.40e-01, avg batch time: 0.9743, average train loss: 13.9010
[11/25 14:54:16 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3095, average loss: 92.5483
[11/25 14:54:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.76	
[11/25 14:54:16 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/25 14:55:59 visual_prompt]: 	Training 100/553. train loss: 1.8484,	0.8280 s / batch. (data: 2.93e-04). ETA=12:03:36, max mem: 20.9 GB 
[11/25 14:57:35 visual_prompt]: 	Training 200/553. train loss: 11.7367,	0.8400 s / batch. (data: 5.43e-03). ETA=12:12:41, max mem: 20.9 GB 
[11/25 14:59:11 visual_prompt]: 	Training 300/553. train loss: 3.2738,	0.8376 s / batch. (data: 3.15e-04). ETA=12:09:11, max mem: 20.9 GB 
[11/25 15:00:53 visual_prompt]: 	Training 400/553. train loss: 8.4288,	0.8614 s / batch. (data: 4.82e-03). ETA=12:28:27, max mem: 20.9 GB 
[11/25 15:02:29 visual_prompt]: 	Training 500/553. train loss: 2.4603,	0.8360 s / batch. (data: 3.50e-04). ETA=12:05:01, max mem: 20.9 GB 
[11/25 15:03:19 visual_prompt]: Epoch 6 / 100: avg data time: 1.49e-01, avg batch time: 0.9819, average train loss: 21.8691
[11/25 15:04:14 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.3108, average loss: 2.3365
[11/25 15:04:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.85	
[11/25 15:04:14 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/25 15:05:54 visual_prompt]: 	Training 100/553. train loss: 13.2510,	0.8520 s / batch. (data: 3.35e-04). ETA=12:16:45, max mem: 20.9 GB 
[11/25 15:07:31 visual_prompt]: 	Training 200/553. train loss: 23.3658,	0.8440 s / batch. (data: 3.28e-04). ETA=12:08:24, max mem: 20.9 GB 
[11/25 15:09:12 visual_prompt]: 	Training 300/553. train loss: 12.4615,	1.7143 s / batch. (data: 8.81e-01). ETA=1 day, 0:36:38, max mem: 20.9 GB 
[11/25 15:10:50 visual_prompt]: 	Training 400/553. train loss: 9.6258,	1.9080 s / batch. (data: 1.08e+00). ETA=1 day, 3:20:17, max mem: 20.9 GB 
[11/25 15:12:26 visual_prompt]: 	Training 500/553. train loss: 4.9146,	0.8404 s / batch. (data: 1.20e-02). ETA=12:01:05, max mem: 20.9 GB 
[11/25 15:13:15 visual_prompt]: Epoch 7 / 100: avg data time: 1.44e-01, avg batch time: 0.9777, average train loss: 20.0644
[11/25 15:14:11 visual_prompt]: Inference (val):avg data time: 2.16e-04, avg batch time: 0.3122, average loss: 8.4642
[11/25 15:14:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.60	
[11/25 15:14:11 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/25 15:15:50 visual_prompt]: 	Training 100/553. train loss: 9.8224,	0.8598 s / batch. (data: 1.05e-02). ETA=12:15:33, max mem: 20.9 GB 
[11/25 15:17:29 visual_prompt]: 	Training 200/553. train loss: 39.4100,	0.8360 s / batch. (data: 3.30e-04). ETA=11:53:47, max mem: 20.9 GB 
[11/25 15:19:06 visual_prompt]: 	Training 300/553. train loss: 13.7431,	0.8257 s / batch. (data: 2.96e-04). ETA=11:43:37, max mem: 20.9 GB 
[11/25 15:20:43 visual_prompt]: 	Training 400/553. train loss: 51.2663,	0.8269 s / batch. (data: 1.05e-02). ETA=11:43:16, max mem: 20.9 GB 
[11/25 15:22:20 visual_prompt]: 	Training 500/553. train loss: 139.0039,	1.3386 s / batch. (data: 5.07e-01). ETA=18:56:12, max mem: 20.9 GB 
[11/25 15:23:11 visual_prompt]: Epoch 8 / 100: avg data time: 1.44e-01, avg batch time: 0.9765, average train loss: 25.8392
[11/25 15:24:06 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3092, average loss: 7.0291
[11/25 15:24:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.43	
[11/25 15:24:06 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/25 15:25:46 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8600 s / batch. (data: 3.02e-04). ETA=12:07:44, max mem: 20.9 GB 
[11/25 15:27:23 visual_prompt]: 	Training 200/553. train loss: 13.8168,	0.8440 s / batch. (data: 3.14e-04). ETA=11:52:51, max mem: 20.9 GB 
[11/25 15:29:01 visual_prompt]: 	Training 300/553. train loss: 3.5134,	1.6997 s / batch. (data: 8.68e-01). ETA=23:52:42, max mem: 20.9 GB 
[11/25 15:30:39 visual_prompt]: 	Training 400/553. train loss: 10.2903,	0.8360 s / batch. (data: 8.13e-04). ETA=11:43:18, max mem: 20.9 GB 
[11/25 15:32:16 visual_prompt]: 	Training 500/553. train loss: 21.3491,	0.8221 s / batch. (data: 3.37e-04). ETA=11:30:12, max mem: 20.9 GB 
[11/25 15:33:06 visual_prompt]: Epoch 9 / 100: avg data time: 1.44e-01, avg batch time: 0.9769, average train loss: 23.0162
[11/25 15:34:02 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3099, average loss: 42.8388
[11/25 15:34:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.46	
[11/25 15:34:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/25 15:35:45 visual_prompt]: 	Training 100/553. train loss: 12.5214,	0.8166 s / batch. (data: 3.06e-04). ETA=11:23:29, max mem: 20.9 GB 
[11/25 15:37:21 visual_prompt]: 	Training 200/553. train loss: 6.6046,	0.8280 s / batch. (data: 3.24e-04). ETA=11:31:41, max mem: 20.9 GB 
[11/25 15:38:58 visual_prompt]: 	Training 300/553. train loss: 6.6750,	2.0432 s / batch. (data: 1.22e+00). ETA=1 day, 4:23:26, max mem: 20.9 GB 
[11/25 15:40:33 visual_prompt]: 	Training 400/553. train loss: 5.2920,	0.8906 s / batch. (data: 5.43e-03). ETA=12:21:01, max mem: 20.9 GB 
[11/25 15:42:12 visual_prompt]: 	Training 500/553. train loss: 19.9925,	0.8320 s / batch. (data: 3.20e-04). ETA=11:30:53, max mem: 20.9 GB 
[11/25 15:43:03 visual_prompt]: Epoch 10 / 100: avg data time: 1.45e-01, avg batch time: 0.9781, average train loss: 30.4111
[11/25 15:43:58 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3086, average loss: 0.9898
[11/25 15:43:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.75	
[11/25 15:43:58 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/25 15:45:41 visual_prompt]: 	Training 100/553. train loss: 15.8778,	0.8440 s / batch. (data: 2.88e-04). ETA=11:38:41, max mem: 20.9 GB 
[11/25 15:47:20 visual_prompt]: 	Training 200/553. train loss: 24.1415,	0.8393 s / batch. (data: 2.98e-04). ETA=11:33:22, max mem: 20.9 GB 
[11/25 15:48:57 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.0113 s / batch. (data: 1.17e+00). ETA=1 day, 3:38:19, max mem: 20.9 GB 
[11/25 15:50:33 visual_prompt]: 	Training 400/553. train loss: 43.0872,	0.8402 s / batch. (data: 5.92e-03). ETA=11:31:20, max mem: 20.9 GB 
[11/25 15:52:09 visual_prompt]: 	Training 500/553. train loss: 45.0347,	0.8170 s / batch. (data: 2.67e-04). ETA=11:10:55, max mem: 20.9 GB 
[11/25 15:52:59 visual_prompt]: Epoch 11 / 100: avg data time: 1.45e-01, avg batch time: 0.9772, average train loss: 34.2581
[11/25 15:53:54 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3089, average loss: 21.7759
[11/25 15:53:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.63	
[11/25 15:53:54 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/25 15:55:36 visual_prompt]: 	Training 100/553. train loss: 39.2219,	0.8247 s / batch. (data: 2.50e-04). ETA=11:15:08, max mem: 20.9 GB 
[11/25 15:57:14 visual_prompt]: 	Training 200/553. train loss: 7.6297,	0.8400 s / batch. (data: 3.13e-04). ETA=11:26:14, max mem: 20.9 GB 
[11/25 15:58:49 visual_prompt]: 	Training 300/553. train loss: 12.9869,	0.8386 s / batch. (data: 1.56e-02). ETA=11:23:43, max mem: 20.9 GB 
[11/25 16:00:26 visual_prompt]: 	Training 400/553. train loss: 20.0534,	0.8230 s / batch. (data: 3.27e-04). ETA=11:09:37, max mem: 20.9 GB 
[11/25 16:02:03 visual_prompt]: 	Training 500/553. train loss: 7.4051,	0.8634 s / batch. (data: 8.34e-04). ETA=11:41:04, max mem: 20.9 GB 
[11/25 16:02:53 visual_prompt]: Epoch 12 / 100: avg data time: 1.43e-01, avg batch time: 0.9749, average train loss: 34.2184
[11/25 16:03:49 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3106, average loss: 33.8616
[11/25 16:03:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.16	
[11/25 16:03:49 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/25 16:05:32 visual_prompt]: 	Training 100/553. train loss: 18.5837,	0.8480 s / batch. (data: 3.42e-04). ETA=11:26:22, max mem: 20.9 GB 
[11/25 16:07:06 visual_prompt]: 	Training 200/553. train loss: 119.0935,	0.8283 s / batch. (data: 3.22e-04). ETA=11:09:04, max mem: 20.9 GB 
[11/25 16:08:45 visual_prompt]: 	Training 300/553. train loss: 21.8950,	1.6888 s / batch. (data: 8.53e-01). ETA=22:41:18, max mem: 20.9 GB 
[11/25 16:10:21 visual_prompt]: 	Training 400/553. train loss: 13.5954,	0.8292 s / batch. (data: 9.15e-03). ETA=11:07:00, max mem: 20.9 GB 
[11/25 16:11:59 visual_prompt]: 	Training 500/553. train loss: 20.9357,	0.8771 s / batch. (data: 5.43e-03). ETA=11:44:04, max mem: 20.9 GB 
[11/25 16:12:49 visual_prompt]: Epoch 13 / 100: avg data time: 1.45e-01, avg batch time: 0.9773, average train loss: 36.6054
[11/25 16:13:45 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3110, average loss: 12.1257
[11/25 16:13:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.44	
[11/25 16:13:45 visual_prompt]: Best epoch 13: best metric: -12.126
[11/25 16:13:45 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/25 16:15:27 visual_prompt]: 	Training 100/553. train loss: 1.8615,	0.8321 s / batch. (data: 1.57e-02). ETA=11:05:48, max mem: 20.9 GB 
[11/25 16:17:04 visual_prompt]: 	Training 200/553. train loss: 6.4021,	0.8880 s / batch. (data: 5.78e-02). ETA=11:49:05, max mem: 20.9 GB 
[11/25 16:18:42 visual_prompt]: 	Training 300/553. train loss: 68.5546,	0.8253 s / batch. (data: 9.25e-03). ETA=10:57:37, max mem: 20.9 GB 
[11/25 16:20:17 visual_prompt]: 	Training 400/553. train loss: 5.1330,	0.8297 s / batch. (data: 5.44e-03). ETA=10:59:46, max mem: 20.9 GB 
[11/25 16:21:54 visual_prompt]: 	Training 500/553. train loss: 9.8359,	0.8190 s / batch. (data: 3.02e-04). ETA=10:49:52, max mem: 20.9 GB 
[11/25 16:22:45 visual_prompt]: Epoch 14 / 100: avg data time: 1.44e-01, avg batch time: 0.9764, average train loss: 32.7403
[11/25 16:23:40 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3116, average loss: 76.2857
[11/25 16:23:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.90	
[11/25 16:23:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/25 16:25:22 visual_prompt]: 	Training 100/553. train loss: 58.4551,	0.8282 s / batch. (data: 1.56e-02). ETA=10:55:06, max mem: 20.9 GB 
[11/25 16:26:57 visual_prompt]: 	Training 200/553. train loss: 188.4184,	0.8360 s / batch. (data: 2.91e-04). ETA=10:59:51, max mem: 20.9 GB 
[11/25 16:28:36 visual_prompt]: 	Training 300/553. train loss: 24.2248,	0.8451 s / batch. (data: 7.01e-04). ETA=11:05:37, max mem: 20.9 GB 
[11/25 16:30:11 visual_prompt]: 	Training 400/553. train loss: 5.2649,	1.1640 s / batch. (data: 2.92e-01). ETA=15:14:50, max mem: 20.9 GB 
[11/25 16:31:48 visual_prompt]: 	Training 500/553. train loss: 15.9856,	0.8255 s / batch. (data: 3.34e-04). ETA=10:47:26, max mem: 20.9 GB 
[11/25 16:32:39 visual_prompt]: Epoch 15 / 100: avg data time: 1.43e-01, avg batch time: 0.9746, average train loss: 36.2378
[11/25 16:33:34 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3123, average loss: 21.1190
[11/25 16:33:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.29	
[11/25 16:33:34 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/25 16:35:14 visual_prompt]: 	Training 100/553. train loss: 136.7389,	0.8357 s / batch. (data: 3.82e-04). ETA=10:53:17, max mem: 20.9 GB 
[11/25 16:36:51 visual_prompt]: 	Training 200/553. train loss: 59.3486,	0.8440 s / batch. (data: 2.93e-04). ETA=10:58:22, max mem: 20.9 GB 
[11/25 16:38:28 visual_prompt]: 	Training 300/553. train loss: 30.4218,	0.8552 s / batch. (data: 1.05e-02). ETA=11:05:44, max mem: 20.9 GB 
[11/25 16:40:05 visual_prompt]: 	Training 400/553. train loss: 17.3393,	0.8272 s / batch. (data: 5.43e-03). ETA=10:42:33, max mem: 20.9 GB 
[11/25 16:41:41 visual_prompt]: 	Training 500/553. train loss: 11.4081,	0.8467 s / batch. (data: 1.05e-02). ETA=10:56:15, max mem: 20.9 GB 
[11/25 16:42:32 visual_prompt]: Epoch 16 / 100: avg data time: 1.39e-01, avg batch time: 0.9717, average train loss: 32.5930
[11/25 16:43:27 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3118, average loss: 44.3229
[11/25 16:43:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.94	
[11/25 16:43:27 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/25 16:45:08 visual_prompt]: 	Training 100/553. train loss: 55.8716,	0.8320 s / batch. (data: 2.91e-04). ETA=10:42:45, max mem: 20.9 GB 
[11/25 16:46:46 visual_prompt]: 	Training 200/553. train loss: 54.9315,	0.8321 s / batch. (data: 2.83e-04). ETA=10:41:25, max mem: 20.9 GB 
[11/25 16:48:23 visual_prompt]: 	Training 300/553. train loss: 15.8055,	0.8488 s / batch. (data: 2.40e-02). ETA=10:52:51, max mem: 20.9 GB 
[11/25 16:50:00 visual_prompt]: 	Training 400/553. train loss: 2.8833,	1.0400 s / batch. (data: 1.86e-01). ETA=13:18:14, max mem: 20.9 GB 
[11/25 16:51:37 visual_prompt]: 	Training 500/553. train loss: 5.9224,	1.3881 s / batch. (data: 5.69e-01). ETA=17:43:04, max mem: 20.9 GB 
[11/25 16:52:28 visual_prompt]: Epoch 17 / 100: avg data time: 1.46e-01, avg batch time: 0.9783, average train loss: 31.9066
[11/25 16:53:26 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3098, average loss: 22.5387
[11/25 16:53:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.52	
[11/25 16:53:26 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/25 16:55:12 visual_prompt]: 	Training 100/553. train loss: 33.3468,	0.8165 s / batch. (data: 6.96e-04). ETA=10:23:16, max mem: 20.9 GB 
[11/25 16:57:07 visual_prompt]: 	Training 200/553. train loss: 15.3985,	0.8316 s / batch. (data: 5.90e-03). ETA=10:33:22, max mem: 20.9 GB 
[11/25 16:58:46 visual_prompt]: 	Training 300/553. train loss: 36.3336,	0.8670 s / batch. (data: 3.10e-04). ETA=10:58:56, max mem: 20.9 GB 
[11/25 17:00:25 visual_prompt]: 	Training 400/553. train loss: 26.4739,	0.8473 s / batch. (data: 1.19e-02). ETA=10:42:29, max mem: 20.9 GB 
[11/25 17:02:02 visual_prompt]: 	Training 500/553. train loss: 26.0536,	1.0160 s / batch. (data: 1.81e-01). ETA=12:48:44, max mem: 20.9 GB 
[11/25 17:03:02 visual_prompt]: Epoch 18 / 100: avg data time: 2.11e-01, avg batch time: 1.0417, average train loss: 32.3624
[11/25 17:03:58 visual_prompt]: Inference (val):avg data time: 6.07e-04, avg batch time: 0.3083, average loss: 28.3415
[11/25 17:03:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.76	
[11/25 17:03:58 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/25 17:05:39 visual_prompt]: 	Training 100/553. train loss: 73.0952,	0.8222 s / batch. (data: 3.24e-04). ETA=10:20:03, max mem: 20.9 GB 
[11/25 17:07:16 visual_prompt]: 	Training 200/553. train loss: 18.0639,	0.8211 s / batch. (data: 2.96e-04). ETA=10:17:50, max mem: 20.9 GB 
[11/25 17:08:54 visual_prompt]: 	Training 300/553. train loss: 0.0051,	0.8587 s / batch. (data: 3.50e-02). ETA=10:44:41, max mem: 20.9 GB 
[11/25 17:10:33 visual_prompt]: 	Training 400/553. train loss: 8.9777,	0.8344 s / batch. (data: 2.99e-04). ETA=10:25:02, max mem: 20.9 GB 
[11/25 17:12:06 visual_prompt]: 	Training 500/553. train loss: 29.4289,	0.8239 s / batch. (data: 5.42e-03). ETA=10:15:48, max mem: 20.9 GB 
[11/25 17:12:57 visual_prompt]: Epoch 19 / 100: avg data time: 1.42e-01, avg batch time: 0.9741, average train loss: 37.7777
[11/25 17:13:52 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3114, average loss: 163.5188
[11/25 17:13:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.06	
[11/25 17:13:52 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/25 17:15:31 visual_prompt]: 	Training 100/553. train loss: 11.4015,	0.8480 s / batch. (data: 7.95e-03). ETA=10:31:38, max mem: 20.9 GB 
[11/25 17:17:11 visual_prompt]: 	Training 200/553. train loss: 34.3164,	0.8078 s / batch. (data: 3.31e-04). ETA=10:00:21, max mem: 20.9 GB 
[11/25 17:18:48 visual_prompt]: 	Training 300/553. train loss: 34.0807,	0.8440 s / batch. (data: 2.98e-04). ETA=10:25:51, max mem: 20.9 GB 
[11/25 17:20:26 visual_prompt]: 	Training 400/553. train loss: 1.3187,	0.8286 s / batch. (data: 3.39e-04). ETA=10:13:03, max mem: 20.9 GB 
[11/25 17:22:02 visual_prompt]: 	Training 500/553. train loss: 16.2321,	0.8360 s / batch. (data: 3.07e-04). ETA=10:17:07, max mem: 20.9 GB 
[11/25 17:22:54 visual_prompt]: Epoch 20 / 100: avg data time: 1.47e-01, avg batch time: 0.9792, average train loss: 33.0191
[11/25 17:23:49 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3099, average loss: 21.6157
[11/25 17:23:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.95	
[11/25 17:23:49 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/25 17:25:34 visual_prompt]: 	Training 100/553. train loss: 13.6234,	0.8154 s / batch. (data: 2.68e-04). ETA=9:59:51, max mem: 20.9 GB 
[11/25 17:27:10 visual_prompt]: 	Training 200/553. train loss: 70.9600,	0.8208 s / batch. (data: 5.44e-03). ETA=10:02:28, max mem: 20.9 GB 
[11/25 17:28:46 visual_prompt]: 	Training 300/553. train loss: 131.5904,	1.0255 s / batch. (data: 2.01e-01). ETA=12:31:00, max mem: 20.9 GB 
[11/25 17:30:22 visual_prompt]: 	Training 400/553. train loss: 4.7542,	0.8202 s / batch. (data: 3.04e-04). ETA=9:59:17, max mem: 20.9 GB 
[11/25 17:32:01 visual_prompt]: 	Training 500/553. train loss: 5.1656,	0.8318 s / batch. (data: 5.42e-03). ETA=10:06:22, max mem: 20.9 GB 
[11/25 17:32:51 visual_prompt]: Epoch 21 / 100: avg data time: 1.47e-01, avg batch time: 0.9786, average train loss: 32.3908
[11/25 17:33:46 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3088, average loss: 59.2093
[11/25 17:33:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.51	
[11/25 17:33:46 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/25 17:35:26 visual_prompt]: 	Training 100/553. train loss: 74.4270,	0.8520 s / batch. (data: 3.06e-04). ETA=10:18:55, max mem: 20.9 GB 
[11/25 17:37:03 visual_prompt]: 	Training 200/553. train loss: 1.8805,	0.8381 s / batch. (data: 1.05e-02). ETA=10:07:26, max mem: 20.9 GB 
[11/25 17:38:38 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8292 s / batch. (data: 3.17e-04). ETA=9:59:38, max mem: 20.9 GB 
[11/25 17:40:17 visual_prompt]: 	Training 400/553. train loss: 60.2997,	0.8359 s / batch. (data: 3.05e-04). ETA=10:03:04, max mem: 20.9 GB 
[11/25 17:41:55 visual_prompt]: 	Training 500/553. train loss: 14.4501,	0.8552 s / batch. (data: 3.21e-04). ETA=10:15:33, max mem: 20.9 GB 
[11/25 17:42:46 visual_prompt]: Epoch 22 / 100: avg data time: 1.45e-01, avg batch time: 0.9776, average train loss: 31.3885
[11/25 17:43:42 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3129, average loss: 37.5095
[11/25 17:43:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.10	
[11/25 17:43:42 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/25 17:45:25 visual_prompt]: 	Training 100/553. train loss: 14.7998,	0.8294 s / batch. (data: 5.44e-03). ETA=9:54:51, max mem: 20.9 GB 
[11/25 17:47:03 visual_prompt]: 	Training 200/553. train loss: 26.9610,	0.8508 s / batch. (data: 1.56e-02). ETA=10:08:46, max mem: 20.9 GB 
[11/25 17:48:42 visual_prompt]: 	Training 300/553. train loss: 38.7181,	0.8594 s / batch. (data: 5.84e-03). ETA=10:13:31, max mem: 20.9 GB 
[11/25 17:50:17 visual_prompt]: 	Training 400/553. train loss: 20.7494,	0.8440 s / batch. (data: 7.62e-04). ETA=10:01:08, max mem: 20.9 GB 
[11/25 17:51:53 visual_prompt]: 	Training 500/553. train loss: 2.8258,	0.8238 s / batch. (data: 2.85e-04). ETA=9:45:24, max mem: 20.9 GB 
[11/25 17:52:44 visual_prompt]: Epoch 23 / 100: avg data time: 1.48e-01, avg batch time: 0.9794, average train loss: 30.7184
[11/25 17:53:39 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3097, average loss: 31.7203
[11/25 17:53:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.50	
[11/25 17:53:39 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/25 17:55:18 visual_prompt]: 	Training 100/553. train loss: 21.1932,	0.8440 s / batch. (data: 3.01e-04). ETA=9:57:33, max mem: 20.9 GB 
[11/25 17:56:55 visual_prompt]: 	Training 200/553. train loss: 4.5434,	0.8320 s / batch. (data: 3.00e-04). ETA=9:47:41, max mem: 20.9 GB 
[11/25 17:58:33 visual_prompt]: 	Training 300/553. train loss: 5.0614,	0.8667 s / batch. (data: 3.14e-02). ETA=10:10:45, max mem: 20.9 GB 
[11/25 18:00:10 visual_prompt]: 	Training 400/553. train loss: 27.5653,	0.8669 s / batch. (data: 1.05e-02). ETA=10:09:25, max mem: 20.9 GB 
[11/25 18:01:50 visual_prompt]: 	Training 500/553. train loss: 37.7972,	0.8369 s / batch. (data: 1.05e-02). ETA=9:46:57, max mem: 20.9 GB 
[11/25 18:02:41 visual_prompt]: Epoch 24 / 100: avg data time: 1.47e-01, avg batch time: 0.9795, average train loss: 29.8137
[11/25 18:03:36 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3111, average loss: 7.4391
[11/25 18:03:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.25	
[11/25 18:03:36 visual_prompt]: Best epoch 24: best metric: -7.439
[11/25 18:03:36 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/25 18:05:21 visual_prompt]: 	Training 100/553. train loss: 53.5473,	0.8290 s / batch. (data: 3.08e-04). ETA=9:39:18, max mem: 20.9 GB 
[11/25 18:06:55 visual_prompt]: 	Training 200/553. train loss: 11.9055,	0.8443 s / batch. (data: 1.05e-02). ETA=9:48:35, max mem: 20.9 GB 
[11/25 18:08:33 visual_prompt]: 	Training 300/553. train loss: 5.0566,	0.8629 s / batch. (data: 3.29e-04). ETA=10:00:08, max mem: 20.9 GB 
[11/25 18:10:10 visual_prompt]: 	Training 400/553. train loss: 9.4415,	1.2073 s / batch. (data: 3.72e-01). ETA=13:57:36, max mem: 20.9 GB 
[11/25 18:11:48 visual_prompt]: 	Training 500/553. train loss: 11.4700,	1.2607 s / batch. (data: 4.31e-01). ETA=14:32:33, max mem: 20.9 GB 
[11/25 18:12:40 visual_prompt]: Epoch 25 / 100: avg data time: 1.51e-01, avg batch time: 0.9831, average train loss: 32.5727
[11/25 18:13:36 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3105, average loss: 24.6369
[11/25 18:13:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.47	
[11/25 18:13:36 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/25 18:15:18 visual_prompt]: 	Training 100/553. train loss: 2.8801,	0.8516 s / batch. (data: 5.56e-03). ETA=9:47:16, max mem: 20.9 GB 
[11/25 18:16:57 visual_prompt]: 	Training 200/553. train loss: 21.5737,	1.7960 s / batch. (data: 9.90e-01). ETA=20:35:31, max mem: 20.9 GB 
[11/25 18:18:37 visual_prompt]: 	Training 300/553. train loss: 17.6660,	0.8480 s / batch. (data: 2.73e-04). ETA=9:41:57, max mem: 20.9 GB 
[11/25 18:20:14 visual_prompt]: 	Training 400/553. train loss: 17.7804,	0.8395 s / batch. (data: 7.40e-03). ETA=9:34:41, max mem: 20.9 GB 
[11/25 18:21:49 visual_prompt]: 	Training 500/553. train loss: 55.5703,	0.8361 s / batch. (data: 7.97e-03). ETA=9:31:00, max mem: 20.9 GB 
[11/25 18:22:40 visual_prompt]: Epoch 26 / 100: avg data time: 1.52e-01, avg batch time: 0.9845, average train loss: 29.8481
[11/25 18:23:36 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3093, average loss: 8.4448
[11/25 18:23:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.15	
[11/25 18:23:36 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[11/25 18:25:17 visual_prompt]: 	Training 100/553. train loss: 73.5949,	0.8565 s / batch. (data: 2.44e-02). ETA=9:42:42, max mem: 20.9 GB 
[11/25 18:26:53 visual_prompt]: 	Training 200/553. train loss: 44.7822,	0.9610 s / batch. (data: 1.34e-01). ETA=10:52:12, max mem: 20.9 GB 
[11/25 18:28:29 visual_prompt]: 	Training 300/553. train loss: 13.6333,	0.8552 s / batch. (data: 2.11e-02). ETA=9:39:00, max mem: 20.9 GB 
[11/25 18:30:08 visual_prompt]: 	Training 400/553. train loss: 92.7445,	0.8328 s / batch. (data: 7.23e-04). ETA=9:22:28, max mem: 20.9 GB 
[11/25 18:31:44 visual_prompt]: 	Training 500/553. train loss: 17.2910,	0.8594 s / batch. (data: 1.45e-02). ETA=9:38:57, max mem: 20.9 GB 
[11/25 18:32:33 visual_prompt]: Epoch 27 / 100: avg data time: 1.40e-01, avg batch time: 0.9720, average train loss: 31.4765
[11/25 18:33:29 visual_prompt]: Inference (val):avg data time: 1.28e-04, avg batch time: 0.3108, average loss: 15.9482
[11/25 18:33:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.11	
[11/25 18:33:29 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[11/25 18:35:09 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8370 s / batch. (data: 3.18e-04). ETA=9:21:45, max mem: 20.9 GB 
[11/25 18:36:47 visual_prompt]: 	Training 200/553. train loss: 32.3063,	0.8379 s / batch. (data: 1.60e-02). ETA=9:20:55, max mem: 20.9 GB 
[11/25 18:38:26 visual_prompt]: 	Training 300/553. train loss: 14.7498,	1.5160 s / batch. (data: 6.99e-01). ETA=16:52:25, max mem: 20.9 GB 
[11/25 18:40:02 visual_prompt]: 	Training 400/553. train loss: 76.6420,	0.8600 s / batch. (data: 1.20e-02). ETA=9:32:53, max mem: 20.9 GB 
[11/25 18:41:37 visual_prompt]: 	Training 500/553. train loss: 21.1955,	0.8680 s / batch. (data: 3.15e-04). ETA=9:36:45, max mem: 20.9 GB 
[11/25 18:42:29 visual_prompt]: Epoch 28 / 100: avg data time: 1.44e-01, avg batch time: 0.9766, average train loss: 31.9558
[11/25 18:43:25 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3115, average loss: 46.2851
[11/25 18:43:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.27	
[11/25 18:43:25 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[11/25 18:45:13 visual_prompt]: 	Training 100/553. train loss: 59.1458,	0.8312 s / batch. (data: 2.86e-04). ETA=9:10:10, max mem: 20.9 GB 
[11/25 18:46:49 visual_prompt]: 	Training 200/553. train loss: 37.0747,	1.8074 s / batch. (data: 9.56e-01). ETA=19:53:23, max mem: 20.9 GB 
[11/25 18:48:25 visual_prompt]: 	Training 300/553. train loss: 23.9400,	0.8431 s / batch. (data: 1.16e-02). ETA=9:15:15, max mem: 20.9 GB 
[11/25 18:49:58 visual_prompt]: 	Training 400/553. train loss: 9.1341,	0.8437 s / batch. (data: 5.47e-03). ETA=9:14:16, max mem: 20.9 GB 
[11/25 18:51:36 visual_prompt]: 	Training 500/553. train loss: 5.2619,	0.8371 s / batch. (data: 3.24e-04). ETA=9:08:32, max mem: 20.9 GB 
[11/25 18:52:27 visual_prompt]: Epoch 29 / 100: avg data time: 1.48e-01, avg batch time: 0.9799, average train loss: 37.2370
[11/25 18:53:22 visual_prompt]: Inference (val):avg data time: 1.76e-04, avg batch time: 0.3095, average loss: 36.1800
[11/25 18:53:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.64	
[11/25 18:53:22 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[11/25 18:55:02 visual_prompt]: 	Training 100/553. train loss: 57.3054,	0.8231 s / batch. (data: 1.05e-02). ETA=8:57:14, max mem: 20.9 GB 
[11/25 18:56:40 visual_prompt]: 	Training 200/553. train loss: 20.0120,	0.8280 s / batch. (data: 4.86e-04). ETA=8:59:02, max mem: 20.9 GB 
[11/25 18:58:18 visual_prompt]: 	Training 300/553. train loss: 58.9110,	1.8745 s / batch. (data: 1.04e+00). ETA=20:17:17, max mem: 20.9 GB 
[11/25 18:59:56 visual_prompt]: 	Training 400/553. train loss: 32.9440,	1.0800 s / batch. (data: 2.37e-01). ETA=11:39:33, max mem: 20.9 GB 
[11/25 19:01:32 visual_prompt]: 	Training 500/553. train loss: 113.4729,	1.4240 s / batch. (data: 5.94e-01). ETA=15:19:57, max mem: 20.9 GB 
[11/25 19:02:25 visual_prompt]: Epoch 30 / 100: avg data time: 1.49e-01, avg batch time: 0.9808, average train loss: 28.7434
[11/25 19:03:20 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3094, average loss: 22.9904
[11/25 19:03:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.86	
[11/25 19:03:20 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[11/25 19:05:02 visual_prompt]: 	Training 100/553. train loss: 23.2558,	0.8400 s / batch. (data: 3.09e-04). ETA=9:00:32, max mem: 20.9 GB 
[11/25 19:06:42 visual_prompt]: 	Training 200/553. train loss: 109.4730,	0.8120 s / batch. (data: 2.99e-04). ETA=8:41:11, max mem: 20.9 GB 
[11/25 19:08:17 visual_prompt]: 	Training 300/553. train loss: 45.3426,	0.8280 s / batch. (data: 3.16e-04). ETA=8:50:03, max mem: 20.9 GB 
[11/25 19:09:53 visual_prompt]: 	Training 400/553. train loss: 52.4664,	0.8515 s / batch. (data: 5.41e-03). ETA=9:03:41, max mem: 20.9 GB 
[11/25 19:11:31 visual_prompt]: 	Training 500/553. train loss: 31.6548,	0.8437 s / batch. (data: 4.08e-03). ETA=8:57:18, max mem: 20.9 GB 
[11/25 19:12:21 visual_prompt]: Epoch 31 / 100: avg data time: 1.45e-01, avg batch time: 0.9777, average train loss: 26.0752
[11/25 19:13:16 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3111, average loss: 31.8197
[11/25 19:13:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.21	
[11/25 19:13:16 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[11/25 19:14:59 visual_prompt]: 	Training 100/553. train loss: 25.7262,	0.8360 s / batch. (data: 3.02e-04). ETA=8:50:17, max mem: 20.9 GB 
[11/25 19:16:36 visual_prompt]: 	Training 200/553. train loss: 15.7171,	0.8366 s / batch. (data: 8.18e-04). ETA=8:49:14, max mem: 20.9 GB 
[11/25 19:18:16 visual_prompt]: 	Training 300/553. train loss: 92.0406,	0.8238 s / batch. (data: 2.73e-04). ETA=8:39:45, max mem: 20.9 GB 
[11/25 19:19:54 visual_prompt]: 	Training 400/553. train loss: 13.8397,	0.8185 s / batch. (data: 3.45e-04). ETA=8:35:03, max mem: 20.9 GB 
[11/25 19:21:29 visual_prompt]: 	Training 500/553. train loss: 23.7470,	0.8307 s / batch. (data: 5.41e-03). ETA=8:41:21, max mem: 20.9 GB 
[11/25 19:22:18 visual_prompt]: Epoch 32 / 100: avg data time: 1.48e-01, avg batch time: 0.9799, average train loss: 30.6038
[11/25 19:23:14 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3095, average loss: 15.6869
[11/25 19:23:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.55	
[11/25 19:23:14 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[11/25 19:24:53 visual_prompt]: 	Training 100/553. train loss: 30.3722,	1.4107 s / batch. (data: 5.79e-01). ETA=14:41:48, max mem: 20.9 GB 
[11/25 19:26:34 visual_prompt]: 	Training 200/553. train loss: 36.4146,	1.3277 s / batch. (data: 5.20e-01). ETA=13:47:41, max mem: 20.9 GB 
[11/25 19:28:12 visual_prompt]: 	Training 300/553. train loss: 26.4358,	0.8309 s / batch. (data: 5.42e-03). ETA=8:36:34, max mem: 20.9 GB 
[11/25 19:29:52 visual_prompt]: 	Training 400/553. train loss: 7.8397,	0.8400 s / batch. (data: 4.08e-04). ETA=8:40:50, max mem: 20.9 GB 
[11/25 19:31:30 visual_prompt]: 	Training 500/553. train loss: 13.7290,	0.8320 s / batch. (data: 3.21e-04). ETA=8:34:29, max mem: 20.9 GB 
[11/25 19:32:21 visual_prompt]: Epoch 33 / 100: avg data time: 1.59e-01, avg batch time: 0.9901, average train loss: 29.5350
[11/25 19:33:18 visual_prompt]: Inference (val):avg data time: 1.64e-04, avg batch time: 0.3133, average loss: 87.5976
[11/25 19:33:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.27	
[11/25 19:33:18 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[11/25 19:35:01 visual_prompt]: 	Training 100/553. train loss: 15.7223,	0.8177 s / batch. (data: 3.57e-04). ETA=8:23:33, max mem: 20.9 GB 
[11/25 19:36:36 visual_prompt]: 	Training 200/553. train loss: 19.8765,	0.8320 s / batch. (data: 3.19e-04). ETA=8:31:01, max mem: 20.9 GB 
[11/25 19:38:12 visual_prompt]: 	Training 300/553. train loss: 59.4217,	0.8206 s / batch. (data: 7.96e-03). ETA=8:22:39, max mem: 20.9 GB 
[11/25 19:39:51 visual_prompt]: 	Training 400/553. train loss: 37.4711,	0.8466 s / batch. (data: 3.08e-04). ETA=8:37:10, max mem: 20.9 GB 
[11/25 19:41:30 visual_prompt]: 	Training 500/553. train loss: 44.6877,	1.4380 s / batch. (data: 6.33e-01). ETA=14:36:02, max mem: 20.9 GB 
[11/25 19:42:20 visual_prompt]: Epoch 34 / 100: avg data time: 1.48e-01, avg batch time: 0.9800, average train loss: 30.3251
[11/25 19:43:15 visual_prompt]: Inference (val):avg data time: 2.79e-04, avg batch time: 0.3113, average loss: 19.5082
[11/25 19:43:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.82	
[11/25 19:43:15 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[11/25 19:44:59 visual_prompt]: 	Training 100/553. train loss: 28.2258,	0.8178 s / batch. (data: 4.75e-04). ETA=8:16:05, max mem: 20.9 GB 
[11/25 19:46:37 visual_prompt]: 	Training 200/553. train loss: 40.1643,	0.8440 s / batch. (data: 3.41e-04). ETA=8:30:34, max mem: 20.9 GB 
[11/25 19:48:13 visual_prompt]: 	Training 300/553. train loss: 20.8998,	0.8206 s / batch. (data: 3.15e-04). ETA=8:15:04, max mem: 20.9 GB 
[11/25 19:49:48 visual_prompt]: 	Training 400/553. train loss: 16.4035,	0.8201 s / batch. (data: 2.98e-04). ETA=8:13:24, max mem: 20.9 GB 
[11/25 19:51:25 visual_prompt]: 	Training 500/553. train loss: 92.4864,	1.0732 s / batch. (data: 2.47e-01). ETA=10:43:52, max mem: 20.9 GB 
[11/25 19:52:17 visual_prompt]: Epoch 35 / 100: avg data time: 1.47e-01, avg batch time: 0.9788, average train loss: 32.1592
[11/25 19:53:12 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3103, average loss: 29.1534
[11/25 19:53:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.99	
[11/25 19:53:12 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[11/25 19:54:52 visual_prompt]: 	Training 100/553. train loss: 5.5278,	0.8284 s / batch. (data: 1.20e-02). ETA=8:14:53, max mem: 20.9 GB 
[11/25 19:56:31 visual_prompt]: 	Training 200/553. train loss: 61.8225,	0.8395 s / batch. (data: 1.56e-02). ETA=8:20:08, max mem: 20.9 GB 
[11/25 19:58:09 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8394 s / batch. (data: 7.40e-03). ETA=8:18:42, max mem: 20.9 GB 
[11/25 19:59:46 visual_prompt]: 	Training 400/553. train loss: 26.8228,	0.8399 s / batch. (data: 3.14e-04). ETA=8:17:35, max mem: 20.9 GB 
[11/25 20:01:23 visual_prompt]: 	Training 500/553. train loss: 8.0681,	0.9135 s / batch. (data: 5.65e-02). ETA=8:59:38, max mem: 20.9 GB 
[11/25 20:02:12 visual_prompt]: Epoch 36 / 100: avg data time: 1.43e-01, avg batch time: 0.9755, average train loss: 27.7423
[11/25 20:03:07 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3111, average loss: 88.0162
[11/25 20:03:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.70	
[11/25 20:03:07 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[11/25 20:04:48 visual_prompt]: 	Training 100/553. train loss: 7.0561,	0.8175 s / batch. (data: 3.32e-04). ETA=8:00:52, max mem: 20.9 GB 
[11/25 20:06:25 visual_prompt]: 	Training 200/553. train loss: 14.3619,	0.8360 s / batch. (data: 3.18e-04). ETA=8:10:20, max mem: 20.9 GB 
[11/25 20:08:02 visual_prompt]: 	Training 300/553. train loss: 26.8646,	1.3281 s / batch. (data: 4.95e-01). ETA=12:56:44, max mem: 20.9 GB 
[11/25 20:09:42 visual_prompt]: 	Training 400/553. train loss: 46.8616,	1.8938 s / batch. (data: 1.07e+00). ETA=18:24:26, max mem: 20.9 GB 
[11/25 20:11:15 visual_prompt]: 	Training 500/553. train loss: 29.4414,	0.9584 s / batch. (data: 1.21e-01). ETA=9:17:19, max mem: 20.9 GB 
[11/25 20:12:08 visual_prompt]: Epoch 37 / 100: avg data time: 1.45e-01, avg batch time: 0.9778, average train loss: 24.0381
[11/25 20:13:03 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.3103, average loss: 10.0697
[11/25 20:13:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.56	
[11/25 20:13:03 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[11/25 20:14:43 visual_prompt]: 	Training 100/553. train loss: 12.8534,	0.8432 s / batch. (data: 5.44e-03). ETA=8:08:10, max mem: 20.9 GB 
[11/25 20:16:22 visual_prompt]: 	Training 200/553. train loss: 10.3631,	1.3982 s / batch. (data: 5.78e-01). ETA=13:27:12, max mem: 20.9 GB 
[11/25 20:18:01 visual_prompt]: 	Training 300/553. train loss: 28.5636,	0.8127 s / batch. (data: 2.98e-04). ETA=7:47:49, max mem: 20.9 GB 
[11/25 20:19:35 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8360 s / batch. (data: 3.13e-04). ETA=7:59:51, max mem: 20.9 GB 
[11/25 20:21:15 visual_prompt]: 	Training 500/553. train loss: 4.4356,	0.8336 s / batch. (data: 3.34e-04). ETA=7:57:04, max mem: 20.9 GB 
[11/25 20:22:04 visual_prompt]: Epoch 38 / 100: avg data time: 1.46e-01, avg batch time: 0.9781, average train loss: 25.6685
[11/25 20:23:00 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3110, average loss: 5.1620
[11/25 20:23:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 53.38	
[11/25 20:23:00 visual_prompt]: Best epoch 38: best metric: -5.162
[11/25 20:23:00 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[11/25 20:24:39 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8480 s / batch. (data: 3.11e-04). ETA=8:03:09, max mem: 20.9 GB 
[11/25 20:26:20 visual_prompt]: 	Training 200/553. train loss: 40.6512,	0.8240 s / batch. (data: 3.06e-04). ETA=7:48:08, max mem: 20.9 GB 
[11/25 20:28:00 visual_prompt]: 	Training 300/553. train loss: 23.4809,	0.8280 s / batch. (data: 2.78e-04). ETA=7:49:01, max mem: 20.9 GB 
[11/25 20:29:35 visual_prompt]: 	Training 400/553. train loss: 0.3322,	0.8270 s / batch. (data: 3.27e-04). ETA=7:47:05, max mem: 20.9 GB 
[11/25 20:31:13 visual_prompt]: 	Training 500/553. train loss: 21.3553,	1.6247 s / batch. (data: 7.81e-01). ETA=15:14:50, max mem: 20.9 GB 
[11/25 20:32:01 visual_prompt]: Epoch 39 / 100: avg data time: 1.46e-01, avg batch time: 0.9789, average train loss: 22.3797
[11/25 20:32:56 visual_prompt]: Inference (val):avg data time: 1.65e-04, avg batch time: 0.3103, average loss: 9.7386
[11/25 20:32:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.07	
[11/25 20:32:56 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[11/25 20:34:38 visual_prompt]: 	Training 100/553. train loss: 39.0284,	0.8264 s / batch. (data: 2.98e-04). ETA=7:43:15, max mem: 20.9 GB 
[11/25 20:36:15 visual_prompt]: 	Training 200/553. train loss: 44.7136,	0.8470 s / batch. (data: 1.05e-02). ETA=7:53:24, max mem: 20.9 GB 
[11/25 20:37:53 visual_prompt]: 	Training 300/553. train loss: 1.4765,	0.8563 s / batch. (data: 1.60e-02). ETA=7:57:09, max mem: 20.9 GB 
[11/25 20:39:31 visual_prompt]: 	Training 400/553. train loss: 12.7666,	0.8334 s / batch. (data: 3.02e-04). ETA=7:43:00, max mem: 20.9 GB 
[11/25 20:41:07 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8182 s / batch. (data: 3.01e-04). ETA=7:33:12, max mem: 20.9 GB 
[11/25 20:42:00 visual_prompt]: Epoch 40 / 100: avg data time: 1.50e-01, avg batch time: 0.9822, average train loss: 26.6628
[11/25 20:42:56 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3104, average loss: 21.7155
[11/25 20:42:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 39.32	
[11/25 20:42:56 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[11/25 20:44:43 visual_prompt]: 	Training 100/553. train loss: 26.5522,	0.8399 s / batch. (data: 5.25e-04). ETA=7:43:03, max mem: 20.9 GB 
[11/25 20:46:29 visual_prompt]: 	Training 200/553. train loss: 25.0975,	0.8343 s / batch. (data: 1.60e-02). ETA=7:38:33, max mem: 20.9 GB 
[11/25 20:48:06 visual_prompt]: 	Training 300/553. train loss: 8.7110,	0.8389 s / batch. (data: 3.16e-04). ETA=7:39:41, max mem: 20.9 GB 
[11/25 20:49:43 visual_prompt]: 	Training 400/553. train loss: 24.0848,	0.8196 s / batch. (data: 2.99e-04). ETA=7:27:47, max mem: 20.9 GB 
[11/25 20:51:20 visual_prompt]: 	Training 500/553. train loss: 2.9224,	0.8498 s / batch. (data: 1.12e-03). ETA=7:42:52, max mem: 20.9 GB 
[11/25 20:52:09 visual_prompt]: Epoch 41 / 100: avg data time: 1.68e-01, avg batch time: 0.9997, average train loss: 26.3409
[11/25 20:53:05 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3092, average loss: 32.4843
[11/25 20:53:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.24	
[11/25 20:53:05 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[11/25 20:54:45 visual_prompt]: 	Training 100/553. train loss: 15.6477,	0.8328 s / batch. (data: 5.42e-03). ETA=7:31:28, max mem: 20.9 GB 
[11/25 20:56:22 visual_prompt]: 	Training 200/553. train loss: 18.8049,	0.8323 s / batch. (data: 5.44e-03). ETA=7:29:47, max mem: 20.9 GB 
[11/25 20:58:01 visual_prompt]: 	Training 300/553. train loss: 6.5058,	0.8440 s / batch. (data: 3.10e-04). ETA=7:34:44, max mem: 20.9 GB 
[11/25 20:59:38 visual_prompt]: 	Training 400/553. train loss: 17.2064,	0.8598 s / batch. (data: 1.98e-02). ETA=7:41:48, max mem: 20.9 GB 
[11/25 21:01:14 visual_prompt]: 	Training 500/553. train loss: 91.9209,	0.8320 s / batch. (data: 7.95e-03). ETA=7:25:29, max mem: 20.9 GB 
[11/25 21:02:06 visual_prompt]: Epoch 42 / 100: avg data time: 1.46e-01, avg batch time: 0.9785, average train loss: 24.4209
[11/25 21:03:02 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3107, average loss: 36.9176
[11/25 21:03:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.04	
[11/25 21:03:02 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[11/25 21:04:45 visual_prompt]: 	Training 100/553. train loss: 7.3888,	0.8246 s / batch. (data: 8.58e-03). ETA=7:19:25, max mem: 20.9 GB 
[11/25 21:06:21 visual_prompt]: 	Training 200/553. train loss: 115.5747,	0.8251 s / batch. (data: 3.29e-04). ETA=7:18:19, max mem: 20.9 GB 
[11/25 21:07:57 visual_prompt]: 	Training 300/553. train loss: 42.1443,	0.8374 s / batch. (data: 7.95e-03). ETA=7:23:26, max mem: 20.9 GB 
[11/25 21:09:32 visual_prompt]: 	Training 400/553. train loss: 59.4686,	0.8480 s / batch. (data: 7.95e-03). ETA=7:27:39, max mem: 20.9 GB 
[11/25 21:11:11 visual_prompt]: 	Training 500/553. train loss: 38.3393,	0.8320 s / batch. (data: 3.26e-04). ETA=7:17:49, max mem: 20.9 GB 
[11/25 21:12:04 visual_prompt]: Epoch 43 / 100: avg data time: 1.48e-01, avg batch time: 0.9803, average train loss: 26.1458
[11/25 21:13:00 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3100, average loss: 18.0466
[11/25 21:13:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.67	
[11/25 21:13:00 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[11/25 21:14:40 visual_prompt]: 	Training 100/553. train loss: 41.6996,	1.3181 s / batch. (data: 4.89e-01). ETA=11:30:15, max mem: 20.9 GB 
[11/25 21:16:19 visual_prompt]: 	Training 200/553. train loss: 24.9968,	0.8227 s / batch. (data: 3.00e-04). ETA=7:09:26, max mem: 20.9 GB 
[11/25 21:17:54 visual_prompt]: 	Training 300/553. train loss: 40.6746,	0.8240 s / batch. (data: 3.18e-04). ETA=7:08:44, max mem: 20.9 GB 
[11/25 21:19:30 visual_prompt]: 	Training 400/553. train loss: 50.0597,	0.8699 s / batch. (data: 1.56e-02). ETA=7:31:12, max mem: 20.9 GB 
[11/25 21:21:07 visual_prompt]: 	Training 500/553. train loss: 11.9130,	0.8212 s / batch. (data: 3.24e-04). ETA=7:04:34, max mem: 20.9 GB 
[11/25 21:21:59 visual_prompt]: Epoch 44 / 100: avg data time: 1.42e-01, avg batch time: 0.9748, average train loss: 26.5415
[11/25 21:22:54 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3099, average loss: 106.2100
[11/25 21:22:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.68	
[11/25 21:22:54 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[11/25 21:24:37 visual_prompt]: 	Training 100/553. train loss: 18.1458,	0.8554 s / batch. (data: 2.02e-02). ETA=7:20:05, max mem: 20.9 GB 
[11/25 21:26:11 visual_prompt]: 	Training 200/553. train loss: 2.2040,	1.2249 s / batch. (data: 3.99e-01). ETA=10:28:08, max mem: 20.9 GB 
[11/25 21:27:51 visual_prompt]: 	Training 300/553. train loss: 35.3088,	0.8341 s / batch. (data: 8.24e-04). ETA=7:06:20, max mem: 20.9 GB 
[11/25 21:29:25 visual_prompt]: 	Training 400/553. train loss: 11.2677,	0.8382 s / batch. (data: 1.20e-02). ETA=7:07:01, max mem: 20.9 GB 
[11/25 21:31:05 visual_prompt]: 	Training 500/553. train loss: 35.0762,	0.8322 s / batch. (data: 5.44e-03). ETA=7:02:35, max mem: 20.9 GB 
[11/25 21:31:56 visual_prompt]: Epoch 45 / 100: avg data time: 1.46e-01, avg batch time: 0.9787, average train loss: 21.2374
[11/25 21:32:51 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3115, average loss: 8.9792
[11/25 21:32:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.90	
[11/25 21:32:51 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[11/25 21:34:32 visual_prompt]: 	Training 100/553. train loss: 0.5297,	1.1732 s / batch. (data: 3.26e-01). ETA=9:52:47, max mem: 20.9 GB 
[11/25 21:36:11 visual_prompt]: 	Training 200/553. train loss: 2.9668,	0.8375 s / batch. (data: 7.79e-04). ETA=7:01:43, max mem: 20.9 GB 
[11/25 21:37:47 visual_prompt]: 	Training 300/553. train loss: 76.9602,	0.8520 s / batch. (data: 5.47e-03). ETA=7:07:37, max mem: 20.9 GB 
[11/25 21:39:25 visual_prompt]: 	Training 400/553. train loss: 27.3665,	0.8468 s / batch. (data: 7.76e-04). ETA=7:03:36, max mem: 20.9 GB 
[11/25 21:40:58 visual_prompt]: 	Training 500/553. train loss: 0.6555,	0.8285 s / batch. (data: 2.86e-04). ETA=6:53:03, max mem: 20.9 GB 
[11/25 21:41:51 visual_prompt]: Epoch 46 / 100: avg data time: 1.44e-01, avg batch time: 0.9759, average train loss: 24.1686
[11/25 21:42:45 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3112, average loss: 13.5975
[11/25 21:42:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.89	
[11/25 21:42:45 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[11/25 21:44:27 visual_prompt]: 	Training 100/553. train loss: 10.5285,	0.8400 s / batch. (data: 4.10e-04). ETA=6:56:38, max mem: 20.9 GB 
[11/25 21:46:02 visual_prompt]: 	Training 200/553. train loss: 78.0075,	1.2420 s / batch. (data: 4.28e-01). ETA=10:13:58, max mem: 20.9 GB 
[11/25 21:47:40 visual_prompt]: 	Training 300/553. train loss: 5.8748,	0.8179 s / batch. (data: 3.35e-04). ETA=6:42:58, max mem: 20.9 GB 
[11/25 21:49:17 visual_prompt]: 	Training 400/553. train loss: 41.5618,	0.8148 s / batch. (data: 3.11e-04). ETA=6:40:05, max mem: 20.9 GB 
[11/25 21:50:53 visual_prompt]: 	Training 500/553. train loss: 12.8149,	0.8476 s / batch. (data: 3.29e-04). ETA=6:54:45, max mem: 20.9 GB 
[11/25 21:51:46 visual_prompt]: Epoch 47 / 100: avg data time: 1.44e-01, avg batch time: 0.9769, average train loss: 22.0154
[11/25 21:52:41 visual_prompt]: Inference (val):avg data time: 4.83e-05, avg batch time: 0.3092, average loss: 19.2693
[11/25 21:52:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.64	
[11/25 21:52:41 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[11/25 21:54:23 visual_prompt]: 	Training 100/553. train loss: 67.7261,	0.8496 s / batch. (data: 2.06e-02). ETA=6:53:37, max mem: 20.9 GB 
[11/25 21:56:01 visual_prompt]: 	Training 200/553. train loss: 3.7868,	0.8244 s / batch. (data: 3.32e-04). ETA=6:39:56, max mem: 20.9 GB 
[11/25 21:57:40 visual_prompt]: 	Training 300/553. train loss: 0.6299,	1.5837 s / batch. (data: 7.52e-01). ETA=12:45:42, max mem: 20.9 GB 
[11/25 21:59:13 visual_prompt]: 	Training 400/553. train loss: 0.0083,	0.8280 s / batch. (data: 3.18e-04). ETA=6:38:56, max mem: 20.9 GB 
[11/25 22:00:50 visual_prompt]: 	Training 500/553. train loss: 20.4112,	0.8260 s / batch. (data: 5.46e-03). ETA=6:36:35, max mem: 20.9 GB 
[11/25 22:01:40 visual_prompt]: Epoch 48 / 100: avg data time: 1.41e-01, avg batch time: 0.9743, average train loss: 20.5710
[11/25 22:02:35 visual_prompt]: Inference (val):avg data time: 4.71e-04, avg batch time: 0.3106, average loss: 63.7910
[11/25 22:02:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.25	
[11/25 22:02:35 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[11/25 22:04:16 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8207 s / batch. (data: 3.07e-04). ETA=6:31:58, max mem: 20.9 GB 
[11/25 22:05:52 visual_prompt]: 	Training 200/553. train loss: 20.9715,	0.8280 s / batch. (data: 2.95e-04). ETA=6:34:04, max mem: 20.9 GB 
[11/25 22:07:30 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8362 s / batch. (data: 3.29e-04). ETA=6:36:34, max mem: 20.9 GB 
[11/25 22:09:08 visual_prompt]: 	Training 400/553. train loss: 2.9353,	0.8531 s / batch. (data: 1.30e-02). ETA=6:43:09, max mem: 20.9 GB 
[11/25 22:10:45 visual_prompt]: 	Training 500/553. train loss: 14.3700,	0.8174 s / batch. (data: 2.64e-04). ETA=6:24:57, max mem: 20.9 GB 
[11/25 22:11:37 visual_prompt]: Epoch 49 / 100: avg data time: 1.46e-01, avg batch time: 0.9788, average train loss: 21.5894
[11/25 22:12:32 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3095, average loss: 18.1432
[11/25 22:12:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.55	
[11/25 22:12:32 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[11/25 22:14:15 visual_prompt]: 	Training 100/553. train loss: 13.4888,	0.8278 s / batch. (data: 2.87e-04). ETA=6:27:44, max mem: 20.9 GB 
[11/25 22:15:53 visual_prompt]: 	Training 200/553. train loss: 18.3344,	0.8311 s / batch. (data: 3.28e-04). ETA=6:27:52, max mem: 20.9 GB 
[11/25 22:17:29 visual_prompt]: 	Training 300/553. train loss: 32.4627,	0.8239 s / batch. (data: 3.19e-04). ETA=6:23:09, max mem: 20.9 GB 
[11/25 22:19:04 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8280 s / batch. (data: 7.95e-03). ETA=6:23:41, max mem: 20.9 GB 
[11/25 22:20:41 visual_prompt]: 	Training 500/553. train loss: 30.3753,	0.8520 s / batch. (data: 3.14e-04). ETA=6:33:23, max mem: 20.9 GB 
[11/25 22:21:32 visual_prompt]: Epoch 50 / 100: avg data time: 1.44e-01, avg batch time: 0.9762, average train loss: 20.2678
[11/25 22:22:27 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3109, average loss: 19.8499
[11/25 22:22:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.79	
[11/25 22:22:27 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[11/25 22:24:08 visual_prompt]: 	Training 100/553. train loss: 18.9532,	1.0145 s / batch. (data: 1.64e-01). ETA=7:45:50, max mem: 20.9 GB 
[11/25 22:25:45 visual_prompt]: 	Training 200/553. train loss: 28.8640,	0.8203 s / batch. (data: 3.32e-04). ETA=6:15:17, max mem: 20.9 GB 
[11/25 22:27:23 visual_prompt]: 	Training 300/553. train loss: 2.2775,	0.8280 s / batch. (data: 3.25e-04). ETA=6:17:26, max mem: 20.9 GB 
[11/25 22:29:02 visual_prompt]: 	Training 400/553. train loss: 2.9341,	1.2972 s / batch. (data: 4.73e-01). ETA=9:49:07, max mem: 20.9 GB 
[11/25 22:30:38 visual_prompt]: 	Training 500/553. train loss: 17.1177,	0.8199 s / batch. (data: 2.68e-04). ETA=6:11:00, max mem: 20.9 GB 
[11/25 22:31:27 visual_prompt]: Epoch 51 / 100: avg data time: 1.44e-01, avg batch time: 0.9768, average train loss: 19.0705
[11/25 22:32:22 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3093, average loss: 5.8342
[11/25 22:32:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.24	
[11/25 22:32:22 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[11/25 22:34:07 visual_prompt]: 	Training 100/553. train loss: 12.4204,	0.8361 s / batch. (data: 5.93e-03). ETA=6:16:11, max mem: 20.9 GB 
[11/25 22:35:43 visual_prompt]: 	Training 200/553. train loss: 8.5228,	0.8480 s / batch. (data: 2.93e-04). ETA=6:20:08, max mem: 20.9 GB 
[11/25 22:37:20 visual_prompt]: 	Training 300/553. train loss: 41.9723,	0.8276 s / batch. (data: 3.14e-04). ETA=6:09:36, max mem: 20.9 GB 
[11/25 22:38:58 visual_prompt]: 	Training 400/553. train loss: 88.8701,	0.8520 s / batch. (data: 7.95e-03). ETA=6:19:05, max mem: 20.9 GB 
[11/25 22:40:30 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8366 s / batch. (data: 5.49e-03). ETA=6:10:50, max mem: 20.9 GB 
[11/25 22:41:20 visual_prompt]: Epoch 52 / 100: avg data time: 1.38e-01, avg batch time: 0.9714, average train loss: 21.3370
[11/25 22:42:15 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3091, average loss: 3.1040
[11/25 22:42:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.22	
[11/25 22:42:15 visual_prompt]: Best epoch 52: best metric: -3.104
[11/25 22:42:15 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[11/25 22:43:56 visual_prompt]: 	Training 100/553. train loss: 34.9381,	0.8234 s / batch. (data: 2.73e-04). ETA=6:02:52, max mem: 20.9 GB 
[11/25 22:45:34 visual_prompt]: 	Training 200/553. train loss: 59.1264,	0.8534 s / batch. (data: 7.70e-04). ETA=6:14:42, max mem: 20.9 GB 
[11/25 22:47:10 visual_prompt]: 	Training 300/553. train loss: 30.7643,	0.8520 s / batch. (data: 7.73e-04). ETA=6:12:39, max mem: 20.9 GB 
[11/25 22:48:50 visual_prompt]: 	Training 400/553. train loss: 6.2285,	0.8633 s / batch. (data: 2.13e-02). ETA=6:16:10, max mem: 20.9 GB 
[11/25 22:50:26 visual_prompt]: 	Training 500/553. train loss: 28.4080,	0.8197 s / batch. (data: 3.02e-04). ETA=5:55:47, max mem: 20.9 GB 
[11/25 22:51:18 visual_prompt]: Epoch 53 / 100: avg data time: 1.49e-01, avg batch time: 0.9818, average train loss: 19.9654
[11/25 22:52:13 visual_prompt]: Inference (val):avg data time: 1.34e-04, avg batch time: 0.3108, average loss: 10.2193
[11/25 22:52:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.50	
[11/25 22:52:13 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[11/25 22:53:56 visual_prompt]: 	Training 100/553. train loss: 0.3219,	0.8280 s / batch. (data: 3.06e-04). ETA=5:57:17, max mem: 20.9 GB 
[11/25 22:55:33 visual_prompt]: 	Training 200/553. train loss: 6.3546,	0.8556 s / batch. (data: 5.43e-03). ETA=6:07:46, max mem: 20.9 GB 
[11/25 22:57:09 visual_prompt]: 	Training 300/553. train loss: 41.7299,	0.8196 s / batch. (data: 3.23e-04). ETA=5:50:55, max mem: 20.9 GB 
[11/25 22:58:45 visual_prompt]: 	Training 400/553. train loss: 34.6027,	0.8356 s / batch. (data: 1.05e-02). ETA=5:56:24, max mem: 20.9 GB 
[11/25 23:00:24 visual_prompt]: 	Training 500/553. train loss: 15.0122,	0.8326 s / batch. (data: 3.30e-04). ETA=5:53:44, max mem: 20.9 GB 
[11/25 23:01:14 visual_prompt]: Epoch 54 / 100: avg data time: 1.45e-01, avg batch time: 0.9788, average train loss: 20.5318
[11/25 23:02:10 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3120, average loss: 55.9242
[11/25 23:02:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.39	
[11/25 23:02:10 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[11/25 23:03:51 visual_prompt]: 	Training 100/553. train loss: 1.7116,	0.8280 s / batch. (data: 5.44e-03). ETA=5:49:39, max mem: 20.9 GB 
[11/25 23:05:27 visual_prompt]: 	Training 200/553. train loss: 6.7196,	0.8328 s / batch. (data: 1.06e-02). ETA=5:50:18, max mem: 20.9 GB 
[11/25 23:07:05 visual_prompt]: 	Training 300/553. train loss: 17.1665,	0.8610 s / batch. (data: 2.39e-02). ETA=6:00:44, max mem: 20.9 GB 
[11/25 23:08:42 visual_prompt]: 	Training 400/553. train loss: 3.7559,	0.8638 s / batch. (data: 3.09e-04). ETA=6:00:27, max mem: 20.9 GB 
[11/25 23:10:19 visual_prompt]: 	Training 500/553. train loss: 7.0962,	1.1773 s / batch. (data: 3.48e-01). ETA=8:09:19, max mem: 20.9 GB 
[11/25 23:11:10 visual_prompt]: Epoch 55 / 100: avg data time: 1.44e-01, avg batch time: 0.9767, average train loss: 18.2025
[11/25 23:12:05 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3092, average loss: 2.4929
[11/25 23:12:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.29	
[11/25 23:12:05 visual_prompt]: Best epoch 55: best metric: -2.493
[11/25 23:12:05 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[11/25 23:13:47 visual_prompt]: 	Training 100/553. train loss: 0.9013,	0.8451 s / batch. (data: 1.56e-02). ETA=5:49:05, max mem: 20.9 GB 
[11/25 23:15:23 visual_prompt]: 	Training 200/553. train loss: 2.3837,	0.8554 s / batch. (data: 2.26e-02). ETA=5:51:56, max mem: 20.9 GB 
[11/25 23:17:01 visual_prompt]: 	Training 300/553. train loss: 17.0712,	0.8290 s / batch. (data: 3.29e-04). ETA=5:39:40, max mem: 20.9 GB 
[11/25 23:18:39 visual_prompt]: 	Training 400/553. train loss: 7.0236,	0.8480 s / batch. (data: 3.18e-04). ETA=5:46:02, max mem: 20.9 GB 
[11/25 23:20:16 visual_prompt]: 	Training 500/553. train loss: 19.5637,	2.0555 s / batch. (data: 1.23e+00). ETA=13:55:23, max mem: 20.9 GB 
[11/25 23:21:05 visual_prompt]: Epoch 56 / 100: avg data time: 1.43e-01, avg batch time: 0.9762, average train loss: 15.3884
[11/25 23:22:01 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3106, average loss: 25.3055
[11/25 23:22:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.47	
[11/25 23:22:01 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[11/25 23:23:45 visual_prompt]: 	Training 100/553. train loss: 15.1756,	0.8252 s / batch. (data: 3.18e-04). ETA=5:33:17, max mem: 20.9 GB 
[11/25 23:25:21 visual_prompt]: 	Training 200/553. train loss: 45.6182,	0.8240 s / batch. (data: 3.42e-04). ETA=5:31:24, max mem: 20.9 GB 
[11/25 23:26:58 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8320 s / batch. (data: 7.95e-03). ETA=5:33:14, max mem: 20.9 GB 
[11/25 23:28:34 visual_prompt]: 	Training 400/553. train loss: 7.1038,	0.8331 s / batch. (data: 6.27e-04). ETA=5:32:18, max mem: 20.9 GB 
[11/25 23:30:09 visual_prompt]: 	Training 500/553. train loss: 67.1635,	0.8362 s / batch. (data: 5.45e-03). ETA=5:32:07, max mem: 20.9 GB 
[11/25 23:31:00 visual_prompt]: Epoch 57 / 100: avg data time: 1.42e-01, avg batch time: 0.9756, average train loss: 15.5547
[11/25 23:31:56 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3100, average loss: 5.7535
[11/25 23:31:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.17	
[11/25 23:31:56 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[11/25 23:33:36 visual_prompt]: 	Training 100/553. train loss: 7.4500,	0.8245 s / batch. (data: 3.09e-04). ETA=5:25:23, max mem: 20.9 GB 
[11/25 23:35:13 visual_prompt]: 	Training 200/553. train loss: 6.8628,	0.8557 s / batch. (data: 1.17e-02). ETA=5:36:16, max mem: 20.9 GB 
[11/25 23:36:52 visual_prompt]: 	Training 300/553. train loss: 24.6648,	0.8191 s / batch. (data: 3.17e-04). ETA=5:20:31, max mem: 20.9 GB 
[11/25 23:38:29 visual_prompt]: 	Training 400/553. train loss: 8.5359,	0.8436 s / batch. (data: 3.12e-04). ETA=5:28:41, max mem: 20.9 GB 
[11/25 23:40:05 visual_prompt]: 	Training 500/553. train loss: 185.6109,	0.8292 s / batch. (data: 3.22e-04). ETA=5:21:43, max mem: 20.9 GB 
[11/25 23:40:55 visual_prompt]: Epoch 58 / 100: avg data time: 1.41e-01, avg batch time: 0.9742, average train loss: 17.6176
[11/25 23:41:50 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3096, average loss: 7.0374
[11/25 23:41:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.23	
[11/25 23:41:50 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[11/25 23:43:33 visual_prompt]: 	Training 100/553. train loss: 1.1424,	0.8355 s / batch. (data: 7.96e-04). ETA=5:22:01, max mem: 20.9 GB 
[11/25 23:45:11 visual_prompt]: 	Training 200/553. train loss: 5.8374,	0.8483 s / batch. (data: 5.43e-03). ETA=5:25:33, max mem: 20.9 GB 
[11/25 23:46:47 visual_prompt]: 	Training 300/553. train loss: 27.1948,	0.8397 s / batch. (data: 3.30e-04). ETA=5:20:51, max mem: 20.9 GB 
[11/25 23:48:23 visual_prompt]: 	Training 400/553. train loss: 11.9470,	0.8334 s / batch. (data: 3.17e-04). ETA=5:17:03, max mem: 20.9 GB 
[11/25 23:50:02 visual_prompt]: 	Training 500/553. train loss: 6.1322,	0.8427 s / batch. (data: 8.41e-04). ETA=5:19:12, max mem: 20.9 GB 
[11/25 23:50:52 visual_prompt]: Epoch 59 / 100: avg data time: 1.46e-01, avg batch time: 0.9797, average train loss: 17.8184
[11/25 23:51:47 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3113, average loss: 3.6967
[11/25 23:51:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.48	
[11/25 23:51:47 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[11/25 23:53:29 visual_prompt]: 	Training 100/553. train loss: 2.9260,	0.8221 s / batch. (data: 5.55e-03). ETA=5:09:16, max mem: 20.9 GB 
[11/25 23:55:06 visual_prompt]: 	Training 200/553. train loss: 42.8757,	0.8314 s / batch. (data: 7.30e-03). ETA=5:11:23, max mem: 20.9 GB 
[11/25 23:56:42 visual_prompt]: 	Training 300/553. train loss: 38.8731,	2.2961 s / batch. (data: 1.47e+00). ETA=14:16:10, max mem: 20.9 GB 
[11/25 23:58:20 visual_prompt]: 	Training 400/553. train loss: 0.8077,	1.0080 s / batch. (data: 1.52e-01). ETA=6:14:11, max mem: 20.9 GB 
[11/25 23:59:58 visual_prompt]: 	Training 500/553. train loss: 3.3147,	0.8483 s / batch. (data: 3.62e-04). ETA=5:13:28, max mem: 20.9 GB 
[11/26 00:00:49 visual_prompt]: Epoch 60 / 100: avg data time: 1.47e-01, avg batch time: 0.9796, average train loss: 15.8165
[11/26 00:01:44 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3096, average loss: 15.3824
[11/26 00:01:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.05	
[11/26 00:01:44 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[11/26 00:03:26 visual_prompt]: 	Training 100/553. train loss: 29.5074,	0.8351 s / batch. (data: 1.20e-02). ETA=5:06:29, max mem: 20.9 GB 
[11/26 00:05:04 visual_prompt]: 	Training 200/553. train loss: 20.0876,	0.8476 s / batch. (data: 9.24e-03). ETA=5:09:39, max mem: 20.9 GB 
[11/26 00:06:42 visual_prompt]: 	Training 300/553. train loss: 8.4036,	0.8440 s / batch. (data: 5.43e-03). ETA=5:06:55, max mem: 20.9 GB 
[11/26 00:08:17 visual_prompt]: 	Training 400/553. train loss: 16.5174,	0.8427 s / batch. (data: 5.41e-03). ETA=5:05:04, max mem: 20.9 GB 
[11/26 00:09:54 visual_prompt]: 	Training 500/553. train loss: 34.2611,	1.9835 s / batch. (data: 1.15e+00). ETA=11:54:44, max mem: 20.9 GB 
[11/26 00:10:44 visual_prompt]: Epoch 61 / 100: avg data time: 1.42e-01, avg batch time: 0.9747, average train loss: 15.8167
[11/26 00:11:39 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3102, average loss: 10.9515
[11/26 00:11:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.38	
[11/26 00:11:39 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[11/26 00:13:20 visual_prompt]: 	Training 100/553. train loss: 22.4552,	0.8440 s / batch. (data: 3.04e-04). ETA=5:01:58, max mem: 20.9 GB 
[11/26 00:14:58 visual_prompt]: 	Training 200/553. train loss: 10.1873,	0.8335 s / batch. (data: 1.11e-02). ETA=4:56:48, max mem: 20.9 GB 
[11/26 00:16:33 visual_prompt]: 	Training 300/553. train loss: 27.1875,	0.8962 s / batch. (data: 3.62e-02). ETA=5:17:39, max mem: 20.9 GB 
[11/26 00:18:11 visual_prompt]: 	Training 400/553. train loss: 9.0329,	0.8243 s / batch. (data: 2.97e-04). ETA=4:50:47, max mem: 20.9 GB 
[11/26 00:19:45 visual_prompt]: 	Training 500/553. train loss: 14.9380,	0.8360 s / batch. (data: 3.14e-04). ETA=4:53:32, max mem: 20.9 GB 
[11/26 00:20:39 visual_prompt]: Epoch 62 / 100: avg data time: 1.43e-01, avg batch time: 0.9760, average train loss: 13.4324
[11/26 00:21:35 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3096, average loss: 13.0417
[11/26 00:21:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.03	
[11/26 00:21:35 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[11/26 00:23:19 visual_prompt]: 	Training 100/553. train loss: 2.1797,	0.8271 s / batch. (data: 3.20e-04). ETA=4:48:18, max mem: 20.9 GB 
[11/26 00:25:00 visual_prompt]: 	Training 200/553. train loss: 5.8354,	0.8320 s / batch. (data: 2.88e-04). ETA=4:48:37, max mem: 20.9 GB 
[11/26 00:26:35 visual_prompt]: 	Training 300/553. train loss: 2.0536,	0.8400 s / batch. (data: 3.62e-04). ETA=4:49:59, max mem: 20.9 GB 
[11/26 00:28:08 visual_prompt]: 	Training 400/553. train loss: 11.0377,	0.8251 s / batch. (data: 7.95e-03). ETA=4:43:28, max mem: 20.9 GB 
[11/26 00:29:43 visual_prompt]: 	Training 500/553. train loss: 7.3771,	0.8466 s / batch. (data: 1.88e-02). ETA=4:49:27, max mem: 20.9 GB 
[11/26 00:30:32 visual_prompt]: Epoch 63 / 100: avg data time: 1.38e-01, avg batch time: 0.9719, average train loss: 12.5359
[11/26 00:31:28 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3106, average loss: 14.8224
[11/26 00:31:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.12	
[11/26 00:31:28 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[11/26 00:33:11 visual_prompt]: 	Training 100/553. train loss: 4.9937,	0.8520 s / batch. (data: 7.71e-04). ETA=4:49:07, max mem: 20.9 GB 
[11/26 00:34:50 visual_prompt]: 	Training 200/553. train loss: 66.3867,	0.8124 s / batch. (data: 3.15e-04). ETA=4:34:20, max mem: 20.9 GB 
[11/26 00:36:24 visual_prompt]: 	Training 300/553. train loss: 8.0966,	0.8320 s / batch. (data: 7.97e-03). ETA=4:39:33, max mem: 20.9 GB 
[11/26 00:38:00 visual_prompt]: 	Training 400/553. train loss: 9.2367,	0.8920 s / batch. (data: 5.96e-02). ETA=4:58:14, max mem: 20.9 GB 
[11/26 00:39:38 visual_prompt]: 	Training 500/553. train loss: 31.7402,	0.8258 s / batch. (data: 7.56e-03). ETA=4:34:43, max mem: 20.9 GB 
[11/26 00:40:28 visual_prompt]: Epoch 64 / 100: avg data time: 1.44e-01, avg batch time: 0.9777, average train loss: 12.2981
[11/26 00:41:24 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3096, average loss: 4.9562
[11/26 00:41:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.95	
[11/26 00:41:24 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[11/26 00:43:08 visual_prompt]: 	Training 100/553. train loss: 8.1335,	1.1200 s / batch. (data: 2.72e-01). ETA=6:09:44, max mem: 20.9 GB 
[11/26 00:44:46 visual_prompt]: 	Training 200/553. train loss: 6.1695,	0.8287 s / batch. (data: 3.30e-04). ETA=4:32:11, max mem: 20.9 GB 
[11/26 00:46:21 visual_prompt]: 	Training 300/553. train loss: 17.0906,	0.8257 s / batch. (data: 2.90e-04). ETA=4:29:50, max mem: 20.9 GB 
[11/26 00:47:58 visual_prompt]: 	Training 400/553. train loss: 9.3688,	0.8248 s / batch. (data: 2.54e-04). ETA=4:28:10, max mem: 20.9 GB 
[11/26 00:49:34 visual_prompt]: 	Training 500/553. train loss: 17.0593,	0.8249 s / batch. (data: 5.41e-03). ETA=4:26:48, max mem: 20.9 GB 
[11/26 00:50:23 visual_prompt]: Epoch 65 / 100: avg data time: 1.41e-01, avg batch time: 0.9751, average train loss: 11.4889
[11/26 00:51:19 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3104, average loss: 7.8149
[11/26 00:51:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.17	
[11/26 00:51:19 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[11/26 00:52:59 visual_prompt]: 	Training 100/553. train loss: 6.1508,	0.8177 s / batch. (data: 3.01e-04). ETA=4:22:25, max mem: 20.9 GB 
[11/26 00:54:36 visual_prompt]: 	Training 200/553. train loss: 1.5804,	1.5136 s / batch. (data: 6.88e-01). ETA=8:03:13, max mem: 20.9 GB 
[11/26 00:56:16 visual_prompt]: 	Training 300/553. train loss: 5.7337,	0.8393 s / batch. (data: 5.41e-03). ETA=4:26:33, max mem: 20.9 GB 
[11/26 00:57:51 visual_prompt]: 	Training 400/553. train loss: 7.5557,	0.8331 s / batch. (data: 7.32e-04). ETA=4:23:11, max mem: 20.9 GB 
[11/26 00:59:27 visual_prompt]: 	Training 500/553. train loss: 17.6168,	0.8412 s / batch. (data: 5.41e-03). ETA=4:24:19, max mem: 20.9 GB 
[11/26 01:00:19 visual_prompt]: Epoch 66 / 100: avg data time: 1.43e-01, avg batch time: 0.9765, average train loss: 10.6113
[11/26 01:01:14 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3111, average loss: 6.4178
[11/26 01:01:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.53	
[11/26 01:01:14 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[11/26 01:02:57 visual_prompt]: 	Training 100/553. train loss: 12.9475,	0.8600 s / batch. (data: 2.80e-02). ETA=4:28:03, max mem: 20.9 GB 
[11/26 01:04:34 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8595 s / batch. (data: 1.05e-02). ETA=4:26:29, max mem: 20.9 GB 
[11/26 01:06:08 visual_prompt]: 	Training 300/553. train loss: 14.8793,	0.8360 s / batch. (data: 3.12e-04). ETA=4:17:47, max mem: 20.9 GB 
[11/26 01:07:45 visual_prompt]: 	Training 400/553. train loss: 13.0800,	0.8600 s / batch. (data: 1.20e-02). ETA=4:23:45, max mem: 20.9 GB 
[11/26 01:09:23 visual_prompt]: 	Training 500/553. train loss: 14.7161,	1.4829 s / batch. (data: 6.66e-01). ETA=7:32:20, max mem: 20.9 GB 
[11/26 01:10:15 visual_prompt]: Epoch 67 / 100: avg data time: 1.44e-01, avg batch time: 0.9776, average train loss: 9.5718
[11/26 01:11:10 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3096, average loss: 7.2660
[11/26 01:11:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 37.47	
[11/26 01:11:10 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[11/26 01:12:51 visual_prompt]: 	Training 100/553. train loss: 0.9755,	0.8165 s / batch. (data: 2.90e-04). ETA=4:06:57, max mem: 20.9 GB 
[11/26 01:14:30 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.0279 s / batch. (data: 1.83e-01). ETA=5:09:12, max mem: 20.9 GB 
[11/26 01:16:05 visual_prompt]: 	Training 300/553. train loss: 6.9514,	0.8337 s / batch. (data: 1.05e-02). ETA=4:09:23, max mem: 20.9 GB 
[11/26 01:17:41 visual_prompt]: 	Training 400/553. train loss: 15.7196,	0.8423 s / batch. (data: 6.22e-04). ETA=4:10:33, max mem: 20.9 GB 
[11/26 01:19:19 visual_prompt]: 	Training 500/553. train loss: 13.9565,	0.8605 s / batch. (data: 8.46e-03). ETA=4:14:32, max mem: 20.9 GB 
[11/26 01:20:10 visual_prompt]: Epoch 68 / 100: avg data time: 1.42e-01, avg batch time: 0.9763, average train loss: 9.5590
[11/26 01:21:05 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3100, average loss: 2.5293
[11/26 01:21:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.56	
[11/26 01:21:05 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[11/26 01:22:45 visual_prompt]: 	Training 100/553. train loss: 10.2932,	0.8480 s / batch. (data: 2.95e-04). ETA=4:08:41, max mem: 20.9 GB 
[11/26 01:24:22 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8320 s / batch. (data: 3.02e-04). ETA=4:02:36, max mem: 20.9 GB 
[11/26 01:25:59 visual_prompt]: 	Training 300/553. train loss: 17.7103,	0.8475 s / batch. (data: 1.05e-02). ETA=4:05:43, max mem: 20.9 GB 
[11/26 01:27:35 visual_prompt]: 	Training 400/553. train loss: 21.1518,	0.8366 s / batch. (data: 3.07e-04). ETA=4:01:09, max mem: 20.9 GB 
[11/26 01:29:11 visual_prompt]: 	Training 500/553. train loss: 9.3876,	0.8254 s / batch. (data: 3.03e-04). ETA=3:56:33, max mem: 20.9 GB 
[11/26 01:30:02 visual_prompt]: Epoch 69 / 100: avg data time: 1.38e-01, avg batch time: 0.9713, average train loss: 9.4999
[11/26 01:30:58 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3124, average loss: 5.6805
[11/26 01:30:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.36	
[11/26 01:30:58 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[11/26 01:32:37 visual_prompt]: 	Training 100/553. train loss: 8.5869,	0.8720 s / batch. (data: 7.96e-03). ETA=4:07:41, max mem: 20.9 GB 
[11/26 01:34:15 visual_prompt]: 	Training 200/553. train loss: 2.5892,	0.9162 s / batch. (data: 9.74e-02). ETA=4:18:43, max mem: 20.9 GB 
[11/26 01:35:51 visual_prompt]: 	Training 300/553. train loss: 36.9494,	0.8416 s / batch. (data: 1.20e-02). ETA=3:56:15, max mem: 20.9 GB 
[11/26 01:37:29 visual_prompt]: 	Training 400/553. train loss: 3.9782,	0.8763 s / batch. (data: 2.36e-02). ETA=4:04:32, max mem: 20.9 GB 
[11/26 01:39:04 visual_prompt]: 	Training 500/553. train loss: 12.8219,	0.8360 s / batch. (data: 5.42e-03). ETA=3:51:53, max mem: 20.9 GB 
[11/26 01:39:55 visual_prompt]: Epoch 70 / 100: avg data time: 1.39e-01, avg batch time: 0.9723, average train loss: 8.2492
[11/26 01:40:51 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3091, average loss: 10.5187
[11/26 01:40:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.50	
[11/26 01:40:51 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[11/26 01:42:32 visual_prompt]: 	Training 100/553. train loss: 11.1027,	0.8568 s / batch. (data: 5.44e-03). ETA=3:55:28, max mem: 20.9 GB 
[11/26 01:44:10 visual_prompt]: 	Training 200/553. train loss: 3.1218,	0.9768 s / batch. (data: 1.31e-01). ETA=4:26:50, max mem: 20.9 GB 
[11/26 01:45:49 visual_prompt]: 	Training 300/553. train loss: 3.3976,	0.8443 s / batch. (data: 9.39e-03). ETA=3:49:14, max mem: 20.9 GB 
[11/26 01:47:24 visual_prompt]: 	Training 400/553. train loss: 19.4440,	0.8520 s / batch. (data: 1.06e-02). ETA=3:49:53, max mem: 20.9 GB 
[11/26 01:49:01 visual_prompt]: 	Training 500/553. train loss: 4.8120,	0.9200 s / batch. (data: 6.71e-02). ETA=4:06:43, max mem: 20.9 GB 
[11/26 01:49:52 visual_prompt]: Epoch 71 / 100: avg data time: 1.43e-01, avg batch time: 0.9773, average train loss: 7.4976
[11/26 01:50:47 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3107, average loss: 6.8668
[11/26 01:50:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.44	
[11/26 01:50:47 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[11/26 01:52:31 visual_prompt]: 	Training 100/553. train loss: 6.5432,	1.0640 s / batch. (data: 2.38e-01). ETA=4:42:37, max mem: 20.9 GB 
[11/26 01:54:06 visual_prompt]: 	Training 200/553. train loss: 6.2367,	0.8409 s / batch. (data: 3.08e-04). ETA=3:41:57, max mem: 20.9 GB 
[11/26 01:55:47 visual_prompt]: 	Training 300/553. train loss: 0.8036,	0.8279 s / batch. (data: 3.22e-04). ETA=3:37:09, max mem: 20.9 GB 
[11/26 01:57:25 visual_prompt]: 	Training 400/553. train loss: 3.9885,	0.8188 s / batch. (data: 3.50e-04). ETA=3:33:23, max mem: 20.9 GB 
[11/26 01:59:08 visual_prompt]: 	Training 500/553. train loss: 3.0142,	0.8458 s / batch. (data: 3.17e-04). ETA=3:39:00, max mem: 20.9 GB 
[11/26 02:00:06 visual_prompt]: Epoch 72 / 100: avg data time: 1.77e-01, avg batch time: 1.0104, average train loss: 7.0124
[11/26 02:01:02 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3106, average loss: 1.4174
[11/26 02:01:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.01	
[11/26 02:01:02 visual_prompt]: Best epoch 72: best metric: -1.417
[11/26 02:01:02 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[11/26 02:02:45 visual_prompt]: 	Training 100/553. train loss: 3.6359,	0.8440 s / batch. (data: 7.95e-03). ETA=3:36:24, max mem: 20.9 GB 
[11/26 02:04:33 visual_prompt]: 	Training 200/553. train loss: 3.5473,	0.8240 s / batch. (data: 2.91e-04). ETA=3:29:54, max mem: 20.9 GB 
[11/26 02:06:07 visual_prompt]: 	Training 300/553. train loss: 5.9879,	0.8440 s / batch. (data: 3.09e-04). ETA=3:33:35, max mem: 20.9 GB 
[11/26 02:07:55 visual_prompt]: 	Training 400/553. train loss: 11.5023,	0.8510 s / batch. (data: 3.37e-04). ETA=3:33:56, max mem: 20.9 GB 
[11/26 02:09:32 visual_prompt]: 	Training 500/553. train loss: 10.1057,	0.8314 s / batch. (data: 5.43e-03). ETA=3:27:37, max mem: 20.9 GB 
[11/26 02:10:23 visual_prompt]: Epoch 73 / 100: avg data time: 1.82e-01, avg batch time: 1.0141, average train loss: 7.5758
[11/26 02:11:19 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3108, average loss: 2.7755
[11/26 02:11:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.55	
[11/26 02:11:19 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[11/26 02:13:05 visual_prompt]: 	Training 100/553. train loss: 5.8129,	1.4880 s / batch. (data: 6.37e-01). ETA=6:07:48, max mem: 20.9 GB 
[11/26 02:14:43 visual_prompt]: 	Training 200/553. train loss: 0.6464,	0.8474 s / batch. (data: 7.10e-04). ETA=3:28:03, max mem: 20.9 GB 
[11/26 02:16:20 visual_prompt]: 	Training 300/553. train loss: 1.5511,	0.8360 s / batch. (data: 3.55e-04). ETA=3:23:51, max mem: 20.9 GB 
[11/26 02:17:55 visual_prompt]: 	Training 400/553. train loss: 16.4838,	0.8465 s / batch. (data: 8.87e-03). ETA=3:25:00, max mem: 20.9 GB 
[11/26 02:19:31 visual_prompt]: 	Training 500/553. train loss: 3.7205,	1.5533 s / batch. (data: 7.29e-01). ETA=6:13:35, max mem: 20.9 GB 
[11/26 02:20:21 visual_prompt]: Epoch 74 / 100: avg data time: 1.47e-01, avg batch time: 0.9807, average train loss: 6.5521
[11/26 02:21:17 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3095, average loss: 5.0643
[11/26 02:21:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.67	
[11/26 02:21:17 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[11/26 02:22:59 visual_prompt]: 	Training 100/553. train loss: 5.9551,	0.8400 s / batch. (data: 3.20e-04). ETA=3:19:53, max mem: 20.9 GB 
[11/26 02:24:37 visual_prompt]: 	Training 200/553. train loss: 8.2169,	0.8240 s / batch. (data: 3.10e-04). ETA=3:14:42, max mem: 20.9 GB 
[11/26 02:26:14 visual_prompt]: 	Training 300/553. train loss: 1.0150,	0.8786 s / batch. (data: 2.95e-02). ETA=3:26:09, max mem: 20.9 GB 
[11/26 02:27:53 visual_prompt]: 	Training 400/553. train loss: 16.9524,	1.8832 s / batch. (data: 1.05e+00). ETA=7:18:42, max mem: 20.9 GB 
[11/26 02:29:29 visual_prompt]: 	Training 500/553. train loss: 6.7834,	0.8360 s / batch. (data: 2.87e-04). ETA=3:13:22, max mem: 20.9 GB 
[11/26 02:30:20 visual_prompt]: Epoch 75 / 100: avg data time: 1.48e-01, avg batch time: 0.9815, average train loss: 5.1183
[11/26 02:31:15 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3110, average loss: 1.4458
[11/26 02:31:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.30	
[11/26 02:31:15 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[11/26 02:32:59 visual_prompt]: 	Training 100/553. train loss: 13.9413,	0.8438 s / batch. (data: 8.23e-04). ETA=3:13:00, max mem: 20.9 GB 
[11/26 02:34:36 visual_prompt]: 	Training 200/553. train loss: 4.9527,	0.8176 s / batch. (data: 3.33e-04). ETA=3:05:40, max mem: 20.9 GB 
[11/26 02:36:12 visual_prompt]: 	Training 300/553. train loss: 0.9832,	0.8311 s / batch. (data: 3.05e-04). ETA=3:07:20, max mem: 20.9 GB 
[11/26 02:37:48 visual_prompt]: 	Training 400/553. train loss: 7.1801,	0.8541 s / batch. (data: 3.01e-02). ETA=3:11:05, max mem: 20.9 GB 
[11/26 02:39:24 visual_prompt]: 	Training 500/553. train loss: 4.4765,	0.8361 s / batch. (data: 7.71e-04). ETA=3:05:41, max mem: 20.9 GB 
[11/26 02:40:16 visual_prompt]: Epoch 76 / 100: avg data time: 1.45e-01, avg batch time: 0.9779, average train loss: 5.7713
[11/26 02:41:12 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3101, average loss: 4.5829
[11/26 02:41:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.87	
[11/26 02:41:12 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[11/26 02:42:53 visual_prompt]: 	Training 100/553. train loss: 3.0491,	1.1826 s / batch. (data: 3.54e-01). ETA=4:19:37, max mem: 20.9 GB 
[11/26 02:44:32 visual_prompt]: 	Training 200/553. train loss: 5.3014,	0.8400 s / batch. (data: 3.32e-04). ETA=3:03:00, max mem: 20.9 GB 
[11/26 02:46:07 visual_prompt]: 	Training 300/553. train loss: 2.1961,	0.8294 s / batch. (data: 3.24e-04). ETA=2:59:19, max mem: 20.9 GB 
[11/26 02:47:45 visual_prompt]: 	Training 400/553. train loss: 2.6444,	0.8200 s / batch. (data: 3.17e-04). ETA=2:55:54, max mem: 20.9 GB 
[11/26 02:49:21 visual_prompt]: 	Training 500/553. train loss: 1.8237,	0.8533 s / batch. (data: 5.40e-03). ETA=3:01:38, max mem: 20.9 GB 
[11/26 02:50:11 visual_prompt]: Epoch 77 / 100: avg data time: 1.42e-01, avg batch time: 0.9759, average train loss: 4.3626
[11/26 02:51:07 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3103, average loss: 0.7304
[11/26 02:51:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.36	
[11/26 02:51:07 visual_prompt]: Best epoch 77: best metric: -0.730
[11/26 02:51:07 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[11/26 02:52:47 visual_prompt]: 	Training 100/553. train loss: 2.0175,	0.8280 s / batch. (data: 3.00e-04). ETA=2:54:08, max mem: 20.9 GB 
[11/26 02:54:24 visual_prompt]: 	Training 200/553. train loss: 16.9373,	0.8605 s / batch. (data: 1.05e-02). ETA=2:59:32, max mem: 20.9 GB 
[11/26 02:56:01 visual_prompt]: 	Training 300/553. train loss: 3.0199,	0.8438 s / batch. (data: 1.92e-02). ETA=2:54:39, max mem: 20.9 GB 
[11/26 02:57:39 visual_prompt]: 	Training 400/553. train loss: 3.3504,	0.8267 s / batch. (data: 7.96e-03). ETA=2:49:44, max mem: 20.9 GB 
[11/26 02:59:15 visual_prompt]: 	Training 500/553. train loss: 1.7894,	0.8325 s / batch. (data: 3.24e-04). ETA=2:49:32, max mem: 20.9 GB 
[11/26 03:00:07 visual_prompt]: Epoch 78 / 100: avg data time: 1.42e-01, avg batch time: 0.9755, average train loss: 4.6980
[11/26 03:01:02 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3103, average loss: 1.2920
[11/26 03:01:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.01	
[11/26 03:01:02 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[11/26 03:02:45 visual_prompt]: 	Training 100/553. train loss: 0.5852,	0.8175 s / batch. (data: 3.09e-04). ETA=2:44:23, max mem: 20.9 GB 
[11/26 03:04:21 visual_prompt]: 	Training 200/553. train loss: 6.9634,	0.8376 s / batch. (data: 1.05e-02). ETA=2:47:02, max mem: 20.9 GB 
[11/26 03:05:55 visual_prompt]: 	Training 300/553. train loss: 0.8293,	1.0816 s / batch. (data: 2.64e-01). ETA=3:33:54, max mem: 20.9 GB 
[11/26 03:07:36 visual_prompt]: 	Training 400/553. train loss: 1.0083,	0.8240 s / batch. (data: 3.13e-04). ETA=2:41:35, max mem: 20.9 GB 
[11/26 03:09:14 visual_prompt]: 	Training 500/553. train loss: 10.3924,	0.8625 s / batch. (data: 7.72e-04). ETA=2:47:42, max mem: 20.9 GB 
[11/26 03:10:03 visual_prompt]: Epoch 79 / 100: avg data time: 1.43e-01, avg batch time: 0.9779, average train loss: 4.3958
[11/26 03:10:59 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3117, average loss: 7.9201
[11/26 03:10:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.35	
[11/26 03:10:59 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[11/26 03:12:39 visual_prompt]: 	Training 100/553. train loss: 0.8313,	0.8440 s / batch. (data: 3.46e-04). ETA=2:41:56, max mem: 20.9 GB 
[11/26 03:14:16 visual_prompt]: 	Training 200/553. train loss: 0.8927,	0.8509 s / batch. (data: 1.05e-02). ETA=2:41:51, max mem: 20.9 GB 
[11/26 03:15:53 visual_prompt]: 	Training 300/553. train loss: 2.7040,	1.3160 s / batch. (data: 4.60e-01). ETA=4:08:07, max mem: 20.9 GB 
[11/26 03:17:32 visual_prompt]: 	Training 400/553. train loss: 14.2382,	1.3320 s / batch. (data: 5.10e-01). ETA=4:08:55, max mem: 20.9 GB 
[11/26 03:19:07 visual_prompt]: 	Training 500/553. train loss: 11.6514,	1.0647 s / batch. (data: 2.39e-01). ETA=3:17:11, max mem: 20.9 GB 
[11/26 03:19:58 visual_prompt]: Epoch 80 / 100: avg data time: 1.41e-01, avg batch time: 0.9749, average train loss: 3.7876
[11/26 03:20:54 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3094, average loss: 4.9617
[11/26 03:20:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.33	
[11/26 03:20:54 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[11/26 03:22:36 visual_prompt]: 	Training 100/553. train loss: 1.7479,	0.8399 s / batch. (data: 3.12e-04). ETA=2:33:25, max mem: 20.9 GB 
[11/26 03:24:16 visual_prompt]: 	Training 200/553. train loss: 10.2824,	0.8754 s / batch. (data: 1.56e-02). ETA=2:38:26, max mem: 20.9 GB 
[11/26 03:25:51 visual_prompt]: 	Training 300/553. train loss: 2.4862,	0.8480 s / batch. (data: 3.24e-04). ETA=2:32:04, max mem: 20.9 GB 
[11/26 03:27:28 visual_prompt]: 	Training 400/553. train loss: 4.9085,	1.5447 s / batch. (data: 7.18e-01). ETA=4:34:26, max mem: 20.9 GB 
[11/26 03:29:03 visual_prompt]: 	Training 500/553. train loss: 1.2232,	1.9243 s / batch. (data: 1.09e+00). ETA=5:38:41, max mem: 20.9 GB 
[11/26 03:29:55 visual_prompt]: Epoch 81 / 100: avg data time: 1.45e-01, avg batch time: 0.9777, average train loss: 3.2534
[11/26 03:30:51 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3104, average loss: 1.6483
[11/26 03:30:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.38	
[11/26 03:30:51 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[11/26 03:32:32 visual_prompt]: 	Training 100/553. train loss: 0.8718,	0.8308 s / batch. (data: 3.26e-04). ETA=2:24:05, max mem: 20.9 GB 
[11/26 03:34:11 visual_prompt]: 	Training 200/553. train loss: 1.0874,	0.8399 s / batch. (data: 3.24e-04). ETA=2:24:17, max mem: 20.9 GB 
[11/26 03:35:47 visual_prompt]: 	Training 300/553. train loss: 0.8942,	1.8360 s / batch. (data: 1.02e+00). ETA=5:12:19, max mem: 20.9 GB 
[11/26 03:37:22 visual_prompt]: 	Training 400/553. train loss: 2.0164,	1.9044 s / batch. (data: 1.08e+00). ETA=5:20:47, max mem: 20.9 GB 
[11/26 03:39:00 visual_prompt]: 	Training 500/553. train loss: 0.6835,	0.8351 s / batch. (data: 5.42e-03). ETA=2:19:16, max mem: 20.9 GB 
[11/26 03:39:50 visual_prompt]: Epoch 82 / 100: avg data time: 1.42e-01, avg batch time: 0.9759, average train loss: 2.6596
[11/26 03:40:46 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3083, average loss: 3.6602
[11/26 03:40:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.97	
[11/26 03:40:46 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[11/26 03:42:28 visual_prompt]: 	Training 100/553. train loss: 3.3865,	0.8639 s / batch. (data: 1.55e-03). ETA=2:21:53, max mem: 20.9 GB 
[11/26 03:44:07 visual_prompt]: 	Training 200/553. train loss: 19.2810,	0.8407 s / batch. (data: 1.95e-02). ETA=2:16:40, max mem: 20.9 GB 
[11/26 03:45:44 visual_prompt]: 	Training 300/553. train loss: 6.4987,	0.8600 s / batch. (data: 2.96e-04). ETA=2:18:22, max mem: 20.9 GB 
[11/26 03:47:22 visual_prompt]: 	Training 400/553. train loss: 3.1918,	0.8520 s / batch. (data: 2.92e-04). ETA=2:15:39, max mem: 20.9 GB 
[11/26 03:49:00 visual_prompt]: 	Training 500/553. train loss: 0.7034,	0.8468 s / batch. (data: 2.28e-02). ETA=2:13:25, max mem: 20.9 GB 
[11/26 03:49:49 visual_prompt]: Epoch 83 / 100: avg data time: 1.48e-01, avg batch time: 0.9816, average train loss: 2.7616
[11/26 03:50:45 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3088, average loss: 3.2479
[11/26 03:50:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.66	
[11/26 03:50:45 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[11/26 03:52:27 visual_prompt]: 	Training 100/553. train loss: 0.7517,	0.8249 s / batch. (data: 3.27e-04). ETA=2:07:52, max mem: 20.9 GB 
[11/26 03:54:03 visual_prompt]: 	Training 200/553. train loss: 6.3723,	1.6167 s / batch. (data: 7.75e-01). ETA=4:07:55, max mem: 20.9 GB 
[11/26 03:55:40 visual_prompt]: 	Training 300/553. train loss: 0.7850,	0.8245 s / batch. (data: 3.07e-04). ETA=2:05:04, max mem: 20.9 GB 
[11/26 03:57:19 visual_prompt]: 	Training 400/553. train loss: 0.5892,	0.8393 s / batch. (data: 2.99e-04). ETA=2:05:54, max mem: 20.9 GB 
[11/26 03:58:54 visual_prompt]: 	Training 500/553. train loss: 0.3949,	0.8669 s / batch. (data: 1.06e-02). ETA=2:08:36, max mem: 20.9 GB 
[11/26 03:59:47 visual_prompt]: Epoch 84 / 100: avg data time: 1.48e-01, avg batch time: 0.9808, average train loss: 2.4993
[11/26 04:00:43 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3109, average loss: 2.2649
[11/26 04:00:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.07	
[11/26 04:00:43 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[11/26 04:02:26 visual_prompt]: 	Training 100/553. train loss: 1.3921,	2.9444 s / batch. (data: 2.13e+00). ETA=7:09:17, max mem: 20.9 GB 
[11/26 04:04:02 visual_prompt]: 	Training 200/553. train loss: 4.1894,	1.0017 s / batch. (data: 1.60e-01). ETA=2:24:22, max mem: 20.9 GB 
[11/26 04:05:39 visual_prompt]: 	Training 300/553. train loss: 1.8140,	0.8366 s / batch. (data: 3.23e-04). ETA=1:59:11, max mem: 20.9 GB 
[11/26 04:07:16 visual_prompt]: 	Training 400/553. train loss: 3.1759,	0.8287 s / batch. (data: 5.45e-03). ETA=1:56:40, max mem: 20.9 GB 
[11/26 04:08:54 visual_prompt]: 	Training 500/553. train loss: 2.5262,	0.8322 s / batch. (data: 2.88e-04). ETA=1:55:46, max mem: 20.9 GB 
[11/26 04:09:44 visual_prompt]: Epoch 85 / 100: avg data time: 1.46e-01, avg batch time: 0.9787, average train loss: 1.9503
[11/26 04:10:39 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3073, average loss: 2.5191
[11/26 04:10:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.71	
[11/26 04:10:39 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[11/26 04:12:23 visual_prompt]: 	Training 100/553. train loss: 0.4905,	2.9187 s / batch. (data: 2.10e+00). ETA=6:38:38, max mem: 20.9 GB 
[11/26 04:13:58 visual_prompt]: 	Training 200/553. train loss: 0.5548,	0.8360 s / batch. (data: 3.35e-04). ETA=1:52:47, max mem: 20.9 GB 
[11/26 04:15:34 visual_prompt]: 	Training 300/553. train loss: 3.0682,	0.8756 s / batch. (data: 3.39e-04). ETA=1:56:40, max mem: 20.9 GB 
[11/26 04:17:13 visual_prompt]: 	Training 400/553. train loss: 1.0020,	0.8633 s / batch. (data: 1.53e-02). ETA=1:53:35, max mem: 20.9 GB 
[11/26 04:18:51 visual_prompt]: 	Training 500/553. train loss: 2.6574,	0.8640 s / batch. (data: 1.20e-02). ETA=1:52:14, max mem: 20.9 GB 
[11/26 04:19:43 visual_prompt]: Epoch 86 / 100: avg data time: 1.48e-01, avg batch time: 0.9822, average train loss: 1.7409
[11/26 04:20:38 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3096, average loss: 0.8590
[11/26 04:20:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 54.43	
[11/26 04:20:38 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[11/26 04:22:21 visual_prompt]: 	Training 100/553. train loss: 0.8736,	0.8373 s / batch. (data: 3.19e-04). ETA=1:46:38, max mem: 20.9 GB 
[11/26 04:24:00 visual_prompt]: 	Training 200/553. train loss: 3.4571,	0.8511 s / batch. (data: 1.05e-02). ETA=1:46:59, max mem: 20.9 GB 
[11/26 04:25:38 visual_prompt]: 	Training 300/553. train loss: 1.0294,	1.5461 s / batch. (data: 7.14e-01). ETA=3:11:45, max mem: 20.9 GB 
[11/26 04:27:13 visual_prompt]: 	Training 400/553. train loss: 0.6380,	0.8500 s / batch. (data: 2.87e-04). ETA=1:44:00, max mem: 20.9 GB 
[11/26 04:28:50 visual_prompt]: 	Training 500/553. train loss: 1.9471,	0.8433 s / batch. (data: 1.20e-02). ETA=1:41:47, max mem: 20.9 GB 
[11/26 04:29:41 visual_prompt]: Epoch 87 / 100: avg data time: 1.46e-01, avg batch time: 0.9806, average train loss: 1.5781
[11/26 04:30:36 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3095, average loss: 1.7448
[11/26 04:30:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.57	
[11/26 04:30:36 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[11/26 04:32:17 visual_prompt]: 	Training 100/553. train loss: 0.0008,	0.8360 s / batch. (data: 3.15e-04). ETA=1:38:46, max mem: 20.9 GB 
[11/26 04:33:54 visual_prompt]: 	Training 200/553. train loss: 1.7567,	0.8244 s / batch. (data: 5.47e-03). ETA=1:36:01, max mem: 20.9 GB 
[11/26 04:35:34 visual_prompt]: 	Training 300/553. train loss: 0.4015,	0.8360 s / batch. (data: 3.14e-04). ETA=1:35:59, max mem: 20.9 GB 
[11/26 04:37:14 visual_prompt]: 	Training 400/553. train loss: 1.1784,	2.2543 s / batch. (data: 1.42e+00). ETA=4:15:04, max mem: 20.9 GB 
[11/26 04:38:48 visual_prompt]: 	Training 500/553. train loss: 3.3776,	0.8480 s / batch. (data: 3.15e-04). ETA=1:34:32, max mem: 20.9 GB 
[11/26 04:39:39 visual_prompt]: Epoch 88 / 100: avg data time: 1.48e-01, avg batch time: 0.9814, average train loss: 1.7065
[11/26 04:40:34 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3089, average loss: 0.7878
[11/26 04:40:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.60	
[11/26 04:40:34 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[11/26 04:42:16 visual_prompt]: 	Training 100/553. train loss: 1.7130,	0.8462 s / batch. (data: 3.28e-04). ETA=1:32:10, max mem: 20.9 GB 
[11/26 04:43:52 visual_prompt]: 	Training 200/553. train loss: 0.5703,	0.8480 s / batch. (data: 3.00e-04). ETA=1:30:57, max mem: 20.9 GB 
[11/26 04:45:31 visual_prompt]: 	Training 300/553. train loss: 1.2648,	0.8360 s / batch. (data: 3.16e-04). ETA=1:28:17, max mem: 20.9 GB 
[11/26 04:47:09 visual_prompt]: 	Training 400/553. train loss: 0.1427,	0.8714 s / batch. (data: 1.56e-02). ETA=1:30:33, max mem: 20.9 GB 
[11/26 04:48:45 visual_prompt]: 	Training 500/553. train loss: 2.9255,	0.8320 s / batch. (data: 2.73e-04). ETA=1:25:05, max mem: 20.9 GB 
[11/26 04:49:35 visual_prompt]: Epoch 89 / 100: avg data time: 1.44e-01, avg batch time: 0.9783, average train loss: 1.1049
[11/26 04:50:31 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3114, average loss: 0.6922
[11/26 04:50:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.77	
[11/26 04:50:31 visual_prompt]: Best epoch 89: best metric: -0.692
[11/26 04:50:31 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[11/26 04:52:14 visual_prompt]: 	Training 100/553. train loss: 0.7318,	0.8480 s / batch. (data: 2.92e-04). ETA=1:24:33, max mem: 20.9 GB 
[11/26 04:53:49 visual_prompt]: 	Training 200/553. train loss: 0.9195,	1.4843 s / batch. (data: 6.63e-01). ETA=2:25:32, max mem: 20.9 GB 
[11/26 04:55:26 visual_prompt]: 	Training 300/553. train loss: 2.4238,	0.8481 s / batch. (data: 2.70e-04). ETA=1:21:44, max mem: 20.9 GB 
[11/26 04:57:04 visual_prompt]: 	Training 400/553. train loss: 0.5415,	1.3955 s / batch. (data: 5.79e-01). ETA=2:12:10, max mem: 20.9 GB 
[11/26 04:58:42 visual_prompt]: 	Training 500/553. train loss: 0.6259,	0.8183 s / batch. (data: 2.58e-04). ETA=1:16:08, max mem: 20.9 GB 
[11/26 04:59:31 visual_prompt]: Epoch 90 / 100: avg data time: 1.44e-01, avg batch time: 0.9777, average train loss: 1.1953
[11/26 05:00:27 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3108, average loss: 0.7164
[11/26 05:00:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.27	
[11/26 05:00:27 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[11/26 05:02:10 visual_prompt]: 	Training 100/553. train loss: 0.7038,	0.8311 s / batch. (data: 5.43e-03). ETA=1:15:13, max mem: 20.9 GB 
[11/26 05:03:49 visual_prompt]: 	Training 200/553. train loss: 1.1274,	0.8405 s / batch. (data: 1.05e-02). ETA=1:14:40, max mem: 20.9 GB 
[11/26 05:05:27 visual_prompt]: 	Training 300/553. train loss: 0.7324,	0.8400 s / batch. (data: 2.93e-04). ETA=1:13:13, max mem: 20.9 GB 
[11/26 05:07:07 visual_prompt]: 	Training 400/553. train loss: 0.8049,	1.5724 s / batch. (data: 7.56e-01). ETA=2:14:26, max mem: 20.9 GB 
[11/26 05:08:41 visual_prompt]: 	Training 500/553. train loss: 0.6277,	0.8442 s / batch. (data: 2.06e-02). ETA=1:10:46, max mem: 20.9 GB 
[11/26 05:09:31 visual_prompt]: Epoch 91 / 100: avg data time: 1.48e-01, avg batch time: 0.9834, average train loss: 1.0164
[11/26 05:10:26 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3110, average loss: 0.7110
[11/26 05:10:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.51	
[11/26 05:10:26 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[11/26 05:12:10 visual_prompt]: 	Training 100/553. train loss: 0.6237,	1.8240 s / batch. (data: 9.99e-01). ETA=2:28:15, max mem: 20.9 GB 
[11/26 05:13:46 visual_prompt]: 	Training 200/553. train loss: 0.5295,	0.8399 s / batch. (data: 3.00e-04). ETA=1:06:52, max mem: 20.9 GB 
[11/26 05:15:22 visual_prompt]: 	Training 300/553. train loss: 0.6465,	0.8505 s / batch. (data: 1.05e-02). ETA=1:06:17, max mem: 20.9 GB 
[11/26 05:17:02 visual_prompt]: 	Training 400/553. train loss: 0.7094,	0.8522 s / batch. (data: 2.41e-02). ETA=1:05:00, max mem: 20.9 GB 
[11/26 05:18:37 visual_prompt]: 	Training 500/553. train loss: 0.9332,	0.8322 s / batch. (data: 1.05e-02). ETA=1:02:05, max mem: 20.9 GB 
[11/26 05:19:28 visual_prompt]: Epoch 92 / 100: avg data time: 1.45e-01, avg batch time: 0.9787, average train loss: 0.9226
[11/26 05:20:23 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3109, average loss: 1.0049
[11/26 05:20:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.61	
[11/26 05:20:23 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[11/26 05:22:04 visual_prompt]: 	Training 100/553. train loss: 0.9730,	0.8633 s / batch. (data: 1.55e-02). ETA=1:02:13, max mem: 20.9 GB 
[11/26 05:23:42 visual_prompt]: 	Training 200/553. train loss: 0.7291,	0.9801 s / batch. (data: 1.48e-01). ETA=1:08:59, max mem: 20.9 GB 
[11/26 05:25:18 visual_prompt]: 	Training 300/553. train loss: 0.5906,	0.8305 s / batch. (data: 1.28e-02). ETA=0:57:04, max mem: 20.9 GB 
[11/26 05:26:56 visual_prompt]: 	Training 400/553. train loss: 0.9301,	0.8295 s / batch. (data: 5.43e-03). ETA=0:55:37, max mem: 20.9 GB 
[11/26 05:28:36 visual_prompt]: 	Training 500/553. train loss: 0.6078,	0.8317 s / batch. (data: 2.92e-04). ETA=0:54:23, max mem: 20.9 GB 
[11/26 05:29:26 visual_prompt]: Epoch 93 / 100: avg data time: 1.47e-01, avg batch time: 0.9809, average train loss: 0.9613
[11/26 05:30:21 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3098, average loss: 0.6954
[11/26 05:30:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.57	
[11/26 05:30:21 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[11/26 05:32:03 visual_prompt]: 	Training 100/553. train loss: 0.8118,	0.9976 s / batch. (data: 1.53e-01). ETA=1:02:41, max mem: 20.9 GB 
[11/26 05:33:39 visual_prompt]: 	Training 200/553. train loss: 0.7761,	0.8360 s / batch. (data: 3.20e-04). ETA=0:51:08, max mem: 20.9 GB 
[11/26 05:35:19 visual_prompt]: 	Training 300/553. train loss: 0.7036,	0.8177 s / batch. (data: 3.12e-04). ETA=0:48:39, max mem: 20.9 GB 
[11/26 05:36:55 visual_prompt]: 	Training 400/553. train loss: 0.7895,	0.8720 s / batch. (data: 3.43e-02). ETA=0:50:26, max mem: 20.9 GB 
[11/26 05:38:31 visual_prompt]: 	Training 500/553. train loss: 0.8331,	0.8400 s / batch. (data: 3.06e-04). ETA=0:47:11, max mem: 20.9 GB 
[11/26 05:39:24 visual_prompt]: Epoch 94 / 100: avg data time: 1.48e-01, avg batch time: 0.9810, average train loss: 0.8435
[11/26 05:40:19 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3102, average loss: 0.7070
[11/26 05:40:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.32	
[11/26 05:40:19 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[11/26 05:42:00 visual_prompt]: 	Training 100/553. train loss: 0.6114,	0.8520 s / batch. (data: 3.05e-04). ETA=0:45:41, max mem: 20.9 GB 
[11/26 05:43:40 visual_prompt]: 	Training 200/553. train loss: 0.4697,	0.8478 s / batch. (data: 1.84e-02). ETA=0:44:03, max mem: 20.9 GB 
[11/26 05:45:16 visual_prompt]: 	Training 300/553. train loss: 0.8146,	1.7080 s / batch. (data: 8.71e-01). ETA=1:25:54, max mem: 20.9 GB 
[11/26 05:46:53 visual_prompt]: 	Training 400/553. train loss: 1.1747,	1.6420 s / batch. (data: 8.08e-01). ETA=1:19:51, max mem: 20.9 GB 
[11/26 05:48:30 visual_prompt]: 	Training 500/553. train loss: 0.2609,	0.8333 s / batch. (data: 7.95e-03). ETA=0:39:08, max mem: 20.9 GB 
[11/26 05:49:21 visual_prompt]: Epoch 95 / 100: avg data time: 1.47e-01, avg batch time: 0.9805, average train loss: 0.7454
[11/26 05:50:17 visual_prompt]: Inference (val):avg data time: 3.59e-04, avg batch time: 0.3109, average loss: 0.7121
[11/26 05:50:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.12	
[11/26 05:50:17 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[11/26 05:52:01 visual_prompt]: 	Training 100/553. train loss: 0.6479,	0.8240 s / batch. (data: 3.09e-04). ETA=0:36:35, max mem: 20.9 GB 
[11/26 05:53:40 visual_prompt]: 	Training 200/553. train loss: 0.7923,	0.8564 s / batch. (data: 1.09e-02). ETA=0:36:36, max mem: 20.9 GB 
[11/26 05:55:15 visual_prompt]: 	Training 300/553. train loss: 0.7058,	0.8495 s / batch. (data: 1.05e-02). ETA=0:34:54, max mem: 20.9 GB 
[11/26 05:56:53 visual_prompt]: 	Training 400/553. train loss: 0.8523,	0.8374 s / batch. (data: 5.43e-03). ETA=0:33:00, max mem: 20.9 GB 
[11/26 05:58:30 visual_prompt]: 	Training 500/553. train loss: 0.8478,	0.8600 s / batch. (data: 3.40e-04). ETA=0:32:27, max mem: 20.9 GB 
[11/26 05:59:19 visual_prompt]: Epoch 96 / 100: avg data time: 1.46e-01, avg batch time: 0.9806, average train loss: 0.7558
[11/26 06:00:15 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3106, average loss: 0.7252
[11/26 06:00:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.07	
[11/26 06:00:15 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[11/26 06:01:55 visual_prompt]: 	Training 100/553. train loss: 0.7071,	0.8361 s / batch. (data: 1.64e-02). ETA=0:29:25, max mem: 20.9 GB 
[11/26 06:03:35 visual_prompt]: 	Training 200/553. train loss: 0.5797,	0.8205 s / batch. (data: 3.08e-04). ETA=0:27:30, max mem: 20.9 GB 
[11/26 06:05:12 visual_prompt]: 	Training 300/553. train loss: 0.6748,	0.8354 s / batch. (data: 3.12e-04). ETA=0:26:37, max mem: 20.9 GB 
[11/26 06:06:50 visual_prompt]: 	Training 400/553. train loss: 0.5209,	0.8764 s / batch. (data: 6.96e-04). ETA=0:26:28, max mem: 20.9 GB 
[11/26 06:08:25 visual_prompt]: 	Training 500/553. train loss: 0.9771,	0.8349 s / batch. (data: 7.56e-04). ETA=0:23:49, max mem: 20.9 GB 
[11/26 06:09:18 visual_prompt]: Epoch 97 / 100: avg data time: 1.48e-01, avg batch time: 0.9821, average train loss: 0.7416
[11/26 06:10:13 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3098, average loss: 0.7607
[11/26 06:10:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.37	
[11/26 06:10:13 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[11/26 06:11:57 visual_prompt]: 	Training 100/553. train loss: 0.7290,	0.8312 s / batch. (data: 7.90e-04). ETA=0:21:35, max mem: 20.9 GB 
[11/26 06:13:33 visual_prompt]: 	Training 200/553. train loss: 0.6622,	0.8704 s / batch. (data: 7.95e-03). ETA=0:21:09, max mem: 20.9 GB 
[11/26 06:15:11 visual_prompt]: 	Training 300/553. train loss: 0.6998,	2.2000 s / batch. (data: 1.36e+00). ETA=0:49:49, max mem: 20.9 GB 
[11/26 06:16:48 visual_prompt]: 	Training 400/553. train loss: 0.6972,	1.4651 s / batch. (data: 6.40e-01). ETA=0:30:44, max mem: 20.9 GB 
[11/26 06:18:25 visual_prompt]: 	Training 500/553. train loss: 0.6701,	0.8222 s / batch. (data: 2.94e-04). ETA=0:15:52, max mem: 20.9 GB 
[11/26 06:19:16 visual_prompt]: Epoch 98 / 100: avg data time: 1.47e-01, avg batch time: 0.9812, average train loss: 0.7065
[11/26 06:20:12 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.3096, average loss: 0.6931
[11/26 06:20:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.29	
[11/26 06:20:12 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[11/26 06:21:53 visual_prompt]: 	Training 100/553. train loss: 0.6800,	0.8450 s / batch. (data: 8.89e-03). ETA=0:14:10, max mem: 20.9 GB 
[11/26 06:23:32 visual_prompt]: 	Training 200/553. train loss: 0.7118,	0.8427 s / batch. (data: 1.05e-02). ETA=0:12:43, max mem: 20.9 GB 
[11/26 06:25:11 visual_prompt]: 	Training 300/553. train loss: 0.6977,	1.3773 s / batch. (data: 5.61e-01). ETA=0:18:30, max mem: 20.9 GB 
[11/26 06:26:46 visual_prompt]: 	Training 400/553. train loss: 0.5337,	0.8276 s / batch. (data: 9.27e-03). ETA=0:09:44, max mem: 20.9 GB 
[11/26 06:28:22 visual_prompt]: 	Training 500/553. train loss: 0.6920,	0.8480 s / batch. (data: 1.20e-02). ETA=0:08:33, max mem: 20.9 GB 
[11/26 06:29:14 visual_prompt]: Epoch 99 / 100: avg data time: 1.46e-01, avg batch time: 0.9799, average train loss: 0.6978
[11/26 06:30:09 visual_prompt]: Inference (val):avg data time: 4.09e-04, avg batch time: 0.3103, average loss: 0.7010
[11/26 06:30:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.53	
[11/26 06:30:09 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[11/26 06:31:54 visual_prompt]: 	Training 100/553. train loss: 0.7001,	0.8400 s / batch. (data: 3.41e-04). ETA=0:06:20, max mem: 20.9 GB 
[11/26 06:33:29 visual_prompt]: 	Training 200/553. train loss: 0.6907,	0.8317 s / batch. (data: 3.35e-04). ETA=0:04:53, max mem: 20.9 GB 
[11/26 06:35:08 visual_prompt]: 	Training 300/553. train loss: 0.6684,	0.8440 s / batch. (data: 1.60e-02). ETA=0:03:33, max mem: 20.9 GB 
[11/26 06:36:46 visual_prompt]: 	Training 400/553. train loss: 0.8026,	0.8400 s / batch. (data: 2.96e-04). ETA=0:02:08, max mem: 20.9 GB 
[11/26 06:38:23 visual_prompt]: 	Training 500/553. train loss: 0.6228,	0.8440 s / batch. (data: 3.16e-04). ETA=0:00:44, max mem: 20.9 GB 
[11/26 06:39:12 visual_prompt]: Epoch 100 / 100: avg data time: 1.49e-01, avg batch time: 0.9822, average train loss: 0.6925
[11/26 06:40:08 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3102, average loss: 0.6868
[11/26 06:40:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.97	
[11/26 06:40:08 visual_prompt]: Best epoch 100: best metric: -0.687
[11/26 06:40:08 visual_prompt]: Rank of current process: 0. World size: 1
[11/26 06:40:08 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/26 06:40:08 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/26 06:40:08 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/26 06:40:08 visual_prompt]: Training with config:
[11/26 06:40:08 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/26 06:40:08 visual_prompt]: Loading training data...
[11/26 06:40:08 visual_prompt]: Constructing mammo-cbis dataset train...
[11/26 06:40:08 visual_prompt]: Loading validation data...
[11/26 06:40:08 visual_prompt]: Constructing mammo-cbis dataset val...
[11/26 06:40:09 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/26 06:40:16 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/26 06:40:16 visual_prompt]: tuned percent:0.525
[11/26 06:40:16 visual_prompt]: Device used for model: 0
[11/26 06:40:16 visual_prompt]: Setting up Evaluator...
[11/26 06:40:16 visual_prompt]: Setting up Trainer...
[11/26 06:40:16 visual_prompt]: 	Setting up the optimizer...
[11/26 06:40:16 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/26 06:41:58 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8303 s / batch. (data: 2.94e-04). ETA=12:43:50, max mem: 20.9 GB 
[11/26 06:43:33 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8280 s / batch. (data: 7.95e-03). ETA=12:40:22, max mem: 20.9 GB 
[11/26 06:45:13 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8360 s / batch. (data: 3.43e-04). ETA=12:46:20, max mem: 20.9 GB 
[11/26 06:46:50 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8280 s / batch. (data: 3.24e-04). ETA=12:37:39, max mem: 20.9 GB 
[11/26 06:48:29 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8360 s / batch. (data: 3.09e-04). ETA=12:43:33, max mem: 20.9 GB 
[11/26 06:49:20 visual_prompt]: Epoch 1 / 100: avg data time: 1.49e-01, avg batch time: 0.9831, average train loss: 1.5403
[11/26 06:50:15 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3095, average loss: 1.5201
[11/26 06:50:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/26 06:50:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/26 06:51:56 visual_prompt]: 	Training 100/553. train loss: 3.8541,	0.8337 s / batch. (data: 5.44e-03). ETA=12:39:17, max mem: 20.9 GB 
[11/26 06:53:33 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8398 s / batch. (data: 7.75e-03). ETA=12:43:27, max mem: 20.9 GB 
[11/26 06:55:12 visual_prompt]: 	Training 300/553. train loss: 2.6410,	0.8360 s / batch. (data: 2.92e-04). ETA=12:38:38, max mem: 20.9 GB 
[11/26 06:56:49 visual_prompt]: 	Training 400/553. train loss: 1.6751,	0.8213 s / batch. (data: 2.98e-04). ETA=12:23:52, max mem: 20.9 GB 
[11/26 06:58:27 visual_prompt]: 	Training 500/553. train loss: 0.5674,	0.8269 s / batch. (data: 2.89e-04). ETA=12:27:35, max mem: 20.9 GB 
[11/26 06:59:17 visual_prompt]: Epoch 2 / 100: avg data time: 1.46e-01, avg batch time: 0.9791, average train loss: 3.2359
[11/26 07:00:13 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3107, average loss: 10.4632
[11/26 07:00:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.18	
[11/26 07:00:13 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/26 07:01:54 visual_prompt]: 	Training 100/553. train loss: 24.3151,	0.8231 s / batch. (data: 3.20e-04). ETA=12:22:04, max mem: 20.9 GB 
[11/26 07:03:32 visual_prompt]: 	Training 200/553. train loss: 4.6712,	0.8368 s / batch. (data: 8.74e-03). ETA=12:33:03, max mem: 20.9 GB 
[11/26 07:05:09 visual_prompt]: 	Training 300/553. train loss: 2.6836,	0.8809 s / batch. (data: 2.09e-02). ETA=13:11:16, max mem: 20.9 GB 
[11/26 07:06:47 visual_prompt]: 	Training 400/553. train loss: 6.9202,	0.8273 s / batch. (data: 5.44e-03). ETA=12:21:42, max mem: 20.9 GB 
[11/26 07:08:26 visual_prompt]: 	Training 500/553. train loss: 2.7821,	1.2070 s / batch. (data: 3.76e-01). ETA=18:00:10, max mem: 20.9 GB 
[11/26 07:09:16 visual_prompt]: Epoch 3 / 100: avg data time: 1.49e-01, avg batch time: 0.9828, average train loss: 6.4887
[11/26 07:10:12 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3107, average loss: 7.2072
[11/26 07:10:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.13	
[11/26 07:10:12 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/26 07:11:56 visual_prompt]: 	Training 100/553. train loss: 21.5205,	0.8520 s / batch. (data: 3.05e-04). ETA=12:40:16, max mem: 20.9 GB 
[11/26 07:13:33 visual_prompt]: 	Training 200/553. train loss: 17.5341,	0.8253 s / batch. (data: 3.03e-04). ETA=12:15:06, max mem: 20.9 GB 
[11/26 07:15:10 visual_prompt]: 	Training 300/553. train loss: 1.5394,	1.0762 s / batch. (data: 2.39e-01). ETA=15:56:43, max mem: 20.9 GB 
[11/26 07:16:44 visual_prompt]: 	Training 400/553. train loss: 17.8045,	1.1907 s / batch. (data: 3.52e-01). ETA=17:36:35, max mem: 20.9 GB 
[11/26 07:18:23 visual_prompt]: 	Training 500/553. train loss: 15.0734,	3.2397 s / batch. (data: 2.42e+00). ETA=1 day, 23:49:20, max mem: 20.9 GB 
[11/26 07:19:15 visual_prompt]: Epoch 4 / 100: avg data time: 1.47e-01, avg batch time: 0.9818, average train loss: 8.8992
[11/26 07:20:11 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3102, average loss: 7.4956
[11/26 07:20:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.73	
[11/26 07:20:11 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/26 07:21:51 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8440 s / batch. (data: 2.96e-04). ETA=12:25:20, max mem: 20.9 GB 
[11/26 07:23:29 visual_prompt]: 	Training 200/553. train loss: 26.0106,	1.2726 s / batch. (data: 4.37e-01). ETA=18:41:42, max mem: 20.9 GB 
[11/26 07:25:08 visual_prompt]: 	Training 300/553. train loss: 27.0224,	0.8596 s / batch. (data: 1.56e-02). ETA=12:36:15, max mem: 20.9 GB 
[11/26 07:26:45 visual_prompt]: 	Training 400/553. train loss: 5.8865,	0.8449 s / batch. (data: 8.87e-03). ETA=12:21:56, max mem: 20.9 GB 
[11/26 07:28:22 visual_prompt]: 	Training 500/553. train loss: 20.3104,	0.8290 s / batch. (data: 3.02e-04). ETA=12:06:33, max mem: 20.9 GB 
[11/26 07:29:14 visual_prompt]: Epoch 5 / 100: avg data time: 1.49e-01, avg batch time: 0.9819, average train loss: 13.5538
[11/26 07:30:10 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3096, average loss: 19.2477
[11/26 07:30:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.87	
[11/26 07:30:10 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/26 07:31:53 visual_prompt]: 	Training 100/553. train loss: 18.3519,	0.8400 s / batch. (data: 3.10e-04). ETA=12:14:05, max mem: 20.9 GB 
[11/26 07:33:29 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8354 s / batch. (data: 2.90e-04). ETA=12:08:41, max mem: 20.9 GB 
[11/26 07:35:06 visual_prompt]: 	Training 300/553. train loss: 10.2809,	0.8288 s / batch. (data: 5.47e-03). ETA=12:01:34, max mem: 20.9 GB 
[11/26 07:36:46 visual_prompt]: 	Training 400/553. train loss: 44.9343,	0.8371 s / batch. (data: 3.19e-04). ETA=12:07:23, max mem: 20.9 GB 
[11/26 07:38:23 visual_prompt]: 	Training 500/553. train loss: 30.3506,	0.8577 s / batch. (data: 3.08e-02). ETA=12:23:50, max mem: 20.9 GB 
[11/26 07:39:13 visual_prompt]: Epoch 6 / 100: avg data time: 1.51e-01, avg batch time: 0.9832, average train loss: 16.0221
[11/26 07:40:09 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3124, average loss: 6.9090
[11/26 07:40:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.85	
[11/26 07:40:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/26 07:41:49 visual_prompt]: 	Training 100/553. train loss: 21.6591,	0.8307 s / batch. (data: 3.33e-04). ETA=11:58:16, max mem: 20.9 GB 
[11/26 07:43:26 visual_prompt]: 	Training 200/553. train loss: 10.9828,	0.8560 s / batch. (data: 5.45e-03). ETA=12:18:44, max mem: 20.9 GB 
[11/26 07:45:07 visual_prompt]: 	Training 300/553. train loss: 16.7040,	1.7148 s / batch. (data: 8.74e-01). ETA=1 day, 0:37:03, max mem: 20.9 GB 
[11/26 07:46:45 visual_prompt]: 	Training 400/553. train loss: 3.1928,	1.9534 s / batch. (data: 1.13e+00). ETA=1 day, 3:59:20, max mem: 20.9 GB 
[11/26 07:48:21 visual_prompt]: 	Training 500/553. train loss: 8.6548,	0.8769 s / batch. (data: 2.08e-02). ETA=12:32:22, max mem: 20.9 GB 
[11/26 07:49:11 visual_prompt]: Epoch 7 / 100: avg data time: 1.47e-01, avg batch time: 0.9792, average train loss: 20.5133
[11/26 07:50:06 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3104, average loss: 20.0773
[11/26 07:50:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.11	
[11/26 07:50:06 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/26 07:51:45 visual_prompt]: 	Training 100/553. train loss: 65.4587,	0.8282 s / batch. (data: 1.05e-02). ETA=11:48:31, max mem: 20.9 GB 
[11/26 07:53:24 visual_prompt]: 	Training 200/553. train loss: 5.9290,	0.8522 s / batch. (data: 3.34e-04). ETA=12:07:39, max mem: 20.9 GB 
[11/26 07:55:01 visual_prompt]: 	Training 300/553. train loss: 20.1993,	0.8218 s / batch. (data: 3.37e-04). ETA=11:40:16, max mem: 20.9 GB 
[11/26 07:56:39 visual_prompt]: 	Training 400/553. train loss: 4.9946,	0.8360 s / batch. (data: 7.96e-03). ETA=11:50:59, max mem: 20.9 GB 
[11/26 07:58:17 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.3840 s / batch. (data: 5.32e-01). ETA=19:34:46, max mem: 20.9 GB 
[11/26 07:59:09 visual_prompt]: Epoch 8 / 100: avg data time: 1.48e-01, avg batch time: 0.9806, average train loss: 21.8628
[11/26 08:00:04 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3103, average loss: 4.8054
[11/26 08:00:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.08	
[11/26 08:00:04 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/26 08:01:45 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8280 s / batch. (data: 4.50e-04). ETA=11:40:40, max mem: 20.9 GB 
[11/26 08:03:22 visual_prompt]: 	Training 200/553. train loss: 19.8405,	0.8598 s / batch. (data: 3.82e-04). ETA=12:06:10, max mem: 20.9 GB 
[11/26 08:05:00 visual_prompt]: 	Training 300/553. train loss: 5.5630,	1.4920 s / batch. (data: 6.40e-01). ETA=20:57:39, max mem: 20.9 GB 
[11/26 08:06:40 visual_prompt]: 	Training 400/553. train loss: 23.2621,	0.8560 s / batch. (data: 2.80e-02). ETA=12:00:07, max mem: 20.9 GB 
[11/26 08:08:18 visual_prompt]: 	Training 500/553. train loss: 11.8249,	0.9387 s / batch. (data: 1.17e-01). ETA=13:08:05, max mem: 20.9 GB 
[11/26 08:09:07 visual_prompt]: Epoch 9 / 100: avg data time: 1.50e-01, avg batch time: 0.9821, average train loss: 22.3752
[11/26 08:10:03 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3104, average loss: 28.7980
[11/26 08:10:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.92	
[11/26 08:10:03 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/26 08:11:48 visual_prompt]: 	Training 100/553. train loss: 51.8149,	0.8246 s / batch. (data: 7.96e-03). ETA=11:30:14, max mem: 20.9 GB 
[11/26 08:13:24 visual_prompt]: 	Training 200/553. train loss: 40.6482,	0.8290 s / batch. (data: 5.43e-03). ETA=11:32:33, max mem: 20.9 GB 
[11/26 08:15:01 visual_prompt]: 	Training 300/553. train loss: 216.0547,	0.8560 s / batch. (data: 7.95e-03). ETA=11:53:40, max mem: 20.9 GB 
[11/26 08:16:35 visual_prompt]: 	Training 400/553. train loss: 9.2732,	0.8221 s / batch. (data: 3.31e-04). ETA=11:24:03, max mem: 20.9 GB 
[11/26 08:18:15 visual_prompt]: 	Training 500/553. train loss: 3.4918,	0.8400 s / batch. (data: 2.04e-02). ETA=11:37:29, max mem: 20.9 GB 
[11/26 08:19:06 visual_prompt]: Epoch 10 / 100: avg data time: 1.51e-01, avg batch time: 0.9810, average train loss: 33.7567
[11/26 08:20:02 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3086, average loss: 27.9118
[11/26 08:20:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 49.09	
[11/26 08:20:02 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/26 08:21:45 visual_prompt]: 	Training 100/553. train loss: 25.8767,	0.8215 s / batch. (data: 5.42e-03). ETA=11:20:06, max mem: 20.9 GB 
[11/26 08:23:24 visual_prompt]: 	Training 200/553. train loss: 43.7321,	0.8234 s / batch. (data: 5.44e-03). ETA=11:20:15, max mem: 20.9 GB 
[11/26 08:25:00 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.0439 s / batch. (data: 1.20e+00). ETA=1 day, 4:05:10, max mem: 20.9 GB 
[11/26 08:26:36 visual_prompt]: 	Training 400/553. train loss: 3.9212,	0.8640 s / batch. (data: 7.96e-03). ETA=11:50:54, max mem: 20.9 GB 
[11/26 08:28:12 visual_prompt]: 	Training 500/553. train loss: 7.5434,	0.8265 s / batch. (data: 2.49e-03). ETA=11:18:39, max mem: 20.9 GB 
[11/26 08:29:02 visual_prompt]: Epoch 11 / 100: avg data time: 1.45e-01, avg batch time: 0.9776, average train loss: 28.0970
[11/26 08:29:58 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3119, average loss: 22.0926
[11/26 08:29:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.95	
[11/26 08:29:58 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/26 08:31:41 visual_prompt]: 	Training 100/553. train loss: 13.9978,	0.8190 s / batch. (data: 3.29e-04). ETA=11:10:29, max mem: 20.9 GB 
[11/26 08:33:19 visual_prompt]: 	Training 200/553. train loss: 86.0450,	0.8351 s / batch. (data: 3.09e-04). ETA=11:22:16, max mem: 20.9 GB 
[11/26 08:34:55 visual_prompt]: 	Training 300/553. train loss: 195.4582,	0.8640 s / batch. (data: 2.79e-02). ETA=11:44:23, max mem: 20.9 GB 
[11/26 08:36:33 visual_prompt]: 	Training 400/553. train loss: 32.3267,	0.8607 s / batch. (data: 3.36e-04). ETA=11:40:19, max mem: 20.9 GB 
[11/26 08:38:10 visual_prompt]: 	Training 500/553. train loss: 104.1752,	0.8474 s / batch. (data: 2.27e-04). ETA=11:28:05, max mem: 20.9 GB 
[11/26 08:39:00 visual_prompt]: Epoch 12 / 100: avg data time: 1.49e-01, avg batch time: 0.9807, average train loss: 34.8851
[11/26 08:39:56 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3116, average loss: 8.3360
[11/26 08:39:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.81	
[11/26 08:39:56 visual_prompt]: Best epoch 12: best metric: -8.336
[11/26 08:39:56 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/26 08:41:39 visual_prompt]: 	Training 100/553. train loss: 12.3251,	0.8231 s / batch. (data: 3.13e-04). ETA=11:06:10, max mem: 20.9 GB 
[11/26 08:43:13 visual_prompt]: 	Training 200/553. train loss: 2.0794,	0.8366 s / batch. (data: 2.69e-04). ETA=11:15:45, max mem: 20.9 GB 
[11/26 08:44:52 visual_prompt]: 	Training 300/553. train loss: 61.6420,	1.7840 s / batch. (data: 9.46e-01). ETA=23:58:01, max mem: 20.9 GB 
[11/26 08:46:29 visual_prompt]: 	Training 400/553. train loss: 211.3320,	0.8162 s / batch. (data: 3.36e-04). ETA=10:56:30, max mem: 20.9 GB 
[11/26 08:48:08 visual_prompt]: 	Training 500/553. train loss: 41.9996,	0.8440 s / batch. (data: 3.12e-04). ETA=11:17:29, max mem: 20.9 GB 
[11/26 08:48:59 visual_prompt]: Epoch 13 / 100: avg data time: 1.51e-01, avg batch time: 0.9815, average train loss: 33.2361
[11/26 08:49:55 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.3117, average loss: 23.4402
[11/26 08:49:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.31	
[11/26 08:49:55 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/26 08:51:38 visual_prompt]: 	Training 100/553. train loss: 58.2859,	0.8313 s / batch. (data: 9.16e-03). ETA=11:05:13, max mem: 20.9 GB 
[11/26 08:53:15 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.0280 s / batch. (data: 1.81e-01). ETA=13:40:51, max mem: 20.9 GB 
[11/26 08:54:52 visual_prompt]: 	Training 300/553. train loss: 8.4469,	0.8531 s / batch. (data: 2.07e-02). ETA=11:19:46, max mem: 20.9 GB 
[11/26 08:56:29 visual_prompt]: 	Training 400/553. train loss: 24.6987,	0.8179 s / batch. (data: 5.42e-03). ETA=10:50:22, max mem: 20.9 GB 
[11/26 08:58:07 visual_prompt]: 	Training 500/553. train loss: 10.9003,	0.8415 s / batch. (data: 1.07e-02). ETA=11:07:42, max mem: 20.9 GB 
[11/26 08:58:56 visual_prompt]: Epoch 14 / 100: avg data time: 1.47e-01, avg batch time: 0.9786, average train loss: 27.9338
[11/26 08:59:52 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3106, average loss: 1.4110
[11/26 08:59:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.01	
[11/26 08:59:52 visual_prompt]: Best epoch 14: best metric: -1.411
[11/26 08:59:52 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/26 09:01:33 visual_prompt]: 	Training 100/553. train loss: 33.2693,	0.8527 s / batch. (data: 3.40e-04). ETA=11:14:29, max mem: 20.9 GB 
[11/26 09:03:09 visual_prompt]: 	Training 200/553. train loss: 175.5086,	0.8520 s / batch. (data: 3.21e-04). ETA=11:12:30, max mem: 20.9 GB 
[11/26 09:04:49 visual_prompt]: 	Training 300/553. train loss: 14.0868,	0.8739 s / batch. (data: 1.56e-02). ETA=11:28:17, max mem: 20.9 GB 
[11/26 09:06:24 visual_prompt]: 	Training 400/553. train loss: 43.9856,	1.0043 s / batch. (data: 1.55e-01). ETA=13:09:22, max mem: 20.9 GB 
[11/26 09:08:02 visual_prompt]: 	Training 500/553. train loss: 22.7769,	0.9272 s / batch. (data: 1.06e-01). ETA=12:07:11, max mem: 20.9 GB 
[11/26 09:08:53 visual_prompt]: Epoch 15 / 100: avg data time: 1.48e-01, avg batch time: 0.9796, average train loss: 30.8940
[11/26 09:09:49 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3117, average loss: 51.1618
[11/26 09:09:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.25	
[11/26 09:09:49 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/26 09:11:29 visual_prompt]: 	Training 100/553. train loss: 29.5155,	0.8205 s / batch. (data: 2.83e-04). ETA=10:41:25, max mem: 20.9 GB 
[11/26 09:13:06 visual_prompt]: 	Training 200/553. train loss: 37.1712,	0.8351 s / batch. (data: 1.05e-02). ETA=10:51:25, max mem: 20.9 GB 
[11/26 09:14:44 visual_prompt]: 	Training 300/553. train loss: 95.8599,	0.8500 s / batch. (data: 5.45e-03). ETA=11:01:37, max mem: 20.9 GB 
[11/26 09:16:21 visual_prompt]: 	Training 400/553. train loss: 46.6106,	0.8360 s / batch. (data: 7.71e-04). ETA=10:49:22, max mem: 20.9 GB 
[11/26 09:17:58 visual_prompt]: 	Training 500/553. train loss: 5.6730,	0.8560 s / batch. (data: 3.13e-04). ETA=11:03:28, max mem: 20.9 GB 
[11/26 09:18:49 visual_prompt]: Epoch 16 / 100: avg data time: 1.45e-01, avg batch time: 0.9771, average train loss: 30.0371
[11/26 09:19:45 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3111, average loss: 16.7110
[11/26 09:19:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.45	
[11/26 09:19:45 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/26 09:21:25 visual_prompt]: 	Training 100/553. train loss: 0.9629,	0.8480 s / batch. (data: 1.19e-02). ETA=10:55:05, max mem: 20.9 GB 
[11/26 09:23:03 visual_prompt]: 	Training 200/553. train loss: 1.0105,	0.8210 s / batch. (data: 2.60e-04). ETA=10:32:53, max mem: 20.9 GB 
[11/26 09:24:40 visual_prompt]: 	Training 300/553. train loss: 31.9760,	0.8523 s / batch. (data: 3.33e-02). ETA=10:55:33, max mem: 20.9 GB 
[11/26 09:26:17 visual_prompt]: 	Training 400/553. train loss: 60.9852,	1.0617 s / batch. (data: 2.45e-01). ETA=13:34:55, max mem: 20.9 GB 
[11/26 09:27:54 visual_prompt]: 	Training 500/553. train loss: 10.4717,	1.3720 s / batch. (data: 5.46e-01). ETA=17:30:45, max mem: 20.9 GB 
[11/26 09:28:46 visual_prompt]: Epoch 17 / 100: avg data time: 1.47e-01, avg batch time: 0.9783, average train loss: 32.3750
[11/26 09:29:41 visual_prompt]: Inference (val):avg data time: 3.03e-04, avg batch time: 0.3100, average loss: 26.2978
[11/26 09:29:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.64	
[11/26 09:29:41 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/26 09:31:23 visual_prompt]: 	Training 100/553. train loss: 37.6888,	0.8368 s / batch. (data: 2.46e-02). ETA=10:38:46, max mem: 20.9 GB 
[11/26 09:33:03 visual_prompt]: 	Training 200/553. train loss: 47.4924,	0.8556 s / batch. (data: 2.63e-03). ETA=10:51:37, max mem: 20.9 GB 
[11/26 09:34:40 visual_prompt]: 	Training 300/553. train loss: 42.6714,	0.8354 s / batch. (data: 3.09e-04). ETA=10:34:51, max mem: 20.9 GB 
[11/26 09:36:18 visual_prompt]: 	Training 400/553. train loss: 5.9467,	0.8288 s / batch. (data: 1.10e-02). ETA=10:28:27, max mem: 20.9 GB 
[11/26 09:37:54 visual_prompt]: 	Training 500/553. train loss: 25.6926,	0.8234 s / batch. (data: 2.96e-04). ETA=10:23:00, max mem: 20.9 GB 
[11/26 09:38:44 visual_prompt]: Epoch 18 / 100: avg data time: 1.52e-01, avg batch time: 0.9816, average train loss: 33.0730
[11/26 09:39:40 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3097, average loss: 18.0114
[11/26 09:39:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.93	
[11/26 09:39:40 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/26 09:41:21 visual_prompt]: 	Training 100/553. train loss: 6.7463,	1.3602 s / batch. (data: 5.43e-01). ETA=17:05:44, max mem: 20.9 GB 
[11/26 09:43:00 visual_prompt]: 	Training 200/553. train loss: 18.9321,	0.8399 s / batch. (data: 7.95e-03). ETA=10:31:59, max mem: 20.9 GB 
[11/26 09:44:38 visual_prompt]: 	Training 300/553. train loss: 18.4357,	0.8284 s / batch. (data: 7.92e-03). ETA=10:21:55, max mem: 20.9 GB 
[11/26 09:46:17 visual_prompt]: 	Training 400/553. train loss: 20.8769,	0.8162 s / batch. (data: 3.48e-04). ETA=10:11:22, max mem: 20.9 GB 
[11/26 09:47:50 visual_prompt]: 	Training 500/553. train loss: 38.7176,	0.8696 s / batch. (data: 2.96e-02). ETA=10:49:57, max mem: 20.9 GB 
[11/26 09:48:41 visual_prompt]: Epoch 19 / 100: avg data time: 1.48e-01, avg batch time: 0.9787, average train loss: 33.2714
[11/26 09:49:37 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3114, average loss: 45.4579
[11/26 09:49:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.63	
[11/26 09:49:37 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/26 09:51:17 visual_prompt]: 	Training 100/553. train loss: 12.9582,	0.8315 s / batch. (data: 7.94e-03). ETA=10:19:22, max mem: 20.9 GB 
[11/26 09:52:55 visual_prompt]: 	Training 200/553. train loss: 39.6115,	0.8400 s / batch. (data: 3.17e-04). ETA=10:24:18, max mem: 20.9 GB 
[11/26 09:54:33 visual_prompt]: 	Training 300/553. train loss: 121.8446,	0.8382 s / batch. (data: 6.72e-04). ETA=10:21:35, max mem: 20.9 GB 
[11/26 09:56:10 visual_prompt]: 	Training 400/553. train loss: 47.2035,	0.8360 s / batch. (data: 7.95e-03). ETA=10:18:31, max mem: 20.9 GB 
[11/26 09:57:46 visual_prompt]: 	Training 500/553. train loss: 39.6276,	0.8321 s / batch. (data: 3.17e-04). ETA=10:14:17, max mem: 20.9 GB 
[11/26 09:58:39 visual_prompt]: Epoch 20 / 100: avg data time: 1.50e-01, avg batch time: 0.9805, average train loss: 35.4635
[11/26 09:59:35 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3094, average loss: 42.7701
[11/26 09:59:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.73	
[11/26 09:59:35 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/26 10:01:19 visual_prompt]: 	Training 100/553. train loss: 47.9980,	0.8340 s / batch. (data: 3.27e-04). ETA=10:13:33, max mem: 20.9 GB 
[11/26 10:02:57 visual_prompt]: 	Training 200/553. train loss: 94.1435,	0.8240 s / batch. (data: 2.97e-04). ETA=10:04:48, max mem: 20.9 GB 
[11/26 10:04:34 visual_prompt]: 	Training 300/553. train loss: 185.8542,	0.8440 s / batch. (data: 1.59e-02). ETA=10:18:04, max mem: 20.9 GB 
[11/26 10:06:12 visual_prompt]: 	Training 400/553. train loss: 1.6639,	0.8181 s / batch. (data: 3.03e-04). ETA=9:57:43, max mem: 20.9 GB 
[11/26 10:07:58 visual_prompt]: 	Training 500/553. train loss: 20.4397,	0.8316 s / batch. (data: 2.89e-04). ETA=10:06:15, max mem: 20.9 GB 
[11/26 10:08:50 visual_prompt]: Epoch 21 / 100: avg data time: 1.73e-01, avg batch time: 1.0029, average train loss: 36.2581
[11/26 10:09:52 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3110, average loss: 52.4051
[11/26 10:09:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.97	
[11/26 10:09:52 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/26 10:11:35 visual_prompt]: 	Training 100/553. train loss: 27.0023,	0.8253 s / batch. (data: 2.87e-04). ETA=9:59:30, max mem: 20.9 GB 
[11/26 10:13:14 visual_prompt]: 	Training 200/553. train loss: 39.1766,	0.8590 s / batch. (data: 2.29e-02). ETA=10:22:37, max mem: 20.9 GB 
[11/26 10:14:51 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8440 s / batch. (data: 2.97e-04). ETA=10:10:18, max mem: 20.9 GB 
[11/26 10:16:31 visual_prompt]: 	Training 400/553. train loss: 20.2337,	0.8280 s / batch. (data: 3.00e-04). ETA=9:57:20, max mem: 20.9 GB 
[11/26 10:18:10 visual_prompt]: 	Training 500/553. train loss: 19.9813,	0.8338 s / batch. (data: 3.08e-04). ETA=10:00:08, max mem: 20.9 GB 
[11/26 10:19:03 visual_prompt]: Epoch 22 / 100: avg data time: 1.66e-01, avg batch time: 0.9966, average train loss: 32.4458
[11/26 10:19:58 visual_prompt]: Inference (val):avg data time: 1.90e-04, avg batch time: 0.3094, average loss: 15.4074
[11/26 10:19:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.64	
[11/26 10:19:58 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/26 10:21:42 visual_prompt]: 	Training 100/553. train loss: 26.0183,	0.8434 s / batch. (data: 7.95e-03). ETA=10:04:56, max mem: 20.9 GB 
[11/26 10:23:20 visual_prompt]: 	Training 200/553. train loss: 13.7293,	0.8440 s / batch. (data: 7.95e-03). ETA=10:03:55, max mem: 20.9 GB 
[11/26 10:25:00 visual_prompt]: 	Training 300/553. train loss: 21.7702,	0.8364 s / batch. (data: 3.55e-04). ETA=9:57:06, max mem: 20.9 GB 
[11/26 10:26:35 visual_prompt]: 	Training 400/553. train loss: 6.2527,	0.8491 s / batch. (data: 5.43e-03). ETA=10:04:43, max mem: 20.9 GB 
[11/26 10:28:11 visual_prompt]: 	Training 500/553. train loss: 15.2459,	0.8280 s / batch. (data: 5.43e-03). ETA=9:48:19, max mem: 20.9 GB 
[11/26 10:29:02 visual_prompt]: Epoch 23 / 100: avg data time: 1.52e-01, avg batch time: 0.9830, average train loss: 34.6301
[11/26 10:29:58 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3100, average loss: 54.3078
[11/26 10:29:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.92	
[11/26 10:29:58 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/26 10:31:36 visual_prompt]: 	Training 100/553. train loss: 3.8505,	0.8400 s / batch. (data: 3.39e-04). ETA=9:54:43, max mem: 20.9 GB 
[11/26 10:33:14 visual_prompt]: 	Training 200/553. train loss: 44.8869,	0.8463 s / batch. (data: 1.05e-02). ETA=9:57:47, max mem: 20.9 GB 
[11/26 10:34:52 visual_prompt]: 	Training 300/553. train loss: 32.5267,	0.8429 s / batch. (data: 8.59e-03). ETA=9:53:57, max mem: 20.9 GB 
[11/26 10:36:29 visual_prompt]: 	Training 400/553. train loss: 2.1398,	0.8480 s / batch. (data: 3.18e-04). ETA=9:56:08, max mem: 20.9 GB 
[11/26 10:38:08 visual_prompt]: 	Training 500/553. train loss: 1.0928,	0.8522 s / batch. (data: 8.13e-03). ETA=9:57:39, max mem: 20.9 GB 
[11/26 10:38:59 visual_prompt]: Epoch 24 / 100: avg data time: 1.47e-01, avg batch time: 0.9783, average train loss: 31.5102
[11/26 10:39:54 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3095, average loss: 17.1132
[11/26 10:39:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.29	
[11/26 10:39:54 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/26 10:41:39 visual_prompt]: 	Training 100/553. train loss: 16.7239,	0.8435 s / batch. (data: 2.35e-02). ETA=9:49:26, max mem: 20.9 GB 
[11/26 10:43:13 visual_prompt]: 	Training 200/553. train loss: 9.5156,	0.8520 s / batch. (data: 3.29e-04). ETA=9:53:56, max mem: 20.9 GB 
[11/26 10:44:51 visual_prompt]: 	Training 300/553. train loss: 21.6935,	0.8289 s / batch. (data: 3.11e-04). ETA=9:36:29, max mem: 20.9 GB 
[11/26 10:46:28 visual_prompt]: 	Training 400/553. train loss: 3.4372,	1.2160 s / batch. (data: 3.67e-01). ETA=14:03:38, max mem: 20.9 GB 
[11/26 10:48:06 visual_prompt]: 	Training 500/553. train loss: 18.2334,	1.2782 s / batch. (data: 4.58e-01). ETA=14:44:39, max mem: 20.9 GB 
[11/26 10:48:57 visual_prompt]: Epoch 25 / 100: avg data time: 1.51e-01, avg batch time: 0.9817, average train loss: 34.4249
[11/26 10:49:53 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3107, average loss: 63.0027
[11/26 10:49:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.89	
[11/26 10:49:53 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/26 10:51:34 visual_prompt]: 	Training 100/553. train loss: 72.3546,	0.8320 s / batch. (data: 3.02e-04). ETA=9:33:43, max mem: 20.9 GB 
[11/26 10:53:13 visual_prompt]: 	Training 200/553. train loss: 101.9321,	1.4494 s / batch. (data: 6.33e-01). ETA=16:37:02, max mem: 20.9 GB 
[11/26 10:54:52 visual_prompt]: 	Training 300/553. train loss: 27.4071,	0.8360 s / batch. (data: 3.27e-04). ETA=9:33:42, max mem: 20.9 GB 
[11/26 10:56:29 visual_prompt]: 	Training 400/553. train loss: 30.2635,	0.8062 s / batch. (data: 3.11e-04). ETA=9:11:54, max mem: 20.9 GB 
[11/26 10:58:05 visual_prompt]: 	Training 500/553. train loss: 83.6241,	0.8352 s / batch. (data: 3.30e-04). ETA=9:30:23, max mem: 20.9 GB 
[11/26 10:58:56 visual_prompt]: Epoch 26 / 100: avg data time: 1.52e-01, avg batch time: 0.9829, average train loss: 31.8418
[11/26 10:59:52 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3109, average loss: 21.1550
[11/26 10:59:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.35	
[11/26 10:59:52 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[11/26 11:01:36 visual_prompt]: 	Training 100/553. train loss: 0.4042,	0.8194 s / batch. (data: 4.46e-04). ETA=9:17:28, max mem: 20.9 GB 
[11/26 11:03:13 visual_prompt]: 	Training 200/553. train loss: 3.8618,	0.9561 s / batch. (data: 1.33e-01). ETA=10:48:54, max mem: 20.9 GB 
[11/26 11:04:49 visual_prompt]: 	Training 300/553. train loss: 61.9464,	0.8360 s / batch. (data: 7.96e-03). ETA=9:26:00, max mem: 20.9 GB 
[11/26 11:06:28 visual_prompt]: 	Training 400/553. train loss: 5.1087,	0.8467 s / batch. (data: 7.84e-04). ETA=9:31:48, max mem: 20.9 GB 
[11/26 11:08:06 visual_prompt]: 	Training 500/553. train loss: 4.4242,	0.8400 s / batch. (data: 3.14e-04). ETA=9:25:55, max mem: 20.9 GB 
[11/26 11:08:55 visual_prompt]: Epoch 27 / 100: avg data time: 1.51e-01, avg batch time: 0.9815, average train loss: 28.6694
[11/26 11:09:51 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3105, average loss: 20.8540
[11/26 11:09:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[11/26 11:09:51 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[11/26 11:11:31 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8119 s / batch. (data: 7.86e-03). ETA=9:04:53, max mem: 20.9 GB 
[11/26 11:13:09 visual_prompt]: 	Training 200/553. train loss: 2.3331,	0.8288 s / batch. (data: 9.61e-03). ETA=9:14:52, max mem: 20.9 GB 
[11/26 11:14:48 visual_prompt]: 	Training 300/553. train loss: 10.2766,	1.5724 s / batch. (data: 7.53e-01). ETA=17:30:05, max mem: 20.9 GB 
[11/26 11:16:25 visual_prompt]: 	Training 400/553. train loss: 74.0288,	0.8480 s / batch. (data: 2.11e-02). ETA=9:24:51, max mem: 20.9 GB 
[11/26 11:18:01 visual_prompt]: 	Training 500/553. train loss: 41.2319,	0.8520 s / batch. (data: 2.97e-04). ETA=9:26:09, max mem: 20.9 GB 
[11/26 11:18:53 visual_prompt]: Epoch 28 / 100: avg data time: 1.49e-01, avg batch time: 0.9810, average train loss: 27.4799
[11/26 11:19:49 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3114, average loss: 42.9842
[11/26 11:19:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.68	
[11/26 11:19:49 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[11/26 11:21:37 visual_prompt]: 	Training 100/553. train loss: 0.1680,	0.8263 s / batch. (data: 3.10e-04). ETA=9:06:56, max mem: 20.9 GB 
[11/26 11:23:13 visual_prompt]: 	Training 200/553. train loss: 58.2281,	1.6760 s / batch. (data: 8.53e-01). ETA=18:26:36, max mem: 20.9 GB 
[11/26 11:24:50 visual_prompt]: 	Training 300/553. train loss: 37.3546,	0.8251 s / batch. (data: 3.27e-04). ETA=9:03:23, max mem: 20.9 GB 
[11/26 11:26:24 visual_prompt]: 	Training 400/553. train loss: 41.1207,	1.2840 s / batch. (data: 4.60e-01). ETA=14:03:29, max mem: 20.9 GB 
[11/26 11:28:01 visual_prompt]: 	Training 500/553. train loss: 12.3421,	0.8199 s / batch. (data: 3.08e-04). ETA=8:57:14, max mem: 20.9 GB 
[11/26 11:28:52 visual_prompt]: Epoch 29 / 100: avg data time: 1.49e-01, avg batch time: 0.9817, average train loss: 26.5714
[11/26 11:29:48 visual_prompt]: Inference (val):avg data time: 1.53e-04, avg batch time: 0.3098, average loss: 48.8234
[11/26 11:29:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.66	
[11/26 11:29:48 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[11/26 11:31:28 visual_prompt]: 	Training 100/553. train loss: 20.2464,	0.8539 s / batch. (data: 2.18e-02). ETA=9:17:20, max mem: 20.9 GB 
[11/26 11:33:06 visual_prompt]: 	Training 200/553. train loss: 11.1758,	0.8200 s / batch. (data: 3.27e-04). ETA=8:53:50, max mem: 20.9 GB 
[11/26 11:34:42 visual_prompt]: 	Training 300/553. train loss: 9.4657,	0.8348 s / batch. (data: 3.24e-04). ETA=9:02:07, max mem: 20.9 GB 
[11/26 11:36:21 visual_prompt]: 	Training 400/553. train loss: 10.0146,	1.0520 s / batch. (data: 2.12e-01). ETA=11:21:23, max mem: 20.9 GB 
[11/26 11:37:57 visual_prompt]: 	Training 500/553. train loss: 17.4269,	1.4204 s / batch. (data: 5.92e-01). ETA=15:17:38, max mem: 20.9 GB 
[11/26 11:38:50 visual_prompt]: Epoch 30 / 100: avg data time: 1.49e-01, avg batch time: 0.9802, average train loss: 26.5943
[11/26 11:39:45 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3101, average loss: 28.5451
[11/26 11:39:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.21	
[11/26 11:39:45 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[11/26 11:41:28 visual_prompt]: 	Training 100/553. train loss: 17.1579,	0.8400 s / batch. (data: 3.04e-04). ETA=9:00:31, max mem: 20.9 GB 
[11/26 11:43:08 visual_prompt]: 	Training 200/553. train loss: 26.0682,	0.8399 s / batch. (data: 7.95e-03). ETA=8:59:03, max mem: 20.9 GB 
[11/26 11:44:43 visual_prompt]: 	Training 300/553. train loss: 37.3984,	0.8440 s / batch. (data: 3.28e-04). ETA=9:00:17, max mem: 20.9 GB 
[11/26 11:46:19 visual_prompt]: 	Training 400/553. train loss: 6.7291,	0.8802 s / batch. (data: 5.05e-02). ETA=9:22:00, max mem: 20.9 GB 
[11/26 11:47:57 visual_prompt]: 	Training 500/553. train loss: 1.2326,	0.8519 s / batch. (data: 1.90e-02). ETA=9:02:29, max mem: 20.9 GB 
[11/26 11:48:48 visual_prompt]: Epoch 31 / 100: avg data time: 1.48e-01, avg batch time: 0.9804, average train loss: 29.0759
[11/26 11:49:44 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3103, average loss: 20.5089
[11/26 11:49:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.16	
[11/26 11:49:44 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[11/26 11:51:27 visual_prompt]: 	Training 100/553. train loss: 11.9108,	0.8560 s / batch. (data: 9.79e-04). ETA=9:02:57, max mem: 20.9 GB 
[11/26 11:53:05 visual_prompt]: 	Training 200/553. train loss: 70.7574,	0.8358 s / batch. (data: 1.18e-02). ETA=8:48:45, max mem: 20.9 GB 
[11/26 11:54:45 visual_prompt]: 	Training 300/553. train loss: 65.8313,	0.8277 s / batch. (data: 5.40e-03). ETA=8:42:15, max mem: 20.9 GB 
[11/26 11:56:23 visual_prompt]: 	Training 400/553. train loss: 4.8384,	0.8469 s / batch. (data: 3.46e-04). ETA=8:52:55, max mem: 20.9 GB 
[11/26 11:57:59 visual_prompt]: 	Training 500/553. train loss: 11.8337,	0.8440 s / batch. (data: 7.97e-03). ETA=8:49:42, max mem: 20.9 GB 
[11/26 11:58:48 visual_prompt]: Epoch 32 / 100: avg data time: 1.52e-01, avg batch time: 0.9837, average train loss: 29.1044
[11/26 11:59:43 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3097, average loss: 59.6293
[11/26 11:59:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.32	
[11/26 11:59:43 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[11/26 12:01:25 visual_prompt]: 	Training 100/553. train loss: 29.7008,	0.8200 s / batch. (data: 2.92e-04). ETA=8:32:33, max mem: 20.9 GB 
[11/26 12:03:04 visual_prompt]: 	Training 200/553. train loss: 10.7101,	1.3759 s / batch. (data: 5.32e-01). ETA=14:17:43, max mem: 20.9 GB 
[11/26 12:04:40 visual_prompt]: 	Training 300/553. train loss: 14.8973,	0.8466 s / batch. (data: 5.45e-03). ETA=8:46:19, max mem: 20.9 GB 
[11/26 12:06:19 visual_prompt]: 	Training 400/553. train loss: 29.4231,	0.8290 s / batch. (data: 8.99e-03). ETA=8:34:03, max mem: 20.9 GB 
[11/26 12:07:56 visual_prompt]: 	Training 500/553. train loss: 3.8533,	0.8289 s / batch. (data: 3.29e-04). ETA=8:32:36, max mem: 20.9 GB 
[11/26 12:08:46 visual_prompt]: Epoch 33 / 100: avg data time: 1.49e-01, avg batch time: 0.9817, average train loss: 30.7971
[11/26 12:09:42 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3091, average loss: 12.2733
[11/26 12:09:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.84	
[11/26 12:09:42 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[11/26 12:11:24 visual_prompt]: 	Training 100/553. train loss: 14.1192,	0.8477 s / batch. (data: 1.05e-02). ETA=8:42:03, max mem: 20.9 GB 
[11/26 12:13:00 visual_prompt]: 	Training 200/553. train loss: 21.7265,	0.8520 s / batch. (data: 1.20e-02). ETA=8:43:16, max mem: 20.9 GB 
[11/26 12:14:36 visual_prompt]: 	Training 300/553. train loss: 30.0338,	0.8600 s / batch. (data: 3.30e-04). ETA=8:46:45, max mem: 20.9 GB 
[11/26 12:16:16 visual_prompt]: 	Training 400/553. train loss: 22.0481,	0.8280 s / batch. (data: 2.97e-04). ETA=8:25:46, max mem: 20.9 GB 
[11/26 12:17:53 visual_prompt]: 	Training 500/553. train loss: 1.1315,	1.2769 s / batch. (data: 4.25e-01). ETA=12:57:50, max mem: 20.9 GB 
[11/26 12:18:43 visual_prompt]: Epoch 34 / 100: avg data time: 1.46e-01, avg batch time: 0.9786, average train loss: 27.4343
[11/26 12:19:39 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3126, average loss: 11.8197
[11/26 12:19:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.98	
[11/26 12:19:39 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[11/26 12:21:23 visual_prompt]: 	Training 100/553. train loss: 14.2905,	0.8356 s / batch. (data: 2.78e-04). ETA=8:26:54, max mem: 20.9 GB 
[11/26 12:23:00 visual_prompt]: 	Training 200/553. train loss: 29.9897,	0.8190 s / batch. (data: 3.52e-04). ETA=8:15:27, max mem: 20.9 GB 
[11/26 12:24:38 visual_prompt]: 	Training 300/553. train loss: 3.0011,	0.8330 s / batch. (data: 2.93e-04). ETA=8:22:31, max mem: 20.9 GB 
[11/26 12:26:14 visual_prompt]: 	Training 400/553. train loss: 0.7721,	0.8392 s / batch. (data: 3.12e-04). ETA=8:24:52, max mem: 20.9 GB 
[11/26 12:27:50 visual_prompt]: 	Training 500/553. train loss: 12.0348,	1.0171 s / batch. (data: 1.99e-01). ETA=10:10:12, max mem: 20.9 GB 
[11/26 12:28:42 visual_prompt]: Epoch 35 / 100: avg data time: 1.49e-01, avg batch time: 0.9807, average train loss: 30.2824
[11/26 12:29:38 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3092, average loss: 96.0170
[11/26 12:29:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.43	
[11/26 12:29:38 visual_prompt]: Stopping early.
[11/26 12:29:38 visual_prompt]: Rank of current process: 0. World size: 1
[11/26 12:29:38 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/26 12:29:38 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/26 12:29:38 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/26 12:29:38 visual_prompt]: Training with config:
[11/26 12:29:38 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/26 12:29:38 visual_prompt]: Loading training data...
[11/26 12:29:38 visual_prompt]: Constructing mammo-cbis dataset train...
[11/26 12:29:38 visual_prompt]: Loading validation data...
[11/26 12:29:38 visual_prompt]: Constructing mammo-cbis dataset val...
[11/26 12:29:38 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/26 12:29:46 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/26 12:29:46 visual_prompt]: tuned percent:0.525
[11/26 12:29:46 visual_prompt]: Device used for model: 0
[11/26 12:29:46 visual_prompt]: Setting up Evaluator...
[11/26 12:29:46 visual_prompt]: Setting up Trainer...
[11/26 12:29:46 visual_prompt]: 	Setting up the optimizer...
[11/26 12:29:46 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/26 12:31:27 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8600 s / batch. (data: 2.90e-04). ETA=13:11:12, max mem: 20.9 GB 
[11/26 12:33:04 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8564 s / batch. (data: 1.55e-02). ETA=13:06:30, max mem: 20.9 GB 
[11/26 12:34:44 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8427 s / batch. (data: 1.05e-02). ETA=12:52:27, max mem: 20.9 GB 
[11/26 12:36:20 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8440 s / batch. (data: 3.02e-04). ETA=12:52:14, max mem: 20.9 GB 
[11/26 12:38:00 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8183 s / batch. (data: 2.97e-04). ETA=12:27:23, max mem: 20.9 GB 
[11/26 12:38:51 visual_prompt]: Epoch 1 / 100: avg data time: 1.51e-01, avg batch time: 0.9847, average train loss: 1.5403
[11/26 12:39:47 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3101, average loss: 1.5201
[11/26 12:39:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/26 12:39:47 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/26 12:41:27 visual_prompt]: 	Training 100/553. train loss: 10.4643,	0.8795 s / batch. (data: 3.55e-02). ETA=13:21:04, max mem: 20.9 GB 
[11/26 12:43:04 visual_prompt]: 	Training 200/553. train loss: 0.0010,	0.8400 s / batch. (data: 3.26e-04). ETA=12:43:39, max mem: 20.9 GB 
[11/26 12:44:44 visual_prompt]: 	Training 300/553. train loss: 5.4646,	1.0904 s / batch. (data: 2.48e-01). ETA=16:29:27, max mem: 20.9 GB 
[11/26 12:46:20 visual_prompt]: 	Training 400/553. train loss: 5.3334,	0.8600 s / batch. (data: 7.96e-03). ETA=12:58:58, max mem: 20.9 GB 
[11/26 12:48:00 visual_prompt]: 	Training 500/553. train loss: 9.4859,	0.8533 s / batch. (data: 3.00e-04). ETA=12:51:30, max mem: 20.9 GB 
[11/26 12:48:50 visual_prompt]: Epoch 2 / 100: avg data time: 1.49e-01, avg batch time: 0.9827, average train loss: 4.9669
[11/26 12:49:46 visual_prompt]: Inference (val):avg data time: 4.16e-04, avg batch time: 0.3088, average loss: 17.1788
[11/26 12:49:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.88	
[11/26 12:49:46 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/26 12:51:27 visual_prompt]: 	Training 100/553. train loss: 5.1646,	0.8439 s / batch. (data: 5.64e-03). ETA=12:40:49, max mem: 20.9 GB 
[11/26 12:53:06 visual_prompt]: 	Training 200/553. train loss: 2.0174,	0.8185 s / batch. (data: 2.93e-04). ETA=12:16:32, max mem: 20.9 GB 
[11/26 12:54:43 visual_prompt]: 	Training 300/553. train loss: 3.7236,	0.8409 s / batch. (data: 5.52e-03). ETA=12:35:19, max mem: 20.9 GB 
[11/26 12:56:21 visual_prompt]: 	Training 400/553. train loss: 2.2168,	0.8280 s / batch. (data: 3.14e-04). ETA=12:22:20, max mem: 20.9 GB 
[11/26 12:58:00 visual_prompt]: 	Training 500/553. train loss: 6.8552,	1.1364 s / batch. (data: 3.10e-01). ETA=16:56:59, max mem: 20.9 GB 
[11/26 12:58:49 visual_prompt]: Epoch 3 / 100: avg data time: 1.49e-01, avg batch time: 0.9821, average train loss: 5.8513
[11/26 12:59:45 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3110, average loss: 14.7225
[11/26 12:59:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.89	
[11/26 12:59:45 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/26 13:01:28 visual_prompt]: 	Training 100/553. train loss: 0.7517,	0.8411 s / batch. (data: 1.05e-02). ETA=12:30:34, max mem: 20.9 GB 
[11/26 13:03:06 visual_prompt]: 	Training 200/553. train loss: 6.1720,	0.8240 s / batch. (data: 3.08e-04). ETA=12:13:55, max mem: 20.9 GB 
[11/26 13:04:43 visual_prompt]: 	Training 300/553. train loss: 2.6758,	1.2040 s / batch. (data: 3.86e-01). ETA=17:50:21, max mem: 20.9 GB 
[11/26 13:06:18 visual_prompt]: 	Training 400/553. train loss: 13.2228,	1.3613 s / batch. (data: 5.06e-01). ETA=20:07:54, max mem: 20.9 GB 
[11/26 13:07:57 visual_prompt]: 	Training 500/553. train loss: 0.6358,	3.3360 s / batch. (data: 2.49e+00). ETA=2 days, 1:14:39, max mem: 20.9 GB 
[11/26 13:08:49 visual_prompt]: Epoch 4 / 100: avg data time: 1.50e-01, avg batch time: 0.9828, average train loss: 10.6225
[11/26 13:09:46 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3104, average loss: 5.6601
[11/26 13:09:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.18	
[11/26 13:09:46 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/26 13:11:26 visual_prompt]: 	Training 100/553. train loss: 28.5359,	0.8449 s / batch. (data: 5.43e-03). ETA=12:26:08, max mem: 20.9 GB 
[11/26 13:13:04 visual_prompt]: 	Training 200/553. train loss: 2.2963,	0.9838 s / batch. (data: 1.63e-01). ETA=14:27:08, max mem: 20.9 GB 
[11/26 13:14:43 visual_prompt]: 	Training 300/553. train loss: 5.5521,	0.8194 s / batch. (data: 3.00e-04). ETA=12:00:55, max mem: 20.9 GB 
[11/26 13:16:19 visual_prompt]: 	Training 400/553. train loss: 3.1498,	0.8323 s / batch. (data: 3.04e-04). ETA=12:10:53, max mem: 20.9 GB 
[11/26 13:17:57 visual_prompt]: 	Training 500/553. train loss: 14.7823,	0.8480 s / batch. (data: 7.92e-03). ETA=12:23:13, max mem: 20.9 GB 
[11/26 13:18:49 visual_prompt]: Epoch 5 / 100: avg data time: 1.48e-01, avg batch time: 0.9819, average train loss: 11.0127
[11/26 13:19:44 visual_prompt]: Inference (val):avg data time: 2.31e-04, avg batch time: 0.3112, average loss: 19.4735
[11/26 13:19:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.91	
[11/26 13:19:44 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/26 13:21:27 visual_prompt]: 	Training 100/553. train loss: 5.0224,	0.8276 s / batch. (data: 3.62e-04). ETA=12:03:14, max mem: 20.9 GB 
[11/26 13:23:05 visual_prompt]: 	Training 200/553. train loss: 19.3753,	0.8582 s / batch. (data: 3.12e-04). ETA=12:28:33, max mem: 20.9 GB 
[11/26 13:24:40 visual_prompt]: 	Training 300/553. train loss: 3.0734,	0.8415 s / batch. (data: 3.18e-04). ETA=12:12:34, max mem: 20.9 GB 
[11/26 13:26:22 visual_prompt]: 	Training 400/553. train loss: 9.9597,	0.8359 s / batch. (data: 1.06e-02). ETA=12:06:19, max mem: 20.9 GB 
[11/26 13:27:59 visual_prompt]: 	Training 500/553. train loss: 16.4667,	0.8309 s / batch. (data: 5.44e-03). ETA=12:00:34, max mem: 20.9 GB 
[11/26 13:28:49 visual_prompt]: Epoch 6 / 100: avg data time: 1.50e-01, avg batch time: 0.9842, average train loss: 10.0695
[11/26 13:29:45 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3124, average loss: 8.6839
[11/26 13:29:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.73	
[11/26 13:29:45 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/26 13:31:25 visual_prompt]: 	Training 100/553. train loss: 8.0542,	0.8437 s / batch. (data: 2.36e-02). ETA=12:09:30, max mem: 20.9 GB 
[11/26 13:33:03 visual_prompt]: 	Training 200/553. train loss: 2.1701,	0.8597 s / batch. (data: 2.76e-02). ETA=12:21:56, max mem: 20.9 GB 
[11/26 13:34:44 visual_prompt]: 	Training 300/553. train loss: 27.3659,	1.9106 s / batch. (data: 1.09e+00). ETA=1 day, 3:25:42, max mem: 20.9 GB 
[11/26 13:36:21 visual_prompt]: 	Training 400/553. train loss: 0.6175,	1.5307 s / batch. (data: 7.00e-01). ETA=21:55:57, max mem: 20.9 GB 
[11/26 13:37:57 visual_prompt]: 	Training 500/553. train loss: 5.7167,	0.8440 s / batch. (data: 3.45e-04). ETA=12:04:10, max mem: 20.9 GB 
[11/26 13:38:47 visual_prompt]: Epoch 7 / 100: avg data time: 1.48e-01, avg batch time: 0.9809, average train loss: 12.7131
[11/26 13:39:43 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3094, average loss: 3.9841
[11/26 13:39:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.10	
[11/26 13:39:43 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/26 13:41:24 visual_prompt]: 	Training 100/553. train loss: 7.3276,	0.8692 s / batch. (data: 5.44e-03). ETA=12:23:34, max mem: 20.9 GB 
[11/26 13:43:03 visual_prompt]: 	Training 200/553. train loss: 51.3404,	0.8418 s / batch. (data: 4.84e-04). ETA=11:58:45, max mem: 20.9 GB 
[11/26 13:44:41 visual_prompt]: 	Training 300/553. train loss: 0.8880,	0.8242 s / batch. (data: 5.44e-03). ETA=11:42:18, max mem: 20.9 GB 
[11/26 13:46:19 visual_prompt]: 	Training 400/553. train loss: 19.9931,	0.8508 s / batch. (data: 5.46e-03). ETA=12:03:33, max mem: 20.9 GB 
[11/26 13:47:57 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.4277 s / batch. (data: 5.70e-01). ETA=20:11:52, max mem: 20.9 GB 
[11/26 13:48:49 visual_prompt]: Epoch 8 / 100: avg data time: 1.54e-01, avg batch time: 0.9862, average train loss: 25.0387
[11/26 13:49:44 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3092, average loss: 11.9663
[11/26 13:49:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.23	
[11/26 13:49:44 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/26 13:51:25 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8267 s / batch. (data: 3.01e-04). ETA=11:39:34, max mem: 20.9 GB 
[11/26 13:53:02 visual_prompt]: 	Training 200/553. train loss: 18.9466,	0.8250 s / batch. (data: 3.11e-04). ETA=11:36:48, max mem: 20.9 GB 
[11/26 13:54:40 visual_prompt]: 	Training 300/553. train loss: 1.4170,	1.5440 s / batch. (data: 7.00e-01). ETA=21:41:30, max mem: 20.9 GB 
[11/26 13:56:19 visual_prompt]: 	Training 400/553. train loss: 2.0662,	0.8445 s / batch. (data: 7.93e-04). ETA=11:50:25, max mem: 20.9 GB 
[11/26 13:57:57 visual_prompt]: 	Training 500/553. train loss: 12.9637,	0.9166 s / batch. (data: 7.76e-02). ETA=12:49:32, max mem: 20.9 GB 
[11/26 13:58:47 visual_prompt]: Epoch 9 / 100: avg data time: 1.49e-01, avg batch time: 0.9812, average train loss: 21.0203
[11/26 13:59:43 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3096, average loss: 80.6455
[11/26 13:59:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.99	
[11/26 13:59:43 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/26 14:01:27 visual_prompt]: 	Training 100/553. train loss: 54.4525,	0.8320 s / batch. (data: 3.12e-04). ETA=11:36:25, max mem: 20.9 GB 
[11/26 14:03:03 visual_prompt]: 	Training 200/553. train loss: 18.4336,	0.8389 s / batch. (data: 1.05e-02). ETA=11:40:49, max mem: 20.9 GB 
[11/26 14:04:39 visual_prompt]: 	Training 300/553. train loss: 29.5771,	1.0923 s / batch. (data: 2.65e-01). ETA=15:10:41, max mem: 20.9 GB 
[11/26 14:06:16 visual_prompt]: 	Training 400/553. train loss: 51.3899,	0.8567 s / batch. (data: 2.57e-02). ETA=11:52:49, max mem: 20.9 GB 
[11/26 14:07:55 visual_prompt]: 	Training 500/553. train loss: 3.2221,	0.8596 s / batch. (data: 1.16e-02). ETA=11:53:48, max mem: 20.9 GB 
[11/26 14:08:46 visual_prompt]: Epoch 10 / 100: avg data time: 1.51e-01, avg batch time: 0.9823, average train loss: 23.4597
[11/26 14:09:42 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3098, average loss: 35.3130
[11/26 14:09:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.92	
[11/26 14:09:42 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/26 14:11:26 visual_prompt]: 	Training 100/553. train loss: 49.0889,	0.8406 s / batch. (data: 2.46e-02). ETA=11:35:52, max mem: 20.9 GB 
[11/26 14:13:06 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8400 s / batch. (data: 1.19e-02). ETA=11:33:58, max mem: 20.9 GB 
[11/26 14:14:43 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.8108 s / batch. (data: 9.86e-01). ETA=1 day, 0:53:02, max mem: 20.9 GB 
[11/26 14:16:18 visual_prompt]: 	Training 400/553. train loss: 47.5715,	0.8260 s / batch. (data: 3.05e-04). ETA=11:19:38, max mem: 20.9 GB 
[11/26 14:17:55 visual_prompt]: 	Training 500/553. train loss: 90.2490,	0.8231 s / batch. (data: 1.13e-02). ETA=11:15:55, max mem: 20.9 GB 
[11/26 14:18:45 visual_prompt]: Epoch 11 / 100: avg data time: 1.52e-01, avg batch time: 0.9817, average train loss: 40.8844
[11/26 14:19:41 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3103, average loss: 19.9013
[11/26 14:19:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.81	
[11/26 14:19:41 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/26 14:21:24 visual_prompt]: 	Training 100/553. train loss: 33.3353,	0.8282 s / batch. (data: 3.52e-04). ETA=11:18:00, max mem: 20.9 GB 
[11/26 14:23:03 visual_prompt]: 	Training 200/553. train loss: 8.5810,	0.8384 s / batch. (data: 2.84e-04). ETA=11:24:53, max mem: 20.9 GB 
[11/26 14:24:39 visual_prompt]: 	Training 300/553. train loss: 37.9235,	0.8320 s / batch. (data: 3.12e-04). ETA=11:18:18, max mem: 20.9 GB 
[11/26 14:26:17 visual_prompt]: 	Training 400/553. train loss: 5.5246,	0.8318 s / batch. (data: 1.30e-03). ETA=11:16:43, max mem: 20.9 GB 
[11/26 14:27:54 visual_prompt]: 	Training 500/553. train loss: 361.9593,	0.8376 s / batch. (data: 7.96e-04). ETA=11:20:06, max mem: 20.9 GB 
[11/26 14:28:44 visual_prompt]: Epoch 12 / 100: avg data time: 1.51e-01, avg batch time: 0.9821, average train loss: 33.7319
[11/26 14:29:40 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3115, average loss: 6.8668
[11/26 14:29:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.22	
[11/26 14:29:40 visual_prompt]: Best epoch 12: best metric: -6.867
[11/26 14:29:40 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/26 14:31:23 visual_prompt]: 	Training 100/553. train loss: 23.7033,	0.8219 s / batch. (data: 2.93e-04). ETA=11:05:14, max mem: 20.9 GB 
[11/26 14:32:57 visual_prompt]: 	Training 200/553. train loss: 24.2083,	0.8460 s / batch. (data: 1.05e-02). ETA=11:23:19, max mem: 20.9 GB 
[11/26 14:34:36 visual_prompt]: 	Training 300/553. train loss: 11.9505,	1.6479 s / batch. (data: 8.00e-01). ETA=22:08:19, max mem: 20.9 GB 
[11/26 14:36:12 visual_prompt]: 	Training 400/553. train loss: 68.1326,	0.8245 s / batch. (data: 3.37e-04). ETA=11:03:15, max mem: 20.9 GB 
[11/26 14:37:51 visual_prompt]: 	Training 500/553. train loss: 43.9326,	0.8320 s / batch. (data: 4.21e-04). ETA=11:07:50, max mem: 20.9 GB 
[11/26 14:38:41 visual_prompt]: Epoch 13 / 100: avg data time: 1.48e-01, avg batch time: 0.9788, average train loss: 31.1751
[11/26 14:39:37 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3093, average loss: 9.2663
[11/26 14:39:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.03	
[11/26 14:39:37 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/26 14:41:20 visual_prompt]: 	Training 100/553. train loss: 9.2130,	0.8374 s / batch. (data: 1.05e-02). ETA=11:10:02, max mem: 20.9 GB 
[11/26 14:42:57 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.0240 s / batch. (data: 1.81e-01). ETA=13:37:39, max mem: 20.9 GB 
[11/26 14:44:35 visual_prompt]: 	Training 300/553. train loss: 19.4283,	0.8323 s / batch. (data: 3.26e-04). ETA=11:03:12, max mem: 20.9 GB 
[11/26 14:46:12 visual_prompt]: 	Training 400/553. train loss: 13.2197,	0.8360 s / batch. (data: 3.00e-04). ETA=11:04:45, max mem: 20.9 GB 
[11/26 14:47:49 visual_prompt]: 	Training 500/553. train loss: 6.1606,	0.8555 s / batch. (data: 3.10e-02). ETA=11:18:50, max mem: 20.9 GB 
[11/26 14:48:39 visual_prompt]: Epoch 14 / 100: avg data time: 1.49e-01, avg batch time: 0.9802, average train loss: 30.6143
[11/26 14:49:35 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3096, average loss: 9.9501
[11/26 14:49:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.47	
[11/26 14:49:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/26 14:51:16 visual_prompt]: 	Training 100/553. train loss: 16.6267,	0.8462 s / batch. (data: 1.01e-02). ETA=11:09:21, max mem: 20.9 GB 
[11/26 14:52:52 visual_prompt]: 	Training 200/553. train loss: 84.0703,	0.8440 s / batch. (data: 4.18e-04). ETA=11:06:08, max mem: 20.9 GB 
[11/26 14:54:32 visual_prompt]: 	Training 300/553. train loss: 69.5441,	0.8559 s / batch. (data: 8.08e-04). ETA=11:14:09, max mem: 20.9 GB 
[11/26 14:56:07 visual_prompt]: 	Training 400/553. train loss: 11.5677,	1.0479 s / batch. (data: 1.95e-01). ETA=13:43:36, max mem: 20.9 GB 
[11/26 14:57:46 visual_prompt]: 	Training 500/553. train loss: 1.6814,	0.8238 s / batch. (data: 4.17e-04). ETA=10:46:04, max mem: 20.9 GB 
[11/26 14:58:37 visual_prompt]: Epoch 15 / 100: avg data time: 1.49e-01, avg batch time: 0.9809, average train loss: 28.5416
[11/26 14:59:33 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3094, average loss: 5.6738
[11/26 14:59:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.66	
[11/26 14:59:33 visual_prompt]: Best epoch 15: best metric: -5.674
[11/26 14:59:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/26 15:01:14 visual_prompt]: 	Training 100/553. train loss: 2.0761,	0.8624 s / batch. (data: 4.22e-04). ETA=11:14:12, max mem: 20.9 GB 
[11/26 15:02:52 visual_prompt]: 	Training 200/553. train loss: 74.7176,	0.8190 s / batch. (data: 3.39e-04). ETA=10:38:54, max mem: 20.9 GB 
[11/26 15:04:30 visual_prompt]: 	Training 300/553. train loss: 70.4559,	0.8253 s / batch. (data: 3.40e-04). ETA=10:42:23, max mem: 20.9 GB 
[11/26 15:06:08 visual_prompt]: 	Training 400/553. train loss: 32.9066,	0.8186 s / batch. (data: 3.00e-04). ETA=10:35:48, max mem: 20.9 GB 
[11/26 15:07:45 visual_prompt]: 	Training 500/553. train loss: 10.4075,	1.2459 s / batch. (data: 4.29e-01). ETA=16:05:40, max mem: 20.9 GB 
[11/26 15:08:37 visual_prompt]: Epoch 16 / 100: avg data time: 1.52e-01, avg batch time: 0.9828, average train loss: 35.6242
[11/26 15:09:32 visual_prompt]: Inference (val):avg data time: 3.73e-04, avg batch time: 0.3117, average loss: 27.4504
[11/26 15:09:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.72	
[11/26 15:09:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/26 15:11:13 visual_prompt]: 	Training 100/553. train loss: 10.0498,	0.8520 s / batch. (data: 7.98e-03). ETA=10:58:11, max mem: 20.9 GB 
[11/26 15:12:52 visual_prompt]: 	Training 200/553. train loss: 2.1062,	0.8313 s / batch. (data: 1.04e-02). ETA=10:40:50, max mem: 20.9 GB 
[11/26 15:14:28 visual_prompt]: 	Training 300/553. train loss: 47.0998,	0.8320 s / batch. (data: 7.95e-03). ETA=10:39:58, max mem: 20.9 GB 
[11/26 15:16:06 visual_prompt]: 	Training 400/553. train loss: 48.3595,	1.1613 s / batch. (data: 3.40e-01). ETA=14:51:19, max mem: 20.9 GB 
[11/26 15:17:43 visual_prompt]: 	Training 500/553. train loss: 1.4778,	1.0080 s / batch. (data: 1.77e-01). ETA=12:51:59, max mem: 20.9 GB 
[11/26 15:18:35 visual_prompt]: Epoch 17 / 100: avg data time: 1.50e-01, avg batch time: 0.9814, average train loss: 27.8454
[11/26 15:19:31 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3110, average loss: 27.0778
[11/26 15:19:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.98	
[11/26 15:19:31 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/26 15:21:12 visual_prompt]: 	Training 100/553. train loss: 38.1946,	0.8282 s / batch. (data: 3.41e-04). ETA=10:32:09, max mem: 20.9 GB 
[11/26 15:22:55 visual_prompt]: 	Training 200/553. train loss: 16.9148,	0.8360 s / batch. (data: 3.14e-04). ETA=10:36:45, max mem: 20.9 GB 
[11/26 15:24:36 visual_prompt]: 	Training 300/553. train loss: 4.3457,	0.8246 s / batch. (data: 9.54e-03). ETA=10:26:40, max mem: 20.9 GB 
[11/26 15:26:16 visual_prompt]: 	Training 400/553. train loss: 22.5713,	0.8329 s / batch. (data: 5.43e-03). ETA=10:31:35, max mem: 20.9 GB 
[11/26 15:27:56 visual_prompt]: 	Training 500/553. train loss: 4.5779,	0.8219 s / batch. (data: 3.15e-04). ETA=10:21:52, max mem: 20.9 GB 
[11/26 15:28:47 visual_prompt]: Epoch 18 / 100: avg data time: 1.76e-01, avg batch time: 1.0052, average train loss: 34.1536
[11/26 15:29:43 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3109, average loss: 8.4810
[11/26 15:29:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.20	
[11/26 15:29:43 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/26 15:31:24 visual_prompt]: 	Training 100/553. train loss: 9.2575,	0.8285 s / batch. (data: 3.14e-04). ETA=10:24:46, max mem: 20.9 GB 
[11/26 15:33:02 visual_prompt]: 	Training 200/553. train loss: 21.5479,	0.8275 s / batch. (data: 5.43e-03). ETA=10:22:39, max mem: 20.9 GB 
[11/26 15:34:41 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8428 s / batch. (data: 3.29e-04). ETA=10:32:45, max mem: 20.9 GB 
[11/26 15:36:18 visual_prompt]: 	Training 400/553. train loss: 9.2315,	0.8321 s / batch. (data: 2.88e-04). ETA=10:23:21, max mem: 20.9 GB 
[11/26 15:37:52 visual_prompt]: 	Training 500/553. train loss: 0.2463,	0.8224 s / batch. (data: 1.87e-03). ETA=10:14:41, max mem: 20.9 GB 
[11/26 15:38:44 visual_prompt]: Epoch 19 / 100: avg data time: 1.45e-01, avg batch time: 0.9773, average train loss: 26.0521
[11/26 15:39:39 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3108, average loss: 82.6336
[11/26 15:39:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.47	
[11/26 15:39:39 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/26 15:41:19 visual_prompt]: 	Training 100/553. train loss: 8.7580,	0.8346 s / batch. (data: 7.95e-03). ETA=10:21:41, max mem: 20.9 GB 
[11/26 15:42:58 visual_prompt]: 	Training 200/553. train loss: 1.7225,	0.8360 s / batch. (data: 1.20e-02). ETA=10:21:20, max mem: 20.9 GB 
[11/26 15:44:36 visual_prompt]: 	Training 300/553. train loss: 5.0633,	0.8331 s / batch. (data: 3.20e-04). ETA=10:17:46, max mem: 20.9 GB 
[11/26 15:46:13 visual_prompt]: 	Training 400/553. train loss: 18.6988,	0.8154 s / batch. (data: 3.46e-04). ETA=10:03:17, max mem: 20.9 GB 
[11/26 15:47:50 visual_prompt]: 	Training 500/553. train loss: 0.4188,	0.8183 s / batch. (data: 2.96e-04). ETA=10:04:03, max mem: 20.9 GB 
[11/26 15:48:42 visual_prompt]: Epoch 20 / 100: avg data time: 1.49e-01, avg batch time: 0.9811, average train loss: 28.4255
[11/26 15:49:38 visual_prompt]: Inference (val):avg data time: 1.29e-04, avg batch time: 0.3094, average loss: 20.9842
[11/26 15:49:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.62	
[11/26 15:49:38 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/26 15:51:21 visual_prompt]: 	Training 100/553. train loss: 31.0035,	0.8280 s / batch. (data: 3.99e-03). ETA=10:09:07, max mem: 20.9 GB 
[11/26 15:52:58 visual_prompt]: 	Training 200/553. train loss: 56.9735,	0.8391 s / batch. (data: 9.61e-03). ETA=10:15:55, max mem: 20.9 GB 
[11/26 15:54:34 visual_prompt]: 	Training 300/553. train loss: 62.3923,	0.8320 s / batch. (data: 3.06e-04). ETA=10:09:19, max mem: 20.9 GB 
[11/26 15:56:13 visual_prompt]: 	Training 400/553. train loss: 42.8589,	0.8320 s / batch. (data: 2.95e-04). ETA=10:07:55, max mem: 20.9 GB 
[11/26 15:57:51 visual_prompt]: 	Training 500/553. train loss: 15.1508,	0.8531 s / batch. (data: 2.16e-02). ETA=10:21:54, max mem: 20.9 GB 
[11/26 15:58:41 visual_prompt]: Epoch 21 / 100: avg data time: 1.51e-01, avg batch time: 0.9831, average train loss: 22.8141
[11/26 15:59:37 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3096, average loss: 1.3783
[11/26 15:59:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.36	
[11/26 15:59:37 visual_prompt]: Best epoch 21: best metric: -1.378
[11/26 15:59:37 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/26 16:01:19 visual_prompt]: 	Training 100/553. train loss: 19.0938,	0.8323 s / batch. (data: 3.02e-04). ETA=10:04:39, max mem: 20.9 GB 
[11/26 16:02:56 visual_prompt]: 	Training 200/553. train loss: 4.2181,	0.8360 s / batch. (data: 3.05e-04). ETA=10:05:55, max mem: 20.9 GB 
[11/26 16:04:32 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8674 s / batch. (data: 5.43e-03). ETA=10:27:11, max mem: 20.9 GB 
[11/26 16:06:10 visual_prompt]: 	Training 400/553. train loss: 35.0689,	0.8329 s / batch. (data: 2.99e-04). ETA=10:00:53, max mem: 20.9 GB 
[11/26 16:07:48 visual_prompt]: 	Training 500/553. train loss: 61.7992,	0.8280 s / batch. (data: 5.45e-03). ETA=9:56:00, max mem: 20.9 GB 
[11/26 16:08:40 visual_prompt]: Epoch 22 / 100: avg data time: 1.49e-01, avg batch time: 0.9811, average train loss: 28.6100
[11/26 16:09:35 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3097, average loss: 40.7791
[11/26 16:09:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.47	
[11/26 16:09:35 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/26 16:11:18 visual_prompt]: 	Training 100/553. train loss: 17.7667,	0.8283 s / batch. (data: 3.06e-04). ETA=9:54:03, max mem: 20.9 GB 
[11/26 16:12:56 visual_prompt]: 	Training 200/553. train loss: 75.8020,	0.8242 s / batch. (data: 1.05e-02). ETA=9:49:47, max mem: 20.9 GB 
[11/26 16:14:35 visual_prompt]: 	Training 300/553. train loss: 12.9878,	0.8211 s / batch. (data: 2.95e-04). ETA=9:46:09, max mem: 20.9 GB 
[11/26 16:16:11 visual_prompt]: 	Training 400/553. train loss: 5.6556,	0.8403 s / batch. (data: 7.24e-04). ETA=9:58:30, max mem: 20.9 GB 
[11/26 16:17:47 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8200 s / batch. (data: 2.88e-04). ETA=9:42:39, max mem: 20.9 GB 
[11/26 16:18:37 visual_prompt]: Epoch 23 / 100: avg data time: 1.46e-01, avg batch time: 0.9788, average train loss: 26.6718
[11/26 16:19:33 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3081, average loss: 2.5826
[11/26 16:19:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.97	
[11/26 16:19:33 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/26 16:21:11 visual_prompt]: 	Training 100/553. train loss: 99.1741,	0.8126 s / batch. (data: 3.40e-04). ETA=9:35:18, max mem: 20.9 GB 
[11/26 16:22:49 visual_prompt]: 	Training 200/553. train loss: 30.3081,	0.9004 s / batch. (data: 2.83e-02). ETA=10:35:58, max mem: 20.9 GB 
[11/26 16:24:28 visual_prompt]: 	Training 300/553. train loss: 11.2261,	0.9800 s / batch. (data: 1.46e-01). ETA=11:30:35, max mem: 20.9 GB 
[11/26 16:26:06 visual_prompt]: 	Training 400/553. train loss: 1.6055,	0.8455 s / batch. (data: 9.16e-03). ETA=9:54:22, max mem: 20.9 GB 
[11/26 16:27:45 visual_prompt]: 	Training 500/553. train loss: 91.2325,	0.8245 s / batch. (data: 5.50e-03). ETA=9:38:15, max mem: 20.9 GB 
[11/26 16:28:37 visual_prompt]: Epoch 24 / 100: avg data time: 1.52e-01, avg batch time: 0.9842, average train loss: 29.2039
[11/26 16:29:33 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3116, average loss: 2.2943
[11/26 16:29:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.61	
[11/26 16:29:33 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/26 16:31:17 visual_prompt]: 	Training 100/553. train loss: 11.0729,	0.8397 s / batch. (data: 3.15e-04). ETA=9:46:46, max mem: 20.9 GB 
[11/26 16:32:52 visual_prompt]: 	Training 200/553. train loss: 41.8671,	0.8560 s / batch. (data: 3.25e-04). ETA=9:56:43, max mem: 20.9 GB 
[11/26 16:34:29 visual_prompt]: 	Training 300/553. train loss: 31.9652,	0.9846 s / batch. (data: 1.34e-01). ETA=11:24:43, max mem: 20.9 GB 
[11/26 16:36:06 visual_prompt]: 	Training 400/553. train loss: 5.9064,	1.2470 s / batch. (data: 4.07e-01). ETA=14:25:10, max mem: 20.9 GB 
[11/26 16:37:45 visual_prompt]: 	Training 500/553. train loss: 37.1354,	1.4120 s / batch. (data: 5.80e-01). ETA=16:17:15, max mem: 20.9 GB 
[11/26 16:38:36 visual_prompt]: Epoch 25 / 100: avg data time: 1.52e-01, avg batch time: 0.9826, average train loss: 39.3230
[11/26 16:39:31 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3107, average loss: 95.5960
[11/26 16:39:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.02	
[11/26 16:39:31 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/26 16:41:13 visual_prompt]: 	Training 100/553. train loss: 20.8582,	0.8384 s / batch. (data: 7.95e-03). ETA=9:38:09, max mem: 20.9 GB 
[11/26 16:42:52 visual_prompt]: 	Training 200/553. train loss: 25.1038,	1.7245 s / batch. (data: 9.01e-01). ETA=19:46:17, max mem: 20.9 GB 
[11/26 16:44:31 visual_prompt]: 	Training 300/553. train loss: 26.8676,	0.8203 s / batch. (data: 3.14e-04). ETA=9:22:55, max mem: 20.9 GB 
[11/26 16:46:07 visual_prompt]: 	Training 400/553. train loss: 48.3365,	0.8160 s / batch. (data: 2.93e-04). ETA=9:18:37, max mem: 20.9 GB 
[11/26 16:47:43 visual_prompt]: 	Training 500/553. train loss: 8.3600,	0.8373 s / batch. (data: 7.96e-03). ETA=9:31:49, max mem: 20.9 GB 
[11/26 16:48:34 visual_prompt]: Epoch 26 / 100: avg data time: 1.49e-01, avg batch time: 0.9807, average train loss: 25.6008
[11/26 16:49:30 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3096, average loss: 31.2810
[11/26 16:49:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.85	
[11/26 16:49:30 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[11/26 16:51:12 visual_prompt]: 	Training 100/553. train loss: 1.9237,	0.8280 s / batch. (data: 2.98e-04). ETA=9:23:20, max mem: 20.9 GB 
[11/26 16:52:49 visual_prompt]: 	Training 200/553. train loss: 16.5352,	1.0200 s / batch. (data: 1.93e-01). ETA=11:32:16, max mem: 20.9 GB 
[11/26 16:54:28 visual_prompt]: 	Training 300/553. train loss: 26.6875,	0.8440 s / batch. (data: 3.19e-04). ETA=9:31:24, max mem: 20.9 GB 
[11/26 16:56:06 visual_prompt]: 	Training 400/553. train loss: 22.0815,	0.8550 s / batch. (data: 7.79e-04). ETA=9:37:27, max mem: 20.9 GB 
[11/26 16:57:45 visual_prompt]: 	Training 500/553. train loss: 22.2407,	0.8673 s / batch. (data: 8.05e-04). ETA=9:44:17, max mem: 20.9 GB 
[11/26 16:58:33 visual_prompt]: Epoch 27 / 100: avg data time: 1.51e-01, avg batch time: 0.9831, average train loss: 29.5681
[11/26 16:59:29 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3088, average loss: 9.5758
[11/26 16:59:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.81	
[11/26 16:59:29 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[11/26 17:01:09 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8218 s / batch. (data: 3.35e-04). ETA=9:11:31, max mem: 20.9 GB 
[11/26 17:02:47 visual_prompt]: 	Training 200/553. train loss: 6.5409,	0.8480 s / batch. (data: 2.67e-04). ETA=9:27:44, max mem: 20.9 GB 
[11/26 17:04:26 visual_prompt]: 	Training 300/553. train loss: 50.0483,	1.3754 s / batch. (data: 5.44e-01). ETA=15:18:32, max mem: 20.9 GB 
[11/26 17:06:02 visual_prompt]: 	Training 400/553. train loss: 83.2198,	0.8278 s / batch. (data: 3.02e-04). ETA=9:11:25, max mem: 20.9 GB 
[11/26 17:07:38 visual_prompt]: 	Training 500/553. train loss: 151.5318,	0.8433 s / batch. (data: 2.77e-04). ETA=9:20:23, max mem: 20.9 GB 
[11/26 17:08:29 visual_prompt]: Epoch 28 / 100: avg data time: 1.46e-01, avg batch time: 0.9770, average train loss: 33.4609
[11/26 17:09:25 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3093, average loss: 80.3306
[11/26 17:09:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.43	
[11/26 17:09:25 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[11/26 17:11:13 visual_prompt]: 	Training 100/553. train loss: 1.9684,	0.8320 s / batch. (data: 7.95e-03). ETA=9:10:43, max mem: 20.9 GB 
[11/26 17:12:50 visual_prompt]: 	Training 200/553. train loss: 0.9400,	1.7051 s / batch. (data: 8.83e-01). ETA=18:45:47, max mem: 20.9 GB 
[11/26 17:14:26 visual_prompt]: 	Training 300/553. train loss: 7.0530,	0.8383 s / batch. (data: 7.17e-04). ETA=9:12:06, max mem: 20.9 GB 
[11/26 17:16:00 visual_prompt]: 	Training 400/553. train loss: 34.1399,	1.1064 s / batch. (data: 2.91e-01). ETA=12:06:51, max mem: 20.9 GB 
[11/26 17:17:38 visual_prompt]: 	Training 500/553. train loss: 18.4471,	0.8281 s / batch. (data: 5.45e-03). ETA=9:02:35, max mem: 20.9 GB 
[11/26 17:18:29 visual_prompt]: Epoch 29 / 100: avg data time: 1.50e-01, avg batch time: 0.9826, average train loss: 24.1689
[11/26 17:19:24 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3100, average loss: 9.8496
[11/26 17:19:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.99	
[11/26 17:19:24 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[11/26 17:21:05 visual_prompt]: 	Training 100/553. train loss: 60.4559,	0.8520 s / batch. (data: 2.69e-04). ETA=9:16:08, max mem: 20.9 GB 
[11/26 17:22:44 visual_prompt]: 	Training 200/553. train loss: 74.9746,	0.8267 s / batch. (data: 3.30e-04). ETA=8:58:12, max mem: 20.9 GB 
[11/26 17:24:19 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.0002 s / batch. (data: 1.81e-01). ETA=10:49:29, max mem: 20.9 GB 
[11/26 17:25:58 visual_prompt]: 	Training 400/553. train loss: 12.4831,	0.9356 s / batch. (data: 9.70e-02). ETA=10:06:01, max mem: 20.9 GB 
[11/26 17:27:35 visual_prompt]: 	Training 500/553. train loss: 11.8023,	1.3111 s / batch. (data: 4.66e-01). ETA=14:07:02, max mem: 20.9 GB 
[11/26 17:28:27 visual_prompt]: Epoch 30 / 100: avg data time: 1.51e-01, avg batch time: 0.9813, average train loss: 32.8756
[11/26 17:29:23 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3098, average loss: 43.5503
[11/26 17:29:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.66	
[11/26 17:29:23 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[11/26 17:31:06 visual_prompt]: 	Training 100/553. train loss: 11.9113,	0.8373 s / batch. (data: 2.07e-02). ETA=8:58:46, max mem: 20.9 GB 
[11/26 17:32:45 visual_prompt]: 	Training 200/553. train loss: 36.5928,	0.8560 s / batch. (data: 3.60e-04). ETA=9:09:22, max mem: 20.9 GB 
[11/26 17:34:20 visual_prompt]: 	Training 300/553. train loss: 67.0444,	0.8520 s / batch. (data: 5.44e-03). ETA=9:05:24, max mem: 20.9 GB 
[11/26 17:35:57 visual_prompt]: 	Training 400/553. train loss: 103.4922,	0.8862 s / batch. (data: 6.52e-02). ETA=9:25:50, max mem: 20.9 GB 
[11/26 17:37:35 visual_prompt]: 	Training 500/553. train loss: 14.1946,	0.8527 s / batch. (data: 2.07e-02). ETA=9:03:03, max mem: 20.9 GB 
[11/26 17:38:25 visual_prompt]: Epoch 31 / 100: avg data time: 1.48e-01, avg batch time: 0.9796, average train loss: 28.0165
[11/26 17:39:21 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3113, average loss: 30.4209
[11/26 17:39:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.14	
[11/26 17:39:21 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[11/26 17:41:04 visual_prompt]: 	Training 100/553. train loss: 2.3228,	0.8389 s / batch. (data: 5.95e-03). ETA=8:52:07, max mem: 20.9 GB 
[11/26 17:42:41 visual_prompt]: 	Training 200/553. train loss: 34.4871,	0.8295 s / batch. (data: 5.43e-03). ETA=8:44:43, max mem: 20.9 GB 
[11/26 17:44:22 visual_prompt]: 	Training 300/553. train loss: 5.7643,	0.8234 s / batch. (data: 2.99e-04). ETA=8:39:32, max mem: 20.9 GB 
[11/26 17:46:00 visual_prompt]: 	Training 400/553. train loss: 13.2650,	0.8399 s / batch. (data: 3.14e-04). ETA=8:48:30, max mem: 20.9 GB 
[11/26 17:47:35 visual_prompt]: 	Training 500/553. train loss: 23.5068,	0.8245 s / batch. (data: 3.18e-04). ETA=8:37:26, max mem: 20.9 GB 
[11/26 17:48:25 visual_prompt]: Epoch 32 / 100: avg data time: 1.51e-01, avg batch time: 0.9827, average train loss: 25.0306
[11/26 17:49:21 visual_prompt]: Inference (val):avg data time: 1.57e-04, avg batch time: 0.3104, average loss: 22.8979
[11/26 17:49:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.84	
[11/26 17:49:21 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[11/26 17:51:01 visual_prompt]: 	Training 100/553. train loss: 0.0000,	1.1520 s / batch. (data: 3.20e-01). ETA=12:00:04, max mem: 20.9 GB 
[11/26 17:52:41 visual_prompt]: 	Training 200/553. train loss: 6.4431,	0.8529 s / batch. (data: 3.32e-02). ETA=8:51:41, max mem: 20.9 GB 
[11/26 17:54:18 visual_prompt]: 	Training 300/553. train loss: 6.4296,	0.8327 s / batch. (data: 7.95e-03). ETA=8:37:41, max mem: 20.9 GB 
[11/26 17:55:57 visual_prompt]: 	Training 400/553. train loss: 9.9908,	0.8367 s / batch. (data: 7.95e-03). ETA=8:38:49, max mem: 20.9 GB 
[11/26 17:57:34 visual_prompt]: 	Training 500/553. train loss: 21.1172,	0.8217 s / batch. (data: 3.00e-04). ETA=8:28:06, max mem: 20.9 GB 
[11/26 17:58:24 visual_prompt]: Epoch 33 / 100: avg data time: 1.50e-01, avg batch time: 0.9822, average train loss: 22.6111
[11/26 17:59:21 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3099, average loss: 99.1680
[11/26 17:59:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.55	
[11/26 17:59:21 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[11/26 18:01:06 visual_prompt]: 	Training 100/553. train loss: 1.5003,	0.8320 s / batch. (data: 4.15e-04). ETA=8:32:21, max mem: 20.9 GB 
[11/26 18:02:44 visual_prompt]: 	Training 200/553. train loss: 5.6217,	0.8481 s / batch. (data: 1.20e-02). ETA=8:40:52, max mem: 20.9 GB 
[11/26 18:04:22 visual_prompt]: 	Training 300/553. train loss: 25.5805,	0.8407 s / batch. (data: 3.83e-04). ETA=8:34:55, max mem: 20.9 GB 
[11/26 18:06:04 visual_prompt]: 	Training 400/553. train loss: 4.7217,	0.8400 s / batch. (data: 2.94e-04). ETA=8:33:06, max mem: 20.9 GB 
[11/26 18:07:45 visual_prompt]: 	Training 500/553. train loss: 1.9305,	1.4943 s / batch. (data: 6.59e-01). ETA=15:10:19, max mem: 20.9 GB 
[11/26 18:08:36 visual_prompt]: Epoch 34 / 100: avg data time: 1.71e-01, avg batch time: 1.0033, average train loss: 21.0316
[11/26 18:09:33 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3105, average loss: 14.2374
[11/26 18:09:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.20	
[11/26 18:09:33 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[11/26 18:11:18 visual_prompt]: 	Training 100/553. train loss: 14.9943,	0.8473 s / batch. (data: 1.12e-02). ETA=8:33:59, max mem: 20.9 GB 
[11/26 18:13:01 visual_prompt]: 	Training 200/553. train loss: 24.9822,	0.8440 s / batch. (data: 3.25e-04). ETA=8:30:36, max mem: 20.9 GB 
[11/26 18:14:39 visual_prompt]: 	Training 300/553. train loss: 18.4030,	0.8560 s / batch. (data: 1.20e-02). ETA=8:36:24, max mem: 20.9 GB 
[11/26 18:16:17 visual_prompt]: 	Training 400/553. train loss: 50.2889,	0.8966 s / batch. (data: 6.53e-02). ETA=8:59:25, max mem: 20.9 GB 
[11/26 18:17:58 visual_prompt]: 	Training 500/553. train loss: 4.6468,	1.0399 s / batch. (data: 2.10e-01). ETA=10:23:53, max mem: 20.9 GB 
[11/26 18:18:50 visual_prompt]: Epoch 35 / 100: avg data time: 1.75e-01, avg batch time: 1.0075, average train loss: 20.0561
[11/26 18:19:48 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3099, average loss: 4.8598
[11/26 18:19:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.05	
[11/26 18:19:48 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[11/26 18:21:32 visual_prompt]: 	Training 100/553. train loss: 38.1920,	0.8374 s / batch. (data: 3.43e-04). ETA=8:20:17, max mem: 20.9 GB 
[11/26 18:23:13 visual_prompt]: 	Training 200/553. train loss: 44.7937,	0.8200 s / batch. (data: 3.31e-04). ETA=8:08:30, max mem: 20.9 GB 
[11/26 18:24:55 visual_prompt]: 	Training 300/553. train loss: 12.5179,	0.8240 s / batch. (data: 3.04e-04). ETA=8:09:32, max mem: 20.9 GB 
[11/26 18:26:35 visual_prompt]: 	Training 400/553. train loss: 16.2229,	0.8442 s / batch. (data: 2.93e-04). ETA=8:20:08, max mem: 20.9 GB 
[11/26 18:28:15 visual_prompt]: 	Training 500/553. train loss: 8.0568,	1.0889 s / batch. (data: 2.69e-01). ETA=10:43:14, max mem: 20.9 GB 
[11/26 18:29:05 visual_prompt]: Epoch 36 / 100: avg data time: 1.75e-01, avg batch time: 1.0062, average train loss: 22.4611
[11/26 18:30:02 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3091, average loss: 71.6065
[11/26 18:30:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.06	
[11/26 18:30:02 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[11/26 18:31:47 visual_prompt]: 	Training 100/553. train loss: 11.0589,	0.8525 s / batch. (data: 1.05e-02). ETA=8:21:26, max mem: 20.9 GB 
[11/26 18:33:27 visual_prompt]: 	Training 200/553. train loss: 17.2239,	0.8274 s / batch. (data: 1.05e-02). ETA=8:05:18, max mem: 20.9 GB 
[11/26 18:35:07 visual_prompt]: 	Training 300/553. train loss: 0.3063,	1.3174 s / batch. (data: 4.92e-01). ETA=12:50:30, max mem: 20.9 GB 
[11/26 18:36:50 visual_prompt]: 	Training 400/553. train loss: 24.4959,	1.6708 s / batch. (data: 8.54e-01). ETA=16:14:23, max mem: 20.9 GB 
[11/26 18:38:27 visual_prompt]: 	Training 500/553. train loss: 12.7340,	1.1207 s / batch. (data: 2.74e-01). ETA=10:51:41, max mem: 20.9 GB 
[11/26 18:39:21 visual_prompt]: Epoch 37 / 100: avg data time: 1.78e-01, avg batch time: 1.0095, average train loss: 22.0481
[11/26 18:40:18 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3098, average loss: 2.0686
[11/26 18:40:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.92	
[11/26 18:40:18 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[11/26 18:42:00 visual_prompt]: 	Training 100/553. train loss: 1.5612,	0.8667 s / batch. (data: 1.05e-02). ETA=8:21:49, max mem: 20.9 GB 
[11/26 18:43:41 visual_prompt]: 	Training 200/553. train loss: 12.0778,	1.0880 s / batch. (data: 2.53e-01). ETA=10:28:08, max mem: 20.9 GB 
[11/26 18:45:23 visual_prompt]: 	Training 300/553. train loss: 26.0816,	0.8440 s / batch. (data: 3.09e-04). ETA=8:05:50, max mem: 20.9 GB 
[11/26 18:47:01 visual_prompt]: 	Training 400/553. train loss: 24.8220,	0.8431 s / batch. (data: 1.19e-02). ETA=8:03:54, max mem: 20.9 GB 
[11/26 18:48:43 visual_prompt]: 	Training 500/553. train loss: 16.6523,	0.8325 s / batch. (data: 2.91e-04). ETA=7:56:26, max mem: 20.9 GB 
[11/26 18:49:34 visual_prompt]: Epoch 38 / 100: avg data time: 1.73e-01, avg batch time: 1.0049, average train loss: 24.5533
[11/26 18:50:31 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3105, average loss: 14.5569
[11/26 18:50:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.09	
[11/26 18:50:31 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[11/26 18:52:14 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8235 s / batch. (data: 3.85e-03). ETA=7:49:11, max mem: 20.9 GB 
[11/26 18:53:57 visual_prompt]: 	Training 200/553. train loss: 34.2023,	0.8200 s / batch. (data: 3.04e-04). ETA=7:45:50, max mem: 20.9 GB 
[11/26 18:55:41 visual_prompt]: 	Training 300/553. train loss: 65.7863,	0.8719 s / batch. (data: 8.28e-04). ETA=8:13:52, max mem: 20.9 GB 
[11/26 18:57:18 visual_prompt]: 	Training 400/553. train loss: 6.2619,	0.8850 s / batch. (data: 5.44e-02). ETA=8:19:49, max mem: 20.9 GB 
[11/26 18:58:59 visual_prompt]: 	Training 500/553. train loss: 12.1819,	1.7015 s / batch. (data: 8.72e-01). ETA=15:58:07, max mem: 20.9 GB 
[11/26 18:59:49 visual_prompt]: Epoch 39 / 100: avg data time: 1.76e-01, avg batch time: 1.0093, average train loss: 22.5044
[11/26 19:00:47 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3106, average loss: 16.7945
[11/26 19:00:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.91	
[11/26 19:00:47 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[11/26 19:02:31 visual_prompt]: 	Training 100/553. train loss: 17.2350,	0.8438 s / batch. (data: 5.43e-03). ETA=7:53:00, max mem: 20.9 GB 
[11/26 19:04:11 visual_prompt]: 	Training 200/553. train loss: 19.9245,	0.8222 s / batch. (data: 5.45e-03). ETA=7:39:29, max mem: 20.9 GB 
[11/26 19:05:52 visual_prompt]: 	Training 300/553. train loss: 35.8514,	0.8360 s / batch. (data: 3.27e-04). ETA=7:45:49, max mem: 20.9 GB 
[11/26 19:07:33 visual_prompt]: 	Training 400/553. train loss: 8.9490,	0.8190 s / batch. (data: 2.93e-04). ETA=7:35:01, max mem: 20.9 GB 
[11/26 19:09:12 visual_prompt]: 	Training 500/553. train loss: 25.1054,	0.8240 s / batch. (data: 2.99e-04). ETA=7:36:24, max mem: 20.9 GB 
[11/26 19:10:05 visual_prompt]: Epoch 40 / 100: avg data time: 1.78e-01, avg batch time: 1.0099, average train loss: 20.1436
[11/26 19:11:03 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3094, average loss: 16.0810
[11/26 19:11:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.86	
[11/26 19:11:03 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[11/26 19:12:52 visual_prompt]: 	Training 100/553. train loss: 0.6623,	0.8270 s / batch. (data: 3.10e-04). ETA=7:35:56, max mem: 20.9 GB 
[11/26 19:14:34 visual_prompt]: 	Training 200/553. train loss: 7.6635,	0.8477 s / batch. (data: 3.80e-04). ETA=7:45:56, max mem: 20.9 GB 
[11/26 19:16:13 visual_prompt]: 	Training 300/553. train loss: 33.7016,	0.8205 s / batch. (data: 3.18e-04). ETA=7:29:38, max mem: 20.9 GB 
[11/26 19:17:53 visual_prompt]: 	Training 400/553. train loss: 22.9489,	0.8400 s / batch. (data: 7.40e-04). ETA=7:38:56, max mem: 20.9 GB 
[11/26 19:19:31 visual_prompt]: 	Training 500/553. train loss: 2.8368,	0.8640 s / batch. (data: 7.16e-04). ETA=7:50:35, max mem: 20.9 GB 
[11/26 19:20:20 visual_prompt]: Epoch 41 / 100: avg data time: 1.77e-01, avg batch time: 1.0078, average train loss: 22.9542
[11/26 19:21:17 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3092, average loss: 16.2562
[11/26 19:21:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.14	
[11/26 19:21:17 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[11/26 19:22:58 visual_prompt]: 	Training 100/553. train loss: 3.2281,	0.8519 s / batch. (data: 2.43e-02). ETA=7:41:50, max mem: 20.9 GB 
[11/26 19:24:35 visual_prompt]: 	Training 200/553. train loss: 105.2772,	0.8433 s / batch. (data: 1.05e-02). ETA=7:35:45, max mem: 20.9 GB 
[11/26 19:26:13 visual_prompt]: 	Training 300/553. train loss: 15.7625,	0.8194 s / batch. (data: 3.66e-04). ETA=7:21:29, max mem: 20.9 GB 
[11/26 19:27:50 visual_prompt]: 	Training 400/553. train loss: 7.3702,	0.8286 s / batch. (data: 3.20e-04). ETA=7:25:01, max mem: 20.9 GB 
[11/26 19:29:26 visual_prompt]: 	Training 500/553. train loss: 47.2387,	0.8282 s / batch. (data: 3.05e-04). ETA=7:23:26, max mem: 20.9 GB 
[11/26 19:30:18 visual_prompt]: Epoch 42 / 100: avg data time: 1.46e-01, avg batch time: 0.9780, average train loss: 23.9258
[11/26 19:31:14 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3116, average loss: 24.8458
[11/26 19:31:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.97	
[11/26 19:31:14 visual_prompt]: Stopping early.
[11/26 19:31:14 visual_prompt]: Rank of current process: 0. World size: 1
[11/26 19:31:14 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/26 19:31:14 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/26 19:31:14 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/26 19:31:14 visual_prompt]: Training with config:
[11/26 19:31:14 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/26 19:31:14 visual_prompt]: Loading training data...
[11/26 19:31:14 visual_prompt]: Constructing mammo-cbis dataset train...
[11/26 19:31:14 visual_prompt]: Loading validation data...
[11/26 19:31:14 visual_prompt]: Constructing mammo-cbis dataset val...
[11/26 19:31:14 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/26 19:31:17 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/26 19:31:17 visual_prompt]: tuned percent:0.525
[11/26 19:31:17 visual_prompt]: Device used for model: 0
[11/26 19:31:17 visual_prompt]: Setting up Evaluator...
[11/26 19:31:17 visual_prompt]: Setting up Trainer...
[11/26 19:31:17 visual_prompt]: 	Setting up the optimizer...
[11/26 19:31:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/26 19:32:58 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8322 s / batch. (data: 3.05e-04). ETA=12:45:36, max mem: 20.9 GB 
[11/26 19:34:33 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8360 s / batch. (data: 6.89e-03). ETA=12:47:43, max mem: 20.9 GB 
[11/26 19:36:13 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.0640 s / batch. (data: 2.11e-01). ETA=16:15:19, max mem: 20.9 GB 
[11/26 19:37:49 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8476 s / batch. (data: 2.77e-02). ETA=12:55:35, max mem: 20.9 GB 
[11/26 19:39:28 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8235 s / batch. (data: 3.63e-04). ETA=12:32:10, max mem: 20.9 GB 
[11/26 19:40:20 visual_prompt]: Epoch 1 / 100: avg data time: 1.47e-01, avg batch time: 0.9811, average train loss: 1.5403
[11/26 19:41:15 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3113, average loss: 1.5201
[11/26 19:41:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/26 19:41:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/26 19:42:57 visual_prompt]: 	Training 100/553. train loss: 2.4547,	0.8177 s / batch. (data: 3.20e-04). ETA=12:24:44, max mem: 20.9 GB 
[11/26 19:44:33 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8191 s / batch. (data: 3.40e-04). ETA=12:24:37, max mem: 20.9 GB 
[11/26 19:46:13 visual_prompt]: 	Training 300/553. train loss: 7.6994,	1.0006 s / batch. (data: 1.80e-01). ETA=15:07:58, max mem: 20.9 GB 
[11/26 19:47:49 visual_prompt]: 	Training 400/553. train loss: 0.7945,	0.8720 s / batch. (data: 3.49e-04). ETA=13:09:53, max mem: 20.9 GB 
[11/26 19:49:28 visual_prompt]: 	Training 500/553. train loss: 0.8968,	0.8461 s / batch. (data: 3.30e-04). ETA=12:44:58, max mem: 20.9 GB 
[11/26 19:50:18 visual_prompt]: Epoch 2 / 100: avg data time: 1.47e-01, avg batch time: 0.9821, average train loss: 4.2426
[11/26 19:51:14 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3103, average loss: 0.7099
[11/26 19:51:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.19	rocauc: 59.89	
[11/26 19:51:14 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/26 19:52:54 visual_prompt]: 	Training 100/553. train loss: 6.4990,	0.8480 s / batch. (data: 7.95e-03). ETA=12:44:31, max mem: 20.9 GB 
[11/26 19:54:33 visual_prompt]: 	Training 200/553. train loss: 4.0049,	0.8362 s / batch. (data: 5.44e-03). ETA=12:32:28, max mem: 20.9 GB 
[11/26 19:56:10 visual_prompt]: 	Training 300/553. train loss: 0.8738,	0.8560 s / batch. (data: 3.07e-04). ETA=12:48:52, max mem: 20.9 GB 
[11/26 19:57:47 visual_prompt]: 	Training 400/553. train loss: 8.9774,	0.8641 s / batch. (data: 1.39e-02). ETA=12:54:43, max mem: 20.9 GB 
[11/26 19:59:25 visual_prompt]: 	Training 500/553. train loss: 7.3340,	1.1421 s / batch. (data: 3.26e-01). ETA=17:02:02, max mem: 20.9 GB 
[11/26 20:00:15 visual_prompt]: Epoch 3 / 100: avg data time: 1.45e-01, avg batch time: 0.9788, average train loss: 7.6226
[11/26 20:01:11 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3102, average loss: 5.2789
[11/26 20:01:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.01	
[11/26 20:01:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/26 20:02:54 visual_prompt]: 	Training 100/553. train loss: 7.9321,	0.8287 s / batch. (data: 2.98e-04). ETA=12:19:28, max mem: 20.9 GB 
[11/26 20:04:32 visual_prompt]: 	Training 200/553. train loss: 12.9808,	0.8200 s / batch. (data: 3.51e-04). ETA=12:10:20, max mem: 20.9 GB 
[11/26 20:06:10 visual_prompt]: 	Training 300/553. train loss: 8.3782,	1.2808 s / batch. (data: 4.59e-01). ETA=18:58:40, max mem: 20.9 GB 
[11/26 20:07:43 visual_prompt]: 	Training 400/553. train loss: 0.5039,	0.8480 s / batch. (data: 3.22e-04). ETA=12:32:28, max mem: 20.9 GB 
[11/26 20:09:23 visual_prompt]: 	Training 500/553. train loss: 31.1022,	3.4872 s / batch. (data: 2.67e+00). ETA=2 days, 3:28:32, max mem: 20.9 GB 
[11/26 20:10:15 visual_prompt]: Epoch 4 / 100: avg data time: 1.50e-01, avg batch time: 0.9835, average train loss: 8.4598
[11/26 20:11:10 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3094, average loss: 31.7809
[11/26 20:11:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.11	
[11/26 20:11:10 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/26 20:12:51 visual_prompt]: 	Training 100/553. train loss: 0.0982,	0.8558 s / batch. (data: 7.77e-03). ETA=12:35:45, max mem: 20.9 GB 
[11/26 20:14:28 visual_prompt]: 	Training 200/553. train loss: 5.2413,	1.1310 s / batch. (data: 3.13e-01). ETA=16:36:57, max mem: 20.9 GB 
[11/26 20:16:07 visual_prompt]: 	Training 300/553. train loss: 3.9706,	0.8423 s / batch. (data: 3.11e-04). ETA=12:21:03, max mem: 20.9 GB 
[11/26 20:17:44 visual_prompt]: 	Training 400/553. train loss: 0.8584,	0.8252 s / batch. (data: 3.24e-04). ETA=12:04:37, max mem: 20.9 GB 
[11/26 20:19:22 visual_prompt]: 	Training 500/553. train loss: 8.9242,	0.8560 s / batch. (data: 3.09e-04). ETA=12:30:16, max mem: 20.9 GB 
[11/26 20:20:14 visual_prompt]: Epoch 5 / 100: avg data time: 1.49e-01, avg batch time: 0.9833, average train loss: 10.1281
[11/26 20:21:10 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3091, average loss: 22.4373
[11/26 20:21:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.64	
[11/26 20:21:10 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/26 20:22:53 visual_prompt]: 	Training 100/553. train loss: 3.7443,	0.8319 s / batch. (data: 5.37e-03). ETA=12:06:58, max mem: 20.9 GB 
[11/26 20:24:30 visual_prompt]: 	Training 200/553. train loss: 5.5833,	0.8320 s / batch. (data: 2.98e-04). ETA=12:05:43, max mem: 20.9 GB 
[11/26 20:26:05 visual_prompt]: 	Training 300/553. train loss: 2.3348,	0.8369 s / batch. (data: 2.78e-04). ETA=12:08:35, max mem: 20.9 GB 
[11/26 20:27:47 visual_prompt]: 	Training 400/553. train loss: 26.8347,	0.8279 s / batch. (data: 3.37e-04). ETA=11:59:23, max mem: 20.9 GB 
[11/26 20:29:22 visual_prompt]: 	Training 500/553. train loss: 1.0403,	0.8284 s / batch. (data: 3.76e-04). ETA=11:58:27, max mem: 20.9 GB 
[11/26 20:30:13 visual_prompt]: Epoch 6 / 100: avg data time: 1.48e-01, avg batch time: 0.9820, average train loss: 11.6939
[11/26 20:31:09 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3107, average loss: 5.0968
[11/26 20:31:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.36	
[11/26 20:31:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/26 20:32:49 visual_prompt]: 	Training 100/553. train loss: 10.6278,	0.8284 s / batch. (data: 3.14e-04). ETA=11:56:18, max mem: 20.9 GB 
[11/26 20:34:26 visual_prompt]: 	Training 200/553. train loss: 8.8718,	1.0044 s / batch. (data: 1.56e-01). ETA=14:26:48, max mem: 20.9 GB 
[11/26 20:36:07 visual_prompt]: 	Training 300/553. train loss: 16.6753,	1.7029 s / batch. (data: 8.75e-01). ETA=1 day, 0:26:48, max mem: 20.9 GB 
[11/26 20:37:45 visual_prompt]: 	Training 400/553. train loss: 9.0358,	1.8120 s / batch. (data: 9.84e-01). ETA=1 day, 1:57:45, max mem: 20.9 GB 
[11/26 20:39:20 visual_prompt]: 	Training 500/553. train loss: 48.6467,	0.8223 s / batch. (data: 3.12e-04). ETA=11:45:35, max mem: 20.9 GB 
[11/26 20:40:10 visual_prompt]: Epoch 7 / 100: avg data time: 1.43e-01, avg batch time: 0.9783, average train loss: 14.1636
[11/26 20:41:05 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3121, average loss: 17.4630
[11/26 20:41:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.64	
[11/26 20:41:05 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/26 20:42:45 visual_prompt]: 	Training 100/553. train loss: 50.7500,	0.8318 s / batch. (data: 3.04e-04). ETA=11:51:36, max mem: 20.9 GB 
[11/26 20:44:23 visual_prompt]: 	Training 200/553. train loss: 3.8612,	0.8280 s / batch. (data: 3.07e-04). ETA=11:46:57, max mem: 20.9 GB 
[11/26 20:46:02 visual_prompt]: 	Training 300/553. train loss: 33.2820,	0.8560 s / batch. (data: 3.05e-04). ETA=12:09:26, max mem: 20.9 GB 
[11/26 20:47:40 visual_prompt]: 	Training 400/553. train loss: 1.3699,	0.8373 s / batch. (data: 3.20e-04). ETA=11:52:04, max mem: 20.9 GB 
[11/26 20:49:18 visual_prompt]: 	Training 500/553. train loss: 0.3061,	1.2920 s / batch. (data: 4.61e-01). ETA=18:16:40, max mem: 20.9 GB 
[11/26 20:50:09 visual_prompt]: Epoch 8 / 100: avg data time: 1.50e-01, avg batch time: 0.9833, average train loss: 14.8445
[11/26 20:51:05 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3109, average loss: 2.0214
[11/26 20:51:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 62.54	
[11/26 20:51:05 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/26 20:52:46 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8280 s / batch. (data: 2.88e-04). ETA=11:40:41, max mem: 20.9 GB 
[11/26 20:54:22 visual_prompt]: 	Training 200/553. train loss: 3.6899,	0.8471 s / batch. (data: 5.43e-03). ETA=11:55:26, max mem: 20.9 GB 
[11/26 20:56:00 visual_prompt]: 	Training 300/553. train loss: 13.8040,	1.5960 s / batch. (data: 7.41e-01). ETA=22:25:17, max mem: 20.9 GB 
[11/26 20:57:40 visual_prompt]: 	Training 400/553. train loss: 12.5228,	0.8499 s / batch. (data: 7.57e-04). ETA=11:54:59, max mem: 20.9 GB 
[11/26 20:59:18 visual_prompt]: 	Training 500/553. train loss: 8.6952,	0.8403 s / batch. (data: 3.43e-04). ETA=11:45:33, max mem: 20.9 GB 
[11/26 21:00:08 visual_prompt]: Epoch 9 / 100: avg data time: 1.48e-01, avg batch time: 0.9815, average train loss: 14.6695
[11/26 21:01:03 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3116, average loss: 9.3240
[11/26 21:01:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.57	
[11/26 21:01:03 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/26 21:02:47 visual_prompt]: 	Training 100/553. train loss: 36.7253,	0.8245 s / batch. (data: 3.31e-04). ETA=11:30:10, max mem: 20.9 GB 
[11/26 21:04:22 visual_prompt]: 	Training 200/553. train loss: 19.9705,	0.8435 s / batch. (data: 5.17e-04). ETA=11:44:39, max mem: 20.9 GB 
[11/26 21:05:58 visual_prompt]: 	Training 300/553. train loss: 46.9929,	0.8398 s / batch. (data: 2.93e-04). ETA=11:40:09, max mem: 20.9 GB 
[11/26 21:07:33 visual_prompt]: 	Training 400/553. train loss: 38.0591,	0.8400 s / batch. (data: 3.32e-04). ETA=11:38:56, max mem: 20.9 GB 
[11/26 21:09:13 visual_prompt]: 	Training 500/553. train loss: 6.8261,	0.8240 s / batch. (data: 3.13e-04). ETA=11:24:14, max mem: 20.9 GB 
[11/26 21:10:04 visual_prompt]: Epoch 10 / 100: avg data time: 1.45e-01, avg batch time: 0.9783, average train loss: 22.4447
[11/26 21:11:00 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3102, average loss: 16.9751
[11/26 21:11:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.92	
[11/26 21:11:00 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/26 21:12:44 visual_prompt]: 	Training 100/553. train loss: 74.9341,	0.8400 s / batch. (data: 3.12e-04). ETA=11:35:23, max mem: 20.9 GB 
[11/26 21:14:23 visual_prompt]: 	Training 200/553. train loss: 29.2614,	0.8397 s / batch. (data: 2.99e-04). ETA=11:33:43, max mem: 20.9 GB 
[11/26 21:16:00 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.8988 s / batch. (data: 1.09e+00). ETA=1 day, 2:05:33, max mem: 20.9 GB 
[11/26 21:17:36 visual_prompt]: 	Training 400/553. train loss: 18.4664,	0.8439 s / batch. (data: 5.40e-03). ETA=11:34:23, max mem: 20.9 GB 
[11/26 21:19:12 visual_prompt]: 	Training 500/553. train loss: 7.8388,	0.8393 s / batch. (data: 5.41e-03). ETA=11:29:14, max mem: 20.9 GB 
[11/26 21:20:02 visual_prompt]: Epoch 11 / 100: avg data time: 1.48e-01, avg batch time: 0.9811, average train loss: 22.0659
[11/26 21:20:58 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3097, average loss: 2.4523
[11/26 21:20:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 69.60	
[11/26 21:20:58 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/26 21:22:42 visual_prompt]: 	Training 100/553. train loss: 3.9416,	0.8244 s / batch. (data: 3.37e-04). ETA=11:14:50, max mem: 20.9 GB 
[11/26 21:24:21 visual_prompt]: 	Training 200/553. train loss: 24.5018,	0.8329 s / batch. (data: 2.78e-04). ETA=11:20:25, max mem: 20.9 GB 
[11/26 21:25:56 visual_prompt]: 	Training 300/553. train loss: 32.0297,	0.8196 s / batch. (data: 3.17e-04). ETA=11:08:13, max mem: 20.9 GB 
[11/26 21:27:35 visual_prompt]: 	Training 400/553. train loss: 28.2493,	0.8392 s / batch. (data: 3.09e-04). ETA=11:22:45, max mem: 20.9 GB 
[11/26 21:29:13 visual_prompt]: 	Training 500/553. train loss: 82.2206,	0.8597 s / batch. (data: 7.89e-04). ETA=11:38:01, max mem: 20.9 GB 
[11/26 21:30:03 visual_prompt]: Epoch 12 / 100: avg data time: 1.52e-01, avg batch time: 0.9843, average train loss: 18.6137
[11/26 21:30:58 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3099, average loss: 22.1416
[11/26 21:30:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.73	
[11/26 21:30:58 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/26 21:32:42 visual_prompt]: 	Training 100/553. train loss: 10.2265,	0.8520 s / batch. (data: 1.20e-02). ETA=11:29:34, max mem: 20.9 GB 
[11/26 21:34:16 visual_prompt]: 	Training 200/553. train loss: 12.1518,	0.8440 s / batch. (data: 2.86e-04). ETA=11:21:44, max mem: 20.9 GB 
[11/26 21:35:55 visual_prompt]: 	Training 300/553. train loss: 6.5078,	1.5560 s / batch. (data: 7.19e-01). ETA=20:54:14, max mem: 20.9 GB 
[11/26 21:37:30 visual_prompt]: 	Training 400/553. train loss: 5.5350,	0.8412 s / batch. (data: 1.14e-02). ETA=11:16:38, max mem: 20.9 GB 
[11/26 21:39:09 visual_prompt]: 	Training 500/553. train loss: 30.8013,	0.8652 s / batch. (data: 1.19e-02). ETA=11:34:29, max mem: 20.9 GB 
[11/26 21:40:00 visual_prompt]: Epoch 13 / 100: avg data time: 1.46e-01, avg batch time: 0.9787, average train loss: 19.2707
[11/26 21:40:55 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3099, average loss: 9.5355
[11/26 21:40:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 68.06	
[11/26 21:40:55 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/26 21:42:38 visual_prompt]: 	Training 100/553. train loss: 3.6630,	0.8730 s / batch. (data: 1.30e-02). ETA=11:38:32, max mem: 20.9 GB 
[11/26 21:44:15 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.0000 s / batch. (data: 1.58e-01). ETA=13:18:30, max mem: 20.9 GB 
[11/26 21:45:53 visual_prompt]: 	Training 300/553. train loss: 13.1360,	0.8520 s / batch. (data: 3.10e-04). ETA=11:18:56, max mem: 20.9 GB 
[11/26 21:47:29 visual_prompt]: 	Training 400/553. train loss: 8.2468,	0.8305 s / batch. (data: 2.97e-04). ETA=11:00:23, max mem: 20.9 GB 
[11/26 21:49:06 visual_prompt]: 	Training 500/553. train loss: 6.0314,	0.8400 s / batch. (data: 3.15e-04). ETA=11:06:33, max mem: 20.9 GB 
[11/26 21:49:56 visual_prompt]: Epoch 14 / 100: avg data time: 1.46e-01, avg batch time: 0.9784, average train loss: 20.2889
[11/26 21:50:52 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3115, average loss: 5.4824
[11/26 21:50:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.59	
[11/26 21:50:52 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/26 21:52:33 visual_prompt]: 	Training 100/553. train loss: 2.4841,	0.8615 s / batch. (data: 1.05e-02). ETA=11:21:25, max mem: 20.9 GB 
[11/26 21:54:09 visual_prompt]: 	Training 200/553. train loss: 0.0061,	0.8360 s / batch. (data: 7.95e-03). ETA=10:59:51, max mem: 20.9 GB 
[11/26 21:55:48 visual_prompt]: 	Training 300/553. train loss: 34.1132,	0.8237 s / batch. (data: 7.71e-04). ETA=10:48:44, max mem: 20.9 GB 
[11/26 21:57:22 visual_prompt]: 	Training 400/553. train loss: 30.7254,	0.8568 s / batch. (data: 3.65e-02). ETA=11:13:27, max mem: 20.9 GB 
[11/26 21:59:00 visual_prompt]: 	Training 500/553. train loss: 8.6655,	0.8188 s / batch. (data: 3.17e-04). ETA=10:42:10, max mem: 20.9 GB 
[11/26 21:59:51 visual_prompt]: Epoch 15 / 100: avg data time: 1.42e-01, avg batch time: 0.9750, average train loss: 21.3577
[11/26 22:00:46 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3098, average loss: 4.9152
[11/26 22:00:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.58	
[11/26 22:00:46 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/26 22:02:27 visual_prompt]: 	Training 100/553. train loss: 25.2839,	0.8200 s / batch. (data: 2.98e-04). ETA=10:41:02, max mem: 20.9 GB 
[11/26 22:04:03 visual_prompt]: 	Training 200/553. train loss: 20.3574,	0.8280 s / batch. (data: 2.57e-04). ETA=10:45:55, max mem: 20.9 GB 
[11/26 22:05:41 visual_prompt]: 	Training 300/553. train loss: 11.8521,	0.8305 s / batch. (data: 9.34e-03). ETA=10:46:26, max mem: 20.9 GB 
[11/26 22:07:17 visual_prompt]: 	Training 400/553. train loss: 19.4738,	0.8320 s / batch. (data: 1.55e-02). ETA=10:46:17, max mem: 20.9 GB 
[11/26 22:08:54 visual_prompt]: 	Training 500/553. train loss: 3.9495,	1.2700 s / batch. (data: 4.45e-01). ETA=16:24:23, max mem: 20.9 GB 
[11/26 22:09:44 visual_prompt]: Epoch 16 / 100: avg data time: 1.40e-01, avg batch time: 0.9731, average train loss: 17.7663
[11/26 22:10:39 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3092, average loss: 2.0187
[11/26 22:10:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 64.72	
[11/26 22:10:39 visual_prompt]: Best epoch 16: best metric: -2.019
[11/26 22:10:39 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/26 22:12:19 visual_prompt]: 	Training 100/553. train loss: 8.9375,	0.8205 s / batch. (data: 3.19e-04). ETA=10:33:52, max mem: 20.9 GB 
[11/26 22:13:57 visual_prompt]: 	Training 200/553. train loss: 45.9730,	0.8200 s / batch. (data: 3.12e-04). ETA=10:32:05, max mem: 20.9 GB 
[11/26 22:15:34 visual_prompt]: 	Training 300/553. train loss: 22.9622,	0.8256 s / batch. (data: 3.08e-04). ETA=10:35:03, max mem: 20.9 GB 
[11/26 22:17:11 visual_prompt]: 	Training 400/553. train loss: 14.7286,	1.3760 s / batch. (data: 5.53e-01). ETA=17:36:09, max mem: 20.9 GB 
[11/26 22:18:48 visual_prompt]: 	Training 500/553. train loss: 7.3184,	1.5840 s / batch. (data: 7.51e-01). ETA=20:13:07, max mem: 20.9 GB 
[11/26 22:19:40 visual_prompt]: Epoch 17 / 100: avg data time: 1.45e-01, avg batch time: 0.9781, average train loss: 19.2786
[11/26 22:20:36 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3116, average loss: 27.4367
[11/26 22:20:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.51	
[11/26 22:20:36 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/26 22:22:18 visual_prompt]: 	Training 100/553. train loss: 5.2559,	0.8495 s / batch. (data: 1.06e-02). ETA=10:48:26, max mem: 20.9 GB 
[11/26 22:23:57 visual_prompt]: 	Training 200/553. train loss: 3.6647,	0.8343 s / batch. (data: 7.45e-04). ETA=10:35:25, max mem: 20.9 GB 
[11/26 22:25:35 visual_prompt]: 	Training 300/553. train loss: 5.1940,	0.8247 s / batch. (data: 2.99e-04). ETA=10:26:44, max mem: 20.9 GB 
[11/26 22:27:12 visual_prompt]: 	Training 400/553. train loss: 25.8491,	0.8372 s / batch. (data: 9.11e-03). ETA=10:34:49, max mem: 20.9 GB 
[11/26 22:28:48 visual_prompt]: 	Training 500/553. train loss: 14.6559,	0.8444 s / batch. (data: 3.29e-04). ETA=10:38:55, max mem: 20.9 GB 
[11/26 22:29:38 visual_prompt]: Epoch 18 / 100: avg data time: 1.48e-01, avg batch time: 0.9805, average train loss: 21.2044
[11/26 22:30:34 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3089, average loss: 31.1111
[11/26 22:30:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.99	
[11/26 22:30:34 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/26 22:32:15 visual_prompt]: 	Training 100/553. train loss: 8.5300,	0.8320 s / batch. (data: 3.25e-04). ETA=10:27:24, max mem: 20.9 GB 
[11/26 22:33:53 visual_prompt]: 	Training 200/553. train loss: 10.2427,	0.8200 s / batch. (data: 3.01e-04). ETA=10:16:59, max mem: 20.9 GB 
[11/26 22:35:31 visual_prompt]: 	Training 300/553. train loss: 0.0001,	0.8389 s / batch. (data: 5.61e-03). ETA=10:29:50, max mem: 20.9 GB 
[11/26 22:37:08 visual_prompt]: 	Training 400/553. train loss: 9.9992,	0.8529 s / batch. (data: 1.05e-02). ETA=10:38:54, max mem: 20.9 GB 
[11/26 22:38:42 visual_prompt]: 	Training 500/553. train loss: 8.1673,	0.8522 s / batch. (data: 3.12e-04). ETA=10:36:59, max mem: 20.9 GB 
[11/26 22:39:32 visual_prompt]: Epoch 19 / 100: avg data time: 1.41e-01, avg batch time: 0.9737, average train loss: 16.9489
[11/26 22:40:28 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3110, average loss: 9.2377
[11/26 22:40:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 66.11	
[11/26 22:40:28 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/26 22:42:07 visual_prompt]: 	Training 100/553. train loss: 25.5115,	0.8240 s / batch. (data: 2.76e-04). ETA=10:13:48, max mem: 20.9 GB 
[11/26 22:43:45 visual_prompt]: 	Training 200/553. train loss: 2.2696,	0.8518 s / batch. (data: 1.05e-02). ETA=10:33:06, max mem: 20.9 GB 
[11/26 22:45:22 visual_prompt]: 	Training 300/553. train loss: 13.7544,	0.8442 s / batch. (data: 7.74e-04). ETA=10:26:02, max mem: 20.9 GB 
[11/26 22:46:58 visual_prompt]: 	Training 400/553. train loss: 30.7075,	0.8297 s / batch. (data: 1.20e-02). ETA=10:13:54, max mem: 20.9 GB 
[11/26 22:48:34 visual_prompt]: 	Training 500/553. train loss: 84.3555,	0.8360 s / batch. (data: 3.04e-04). ETA=10:17:07, max mem: 20.9 GB 
[11/26 22:49:26 visual_prompt]: Epoch 20 / 100: avg data time: 1.41e-01, avg batch time: 0.9731, average train loss: 21.6830
[11/26 22:50:21 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3100, average loss: 7.5366
[11/26 22:50:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 68.91	
[11/26 22:50:21 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/26 22:52:05 visual_prompt]: 	Training 100/553. train loss: 3.1234,	0.8286 s / batch. (data: 1.05e-02). ETA=10:09:34, max mem: 20.9 GB 
[11/26 22:53:40 visual_prompt]: 	Training 200/553. train loss: 36.0146,	0.8320 s / batch. (data: 3.16e-04). ETA=10:10:43, max mem: 20.9 GB 
[11/26 22:55:17 visual_prompt]: 	Training 300/553. train loss: 81.9775,	1.1158 s / batch. (data: 2.82e-01). ETA=13:37:06, max mem: 20.9 GB 
[11/26 22:56:52 visual_prompt]: 	Training 400/553. train loss: 1.2556,	0.8270 s / batch. (data: 7.95e-03). ETA=10:04:17, max mem: 20.9 GB 
[11/26 22:58:31 visual_prompt]: 	Training 500/553. train loss: 7.6277,	0.8516 s / batch. (data: 2.92e-04). ETA=10:20:48, max mem: 20.9 GB 
[11/26 22:59:21 visual_prompt]: Epoch 21 / 100: avg data time: 1.42e-01, avg batch time: 0.9751, average train loss: 16.5011
[11/26 23:00:16 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3083, average loss: 7.3796
[11/26 23:00:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 67.41	
[11/26 23:00:16 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/26 23:01:55 visual_prompt]: 	Training 100/553. train loss: 47.6375,	0.8200 s / batch. (data: 3.04e-04). ETA=9:55:40, max mem: 20.9 GB 
[11/26 23:03:32 visual_prompt]: 	Training 200/553. train loss: 13.8386,	0.8511 s / batch. (data: 5.44e-03). ETA=10:16:52, max mem: 20.9 GB 
[11/26 23:05:07 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8467 s / batch. (data: 3.55e-04). ETA=10:12:16, max mem: 20.9 GB 
[11/26 23:06:44 visual_prompt]: 	Training 400/553. train loss: 4.3740,	0.8267 s / batch. (data: 3.17e-04). ETA=9:56:26, max mem: 20.9 GB 
[11/26 23:08:22 visual_prompt]: 	Training 500/553. train loss: 23.4144,	0.8272 s / batch. (data: 5.37e-03). ETA=9:55:22, max mem: 20.9 GB 
[11/26 23:09:14 visual_prompt]: Epoch 22 / 100: avg data time: 1.39e-01, avg batch time: 0.9725, average train loss: 15.0954
[11/26 23:10:09 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3107, average loss: 4.8684
[11/26 23:10:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 68.48	
[11/26 23:10:09 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/26 23:11:51 visual_prompt]: 	Training 100/553. train loss: 9.3038,	0.8320 s / batch. (data: 3.14e-04). ETA=9:56:45, max mem: 20.9 GB 
[11/26 23:13:28 visual_prompt]: 	Training 200/553. train loss: 66.7489,	0.8266 s / batch. (data: 3.18e-04). ETA=9:51:28, max mem: 20.9 GB 
[11/26 23:15:06 visual_prompt]: 	Training 300/553. train loss: 12.7685,	0.8459 s / batch. (data: 7.37e-04). ETA=10:03:51, max mem: 20.9 GB 
[11/26 23:16:41 visual_prompt]: 	Training 400/553. train loss: 9.6662,	0.8859 s / batch. (data: 3.54e-02). ETA=10:30:59, max mem: 20.9 GB 
[11/26 23:18:17 visual_prompt]: 	Training 500/553. train loss: 29.2016,	0.8336 s / batch. (data: 3.01e-04). ETA=9:52:19, max mem: 20.9 GB 
[11/26 23:19:08 visual_prompt]: Epoch 23 / 100: avg data time: 1.43e-01, avg batch time: 0.9756, average train loss: 19.5543
[11/26 23:20:04 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3114, average loss: 8.3050
[11/26 23:20:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 67.63	
[11/26 23:20:04 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/26 23:21:43 visual_prompt]: 	Training 100/553. train loss: 40.1461,	0.8392 s / batch. (data: 7.19e-03). ETA=9:54:11, max mem: 20.9 GB 
[11/26 23:23:19 visual_prompt]: 	Training 200/553. train loss: 23.6031,	0.8480 s / batch. (data: 2.97e-04). ETA=9:58:58, max mem: 20.9 GB 
[11/26 23:24:58 visual_prompt]: 	Training 300/553. train loss: 2.1520,	0.8597 s / batch. (data: 4.12e-02). ETA=10:05:48, max mem: 20.9 GB 
[11/26 23:26:35 visual_prompt]: 	Training 400/553. train loss: 34.9383,	0.8200 s / batch. (data: 3.25e-04). ETA=9:36:27, max mem: 20.9 GB 
[11/26 23:28:14 visual_prompt]: 	Training 500/553. train loss: 33.8811,	0.8433 s / batch. (data: 3.43e-04). ETA=9:51:28, max mem: 20.9 GB 
[11/26 23:29:06 visual_prompt]: Epoch 24 / 100: avg data time: 1.47e-01, avg batch time: 0.9796, average train loss: 22.2876
[11/26 23:30:01 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.3115, average loss: 48.8690
[11/26 23:30:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.95	
[11/26 23:30:01 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/26 23:31:46 visual_prompt]: 	Training 100/553. train loss: 26.6344,	0.8728 s / batch. (data: 2.08e-02). ETA=10:09:54, max mem: 20.9 GB 
[11/26 23:33:21 visual_prompt]: 	Training 200/553. train loss: 9.9352,	1.1571 s / batch. (data: 3.33e-01). ETA=13:26:39, max mem: 20.9 GB 
[11/26 23:34:58 visual_prompt]: 	Training 300/553. train loss: 7.5309,	0.8520 s / batch. (data: 5.42e-03). ETA=9:52:32, max mem: 20.9 GB 
[11/26 23:36:35 visual_prompt]: 	Training 400/553. train loss: 1.2932,	1.0752 s / batch. (data: 2.57e-01). ETA=12:25:59, max mem: 20.9 GB 
[11/26 23:38:13 visual_prompt]: 	Training 500/553. train loss: 9.2158,	1.4107 s / batch. (data: 5.54e-01). ETA=16:16:21, max mem: 20.9 GB 
[11/26 23:39:04 visual_prompt]: Epoch 25 / 100: avg data time: 1.47e-01, avg batch time: 0.9803, average train loss: 16.1054
[11/26 23:39:59 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.3103, average loss: 20.5659
[11/26 23:39:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 69.31	
[11/26 23:39:59 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/26 23:41:40 visual_prompt]: 	Training 100/553. train loss: 25.2308,	0.8360 s / batch. (data: 3.17e-04). ETA=9:36:28, max mem: 20.9 GB 
[11/26 23:43:18 visual_prompt]: 	Training 200/553. train loss: 77.3484,	1.6704 s / batch. (data: 8.35e-01). ETA=19:09:06, max mem: 20.9 GB 
[11/26 23:44:57 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8595 s / batch. (data: 5.43e-03). ETA=9:49:49, max mem: 20.9 GB 
[11/26 23:46:34 visual_prompt]: 	Training 400/553. train loss: 18.4817,	0.8370 s / batch. (data: 1.05e-02). ETA=9:32:58, max mem: 20.9 GB 
[11/26 23:48:09 visual_prompt]: 	Training 500/553. train loss: 1.8468,	0.8360 s / batch. (data: 3.00e-04). ETA=9:30:55, max mem: 20.9 GB 
[11/26 23:49:00 visual_prompt]: Epoch 26 / 100: avg data time: 1.45e-01, avg batch time: 0.9781, average train loss: 16.2934
[11/26 23:49:55 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3100, average loss: 30.2772
[11/26 23:49:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.28	
[11/26 23:49:55 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[11/26 23:51:38 visual_prompt]: 	Training 100/553. train loss: 0.0244,	0.8167 s / batch. (data: 3.16e-04). ETA=9:15:39, max mem: 20.9 GB 
[11/26 23:53:14 visual_prompt]: 	Training 200/553. train loss: 27.9886,	1.2280 s / batch. (data: 4.01e-01). ETA=13:53:26, max mem: 20.9 GB 
[11/26 23:54:52 visual_prompt]: 	Training 300/553. train loss: 10.9421,	0.8334 s / batch. (data: 3.06e-04). ETA=9:24:13, max mem: 20.9 GB 
[11/26 23:56:30 visual_prompt]: 	Training 400/553. train loss: 11.9090,	0.8370 s / batch. (data: 8.08e-04). ETA=9:25:16, max mem: 20.9 GB 
[11/26 23:58:08 visual_prompt]: 	Training 500/553. train loss: 27.1996,	0.8668 s / batch. (data: 1.05e-02). ETA=9:43:58, max mem: 20.9 GB 
[11/26 23:58:57 visual_prompt]: Epoch 27 / 100: avg data time: 1.45e-01, avg batch time: 0.9785, average train loss: 16.6082
[11/26 23:59:52 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3113, average loss: 9.7985
[11/26 23:59:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 70.27	
[11/26 23:59:52 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[11/27 00:01:31 visual_prompt]: 	Training 100/553. train loss: 10.9440,	0.8492 s / batch. (data: 1.56e-02). ETA=9:29:57, max mem: 20.9 GB 
[11/27 00:03:09 visual_prompt]: 	Training 200/553. train loss: 19.2374,	0.8288 s / batch. (data: 2.73e-04). ETA=9:14:52, max mem: 20.9 GB 
[11/27 00:04:47 visual_prompt]: 	Training 300/553. train loss: 10.8698,	1.1040 s / batch. (data: 2.60e-01). ETA=12:17:16, max mem: 20.9 GB 
[11/27 00:06:23 visual_prompt]: 	Training 400/553. train loss: 2.6526,	0.8452 s / batch. (data: 1.60e-02). ETA=9:23:02, max mem: 20.9 GB 
[11/27 00:07:59 visual_prompt]: 	Training 500/553. train loss: 4.4598,	0.8428 s / batch. (data: 7.20e-04). ETA=9:20:02, max mem: 20.9 GB 
[11/27 00:08:50 visual_prompt]: Epoch 28 / 100: avg data time: 1.40e-01, avg batch time: 0.9725, average train loss: 18.9422
[11/27 00:09:45 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3100, average loss: 4.2045
[11/27 00:09:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 70.05	
[11/27 00:09:45 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[11/27 00:11:31 visual_prompt]: 	Training 100/553. train loss: 7.5982,	0.8584 s / batch. (data: 1.44e-02). ETA=9:28:12, max mem: 20.9 GB 
[11/27 00:13:08 visual_prompt]: 	Training 200/553. train loss: 63.5579,	1.8079 s / batch. (data: 9.97e-01). ETA=19:53:43, max mem: 20.9 GB 
[11/27 00:14:43 visual_prompt]: 	Training 300/553. train loss: 7.7657,	0.8253 s / batch. (data: 2.95e-04). ETA=9:03:32, max mem: 20.9 GB 
[11/27 00:16:16 visual_prompt]: 	Training 400/553. train loss: 9.4884,	1.2800 s / batch. (data: 4.58e-01). ETA=14:00:50, max mem: 20.9 GB 
[11/27 00:17:53 visual_prompt]: 	Training 500/553. train loss: 23.7843,	0.8343 s / batch. (data: 5.44e-03). ETA=9:06:42, max mem: 20.9 GB 
[11/27 00:18:43 visual_prompt]: Epoch 29 / 100: avg data time: 1.41e-01, avg batch time: 0.9736, average train loss: 19.7971
[11/27 00:19:39 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.3100, average loss: 5.3655
[11/27 00:19:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 69.41	
[11/27 00:19:39 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[11/27 00:21:18 visual_prompt]: 	Training 100/553. train loss: 22.5831,	0.8640 s / batch. (data: 1.59e-02). ETA=9:23:55, max mem: 20.9 GB 
[11/27 00:22:57 visual_prompt]: 	Training 200/553. train loss: 40.8704,	0.8201 s / batch. (data: 3.19e-04). ETA=8:53:55, max mem: 20.9 GB 
[11/27 00:24:32 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8300 s / batch. (data: 5.44e-03). ETA=8:59:00, max mem: 20.9 GB 
[11/27 00:26:11 visual_prompt]: 	Training 400/553. train loss: 15.9673,	0.9850 s / batch. (data: 1.65e-01). ETA=10:38:00, max mem: 20.9 GB 
[11/27 00:27:47 visual_prompt]: 	Training 500/553. train loss: 6.7287,	1.0934 s / batch. (data: 2.62e-01). ETA=11:46:25, max mem: 20.9 GB 
[11/27 00:28:40 visual_prompt]: Epoch 30 / 100: avg data time: 1.45e-01, avg batch time: 0.9783, average train loss: 13.3866
[11/27 00:29:35 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3094, average loss: 33.6132
[11/27 00:29:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 72.45	
[11/27 00:29:35 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[11/27 00:31:18 visual_prompt]: 	Training 100/553. train loss: 7.6240,	0.8322 s / batch. (data: 3.19e-04). ETA=8:55:29, max mem: 20.9 GB 
[11/27 00:32:57 visual_prompt]: 	Training 200/553. train loss: 32.3795,	0.8440 s / batch. (data: 3.31e-04). ETA=9:01:43, max mem: 20.9 GB 
[11/27 00:34:32 visual_prompt]: 	Training 300/553. train loss: 14.8131,	0.8217 s / batch. (data: 2.99e-04). ETA=8:46:02, max mem: 20.9 GB 
[11/27 00:36:09 visual_prompt]: 	Training 400/553. train loss: 18.6171,	1.2957 s / batch. (data: 4.52e-01). ETA=13:47:17, max mem: 20.9 GB 
[11/27 00:37:46 visual_prompt]: 	Training 500/553. train loss: 23.6077,	0.8353 s / batch. (data: 3.24e-04). ETA=8:51:56, max mem: 20.9 GB 
[11/27 00:38:35 visual_prompt]: Epoch 31 / 100: avg data time: 1.43e-01, avg batch time: 0.9759, average train loss: 19.2744
[11/27 00:39:31 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3105, average loss: 20.5524
[11/27 00:39:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.05	
[11/27 00:39:31 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[11/27 00:41:13 visual_prompt]: 	Training 100/553. train loss: 0.0099,	0.8627 s / batch. (data: 1.05e-02). ETA=9:07:11, max mem: 20.9 GB 
[11/27 00:42:51 visual_prompt]: 	Training 200/553. train loss: 0.2769,	0.8369 s / batch. (data: 8.28e-04). ETA=8:49:27, max mem: 20.9 GB 
[11/27 00:44:30 visual_prompt]: 	Training 300/553. train loss: 20.3937,	1.1760 s / batch. (data: 3.27e-01). ETA=12:21:59, max mem: 20.9 GB 
[11/27 00:46:09 visual_prompt]: 	Training 400/553. train loss: 13.2891,	0.8594 s / batch. (data: 1.14e-02). ETA=9:00:48, max mem: 20.9 GB 
[11/27 00:47:43 visual_prompt]: 	Training 500/553. train loss: 7.3354,	0.8218 s / batch. (data: 3.00e-04). ETA=8:35:46, max mem: 20.9 GB 
[11/27 00:48:33 visual_prompt]: Epoch 32 / 100: avg data time: 1.47e-01, avg batch time: 0.9802, average train loss: 14.4753
[11/27 00:49:28 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3111, average loss: 6.4045
[11/27 00:49:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 70.50	
[11/27 00:49:28 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[11/27 00:51:08 visual_prompt]: 	Training 100/553. train loss: 0.0000,	1.1352 s / batch. (data: 3.05e-01). ETA=11:49:32, max mem: 20.9 GB 
[11/27 00:52:47 visual_prompt]: 	Training 200/553. train loss: 5.2247,	0.8737 s / batch. (data: 5.01e-02). ETA=9:04:39, max mem: 20.9 GB 
[11/27 00:54:23 visual_prompt]: 	Training 300/553. train loss: 0.2166,	0.8520 s / batch. (data: 2.75e-04). ETA=8:49:42, max mem: 20.9 GB 
[11/27 00:56:00 visual_prompt]: 	Training 400/553. train loss: 19.4482,	0.8362 s / batch. (data: 2.92e-04). ETA=8:38:31, max mem: 20.9 GB 
[11/27 00:57:36 visual_prompt]: 	Training 500/553. train loss: 16.2498,	0.8520 s / batch. (data: 5.45e-03). ETA=8:46:52, max mem: 20.9 GB 
[11/27 00:58:26 visual_prompt]: Epoch 33 / 100: avg data time: 1.40e-01, avg batch time: 0.9728, average train loss: 16.3991
[11/27 00:59:21 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3111, average loss: 7.3089
[11/27 00:59:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 71.01	
[11/27 00:59:21 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[11/27 01:01:03 visual_prompt]: 	Training 100/553. train loss: 6.8604,	0.8403 s / batch. (data: 1.77e-02). ETA=8:37:28, max mem: 20.9 GB 
[11/27 01:02:38 visual_prompt]: 	Training 200/553. train loss: 36.1142,	0.8360 s / batch. (data: 3.16e-04). ETA=8:33:28, max mem: 20.9 GB 
[11/27 01:04:14 visual_prompt]: 	Training 300/553. train loss: 9.7900,	0.8434 s / batch. (data: 7.95e-03). ETA=8:36:36, max mem: 20.9 GB 
[11/27 01:05:51 visual_prompt]: 	Training 400/553. train loss: 5.5626,	0.8680 s / batch. (data: 7.95e-03). ETA=8:50:13, max mem: 20.9 GB 
[11/27 01:07:29 visual_prompt]: 	Training 500/553. train loss: 2.1267,	1.3703 s / batch. (data: 5.38e-01). ETA=13:54:44, max mem: 20.9 GB 
[11/27 01:08:18 visual_prompt]: Epoch 34 / 100: avg data time: 1.38e-01, avg batch time: 0.9710, average train loss: 15.2596
[11/27 01:09:14 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3108, average loss: 10.1406
[11/27 01:09:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 70.73	
[11/27 01:09:14 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[11/27 01:10:56 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8400 s / batch. (data: 3.18e-04). ETA=8:29:33, max mem: 20.9 GB 
[11/27 01:12:33 visual_prompt]: 	Training 200/553. train loss: 10.2225,	0.8440 s / batch. (data: 3.23e-04). ETA=8:30:35, max mem: 20.9 GB 
[11/27 01:14:09 visual_prompt]: 	Training 300/553. train loss: 46.3032,	0.8320 s / batch. (data: 3.11e-04). ETA=8:21:55, max mem: 20.9 GB 
[11/27 01:15:46 visual_prompt]: 	Training 400/553. train loss: 7.1608,	0.8480 s / batch. (data: 1.53e-02). ETA=8:30:11, max mem: 20.9 GB 
[11/27 01:17:22 visual_prompt]: 	Training 500/553. train loss: 9.5867,	1.0080 s / batch. (data: 1.78e-01). ETA=10:04:46, max mem: 20.9 GB 
[11/27 01:18:14 visual_prompt]: Epoch 35 / 100: avg data time: 1.45e-01, avg batch time: 0.9767, average train loss: 18.8911
[11/27 01:19:09 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3097, average loss: 52.2399
[11/27 01:19:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 70.78	
[11/27 01:19:09 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[11/27 01:20:49 visual_prompt]: 	Training 100/553. train loss: 14.2369,	0.8473 s / batch. (data: 2.38e-02). ETA=8:26:11, max mem: 20.9 GB 
[11/27 01:22:27 visual_prompt]: 	Training 200/553. train loss: 21.4093,	0.8320 s / batch. (data: 3.22e-04). ETA=8:15:39, max mem: 20.9 GB 
[11/27 01:24:07 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8400 s / batch. (data: 3.12e-04). ETA=8:19:01, max mem: 20.9 GB 
[11/27 01:25:43 visual_prompt]: 	Training 400/553. train loss: 3.3295,	1.4800 s / batch. (data: 6.42e-01). ETA=14:36:45, max mem: 20.9 GB 
[11/27 01:27:21 visual_prompt]: 	Training 500/553. train loss: 7.8547,	1.1330 s / batch. (data: 2.81e-01). ETA=11:09:20, max mem: 20.9 GB 
[11/27 01:28:10 visual_prompt]: Epoch 36 / 100: avg data time: 1.44e-01, avg batch time: 0.9776, average train loss: 13.3187
[11/27 01:29:06 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3109, average loss: 3.2688
[11/27 01:29:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 70.25	
[11/27 01:29:06 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[11/27 01:30:47 visual_prompt]: 	Training 100/553. train loss: 37.2811,	0.8174 s / batch. (data: 3.35e-04). ETA=8:00:48, max mem: 20.9 GB 
[11/27 01:32:24 visual_prompt]: 	Training 200/553. train loss: 4.0412,	0.8201 s / batch. (data: 2.97e-04). ETA=8:01:00, max mem: 20.9 GB 
[11/27 01:34:02 visual_prompt]: 	Training 300/553. train loss: 25.0179,	1.5268 s / batch. (data: 7.09e-01). ETA=14:52:59, max mem: 20.9 GB 
[11/27 01:35:41 visual_prompt]: 	Training 400/553. train loss: 13.3034,	1.9360 s / batch. (data: 1.11e+00). ETA=18:49:05, max mem: 20.9 GB 
[11/27 01:37:15 visual_prompt]: 	Training 500/553. train loss: 0.8058,	1.0776 s / batch. (data: 2.42e-01). ETA=10:26:40, max mem: 20.9 GB 
[11/27 01:38:08 visual_prompt]: Epoch 37 / 100: avg data time: 1.47e-01, avg batch time: 0.9802, average train loss: 13.7613
[11/27 01:39:02 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3099, average loss: 5.6956
[11/27 01:39:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 70.76	
[11/27 01:39:02 visual_prompt]: Stopping early.
[11/27 01:39:02 visual_prompt]: Rank of current process: 0. World size: 1
[11/27 01:39:02 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/27 01:39:02 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/27 01:39:02 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/27 01:39:02 visual_prompt]: Training with config:
[11/27 01:39:02 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr5.0_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/27 01:39:02 visual_prompt]: Loading training data...
[11/27 01:39:02 visual_prompt]: Constructing mammo-cbis dataset train...
[11/27 01:39:02 visual_prompt]: Loading validation data...
[11/27 01:39:02 visual_prompt]: Constructing mammo-cbis dataset val...
[11/27 01:39:02 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/27 01:39:05 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/27 01:39:05 visual_prompt]: tuned percent:0.525
[11/27 01:39:05 visual_prompt]: Device used for model: 0
[11/27 01:39:05 visual_prompt]: Setting up Evaluator...
[11/27 01:39:05 visual_prompt]: Setting up Trainer...
[11/27 01:39:05 visual_prompt]: 	Setting up the optimizer...
[11/27 01:39:05 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/27 01:40:45 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8240 s / batch. (data: 3.05e-04). ETA=12:38:05, max mem: 20.9 GB 
[11/27 01:42:21 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8480 s / batch. (data: 3.17e-04). ETA=12:58:43, max mem: 20.9 GB 
[11/27 01:44:01 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8440 s / batch. (data: 1.19e-02). ETA=12:53:39, max mem: 20.9 GB 
[11/27 01:45:37 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8280 s / batch. (data: 2.54e-04). ETA=12:37:38, max mem: 20.9 GB 
[11/27 01:47:17 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8426 s / batch. (data: 7.85e-04). ETA=12:49:36, max mem: 20.9 GB 
[11/27 01:48:08 visual_prompt]: Epoch 1 / 100: avg data time: 1.47e-01, avg batch time: 0.9810, average train loss: 1.5403
[11/27 01:49:03 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3089, average loss: 1.5201
[11/27 01:49:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/27 01:49:03 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/27 01:50:44 visual_prompt]: 	Training 100/553. train loss: 0.7798,	0.8280 s / batch. (data: 2.84e-04). ETA=12:34:10, max mem: 20.9 GB 
[11/27 01:52:20 visual_prompt]: 	Training 200/553. train loss: 0.0007,	0.8215 s / batch. (data: 3.06e-04). ETA=12:26:52, max mem: 20.9 GB 
[11/27 01:53:58 visual_prompt]: 	Training 300/553. train loss: 0.8430,	0.9280 s / batch. (data: 1.06e-01). ETA=14:02:06, max mem: 20.9 GB 
[11/27 01:55:34 visual_prompt]: 	Training 400/553. train loss: 0.8061,	0.8480 s / batch. (data: 7.95e-03). ETA=12:48:06, max mem: 20.9 GB 
[11/27 01:57:12 visual_prompt]: 	Training 500/553. train loss: 0.6105,	0.8297 s / batch. (data: 1.05e-02). ETA=12:30:07, max mem: 20.9 GB 
[11/27 01:58:03 visual_prompt]: Epoch 2 / 100: avg data time: 1.41e-01, avg batch time: 0.9752, average train loss: 1.5226
[11/27 01:58:58 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3101, average loss: 0.8135
[11/27 01:58:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.50	
[11/27 01:58:58 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/27 02:00:38 visual_prompt]: 	Training 100/553. train loss: 3.1812,	0.8230 s / batch. (data: 5.48e-03). ETA=12:22:01, max mem: 20.9 GB 
[11/27 02:02:16 visual_prompt]: 	Training 200/553. train loss: 2.0711,	0.8188 s / batch. (data: 3.01e-04). ETA=12:16:49, max mem: 20.9 GB 
[11/27 02:03:52 visual_prompt]: 	Training 300/553. train loss: 0.9746,	0.8332 s / batch. (data: 5.39e-03). ETA=12:28:23, max mem: 20.9 GB 
[11/27 02:05:29 visual_prompt]: 	Training 400/553. train loss: 0.0115,	0.8465 s / batch. (data: 2.87e-04). ETA=12:38:59, max mem: 20.9 GB 
[11/27 02:07:08 visual_prompt]: 	Training 500/553. train loss: 4.7185,	1.0880 s / batch. (data: 2.41e-01). ETA=16:13:40, max mem: 20.9 GB 
[11/27 02:07:57 visual_prompt]: Epoch 3 / 100: avg data time: 1.40e-01, avg batch time: 0.9741, average train loss: 2.5513
[11/27 02:08:52 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3108, average loss: 7.7699
[11/27 02:08:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.54	
[11/27 02:08:52 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/27 02:10:34 visual_prompt]: 	Training 100/553. train loss: 8.1876,	0.8354 s / batch. (data: 1.19e-02). ETA=12:25:26, max mem: 20.9 GB 
[11/27 02:12:11 visual_prompt]: 	Training 200/553. train loss: 2.3320,	0.8391 s / batch. (data: 7.95e-03). ETA=12:27:22, max mem: 20.9 GB 
[11/27 02:13:49 visual_prompt]: 	Training 300/553. train loss: 1.5272,	1.1400 s / batch. (data: 3.02e-01). ETA=16:53:30, max mem: 20.9 GB 
[11/27 02:15:21 visual_prompt]: 	Training 400/553. train loss: 2.3384,	0.9962 s / batch. (data: 1.35e-01). ETA=14:43:57, max mem: 20.9 GB 
[11/27 02:17:01 visual_prompt]: 	Training 500/553. train loss: 15.5396,	3.2112 s / batch. (data: 2.39e+00). ETA=1 day, 23:24:05, max mem: 20.9 GB 
[11/27 02:17:52 visual_prompt]: Epoch 4 / 100: avg data time: 1.42e-01, avg batch time: 0.9763, average train loss: 4.5100
[11/27 02:18:48 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3110, average loss: 4.4083
[11/27 02:18:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.16	
[11/27 02:18:48 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/27 02:20:28 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8387 s / batch. (data: 1.05e-02). ETA=12:20:41, max mem: 20.9 GB 
[11/27 02:22:04 visual_prompt]: 	Training 200/553. train loss: 6.5708,	1.0846 s / batch. (data: 2.53e-01). ETA=15:55:59, max mem: 20.9 GB 
[11/27 02:23:41 visual_prompt]: 	Training 300/553. train loss: 5.6033,	0.8320 s / batch. (data: 3.02e-04). ETA=12:11:59, max mem: 20.9 GB 
[11/27 02:25:18 visual_prompt]: 	Training 400/553. train loss: 16.1834,	0.8440 s / batch. (data: 2.88e-04). ETA=12:21:09, max mem: 20.9 GB 
[11/27 02:26:55 visual_prompt]: 	Training 500/553. train loss: 1.6618,	0.8440 s / batch. (data: 2.19e-02). ETA=12:19:45, max mem: 20.9 GB 
[11/27 02:27:46 visual_prompt]: Epoch 5 / 100: avg data time: 1.41e-01, avg batch time: 0.9742, average train loss: 5.5591
[11/27 02:28:42 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3098, average loss: 6.7012
[11/27 02:28:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.04	
[11/27 02:28:42 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/27 02:30:24 visual_prompt]: 	Training 100/553. train loss: 14.3782,	0.8440 s / batch. (data: 7.68e-04). ETA=12:17:36, max mem: 20.9 GB 
[11/27 02:32:00 visual_prompt]: 	Training 200/553. train loss: 7.5807,	0.8480 s / batch. (data: 3.40e-04). ETA=12:19:38, max mem: 20.9 GB 
[11/27 02:33:35 visual_prompt]: 	Training 300/553. train loss: 8.5515,	0.8351 s / batch. (data: 3.28e-04). ETA=12:07:02, max mem: 20.9 GB 
[11/27 02:35:17 visual_prompt]: 	Training 400/553. train loss: 6.8434,	0.8320 s / batch. (data: 3.57e-04). ETA=12:02:55, max mem: 20.9 GB 
[11/27 02:36:52 visual_prompt]: 	Training 500/553. train loss: 6.3768,	0.8520 s / batch. (data: 3.49e-04). ETA=12:18:53, max mem: 20.9 GB 
[11/27 02:37:43 visual_prompt]: Epoch 6 / 100: avg data time: 1.44e-01, avg batch time: 0.9778, average train loss: 7.4405
[11/27 02:38:38 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3099, average loss: 0.7440
[11/27 02:38:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.26	
[11/27 02:38:38 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/27 02:40:19 visual_prompt]: 	Training 100/553. train loss: 4.6580,	0.8362 s / batch. (data: 7.95e-03). ETA=12:03:05, max mem: 20.9 GB 
[11/27 02:41:56 visual_prompt]: 	Training 200/553. train loss: 6.2999,	0.8281 s / batch. (data: 3.08e-04). ETA=11:54:39, max mem: 20.9 GB 
[11/27 02:43:37 visual_prompt]: 	Training 300/553. train loss: 1.2756,	1.8372 s / batch. (data: 1.01e+00). ETA=1 day, 2:22:28, max mem: 20.9 GB 
[11/27 02:45:14 visual_prompt]: 	Training 400/553. train loss: 2.8603,	1.6547 s / batch. (data: 8.35e-01). ETA=23:42:32, max mem: 20.9 GB 
[11/27 02:46:50 visual_prompt]: 	Training 500/553. train loss: 7.7095,	0.8481 s / batch. (data: 3.18e-04). ETA=12:07:40, max mem: 20.9 GB 
[11/27 02:47:39 visual_prompt]: Epoch 7 / 100: avg data time: 1.43e-01, avg batch time: 0.9770, average train loss: 8.8655
[11/27 02:48:34 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3097, average loss: 0.8059
[11/27 02:48:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.19	
[11/27 02:48:34 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/27 02:50:14 visual_prompt]: 	Training 100/553. train loss: 5.4048,	0.8800 s / batch. (data: 3.15e-04). ETA=12:32:50, max mem: 20.9 GB 
[11/27 02:51:52 visual_prompt]: 	Training 200/553. train loss: 13.6669,	0.8373 s / batch. (data: 2.92e-04). ETA=11:54:56, max mem: 20.9 GB 
[11/27 02:53:29 visual_prompt]: 	Training 300/553. train loss: 3.8111,	0.8400 s / batch. (data: 2.89e-04). ETA=11:55:49, max mem: 20.9 GB 
[11/27 02:55:07 visual_prompt]: 	Training 400/553. train loss: 13.1761,	0.8365 s / batch. (data: 8.73e-03). ETA=11:51:27, max mem: 20.9 GB 
[11/27 02:56:43 visual_prompt]: 	Training 500/553. train loss: 58.6027,	1.3623 s / batch. (data: 5.34e-01). ETA=19:16:19, max mem: 20.9 GB 
[11/27 02:57:34 visual_prompt]: Epoch 8 / 100: avg data time: 1.41e-01, avg batch time: 0.9757, average train loss: 10.5169
[11/27 02:58:30 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3113, average loss: 7.4940
[11/27 02:58:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.82	
[11/27 02:58:30 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/27 03:00:10 visual_prompt]: 	Training 100/553. train loss: 11.2248,	0.8818 s / batch. (data: 2.58e-02). ETA=12:26:16, max mem: 20.9 GB 
[11/27 03:01:46 visual_prompt]: 	Training 200/553. train loss: 17.2262,	0.8314 s / batch. (data: 7.96e-03). ETA=11:42:09, max mem: 20.9 GB 
[11/27 03:03:24 visual_prompt]: 	Training 300/553. train loss: 1.2406,	1.4097 s / batch. (data: 5.70e-01). ETA=19:48:17, max mem: 20.9 GB 
[11/27 03:05:02 visual_prompt]: 	Training 400/553. train loss: 14.1322,	0.8264 s / batch. (data: 7.48e-04). ETA=11:35:15, max mem: 20.9 GB 
[11/27 03:06:39 visual_prompt]: 	Training 500/553. train loss: 28.9069,	0.8885 s / batch. (data: 5.56e-02). ETA=12:26:01, max mem: 20.9 GB 
[11/27 03:07:29 visual_prompt]: Epoch 9 / 100: avg data time: 1.41e-01, avg batch time: 0.9746, average train loss: 14.5256
[11/27 03:08:24 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3105, average loss: 7.8257
[11/27 03:08:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.12	
[11/27 03:08:24 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/27 03:10:08 visual_prompt]: 	Training 100/553. train loss: 14.5590,	0.8560 s / batch. (data: 3.09e-04). ETA=11:56:29, max mem: 20.9 GB 
[11/27 03:11:43 visual_prompt]: 	Training 200/553. train loss: 1.0110,	0.8517 s / batch. (data: 1.04e-02). ETA=11:51:29, max mem: 20.9 GB 
[11/27 03:13:19 visual_prompt]: 	Training 300/553. train loss: 35.9825,	1.5680 s / batch. (data: 7.41e-01). ETA=21:47:17, max mem: 20.9 GB 
[11/27 03:14:55 visual_prompt]: 	Training 400/553. train loss: 25.0067,	0.8459 s / batch. (data: 5.56e-03). ETA=11:43:48, max mem: 20.9 GB 
[11/27 03:16:33 visual_prompt]: 	Training 500/553. train loss: 3.2665,	0.8360 s / batch. (data: 3.58e-04). ETA=11:34:11, max mem: 20.9 GB 
[11/27 03:17:24 visual_prompt]: Epoch 10 / 100: avg data time: 1.42e-01, avg batch time: 0.9762, average train loss: 15.3921
[11/27 03:18:19 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3112, average loss: 8.6444
[11/27 03:18:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.01	
[11/27 03:18:19 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/27 03:20:02 visual_prompt]: 	Training 100/553. train loss: 2.8445,	0.8326 s / batch. (data: 8.51e-03). ETA=11:29:13, max mem: 20.9 GB 
[11/27 03:21:41 visual_prompt]: 	Training 200/553. train loss: 48.9576,	0.8199 s / batch. (data: 4.67e-04). ETA=11:17:24, max mem: 20.9 GB 
[11/27 03:23:18 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.6855 s / batch. (data: 8.51e-01). ETA=23:09:43, max mem: 20.9 GB 
[11/27 03:24:54 visual_prompt]: 	Training 400/553. train loss: 2.8517,	0.8480 s / batch. (data: 2.96e-04). ETA=11:37:45, max mem: 20.9 GB 
[11/27 03:26:29 visual_prompt]: 	Training 500/553. train loss: 48.7725,	0.8175 s / batch. (data: 3.85e-03). ETA=11:11:17, max mem: 20.9 GB 
[11/27 03:27:21 visual_prompt]: Epoch 11 / 100: avg data time: 1.45e-01, avg batch time: 0.9783, average train loss: 15.8937
[11/27 03:28:16 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3104, average loss: 2.6196
[11/27 03:28:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.89	
[11/27 03:28:16 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/27 03:29:59 visual_prompt]: 	Training 100/553. train loss: 16.8725,	0.8792 s / batch. (data: 5.99e-02). ETA=11:59:43, max mem: 20.9 GB 
[11/27 03:31:37 visual_prompt]: 	Training 200/553. train loss: 10.2299,	1.1240 s / batch. (data: 3.00e-01). ETA=15:18:16, max mem: 20.9 GB 
[11/27 03:33:13 visual_prompt]: 	Training 300/553. train loss: 23.4843,	0.8407 s / batch. (data: 2.92e-04). ETA=11:25:22, max mem: 20.9 GB 
[11/27 03:34:52 visual_prompt]: 	Training 400/553. train loss: 21.7349,	0.8566 s / batch. (data: 9.41e-03). ETA=11:36:58, max mem: 20.9 GB 
[11/27 03:36:29 visual_prompt]: 	Training 500/553. train loss: 6.1085,	0.8520 s / batch. (data: 3.36e-04). ETA=11:31:47, max mem: 20.9 GB 
[11/27 03:37:19 visual_prompt]: Epoch 12 / 100: avg data time: 1.47e-01, avg batch time: 0.9809, average train loss: 15.7183
[11/27 03:38:15 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3106, average loss: 9.6142
[11/27 03:38:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.14	
[11/27 03:38:15 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/27 03:39:58 visual_prompt]: 	Training 100/553. train loss: 5.4204,	0.8565 s / batch. (data: 2.04e-02). ETA=11:33:13, max mem: 20.9 GB 
[11/27 03:41:31 visual_prompt]: 	Training 200/553. train loss: 18.7901,	0.8705 s / batch. (data: 2.25e-02). ETA=11:43:07, max mem: 20.9 GB 
[11/27 03:43:10 visual_prompt]: 	Training 300/553. train loss: 4.6744,	1.6965 s / batch. (data: 8.45e-01). ETA=22:47:29, max mem: 20.9 GB 
[11/27 03:44:47 visual_prompt]: 	Training 400/553. train loss: 22.9821,	0.8477 s / batch. (data: 7.95e-03). ETA=11:21:53, max mem: 20.9 GB 
[11/27 03:46:25 visual_prompt]: 	Training 500/553. train loss: 55.6965,	0.8579 s / batch. (data: 9.73e-03). ETA=11:28:37, max mem: 20.9 GB 
[11/27 03:47:17 visual_prompt]: Epoch 13 / 100: avg data time: 1.47e-01, avg batch time: 0.9798, average train loss: 18.4516
[11/27 03:48:12 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3102, average loss: 39.4750
[11/27 03:48:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.53	
[11/27 03:48:12 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/27 03:49:55 visual_prompt]: 	Training 100/553. train loss: 7.5923,	0.8400 s / batch. (data: 3.52e-04). ETA=11:12:08, max mem: 20.9 GB 
[11/27 03:51:32 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.9800 s / batch. (data: 1.54e-01). ETA=13:02:32, max mem: 20.9 GB 
[11/27 03:53:08 visual_prompt]: 	Training 300/553. train loss: 13.0603,	0.8318 s / batch. (data: 1.08e-02). ETA=11:02:48, max mem: 20.9 GB 
[11/27 03:54:45 visual_prompt]: 	Training 400/553. train loss: 15.3959,	0.8445 s / batch. (data: 1.19e-02). ETA=11:11:33, max mem: 20.9 GB 
[11/27 03:56:22 visual_prompt]: 	Training 500/553. train loss: 9.6030,	0.8259 s / batch. (data: 2.99e-04). ETA=10:55:21, max mem: 20.9 GB 
[11/27 03:57:12 visual_prompt]: Epoch 14 / 100: avg data time: 1.43e-01, avg batch time: 0.9764, average train loss: 17.0693
[11/27 03:58:08 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3089, average loss: 30.5229
[11/27 03:58:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.87	
[11/27 03:58:08 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/27 03:59:49 visual_prompt]: 	Training 100/553. train loss: 9.4049,	0.8417 s / batch. (data: 3.17e-04). ETA=11:05:43, max mem: 20.9 GB 
[11/27 04:01:25 visual_prompt]: 	Training 200/553. train loss: 97.4315,	0.8274 s / batch. (data: 3.02e-04). ETA=10:53:04, max mem: 20.9 GB 
[11/27 04:03:05 visual_prompt]: 	Training 300/553. train loss: 53.9615,	0.8569 s / batch. (data: 3.45e-04). ETA=11:14:54, max mem: 20.9 GB 
[11/27 04:04:40 visual_prompt]: 	Training 400/553. train loss: 5.0474,	0.8628 s / batch. (data: 1.56e-02). ETA=11:18:08, max mem: 20.9 GB 
[11/27 04:06:19 visual_prompt]: 	Training 500/553. train loss: 2.5343,	0.8358 s / batch. (data: 7.95e-03). ETA=10:55:30, max mem: 20.9 GB 
[11/27 04:07:09 visual_prompt]: Epoch 15 / 100: avg data time: 1.46e-01, avg batch time: 0.9793, average train loss: 15.8687
[11/27 04:08:05 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3106, average loss: 5.2078
[11/27 04:08:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.60	
[11/27 04:08:05 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/27 04:09:44 visual_prompt]: 	Training 100/553. train loss: 2.0542,	0.8298 s / batch. (data: 3.10e-04). ETA=10:48:42, max mem: 20.9 GB 
[11/27 04:11:22 visual_prompt]: 	Training 200/553. train loss: 7.6370,	0.8493 s / batch. (data: 1.55e-02). ETA=11:02:31, max mem: 20.9 GB 
[11/27 04:13:00 visual_prompt]: 	Training 300/553. train loss: 33.3452,	0.8449 s / batch. (data: 7.98e-03). ETA=10:57:41, max mem: 20.9 GB 
[11/27 04:14:37 visual_prompt]: 	Training 400/553. train loss: 29.9088,	0.8240 s / batch. (data: 3.44e-04). ETA=10:40:02, max mem: 20.9 GB 
[11/27 04:16:13 visual_prompt]: 	Training 500/553. train loss: 51.7052,	1.4320 s / batch. (data: 5.99e-01). ETA=18:29:53, max mem: 20.9 GB 
[11/27 04:17:05 visual_prompt]: Epoch 16 / 100: avg data time: 1.43e-01, avg batch time: 0.9763, average train loss: 16.8948
[11/27 04:18:00 visual_prompt]: Inference (val):avg data time: 1.60e-04, avg batch time: 0.3103, average loss: 15.7914
[11/27 04:18:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.27	
[11/27 04:18:00 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/27 04:19:40 visual_prompt]: 	Training 100/553. train loss: 4.7427,	0.8520 s / batch. (data: 1.19e-02). ETA=10:58:10, max mem: 20.9 GB 
[11/27 04:21:19 visual_prompt]: 	Training 200/553. train loss: 38.9712,	0.8185 s / batch. (data: 3.04e-04). ETA=10:30:54, max mem: 20.9 GB 
[11/27 04:22:56 visual_prompt]: 	Training 300/553. train loss: 31.7634,	0.8329 s / batch. (data: 2.92e-04). ETA=10:40:37, max mem: 20.9 GB 
[11/27 04:24:33 visual_prompt]: 	Training 400/553. train loss: 12.3765,	1.1040 s / batch. (data: 2.77e-01). ETA=14:07:21, max mem: 20.9 GB 
[11/27 04:26:10 visual_prompt]: 	Training 500/553. train loss: 48.7165,	1.5800 s / batch. (data: 7.16e-01). ETA=20:10:03, max mem: 20.9 GB 
[11/27 04:27:02 visual_prompt]: Epoch 17 / 100: avg data time: 1.45e-01, avg batch time: 0.9795, average train loss: 15.6109
[11/27 04:27:58 visual_prompt]: Inference (val):avg data time: 1.99e-04, avg batch time: 0.3094, average loss: 55.4827
[11/27 04:27:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.53	
[11/27 04:27:58 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/27 04:29:39 visual_prompt]: 	Training 100/553. train loss: 9.5563,	0.8360 s / batch. (data: 3.20e-04). ETA=10:38:06, max mem: 20.9 GB 
[11/27 04:31:19 visual_prompt]: 	Training 200/553. train loss: 3.4535,	0.8512 s / batch. (data: 1.04e-02). ETA=10:48:17, max mem: 20.9 GB 
[11/27 04:32:56 visual_prompt]: 	Training 300/553. train loss: 1.8644,	0.8362 s / batch. (data: 3.51e-03). ETA=10:35:29, max mem: 20.9 GB 
[11/27 04:34:33 visual_prompt]: 	Training 400/553. train loss: 1.3397,	0.8248 s / batch. (data: 5.46e-03). ETA=10:25:29, max mem: 20.9 GB 
[11/27 04:36:10 visual_prompt]: 	Training 500/553. train loss: 4.7031,	0.8332 s / batch. (data: 3.03e-04). ETA=10:30:28, max mem: 20.9 GB 
[11/27 04:37:00 visual_prompt]: Epoch 18 / 100: avg data time: 1.46e-01, avg batch time: 0.9800, average train loss: 14.6306
[11/27 04:37:55 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3106, average loss: 12.1063
[11/27 04:37:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.99	
[11/27 04:37:55 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/27 04:39:36 visual_prompt]: 	Training 100/553. train loss: 19.3821,	1.1885 s / batch. (data: 3.59e-01). ETA=14:56:16, max mem: 20.9 GB 
[11/27 04:41:15 visual_prompt]: 	Training 200/553. train loss: 1.5416,	0.8695 s / batch. (data: 3.74e-02). ETA=10:54:14, max mem: 20.9 GB 
[11/27 04:42:52 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8188 s / batch. (data: 3.39e-04). ETA=10:14:42, max mem: 20.9 GB 
[11/27 04:44:31 visual_prompt]: 	Training 400/553. train loss: 3.8163,	0.8444 s / batch. (data: 7.94e-04). ETA=10:32:34, max mem: 20.9 GB 
[11/27 04:46:04 visual_prompt]: 	Training 500/553. train loss: 1.2225,	0.8355 s / batch. (data: 5.47e-03). ETA=10:24:28, max mem: 20.9 GB 
[11/27 04:46:55 visual_prompt]: Epoch 19 / 100: avg data time: 1.42e-01, avg batch time: 0.9763, average train loss: 17.8823
[11/27 04:47:51 visual_prompt]: Inference (val):avg data time: 3.70e-04, avg batch time: 0.3112, average loss: 38.2857
[11/27 04:47:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.57	
[11/27 04:47:51 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/27 04:49:31 visual_prompt]: 	Training 100/553. train loss: 8.3854,	0.8320 s / batch. (data: 3.21e-04). ETA=10:19:43, max mem: 20.9 GB 
[11/27 04:51:10 visual_prompt]: 	Training 200/553. train loss: 15.4685,	0.8320 s / batch. (data: 7.95e-03). ETA=10:18:20, max mem: 20.9 GB 
[11/27 04:52:48 visual_prompt]: 	Training 300/553. train loss: 7.9870,	0.8280 s / batch. (data: 2.96e-04). ETA=10:14:00, max mem: 20.9 GB 
[11/27 04:54:25 visual_prompt]: 	Training 400/553. train loss: 4.9703,	0.8360 s / batch. (data: 3.01e-04). ETA=10:18:32, max mem: 20.9 GB 
[11/27 04:56:02 visual_prompt]: 	Training 500/553. train loss: 7.5007,	0.8560 s / batch. (data: 2.98e-04). ETA=10:31:54, max mem: 20.9 GB 
[11/27 04:56:55 visual_prompt]: Epoch 20 / 100: avg data time: 1.49e-01, avg batch time: 0.9824, average train loss: 15.1892
[11/27 04:57:50 visual_prompt]: Inference (val):avg data time: 2.13e-04, avg batch time: 0.3087, average loss: 4.2038
[11/27 04:57:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.69	
[11/27 04:57:50 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[11/27 04:59:33 visual_prompt]: 	Training 100/553. train loss: 34.4833,	0.8884 s / batch. (data: 6.22e-02). ETA=10:53:35, max mem: 20.9 GB 
[11/27 05:01:10 visual_prompt]: 	Training 200/553. train loss: 46.2701,	0.8366 s / batch. (data: 2.93e-04). ETA=10:14:04, max mem: 20.9 GB 
[11/27 05:02:46 visual_prompt]: 	Training 300/553. train loss: 20.9474,	1.0880 s / batch. (data: 2.53e-01). ETA=13:16:47, max mem: 20.9 GB 
[11/27 05:04:22 visual_prompt]: 	Training 400/553. train loss: 60.0103,	0.8327 s / batch. (data: 7.95e-03). ETA=10:08:23, max mem: 20.9 GB 
[11/27 05:06:00 visual_prompt]: 	Training 500/553. train loss: 14.5543,	0.8360 s / batch. (data: 3.12e-04). ETA=10:09:27, max mem: 20.9 GB 
[11/27 05:06:50 visual_prompt]: Epoch 21 / 100: avg data time: 1.42e-01, avg batch time: 0.9760, average train loss: 16.8275
[11/27 05:07:45 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3106, average loss: 16.6812
[11/27 05:07:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.55	
[11/27 05:07:45 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[11/27 05:09:25 visual_prompt]: 	Training 100/553. train loss: 5.9140,	0.8247 s / batch. (data: 4.13e-03). ETA=9:59:05, max mem: 20.9 GB 
[11/27 05:11:03 visual_prompt]: 	Training 200/553. train loss: 5.5664,	0.8395 s / batch. (data: 1.20e-02). ETA=10:08:27, max mem: 20.9 GB 
[11/27 05:12:37 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8272 s / batch. (data: 3.10e-04). ETA=9:58:11, max mem: 20.9 GB 
[11/27 05:14:15 visual_prompt]: 	Training 400/553. train loss: 9.2249,	0.8492 s / batch. (data: 1.60e-02). ETA=10:12:40, max mem: 20.9 GB 
[11/27 05:15:53 visual_prompt]: 	Training 500/553. train loss: 17.4958,	0.8347 s / batch. (data: 3.34e-04). ETA=10:00:46, max mem: 20.9 GB 
[11/27 05:16:44 visual_prompt]: Epoch 22 / 100: avg data time: 1.42e-01, avg batch time: 0.9746, average train loss: 14.0558
[11/27 05:17:40 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3104, average loss: 25.2010
[11/27 05:17:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.91	
[11/27 05:17:40 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[11/27 05:19:23 visual_prompt]: 	Training 100/553. train loss: 2.0199,	0.8240 s / batch. (data: 3.15e-04). ETA=9:51:00, max mem: 20.9 GB 
[11/27 05:21:00 visual_prompt]: 	Training 200/553. train loss: 18.6884,	0.8308 s / batch. (data: 1.21e-02). ETA=9:54:29, max mem: 20.9 GB 
[11/27 05:22:40 visual_prompt]: 	Training 300/553. train loss: 11.0697,	0.8432 s / batch. (data: 5.44e-03). ETA=10:01:59, max mem: 20.9 GB 
[11/27 05:24:16 visual_prompt]: 	Training 400/553. train loss: 3.5827,	0.8192 s / batch. (data: 2.84e-04). ETA=9:43:26, max mem: 20.9 GB 
[11/27 05:25:51 visual_prompt]: 	Training 500/553. train loss: 3.3811,	0.8559 s / batch. (data: 2.06e-02). ETA=10:08:09, max mem: 20.9 GB 
[11/27 05:26:42 visual_prompt]: Epoch 23 / 100: avg data time: 1.48e-01, avg batch time: 0.9810, average train loss: 15.2128
[11/27 05:27:38 visual_prompt]: Inference (val):avg data time: 2.65e-04, avg batch time: 0.3116, average loss: 10.3884
[11/27 05:27:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.68	
[11/27 05:27:38 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[11/27 05:29:16 visual_prompt]: 	Training 100/553. train loss: 5.0066,	0.8249 s / batch. (data: 7.90e-03). ETA=9:44:00, max mem: 20.9 GB 
[11/27 05:30:52 visual_prompt]: 	Training 200/553. train loss: 35.4084,	0.8289 s / batch. (data: 5.39e-03). ETA=9:45:27, max mem: 20.9 GB 
[11/27 05:32:30 visual_prompt]: 	Training 300/553. train loss: 17.1040,	0.8326 s / batch. (data: 3.08e-04). ETA=9:46:43, max mem: 20.9 GB 
[11/27 05:34:07 visual_prompt]: 	Training 400/553. train loss: 8.9767,	0.8500 s / batch. (data: 3.21e-04). ETA=9:57:35, max mem: 20.9 GB 
[11/27 05:35:46 visual_prompt]: 	Training 500/553. train loss: 4.3326,	0.8560 s / batch. (data: 1.20e-02). ETA=10:00:20, max mem: 20.9 GB 
[11/27 05:36:37 visual_prompt]: Epoch 24 / 100: avg data time: 1.42e-01, avg batch time: 0.9756, average train loss: 16.5487
[11/27 05:37:33 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3106, average loss: 10.6571
[11/27 05:37:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.46	
[11/27 05:37:33 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[11/27 05:39:16 visual_prompt]: 	Training 100/553. train loss: 18.5648,	0.8312 s / batch. (data: 3.53e-04). ETA=9:40:50, max mem: 20.9 GB 
[11/27 05:40:48 visual_prompt]: 	Training 200/553. train loss: 16.5736,	0.9695 s / batch. (data: 1.44e-01). ETA=11:15:51, max mem: 20.9 GB 
[11/27 05:42:24 visual_prompt]: 	Training 300/553. train loss: 13.8536,	0.8364 s / batch. (data: 5.47e-03). ETA=9:41:43, max mem: 20.9 GB 
[11/27 05:44:01 visual_prompt]: 	Training 400/553. train loss: 37.6028,	1.1735 s / batch. (data: 3.57e-01). ETA=13:34:09, max mem: 20.9 GB 
[11/27 05:45:39 visual_prompt]: 	Training 500/553. train loss: 9.4053,	1.3135 s / batch. (data: 4.85e-01). ETA=15:09:08, max mem: 20.9 GB 
[11/27 05:46:29 visual_prompt]: Epoch 25 / 100: avg data time: 1.37e-01, avg batch time: 0.9707, average train loss: 14.4637
[11/27 05:47:25 visual_prompt]: Inference (val):avg data time: 4.40e-05, avg batch time: 0.3097, average loss: 13.4995
[11/27 05:47:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.05	
[11/27 05:47:25 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[11/27 05:49:06 visual_prompt]: 	Training 100/553. train loss: 36.1180,	0.8316 s / batch. (data: 3.05e-04). ETA=9:33:26, max mem: 20.9 GB 
[11/27 05:50:45 visual_prompt]: 	Training 200/553. train loss: 11.8235,	1.7003 s / batch. (data: 8.64e-01). ETA=19:29:40, max mem: 20.9 GB 
[11/27 05:52:24 visual_prompt]: 	Training 300/553. train loss: 0.1046,	0.8621 s / batch. (data: 6.98e-04). ETA=9:51:35, max mem: 20.9 GB 
[11/27 05:54:01 visual_prompt]: 	Training 400/553. train loss: 7.7271,	0.8400 s / batch. (data: 2.92e-04). ETA=9:35:02, max mem: 20.9 GB 
[11/27 05:55:36 visual_prompt]: 	Training 500/553. train loss: 35.4509,	0.8448 s / batch. (data: 3.20e-04). ETA=9:36:56, max mem: 20.9 GB 
[11/27 05:56:26 visual_prompt]: Epoch 26 / 100: avg data time: 1.46e-01, avg batch time: 0.9788, average train loss: 16.9598
[11/27 05:57:22 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3110, average loss: 6.9093
[11/27 05:57:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.78	
[11/27 05:57:22 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[11/27 05:59:05 visual_prompt]: 	Training 100/553. train loss: 14.0180,	0.8608 s / batch. (data: 9.76e-03). ETA=9:45:38, max mem: 20.9 GB 
[11/27 06:00:42 visual_prompt]: 	Training 200/553. train loss: 2.9926,	0.9879 s / batch. (data: 1.59e-01). ETA=11:10:31, max mem: 20.9 GB 
[11/27 06:02:19 visual_prompt]: 	Training 300/553. train loss: 3.9115,	0.8280 s / batch. (data: 3.36e-04). ETA=9:20:34, max mem: 20.9 GB 
[11/27 06:03:58 visual_prompt]: 	Training 400/553. train loss: 6.8911,	0.8199 s / batch. (data: 3.17e-04). ETA=9:13:44, max mem: 20.9 GB 
[11/27 06:05:36 visual_prompt]: 	Training 500/553. train loss: 5.6276,	0.8362 s / batch. (data: 7.93e-04). ETA=9:23:20, max mem: 20.9 GB 
[11/27 06:06:24 visual_prompt]: Epoch 27 / 100: avg data time: 1.47e-01, avg batch time: 0.9808, average train loss: 15.7101
[11/27 06:07:20 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3102, average loss: 21.7686
[11/27 06:07:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.08	
[11/27 06:07:20 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[11/27 06:09:00 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8320 s / batch. (data: 7.95e-03). ETA=9:18:23, max mem: 20.9 GB 
[11/27 06:10:38 visual_prompt]: 	Training 200/553. train loss: 5.6202,	0.8491 s / batch. (data: 1.19e-02). ETA=9:28:28, max mem: 20.9 GB 
[11/27 06:12:16 visual_prompt]: 	Training 300/553. train loss: 5.2136,	1.4679 s / batch. (data: 6.24e-01). ETA=16:20:17, max mem: 20.9 GB 
[11/27 06:13:51 visual_prompt]: 	Training 400/553. train loss: 2.7923,	0.8302 s / batch. (data: 5.41e-03). ETA=9:13:03, max mem: 20.9 GB 
[11/27 06:15:27 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8200 s / batch. (data: 3.13e-04). ETA=9:04:54, max mem: 20.9 GB 
[11/27 06:16:19 visual_prompt]: Epoch 28 / 100: avg data time: 1.40e-01, avg batch time: 0.9743, average train loss: 13.6394
[11/27 06:17:14 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3096, average loss: 7.5203
[11/27 06:17:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.00	
[11/27 06:17:14 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[11/27 06:19:01 visual_prompt]: 	Training 100/553. train loss: 22.3982,	0.8213 s / batch. (data: 3.03e-04). ETA=9:03:37, max mem: 20.9 GB 
[11/27 06:20:37 visual_prompt]: 	Training 200/553. train loss: 2.4717,	1.7120 s / batch. (data: 8.73e-01). ETA=18:50:22, max mem: 20.9 GB 
[11/27 06:22:12 visual_prompt]: 	Training 300/553. train loss: 3.4475,	0.8630 s / batch. (data: 1.09e-02). ETA=9:28:22, max mem: 20.9 GB 
[11/27 06:23:46 visual_prompt]: 	Training 400/553. train loss: 17.5801,	1.2057 s / batch. (data: 3.65e-01). ETA=13:12:02, max mem: 20.9 GB 
[11/27 06:25:24 visual_prompt]: 	Training 500/553. train loss: 6.5623,	0.8200 s / batch. (data: 3.33e-04). ETA=8:57:18, max mem: 20.9 GB 
[11/27 06:26:14 visual_prompt]: Epoch 29 / 100: avg data time: 1.42e-01, avg batch time: 0.9761, average train loss: 15.3266
[11/27 06:27:09 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3107, average loss: 16.0214
[11/27 06:27:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.26	
[11/27 06:27:09 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[11/27 06:28:49 visual_prompt]: 	Training 100/553. train loss: 25.7991,	0.8315 s / batch. (data: 5.43e-03). ETA=9:02:45, max mem: 20.9 GB 
[11/27 06:30:27 visual_prompt]: 	Training 200/553. train loss: 70.0805,	0.8476 s / batch. (data: 2.24e-02). ETA=9:11:50, max mem: 20.9 GB 
[11/27 06:32:03 visual_prompt]: 	Training 300/553. train loss: 0.0962,	1.9277 s / batch. (data: 1.10e+00). ETA=20:51:50, max mem: 20.9 GB 
[11/27 06:33:41 visual_prompt]: 	Training 400/553. train loss: 31.9951,	1.0216 s / batch. (data: 1.69e-01). ETA=11:01:44, max mem: 20.9 GB 
[11/27 06:35:16 visual_prompt]: 	Training 500/553. train loss: 27.1111,	1.2040 s / batch. (data: 3.64e-01). ETA=12:57:50, max mem: 20.9 GB 
[11/27 06:36:09 visual_prompt]: Epoch 30 / 100: avg data time: 1.42e-01, avg batch time: 0.9753, average train loss: 15.6942
[11/27 06:37:05 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3096, average loss: 13.3111
[11/27 06:37:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.03	
[11/27 06:37:05 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[11/27 06:38:48 visual_prompt]: 	Training 100/553. train loss: 11.2105,	0.8407 s / batch. (data: 5.41e-03). ETA=9:00:58, max mem: 20.9 GB 
[11/27 06:40:27 visual_prompt]: 	Training 200/553. train loss: 24.2543,	0.8680 s / batch. (data: 1.19e-02). ETA=9:17:07, max mem: 20.9 GB 
[11/27 06:42:02 visual_prompt]: 	Training 300/553. train loss: 19.0948,	0.8520 s / batch. (data: 5.79e-03). ETA=9:05:25, max mem: 20.9 GB 
[11/27 06:43:39 visual_prompt]: 	Training 400/553. train loss: 37.6698,	1.2385 s / batch. (data: 4.15e-01). ETA=13:10:46, max mem: 20.9 GB 
[11/27 06:45:17 visual_prompt]: 	Training 500/553. train loss: 2.2036,	0.8440 s / batch. (data: 3.19e-04). ETA=8:57:28, max mem: 20.9 GB 
[11/27 06:46:07 visual_prompt]: Epoch 31 / 100: avg data time: 1.47e-01, avg batch time: 0.9804, average train loss: 14.0625
[11/27 06:47:03 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3117, average loss: 1.2447
[11/27 06:47:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.04	
[11/27 06:47:03 visual_prompt]: Best epoch 31: best metric: -1.245
[11/27 06:47:03 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[11/27 06:48:45 visual_prompt]: 	Training 100/553. train loss: 6.3096,	0.8320 s / batch. (data: 3.07e-04). ETA=8:47:43, max mem: 20.9 GB 
[11/27 06:50:23 visual_prompt]: 	Training 200/553. train loss: 39.0313,	0.8187 s / batch. (data: 3.12e-04). ETA=8:37:55, max mem: 20.9 GB 
[11/27 06:52:02 visual_prompt]: 	Training 300/553. train loss: 6.9512,	0.8280 s / batch. (data: 3.29e-04). ETA=8:42:25, max mem: 20.9 GB 
[11/27 06:53:41 visual_prompt]: 	Training 400/553. train loss: 22.4263,	0.8440 s / batch. (data: 3.04e-04). ETA=8:51:06, max mem: 20.9 GB 
[11/27 06:55:16 visual_prompt]: 	Training 500/553. train loss: 3.6112,	0.8273 s / batch. (data: 7.95e-03). ETA=8:39:14, max mem: 20.9 GB 
[11/27 06:56:05 visual_prompt]: Epoch 32 / 100: avg data time: 1.46e-01, avg batch time: 0.9800, average train loss: 14.8127
[11/27 06:57:00 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3086, average loss: 42.6813
[11/27 06:57:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.59	
[11/27 06:57:00 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[11/27 06:58:40 visual_prompt]: 	Training 100/553. train loss: 0.0000,	1.2521 s / batch. (data: 4.10e-01). ETA=13:02:37, max mem: 20.9 GB 
[11/27 07:00:19 visual_prompt]: 	Training 200/553. train loss: 4.3486,	0.8196 s / batch. (data: 3.10e-04). ETA=8:30:56, max mem: 20.9 GB 
[11/27 07:01:56 visual_prompt]: 	Training 300/553. train loss: 11.4001,	0.8905 s / batch. (data: 1.56e-02). ETA=9:13:39, max mem: 20.9 GB 
[11/27 07:03:34 visual_prompt]: 	Training 400/553. train loss: 0.8651,	0.8200 s / batch. (data: 3.14e-04). ETA=8:28:26, max mem: 20.9 GB 
[11/27 07:05:10 visual_prompt]: 	Training 500/553. train loss: 3.4264,	0.8320 s / batch. (data: 3.34e-04). ETA=8:34:30, max mem: 20.9 GB 
[11/27 07:06:01 visual_prompt]: Epoch 33 / 100: avg data time: 1.43e-01, avg batch time: 0.9772, average train loss: 13.1241
[11/27 07:06:56 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3086, average loss: 7.1616
[11/27 07:06:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.67	
[11/27 07:06:56 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[11/27 07:08:39 visual_prompt]: 	Training 100/553. train loss: 11.0495,	1.1024 s / batch. (data: 2.87e-01). ETA=11:18:55, max mem: 20.9 GB 
[11/27 07:10:14 visual_prompt]: 	Training 200/553. train loss: 8.1172,	1.0615 s / batch. (data: 2.38e-01). ETA=10:51:55, max mem: 20.9 GB 
[11/27 07:11:50 visual_prompt]: 	Training 300/553. train loss: 14.3706,	0.8480 s / batch. (data: 3.50e-04). ETA=8:39:23, max mem: 20.9 GB 
[11/27 07:13:28 visual_prompt]: 	Training 400/553. train loss: 3.3792,	0.8400 s / batch. (data: 3.28e-04). ETA=8:33:06, max mem: 20.9 GB 
[11/27 07:15:05 visual_prompt]: 	Training 500/553. train loss: 4.9781,	1.1740 s / batch. (data: 3.50e-01). ETA=11:55:11, max mem: 20.9 GB 
[11/27 07:15:56 visual_prompt]: Epoch 34 / 100: avg data time: 1.41e-01, avg batch time: 0.9750, average train loss: 13.8015
[11/27 07:16:51 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3102, average loss: 20.8252
[11/27 07:16:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.94	
[11/27 07:16:51 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[11/27 07:18:33 visual_prompt]: 	Training 100/553. train loss: 7.3974,	0.8295 s / batch. (data: 3.27e-04). ETA=8:23:10, max mem: 20.9 GB 
[11/27 07:20:11 visual_prompt]: 	Training 200/553. train loss: 4.7591,	0.8200 s / batch. (data: 3.21e-04). ETA=8:16:04, max mem: 20.9 GB 
[11/27 07:21:47 visual_prompt]: 	Training 300/553. train loss: 6.4350,	0.8200 s / batch. (data: 3.20e-04). ETA=8:14:41, max mem: 20.9 GB 
[11/27 07:23:22 visual_prompt]: 	Training 400/553. train loss: 12.5995,	0.8485 s / batch. (data: 3.39e-04). ETA=8:30:27, max mem: 20.9 GB 
[11/27 07:24:58 visual_prompt]: 	Training 500/553. train loss: 3.3689,	1.0397 s / batch. (data: 1.88e-01). ETA=10:23:47, max mem: 20.9 GB 
[11/27 07:25:49 visual_prompt]: Epoch 35 / 100: avg data time: 1.39e-01, avg batch time: 0.9730, average train loss: 13.4037
[11/27 07:26:45 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3099, average loss: 7.7906
[11/27 07:26:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.24	
[11/27 07:26:45 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[11/27 07:28:25 visual_prompt]: 	Training 100/553. train loss: 9.8696,	0.8593 s / batch. (data: 1.57e-02). ETA=8:33:22, max mem: 20.9 GB 
[11/27 07:30:02 visual_prompt]: 	Training 200/553. train loss: 16.3175,	0.8234 s / batch. (data: 3.39e-04). ETA=8:10:31, max mem: 20.9 GB 
[11/27 07:31:41 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8440 s / batch. (data: 4.19e-04). ETA=8:21:23, max mem: 20.9 GB 
[11/27 07:33:17 visual_prompt]: 	Training 400/553. train loss: 84.3921,	0.8248 s / batch. (data: 3.10e-04). ETA=8:08:35, max mem: 20.9 GB 
[11/27 07:34:55 visual_prompt]: 	Training 500/553. train loss: 35.7780,	0.8354 s / batch. (data: 3.36e-04). ETA=8:13:29, max mem: 20.9 GB 
[11/27 07:35:43 visual_prompt]: Epoch 36 / 100: avg data time: 1.38e-01, avg batch time: 0.9724, average train loss: 13.2126
[11/27 07:36:38 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3100, average loss: 44.4674
[11/27 07:36:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.32	
[11/27 07:36:38 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[11/27 07:38:19 visual_prompt]: 	Training 100/553. train loss: 19.8949,	0.8649 s / batch. (data: 5.43e-03). ETA=8:28:44, max mem: 20.9 GB 
[11/27 07:39:56 visual_prompt]: 	Training 200/553. train loss: 6.0677,	0.8259 s / batch. (data: 7.94e-03). ETA=8:04:25, max mem: 20.9 GB 
[11/27 07:41:33 visual_prompt]: 	Training 300/553. train loss: 10.1501,	1.1720 s / batch. (data: 3.54e-01). ETA=11:25:26, max mem: 20.9 GB 
[11/27 07:43:12 visual_prompt]: 	Training 400/553. train loss: 29.3611,	1.6186 s / batch. (data: 8.06e-01). ETA=15:43:57, max mem: 20.9 GB 
[11/27 07:44:46 visual_prompt]: 	Training 500/553. train loss: 9.0529,	1.0772 s / batch. (data: 2.58e-01). ETA=10:26:26, max mem: 20.9 GB 
[11/27 07:45:38 visual_prompt]: Epoch 37 / 100: avg data time: 1.41e-01, avg batch time: 0.9758, average train loss: 13.5388
[11/27 07:46:33 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3099, average loss: 9.7950
[11/27 07:46:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.77	
[11/27 07:46:33 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[11/27 07:48:11 visual_prompt]: 	Training 100/553. train loss: 7.4689,	0.8277 s / batch. (data: 5.43e-03). ETA=7:59:11, max mem: 20.9 GB 
[11/27 07:49:50 visual_prompt]: 	Training 200/553. train loss: 15.4342,	1.2200 s / batch. (data: 3.64e-01). ETA=11:44:19, max mem: 20.9 GB 
[11/27 07:51:28 visual_prompt]: 	Training 300/553. train loss: 4.2719,	0.8521 s / batch. (data: 2.58e-04). ETA=8:10:29, max mem: 20.9 GB 
[11/27 07:53:03 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8199 s / batch. (data: 3.02e-04). ETA=7:50:35, max mem: 20.9 GB 
[11/27 07:54:43 visual_prompt]: 	Training 500/553. train loss: 31.6885,	0.8476 s / batch. (data: 1.05e-02). ETA=8:05:06, max mem: 20.9 GB 
[11/27 07:55:32 visual_prompt]: Epoch 38 / 100: avg data time: 1.39e-01, avg batch time: 0.9739, average train loss: 12.6907
[11/27 07:56:27 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3077, average loss: 7.2202
[11/27 07:56:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.49	
[11/27 07:56:27 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[11/27 07:58:07 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8560 s / batch. (data: 5.45e-03). ETA=8:07:43, max mem: 20.9 GB 
[11/27 07:59:48 visual_prompt]: 	Training 200/553. train loss: 47.7523,	0.8313 s / batch. (data: 2.91e-04). ETA=7:52:14, max mem: 20.9 GB 
[11/27 08:01:27 visual_prompt]: 	Training 300/553. train loss: 17.8667,	0.8280 s / batch. (data: 2.91e-04). ETA=7:49:00, max mem: 20.9 GB 
[11/27 08:03:01 visual_prompt]: 	Training 400/553. train loss: 23.3660,	0.8294 s / batch. (data: 3.02e-04). ETA=7:48:25, max mem: 20.9 GB 
[11/27 08:04:39 visual_prompt]: 	Training 500/553. train loss: 0.7639,	1.6158 s / batch. (data: 7.82e-01). ETA=15:09:51, max mem: 20.9 GB 
[11/27 08:05:27 visual_prompt]: Epoch 39 / 100: avg data time: 1.42e-01, avg batch time: 0.9761, average train loss: 11.4451
[11/27 08:06:22 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3111, average loss: 2.7492
[11/27 08:06:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.50	
[11/27 08:06:22 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[11/27 08:08:04 visual_prompt]: 	Training 100/553. train loss: 9.1090,	0.8320 s / batch. (data: 2.99e-04). ETA=7:46:23, max mem: 20.9 GB 
[11/27 08:09:41 visual_prompt]: 	Training 200/553. train loss: 22.3345,	0.8244 s / batch. (data: 5.41e-03). ETA=7:40:43, max mem: 20.9 GB 
[11/27 08:11:19 visual_prompt]: 	Training 300/553. train loss: 4.3439,	0.8185 s / batch. (data: 3.09e-04). ETA=7:36:05, max mem: 20.9 GB 
[11/27 08:12:57 visual_prompt]: 	Training 400/553. train loss: 2.9576,	0.8520 s / batch. (data: 8.03e-04). ETA=7:53:19, max mem: 20.9 GB 
[11/27 08:14:34 visual_prompt]: 	Training 500/553. train loss: 40.9235,	0.8197 s / batch. (data: 2.89e-04). ETA=7:34:00, max mem: 20.9 GB 
[11/27 08:15:26 visual_prompt]: Epoch 40 / 100: avg data time: 1.49e-01, avg batch time: 0.9832, average train loss: 12.8938
[11/27 08:16:22 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.3111, average loss: 19.6490
[11/27 08:16:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.70	
[11/27 08:16:22 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[11/27 08:18:08 visual_prompt]: 	Training 100/553. train loss: 11.0489,	0.8408 s / batch. (data: 2.93e-04). ETA=7:43:33, max mem: 20.9 GB 
[11/27 08:19:47 visual_prompt]: 	Training 200/553. train loss: 6.3381,	0.8395 s / batch. (data: 8.83e-04). ETA=7:41:28, max mem: 20.9 GB 
[11/27 08:21:23 visual_prompt]: 	Training 300/553. train loss: 20.3824,	0.8360 s / batch. (data: 2.97e-04). ETA=7:38:07, max mem: 20.9 GB 
[11/27 08:23:00 visual_prompt]: 	Training 400/553. train loss: 3.8152,	0.8268 s / batch. (data: 7.95e-03). ETA=7:31:41, max mem: 20.9 GB 
[11/27 08:24:35 visual_prompt]: 	Training 500/553. train loss: 7.7854,	0.8600 s / batch. (data: 7.98e-04). ETA=7:48:24, max mem: 20.9 GB 
[11/27 08:25:23 visual_prompt]: Epoch 41 / 100: avg data time: 1.44e-01, avg batch time: 0.9779, average train loss: 10.7878
[11/27 08:26:19 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3119, average loss: 9.9622
[11/27 08:26:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.04	
[11/27 08:26:19 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[11/27 08:27:58 visual_prompt]: 	Training 100/553. train loss: 14.1490,	0.8520 s / batch. (data: 1.20e-02). ETA=7:41:53, max mem: 20.9 GB 
[11/27 08:29:36 visual_prompt]: 	Training 200/553. train loss: 4.1314,	0.8702 s / batch. (data: 3.79e-02). ETA=7:50:16, max mem: 20.9 GB 
[11/27 08:31:14 visual_prompt]: 	Training 300/553. train loss: 84.9100,	0.8320 s / batch. (data: 2.65e-04). ETA=7:28:16, max mem: 20.9 GB 
[11/27 08:32:52 visual_prompt]: 	Training 400/553. train loss: 26.1052,	0.8503 s / batch. (data: 1.05e-02). ETA=7:36:43, max mem: 20.9 GB 
[11/27 08:34:28 visual_prompt]: 	Training 500/553. train loss: 40.4194,	0.8364 s / batch. (data: 1.20e-02). ETA=7:27:50, max mem: 20.9 GB 
[11/27 08:35:20 visual_prompt]: Epoch 42 / 100: avg data time: 1.45e-01, avg batch time: 0.9790, average train loss: 12.7187
[11/27 08:36:16 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3115, average loss: 12.5974
[11/27 08:36:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.95	
[11/27 08:36:16 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[11/27 08:37:59 visual_prompt]: 	Training 100/553. train loss: 6.3088,	0.8348 s / batch. (data: 1.09e-02). ETA=7:24:50, max mem: 20.9 GB 
[11/27 08:39:36 visual_prompt]: 	Training 200/553. train loss: 9.3037,	0.8309 s / batch. (data: 3.02e-04). ETA=7:21:24, max mem: 20.9 GB 
[11/27 08:41:11 visual_prompt]: 	Training 300/553. train loss: 23.7119,	0.8440 s / batch. (data: 7.96e-03). ETA=7:26:57, max mem: 20.9 GB 
[11/27 08:42:47 visual_prompt]: 	Training 400/553. train loss: 15.8980,	0.8446 s / batch. (data: 5.42e-03). ETA=7:25:52, max mem: 20.9 GB 
[11/27 08:44:25 visual_prompt]: 	Training 500/553. train loss: 24.1402,	0.8440 s / batch. (data: 7.95e-03). ETA=7:24:08, max mem: 20.9 GB 
[11/27 08:45:18 visual_prompt]: Epoch 43 / 100: avg data time: 1.48e-01, avg batch time: 0.9815, average train loss: 11.7274
[11/27 08:46:14 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3095, average loss: 0.7868
[11/27 08:46:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.77	
[11/27 08:46:14 visual_prompt]: Best epoch 43: best metric: -0.787
[11/27 08:46:14 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[11/27 08:47:55 visual_prompt]: 	Training 100/553. train loss: 9.9746,	0.8589 s / batch. (data: 3.23e-02). ETA=7:29:48, max mem: 20.9 GB 
[11/27 08:49:36 visual_prompt]: 	Training 200/553. train loss: 6.3026,	0.8288 s / batch. (data: 3.94e-04). ETA=7:12:38, max mem: 20.9 GB 
[11/27 08:51:11 visual_prompt]: 	Training 300/553. train loss: 2.8780,	0.8637 s / batch. (data: 2.37e-02). ETA=7:29:26, max mem: 20.9 GB 
[11/27 08:52:48 visual_prompt]: 	Training 400/553. train loss: 2.2921,	0.8560 s / batch. (data: 1.19e-02). ETA=7:23:59, max mem: 20.9 GB 
[11/27 08:54:25 visual_prompt]: 	Training 500/553. train loss: 9.7416,	0.8213 s / batch. (data: 3.50e-04). ETA=7:04:37, max mem: 20.9 GB 
[11/27 08:55:16 visual_prompt]: Epoch 44 / 100: avg data time: 1.46e-01, avg batch time: 0.9803, average train loss: 13.5504
[11/27 08:56:12 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3100, average loss: 16.0814
[11/27 08:56:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.62	
[11/27 08:56:12 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[11/27 08:57:56 visual_prompt]: 	Training 100/553. train loss: 13.3801,	0.8438 s / batch. (data: 1.17e-02). ETA=7:14:05, max mem: 20.9 GB 
[11/27 08:59:29 visual_prompt]: 	Training 200/553. train loss: 4.4990,	0.8208 s / batch. (data: 3.51e-04). ETA=7:00:53, max mem: 20.9 GB 
[11/27 09:01:08 visual_prompt]: 	Training 300/553. train loss: 13.6453,	0.8415 s / batch. (data: 5.44e-03). ETA=7:10:07, max mem: 20.9 GB 
[11/27 09:02:43 visual_prompt]: 	Training 400/553. train loss: 1.0923,	0.9006 s / batch. (data: 4.06e-02). ETA=7:38:50, max mem: 20.9 GB 
[11/27 09:04:23 visual_prompt]: 	Training 500/553. train loss: 8.1593,	0.8299 s / batch. (data: 1.20e-02). ETA=7:01:26, max mem: 20.9 GB 
[11/27 09:05:14 visual_prompt]: Epoch 45 / 100: avg data time: 1.45e-01, avg batch time: 0.9789, average train loss: 9.7635
[11/27 09:06:10 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3111, average loss: 12.0047
[11/27 09:06:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.85	
[11/27 09:06:10 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[11/27 09:07:52 visual_prompt]: 	Training 100/553. train loss: 11.9958,	1.1720 s / batch. (data: 3.31e-01). ETA=9:52:09, max mem: 20.9 GB 
[11/27 09:09:30 visual_prompt]: 	Training 200/553. train loss: 5.7200,	0.8681 s / batch. (data: 8.16e-04). ETA=7:17:09, max mem: 20.9 GB 
[11/27 09:11:06 visual_prompt]: 	Training 300/553. train loss: 2.5024,	0.8280 s / batch. (data: 3.02e-04). ETA=6:55:35, max mem: 20.9 GB 
[11/27 09:12:45 visual_prompt]: 	Training 400/553. train loss: 34.9781,	0.8401 s / batch. (data: 3.31e-04). ETA=7:00:14, max mem: 20.9 GB 
[11/27 09:14:18 visual_prompt]: 	Training 500/553. train loss: 23.0178,	0.8560 s / batch. (data: 3.09e-04). ETA=7:06:47, max mem: 20.9 GB 
[11/27 09:15:11 visual_prompt]: Epoch 46 / 100: avg data time: 1.45e-01, avg batch time: 0.9798, average train loss: 10.1163
[11/27 09:16:07 visual_prompt]: Inference (val):avg data time: 4.14e-04, avg batch time: 0.3103, average loss: 0.8559
[11/27 09:16:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.24	
[11/27 09:16:07 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[11/27 09:17:49 visual_prompt]: 	Training 100/553. train loss: 6.0168,	0.8562 s / batch. (data: 1.64e-02). ETA=7:04:42, max mem: 20.9 GB 
[11/27 09:19:23 visual_prompt]: 	Training 200/553. train loss: 23.3625,	1.1080 s / batch. (data: 2.60e-01). ETA=9:07:44, max mem: 20.9 GB 
[11/27 09:21:00 visual_prompt]: 	Training 300/553. train loss: 8.4394,	0.8392 s / batch. (data: 3.17e-04). ETA=6:53:28, max mem: 20.9 GB 
[11/27 09:22:38 visual_prompt]: 	Training 400/553. train loss: 4.9957,	0.8590 s / batch. (data: 1.06e-02). ETA=7:01:47, max mem: 20.9 GB 
[11/27 09:24:14 visual_prompt]: 	Training 500/553. train loss: 65.7577,	0.8560 s / batch. (data: 7.93e-03). ETA=6:58:53, max mem: 20.9 GB 
[11/27 09:25:06 visual_prompt]: Epoch 47 / 100: avg data time: 1.41e-01, avg batch time: 0.9757, average train loss: 9.7706
[11/27 09:26:02 visual_prompt]: Inference (val):avg data time: 8.55e-05, avg batch time: 0.3109, average loss: 12.2605
[11/27 09:26:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.52	
[11/27 09:26:02 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[11/27 09:27:45 visual_prompt]: 	Training 100/553. train loss: 11.4915,	0.8436 s / batch. (data: 1.60e-02). ETA=6:50:40, max mem: 20.9 GB 
[11/27 09:29:23 visual_prompt]: 	Training 200/553. train loss: 2.3317,	0.8420 s / batch. (data: 1.06e-02). ETA=6:48:28, max mem: 20.9 GB 
[11/27 09:31:01 visual_prompt]: 	Training 300/553. train loss: 3.6406,	1.4927 s / batch. (data: 6.76e-01). ETA=12:01:42, max mem: 20.9 GB 
[11/27 09:32:36 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8431 s / batch. (data: 3.69e-04). ETA=6:46:13, max mem: 20.9 GB 
[11/27 09:34:13 visual_prompt]: 	Training 500/553. train loss: 1.0134,	0.8430 s / batch. (data: 5.40e-03). ETA=6:44:44, max mem: 20.9 GB 
[11/27 09:35:04 visual_prompt]: Epoch 48 / 100: avg data time: 1.47e-01, avg batch time: 0.9805, average train loss: 10.0806
[11/27 09:36:00 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3115, average loss: 8.3874
[11/27 09:36:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 54.46	
[11/27 09:36:00 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[11/27 09:37:40 visual_prompt]: 	Training 100/553. train loss: 4.2137,	0.8288 s / batch. (data: 3.34e-04). ETA=6:35:49, max mem: 20.9 GB 
[11/27 09:39:16 visual_prompt]: 	Training 200/553. train loss: 6.3268,	0.8560 s / batch. (data: 5.42e-03). ETA=6:47:23, max mem: 20.9 GB 
[11/27 09:40:53 visual_prompt]: 	Training 300/553. train loss: 5.4387,	0.8306 s / batch. (data: 5.44e-03). ETA=6:33:56, max mem: 20.9 GB 
[11/27 09:42:32 visual_prompt]: 	Training 400/553. train loss: 9.2339,	0.8320 s / batch. (data: 3.01e-04). ETA=6:33:12, max mem: 20.9 GB 
[11/27 09:44:09 visual_prompt]: 	Training 500/553. train loss: 20.2864,	0.8450 s / batch. (data: 5.91e-03). ETA=6:37:55, max mem: 20.9 GB 
[11/27 09:45:01 visual_prompt]: Epoch 49 / 100: avg data time: 1.44e-01, avg batch time: 0.9789, average train loss: 10.2483
[11/27 09:45:57 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3113, average loss: 31.1753
[11/27 09:45:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.42	
[11/27 09:45:57 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[11/27 09:47:39 visual_prompt]: 	Training 100/553. train loss: 13.0194,	0.8312 s / batch. (data: 7.78e-04). ETA=6:29:19, max mem: 20.9 GB 
[11/27 09:49:17 visual_prompt]: 	Training 200/553. train loss: 6.3970,	0.8466 s / batch. (data: 2.73e-04). ETA=6:35:07, max mem: 20.9 GB 
[11/27 09:50:58 visual_prompt]: 	Training 300/553. train loss: 4.3012,	0.8421 s / batch. (data: 8.05e-04). ETA=6:31:36, max mem: 20.9 GB 
[11/27 09:52:33 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8320 s / batch. (data: 2.88e-04). ETA=6:25:32, max mem: 20.9 GB 
[11/27 09:54:12 visual_prompt]: 	Training 500/553. train loss: 17.3486,	0.8240 s / batch. (data: 4.54e-04). ETA=6:20:26, max mem: 20.9 GB 
[11/27 09:55:01 visual_prompt]: Epoch 50 / 100: avg data time: 1.51e-01, avg batch time: 0.9845, average train loss: 9.8647
[11/27 09:55:57 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.3101, average loss: 5.1785
[11/27 09:55:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.00	
[11/27 09:55:57 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[11/27 09:57:38 visual_prompt]: 	Training 100/553. train loss: 0.5580,	1.0197 s / batch. (data: 1.91e-01). ETA=7:48:14, max mem: 20.9 GB 
[11/27 09:59:16 visual_prompt]: 	Training 200/553. train loss: 15.9955,	0.8400 s / batch. (data: 2.66e-04). ETA=6:24:18, max mem: 20.9 GB 
[11/27 10:00:54 visual_prompt]: 	Training 300/553. train loss: 0.6149,	0.8311 s / batch. (data: 1.05e-02). ETA=6:18:49, max mem: 20.9 GB 
[11/27 10:02:32 visual_prompt]: 	Training 400/553. train loss: 0.6389,	1.3863 s / batch. (data: 5.28e-01). ETA=10:29:37, max mem: 20.9 GB 
[11/27 10:04:08 visual_prompt]: 	Training 500/553. train loss: 6.4993,	0.8526 s / batch. (data: 2.05e-02). ETA=6:25:47, max mem: 20.9 GB 
[11/27 10:04:58 visual_prompt]: Epoch 51 / 100: avg data time: 1.44e-01, avg batch time: 0.9782, average train loss: 8.9297
[11/27 10:05:54 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3111, average loss: 6.8355
[11/27 10:05:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.55	
[11/27 10:05:54 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[11/27 10:07:39 visual_prompt]: 	Training 100/553. train loss: 4.5848,	0.8204 s / batch. (data: 3.20e-04). ETA=6:09:07, max mem: 20.9 GB 
[11/27 10:09:19 visual_prompt]: 	Training 200/553. train loss: 7.6705,	0.8186 s / batch. (data: 3.03e-04). ETA=6:06:58, max mem: 20.9 GB 
[11/27 10:10:58 visual_prompt]: 	Training 300/553. train loss: 14.7017,	0.8320 s / batch. (data: 2.99e-04). ETA=6:11:34, max mem: 20.9 GB 
[11/27 10:12:36 visual_prompt]: 	Training 400/553. train loss: 0.0287,	0.8200 s / batch. (data: 2.98e-04). ETA=6:04:50, max mem: 20.9 GB 
[11/27 10:14:09 visual_prompt]: 	Training 500/553. train loss: 35.5324,	0.8407 s / batch. (data: 2.83e-04). ETA=6:12:38, max mem: 20.9 GB 
[11/27 10:15:00 visual_prompt]: Epoch 52 / 100: avg data time: 1.55e-01, avg batch time: 0.9887, average train loss: 8.8161
[11/27 10:15:59 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3100, average loss: 24.7753
[11/27 10:15:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.00	
[11/27 10:15:59 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[11/27 10:17:41 visual_prompt]: 	Training 100/553. train loss: 8.9939,	0.8243 s / batch. (data: 2.91e-04). ETA=6:03:18, max mem: 20.9 GB 
[11/27 10:19:19 visual_prompt]: 	Training 200/553. train loss: 2.5567,	0.8480 s / batch. (data: 3.12e-04). ETA=6:12:19, max mem: 20.9 GB 
[11/27 10:20:58 visual_prompt]: 	Training 300/553. train loss: 12.7372,	0.8526 s / batch. (data: 1.56e-02). ETA=6:12:55, max mem: 20.9 GB 
[11/27 10:22:38 visual_prompt]: 	Training 400/553. train loss: 12.6790,	0.8436 s / batch. (data: 1.20e-02). ETA=6:07:34, max mem: 20.9 GB 
[11/27 10:24:14 visual_prompt]: 	Training 500/553. train loss: 0.4942,	0.8280 s / batch. (data: 5.41e-03). ETA=5:59:24, max mem: 20.9 GB 
[11/27 10:25:06 visual_prompt]: Epoch 53 / 100: avg data time: 1.55e-01, avg batch time: 0.9889, average train loss: 8.1290
[11/27 10:26:01 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3113, average loss: 2.3776
[11/27 10:26:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 49.26	
[11/27 10:26:01 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[11/27 10:27:45 visual_prompt]: 	Training 100/553. train loss: 29.1194,	0.8600 s / batch. (data: 1.19e-02). ETA=6:11:05, max mem: 20.9 GB 
[11/27 10:29:23 visual_prompt]: 	Training 200/553. train loss: 3.0860,	0.8217 s / batch. (data: 3.71e-04). ETA=5:53:12, max mem: 20.9 GB 
[11/27 10:30:59 visual_prompt]: 	Training 300/553. train loss: 10.0813,	0.8474 s / batch. (data: 7.97e-03). ETA=6:02:51, max mem: 20.9 GB 
[11/27 10:32:36 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8240 s / batch. (data: 1.21e-02). ETA=5:51:27, max mem: 20.9 GB 
[11/27 10:34:13 visual_prompt]: 	Training 500/553. train loss: 23.6134,	0.8195 s / batch. (data: 3.08e-04). ETA=5:48:09, max mem: 20.9 GB 
[11/27 10:35:05 visual_prompt]: Epoch 54 / 100: avg data time: 1.48e-01, avg batch time: 0.9827, average train loss: 8.0860
[11/27 10:36:01 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3103, average loss: 8.8044
[11/27 10:36:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.25	
[11/27 10:36:01 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[11/27 10:37:42 visual_prompt]: 	Training 100/553. train loss: 3.3041,	0.8473 s / batch. (data: 3.20e-04). ETA=5:57:47, max mem: 20.9 GB 
[11/27 10:39:17 visual_prompt]: 	Training 200/553. train loss: 1.0561,	0.8256 s / batch. (data: 3.22e-04). ETA=5:47:17, max mem: 20.9 GB 
[11/27 10:40:54 visual_prompt]: 	Training 300/553. train loss: 9.7359,	0.8440 s / batch. (data: 2.95e-04). ETA=5:53:35, max mem: 20.9 GB 
[11/27 10:42:31 visual_prompt]: 	Training 400/553. train loss: 25.0819,	0.8280 s / batch. (data: 3.15e-04). ETA=5:45:31, max mem: 20.9 GB 
[11/27 10:44:08 visual_prompt]: 	Training 500/553. train loss: 23.9369,	0.8320 s / batch. (data: 1.20e-02). ETA=5:45:48, max mem: 20.9 GB 
[11/27 10:45:00 visual_prompt]: Epoch 55 / 100: avg data time: 1.40e-01, avg batch time: 0.9749, average train loss: 9.2435
[11/27 10:45:56 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3081, average loss: 25.2491
[11/27 10:45:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.33	
[11/27 10:45:56 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[11/27 10:47:38 visual_prompt]: 	Training 100/553. train loss: 2.7082,	0.8210 s / batch. (data: 3.11e-04). ETA=5:39:09, max mem: 20.9 GB 
[11/27 10:49:15 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8373 s / batch. (data: 1.20e-02). ETA=5:44:28, max mem: 20.9 GB 
[11/27 10:50:53 visual_prompt]: 	Training 300/553. train loss: 3.9432,	0.8440 s / batch. (data: 2.96e-04). ETA=5:45:49, max mem: 20.9 GB 
[11/27 10:52:31 visual_prompt]: 	Training 400/553. train loss: 0.8440,	0.8360 s / batch. (data: 7.38e-04). ETA=5:41:10, max mem: 20.9 GB 
[11/27 10:54:08 visual_prompt]: 	Training 500/553. train loss: 2.1989,	1.7932 s / batch. (data: 9.70e-01). ETA=12:08:47, max mem: 20.9 GB 
[11/27 10:54:57 visual_prompt]: Epoch 56 / 100: avg data time: 1.44e-01, avg batch time: 0.9786, average train loss: 7.8256
[11/27 10:55:52 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3097, average loss: 4.9500
[11/27 10:55:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.65	
[11/27 10:55:52 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[11/27 10:57:36 visual_prompt]: 	Training 100/553. train loss: 4.5333,	0.8172 s / batch. (data: 3.23e-04). ETA=5:30:02, max mem: 20.9 GB 
[11/27 10:59:12 visual_prompt]: 	Training 200/553. train loss: 2.5458,	0.8351 s / batch. (data: 3.31e-04). ETA=5:35:52, max mem: 20.9 GB 
[11/27 11:00:48 visual_prompt]: 	Training 300/553. train loss: 11.6346,	0.8329 s / batch. (data: 5.44e-03). ETA=5:33:36, max mem: 20.9 GB 
[11/27 11:02:24 visual_prompt]: 	Training 400/553. train loss: 10.6415,	0.8459 s / batch. (data: 1.19e-02). ETA=5:37:23, max mem: 20.9 GB 
[11/27 11:03:58 visual_prompt]: 	Training 500/553. train loss: 17.6957,	0.8320 s / batch. (data: 5.43e-03). ETA=5:30:27, max mem: 20.9 GB 
[11/27 11:04:50 visual_prompt]: Epoch 57 / 100: avg data time: 1.38e-01, avg batch time: 0.9727, average train loss: 7.8701
[11/27 11:05:46 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3094, average loss: 10.9908
[11/27 11:05:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.54	
[11/27 11:05:46 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[11/27 11:07:28 visual_prompt]: 	Training 100/553. train loss: 2.7099,	1.1720 s / batch. (data: 3.45e-01). ETA=7:42:31, max mem: 20.9 GB 
[11/27 11:09:06 visual_prompt]: 	Training 200/553. train loss: 7.6866,	0.8240 s / batch. (data: 2.98e-04). ETA=5:23:49, max mem: 20.9 GB 
[11/27 11:10:45 visual_prompt]: 	Training 300/553. train loss: 8.2824,	0.8185 s / batch. (data: 2.86e-04). ETA=5:20:18, max mem: 20.9 GB 
[11/27 11:12:22 visual_prompt]: 	Training 400/553. train loss: 8.0013,	0.8237 s / batch. (data: 3.07e-04). ETA=5:20:56, max mem: 20.9 GB 
[11/27 11:13:58 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8285 s / batch. (data: 2.98e-04). ETA=5:21:25, max mem: 20.9 GB 
[11/27 11:14:48 visual_prompt]: Epoch 58 / 100: avg data time: 1.45e-01, avg batch time: 0.9790, average train loss: 8.1003
[11/27 11:15:43 visual_prompt]: Inference (val):avg data time: 1.40e-04, avg batch time: 0.3106, average loss: 9.1299
[11/27 11:15:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.18	
[11/27 11:15:43 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[11/27 11:17:25 visual_prompt]: 	Training 100/553. train loss: 2.8699,	0.8581 s / batch. (data: 1.41e-02). ETA=5:30:43, max mem: 20.9 GB 
[11/27 11:19:03 visual_prompt]: 	Training 200/553. train loss: 2.2821,	0.8320 s / batch. (data: 3.21e-04). ETA=5:19:16, max mem: 20.9 GB 
[11/27 11:20:38 visual_prompt]: 	Training 300/553. train loss: 7.4595,	0.8800 s / batch. (data: 7.85e-04). ETA=5:36:14, max mem: 20.9 GB 
[11/27 11:22:14 visual_prompt]: 	Training 400/553. train loss: 1.5838,	0.8510 s / batch. (data: 3.53e-04). ETA=5:23:43, max mem: 20.9 GB 
[11/27 11:23:52 visual_prompt]: 	Training 500/553. train loss: 8.1765,	0.8480 s / batch. (data: 2.91e-04). ETA=5:21:11, max mem: 20.9 GB 
[11/27 11:24:42 visual_prompt]: Epoch 59 / 100: avg data time: 1.40e-01, avg batch time: 0.9746, average train loss: 6.8542
[11/27 11:25:37 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3098, average loss: 3.3368
[11/27 11:25:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.76	
[11/27 11:25:37 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[11/27 11:27:19 visual_prompt]: 	Training 100/553. train loss: 5.9352,	0.8474 s / batch. (data: 1.05e-02). ETA=5:18:49, max mem: 20.9 GB 
[11/27 11:28:56 visual_prompt]: 	Training 200/553. train loss: 19.6947,	0.8240 s / batch. (data: 3.02e-04). ETA=5:08:38, max mem: 20.9 GB 
[11/27 11:30:31 visual_prompt]: 	Training 300/553. train loss: 29.0580,	2.1401 s / batch. (data: 1.32e+00). ETA=13:18:00, max mem: 20.9 GB 
[11/27 11:32:09 visual_prompt]: 	Training 400/553. train loss: 1.0867,	0.8510 s / batch. (data: 3.27e-02). ETA=5:15:53, max mem: 20.9 GB 
[11/27 11:33:46 visual_prompt]: 	Training 500/553. train loss: 4.7463,	0.8499 s / batch. (data: 1.12e-02). ETA=5:14:03, max mem: 20.9 GB 
[11/27 11:34:38 visual_prompt]: Epoch 60 / 100: avg data time: 1.43e-01, avg batch time: 0.9774, average train loss: 7.1085
[11/27 11:35:33 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.3106, average loss: 18.0813
[11/27 11:35:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.35	
[11/27 11:35:33 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[11/27 11:37:15 visual_prompt]: 	Training 100/553. train loss: 6.3321,	0.8383 s / batch. (data: 3.27e-04). ETA=5:07:39, max mem: 20.9 GB 
[11/27 11:38:54 visual_prompt]: 	Training 200/553. train loss: 4.7226,	1.3240 s / batch. (data: 4.94e-01). ETA=8:03:41, max mem: 20.9 GB 
[11/27 11:40:32 visual_prompt]: 	Training 300/553. train loss: 2.4420,	1.2760 s / batch. (data: 4.56e-01). ETA=7:44:02, max mem: 20.9 GB 
[11/27 11:42:07 visual_prompt]: 	Training 400/553. train loss: 3.1152,	0.8319 s / batch. (data: 3.86e-04). ETA=5:01:09, max mem: 20.9 GB 
[11/27 11:43:45 visual_prompt]: 	Training 500/553. train loss: 12.1293,	2.5033 s / batch. (data: 1.67e+00). ETA=15:02:00, max mem: 20.9 GB 
[11/27 11:44:34 visual_prompt]: Epoch 61 / 100: avg data time: 1.43e-01, avg batch time: 0.9783, average train loss: 6.6655
[11/27 11:45:29 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3098, average loss: 4.2397
[11/27 11:45:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.59	
[11/27 11:45:29 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[11/27 11:47:10 visual_prompt]: 	Training 100/553. train loss: 2.5879,	0.8347 s / batch. (data: 5.44e-03). ETA=4:58:39, max mem: 20.9 GB 
[11/27 11:48:47 visual_prompt]: 	Training 200/553. train loss: 1.6795,	0.8486 s / batch. (data: 1.05e-02). ETA=5:02:11, max mem: 20.9 GB 
[11/27 11:50:24 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8279 s / batch. (data: 3.35e-04). ETA=4:53:27, max mem: 20.9 GB 
[11/27 11:52:01 visual_prompt]: 	Training 400/553. train loss: 1.7692,	0.8549 s / batch. (data: 1.08e-02). ETA=5:01:35, max mem: 20.9 GB 
[11/27 11:53:36 visual_prompt]: 	Training 500/553. train loss: 6.1887,	0.8545 s / batch. (data: 1.05e-02). ETA=5:00:00, max mem: 20.9 GB 
[11/27 11:54:29 visual_prompt]: Epoch 62 / 100: avg data time: 1.41e-01, avg batch time: 0.9751, average train loss: 5.6053
[11/27 11:55:24 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3102, average loss: 2.3602
[11/27 11:55:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.07	
[11/27 11:55:24 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[11/27 11:57:09 visual_prompt]: 	Training 100/553. train loss: 5.6347,	0.8600 s / batch. (data: 2.11e-02). ETA=4:59:47, max mem: 20.9 GB 
[11/27 11:58:49 visual_prompt]: 	Training 200/553. train loss: 2.5757,	0.8240 s / batch. (data: 3.22e-04). ETA=4:45:50, max mem: 20.9 GB 
[11/27 12:00:25 visual_prompt]: 	Training 300/553. train loss: 2.1667,	0.8447 s / batch. (data: 2.82e-04). ETA=4:51:37, max mem: 20.9 GB 
[11/27 12:01:57 visual_prompt]: 	Training 400/553. train loss: 9.1465,	0.8453 s / batch. (data: 3.29e-04). ETA=4:50:25, max mem: 20.9 GB 
[11/27 12:03:32 visual_prompt]: 	Training 500/553. train loss: 6.9035,	0.8460 s / batch. (data: 1.39e-02). ETA=4:49:14, max mem: 20.9 GB 
[11/27 12:04:22 visual_prompt]: Epoch 63 / 100: avg data time: 1.39e-01, avg batch time: 0.9725, average train loss: 6.1301
[11/27 12:05:18 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3101, average loss: 3.9931
[11/27 12:05:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.69	
[11/27 12:05:18 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[11/27 12:07:01 visual_prompt]: 	Training 100/553. train loss: 0.0087,	0.8600 s / batch. (data: 7.97e-04). ETA=4:51:50, max mem: 20.9 GB 
[11/27 12:08:39 visual_prompt]: 	Training 200/553. train loss: 2.7041,	0.8447 s / batch. (data: 6.80e-04). ETA=4:45:15, max mem: 20.9 GB 
[11/27 12:10:12 visual_prompt]: 	Training 300/553. train loss: 2.3742,	0.8343 s / batch. (data: 3.44e-04). ETA=4:40:20, max mem: 20.9 GB 
[11/27 12:11:50 visual_prompt]: 	Training 400/553. train loss: 4.6672,	0.8200 s / batch. (data: 3.29e-04). ETA=4:34:09, max mem: 20.9 GB 
[11/27 12:13:26 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8509 s / batch. (data: 5.42e-03). ETA=4:43:04, max mem: 20.9 GB 
[11/27 12:14:16 visual_prompt]: Epoch 64 / 100: avg data time: 1.39e-01, avg batch time: 0.9730, average train loss: 5.4880
[11/27 12:15:12 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3104, average loss: 2.1380
[11/27 12:15:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.45	
[11/27 12:15:12 visual_prompt]: Stopping early.
[11/27 12:15:12 visual_prompt]: Rank of current process: 0. World size: 1
[11/27 12:15:12 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/27 12:15:12 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/27 12:15:12 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/27 12:15:12 visual_prompt]: Training with config:
[11/27 12:15:12 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr5.0_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/27 12:15:12 visual_prompt]: Loading training data...
[11/27 12:15:12 visual_prompt]: Constructing mammo-cbis dataset train...
[11/27 12:15:12 visual_prompt]: Loading validation data...
[11/27 12:15:12 visual_prompt]: Constructing mammo-cbis dataset val...
[11/27 12:15:12 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/27 12:15:14 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/27 12:15:14 visual_prompt]: tuned percent:0.525
[11/27 12:15:15 visual_prompt]: Device used for model: 0
[11/27 12:15:15 visual_prompt]: Setting up Evaluator...
[11/27 12:15:15 visual_prompt]: Setting up Trainer...
[11/27 12:15:15 visual_prompt]: 	Setting up the optimizer...
[11/27 12:15:15 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/27 12:16:55 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8204 s / batch. (data: 4.16e-04). ETA=12:34:45, max mem: 20.9 GB 
[11/27 12:18:30 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8394 s / batch. (data: 3.98e-04). ETA=12:50:53, max mem: 20.9 GB 
[11/27 12:20:09 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8334 s / batch. (data: 3.08e-04). ETA=12:43:56, max mem: 20.9 GB 
[11/27 12:21:45 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8337 s / batch. (data: 5.42e-03). ETA=12:42:47, max mem: 20.9 GB 
[11/27 12:23:24 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8536 s / batch. (data: 5.59e-03). ETA=12:59:39, max mem: 20.9 GB 
[11/27 12:24:16 visual_prompt]: Epoch 1 / 100: avg data time: 1.44e-01, avg batch time: 0.9790, average train loss: 1.5403
[11/27 12:25:12 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3114, average loss: 1.5201
[11/27 12:25:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/27 12:25:12 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/27 12:26:52 visual_prompt]: 	Training 100/553. train loss: 0.9062,	1.4704 s / batch. (data: 6.40e-01). ETA=22:19:12, max mem: 20.9 GB 
[11/27 12:28:28 visual_prompt]: 	Training 200/553. train loss: 0.0004,	0.8193 s / batch. (data: 3.35e-04). ETA=12:24:48, max mem: 20.9 GB 
[11/27 12:30:07 visual_prompt]: 	Training 300/553. train loss: 2.3715,	0.8915 s / batch. (data: 7.23e-02). ETA=13:29:01, max mem: 20.9 GB 
[11/27 12:31:43 visual_prompt]: 	Training 400/553. train loss: 0.8650,	0.8296 s / batch. (data: 6.35e-03). ETA=12:31:27, max mem: 20.9 GB 
[11/27 12:33:22 visual_prompt]: 	Training 500/553. train loss: 0.7718,	0.8360 s / batch. (data: 3.33e-04). ETA=12:35:51, max mem: 20.9 GB 
[11/27 12:34:11 visual_prompt]: Epoch 2 / 100: avg data time: 1.40e-01, avg batch time: 0.9756, average train loss: 1.8079
[11/27 12:35:06 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3093, average loss: 2.7599
[11/27 12:35:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.04	
[11/27 12:35:06 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/27 12:36:46 visual_prompt]: 	Training 100/553. train loss: 0.8204,	0.8252 s / batch. (data: 2.50e-04). ETA=12:23:57, max mem: 20.9 GB 
[11/27 12:38:24 visual_prompt]: 	Training 200/553. train loss: 0.8124,	0.8458 s / batch. (data: 3.18e-04). ETA=12:41:09, max mem: 20.9 GB 
[11/27 12:39:59 visual_prompt]: 	Training 300/553. train loss: 1.1878,	0.8382 s / batch. (data: 7.95e-03). ETA=12:32:54, max mem: 20.9 GB 
[11/27 12:41:36 visual_prompt]: 	Training 400/553. train loss: 3.5394,	0.8414 s / batch. (data: 3.02e-04). ETA=12:34:21, max mem: 20.9 GB 
[11/27 12:43:14 visual_prompt]: 	Training 500/553. train loss: 0.9536,	1.0029 s / batch. (data: 1.85e-01). ETA=14:57:32, max mem: 20.9 GB 
[11/27 12:44:04 visual_prompt]: Epoch 3 / 100: avg data time: 1.37e-01, avg batch time: 0.9712, average train loss: 2.4124
[11/27 12:44:59 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3113, average loss: 3.8518
[11/27 12:44:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.31	
[11/27 12:44:59 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/27 12:46:41 visual_prompt]: 	Training 100/553. train loss: 5.8539,	0.8360 s / batch. (data: 3.39e-04). ETA=12:26:00, max mem: 20.9 GB 
[11/27 12:48:18 visual_prompt]: 	Training 200/553. train loss: 4.1352,	0.8520 s / batch. (data: 2.32e-02). ETA=12:38:52, max mem: 20.9 GB 
[11/27 12:49:56 visual_prompt]: 	Training 300/553. train loss: 1.8282,	1.1840 s / batch. (data: 3.48e-01). ETA=17:32:34, max mem: 20.9 GB 
[11/27 12:51:28 visual_prompt]: 	Training 400/553. train loss: 22.1794,	0.9710 s / batch. (data: 1.31e-01). ETA=14:21:36, max mem: 20.9 GB 
[11/27 12:53:07 visual_prompt]: 	Training 500/553. train loss: 0.6721,	3.3240 s / batch. (data: 2.48e+00). ETA=2 days, 1:04:01, max mem: 20.9 GB 
[11/27 12:53:59 visual_prompt]: Epoch 4 / 100: avg data time: 1.42e-01, avg batch time: 0.9763, average train loss: 3.9943
[11/27 12:54:54 visual_prompt]: Inference (val):avg data time: 2.90e-04, avg batch time: 0.3106, average loss: 1.0283
[11/27 12:54:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.76	
[11/27 12:54:54 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/27 12:56:35 visual_prompt]: 	Training 100/553. train loss: 15.8534,	0.8360 s / batch. (data: 3.16e-04). ETA=12:18:17, max mem: 20.9 GB 
[11/27 12:58:12 visual_prompt]: 	Training 200/553. train loss: 0.5989,	1.1628 s / batch. (data: 3.33e-01). ETA=17:04:57, max mem: 20.9 GB 
[11/27 12:59:51 visual_prompt]: 	Training 300/553. train loss: 20.0197,	0.8440 s / batch. (data: 2.73e-04). ETA=12:22:33, max mem: 20.9 GB 
[11/27 13:01:27 visual_prompt]: 	Training 400/553. train loss: 3.1582,	0.8310 s / batch. (data: 3.05e-04). ETA=12:09:43, max mem: 20.9 GB 
[11/27 13:03:04 visual_prompt]: 	Training 500/553. train loss: 6.6953,	0.8440 s / batch. (data: 3.39e-04). ETA=12:19:43, max mem: 20.9 GB 
[11/27 13:03:55 visual_prompt]: Epoch 5 / 100: avg data time: 1.42e-01, avg batch time: 0.9776, average train loss: 5.6487
[11/27 13:04:50 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.3094, average loss: 10.8251
[11/27 13:04:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.64	
[11/27 13:04:50 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/27 13:06:33 visual_prompt]: 	Training 100/553. train loss: 0.5371,	0.8432 s / batch. (data: 7.40e-04). ETA=12:16:51, max mem: 20.9 GB 
[11/27 13:08:09 visual_prompt]: 	Training 200/553. train loss: 22.6648,	0.8200 s / batch. (data: 3.19e-04). ETA=11:55:15, max mem: 20.9 GB 
[11/27 13:09:44 visual_prompt]: 	Training 300/553. train loss: 5.7618,	0.8258 s / batch. (data: 3.01e-04). ETA=11:58:54, max mem: 20.9 GB 
[11/27 13:11:25 visual_prompt]: 	Training 400/553. train loss: 2.8443,	0.8375 s / batch. (data: 3.70e-04). ETA=12:07:45, max mem: 20.9 GB 
[11/27 13:13:00 visual_prompt]: 	Training 500/553. train loss: 16.9213,	0.8639 s / batch. (data: 5.32e-04). ETA=12:29:14, max mem: 20.9 GB 
[11/27 13:13:50 visual_prompt]: Epoch 6 / 100: avg data time: 1.42e-01, avg batch time: 0.9762, average train loss: 7.5020
[11/27 13:14:46 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3102, average loss: 3.5944
[11/27 13:14:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.87	
[11/27 13:14:46 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/27 13:16:27 visual_prompt]: 	Training 100/553. train loss: 8.8278,	0.8573 s / batch. (data: 1.16e-03). ETA=12:21:20, max mem: 20.9 GB 
[11/27 13:18:03 visual_prompt]: 	Training 200/553. train loss: 3.8332,	0.8176 s / batch. (data: 3.09e-04). ETA=11:45:38, max mem: 20.9 GB 
[11/27 13:19:44 visual_prompt]: 	Training 300/553. train loss: 4.1304,	1.8483 s / batch. (data: 1.03e+00). ETA=1 day, 2:32:02, max mem: 20.9 GB 
[11/27 13:21:21 visual_prompt]: 	Training 400/553. train loss: 4.4043,	1.6327 s / batch. (data: 8.01e-01). ETA=23:23:39, max mem: 20.9 GB 
[11/27 13:22:56 visual_prompt]: 	Training 500/553. train loss: 33.8506,	0.8400 s / batch. (data: 3.21e-04). ETA=12:00:45, max mem: 20.9 GB 
[11/27 13:23:47 visual_prompt]: Epoch 7 / 100: avg data time: 1.43e-01, avg batch time: 0.9773, average train loss: 9.9244
[11/27 13:24:43 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3095, average loss: 0.8425
[11/27 13:24:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.95	
[11/27 13:24:43 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/27 13:26:22 visual_prompt]: 	Training 100/553. train loss: 7.2772,	0.8520 s / batch. (data: 7.96e-03). ETA=12:08:52, max mem: 20.9 GB 
[11/27 13:28:01 visual_prompt]: 	Training 200/553. train loss: 4.0536,	0.8546 s / batch. (data: 2.25e-02). ETA=12:09:38, max mem: 20.9 GB 
[11/27 13:29:39 visual_prompt]: 	Training 300/553. train loss: 4.9816,	0.8290 s / batch. (data: 2.88e-04). ETA=11:46:25, max mem: 20.9 GB 
[11/27 13:31:16 visual_prompt]: 	Training 400/553. train loss: 9.5666,	0.8549 s / batch. (data: 9.98e-03). ETA=12:07:05, max mem: 20.9 GB 
[11/27 13:32:53 visual_prompt]: 	Training 500/553. train loss: 32.3815,	1.4095 s / batch. (data: 5.79e-01). ETA=19:56:26, max mem: 20.9 GB 
[11/27 13:33:44 visual_prompt]: Epoch 8 / 100: avg data time: 1.44e-01, avg batch time: 0.9781, average train loss: 9.7738
[11/27 13:34:39 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3100, average loss: 8.3319
[11/27 13:34:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.51	
[11/27 13:34:39 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/27 13:36:21 visual_prompt]: 	Training 100/553. train loss: 8.8853,	0.8627 s / batch. (data: 1.55e-02). ETA=12:10:02, max mem: 20.9 GB 
[11/27 13:37:57 visual_prompt]: 	Training 200/553. train loss: 10.7656,	0.8480 s / batch. (data: 1.20e-02). ETA=11:56:12, max mem: 20.9 GB 
[11/27 13:39:35 visual_prompt]: 	Training 300/553. train loss: 3.2274,	1.4262 s / batch. (data: 6.02e-01). ETA=20:02:09, max mem: 20.9 GB 
[11/27 13:41:14 visual_prompt]: 	Training 400/553. train loss: 3.7891,	0.8570 s / batch. (data: 3.78e-02). ETA=12:00:59, max mem: 20.9 GB 
[11/27 13:42:50 visual_prompt]: 	Training 500/553. train loss: 5.6567,	0.8444 s / batch. (data: 3.01e-04). ETA=11:48:57, max mem: 20.9 GB 
[11/27 13:43:40 visual_prompt]: Epoch 9 / 100: avg data time: 1.44e-01, avg batch time: 0.9783, average train loss: 9.8596
[11/27 13:44:36 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3116, average loss: 9.2470
[11/27 13:44:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.70	
[11/27 13:44:36 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/27 13:46:20 visual_prompt]: 	Training 100/553. train loss: 79.6099,	0.8200 s / batch. (data: 3.04e-04). ETA=11:26:23, max mem: 20.9 GB 
[11/27 13:47:56 visual_prompt]: 	Training 200/553. train loss: 2.5538,	0.8350 s / batch. (data: 7.95e-03). ETA=11:37:34, max mem: 20.9 GB 
[11/27 13:49:33 visual_prompt]: 	Training 300/553. train loss: 30.6063,	1.7693 s / batch. (data: 9.50e-01). ETA=1 day, 0:35:03, max mem: 20.9 GB 
[11/27 13:51:12 visual_prompt]: 	Training 400/553. train loss: 14.8067,	2.1280 s / batch. (data: 1.31e+00). ETA=1 day, 5:30:35, max mem: 20.9 GB 
[11/27 13:52:52 visual_prompt]: 	Training 500/553. train loss: 11.6864,	0.8350 s / batch. (data: 9.67e-03). ETA=11:33:20, max mem: 20.9 GB 
[11/27 13:53:44 visual_prompt]: Epoch 10 / 100: avg data time: 1.57e-01, avg batch time: 0.9898, average train loss: 16.9430
[11/27 13:54:39 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3115, average loss: 3.1175
[11/27 13:54:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.73	
[11/27 13:54:39 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/27 13:56:22 visual_prompt]: 	Training 100/553. train loss: 8.8875,	0.8645 s / batch. (data: 3.24e-02). ETA=11:55:39, max mem: 20.9 GB 
[11/27 13:58:01 visual_prompt]: 	Training 200/553. train loss: 1.5306,	0.8360 s / batch. (data: 1.19e-02). ETA=11:30:40, max mem: 20.9 GB 
[11/27 13:59:39 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.2830 s / batch. (data: 1.46e+00). ETA=1 day, 7:22:19, max mem: 20.9 GB 
[11/27 14:01:14 visual_prompt]: 	Training 400/553. train loss: 8.2091,	0.8480 s / batch. (data: 2.50e-03). ETA=11:37:45, max mem: 20.9 GB 
[11/27 14:02:50 visual_prompt]: 	Training 500/553. train loss: 4.4132,	0.8424 s / batch. (data: 2.24e-02). ETA=11:31:43, max mem: 20.9 GB 
[11/27 14:03:40 visual_prompt]: Epoch 11 / 100: avg data time: 1.43e-01, avg batch time: 0.9783, average train loss: 17.7631
[11/27 14:04:36 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3118, average loss: 2.8473
[11/27 14:04:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.31	
[11/27 14:04:36 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/27 14:06:19 visual_prompt]: 	Training 100/553. train loss: 6.9889,	0.8395 s / batch. (data: 3.13e-04). ETA=11:27:12, max mem: 20.9 GB 
[11/27 14:07:57 visual_prompt]: 	Training 200/553. train loss: 3.7040,	0.8584 s / batch. (data: 2.24e-02). ETA=11:41:18, max mem: 20.9 GB 
[11/27 14:09:33 visual_prompt]: 	Training 300/553. train loss: 6.6281,	0.8344 s / batch. (data: 3.71e-04). ETA=11:20:15, max mem: 20.9 GB 
[11/27 14:11:11 visual_prompt]: 	Training 400/553. train loss: 1.5824,	0.8306 s / batch. (data: 1.05e-02). ETA=11:15:46, max mem: 20.9 GB 
[11/27 14:12:48 visual_prompt]: 	Training 500/553. train loss: 67.3412,	0.8090 s / batch. (data: 3.28e-04). ETA=10:56:49, max mem: 20.9 GB 
[11/27 14:13:37 visual_prompt]: Epoch 12 / 100: avg data time: 1.44e-01, avg batch time: 0.9781, average train loss: 17.0899
[11/27 14:14:33 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.3114, average loss: 4.5959
[11/27 14:14:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.78	
[11/27 14:14:33 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/27 14:16:15 visual_prompt]: 	Training 100/553. train loss: 5.7661,	0.8594 s / batch. (data: 1.55e-02). ETA=11:35:37, max mem: 20.9 GB 
[11/27 14:17:49 visual_prompt]: 	Training 200/553. train loss: 3.9897,	0.8509 s / batch. (data: 3.35e-04). ETA=11:27:18, max mem: 20.9 GB 
[11/27 14:19:27 visual_prompt]: 	Training 300/553. train loss: 12.3657,	1.6753 s / batch. (data: 8.28e-01). ETA=22:30:23, max mem: 20.9 GB 
[11/27 14:21:03 visual_prompt]: 	Training 400/553. train loss: 35.4608,	0.8200 s / batch. (data: 3.35e-04). ETA=10:59:34, max mem: 20.9 GB 
[11/27 14:22:40 visual_prompt]: 	Training 500/553. train loss: 4.5832,	0.8461 s / batch. (data: 1.01e-02). ETA=11:19:12, max mem: 20.9 GB 
[11/27 14:23:30 visual_prompt]: Epoch 13 / 100: avg data time: 1.38e-01, avg batch time: 0.9718, average train loss: 15.4765
[11/27 14:24:25 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3115, average loss: 11.7818
[11/27 14:24:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.04	
[11/27 14:24:26 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/27 14:26:08 visual_prompt]: 	Training 100/553. train loss: 21.9701,	0.8400 s / batch. (data: 3.14e-04). ETA=11:12:09, max mem: 20.9 GB 
[11/27 14:27:44 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.9462 s / batch. (data: 1.17e-01). ETA=12:35:35, max mem: 20.9 GB 
[11/27 14:29:22 visual_prompt]: 	Training 300/553. train loss: 42.3929,	0.8320 s / batch. (data: 7.95e-03). ETA=11:02:57, max mem: 20.9 GB 
[11/27 14:30:59 visual_prompt]: 	Training 400/553. train loss: 13.3594,	0.8412 s / batch. (data: 3.14e-04). ETA=11:08:55, max mem: 20.9 GB 
[11/27 14:32:36 visual_prompt]: 	Training 500/553. train loss: 32.8339,	0.8619 s / batch. (data: 1.55e-02). ETA=11:23:58, max mem: 20.9 GB 
[11/27 14:33:26 visual_prompt]: Epoch 14 / 100: avg data time: 1.43e-01, avg batch time: 0.9765, average train loss: 14.9519
[11/27 14:34:21 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3103, average loss: 8.8228
[11/27 14:34:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.32	
[11/27 14:34:21 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/27 14:36:03 visual_prompt]: 	Training 100/553. train loss: 8.3991,	0.8288 s / batch. (data: 3.19e-04). ETA=10:55:35, max mem: 20.9 GB 
[11/27 14:37:38 visual_prompt]: 	Training 200/553. train loss: 28.2339,	0.8250 s / batch. (data: 3.26e-04). ETA=10:51:10, max mem: 20.9 GB 
[11/27 14:39:18 visual_prompt]: 	Training 300/553. train loss: 34.7296,	0.8382 s / batch. (data: 7.35e-04). ETA=11:00:10, max mem: 20.9 GB 
[11/27 14:40:52 visual_prompt]: 	Training 400/553. train loss: 0.6504,	0.9895 s / batch. (data: 1.70e-01). ETA=12:57:40, max mem: 20.9 GB 
[11/27 14:42:31 visual_prompt]: 	Training 500/553. train loss: 9.5385,	0.8479 s / batch. (data: 1.19e-02). ETA=11:05:02, max mem: 20.9 GB 
[11/27 14:43:22 visual_prompt]: Epoch 15 / 100: avg data time: 1.45e-01, avg batch time: 0.9775, average train loss: 18.0007
[11/27 14:44:18 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3095, average loss: 21.3152
[11/27 14:44:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.15	
[11/27 14:44:18 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/27 14:45:58 visual_prompt]: 	Training 100/553. train loss: 24.1684,	0.8298 s / batch. (data: 3.26e-04). ETA=10:48:39, max mem: 20.9 GB 
[11/27 14:47:34 visual_prompt]: 	Training 200/553. train loss: 22.2943,	0.8626 s / batch. (data: 1.55e-02). ETA=11:12:54, max mem: 20.9 GB 
[11/27 14:49:12 visual_prompt]: 	Training 300/553. train loss: 19.5646,	0.8240 s / batch. (data: 2.50e-04). ETA=10:41:26, max mem: 20.9 GB 
[11/27 14:50:48 visual_prompt]: 	Training 400/553. train loss: 10.7294,	0.8523 s / batch. (data: 1.64e-02). ETA=11:02:00, max mem: 20.9 GB 
[11/27 14:52:24 visual_prompt]: 	Training 500/553. train loss: 7.4963,	1.2466 s / batch. (data: 4.26e-01). ETA=16:06:13, max mem: 20.9 GB 
[11/27 14:53:15 visual_prompt]: Epoch 16 / 100: avg data time: 1.39e-01, avg batch time: 0.9722, average train loss: 16.8012
[11/27 14:54:11 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3110, average loss: 1.8160
[11/27 14:54:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.65	
[11/27 14:54:11 visual_prompt]: Best epoch 16: best metric: -1.816
[11/27 14:54:11 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/27 14:55:51 visual_prompt]: 	Training 100/553. train loss: 3.3493,	0.8287 s / batch. (data: 1.20e-02). ETA=10:40:09, max mem: 20.9 GB 
[11/27 14:57:29 visual_prompt]: 	Training 200/553. train loss: 6.1398,	0.8335 s / batch. (data: 2.49e-04). ETA=10:42:29, max mem: 20.9 GB 
[11/27 14:59:05 visual_prompt]: 	Training 300/553. train loss: 0.5005,	0.8519 s / batch. (data: 1.05e-02). ETA=10:55:18, max mem: 20.9 GB 
[11/27 15:00:42 visual_prompt]: 	Training 400/553. train loss: 5.0081,	1.0100 s / batch. (data: 1.86e-01). ETA=12:55:13, max mem: 20.9 GB 
[11/27 15:02:17 visual_prompt]: 	Training 500/553. train loss: 4.7998,	1.4618 s / batch. (data: 6.29e-01). ETA=18:39:32, max mem: 20.9 GB 
[11/27 15:03:09 visual_prompt]: Epoch 17 / 100: avg data time: 1.41e-01, avg batch time: 0.9737, average train loss: 15.2727
[11/27 15:04:05 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3101, average loss: 18.4460
[11/27 15:04:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.13	
[11/27 15:04:05 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/27 15:05:46 visual_prompt]: 	Training 100/553. train loss: 28.6263,	0.8400 s / batch. (data: 3.14e-04). ETA=10:41:12, max mem: 20.9 GB 
[11/27 15:07:25 visual_prompt]: 	Training 200/553. train loss: 32.5979,	0.8278 s / batch. (data: 5.41e-03). ETA=10:30:29, max mem: 20.9 GB 
[11/27 15:09:02 visual_prompt]: 	Training 300/553. train loss: 8.1456,	0.8319 s / batch. (data: 1.20e-02). ETA=10:32:14, max mem: 20.9 GB 
[11/27 15:10:38 visual_prompt]: 	Training 400/553. train loss: 0.7908,	0.8320 s / batch. (data: 5.42e-03). ETA=10:30:56, max mem: 20.9 GB 
[11/27 15:12:14 visual_prompt]: 	Training 500/553. train loss: 7.3902,	0.8326 s / batch. (data: 3.18e-04). ETA=10:30:00, max mem: 20.9 GB 
[11/27 15:13:04 visual_prompt]: Epoch 18 / 100: avg data time: 1.41e-01, avg batch time: 0.9743, average train loss: 14.4648
[11/27 15:13:59 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3095, average loss: 4.8361
[11/27 15:13:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.09	
[11/27 15:13:59 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/27 15:15:39 visual_prompt]: 	Training 100/553. train loss: 2.9938,	1.1835 s / batch. (data: 3.59e-01). ETA=14:52:26, max mem: 20.9 GB 
[11/27 15:17:16 visual_prompt]: 	Training 200/553. train loss: 9.4797,	0.8240 s / batch. (data: 2.78e-04). ETA=10:20:01, max mem: 20.9 GB 
[11/27 15:18:53 visual_prompt]: 	Training 300/553. train loss: 67.5854,	0.8360 s / batch. (data: 3.24e-04). ETA=10:27:38, max mem: 20.9 GB 
[11/27 15:20:32 visual_prompt]: 	Training 400/553. train loss: 6.8751,	0.8639 s / batch. (data: 5.91e-03). ETA=10:47:06, max mem: 20.9 GB 
[11/27 15:22:05 visual_prompt]: 	Training 500/553. train loss: 5.4041,	0.8483 s / batch. (data: 8.21e-03). ETA=10:34:02, max mem: 20.9 GB 
[11/27 15:22:55 visual_prompt]: Epoch 19 / 100: avg data time: 1.35e-01, avg batch time: 0.9696, average train loss: 14.8815
[11/27 15:23:50 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3112, average loss: 20.3297
[11/27 15:23:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.58	
[11/27 15:23:50 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/27 15:25:29 visual_prompt]: 	Training 100/553. train loss: 3.2001,	0.9560 s / batch. (data: 1.38e-01). ETA=11:52:06, max mem: 20.9 GB 
[11/27 15:27:08 visual_prompt]: 	Training 200/553. train loss: 48.3757,	0.8233 s / batch. (data: 3.32e-04). ETA=10:11:52, max mem: 20.9 GB 
[11/27 15:28:44 visual_prompt]: 	Training 300/553. train loss: 2.0789,	0.8760 s / batch. (data: 1.20e-02). ETA=10:49:36, max mem: 20.9 GB 
[11/27 15:30:21 visual_prompt]: 	Training 400/553. train loss: 1.1873,	0.8520 s / batch. (data: 5.43e-03). ETA=10:30:23, max mem: 20.9 GB 
[11/27 15:31:57 visual_prompt]: 	Training 500/553. train loss: 11.6085,	0.8629 s / batch. (data: 2.32e-02). ETA=10:37:01, max mem: 20.9 GB 
[11/27 15:32:48 visual_prompt]: Epoch 20 / 100: avg data time: 1.39e-01, avg batch time: 0.9724, average train loss: 16.6486
[11/27 15:33:43 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3107, average loss: 26.0773
[11/27 15:33:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.82	
[11/27 15:33:43 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[11/27 15:35:25 visual_prompt]: 	Training 100/553. train loss: 11.1664,	0.8304 s / batch. (data: 3.21e-04). ETA=10:10:53, max mem: 20.9 GB 
[11/27 15:36:59 visual_prompt]: 	Training 200/553. train loss: 27.7361,	0.8476 s / batch. (data: 2.35e-02). ETA=10:22:06, max mem: 20.9 GB 
[11/27 15:38:35 visual_prompt]: 	Training 300/553. train loss: 31.1452,	0.8360 s / batch. (data: 3.06e-04). ETA=10:12:13, max mem: 20.9 GB 
[11/27 15:40:12 visual_prompt]: 	Training 400/553. train loss: 5.8189,	0.8383 s / batch. (data: 8.00e-03). ETA=10:12:29, max mem: 20.9 GB 
[11/27 15:41:50 visual_prompt]: 	Training 500/553. train loss: 26.2486,	0.8440 s / batch. (data: 3.09e-04). ETA=10:15:15, max mem: 20.9 GB 
[11/27 15:42:39 visual_prompt]: Epoch 21 / 100: avg data time: 1.36e-01, avg batch time: 0.9697, average train loss: 15.1601
[11/27 15:43:35 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3098, average loss: 8.8783
[11/27 15:43:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 51.47	
[11/27 15:43:35 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[11/27 15:45:14 visual_prompt]: 	Training 100/553. train loss: 46.0157,	0.8379 s / batch. (data: 2.18e-02). ETA=10:08:39, max mem: 20.9 GB 
[11/27 15:46:51 visual_prompt]: 	Training 200/553. train loss: 3.3613,	0.8440 s / batch. (data: 2.85e-04). ETA=10:11:43, max mem: 20.9 GB 
[11/27 15:48:26 visual_prompt]: 	Training 300/553. train loss: 0.0013,	0.8278 s / batch. (data: 7.96e-03). ETA=9:58:35, max mem: 20.9 GB 
[11/27 15:50:04 visual_prompt]: 	Training 400/553. train loss: 23.6424,	0.8320 s / batch. (data: 2.95e-04). ETA=10:00:13, max mem: 20.9 GB 
[11/27 15:51:41 visual_prompt]: 	Training 500/553. train loss: 4.7286,	0.8306 s / batch. (data: 1.05e-02). ETA=9:57:52, max mem: 20.9 GB 
[11/27 15:52:33 visual_prompt]: Epoch 22 / 100: avg data time: 1.40e-01, avg batch time: 0.9737, average train loss: 13.4969
[11/27 15:53:28 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3124, average loss: 11.3088
[11/27 15:53:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.83	
[11/27 15:53:28 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[11/27 15:55:11 visual_prompt]: 	Training 100/553. train loss: 2.1520,	0.8312 s / batch. (data: 5.44e-03). ETA=9:56:11, max mem: 20.9 GB 
[11/27 15:56:48 visual_prompt]: 	Training 200/553. train loss: 30.8022,	0.8336 s / batch. (data: 3.93e-04). ETA=9:56:28, max mem: 20.9 GB 
[11/27 15:58:26 visual_prompt]: 	Training 300/553. train loss: 8.1021,	0.8685 s / batch. (data: 1.59e-02). ETA=10:20:00, max mem: 20.9 GB 
[11/27 16:00:01 visual_prompt]: 	Training 400/553. train loss: 5.6357,	0.8723 s / batch. (data: 5.90e-03). ETA=10:21:18, max mem: 20.9 GB 
[11/27 16:01:36 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8240 s / batch. (data: 3.12e-04). ETA=9:45:29, max mem: 20.9 GB 
[11/27 16:02:26 visual_prompt]: Epoch 23 / 100: avg data time: 1.39e-01, avg batch time: 0.9724, average train loss: 16.6052
[11/27 16:03:21 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3103, average loss: 17.7758
[11/27 16:03:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.79	
[11/27 16:03:21 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[11/27 16:05:00 visual_prompt]: 	Training 100/553. train loss: 23.6612,	0.8440 s / batch. (data: 3.98e-04). ETA=9:57:31, max mem: 20.9 GB 
[11/27 16:06:36 visual_prompt]: 	Training 200/553. train loss: 9.9993,	0.8440 s / batch. (data: 3.20e-04). ETA=9:56:08, max mem: 20.9 GB 
[11/27 16:08:13 visual_prompt]: 	Training 300/553. train loss: 4.3296,	0.8520 s / batch. (data: 1.96e-02). ETA=10:00:22, max mem: 20.9 GB 
[11/27 16:09:51 visual_prompt]: 	Training 400/553. train loss: 7.3109,	0.8312 s / batch. (data: 1.18e-02). ETA=9:44:20, max mem: 20.9 GB 
[11/27 16:11:29 visual_prompt]: 	Training 500/553. train loss: 12.1950,	0.8210 s / batch. (data: 5.17e-04). ETA=9:35:46, max mem: 20.9 GB 
[11/27 16:12:20 visual_prompt]: Epoch 24 / 100: avg data time: 1.39e-01, avg batch time: 0.9731, average train loss: 15.6678
[11/27 16:13:15 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3109, average loss: 12.2164
[11/27 16:13:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.22	
[11/27 16:13:15 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[11/27 16:14:59 visual_prompt]: 	Training 100/553. train loss: 1.2045,	0.8656 s / batch. (data: 1.76e-02). ETA=10:04:52, max mem: 20.9 GB 
[11/27 16:16:33 visual_prompt]: 	Training 200/553. train loss: 7.4703,	0.8400 s / batch. (data: 1.19e-02). ETA=9:45:36, max mem: 20.9 GB 
[11/27 16:18:10 visual_prompt]: 	Training 300/553. train loss: 4.8980,	0.8323 s / batch. (data: 1.15e-02). ETA=9:38:48, max mem: 20.9 GB 
[11/27 16:19:46 visual_prompt]: 	Training 400/553. train loss: 1.9689,	1.0897 s / batch. (data: 2.71e-01). ETA=12:36:01, max mem: 20.9 GB 
[11/27 16:21:27 visual_prompt]: 	Training 500/553. train loss: 35.4260,	1.2991 s / batch. (data: 4.54e-01). ETA=14:59:07, max mem: 20.9 GB 
[11/27 16:22:18 visual_prompt]: Epoch 25 / 100: avg data time: 1.48e-01, avg batch time: 0.9809, average train loss: 15.9382
[11/27 16:23:13 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3102, average loss: 8.8091
[11/27 16:23:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.45	
[11/27 16:23:13 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[11/27 16:24:57 visual_prompt]: 	Training 100/553. train loss: 2.1147,	0.8440 s / batch. (data: 3.01e-04). ETA=9:41:59, max mem: 20.9 GB 
[11/27 16:26:34 visual_prompt]: 	Training 200/553. train loss: 17.9493,	0.8923 s / batch. (data: 5.18e-02). ETA=10:13:49, max mem: 20.9 GB 
[11/27 16:28:14 visual_prompt]: 	Training 300/553. train loss: 0.0001,	0.8281 s / batch. (data: 3.02e-04). ETA=9:28:15, max mem: 20.9 GB 
[11/27 16:29:49 visual_prompt]: 	Training 400/553. train loss: 16.3151,	0.8440 s / batch. (data: 7.95e-03). ETA=9:37:46, max mem: 20.9 GB 
[11/27 16:31:25 visual_prompt]: 	Training 500/553. train loss: 10.0856,	0.8554 s / batch. (data: 1.13e-02). ETA=9:44:08, max mem: 20.9 GB 
[11/27 16:32:15 visual_prompt]: Epoch 26 / 100: avg data time: 1.47e-01, avg batch time: 0.9807, average train loss: 12.4177
[11/27 16:33:11 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3110, average loss: 10.5732
[11/27 16:33:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.24	
[11/27 16:33:11 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[11/27 16:34:52 visual_prompt]: 	Training 100/553. train loss: 2.7791,	0.8409 s / batch. (data: 3.29e-04). ETA=9:32:05, max mem: 20.9 GB 
[11/27 16:36:30 visual_prompt]: 	Training 200/553. train loss: 93.5871,	0.8939 s / batch. (data: 7.92e-02). ETA=10:06:42, max mem: 20.9 GB 
[11/27 16:38:08 visual_prompt]: 	Training 300/553. train loss: 2.9835,	0.8560 s / batch. (data: 5.64e-03). ETA=9:39:33, max mem: 20.9 GB 
[11/27 16:39:45 visual_prompt]: 	Training 400/553. train loss: 11.9248,	0.8391 s / batch. (data: 3.82e-04). ETA=9:26:42, max mem: 20.9 GB 
[11/27 16:41:23 visual_prompt]: 	Training 500/553. train loss: 10.5463,	0.8480 s / batch. (data: 8.23e-04). ETA=9:31:17, max mem: 20.9 GB 
[11/27 16:42:12 visual_prompt]: Epoch 27 / 100: avg data time: 1.44e-01, avg batch time: 0.9781, average train loss: 14.5366
[11/27 16:43:07 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3105, average loss: 13.4303
[11/27 16:43:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.34	
[11/27 16:43:07 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[11/27 16:44:47 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8200 s / batch. (data: 3.05e-04). ETA=9:10:20, max mem: 20.9 GB 
[11/27 16:46:25 visual_prompt]: 	Training 200/553. train loss: 6.1713,	0.8360 s / batch. (data: 7.98e-03). ETA=9:19:43, max mem: 20.9 GB 
[11/27 16:48:02 visual_prompt]: 	Training 300/553. train loss: 3.1755,	1.4152 s / batch. (data: 5.98e-01). ETA=15:45:04, max mem: 20.9 GB 
[11/27 16:49:39 visual_prompt]: 	Training 400/553. train loss: 91.5033,	0.8289 s / batch. (data: 3.00e-04). ETA=9:12:10, max mem: 20.9 GB 
[11/27 16:51:14 visual_prompt]: 	Training 500/553. train loss: 14.5788,	0.8481 s / batch. (data: 2.54e-04). ETA=9:23:31, max mem: 20.9 GB 
[11/27 16:52:06 visual_prompt]: Epoch 28 / 100: avg data time: 1.40e-01, avg batch time: 0.9739, average train loss: 16.3554
[11/27 16:53:01 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3103, average loss: 1.2334
[11/27 16:53:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.91	
[11/27 16:53:01 visual_prompt]: Best epoch 28: best metric: -1.233
[11/27 16:53:01 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[11/27 16:54:49 visual_prompt]: 	Training 100/553. train loss: 0.0007,	0.8360 s / batch. (data: 3.22e-04). ETA=9:13:22, max mem: 20.9 GB 
[11/27 16:56:25 visual_prompt]: 	Training 200/553. train loss: 12.2392,	1.6732 s / batch. (data: 8.21e-01). ETA=18:24:45, max mem: 20.9 GB 
[11/27 16:58:00 visual_prompt]: 	Training 300/553. train loss: 7.0682,	0.8280 s / batch. (data: 3.18e-04). ETA=9:05:18, max mem: 20.9 GB 
[11/27 16:59:34 visual_prompt]: 	Training 400/553. train loss: 2.5316,	1.3024 s / batch. (data: 4.74e-01). ETA=14:15:36, max mem: 20.9 GB 
[11/27 17:01:12 visual_prompt]: 	Training 500/553. train loss: 6.7069,	0.8320 s / batch. (data: 3.52e-04). ETA=9:05:10, max mem: 20.9 GB 
[11/27 17:02:03 visual_prompt]: Epoch 29 / 100: avg data time: 1.45e-01, avg batch time: 0.9788, average train loss: 16.1975
[11/27 17:02:58 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3111, average loss: 42.7478
[11/27 17:02:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.75	
[11/27 17:02:58 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[11/27 17:04:38 visual_prompt]: 	Training 100/553. train loss: 12.7678,	0.8560 s / batch. (data: 2.96e-04). ETA=9:18:44, max mem: 20.9 GB 
[11/27 17:06:16 visual_prompt]: 	Training 200/553. train loss: 37.2850,	0.8220 s / batch. (data: 3.40e-04). ETA=8:55:10, max mem: 20.9 GB 
[11/27 17:08:00 visual_prompt]: 	Training 300/553. train loss: 30.1160,	0.8355 s / batch. (data: 5.51e-03). ETA=9:02:34, max mem: 20.9 GB 
[11/27 17:09:39 visual_prompt]: 	Training 400/553. train loss: 19.1996,	1.1280 s / batch. (data: 2.97e-01). ETA=12:10:37, max mem: 20.9 GB 
[11/27 17:11:16 visual_prompt]: 	Training 500/553. train loss: 8.1708,	1.4425 s / batch. (data: 5.96e-01). ETA=15:31:53, max mem: 20.9 GB 
[11/27 17:12:08 visual_prompt]: Epoch 30 / 100: avg data time: 1.60e-01, avg batch time: 0.9947, average train loss: 15.8050
[11/27 17:13:04 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3095, average loss: 7.5050
[11/27 17:13:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.89	
[11/27 17:13:04 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[11/27 17:14:47 visual_prompt]: 	Training 100/553. train loss: 5.6367,	0.8560 s / batch. (data: 2.94e-04). ETA=9:10:50, max mem: 20.9 GB 
[11/27 17:16:27 visual_prompt]: 	Training 200/553. train loss: 6.6392,	0.8392 s / batch. (data: 2.05e-02). ETA=8:58:38, max mem: 20.9 GB 
[11/27 17:18:02 visual_prompt]: 	Training 300/553. train loss: 41.8236,	0.8439 s / batch. (data: 5.44e-03). ETA=9:00:14, max mem: 20.9 GB 
[11/27 17:19:39 visual_prompt]: 	Training 400/553. train loss: 0.9660,	1.1416 s / batch. (data: 2.95e-01). ETA=12:08:55, max mem: 20.9 GB 
[11/27 17:21:17 visual_prompt]: 	Training 500/553. train loss: 3.8787,	0.8555 s / batch. (data: 3.16e-02). ETA=9:04:48, max mem: 20.9 GB 
[11/27 17:22:07 visual_prompt]: Epoch 31 / 100: avg data time: 1.47e-01, avg batch time: 0.9807, average train loss: 13.4559
[11/27 17:23:02 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3097, average loss: 26.2762
[11/27 17:23:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 45.88	
[11/27 17:23:02 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[11/27 17:24:45 visual_prompt]: 	Training 100/553. train loss: 0.9339,	0.8340 s / batch. (data: 5.42e-03). ETA=8:48:59, max mem: 20.9 GB 
[11/27 17:26:22 visual_prompt]: 	Training 200/553. train loss: 30.8756,	0.8388 s / batch. (data: 3.11e-04). ETA=8:50:36, max mem: 20.9 GB 
[11/27 17:28:02 visual_prompt]: 	Training 300/553. train loss: 6.8317,	0.8277 s / batch. (data: 7.97e-03). ETA=8:42:12, max mem: 20.9 GB 
[11/27 17:29:40 visual_prompt]: 	Training 400/553. train loss: 3.5445,	0.8484 s / batch. (data: 3.28e-04). ETA=8:53:54, max mem: 20.9 GB 
[11/27 17:31:14 visual_prompt]: 	Training 500/553. train loss: 9.5365,	0.8365 s / batch. (data: 2.99e-04). ETA=8:44:58, max mem: 20.9 GB 
[11/27 17:32:03 visual_prompt]: Epoch 32 / 100: avg data time: 1.44e-01, avg batch time: 0.9783, average train loss: 12.0881
[11/27 17:32:59 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.3109, average loss: 9.6817
[11/27 17:32:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.85	
[11/27 17:32:59 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[11/27 17:34:39 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8514 s / batch. (data: 1.13e-02). ETA=8:52:09, max mem: 20.9 GB 
[11/27 17:36:18 visual_prompt]: 	Training 200/553. train loss: 16.7587,	1.0646 s / batch. (data: 2.32e-01). ETA=11:03:40, max mem: 20.9 GB 
[11/27 17:37:54 visual_prompt]: 	Training 300/553. train loss: 19.7131,	0.8201 s / batch. (data: 3.00e-04). ETA=8:29:52, max mem: 20.9 GB 
[11/27 17:39:33 visual_prompt]: 	Training 400/553. train loss: 7.7959,	0.8621 s / batch. (data: 3.13e-04). ETA=8:54:35, max mem: 20.9 GB 
[11/27 17:41:09 visual_prompt]: 	Training 500/553. train loss: 2.3726,	0.8299 s / batch. (data: 1.04e-02). ETA=8:33:12, max mem: 20.9 GB 
[11/27 17:41:59 visual_prompt]: Epoch 33 / 100: avg data time: 1.42e-01, avg batch time: 0.9762, average train loss: 14.4954
[11/27 17:42:54 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3098, average loss: 1.2949
[11/27 17:42:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.83	
[11/27 17:42:54 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[11/27 17:44:37 visual_prompt]: 	Training 100/553. train loss: 21.6753,	1.0680 s / batch. (data: 2.28e-01). ETA=10:57:44, max mem: 20.9 GB 
[11/27 17:46:12 visual_prompt]: 	Training 200/553. train loss: 4.5809,	0.8320 s / batch. (data: 3.35e-04). ETA=8:31:00, max mem: 20.9 GB 
[11/27 17:47:49 visual_prompt]: 	Training 300/553. train loss: 2.4369,	0.8520 s / batch. (data: 1.05e-02). ETA=8:41:50, max mem: 20.9 GB 
[11/27 17:49:27 visual_prompt]: 	Training 400/553. train loss: 0.8394,	0.8286 s / batch. (data: 5.43e-03). ETA=8:26:10, max mem: 20.9 GB 
[11/27 17:51:04 visual_prompt]: 	Training 500/553. train loss: 5.2098,	1.4431 s / batch. (data: 6.05e-01). ETA=14:39:07, max mem: 20.9 GB 
[11/27 17:51:54 visual_prompt]: Epoch 34 / 100: avg data time: 1.43e-01, avg batch time: 0.9766, average train loss: 15.0269
[11/27 17:52:50 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3111, average loss: 12.5809
[11/27 17:52:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.34	
[11/27 17:52:50 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[11/27 17:54:33 visual_prompt]: 	Training 100/553. train loss: 1.5603,	0.8421 s / batch. (data: 1.05e-02). ETA=8:30:49, max mem: 20.9 GB 
[11/27 17:56:11 visual_prompt]: 	Training 200/553. train loss: 7.1569,	0.8568 s / batch. (data: 5.41e-03). ETA=8:38:19, max mem: 20.9 GB 
[11/27 17:57:47 visual_prompt]: 	Training 300/553. train loss: 9.4010,	0.8374 s / batch. (data: 9.31e-03). ETA=8:25:11, max mem: 20.9 GB 
[11/27 17:59:23 visual_prompt]: 	Training 400/553. train loss: 3.3447,	0.8329 s / batch. (data: 3.08e-04). ETA=8:21:04, max mem: 20.9 GB 
[11/27 18:01:00 visual_prompt]: 	Training 500/553. train loss: 5.2290,	0.8247 s / batch. (data: 5.45e-03). ETA=8:14:46, max mem: 20.9 GB 
[11/27 18:01:51 visual_prompt]: Epoch 35 / 100: avg data time: 1.44e-01, avg batch time: 0.9778, average train loss: 13.1977
[11/27 18:02:47 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3120, average loss: 1.6425
[11/27 18:02:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.10	
[11/27 18:02:47 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[11/27 18:04:27 visual_prompt]: 	Training 100/553. train loss: 0.5133,	0.8440 s / batch. (data: 7.95e-03). ETA=8:24:12, max mem: 20.9 GB 
[11/27 18:06:06 visual_prompt]: 	Training 200/553. train loss: 56.9221,	0.8439 s / batch. (data: 1.59e-02). ETA=8:22:45, max mem: 20.9 GB 
[11/27 18:07:44 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8241 s / batch. (data: 3.19e-04). ETA=8:09:33, max mem: 20.9 GB 
[11/27 18:09:20 visual_prompt]: 	Training 400/553. train loss: 10.9672,	0.8560 s / batch. (data: 3.31e-04). ETA=8:27:06, max mem: 20.9 GB 
[11/27 18:10:59 visual_prompt]: 	Training 500/553. train loss: 10.8781,	1.0562 s / batch. (data: 2.39e-01). ETA=10:23:56, max mem: 20.9 GB 
[11/27 18:11:47 visual_prompt]: Epoch 36 / 100: avg data time: 1.43e-01, avg batch time: 0.9769, average train loss: 11.3525
[11/27 18:12:43 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3115, average loss: 42.5914
[11/27 18:12:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.18	
[11/27 18:12:43 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[11/27 18:14:23 visual_prompt]: 	Training 100/553. train loss: 4.1352,	0.8303 s / batch. (data: 5.42e-03). ETA=8:08:22, max mem: 20.9 GB 
[11/27 18:16:00 visual_prompt]: 	Training 200/553. train loss: 6.0755,	0.8410 s / batch. (data: 1.17e-02). ETA=8:13:15, max mem: 20.9 GB 
[11/27 18:17:37 visual_prompt]: 	Training 300/553. train loss: 57.1658,	1.4719 s / batch. (data: 6.45e-01). ETA=14:20:50, max mem: 20.9 GB 
[11/27 18:19:16 visual_prompt]: 	Training 400/553. train loss: 5.1942,	1.2280 s / batch. (data: 3.84e-01). ETA=11:56:09, max mem: 20.9 GB 
[11/27 18:20:49 visual_prompt]: 	Training 500/553. train loss: 6.9139,	0.8760 s / batch. (data: 2.41e-02). ETA=8:29:24, max mem: 20.9 GB 
[11/27 18:21:41 visual_prompt]: Epoch 37 / 100: avg data time: 1.40e-01, avg batch time: 0.9741, average train loss: 14.5513
[11/27 18:22:37 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3096, average loss: 26.6352
[11/27 18:22:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.11	
[11/27 18:22:37 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[11/27 18:24:16 visual_prompt]: 	Training 100/553. train loss: 18.7491,	0.8321 s / batch. (data: 3.11e-04). ETA=8:01:46, max mem: 20.9 GB 
[11/27 18:25:53 visual_prompt]: 	Training 200/553. train loss: 3.7498,	0.8553 s / batch. (data: 1.52e-02). ETA=8:13:45, max mem: 20.9 GB 
[11/27 18:27:32 visual_prompt]: 	Training 300/553. train loss: 22.9882,	0.8189 s / batch. (data: 2.99e-04). ETA=7:51:22, max mem: 20.9 GB 
[11/27 18:29:06 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8600 s / batch. (data: 2.00e-02). ETA=8:13:37, max mem: 20.9 GB 
[11/27 18:30:45 visual_prompt]: 	Training 500/553. train loss: 12.5993,	0.8333 s / batch. (data: 1.02e-02). ETA=7:56:53, max mem: 20.9 GB 
[11/27 18:31:34 visual_prompt]: Epoch 38 / 100: avg data time: 1.38e-01, avg batch time: 0.9715, average train loss: 13.2105
[11/27 18:32:29 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3104, average loss: 1.1450
[11/27 18:32:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.30	
[11/27 18:32:29 visual_prompt]: Best epoch 38: best metric: -1.145
[11/27 18:32:29 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[11/27 18:34:09 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8364 s / batch. (data: 6.32e-03). ETA=7:56:32, max mem: 20.9 GB 
[11/27 18:35:49 visual_prompt]: 	Training 200/553. train loss: 7.1017,	0.8437 s / batch. (data: 1.20e-02). ETA=7:59:18, max mem: 20.9 GB 
[11/27 18:37:29 visual_prompt]: 	Training 300/553. train loss: 19.4099,	0.8195 s / batch. (data: 2.93e-04). ETA=7:44:10, max mem: 20.9 GB 
[11/27 18:39:03 visual_prompt]: 	Training 400/553. train loss: 25.4852,	1.0280 s / batch. (data: 1.75e-01). ETA=9:40:35, max mem: 20.9 GB 
[11/27 18:40:39 visual_prompt]: 	Training 500/553. train loss: 7.9714,	1.5468 s / batch. (data: 7.28e-01). ETA=14:31:00, max mem: 20.9 GB 
[11/27 18:41:28 visual_prompt]: Epoch 39 / 100: avg data time: 1.40e-01, avg batch time: 0.9746, average train loss: 11.1693
[11/27 18:42:23 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3119, average loss: 11.1331
[11/27 18:42:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.75	
[11/27 18:42:23 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[11/27 18:44:05 visual_prompt]: 	Training 100/553. train loss: 12.8464,	0.8560 s / batch. (data: 2.00e-02). ETA=7:59:49, max mem: 20.9 GB 
[11/27 18:45:41 visual_prompt]: 	Training 200/553. train loss: 8.2352,	0.8284 s / batch. (data: 3.13e-04). ETA=7:43:00, max mem: 20.9 GB 
[11/27 18:47:20 visual_prompt]: 	Training 300/553. train loss: 8.4137,	0.8459 s / batch. (data: 1.39e-02). ETA=7:51:22, max mem: 20.9 GB 
[11/27 18:48:57 visual_prompt]: 	Training 400/553. train loss: 3.6512,	0.8525 s / batch. (data: 5.89e-03). ETA=7:53:36, max mem: 20.9 GB 
[11/27 18:50:32 visual_prompt]: 	Training 500/553. train loss: 17.7791,	0.8440 s / batch. (data: 3.07e-04). ETA=7:47:29, max mem: 20.9 GB 
[11/27 18:51:24 visual_prompt]: Epoch 40 / 100: avg data time: 1.43e-01, avg batch time: 0.9771, average train loss: 12.2651
[11/27 18:52:19 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3100, average loss: 7.9841
[11/27 18:52:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.35	
[11/27 18:52:19 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[11/27 18:54:04 visual_prompt]: 	Training 100/553. train loss: 4.6326,	0.8400 s / batch. (data: 5.45e-03). ETA=7:43:06, max mem: 20.9 GB 
[11/27 18:55:44 visual_prompt]: 	Training 200/553. train loss: 11.2113,	0.8433 s / batch. (data: 7.99e-04). ETA=7:43:33, max mem: 20.9 GB 
[11/27 18:57:20 visual_prompt]: 	Training 300/553. train loss: 74.7709,	0.8474 s / batch. (data: 3.04e-04). ETA=7:44:22, max mem: 20.9 GB 
[11/27 18:58:57 visual_prompt]: 	Training 400/553. train loss: 10.3165,	0.8402 s / batch. (data: 2.06e-02). ETA=7:39:02, max mem: 20.9 GB 
[11/27 19:00:32 visual_prompt]: 	Training 500/553. train loss: 5.7795,	0.8480 s / batch. (data: 7.99e-04). ETA=7:41:52, max mem: 20.9 GB 
[11/27 19:01:20 visual_prompt]: Epoch 41 / 100: avg data time: 1.44e-01, avg batch time: 0.9783, average train loss: 15.6686
[11/27 19:02:16 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3121, average loss: 1.5621
[11/27 19:02:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.75	
[11/27 19:02:16 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[11/27 19:03:56 visual_prompt]: 	Training 100/553. train loss: 40.2209,	0.8325 s / batch. (data: 8.46e-03). ETA=7:31:19, max mem: 20.9 GB 
[11/27 19:05:34 visual_prompt]: 	Training 200/553. train loss: 75.6881,	0.8209 s / batch. (data: 3.16e-04). ETA=7:23:38, max mem: 20.9 GB 
[11/27 19:07:12 visual_prompt]: 	Training 300/553. train loss: 19.6024,	0.8400 s / batch. (data: 3.97e-04). ETA=7:32:33, max mem: 20.9 GB 
[11/27 19:08:49 visual_prompt]: 	Training 400/553. train loss: 5.8587,	0.8458 s / batch. (data: 1.20e-02). ETA=7:34:16, max mem: 20.9 GB 
[11/27 19:10:25 visual_prompt]: 	Training 500/553. train loss: 104.2383,	0.8240 s / batch. (data: 3.44e-04). ETA=7:21:12, max mem: 20.9 GB 
[11/27 19:11:17 visual_prompt]: Epoch 42 / 100: avg data time: 1.45e-01, avg batch time: 0.9792, average train loss: 14.3884
[11/27 19:12:13 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3113, average loss: 36.3718
[11/27 19:12:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.19	
[11/27 19:12:13 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[11/27 19:13:57 visual_prompt]: 	Training 100/553. train loss: 4.2328,	0.8351 s / batch. (data: 1.05e-02). ETA=7:25:00, max mem: 20.9 GB 
[11/27 19:15:33 visual_prompt]: 	Training 200/553. train loss: 17.3884,	0.8288 s / batch. (data: 3.22e-04). ETA=7:20:18, max mem: 20.9 GB 
[11/27 19:17:08 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8199 s / batch. (data: 3.19e-04). ETA=7:14:11, max mem: 20.9 GB 
[11/27 19:18:44 visual_prompt]: 	Training 400/553. train loss: 3.4249,	0.8205 s / batch. (data: 2.79e-04). ETA=7:13:07, max mem: 20.9 GB 
[11/27 19:20:23 visual_prompt]: 	Training 500/553. train loss: 29.2855,	0.8366 s / batch. (data: 5.41e-03). ETA=7:20:15, max mem: 20.9 GB 
[11/27 19:21:15 visual_prompt]: Epoch 43 / 100: avg data time: 1.45e-01, avg batch time: 0.9792, average train loss: 12.4036
[11/27 19:22:11 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3103, average loss: 24.4953
[11/27 19:22:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.87	
[11/27 19:22:11 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[11/27 19:23:52 visual_prompt]: 	Training 100/553. train loss: 0.9345,	0.8396 s / batch. (data: 1.05e-02). ETA=7:19:39, max mem: 20.9 GB 
[11/27 19:25:32 visual_prompt]: 	Training 200/553. train loss: 2.4608,	0.8520 s / batch. (data: 3.04e-04). ETA=7:24:45, max mem: 20.9 GB 
[11/27 19:27:07 visual_prompt]: 	Training 300/553. train loss: 8.1179,	0.8425 s / batch. (data: 3.01e-04). ETA=7:18:23, max mem: 20.9 GB 
[11/27 19:28:43 visual_prompt]: 	Training 400/553. train loss: 0.3930,	0.8600 s / batch. (data: 2.67e-02). ETA=7:26:05, max mem: 20.9 GB 
[11/27 19:30:20 visual_prompt]: 	Training 500/553. train loss: 7.7799,	0.8440 s / batch. (data: 7.95e-03). ETA=7:16:21, max mem: 20.9 GB 
[11/27 19:31:10 visual_prompt]: Epoch 44 / 100: avg data time: 1.41e-01, avg batch time: 0.9756, average train loss: 12.4275
[11/27 19:32:05 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3108, average loss: 14.9444
[11/27 19:32:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.96	
[11/27 19:32:05 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[11/27 19:33:49 visual_prompt]: 	Training 100/553. train loss: 7.5158,	0.8360 s / batch. (data: 3.00e-04). ETA=7:10:05, max mem: 20.9 GB 
[11/27 19:35:22 visual_prompt]: 	Training 200/553. train loss: 9.6313,	0.8526 s / batch. (data: 5.43e-03). ETA=7:17:12, max mem: 20.9 GB 
[11/27 19:37:01 visual_prompt]: 	Training 300/553. train loss: 2.9258,	0.8480 s / batch. (data: 3.04e-04). ETA=7:13:26, max mem: 20.9 GB 
[11/27 19:38:35 visual_prompt]: 	Training 400/553. train loss: 4.2912,	0.8280 s / batch. (data: 3.27e-04). ETA=7:01:49, max mem: 20.9 GB 
[11/27 19:40:14 visual_prompt]: 	Training 500/553. train loss: 0.7238,	0.8287 s / batch. (data: 5.45e-03). ETA=7:00:49, max mem: 20.9 GB 
[11/27 19:41:04 visual_prompt]: Epoch 45 / 100: avg data time: 1.39e-01, avg batch time: 0.9737, average train loss: 9.5140
[11/27 19:42:00 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3090, average loss: 18.0233
[11/27 19:42:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.83	
[11/27 19:42:00 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[11/27 19:43:42 visual_prompt]: 	Training 100/553. train loss: 17.1797,	1.0499 s / batch. (data: 2.15e-01). ETA=8:50:28, max mem: 20.9 GB 
[11/27 19:45:20 visual_prompt]: 	Training 200/553. train loss: 8.9671,	0.8336 s / batch. (data: 8.47e-04). ETA=6:59:46, max mem: 20.9 GB 
[11/27 19:46:56 visual_prompt]: 	Training 300/553. train loss: 12.5220,	0.8360 s / batch. (data: 2.87e-04). ETA=6:59:36, max mem: 20.9 GB 
[11/27 19:48:33 visual_prompt]: 	Training 400/553. train loss: 1.9691,	0.8453 s / batch. (data: 1.20e-02). ETA=7:02:51, max mem: 20.9 GB 
[11/27 19:50:07 visual_prompt]: 	Training 500/553. train loss: 40.2306,	0.8403 s / batch. (data: 5.44e-03). ETA=6:58:58, max mem: 20.9 GB 
[11/27 19:50:59 visual_prompt]: Epoch 46 / 100: avg data time: 1.41e-01, avg batch time: 0.9759, average train loss: 8.7812
[11/27 19:51:55 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3102, average loss: 6.7670
[11/27 19:51:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.64	
[11/27 19:51:55 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[11/27 19:53:37 visual_prompt]: 	Training 100/553. train loss: 6.5043,	0.8395 s / batch. (data: 1.12e-02). ETA=6:56:23, max mem: 20.9 GB 
[11/27 19:55:11 visual_prompt]: 	Training 200/553. train loss: 19.0623,	1.1638 s / batch. (data: 3.37e-01). ETA=9:35:19, max mem: 20.9 GB 
[11/27 19:56:49 visual_prompt]: 	Training 300/553. train loss: 2.0360,	0.8391 s / batch. (data: 3.29e-04). ETA=6:53:26, max mem: 20.9 GB 
[11/27 19:58:27 visual_prompt]: 	Training 400/553. train loss: 17.8990,	0.8416 s / batch. (data: 1.20e-02). ETA=6:53:15, max mem: 20.9 GB 
[11/27 20:00:03 visual_prompt]: 	Training 500/553. train loss: 15.1629,	0.8600 s / batch. (data: 2.99e-04). ETA=7:00:51, max mem: 20.9 GB 
[11/27 20:00:55 visual_prompt]: Epoch 47 / 100: avg data time: 1.41e-01, avg batch time: 0.9759, average train loss: 10.7019
[11/27 20:01:50 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3093, average loss: 13.3165
[11/27 20:01:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.68	
[11/27 20:01:50 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[11/27 20:03:32 visual_prompt]: 	Training 100/553. train loss: 0.7990,	0.8563 s / batch. (data: 1.56e-02). ETA=6:56:50, max mem: 20.9 GB 
[11/27 20:05:09 visual_prompt]: 	Training 200/553. train loss: 3.6159,	0.8251 s / batch. (data: 3.01e-04). ETA=6:40:18, max mem: 20.9 GB 
[11/27 20:06:47 visual_prompt]: 	Training 300/553. train loss: 23.6609,	1.4848 s / batch. (data: 6.67e-01). ETA=11:57:52, max mem: 20.9 GB 
[11/27 20:08:21 visual_prompt]: 	Training 400/553. train loss: 16.5899,	0.8280 s / batch. (data: 3.26e-04). ETA=6:38:56, max mem: 20.9 GB 
[11/27 20:09:58 visual_prompt]: 	Training 500/553. train loss: 6.4613,	0.8360 s / batch. (data: 3.36e-04). ETA=6:41:23, max mem: 20.9 GB 
[11/27 20:10:48 visual_prompt]: Epoch 48 / 100: avg data time: 1.37e-01, avg batch time: 0.9720, average train loss: 8.3151
[11/27 20:11:43 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3111, average loss: 10.6992
[11/27 20:11:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.03	
[11/27 20:11:43 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[11/27 20:13:24 visual_prompt]: 	Training 100/553. train loss: 7.3680,	0.8265 s / batch. (data: 7.23e-04). ETA=6:34:44, max mem: 20.9 GB 
[11/27 20:14:59 visual_prompt]: 	Training 200/553. train loss: 4.8656,	0.8438 s / batch. (data: 1.56e-02). ETA=6:41:35, max mem: 20.9 GB 
[11/27 20:16:37 visual_prompt]: 	Training 300/553. train loss: 0.0002,	0.8200 s / batch. (data: 2.47e-04). ETA=6:28:54, max mem: 20.9 GB 
[11/27 20:18:15 visual_prompt]: 	Training 400/553. train loss: 1.1833,	0.8374 s / batch. (data: 7.93e-03). ETA=6:35:46, max mem: 20.9 GB 
[11/27 20:19:52 visual_prompt]: 	Training 500/553. train loss: 1.8481,	0.8321 s / batch. (data: 2.64e-04). ETA=6:31:50, max mem: 20.9 GB 
[11/27 20:20:43 visual_prompt]: Epoch 49 / 100: avg data time: 1.41e-01, avg batch time: 0.9759, average train loss: 7.7641
[11/27 20:21:38 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3108, average loss: 4.4623
[11/27 20:21:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.18	
[11/27 20:21:38 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[11/27 20:23:21 visual_prompt]: 	Training 100/553. train loss: 11.2692,	0.8192 s / batch. (data: 3.12e-04). ETA=6:23:40, max mem: 20.9 GB 
[11/27 20:24:59 visual_prompt]: 	Training 200/553. train loss: 6.6445,	0.8400 s / batch. (data: 3.04e-04). ETA=6:32:02, max mem: 20.9 GB 
[11/27 20:26:34 visual_prompt]: 	Training 300/553. train loss: 16.0209,	0.8480 s / batch. (data: 1.20e-02). ETA=6:34:21, max mem: 20.9 GB 
[11/27 20:28:10 visual_prompt]: 	Training 400/553. train loss: 44.0007,	0.8501 s / batch. (data: 1.04e-02). ETA=6:33:56, max mem: 20.9 GB 
[11/27 20:29:49 visual_prompt]: 	Training 500/553. train loss: 2.4115,	0.8538 s / batch. (data: 3.15e-04). ETA=6:34:12, max mem: 20.9 GB 
[11/27 20:30:39 visual_prompt]: Epoch 50 / 100: avg data time: 1.43e-01, avg batch time: 0.9773, average train loss: 9.6166
[11/27 20:31:34 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3112, average loss: 7.1900
[11/27 20:31:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.40	
[11/27 20:31:34 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[11/27 20:33:16 visual_prompt]: 	Training 100/553. train loss: 0.7930,	1.1929 s / batch. (data: 3.30e-01). ETA=9:07:44, max mem: 20.9 GB 
[11/27 20:34:54 visual_prompt]: 	Training 200/553. train loss: 1.1470,	0.8640 s / batch. (data: 1.20e-02). ETA=6:35:16, max mem: 20.9 GB 
[11/27 20:36:32 visual_prompt]: 	Training 300/553. train loss: 9.3558,	0.8189 s / batch. (data: 3.16e-04). ETA=6:13:17, max mem: 20.9 GB 
[11/27 20:38:10 visual_prompt]: 	Training 400/553. train loss: 2.7052,	1.3846 s / batch. (data: 5.62e-01). ETA=10:28:50, max mem: 20.9 GB 
[11/27 20:39:47 visual_prompt]: 	Training 500/553. train loss: 3.3948,	0.8560 s / batch. (data: 3.96e-04). ETA=6:27:21, max mem: 20.9 GB 
[11/27 20:40:36 visual_prompt]: Epoch 51 / 100: avg data time: 1.46e-01, avg batch time: 0.9796, average train loss: 8.6294
[11/27 20:41:32 visual_prompt]: Inference (val):avg data time: 3.82e-04, avg batch time: 0.3118, average loss: 9.7046
[11/27 20:41:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.04	
[11/27 20:41:32 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[11/27 20:43:17 visual_prompt]: 	Training 100/553. train loss: 5.6225,	0.8415 s / batch. (data: 8.20e-04). ETA=6:18:38, max mem: 20.9 GB 
[11/27 20:44:53 visual_prompt]: 	Training 200/553. train loss: 2.4858,	0.8360 s / batch. (data: 3.18e-04). ETA=6:14:45, max mem: 20.9 GB 
[11/27 20:46:32 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8286 s / batch. (data: 2.73e-04). ETA=6:10:03, max mem: 20.9 GB 
[11/27 20:48:11 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8270 s / batch. (data: 3.21e-04). ETA=6:07:59, max mem: 20.9 GB 
[11/27 20:49:44 visual_prompt]: 	Training 500/553. train loss: 4.0076,	0.8400 s / batch. (data: 3.05e-04). ETA=6:12:22, max mem: 20.9 GB 
[11/27 20:50:33 visual_prompt]: Epoch 52 / 100: avg data time: 1.45e-01, avg batch time: 0.9792, average train loss: 9.5048
[11/27 20:51:29 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3113, average loss: 33.6538
[11/27 20:51:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.28	
[11/27 20:51:29 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[11/27 20:53:10 visual_prompt]: 	Training 100/553. train loss: 1.9284,	0.8292 s / batch. (data: 7.96e-03). ETA=6:05:28, max mem: 20.9 GB 
[11/27 20:54:48 visual_prompt]: 	Training 200/553. train loss: 17.4840,	0.8752 s / batch. (data: 1.57e-02). ETA=6:24:15, max mem: 20.9 GB 
[11/27 20:56:24 visual_prompt]: 	Training 300/553. train loss: 4.5768,	0.8320 s / batch. (data: 3.03e-04). ETA=6:03:54, max mem: 20.9 GB 
[11/27 20:58:04 visual_prompt]: 	Training 400/553. train loss: 27.4502,	0.8680 s / batch. (data: 3.81e-02). ETA=6:18:12, max mem: 20.9 GB 
[11/27 20:59:39 visual_prompt]: 	Training 500/553. train loss: 15.7527,	0.8323 s / batch. (data: 3.24e-04). ETA=6:01:15, max mem: 20.9 GB 
[11/27 21:00:30 visual_prompt]: Epoch 53 / 100: avg data time: 1.44e-01, avg batch time: 0.9786, average train loss: 8.4135
[11/27 21:01:25 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.3098, average loss: 13.4247
[11/27 21:01:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.34	
[11/27 21:01:25 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[11/27 21:03:08 visual_prompt]: 	Training 100/553. train loss: 6.2089,	0.8165 s / batch. (data: 3.25e-04). ETA=5:52:20, max mem: 20.9 GB 
[11/27 21:04:45 visual_prompt]: 	Training 200/553. train loss: 0.9815,	0.8432 s / batch. (data: 3.06e-04). ETA=6:02:28, max mem: 20.9 GB 
[11/27 21:06:20 visual_prompt]: 	Training 300/553. train loss: 12.3980,	0.8440 s / batch. (data: 3.17e-04). ETA=6:01:22, max mem: 20.9 GB 
[11/27 21:07:56 visual_prompt]: 	Training 400/553. train loss: 0.0163,	0.8319 s / batch. (data: 5.28e-04). ETA=5:54:49, max mem: 20.9 GB 
[11/27 21:09:33 visual_prompt]: 	Training 500/553. train loss: 6.7592,	0.8640 s / batch. (data: 3.22e-04). ETA=6:07:03, max mem: 20.9 GB 
[11/27 21:10:24 visual_prompt]: Epoch 54 / 100: avg data time: 1.40e-01, avg batch time: 0.9745, average train loss: 7.7581
[11/27 21:11:19 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3103, average loss: 4.7546
[11/27 21:11:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.15	
[11/27 21:11:19 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[11/27 21:13:00 visual_prompt]: 	Training 100/553. train loss: 3.4486,	0.8361 s / batch. (data: 2.72e-04). ETA=5:53:05, max mem: 20.9 GB 
[11/27 21:14:35 visual_prompt]: 	Training 200/553. train loss: 9.7509,	0.8511 s / batch. (data: 2.16e-02). ETA=5:58:00, max mem: 20.9 GB 
[11/27 21:16:13 visual_prompt]: 	Training 300/553. train loss: 15.2804,	0.8680 s / batch. (data: 7.95e-03). ETA=6:03:39, max mem: 20.9 GB 
[11/27 21:17:50 visual_prompt]: 	Training 400/553. train loss: 23.5790,	1.1000 s / batch. (data: 2.51e-01). ETA=7:39:01, max mem: 20.9 GB 
[11/27 21:19:25 visual_prompt]: 	Training 500/553. train loss: 4.6534,	1.0827 s / batch. (data: 2.16e-01). ETA=7:30:00, max mem: 20.9 GB 
[11/27 21:20:17 visual_prompt]: Epoch 55 / 100: avg data time: 1.36e-01, avg batch time: 0.9721, average train loss: 7.0936
[11/27 21:21:12 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3099, average loss: 3.5381
[11/27 21:21:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.52	
[11/27 21:21:12 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[11/27 21:22:54 visual_prompt]: 	Training 100/553. train loss: 21.8858,	0.8280 s / batch. (data: 3.21e-04). ETA=5:42:01, max mem: 20.9 GB 
[11/27 21:24:30 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8444 s / batch. (data: 5.41e-03). ETA=5:47:23, max mem: 20.9 GB 
[11/27 21:26:08 visual_prompt]: 	Training 300/553. train loss: 8.2923,	0.8521 s / batch. (data: 2.89e-04). ETA=5:49:07, max mem: 20.9 GB 
[11/27 21:27:46 visual_prompt]: 	Training 400/553. train loss: 3.0326,	0.8627 s / batch. (data: 2.11e-02). ETA=5:52:02, max mem: 20.9 GB 
[11/27 21:29:22 visual_prompt]: 	Training 500/553. train loss: 3.9480,	1.7920 s / batch. (data: 9.47e-01). ETA=12:08:17, max mem: 20.9 GB 
[11/27 21:30:12 visual_prompt]: Epoch 56 / 100: avg data time: 1.40e-01, avg batch time: 0.9750, average train loss: 7.8403
[11/27 21:31:07 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3115, average loss: 5.8923
[11/27 21:31:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.80	
[11/27 21:31:07 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[11/27 21:32:51 visual_prompt]: 	Training 100/553. train loss: 13.1561,	0.8370 s / batch. (data: 7.68e-04). ETA=5:38:01, max mem: 20.9 GB 
[11/27 21:34:28 visual_prompt]: 	Training 200/553. train loss: 2.6305,	0.8434 s / batch. (data: 8.00e-04). ETA=5:39:13, max mem: 20.9 GB 
[11/27 21:36:04 visual_prompt]: 	Training 300/553. train loss: 0.2899,	0.8409 s / batch. (data: 3.14e-04). ETA=5:36:48, max mem: 20.9 GB 
[11/27 21:37:40 visual_prompt]: 	Training 400/553. train loss: 3.7319,	0.8639 s / batch. (data: 1.19e-02). ETA=5:44:35, max mem: 20.9 GB 
[11/27 21:39:14 visual_prompt]: 	Training 500/553. train loss: 9.5495,	0.8461 s / batch. (data: 1.55e-02). ETA=5:36:04, max mem: 20.9 GB 
[11/27 21:40:06 visual_prompt]: Epoch 57 / 100: avg data time: 1.40e-01, avg batch time: 0.9753, average train loss: 6.2053
[11/27 21:41:02 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.3093, average loss: 10.1785
[11/27 21:41:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.08	
[11/27 21:41:02 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[11/27 21:42:43 visual_prompt]: 	Training 100/553. train loss: 0.6905,	1.1835 s / batch. (data: 3.56e-01). ETA=7:47:04, max mem: 20.9 GB 
[11/27 21:44:21 visual_prompt]: 	Training 200/553. train loss: 2.1621,	0.8424 s / batch. (data: 7.93e-03). ETA=5:31:02, max mem: 20.9 GB 
[11/27 21:46:01 visual_prompt]: 	Training 300/553. train loss: 11.4583,	0.8192 s / batch. (data: 8.34e-04). ETA=5:20:33, max mem: 20.9 GB 
[11/27 21:47:37 visual_prompt]: 	Training 400/553. train loss: 1.1007,	0.8320 s / batch. (data: 2.97e-04). ETA=5:24:11, max mem: 20.9 GB 
[11/27 21:49:13 visual_prompt]: 	Training 500/553. train loss: 9.4521,	0.8195 s / batch. (data: 3.01e-04). ETA=5:17:56, max mem: 20.9 GB 
[11/27 21:50:02 visual_prompt]: Epoch 58 / 100: avg data time: 1.43e-01, avg batch time: 0.9772, average train loss: 6.1276
[11/27 21:50:57 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.3099, average loss: 21.7529
[11/27 21:50:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.84	
[11/27 21:50:57 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[11/27 21:52:40 visual_prompt]: 	Training 100/553. train loss: 0.6619,	0.8175 s / batch. (data: 4.43e-04). ETA=5:15:04, max mem: 20.9 GB 
[11/27 21:54:17 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8329 s / batch. (data: 3.37e-04). ETA=5:19:39, max mem: 20.9 GB 
[11/27 21:55:52 visual_prompt]: 	Training 300/553. train loss: 15.4370,	0.8280 s / batch. (data: 3.00e-04). ETA=5:16:23, max mem: 20.9 GB 
[11/27 21:57:28 visual_prompt]: 	Training 400/553. train loss: 4.9001,	0.8680 s / batch. (data: 3.23e-04). ETA=5:30:12, max mem: 20.9 GB 
[11/27 21:59:07 visual_prompt]: 	Training 500/553. train loss: 6.7450,	0.8551 s / batch. (data: 1.19e-03). ETA=5:23:52, max mem: 20.9 GB 
[11/27 21:59:55 visual_prompt]: Epoch 59 / 100: avg data time: 1.38e-01, avg batch time: 0.9721, average train loss: 5.8436
[11/27 22:00:50 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3106, average loss: 10.6170
[11/27 22:00:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.77	
[11/27 22:00:50 visual_prompt]: Stopping early.
[11/27 22:00:51 visual_prompt]: Rank of current process: 0. World size: 1
[11/27 22:00:51 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/27 22:00:51 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/27 22:00:51 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/27 22:00:51 visual_prompt]: Training with config:
[11/27 22:00:51 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr5.0_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/27 22:00:51 visual_prompt]: Loading training data...
[11/27 22:00:51 visual_prompt]: Constructing mammo-cbis dataset train...
[11/27 22:00:51 visual_prompt]: Loading validation data...
[11/27 22:00:51 visual_prompt]: Constructing mammo-cbis dataset val...
[11/27 22:00:51 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/27 22:00:53 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/27 22:00:53 visual_prompt]: tuned percent:0.525
[11/27 22:00:54 visual_prompt]: Device used for model: 0
[11/27 22:00:54 visual_prompt]: Setting up Evaluator...
[11/27 22:00:54 visual_prompt]: Setting up Trainer...
[11/27 22:00:54 visual_prompt]: 	Setting up the optimizer...
[11/27 22:00:54 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/27 22:02:34 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8440 s / batch. (data: 7.96e-03). ETA=12:56:30, max mem: 20.9 GB 
[11/27 22:04:10 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8339 s / batch. (data: 1.19e-02). ETA=12:45:47, max mem: 20.9 GB 
[11/27 22:05:49 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8994 s / batch. (data: 6.69e-02). ETA=13:44:29, max mem: 20.9 GB 
[11/27 22:07:24 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8440 s / batch. (data: 7.96e-03). ETA=12:52:16, max mem: 20.9 GB 
[11/27 22:09:02 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8192 s / batch. (data: 2.93e-04). ETA=12:28:13, max mem: 20.9 GB 
[11/27 22:09:54 visual_prompt]: Epoch 1 / 100: avg data time: 1.42e-01, avg batch time: 0.9762, average train loss: 1.5403
[11/27 22:10:49 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3091, average loss: 1.5201
[11/27 22:10:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/27 22:10:49 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/27 22:12:29 visual_prompt]: 	Training 100/553. train loss: 3.5035,	0.8480 s / batch. (data: 3.62e-04). ETA=12:52:20, max mem: 20.9 GB 
[11/27 22:14:06 visual_prompt]: 	Training 200/553. train loss: 0.0003,	0.8419 s / batch. (data: 5.49e-03). ETA=12:45:22, max mem: 20.9 GB 
[11/27 22:15:44 visual_prompt]: 	Training 300/553. train loss: 0.8492,	0.9195 s / batch. (data: 8.57e-02). ETA=13:54:25, max mem: 20.9 GB 
[11/27 22:17:20 visual_prompt]: 	Training 400/553. train loss: 4.5294,	0.8401 s / batch. (data: 3.21e-04). ETA=12:40:59, max mem: 20.9 GB 
[11/27 22:18:59 visual_prompt]: 	Training 500/553. train loss: 0.4798,	0.8190 s / batch. (data: 2.88e-04). ETA=12:20:26, max mem: 20.9 GB 
[11/27 22:19:48 visual_prompt]: Epoch 2 / 100: avg data time: 1.41e-01, avg batch time: 0.9746, average train loss: 2.0718
[11/27 22:20:44 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3114, average loss: 1.9715
[11/27 22:20:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.22	
[11/27 22:20:44 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/27 22:22:24 visual_prompt]: 	Training 100/553. train loss: 0.6295,	0.8428 s / batch. (data: 2.28e-02). ETA=12:39:52, max mem: 20.9 GB 
[11/27 22:24:02 visual_prompt]: 	Training 200/553. train loss: 0.9610,	1.3360 s / batch. (data: 4.94e-01). ETA=20:02:16, max mem: 20.9 GB 
[11/27 22:25:38 visual_prompt]: 	Training 300/553. train loss: 2.4749,	0.8208 s / batch. (data: 2.62e-04). ETA=12:17:14, max mem: 20.9 GB 
[11/27 22:27:16 visual_prompt]: 	Training 400/553. train loss: 0.2250,	0.8192 s / batch. (data: 3.02e-04). ETA=12:14:30, max mem: 20.9 GB 
[11/27 22:28:54 visual_prompt]: 	Training 500/553. train loss: 1.0738,	1.0118 s / batch. (data: 1.80e-01). ETA=15:05:29, max mem: 20.9 GB 
[11/27 22:29:44 visual_prompt]: Epoch 3 / 100: avg data time: 1.42e-01, avg batch time: 0.9758, average train loss: 2.3930
[11/27 22:30:39 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.3097, average loss: 2.9331
[11/27 22:30:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.74	
[11/27 22:30:39 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/27 22:32:21 visual_prompt]: 	Training 100/553. train loss: 2.0854,	0.8765 s / batch. (data: 2.85e-02). ETA=13:02:09, max mem: 20.9 GB 
[11/27 22:33:58 visual_prompt]: 	Training 200/553. train loss: 1.2467,	0.8330 s / batch. (data: 5.43e-03). ETA=12:21:54, max mem: 20.9 GB 
[11/27 22:35:36 visual_prompt]: 	Training 300/553. train loss: 0.7352,	1.2760 s / batch. (data: 4.21e-01). ETA=18:54:23, max mem: 20.9 GB 
[11/27 22:37:09 visual_prompt]: 	Training 400/553. train loss: 3.3312,	0.8503 s / batch. (data: 3.40e-04). ETA=12:34:28, max mem: 20.9 GB 
[11/27 22:38:47 visual_prompt]: 	Training 500/553. train loss: 13.1551,	3.2401 s / batch. (data: 2.41e+00). ETA=1 day, 23:49:42, max mem: 20.9 GB 
[11/27 22:39:40 visual_prompt]: Epoch 4 / 100: avg data time: 1.43e-01, avg batch time: 0.9777, average train loss: 3.2573
[11/27 22:40:35 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3108, average loss: 3.1964
[11/27 22:40:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.71	
[11/27 22:40:35 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/27 22:42:15 visual_prompt]: 	Training 100/553. train loss: 4.0519,	0.8620 s / batch. (data: 5.46e-03). ETA=12:41:13, max mem: 20.9 GB 
[11/27 22:43:52 visual_prompt]: 	Training 200/553. train loss: 8.2089,	1.1192 s / batch. (data: 2.88e-01). ETA=16:26:31, max mem: 20.9 GB 
[11/27 22:45:30 visual_prompt]: 	Training 300/553. train loss: 2.9179,	0.8506 s / batch. (data: 1.48e-02). ETA=12:28:21, max mem: 20.9 GB 
[11/27 22:47:06 visual_prompt]: 	Training 400/553. train loss: 3.8703,	0.8516 s / batch. (data: 3.39e-04). ETA=12:27:48, max mem: 20.9 GB 
[11/27 22:48:43 visual_prompt]: 	Training 500/553. train loss: 9.9950,	0.8386 s / batch. (data: 5.44e-03). ETA=12:14:57, max mem: 20.9 GB 
[11/27 22:49:35 visual_prompt]: Epoch 5 / 100: avg data time: 1.41e-01, avg batch time: 0.9754, average train loss: 6.9270
[11/27 22:50:30 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3107, average loss: 4.2445
[11/27 22:50:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.30	
[11/27 22:50:30 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/27 22:52:13 visual_prompt]: 	Training 100/553. train loss: 4.6621,	0.8363 s / batch. (data: 7.79e-04). ETA=12:10:52, max mem: 20.9 GB 
[11/27 22:53:49 visual_prompt]: 	Training 200/553. train loss: 21.5458,	0.8480 s / batch. (data: 7.99e-03). ETA=12:19:40, max mem: 20.9 GB 
[11/27 22:55:24 visual_prompt]: 	Training 300/553. train loss: 1.0072,	0.8215 s / batch. (data: 3.15e-04). ETA=11:55:08, max mem: 20.9 GB 
[11/27 22:57:06 visual_prompt]: 	Training 400/553. train loss: 11.3812,	0.8396 s / batch. (data: 1.56e-02). ETA=12:09:32, max mem: 20.9 GB 
[11/27 22:58:41 visual_prompt]: 	Training 500/553. train loss: 8.4550,	0.8406 s / batch. (data: 3.35e-04). ETA=12:09:01, max mem: 20.9 GB 
[11/27 22:59:31 visual_prompt]: Epoch 6 / 100: avg data time: 1.44e-01, avg batch time: 0.9778, average train loss: 6.8165
[11/27 23:00:26 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3094, average loss: 2.8051
[11/27 23:00:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.58	
[11/27 23:00:26 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/27 23:02:06 visual_prompt]: 	Training 100/553. train loss: 6.3153,	0.8316 s / batch. (data: 3.60e-04). ETA=11:59:02, max mem: 20.9 GB 
[11/27 23:03:43 visual_prompt]: 	Training 200/553. train loss: 0.6921,	0.8434 s / batch. (data: 5.44e-03). ETA=12:07:50, max mem: 20.9 GB 
[11/27 23:05:24 visual_prompt]: 	Training 300/553. train loss: 3.9191,	1.6400 s / batch. (data: 7.88e-01). ETA=23:32:37, max mem: 20.9 GB 
[11/27 23:07:01 visual_prompt]: 	Training 400/553. train loss: 2.8755,	1.9876 s / batch. (data: 1.17e+00). ETA=1 day, 4:28:44, max mem: 20.9 GB 
[11/27 23:08:37 visual_prompt]: 	Training 500/553. train loss: 3.8488,	0.8429 s / batch. (data: 3.44e-04). ETA=12:03:12, max mem: 20.9 GB 
[11/27 23:09:27 visual_prompt]: Epoch 7 / 100: avg data time: 1.43e-01, avg batch time: 0.9771, average train loss: 4.8161
[11/27 23:10:23 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3111, average loss: 4.5820
[11/27 23:10:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.39	
[11/27 23:10:23 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/27 23:12:03 visual_prompt]: 	Training 100/553. train loss: 3.1925,	0.8480 s / batch. (data: 5.94e-03). ETA=12:05:26, max mem: 20.9 GB 
[11/27 23:13:41 visual_prompt]: 	Training 200/553. train loss: 21.3880,	0.8480 s / batch. (data: 3.35e-04). ETA=12:04:01, max mem: 20.9 GB 
[11/27 23:15:19 visual_prompt]: 	Training 300/553. train loss: 1.0200,	0.8600 s / batch. (data: 2.96e-04). ETA=12:12:50, max mem: 20.9 GB 
[11/27 23:16:56 visual_prompt]: 	Training 400/553. train loss: 13.3662,	0.8696 s / batch. (data: 3.53e-03). ETA=12:19:34, max mem: 20.9 GB 
[11/27 23:18:33 visual_prompt]: 	Training 500/553. train loss: 0.2706,	0.8561 s / batch. (data: 1.83e-02). ETA=12:06:38, max mem: 20.9 GB 
[11/27 23:19:26 visual_prompt]: Epoch 8 / 100: avg data time: 1.46e-01, avg batch time: 0.9812, average train loss: 8.8876
[11/27 23:20:21 visual_prompt]: Inference (val):avg data time: 2.07e-04, avg batch time: 0.3108, average loss: 1.0892
[11/27 23:20:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.81	
[11/27 23:20:21 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/27 23:22:03 visual_prompt]: 	Training 100/553. train loss: 0.0001,	0.8180 s / batch. (data: 3.26e-04). ETA=11:32:12, max mem: 20.9 GB 
[11/27 23:23:39 visual_prompt]: 	Training 200/553. train loss: 3.8978,	0.8331 s / batch. (data: 3.31e-04). ETA=11:43:36, max mem: 20.9 GB 
[11/27 23:25:17 visual_prompt]: 	Training 300/553. train loss: 3.0510,	1.4513 s / batch. (data: 6.35e-01). ETA=20:23:19, max mem: 20.9 GB 
[11/27 23:26:55 visual_prompt]: 	Training 400/553. train loss: 1.7741,	0.8394 s / batch. (data: 3.25e-04). ETA=11:46:08, max mem: 20.9 GB 
[11/27 23:28:33 visual_prompt]: 	Training 500/553. train loss: 5.9535,	0.8519 s / batch. (data: 3.71e-04). ETA=11:55:17, max mem: 20.9 GB 
[11/27 23:29:23 visual_prompt]: Epoch 9 / 100: avg data time: 1.44e-01, avg batch time: 0.9792, average train loss: 7.6257
[11/27 23:30:19 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3103, average loss: 15.7628
[11/27 23:30:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.76	
[11/27 23:30:19 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/27 23:32:02 visual_prompt]: 	Training 100/553. train loss: 19.4007,	0.8480 s / batch. (data: 3.50e-04). ETA=11:49:48, max mem: 20.9 GB 
[11/27 23:33:37 visual_prompt]: 	Training 200/553. train loss: 0.5049,	0.8463 s / batch. (data: 1.05e-02). ETA=11:47:00, max mem: 20.9 GB 
[11/27 23:35:14 visual_prompt]: 	Training 300/553. train loss: 6.9126,	2.1720 s / batch. (data: 1.33e+00). ETA=1 day, 6:10:50, max mem: 20.9 GB 
[11/27 23:36:49 visual_prompt]: 	Training 400/553. train loss: 13.4654,	0.8584 s / batch. (data: 1.04e-02). ETA=11:54:15, max mem: 20.9 GB 
[11/27 23:38:27 visual_prompt]: 	Training 500/553. train loss: 2.0865,	0.8632 s / batch. (data: 5.44e-03). ETA=11:56:49, max mem: 20.9 GB 
[11/27 23:39:17 visual_prompt]: Epoch 10 / 100: avg data time: 1.40e-01, avg batch time: 0.9740, average train loss: 10.7126
[11/27 23:40:13 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3089, average loss: 5.2410
[11/27 23:40:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.73	
[11/27 23:40:13 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/27 23:41:57 visual_prompt]: 	Training 100/553. train loss: 12.9361,	0.8346 s / batch. (data: 1.05e-02). ETA=11:30:52, max mem: 20.9 GB 
[11/27 23:43:36 visual_prompt]: 	Training 200/553. train loss: 5.3582,	0.8480 s / batch. (data: 2.95e-04). ETA=11:40:35, max mem: 20.9 GB 
[11/27 23:45:14 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.0129 s / batch. (data: 1.19e+00). ETA=1 day, 3:39:38, max mem: 20.9 GB 
[11/27 23:46:49 visual_prompt]: 	Training 400/553. train loss: 7.4933,	0.8329 s / batch. (data: 2.60e-04). ETA=11:25:20, max mem: 20.9 GB 
[11/27 23:48:25 visual_prompt]: 	Training 500/553. train loss: 0.5588,	0.8203 s / batch. (data: 3.03e-04). ETA=11:13:34, max mem: 20.9 GB 
[11/27 23:49:15 visual_prompt]: Epoch 11 / 100: avg data time: 1.45e-01, avg batch time: 0.9793, average train loss: 8.1792
[11/27 23:50:11 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3102, average loss: 11.4596
[11/27 23:50:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.25	
[11/27 23:50:11 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/27 23:51:54 visual_prompt]: 	Training 100/553. train loss: 0.3006,	0.8680 s / batch. (data: 3.95e-02). ETA=11:50:34, max mem: 20.9 GB 
[11/27 23:53:32 visual_prompt]: 	Training 200/553. train loss: 5.7857,	0.8168 s / batch. (data: 3.27e-04). ETA=11:07:16, max mem: 20.9 GB 
[11/27 23:55:08 visual_prompt]: 	Training 300/553. train loss: 13.7108,	0.8189 s / batch. (data: 3.32e-04). ETA=11:07:38, max mem: 20.9 GB 
[11/27 23:56:46 visual_prompt]: 	Training 400/553. train loss: 19.6701,	0.8322 s / batch. (data: 3.14e-04). ETA=11:17:04, max mem: 20.9 GB 
[11/27 23:58:24 visual_prompt]: 	Training 500/553. train loss: 23.8542,	0.8434 s / batch. (data: 2.97e-04). ETA=11:24:47, max mem: 20.9 GB 
[11/27 23:59:13 visual_prompt]: Epoch 12 / 100: avg data time: 1.48e-01, avg batch time: 0.9812, average train loss: 15.1743
[11/28 00:00:09 visual_prompt]: Inference (val):avg data time: 5.04e-05, avg batch time: 0.3110, average loss: 90.2101
[11/28 00:00:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.14	
[11/28 00:00:09 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/28 00:01:53 visual_prompt]: 	Training 100/553. train loss: 1.5743,	0.8565 s / batch. (data: 1.56e-02). ETA=11:33:16, max mem: 20.9 GB 
[11/28 00:03:27 visual_prompt]: 	Training 200/553. train loss: 3.3038,	0.8452 s / batch. (data: 3.01e-04). ETA=11:22:43, max mem: 20.9 GB 
[11/28 00:05:05 visual_prompt]: 	Training 300/553. train loss: 8.8213,	1.5200 s / batch. (data: 6.82e-01). ETA=20:25:13, max mem: 20.9 GB 
[11/28 00:06:41 visual_prompt]: 	Training 400/553. train loss: 14.4938,	0.8640 s / batch. (data: 1.19e-02). ETA=11:34:59, max mem: 20.9 GB 
[11/28 00:08:19 visual_prompt]: 	Training 500/553. train loss: 3.1417,	0.8455 s / batch. (data: 2.55e-02). ETA=11:18:42, max mem: 20.9 GB 
[11/28 00:09:11 visual_prompt]: Epoch 13 / 100: avg data time: 1.44e-01, avg batch time: 0.9791, average train loss: 10.2507
[11/28 00:10:08 visual_prompt]: Inference (val):avg data time: 7.57e-04, avg batch time: 0.3098, average loss: 3.3825
[11/28 00:10:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.35	
[11/28 00:10:08 visual_prompt]: Best epoch 13: best metric: -3.383
[11/28 00:10:08 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/28 00:11:51 visual_prompt]: 	Training 100/553. train loss: 1.9361,	0.8529 s / batch. (data: 2.06e-02). ETA=11:22:27, max mem: 20.9 GB 
[11/28 00:13:28 visual_prompt]: 	Training 200/553. train loss: 0.0023,	0.9280 s / batch. (data: 7.35e-02). ETA=12:21:01, max mem: 20.9 GB 
[11/28 00:15:25 visual_prompt]: 	Training 300/553. train loss: 0.9843,	0.8440 s / batch. (data: 7.95e-03). ETA=11:12:31, max mem: 20.9 GB 
[11/28 00:17:02 visual_prompt]: 	Training 400/553. train loss: 11.0351,	0.8480 s / batch. (data: 3.23e-04). ETA=11:14:17, max mem: 20.9 GB 
[11/28 00:18:39 visual_prompt]: 	Training 500/553. train loss: 21.5263,	1.1971 s / batch. (data: 3.76e-01). ETA=15:49:57, max mem: 20.9 GB 
[11/28 00:19:29 visual_prompt]: Epoch 14 / 100: avg data time: 1.81e-01, avg batch time: 1.0141, average train loss: 9.0660
[11/28 00:20:24 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3121, average loss: 5.8439
[11/28 00:20:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.18	
[11/28 00:20:24 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/28 00:22:05 visual_prompt]: 	Training 100/553. train loss: 15.9422,	0.8325 s / batch. (data: 3.22e-04). ETA=10:58:28, max mem: 20.9 GB 
[11/28 00:23:40 visual_prompt]: 	Training 200/553. train loss: 24.2661,	0.8325 s / batch. (data: 3.04e-04). ETA=10:57:05, max mem: 20.9 GB 
[11/28 00:25:19 visual_prompt]: 	Training 300/553. train loss: 1.1437,	0.8542 s / batch. (data: 7.07e-04). ETA=11:12:45, max mem: 20.9 GB 
[11/28 00:26:53 visual_prompt]: 	Training 400/553. train loss: 1.0355,	0.8283 s / batch. (data: 3.59e-04). ETA=10:51:01, max mem: 20.9 GB 
[11/28 00:28:31 visual_prompt]: 	Training 500/553. train loss: 90.8781,	0.8493 s / batch. (data: 1.60e-02). ETA=11:06:07, max mem: 20.9 GB 
[11/28 00:29:22 visual_prompt]: Epoch 15 / 100: avg data time: 1.38e-01, avg batch time: 0.9727, average train loss: 17.6134
[11/28 00:30:17 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3108, average loss: 23.9815
[11/28 00:30:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.29	
[11/28 00:30:17 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/28 00:31:57 visual_prompt]: 	Training 100/553. train loss: 25.2271,	0.8176 s / batch. (data: 3.19e-04). ETA=10:39:08, max mem: 20.9 GB 
[11/28 00:33:35 visual_prompt]: 	Training 200/553. train loss: 25.7270,	0.8600 s / batch. (data: 1.60e-02). ETA=11:10:52, max mem: 20.9 GB 
[11/28 00:35:16 visual_prompt]: 	Training 300/553. train loss: 57.3220,	0.8165 s / batch. (data: 3.07e-04). ETA=10:35:32, max mem: 20.9 GB 
[11/28 00:36:54 visual_prompt]: 	Training 400/553. train loss: 5.9560,	0.8399 s / batch. (data: 3.08e-04). ETA=10:52:22, max mem: 20.9 GB 
[11/28 00:38:29 visual_prompt]: 	Training 500/553. train loss: 8.8824,	0.8590 s / batch. (data: 4.09e-02). ETA=11:05:50, max mem: 20.9 GB 
[11/28 00:39:21 visual_prompt]: Epoch 16 / 100: avg data time: 1.50e-01, avg batch time: 0.9833, average train loss: 14.8550
[11/28 00:40:17 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3103, average loss: 13.6446
[11/28 00:40:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.17	
[11/28 00:40:17 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/28 00:41:57 visual_prompt]: 	Training 100/553. train loss: 26.1969,	0.8114 s / batch. (data: 3.09e-04). ETA=10:26:47, max mem: 20.9 GB 
[11/28 00:43:35 visual_prompt]: 	Training 200/553. train loss: 56.1493,	0.8400 s / batch. (data: 2.89e-04). ETA=10:47:31, max mem: 20.9 GB 
[11/28 00:45:11 visual_prompt]: 	Training 300/553. train loss: 1.6608,	0.8399 s / batch. (data: 3.00e-04). ETA=10:46:03, max mem: 20.9 GB 
[11/28 00:46:47 visual_prompt]: 	Training 400/553. train loss: 58.0613,	0.9902 s / batch. (data: 1.73e-01). ETA=12:40:00, max mem: 20.9 GB 
[11/28 00:48:23 visual_prompt]: 	Training 500/553. train loss: 10.7328,	1.5249 s / batch. (data: 7.04e-01). ETA=19:27:50, max mem: 20.9 GB 
[11/28 00:49:15 visual_prompt]: Epoch 17 / 100: avg data time: 1.41e-01, avg batch time: 0.9737, average train loss: 18.0756
[11/28 00:50:11 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3096, average loss: 18.8841
[11/28 00:50:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.52	
[11/28 00:50:11 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/28 00:51:52 visual_prompt]: 	Training 100/553. train loss: 27.9208,	0.8440 s / batch. (data: 7.96e-03). ETA=10:44:13, max mem: 20.9 GB 
[11/28 00:53:32 visual_prompt]: 	Training 200/553. train loss: 27.2477,	0.8499 s / batch. (data: 1.10e-02). ETA=10:47:17, max mem: 20.9 GB 
[11/28 00:55:09 visual_prompt]: 	Training 300/553. train loss: 2.7293,	0.8400 s / batch. (data: 7.96e-03). ETA=10:38:22, max mem: 20.9 GB 
[11/28 00:56:46 visual_prompt]: 	Training 400/553. train loss: 2.6912,	0.8280 s / batch. (data: 2.77e-04). ETA=10:27:52, max mem: 20.9 GB 
[11/28 00:58:23 visual_prompt]: 	Training 500/553. train loss: 39.4254,	0.8520 s / batch. (data: 7.95e-03). ETA=10:44:39, max mem: 20.9 GB 
[11/28 00:59:13 visual_prompt]: Epoch 18 / 100: avg data time: 1.47e-01, avg batch time: 0.9800, average train loss: 21.2450
[11/28 01:00:08 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3117, average loss: 20.1407
[11/28 01:00:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.29	
[11/28 01:00:08 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/28 01:01:50 visual_prompt]: 	Training 100/553. train loss: 1.8607,	1.3691 s / batch. (data: 5.53e-01). ETA=17:12:25, max mem: 20.9 GB 
[11/28 01:03:28 visual_prompt]: 	Training 200/553. train loss: 2.2904,	0.8396 s / batch. (data: 3.00e-04). ETA=10:31:43, max mem: 20.9 GB 
[11/28 01:05:06 visual_prompt]: 	Training 300/553. train loss: 7.7977,	0.8487 s / batch. (data: 2.40e-02). ETA=10:37:11, max mem: 20.9 GB 
[11/28 01:06:45 visual_prompt]: 	Training 400/553. train loss: 4.5759,	0.8399 s / batch. (data: 7.61e-04). ETA=10:29:11, max mem: 20.9 GB 
[11/28 01:08:17 visual_prompt]: 	Training 500/553. train loss: 25.7462,	0.8320 s / batch. (data: 3.19e-04). ETA=10:21:51, max mem: 20.9 GB 
[11/28 01:09:08 visual_prompt]: Epoch 19 / 100: avg data time: 1.41e-01, avg batch time: 0.9752, average train loss: 13.8323
[11/28 01:10:03 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3089, average loss: 33.4310
[11/28 01:10:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.05	
[11/28 01:10:03 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/28 01:11:42 visual_prompt]: 	Training 100/553. train loss: 5.0464,	0.8460 s / batch. (data: 7.98e-03). ETA=10:30:11, max mem: 20.9 GB 
[11/28 01:13:20 visual_prompt]: 	Training 200/553. train loss: 8.0982,	0.8191 s / batch. (data: 3.03e-04). ETA=10:08:46, max mem: 20.9 GB 
[11/28 01:14:58 visual_prompt]: 	Training 300/553. train loss: 6.0087,	0.8520 s / batch. (data: 3.11e-04). ETA=10:31:48, max mem: 20.9 GB 
[11/28 01:16:34 visual_prompt]: 	Training 400/553. train loss: 1.3213,	0.8560 s / batch. (data: 3.09e-04). ETA=10:33:20, max mem: 20.9 GB 
[11/28 01:18:10 visual_prompt]: 	Training 500/553. train loss: 14.1554,	0.8254 s / batch. (data: 5.50e-03). ETA=10:09:17, max mem: 20.9 GB 
[11/28 01:19:02 visual_prompt]: Epoch 20 / 100: avg data time: 1.42e-01, avg batch time: 0.9748, average train loss: 15.2412
[11/28 01:19:58 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3096, average loss: 41.8419
[11/28 01:19:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.45	
[11/28 01:19:58 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[11/28 01:21:40 visual_prompt]: 	Training 100/553. train loss: 1.8224,	0.8400 s / batch. (data: 3.21e-04). ETA=10:17:57, max mem: 20.9 GB 
[11/28 01:23:16 visual_prompt]: 	Training 200/553. train loss: 2.8603,	0.8183 s / batch. (data: 3.19e-04). ETA=10:00:39, max mem: 20.9 GB 
[11/28 01:24:53 visual_prompt]: 	Training 300/553. train loss: 6.2862,	0.8479 s / batch. (data: 1.19e-02). ETA=10:20:58, max mem: 20.9 GB 
[11/28 01:26:29 visual_prompt]: 	Training 400/553. train loss: 6.4808,	0.8265 s / batch. (data: 5.42e-03). ETA=10:03:54, max mem: 20.9 GB 
[11/28 01:28:06 visual_prompt]: 	Training 500/553. train loss: 13.3490,	0.8481 s / batch. (data: 2.74e-04). ETA=10:18:13, max mem: 20.9 GB 
[11/28 01:28:56 visual_prompt]: Epoch 21 / 100: avg data time: 1.39e-01, avg batch time: 0.9725, average train loss: 12.1980
[11/28 01:29:51 visual_prompt]: Inference (val):avg data time: 2.78e-04, avg batch time: 0.3116, average loss: 3.1222
[11/28 01:29:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.84	
[11/28 01:29:51 visual_prompt]: Best epoch 21: best metric: -3.122
[11/28 01:29:51 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[11/28 01:31:31 visual_prompt]: 	Training 100/553. train loss: 0.8268,	0.8320 s / batch. (data: 7.96e-03). ETA=10:04:24, max mem: 20.9 GB 
[11/28 01:33:08 visual_prompt]: 	Training 200/553. train loss: 16.9778,	0.8206 s / batch. (data: 2.72e-04). ETA=9:54:47, max mem: 20.9 GB 
[11/28 01:34:43 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8400 s / batch. (data: 3.98e-03). ETA=10:07:23, max mem: 20.9 GB 
[11/28 01:36:21 visual_prompt]: 	Training 400/553. train loss: 1.0901,	0.8214 s / batch. (data: 3.13e-04). ETA=9:52:33, max mem: 20.9 GB 
[11/28 01:37:59 visual_prompt]: 	Training 500/553. train loss: 1.1772,	0.8519 s / batch. (data: 3.47e-04). ETA=10:13:12, max mem: 20.9 GB 
[11/28 01:38:51 visual_prompt]: Epoch 22 / 100: avg data time: 1.42e-01, avg batch time: 0.9759, average train loss: 7.3076
[11/28 01:39:45 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3094, average loss: 2.9483
[11/28 01:39:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.49	
[11/28 01:39:45 visual_prompt]: Best epoch 22: best metric: -2.948
[11/28 01:39:45 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[11/28 01:41:27 visual_prompt]: 	Training 100/553. train loss: 1.9362,	0.8320 s / batch. (data: 5.47e-03). ETA=9:56:43, max mem: 20.9 GB 
[11/28 01:43:05 visual_prompt]: 	Training 200/553. train loss: 60.2175,	0.8320 s / batch. (data: 1.20e-02). ETA=9:55:21, max mem: 20.9 GB 
[11/28 01:44:43 visual_prompt]: 	Training 300/553. train loss: 0.7793,	0.8564 s / batch. (data: 6.21e-03). ETA=10:11:24, max mem: 20.9 GB 
[11/28 01:46:19 visual_prompt]: 	Training 400/553. train loss: 8.3411,	0.8360 s / batch. (data: 3.06e-04). ETA=9:55:26, max mem: 20.9 GB 
[11/28 01:47:55 visual_prompt]: 	Training 500/553. train loss: 1.2086,	0.8482 s / batch. (data: 3.14e-04). ETA=10:02:41, max mem: 20.9 GB 
[11/28 01:48:45 visual_prompt]: Epoch 23 / 100: avg data time: 1.43e-01, avg batch time: 0.9752, average train loss: 12.7887
[11/28 01:49:40 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3102, average loss: 2.1401
[11/28 01:49:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.53	
[11/28 01:49:40 visual_prompt]: Best epoch 23: best metric: -2.140
[11/28 01:49:40 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[11/28 01:51:19 visual_prompt]: 	Training 100/553. train loss: 9.5365,	0.9804 s / batch. (data: 1.41e-01). ETA=11:34:06, max mem: 20.9 GB 
[11/28 01:52:56 visual_prompt]: 	Training 200/553. train loss: 9.0446,	0.8186 s / batch. (data: 2.90e-04). ETA=9:38:13, max mem: 20.9 GB 
[11/28 01:54:33 visual_prompt]: 	Training 300/553. train loss: 2.1329,	0.8760 s / batch. (data: 4.62e-02). ETA=10:17:18, max mem: 20.9 GB 
[11/28 01:56:10 visual_prompt]: 	Training 400/553. train loss: 12.1201,	0.8377 s / batch. (data: 7.96e-03). ETA=9:48:54, max mem: 20.9 GB 
[11/28 01:57:49 visual_prompt]: 	Training 500/553. train loss: 47.7156,	0.8111 s / batch. (data: 3.51e-04). ETA=9:28:51, max mem: 20.9 GB 
[11/28 01:58:40 visual_prompt]: Epoch 24 / 100: avg data time: 1.42e-01, avg batch time: 0.9763, average train loss: 12.5920
[11/28 01:59:36 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3102, average loss: 7.1258
[11/28 01:59:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.65	
[11/28 01:59:36 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[11/28 02:01:20 visual_prompt]: 	Training 100/553. train loss: 1.2235,	0.8613 s / batch. (data: 1.55e-02). ETA=10:01:51, max mem: 20.9 GB 
[11/28 02:02:55 visual_prompt]: 	Training 200/553. train loss: 8.7227,	0.8440 s / batch. (data: 7.97e-03). ETA=9:48:22, max mem: 20.9 GB 
[11/28 02:04:32 visual_prompt]: 	Training 300/553. train loss: 2.6724,	0.8215 s / batch. (data: 3.42e-04). ETA=9:31:17, max mem: 20.9 GB 
[11/28 02:06:09 visual_prompt]: 	Training 400/553. train loss: 1.7222,	1.2560 s / batch. (data: 4.32e-01). ETA=14:31:25, max mem: 20.9 GB 
[11/28 02:07:47 visual_prompt]: 	Training 500/553. train loss: 17.6020,	1.6524 s / batch. (data: 8.22e-01). ETA=19:03:40, max mem: 20.9 GB 
[11/28 02:08:38 visual_prompt]: Epoch 25 / 100: avg data time: 1.46e-01, avg batch time: 0.9799, average train loss: 16.1932
[11/28 02:09:34 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3095, average loss: 15.1664
[11/28 02:09:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.57	
[11/28 02:09:34 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[11/28 02:11:16 visual_prompt]: 	Training 100/553. train loss: 10.9355,	0.8171 s / batch. (data: 3.15e-04). ETA=9:23:27, max mem: 20.9 GB 
[11/28 02:12:54 visual_prompt]: 	Training 200/553. train loss: 48.2566,	1.6719 s / batch. (data: 8.34e-01). ETA=19:10:07, max mem: 20.9 GB 
[11/28 02:14:33 visual_prompt]: 	Training 300/553. train loss: 0.4749,	0.8248 s / batch. (data: 2.95e-04). ETA=9:26:02, max mem: 20.9 GB 
[11/28 02:16:10 visual_prompt]: 	Training 400/553. train loss: 1.9479,	0.8255 s / batch. (data: 2.97e-04). ETA=9:25:07, max mem: 20.9 GB 
[11/28 02:17:45 visual_prompt]: 	Training 500/553. train loss: 38.6370,	0.8323 s / batch. (data: 3.14e-04). ETA=9:28:23, max mem: 20.9 GB 
[11/28 02:18:36 visual_prompt]: Epoch 26 / 100: avg data time: 1.46e-01, avg batch time: 0.9802, average train loss: 9.0953
[11/28 02:19:31 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3115, average loss: 13.0650
[11/28 02:19:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.30	
[11/28 02:19:31 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[11/28 02:21:13 visual_prompt]: 	Training 100/553. train loss: 12.2426,	0.8358 s / batch. (data: 5.43e-03). ETA=9:28:39, max mem: 20.9 GB 
[11/28 02:22:50 visual_prompt]: 	Training 200/553. train loss: 53.9378,	0.8962 s / batch. (data: 7.35e-02). ETA=10:08:14, max mem: 20.9 GB 
[11/28 02:24:27 visual_prompt]: 	Training 300/553. train loss: 9.5595,	0.8189 s / batch. (data: 3.17e-04). ETA=9:14:25, max mem: 20.9 GB 
[11/28 02:26:06 visual_prompt]: 	Training 400/553. train loss: 0.7024,	0.8439 s / batch. (data: 2.56e-02). ETA=9:29:57, max mem: 20.9 GB 
[11/28 02:27:44 visual_prompt]: 	Training 500/553. train loss: 2.0934,	0.8338 s / batch. (data: 3.04e-04). ETA=9:21:44, max mem: 20.9 GB 
[11/28 02:28:32 visual_prompt]: Epoch 27 / 100: avg data time: 1.45e-01, avg batch time: 0.9782, average train loss: 9.9040
[11/28 02:29:28 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3105, average loss: 7.5517
[11/28 02:29:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.10	
[11/28 02:29:28 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[11/28 02:31:08 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8280 s / batch. (data: 3.35e-04). ETA=9:15:43, max mem: 20.9 GB 
[11/28 02:32:46 visual_prompt]: 	Training 200/553. train loss: 24.6397,	0.8352 s / batch. (data: 1.71e-02). ETA=9:19:08, max mem: 20.9 GB 
[11/28 02:34:25 visual_prompt]: 	Training 300/553. train loss: 27.1145,	1.5041 s / batch. (data: 6.76e-01). ETA=16:44:27, max mem: 20.9 GB 
[11/28 02:36:01 visual_prompt]: 	Training 400/553. train loss: 6.8131,	0.8440 s / batch. (data: 2.74e-04). ETA=9:22:14, max mem: 20.9 GB 
[11/28 02:37:38 visual_prompt]: 	Training 500/553. train loss: 21.5959,	0.8400 s / batch. (data: 7.73e-04). ETA=9:18:10, max mem: 20.9 GB 
[11/28 02:38:28 visual_prompt]: Epoch 28 / 100: avg data time: 1.44e-01, avg batch time: 0.9778, average train loss: 11.7015
[11/28 02:39:24 visual_prompt]: Inference (val):avg data time: 9.73e-05, avg batch time: 0.3103, average loss: 6.2508
[11/28 02:39:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.10	
[11/28 02:39:24 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[11/28 02:41:12 visual_prompt]: 	Training 100/553. train loss: 15.6510,	0.8280 s / batch. (data: 2.70e-04). ETA=9:08:04, max mem: 20.9 GB 
[11/28 02:42:49 visual_prompt]: 	Training 200/553. train loss: 8.5501,	1.7611 s / batch. (data: 9.39e-01). ETA=19:22:46, max mem: 20.9 GB 
[11/28 02:44:23 visual_prompt]: 	Training 300/553. train loss: 3.5969,	0.8361 s / batch. (data: 3.04e-04). ETA=9:10:38, max mem: 20.9 GB 
[11/28 02:45:58 visual_prompt]: 	Training 400/553. train loss: 21.7936,	0.8300 s / batch. (data: 3.65e-04). ETA=9:05:16, max mem: 20.9 GB 
[11/28 02:47:36 visual_prompt]: 	Training 500/553. train loss: 11.5920,	0.8354 s / batch. (data: 5.12e-04). ETA=9:07:23, max mem: 20.9 GB 
[11/28 02:48:27 visual_prompt]: Epoch 29 / 100: avg data time: 1.48e-01, avg batch time: 0.9815, average train loss: 8.9642
[11/28 02:49:23 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3110, average loss: 2.9152
[11/28 02:49:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.47	
[11/28 02:49:23 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[11/28 02:51:03 visual_prompt]: 	Training 100/553. train loss: 3.1779,	0.8514 s / batch. (data: 1.14e-02). ETA=9:15:43, max mem: 20.9 GB 
[11/28 02:52:41 visual_prompt]: 	Training 200/553. train loss: 5.0161,	0.8323 s / batch. (data: 1.17e-02). ETA=9:01:53, max mem: 20.9 GB 
[11/28 02:54:17 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.2544 s / batch. (data: 4.17e-01). ETA=13:34:33, max mem: 20.9 GB 
[11/28 02:55:56 visual_prompt]: 	Training 400/553. train loss: 4.8886,	0.9720 s / batch. (data: 1.13e-01). ETA=10:29:35, max mem: 20.9 GB 
[11/28 02:57:32 visual_prompt]: 	Training 500/553. train loss: 2.4504,	1.3010 s / batch. (data: 4.68e-01). ETA=14:00:32, max mem: 20.9 GB 
[11/28 02:58:25 visual_prompt]: Epoch 30 / 100: avg data time: 1.46e-01, avg batch time: 0.9792, average train loss: 12.4585
[11/28 02:59:20 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3115, average loss: 1.8207
[11/28 02:59:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.72	
[11/28 02:59:20 visual_prompt]: Best epoch 30: best metric: -1.821
[11/28 02:59:20 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[11/28 03:01:03 visual_prompt]: 	Training 100/553. train loss: 9.1074,	0.8400 s / batch. (data: 3.23e-04). ETA=9:00:32, max mem: 20.9 GB 
[11/28 03:02:42 visual_prompt]: 	Training 200/553. train loss: 8.3460,	0.8604 s / batch. (data: 3.24e-02). ETA=9:12:13, max mem: 20.9 GB 
[11/28 03:04:17 visual_prompt]: 	Training 300/553. train loss: 14.9731,	0.8360 s / batch. (data: 2.96e-04). ETA=8:55:11, max mem: 20.9 GB 
[11/28 03:05:54 visual_prompt]: 	Training 400/553. train loss: 5.0719,	1.2823 s / batch. (data: 4.38e-01). ETA=13:38:44, max mem: 20.9 GB 
[11/28 03:07:31 visual_prompt]: 	Training 500/553. train loss: 2.2913,	0.8329 s / batch. (data: 3.22e-04). ETA=8:50:26, max mem: 20.9 GB 
[11/28 03:08:21 visual_prompt]: Epoch 31 / 100: avg data time: 1.44e-01, avg batch time: 0.9779, average train loss: 9.2928
[11/28 03:09:17 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3094, average loss: 3.2092
[11/28 03:09:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.53	
[11/28 03:09:17 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[11/28 03:11:00 visual_prompt]: 	Training 100/553. train loss: 0.6588,	0.8402 s / batch. (data: 2.79e-04). ETA=8:52:55, max mem: 20.9 GB 
[11/28 03:12:37 visual_prompt]: 	Training 200/553. train loss: 32.1023,	0.8850 s / batch. (data: 1.10e-02). ETA=9:19:52, max mem: 20.9 GB 
[11/28 03:14:17 visual_prompt]: 	Training 300/553. train loss: 7.0660,	0.8173 s / batch. (data: 3.13e-04). ETA=8:35:40, max mem: 20.9 GB 
[11/28 03:15:54 visual_prompt]: 	Training 400/553. train loss: 4.8223,	0.8320 s / batch. (data: 2.91e-04). ETA=8:43:33, max mem: 20.9 GB 
[11/28 03:17:29 visual_prompt]: 	Training 500/553. train loss: 10.7617,	0.8400 s / batch. (data: 3.02e-04). ETA=8:47:12, max mem: 20.9 GB 
[11/28 03:18:18 visual_prompt]: Epoch 32 / 100: avg data time: 1.44e-01, avg batch time: 0.9776, average train loss: 9.7804
[11/28 03:19:13 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3110, average loss: 3.2846
[11/28 03:19:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.36	
[11/28 03:19:13 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[11/28 03:20:53 visual_prompt]: 	Training 100/553. train loss: 34.7115,	0.8362 s / batch. (data: 1.20e-02). ETA=8:42:41, max mem: 20.9 GB 
[11/28 03:22:32 visual_prompt]: 	Training 200/553. train loss: 38.2422,	1.0240 s / batch. (data: 1.86e-01). ETA=10:38:22, max mem: 20.9 GB 
[11/28 03:24:09 visual_prompt]: 	Training 300/553. train loss: 14.4613,	0.8377 s / batch. (data: 1.64e-02). ETA=8:40:49, max mem: 20.9 GB 
[11/28 03:25:47 visual_prompt]: 	Training 400/553. train loss: 5.4463,	0.8331 s / batch. (data: 1.17e-02). ETA=8:36:33, max mem: 20.9 GB 
[11/28 03:27:24 visual_prompt]: 	Training 500/553. train loss: 7.4308,	0.8546 s / batch. (data: 5.44e-03). ETA=8:48:27, max mem: 20.9 GB 
[11/28 03:28:14 visual_prompt]: Epoch 33 / 100: avg data time: 1.45e-01, avg batch time: 0.9785, average train loss: 19.5149
[11/28 03:29:10 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3101, average loss: 24.6686
[11/28 03:29:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.60	
[11/28 03:29:10 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[11/28 03:30:53 visual_prompt]: 	Training 100/553. train loss: 1.7167,	0.8341 s / batch. (data: 3.36e-04). ETA=8:33:41, max mem: 20.9 GB 
[11/28 03:32:29 visual_prompt]: 	Training 200/553. train loss: 14.8736,	0.8438 s / batch. (data: 1.05e-02). ETA=8:38:14, max mem: 20.9 GB 
[11/28 03:34:05 visual_prompt]: 	Training 300/553. train loss: 20.2678,	0.8560 s / batch. (data: 1.19e-02). ETA=8:44:18, max mem: 20.9 GB 
[11/28 03:35:43 visual_prompt]: 	Training 400/553. train loss: 8.5092,	0.8721 s / batch. (data: 2.94e-04). ETA=8:52:41, max mem: 20.9 GB 
[11/28 03:37:20 visual_prompt]: 	Training 500/553. train loss: 3.7622,	1.3040 s / batch. (data: 4.64e-01). ETA=13:14:22, max mem: 20.9 GB 
[11/28 03:38:10 visual_prompt]: Epoch 34 / 100: avg data time: 1.42e-01, avg batch time: 0.9762, average train loss: 12.8303
[11/28 03:39:05 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3109, average loss: 1.4369
[11/28 03:39:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.19	
[11/28 03:39:05 visual_prompt]: Best epoch 34: best metric: -1.437
[11/28 03:39:05 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[11/28 03:40:48 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8301 s / batch. (data: 1.17e-02). ETA=8:23:35, max mem: 20.9 GB 
[11/28 03:42:27 visual_prompt]: 	Training 200/553. train loss: 27.4742,	0.8527 s / batch. (data: 2.46e-02). ETA=8:35:49, max mem: 20.9 GB 
[11/28 03:44:01 visual_prompt]: 	Training 300/553. train loss: 29.2407,	0.8400 s / batch. (data: 2.95e-04). ETA=8:26:47, max mem: 20.9 GB 
[11/28 03:45:37 visual_prompt]: 	Training 400/553. train loss: 15.7028,	0.8350 s / batch. (data: 7.34e-04). ETA=8:22:23, max mem: 20.9 GB 
[11/28 03:47:12 visual_prompt]: 	Training 500/553. train loss: 4.1077,	0.8607 s / batch. (data: 3.39e-04). ETA=8:36:23, max mem: 20.9 GB 
[11/28 03:48:04 visual_prompt]: Epoch 35 / 100: avg data time: 1.40e-01, avg batch time: 0.9741, average train loss: 11.6603
[11/28 03:49:00 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3135, average loss: 23.6302
[11/28 03:49:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.73	
[11/28 03:49:00 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[11/28 03:50:41 visual_prompt]: 	Training 100/553. train loss: 7.9954,	0.8560 s / batch. (data: 7.94e-03). ETA=8:31:23, max mem: 20.9 GB 
[11/28 03:52:22 visual_prompt]: 	Training 200/553. train loss: 13.3117,	0.8445 s / batch. (data: 2.44e-02). ETA=8:23:05, max mem: 20.9 GB 
[11/28 03:54:04 visual_prompt]: 	Training 300/553. train loss: 0.1916,	0.8400 s / batch. (data: 3.00e-04). ETA=8:19:01, max mem: 20.9 GB 
[11/28 03:55:42 visual_prompt]: 	Training 400/553. train loss: 0.6452,	0.8466 s / batch. (data: 3.29e-04). ETA=8:21:32, max mem: 20.9 GB 
[11/28 03:57:21 visual_prompt]: 	Training 500/553. train loss: 0.7493,	0.8950 s / batch. (data: 7.67e-02). ETA=8:48:41, max mem: 20.9 GB 
[11/28 03:58:10 visual_prompt]: Epoch 36 / 100: avg data time: 1.61e-01, avg batch time: 0.9946, average train loss: 8.5697
[11/28 03:59:07 visual_prompt]: Inference (val):avg data time: 2.07e-04, avg batch time: 0.3104, average loss: 18.4189
[11/28 03:59:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.89	
[11/28 03:59:07 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[11/28 04:00:49 visual_prompt]: 	Training 100/553. train loss: 1.4602,	0.8679 s / batch. (data: 4.76e-04). ETA=8:30:28, max mem: 20.9 GB 
[11/28 04:02:25 visual_prompt]: 	Training 200/553. train loss: 15.3461,	0.8521 s / batch. (data: 7.96e-03). ETA=8:19:46, max mem: 20.9 GB 
[11/28 04:04:02 visual_prompt]: 	Training 300/553. train loss: 15.0805,	1.3146 s / batch. (data: 4.85e-01). ETA=12:48:53, max mem: 20.9 GB 
[11/28 04:05:43 visual_prompt]: 	Training 400/553. train loss: 8.8508,	1.5840 s / batch. (data: 7.56e-01). ETA=15:23:46, max mem: 20.9 GB 
[11/28 04:07:16 visual_prompt]: 	Training 500/553. train loss: 16.4601,	0.9960 s / batch. (data: 1.49e-01). ETA=9:39:12, max mem: 20.9 GB 
[11/28 04:08:08 visual_prompt]: Epoch 37 / 100: avg data time: 1.45e-01, avg batch time: 0.9789, average train loss: 10.1057
[11/28 04:09:04 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3109, average loss: 0.8617
[11/28 04:09:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.60	
[11/28 04:09:04 visual_prompt]: Best epoch 37: best metric: -0.862
[11/28 04:09:04 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[11/28 04:10:43 visual_prompt]: 	Training 100/553. train loss: 2.4563,	0.8400 s / batch. (data: 3.13e-04). ETA=8:06:21, max mem: 20.9 GB 
[11/28 04:12:20 visual_prompt]: 	Training 200/553. train loss: 1.2625,	0.8194 s / batch. (data: 3.21e-04). ETA=7:53:04, max mem: 20.9 GB 
[11/28 04:13:59 visual_prompt]: 	Training 300/553. train loss: 0.6433,	0.8252 s / batch. (data: 3.14e-04). ETA=7:55:00, max mem: 20.9 GB 
[11/28 04:15:34 visual_prompt]: 	Training 400/553. train loss: 0.0056,	0.8474 s / batch. (data: 2.81e-02). ETA=8:06:24, max mem: 20.9 GB 
[11/28 04:17:13 visual_prompt]: 	Training 500/553. train loss: 10.5995,	0.8240 s / batch. (data: 3.29e-04). ETA=7:51:34, max mem: 20.9 GB 
[11/28 04:18:02 visual_prompt]: Epoch 38 / 100: avg data time: 1.38e-01, avg batch time: 0.9732, average train loss: 9.1150
[11/28 04:18:57 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3106, average loss: 3.2880
[11/28 04:18:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.51	
[11/28 04:18:57 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[11/28 04:20:36 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8433 s / batch. (data: 7.96e-03). ETA=8:00:30, max mem: 20.9 GB 
[11/28 04:22:16 visual_prompt]: 	Training 200/553. train loss: 6.0318,	0.8308 s / batch. (data: 3.10e-04). ETA=7:52:00, max mem: 20.9 GB 
[11/28 04:23:56 visual_prompt]: 	Training 300/553. train loss: 1.6360,	0.8543 s / batch. (data: 1.42e-02). ETA=8:03:54, max mem: 20.9 GB 
[11/28 04:25:30 visual_prompt]: 	Training 400/553. train loss: 0.5821,	0.8290 s / batch. (data: 2.94e-04). ETA=7:48:12, max mem: 20.9 GB 
[11/28 04:27:08 visual_prompt]: 	Training 500/553. train loss: 1.3739,	1.3240 s / batch. (data: 4.85e-01). ETA=12:25:32, max mem: 20.9 GB 
[11/28 04:27:57 visual_prompt]: Epoch 39 / 100: avg data time: 1.42e-01, avg batch time: 0.9761, average train loss: 5.6180
[11/28 04:28:52 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3087, average loss: 1.1674
[11/28 04:28:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.95	
[11/28 04:28:52 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[11/28 04:30:33 visual_prompt]: 	Training 100/553. train loss: 16.0991,	0.8312 s / batch. (data: 3.07e-04). ETA=7:45:57, max mem: 20.9 GB 
[11/28 04:32:09 visual_prompt]: 	Training 200/553. train loss: 6.9398,	0.8440 s / batch. (data: 5.43e-03). ETA=7:51:40, max mem: 20.9 GB 
[11/28 04:33:47 visual_prompt]: 	Training 300/553. train loss: 4.2776,	0.8274 s / batch. (data: 3.33e-04). ETA=7:41:03, max mem: 20.9 GB 
[11/28 04:35:25 visual_prompt]: 	Training 400/553. train loss: 7.2788,	0.8640 s / batch. (data: 8.96e-04). ETA=8:00:00, max mem: 20.9 GB 
[11/28 04:37:01 visual_prompt]: 	Training 500/553. train loss: 0.1957,	0.8440 s / batch. (data: 3.34e-04). ETA=7:47:28, max mem: 20.9 GB 
[11/28 04:37:53 visual_prompt]: Epoch 40 / 100: avg data time: 1.43e-01, avg batch time: 0.9779, average train loss: 8.9193
[11/28 04:38:47 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3108, average loss: 6.3768
[11/28 04:38:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.94	
[11/28 04:38:47 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[11/28 04:40:33 visual_prompt]: 	Training 100/553. train loss: 2.3122,	0.8244 s / batch. (data: 8.40e-03). ETA=7:34:32, max mem: 20.9 GB 
[11/28 04:42:12 visual_prompt]: 	Training 200/553. train loss: 1.0709,	0.8400 s / batch. (data: 2.96e-04). ETA=7:41:42, max mem: 20.9 GB 
[11/28 04:43:47 visual_prompt]: 	Training 300/553. train loss: 1.7893,	0.8199 s / batch. (data: 3.02e-04). ETA=7:29:19, max mem: 20.9 GB 
[11/28 04:45:22 visual_prompt]: 	Training 400/553. train loss: 0.7781,	0.8320 s / batch. (data: 2.88e-04). ETA=7:34:33, max mem: 20.9 GB 
[11/28 04:46:56 visual_prompt]: 	Training 500/553. train loss: 2.0934,	0.8320 s / batch. (data: 3.17e-04). ETA=7:33:09, max mem: 20.9 GB 
[11/28 04:47:45 visual_prompt]: Epoch 41 / 100: avg data time: 1.37e-01, avg batch time: 0.9715, average train loss: 9.9561
[11/28 04:48:40 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3104, average loss: 6.7181
[11/28 04:48:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.14	
[11/28 04:48:40 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[11/28 04:50:20 visual_prompt]: 	Training 100/553. train loss: 2.5553,	0.8170 s / batch. (data: 2.92e-04). ETA=7:22:53, max mem: 20.9 GB 
[11/28 04:51:58 visual_prompt]: 	Training 200/553. train loss: 2.2946,	0.8480 s / batch. (data: 3.23e-04). ETA=7:38:17, max mem: 20.9 GB 
[11/28 04:53:36 visual_prompt]: 	Training 300/553. train loss: 44.8118,	0.8194 s / batch. (data: 3.10e-04). ETA=7:21:27, max mem: 20.9 GB 
[11/28 04:55:13 visual_prompt]: 	Training 400/553. train loss: 12.1536,	0.8194 s / batch. (data: 3.29e-04). ETA=7:20:07, max mem: 20.9 GB 
[11/28 04:56:50 visual_prompt]: 	Training 500/553. train loss: 0.0021,	0.8297 s / batch. (data: 2.80e-04). ETA=7:24:15, max mem: 20.9 GB 
[11/28 04:57:42 visual_prompt]: Epoch 42 / 100: avg data time: 1.44e-01, avg batch time: 0.9794, average train loss: 6.4853
[11/28 04:58:37 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3104, average loss: 7.4987
[11/28 04:58:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.33	
[11/28 04:58:37 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[11/28 05:00:21 visual_prompt]: 	Training 100/553. train loss: 0.5445,	0.8367 s / batch. (data: 5.91e-03). ETA=7:25:52, max mem: 20.9 GB 
[11/28 05:01:57 visual_prompt]: 	Training 200/553. train loss: 0.9264,	0.8192 s / batch. (data: 3.30e-04). ETA=7:15:09, max mem: 20.9 GB 
[11/28 05:03:32 visual_prompt]: 	Training 300/553. train loss: 5.0809,	0.8399 s / batch. (data: 3.12e-04). ETA=7:24:47, max mem: 20.9 GB 
[11/28 05:05:07 visual_prompt]: 	Training 400/553. train loss: 1.0758,	0.8320 s / batch. (data: 3.01e-04). ETA=7:19:12, max mem: 20.9 GB 
[11/28 05:06:46 visual_prompt]: 	Training 500/553. train loss: 1.8287,	0.8281 s / batch. (data: 5.42e-03). ETA=7:15:46, max mem: 20.9 GB 
[11/28 05:07:38 visual_prompt]: Epoch 43 / 100: avg data time: 1.44e-01, avg batch time: 0.9772, average train loss: 3.7523
[11/28 05:08:33 visual_prompt]: Inference (val):avg data time: 3.07e-04, avg batch time: 0.3092, average loss: 1.0851
[11/28 05:08:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.34	
[11/28 05:08:33 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[11/28 05:10:14 visual_prompt]: 	Training 100/553. train loss: 1.5960,	1.5346 s / batch. (data: 6.99e-01). ETA=13:23:39, max mem: 20.9 GB 
[11/28 05:11:53 visual_prompt]: 	Training 200/553. train loss: 8.2660,	0.8329 s / batch. (data: 3.00e-04). ETA=7:14:47, max mem: 20.9 GB 
[11/28 05:13:28 visual_prompt]: 	Training 300/553. train loss: 4.5938,	0.8280 s / batch. (data: 3.12e-04). ETA=7:10:52, max mem: 20.9 GB 
[11/28 05:15:05 visual_prompt]: 	Training 400/553. train loss: 15.1157,	0.8320 s / batch. (data: 3.50e-04). ETA=7:11:32, max mem: 20.9 GB 
[11/28 05:16:43 visual_prompt]: 	Training 500/553. train loss: 3.6249,	0.8440 s / batch. (data: 3.16e-04). ETA=7:16:21, max mem: 20.9 GB 
[11/28 05:17:33 visual_prompt]: Epoch 44 / 100: avg data time: 1.43e-01, avg batch time: 0.9774, average train loss: 10.5993
[11/28 05:18:28 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.3097, average loss: 16.9967
[11/28 05:18:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.53	
[11/28 05:18:28 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[11/28 05:20:10 visual_prompt]: 	Training 100/553. train loss: 10.2564,	0.8596 s / batch. (data: 6.77e-04). ETA=7:22:12, max mem: 20.9 GB 
[11/28 05:21:43 visual_prompt]: 	Training 200/553. train loss: 6.4706,	0.8473 s / batch. (data: 2.72e-04). ETA=7:14:28, max mem: 20.9 GB 
[11/28 05:23:21 visual_prompt]: 	Training 300/553. train loss: 8.1185,	0.8519 s / batch. (data: 3.00e-04). ETA=7:15:26, max mem: 20.9 GB 
[11/28 05:24:57 visual_prompt]: 	Training 400/553. train loss: 5.1428,	0.8424 s / batch. (data: 5.41e-03). ETA=7:09:09, max mem: 20.9 GB 
[11/28 05:26:36 visual_prompt]: 	Training 500/553. train loss: 4.8661,	0.8318 s / batch. (data: 3.19e-04). ETA=7:02:22, max mem: 20.9 GB 
[11/28 05:27:27 visual_prompt]: Epoch 45 / 100: avg data time: 1.39e-01, avg batch time: 0.9739, average train loss: 5.8682
[11/28 05:28:21 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3107, average loss: 1.5019
[11/28 05:28:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.04	
[11/28 05:28:21 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[11/28 05:30:02 visual_prompt]: 	Training 100/553. train loss: 4.5672,	1.2040 s / batch. (data: 3.63e-01). ETA=10:08:19, max mem: 20.9 GB 
[11/28 05:31:41 visual_prompt]: 	Training 200/553. train loss: 11.4203,	0.8289 s / batch. (data: 3.05e-04). ETA=6:57:26, max mem: 20.9 GB 
[11/28 05:33:16 visual_prompt]: 	Training 300/553. train loss: 3.6374,	0.8276 s / batch. (data: 5.40e-03). ETA=6:55:21, max mem: 20.9 GB 
[11/28 05:34:55 visual_prompt]: 	Training 400/553. train loss: 2.0307,	0.8439 s / batch. (data: 7.96e-04). ETA=7:02:09, max mem: 20.9 GB 
[11/28 05:36:27 visual_prompt]: 	Training 500/553. train loss: 31.1081,	0.8483 s / batch. (data: 3.32e-04). ETA=7:02:55, max mem: 20.9 GB 
[11/28 05:37:21 visual_prompt]: Epoch 46 / 100: avg data time: 1.41e-01, avg batch time: 0.9762, average train loss: 5.2327
[11/28 05:38:17 visual_prompt]: Inference (val):avg data time: 2.12e-04, avg batch time: 0.3100, average loss: 3.8369
[11/28 05:38:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.58	
[11/28 05:38:17 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[11/28 05:39:59 visual_prompt]: 	Training 100/553. train loss: 7.1362,	0.8242 s / batch. (data: 3.45e-04). ETA=6:48:50, max mem: 20.9 GB 
[11/28 05:41:33 visual_prompt]: 	Training 200/553. train loss: 11.4090,	1.3080 s / batch. (data: 4.77e-01). ETA=10:46:37, max mem: 20.9 GB 
[11/28 05:43:10 visual_prompt]: 	Training 300/553. train loss: 4.9667,	0.8265 s / batch. (data: 5.43e-03). ETA=6:47:13, max mem: 20.9 GB 
[11/28 05:44:49 visual_prompt]: 	Training 400/553. train loss: 1.0379,	0.8307 s / batch. (data: 3.94e-04). ETA=6:47:54, max mem: 20.9 GB 
[11/28 05:46:24 visual_prompt]: 	Training 500/553. train loss: 12.5615,	0.8440 s / batch. (data: 2.95e-04). ETA=6:53:01, max mem: 20.9 GB 
[11/28 05:47:16 visual_prompt]: Epoch 47 / 100: avg data time: 1.40e-01, avg batch time: 0.9747, average train loss: 5.7454
[11/28 05:48:11 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.3079, average loss: 0.6999
[11/28 05:48:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 43.50	rocauc: 56.76	
[11/28 05:48:11 visual_prompt]: Best epoch 47: best metric: -0.700
[11/28 05:48:11 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[11/28 05:49:52 visual_prompt]: 	Training 100/553. train loss: 3.1295,	0.8520 s / batch. (data: 2.93e-04). ETA=6:54:46, max mem: 20.9 GB 
[11/28 05:51:29 visual_prompt]: 	Training 200/553. train loss: 0.5968,	0.8400 s / batch. (data: 2.85e-04). ETA=6:47:32, max mem: 20.9 GB 
[11/28 05:53:07 visual_prompt]: 	Training 300/553. train loss: 5.2766,	1.6678 s / batch. (data: 8.50e-01). ETA=13:26:20, max mem: 20.9 GB 
[11/28 05:54:41 visual_prompt]: 	Training 400/553. train loss: 0.0635,	0.8479 s / batch. (data: 3.54e-04). ETA=6:48:33, max mem: 20.9 GB 
[11/28 05:56:18 visual_prompt]: 	Training 500/553. train loss: 5.1195,	0.8280 s / batch. (data: 2.90e-03). ETA=6:37:34, max mem: 20.9 GB 
[11/28 05:57:09 visual_prompt]: Epoch 48 / 100: avg data time: 1.40e-01, avg batch time: 0.9727, average train loss: 4.9506
[11/28 05:58:04 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3101, average loss: 11.4459
[11/28 05:58:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.97	
[11/28 05:58:04 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[11/28 05:59:45 visual_prompt]: 	Training 100/553. train loss: 4.8152,	0.8157 s / batch. (data: 3.23e-04). ETA=6:29:35, max mem: 20.9 GB 
[11/28 06:01:22 visual_prompt]: 	Training 200/553. train loss: 5.7341,	0.8466 s / batch. (data: 7.96e-03). ETA=6:42:55, max mem: 20.9 GB 
[11/28 06:03:00 visual_prompt]: 	Training 300/553. train loss: 7.1820,	0.8289 s / batch. (data: 3.12e-04). ETA=6:33:07, max mem: 20.9 GB 
[11/28 06:04:38 visual_prompt]: 	Training 400/553. train loss: 9.0521,	0.8200 s / batch. (data: 3.04e-04). ETA=6:27:31, max mem: 20.9 GB 
[11/28 06:06:16 visual_prompt]: 	Training 500/553. train loss: 0.6756,	0.8480 s / batch. (data: 7.97e-03). ETA=6:39:20, max mem: 20.9 GB 
[11/28 06:07:07 visual_prompt]: Epoch 49 / 100: avg data time: 1.48e-01, avg batch time: 0.9818, average train loss: 4.6919
[11/28 06:08:03 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3105, average loss: 1.8727
[11/28 06:08:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.81	
[11/28 06:08:03 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[11/28 06:09:46 visual_prompt]: 	Training 100/553. train loss: 0.7409,	0.8493 s / batch. (data: 7.74e-04). ETA=6:37:46, max mem: 20.9 GB 
[11/28 06:11:23 visual_prompt]: 	Training 200/553. train loss: 16.3499,	0.8399 s / batch. (data: 7.18e-04). ETA=6:31:58, max mem: 20.9 GB 
[11/28 06:12:59 visual_prompt]: 	Training 300/553. train loss: 3.4335,	0.8331 s / batch. (data: 1.56e-02). ETA=6:27:26, max mem: 20.9 GB 
[11/28 06:14:34 visual_prompt]: 	Training 400/553. train loss: 0.2178,	0.8401 s / batch. (data: 5.43e-03). ETA=6:29:16, max mem: 20.9 GB 
[11/28 06:16:13 visual_prompt]: 	Training 500/553. train loss: 7.9378,	0.8360 s / batch. (data: 3.09e-04). ETA=6:25:59, max mem: 20.9 GB 
[11/28 06:17:03 visual_prompt]: Epoch 50 / 100: avg data time: 1.44e-01, avg batch time: 0.9761, average train loss: 3.8544
[11/28 06:17:58 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3100, average loss: 1.1306
[11/28 06:17:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.34	
[11/28 06:17:58 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[11/28 06:19:39 visual_prompt]: 	Training 100/553. train loss: 3.7684,	1.1400 s / batch. (data: 2.98e-01). ETA=8:43:27, max mem: 20.9 GB 
[11/28 06:21:17 visual_prompt]: 	Training 200/553. train loss: 8.2651,	0.8236 s / batch. (data: 7.95e-03). ETA=6:16:47, max mem: 20.9 GB 
[11/28 06:22:55 visual_prompt]: 	Training 300/553. train loss: 0.5341,	0.8278 s / batch. (data: 1.05e-02). ETA=6:17:20, max mem: 20.9 GB 
[11/28 06:24:32 visual_prompt]: 	Training 400/553. train loss: 8.5789,	1.3960 s / batch. (data: 5.73e-01). ETA=10:34:00, max mem: 20.9 GB 
[11/28 06:26:09 visual_prompt]: 	Training 500/553. train loss: 0.8546,	0.8280 s / batch. (data: 3.43e-04). ETA=6:14:39, max mem: 20.9 GB 
[11/28 06:26:58 visual_prompt]: Epoch 51 / 100: avg data time: 1.43e-01, avg batch time: 0.9767, average train loss: 3.7384
[11/28 06:27:54 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3105, average loss: 0.6999
[11/28 06:27:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.23	
[11/28 06:27:54 visual_prompt]: Best epoch 51: best metric: -0.700
[11/28 06:27:54 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[11/28 06:29:38 visual_prompt]: 	Training 100/553. train loss: 11.8221,	0.8312 s / batch. (data: 5.48e-03). ETA=6:14:00, max mem: 20.9 GB 
[11/28 06:31:14 visual_prompt]: 	Training 200/553. train loss: 0.6621,	0.8359 s / batch. (data: 7.95e-03). ETA=6:14:42, max mem: 20.9 GB 
[11/28 06:32:51 visual_prompt]: 	Training 300/553. train loss: 0.3386,	0.8515 s / batch. (data: 5.48e-03). ETA=6:20:16, max mem: 20.9 GB 
[11/28 06:34:29 visual_prompt]: 	Training 400/553. train loss: 7.0128,	0.8391 s / batch. (data: 1.05e-02). ETA=6:13:21, max mem: 20.9 GB 
[11/28 06:36:01 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8440 s / batch. (data: 2.98e-04). ETA=6:14:07, max mem: 20.9 GB 
[11/28 06:36:50 visual_prompt]: Epoch 52 / 100: avg data time: 1.38e-01, avg batch time: 0.9704, average train loss: 8.5044
[11/28 06:37:46 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3100, average loss: 16.2109
[11/28 06:37:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.94	
[11/28 06:37:46 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[11/28 06:39:28 visual_prompt]: 	Training 100/553. train loss: 1.3408,	0.8270 s / batch. (data: 3.03e-04). ETA=6:04:27, max mem: 20.9 GB 
[11/28 06:41:05 visual_prompt]: 	Training 200/553. train loss: 0.2647,	0.8302 s / batch. (data: 3.14e-04). ETA=6:04:30, max mem: 20.9 GB 
[11/28 06:42:40 visual_prompt]: 	Training 300/553. train loss: 9.9964,	0.8332 s / batch. (data: 1.20e-02). ETA=6:04:26, max mem: 20.9 GB 
[11/28 06:44:20 visual_prompt]: 	Training 400/553. train loss: 1.2182,	0.8360 s / batch. (data: 3.31e-04). ETA=6:04:16, max mem: 20.9 GB 
[11/28 06:45:55 visual_prompt]: 	Training 500/553. train loss: 7.5149,	0.8489 s / batch. (data: 2.04e-02). ETA=6:08:30, max mem: 20.9 GB 
[11/28 06:46:46 visual_prompt]: Epoch 53 / 100: avg data time: 1.43e-01, avg batch time: 0.9772, average train loss: 5.3401
[11/28 06:47:41 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.3116, average loss: 14.8889
[11/28 06:47:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.86	
[11/28 06:47:41 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[11/28 06:49:22 visual_prompt]: 	Training 100/553. train loss: 2.1315,	0.8384 s / batch. (data: 2.06e-02). ETA=6:01:46, max mem: 20.9 GB 
[11/28 06:50:59 visual_prompt]: 	Training 200/553. train loss: 10.6493,	0.8916 s / batch. (data: 2.78e-02). ETA=6:23:15, max mem: 20.9 GB 
[11/28 06:52:34 visual_prompt]: 	Training 300/553. train loss: 5.9574,	0.8371 s / batch. (data: 3.40e-04). ETA=5:58:24, max mem: 20.9 GB 
[11/28 06:54:10 visual_prompt]: 	Training 400/553. train loss: 4.9209,	0.8399 s / batch. (data: 3.32e-04). ETA=5:58:14, max mem: 20.9 GB 
[11/28 06:55:47 visual_prompt]: 	Training 500/553. train loss: 2.9053,	0.8337 s / batch. (data: 5.44e-03). ETA=5:54:12, max mem: 20.9 GB 
[11/28 06:56:38 visual_prompt]: Epoch 54 / 100: avg data time: 1.34e-01, avg batch time: 0.9699, average train loss: 5.2600
[11/28 06:57:33 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3092, average loss: 11.6650
[11/28 06:57:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.79	
[11/28 06:57:33 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[11/28 06:59:14 visual_prompt]: 	Training 100/553. train loss: 1.2028,	0.8240 s / batch. (data: 3.01e-04). ETA=5:47:58, max mem: 20.9 GB 
[11/28 07:00:50 visual_prompt]: 	Training 200/553. train loss: 16.6755,	0.8480 s / batch. (data: 3.04e-04). ETA=5:56:41, max mem: 20.9 GB 
[11/28 07:02:27 visual_prompt]: 	Training 300/553. train loss: 3.5134,	0.8487 s / batch. (data: 1.56e-02). ETA=5:55:34, max mem: 20.9 GB 
[11/28 07:04:04 visual_prompt]: 	Training 400/553. train loss: 10.3159,	1.4872 s / batch. (data: 6.53e-01). ETA=10:20:37, max mem: 20.9 GB 
[11/28 07:05:39 visual_prompt]: 	Training 500/553. train loss: 4.8559,	0.8575 s / batch. (data: 5.46e-03). ETA=5:56:23, max mem: 20.9 GB 
[11/28 07:06:31 visual_prompt]: Epoch 55 / 100: avg data time: 1.37e-01, avg batch time: 0.9716, average train loss: 5.5410
[11/28 07:07:26 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3112, average loss: 9.3833
[11/28 07:07:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.64	
[11/28 07:07:26 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[11/28 07:09:09 visual_prompt]: 	Training 100/553. train loss: 1.1103,	0.8216 s / batch. (data: 3.03e-04). ETA=5:39:23, max mem: 20.9 GB 
[11/28 07:10:44 visual_prompt]: 	Training 200/553. train loss: 0.2164,	0.8395 s / batch. (data: 3.08e-04). ETA=5:45:22, max mem: 20.9 GB 
[11/28 07:12:24 visual_prompt]: 	Training 300/553. train loss: 1.0083,	0.8360 s / batch. (data: 3.41e-04). ETA=5:42:33, max mem: 20.9 GB 
[11/28 07:14:02 visual_prompt]: 	Training 400/553. train loss: 1.0088,	0.8716 s / batch. (data: 5.90e-03). ETA=5:55:41, max mem: 20.9 GB 
[11/28 07:15:39 visual_prompt]: 	Training 500/553. train loss: 4.3350,	2.1640 s / batch. (data: 1.33e+00). ETA=14:39:28, max mem: 20.9 GB 
[11/28 07:16:28 visual_prompt]: Epoch 56 / 100: avg data time: 1.45e-01, avg batch time: 0.9785, average train loss: 3.4236
[11/28 07:17:23 visual_prompt]: Inference (val):avg data time: 5.88e-05, avg batch time: 0.3112, average loss: 4.8735
[11/28 07:17:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.04	
[11/28 07:17:23 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[11/28 07:19:07 visual_prompt]: 	Training 100/553. train loss: 0.6384,	0.8307 s / batch. (data: 2.72e-04). ETA=5:35:28, max mem: 20.9 GB 
[11/28 07:20:44 visual_prompt]: 	Training 200/553. train loss: 0.6226,	0.8390 s / batch. (data: 5.44e-03). ETA=5:37:26, max mem: 20.9 GB 
[11/28 07:22:20 visual_prompt]: 	Training 300/553. train loss: 0.0616,	0.8480 s / batch. (data: 1.99e-02). ETA=5:39:38, max mem: 20.9 GB 
[11/28 07:23:56 visual_prompt]: 	Training 400/553. train loss: 1.2673,	0.8480 s / batch. (data: 3.85e-04). ETA=5:38:14, max mem: 20.9 GB 
[11/28 07:25:31 visual_prompt]: 	Training 500/553. train loss: 1.4663,	0.8506 s / batch. (data: 1.04e-02). ETA=5:37:52, max mem: 20.9 GB 
[11/28 07:26:24 visual_prompt]: Epoch 57 / 100: avg data time: 1.42e-01, avg batch time: 0.9771, average train loss: 3.2758
[11/28 07:27:19 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3102, average loss: 6.1833
[11/28 07:27:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.85	
[11/28 07:27:19 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[11/28 07:29:01 visual_prompt]: 	Training 100/553. train loss: 1.7930,	1.0594 s / batch. (data: 2.32e-01). ETA=6:58:05, max mem: 20.9 GB 
[11/28 07:30:39 visual_prompt]: 	Training 200/553. train loss: 2.8647,	0.8367 s / batch. (data: 5.51e-03). ETA=5:28:47, max mem: 20.9 GB 
[11/28 07:32:18 visual_prompt]: 	Training 300/553. train loss: 3.5130,	0.8440 s / batch. (data: 1.20e-02). ETA=5:30:16, max mem: 20.9 GB 
[11/28 07:33:55 visual_prompt]: 	Training 400/553. train loss: 3.5694,	0.8288 s / batch. (data: 3.14e-04). ETA=5:22:56, max mem: 20.9 GB 
[11/28 07:35:31 visual_prompt]: 	Training 500/553. train loss: 4.1884,	0.8457 s / batch. (data: 3.00e-04). ETA=5:28:07, max mem: 20.9 GB 
[11/28 07:36:21 visual_prompt]: Epoch 58 / 100: avg data time: 1.45e-01, avg batch time: 0.9795, average train loss: 4.2337
[11/28 07:37:17 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3114, average loss: 1.2878
[11/28 07:37:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.34	
[11/28 07:37:17 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[11/28 07:39:00 visual_prompt]: 	Training 100/553. train loss: 1.5850,	0.8280 s / batch. (data: 4.96e-04). ETA=5:19:07, max mem: 20.9 GB 
[11/28 07:40:38 visual_prompt]: 	Training 200/553. train loss: 0.0001,	0.8309 s / batch. (data: 5.43e-03). ETA=5:18:51, max mem: 20.9 GB 
[11/28 07:42:15 visual_prompt]: 	Training 300/553. train loss: 6.9652,	0.8594 s / batch. (data: 5.61e-03). ETA=5:28:23, max mem: 20.9 GB 
[11/28 07:43:51 visual_prompt]: 	Training 400/553. train loss: 1.0044,	0.8325 s / batch. (data: 8.42e-03). ETA=5:16:42, max mem: 20.9 GB 
[11/28 07:45:30 visual_prompt]: 	Training 500/553. train loss: 4.3351,	0.8602 s / batch. (data: 1.09e-02). ETA=5:25:48, max mem: 20.9 GB 
[11/28 07:46:18 visual_prompt]: Epoch 59 / 100: avg data time: 1.44e-01, avg batch time: 0.9781, average train loss: 3.7559
[11/28 07:47:13 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3097, average loss: 2.3761
[11/28 07:47:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.99	
[11/28 07:47:13 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[11/28 07:48:53 visual_prompt]: 	Training 100/553. train loss: 2.9310,	0.8370 s / batch. (data: 1.55e-02). ETA=5:14:54, max mem: 20.9 GB 
[11/28 07:50:30 visual_prompt]: 	Training 200/553. train loss: 3.4186,	0.8360 s / batch. (data: 3.24e-04). ETA=5:13:07, max mem: 20.9 GB 
[11/28 07:52:06 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.9638 s / batch. (data: 1.12e+00). ETA=12:12:15, max mem: 20.9 GB 
[11/28 07:53:44 visual_prompt]: 	Training 400/553. train loss: 0.8414,	0.9359 s / batch. (data: 1.13e-01). ETA=5:47:24, max mem: 20.9 GB 
[11/28 07:55:22 visual_prompt]: 	Training 500/553. train loss: 0.6160,	0.8294 s / batch. (data: 3.19e-04). ETA=5:06:29, max mem: 20.9 GB 
[11/28 07:56:13 visual_prompt]: Epoch 60 / 100: avg data time: 1.44e-01, avg batch time: 0.9775, average train loss: 2.8352
[11/28 07:57:09 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3106, average loss: 8.2320
[11/28 07:57:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.87	
[11/28 07:57:09 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[11/28 07:58:50 visual_prompt]: 	Training 100/553. train loss: 2.5247,	0.8661 s / batch. (data: 4.93e-02). ETA=5:17:51, max mem: 20.9 GB 
[11/28 08:00:28 visual_prompt]: 	Training 200/553. train loss: 2.5481,	1.6803 s / batch. (data: 8.36e-01). ETA=10:13:51, max mem: 20.9 GB 
[11/28 08:02:05 visual_prompt]: 	Training 300/553. train loss: 8.1168,	1.3039 s / batch. (data: 4.66e-01). ETA=7:54:11, max mem: 20.9 GB 
[11/28 08:03:39 visual_prompt]: 	Training 400/553. train loss: 0.9167,	0.8299 s / batch. (data: 3.32e-04). ETA=5:00:24, max mem: 20.9 GB 
[11/28 08:05:18 visual_prompt]: 	Training 500/553. train loss: 2.1288,	2.6089 s / batch. (data: 1.79e+00). ETA=15:40:05, max mem: 20.9 GB 
[11/28 08:06:07 visual_prompt]: Epoch 61 / 100: avg data time: 1.40e-01, avg batch time: 0.9734, average train loss: 5.1663
[11/28 08:07:03 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3081, average loss: 4.0654
[11/28 08:07:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.58	
[11/28 08:07:03 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[11/28 08:08:44 visual_prompt]: 	Training 100/553. train loss: 0.6616,	0.8413 s / batch. (data: 3.00e-04). ETA=5:01:01, max mem: 20.9 GB 
[11/28 08:10:22 visual_prompt]: 	Training 200/553. train loss: 0.7322,	0.8389 s / batch. (data: 2.06e-02). ETA=4:58:44, max mem: 20.9 GB 
[11/28 08:11:57 visual_prompt]: 	Training 300/553. train loss: 3.3843,	0.8280 s / batch. (data: 3.40e-04). ETA=4:53:29, max mem: 20.9 GB 
[11/28 08:13:35 visual_prompt]: 	Training 400/553. train loss: 4.0434,	0.8288 s / batch. (data: 3.12e-04). ETA=4:52:23, max mem: 20.9 GB 
[11/28 08:15:10 visual_prompt]: 	Training 500/553. train loss: 2.6342,	0.8359 s / batch. (data: 7.84e-03). ETA=4:53:29, max mem: 20.9 GB 
[11/28 08:16:04 visual_prompt]: Epoch 62 / 100: avg data time: 1.44e-01, avg batch time: 0.9776, average train loss: 2.2123
[11/28 08:16:59 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.3111, average loss: 6.2874
[11/28 08:16:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.54	
[11/28 08:16:59 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[11/28 08:18:44 visual_prompt]: 	Training 100/553. train loss: 6.2589,	0.8492 s / batch. (data: 7.95e-03). ETA=4:56:00, max mem: 20.9 GB 
[11/28 08:20:24 visual_prompt]: 	Training 200/553. train loss: 5.9336,	0.8544 s / batch. (data: 2.24e-02). ETA=4:56:24, max mem: 20.9 GB 
[11/28 08:21:59 visual_prompt]: 	Training 300/553. train loss: 0.8044,	0.8280 s / batch. (data: 3.03e-04). ETA=4:45:51, max mem: 20.9 GB 
[11/28 08:23:31 visual_prompt]: 	Training 400/553. train loss: 2.8658,	0.8200 s / batch. (data: 3.10e-04). ETA=4:41:43, max mem: 20.9 GB 
[11/28 08:25:06 visual_prompt]: 	Training 500/553. train loss: 0.8137,	0.8397 s / batch. (data: 5.43e-03). ETA=4:47:04, max mem: 20.9 GB 
[11/28 08:25:55 visual_prompt]: Epoch 63 / 100: avg data time: 1.37e-01, avg batch time: 0.9700, average train loss: 2.6216
[11/28 08:26:51 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.3101, average loss: 2.3884
[11/28 08:26:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.03	
[11/28 08:26:51 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[11/28 08:28:34 visual_prompt]: 	Training 100/553. train loss: 0.0482,	0.8335 s / batch. (data: 8.08e-04). ETA=4:42:50, max mem: 20.9 GB 
[11/28 08:30:13 visual_prompt]: 	Training 200/553. train loss: 1.9375,	0.8631 s / batch. (data: 2.71e-02). ETA=4:51:27, max mem: 20.9 GB 
[11/28 08:31:47 visual_prompt]: 	Training 300/553. train loss: 8.7293,	0.8540 s / batch. (data: 5.37e-03). ETA=4:46:57, max mem: 20.9 GB 
[11/28 08:33:23 visual_prompt]: 	Training 400/553. train loss: 5.6863,	0.8360 s / batch. (data: 1.22e-02). ETA=4:39:30, max mem: 20.9 GB 
[11/28 08:34:59 visual_prompt]: 	Training 500/553. train loss: 0.1400,	0.8299 s / batch. (data: 2.89e-04). ETA=4:36:04, max mem: 20.9 GB 
[11/28 08:35:50 visual_prompt]: Epoch 64 / 100: avg data time: 1.41e-01, avg batch time: 0.9744, average train loss: 4.3293
[11/28 08:36:45 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3097, average loss: 8.6779
[11/28 08:36:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.02	
[11/28 08:36:45 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[11/28 08:38:29 visual_prompt]: 	Training 100/553. train loss: 4.5883,	0.8428 s / batch. (data: 1.08e-02). ETA=4:38:13, max mem: 20.9 GB 
[11/28 08:40:07 visual_prompt]: 	Training 200/553. train loss: 0.8529,	1.2918 s / batch. (data: 4.74e-01). ETA=7:04:19, max mem: 20.9 GB 
[11/28 08:41:42 visual_prompt]: 	Training 300/553. train loss: 1.6087,	1.0846 s / batch. (data: 2.65e-01). ETA=5:54:27, max mem: 20.9 GB 
[11/28 08:43:17 visual_prompt]: 	Training 400/553. train loss: 2.3950,	0.8360 s / batch. (data: 2.90e-04). ETA=4:31:49, max mem: 20.9 GB 
[11/28 08:44:54 visual_prompt]: 	Training 500/553. train loss: 0.6915,	0.8440 s / batch. (data: 3.11e-04). ETA=4:33:00, max mem: 20.9 GB 
[11/28 08:45:43 visual_prompt]: Epoch 65 / 100: avg data time: 1.38e-01, avg batch time: 0.9723, average train loss: 2.8459
[11/28 08:46:38 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3106, average loss: 1.9032
[11/28 08:46:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.48	
[11/28 08:46:38 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[11/28 08:48:18 visual_prompt]: 	Training 100/553. train loss: 2.3882,	0.8318 s / batch. (data: 2.81e-04). ETA=4:26:55, max mem: 20.9 GB 
[11/28 08:49:53 visual_prompt]: 	Training 200/553. train loss: 1.3214,	1.4250 s / batch. (data: 6.01e-01). ETA=7:34:56, max mem: 20.9 GB 
[11/28 08:51:33 visual_prompt]: 	Training 300/553. train loss: 3.5978,	0.8283 s / batch. (data: 2.89e-04). ETA=4:23:02, max mem: 20.9 GB 
[11/28 08:53:08 visual_prompt]: 	Training 400/553. train loss: 1.0606,	0.8200 s / batch. (data: 2.93e-04). ETA=4:19:03, max mem: 20.9 GB 
[11/28 08:54:44 visual_prompt]: 	Training 500/553. train loss: 1.7128,	0.8259 s / batch. (data: 3.43e-04). ETA=4:19:31, max mem: 20.9 GB 
[11/28 08:55:36 visual_prompt]: Epoch 66 / 100: avg data time: 1.38e-01, avg batch time: 0.9723, average train loss: 2.3398
[11/28 08:56:31 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3087, average loss: 1.5432
[11/28 08:56:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.83	
[11/28 08:56:31 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[11/28 08:58:14 visual_prompt]: 	Training 100/553. train loss: 0.4442,	0.8284 s / batch. (data: 3.13e-04). ETA=4:18:12, max mem: 20.9 GB 
[11/28 08:59:52 visual_prompt]: 	Training 200/553. train loss: 0.0358,	0.8369 s / batch. (data: 1.05e-02). ETA=4:19:27, max mem: 20.9 GB 
[11/28 09:01:26 visual_prompt]: 	Training 300/553. train loss: 1.6564,	0.8560 s / batch. (data: 7.95e-03). ETA=4:23:57, max mem: 20.9 GB 
[11/28 09:02:59 visual_prompt]: 	Training 400/553. train loss: 0.8198,	0.8600 s / batch. (data: 3.07e-04). ETA=4:23:46, max mem: 20.9 GB 
[11/28 09:04:38 visual_prompt]: 	Training 500/553. train loss: 6.0533,	1.3600 s / batch. (data: 5.17e-01). ETA=6:54:50, max mem: 20.9 GB 
[11/28 09:05:30 visual_prompt]: Epoch 67 / 100: avg data time: 1.39e-01, avg batch time: 0.9728, average train loss: 2.1030
[11/28 09:06:25 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3089, average loss: 1.6451
[11/28 09:06:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.40	
[11/28 09:06:25 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[11/28 09:08:07 visual_prompt]: 	Training 100/553. train loss: 0.9233,	0.8369 s / batch. (data: 3.06e-04). ETA=4:13:09, max mem: 20.9 GB 
[11/28 09:09:46 visual_prompt]: 	Training 200/553. train loss: 2.2552,	1.0870 s / batch. (data: 2.45e-01). ETA=5:26:59, max mem: 20.9 GB 
[11/28 09:11:21 visual_prompt]: 	Training 300/553. train loss: 1.7263,	0.8482 s / batch. (data: 8.15e-04). ETA=4:13:43, max mem: 20.9 GB 
[11/28 09:12:57 visual_prompt]: 	Training 400/553. train loss: 1.9842,	0.8373 s / batch. (data: 7.95e-03). ETA=4:09:04, max mem: 20.9 GB 
[11/28 09:14:34 visual_prompt]: 	Training 500/553. train loss: 1.7006,	0.8680 s / batch. (data: 2.81e-04). ETA=4:16:46, max mem: 20.9 GB 
[11/28 09:15:25 visual_prompt]: Epoch 68 / 100: avg data time: 1.41e-01, avg batch time: 0.9757, average train loss: 1.6055
[11/28 09:16:21 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3122, average loss: 0.9274
[11/28 09:16:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.49	
[11/28 09:16:21 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[11/28 09:18:02 visual_prompt]: 	Training 100/553. train loss: 1.3756,	0.8480 s / batch. (data: 2.90e-04). ETA=4:08:41, max mem: 20.9 GB 
[11/28 09:19:40 visual_prompt]: 	Training 200/553. train loss: 1.4827,	0.8320 s / batch. (data: 3.03e-04). ETA=4:02:37, max mem: 20.9 GB 
[11/28 09:21:17 visual_prompt]: 	Training 300/553. train loss: 2.3488,	0.8200 s / batch. (data: 2.94e-04). ETA=3:57:44, max mem: 20.9 GB 
[11/28 09:22:54 visual_prompt]: 	Training 400/553. train loss: 1.7498,	0.8315 s / batch. (data: 1.07e-02). ETA=3:59:40, max mem: 20.9 GB 
[11/28 09:24:31 visual_prompt]: 	Training 500/553. train loss: 0.9750,	0.8432 s / batch. (data: 2.49e-02). ETA=4:01:39, max mem: 20.9 GB 
[11/28 09:25:22 visual_prompt]: Epoch 69 / 100: avg data time: 1.45e-01, avg batch time: 0.9794, average train loss: 1.6998
[11/28 09:26:18 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3087, average loss: 1.5392
[11/28 09:26:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.83	
[11/28 09:26:18 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[11/28 09:27:59 visual_prompt]: 	Training 100/553. train loss: 0.7165,	0.8319 s / batch. (data: 5.43e-03). ETA=3:56:17, max mem: 20.9 GB 
[11/28 09:29:35 visual_prompt]: 	Training 200/553. train loss: 3.0905,	0.8295 s / batch. (data: 1.05e-02). ETA=3:54:14, max mem: 20.9 GB 
[11/28 09:31:12 visual_prompt]: 	Training 300/553. train loss: 8.4204,	0.8516 s / batch. (data: 5.44e-03). ETA=3:59:04, max mem: 20.9 GB 
[11/28 09:32:50 visual_prompt]: 	Training 400/553. train loss: 0.7865,	1.1484 s / batch. (data: 3.12e-01). ETA=5:20:27, max mem: 20.9 GB 
[11/28 09:34:25 visual_prompt]: 	Training 500/553. train loss: 3.3952,	0.8332 s / batch. (data: 1.46e-02). ETA=3:51:06, max mem: 20.9 GB 
[11/28 09:35:17 visual_prompt]: Epoch 70 / 100: avg data time: 1.40e-01, avg batch time: 0.9735, average train loss: 1.6299
[11/28 09:36:12 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3120, average loss: 0.8393
[11/28 09:36:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.97	
[11/28 09:36:12 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[11/28 09:37:53 visual_prompt]: 	Training 100/553. train loss: 4.0746,	0.8480 s / batch. (data: 1.19e-02). ETA=3:53:03, max mem: 20.9 GB 
[11/28 09:39:30 visual_prompt]: 	Training 200/553. train loss: 1.2399,	0.8440 s / batch. (data: 3.25e-04). ETA=3:50:33, max mem: 20.9 GB 
[11/28 09:41:09 visual_prompt]: 	Training 300/553. train loss: 1.0314,	0.8351 s / batch. (data: 5.45e-03). ETA=3:46:44, max mem: 20.9 GB 
[11/28 09:42:43 visual_prompt]: 	Training 400/553. train loss: 2.2785,	0.8600 s / batch. (data: 6.74e-03). ETA=3:52:02, max mem: 20.9 GB 
[11/28 09:44:19 visual_prompt]: 	Training 500/553. train loss: 0.7954,	0.8480 s / batch. (data: 3.31e-04). ETA=3:47:23, max mem: 20.9 GB 
[11/28 09:45:10 visual_prompt]: Epoch 71 / 100: avg data time: 1.39e-01, avg batch time: 0.9736, average train loss: 1.5987
[11/28 09:46:06 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.3101, average loss: 0.6879
[11/28 09:46:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.10	
[11/28 09:46:06 visual_prompt]: Best epoch 71: best metric: -0.688
[11/28 09:46:06 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[11/28 09:47:49 visual_prompt]: 	Training 100/553. train loss: 0.7532,	1.0920 s / batch. (data: 2.72e-01). ETA=4:50:02, max mem: 20.9 GB 
[11/28 09:49:25 visual_prompt]: 	Training 200/553. train loss: 1.5626,	0.8473 s / batch. (data: 3.12e-04). ETA=3:43:38, max mem: 20.9 GB 
[11/28 09:51:02 visual_prompt]: 	Training 300/553. train loss: 0.6855,	0.8442 s / batch. (data: 7.98e-03). ETA=3:41:24, max mem: 20.9 GB 
[11/28 09:52:39 visual_prompt]: 	Training 400/553. train loss: 0.5587,	1.9036 s / batch. (data: 1.06e+00). ETA=8:16:07, max mem: 20.9 GB 
[11/28 09:54:12 visual_prompt]: 	Training 500/553. train loss: 0.7960,	0.8574 s / batch. (data: 2.96e-04). ETA=3:42:01, max mem: 20.9 GB 
[11/28 09:55:04 visual_prompt]: Epoch 72 / 100: avg data time: 1.40e-01, avg batch time: 0.9732, average train loss: 1.2615
[11/28 09:55:59 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3103, average loss: 1.1497
[11/28 09:55:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.65	
[11/28 09:55:59 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[11/28 09:57:40 visual_prompt]: 	Training 100/553. train loss: 0.8194,	0.8279 s / batch. (data: 1.11e-02). ETA=3:32:17, max mem: 20.9 GB 
[11/28 09:59:19 visual_prompt]: 	Training 200/553. train loss: 5.3167,	0.8182 s / batch. (data: 2.99e-04). ETA=3:28:24, max mem: 20.9 GB 
[11/28 10:00:52 visual_prompt]: 	Training 300/553. train loss: 0.7773,	0.8359 s / batch. (data: 1.03e-02). ETA=3:31:32, max mem: 20.9 GB 
[11/28 10:02:30 visual_prompt]: 	Training 400/553. train loss: 0.5624,	0.8480 s / batch. (data: 3.07e-04). ETA=3:33:10, max mem: 20.9 GB 
[11/28 10:04:07 visual_prompt]: 	Training 500/553. train loss: 0.9740,	0.8292 s / batch. (data: 3.65e-04). ETA=3:27:05, max mem: 20.9 GB 
[11/28 10:04:58 visual_prompt]: Epoch 73 / 100: avg data time: 1.39e-01, avg batch time: 0.9728, average train loss: 2.8037
[11/28 10:05:53 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3105, average loss: 4.5696
[11/28 10:05:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.72	
[11/28 10:05:53 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[11/28 10:07:39 visual_prompt]: 	Training 100/553. train loss: 4.5404,	1.3299 s / batch. (data: 5.01e-01). ETA=5:28:43, max mem: 20.9 GB 
[11/28 10:09:16 visual_prompt]: 	Training 200/553. train loss: 0.9309,	0.8252 s / batch. (data: 2.79e-04). ETA=3:22:36, max mem: 20.9 GB 
[11/28 10:10:52 visual_prompt]: 	Training 300/553. train loss: 0.6000,	0.8196 s / batch. (data: 3.17e-04). ETA=3:19:52, max mem: 20.9 GB 
[11/28 10:12:28 visual_prompt]: 	Training 400/553. train loss: 3.3900,	0.8440 s / batch. (data: 1.20e-02). ETA=3:24:24, max mem: 20.9 GB 
[11/28 10:14:03 visual_prompt]: 	Training 500/553. train loss: 1.8797,	1.9120 s / batch. (data: 1.08e+00). ETA=7:39:51, max mem: 20.9 GB 
[11/28 10:14:53 visual_prompt]: Epoch 74 / 100: avg data time: 1.43e-01, avg batch time: 0.9771, average train loss: 2.1771
[11/28 10:15:48 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3086, average loss: 2.7001
[11/28 10:15:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.13	
[11/28 10:15:48 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[11/28 10:17:31 visual_prompt]: 	Training 100/553. train loss: 0.7296,	1.5360 s / batch. (data: 6.82e-01). ETA=6:05:31, max mem: 20.9 GB 
[11/28 10:19:08 visual_prompt]: 	Training 200/553. train loss: 0.6479,	1.4288 s / batch. (data: 6.13e-01). ETA=5:37:37, max mem: 20.9 GB 
[11/28 10:20:44 visual_prompt]: 	Training 300/553. train loss: 1.5655,	0.8280 s / batch. (data: 5.43e-03). ETA=3:14:17, max mem: 20.9 GB 
[11/28 10:22:23 visual_prompt]: 	Training 400/553. train loss: 4.6580,	1.9838 s / batch. (data: 1.17e+00). ETA=7:42:09, max mem: 20.9 GB 
[11/28 10:23:59 visual_prompt]: 	Training 500/553. train loss: 0.8275,	0.8232 s / batch. (data: 2.89e-04). ETA=3:10:24, max mem: 20.9 GB 
[11/28 10:24:50 visual_prompt]: Epoch 75 / 100: avg data time: 1.47e-01, avg batch time: 0.9795, average train loss: 1.5086
[11/28 10:25:45 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3117, average loss: 4.3480
[11/28 10:25:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.35	
[11/28 10:25:45 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[11/28 10:27:29 visual_prompt]: 	Training 100/553. train loss: 2.0288,	0.8452 s / batch. (data: 1.03e-03). ETA=3:13:20, max mem: 20.9 GB 
[11/28 10:29:05 visual_prompt]: 	Training 200/553. train loss: 1.7805,	0.8406 s / batch. (data: 1.60e-02). ETA=3:10:53, max mem: 20.9 GB 
[11/28 10:30:41 visual_prompt]: 	Training 300/553. train loss: 1.4685,	0.8483 s / batch. (data: 3.26e-04). ETA=3:11:13, max mem: 20.9 GB 
[11/28 10:32:17 visual_prompt]: 	Training 400/553. train loss: 0.6902,	0.9104 s / batch. (data: 4.87e-02). ETA=3:23:42, max mem: 20.9 GB 
[11/28 10:33:53 visual_prompt]: 	Training 500/553. train loss: 0.8605,	0.8658 s / batch. (data: 2.59e-02). ETA=3:12:17, max mem: 20.9 GB 
[11/28 10:34:44 visual_prompt]: Epoch 76 / 100: avg data time: 1.39e-01, avg batch time: 0.9735, average train loss: 1.5438
[11/28 10:35:39 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3098, average loss: 0.6868
[11/28 10:35:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 54.47	
[11/28 10:35:39 visual_prompt]: Best epoch 76: best metric: -0.687
[11/28 10:35:39 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[11/28 10:37:21 visual_prompt]: 	Training 100/553. train loss: 0.9176,	2.0760 s / batch. (data: 1.23e+00). ETA=7:35:44, max mem: 20.9 GB 
[11/28 10:39:00 visual_prompt]: 	Training 200/553. train loss: 2.4776,	0.8331 s / batch. (data: 3.23e-04). ETA=3:01:30, max mem: 20.9 GB 
[11/28 10:40:34 visual_prompt]: 	Training 300/553. train loss: 0.5595,	0.8346 s / batch. (data: 3.45e-04). ETA=3:00:26, max mem: 20.9 GB 
[11/28 10:42:13 visual_prompt]: 	Training 400/553. train loss: 0.7289,	0.8670 s / batch. (data: 1.09e-02). ETA=3:05:59, max mem: 20.9 GB 
[11/28 10:43:49 visual_prompt]: 	Training 500/553. train loss: 1.7833,	0.8476 s / batch. (data: 3.04e-04). ETA=3:00:25, max mem: 20.9 GB 
[11/28 10:44:40 visual_prompt]: Epoch 77 / 100: avg data time: 1.43e-01, avg batch time: 0.9770, average train loss: 1.6303
[11/28 10:45:35 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3098, average loss: 1.0934
[11/28 10:45:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.17	
[11/28 10:45:35 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[11/28 10:47:15 visual_prompt]: 	Training 100/553. train loss: 0.7147,	0.8360 s / batch. (data: 3.10e-04). ETA=2:55:49, max mem: 20.9 GB 
[11/28 10:48:52 visual_prompt]: 	Training 200/553. train loss: 1.9571,	0.8440 s / batch. (data: 2.95e-04). ETA=2:56:06, max mem: 20.9 GB 
[11/28 10:50:29 visual_prompt]: 	Training 300/553. train loss: 1.8316,	0.8200 s / batch. (data: 3.02e-04). ETA=2:49:43, max mem: 20.9 GB 
[11/28 10:52:15 visual_prompt]: 	Training 400/553. train loss: 3.6737,	0.8280 s / batch. (data: 3.80e-04). ETA=2:50:00, max mem: 20.9 GB 
[11/28 10:53:53 visual_prompt]: 	Training 500/553. train loss: 0.6867,	0.8396 s / batch. (data: 3.48e-04). ETA=2:50:59, max mem: 20.9 GB 
[11/28 10:54:46 visual_prompt]: Epoch 78 / 100: avg data time: 1.63e-01, avg batch time: 0.9961, average train loss: 1.0529
[11/28 10:55:44 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3087, average loss: 0.8153
[11/28 10:55:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.69	
[11/28 10:55:44 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[11/28 10:57:27 visual_prompt]: 	Training 100/553. train loss: 1.8138,	0.8347 s / batch. (data: 3.06e-04). ETA=2:47:51, max mem: 20.9 GB 
[11/28 10:59:07 visual_prompt]: 	Training 200/553. train loss: 0.7785,	0.8311 s / batch. (data: 4.38e-04). ETA=2:45:44, max mem: 20.9 GB 
[11/28 11:00:42 visual_prompt]: 	Training 300/553. train loss: 0.7412,	2.1120 s / batch. (data: 1.25e+00). ETA=6:57:41, max mem: 20.9 GB 
[11/28 11:02:24 visual_prompt]: 	Training 400/553. train loss: 0.6647,	0.8589 s / batch. (data: 2.62e-02). ETA=2:48:25, max mem: 20.9 GB 
[11/28 11:04:04 visual_prompt]: 	Training 500/553. train loss: 2.2500,	0.8437 s / batch. (data: 1.19e-02). ETA=2:44:03, max mem: 20.9 GB 
[11/28 11:04:54 visual_prompt]: Epoch 79 / 100: avg data time: 1.62e-01, avg batch time: 0.9947, average train loss: 1.2480
[11/28 11:05:50 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3114, average loss: 0.9160
[11/28 11:05:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.13	
[11/28 11:05:50 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[11/28 11:07:31 visual_prompt]: 	Training 100/553. train loss: 1.7955,	0.8543 s / batch. (data: 1.58e-02). ETA=2:43:55, max mem: 20.9 GB 
[11/28 11:09:08 visual_prompt]: 	Training 200/553. train loss: 0.8393,	0.8400 s / batch. (data: 3.41e-04). ETA=2:39:47, max mem: 20.9 GB 
[11/28 11:10:44 visual_prompt]: 	Training 300/553. train loss: 0.6311,	1.3280 s / batch. (data: 4.80e-01). ETA=4:10:23, max mem: 20.9 GB 
[11/28 11:12:25 visual_prompt]: 	Training 400/553. train loss: 1.6594,	0.8440 s / batch. (data: 8.53e-03). ETA=2:37:43, max mem: 20.9 GB 
[11/28 11:14:02 visual_prompt]: 	Training 500/553. train loss: 1.3117,	1.2484 s / batch. (data: 4.19e-01). ETA=3:51:13, max mem: 20.9 GB 
[11/28 11:14:54 visual_prompt]: Epoch 80 / 100: avg data time: 1.49e-01, avg batch time: 0.9836, average train loss: 1.0760
[11/28 11:15:50 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.3108, average loss: 0.9694
[11/28 11:15:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.84	
[11/28 11:15:50 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[11/28 11:17:37 visual_prompt]: 	Training 100/553. train loss: 0.5599,	0.8384 s / batch. (data: 3.15e-04). ETA=2:33:08, max mem: 20.9 GB 
[11/28 11:19:17 visual_prompt]: 	Training 200/553. train loss: 0.3914,	0.8280 s / batch. (data: 3.39e-04). ETA=2:29:52, max mem: 20.9 GB 
[11/28 11:20:53 visual_prompt]: 	Training 300/553. train loss: 0.6530,	0.8226 s / batch. (data: 3.24e-04). ETA=2:27:31, max mem: 20.9 GB 
[11/28 11:22:31 visual_prompt]: 	Training 400/553. train loss: 0.9743,	1.2963 s / batch. (data: 4.47e-01). ETA=3:50:18, max mem: 20.9 GB 
[11/28 11:24:06 visual_prompt]: 	Training 500/553. train loss: 1.1581,	1.7991 s / batch. (data: 9.72e-01). ETA=5:16:38, max mem: 20.9 GB 
[11/28 11:24:57 visual_prompt]: Epoch 81 / 100: avg data time: 1.54e-01, avg batch time: 0.9880, average train loss: 1.0089
[11/28 11:25:52 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.3093, average loss: 0.7145
[11/28 11:25:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.09	
[11/28 11:25:52 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[11/28 11:27:34 visual_prompt]: 	Training 100/553. train loss: 1.1227,	0.8373 s / batch. (data: 3.27e-04). ETA=2:25:14, max mem: 20.9 GB 
[11/28 11:29:13 visual_prompt]: 	Training 200/553. train loss: 1.4934,	0.8240 s / batch. (data: 3.33e-04). ETA=2:21:33, max mem: 20.9 GB 
[11/28 11:30:52 visual_prompt]: 	Training 300/553. train loss: 0.5479,	2.2077 s / batch. (data: 1.38e+00). ETA=6:15:33, max mem: 20.9 GB 
[11/28 11:32:30 visual_prompt]: 	Training 400/553. train loss: 3.7751,	1.8880 s / batch. (data: 1.06e+00). ETA=5:18:01, max mem: 20.9 GB 
[11/28 11:34:21 visual_prompt]: 	Training 500/553. train loss: 0.7632,	0.8144 s / batch. (data: 3.55e-04). ETA=2:15:50, max mem: 20.9 GB 
[11/28 11:35:19 visual_prompt]: Epoch 82 / 100: avg data time: 1.92e-01, avg batch time: 1.0247, average train loss: 1.0113
[11/28 11:36:30 visual_prompt]: Inference (val):avg data time: 7.53e-05, avg batch time: 0.3095, average loss: 3.1171
[11/28 11:36:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.84	
[11/28 11:36:30 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[11/28 11:38:29 visual_prompt]: 	Training 100/553. train loss: 0.2367,	0.8338 s / batch. (data: 8.88e-04). ETA=2:16:56, max mem: 20.9 GB 
[11/28 11:40:24 visual_prompt]: 	Training 200/553. train loss: 3.0343,	0.8490 s / batch. (data: 3.34e-04). ETA=2:18:01, max mem: 20.9 GB 
[11/28 11:42:14 visual_prompt]: 	Training 300/553. train loss: 0.9125,	0.8318 s / batch. (data: 5.43e-03). ETA=2:13:50, max mem: 20.9 GB 
[11/28 11:44:10 visual_prompt]: 	Training 400/553. train loss: 0.7300,	0.8504 s / batch. (data: 1.07e-02). ETA=2:15:25, max mem: 20.9 GB 
[11/28 11:46:06 visual_prompt]: 	Training 500/553. train loss: 0.6911,	0.8230 s / batch. (data: 3.34e-04). ETA=2:09:41, max mem: 20.9 GB 
[11/28 11:47:04 visual_prompt]: Epoch 83 / 100: avg data time: 3.12e-01, avg batch time: 1.1451, average train loss: 0.9261
[11/28 11:48:14 visual_prompt]: Inference (val):avg data time: 9.47e-05, avg batch time: 0.3113, average loss: 0.7916
[11/28 11:48:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.45	
[11/28 11:48:14 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[11/28 11:50:17 visual_prompt]: 	Training 100/553. train loss: 0.6994,	0.8542 s / batch. (data: 3.46e-04). ETA=2:12:25, max mem: 20.9 GB 
[11/28 11:52:12 visual_prompt]: 	Training 200/553. train loss: 1.0073,	2.1752 s / batch. (data: 1.34e+00). ETA=5:33:34, max mem: 20.9 GB 
[11/28 11:54:08 visual_prompt]: 	Training 300/553. train loss: 0.5363,	0.8491 s / batch. (data: 1.34e-02). ETA=2:08:47, max mem: 20.9 GB 
[11/28 11:56:04 visual_prompt]: 	Training 400/553. train loss: 0.5649,	0.8391 s / batch. (data: 1.12e-02). ETA=2:05:52, max mem: 20.9 GB 
[11/28 11:57:55 visual_prompt]: 	Training 500/553. train loss: 0.3171,	0.8480 s / batch. (data: 5.25e-04). ETA=2:05:47, max mem: 20.9 GB 
[11/28 11:58:57 visual_prompt]: Epoch 84 / 100: avg data time: 3.29e-01, avg batch time: 1.1632, average train loss: 0.8315
[11/28 12:00:05 visual_prompt]: Inference (val):avg data time: 7.48e-05, avg batch time: 0.3109, average loss: 0.9600
[11/28 12:00:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.11	
[11/28 12:00:05 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[11/28 12:02:07 visual_prompt]: 	Training 100/553. train loss: 0.6242,	1.8181 s / batch. (data: 9.51e-01). ETA=4:25:05, max mem: 20.9 GB 
[11/28 12:04:00 visual_prompt]: 	Training 200/553. train loss: 0.5619,	0.8507 s / batch. (data: 1.26e-03). ETA=2:02:36, max mem: 20.9 GB 
[11/28 12:05:52 visual_prompt]: 	Training 300/553. train loss: 0.5595,	0.8166 s / batch. (data: 3.23e-04). ETA=1:56:19, max mem: 20.9 GB 
[11/28 12:07:43 visual_prompt]: 	Training 400/553. train loss: 1.1902,	0.8400 s / batch. (data: 1.20e-02). ETA=1:58:16, max mem: 20.9 GB 
[11/28 12:09:38 visual_prompt]: 	Training 500/553. train loss: 0.6516,	0.8520 s / batch. (data: 8.59e-04). ETA=1:58:32, max mem: 20.9 GB 
[11/28 12:10:35 visual_prompt]: Epoch 85 / 100: avg data time: 3.06e-01, avg batch time: 1.1391, average train loss: 0.8607
[11/28 12:11:40 visual_prompt]: Inference (val):avg data time: 2.34e-04, avg batch time: 0.3106, average loss: 0.8210
[11/28 12:11:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.08	
[11/28 12:11:40 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[11/28 12:13:39 visual_prompt]: 	Training 100/553. train loss: 0.5173,	3.6882 s / batch. (data: 2.86e+00). ETA=8:23:44, max mem: 20.9 GB 
[11/28 12:15:28 visual_prompt]: 	Training 200/553. train loss: 0.5713,	0.8300 s / batch. (data: 1.20e-03). ETA=1:51:59, max mem: 20.9 GB 
[11/28 12:17:19 visual_prompt]: 	Training 300/553. train loss: 1.4493,	0.8442 s / batch. (data: 4.92e-04). ETA=1:52:29, max mem: 20.9 GB 
[11/28 12:19:13 visual_prompt]: 	Training 400/553. train loss: 0.6979,	0.8494 s / batch. (data: 1.11e-02). ETA=1:51:45, max mem: 20.9 GB 
[11/28 12:21:08 visual_prompt]: 	Training 500/553. train loss: 0.7469,	0.8520 s / batch. (data: 5.32e-03). ETA=1:50:41, max mem: 20.9 GB 
[11/28 12:22:09 visual_prompt]: Epoch 86 / 100: avg data time: 3.03e-01, avg batch time: 1.1361, average train loss: 0.8593
[11/28 12:23:17 visual_prompt]: Inference (val):avg data time: 8.31e-05, avg batch time: 0.3104, average loss: 0.6983
[11/28 12:23:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.56	
[11/28 12:23:17 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[11/28 12:25:22 visual_prompt]: 	Training 100/553. train loss: 0.8616,	0.8153 s / batch. (data: 3.28e-04). ETA=1:43:50, max mem: 20.9 GB 
[11/28 12:27:18 visual_prompt]: 	Training 200/553. train loss: 0.8722,	0.8319 s / batch. (data: 4.95e-04). ETA=1:44:34, max mem: 20.9 GB 
[11/28 12:29:14 visual_prompt]: 	Training 300/553. train loss: 0.6174,	1.7120 s / batch. (data: 8.78e-01). ETA=3:32:20, max mem: 20.9 GB 
[11/28 12:31:06 visual_prompt]: 	Training 400/553. train loss: 0.9162,	0.8438 s / batch. (data: 9.82e-04). ETA=1:43:14, max mem: 20.9 GB 
[11/28 12:33:00 visual_prompt]: 	Training 500/553. train loss: 0.7152,	0.8556 s / batch. (data: 7.60e-03). ETA=1:43:16, max mem: 20.9 GB 
[11/28 12:34:01 visual_prompt]: Epoch 87 / 100: avg data time: 3.29e-01, avg batch time: 1.1635, average train loss: 0.7880
[11/28 12:35:09 visual_prompt]: Inference (val):avg data time: 5.65e-05, avg batch time: 0.3096, average loss: 0.8593
[11/28 12:35:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.30	
[11/28 12:35:09 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[11/28 12:37:06 visual_prompt]: 	Training 100/553. train loss: 0.3169,	0.8512 s / batch. (data: 2.10e-02). ETA=1:40:34, max mem: 20.9 GB 
[11/28 12:39:00 visual_prompt]: 	Training 200/553. train loss: 0.4857,	0.8280 s / batch. (data: 3.65e-04). ETA=1:36:27, max mem: 20.9 GB 
[11/28 12:40:58 visual_prompt]: 	Training 300/553. train loss: 0.0617,	0.8320 s / batch. (data: 3.76e-04). ETA=1:35:31, max mem: 20.9 GB 
[11/28 12:42:54 visual_prompt]: 	Training 400/553. train loss: 0.8329,	2.8449 s / batch. (data: 2.02e+00). ETA=5:21:54, max mem: 20.9 GB 
[11/28 12:44:42 visual_prompt]: 	Training 500/553. train loss: 0.8370,	1.4050 s / batch. (data: 5.67e-01). ETA=2:36:38, max mem: 20.9 GB 
[11/28 12:45:40 visual_prompt]: Epoch 88 / 100: avg data time: 3.08e-01, avg batch time: 1.1410, average train loss: 0.8413
[11/28 12:46:45 visual_prompt]: Inference (val):avg data time: 5.57e-05, avg batch time: 0.3099, average loss: 0.7564
[11/28 12:46:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.88	
[11/28 12:46:45 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[11/28 12:48:43 visual_prompt]: 	Training 100/553. train loss: 0.7168,	0.8291 s / batch. (data: 5.77e-03). ETA=1:30:18, max mem: 20.9 GB 
[11/28 12:50:33 visual_prompt]: 	Training 200/553. train loss: 0.9254,	0.8352 s / batch. (data: 2.95e-03). ETA=1:29:35, max mem: 20.9 GB 
[11/28 12:52:26 visual_prompt]: 	Training 300/553. train loss: 0.7014,	0.8200 s / batch. (data: 3.33e-04). ETA=1:26:35, max mem: 20.9 GB 
[11/28 12:54:18 visual_prompt]: 	Training 400/553. train loss: 1.1130,	0.8309 s / batch. (data: 3.42e-04). ETA=1:26:21, max mem: 20.9 GB 
[11/28 12:56:09 visual_prompt]: 	Training 500/553. train loss: 0.6733,	0.8263 s / batch. (data: 8.62e-04). ETA=1:24:30, max mem: 20.9 GB 
[11/28 12:57:07 visual_prompt]: Epoch 89 / 100: avg data time: 2.91e-01, avg batch time: 1.1234, average train loss: 0.7850
[11/28 12:58:12 visual_prompt]: Inference (val):avg data time: 4.83e-05, avg batch time: 0.3097, average loss: 0.8713
[11/28 12:58:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.68	
[11/28 12:58:12 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[11/28 13:00:11 visual_prompt]: 	Training 100/553. train loss: 0.5887,	0.8320 s / batch. (data: 9.05e-04). ETA=1:22:57, max mem: 20.9 GB 
[11/28 13:02:09 visual_prompt]: 	Training 200/553. train loss: 0.6307,	1.8002 s / batch. (data: 9.72e-01). ETA=2:56:30, max mem: 20.9 GB 
[11/28 13:04:03 visual_prompt]: 	Training 300/553. train loss: 1.2918,	0.8298 s / batch. (data: 7.80e-03). ETA=1:19:58, max mem: 20.9 GB 
[11/28 13:05:59 visual_prompt]: 	Training 400/553. train loss: 0.8055,	0.8197 s / batch. (data: 5.47e-03). ETA=1:17:38, max mem: 20.9 GB 
[11/28 13:07:54 visual_prompt]: 	Training 500/553. train loss: 0.6507,	0.8560 s / batch. (data: 3.49e-04). ETA=1:19:39, max mem: 20.9 GB 
[11/28 13:08:52 visual_prompt]: Epoch 90 / 100: avg data time: 3.25e-01, avg batch time: 1.1579, average train loss: 0.7644
[11/28 13:10:01 visual_prompt]: Inference (val):avg data time: 8.30e-05, avg batch time: 0.3121, average loss: 0.9329
[11/28 13:10:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.70	
[11/28 13:10:01 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[11/28 13:12:04 visual_prompt]: 	Training 100/553. train loss: 0.7190,	0.8308 s / batch. (data: 3.31e-04). ETA=1:15:11, max mem: 20.9 GB 
[11/28 13:14:01 visual_prompt]: 	Training 200/553. train loss: 0.4803,	0.8400 s / batch. (data: 3.04e-04). ETA=1:14:37, max mem: 20.9 GB 
[11/28 13:15:58 visual_prompt]: 	Training 300/553. train loss: 0.8297,	0.8606 s / batch. (data: 1.78e-02). ETA=1:15:01, max mem: 20.9 GB 
[11/28 13:17:57 visual_prompt]: 	Training 400/553. train loss: 0.7352,	2.2151 s / batch. (data: 1.39e+00). ETA=3:09:23, max mem: 20.9 GB 
[11/28 13:19:48 visual_prompt]: 	Training 500/553. train loss: 0.9635,	0.8283 s / batch. (data: 3.14e-04). ETA=1:09:26, max mem: 20.9 GB 
[11/28 13:20:47 visual_prompt]: Epoch 91 / 100: avg data time: 3.34e-01, avg batch time: 1.1672, average train loss: 0.7431
[11/28 13:21:56 visual_prompt]: Inference (val):avg data time: 8.11e-05, avg batch time: 0.3100, average loss: 0.7229
[11/28 13:21:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 68.92	
[11/28 13:21:56 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[11/28 13:24:00 visual_prompt]: 	Training 100/553. train loss: 0.8772,	2.8199 s / batch. (data: 1.97e+00). ETA=3:49:12, max mem: 20.9 GB 
[11/28 13:25:52 visual_prompt]: 	Training 200/553. train loss: 1.1519,	0.8512 s / batch. (data: 6.38e-03). ETA=1:07:46, max mem: 20.9 GB 
[11/28 13:27:44 visual_prompt]: 	Training 300/553. train loss: 0.6088,	1.1895 s / batch. (data: 3.62e-01). ETA=1:32:43, max mem: 20.9 GB 
[11/28 13:29:40 visual_prompt]: 	Training 400/553. train loss: 0.6533,	0.8258 s / batch. (data: 1.02e-02). ETA=1:02:59, max mem: 20.9 GB 
[11/28 13:31:34 visual_prompt]: 	Training 500/553. train loss: 0.9115,	0.8360 s / batch. (data: 7.95e-03). ETA=1:02:22, max mem: 20.9 GB 
[11/28 13:32:34 visual_prompt]: Epoch 92 / 100: avg data time: 3.20e-01, avg batch time: 1.1528, average train loss: 0.7151
[11/28 13:33:44 visual_prompt]: Inference (val):avg data time: 9.26e-05, avg batch time: 0.3109, average loss: 1.3657
[11/28 13:33:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.17	
[11/28 13:33:44 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[11/28 13:35:45 visual_prompt]: 	Training 100/553. train loss: 0.4733,	0.8382 s / batch. (data: 5.96e-03). ETA=1:00:24, max mem: 20.9 GB 
[11/28 13:37:42 visual_prompt]: 	Training 200/553. train loss: 0.9180,	1.6588 s / batch. (data: 8.27e-01). ETA=1:56:46, max mem: 20.9 GB 
[11/28 13:39:39 visual_prompt]: 	Training 300/553. train loss: 0.6854,	2.5280 s / batch. (data: 1.70e+00). ETA=2:53:45, max mem: 20.9 GB 
[11/28 13:41:35 visual_prompt]: 	Training 400/553. train loss: 0.5116,	1.7059 s / batch. (data: 8.88e-01). ETA=1:54:24, max mem: 20.9 GB 
[11/28 13:43:34 visual_prompt]: 	Training 500/553. train loss: 0.9180,	0.8465 s / batch. (data: 1.13e-02). ETA=0:55:21, max mem: 20.9 GB 
[11/28 13:44:33 visual_prompt]: Epoch 93 / 100: avg data time: 3.38e-01, avg batch time: 1.1728, average train loss: 0.7061
[11/28 13:45:42 visual_prompt]: Inference (val):avg data time: 1.36e-04, avg batch time: 0.3108, average loss: 0.6318
[11/28 13:45:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.48	
[11/28 13:45:42 visual_prompt]: Best epoch 93: best metric: -0.632
[11/28 13:45:42 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[11/28 13:47:43 visual_prompt]: 	Training 100/553. train loss: 0.8262,	0.8277 s / batch. (data: 1.09e-03). ETA=0:52:01, max mem: 20.9 GB 
[11/28 13:49:36 visual_prompt]: 	Training 200/553. train loss: 1.0453,	1.4594 s / batch. (data: 6.45e-01). ETA=1:29:17, max mem: 20.9 GB 
[11/28 13:51:32 visual_prompt]: 	Training 300/553. train loss: 0.9138,	0.8274 s / batch. (data: 3.52e-04). ETA=0:49:14, max mem: 20.9 GB 
[11/28 13:53:28 visual_prompt]: 	Training 400/553. train loss: 0.8219,	1.2413 s / batch. (data: 4.10e-01). ETA=1:11:48, max mem: 20.9 GB 
[11/28 13:55:22 visual_prompt]: 	Training 500/553. train loss: 0.6527,	1.0630 s / batch. (data: 2.29e-01). ETA=0:59:43, max mem: 20.9 GB 
[11/28 13:56:23 visual_prompt]: Epoch 94 / 100: avg data time: 3.25e-01, avg batch time: 1.1593, average train loss: 0.6658
[11/28 13:57:32 visual_prompt]: Inference (val):avg data time: 4.92e-04, avg batch time: 0.3102, average loss: 0.7013
[11/28 13:57:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 69.60	
[11/28 13:57:32 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[11/28 13:59:35 visual_prompt]: 	Training 100/553. train loss: 0.7940,	0.8467 s / batch. (data: 6.11e-03). ETA=0:45:24, max mem: 20.9 GB 
[11/28 14:01:34 visual_prompt]: 	Training 200/553. train loss: 0.4286,	1.3749 s / batch. (data: 5.42e-01). ETA=1:11:27, max mem: 20.9 GB 
[11/28 14:03:28 visual_prompt]: 	Training 300/553. train loss: 0.7362,	2.3474 s / batch. (data: 1.52e+00). ETA=1:58:04, max mem: 20.9 GB 
[11/28 14:05:26 visual_prompt]: 	Training 400/553. train loss: 1.1178,	3.0752 s / batch. (data: 2.26e+00). ETA=2:29:33, max mem: 20.9 GB 
[11/28 14:07:20 visual_prompt]: 	Training 500/553. train loss: 0.5274,	0.8338 s / batch. (data: 3.19e-04). ETA=0:39:09, max mem: 20.9 GB 
[11/28 14:08:20 visual_prompt]: Epoch 95 / 100: avg data time: 3.37e-01, avg batch time: 1.1713, average train loss: 0.6509
[11/28 14:09:26 visual_prompt]: Inference (val):avg data time: 5.73e-05, avg batch time: 0.3105, average loss: 0.6308
[11/28 14:09:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 70.32	
[11/28 14:09:26 visual_prompt]: Best epoch 95: best metric: -0.631
[11/28 14:09:26 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[11/28 14:11:29 visual_prompt]: 	Training 100/553. train loss: 1.0444,	0.8240 s / batch. (data: 3.68e-04). ETA=0:36:35, max mem: 20.9 GB 
[11/28 14:13:26 visual_prompt]: 	Training 200/553. train loss: 0.5652,	0.8518 s / batch. (data: 2.29e-03). ETA=0:36:24, max mem: 20.9 GB 
[11/28 14:15:17 visual_prompt]: 	Training 300/553. train loss: 0.5882,	1.7786 s / batch. (data: 9.64e-01). ETA=1:13:04, max mem: 20.9 GB 
[11/28 14:17:10 visual_prompt]: 	Training 400/553. train loss: 0.3947,	0.8319 s / batch. (data: 7.76e-04). ETA=0:32:47, max mem: 20.9 GB 
[11/28 14:19:04 visual_prompt]: 	Training 500/553. train loss: 0.9310,	0.8329 s / batch. (data: 7.92e-03). ETA=0:31:26, max mem: 20.9 GB 
[11/28 14:20:03 visual_prompt]: Epoch 96 / 100: avg data time: 3.16e-01, avg batch time: 1.1502, average train loss: 0.6444
[11/28 14:21:12 visual_prompt]: Inference (val):avg data time: 3.68e-04, avg batch time: 0.3111, average loss: 0.6290
[11/28 14:21:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 70.18	
[11/28 14:21:12 visual_prompt]: Best epoch 96: best metric: -0.629
[11/28 14:21:12 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[11/28 14:23:11 visual_prompt]: 	Training 100/553. train loss: 0.8252,	0.8201 s / batch. (data: 3.53e-04). ETA=0:28:51, max mem: 20.9 GB 
[11/28 14:25:09 visual_prompt]: 	Training 200/553. train loss: 0.4050,	0.8400 s / batch. (data: 7.96e-03). ETA=0:28:10, max mem: 20.9 GB 
[11/28 14:27:04 visual_prompt]: 	Training 300/553. train loss: 0.7936,	0.8681 s / batch. (data: 2.92e-02). ETA=0:27:39, max mem: 20.9 GB 
[11/28 14:28:59 visual_prompt]: 	Training 400/553. train loss: 0.3982,	0.8337 s / batch. (data: 6.79e-04). ETA=0:25:10, max mem: 20.9 GB 
[11/28 14:30:52 visual_prompt]: 	Training 500/553. train loss: 1.2295,	0.8300 s / batch. (data: 1.15e-03). ETA=0:23:40, max mem: 20.9 GB 
[11/28 14:31:54 visual_prompt]: Epoch 97 / 100: avg data time: 3.26e-01, avg batch time: 1.1597, average train loss: 0.6328
[11/28 14:33:01 visual_prompt]: Inference (val):avg data time: 6.93e-05, avg batch time: 0.3108, average loss: 0.6306
[11/28 14:33:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 70.85	
[11/28 14:33:01 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[11/28 14:35:05 visual_prompt]: 	Training 100/553. train loss: 0.5270,	0.8574 s / batch. (data: 1.34e-02). ETA=0:22:16, max mem: 20.9 GB 
[11/28 14:36:57 visual_prompt]: 	Training 200/553. train loss: 0.7179,	0.8390 s / batch. (data: 1.58e-02). ETA=0:20:24, max mem: 20.9 GB 
[11/28 14:38:53 visual_prompt]: 	Training 300/553. train loss: 0.8244,	2.9311 s / batch. (data: 2.12e+00). ETA=1:06:23, max mem: 20.9 GB 
[11/28 14:40:48 visual_prompt]: 	Training 400/553. train loss: 0.7800,	2.4543 s / batch. (data: 1.63e+00). ETA=0:51:29, max mem: 20.9 GB 
[11/28 14:42:42 visual_prompt]: 	Training 500/553. train loss: 0.8072,	0.8449 s / batch. (data: 1.05e-02). ETA=0:16:19, max mem: 20.9 GB 
[11/28 14:43:43 visual_prompt]: Epoch 98 / 100: avg data time: 3.26e-01, avg batch time: 1.1589, average train loss: 0.6228
[11/28 14:44:50 visual_prompt]: Inference (val):avg data time: 2.35e-04, avg batch time: 0.3107, average loss: 0.6224
[11/28 14:44:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 70.40	
[11/28 14:44:50 visual_prompt]: Best epoch 98: best metric: -0.622
[11/28 14:44:50 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[11/28 14:46:50 visual_prompt]: 	Training 100/553. train loss: 0.7898,	0.8280 s / batch. (data: 7.65e-04). ETA=0:13:52, max mem: 20.9 GB 
[11/28 14:48:47 visual_prompt]: 	Training 200/553. train loss: 0.7750,	0.8251 s / batch. (data: 7.62e-04). ETA=0:12:27, max mem: 20.9 GB 
[11/28 14:50:46 visual_prompt]: 	Training 300/553. train loss: 1.2188,	2.2395 s / batch. (data: 1.42e+00). ETA=0:30:05, max mem: 20.9 GB 
[11/28 14:52:37 visual_prompt]: 	Training 400/553. train loss: 0.7227,	1.5762 s / batch. (data: 7.28e-01). ETA=0:18:32, max mem: 20.9 GB 
[11/28 14:54:31 visual_prompt]: 	Training 500/553. train loss: 0.6670,	0.8579 s / batch. (data: 7.61e-04). ETA=0:08:39, max mem: 20.9 GB 
[11/28 14:55:31 visual_prompt]: Epoch 99 / 100: avg data time: 3.25e-01, avg batch time: 1.1588, average train loss: 0.6092
[11/28 14:56:39 visual_prompt]: Inference (val):avg data time: 7.52e-05, avg batch time: 0.3105, average loss: 0.6209
[11/28 14:56:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 70.80	
[11/28 14:56:39 visual_prompt]: Best epoch 99: best metric: -0.621
[11/28 14:56:39 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[11/28 14:58:44 visual_prompt]: 	Training 100/553. train loss: 0.6347,	1.8056 s / batch. (data: 9.69e-01). ETA=0:13:37, max mem: 20.9 GB 
[11/28 15:00:37 visual_prompt]: 	Training 200/553. train loss: 0.6811,	1.3322 s / batch. (data: 4.79e-01). ETA=0:07:50, max mem: 20.9 GB 
[11/28 15:02:34 visual_prompt]: 	Training 300/553. train loss: 0.4746,	0.8939 s / batch. (data: 7.55e-02). ETA=0:03:46, max mem: 20.9 GB 
[11/28 15:04:29 visual_prompt]: 	Training 400/553. train loss: 0.5761,	0.8160 s / batch. (data: 3.90e-04). ETA=0:02:04, max mem: 20.9 GB 
[11/28 15:06:20 visual_prompt]: 	Training 500/553. train loss: 0.5968,	0.8349 s / batch. (data: 5.47e-03). ETA=0:00:44, max mem: 20.9 GB 
[11/28 15:07:19 visual_prompt]: Epoch 100 / 100: avg data time: 3.24e-01, avg batch time: 1.1580, average train loss: 0.6025
[11/28 15:08:26 visual_prompt]: Inference (val):avg data time: 6.75e-05, avg batch time: 0.3101, average loss: 0.6216
[11/28 15:08:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 70.97	
[11/28 15:08:27 visual_prompt]: Rank of current process: 0. World size: 1
[11/28 15:08:27 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 15:08:27 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss', 'RECORD_GPU_SNAPSHOT', 'True'])
[11/28 15:08:27 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/28 15:08:27 visual_prompt]: Training with config:
[11/28 15:08:27 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr5.0_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'RECORD_GPU_SNAPSHOT': True})
[11/28 15:08:27 visual_prompt]: Loading training data...
[11/28 15:08:27 visual_prompt]: Constructing mammo-cbis dataset train...
[11/28 15:08:27 visual_prompt]: Loading validation data...
[11/28 15:08:27 visual_prompt]: Constructing mammo-cbis dataset val...
[11/28 15:08:27 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/28 15:08:30 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/28 15:08:30 visual_prompt]: tuned percent:0.525
[11/28 15:08:30 visual_prompt]: Device used for model: 0
[11/28 15:08:30 visual_prompt]: Setting up Evaluator...
[11/28 15:08:30 visual_prompt]: Setting up Trainer...
[11/28 15:08:30 visual_prompt]: 	Setting up the optimizer...
[11/28 15:08:30 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/28 15:10:30 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8283 s / batch. (data: 7.95e-03). ETA=12:41:59, max mem: 20.9 GB 
[11/28 15:12:24 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8361 s / batch. (data: 5.44e-03). ETA=12:47:46, max mem: 20.9 GB 
[11/28 15:14:22 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.4882 s / batch. (data: 1.65e+00). ETA=1 day, 14:00:49, max mem: 20.9 GB 
[11/28 15:16:14 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8337 s / batch. (data: 9.01e-04). ETA=12:42:47, max mem: 20.9 GB 
[11/28 15:18:13 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8479 s / batch. (data: 6.41e-03). ETA=12:54:24, max mem: 20.9 GB 
[11/28 15:19:13 visual_prompt]: Epoch 1 / 100: avg data time: 3.28e-01, avg batch time: 1.1624, average train loss: 1.5403
[11/28 15:20:20 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3113, average loss: 1.5201
[11/28 15:20:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/28 15:20:20 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/28 15:22:20 visual_prompt]: 	Training 100/553. train loss: 3.0043,	0.9945 s / batch. (data: 1.69e-01). ETA=15:05:47, max mem: 20.9 GB 
[11/28 15:24:15 visual_prompt]: 	Training 200/553. train loss: 0.0004,	2.1080 s / batch. (data: 1.27e+00). ETA=1 day, 7:56:22, max mem: 20.9 GB 
[11/28 15:26:12 visual_prompt]: 	Training 300/553. train loss: 1.0276,	1.6763 s / batch. (data: 8.19e-01). ETA=1 day, 1:21:10, max mem: 20.9 GB 
[11/28 15:28:05 visual_prompt]: 	Training 400/553. train loss: 2.9529,	0.8270 s / batch. (data: 3.28e-04). ETA=12:29:04, max mem: 20.9 GB 
[11/28 15:30:02 visual_prompt]: 	Training 500/553. train loss: 1.0274,	0.8163 s / batch. (data: 3.15e-04). ETA=12:17:59, max mem: 20.9 GB 
[11/28 15:31:01 visual_prompt]: Epoch 2 / 100: avg data time: 3.24e-01, avg batch time: 1.1581, average train loss: 1.8527
[11/28 15:32:09 visual_prompt]: Inference (val):avg data time: 7.73e-05, avg batch time: 0.3092, average loss: 3.8356
[11/28 15:32:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.38	
[11/28 15:32:09 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/28 15:34:07 visual_prompt]: 	Training 100/553. train loss: 4.4338,	0.8331 s / batch. (data: 6.11e-03). ETA=12:31:08, max mem: 20.9 GB 
[11/28 15:36:04 visual_prompt]: 	Training 200/553. train loss: 0.7001,	0.8321 s / batch. (data: 1.12e-03). ETA=12:28:50, max mem: 20.9 GB 
[11/28 15:37:58 visual_prompt]: 	Training 300/553. train loss: 2.6151,	0.8186 s / batch. (data: 1.10e-03). ETA=12:15:19, max mem: 20.9 GB 
[11/28 15:39:53 visual_prompt]: 	Training 400/553. train loss: 1.4173,	0.8400 s / batch. (data: 3.34e-04). ETA=12:33:07, max mem: 20.9 GB 
[11/28 15:41:49 visual_prompt]: 	Training 500/553. train loss: 1.0324,	1.6782 s / batch. (data: 8.32e-01). ETA=1 day, 1:01:46, max mem: 20.9 GB 
[11/28 15:42:46 visual_prompt]: Epoch 3 / 100: avg data time: 3.19e-01, avg batch time: 1.1529, average train loss: 2.5510
[11/28 15:43:53 visual_prompt]: Inference (val):avg data time: 7.27e-05, avg batch time: 0.3124, average loss: 1.8970
[11/28 15:43:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.23	
[11/28 15:43:53 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/28 15:45:55 visual_prompt]: 	Training 100/553. train loss: 2.2802,	0.8340 s / batch. (data: 1.19e-02). ETA=12:24:11, max mem: 20.9 GB 
[11/28 15:47:51 visual_prompt]: 	Training 200/553. train loss: 2.3283,	0.8265 s / batch. (data: 1.30e-03). ETA=12:16:08, max mem: 20.9 GB 
[11/28 15:49:46 visual_prompt]: 	Training 300/553. train loss: 1.8069,	1.6558 s / batch. (data: 8.20e-01). ETA=1 day, 0:32:00, max mem: 20.9 GB 
[11/28 15:51:36 visual_prompt]: 	Training 400/553. train loss: 1.2089,	1.2399 s / batch. (data: 4.10e-01). ETA=18:20:14, max mem: 20.9 GB 
[11/28 15:53:32 visual_prompt]: 	Training 500/553. train loss: 0.0000,	4.2838 s / batch. (data: 3.45e+00). ETA=2 days, 15:14:08, max mem: 20.9 GB 
[11/28 15:54:32 visual_prompt]: Epoch 4 / 100: avg data time: 3.22e-01, avg batch time: 1.1555, average train loss: 2.9854
[11/28 15:55:38 visual_prompt]: Inference (val):avg data time: 6.26e-05, avg batch time: 0.3108, average loss: 12.4888
[11/28 15:55:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.00	
[11/28 15:55:38 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/28 15:57:36 visual_prompt]: 	Training 100/553. train loss: 0.0002,	0.8619 s / batch. (data: 7.56e-04). ETA=12:41:12, max mem: 20.9 GB 
[11/28 15:59:29 visual_prompt]: 	Training 200/553. train loss: 6.0476,	1.3375 s / batch. (data: 5.18e-01). ETA=19:38:57, max mem: 20.9 GB 
[11/28 16:01:24 visual_prompt]: 	Training 300/553. train loss: 16.7818,	0.8480 s / batch. (data: 7.95e-03). ETA=12:26:05, max mem: 20.9 GB 
[11/28 16:03:17 visual_prompt]: 	Training 400/553. train loss: 16.1333,	0.8306 s / batch. (data: 3.35e-04). ETA=12:09:23, max mem: 20.9 GB 
[11/28 16:05:13 visual_prompt]: 	Training 500/553. train loss: 4.3187,	0.8351 s / batch. (data: 1.19e-02). ETA=12:11:55, max mem: 20.9 GB 
[11/28 16:06:13 visual_prompt]: Epoch 5 / 100: avg data time: 3.14e-01, avg batch time: 1.1470, average train loss: 5.7854
[11/28 16:07:21 visual_prompt]: Inference (val):avg data time: 8.12e-05, avg batch time: 0.3099, average loss: 14.4886
[11/28 16:07:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.19	
[11/28 16:07:21 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/28 16:09:24 visual_prompt]: 	Training 100/553. train loss: 11.8735,	0.8710 s / batch. (data: 5.97e-03). ETA=12:41:10, max mem: 20.9 GB 
[11/28 16:11:19 visual_prompt]: 	Training 200/553. train loss: 8.3384,	0.8320 s / batch. (data: 3.40e-04). ETA=12:05:41, max mem: 20.9 GB 

/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 06:37:27 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 06:37:27 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 06:37:27 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 06:37:27 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 06:37:27 visual_prompt]: Training with config:
[09/26 06:37:27 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr50.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 06:37:27 visual_prompt]: Loading training data...
2023-09-26 06:37:28.329581: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-26 06:37:28.379938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-26 06:37:33.332446: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[09/26 06:37:43 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 06:37:44 visual_prompt]: Number of images: 800
[09/26 06:37:44 visual_prompt]: Number of classes: 10 / 10
[09/26 06:37:44 visual_prompt]: Loading validation data...
[09/26 06:37:44 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 06:37:45 visual_prompt]: Number of images: 200
[09/26 06:37:45 visual_prompt]: Number of classes: 10 / 10
[09/26 06:37:45 visual_prompt]: Constructing models...
[09/26 06:37:48 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 06:37:48 visual_prompt]: tuned percent:0.543
[09/26 06:37:50 visual_prompt]: Device used for model: 0
[09/26 06:37:50 visual_prompt]: Setting up Evaluator...
[09/26 06:37:50 visual_prompt]: Setting up Trainer...
[09/26 06:37:50 visual_prompt]: 	Setting up the optimizer...
[09/26 06:37:50 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 06:38:00 visual_prompt]: Epoch 1 / 100: avg data time: 2.13e-01, avg batch time: 0.8018, average train loss: 2.6764
[09/26 06:38:02 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1574, average loss: 2.6214
[09/26 06:38:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 06:38:02 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 06:38:02 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[09/26 06:38:08 visual_prompt]: Epoch 2 / 100: avg data time: 4.22e-02, avg batch time: 0.4529, average train loss: 26.7388
[09/26 06:38:10 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1578, average loss: 17.3817
[09/26 06:38:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 44.50	
[09/26 06:38:10 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[09/26 06:38:16 visual_prompt]: Epoch 3 / 100: avg data time: 5.62e-02, avg batch time: 0.4670, average train loss: 33.6036
[09/26 06:38:17 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1584, average loss: 47.2588
[09/26 06:38:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.00	
[09/26 06:38:17 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[09/26 06:38:24 visual_prompt]: Epoch 4 / 100: avg data time: 4.79e-02, avg batch time: 0.4627, average train loss: 63.5908
[09/26 06:38:25 visual_prompt]: Inference (val):avg data time: 4.63e-05, avg batch time: 0.1580, average loss: 47.4370
[09/26 06:38:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.00	
[09/26 06:38:25 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[09/26 06:38:31 visual_prompt]: Epoch 5 / 100: avg data time: 5.50e-02, avg batch time: 0.4679, average train loss: 74.3704
[09/26 06:38:33 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1581, average loss: 46.2899
[09/26 06:38:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 59.50	
[09/26 06:38:33 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[09/26 06:38:39 visual_prompt]: Epoch 6 / 100: avg data time: 5.04e-02, avg batch time: 0.4659, average train loss: 95.0704
[09/26 06:38:40 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1589, average loss: 86.0505
[09/26 06:38:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 53.50	
[09/26 06:38:40 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[09/26 06:38:47 visual_prompt]: Epoch 7 / 100: avg data time: 5.68e-02, avg batch time: 0.4720, average train loss: 118.5119
[09/26 06:38:48 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1596, average loss: 91.8171
[09/26 06:38:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 44.50	
[09/26 06:38:48 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[09/26 06:38:55 visual_prompt]: Epoch 8 / 100: avg data time: 5.76e-02, avg batch time: 0.4730, average train loss: 130.5568
[09/26 06:38:56 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1610, average loss: 135.1672
[09/26 06:38:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 06:38:56 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[09/26 06:39:03 visual_prompt]: Epoch 9 / 100: avg data time: 5.43e-02, avg batch time: 0.4728, average train loss: 162.4946
[09/26 06:39:04 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1605, average loss: 146.1295
[09/26 06:39:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 21.00	top5: 54.50	
[09/26 06:39:04 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[09/26 06:39:10 visual_prompt]: Epoch 10 / 100: avg data time: 5.75e-02, avg batch time: 0.4755, average train loss: 151.7446
[09/26 06:39:12 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1609, average loss: 184.1207
[09/26 06:39:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 46.50	
[09/26 06:39:12 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[09/26 06:39:18 visual_prompt]: Epoch 11 / 100: avg data time: 4.99e-02, avg batch time: 0.4700, average train loss: 191.5568
[09/26 06:39:20 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1612, average loss: 269.0050
[09/26 06:39:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 06:39:20 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[09/26 06:39:26 visual_prompt]: Epoch 12 / 100: avg data time: 6.23e-02, avg batch time: 0.4811, average train loss: 239.3461
[09/26 06:39:28 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1607, average loss: 305.8375
[09/26 06:39:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 06:39:28 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[09/26 06:39:34 visual_prompt]: Epoch 13 / 100: avg data time: 5.27e-02, avg batch time: 0.4725, average train loss: 284.3139
[09/26 06:39:35 visual_prompt]: Inference (val):avg data time: 1.83e-05, avg batch time: 0.1616, average loss: 52.0976
[09/26 06:39:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.00	
[09/26 06:39:35 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[09/26 06:39:42 visual_prompt]: Epoch 14 / 100: avg data time: 5.89e-02, avg batch time: 0.4778, average train loss: 250.0098
[09/26 06:39:43 visual_prompt]: Inference (val):avg data time: 1.95e-05, avg batch time: 0.1614, average loss: 335.5733
[09/26 06:39:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.00	
[09/26 06:39:43 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[09/26 06:39:50 visual_prompt]: Epoch 15 / 100: avg data time: 5.19e-02, avg batch time: 0.4720, average train loss: 209.0493
[09/26 06:39:51 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1605, average loss: 216.9847
[09/26 06:39:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/26 06:39:51 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[09/26 06:39:58 visual_prompt]: Epoch 16 / 100: avg data time: 5.38e-02, avg batch time: 0.4752, average train loss: 247.9628
[09/26 06:39:59 visual_prompt]: Inference (val):avg data time: 1.81e-05, avg batch time: 0.1609, average loss: 190.4084
[09/26 06:39:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 56.00	
[09/26 06:39:59 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[09/26 06:40:05 visual_prompt]: Epoch 17 / 100: avg data time: 5.03e-02, avg batch time: 0.4694, average train loss: 234.3439
[09/26 06:40:07 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1612, average loss: 387.5067
[09/26 06:40:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 49.50	
[09/26 06:40:07 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[09/26 06:40:13 visual_prompt]: Epoch 18 / 100: avg data time: 5.56e-02, avg batch time: 0.4746, average train loss: 261.5095
[09/26 06:40:15 visual_prompt]: Inference (val):avg data time: 1.99e-05, avg batch time: 0.1617, average loss: 173.9918
[09/26 06:40:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 50.50	
[09/26 06:40:15 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[09/26 06:40:21 visual_prompt]: Epoch 19 / 100: avg data time: 4.93e-02, avg batch time: 0.4687, average train loss: 253.6291
[09/26 06:40:22 visual_prompt]: Inference (val):avg data time: 1.83e-05, avg batch time: 0.1610, average loss: 174.8519
[09/26 06:40:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 40.00	
[09/26 06:40:22 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[09/26 06:40:29 visual_prompt]: Epoch 20 / 100: avg data time: 4.64e-02, avg batch time: 0.4660, average train loss: 193.3338
[09/26 06:40:30 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1612, average loss: 221.1548
[09/26 06:40:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 45.00	
[09/26 06:40:30 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[09/26 06:40:37 visual_prompt]: Epoch 21 / 100: avg data time: 4.74e-02, avg batch time: 0.4659, average train loss: 168.1774
[09/26 06:40:38 visual_prompt]: Inference (val):avg data time: 1.63e-05, avg batch time: 0.1614, average loss: 106.2093
[09/26 06:40:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.00	
[09/26 06:40:38 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[09/26 06:40:44 visual_prompt]: Epoch 22 / 100: avg data time: 5.46e-02, avg batch time: 0.4729, average train loss: 139.6146
[09/26 06:40:46 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1599, average loss: 205.3986
[09/26 06:40:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 45.00	
[09/26 06:40:46 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[09/26 06:40:52 visual_prompt]: Epoch 23 / 100: avg data time: 5.01e-02, avg batch time: 0.4679, average train loss: 184.8214
[09/26 06:40:53 visual_prompt]: Inference (val):avg data time: 1.87e-05, avg batch time: 0.1599, average loss: 229.1562
[09/26 06:40:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.50	
[09/26 06:40:53 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[09/26 06:41:00 visual_prompt]: Epoch 24 / 100: avg data time: 4.69e-02, avg batch time: 0.4638, average train loss: 208.0715
[09/26 06:41:01 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1599, average loss: 187.0793
[09/26 06:41:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 46.00	
[09/26 06:41:01 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[09/26 06:41:08 visual_prompt]: Epoch 25 / 100: avg data time: 5.38e-02, avg batch time: 0.4719, average train loss: 154.7050
[09/26 06:41:09 visual_prompt]: Inference (val):avg data time: 1.99e-05, avg batch time: 0.1602, average loss: 158.5285
[09/26 06:41:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 56.50	
[09/26 06:41:09 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[09/26 06:41:15 visual_prompt]: Epoch 26 / 100: avg data time: 4.45e-02, avg batch time: 0.4612, average train loss: 149.2466
[09/26 06:41:17 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1596, average loss: 124.0892
[09/26 06:41:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 64.00	
[09/26 06:41:17 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[09/26 06:41:23 visual_prompt]: Epoch 27 / 100: avg data time: 4.29e-02, avg batch time: 0.4600, average train loss: 199.9704
[09/26 06:41:24 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1595, average loss: 203.0517
[09/26 06:41:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 60.00	
[09/26 06:41:24 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[09/26 06:41:31 visual_prompt]: Epoch 28 / 100: avg data time: 5.21e-02, avg batch time: 0.4681, average train loss: 182.6116
[09/26 06:41:32 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1597, average loss: 169.1249
[09/26 06:41:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 50.50	
[09/26 06:41:32 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[09/26 06:41:39 visual_prompt]: Epoch 29 / 100: avg data time: 5.70e-02, avg batch time: 0.4716, average train loss: 206.2966
[09/26 06:41:40 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1594, average loss: 186.2893
[09/26 06:41:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 52.50	
[09/26 06:41:40 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[09/26 06:41:46 visual_prompt]: Epoch 30 / 100: avg data time: 4.42e-02, avg batch time: 0.4589, average train loss: 185.6130
[09/26 06:41:48 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1597, average loss: 87.0612
[09/26 06:41:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.50	top5: 40.50	
[09/26 06:41:48 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[09/26 06:41:54 visual_prompt]: Epoch 31 / 100: avg data time: 5.93e-02, avg batch time: 0.4734, average train loss: 139.3529
[09/26 06:41:56 visual_prompt]: Inference (val):avg data time: 1.91e-05, avg batch time: 0.1587, average loss: 143.6555
[09/26 06:41:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 59.00	
[09/26 06:41:56 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[09/26 06:42:02 visual_prompt]: Epoch 32 / 100: avg data time: 5.71e-02, avg batch time: 0.4712, average train loss: 187.2082
[09/26 06:42:04 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1582, average loss: 290.4631
[09/26 06:42:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 41.50	
[09/26 06:42:04 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[09/26 06:42:10 visual_prompt]: Epoch 33 / 100: avg data time: 5.59e-02, avg batch time: 0.4698, average train loss: 201.8206
[09/26 06:42:11 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1588, average loss: 249.3602
[09/26 06:42:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 06:42:11 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[09/26 06:42:18 visual_prompt]: Epoch 34 / 100: avg data time: 4.73e-02, avg batch time: 0.4620, average train loss: 222.6652
[09/26 06:42:19 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1578, average loss: 138.7314
[09/26 06:42:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 06:42:19 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[09/26 06:42:25 visual_prompt]: Epoch 35 / 100: avg data time: 4.10e-02, avg batch time: 0.4585, average train loss: 184.6381
[09/26 06:42:27 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1585, average loss: 178.9688
[09/26 06:42:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 46.00	
[09/26 06:42:27 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[09/26 06:42:33 visual_prompt]: Epoch 36 / 100: avg data time: 4.27e-02, avg batch time: 0.4560, average train loss: 212.6024
[09/26 06:42:35 visual_prompt]: Inference (val):avg data time: 1.74e-05, avg batch time: 0.1582, average loss: 129.6441
[09/26 06:42:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.00	top5: 58.00	
[09/26 06:42:35 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[09/26 06:42:41 visual_prompt]: Epoch 37 / 100: avg data time: 4.83e-02, avg batch time: 0.4628, average train loss: 169.9126
[09/26 06:42:42 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1585, average loss: 170.2849
[09/26 06:42:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.00	
[09/26 06:42:42 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[09/26 06:42:49 visual_prompt]: Epoch 38 / 100: avg data time: 5.68e-02, avg batch time: 0.4713, average train loss: 181.8471
[09/26 06:42:50 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1582, average loss: 122.6889
[09/26 06:42:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 54.50	
[09/26 06:42:50 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[09/26 06:42:56 visual_prompt]: Epoch 39 / 100: avg data time: 4.58e-02, avg batch time: 0.4609, average train loss: 140.6725
[09/26 06:42:58 visual_prompt]: Inference (val):avg data time: 1.95e-05, avg batch time: 0.1585, average loss: 239.3839
[09/26 06:42:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 56.50	
[09/26 06:42:58 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[09/26 06:43:04 visual_prompt]: Epoch 40 / 100: avg data time: 4.99e-02, avg batch time: 0.4623, average train loss: 178.0455
[09/26 06:43:06 visual_prompt]: Inference (val):avg data time: 1.72e-05, avg batch time: 0.1583, average loss: 210.0015
[09/26 06:43:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.50	
[09/26 06:43:06 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[09/26 06:43:12 visual_prompt]: Epoch 41 / 100: avg data time: 5.36e-02, avg batch time: 0.4670, average train loss: 155.1758
[09/26 06:43:13 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1582, average loss: 198.9606
[09/26 06:43:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.50	
[09/26 06:43:13 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[09/26 06:43:20 visual_prompt]: Epoch 42 / 100: avg data time: 4.14e-02, avg batch time: 0.4546, average train loss: 166.4297
[09/26 06:43:21 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.1584, average loss: 102.2497
[09/26 06:43:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 46.50	
[09/26 06:43:21 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[09/26 06:43:27 visual_prompt]: Epoch 43 / 100: avg data time: 5.10e-02, avg batch time: 0.4634, average train loss: 139.8804
[09/26 06:43:29 visual_prompt]: Inference (val):avg data time: 1.82e-05, avg batch time: 0.1583, average loss: 176.6185
[09/26 06:43:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 06:43:29 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[09/26 06:43:35 visual_prompt]: Epoch 44 / 100: avg data time: 5.61e-02, avg batch time: 0.4700, average train loss: 176.2125
[09/26 06:43:37 visual_prompt]: Inference (val):avg data time: 1.78e-05, avg batch time: 0.1577, average loss: 140.8978
[09/26 06:43:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 50.50	
[09/26 06:43:37 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[09/26 06:43:43 visual_prompt]: Epoch 45 / 100: avg data time: 4.93e-02, avg batch time: 0.4611, average train loss: 159.3642
[09/26 06:43:44 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1583, average loss: 182.1372
[09/26 06:43:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 41.50	
[09/26 06:43:44 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[09/26 06:43:50 visual_prompt]: Epoch 46 / 100: avg data time: 4.73e-02, avg batch time: 0.4602, average train loss: 141.8662
[09/26 06:43:52 visual_prompt]: Inference (val):avg data time: 1.99e-05, avg batch time: 0.1582, average loss: 111.4953
[09/26 06:43:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.00	
[09/26 06:43:52 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[09/26 06:43:58 visual_prompt]: Epoch 47 / 100: avg data time: 4.72e-02, avg batch time: 0.4611, average train loss: 148.8168
[09/26 06:43:59 visual_prompt]: Inference (val):avg data time: 1.87e-05, avg batch time: 0.1585, average loss: 118.3812
[09/26 06:43:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.00	
[09/26 06:43:59 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[09/26 06:44:06 visual_prompt]: Epoch 48 / 100: avg data time: 5.99e-02, avg batch time: 0.4717, average train loss: 144.2512
[09/26 06:44:07 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1577, average loss: 86.9838
[09/26 06:44:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 61.00	
[09/26 06:44:07 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[09/26 06:44:14 visual_prompt]: Epoch 49 / 100: avg data time: 5.38e-02, avg batch time: 0.4661, average train loss: 131.2423
[09/26 06:44:15 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1575, average loss: 84.2180
[09/26 06:44:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.00	top5: 47.50	
[09/26 06:44:15 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[09/26 06:44:21 visual_prompt]: Epoch 50 / 100: avg data time: 4.08e-02, avg batch time: 0.4528, average train loss: 123.5266
[09/26 06:44:23 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 99.9660
[09/26 06:44:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 58.00	
[09/26 06:44:23 visual_prompt]: Training 51 / 100 epoch, with learning rate 29.341204441673263
[09/26 06:44:29 visual_prompt]: Epoch 51 / 100: avg data time: 4.14e-02, avg batch time: 0.4539, average train loss: 106.8164
[09/26 06:44:30 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1584, average loss: 74.2317
[09/26 06:44:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 41.00	
[09/26 06:44:30 visual_prompt]: Training 52 / 100 epoch, with learning rate 28.479327524001636
[09/26 06:44:36 visual_prompt]: Epoch 52 / 100: avg data time: 5.23e-02, avg batch time: 0.4639, average train loss: 122.3831
[09/26 06:44:38 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1583, average loss: 117.6380
[09/26 06:44:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.50	
[09/26 06:44:38 visual_prompt]: Training 53 / 100 epoch, with learning rate 27.61321158169134
[09/26 06:44:44 visual_prompt]: Epoch 53 / 100: avg data time: 5.93e-02, avg batch time: 0.4718, average train loss: 138.0138
[09/26 06:44:46 visual_prompt]: Inference (val):avg data time: 1.65e-05, avg batch time: 0.1578, average loss: 149.8752
[09/26 06:44:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 43.00	
[09/26 06:44:46 visual_prompt]: Training 54 / 100 epoch, with learning rate 26.74391184360313
[09/26 06:44:52 visual_prompt]: Epoch 54 / 100: avg data time: 5.76e-02, avg batch time: 0.4706, average train loss: 142.6505
[09/26 06:44:54 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1583, average loss: 90.4569
[09/26 06:44:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 56.50	
[09/26 06:44:54 visual_prompt]: Training 55 / 100 epoch, with learning rate 25.872487417562528
[09/26 06:45:00 visual_prompt]: Epoch 55 / 100: avg data time: 5.26e-02, avg batch time: 0.4656, average train loss: 114.6644
[09/26 06:45:01 visual_prompt]: Inference (val):avg data time: 1.83e-05, avg batch time: 0.1580, average loss: 89.4329
[09/26 06:45:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.50	top5: 59.50	
[09/26 06:45:01 visual_prompt]: Training 56 / 100 epoch, with learning rate 25.0
[09/26 06:45:08 visual_prompt]: Epoch 56 / 100: avg data time: 5.45e-02, avg batch time: 0.4683, average train loss: 94.0219
[09/26 06:45:09 visual_prompt]: Inference (val):avg data time: 1.80e-05, avg batch time: 0.1583, average loss: 87.7993
[09/26 06:45:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 60.00	
[09/26 06:45:09 visual_prompt]: Training 57 / 100 epoch, with learning rate 24.127512582437483
[09/26 06:45:15 visual_prompt]: Epoch 57 / 100: avg data time: 5.62e-02, avg batch time: 0.4686, average train loss: 105.9060
[09/26 06:45:17 visual_prompt]: Inference (val):avg data time: 1.84e-05, avg batch time: 0.1585, average loss: 117.8753
[09/26 06:45:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.00	
[09/26 06:45:17 visual_prompt]: Training 58 / 100 epoch, with learning rate 23.256088156396867
[09/26 06:45:23 visual_prompt]: Epoch 58 / 100: avg data time: 4.51e-02, avg batch time: 0.4594, average train loss: 108.5582
[09/26 06:45:24 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1578, average loss: 116.3198
[09/26 06:45:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 44.00	
[09/26 06:45:24 visual_prompt]: Training 59 / 100 epoch, with learning rate 22.38678841830867
[09/26 06:45:31 visual_prompt]: Epoch 59 / 100: avg data time: 5.81e-02, avg batch time: 0.4700, average train loss: 108.3774
[09/26 06:45:32 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1577, average loss: 84.4958
[09/26 06:45:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 41.50	
[09/26 06:45:32 visual_prompt]: Training 60 / 100 epoch, with learning rate 21.52067247599837
[09/26 06:45:39 visual_prompt]: Epoch 60 / 100: avg data time: 5.18e-02, avg batch time: 0.4650, average train loss: 113.8531
[09/26 06:45:40 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1581, average loss: 69.2506
[09/26 06:45:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 39.00	
[09/26 06:45:40 visual_prompt]: Training 61 / 100 epoch, with learning rate 20.65879555832674
[09/26 06:45:46 visual_prompt]: Epoch 61 / 100: avg data time: 5.38e-02, avg batch time: 0.4660, average train loss: 62.8852
[09/26 06:45:48 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1582, average loss: 83.1610
[09/26 06:45:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 41.50	
[09/26 06:45:48 visual_prompt]: Training 62 / 100 epoch, with learning rate 19.80220772955602
[09/26 06:45:54 visual_prompt]: Epoch 62 / 100: avg data time: 4.41e-02, avg batch time: 0.4565, average train loss: 76.1697
[09/26 06:45:55 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.1579, average loss: 77.2868
[09/26 06:45:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 36.00	
[09/26 06:45:55 visual_prompt]: Training 63 / 100 epoch, with learning rate 18.95195261000831
[09/26 06:46:02 visual_prompt]: Epoch 63 / 100: avg data time: 5.24e-02, avg batch time: 0.4640, average train loss: 88.2231
[09/26 06:46:03 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1586, average loss: 91.6101
[09/26 06:46:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.00	
[09/26 06:46:03 visual_prompt]: Training 64 / 100 epoch, with learning rate 18.10906610457502
[09/26 06:46:10 visual_prompt]: Epoch 64 / 100: avg data time: 6.51e-02, avg batch time: 0.4775, average train loss: 80.4170
[09/26 06:46:11 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1578, average loss: 132.3746
[09/26 06:46:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.50	
[09/26 06:46:11 visual_prompt]: Training 65 / 100 epoch, with learning rate 17.274575140626318
[09/26 06:46:17 visual_prompt]: Epoch 65 / 100: avg data time: 4.31e-02, avg batch time: 0.4583, average train loss: 95.4028
[09/26 06:46:19 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1579, average loss: 98.7561
[09/26 06:46:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 50.50	
[09/26 06:46:19 visual_prompt]: Training 66 / 100 epoch, with learning rate 16.449496416858285
[09/26 06:46:25 visual_prompt]: Epoch 66 / 100: avg data time: 4.81e-02, avg batch time: 0.4619, average train loss: 79.4471
[09/26 06:46:26 visual_prompt]: Inference (val):avg data time: 1.67e-05, avg batch time: 0.1582, average loss: 88.9128
[09/26 06:46:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.50	top5: 58.50	
[09/26 06:46:26 visual_prompt]: Training 67 / 100 epoch, with learning rate 15.634835164602197
[09/26 06:46:33 visual_prompt]: Epoch 67 / 100: avg data time: 5.60e-02, avg batch time: 0.4688, average train loss: 81.7727
[09/26 06:46:34 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1584, average loss: 66.8225
[09/26 06:46:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 52.00	
[09/26 06:46:34 visual_prompt]: Training 68 / 100 epoch, with learning rate 14.831583923104999
[09/26 06:46:41 visual_prompt]: Epoch 68 / 100: avg data time: 4.99e-02, avg batch time: 0.4621, average train loss: 67.0968
[09/26 06:46:42 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1571, average loss: 48.3363
[09/26 06:46:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.50	
[09/26 06:46:42 visual_prompt]: Training 69 / 100 epoch, with learning rate 14.040721330273062
[09/26 06:46:48 visual_prompt]: Epoch 69 / 100: avg data time: 5.68e-02, avg batch time: 0.4701, average train loss: 53.9305
[09/26 06:46:50 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1583, average loss: 45.7090
[09/26 06:46:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.00	
[09/26 06:46:50 visual_prompt]: Training 70 / 100 epoch, with learning rate 13.263210930352736
[09/26 06:46:56 visual_prompt]: Epoch 70 / 100: avg data time: 5.42e-02, avg batch time: 0.4666, average train loss: 58.6002
[09/26 06:46:58 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1581, average loss: 57.4257
[09/26 06:46:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/26 06:46:58 visual_prompt]: Training 71 / 100 epoch, with learning rate 12.500000000000005
[09/26 06:47:04 visual_prompt]: Epoch 71 / 100: avg data time: 6.24e-02, avg batch time: 0.4750, average train loss: 47.0402
[09/26 06:47:06 visual_prompt]: Inference (val):avg data time: 1.85e-05, avg batch time: 0.1586, average loss: 43.6202
[09/26 06:47:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 39.50	
[09/26 06:47:06 visual_prompt]: Training 72 / 100 epoch, with learning rate 11.75201839416988
[09/26 06:47:12 visual_prompt]: Epoch 72 / 100: avg data time: 4.48e-02, avg batch time: 0.4573, average train loss: 59.6866
[09/26 06:47:13 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1582, average loss: 43.1365
[09/26 06:47:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.00	
[09/26 06:47:13 visual_prompt]: Training 73 / 100 epoch, with learning rate 11.020177413231332
[09/26 06:47:20 visual_prompt]: Epoch 73 / 100: avg data time: 5.47e-02, avg batch time: 0.4675, average train loss: 49.4380
[09/26 06:47:21 visual_prompt]: Inference (val):avg data time: 1.90e-05, avg batch time: 0.1583, average loss: 54.0450
[09/26 06:47:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 58.00	
[09/26 06:47:21 visual_prompt]: Training 74 / 100 epoch, with learning rate 10.305368692688175
[09/26 06:47:27 visual_prompt]: Epoch 74 / 100: avg data time: 4.89e-02, avg batch time: 0.4634, average train loss: 46.2353
[09/26 06:47:29 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1581, average loss: 36.2301
[09/26 06:47:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 06:47:29 visual_prompt]: Training 75 / 100 epoch, with learning rate 9.608463116858543
[09/26 06:47:35 visual_prompt]: Epoch 75 / 100: avg data time: 5.39e-02, avg batch time: 0.4677, average train loss: 41.7356
[09/26 06:47:37 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1586, average loss: 44.2953
[09/26 06:47:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 06:47:37 visual_prompt]: Training 76 / 100 epoch, with learning rate 8.930309757836516
[09/26 06:47:43 visual_prompt]: Epoch 76 / 100: avg data time: 6.10e-02, avg batch time: 0.4752, average train loss: 49.2974
[09/26 06:47:44 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1585, average loss: 26.9876
[09/26 06:47:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.00	
[09/26 06:47:44 visual_prompt]: Training 77 / 100 epoch, with learning rate 8.271734841028552
[09/26 06:47:51 visual_prompt]: Epoch 77 / 100: avg data time: 5.33e-02, avg batch time: 0.4704, average train loss: 36.6322
[09/26 06:47:52 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1583, average loss: 31.6998
[09/26 06:47:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.50	
[09/26 06:47:52 visual_prompt]: Training 78 / 100 epoch, with learning rate 7.633540738525066
[09/26 06:47:59 visual_prompt]: Epoch 78 / 100: avg data time: 6.34e-02, avg batch time: 0.4776, average train loss: 30.6009
[09/26 06:48:00 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1586, average loss: 48.5093
[09/26 06:48:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 41.00	
[09/26 06:48:00 visual_prompt]: Training 79 / 100 epoch, with learning rate 7.016504991533726
[09/26 06:48:07 visual_prompt]: Epoch 79 / 100: avg data time: 5.76e-02, avg batch time: 0.4709, average train loss: 28.0423
[09/26 06:48:08 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1643, average loss: 14.6744
[09/26 06:48:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 58.00	
[09/26 06:48:08 visual_prompt]: Training 80 / 100 epoch, with learning rate 6.4213793630651415
[09/26 06:48:14 visual_prompt]: Epoch 80 / 100: avg data time: 4.24e-02, avg batch time: 0.4566, average train loss: 31.4625
[09/26 06:48:16 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1586, average loss: 23.3707
[09/26 06:48:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 14.50	top5: 59.50	
[09/26 06:48:16 visual_prompt]: Training 81 / 100 epoch, with learning rate 5.848888922025552
[09/26 06:48:22 visual_prompt]: Epoch 81 / 100: avg data time: 5.52e-02, avg batch time: 0.4680, average train loss: 25.9475
[09/26 06:48:24 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1584, average loss: 30.0691
[09/26 06:48:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.50	
[09/26 06:48:24 visual_prompt]: Training 82 / 100 epoch, with learning rate 5.2997311598319525
[09/26 06:48:30 visual_prompt]: Epoch 82 / 100: avg data time: 4.77e-02, avg batch time: 0.4626, average train loss: 22.1270
[09/26 06:48:31 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1582, average loss: 22.4716
[09/26 06:48:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 53.00	
[09/26 06:48:31 visual_prompt]: Training 83 / 100 epoch, with learning rate 4.774575140626316
[09/26 06:48:38 visual_prompt]: Epoch 83 / 100: avg data time: 5.69e-02, avg batch time: 0.4714, average train loss: 20.4267
[09/26 06:48:39 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1585, average loss: 20.6986
[09/26 06:48:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 46.00	
[09/26 06:48:39 visual_prompt]: Training 84 / 100 epoch, with learning rate 4.27406068612396
[09/26 06:48:45 visual_prompt]: Epoch 84 / 100: avg data time: 4.93e-02, avg batch time: 0.4654, average train loss: 17.3113
[09/26 06:48:47 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1588, average loss: 11.3996
[09/26 06:48:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 41.50	
[09/26 06:48:47 visual_prompt]: Training 85 / 100 epoch, with learning rate 3.798797596089351
[09/26 06:48:53 visual_prompt]: Epoch 85 / 100: avg data time: 5.86e-02, avg batch time: 0.4725, average train loss: 11.7349
[09/26 06:48:55 visual_prompt]: Inference (val):avg data time: 1.78e-05, avg batch time: 0.1583, average loss: 10.7622
[09/26 06:48:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.00	
[09/26 06:48:55 visual_prompt]: Training 86 / 100 epoch, with learning rate 3.349364905389032
[09/26 06:49:01 visual_prompt]: Epoch 86 / 100: avg data time: 5.84e-02, avg batch time: 0.4715, average train loss: 10.1413
[09/26 06:49:03 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1585, average loss: 8.2078
[09/26 06:49:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 45.00	
[09/26 06:49:03 visual_prompt]: Training 87 / 100 epoch, with learning rate 2.9263101785268253
[09/26 06:49:09 visual_prompt]: Epoch 87 / 100: avg data time: 5.47e-02, avg batch time: 0.4679, average train loss: 6.9916
[09/26 06:49:10 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1585, average loss: 8.5322
[09/26 06:49:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 46.50	
[09/26 06:49:10 visual_prompt]: Training 88 / 100 epoch, with learning rate 2.5301488425208296
[09/26 06:49:17 visual_prompt]: Epoch 88 / 100: avg data time: 5.10e-02, avg batch time: 0.4649, average train loss: 9.3850
[09/26 06:49:18 visual_prompt]: Inference (val):avg data time: 1.91e-05, avg batch time: 0.1582, average loss: 4.5720
[09/26 06:49:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 59.00	
[09/26 06:49:18 visual_prompt]: Training 89 / 100 epoch, with learning rate 2.1613635589349753
[09/26 06:49:24 visual_prompt]: Epoch 89 / 100: avg data time: 4.72e-02, avg batch time: 0.4615, average train loss: 5.0083
[09/26 06:49:26 visual_prompt]: Inference (val):avg data time: 1.80e-05, avg batch time: 0.1588, average loss: 5.5547
[09/26 06:49:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 53.50	
[09/26 06:49:26 visual_prompt]: Training 90 / 100 epoch, with learning rate 1.820403635830317
[09/26 06:49:32 visual_prompt]: Epoch 90 / 100: avg data time: 6.16e-02, avg batch time: 0.4750, average train loss: 4.4791
[09/26 06:49:34 visual_prompt]: Inference (val):avg data time: 1.91e-05, avg batch time: 0.1587, average loss: 3.7166
[09/26 06:49:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 06:49:34 visual_prompt]: Training 91 / 100 epoch, with learning rate 1.507684480352292
[09/26 06:49:40 visual_prompt]: Epoch 91 / 100: avg data time: 4.69e-02, avg batch time: 0.4596, average train loss: 3.1588
[09/26 06:49:41 visual_prompt]: Inference (val):avg data time: 1.85e-05, avg batch time: 0.1583, average loss: 3.0326
[09/26 06:49:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 06:49:41 visual_prompt]: Training 92 / 100 epoch, with learning rate 1.2235870926211616
[09/26 06:49:48 visual_prompt]: Epoch 92 / 100: avg data time: 5.78e-02, avg batch time: 0.4730, average train loss: 3.6163
[09/26 06:49:49 visual_prompt]: Inference (val):avg data time: 1.92e-05, avg batch time: 0.1590, average loss: 2.9962
[09/26 06:49:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 64.00	
[09/26 06:49:49 visual_prompt]: Best epoch 92: best metric: 0.235
[09/26 06:49:49 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.9684576015420276
[09/26 06:49:56 visual_prompt]: Epoch 93 / 100: avg data time: 4.36e-02, avg batch time: 0.4569, average train loss: 3.2128
[09/26 06:49:57 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1583, average loss: 2.8504
[09/26 06:49:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.00	
[09/26 06:49:57 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.7426068431000882
[09/26 06:50:03 visual_prompt]: Epoch 94 / 100: avg data time: 5.84e-02, avg batch time: 0.4711, average train loss: 3.2813
[09/26 06:50:05 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1582, average loss: 2.2800
[09/26 06:50:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/26 06:50:05 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.5463099816548578
[09/26 06:50:11 visual_prompt]: Epoch 95 / 100: avg data time: 5.06e-02, avg batch time: 0.4648, average train loss: 2.4192
[09/26 06:50:13 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1587, average loss: 2.3929
[09/26 06:50:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 55.00	
[09/26 06:50:13 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.3798061746947995
[09/26 06:50:19 visual_prompt]: Epoch 96 / 100: avg data time: 5.50e-02, avg batch time: 0.4694, average train loss: 2.3039
[09/26 06:50:20 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1586, average loss: 2.2662
[09/26 06:50:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 06:50:20 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.24329828146074095
[09/26 06:50:27 visual_prompt]: Epoch 97 / 100: avg data time: 4.93e-02, avg batch time: 0.4628, average train loss: 2.2931
[09/26 06:50:28 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1586, average loss: 2.2561
[09/26 06:50:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 06:50:28 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.13695261579316775
[09/26 06:50:35 visual_prompt]: Epoch 98 / 100: avg data time: 6.14e-02, avg batch time: 0.4746, average train loss: 2.2616
[09/26 06:50:36 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1585, average loss: 2.2364
[09/26 06:50:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 06:50:36 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.06089874350439506
[09/26 06:50:42 visual_prompt]: Epoch 99 / 100: avg data time: 5.09e-02, avg batch time: 0.4656, average train loss: 2.2524
[09/26 06:50:44 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1588, average loss: 2.2194
[09/26 06:50:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 06:50:44 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.015229324522605947
[09/26 06:50:50 visual_prompt]: Epoch 100 / 100: avg data time: 5.31e-02, avg batch time: 0.4666, average train loss: 2.2343
[09/26 06:50:51 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1579, average loss: 2.2199
[09/26 06:50:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 06:50:52 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 06:50:52 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 06:50:52 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 06:50:52 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 06:50:52 visual_prompt]: Training with config:
[09/26 06:50:52 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr50.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 06:50:52 visual_prompt]: Loading training data...
[09/26 06:50:52 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 06:50:53 visual_prompt]: Number of images: 800
[09/26 06:50:53 visual_prompt]: Number of classes: 10 / 10
[09/26 06:50:53 visual_prompt]: Loading validation data...
[09/26 06:50:53 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 06:50:53 visual_prompt]: Number of images: 200
[09/26 06:50:53 visual_prompt]: Number of classes: 10 / 10
[09/26 06:50:53 visual_prompt]: Constructing models...
[09/26 06:50:55 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 06:50:55 visual_prompt]: tuned percent:0.543
[09/26 06:50:55 visual_prompt]: Device used for model: 0
[09/26 06:50:55 visual_prompt]: Setting up Evaluator...
[09/26 06:50:55 visual_prompt]: Setting up Trainer...
[09/26 06:50:55 visual_prompt]: 	Setting up the optimizer...
[09/26 06:50:56 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 06:51:02 visual_prompt]: Epoch 1 / 100: avg data time: 5.76e-02, avg batch time: 0.4778, average train loss: 2.6636
[09/26 06:51:04 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1584, average loss: 2.6214
[09/26 06:51:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 06:51:04 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 06:51:04 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[09/26 06:51:10 visual_prompt]: Epoch 2 / 100: avg data time: 5.93e-02, avg batch time: 0.4713, average train loss: 27.0848
[09/26 06:51:11 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1580, average loss: 24.2607
[09/26 06:51:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 47.00	
[09/26 06:51:11 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[09/26 06:51:18 visual_prompt]: Epoch 3 / 100: avg data time: 4.87e-02, avg batch time: 0.4613, average train loss: 32.2557
[09/26 06:51:19 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1583, average loss: 23.3504
[09/26 06:51:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 48.50	
[09/26 06:51:19 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[09/26 06:51:26 visual_prompt]: Epoch 4 / 100: avg data time: 6.55e-02, avg batch time: 0.4775, average train loss: 36.6588
[09/26 06:51:27 visual_prompt]: Inference (val):avg data time: 1.95e-05, avg batch time: 0.1581, average loss: 60.0811
[09/26 06:51:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 42.00	
[09/26 06:51:27 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[09/26 06:51:34 visual_prompt]: Epoch 5 / 100: avg data time: 4.72e-02, avg batch time: 0.4613, average train loss: 112.2521
[09/26 06:51:35 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1580, average loss: 83.6860
[09/26 06:51:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 47.50	
[09/26 06:51:35 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[09/26 06:51:42 visual_prompt]: Epoch 6 / 100: avg data time: 5.67e-02, avg batch time: 0.4684, average train loss: 105.9245
[09/26 06:51:43 visual_prompt]: Inference (val):avg data time: 1.81e-05, avg batch time: 0.1581, average loss: 101.9780
[09/26 06:51:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 06:51:43 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[09/26 06:51:50 visual_prompt]: Epoch 7 / 100: avg data time: 6.21e-02, avg batch time: 0.4729, average train loss: 119.6179
[09/26 06:51:51 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1578, average loss: 75.7074
[09/26 06:51:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 57.00	
[09/26 06:51:51 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[09/26 06:51:58 visual_prompt]: Epoch 8 / 100: avg data time: 6.03e-02, avg batch time: 0.4715, average train loss: 145.7958
[09/26 06:51:59 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1580, average loss: 156.4571
[09/26 06:51:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 51.50	
[09/26 06:51:59 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[09/26 06:52:06 visual_prompt]: Epoch 9 / 100: avg data time: 5.55e-02, avg batch time: 0.4662, average train loss: 149.8901
[09/26 06:52:07 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1582, average loss: 129.3610
[09/26 06:52:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.50	
[09/26 06:52:07 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[09/26 06:52:14 visual_prompt]: Epoch 10 / 100: avg data time: 6.39e-02, avg batch time: 0.4759, average train loss: 162.0277
[09/26 06:52:15 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1584, average loss: 139.7456
[09/26 06:52:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 52.50	
[09/26 06:52:15 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[09/26 06:52:21 visual_prompt]: Epoch 11 / 100: avg data time: 4.52e-02, avg batch time: 0.4586, average train loss: 180.5580
[09/26 06:52:23 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1583, average loss: 103.1128
[09/26 06:52:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 64.00	
[09/26 06:52:23 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[09/26 06:52:29 visual_prompt]: Epoch 12 / 100: avg data time: 5.28e-02, avg batch time: 0.4636, average train loss: 216.8293
[09/26 06:52:31 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1579, average loss: 126.7109
[09/26 06:52:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 44.50	
[09/26 06:52:31 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[09/26 06:52:37 visual_prompt]: Epoch 13 / 100: avg data time: 5.60e-02, avg batch time: 0.4685, average train loss: 162.0597
[09/26 06:52:39 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1581, average loss: 225.4601
[09/26 06:52:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 46.00	
[09/26 06:52:39 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[09/26 06:52:45 visual_prompt]: Epoch 14 / 100: avg data time: 5.77e-02, avg batch time: 0.4688, average train loss: 200.8064
[09/26 06:52:47 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1575, average loss: 250.9895
[09/26 06:52:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.50	
[09/26 06:52:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[09/26 06:52:53 visual_prompt]: Epoch 15 / 100: avg data time: 5.83e-02, avg batch time: 0.4703, average train loss: 248.3186
[09/26 06:52:55 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1579, average loss: 418.4540
[09/26 06:52:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 41.50	
[09/26 06:52:55 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[09/26 06:53:01 visual_prompt]: Epoch 16 / 100: avg data time: 6.16e-02, avg batch time: 0.4722, average train loss: 248.1787
[09/26 06:53:03 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1579, average loss: 175.5221
[09/26 06:53:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 42.50	
[09/26 06:53:03 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[09/26 06:53:09 visual_prompt]: Epoch 17 / 100: avg data time: 4.33e-02, avg batch time: 0.4577, average train loss: 213.5586
[09/26 06:53:10 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1577, average loss: 233.7857
[09/26 06:53:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 52.00	
[09/26 06:53:10 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[09/26 06:53:17 visual_prompt]: Epoch 18 / 100: avg data time: 5.54e-02, avg batch time: 0.4676, average train loss: 209.7701
[09/26 06:53:18 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1581, average loss: 202.5284
[09/26 06:53:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 42.00	
[09/26 06:53:18 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[09/26 06:53:25 visual_prompt]: Epoch 19 / 100: avg data time: 6.32e-02, avg batch time: 0.4756, average train loss: 272.9620
[09/26 06:53:26 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1577, average loss: 179.8170
[09/26 06:53:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.00	
[09/26 06:53:26 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[09/26 06:53:33 visual_prompt]: Epoch 20 / 100: avg data time: 5.94e-02, avg batch time: 0.4706, average train loss: 251.4911
[09/26 06:53:34 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1582, average loss: 158.8612
[09/26 06:53:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/26 06:53:34 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[09/26 06:53:41 visual_prompt]: Epoch 21 / 100: avg data time: 6.19e-02, avg batch time: 0.4739, average train loss: 195.6153
[09/26 06:53:42 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1583, average loss: 116.0886
[09/26 06:53:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 06:53:42 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[09/26 06:53:48 visual_prompt]: Epoch 22 / 100: avg data time: 4.19e-02, avg batch time: 0.4542, average train loss: 171.4083
[09/26 06:53:50 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1576, average loss: 126.1988
[09/26 06:53:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 35.50	
[09/26 06:53:50 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[09/26 06:53:56 visual_prompt]: Epoch 23 / 100: avg data time: 5.89e-02, avg batch time: 0.4709, average train loss: 196.2528
[09/26 06:53:58 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1581, average loss: 305.8786
[09/26 06:53:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 62.00	
[09/26 06:53:58 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[09/26 06:54:04 visual_prompt]: Epoch 24 / 100: avg data time: 5.00e-02, avg batch time: 0.4615, average train loss: 198.4610
[09/26 06:54:06 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1575, average loss: 98.6758
[09/26 06:54:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 06:54:06 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[09/26 06:54:12 visual_prompt]: Epoch 25 / 100: avg data time: 6.12e-02, avg batch time: 0.4719, average train loss: 244.3856
[09/26 06:54:14 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1579, average loss: 250.4776
[09/26 06:54:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 46.50	
[09/26 06:54:14 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[09/26 06:54:20 visual_prompt]: Epoch 26 / 100: avg data time: 4.83e-02, avg batch time: 0.4605, average train loss: 189.6175
[09/26 06:54:22 visual_prompt]: Inference (val):avg data time: 1.84e-05, avg batch time: 0.1582, average loss: 195.5759
[09/26 06:54:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 56.00	
[09/26 06:54:22 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[09/26 06:54:28 visual_prompt]: Epoch 27 / 100: avg data time: 5.87e-02, avg batch time: 0.4696, average train loss: 205.1299
[09/26 06:54:30 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1581, average loss: 288.2400
[09/26 06:54:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 42.00	
[09/26 06:54:30 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[09/26 06:54:36 visual_prompt]: Epoch 28 / 100: avg data time: 6.85e-02, avg batch time: 0.4789, average train loss: 288.4567
[09/26 06:54:38 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1573, average loss: 256.4991
[09/26 06:54:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 43.00	
[09/26 06:54:38 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[09/26 06:54:44 visual_prompt]: Epoch 29 / 100: avg data time: 6.26e-02, avg batch time: 0.4730, average train loss: 227.6228
[09/26 06:54:46 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1581, average loss: 231.1377
[09/26 06:54:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 59.00	
[09/26 06:54:46 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[09/26 06:54:52 visual_prompt]: Epoch 30 / 100: avg data time: 4.59e-02, avg batch time: 0.4595, average train loss: 231.2861
[09/26 06:54:53 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1582, average loss: 282.0821
[09/26 06:54:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 54.50	
[09/26 06:54:53 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[09/26 06:55:00 visual_prompt]: Epoch 31 / 100: avg data time: 6.19e-02, avg batch time: 0.4728, average train loss: 199.7700
[09/26 06:55:01 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1582, average loss: 144.1265
[09/26 06:55:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.00	
[09/26 06:55:01 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[09/26 06:55:08 visual_prompt]: Epoch 32 / 100: avg data time: 4.96e-02, avg batch time: 0.4635, average train loss: 181.7468
[09/26 06:55:09 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1581, average loss: 203.7797
[09/26 06:55:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.00	
[09/26 06:55:09 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[09/26 06:55:15 visual_prompt]: Epoch 33 / 100: avg data time: 4.60e-02, avg batch time: 0.4578, average train loss: 192.8306
[09/26 06:55:17 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1581, average loss: 182.1318
[09/26 06:55:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/26 06:55:17 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[09/26 06:55:23 visual_prompt]: Epoch 34 / 100: avg data time: 5.77e-02, avg batch time: 0.4696, average train loss: 217.0993
[09/26 06:55:25 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.1582, average loss: 316.2648
[09/26 06:55:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 06:55:25 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[09/26 06:55:31 visual_prompt]: Epoch 35 / 100: avg data time: 6.08e-02, avg batch time: 0.4726, average train loss: 245.5615
[09/26 06:55:33 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1578, average loss: 310.9063
[09/26 06:55:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 42.00	
[09/26 06:55:33 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[09/26 06:55:39 visual_prompt]: Epoch 36 / 100: avg data time: 5.87e-02, avg batch time: 0.4700, average train loss: 220.3676
[09/26 06:55:41 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1585, average loss: 229.4222
[09/26 06:55:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 53.00	
[09/26 06:55:41 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[09/26 06:55:47 visual_prompt]: Epoch 37 / 100: avg data time: 4.79e-02, avg batch time: 0.4614, average train loss: 164.1943
[09/26 06:55:49 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1581, average loss: 145.8094
[09/26 06:55:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 06:55:49 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[09/26 06:55:55 visual_prompt]: Epoch 38 / 100: avg data time: 6.07e-02, avg batch time: 0.4719, average train loss: 168.4469
[09/26 06:55:57 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1581, average loss: 222.5798
[09/26 06:55:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 51.50	
[09/26 06:55:57 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[09/26 06:56:03 visual_prompt]: Epoch 39 / 100: avg data time: 6.26e-02, avg batch time: 0.4741, average train loss: 160.2195
[09/26 06:56:04 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1581, average loss: 131.9699
[09/26 06:56:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 46.50	
[09/26 06:56:04 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[09/26 06:56:11 visual_prompt]: Epoch 40 / 100: avg data time: 5.98e-02, avg batch time: 0.4716, average train loss: 155.8361
[09/26 06:56:12 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1583, average loss: 199.2983
[09/26 06:56:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.50	
[09/26 06:56:12 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[09/26 06:56:19 visual_prompt]: Epoch 41 / 100: avg data time: 5.18e-02, avg batch time: 0.4646, average train loss: 134.9113
[09/26 06:56:20 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1578, average loss: 302.9467
[09/26 06:56:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 55.50	
[09/26 06:56:20 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[09/26 06:56:27 visual_prompt]: Epoch 42 / 100: avg data time: 5.74e-02, avg batch time: 0.4695, average train loss: 208.1361
[09/26 06:56:28 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1580, average loss: 170.5121
[09/26 06:56:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 49.50	
[09/26 06:56:28 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[09/26 06:56:35 visual_prompt]: Epoch 43 / 100: avg data time: 5.63e-02, avg batch time: 0.4678, average train loss: 159.0153
[09/26 06:56:36 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1581, average loss: 59.1267
[09/26 06:56:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 15.50	top5: 58.50	
[09/26 06:56:36 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[09/26 06:56:43 visual_prompt]: Epoch 44 / 100: avg data time: 6.85e-02, avg batch time: 0.4807, average train loss: 148.3806
[09/26 06:56:44 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1580, average loss: 132.7350
[09/26 06:56:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 06:56:44 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[09/26 06:56:50 visual_prompt]: Epoch 45 / 100: avg data time: 4.47e-02, avg batch time: 0.4559, average train loss: 138.5323
[09/26 06:56:52 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1584, average loss: 220.8445
[09/26 06:56:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 57.50	
[09/26 06:56:52 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[09/26 06:56:58 visual_prompt]: Epoch 46 / 100: avg data time: 5.22e-02, avg batch time: 0.4657, average train loss: 138.9997
[09/26 06:57:00 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1583, average loss: 111.9779
[09/26 06:57:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 57.50	
[09/26 06:57:00 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[09/26 06:57:06 visual_prompt]: Epoch 47 / 100: avg data time: 5.98e-02, avg batch time: 0.4716, average train loss: 127.4566
[09/26 06:57:08 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1582, average loss: 101.2840
[09/26 06:57:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.50	
[09/26 06:57:08 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[09/26 06:57:14 visual_prompt]: Epoch 48 / 100: avg data time: 5.61e-02, avg batch time: 0.4690, average train loss: 160.9309
[09/26 06:57:16 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1585, average loss: 96.8452
[09/26 06:57:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/26 06:57:16 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[09/26 06:57:22 visual_prompt]: Epoch 49 / 100: avg data time: 4.24e-02, avg batch time: 0.4553, average train loss: 164.8663
[09/26 06:57:24 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1581, average loss: 97.2833
[09/26 06:57:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 06:57:24 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[09/26 06:57:30 visual_prompt]: Epoch 50 / 100: avg data time: 6.59e-02, avg batch time: 0.4774, average train loss: 156.3409
[09/26 06:57:32 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1579, average loss: 153.4642
[09/26 06:57:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 06:57:32 visual_prompt]: Training 51 / 100 epoch, with learning rate 29.341204441673263
[09/26 06:57:38 visual_prompt]: Epoch 51 / 100: avg data time: 6.07e-02, avg batch time: 0.4719, average train loss: 169.1427
[09/26 06:57:40 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1582, average loss: 148.2350
[09/26 06:57:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.00	
[09/26 06:57:40 visual_prompt]: Training 52 / 100 epoch, with learning rate 28.479327524001636
[09/26 06:57:46 visual_prompt]: Epoch 52 / 100: avg data time: 5.16e-02, avg batch time: 0.4640, average train loss: 121.8344
[09/26 06:57:47 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1584, average loss: 143.8644
[09/26 06:57:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 58.50	
[09/26 06:57:47 visual_prompt]: Training 53 / 100 epoch, with learning rate 27.61321158169134
[09/26 06:57:54 visual_prompt]: Epoch 53 / 100: avg data time: 6.03e-02, avg batch time: 0.4739, average train loss: 143.9101
[09/26 06:57:55 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1583, average loss: 160.3365
[09/26 06:57:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 45.00	
[09/26 06:57:55 visual_prompt]: Training 54 / 100 epoch, with learning rate 26.74391184360313
[09/26 06:58:02 visual_prompt]: Epoch 54 / 100: avg data time: 5.33e-02, avg batch time: 0.4654, average train loss: 144.1097
[09/26 06:58:03 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1585, average loss: 95.2842
[09/26 06:58:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 44.50	
[09/26 06:58:03 visual_prompt]: Training 55 / 100 epoch, with learning rate 25.872487417562528
[09/26 06:58:10 visual_prompt]: Epoch 55 / 100: avg data time: 5.89e-02, avg batch time: 0.4698, average train loss: 130.5558
[09/26 06:58:11 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1583, average loss: 166.8082
[09/26 06:58:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 54.00	
[09/26 06:58:11 visual_prompt]: Training 56 / 100 epoch, with learning rate 25.0
[09/26 06:58:18 visual_prompt]: Epoch 56 / 100: avg data time: 4.52e-02, avg batch time: 0.4587, average train loss: 136.3580
[09/26 06:58:19 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1580, average loss: 105.1383
[09/26 06:58:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 5.50	top5: 56.50	
[09/26 06:58:19 visual_prompt]: Training 57 / 100 epoch, with learning rate 24.127512582437483
[09/26 06:58:26 visual_prompt]: Epoch 57 / 100: avg data time: 6.63e-02, avg batch time: 0.4782, average train loss: 144.3687
[09/26 06:58:27 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1573, average loss: 141.3472
[09/26 06:58:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 57.50	
[09/26 06:58:27 visual_prompt]: Training 58 / 100 epoch, with learning rate 23.256088156396867
[09/26 06:58:34 visual_prompt]: Epoch 58 / 100: avg data time: 5.29e-02, avg batch time: 0.4656, average train loss: 97.9056
[09/26 06:58:35 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1575, average loss: 76.9771
[09/26 06:58:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 46.00	
[09/26 06:58:35 visual_prompt]: Training 59 / 100 epoch, with learning rate 22.38678841830867
[09/26 06:58:41 visual_prompt]: Epoch 59 / 100: avg data time: 5.97e-02, avg batch time: 0.4718, average train loss: 116.9451
[09/26 06:58:43 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1584, average loss: 61.1577
[09/26 06:58:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 41.50	
[09/26 06:58:43 visual_prompt]: Training 60 / 100 epoch, with learning rate 21.52067247599837
[09/26 06:58:50 visual_prompt]: Epoch 60 / 100: avg data time: 6.49e-02, avg batch time: 0.4782, average train loss: 119.0885
[09/26 06:58:51 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1581, average loss: 104.0442
[09/26 06:58:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 06:58:51 visual_prompt]: Training 61 / 100 epoch, with learning rate 20.65879555832674
[09/26 06:58:57 visual_prompt]: Epoch 61 / 100: avg data time: 5.33e-02, avg batch time: 0.4649, average train loss: 100.3000
[09/26 06:58:59 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1584, average loss: 69.8586
[09/26 06:58:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 51.00	
[09/26 06:58:59 visual_prompt]: Training 62 / 100 epoch, with learning rate 19.80220772955602
[09/26 06:59:05 visual_prompt]: Epoch 62 / 100: avg data time: 5.90e-02, avg batch time: 0.4709, average train loss: 93.2903
[09/26 06:59:07 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1581, average loss: 84.6723
[09/26 06:59:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.00	
[09/26 06:59:07 visual_prompt]: Training 63 / 100 epoch, with learning rate 18.95195261000831
[09/26 06:59:13 visual_prompt]: Epoch 63 / 100: avg data time: 4.77e-02, avg batch time: 0.4590, average train loss: 123.7774
[09/26 06:59:15 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1578, average loss: 313.0699
[09/26 06:59:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.00	
[09/26 06:59:15 visual_prompt]: Training 64 / 100 epoch, with learning rate 18.10906610457502
[09/26 06:59:21 visual_prompt]: Epoch 64 / 100: avg data time: 6.48e-02, avg batch time: 0.4776, average train loss: 130.6191
[09/26 06:59:23 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1584, average loss: 94.6035
[09/26 06:59:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.00	
[09/26 06:59:23 visual_prompt]: Training 65 / 100 epoch, with learning rate 17.274575140626318
[09/26 06:59:29 visual_prompt]: Epoch 65 / 100: avg data time: 6.33e-02, avg batch time: 0.4763, average train loss: 89.5904
[09/26 06:59:31 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1580, average loss: 65.6761
[09/26 06:59:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 59.50	
[09/26 06:59:31 visual_prompt]: Training 66 / 100 epoch, with learning rate 16.449496416858285
[09/26 06:59:37 visual_prompt]: Epoch 66 / 100: avg data time: 4.36e-02, avg batch time: 0.4582, average train loss: 72.9583
[09/26 06:59:38 visual_prompt]: Inference (val):avg data time: 1.84e-05, avg batch time: 0.1583, average loss: 51.9677
[09/26 06:59:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.50	
[09/26 06:59:38 visual_prompt]: Training 67 / 100 epoch, with learning rate 15.634835164602197
[09/26 06:59:45 visual_prompt]: Epoch 67 / 100: avg data time: 4.52e-02, avg batch time: 0.4596, average train loss: 61.4580
[09/26 06:59:46 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1582, average loss: 56.7899
[09/26 06:59:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 06:59:46 visual_prompt]: Training 68 / 100 epoch, with learning rate 14.831583923104999
[09/26 06:59:53 visual_prompt]: Epoch 68 / 100: avg data time: 5.40e-02, avg batch time: 0.4675, average train loss: 75.2334
[09/26 06:59:54 visual_prompt]: Inference (val):avg data time: 4.50e-05, avg batch time: 0.1580, average loss: 82.3597
[09/26 06:59:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 41.50	
[09/26 06:59:54 visual_prompt]: Training 69 / 100 epoch, with learning rate 14.040721330273062
[09/26 07:00:01 visual_prompt]: Epoch 69 / 100: avg data time: 6.30e-02, avg batch time: 0.4753, average train loss: 55.7148
[09/26 07:00:02 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1583, average loss: 34.3294
[09/26 07:00:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 53.00	
[09/26 07:00:02 visual_prompt]: Training 70 / 100 epoch, with learning rate 13.263210930352736
[09/26 07:00:08 visual_prompt]: Epoch 70 / 100: avg data time: 4.72e-02, avg batch time: 0.4618, average train loss: 44.9132
[09/26 07:00:10 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1577, average loss: 22.7999
[09/26 07:00:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 44.00	
[09/26 07:00:10 visual_prompt]: Training 71 / 100 epoch, with learning rate 12.500000000000005
[09/26 07:00:16 visual_prompt]: Epoch 71 / 100: avg data time: 4.48e-02, avg batch time: 0.4585, average train loss: 39.0325
[09/26 07:00:18 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1584, average loss: 56.3647
[09/26 07:00:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 50.50	
[09/26 07:00:18 visual_prompt]: Training 72 / 100 epoch, with learning rate 11.75201839416988
[09/26 07:00:24 visual_prompt]: Epoch 72 / 100: avg data time: 6.21e-02, avg batch time: 0.4754, average train loss: 37.9930
[09/26 07:00:26 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1583, average loss: 52.9815
[09/26 07:00:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.00	
[09/26 07:00:26 visual_prompt]: Training 73 / 100 epoch, with learning rate 11.020177413231332
[09/26 07:00:32 visual_prompt]: Epoch 73 / 100: avg data time: 5.71e-02, avg batch time: 0.4696, average train loss: 64.1231
[09/26 07:00:34 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1580, average loss: 108.0531
[09/26 07:00:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.00	
[09/26 07:00:34 visual_prompt]: Training 74 / 100 epoch, with learning rate 10.305368692688175
[09/26 07:00:40 visual_prompt]: Epoch 74 / 100: avg data time: 4.33e-02, avg batch time: 0.4566, average train loss: 87.6494
[09/26 07:00:42 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1580, average loss: 62.9906
[09/26 07:00:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 44.00	
[09/26 07:00:42 visual_prompt]: Training 75 / 100 epoch, with learning rate 9.608463116858543
[09/26 07:00:48 visual_prompt]: Epoch 75 / 100: avg data time: 4.38e-02, avg batch time: 0.4577, average train loss: 47.9817
[09/26 07:00:49 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1584, average loss: 42.3219
[09/26 07:00:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 46.50	
[09/26 07:00:49 visual_prompt]: Training 76 / 100 epoch, with learning rate 8.930309757836516
[09/26 07:00:56 visual_prompt]: Epoch 76 / 100: avg data time: 5.15e-02, avg batch time: 0.4642, average train loss: 40.8772
[09/26 07:00:57 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1585, average loss: 52.8368
[09/26 07:00:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.00	
[09/26 07:00:57 visual_prompt]: Training 77 / 100 epoch, with learning rate 8.271734841028552
[09/26 07:01:04 visual_prompt]: Epoch 77 / 100: avg data time: 5.71e-02, avg batch time: 0.4695, average train loss: 49.8851
[09/26 07:01:05 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1581, average loss: 59.1284
[09/26 07:01:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 07:01:05 visual_prompt]: Training 78 / 100 epoch, with learning rate 7.633540738525066
[09/26 07:01:12 visual_prompt]: Epoch 78 / 100: avg data time: 6.10e-02, avg batch time: 0.4738, average train loss: 56.7747
[09/26 07:01:13 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1580, average loss: 58.4508
[09/26 07:01:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 07:01:13 visual_prompt]: Training 79 / 100 epoch, with learning rate 7.016504991533726
[09/26 07:01:19 visual_prompt]: Epoch 79 / 100: avg data time: 5.25e-02, avg batch time: 0.4651, average train loss: 34.6094
[09/26 07:01:21 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1579, average loss: 40.8145
[09/26 07:01:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.50	
[09/26 07:01:21 visual_prompt]: Training 80 / 100 epoch, with learning rate 6.4213793630651415
[09/26 07:01:27 visual_prompt]: Epoch 80 / 100: avg data time: 5.58e-02, avg batch time: 0.4674, average train loss: 34.4657
[09/26 07:01:29 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1582, average loss: 24.2868
[09/26 07:01:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 42.00	
[09/26 07:01:29 visual_prompt]: Training 81 / 100 epoch, with learning rate 5.848888922025552
[09/26 07:01:35 visual_prompt]: Epoch 81 / 100: avg data time: 5.86e-02, avg batch time: 0.4704, average train loss: 26.5163
[09/26 07:01:37 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1581, average loss: 52.0499
[09/26 07:01:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 57.50	
[09/26 07:01:37 visual_prompt]: Training 82 / 100 epoch, with learning rate 5.2997311598319525
[09/26 07:01:43 visual_prompt]: Epoch 82 / 100: avg data time: 6.34e-02, avg batch time: 0.4751, average train loss: 28.1771
[09/26 07:01:45 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1574, average loss: 24.6970
[09/26 07:01:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 45.00	
[09/26 07:01:45 visual_prompt]: Training 83 / 100 epoch, with learning rate 4.774575140626316
[09/26 07:01:51 visual_prompt]: Epoch 83 / 100: avg data time: 5.87e-02, avg batch time: 0.4707, average train loss: 26.0690
[09/26 07:01:53 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1584, average loss: 13.4068
[09/26 07:01:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 57.00	
[09/26 07:01:53 visual_prompt]: Training 84 / 100 epoch, with learning rate 4.27406068612396
[09/26 07:01:59 visual_prompt]: Epoch 84 / 100: avg data time: 4.49e-02, avg batch time: 0.4573, average train loss: 20.2443
[09/26 07:02:01 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1581, average loss: 24.9689
[09/26 07:02:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 50.00	
[09/26 07:02:01 visual_prompt]: Training 85 / 100 epoch, with learning rate 3.798797596089351
[09/26 07:02:07 visual_prompt]: Epoch 85 / 100: avg data time: 6.10e-02, avg batch time: 0.4748, average train loss: 14.5363
[09/26 07:02:09 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1583, average loss: 10.0585
[09/26 07:02:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 58.00	
[09/26 07:02:09 visual_prompt]: Training 86 / 100 epoch, with learning rate 3.349364905389032
[09/26 07:02:15 visual_prompt]: Epoch 86 / 100: avg data time: 5.09e-02, avg batch time: 0.4642, average train loss: 6.1242
[09/26 07:02:17 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1580, average loss: 3.9915
[09/26 07:02:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:02:17 visual_prompt]: Training 87 / 100 epoch, with learning rate 2.9263101785268253
[09/26 07:02:23 visual_prompt]: Epoch 87 / 100: avg data time: 5.85e-02, avg batch time: 0.4715, average train loss: 3.7216
[09/26 07:02:25 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1584, average loss: 3.3879
[09/26 07:02:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 58.50	
[09/26 07:02:25 visual_prompt]: Training 88 / 100 epoch, with learning rate 2.5301488425208296
[09/26 07:02:31 visual_prompt]: Epoch 88 / 100: avg data time: 5.35e-02, avg batch time: 0.4678, average train loss: 3.0642
[09/26 07:02:32 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1579, average loss: 2.5017
[09/26 07:02:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/26 07:02:32 visual_prompt]: Training 89 / 100 epoch, with learning rate 2.1613635589349753
[09/26 07:02:39 visual_prompt]: Epoch 89 / 100: avg data time: 5.46e-02, avg batch time: 0.4668, average train loss: 2.6108
[09/26 07:02:40 visual_prompt]: Inference (val):avg data time: 1.95e-05, avg batch time: 0.1581, average loss: 2.5205
[09/26 07:02:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 07:02:40 visual_prompt]: Training 90 / 100 epoch, with learning rate 1.820403635830317
[09/26 07:02:47 visual_prompt]: Epoch 90 / 100: avg data time: 5.20e-02, avg batch time: 0.4672, average train loss: 2.5109
[09/26 07:02:48 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1585, average loss: 2.4652
[09/26 07:02:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.50	
[09/26 07:02:48 visual_prompt]: Training 91 / 100 epoch, with learning rate 1.507684480352292
[09/26 07:02:55 visual_prompt]: Epoch 91 / 100: avg data time: 5.98e-02, avg batch time: 0.4728, average train loss: 2.4329
[09/26 07:02:56 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1577, average loss: 2.2336
[09/26 07:02:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:02:56 visual_prompt]: Training 92 / 100 epoch, with learning rate 1.2235870926211616
[09/26 07:03:03 visual_prompt]: Epoch 92 / 100: avg data time: 6.21e-02, avg batch time: 0.4749, average train loss: 2.2853
[09/26 07:03:04 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1583, average loss: 2.2912
[09/26 07:03:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 07:03:04 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.9684576015420276
[09/26 07:03:11 visual_prompt]: Epoch 93 / 100: avg data time: 5.71e-02, avg batch time: 0.4706, average train loss: 2.2970
[09/26 07:03:12 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1573, average loss: 2.2649
[09/26 07:03:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.50	
[09/26 07:03:12 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.7426068431000882
[09/26 07:03:19 visual_prompt]: Epoch 94 / 100: avg data time: 6.44e-02, avg batch time: 0.4771, average train loss: 2.2975
[09/26 07:03:20 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1582, average loss: 2.2935
[09/26 07:03:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 07:03:20 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.5463099816548578
[09/26 07:03:27 visual_prompt]: Epoch 95 / 100: avg data time: 5.20e-02, avg batch time: 0.4644, average train loss: 2.2751
[09/26 07:03:28 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1583, average loss: 2.2196
[09/26 07:03:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:03:28 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.3798061746947995
[09/26 07:03:35 visual_prompt]: Epoch 96 / 100: avg data time: 5.96e-02, avg batch time: 0.4728, average train loss: 2.2587
[09/26 07:03:36 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1582, average loss: 2.2350
[09/26 07:03:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.50	
[09/26 07:03:36 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.24329828146074095
[09/26 07:03:43 visual_prompt]: Epoch 97 / 100: avg data time: 5.78e-02, avg batch time: 0.4695, average train loss: 2.2485
[09/26 07:03:44 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1578, average loss: 2.2193
[09/26 07:03:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:03:44 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.13695261579316775
[09/26 07:03:50 visual_prompt]: Epoch 98 / 100: avg data time: 5.04e-02, avg batch time: 0.4655, average train loss: 2.2366
[09/26 07:03:52 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1580, average loss: 2.2192
[09/26 07:03:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 07:03:52 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.06089874350439506
[09/26 07:03:58 visual_prompt]: Epoch 99 / 100: avg data time: 4.64e-02, avg batch time: 0.4609, average train loss: 2.2291
[09/26 07:04:00 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1580, average loss: 2.2150
[09/26 07:04:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 07:04:00 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.015229324522605947
[09/26 07:04:06 visual_prompt]: Epoch 100 / 100: avg data time: 4.94e-02, avg batch time: 0.4635, average train loss: 2.2264
[09/26 07:04:08 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1580, average loss: 2.2147
[09/26 07:04:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 07:04:08 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 07:04:08 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 07:04:08 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 07:04:08 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 07:04:08 visual_prompt]: Training with config:
[09/26 07:04:08 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr50.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 07:04:08 visual_prompt]: Loading training data...
[09/26 07:04:08 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 07:04:09 visual_prompt]: Number of images: 800
[09/26 07:04:09 visual_prompt]: Number of classes: 10 / 10
[09/26 07:04:09 visual_prompt]: Loading validation data...
[09/26 07:04:09 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 07:04:10 visual_prompt]: Number of images: 200
[09/26 07:04:10 visual_prompt]: Number of classes: 10 / 10
[09/26 07:04:10 visual_prompt]: Constructing models...
[09/26 07:04:12 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 07:04:12 visual_prompt]: tuned percent:0.543
[09/26 07:04:12 visual_prompt]: Device used for model: 0
[09/26 07:04:12 visual_prompt]: Setting up Evaluator...
[09/26 07:04:12 visual_prompt]: Setting up Trainer...
[09/26 07:04:12 visual_prompt]: 	Setting up the optimizer...
[09/26 07:04:12 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 07:04:18 visual_prompt]: Epoch 1 / 100: avg data time: 5.73e-02, avg batch time: 0.4731, average train loss: 2.6641
[09/26 07:04:20 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1581, average loss: 2.6214
[09/26 07:04:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 07:04:20 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 07:04:20 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[09/26 07:04:26 visual_prompt]: Epoch 2 / 100: avg data time: 4.46e-02, avg batch time: 0.4579, average train loss: 36.9882
[09/26 07:04:28 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1582, average loss: 40.3400
[09/26 07:04:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 40.50	
[09/26 07:04:28 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[09/26 07:04:34 visual_prompt]: Epoch 3 / 100: avg data time: 6.07e-02, avg batch time: 0.4727, average train loss: 40.6383
[09/26 07:04:36 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1577, average loss: 36.0260
[09/26 07:04:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 41.50	
[09/26 07:04:36 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[09/26 07:04:42 visual_prompt]: Epoch 4 / 100: avg data time: 5.99e-02, avg batch time: 0.4720, average train loss: 71.7444
[09/26 07:04:44 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1581, average loss: 67.6122
[09/26 07:04:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 61.00	
[09/26 07:04:44 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[09/26 07:04:50 visual_prompt]: Epoch 5 / 100: avg data time: 4.43e-02, avg batch time: 0.4559, average train loss: 82.4497
[09/26 07:04:52 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1582, average loss: 99.9342
[09/26 07:04:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 56.00	
[09/26 07:04:52 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[09/26 07:04:58 visual_prompt]: Epoch 6 / 100: avg data time: 4.70e-02, avg batch time: 0.4603, average train loss: 92.5882
[09/26 07:04:59 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1582, average loss: 122.4857
[09/26 07:04:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 41.50	
[09/26 07:04:59 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[09/26 07:05:06 visual_prompt]: Epoch 7 / 100: avg data time: 6.06e-02, avg batch time: 0.4719, average train loss: 97.0601
[09/26 07:05:07 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1578, average loss: 59.5646
[09/26 07:05:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 07:05:07 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[09/26 07:05:14 visual_prompt]: Epoch 8 / 100: avg data time: 5.36e-02, avg batch time: 0.4656, average train loss: 93.6291
[09/26 07:05:15 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1578, average loss: 197.4993
[09/26 07:05:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 55.50	
[09/26 07:05:15 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[09/26 07:05:22 visual_prompt]: Epoch 9 / 100: avg data time: 5.44e-02, avg batch time: 0.4659, average train loss: 117.2421
[09/26 07:05:23 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1581, average loss: 153.1895
[09/26 07:05:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 38.00	
[09/26 07:05:23 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[09/26 07:05:30 visual_prompt]: Epoch 10 / 100: avg data time: 5.62e-02, avg batch time: 0.4685, average train loss: 170.6523
[09/26 07:05:31 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1582, average loss: 153.3020
[09/26 07:05:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 50.50	
[09/26 07:05:31 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[09/26 07:05:37 visual_prompt]: Epoch 11 / 100: avg data time: 4.47e-02, avg batch time: 0.4570, average train loss: 231.0356
[09/26 07:05:39 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1581, average loss: 230.1874
[09/26 07:05:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.50	
[09/26 07:05:39 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[09/26 07:05:45 visual_prompt]: Epoch 12 / 100: avg data time: 4.83e-02, avg batch time: 0.4607, average train loss: 189.7724
[09/26 07:05:47 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1578, average loss: 449.7527
[09/26 07:05:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.00	
[09/26 07:05:47 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[09/26 07:05:53 visual_prompt]: Epoch 13 / 100: avg data time: 5.09e-02, avg batch time: 0.4639, average train loss: 229.6661
[09/26 07:05:55 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1580, average loss: 247.1896
[09/26 07:05:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 41.50	
[09/26 07:05:55 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[09/26 07:06:01 visual_prompt]: Epoch 14 / 100: avg data time: 5.78e-02, avg batch time: 0.4687, average train loss: 282.5323
[09/26 07:06:03 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1582, average loss: 241.0406
[09/26 07:06:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 42.50	
[09/26 07:06:03 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[09/26 07:06:09 visual_prompt]: Epoch 15 / 100: avg data time: 6.16e-02, avg batch time: 0.4727, average train loss: 176.6833
[09/26 07:06:11 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1582, average loss: 137.1145
[09/26 07:06:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 07:06:11 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[09/26 07:06:17 visual_prompt]: Epoch 16 / 100: avg data time: 5.60e-02, avg batch time: 0.4682, average train loss: 156.6550
[09/26 07:06:18 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1587, average loss: 220.0717
[09/26 07:06:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 63.50	
[09/26 07:06:18 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[09/26 07:06:25 visual_prompt]: Epoch 17 / 100: avg data time: 6.33e-02, avg batch time: 0.4762, average train loss: 211.5357
[09/26 07:06:27 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1585, average loss: 298.2380
[09/26 07:06:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.50	
[09/26 07:06:27 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[09/26 07:06:33 visual_prompt]: Epoch 18 / 100: avg data time: 5.42e-02, avg batch time: 0.4671, average train loss: 187.6268
[09/26 07:06:34 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1584, average loss: 116.5666
[09/26 07:06:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 07:06:34 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[09/26 07:06:41 visual_prompt]: Epoch 19 / 100: avg data time: 4.77e-02, avg batch time: 0.4603, average train loss: 241.7492
[09/26 07:06:42 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1586, average loss: 252.0152
[09/26 07:06:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/26 07:06:42 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[09/26 07:06:49 visual_prompt]: Epoch 20 / 100: avg data time: 5.33e-02, avg batch time: 0.4686, average train loss: 197.8154
[09/26 07:06:50 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1584, average loss: 142.5555
[09/26 07:06:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 07:06:50 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[09/26 07:06:57 visual_prompt]: Epoch 21 / 100: avg data time: 4.85e-02, avg batch time: 0.4625, average train loss: 165.9126
[09/26 07:06:58 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1581, average loss: 84.8309
[09/26 07:06:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.00	top5: 65.00	
[09/26 07:06:58 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[09/26 07:07:05 visual_prompt]: Epoch 22 / 100: avg data time: 4.98e-02, avg batch time: 0.4637, average train loss: 206.9257
[09/26 07:07:06 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1583, average loss: 148.7167
[09/26 07:07:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 07:07:06 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[09/26 07:07:12 visual_prompt]: Epoch 23 / 100: avg data time: 5.39e-02, avg batch time: 0.4661, average train loss: 187.8553
[09/26 07:07:14 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1582, average loss: 257.4510
[09/26 07:07:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 62.50	
[09/26 07:07:14 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[09/26 07:07:20 visual_prompt]: Epoch 24 / 100: avg data time: 6.01e-02, avg batch time: 0.4728, average train loss: 225.3327
[09/26 07:07:22 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1575, average loss: 155.7433
[09/26 07:07:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.50	
[09/26 07:07:22 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[09/26 07:07:28 visual_prompt]: Epoch 25 / 100: avg data time: 6.38e-02, avg batch time: 0.4762, average train loss: 147.1930
[09/26 07:07:30 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1582, average loss: 261.9108
[09/26 07:07:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 42.50	
[09/26 07:07:30 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[09/26 07:07:36 visual_prompt]: Epoch 26 / 100: avg data time: 5.52e-02, avg batch time: 0.4672, average train loss: 209.4604
[09/26 07:07:38 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1583, average loss: 282.5757
[09/26 07:07:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 43.50	
[09/26 07:07:38 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[09/26 07:07:44 visual_prompt]: Epoch 27 / 100: avg data time: 4.77e-02, avg batch time: 0.4617, average train loss: 274.4598
[09/26 07:07:46 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1582, average loss: 360.5807
[09/26 07:07:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 64.50	
[09/26 07:07:46 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[09/26 07:07:52 visual_prompt]: Epoch 28 / 100: avg data time: 5.68e-02, avg batch time: 0.4685, average train loss: 225.6320
[09/26 07:07:53 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1583, average loss: 222.1647
[09/26 07:07:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:07:53 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[09/26 07:08:00 visual_prompt]: Epoch 29 / 100: avg data time: 6.69e-02, avg batch time: 0.4788, average train loss: 175.0364
[09/26 07:08:02 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1580, average loss: 237.3907
[09/26 07:08:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 57.00	
[09/26 07:08:02 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[09/26 07:08:08 visual_prompt]: Epoch 30 / 100: avg data time: 6.31e-02, avg batch time: 0.4759, average train loss: 142.9726
[09/26 07:08:10 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1579, average loss: 193.5482
[09/26 07:08:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.50	
[09/26 07:08:10 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[09/26 07:08:16 visual_prompt]: Epoch 31 / 100: avg data time: 4.94e-02, avg batch time: 0.4628, average train loss: 216.3091
[09/26 07:08:18 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1576, average loss: 228.2569
[09/26 07:08:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 43.50	
[09/26 07:08:18 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[09/26 07:08:24 visual_prompt]: Epoch 32 / 100: avg data time: 5.51e-02, avg batch time: 0.4665, average train loss: 209.4055
[09/26 07:08:25 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1582, average loss: 171.1598
[09/26 07:08:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 55.50	
[09/26 07:08:25 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[09/26 07:08:32 visual_prompt]: Epoch 33 / 100: avg data time: 6.03e-02, avg batch time: 0.4718, average train loss: 173.9688
[09/26 07:08:33 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1582, average loss: 97.0701
[09/26 07:08:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.00	
[09/26 07:08:33 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[09/26 07:08:40 visual_prompt]: Epoch 34 / 100: avg data time: 6.44e-02, avg batch time: 0.4758, average train loss: 152.5656
[09/26 07:08:41 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1581, average loss: 195.5968
[09/26 07:08:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 43.50	
[09/26 07:08:41 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[09/26 07:08:48 visual_prompt]: Epoch 35 / 100: avg data time: 6.10e-02, avg batch time: 0.4744, average train loss: 146.2185
[09/26 07:08:49 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1585, average loss: 166.6713
[09/26 07:08:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 07:08:49 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[09/26 07:08:56 visual_prompt]: Epoch 36 / 100: avg data time: 5.28e-02, avg batch time: 0.4663, average train loss: 131.4237
[09/26 07:08:57 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.1583, average loss: 55.3747
[09/26 07:08:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 58.00	
[09/26 07:08:57 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[09/26 07:09:04 visual_prompt]: Epoch 37 / 100: avg data time: 5.46e-02, avg batch time: 0.4662, average train loss: 186.5864
[09/26 07:09:05 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1584, average loss: 118.9289
[09/26 07:09:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 61.00	
[09/26 07:09:05 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[09/26 07:09:11 visual_prompt]: Epoch 38 / 100: avg data time: 5.08e-02, avg batch time: 0.4630, average train loss: 193.6968
[09/26 07:09:13 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1574, average loss: 309.6232
[09/26 07:09:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 54.50	
[09/26 07:09:13 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[09/26 07:09:19 visual_prompt]: Epoch 39 / 100: avg data time: 4.84e-02, avg batch time: 0.4608, average train loss: 222.3867
[09/26 07:09:21 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1579, average loss: 245.7231
[09/26 07:09:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/26 07:09:21 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[09/26 07:09:27 visual_prompt]: Epoch 40 / 100: avg data time: 5.25e-02, avg batch time: 0.4663, average train loss: 199.2894
[09/26 07:09:29 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1581, average loss: 204.0802
[09/26 07:09:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.00	
[09/26 07:09:29 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[09/26 07:09:35 visual_prompt]: Epoch 41 / 100: avg data time: 4.84e-02, avg batch time: 0.4639, average train loss: 199.6112
[09/26 07:09:37 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1581, average loss: 174.0256
[09/26 07:09:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.00	
[09/26 07:09:37 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[09/26 07:09:43 visual_prompt]: Epoch 42 / 100: avg data time: 5.07e-02, avg batch time: 0.4654, average train loss: 229.7186
[09/26 07:09:45 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1583, average loss: 287.0360
[09/26 07:09:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 07:09:45 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[09/26 07:09:51 visual_prompt]: Epoch 43 / 100: avg data time: 5.61e-02, avg batch time: 0.4684, average train loss: 227.3305
[09/26 07:09:53 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1580, average loss: 150.9723
[09/26 07:09:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 07:09:53 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[09/26 07:09:59 visual_prompt]: Epoch 44 / 100: avg data time: 6.45e-02, avg batch time: 0.4761, average train loss: 119.9733
[09/26 07:10:01 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1583, average loss: 154.3150
[09/26 07:10:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 56.50	
[09/26 07:10:01 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[09/26 07:10:07 visual_prompt]: Epoch 45 / 100: avg data time: 6.38e-02, avg batch time: 0.4754, average train loss: 118.0939
[09/26 07:10:09 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1581, average loss: 129.0083
[09/26 07:10:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 07:10:09 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[09/26 07:10:15 visual_prompt]: Epoch 46 / 100: avg data time: 6.11e-02, avg batch time: 0.4737, average train loss: 101.2696
[09/26 07:10:17 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1579, average loss: 91.9783
[09/26 07:10:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/26 07:10:17 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[09/26 07:10:23 visual_prompt]: Epoch 47 / 100: avg data time: 5.33e-02, avg batch time: 0.4665, average train loss: 99.0352
[09/26 07:10:25 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1581, average loss: 122.4424
[09/26 07:10:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.50	
[09/26 07:10:25 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[09/26 07:10:31 visual_prompt]: Epoch 48 / 100: avg data time: 6.15e-02, avg batch time: 0.4734, average train loss: 83.9580
[09/26 07:10:33 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1577, average loss: 103.3479
[09/26 07:10:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 50.50	
[09/26 07:10:33 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[09/26 07:10:39 visual_prompt]: Epoch 49 / 100: avg data time: 6.52e-02, avg batch time: 0.4782, average train loss: 115.0691
[09/26 07:10:41 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1582, average loss: 212.9613
[09/26 07:10:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 46.00	
[09/26 07:10:41 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[09/26 07:10:47 visual_prompt]: Epoch 50 / 100: avg data time: 4.67e-02, avg batch time: 0.4599, average train loss: 162.6431
[09/26 07:10:49 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1583, average loss: 139.2197
[09/26 07:10:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 61.50	
[09/26 07:10:49 visual_prompt]: Training 51 / 100 epoch, with learning rate 29.341204441673263
[09/26 07:10:55 visual_prompt]: Epoch 51 / 100: avg data time: 4.68e-02, avg batch time: 0.4600, average train loss: 116.4297
[09/26 07:10:56 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1585, average loss: 119.7181
[09/26 07:10:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 07:10:56 visual_prompt]: Training 52 / 100 epoch, with learning rate 28.479327524001636
[09/26 07:11:03 visual_prompt]: Epoch 52 / 100: avg data time: 4.84e-02, avg batch time: 0.4618, average train loss: 111.2806
[09/26 07:11:04 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1584, average loss: 48.4329
[09/26 07:11:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.50	
[09/26 07:11:04 visual_prompt]: Training 53 / 100 epoch, with learning rate 27.61321158169134
[09/26 07:11:11 visual_prompt]: Epoch 53 / 100: avg data time: 6.09e-02, avg batch time: 0.4720, average train loss: 74.5457
[09/26 07:11:12 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1582, average loss: 30.8049
[09/26 07:11:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 07:11:12 visual_prompt]: Training 54 / 100 epoch, with learning rate 26.74391184360313
[09/26 07:11:19 visual_prompt]: Epoch 54 / 100: avg data time: 5.41e-02, avg batch time: 0.4653, average train loss: 66.2070
[09/26 07:11:20 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1580, average loss: 64.1493
[09/26 07:11:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 5.50	top5: 39.50	
[09/26 07:11:20 visual_prompt]: Training 55 / 100 epoch, with learning rate 25.872487417562528
[09/26 07:11:27 visual_prompt]: Epoch 55 / 100: avg data time: 6.13e-02, avg batch time: 0.4745, average train loss: 73.4711
[09/26 07:11:28 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1584, average loss: 62.8532
[09/26 07:11:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 55.00	
[09/26 07:11:28 visual_prompt]: Training 56 / 100 epoch, with learning rate 25.0
[09/26 07:11:35 visual_prompt]: Epoch 56 / 100: avg data time: 5.82e-02, avg batch time: 0.4698, average train loss: 66.6416
[09/26 07:11:36 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1582, average loss: 46.1792
[09/26 07:11:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 64.00	
[09/26 07:11:36 visual_prompt]: Training 57 / 100 epoch, with learning rate 24.127512582437483
[09/26 07:11:42 visual_prompt]: Epoch 57 / 100: avg data time: 5.04e-02, avg batch time: 0.4640, average train loss: 65.6862
[09/26 07:11:44 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1582, average loss: 68.1778
[09/26 07:11:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.00	
[09/26 07:11:44 visual_prompt]: Training 58 / 100 epoch, with learning rate 23.256088156396867
[09/26 07:11:50 visual_prompt]: Epoch 58 / 100: avg data time: 5.70e-02, avg batch time: 0.4697, average train loss: 56.1173
[09/26 07:11:52 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1583, average loss: 77.6300
[09/26 07:11:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 07:11:52 visual_prompt]: Training 59 / 100 epoch, with learning rate 22.38678841830867
[09/26 07:11:58 visual_prompt]: Epoch 59 / 100: avg data time: 5.97e-02, avg batch time: 0.4723, average train loss: 63.2906
[09/26 07:12:00 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1584, average loss: 70.8804
[09/26 07:12:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 07:12:00 visual_prompt]: Training 60 / 100 epoch, with learning rate 21.52067247599837
[09/26 07:12:06 visual_prompt]: Epoch 60 / 100: avg data time: 4.40e-02, avg batch time: 0.4588, average train loss: 68.3307
[09/26 07:12:08 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1579, average loss: 69.8023
[09/26 07:12:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 40.00	
[09/26 07:12:08 visual_prompt]: Training 61 / 100 epoch, with learning rate 20.65879555832674
[09/26 07:12:14 visual_prompt]: Epoch 61 / 100: avg data time: 6.03e-02, avg batch time: 0.4734, average train loss: 46.2279
[09/26 07:12:15 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1579, average loss: 28.7596
[09/26 07:12:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 46.50	
[09/26 07:12:15 visual_prompt]: Training 62 / 100 epoch, with learning rate 19.80220772955602
[09/26 07:12:22 visual_prompt]: Epoch 62 / 100: avg data time: 5.69e-02, avg batch time: 0.4707, average train loss: 30.1807
[09/26 07:12:23 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1579, average loss: 50.8578
[09/26 07:12:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.00	
[09/26 07:12:23 visual_prompt]: Training 63 / 100 epoch, with learning rate 18.95195261000831
[09/26 07:12:30 visual_prompt]: Epoch 63 / 100: avg data time: 5.41e-02, avg batch time: 0.4666, average train loss: 42.0725
[09/26 07:12:31 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1584, average loss: 50.1216
[09/26 07:12:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.00	
[09/26 07:12:31 visual_prompt]: Training 64 / 100 epoch, with learning rate 18.10906610457502
[09/26 07:12:38 visual_prompt]: Epoch 64 / 100: avg data time: 4.92e-02, avg batch time: 0.4638, average train loss: 46.1485
[09/26 07:12:39 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1579, average loss: 34.8791
[09/26 07:12:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.50	
[09/26 07:12:39 visual_prompt]: Training 65 / 100 epoch, with learning rate 17.274575140626318
[09/26 07:12:45 visual_prompt]: Epoch 65 / 100: avg data time: 5.40e-02, avg batch time: 0.4671, average train loss: 43.5609
[09/26 07:12:47 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1582, average loss: 53.5766
[09/26 07:12:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:12:47 visual_prompt]: Training 66 / 100 epoch, with learning rate 16.449496416858285
[09/26 07:12:54 visual_prompt]: Epoch 66 / 100: avg data time: 6.32e-02, avg batch time: 0.4754, average train loss: 41.8120
[09/26 07:12:55 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1587, average loss: 23.4665
[09/26 07:12:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 49.50	
[09/26 07:12:55 visual_prompt]: Training 67 / 100 epoch, with learning rate 15.634835164602197
[09/26 07:13:01 visual_prompt]: Epoch 67 / 100: avg data time: 5.96e-02, avg batch time: 0.4726, average train loss: 38.2964
[09/26 07:13:03 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1584, average loss: 24.1120
[09/26 07:13:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 43.50	
[09/26 07:13:03 visual_prompt]: Training 68 / 100 epoch, with learning rate 14.831583923104999
[09/26 07:13:09 visual_prompt]: Epoch 68 / 100: avg data time: 4.50e-02, avg batch time: 0.4589, average train loss: 37.8784
[09/26 07:13:11 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1581, average loss: 24.4923
[09/26 07:13:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 47.50	
[09/26 07:13:11 visual_prompt]: Training 69 / 100 epoch, with learning rate 14.040721330273062
[09/26 07:13:17 visual_prompt]: Epoch 69 / 100: avg data time: 4.92e-02, avg batch time: 0.4640, average train loss: 28.0874
[09/26 07:13:19 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1584, average loss: 37.6087
[09/26 07:13:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 07:13:19 visual_prompt]: Training 70 / 100 epoch, with learning rate 13.263210930352736
[09/26 07:13:25 visual_prompt]: Epoch 70 / 100: avg data time: 5.60e-02, avg batch time: 0.4703, average train loss: 32.0880
[09/26 07:13:27 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1582, average loss: 31.8345
[09/26 07:13:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 41.00	
[09/26 07:13:27 visual_prompt]: Training 71 / 100 epoch, with learning rate 12.500000000000005
[09/26 07:13:33 visual_prompt]: Epoch 71 / 100: avg data time: 6.18e-02, avg batch time: 0.4747, average train loss: 25.0348
[09/26 07:13:35 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1583, average loss: 18.4544
[09/26 07:13:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.00	
[09/26 07:13:35 visual_prompt]: Training 72 / 100 epoch, with learning rate 11.75201839416988
[09/26 07:13:41 visual_prompt]: Epoch 72 / 100: avg data time: 5.45e-02, avg batch time: 0.4673, average train loss: 22.2653
[09/26 07:13:43 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1583, average loss: 10.0143
[09/26 07:13:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 64.50	
[09/26 07:13:43 visual_prompt]: Training 73 / 100 epoch, with learning rate 11.020177413231332
[09/26 07:13:49 visual_prompt]: Epoch 73 / 100: avg data time: 5.79e-02, avg batch time: 0.4719, average train loss: 13.0649
[09/26 07:13:51 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1587, average loss: 10.6745
[09/26 07:13:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 43.00	
[09/26 07:13:51 visual_prompt]: Training 74 / 100 epoch, with learning rate 10.305368692688175
[09/26 07:13:57 visual_prompt]: Epoch 74 / 100: avg data time: 5.56e-02, avg batch time: 0.4693, average train loss: 15.4177
[09/26 07:13:59 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1581, average loss: 14.6087
[09/26 07:13:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 62.00	
[09/26 07:13:59 visual_prompt]: Training 75 / 100 epoch, with learning rate 9.608463116858543
[09/26 07:14:05 visual_prompt]: Epoch 75 / 100: avg data time: 5.44e-02, avg batch time: 0.4670, average train loss: 16.0445
[09/26 07:14:06 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1577, average loss: 14.4394
[09/26 07:14:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 19.00	top5: 58.50	
[09/26 07:14:06 visual_prompt]: Training 76 / 100 epoch, with learning rate 8.930309757836516
[09/26 07:14:13 visual_prompt]: Epoch 76 / 100: avg data time: 5.30e-02, avg batch time: 0.4665, average train loss: 12.4739
[09/26 07:14:14 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1583, average loss: 13.0235
[09/26 07:14:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 58.50	
[09/26 07:14:14 visual_prompt]: Training 77 / 100 epoch, with learning rate 8.271734841028552
[09/26 07:14:21 visual_prompt]: Epoch 77 / 100: avg data time: 5.01e-02, avg batch time: 0.4636, average train loss: 16.4904
[09/26 07:14:22 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1582, average loss: 22.0831
[09/26 07:14:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/26 07:14:22 visual_prompt]: Training 78 / 100 epoch, with learning rate 7.633540738525066
[09/26 07:14:29 visual_prompt]: Epoch 78 / 100: avg data time: 5.39e-02, avg batch time: 0.4676, average train loss: 16.6780
[09/26 07:14:30 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1581, average loss: 15.1531
[09/26 07:14:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 07:14:30 visual_prompt]: Training 79 / 100 epoch, with learning rate 7.016504991533726
[09/26 07:14:37 visual_prompt]: Epoch 79 / 100: avg data time: 5.28e-02, avg batch time: 0.4669, average train loss: 12.1108
[09/26 07:14:38 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1585, average loss: 9.1013
[09/26 07:14:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 42.50	
[09/26 07:14:38 visual_prompt]: Training 80 / 100 epoch, with learning rate 6.4213793630651415
[09/26 07:14:44 visual_prompt]: Epoch 80 / 100: avg data time: 4.64e-02, avg batch time: 0.4600, average train loss: 7.3991
[09/26 07:14:46 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1583, average loss: 6.2563
[09/26 07:14:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 13.00	top5: 57.00	
[09/26 07:14:46 visual_prompt]: Training 81 / 100 epoch, with learning rate 5.848888922025552
[09/26 07:14:53 visual_prompt]: Epoch 81 / 100: avg data time: 6.73e-02, avg batch time: 0.4803, average train loss: 4.3750
[09/26 07:14:54 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1587, average loss: 3.5933
[09/26 07:14:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/26 07:14:54 visual_prompt]: Training 82 / 100 epoch, with learning rate 5.2997311598319525
[09/26 07:15:00 visual_prompt]: Epoch 82 / 100: avg data time: 5.61e-02, avg batch time: 0.4697, average train loss: 3.0650
[09/26 07:15:02 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1582, average loss: 2.6141
[09/26 07:15:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.00	
[09/26 07:15:02 visual_prompt]: Training 83 / 100 epoch, with learning rate 4.774575140626316
[09/26 07:15:08 visual_prompt]: Epoch 83 / 100: avg data time: 5.63e-02, avg batch time: 0.4687, average train loss: 2.6013
[09/26 07:15:10 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1580, average loss: 2.6603
[09/26 07:15:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.50	
[09/26 07:15:10 visual_prompt]: Training 84 / 100 epoch, with learning rate 4.27406068612396
[09/26 07:15:16 visual_prompt]: Epoch 84 / 100: avg data time: 4.39e-02, avg batch time: 0.4585, average train loss: 2.7267
[09/26 07:15:18 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1580, average loss: 2.4812
[09/26 07:15:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.00	
[09/26 07:15:18 visual_prompt]: Training 85 / 100 epoch, with learning rate 3.798797596089351
[09/26 07:15:24 visual_prompt]: Epoch 85 / 100: avg data time: 6.17e-02, avg batch time: 0.4753, average train loss: 2.7282
[09/26 07:15:26 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1576, average loss: 2.5984
[09/26 07:15:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 42.50	
[09/26 07:15:26 visual_prompt]: Training 86 / 100 epoch, with learning rate 3.349364905389032
[09/26 07:15:32 visual_prompt]: Epoch 86 / 100: avg data time: 5.10e-02, avg batch time: 0.4662, average train loss: 2.5598
[09/26 07:15:34 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1581, average loss: 2.4461
[09/26 07:15:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 48.00	
[09/26 07:15:34 visual_prompt]: Training 87 / 100 epoch, with learning rate 2.9263101785268253
[09/26 07:15:40 visual_prompt]: Epoch 87 / 100: avg data time: 5.86e-02, avg batch time: 0.4721, average train loss: 2.5055
[09/26 07:15:42 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1581, average loss: 2.3299
[09/26 07:15:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 62.50	
[09/26 07:15:42 visual_prompt]: Training 88 / 100 epoch, with learning rate 2.5301488425208296
[09/26 07:15:48 visual_prompt]: Epoch 88 / 100: avg data time: 5.12e-02, avg batch time: 0.4649, average train loss: 2.3960
[09/26 07:15:50 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1579, average loss: 2.3829
[09/26 07:15:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 07:15:50 visual_prompt]: Training 89 / 100 epoch, with learning rate 2.1613635589349753
[09/26 07:15:56 visual_prompt]: Epoch 89 / 100: avg data time: 5.66e-02, avg batch time: 0.4692, average train loss: 2.3761
[09/26 07:15:58 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.1584, average loss: 2.4130
[09/26 07:15:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.50	
[09/26 07:15:58 visual_prompt]: Training 90 / 100 epoch, with learning rate 1.820403635830317
[09/26 07:16:04 visual_prompt]: Epoch 90 / 100: avg data time: 5.74e-02, avg batch time: 0.4713, average train loss: 2.3362
[09/26 07:16:05 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1581, average loss: 2.2647
[09/26 07:16:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 07:16:06 visual_prompt]: Training 91 / 100 epoch, with learning rate 1.507684480352292
[09/26 07:16:12 visual_prompt]: Epoch 91 / 100: avg data time: 5.54e-02, avg batch time: 0.4698, average train loss: 2.2849
[09/26 07:16:13 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1585, average loss: 2.2904
[09/26 07:16:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 63.00	
[09/26 07:16:13 visual_prompt]: Training 92 / 100 epoch, with learning rate 1.2235870926211616
[09/26 07:16:20 visual_prompt]: Epoch 92 / 100: avg data time: 4.68e-02, avg batch time: 0.4603, average train loss: 2.2855
[09/26 07:16:21 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1589, average loss: 2.2305
[09/26 07:16:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 07:16:21 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.9684576015420276
[09/26 07:16:28 visual_prompt]: Epoch 93 / 100: avg data time: 6.48e-02, avg batch time: 0.4770, average train loss: 2.2903
[09/26 07:16:29 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1577, average loss: 2.2602
[09/26 07:16:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 07:16:29 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.7426068431000882
[09/26 07:16:36 visual_prompt]: Epoch 94 / 100: avg data time: 4.58e-02, avg batch time: 0.4591, average train loss: 2.2882
[09/26 07:16:37 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1580, average loss: 2.2773
[09/26 07:16:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 07:16:37 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.5463099816548578
[09/26 07:16:44 visual_prompt]: Epoch 95 / 100: avg data time: 4.93e-02, avg batch time: 0.4643, average train loss: 2.2636
[09/26 07:16:45 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1582, average loss: 2.2241
[09/26 07:16:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 21.50	top5: 65.00	
[09/26 07:16:45 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.3798061746947995
[09/26 07:16:51 visual_prompt]: Epoch 96 / 100: avg data time: 4.84e-02, avg batch time: 0.4627, average train loss: 2.2441
[09/26 07:16:53 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1583, average loss: 2.2271
[09/26 07:16:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:16:53 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.24329828146074095
[09/26 07:16:59 visual_prompt]: Epoch 97 / 100: avg data time: 6.30e-02, avg batch time: 0.4759, average train loss: 2.2402
[09/26 07:17:01 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1584, average loss: 2.2205
[09/26 07:17:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:17:01 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.13695261579316775
[09/26 07:17:07 visual_prompt]: Epoch 98 / 100: avg data time: 5.03e-02, avg batch time: 0.4647, average train loss: 2.2360
[09/26 07:17:09 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1582, average loss: 2.2109
[09/26 07:17:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:17:09 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.06089874350439506
[09/26 07:17:15 visual_prompt]: Epoch 99 / 100: avg data time: 5.62e-02, avg batch time: 0.4692, average train loss: 2.2360
[09/26 07:17:17 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1580, average loss: 2.2123
[09/26 07:17:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:17:17 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.015229324522605947
[09/26 07:17:23 visual_prompt]: Epoch 100 / 100: avg data time: 4.82e-02, avg batch time: 0.4633, average train loss: 2.2329
[09/26 07:17:25 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1583, average loss: 2.2129
[09/26 07:17:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:17:25 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 07:17:25 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 07:17:25 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 07:17:25 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 07:17:25 visual_prompt]: Training with config:
[09/26 07:17:25 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr50.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 07:17:25 visual_prompt]: Loading training data...
[09/26 07:17:25 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 07:17:26 visual_prompt]: Number of images: 800
[09/26 07:17:26 visual_prompt]: Number of classes: 10 / 10
[09/26 07:17:26 visual_prompt]: Loading validation data...
[09/26 07:17:26 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 07:17:26 visual_prompt]: Number of images: 200
[09/26 07:17:26 visual_prompt]: Number of classes: 10 / 10
[09/26 07:17:26 visual_prompt]: Constructing models...
[09/26 07:17:28 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 07:17:28 visual_prompt]: tuned percent:0.543
[09/26 07:17:29 visual_prompt]: Device used for model: 0
[09/26 07:17:29 visual_prompt]: Setting up Evaluator...
[09/26 07:17:29 visual_prompt]: Setting up Trainer...
[09/26 07:17:29 visual_prompt]: 	Setting up the optimizer...
[09/26 07:17:29 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 07:17:35 visual_prompt]: Epoch 1 / 100: avg data time: 5.62e-02, avg batch time: 0.4767, average train loss: 2.6858
[09/26 07:17:37 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1582, average loss: 2.6214
[09/26 07:17:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 07:17:37 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 07:17:37 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[09/26 07:17:43 visual_prompt]: Epoch 2 / 100: avg data time: 5.47e-02, avg batch time: 0.4675, average train loss: 26.8811
[09/26 07:17:45 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.1582, average loss: 28.4553
[09/26 07:17:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.50	
[09/26 07:17:45 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[09/26 07:17:51 visual_prompt]: Epoch 3 / 100: avg data time: 5.29e-02, avg batch time: 0.4655, average train loss: 36.8023
[09/26 07:17:52 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1581, average loss: 48.0103
[09/26 07:17:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 44.00	
[09/26 07:17:52 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[09/26 07:17:59 visual_prompt]: Epoch 4 / 100: avg data time: 5.67e-02, avg batch time: 0.4688, average train loss: 47.7339
[09/26 07:18:00 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1576, average loss: 35.2259
[09/26 07:18:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.00	
[09/26 07:18:00 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[09/26 07:18:07 visual_prompt]: Epoch 5 / 100: avg data time: 6.45e-02, avg batch time: 0.4763, average train loss: 52.7448
[09/26 07:18:08 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1583, average loss: 57.7556
[09/26 07:18:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 55.50	
[09/26 07:18:08 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[09/26 07:18:15 visual_prompt]: Epoch 6 / 100: avg data time: 5.32e-02, avg batch time: 0.4672, average train loss: 87.0640
[09/26 07:18:16 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1580, average loss: 101.2761
[09/26 07:18:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.50	
[09/26 07:18:16 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[09/26 07:18:23 visual_prompt]: Epoch 7 / 100: avg data time: 5.98e-02, avg batch time: 0.4709, average train loss: 79.5370
[09/26 07:18:24 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1580, average loss: 124.7472
[09/26 07:18:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 42.00	
[09/26 07:18:24 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[09/26 07:18:31 visual_prompt]: Epoch 8 / 100: avg data time: 5.16e-02, avg batch time: 0.4650, average train loss: 134.1774
[09/26 07:18:32 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1584, average loss: 108.3304
[09/26 07:18:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 45.00	
[09/26 07:18:32 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[09/26 07:18:39 visual_prompt]: Epoch 9 / 100: avg data time: 5.90e-02, avg batch time: 0.4699, average train loss: 101.1508
[09/26 07:18:40 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1575, average loss: 82.4699
[09/26 07:18:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.50	
[09/26 07:18:40 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[09/26 07:18:47 visual_prompt]: Epoch 10 / 100: avg data time: 6.26e-02, avg batch time: 0.4747, average train loss: 109.3660
[09/26 07:18:48 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1583, average loss: 143.1375
[09/26 07:18:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 07:18:48 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[09/26 07:18:55 visual_prompt]: Epoch 11 / 100: avg data time: 5.59e-02, avg batch time: 0.4679, average train loss: 136.4737
[09/26 07:18:56 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1579, average loss: 128.8159
[09/26 07:18:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 57.50	
[09/26 07:18:56 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[09/26 07:19:03 visual_prompt]: Epoch 12 / 100: avg data time: 5.72e-02, avg batch time: 0.4698, average train loss: 125.6060
[09/26 07:19:04 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1582, average loss: 157.8463
[09/26 07:19:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 42.00	
[09/26 07:19:04 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[09/26 07:19:10 visual_prompt]: Epoch 13 / 100: avg data time: 5.18e-02, avg batch time: 0.4648, average train loss: 161.6726
[09/26 07:19:12 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1580, average loss: 131.5099
[09/26 07:19:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 07:19:12 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[09/26 07:19:18 visual_prompt]: Epoch 14 / 100: avg data time: 5.79e-02, avg batch time: 0.4697, average train loss: 147.4924
[09/26 07:19:20 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1580, average loss: 91.6539
[09/26 07:19:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 57.50	
[09/26 07:19:20 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[09/26 07:19:26 visual_prompt]: Epoch 15 / 100: avg data time: 5.96e-02, avg batch time: 0.4713, average train loss: 154.2078
[09/26 07:19:28 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1582, average loss: 115.0604
[09/26 07:19:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.50	top5: 61.00	
[09/26 07:19:28 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[09/26 07:19:34 visual_prompt]: Epoch 16 / 100: avg data time: 6.00e-02, avg batch time: 0.4720, average train loss: 126.2195
[09/26 07:19:36 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.1583, average loss: 86.0706
[09/26 07:19:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 54.50	
[09/26 07:19:36 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[09/26 07:19:42 visual_prompt]: Epoch 17 / 100: avg data time: 6.15e-02, avg batch time: 0.4727, average train loss: 103.4329
[09/26 07:19:44 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.1579, average loss: 132.5151
[09/26 07:19:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 61.50	
[09/26 07:19:44 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[09/26 07:19:50 visual_prompt]: Epoch 18 / 100: avg data time: 4.56e-02, avg batch time: 0.4594, average train loss: 107.1020
[09/26 07:19:52 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1581, average loss: 113.0259
[09/26 07:19:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 64.00	
[09/26 07:19:52 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[09/26 07:19:58 visual_prompt]: Epoch 19 / 100: avg data time: 5.47e-02, avg batch time: 0.4677, average train loss: 100.5155
[09/26 07:20:00 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1580, average loss: 107.7384
[09/26 07:20:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 39.00	
[09/26 07:20:00 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[09/26 07:20:06 visual_prompt]: Epoch 20 / 100: avg data time: 5.62e-02, avg batch time: 0.4685, average train loss: 124.1707
[09/26 07:20:08 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1578, average loss: 111.4066
[09/26 07:20:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 47.50	
[09/26 07:20:08 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[09/26 07:20:14 visual_prompt]: Epoch 21 / 100: avg data time: 5.71e-02, avg batch time: 0.4698, average train loss: 94.6217
[09/26 07:20:15 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1582, average loss: 71.8376
[09/26 07:20:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 44.50	
[09/26 07:20:15 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[09/26 07:20:22 visual_prompt]: Epoch 22 / 100: avg data time: 5.75e-02, avg batch time: 0.4691, average train loss: 76.6919
[09/26 07:20:23 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1583, average loss: 88.0184
[09/26 07:20:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.00	
[09/26 07:20:23 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[09/26 07:20:30 visual_prompt]: Epoch 23 / 100: avg data time: 6.23e-02, avg batch time: 0.4739, average train loss: 105.5692
[09/26 07:20:31 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1583, average loss: 88.2578
[09/26 07:20:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 53.50	
[09/26 07:20:31 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[09/26 07:20:38 visual_prompt]: Epoch 24 / 100: avg data time: 4.52e-02, avg batch time: 0.4597, average train loss: 97.1947
[09/26 07:20:39 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1585, average loss: 79.9549
[09/26 07:20:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 49.50	
[09/26 07:20:39 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[09/26 07:20:46 visual_prompt]: Epoch 25 / 100: avg data time: 6.17e-02, avg batch time: 0.4733, average train loss: 72.9518
[09/26 07:20:47 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1577, average loss: 62.0534
[09/26 07:20:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 38.50	
[09/26 07:20:47 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[09/26 07:20:54 visual_prompt]: Epoch 26 / 100: avg data time: 6.33e-02, avg batch time: 0.4763, average train loss: 88.6302
[09/26 07:20:55 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1578, average loss: 104.2398
[09/26 07:20:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.50	
[09/26 07:20:55 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[09/26 07:21:02 visual_prompt]: Epoch 27 / 100: avg data time: 6.66e-02, avg batch time: 0.4785, average train loss: 93.0262
[09/26 07:21:03 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1577, average loss: 170.2960
[09/26 07:21:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 36.00	
[09/26 07:21:03 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[09/26 07:21:10 visual_prompt]: Epoch 28 / 100: avg data time: 4.36e-02, avg batch time: 0.4573, average train loss: 102.9049
[09/26 07:21:11 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1583, average loss: 128.8036
[09/26 07:21:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.00	
[09/26 07:21:11 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[09/26 07:21:18 visual_prompt]: Epoch 29 / 100: avg data time: 5.42e-02, avg batch time: 0.4663, average train loss: 125.7219
[09/26 07:21:19 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1582, average loss: 102.7836
[09/26 07:21:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 53.50	
[09/26 07:21:19 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[09/26 07:21:25 visual_prompt]: Epoch 30 / 100: avg data time: 5.68e-02, avg batch time: 0.4691, average train loss: 91.0067
[09/26 07:21:27 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1581, average loss: 130.1915
[09/26 07:21:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 56.50	
[09/26 07:21:27 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[09/26 07:21:33 visual_prompt]: Epoch 31 / 100: avg data time: 5.84e-02, avg batch time: 0.4705, average train loss: 108.3632
[09/26 07:21:35 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.1583, average loss: 98.2403
[09/26 07:21:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 07:21:35 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[09/26 07:21:41 visual_prompt]: Epoch 32 / 100: avg data time: 4.86e-02, avg batch time: 0.4637, average train loss: 120.5440
[09/26 07:21:43 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1583, average loss: 130.2486
[09/26 07:21:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 45.00	
[09/26 07:21:43 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[09/26 07:21:49 visual_prompt]: Epoch 33 / 100: avg data time: 4.57e-02, avg batch time: 0.4588, average train loss: 121.2731
[09/26 07:21:51 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1585, average loss: 86.8609
[09/26 07:21:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 07:21:51 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[09/26 07:21:57 visual_prompt]: Epoch 34 / 100: avg data time: 5.86e-02, avg batch time: 0.4713, average train loss: 92.3834
[09/26 07:21:59 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1583, average loss: 73.6055
[09/26 07:21:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 62.50	
[09/26 07:21:59 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[09/26 07:22:05 visual_prompt]: Epoch 35 / 100: avg data time: 6.23e-02, avg batch time: 0.4746, average train loss: 76.0229
[09/26 07:22:07 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1581, average loss: 87.6416
[09/26 07:22:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.00	
[09/26 07:22:07 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[09/26 07:22:13 visual_prompt]: Epoch 36 / 100: avg data time: 5.49e-02, avg batch time: 0.4677, average train loss: 109.1202
[09/26 07:22:15 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1582, average loss: 95.4363
[09/26 07:22:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 07:22:15 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[09/26 07:22:21 visual_prompt]: Epoch 37 / 100: avg data time: 5.23e-02, avg batch time: 0.4644, average train loss: 99.7805
[09/26 07:22:22 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1584, average loss: 91.5819
[09/26 07:22:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 46.00	
[09/26 07:22:22 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[09/26 07:22:29 visual_prompt]: Epoch 38 / 100: avg data time: 5.96e-02, avg batch time: 0.4720, average train loss: 87.6259
[09/26 07:22:30 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1578, average loss: 83.3132
[09/26 07:22:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.50	
[09/26 07:22:30 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[09/26 07:22:37 visual_prompt]: Epoch 39 / 100: avg data time: 4.92e-02, avg batch time: 0.4622, average train loss: 95.6598
[09/26 07:22:38 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1578, average loss: 118.3018
[09/26 07:22:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 40.00	
[09/26 07:22:38 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[09/26 07:22:45 visual_prompt]: Epoch 40 / 100: avg data time: 4.85e-02, avg batch time: 0.4607, average train loss: 101.0091
[09/26 07:22:46 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1587, average loss: 62.2555
[09/26 07:22:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.00	
[09/26 07:22:46 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[09/26 07:22:52 visual_prompt]: Epoch 41 / 100: avg data time: 4.62e-02, avg batch time: 0.4596, average train loss: 85.6379
[09/26 07:22:54 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1583, average loss: 71.6640
[09/26 07:22:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 53.50	
[09/26 07:22:54 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[09/26 07:23:00 visual_prompt]: Epoch 42 / 100: avg data time: 6.31e-02, avg batch time: 0.4754, average train loss: 71.3222
[09/26 07:23:02 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1581, average loss: 107.7528
[09/26 07:23:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.00	
[09/26 07:23:02 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[09/26 07:23:08 visual_prompt]: Epoch 43 / 100: avg data time: 5.98e-02, avg batch time: 0.4738, average train loss: 72.3652
[09/26 07:23:10 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1583, average loss: 106.0249
[09/26 07:23:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 59.50	
[09/26 07:23:10 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[09/26 07:23:16 visual_prompt]: Epoch 44 / 100: avg data time: 4.72e-02, avg batch time: 0.4605, average train loss: 82.2699
[09/26 07:23:18 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1586, average loss: 66.1405
[09/26 07:23:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 45.00	
[09/26 07:23:18 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[09/26 07:23:24 visual_prompt]: Epoch 45 / 100: avg data time: 5.55e-02, avg batch time: 0.4686, average train loss: 71.4162
[09/26 07:23:26 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1582, average loss: 66.0155
[09/26 07:23:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 39.00	
[09/26 07:23:26 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[09/26 07:23:32 visual_prompt]: Epoch 46 / 100: avg data time: 5.55e-02, avg batch time: 0.4686, average train loss: 79.1655
[09/26 07:23:34 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1582, average loss: 78.8075
[09/26 07:23:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 48.50	
[09/26 07:23:34 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[09/26 07:23:40 visual_prompt]: Epoch 47 / 100: avg data time: 4.60e-02, avg batch time: 0.4585, average train loss: 77.9887
[09/26 07:23:42 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1583, average loss: 96.8008
[09/26 07:23:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.00	
[09/26 07:23:42 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[09/26 07:23:48 visual_prompt]: Epoch 48 / 100: avg data time: 5.31e-02, avg batch time: 0.4668, average train loss: 95.0586
[09/26 07:23:50 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1585, average loss: 128.9244
[09/26 07:23:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 44.50	
[09/26 07:23:50 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[09/26 07:23:56 visual_prompt]: Epoch 49 / 100: avg data time: 5.23e-02, avg batch time: 0.4673, average train loss: 69.6665
[09/26 07:23:57 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1581, average loss: 53.4967
[09/26 07:23:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 52.50	
[09/26 07:23:57 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[09/26 07:24:04 visual_prompt]: Epoch 50 / 100: avg data time: 6.18e-02, avg batch time: 0.4743, average train loss: 52.6707
[09/26 07:24:05 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1578, average loss: 54.4187
[09/26 07:24:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 42.00	
[09/26 07:24:05 visual_prompt]: Training 51 / 100 epoch, with learning rate 29.341204441673263
[09/26 07:24:12 visual_prompt]: Epoch 51 / 100: avg data time: 5.75e-02, avg batch time: 0.4702, average train loss: 62.9667
[09/26 07:24:13 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1583, average loss: 67.3428
[09/26 07:24:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 44.00	
[09/26 07:24:13 visual_prompt]: Training 52 / 100 epoch, with learning rate 28.479327524001636
[09/26 07:24:20 visual_prompt]: Epoch 52 / 100: avg data time: 5.34e-02, avg batch time: 0.4659, average train loss: 68.8965
[09/26 07:24:21 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1583, average loss: 94.8217
[09/26 07:24:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.00	
[09/26 07:24:21 visual_prompt]: Training 53 / 100 epoch, with learning rate 27.61321158169134
[09/26 07:24:28 visual_prompt]: Epoch 53 / 100: avg data time: 6.10e-02, avg batch time: 0.4732, average train loss: 69.1932
[09/26 07:24:29 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.1582, average loss: 80.1194
[09/26 07:24:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 43.00	
[09/26 07:24:29 visual_prompt]: Training 54 / 100 epoch, with learning rate 26.74391184360313
[09/26 07:24:36 visual_prompt]: Epoch 54 / 100: avg data time: 6.44e-02, avg batch time: 0.4759, average train loss: 67.4892
[09/26 07:24:37 visual_prompt]: Inference (val):avg data time: 1.92e-05, avg batch time: 0.1580, average loss: 50.9985
[09/26 07:24:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 61.00	
[09/26 07:24:37 visual_prompt]: Training 55 / 100 epoch, with learning rate 25.872487417562528
[09/26 07:24:44 visual_prompt]: Epoch 55 / 100: avg data time: 4.68e-02, avg batch time: 0.4596, average train loss: 53.1549
[09/26 07:24:45 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1578, average loss: 55.9384
[09/26 07:24:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 45.00	
[09/26 07:24:45 visual_prompt]: Training 56 / 100 epoch, with learning rate 25.0
[09/26 07:24:51 visual_prompt]: Epoch 56 / 100: avg data time: 4.51e-02, avg batch time: 0.4596, average train loss: 57.0225
[09/26 07:24:53 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1582, average loss: 51.2470
[09/26 07:24:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 42.50	
[09/26 07:24:53 visual_prompt]: Training 57 / 100 epoch, with learning rate 24.127512582437483
[09/26 07:24:59 visual_prompt]: Epoch 57 / 100: avg data time: 5.50e-02, avg batch time: 0.4680, average train loss: 49.2716
[09/26 07:25:01 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.1581, average loss: 24.9420
[09/26 07:25:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 53.00	
[09/26 07:25:01 visual_prompt]: Training 58 / 100 epoch, with learning rate 23.256088156396867
[09/26 07:25:07 visual_prompt]: Epoch 58 / 100: avg data time: 5.52e-02, avg batch time: 0.4696, average train loss: 43.6362
[09/26 07:25:09 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1579, average loss: 42.6752
[09/26 07:25:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 41.00	
[09/26 07:25:09 visual_prompt]: Training 59 / 100 epoch, with learning rate 22.38678841830867
[09/26 07:25:15 visual_prompt]: Epoch 59 / 100: avg data time: 4.62e-02, avg batch time: 0.4597, average train loss: 44.6521
[09/26 07:25:17 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1584, average loss: 22.6648
[09/26 07:25:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 40.00	
[09/26 07:25:17 visual_prompt]: Training 60 / 100 epoch, with learning rate 21.52067247599837
[09/26 07:25:23 visual_prompt]: Epoch 60 / 100: avg data time: 4.99e-02, avg batch time: 0.4637, average train loss: 33.3577
[09/26 07:25:25 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1585, average loss: 39.9208
[09/26 07:25:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 47.50	
[09/26 07:25:25 visual_prompt]: Training 61 / 100 epoch, with learning rate 20.65879555832674
[09/26 07:25:31 visual_prompt]: Epoch 61 / 100: avg data time: 5.98e-02, avg batch time: 0.4735, average train loss: 38.3416
[09/26 07:25:33 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1581, average loss: 43.4488
[09/26 07:25:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 07:25:33 visual_prompt]: Training 62 / 100 epoch, with learning rate 19.80220772955602
[09/26 07:25:39 visual_prompt]: Epoch 62 / 100: avg data time: 4.62e-02, avg batch time: 0.4579, average train loss: 51.2472
[09/26 07:25:40 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1580, average loss: 47.2972
[09/26 07:25:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 45.00	
[09/26 07:25:40 visual_prompt]: Training 63 / 100 epoch, with learning rate 18.95195261000831
[09/26 07:25:47 visual_prompt]: Epoch 63 / 100: avg data time: 4.97e-02, avg batch time: 0.4626, average train loss: 46.1884
[09/26 07:25:48 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1584, average loss: 52.8176
[09/26 07:25:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 49.50	
[09/26 07:25:48 visual_prompt]: Training 64 / 100 epoch, with learning rate 18.10906610457502
[09/26 07:25:55 visual_prompt]: Epoch 64 / 100: avg data time: 6.50e-02, avg batch time: 0.4771, average train loss: 51.6391
[09/26 07:25:56 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1582, average loss: 21.4206
[09/26 07:25:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 44.00	
[09/26 07:25:56 visual_prompt]: Training 65 / 100 epoch, with learning rate 17.274575140626318
[09/26 07:26:03 visual_prompt]: Epoch 65 / 100: avg data time: 4.94e-02, avg batch time: 0.4637, average train loss: 32.7248
[09/26 07:26:04 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1584, average loss: 20.3428
[09/26 07:26:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 37.00	
[09/26 07:26:04 visual_prompt]: Training 66 / 100 epoch, with learning rate 16.449496416858285
[09/26 07:26:11 visual_prompt]: Epoch 66 / 100: avg data time: 6.34e-02, avg batch time: 0.4753, average train loss: 29.6354
[09/26 07:26:12 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1582, average loss: 19.7525
[09/26 07:26:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 07:26:12 visual_prompt]: Training 67 / 100 epoch, with learning rate 15.634835164602197
[09/26 07:26:19 visual_prompt]: Epoch 67 / 100: avg data time: 5.93e-02, avg batch time: 0.4719, average train loss: 23.0637
[09/26 07:26:20 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1586, average loss: 17.4046
[09/26 07:26:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 58.50	
[09/26 07:26:20 visual_prompt]: Training 68 / 100 epoch, with learning rate 14.831583923104999
[09/26 07:26:27 visual_prompt]: Epoch 68 / 100: avg data time: 6.32e-02, avg batch time: 0.4756, average train loss: 27.0025
[09/26 07:26:28 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1576, average loss: 18.6701
[09/26 07:26:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.00	
[09/26 07:26:28 visual_prompt]: Training 69 / 100 epoch, with learning rate 14.040721330273062
[09/26 07:26:35 visual_prompt]: Epoch 69 / 100: avg data time: 5.83e-02, avg batch time: 0.4706, average train loss: 24.8166
[09/26 07:26:36 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1583, average loss: 23.6799
[09/26 07:26:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 49.50	
[09/26 07:26:36 visual_prompt]: Training 70 / 100 epoch, with learning rate 13.263210930352736
[09/26 07:26:43 visual_prompt]: Epoch 70 / 100: avg data time: 6.40e-02, avg batch time: 0.4760, average train loss: 22.9592
[09/26 07:26:44 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1582, average loss: 19.7840
[09/26 07:26:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 50.00	
[09/26 07:26:44 visual_prompt]: Training 71 / 100 epoch, with learning rate 12.500000000000005
[09/26 07:26:50 visual_prompt]: Epoch 71 / 100: avg data time: 4.53e-02, avg batch time: 0.4582, average train loss: 16.5261
[09/26 07:26:52 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1581, average loss: 18.1747
[09/26 07:26:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.00	
[09/26 07:26:52 visual_prompt]: Training 72 / 100 epoch, with learning rate 11.75201839416988
[09/26 07:26:58 visual_prompt]: Epoch 72 / 100: avg data time: 6.22e-02, avg batch time: 0.4747, average train loss: 14.4124
[09/26 07:27:00 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1583, average loss: 17.7827
[09/26 07:27:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 07:27:00 visual_prompt]: Training 73 / 100 epoch, with learning rate 11.020177413231332
[09/26 07:27:06 visual_prompt]: Epoch 73 / 100: avg data time: 4.44e-02, avg batch time: 0.4587, average train loss: 19.0447
[09/26 07:27:08 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1580, average loss: 20.1017
[09/26 07:27:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.00	
[09/26 07:27:08 visual_prompt]: Training 74 / 100 epoch, with learning rate 10.305368692688175
[09/26 07:27:14 visual_prompt]: Epoch 74 / 100: avg data time: 5.36e-02, avg batch time: 0.4658, average train loss: 12.6809
[09/26 07:27:16 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1583, average loss: 11.2194
[09/26 07:27:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 57.50	
[09/26 07:27:16 visual_prompt]: Training 75 / 100 epoch, with learning rate 9.608463116858543
[09/26 07:27:22 visual_prompt]: Epoch 75 / 100: avg data time: 6.33e-02, avg batch time: 0.4748, average train loss: 10.0845
[09/26 07:27:24 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1582, average loss: 11.2669
[09/26 07:27:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 57.50	
[09/26 07:27:24 visual_prompt]: Training 76 / 100 epoch, with learning rate 8.930309757836516
[09/26 07:27:30 visual_prompt]: Epoch 76 / 100: avg data time: 6.42e-02, avg batch time: 0.4765, average train loss: 10.2243
[09/26 07:27:32 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1578, average loss: 11.1936
[09/26 07:27:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 07:27:32 visual_prompt]: Training 77 / 100 epoch, with learning rate 8.271734841028552
[09/26 07:27:38 visual_prompt]: Epoch 77 / 100: avg data time: 5.32e-02, avg batch time: 0.4662, average train loss: 6.9580
[09/26 07:27:40 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1578, average loss: 6.4467
[09/26 07:27:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 51.00	
[09/26 07:27:40 visual_prompt]: Training 78 / 100 epoch, with learning rate 7.633540738525066
[09/26 07:27:46 visual_prompt]: Epoch 78 / 100: avg data time: 5.58e-02, avg batch time: 0.4670, average train loss: 5.3758
[09/26 07:27:48 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1579, average loss: 4.4482
[09/26 07:27:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 07:27:48 visual_prompt]: Training 79 / 100 epoch, with learning rate 7.016504991533726
[09/26 07:27:54 visual_prompt]: Epoch 79 / 100: avg data time: 5.01e-02, avg batch time: 0.4622, average train loss: 3.4105
[09/26 07:27:55 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1578, average loss: 2.9215
[09/26 07:27:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.00	top5: 47.00	
[09/26 07:27:55 visual_prompt]: Training 80 / 100 epoch, with learning rate 6.4213793630651415
[09/26 07:28:02 visual_prompt]: Epoch 80 / 100: avg data time: 5.86e-02, avg batch time: 0.4697, average train loss: 2.8807
[09/26 07:28:03 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1584, average loss: 3.4601
[09/26 07:28:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 66.00	
[09/26 07:28:03 visual_prompt]: Training 81 / 100 epoch, with learning rate 5.848888922025552
[09/26 07:28:10 visual_prompt]: Epoch 81 / 100: avg data time: 4.63e-02, avg batch time: 0.4582, average train loss: 3.0914
[09/26 07:28:11 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1581, average loss: 3.2949
[09/26 07:28:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 44.00	
[09/26 07:28:11 visual_prompt]: Training 82 / 100 epoch, with learning rate 5.2997311598319525
[09/26 07:28:18 visual_prompt]: Epoch 82 / 100: avg data time: 5.05e-02, avg batch time: 0.4636, average train loss: 2.9800
[09/26 07:28:19 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1577, average loss: 3.0106
[09/26 07:28:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 47.50	
[09/26 07:28:19 visual_prompt]: Training 83 / 100 epoch, with learning rate 4.774575140626316
[09/26 07:28:26 visual_prompt]: Epoch 83 / 100: avg data time: 6.92e-02, avg batch time: 0.4816, average train loss: 2.7379
[09/26 07:28:27 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1581, average loss: 2.5971
[09/26 07:28:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 62.50	
[09/26 07:28:27 visual_prompt]: Training 84 / 100 epoch, with learning rate 4.27406068612396
[09/26 07:28:34 visual_prompt]: Epoch 84 / 100: avg data time: 5.18e-02, avg batch time: 0.4645, average train loss: 2.6254
[09/26 07:28:35 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1584, average loss: 2.5960
[09/26 07:28:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 49.00	
[09/26 07:28:35 visual_prompt]: Training 85 / 100 epoch, with learning rate 3.798797596089351
[09/26 07:28:42 visual_prompt]: Epoch 85 / 100: avg data time: 5.98e-02, avg batch time: 0.4710, average train loss: 2.6646
[09/26 07:28:43 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1580, average loss: 2.4928
[09/26 07:28:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 15.00	top5: 49.00	
[09/26 07:28:43 visual_prompt]: Training 86 / 100 epoch, with learning rate 3.349364905389032
[09/26 07:28:50 visual_prompt]: Epoch 86 / 100: avg data time: 5.94e-02, avg batch time: 0.4726, average train loss: 2.6620
[09/26 07:28:51 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1580, average loss: 2.6093
[09/26 07:28:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 43.50	
[09/26 07:28:51 visual_prompt]: Training 87 / 100 epoch, with learning rate 2.9263101785268253
[09/26 07:28:58 visual_prompt]: Epoch 87 / 100: avg data time: 6.03e-02, avg batch time: 0.4737, average train loss: 2.5714
[09/26 07:28:59 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.1583, average loss: 2.3085
[09/26 07:28:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 07:28:59 visual_prompt]: Training 88 / 100 epoch, with learning rate 2.5301488425208296
[09/26 07:29:06 visual_prompt]: Epoch 88 / 100: avg data time: 5.72e-02, avg batch time: 0.4686, average train loss: 2.4478
[09/26 07:29:07 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1585, average loss: 2.4258
[09/26 07:29:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 65.00	
[09/26 07:29:07 visual_prompt]: Training 89 / 100 epoch, with learning rate 2.1613635589349753
[09/26 07:29:13 visual_prompt]: Epoch 89 / 100: avg data time: 5.43e-02, avg batch time: 0.4657, average train loss: 2.5144
[09/26 07:29:15 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1583, average loss: 2.3375
[09/26 07:29:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 64.00	
[09/26 07:29:15 visual_prompt]: Best epoch 89: best metric: 0.245
[09/26 07:29:15 visual_prompt]: Training 90 / 100 epoch, with learning rate 1.820403635830317
[09/26 07:29:22 visual_prompt]: Epoch 90 / 100: avg data time: 6.35e-02, avg batch time: 0.4749, average train loss: 2.4585
[09/26 07:29:23 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1581, average loss: 2.3386
[09/26 07:29:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 66.50	
[09/26 07:29:23 visual_prompt]: Training 91 / 100 epoch, with learning rate 1.507684480352292
[09/26 07:29:30 visual_prompt]: Epoch 91 / 100: avg data time: 6.32e-02, avg batch time: 0.4760, average train loss: 2.4534
[09/26 07:29:31 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1576, average loss: 2.3858
[09/26 07:29:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 65.50	
[09/26 07:29:31 visual_prompt]: Training 92 / 100 epoch, with learning rate 1.2235870926211616
[09/26 07:29:37 visual_prompt]: Epoch 92 / 100: avg data time: 5.19e-02, avg batch time: 0.4657, average train loss: 2.4325
[09/26 07:29:39 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1578, average loss: 2.2406
[09/26 07:29:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 63.00	
[09/26 07:29:39 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.9684576015420276
[09/26 07:29:45 visual_prompt]: Epoch 93 / 100: avg data time: 4.65e-02, avg batch time: 0.4610, average train loss: 2.3726
[09/26 07:29:47 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 2.2903
[09/26 07:29:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 66.00	
[09/26 07:29:47 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.7426068431000882
[09/26 07:29:53 visual_prompt]: Epoch 94 / 100: avg data time: 5.64e-02, avg batch time: 0.4685, average train loss: 2.3957
[09/26 07:29:55 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1578, average loss: 2.2180
[09/26 07:29:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.00	top5: 66.50	
[09/26 07:29:55 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.5463099816548578
[09/26 07:30:01 visual_prompt]: Epoch 95 / 100: avg data time: 6.08e-02, avg batch time: 0.4732, average train loss: 2.4057
[09/26 07:30:03 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1584, average loss: 2.2520
[09/26 07:30:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 16.50	top5: 66.00	
[09/26 07:30:03 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.3798061746947995
[09/26 07:30:09 visual_prompt]: Epoch 96 / 100: avg data time: 5.87e-02, avg batch time: 0.4716, average train loss: 2.3620
[09/26 07:30:11 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1582, average loss: 2.2401
[09/26 07:30:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.00	top5: 61.00	
[09/26 07:30:11 visual_prompt]: Best epoch 96: best metric: 0.250
[09/26 07:30:11 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.24329828146074095
[09/26 07:30:17 visual_prompt]: Epoch 97 / 100: avg data time: 6.09e-02, avg batch time: 0.4734, average train loss: 2.3518
[09/26 07:30:19 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1583, average loss: 2.2388
[09/26 07:30:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 65.50	
[09/26 07:30:19 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.13695261579316775
[09/26 07:30:25 visual_prompt]: Epoch 98 / 100: avg data time: 4.52e-02, avg batch time: 0.4590, average train loss: 2.3732
[09/26 07:30:26 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1583, average loss: 2.2353
[09/26 07:30:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.00	top5: 63.50	
[09/26 07:30:26 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.06089874350439506
[09/26 07:30:33 visual_prompt]: Epoch 99 / 100: avg data time: 5.53e-02, avg batch time: 0.4673, average train loss: 2.3689
[09/26 07:30:34 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1577, average loss: 2.2330
[09/26 07:30:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.00	top5: 63.00	
[09/26 07:30:34 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.015229324522605947
[09/26 07:30:41 visual_prompt]: Epoch 100 / 100: avg data time: 6.35e-02, avg batch time: 0.4752, average train loss: 2.3415
[09/26 07:30:42 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1580, average loss: 2.2347
[09/26 07:30:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.00	top5: 63.00	
[09/26 07:30:42 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 07:30:42 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 07:30:42 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 07:30:42 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 07:30:42 visual_prompt]: Training with config:
[09/26 07:30:42 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr25.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 07:30:42 visual_prompt]: Loading training data...
[09/26 07:30:42 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 07:30:43 visual_prompt]: Number of images: 800
[09/26 07:30:43 visual_prompt]: Number of classes: 10 / 10
[09/26 07:30:43 visual_prompt]: Loading validation data...
[09/26 07:30:43 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 07:30:44 visual_prompt]: Number of images: 200
[09/26 07:30:44 visual_prompt]: Number of classes: 10 / 10
[09/26 07:30:44 visual_prompt]: Constructing models...
[09/26 07:30:46 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 07:30:46 visual_prompt]: tuned percent:0.543
[09/26 07:30:46 visual_prompt]: Device used for model: 0
[09/26 07:30:46 visual_prompt]: Setting up Evaluator...
[09/26 07:30:46 visual_prompt]: Setting up Trainer...
[09/26 07:30:46 visual_prompt]: 	Setting up the optimizer...
[09/26 07:30:46 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 07:30:53 visual_prompt]: Epoch 1 / 100: avg data time: 5.57e-02, avg batch time: 0.4766, average train loss: 2.6737
[09/26 07:30:54 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1587, average loss: 2.6214
[09/26 07:30:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 07:30:54 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 07:30:54 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[09/26 07:31:01 visual_prompt]: Epoch 2 / 100: avg data time: 5.98e-02, avg batch time: 0.4720, average train loss: 10.9562
[09/26 07:31:02 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1584, average loss: 12.0271
[09/26 07:31:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 47.50	
[09/26 07:31:02 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[09/26 07:31:09 visual_prompt]: Epoch 3 / 100: avg data time: 6.03e-02, avg batch time: 0.4733, average train loss: 12.6647
[09/26 07:31:10 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.1584, average loss: 16.8761
[09/26 07:31:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 43.00	
[09/26 07:31:10 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[09/26 07:31:17 visual_prompt]: Epoch 4 / 100: avg data time: 6.23e-02, avg batch time: 0.4757, average train loss: 25.9978
[09/26 07:31:18 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1580, average loss: 27.0480
[09/26 07:31:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 39.50	
[09/26 07:31:18 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[09/26 07:31:25 visual_prompt]: Epoch 5 / 100: avg data time: 5.27e-02, avg batch time: 0.4662, average train loss: 39.5602
[09/26 07:31:26 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1580, average loss: 54.7360
[09/26 07:31:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 42.00	
[09/26 07:31:26 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[09/26 07:31:32 visual_prompt]: Epoch 6 / 100: avg data time: 5.22e-02, avg batch time: 0.4655, average train loss: 48.0106
[09/26 07:31:34 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1582, average loss: 62.8632
[09/26 07:31:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 39.00	
[09/26 07:31:34 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[09/26 07:31:40 visual_prompt]: Epoch 7 / 100: avg data time: 6.13e-02, avg batch time: 0.4729, average train loss: 52.4137
[09/26 07:31:42 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1582, average loss: 53.0459
[09/26 07:31:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 55.00	
[09/26 07:31:42 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[09/26 07:31:48 visual_prompt]: Epoch 8 / 100: avg data time: 6.60e-02, avg batch time: 0.4780, average train loss: 63.5865
[09/26 07:31:50 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1583, average loss: 90.2580
[09/26 07:31:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 57.50	
[09/26 07:31:50 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[09/26 07:31:56 visual_prompt]: Epoch 9 / 100: avg data time: 6.33e-02, avg batch time: 0.4759, average train loss: 64.1962
[09/26 07:31:58 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1582, average loss: 43.3364
[09/26 07:31:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 55.00	
[09/26 07:31:58 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[09/26 07:32:04 visual_prompt]: Epoch 10 / 100: avg data time: 5.76e-02, avg batch time: 0.4701, average train loss: 75.2871
[09/26 07:32:06 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1580, average loss: 117.0066
[09/26 07:32:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 19.00	top5: 53.50	
[09/26 07:32:06 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[09/26 07:32:12 visual_prompt]: Epoch 11 / 100: avg data time: 5.92e-02, avg batch time: 0.4713, average train loss: 109.3337
[09/26 07:32:14 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1583, average loss: 86.7918
[09/26 07:32:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 46.50	
[09/26 07:32:14 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[09/26 07:32:20 visual_prompt]: Epoch 12 / 100: avg data time: 5.91e-02, avg batch time: 0.4722, average train loss: 100.7305
[09/26 07:32:22 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1580, average loss: 56.7938
[09/26 07:32:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 52.50	
[09/26 07:32:22 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[09/26 07:32:28 visual_prompt]: Epoch 13 / 100: avg data time: 6.03e-02, avg batch time: 0.4725, average train loss: 113.2913
[09/26 07:32:30 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1581, average loss: 106.2624
[09/26 07:32:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 54.50	
[09/26 07:32:30 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[09/26 07:32:36 visual_prompt]: Epoch 14 / 100: avg data time: 5.34e-02, avg batch time: 0.4659, average train loss: 99.4294
[09/26 07:32:38 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1583, average loss: 79.9718
[09/26 07:32:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 40.50	
[09/26 07:32:38 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[09/26 07:32:44 visual_prompt]: Epoch 15 / 100: avg data time: 4.59e-02, avg batch time: 0.4596, average train loss: 76.7841
[09/26 07:32:46 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1584, average loss: 59.5247
[09/26 07:32:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.00	
[09/26 07:32:46 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[09/26 07:32:52 visual_prompt]: Epoch 16 / 100: avg data time: 5.31e-02, avg batch time: 0.4671, average train loss: 93.5882
[09/26 07:32:54 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1579, average loss: 104.8609
[09/26 07:32:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 59.00	
[09/26 07:32:54 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[09/26 07:33:00 visual_prompt]: Epoch 17 / 100: avg data time: 6.37e-02, avg batch time: 0.4762, average train loss: 109.1174
[09/26 07:33:02 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1584, average loss: 119.3305
[09/26 07:33:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 41.50	
[09/26 07:33:02 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[09/26 07:33:08 visual_prompt]: Epoch 18 / 100: avg data time: 5.78e-02, avg batch time: 0.4708, average train loss: 122.2520
[09/26 07:33:10 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1580, average loss: 117.8243
[09/26 07:33:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 57.50	
[09/26 07:33:10 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[09/26 07:33:16 visual_prompt]: Epoch 19 / 100: avg data time: 5.62e-02, avg batch time: 0.4699, average train loss: 99.1531
[09/26 07:33:18 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1585, average loss: 60.1970
[09/26 07:33:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.00	
[09/26 07:33:18 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[09/26 07:33:24 visual_prompt]: Epoch 20 / 100: avg data time: 5.60e-02, avg batch time: 0.4700, average train loss: 94.3589
[09/26 07:33:25 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1579, average loss: 57.5718
[09/26 07:33:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 53.50	
[09/26 07:33:25 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[09/26 07:33:32 visual_prompt]: Epoch 21 / 100: avg data time: 6.18e-02, avg batch time: 0.4767, average train loss: 118.6908
[09/26 07:33:34 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 78.1891
[09/26 07:33:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 43.00	
[09/26 07:33:34 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[09/26 07:33:40 visual_prompt]: Epoch 22 / 100: avg data time: 6.03e-02, avg batch time: 0.4733, average train loss: 89.8798
[09/26 07:33:41 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1583, average loss: 150.3620
[09/26 07:33:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 44.50	
[09/26 07:33:41 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[09/26 07:33:48 visual_prompt]: Epoch 23 / 100: avg data time: 6.06e-02, avg batch time: 0.4741, average train loss: 101.6805
[09/26 07:33:49 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 110.1272
[09/26 07:33:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.00	top5: 47.00	
[09/26 07:33:49 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[09/26 07:33:56 visual_prompt]: Epoch 24 / 100: avg data time: 5.40e-02, avg batch time: 0.4680, average train loss: 115.7884
[09/26 07:33:57 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1586, average loss: 72.6381
[09/26 07:33:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 56.00	
[09/26 07:33:57 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[09/26 07:34:04 visual_prompt]: Epoch 25 / 100: avg data time: 5.07e-02, avg batch time: 0.4643, average train loss: 105.2811
[09/26 07:34:05 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1584, average loss: 93.4638
[09/26 07:34:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 45.00	
[09/26 07:34:05 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[09/26 07:34:12 visual_prompt]: Epoch 26 / 100: avg data time: 5.63e-02, avg batch time: 0.4688, average train loss: 107.3554
[09/26 07:34:13 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1581, average loss: 109.3367
[09/26 07:34:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 40.50	
[09/26 07:34:13 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[09/26 07:34:20 visual_prompt]: Epoch 27 / 100: avg data time: 5.52e-02, avg batch time: 0.4672, average train loss: 128.0330
[09/26 07:34:21 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1581, average loss: 115.0218
[09/26 07:34:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.00	
[09/26 07:34:21 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[09/26 07:34:28 visual_prompt]: Epoch 28 / 100: avg data time: 5.98e-02, avg batch time: 0.4718, average train loss: 116.3398
[09/26 07:34:29 visual_prompt]: Inference (val):avg data time: 4.64e-05, avg batch time: 0.1582, average loss: 141.8622
[09/26 07:34:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 07:34:29 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[09/26 07:34:36 visual_prompt]: Epoch 29 / 100: avg data time: 6.78e-02, avg batch time: 0.4796, average train loss: 129.0748
[09/26 07:34:37 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1583, average loss: 115.7984
[09/26 07:34:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 56.00	
[09/26 07:34:37 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[09/26 07:34:44 visual_prompt]: Epoch 30 / 100: avg data time: 5.65e-02, avg batch time: 0.4686, average train loss: 111.6415
[09/26 07:34:45 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1585, average loss: 79.9739
[09/26 07:34:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 55.00	
[09/26 07:34:45 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[09/26 07:34:52 visual_prompt]: Epoch 31 / 100: avg data time: 5.94e-02, avg batch time: 0.4714, average train loss: 139.3850
[09/26 07:34:53 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1583, average loss: 339.2245
[09/26 07:34:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 56.00	
[09/26 07:34:53 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[09/26 07:35:00 visual_prompt]: Epoch 32 / 100: avg data time: 5.78e-02, avg batch time: 0.4705, average train loss: 135.4453
[09/26 07:35:01 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1585, average loss: 92.6328
[09/26 07:35:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.00	
[09/26 07:35:01 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[09/26 07:35:08 visual_prompt]: Epoch 33 / 100: avg data time: 5.66e-02, avg batch time: 0.4706, average train loss: 101.7867
[09/26 07:35:09 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1582, average loss: 106.8992
[09/26 07:35:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 44.00	
[09/26 07:35:09 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[09/26 07:35:15 visual_prompt]: Epoch 34 / 100: avg data time: 4.72e-02, avg batch time: 0.4615, average train loss: 92.3388
[09/26 07:35:17 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1583, average loss: 43.8652
[09/26 07:35:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.50	
[09/26 07:35:17 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[09/26 07:35:23 visual_prompt]: Epoch 35 / 100: avg data time: 6.37e-02, avg batch time: 0.4769, average train loss: 84.0241
[09/26 07:35:25 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1580, average loss: 73.9159
[09/26 07:35:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 62.50	
[09/26 07:35:25 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[09/26 07:35:31 visual_prompt]: Epoch 36 / 100: avg data time: 6.23e-02, avg batch time: 0.4745, average train loss: 102.5308
[09/26 07:35:33 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1582, average loss: 143.4370
[09/26 07:35:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 43.50	
[09/26 07:35:33 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[09/26 07:35:39 visual_prompt]: Epoch 37 / 100: avg data time: 5.51e-02, avg batch time: 0.4687, average train loss: 94.2350
[09/26 07:35:41 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1582, average loss: 71.9544
[09/26 07:35:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 43.50	
[09/26 07:35:41 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[09/26 07:35:47 visual_prompt]: Epoch 38 / 100: avg data time: 5.90e-02, avg batch time: 0.4715, average train loss: 82.0553
[09/26 07:35:49 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1580, average loss: 88.5307
[09/26 07:35:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.50	top5: 42.50	
[09/26 07:35:49 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[09/26 07:35:55 visual_prompt]: Epoch 39 / 100: avg data time: 6.29e-02, avg batch time: 0.4762, average train loss: 77.5823
[09/26 07:35:57 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1581, average loss: 63.0702
[09/26 07:35:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 07:35:57 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[09/26 07:36:03 visual_prompt]: Epoch 40 / 100: avg data time: 5.75e-02, avg batch time: 0.4696, average train loss: 80.2811
[09/26 07:36:05 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1582, average loss: 60.8054
[09/26 07:36:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 07:36:05 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[09/26 07:36:11 visual_prompt]: Epoch 41 / 100: avg data time: 6.30e-02, avg batch time: 0.4755, average train loss: 83.5108
[09/26 07:36:13 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1583, average loss: 58.3686
[09/26 07:36:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 47.50	
[09/26 07:36:13 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[09/26 07:36:19 visual_prompt]: Epoch 42 / 100: avg data time: 5.75e-02, avg batch time: 0.4714, average train loss: 73.2795
[09/26 07:36:21 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1578, average loss: 57.9879
[09/26 07:36:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 52.50	
[09/26 07:36:21 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[09/26 07:36:27 visual_prompt]: Epoch 43 / 100: avg data time: 5.55e-02, avg batch time: 0.4676, average train loss: 71.9724
[09/26 07:36:29 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1582, average loss: 57.0243
[09/26 07:36:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 52.50	
[09/26 07:36:29 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[09/26 07:36:35 visual_prompt]: Epoch 44 / 100: avg data time: 5.97e-02, avg batch time: 0.4725, average train loss: 61.2455
[09/26 07:36:37 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1581, average loss: 51.3646
[09/26 07:36:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 59.00	
[09/26 07:36:37 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[09/26 07:36:43 visual_prompt]: Epoch 45 / 100: avg data time: 6.59e-02, avg batch time: 0.4777, average train loss: 71.1643
[09/26 07:36:45 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1581, average loss: 113.3981
[09/26 07:36:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 07:36:45 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[09/26 07:36:51 visual_prompt]: Epoch 46 / 100: avg data time: 5.39e-02, avg batch time: 0.4662, average train loss: 84.1573
[09/26 07:36:53 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1585, average loss: 51.4445
[09/26 07:36:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 07:36:53 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[09/26 07:36:59 visual_prompt]: Epoch 47 / 100: avg data time: 5.72e-02, avg batch time: 0.4698, average train loss: 77.5903
[09/26 07:37:01 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1582, average loss: 51.2596
[09/26 07:37:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 61.00	
[09/26 07:37:01 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[09/26 07:37:07 visual_prompt]: Epoch 48 / 100: avg data time: 5.09e-02, avg batch time: 0.4628, average train loss: 70.7787
[09/26 07:37:08 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1582, average loss: 89.9330
[09/26 07:37:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 46.50	
[09/26 07:37:08 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[09/26 07:37:15 visual_prompt]: Epoch 49 / 100: avg data time: 6.08e-02, avg batch time: 0.4737, average train loss: 74.4948
[09/26 07:37:16 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.1583, average loss: 47.0226
[09/26 07:37:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 43.50	
[09/26 07:37:16 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[09/26 07:37:23 visual_prompt]: Epoch 50 / 100: avg data time: 6.08e-02, avg batch time: 0.4734, average train loss: 70.3439
[09/26 07:37:24 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1584, average loss: 49.8766
[09/26 07:37:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 5.50	top5: 43.50	
[09/26 07:37:25 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[09/26 07:37:31 visual_prompt]: Epoch 51 / 100: avg data time: 5.55e-02, avg batch time: 0.4677, average train loss: 53.3782
[09/26 07:37:32 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.1581, average loss: 30.8162
[09/26 07:37:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 58.00	
[09/26 07:37:32 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[09/26 07:37:39 visual_prompt]: Epoch 52 / 100: avg data time: 5.72e-02, avg batch time: 0.4696, average train loss: 54.0517
[09/26 07:37:40 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.1578, average loss: 68.1375
[09/26 07:37:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 60.00	
[09/26 07:37:40 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[09/26 07:37:47 visual_prompt]: Epoch 53 / 100: avg data time: 5.36e-02, avg batch time: 0.4667, average train loss: 55.2550
[09/26 07:37:48 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1580, average loss: 61.5623
[09/26 07:37:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 41.50	
[09/26 07:37:48 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[09/26 07:37:55 visual_prompt]: Epoch 54 / 100: avg data time: 4.81e-02, avg batch time: 0.4621, average train loss: 61.5255
[09/26 07:37:56 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1582, average loss: 66.5783
[09/26 07:37:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 39.50	
[09/26 07:37:56 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[09/26 07:38:03 visual_prompt]: Epoch 55 / 100: avg data time: 6.23e-02, avg batch time: 0.4742, average train loss: 60.6951
[09/26 07:38:04 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1584, average loss: 26.9524
[09/26 07:38:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 64.00	
[09/26 07:38:04 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[09/26 07:38:11 visual_prompt]: Epoch 56 / 100: avg data time: 5.61e-02, avg batch time: 0.4690, average train loss: 57.0690
[09/26 07:38:12 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1578, average loss: 61.4865
[09/26 07:38:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 38.00	
[09/26 07:38:12 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[09/26 07:38:19 visual_prompt]: Epoch 57 / 100: avg data time: 6.06e-02, avg batch time: 0.4719, average train loss: 49.7708
[09/26 07:38:20 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.1578, average loss: 37.4775
[09/26 07:38:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 07:38:20 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[09/26 07:38:27 visual_prompt]: Epoch 58 / 100: avg data time: 4.96e-02, avg batch time: 0.4619, average train loss: 55.2074
[09/26 07:38:28 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1575, average loss: 66.1961
[09/26 07:38:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 58.50	
[09/26 07:38:28 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[09/26 07:38:35 visual_prompt]: Epoch 59 / 100: avg data time: 5.18e-02, avg batch time: 0.4649, average train loss: 43.1714
[09/26 07:38:36 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1582, average loss: 30.2631
[09/26 07:38:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.50	top5: 46.50	
[09/26 07:38:36 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[09/26 07:38:42 visual_prompt]: Epoch 60 / 100: avg data time: 5.99e-02, avg batch time: 0.4710, average train loss: 42.9659
[09/26 07:38:44 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1577, average loss: 50.0866
[09/26 07:38:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.50	
[09/26 07:38:44 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[09/26 07:38:50 visual_prompt]: Epoch 61 / 100: avg data time: 6.14e-02, avg batch time: 0.4729, average train loss: 45.9991
[09/26 07:38:52 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1584, average loss: 49.1856
[09/26 07:38:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/26 07:38:52 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[09/26 07:38:58 visual_prompt]: Epoch 62 / 100: avg data time: 5.06e-02, avg batch time: 0.4644, average train loss: 44.9897
[09/26 07:39:00 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1583, average loss: 35.2736
[09/26 07:39:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 07:39:00 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[09/26 07:39:06 visual_prompt]: Epoch 63 / 100: avg data time: 4.52e-02, avg batch time: 0.4577, average train loss: 38.8924
[09/26 07:39:08 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1585, average loss: 34.0893
[09/26 07:39:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.00	
[09/26 07:39:08 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[09/26 07:39:14 visual_prompt]: Epoch 64 / 100: avg data time: 6.24e-02, avg batch time: 0.4763, average train loss: 30.3869
[09/26 07:39:16 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1584, average loss: 40.1173
[09/26 07:39:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 59.00	
[09/26 07:39:16 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[09/26 07:39:22 visual_prompt]: Epoch 65 / 100: avg data time: 5.40e-02, avg batch time: 0.4665, average train loss: 36.9657
[09/26 07:39:24 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1582, average loss: 32.1357
[09/26 07:39:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 41.50	
[09/26 07:39:24 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[09/26 07:39:30 visual_prompt]: Epoch 66 / 100: avg data time: 5.97e-02, avg batch time: 0.4714, average train loss: 34.5874
[09/26 07:39:32 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1579, average loss: 42.3074
[09/26 07:39:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 54.50	
[09/26 07:39:32 visual_prompt]: Training 67 / 100 epoch, with learning rate 7.817417582301099
[09/26 07:39:38 visual_prompt]: Epoch 67 / 100: avg data time: 5.92e-02, avg batch time: 0.4713, average train loss: 32.0297
[09/26 07:39:40 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1580, average loss: 39.3375
[09/26 07:39:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 54.50	
[09/26 07:39:40 visual_prompt]: Training 68 / 100 epoch, with learning rate 7.415791961552499
[09/26 07:39:46 visual_prompt]: Epoch 68 / 100: avg data time: 6.17e-02, avg batch time: 0.4738, average train loss: 34.7852
[09/26 07:39:48 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1582, average loss: 58.7821
[09/26 07:39:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 40.00	
[09/26 07:39:48 visual_prompt]: Training 69 / 100 epoch, with learning rate 7.020360665136531
[09/26 07:39:54 visual_prompt]: Epoch 69 / 100: avg data time: 4.89e-02, avg batch time: 0.4617, average train loss: 46.5584
[09/26 07:39:55 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1580, average loss: 19.0360
[09/26 07:39:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.00	
[09/26 07:39:55 visual_prompt]: Training 70 / 100 epoch, with learning rate 6.631605465176368
[09/26 07:40:02 visual_prompt]: Epoch 70 / 100: avg data time: 4.60e-02, avg batch time: 0.4597, average train loss: 26.7796
[09/26 07:40:03 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1579, average loss: 26.8993
[09/26 07:40:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 07:40:03 visual_prompt]: Training 71 / 100 epoch, with learning rate 6.250000000000003
[09/26 07:40:10 visual_prompt]: Epoch 71 / 100: avg data time: 6.00e-02, avg batch time: 0.4713, average train loss: 22.8185
[09/26 07:40:11 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1582, average loss: 37.0745
[09/26 07:40:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/26 07:40:11 visual_prompt]: Training 72 / 100 epoch, with learning rate 5.87600919708494
[09/26 07:40:18 visual_prompt]: Epoch 72 / 100: avg data time: 5.50e-02, avg batch time: 0.4666, average train loss: 26.6034
[09/26 07:40:19 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1578, average loss: 35.0347
[09/26 07:40:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 43.50	
[09/26 07:40:19 visual_prompt]: Training 73 / 100 epoch, with learning rate 5.510088706615666
[09/26 07:40:26 visual_prompt]: Epoch 73 / 100: avg data time: 6.14e-02, avg batch time: 0.4731, average train loss: 23.9937
[09/26 07:40:27 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1582, average loss: 16.8988
[09/26 07:40:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 07:40:27 visual_prompt]: Training 74 / 100 epoch, with learning rate 5.1526843463440875
[09/26 07:40:34 visual_prompt]: Epoch 74 / 100: avg data time: 5.70e-02, avg batch time: 0.4705, average train loss: 23.0988
[09/26 07:40:35 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1578, average loss: 22.6842
[09/26 07:40:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/26 07:40:35 visual_prompt]: Training 75 / 100 epoch, with learning rate 4.804231558429271
[09/26 07:40:41 visual_prompt]: Epoch 75 / 100: avg data time: 5.40e-02, avg batch time: 0.4656, average train loss: 14.1176
[09/26 07:40:43 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1579, average loss: 16.6921
[09/26 07:40:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.50	
[09/26 07:40:43 visual_prompt]: Training 76 / 100 epoch, with learning rate 4.465154878918258
[09/26 07:40:49 visual_prompt]: Epoch 76 / 100: avg data time: 6.19e-02, avg batch time: 0.4746, average train loss: 19.0520
[09/26 07:40:51 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.1578, average loss: 31.4917
[09/26 07:40:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 52.50	
[09/26 07:40:51 visual_prompt]: Training 77 / 100 epoch, with learning rate 4.135867420514276
[09/26 07:40:57 visual_prompt]: Epoch 77 / 100: avg data time: 5.36e-02, avg batch time: 0.4664, average train loss: 22.3765
[09/26 07:40:59 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1581, average loss: 20.8340
[09/26 07:40:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.50	
[09/26 07:40:59 visual_prompt]: Training 78 / 100 epoch, with learning rate 3.816770369262533
[09/26 07:41:05 visual_prompt]: Epoch 78 / 100: avg data time: 5.74e-02, avg batch time: 0.4690, average train loss: 21.1358
[09/26 07:41:07 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1582, average loss: 9.3110
[09/26 07:41:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.50	top5: 61.00	
[09/26 07:41:07 visual_prompt]: Training 79 / 100 epoch, with learning rate 3.508252495766863
[09/26 07:41:13 visual_prompt]: Epoch 79 / 100: avg data time: 5.81e-02, avg batch time: 0.4696, average train loss: 14.8657
[09/26 07:41:15 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1580, average loss: 15.3802
[09/26 07:41:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 42.50	
[09/26 07:41:15 visual_prompt]: Training 80 / 100 epoch, with learning rate 3.2106896815325707
[09/26 07:41:21 visual_prompt]: Epoch 80 / 100: avg data time: 5.28e-02, avg batch time: 0.4645, average train loss: 10.8823
[09/26 07:41:23 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1583, average loss: 10.4971
[09/26 07:41:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 07:41:23 visual_prompt]: Training 81 / 100 epoch, with learning rate 2.924444461012776
[09/26 07:41:29 visual_prompt]: Epoch 81 / 100: avg data time: 5.35e-02, avg batch time: 0.4665, average train loss: 10.2308
[09/26 07:41:31 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1578, average loss: 7.3893
[09/26 07:41:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 45.00	
[09/26 07:41:31 visual_prompt]: Training 82 / 100 epoch, with learning rate 2.6498655799159763
[09/26 07:41:37 visual_prompt]: Epoch 82 / 100: avg data time: 6.17e-02, avg batch time: 0.4737, average train loss: 7.0658
[09/26 07:41:39 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1581, average loss: 4.9246
[09/26 07:41:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 63.50	
[09/26 07:41:39 visual_prompt]: Training 83 / 100 epoch, with learning rate 2.387287570313158
[09/26 07:41:45 visual_prompt]: Epoch 83 / 100: avg data time: 6.00e-02, avg batch time: 0.4718, average train loss: 7.3318
[09/26 07:41:47 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1581, average loss: 8.8935
[09/26 07:41:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/26 07:41:47 visual_prompt]: Training 84 / 100 epoch, with learning rate 2.13703034306198
[09/26 07:41:53 visual_prompt]: Epoch 84 / 100: avg data time: 6.82e-02, avg batch time: 0.4798, average train loss: 7.3962
[09/26 07:41:55 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1582, average loss: 6.1209
[09/26 07:41:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.50	
[09/26 07:41:55 visual_prompt]: Training 85 / 100 epoch, with learning rate 1.8993987980446756
[09/26 07:42:01 visual_prompt]: Epoch 85 / 100: avg data time: 5.98e-02, avg batch time: 0.4724, average train loss: 4.9296
[09/26 07:42:03 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1585, average loss: 7.0846
[09/26 07:42:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 53.50	
[09/26 07:42:03 visual_prompt]: Training 86 / 100 epoch, with learning rate 1.674682452694516
[09/26 07:42:09 visual_prompt]: Epoch 86 / 100: avg data time: 5.83e-02, avg batch time: 0.4697, average train loss: 5.1737
[09/26 07:42:11 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1577, average loss: 4.8936
[09/26 07:42:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 07:42:11 visual_prompt]: Training 87 / 100 epoch, with learning rate 1.4631550892634126
[09/26 07:42:17 visual_prompt]: Epoch 87 / 100: avg data time: 5.69e-02, avg batch time: 0.4695, average train loss: 3.9719
[09/26 07:42:19 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1580, average loss: 2.7890
[09/26 07:42:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 57.00	
[09/26 07:42:19 visual_prompt]: Training 88 / 100 epoch, with learning rate 1.2650744212604148
[09/26 07:42:25 visual_prompt]: Epoch 88 / 100: avg data time: 5.86e-02, avg batch time: 0.4701, average train loss: 3.0944
[09/26 07:42:27 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1582, average loss: 3.6442
[09/26 07:42:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 43.00	
[09/26 07:42:27 visual_prompt]: Training 89 / 100 epoch, with learning rate 1.0806817794674877
[09/26 07:42:33 visual_prompt]: Epoch 89 / 100: avg data time: 6.21e-02, avg batch time: 0.4732, average train loss: 2.6304
[09/26 07:42:35 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1582, average loss: 2.2620
[09/26 07:42:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 07:42:35 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.9102018179151585
[09/26 07:42:41 visual_prompt]: Epoch 90 / 100: avg data time: 5.60e-02, avg batch time: 0.4685, average train loss: 2.4995
[09/26 07:42:43 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1582, average loss: 2.3597
[09/26 07:42:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:42:43 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.753842240176146
[09/26 07:42:49 visual_prompt]: Epoch 91 / 100: avg data time: 4.85e-02, avg batch time: 0.4603, average train loss: 2.3754
[09/26 07:42:50 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1580, average loss: 2.4371
[09/26 07:42:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 07:42:50 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.6117935463105808
[09/26 07:42:57 visual_prompt]: Epoch 92 / 100: avg data time: 4.67e-02, avg batch time: 0.4594, average train loss: 2.3603
[09/26 07:42:58 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1580, average loss: 2.3017
[09/26 07:42:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.50	top5: 64.00	
[09/26 07:42:58 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.4842288007710138
[09/26 07:43:05 visual_prompt]: Epoch 93 / 100: avg data time: 6.46e-02, avg batch time: 0.4782, average train loss: 2.2923
[09/26 07:43:06 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.1577, average loss: 2.2482
[09/26 07:43:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 07:43:06 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.3713034215500441
[09/26 07:43:13 visual_prompt]: Epoch 94 / 100: avg data time: 5.89e-02, avg batch time: 0.4719, average train loss: 2.2669
[09/26 07:43:14 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1583, average loss: 2.2309
[09/26 07:43:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:43:14 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.2731549908274289
[09/26 07:43:21 visual_prompt]: Epoch 95 / 100: avg data time: 5.36e-02, avg batch time: 0.4663, average train loss: 2.2539
[09/26 07:43:22 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1585, average loss: 2.2342
[09/26 07:43:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 07:43:22 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.18990308734739975
[09/26 07:43:29 visual_prompt]: Epoch 96 / 100: avg data time: 6.01e-02, avg batch time: 0.4731, average train loss: 2.2605
[09/26 07:43:30 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1584, average loss: 2.2187
[09/26 07:43:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 07:43:30 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.12164914073037048
[09/26 07:43:37 visual_prompt]: Epoch 97 / 100: avg data time: 6.72e-02, avg batch time: 0.4794, average train loss: 2.2408
[09/26 07:43:38 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1581, average loss: 2.2132
[09/26 07:43:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:43:38 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.06847630789658388
[09/26 07:43:45 visual_prompt]: Epoch 98 / 100: avg data time: 5.05e-02, avg batch time: 0.4626, average train loss: 2.2352
[09/26 07:43:46 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1585, average loss: 2.2192
[09/26 07:43:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:43:46 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.03044937175219753
[09/26 07:43:52 visual_prompt]: Epoch 99 / 100: avg data time: 4.25e-02, avg batch time: 0.4568, average train loss: 2.2350
[09/26 07:43:54 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1583, average loss: 2.2191
[09/26 07:43:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:43:54 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0076146622613029735
[09/26 07:44:00 visual_prompt]: Epoch 100 / 100: avg data time: 4.91e-02, avg batch time: 0.4613, average train loss: 2.2312
[09/26 07:44:02 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1583, average loss: 2.2178
[09/26 07:44:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:44:02 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 07:44:02 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 07:44:02 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 07:44:02 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 07:44:02 visual_prompt]: Training with config:
[09/26 07:44:02 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr25.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 07:44:02 visual_prompt]: Loading training data...
[09/26 07:44:02 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 07:44:03 visual_prompt]: Number of images: 800
[09/26 07:44:03 visual_prompt]: Number of classes: 10 / 10
[09/26 07:44:03 visual_prompt]: Loading validation data...
[09/26 07:44:03 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 07:44:03 visual_prompt]: Number of images: 200
[09/26 07:44:03 visual_prompt]: Number of classes: 10 / 10
[09/26 07:44:03 visual_prompt]: Constructing models...
[09/26 07:44:05 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 07:44:05 visual_prompt]: tuned percent:0.543
[09/26 07:44:05 visual_prompt]: Device used for model: 0
[09/26 07:44:05 visual_prompt]: Setting up Evaluator...
[09/26 07:44:05 visual_prompt]: Setting up Trainer...
[09/26 07:44:05 visual_prompt]: 	Setting up the optimizer...
[09/26 07:44:05 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 07:44:12 visual_prompt]: Epoch 1 / 100: avg data time: 4.93e-02, avg batch time: 0.4690, average train loss: 2.6780
[09/26 07:44:13 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1580, average loss: 2.6214
[09/26 07:44:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 07:44:13 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 07:44:13 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[09/26 07:44:20 visual_prompt]: Epoch 2 / 100: avg data time: 4.57e-02, avg batch time: 0.4610, average train loss: 23.8058
[09/26 07:44:21 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1583, average loss: 14.1583
[09/26 07:44:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 44.00	
[09/26 07:44:21 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[09/26 07:44:28 visual_prompt]: Epoch 3 / 100: avg data time: 5.49e-02, avg batch time: 0.4682, average train loss: 17.8306
[09/26 07:44:29 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1586, average loss: 25.8099
[09/26 07:44:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 41.50	
[09/26 07:44:29 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[09/26 07:44:35 visual_prompt]: Epoch 4 / 100: avg data time: 4.56e-02, avg batch time: 0.4560, average train loss: 34.0561
[09/26 07:44:37 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1576, average loss: 20.1141
[09/26 07:44:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 63.50	
[09/26 07:44:37 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[09/26 07:44:43 visual_prompt]: Epoch 5 / 100: avg data time: 5.50e-02, avg batch time: 0.4667, average train loss: 40.2081
[09/26 07:44:45 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1579, average loss: 31.7725
[09/26 07:44:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 52.50	
[09/26 07:44:45 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[09/26 07:44:51 visual_prompt]: Epoch 6 / 100: avg data time: 5.82e-02, avg batch time: 0.4699, average train loss: 57.2899
[09/26 07:44:53 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1576, average loss: 44.3994
[09/26 07:44:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 59.50	
[09/26 07:44:53 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[09/26 07:44:59 visual_prompt]: Epoch 7 / 100: avg data time: 5.75e-02, avg batch time: 0.4685, average train loss: 59.5499
[09/26 07:45:01 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1586, average loss: 89.5177
[09/26 07:45:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 40.00	
[09/26 07:45:01 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[09/26 07:45:07 visual_prompt]: Epoch 8 / 100: avg data time: 5.52e-02, avg batch time: 0.4682, average train loss: 63.2810
[09/26 07:45:09 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1580, average loss: 83.9599
[09/26 07:45:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 41.50	
[09/26 07:45:09 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[09/26 07:45:15 visual_prompt]: Epoch 9 / 100: avg data time: 6.10e-02, avg batch time: 0.4736, average train loss: 98.0184
[09/26 07:45:17 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1580, average loss: 50.6336
[09/26 07:45:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 43.50	
[09/26 07:45:17 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[09/26 07:45:23 visual_prompt]: Epoch 10 / 100: avg data time: 5.96e-02, avg batch time: 0.4721, average train loss: 115.2493
[09/26 07:45:25 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1578, average loss: 106.3541
[09/26 07:45:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/26 07:45:25 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[09/26 07:45:31 visual_prompt]: Epoch 11 / 100: avg data time: 6.42e-02, avg batch time: 0.4758, average train loss: 118.6999
[09/26 07:45:33 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1580, average loss: 109.6918
[09/26 07:45:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 54.00	
[09/26 07:45:33 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[09/26 07:45:39 visual_prompt]: Epoch 12 / 100: avg data time: 5.77e-02, avg batch time: 0.4703, average train loss: 128.5369
[09/26 07:45:41 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.1576, average loss: 108.2442
[09/26 07:45:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 45.50	
[09/26 07:45:41 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[09/26 07:45:47 visual_prompt]: Epoch 13 / 100: avg data time: 5.41e-02, avg batch time: 0.4652, average train loss: 137.7608
[09/26 07:45:49 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1582, average loss: 78.9307
[09/26 07:45:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 07:45:49 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[09/26 07:45:55 visual_prompt]: Epoch 14 / 100: avg data time: 6.21e-02, avg batch time: 0.4740, average train loss: 114.5980
[09/26 07:45:57 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1582, average loss: 112.2000
[09/26 07:45:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 07:45:57 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[09/26 07:46:03 visual_prompt]: Epoch 15 / 100: avg data time: 6.36e-02, avg batch time: 0.4762, average train loss: 110.4917
[09/26 07:46:05 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.1581, average loss: 114.1884
[09/26 07:46:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 42.50	
[09/26 07:46:05 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[09/26 07:46:11 visual_prompt]: Epoch 16 / 100: avg data time: 5.38e-02, avg batch time: 0.4661, average train loss: 161.4240
[09/26 07:46:12 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1584, average loss: 139.0879
[09/26 07:46:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.50	
[09/26 07:46:12 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[09/26 07:46:19 visual_prompt]: Epoch 17 / 100: avg data time: 5.69e-02, avg batch time: 0.4701, average train loss: 130.3425
[09/26 07:46:20 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1579, average loss: 168.6745
[09/26 07:46:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 43.00	
[09/26 07:46:20 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[09/26 07:46:27 visual_prompt]: Epoch 18 / 100: avg data time: 4.73e-02, avg batch time: 0.4585, average train loss: 120.1346
[09/26 07:46:28 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1582, average loss: 71.5325
[09/26 07:46:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 61.50	
[09/26 07:46:28 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[09/26 07:46:35 visual_prompt]: Epoch 19 / 100: avg data time: 5.33e-02, avg batch time: 0.4655, average train loss: 104.0166
[09/26 07:46:36 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1572, average loss: 58.7983
[09/26 07:46:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 07:46:36 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[09/26 07:46:43 visual_prompt]: Epoch 20 / 100: avg data time: 5.80e-02, avg batch time: 0.4700, average train loss: 100.3291
[09/26 07:46:44 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.1582, average loss: 185.4103
[09/26 07:46:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 43.50	
[09/26 07:46:44 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[09/26 07:46:51 visual_prompt]: Epoch 21 / 100: avg data time: 6.27e-02, avg batch time: 0.4745, average train loss: 165.9663
[09/26 07:46:52 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1581, average loss: 108.9629
[09/26 07:46:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/26 07:46:52 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[09/26 07:46:59 visual_prompt]: Epoch 22 / 100: avg data time: 6.21e-02, avg batch time: 0.4738, average train loss: 143.8906
[09/26 07:47:00 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1589, average loss: 172.4667
[09/26 07:47:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.50	
[09/26 07:47:00 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[09/26 07:47:07 visual_prompt]: Epoch 23 / 100: avg data time: 4.93e-02, avg batch time: 0.4620, average train loss: 106.4574
[09/26 07:47:08 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1581, average loss: 114.8075
[09/26 07:47:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 07:47:08 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[09/26 07:47:15 visual_prompt]: Epoch 24 / 100: avg data time: 5.99e-02, avg batch time: 0.4733, average train loss: 90.6558
[09/26 07:47:16 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1583, average loss: 68.8136
[09/26 07:47:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 45.50	
[09/26 07:47:16 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[09/26 07:47:22 visual_prompt]: Epoch 25 / 100: avg data time: 4.83e-02, avg batch time: 0.4621, average train loss: 123.7882
[09/26 07:47:24 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1583, average loss: 61.0856
[09/26 07:47:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 53.50	
[09/26 07:47:24 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[09/26 07:47:30 visual_prompt]: Epoch 26 / 100: avg data time: 5.83e-02, avg batch time: 0.4698, average train loss: 139.6717
[09/26 07:47:32 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1579, average loss: 66.2753
[09/26 07:47:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 07:47:32 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[09/26 07:47:38 visual_prompt]: Epoch 27 / 100: avg data time: 5.96e-02, avg batch time: 0.4718, average train loss: 123.9401
[09/26 07:47:40 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1581, average loss: 95.6046
[09/26 07:47:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 43.00	
[09/26 07:47:40 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[09/26 07:47:46 visual_prompt]: Epoch 28 / 100: avg data time: 5.78e-02, avg batch time: 0.4706, average train loss: 163.6540
[09/26 07:47:48 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1576, average loss: 101.7603
[09/26 07:47:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 07:47:48 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[09/26 07:47:54 visual_prompt]: Epoch 29 / 100: avg data time: 5.67e-02, avg batch time: 0.4696, average train loss: 104.0992
[09/26 07:47:56 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1582, average loss: 125.3059
[09/26 07:47:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 41.50	
[09/26 07:47:56 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[09/26 07:48:02 visual_prompt]: Epoch 30 / 100: avg data time: 6.10e-02, avg batch time: 0.4746, average train loss: 124.6589
[09/26 07:48:04 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1579, average loss: 99.0052
[09/26 07:48:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 64.50	
[09/26 07:48:04 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[09/26 07:48:10 visual_prompt]: Epoch 31 / 100: avg data time: 6.11e-02, avg batch time: 0.4733, average train loss: 102.0676
[09/26 07:48:12 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.1584, average loss: 117.0139
[09/26 07:48:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 57.50	
[09/26 07:48:12 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[09/26 07:48:18 visual_prompt]: Epoch 32 / 100: avg data time: 5.17e-02, avg batch time: 0.4646, average train loss: 104.5129
[09/26 07:48:20 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.1580, average loss: 107.5560
[09/26 07:48:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 55.00	
[09/26 07:48:20 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[09/26 07:48:26 visual_prompt]: Epoch 33 / 100: avg data time: 5.98e-02, avg batch time: 0.4725, average train loss: 128.8237
[09/26 07:48:28 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1582, average loss: 106.5278
[09/26 07:48:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 49.50	
[09/26 07:48:28 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[09/26 07:48:34 visual_prompt]: Epoch 34 / 100: avg data time: 5.89e-02, avg batch time: 0.4732, average train loss: 109.2924
[09/26 07:48:36 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1585, average loss: 140.6323
[09/26 07:48:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 56.00	
[09/26 07:48:36 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[09/26 07:48:42 visual_prompt]: Epoch 35 / 100: avg data time: 4.96e-02, avg batch time: 0.4645, average train loss: 110.5729
[09/26 07:48:44 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1581, average loss: 103.2343
[09/26 07:48:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 38.00	
[09/26 07:48:44 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[09/26 07:48:50 visual_prompt]: Epoch 36 / 100: avg data time: 4.72e-02, avg batch time: 0.4599, average train loss: 109.5438
[09/26 07:48:52 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1584, average loss: 81.6685
[09/26 07:48:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 07:48:52 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[09/26 07:48:58 visual_prompt]: Epoch 37 / 100: avg data time: 5.05e-02, avg batch time: 0.4644, average train loss: 81.4522
[09/26 07:49:00 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1582, average loss: 127.8598
[09/26 07:49:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 52.00	
[09/26 07:49:00 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[09/26 07:49:06 visual_prompt]: Epoch 38 / 100: avg data time: 5.89e-02, avg batch time: 0.4712, average train loss: 115.9480
[09/26 07:49:08 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1579, average loss: 104.3807
[09/26 07:49:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 53.00	
[09/26 07:49:08 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[09/26 07:49:14 visual_prompt]: Epoch 39 / 100: avg data time: 5.68e-02, avg batch time: 0.4698, average train loss: 101.9930
[09/26 07:49:16 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1581, average loss: 91.9776
[09/26 07:49:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 45.00	
[09/26 07:49:16 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[09/26 07:49:22 visual_prompt]: Epoch 40 / 100: avg data time: 5.73e-02, avg batch time: 0.4693, average train loss: 84.8508
[09/26 07:49:23 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1584, average loss: 57.5753
[09/26 07:49:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/26 07:49:23 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[09/26 07:49:30 visual_prompt]: Epoch 41 / 100: avg data time: 5.92e-02, avg batch time: 0.4712, average train loss: 76.5783
[09/26 07:49:31 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1582, average loss: 141.8202
[09/26 07:49:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 42.00	
[09/26 07:49:31 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[09/26 07:49:38 visual_prompt]: Epoch 42 / 100: avg data time: 5.78e-02, avg batch time: 0.4710, average train loss: 88.3337
[09/26 07:49:39 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1583, average loss: 103.0876
[09/26 07:49:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 41.50	
[09/26 07:49:39 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[09/26 07:49:46 visual_prompt]: Epoch 43 / 100: avg data time: 5.10e-02, avg batch time: 0.4648, average train loss: 85.3547
[09/26 07:49:47 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1580, average loss: 40.9546
[09/26 07:49:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 60.00	
[09/26 07:49:47 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[09/26 07:49:54 visual_prompt]: Epoch 44 / 100: avg data time: 6.18e-02, avg batch time: 0.4744, average train loss: 51.6851
[09/26 07:49:55 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1580, average loss: 106.3501
[09/26 07:49:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.50	top5: 62.50	
[09/26 07:49:55 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[09/26 07:50:02 visual_prompt]: Epoch 45 / 100: avg data time: 5.78e-02, avg batch time: 0.4701, average train loss: 110.8147
[09/26 07:50:03 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1581, average loss: 143.7670
[09/26 07:50:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 14.50	top5: 63.00	
[09/26 07:50:03 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[09/26 07:50:10 visual_prompt]: Epoch 46 / 100: avg data time: 6.33e-02, avg batch time: 0.4737, average train loss: 86.5823
[09/26 07:50:11 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1583, average loss: 92.0054
[09/26 07:50:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 07:50:11 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[09/26 07:50:18 visual_prompt]: Epoch 47 / 100: avg data time: 4.89e-02, avg batch time: 0.4608, average train loss: 82.6447
[09/26 07:50:19 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1585, average loss: 61.5563
[09/26 07:50:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 07:50:19 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[09/26 07:50:25 visual_prompt]: Epoch 48 / 100: avg data time: 5.60e-02, avg batch time: 0.4671, average train loss: 70.2194
[09/26 07:50:27 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1581, average loss: 78.2200
[09/26 07:50:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 61.50	
[09/26 07:50:27 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[09/26 07:50:33 visual_prompt]: Epoch 49 / 100: avg data time: 5.61e-02, avg batch time: 0.4685, average train loss: 62.2225
[09/26 07:50:35 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1579, average loss: 58.1434
[09/26 07:50:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 07:50:35 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[09/26 07:50:41 visual_prompt]: Epoch 50 / 100: avg data time: 6.01e-02, avg batch time: 0.4722, average train loss: 77.8979
[09/26 07:50:43 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.1580, average loss: 71.1816
[09/26 07:50:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.50	
[09/26 07:50:43 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[09/26 07:50:49 visual_prompt]: Epoch 51 / 100: avg data time: 6.32e-02, avg batch time: 0.4749, average train loss: 94.7688
[09/26 07:50:51 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1574, average loss: 90.9821
[09/26 07:50:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 59.00	
[09/26 07:50:51 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[09/26 07:50:57 visual_prompt]: Epoch 52 / 100: avg data time: 4.70e-02, avg batch time: 0.4603, average train loss: 70.3482
[09/26 07:50:59 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1582, average loss: 29.1967
[09/26 07:50:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.00	
[09/26 07:50:59 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[09/26 07:51:05 visual_prompt]: Epoch 53 / 100: avg data time: 6.35e-02, avg batch time: 0.4760, average train loss: 58.9332
[09/26 07:51:07 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1584, average loss: 77.7549
[09/26 07:51:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 44.50	
[09/26 07:51:07 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[09/26 07:51:13 visual_prompt]: Epoch 54 / 100: avg data time: 5.62e-02, avg batch time: 0.4676, average train loss: 56.8827
[09/26 07:51:15 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1586, average loss: 49.9297
[09/26 07:51:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 62.50	
[09/26 07:51:15 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[09/26 07:51:21 visual_prompt]: Epoch 55 / 100: avg data time: 5.40e-02, avg batch time: 0.4659, average train loss: 59.1044
[09/26 07:51:23 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1580, average loss: 76.1061
[09/26 07:51:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 59.50	
[09/26 07:51:23 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[09/26 07:51:29 visual_prompt]: Epoch 56 / 100: avg data time: 5.38e-02, avg batch time: 0.4656, average train loss: 57.4793
[09/26 07:51:31 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1582, average loss: 62.3255
[09/26 07:51:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.00	
[09/26 07:51:31 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[09/26 07:51:37 visual_prompt]: Epoch 57 / 100: avg data time: 5.88e-02, avg batch time: 0.4715, average train loss: 71.1154
[09/26 07:51:39 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1580, average loss: 111.6769
[09/26 07:51:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 54.50	
[09/26 07:51:39 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[09/26 07:51:45 visual_prompt]: Epoch 58 / 100: avg data time: 6.11e-02, avg batch time: 0.4740, average train loss: 72.5828
[09/26 07:51:47 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1580, average loss: 90.1852
[09/26 07:51:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 07:51:47 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[09/26 07:51:53 visual_prompt]: Epoch 59 / 100: avg data time: 5.92e-02, avg batch time: 0.4714, average train loss: 59.9706
[09/26 07:51:55 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1582, average loss: 48.8450
[09/26 07:51:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 07:51:55 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[09/26 07:52:01 visual_prompt]: Epoch 60 / 100: avg data time: 6.14e-02, avg batch time: 0.4734, average train loss: 47.0651
[09/26 07:52:03 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1581, average loss: 56.2778
[09/26 07:52:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 07:52:03 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[09/26 07:52:09 visual_prompt]: Epoch 61 / 100: avg data time: 5.19e-02, avg batch time: 0.4632, average train loss: 40.3367
[09/26 07:52:11 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1581, average loss: 50.5350
[09/26 07:52:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.00	
[09/26 07:52:11 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[09/26 07:52:17 visual_prompt]: Epoch 62 / 100: avg data time: 5.21e-02, avg batch time: 0.4653, average train loss: 55.0143
[09/26 07:52:18 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1581, average loss: 73.8165
[09/26 07:52:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 44.00	
[09/26 07:52:18 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[09/26 07:52:25 visual_prompt]: Epoch 63 / 100: avg data time: 5.35e-02, avg batch time: 0.4665, average train loss: 52.7028
[09/26 07:52:26 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1583, average loss: 35.6649
[09/26 07:52:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 43.50	
[09/26 07:52:26 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[09/26 07:52:33 visual_prompt]: Epoch 64 / 100: avg data time: 6.50e-02, avg batch time: 0.4771, average train loss: 45.8706
[09/26 07:52:34 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1587, average loss: 43.1206
[09/26 07:52:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 42.50	
[09/26 07:52:34 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[09/26 07:52:41 visual_prompt]: Epoch 65 / 100: avg data time: 5.32e-02, avg batch time: 0.4661, average train loss: 52.3290
[09/26 07:52:42 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1576, average loss: 93.3332
[09/26 07:52:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.00	
[09/26 07:52:42 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[09/26 07:52:49 visual_prompt]: Epoch 66 / 100: avg data time: 6.05e-02, avg batch time: 0.4724, average train loss: 45.3414
[09/26 07:52:50 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1586, average loss: 23.2439
[09/26 07:52:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 07:52:50 visual_prompt]: Training 67 / 100 epoch, with learning rate 7.817417582301099
[09/26 07:52:57 visual_prompt]: Epoch 67 / 100: avg data time: 5.71e-02, avg batch time: 0.4697, average train loss: 28.8109
[09/26 07:52:58 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.1581, average loss: 27.6757
[09/26 07:52:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 43.50	
[09/26 07:52:58 visual_prompt]: Training 68 / 100 epoch, with learning rate 7.415791961552499
[09/26 07:53:05 visual_prompt]: Epoch 68 / 100: avg data time: 5.27e-02, avg batch time: 0.4654, average train loss: 30.1375
[09/26 07:53:06 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1582, average loss: 30.3278
[09/26 07:53:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/26 07:53:06 visual_prompt]: Training 69 / 100 epoch, with learning rate 7.020360665136531
[09/26 07:53:12 visual_prompt]: Epoch 69 / 100: avg data time: 4.95e-02, avg batch time: 0.4628, average train loss: 30.6610
[09/26 07:53:14 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1582, average loss: 42.0237
[09/26 07:53:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/26 07:53:14 visual_prompt]: Training 70 / 100 epoch, with learning rate 6.631605465176368
[09/26 07:53:20 visual_prompt]: Epoch 70 / 100: avg data time: 5.99e-02, avg batch time: 0.4725, average train loss: 32.5766
[09/26 07:53:22 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1578, average loss: 27.8832
[09/26 07:53:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 55.50	
[09/26 07:53:22 visual_prompt]: Training 71 / 100 epoch, with learning rate 6.250000000000003
[09/26 07:53:28 visual_prompt]: Epoch 71 / 100: avg data time: 5.44e-02, avg batch time: 0.4677, average train loss: 24.5638
[09/26 07:53:30 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1577, average loss: 26.0884
[09/26 07:53:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 07:53:30 visual_prompt]: Training 72 / 100 epoch, with learning rate 5.87600919708494
[09/26 07:53:36 visual_prompt]: Epoch 72 / 100: avg data time: 6.29e-02, avg batch time: 0.4757, average train loss: 31.9174
[09/26 07:53:38 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.1581, average loss: 28.1933
[09/26 07:53:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 37.00	
[09/26 07:53:38 visual_prompt]: Training 73 / 100 epoch, with learning rate 5.510088706615666
[09/26 07:53:44 visual_prompt]: Epoch 73 / 100: avg data time: 4.89e-02, avg batch time: 0.4623, average train loss: 30.4030
[09/26 07:53:46 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1583, average loss: 24.8385
[09/26 07:53:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 07:53:46 visual_prompt]: Training 74 / 100 epoch, with learning rate 5.1526843463440875
[09/26 07:53:52 visual_prompt]: Epoch 74 / 100: avg data time: 6.03e-02, avg batch time: 0.4734, average train loss: 27.7615
[09/26 07:53:54 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1582, average loss: 19.7679
[09/26 07:53:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 42.50	
[09/26 07:53:54 visual_prompt]: Training 75 / 100 epoch, with learning rate 4.804231558429271
[09/26 07:54:00 visual_prompt]: Epoch 75 / 100: avg data time: 5.36e-02, avg batch time: 0.4683, average train loss: 22.2909
[09/26 07:54:02 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1584, average loss: 21.7931
[09/26 07:54:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.00	
[09/26 07:54:02 visual_prompt]: Training 76 / 100 epoch, with learning rate 4.465154878918258
[09/26 07:54:08 visual_prompt]: Epoch 76 / 100: avg data time: 4.44e-02, avg batch time: 0.4564, average train loss: 21.7473
[09/26 07:54:10 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 15.0548
[09/26 07:54:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.50	
[09/26 07:54:10 visual_prompt]: Training 77 / 100 epoch, with learning rate 4.135867420514276
[09/26 07:54:16 visual_prompt]: Epoch 77 / 100: avg data time: 6.07e-02, avg batch time: 0.4747, average train loss: 15.8638
[09/26 07:54:18 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1582, average loss: 19.4986
[09/26 07:54:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 40.50	
[09/26 07:54:18 visual_prompt]: Training 78 / 100 epoch, with learning rate 3.816770369262533
[09/26 07:54:24 visual_prompt]: Epoch 78 / 100: avg data time: 5.60e-02, avg batch time: 0.4684, average train loss: 14.3935
[09/26 07:54:26 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1582, average loss: 10.0238
[09/26 07:54:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.50	
[09/26 07:54:26 visual_prompt]: Training 79 / 100 epoch, with learning rate 3.508252495766863
[09/26 07:54:32 visual_prompt]: Epoch 79 / 100: avg data time: 6.39e-02, avg batch time: 0.4765, average train loss: 9.0487
[09/26 07:54:34 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1587, average loss: 11.4847
[09/26 07:54:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 58.50	
[09/26 07:54:34 visual_prompt]: Training 80 / 100 epoch, with learning rate 3.2106896815325707
[09/26 07:54:40 visual_prompt]: Epoch 80 / 100: avg data time: 5.84e-02, avg batch time: 0.4701, average train loss: 8.2808
[09/26 07:54:42 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.1582, average loss: 6.9634
[09/26 07:54:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.50	
[09/26 07:54:42 visual_prompt]: Training 81 / 100 epoch, with learning rate 2.924444461012776
[09/26 07:54:48 visual_prompt]: Epoch 81 / 100: avg data time: 6.14e-02, avg batch time: 0.4731, average train loss: 5.8182
[09/26 07:54:50 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1584, average loss: 3.6272
[09/26 07:54:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.00	
[09/26 07:54:50 visual_prompt]: Training 82 / 100 epoch, with learning rate 2.6498655799159763
[09/26 07:54:56 visual_prompt]: Epoch 82 / 100: avg data time: 5.28e-02, avg batch time: 0.4651, average train loss: 2.8870
[09/26 07:54:58 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1581, average loss: 2.7271
[09/26 07:54:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 41.00	
[09/26 07:54:58 visual_prompt]: Training 83 / 100 epoch, with learning rate 2.387287570313158
[09/26 07:55:04 visual_prompt]: Epoch 83 / 100: avg data time: 6.26e-02, avg batch time: 0.4757, average train loss: 2.6426
[09/26 07:55:06 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1580, average loss: 2.5940
[09/26 07:55:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 07:55:06 visual_prompt]: Training 84 / 100 epoch, with learning rate 2.13703034306198
[09/26 07:55:12 visual_prompt]: Epoch 84 / 100: avg data time: 5.43e-02, avg batch time: 0.4664, average train loss: 2.5799
[09/26 07:55:14 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1581, average loss: 3.4869
[09/26 07:55:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 59.00	
[09/26 07:55:14 visual_prompt]: Training 85 / 100 epoch, with learning rate 1.8993987980446756
[09/26 07:55:20 visual_prompt]: Epoch 85 / 100: avg data time: 5.38e-02, avg batch time: 0.4661, average train loss: 2.8155
[09/26 07:55:21 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1582, average loss: 2.6228
[09/26 07:55:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 56.00	
[09/26 07:55:21 visual_prompt]: Training 86 / 100 epoch, with learning rate 1.674682452694516
[09/26 07:55:28 visual_prompt]: Epoch 86 / 100: avg data time: 5.33e-02, avg batch time: 0.4657, average train loss: 2.5562
[09/26 07:55:29 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1586, average loss: 2.4296
[09/26 07:55:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 62.00	
[09/26 07:55:29 visual_prompt]: Training 87 / 100 epoch, with learning rate 1.4631550892634126
[09/26 07:55:36 visual_prompt]: Epoch 87 / 100: avg data time: 6.25e-02, avg batch time: 0.4746, average train loss: 2.4508
[09/26 07:55:37 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1582, average loss: 2.3824
[09/26 07:55:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:55:37 visual_prompt]: Training 88 / 100 epoch, with learning rate 1.2650744212604148
[09/26 07:55:44 visual_prompt]: Epoch 88 / 100: avg data time: 6.25e-02, avg batch time: 0.4736, average train loss: 2.4027
[09/26 07:55:45 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1582, average loss: 2.3759
[09/26 07:55:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 07:55:45 visual_prompt]: Training 89 / 100 epoch, with learning rate 1.0806817794674877
[09/26 07:55:52 visual_prompt]: Epoch 89 / 100: avg data time: 4.19e-02, avg batch time: 0.4546, average train loss: 2.4293
[09/26 07:55:53 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1582, average loss: 2.3825
[09/26 07:55:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 50.00	
[09/26 07:55:53 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.9102018179151585
[09/26 07:55:59 visual_prompt]: Epoch 90 / 100: avg data time: 5.95e-02, avg batch time: 0.4702, average train loss: 2.3234
[09/26 07:56:01 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1582, average loss: 2.3335
[09/26 07:56:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.50	
[09/26 07:56:01 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.753842240176146
[09/26 07:56:07 visual_prompt]: Epoch 91 / 100: avg data time: 6.04e-02, avg batch time: 0.4714, average train loss: 2.3156
[09/26 07:56:09 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1585, average loss: 2.2591
[09/26 07:56:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:56:09 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.6117935463105808
[09/26 07:56:15 visual_prompt]: Epoch 92 / 100: avg data time: 6.01e-02, avg batch time: 0.4713, average train loss: 2.2822
[09/26 07:56:17 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1580, average loss: 2.2274
[09/26 07:56:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 07:56:17 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.4842288007710138
[09/26 07:56:23 visual_prompt]: Epoch 93 / 100: avg data time: 4.42e-02, avg batch time: 0.4575, average train loss: 2.2583
[09/26 07:56:25 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.1580, average loss: 2.2913
[09/26 07:56:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.50	
[09/26 07:56:25 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.3713034215500441
[09/26 07:56:31 visual_prompt]: Epoch 94 / 100: avg data time: 6.39e-02, avg batch time: 0.4765, average train loss: 2.2778
[09/26 07:56:33 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1578, average loss: 2.2404
[09/26 07:56:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 07:56:33 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.2731549908274289
[09/26 07:56:39 visual_prompt]: Epoch 95 / 100: avg data time: 5.49e-02, avg batch time: 0.4678, average train loss: 2.2450
[09/26 07:56:41 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1579, average loss: 2.2353
[09/26 07:56:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:56:41 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.18990308734739975
[09/26 07:56:47 visual_prompt]: Epoch 96 / 100: avg data time: 6.36e-02, avg batch time: 0.4749, average train loss: 2.2488
[09/26 07:56:49 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1578, average loss: 2.2131
[09/26 07:56:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.50	
[09/26 07:56:49 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.12164914073037048
[09/26 07:56:55 visual_prompt]: Epoch 97 / 100: avg data time: 5.81e-02, avg batch time: 0.4704, average train loss: 2.2444
[09/26 07:56:57 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1583, average loss: 2.2172
[09/26 07:56:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:56:57 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.06847630789658388
[09/26 07:57:03 visual_prompt]: Epoch 98 / 100: avg data time: 5.26e-02, avg batch time: 0.4647, average train loss: 2.2315
[09/26 07:57:05 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1579, average loss: 2.2179
[09/26 07:57:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:57:05 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.03044937175219753
[09/26 07:57:11 visual_prompt]: Epoch 99 / 100: avg data time: 5.53e-02, avg batch time: 0.4680, average train loss: 2.2314
[09/26 07:57:13 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.1573, average loss: 2.2173
[09/26 07:57:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 07:57:13 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0076146622613029735
[09/26 07:57:19 visual_prompt]: Epoch 100 / 100: avg data time: 5.54e-02, avg batch time: 0.4669, average train loss: 2.2301
[09/26 07:57:21 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.1582, average loss: 2.2164
[09/26 07:57:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 07:57:21 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 07:57:21 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 07:57:21 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 07:57:21 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 07:57:21 visual_prompt]: Training with config:
[09/26 07:57:21 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr25.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 07:57:21 visual_prompt]: Loading training data...
[09/26 07:57:21 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 07:57:22 visual_prompt]: Number of images: 800
[09/26 07:57:22 visual_prompt]: Number of classes: 10 / 10
[09/26 07:57:22 visual_prompt]: Loading validation data...
[09/26 07:57:22 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 07:57:23 visual_prompt]: Number of images: 200
[09/26 07:57:23 visual_prompt]: Number of classes: 10 / 10
[09/26 07:57:23 visual_prompt]: Constructing models...
[09/26 07:57:25 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 07:57:25 visual_prompt]: tuned percent:0.543
[09/26 07:57:25 visual_prompt]: Device used for model: 0
[09/26 07:57:25 visual_prompt]: Setting up Evaluator...
[09/26 07:57:25 visual_prompt]: Setting up Trainer...
[09/26 07:57:25 visual_prompt]: 	Setting up the optimizer...
[09/26 07:57:25 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 07:57:31 visual_prompt]: Epoch 1 / 100: avg data time: 4.62e-02, avg batch time: 0.4630, average train loss: 2.6798
[09/26 07:57:33 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1582, average loss: 2.6214
[09/26 07:57:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 07:57:33 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 07:57:33 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[09/26 07:57:39 visual_prompt]: Epoch 2 / 100: avg data time: 4.61e-02, avg batch time: 0.4598, average train loss: 21.8691
[09/26 07:57:41 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1582, average loss: 21.0116
[09/26 07:57:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 41.50	
[09/26 07:57:41 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[09/26 07:57:47 visual_prompt]: Epoch 3 / 100: avg data time: 5.43e-02, avg batch time: 0.4649, average train loss: 19.8318
[09/26 07:57:49 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1575, average loss: 20.6358
[09/26 07:57:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.00	
[09/26 07:57:49 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[09/26 07:57:55 visual_prompt]: Epoch 4 / 100: avg data time: 5.95e-02, avg batch time: 0.4703, average train loss: 22.4606
[09/26 07:57:57 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1582, average loss: 21.1657
[09/26 07:57:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 46.00	
[09/26 07:57:57 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[09/26 07:58:03 visual_prompt]: Epoch 5 / 100: avg data time: 5.60e-02, avg batch time: 0.4674, average train loss: 27.5248
[09/26 07:58:05 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1581, average loss: 28.9763
[09/26 07:58:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 57.50	
[09/26 07:58:05 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[09/26 07:58:11 visual_prompt]: Epoch 6 / 100: avg data time: 4.70e-02, avg batch time: 0.4599, average train loss: 36.1140
[09/26 07:58:13 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1581, average loss: 33.2897
[09/26 07:58:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/26 07:58:13 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[09/26 07:58:19 visual_prompt]: Epoch 7 / 100: avg data time: 5.61e-02, avg batch time: 0.4682, average train loss: 41.1933
[09/26 07:58:20 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1579, average loss: 70.1381
[09/26 07:58:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 46.00	
[09/26 07:58:20 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[09/26 07:58:27 visual_prompt]: Epoch 8 / 100: avg data time: 4.58e-02, avg batch time: 0.4585, average train loss: 55.6418
[09/26 07:58:28 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1576, average loss: 127.4428
[09/26 07:58:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 54.50	
[09/26 07:58:28 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[09/26 07:58:35 visual_prompt]: Epoch 9 / 100: avg data time: 4.53e-02, avg batch time: 0.4596, average train loss: 85.7965
[09/26 07:58:36 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1579, average loss: 87.6398
[09/26 07:58:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.00	
[09/26 07:58:36 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[09/26 07:58:43 visual_prompt]: Epoch 10 / 100: avg data time: 6.25e-02, avg batch time: 0.4741, average train loss: 84.2924
[09/26 07:58:44 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1582, average loss: 86.0716
[09/26 07:58:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 53.00	
[09/26 07:58:44 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[09/26 07:58:51 visual_prompt]: Epoch 11 / 100: avg data time: 5.81e-02, avg batch time: 0.4689, average train loss: 78.0373
[09/26 07:58:52 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1577, average loss: 100.5984
[09/26 07:58:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/26 07:58:52 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[09/26 07:58:59 visual_prompt]: Epoch 12 / 100: avg data time: 5.61e-02, avg batch time: 0.4676, average train loss: 84.1349
[09/26 07:59:00 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1575, average loss: 82.8793
[09/26 07:59:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 07:59:00 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[09/26 07:59:06 visual_prompt]: Epoch 13 / 100: avg data time: 4.44e-02, avg batch time: 0.4570, average train loss: 66.7483
[09/26 07:59:08 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1579, average loss: 55.3923
[09/26 07:59:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 55.50	
[09/26 07:59:08 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[09/26 07:59:14 visual_prompt]: Epoch 14 / 100: avg data time: 5.93e-02, avg batch time: 0.4700, average train loss: 69.2638
[09/26 07:59:16 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1583, average loss: 67.0275
[09/26 07:59:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.50	
[09/26 07:59:16 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[09/26 07:59:22 visual_prompt]: Epoch 15 / 100: avg data time: 5.87e-02, avg batch time: 0.4708, average train loss: 63.7999
[09/26 07:59:24 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1583, average loss: 47.6996
[09/26 07:59:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.00	
[09/26 07:59:24 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[09/26 07:59:30 visual_prompt]: Epoch 16 / 100: avg data time: 5.40e-02, avg batch time: 0.4654, average train loss: 83.4164
[09/26 07:59:32 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1585, average loss: 125.2807
[09/26 07:59:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/26 07:59:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[09/26 07:59:38 visual_prompt]: Epoch 17 / 100: avg data time: 6.04e-02, avg batch time: 0.4726, average train loss: 107.2882
[09/26 07:59:40 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1580, average loss: 118.4299
[09/26 07:59:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 59.50	
[09/26 07:59:40 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[09/26 07:59:46 visual_prompt]: Epoch 18 / 100: avg data time: 5.11e-02, avg batch time: 0.4634, average train loss: 124.3046
[09/26 07:59:48 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.1584, average loss: 139.9588
[09/26 07:59:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.50	
[09/26 07:59:48 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[09/26 07:59:54 visual_prompt]: Epoch 19 / 100: avg data time: 5.75e-02, avg batch time: 0.4689, average train loss: 101.4103
[09/26 07:59:56 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1583, average loss: 94.8193
[09/26 07:59:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 44.00	
[09/26 07:59:56 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[09/26 08:00:02 visual_prompt]: Epoch 20 / 100: avg data time: 5.73e-02, avg batch time: 0.4701, average train loss: 102.7187
[09/26 08:00:03 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1580, average loss: 54.6062
[09/26 08:00:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 53.50	
[09/26 08:00:03 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[09/26 08:00:10 visual_prompt]: Epoch 21 / 100: avg data time: 4.71e-02, avg batch time: 0.4605, average train loss: 83.8332
[09/26 08:00:11 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.1580, average loss: 110.0447
[09/26 08:00:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 08:00:11 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[09/26 08:00:18 visual_prompt]: Epoch 22 / 100: avg data time: 5.78e-02, avg batch time: 0.4706, average train loss: 85.3968
[09/26 08:00:19 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1578, average loss: 96.9969
[09/26 08:00:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 08:00:19 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[09/26 08:00:26 visual_prompt]: Epoch 23 / 100: avg data time: 5.59e-02, avg batch time: 0.4672, average train loss: 72.8038
[09/26 08:00:27 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.1579, average loss: 70.8589
[09/26 08:00:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 58.50	
[09/26 08:00:27 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[09/26 08:00:34 visual_prompt]: Epoch 24 / 100: avg data time: 5.68e-02, avg batch time: 0.4691, average train loss: 87.7103
[09/26 08:00:35 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1580, average loss: 84.4845
[09/26 08:00:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 08:00:35 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[09/26 08:00:42 visual_prompt]: Epoch 25 / 100: avg data time: 6.94e-02, avg batch time: 0.4809, average train loss: 83.6692
[09/26 08:00:43 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1583, average loss: 79.6322
[09/26 08:00:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 58.50	
[09/26 08:00:43 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[09/26 08:00:50 visual_prompt]: Epoch 26 / 100: avg data time: 4.59e-02, avg batch time: 0.4579, average train loss: 79.2042
[09/26 08:00:51 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1584, average loss: 74.9340
[09/26 08:00:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 08:00:51 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[09/26 08:00:58 visual_prompt]: Epoch 27 / 100: avg data time: 6.40e-02, avg batch time: 0.4753, average train loss: 51.4811
[09/26 08:00:59 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1581, average loss: 59.6543
[09/26 08:00:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 08:00:59 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[09/26 08:01:06 visual_prompt]: Epoch 28 / 100: avg data time: 6.00e-02, avg batch time: 0.4708, average train loss: 67.7272
[09/26 08:01:07 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1582, average loss: 52.1294
[09/26 08:01:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 08:01:07 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[09/26 08:01:14 visual_prompt]: Epoch 29 / 100: avg data time: 6.12e-02, avg batch time: 0.4731, average train loss: 61.3936
[09/26 08:01:15 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1582, average loss: 33.8670
[09/26 08:01:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 08:01:15 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[09/26 08:01:22 visual_prompt]: Epoch 30 / 100: avg data time: 5.29e-02, avg batch time: 0.4645, average train loss: 112.1598
[09/26 08:01:23 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1581, average loss: 135.1580
[09/26 08:01:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.00	
[09/26 08:01:23 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[09/26 08:01:29 visual_prompt]: Epoch 31 / 100: avg data time: 4.53e-02, avg batch time: 0.4605, average train loss: 140.4368
[09/26 08:01:31 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1585, average loss: 92.9110
[09/26 08:01:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 52.50	
[09/26 08:01:31 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[09/26 08:01:37 visual_prompt]: Epoch 32 / 100: avg data time: 5.44e-02, avg batch time: 0.4685, average train loss: 110.2692
[09/26 08:01:39 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.1586, average loss: 124.3720
[09/26 08:01:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 08:01:39 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[09/26 08:01:45 visual_prompt]: Epoch 33 / 100: avg data time: 5.60e-02, avg batch time: 0.4672, average train loss: 87.6340
[09/26 08:01:47 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1585, average loss: 86.2546
[09/26 08:01:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 57.00	
[09/26 08:01:47 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[09/26 08:01:53 visual_prompt]: Epoch 34 / 100: avg data time: 4.96e-02, avg batch time: 0.4618, average train loss: 102.3967
[09/26 08:01:55 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1580, average loss: 130.2624
[09/26 08:01:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 53.00	
[09/26 08:01:55 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[09/26 08:02:01 visual_prompt]: Epoch 35 / 100: avg data time: 4.89e-02, avg batch time: 0.4624, average train loss: 87.7874
[09/26 08:02:03 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1583, average loss: 53.7816
[09/26 08:02:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.50	
[09/26 08:02:03 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[09/26 08:02:09 visual_prompt]: Epoch 36 / 100: avg data time: 4.77e-02, avg batch time: 0.4596, average train loss: 92.3722
[09/26 08:02:11 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1586, average loss: 85.7719
[09/26 08:02:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 56.00	
[09/26 08:02:11 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[09/26 08:02:17 visual_prompt]: Epoch 37 / 100: avg data time: 5.80e-02, avg batch time: 0.4694, average train loss: 70.6361
[09/26 08:02:19 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1583, average loss: 54.5528
[09/26 08:02:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 08:02:19 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[09/26 08:02:25 visual_prompt]: Epoch 38 / 100: avg data time: 6.09e-02, avg batch time: 0.4718, average train loss: 72.4191
[09/26 08:02:27 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1581, average loss: 102.1586
[09/26 08:02:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 57.50	
[09/26 08:02:27 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[09/26 08:02:33 visual_prompt]: Epoch 39 / 100: avg data time: 6.36e-02, avg batch time: 0.4751, average train loss: 107.8644
[09/26 08:02:35 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1582, average loss: 169.1083
[09/26 08:02:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 43.50	
[09/26 08:02:35 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[09/26 08:02:41 visual_prompt]: Epoch 40 / 100: avg data time: 5.89e-02, avg batch time: 0.4715, average train loss: 76.1646
[09/26 08:02:43 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1585, average loss: 64.1812
[09/26 08:02:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 44.00	
[09/26 08:02:43 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[09/26 08:02:49 visual_prompt]: Epoch 41 / 100: avg data time: 6.31e-02, avg batch time: 0.4748, average train loss: 94.6193
[09/26 08:02:51 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1581, average loss: 131.2621
[09/26 08:02:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 53.50	
[09/26 08:02:51 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[09/26 08:02:57 visual_prompt]: Epoch 42 / 100: avg data time: 6.29e-02, avg batch time: 0.4751, average train loss: 93.3634
[09/26 08:02:59 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1578, average loss: 57.6421
[09/26 08:02:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 41.50	
[09/26 08:02:59 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[09/26 08:03:05 visual_prompt]: Epoch 43 / 100: avg data time: 5.62e-02, avg batch time: 0.4700, average train loss: 69.0353
[09/26 08:03:07 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1581, average loss: 66.0185
[09/26 08:03:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.00	
[09/26 08:03:07 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[09/26 08:03:13 visual_prompt]: Epoch 44 / 100: avg data time: 6.24e-02, avg batch time: 0.4751, average train loss: 74.9369
[09/26 08:03:15 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1581, average loss: 53.3955
[09/26 08:03:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 08:03:15 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[09/26 08:03:21 visual_prompt]: Epoch 45 / 100: avg data time: 4.53e-02, avg batch time: 0.4578, average train loss: 60.0272
[09/26 08:03:23 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1584, average loss: 34.0908
[09/26 08:03:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.00	
[09/26 08:03:23 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[09/26 08:03:29 visual_prompt]: Epoch 46 / 100: avg data time: 5.05e-02, avg batch time: 0.4635, average train loss: 37.2919
[09/26 08:03:30 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1580, average loss: 34.2228
[09/26 08:03:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 55.50	
[09/26 08:03:30 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[09/26 08:03:37 visual_prompt]: Epoch 47 / 100: avg data time: 6.07e-02, avg batch time: 0.4716, average train loss: 40.0422
[09/26 08:03:38 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1580, average loss: 38.4022
[09/26 08:03:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 46.50	
[09/26 08:03:38 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[09/26 08:03:45 visual_prompt]: Epoch 48 / 100: avg data time: 6.02e-02, avg batch time: 0.4712, average train loss: 31.9068
[09/26 08:03:46 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1580, average loss: 20.1361
[09/26 08:03:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 44.00	
[09/26 08:03:46 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[09/26 08:03:53 visual_prompt]: Epoch 49 / 100: avg data time: 6.17e-02, avg batch time: 0.4743, average train loss: 26.8742
[09/26 08:03:55 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1582, average loss: 23.2513
[09/26 08:03:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 08:03:55 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[09/26 08:04:01 visual_prompt]: Epoch 50 / 100: avg data time: 5.29e-02, avg batch time: 0.4666, average train loss: 35.2901
[09/26 08:04:02 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1582, average loss: 41.9830
[09/26 08:04:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 42.50	
[09/26 08:04:02 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[09/26 08:04:09 visual_prompt]: Epoch 51 / 100: avg data time: 6.22e-02, avg batch time: 0.4744, average train loss: 32.4889
[09/26 08:04:10 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1587, average loss: 33.4866
[09/26 08:04:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 59.00	
[09/26 08:04:10 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[09/26 08:04:17 visual_prompt]: Epoch 52 / 100: avg data time: 5.75e-02, avg batch time: 0.4683, average train loss: 36.7311
[09/26 08:04:18 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1581, average loss: 50.9613
[09/26 08:04:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 50.00	
[09/26 08:04:18 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[09/26 08:04:25 visual_prompt]: Epoch 53 / 100: avg data time: 5.07e-02, avg batch time: 0.4641, average train loss: 46.6321
[09/26 08:04:26 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1585, average loss: 69.7411
[09/26 08:04:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 41.50	
[09/26 08:04:26 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[09/26 08:04:33 visual_prompt]: Epoch 54 / 100: avg data time: 5.27e-02, avg batch time: 0.4651, average train loss: 46.2023
[09/26 08:04:34 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1584, average loss: 28.2243
[09/26 08:04:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 18.00	top5: 56.00	
[09/26 08:04:34 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[09/26 08:04:41 visual_prompt]: Epoch 55 / 100: avg data time: 5.64e-02, avg batch time: 0.4688, average train loss: 38.9014
[09/26 08:04:42 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1583, average loss: 27.2075
[09/26 08:04:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.00	
[09/26 08:04:42 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[09/26 08:04:48 visual_prompt]: Epoch 56 / 100: avg data time: 4.41e-02, avg batch time: 0.4557, average train loss: 38.0041
[09/26 08:04:50 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1580, average loss: 24.0840
[09/26 08:04:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 59.00	
[09/26 08:04:50 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[09/26 08:04:56 visual_prompt]: Epoch 57 / 100: avg data time: 5.86e-02, avg batch time: 0.4706, average train loss: 29.9982
[09/26 08:04:58 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1584, average loss: 20.0848
[09/26 08:04:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 53.00	
[09/26 08:04:58 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[09/26 08:05:04 visual_prompt]: Epoch 58 / 100: avg data time: 5.96e-02, avg batch time: 0.4730, average train loss: 20.3935
[09/26 08:05:06 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1583, average loss: 16.3575
[09/26 08:05:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 63.50	
[09/26 08:05:06 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[09/26 08:05:12 visual_prompt]: Epoch 59 / 100: avg data time: 6.15e-02, avg batch time: 0.4731, average train loss: 17.6218
[09/26 08:05:14 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1583, average loss: 19.0868
[09/26 08:05:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.50	
[09/26 08:05:14 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[09/26 08:05:20 visual_prompt]: Epoch 60 / 100: avg data time: 5.53e-02, avg batch time: 0.4669, average train loss: 20.4369
[09/26 08:05:22 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1582, average loss: 19.6799
[09/26 08:05:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 08:05:22 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[09/26 08:05:28 visual_prompt]: Epoch 61 / 100: avg data time: 5.65e-02, avg batch time: 0.4683, average train loss: 10.7594
[09/26 08:05:30 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1577, average loss: 10.1935
[09/26 08:05:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 54.50	
[09/26 08:05:30 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[09/26 08:05:36 visual_prompt]: Epoch 62 / 100: avg data time: 4.93e-02, avg batch time: 0.4621, average train loss: 12.4723
[09/26 08:05:37 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1581, average loss: 17.3638
[09/26 08:05:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 57.50	
[09/26 08:05:37 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[09/26 08:05:44 visual_prompt]: Epoch 63 / 100: avg data time: 5.42e-02, avg batch time: 0.4659, average train loss: 14.8173
[09/26 08:05:45 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1583, average loss: 15.5317
[09/26 08:05:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 08:05:45 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[09/26 08:05:52 visual_prompt]: Epoch 64 / 100: avg data time: 6.15e-02, avg batch time: 0.4729, average train loss: 12.8535
[09/26 08:05:53 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1580, average loss: 8.1674
[09/26 08:05:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 45.00	
[09/26 08:05:53 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[09/26 08:06:00 visual_prompt]: Epoch 65 / 100: avg data time: 5.34e-02, avg batch time: 0.4665, average train loss: 10.2388
[09/26 08:06:01 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1581, average loss: 13.3472
[09/26 08:06:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 55.00	
[09/26 08:06:01 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[09/26 08:06:08 visual_prompt]: Epoch 66 / 100: avg data time: 5.07e-02, avg batch time: 0.4640, average train loss: 13.9009
[09/26 08:06:09 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1587, average loss: 8.3075
[09/26 08:06:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 08:06:09 visual_prompt]: Training 67 / 100 epoch, with learning rate 7.817417582301099
[09/26 08:06:16 visual_prompt]: Epoch 67 / 100: avg data time: 5.75e-02, avg batch time: 0.4696, average train loss: 11.0445
[09/26 08:06:17 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.1586, average loss: 18.5556
[09/26 08:06:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 52.00	
[09/26 08:06:17 visual_prompt]: Training 68 / 100 epoch, with learning rate 7.415791961552499
[09/26 08:06:24 visual_prompt]: Epoch 68 / 100: avg data time: 5.34e-02, avg batch time: 0.4655, average train loss: 13.8053
[09/26 08:06:25 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1580, average loss: 15.9299
[09/26 08:06:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 45.00	
[09/26 08:06:25 visual_prompt]: Training 69 / 100 epoch, with learning rate 7.020360665136531
[09/26 08:06:31 visual_prompt]: Epoch 69 / 100: avg data time: 4.99e-02, avg batch time: 0.4625, average train loss: 10.0899
[09/26 08:06:33 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1581, average loss: 10.4953
[09/26 08:06:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 41.00	
[09/26 08:06:33 visual_prompt]: Training 70 / 100 epoch, with learning rate 6.631605465176368
[09/26 08:06:39 visual_prompt]: Epoch 70 / 100: avg data time: 4.68e-02, avg batch time: 0.4580, average train loss: 11.6155
[09/26 08:06:41 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1584, average loss: 10.0449
[09/26 08:06:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 08:06:41 visual_prompt]: Training 71 / 100 epoch, with learning rate 6.250000000000003
[09/26 08:06:47 visual_prompt]: Epoch 71 / 100: avg data time: 4.98e-02, avg batch time: 0.4625, average train loss: 9.9777
[09/26 08:06:49 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1581, average loss: 11.2190
[09/26 08:06:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.00	
[09/26 08:06:49 visual_prompt]: Training 72 / 100 epoch, with learning rate 5.87600919708494
[09/26 08:06:55 visual_prompt]: Epoch 72 / 100: avg data time: 4.98e-02, avg batch time: 0.4605, average train loss: 6.9761
[09/26 08:06:56 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1582, average loss: 5.6166
[09/26 08:06:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.00	
[09/26 08:06:56 visual_prompt]: Training 73 / 100 epoch, with learning rate 5.510088706615666
[09/26 08:07:03 visual_prompt]: Epoch 73 / 100: avg data time: 6.19e-02, avg batch time: 0.4730, average train loss: 5.7189
[09/26 08:07:04 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1582, average loss: 3.6101
[09/26 08:07:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 61.00	
[09/26 08:07:04 visual_prompt]: Training 74 / 100 epoch, with learning rate 5.1526843463440875
[09/26 08:07:11 visual_prompt]: Epoch 74 / 100: avg data time: 4.99e-02, avg batch time: 0.4619, average train loss: 4.2176
[09/26 08:07:12 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1580, average loss: 3.1128
[09/26 08:07:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 08:07:12 visual_prompt]: Training 75 / 100 epoch, with learning rate 4.804231558429271
[09/26 08:07:19 visual_prompt]: Epoch 75 / 100: avg data time: 6.02e-02, avg batch time: 0.4724, average train loss: 2.7904
[09/26 08:07:20 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1582, average loss: 2.4367
[09/26 08:07:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 51.50	
[09/26 08:07:20 visual_prompt]: Training 76 / 100 epoch, with learning rate 4.465154878918258
[09/26 08:07:27 visual_prompt]: Epoch 76 / 100: avg data time: 5.52e-02, avg batch time: 0.4668, average train loss: 2.5453
[09/26 08:07:28 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1577, average loss: 2.4571
[09/26 08:07:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.50	
[09/26 08:07:28 visual_prompt]: Training 77 / 100 epoch, with learning rate 4.135867420514276
[09/26 08:07:35 visual_prompt]: Epoch 77 / 100: avg data time: 6.36e-02, avg batch time: 0.4742, average train loss: 2.6110
[09/26 08:07:36 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1583, average loss: 2.6427
[09/26 08:07:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 42.50	
[09/26 08:07:36 visual_prompt]: Training 78 / 100 epoch, with learning rate 3.816770369262533
[09/26 08:07:43 visual_prompt]: Epoch 78 / 100: avg data time: 5.90e-02, avg batch time: 0.4710, average train loss: 2.4461
[09/26 08:07:44 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1580, average loss: 2.3300
[09/26 08:07:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 16.00	top5: 64.50	
[09/26 08:07:44 visual_prompt]: Training 79 / 100 epoch, with learning rate 3.508252495766863
[09/26 08:07:51 visual_prompt]: Epoch 79 / 100: avg data time: 6.12e-02, avg batch time: 0.4718, average train loss: 2.3717
[09/26 08:07:52 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1584, average loss: 2.3281
[09/26 08:07:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.50	top5: 56.50	
[09/26 08:07:52 visual_prompt]: Training 80 / 100 epoch, with learning rate 3.2106896815325707
[09/26 08:07:59 visual_prompt]: Epoch 80 / 100: avg data time: 5.74e-02, avg batch time: 0.4690, average train loss: 2.3963
[09/26 08:08:00 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.1576, average loss: 2.2925
[09/26 08:08:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 08:08:00 visual_prompt]: Training 81 / 100 epoch, with learning rate 2.924444461012776
[09/26 08:08:07 visual_prompt]: Epoch 81 / 100: avg data time: 5.74e-02, avg batch time: 0.4682, average train loss: 2.3682
[09/26 08:08:08 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1579, average loss: 2.3540
[09/26 08:08:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 08:08:08 visual_prompt]: Training 82 / 100 epoch, with learning rate 2.6498655799159763
[09/26 08:08:15 visual_prompt]: Epoch 82 / 100: avg data time: 6.09e-02, avg batch time: 0.4723, average train loss: 2.3886
[09/26 08:08:16 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1575, average loss: 2.4645
[09/26 08:08:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 64.50	
[09/26 08:08:16 visual_prompt]: Training 83 / 100 epoch, with learning rate 2.387287570313158
[09/26 08:08:22 visual_prompt]: Epoch 83 / 100: avg data time: 6.12e-02, avg batch time: 0.4743, average train loss: 2.3879
[09/26 08:08:24 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1581, average loss: 2.3081
[09/26 08:08:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.50	top5: 59.50	
[09/26 08:08:24 visual_prompt]: Training 84 / 100 epoch, with learning rate 2.13703034306198
[09/26 08:08:30 visual_prompt]: Epoch 84 / 100: avg data time: 5.92e-02, avg batch time: 0.4706, average train loss: 2.3312
[09/26 08:08:32 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1581, average loss: 2.2694
[09/26 08:08:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 18.50	top5: 60.50	
[09/26 08:08:32 visual_prompt]: Training 85 / 100 epoch, with learning rate 1.8993987980446756
[09/26 08:08:38 visual_prompt]: Epoch 85 / 100: avg data time: 5.48e-02, avg batch time: 0.4670, average train loss: 2.3161
[09/26 08:08:40 visual_prompt]: Inference (val):avg data time: 4.54e-05, avg batch time: 0.1585, average loss: 2.2592
[09/26 08:08:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 08:08:40 visual_prompt]: Training 86 / 100 epoch, with learning rate 1.674682452694516
[09/26 08:08:46 visual_prompt]: Epoch 86 / 100: avg data time: 5.64e-02, avg batch time: 0.4680, average train loss: 2.3444
[09/26 08:08:48 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1581, average loss: 2.2660
[09/26 08:08:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 19.50	top5: 60.50	
[09/26 08:08:48 visual_prompt]: Training 87 / 100 epoch, with learning rate 1.4631550892634126
[09/26 08:08:54 visual_prompt]: Epoch 87 / 100: avg data time: 5.86e-02, avg batch time: 0.4718, average train loss: 2.3180
[09/26 08:08:56 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1579, average loss: 2.3459
[09/26 08:08:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 08:08:56 visual_prompt]: Training 88 / 100 epoch, with learning rate 1.2650744212604148
[09/26 08:09:02 visual_prompt]: Epoch 88 / 100: avg data time: 5.42e-02, avg batch time: 0.4662, average train loss: 2.3315
[09/26 08:09:04 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1581, average loss: 2.3640
[09/26 08:09:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 08:09:04 visual_prompt]: Training 89 / 100 epoch, with learning rate 1.0806817794674877
[09/26 08:09:10 visual_prompt]: Epoch 89 / 100: avg data time: 6.07e-02, avg batch time: 0.4716, average train loss: 2.3466
[09/26 08:09:12 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1583, average loss: 2.2822
[09/26 08:09:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 08:09:12 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.9102018179151585
[09/26 08:09:18 visual_prompt]: Epoch 90 / 100: avg data time: 5.17e-02, avg batch time: 0.4638, average train loss: 2.3034
[09/26 08:09:20 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1584, average loss: 2.3014
[09/26 08:09:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 08:09:20 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.753842240176146
[09/26 08:09:26 visual_prompt]: Epoch 91 / 100: avg data time: 5.94e-02, avg batch time: 0.4706, average train loss: 2.3127
[09/26 08:09:28 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.1577, average loss: 2.2259
[09/26 08:09:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 08:09:28 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.6117935463105808
[09/26 08:09:34 visual_prompt]: Epoch 92 / 100: avg data time: 6.30e-02, avg batch time: 0.4742, average train loss: 2.2813
[09/26 08:09:36 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1581, average loss: 2.2403
[09/26 08:09:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 08:09:36 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.4842288007710138
[09/26 08:09:42 visual_prompt]: Epoch 93 / 100: avg data time: 5.86e-02, avg batch time: 0.4701, average train loss: 2.2592
[09/26 08:09:44 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1582, average loss: 2.2408
[09/26 08:09:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 65.00	
[09/26 08:09:44 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.3713034215500441
[09/26 08:09:50 visual_prompt]: Epoch 94 / 100: avg data time: 6.26e-02, avg batch time: 0.4737, average train loss: 2.2498
[09/26 08:09:52 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1584, average loss: 2.2129
[09/26 08:09:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 08:09:52 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.2731549908274289
[09/26 08:09:58 visual_prompt]: Epoch 95 / 100: avg data time: 4.79e-02, avg batch time: 0.4597, average train loss: 2.2457
[09/26 08:09:59 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1578, average loss: 2.2214
[09/26 08:09:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 08:09:59 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.18990308734739975
[09/26 08:10:06 visual_prompt]: Epoch 96 / 100: avg data time: 6.17e-02, avg batch time: 0.4725, average train loss: 2.2363
[09/26 08:10:07 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1577, average loss: 2.2145
[09/26 08:10:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 08:10:07 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.12164914073037048
[09/26 08:10:14 visual_prompt]: Epoch 97 / 100: avg data time: 5.55e-02, avg batch time: 0.4674, average train loss: 2.2356
[09/26 08:10:15 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1578, average loss: 2.2117
[09/26 08:10:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 08:10:15 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.06847630789658388
[09/26 08:10:22 visual_prompt]: Epoch 98 / 100: avg data time: 4.68e-02, avg batch time: 0.4585, average train loss: 2.2338
[09/26 08:10:23 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1583, average loss: 2.2160
[09/26 08:10:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.50	
[09/26 08:10:23 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.03044937175219753
[09/26 08:10:30 visual_prompt]: Epoch 99 / 100: avg data time: 5.83e-02, avg batch time: 0.4723, average train loss: 2.2297
[09/26 08:10:31 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1578, average loss: 2.2124
[09/26 08:10:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.50	
[09/26 08:10:31 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0076146622613029735
[09/26 08:10:38 visual_prompt]: Epoch 100 / 100: avg data time: 5.64e-02, avg batch time: 0.4674, average train loss: 2.2258
[09/26 08:10:39 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 2.2125
[09/26 08:10:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.00	
[09/26 08:10:39 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 08:10:39 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 08:10:39 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 08:10:39 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 08:10:39 visual_prompt]: Training with config:
[09/26 08:10:39 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr25.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 08:10:39 visual_prompt]: Loading training data...
[09/26 08:10:39 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 08:10:40 visual_prompt]: Number of images: 800
[09/26 08:10:40 visual_prompt]: Number of classes: 10 / 10
[09/26 08:10:40 visual_prompt]: Loading validation data...
[09/26 08:10:40 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 08:10:40 visual_prompt]: Number of images: 200
[09/26 08:10:40 visual_prompt]: Number of classes: 10 / 10
[09/26 08:10:40 visual_prompt]: Constructing models...
[09/26 08:10:43 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 08:10:43 visual_prompt]: tuned percent:0.543
[09/26 08:10:43 visual_prompt]: Device used for model: 0
[09/26 08:10:43 visual_prompt]: Setting up Evaluator...
[09/26 08:10:43 visual_prompt]: Setting up Trainer...
[09/26 08:10:43 visual_prompt]: 	Setting up the optimizer...
[09/26 08:10:43 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 08:10:50 visual_prompt]: Epoch 1 / 100: avg data time: 5.27e-02, avg batch time: 0.4736, average train loss: 2.6754
[09/26 08:10:51 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1583, average loss: 2.6214
[09/26 08:10:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 08:10:51 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 08:10:51 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[09/26 08:10:58 visual_prompt]: Epoch 2 / 100: avg data time: 5.82e-02, avg batch time: 0.4698, average train loss: 14.5957
[09/26 08:10:59 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1582, average loss: 17.5102
[09/26 08:10:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 43.00	
[09/26 08:10:59 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[09/26 08:11:06 visual_prompt]: Epoch 3 / 100: avg data time: 6.25e-02, avg batch time: 0.4749, average train loss: 17.5886
[09/26 08:11:07 visual_prompt]: Inference (val):avg data time: 1.90e-05, avg batch time: 0.1583, average loss: 16.0547
[09/26 08:11:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 62.00	
[09/26 08:11:07 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[09/26 08:11:14 visual_prompt]: Epoch 4 / 100: avg data time: 6.06e-02, avg batch time: 0.4718, average train loss: 29.5503
[09/26 08:11:15 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1585, average loss: 27.8507
[09/26 08:11:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 50.00	
[09/26 08:11:15 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[09/26 08:11:21 visual_prompt]: Epoch 5 / 100: avg data time: 4.69e-02, avg batch time: 0.4613, average train loss: 41.9017
[09/26 08:11:23 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1582, average loss: 33.2996
[09/26 08:11:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.00	
[09/26 08:11:23 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[09/26 08:11:29 visual_prompt]: Epoch 6 / 100: avg data time: 5.63e-02, avg batch time: 0.4686, average train loss: 30.3737
[09/26 08:11:31 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1586, average loss: 42.9271
[09/26 08:11:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 53.00	
[09/26 08:11:31 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[09/26 08:11:37 visual_prompt]: Epoch 7 / 100: avg data time: 5.58e-02, avg batch time: 0.4678, average train loss: 48.8818
[09/26 08:11:39 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1583, average loss: 81.9768
[09/26 08:11:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 42.50	
[09/26 08:11:39 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[09/26 08:11:45 visual_prompt]: Epoch 8 / 100: avg data time: 6.32e-02, avg batch time: 0.4749, average train loss: 109.4659
[09/26 08:11:47 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1584, average loss: 94.0044
[09/26 08:11:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 61.00	
[09/26 08:11:47 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[09/26 08:11:53 visual_prompt]: Epoch 9 / 100: avg data time: 5.29e-02, avg batch time: 0.4648, average train loss: 100.0696
[09/26 08:11:55 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1586, average loss: 141.4509
[09/26 08:11:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 08:11:55 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[09/26 08:12:01 visual_prompt]: Epoch 10 / 100: avg data time: 6.42e-02, avg batch time: 0.4754, average train loss: 135.1810
[09/26 08:12:03 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1584, average loss: 101.3022
[09/26 08:12:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 59.50	
[09/26 08:12:03 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[09/26 08:12:09 visual_prompt]: Epoch 11 / 100: avg data time: 5.45e-02, avg batch time: 0.4665, average train loss: 93.7725
[09/26 08:12:11 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1585, average loss: 67.2734
[09/26 08:12:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 08:12:11 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[09/26 08:12:17 visual_prompt]: Epoch 12 / 100: avg data time: 5.53e-02, avg batch time: 0.4686, average train loss: 102.7865
[09/26 08:12:19 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1579, average loss: 100.9726
[09/26 08:12:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 58.50	
[09/26 08:12:19 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[09/26 08:12:25 visual_prompt]: Epoch 13 / 100: avg data time: 6.23e-02, avg batch time: 0.4746, average train loss: 124.7902
[09/26 08:12:27 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1583, average loss: 96.9489
[09/26 08:12:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.50	
[09/26 08:12:27 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[09/26 08:12:33 visual_prompt]: Epoch 14 / 100: avg data time: 4.57e-02, avg batch time: 0.4588, average train loss: 106.3706
[09/26 08:12:35 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1584, average loss: 46.2492
[09/26 08:12:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 08:12:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[09/26 08:12:41 visual_prompt]: Epoch 15 / 100: avg data time: 6.64e-02, avg batch time: 0.4793, average train loss: 68.0632
[09/26 08:12:43 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1575, average loss: 90.2611
[09/26 08:12:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 54.00	
[09/26 08:12:43 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[09/26 08:12:49 visual_prompt]: Epoch 16 / 100: avg data time: 6.15e-02, avg batch time: 0.4764, average train loss: 83.8217
[09/26 08:12:51 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1584, average loss: 82.0300
[09/26 08:12:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 45.00	
[09/26 08:12:51 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[09/26 08:12:57 visual_prompt]: Epoch 17 / 100: avg data time: 5.73e-02, avg batch time: 0.4708, average train loss: 64.4543
[09/26 08:12:59 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1583, average loss: 73.5350
[09/26 08:12:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 64.00	
[09/26 08:12:59 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[09/26 08:13:05 visual_prompt]: Epoch 18 / 100: avg data time: 6.34e-02, avg batch time: 0.4752, average train loss: 51.6820
[09/26 08:13:07 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1579, average loss: 49.9583
[09/26 08:13:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 08:13:07 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[09/26 08:13:13 visual_prompt]: Epoch 19 / 100: avg data time: 5.85e-02, avg batch time: 0.4713, average train loss: 60.0102
[09/26 08:13:15 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1582, average loss: 65.6068
[09/26 08:13:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 53.50	
[09/26 08:13:15 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[09/26 08:13:21 visual_prompt]: Epoch 20 / 100: avg data time: 5.80e-02, avg batch time: 0.4704, average train loss: 75.4743
[09/26 08:13:23 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1584, average loss: 92.0489
[09/26 08:13:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 47.50	
[09/26 08:13:23 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[09/26 08:13:29 visual_prompt]: Epoch 21 / 100: avg data time: 5.43e-02, avg batch time: 0.4671, average train loss: 63.7719
[09/26 08:13:31 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1579, average loss: 74.2594
[09/26 08:13:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 08:13:31 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[09/26 08:13:37 visual_prompt]: Epoch 22 / 100: avg data time: 5.10e-02, avg batch time: 0.4630, average train loss: 75.6425
[09/26 08:13:38 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1580, average loss: 56.9052
[09/26 08:13:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.00	
[09/26 08:13:38 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[09/26 08:13:45 visual_prompt]: Epoch 23 / 100: avg data time: 5.46e-02, avg batch time: 0.4677, average train loss: 63.2783
[09/26 08:13:46 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1587, average loss: 45.4107
[09/26 08:13:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 38.00	
[09/26 08:13:46 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[09/26 08:13:53 visual_prompt]: Epoch 24 / 100: avg data time: 5.75e-02, avg batch time: 0.4702, average train loss: 52.9172
[09/26 08:13:54 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1583, average loss: 60.4596
[09/26 08:13:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 36.50	
[09/26 08:13:54 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[09/26 08:14:01 visual_prompt]: Epoch 25 / 100: avg data time: 6.01e-02, avg batch time: 0.4716, average train loss: 51.9405
[09/26 08:14:02 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1583, average loss: 55.8174
[09/26 08:14:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 55.50	
[09/26 08:14:02 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[09/26 08:14:09 visual_prompt]: Epoch 26 / 100: avg data time: 5.68e-02, avg batch time: 0.4692, average train loss: 49.7362
[09/26 08:14:10 visual_prompt]: Inference (val):avg data time: 1.85e-05, avg batch time: 0.1579, average loss: 43.0931
[09/26 08:14:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 53.50	
[09/26 08:14:10 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[09/26 08:14:17 visual_prompt]: Epoch 27 / 100: avg data time: 6.25e-02, avg batch time: 0.4749, average train loss: 51.5010
[09/26 08:14:18 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1582, average loss: 43.7078
[09/26 08:14:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.00	
[09/26 08:14:18 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[09/26 08:14:25 visual_prompt]: Epoch 28 / 100: avg data time: 6.08e-02, avg batch time: 0.4734, average train loss: 49.2380
[09/26 08:14:26 visual_prompt]: Inference (val):avg data time: 1.85e-05, avg batch time: 0.1584, average loss: 55.3235
[09/26 08:14:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/26 08:14:26 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[09/26 08:14:33 visual_prompt]: Epoch 29 / 100: avg data time: 4.41e-02, avg batch time: 0.4560, average train loss: 31.1958
[09/26 08:14:34 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1582, average loss: 24.3145
[09/26 08:14:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 65.00	
[09/26 08:14:34 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[09/26 08:14:41 visual_prompt]: Epoch 30 / 100: avg data time: 5.94e-02, avg batch time: 0.4714, average train loss: 24.5939
[09/26 08:14:42 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1581, average loss: 26.1173
[09/26 08:14:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 36.50	
[09/26 08:14:42 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[09/26 08:14:49 visual_prompt]: Epoch 31 / 100: avg data time: 5.65e-02, avg batch time: 0.4686, average train loss: 31.4226
[09/26 08:14:50 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1576, average loss: 26.6118
[09/26 08:14:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 48.00	
[09/26 08:14:50 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[09/26 08:14:56 visual_prompt]: Epoch 32 / 100: avg data time: 5.56e-02, avg batch time: 0.4678, average train loss: 34.9108
[09/26 08:14:58 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1582, average loss: 48.2060
[09/26 08:14:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 41.50	
[09/26 08:14:58 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[09/26 08:15:04 visual_prompt]: Epoch 33 / 100: avg data time: 5.77e-02, avg batch time: 0.4698, average train loss: 34.1164
[09/26 08:15:06 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1583, average loss: 42.8497
[09/26 08:15:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 63.50	
[09/26 08:15:06 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[09/26 08:15:12 visual_prompt]: Epoch 34 / 100: avg data time: 5.83e-02, avg batch time: 0.4721, average train loss: 39.0366
[09/26 08:15:14 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1582, average loss: 35.5667
[09/26 08:15:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 08:15:14 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[09/26 08:15:20 visual_prompt]: Epoch 35 / 100: avg data time: 6.15e-02, avg batch time: 0.4755, average train loss: 44.0963
[09/26 08:15:22 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1581, average loss: 38.2910
[09/26 08:15:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 55.50	
[09/26 08:15:22 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[09/26 08:15:28 visual_prompt]: Epoch 36 / 100: avg data time: 4.65e-02, avg batch time: 0.4631, average train loss: 31.1670
[09/26 08:15:30 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1581, average loss: 32.0388
[09/26 08:15:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.50	
[09/26 08:15:30 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[09/26 08:15:36 visual_prompt]: Epoch 37 / 100: avg data time: 5.92e-02, avg batch time: 0.4723, average train loss: 39.8783
[09/26 08:15:38 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.1582, average loss: 37.7028
[09/26 08:15:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 08:15:38 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[09/26 08:15:44 visual_prompt]: Epoch 38 / 100: avg data time: 6.04e-02, avg batch time: 0.4740, average train loss: 39.8773
[09/26 08:15:46 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1582, average loss: 51.5453
[09/26 08:15:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 41.00	
[09/26 08:15:46 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[09/26 08:15:52 visual_prompt]: Epoch 39 / 100: avg data time: 5.55e-02, avg batch time: 0.4685, average train loss: 39.5211
[09/26 08:15:54 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1580, average loss: 47.2883
[09/26 08:15:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 43.00	
[09/26 08:15:54 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[09/26 08:16:00 visual_prompt]: Epoch 40 / 100: avg data time: 4.57e-02, avg batch time: 0.4583, average train loss: 44.3367
[09/26 08:16:02 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1582, average loss: 37.4416
[09/26 08:16:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 08:16:02 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[09/26 08:16:08 visual_prompt]: Epoch 41 / 100: avg data time: 5.82e-02, avg batch time: 0.4705, average train loss: 40.0373
[09/26 08:16:10 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1581, average loss: 29.9073
[09/26 08:16:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 44.50	
[09/26 08:16:10 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[09/26 08:16:16 visual_prompt]: Epoch 42 / 100: avg data time: 6.47e-02, avg batch time: 0.4775, average train loss: 33.3493
[09/26 08:16:18 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1582, average loss: 23.8046
[09/26 08:16:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 45.50	
[09/26 08:16:18 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[09/26 08:16:24 visual_prompt]: Epoch 43 / 100: avg data time: 5.43e-02, avg batch time: 0.4678, average train loss: 24.8898
[09/26 08:16:26 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1581, average loss: 21.6291
[09/26 08:16:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 08:16:26 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[09/26 08:16:32 visual_prompt]: Epoch 44 / 100: avg data time: 4.80e-02, avg batch time: 0.4606, average train loss: 29.3902
[09/26 08:16:33 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1581, average loss: 15.2149
[09/26 08:16:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 45.00	
[09/26 08:16:33 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[09/26 08:16:40 visual_prompt]: Epoch 45 / 100: avg data time: 5.06e-02, avg batch time: 0.4636, average train loss: 30.5907
[09/26 08:16:41 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1578, average loss: 18.7695
[09/26 08:16:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 08:16:41 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[09/26 08:16:48 visual_prompt]: Epoch 46 / 100: avg data time: 6.04e-02, avg batch time: 0.4726, average train loss: 25.8751
[09/26 08:16:49 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1580, average loss: 22.7272
[09/26 08:16:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 42.00	
[09/26 08:16:49 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[09/26 08:16:56 visual_prompt]: Epoch 47 / 100: avg data time: 6.21e-02, avg batch time: 0.4753, average train loss: 24.4284
[09/26 08:16:57 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1578, average loss: 17.7580
[09/26 08:16:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 56.50	
[09/26 08:16:57 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[09/26 08:17:04 visual_prompt]: Epoch 48 / 100: avg data time: 4.96e-02, avg batch time: 0.4627, average train loss: 18.5985
[09/26 08:17:05 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1582, average loss: 21.6518
[09/26 08:17:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 57.00	
[09/26 08:17:05 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[09/26 08:17:12 visual_prompt]: Epoch 49 / 100: avg data time: 4.91e-02, avg batch time: 0.4623, average train loss: 22.9260
[09/26 08:17:13 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1582, average loss: 17.3073
[09/26 08:17:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 60.50	
[09/26 08:17:13 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[09/26 08:17:20 visual_prompt]: Epoch 50 / 100: avg data time: 5.68e-02, avg batch time: 0.4711, average train loss: 21.0702
[09/26 08:17:21 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1582, average loss: 18.6792
[09/26 08:17:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 50.50	
[09/26 08:17:21 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[09/26 08:17:28 visual_prompt]: Epoch 51 / 100: avg data time: 6.00e-02, avg batch time: 0.4724, average train loss: 21.7107
[09/26 08:17:29 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1582, average loss: 22.1232
[09/26 08:17:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 58.50	
[09/26 08:17:29 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[09/26 08:17:36 visual_prompt]: Epoch 52 / 100: avg data time: 5.89e-02, avg batch time: 0.4711, average train loss: 26.1542
[09/26 08:17:37 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1584, average loss: 29.9422
[09/26 08:17:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.50	
[09/26 08:17:37 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[09/26 08:17:44 visual_prompt]: Epoch 53 / 100: avg data time: 5.95e-02, avg batch time: 0.4730, average train loss: 26.7690
[09/26 08:17:45 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1582, average loss: 18.7818
[09/26 08:17:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 44.50	
[09/26 08:17:45 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[09/26 08:17:52 visual_prompt]: Epoch 54 / 100: avg data time: 6.12e-02, avg batch time: 0.4732, average train loss: 24.1952
[09/26 08:17:53 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1580, average loss: 22.5186
[09/26 08:17:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 08:17:53 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[09/26 08:17:59 visual_prompt]: Epoch 55 / 100: avg data time: 5.64e-02, avg batch time: 0.4696, average train loss: 24.0596
[09/26 08:18:01 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1584, average loss: 27.3795
[09/26 08:18:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.00	
[09/26 08:18:01 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[09/26 08:18:07 visual_prompt]: Epoch 56 / 100: avg data time: 5.59e-02, avg batch time: 0.4684, average train loss: 29.8445
[09/26 08:18:09 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1580, average loss: 18.1048
[09/26 08:18:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.50	
[09/26 08:18:09 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[09/26 08:18:15 visual_prompt]: Epoch 57 / 100: avg data time: 4.82e-02, avg batch time: 0.4623, average train loss: 17.4568
[09/26 08:18:17 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1584, average loss: 16.4259
[09/26 08:18:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 54.00	
[09/26 08:18:17 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[09/26 08:18:23 visual_prompt]: Epoch 58 / 100: avg data time: 4.82e-02, avg batch time: 0.4619, average train loss: 13.8750
[09/26 08:18:25 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1581, average loss: 10.2527
[09/26 08:18:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.00	top5: 40.50	
[09/26 08:18:25 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[09/26 08:18:31 visual_prompt]: Epoch 59 / 100: avg data time: 5.45e-02, avg batch time: 0.4694, average train loss: 9.8839
[09/26 08:18:33 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1586, average loss: 9.7912
[09/26 08:18:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 53.50	
[09/26 08:18:33 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[09/26 08:18:39 visual_prompt]: Epoch 60 / 100: avg data time: 5.74e-02, avg batch time: 0.4694, average train loss: 12.4895
[09/26 08:18:41 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1581, average loss: 9.6159
[09/26 08:18:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 47.50	
[09/26 08:18:41 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[09/26 08:18:47 visual_prompt]: Epoch 61 / 100: avg data time: 5.26e-02, avg batch time: 0.4656, average train loss: 10.6305
[09/26 08:18:48 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1585, average loss: 11.0300
[09/26 08:18:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.50	top5: 42.50	
[09/26 08:18:48 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[09/26 08:18:55 visual_prompt]: Epoch 62 / 100: avg data time: 6.14e-02, avg batch time: 0.4746, average train loss: 11.6299
[09/26 08:18:56 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1579, average loss: 9.9008
[09/26 08:18:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 60.00	
[09/26 08:18:56 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[09/26 08:19:03 visual_prompt]: Epoch 63 / 100: avg data time: 4.71e-02, avg batch time: 0.4617, average train loss: 8.2826
[09/26 08:19:04 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1581, average loss: 6.6631
[09/26 08:19:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 56.50	
[09/26 08:19:04 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[09/26 08:19:11 visual_prompt]: Epoch 64 / 100: avg data time: 4.96e-02, avg batch time: 0.4652, average train loss: 7.4077
[09/26 08:19:12 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1581, average loss: 10.0173
[09/26 08:19:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 54.00	
[09/26 08:19:12 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[09/26 08:19:19 visual_prompt]: Epoch 65 / 100: avg data time: 5.99e-02, avg batch time: 0.4732, average train loss: 6.2171
[09/26 08:19:20 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1574, average loss: 6.7926
[09/26 08:19:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 52.00	
[09/26 08:19:20 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[09/26 08:19:27 visual_prompt]: Epoch 66 / 100: avg data time: 6.02e-02, avg batch time: 0.4728, average train loss: 6.5434
[09/26 08:19:28 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1585, average loss: 6.0161
[09/26 08:19:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 45.00	
[09/26 08:19:28 visual_prompt]: Training 67 / 100 epoch, with learning rate 7.817417582301099
[09/26 08:19:35 visual_prompt]: Epoch 67 / 100: avg data time: 6.03e-02, avg batch time: 0.4731, average train loss: 6.7987
[09/26 08:19:36 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1583, average loss: 5.2905
[09/26 08:19:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.50	
[09/26 08:19:36 visual_prompt]: Training 68 / 100 epoch, with learning rate 7.415791961552499
[09/26 08:19:43 visual_prompt]: Epoch 68 / 100: avg data time: 5.77e-02, avg batch time: 0.4699, average train loss: 4.1016
[09/26 08:19:44 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1586, average loss: 3.9530
[09/26 08:19:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.50	top5: 59.50	
[09/26 08:19:44 visual_prompt]: Training 69 / 100 epoch, with learning rate 7.020360665136531
[09/26 08:19:51 visual_prompt]: Epoch 69 / 100: avg data time: 5.97e-02, avg batch time: 0.4715, average train loss: 3.4733
[09/26 08:19:52 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1583, average loss: 2.6556
[09/26 08:19:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 14.50	top5: 57.50	
[09/26 08:19:52 visual_prompt]: Training 70 / 100 epoch, with learning rate 6.631605465176368
[09/26 08:19:58 visual_prompt]: Epoch 70 / 100: avg data time: 5.48e-02, avg batch time: 0.4691, average train loss: 2.8327
[09/26 08:20:00 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1580, average loss: 2.6712
[09/26 08:20:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 55.50	
[09/26 08:20:00 visual_prompt]: Training 71 / 100 epoch, with learning rate 6.250000000000003
[09/26 08:20:06 visual_prompt]: Epoch 71 / 100: avg data time: 5.04e-02, avg batch time: 0.4642, average train loss: 2.9104
[09/26 08:20:08 visual_prompt]: Inference (val):avg data time: 1.91e-05, avg batch time: 0.1581, average loss: 2.9652
[09/26 08:20:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 08:20:08 visual_prompt]: Training 72 / 100 epoch, with learning rate 5.87600919708494
[09/26 08:20:14 visual_prompt]: Epoch 72 / 100: avg data time: 4.43e-02, avg batch time: 0.4569, average train loss: 2.8546
[09/26 08:20:16 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1582, average loss: 2.8822
[09/26 08:20:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 43.50	
[09/26 08:20:16 visual_prompt]: Training 73 / 100 epoch, with learning rate 5.510088706615666
[09/26 08:20:22 visual_prompt]: Epoch 73 / 100: avg data time: 5.11e-02, avg batch time: 0.4647, average train loss: 2.5752
[09/26 08:20:24 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1587, average loss: 2.6453
[09/26 08:20:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 45.00	
[09/26 08:20:24 visual_prompt]: Training 74 / 100 epoch, with learning rate 5.1526843463440875
[09/26 08:20:30 visual_prompt]: Epoch 74 / 100: avg data time: 6.31e-02, avg batch time: 0.4762, average train loss: 2.6816
[09/26 08:20:32 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1582, average loss: 2.5382
[09/26 08:20:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.50	top5: 58.00	
[09/26 08:20:32 visual_prompt]: Training 75 / 100 epoch, with learning rate 4.804231558429271
[09/26 08:20:38 visual_prompt]: Epoch 75 / 100: avg data time: 6.34e-02, avg batch time: 0.4755, average train loss: 2.6212
[09/26 08:20:40 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1582, average loss: 2.5009
[09/26 08:20:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 21.00	top5: 62.50	
[09/26 08:20:40 visual_prompt]: Training 76 / 100 epoch, with learning rate 4.465154878918258
[09/26 08:20:46 visual_prompt]: Epoch 76 / 100: avg data time: 5.08e-02, avg batch time: 0.4634, average train loss: 2.5081
[09/26 08:20:48 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1590, average loss: 2.5208
[09/26 08:20:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.00	
[09/26 08:20:48 visual_prompt]: Training 77 / 100 epoch, with learning rate 4.135867420514276
[09/26 08:20:54 visual_prompt]: Epoch 77 / 100: avg data time: 6.05e-02, avg batch time: 0.4732, average train loss: 2.4731
[09/26 08:20:56 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1584, average loss: 2.3685
[09/26 08:20:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.00	top5: 64.50	
[09/26 08:20:56 visual_prompt]: Training 78 / 100 epoch, with learning rate 3.816770369262533
[09/26 08:21:02 visual_prompt]: Epoch 78 / 100: avg data time: 5.58e-02, avg batch time: 0.4697, average train loss: 2.3865
[09/26 08:21:04 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1580, average loss: 2.5130
[09/26 08:21:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 14.00	top5: 51.50	
[09/26 08:21:04 visual_prompt]: Training 79 / 100 epoch, with learning rate 3.508252495766863
[09/26 08:21:10 visual_prompt]: Epoch 79 / 100: avg data time: 5.63e-02, avg batch time: 0.4681, average train loss: 2.6127
[09/26 08:21:12 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1582, average loss: 2.4776
[09/26 08:21:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 58.00	
[09/26 08:21:12 visual_prompt]: Training 80 / 100 epoch, with learning rate 3.2106896815325707
[09/26 08:21:18 visual_prompt]: Epoch 80 / 100: avg data time: 6.17e-02, avg batch time: 0.4751, average train loss: 2.5075
[09/26 08:21:20 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1579, average loss: 2.5249
[09/26 08:21:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 08:21:20 visual_prompt]: Training 81 / 100 epoch, with learning rate 2.924444461012776
[09/26 08:21:26 visual_prompt]: Epoch 81 / 100: avg data time: 6.26e-02, avg batch time: 0.4746, average train loss: 2.5063
[09/26 08:21:28 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1583, average loss: 2.5164
[09/26 08:21:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 13.50	top5: 63.50	
[09/26 08:21:28 visual_prompt]: Training 82 / 100 epoch, with learning rate 2.6498655799159763
[09/26 08:21:34 visual_prompt]: Epoch 82 / 100: avg data time: 5.76e-02, avg batch time: 0.4706, average train loss: 2.5036
[09/26 08:21:36 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1584, average loss: 2.4859
[09/26 08:21:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.00	
[09/26 08:21:36 visual_prompt]: Training 83 / 100 epoch, with learning rate 2.387287570313158
[09/26 08:21:42 visual_prompt]: Epoch 83 / 100: avg data time: 5.76e-02, avg batch time: 0.4703, average train loss: 2.4363
[09/26 08:21:44 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1583, average loss: 2.5559
[09/26 08:21:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 17.00	top5: 56.50	
[09/26 08:21:44 visual_prompt]: Training 84 / 100 epoch, with learning rate 2.13703034306198
[09/26 08:21:50 visual_prompt]: Epoch 84 / 100: avg data time: 6.39e-02, avg batch time: 0.4765, average train loss: 2.4409
[09/26 08:21:52 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1580, average loss: 2.3410
[09/26 08:21:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 14.00	top5: 63.00	
[09/26 08:21:52 visual_prompt]: Training 85 / 100 epoch, with learning rate 1.8993987980446756
[09/26 08:21:58 visual_prompt]: Epoch 85 / 100: avg data time: 6.09e-02, avg batch time: 0.4732, average train loss: 2.4478
[09/26 08:22:00 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1583, average loss: 2.4138
[09/26 08:22:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 13.50	top5: 64.50	
[09/26 08:22:00 visual_prompt]: Training 86 / 100 epoch, with learning rate 1.674682452694516
[09/26 08:22:06 visual_prompt]: Epoch 86 / 100: avg data time: 6.09e-02, avg batch time: 0.4728, average train loss: 2.3494
[09/26 08:22:08 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1581, average loss: 2.3729
[09/26 08:22:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 62.00	
[09/26 08:22:08 visual_prompt]: Training 87 / 100 epoch, with learning rate 1.4631550892634126
[09/26 08:22:14 visual_prompt]: Epoch 87 / 100: avg data time: 5.44e-02, avg batch time: 0.4667, average train loss: 2.3703
[09/26 08:22:15 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1582, average loss: 2.4228
[09/26 08:22:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 57.00	
[09/26 08:22:15 visual_prompt]: Training 88 / 100 epoch, with learning rate 1.2650744212604148
[09/26 08:22:22 visual_prompt]: Epoch 88 / 100: avg data time: 5.68e-02, avg batch time: 0.4685, average train loss: 2.3489
[09/26 08:22:23 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1577, average loss: 2.3650
[09/26 08:22:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 16.50	top5: 62.00	
[09/26 08:22:23 visual_prompt]: Training 89 / 100 epoch, with learning rate 1.0806817794674877
[09/26 08:22:30 visual_prompt]: Epoch 89 / 100: avg data time: 5.89e-02, avg batch time: 0.4723, average train loss: 2.3754
[09/26 08:22:31 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1580, average loss: 2.2858
[09/26 08:22:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 64.50	
[09/26 08:22:31 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.9102018179151585
[09/26 08:22:38 visual_prompt]: Epoch 90 / 100: avg data time: 5.51e-02, avg batch time: 0.4682, average train loss: 2.3080
[09/26 08:22:39 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1584, average loss: 2.2830
[09/26 08:22:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 64.50	
[09/26 08:22:39 visual_prompt]: Best epoch 90: best metric: 0.235
[09/26 08:22:39 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.753842240176146
[09/26 08:22:46 visual_prompt]: Epoch 91 / 100: avg data time: 6.09e-02, avg batch time: 0.4733, average train loss: 2.3400
[09/26 08:22:47 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1584, average loss: 2.2973
[09/26 08:22:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 14.50	top5: 60.50	
[09/26 08:22:47 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.6117935463105808
[09/26 08:22:54 visual_prompt]: Epoch 92 / 100: avg data time: 5.61e-02, avg batch time: 0.4687, average train loss: 2.2994
[09/26 08:22:55 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1585, average loss: 2.2888
[09/26 08:22:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.00	top5: 64.00	
[09/26 08:22:55 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.4842288007710138
[09/26 08:23:01 visual_prompt]: Epoch 93 / 100: avg data time: 5.37e-02, avg batch time: 0.4664, average train loss: 2.2959
[09/26 08:23:03 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1584, average loss: 2.2772
[09/26 08:23:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 18.50	top5: 64.00	
[09/26 08:23:03 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.3713034215500441
[09/26 08:23:09 visual_prompt]: Epoch 94 / 100: avg data time: 4.92e-02, avg batch time: 0.4641, average train loss: 2.2847
[09/26 08:23:11 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1587, average loss: 2.2784
[09/26 08:23:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 16.00	top5: 64.50	
[09/26 08:23:11 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.2731549908274289
[09/26 08:23:17 visual_prompt]: Epoch 95 / 100: avg data time: 5.21e-02, avg batch time: 0.4642, average train loss: 2.2841
[09/26 08:23:19 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1585, average loss: 2.2574
[09/26 08:23:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.00	top5: 64.50	
[09/26 08:23:19 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.18990308734739975
[09/26 08:23:25 visual_prompt]: Epoch 96 / 100: avg data time: 5.16e-02, avg batch time: 0.4644, average train loss: 2.2793
[09/26 08:23:27 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1580, average loss: 2.2648
[09/26 08:23:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 19.00	top5: 63.50	
[09/26 08:23:27 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.12164914073037048
[09/26 08:23:33 visual_prompt]: Epoch 97 / 100: avg data time: 5.83e-02, avg batch time: 0.4702, average train loss: 2.2670
[09/26 08:23:35 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1582, average loss: 2.2759
[09/26 08:23:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 18.50	top5: 63.00	
[09/26 08:23:35 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.06847630789658388
[09/26 08:23:41 visual_prompt]: Epoch 98 / 100: avg data time: 5.83e-02, avg batch time: 0.4711, average train loss: 2.2665
[09/26 08:23:43 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1581, average loss: 2.2644
[09/26 08:23:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 21.50	top5: 65.00	
[09/26 08:23:43 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.03044937175219753
[09/26 08:23:49 visual_prompt]: Epoch 99 / 100: avg data time: 5.49e-02, avg batch time: 0.4679, average train loss: 2.2853
[09/26 08:23:51 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1580, average loss: 2.2655
[09/26 08:23:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.00	top5: 64.50	
[09/26 08:23:51 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0076146622613029735
[09/26 08:23:57 visual_prompt]: Epoch 100 / 100: avg data time: 5.03e-02, avg batch time: 0.4644, average train loss: 2.2346
[09/26 08:23:59 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1583, average loss: 2.2652
[09/26 08:23:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 64.50	
[09/26 08:23:59 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 08:23:59 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 08:23:59 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 08:23:59 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 08:23:59 visual_prompt]: Training with config:
[09/26 08:23:59 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr10.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 08:23:59 visual_prompt]: Loading training data...
[09/26 08:23:59 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 08:24:00 visual_prompt]: Number of images: 800
[09/26 08:24:00 visual_prompt]: Number of classes: 10 / 10
[09/26 08:24:00 visual_prompt]: Loading validation data...
[09/26 08:24:00 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 08:24:01 visual_prompt]: Number of images: 200
[09/26 08:24:01 visual_prompt]: Number of classes: 10 / 10
[09/26 08:24:01 visual_prompt]: Constructing models...
[09/26 08:24:03 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 08:24:03 visual_prompt]: tuned percent:0.543
[09/26 08:24:03 visual_prompt]: Device used for model: 0
[09/26 08:24:03 visual_prompt]: Setting up Evaluator...
[09/26 08:24:03 visual_prompt]: Setting up Trainer...
[09/26 08:24:03 visual_prompt]: 	Setting up the optimizer...
[09/26 08:24:03 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 08:24:10 visual_prompt]: Epoch 1 / 100: avg data time: 5.68e-02, avg batch time: 0.4777, average train loss: 2.6867
[09/26 08:24:11 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1580, average loss: 2.6214
[09/26 08:24:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 08:24:11 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 08:24:11 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[09/26 08:24:18 visual_prompt]: Epoch 2 / 100: avg data time: 6.32e-02, avg batch time: 0.4747, average train loss: 6.3093
[09/26 08:24:19 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1582, average loss: 5.0925
[09/26 08:24:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.00	top5: 46.00	
[09/26 08:24:19 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[09/26 08:24:26 visual_prompt]: Epoch 3 / 100: avg data time: 5.24e-02, avg batch time: 0.4651, average train loss: 3.8444
[09/26 08:24:27 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1582, average loss: 3.7055
[09/26 08:24:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 40.50	
[09/26 08:24:27 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[09/26 08:24:34 visual_prompt]: Epoch 4 / 100: avg data time: 5.42e-02, avg batch time: 0.4671, average train loss: 5.5321
[09/26 08:24:35 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1583, average loss: 6.4547
[09/26 08:24:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.00	
[09/26 08:24:35 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[09/26 08:24:42 visual_prompt]: Epoch 5 / 100: avg data time: 6.03e-02, avg batch time: 0.4741, average train loss: 6.1114
[09/26 08:24:43 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1580, average loss: 12.6649
[09/26 08:24:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 44.50	
[09/26 08:24:43 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[09/26 08:24:49 visual_prompt]: Epoch 6 / 100: avg data time: 4.95e-02, avg batch time: 0.4629, average train loss: 20.8268
[09/26 08:24:51 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1585, average loss: 31.8282
[09/26 08:24:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 45.00	
[09/26 08:24:51 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[09/26 08:24:57 visual_prompt]: Epoch 7 / 100: avg data time: 4.72e-02, avg batch time: 0.4618, average train loss: 20.8850
[09/26 08:24:59 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1576, average loss: 21.2542
[09/26 08:24:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 55.50	
[09/26 08:24:59 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[09/26 08:25:05 visual_prompt]: Epoch 8 / 100: avg data time: 5.72e-02, avg batch time: 0.4699, average train loss: 22.3062
[09/26 08:25:07 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1583, average loss: 28.7844
[09/26 08:25:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.50	top5: 56.00	
[09/26 08:25:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[09/26 08:25:13 visual_prompt]: Epoch 9 / 100: avg data time: 4.48e-02, avg batch time: 0.4609, average train loss: 27.5109
[09/26 08:25:15 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1581, average loss: 26.4945
[09/26 08:25:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 08:25:15 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[09/26 08:25:21 visual_prompt]: Epoch 10 / 100: avg data time: 5.66e-02, avg batch time: 0.4709, average train loss: 36.2158
[09/26 08:25:23 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1585, average loss: 20.8186
[09/26 08:25:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.50	
[09/26 08:25:23 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[09/26 08:25:29 visual_prompt]: Epoch 11 / 100: avg data time: 5.32e-02, avg batch time: 0.4660, average train loss: 27.7659
[09/26 08:25:30 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1581, average loss: 38.7931
[09/26 08:25:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 39.50	
[09/26 08:25:30 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[09/26 08:25:37 visual_prompt]: Epoch 12 / 100: avg data time: 5.89e-02, avg batch time: 0.4715, average train loss: 37.6736
[09/26 08:25:38 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1586, average loss: 36.8225
[09/26 08:25:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.00	
[09/26 08:25:38 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[09/26 08:25:45 visual_prompt]: Epoch 13 / 100: avg data time: 5.87e-02, avg batch time: 0.4711, average train loss: 40.6662
[09/26 08:25:46 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1583, average loss: 28.9099
[09/26 08:25:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 58.50	
[09/26 08:25:46 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[09/26 08:25:53 visual_prompt]: Epoch 14 / 100: avg data time: 5.81e-02, avg batch time: 0.4708, average train loss: 51.1550
[09/26 08:25:54 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1580, average loss: 46.0458
[09/26 08:25:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 55.50	
[09/26 08:25:54 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[09/26 08:26:01 visual_prompt]: Epoch 15 / 100: avg data time: 5.93e-02, avg batch time: 0.4715, average train loss: 49.1797
[09/26 08:26:02 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1582, average loss: 31.5284
[09/26 08:26:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.00	
[09/26 08:26:02 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[09/26 08:26:09 visual_prompt]: Epoch 16 / 100: avg data time: 4.51e-02, avg batch time: 0.4584, average train loss: 37.1969
[09/26 08:26:10 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1586, average loss: 44.5699
[09/26 08:26:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 58.50	
[09/26 08:26:10 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[09/26 08:26:17 visual_prompt]: Epoch 17 / 100: avg data time: 5.90e-02, avg batch time: 0.4707, average train loss: 37.6787
[09/26 08:26:18 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1581, average loss: 41.1831
[09/26 08:26:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 38.50	
[09/26 08:26:18 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[09/26 08:26:25 visual_prompt]: Epoch 18 / 100: avg data time: 6.05e-02, avg batch time: 0.4740, average train loss: 54.4578
[09/26 08:26:26 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1578, average loss: 51.5816
[09/26 08:26:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 53.50	
[09/26 08:26:26 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[09/26 08:26:33 visual_prompt]: Epoch 19 / 100: avg data time: 6.34e-02, avg batch time: 0.4754, average train loss: 43.1344
[09/26 08:26:34 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1580, average loss: 45.5917
[09/26 08:26:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 54.50	
[09/26 08:26:34 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[09/26 08:26:41 visual_prompt]: Epoch 20 / 100: avg data time: 5.70e-02, avg batch time: 0.4704, average train loss: 33.3875
[09/26 08:26:42 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1583, average loss: 37.0095
[09/26 08:26:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.00	top5: 61.00	
[09/26 08:26:42 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[09/26 08:26:48 visual_prompt]: Epoch 21 / 100: avg data time: 4.33e-02, avg batch time: 0.4570, average train loss: 35.5872
[09/26 08:26:50 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1583, average loss: 37.6308
[09/26 08:26:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 53.50	
[09/26 08:26:50 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[09/26 08:26:56 visual_prompt]: Epoch 22 / 100: avg data time: 5.83e-02, avg batch time: 0.4707, average train loss: 41.6131
[09/26 08:26:58 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1583, average loss: 61.2366
[09/26 08:26:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 58.00	
[09/26 08:26:58 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[09/26 08:27:04 visual_prompt]: Epoch 23 / 100: avg data time: 5.69e-02, avg batch time: 0.4712, average train loss: 51.7490
[09/26 08:27:06 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1578, average loss: 33.2708
[09/26 08:27:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 08:27:06 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[09/26 08:27:12 visual_prompt]: Epoch 24 / 100: avg data time: 5.15e-02, avg batch time: 0.4680, average train loss: 44.1741
[09/26 08:27:14 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1581, average loss: 69.3265
[09/26 08:27:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 08:27:14 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[09/26 08:27:20 visual_prompt]: Epoch 25 / 100: avg data time: 6.03e-02, avg batch time: 0.4721, average train loss: 44.9173
[09/26 08:27:22 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1581, average loss: 16.4706
[09/26 08:27:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.50	
[09/26 08:27:22 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[09/26 08:27:28 visual_prompt]: Epoch 26 / 100: avg data time: 5.68e-02, avg batch time: 0.4696, average train loss: 37.0228
[09/26 08:27:30 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1581, average loss: 31.5154
[09/26 08:27:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 46.50	
[09/26 08:27:30 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[09/26 08:27:36 visual_prompt]: Epoch 27 / 100: avg data time: 5.36e-02, avg batch time: 0.4679, average train loss: 42.2160
[09/26 08:27:38 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1583, average loss: 20.9551
[09/26 08:27:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 49.50	
[09/26 08:27:38 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[09/26 08:27:44 visual_prompt]: Epoch 28 / 100: avg data time: 5.47e-02, avg batch time: 0.4679, average train loss: 30.8359
[09/26 08:27:46 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1585, average loss: 40.8622
[09/26 08:27:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.50	top5: 41.50	
[09/26 08:27:46 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[09/26 08:27:52 visual_prompt]: Epoch 29 / 100: avg data time: 6.43e-02, avg batch time: 0.4759, average train loss: 39.3972
[09/26 08:27:54 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1582, average loss: 51.0147
[09/26 08:27:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 08:27:54 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[09/26 08:28:00 visual_prompt]: Epoch 30 / 100: avg data time: 5.30e-02, avg batch time: 0.4654, average train loss: 42.6707
[09/26 08:28:02 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1585, average loss: 46.7067
[09/26 08:28:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.00	
[09/26 08:28:02 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[09/26 08:28:08 visual_prompt]: Epoch 31 / 100: avg data time: 5.95e-02, avg batch time: 0.4706, average train loss: 39.9577
[09/26 08:28:10 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 39.7264
[09/26 08:28:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 55.00	
[09/26 08:28:10 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[09/26 08:28:16 visual_prompt]: Epoch 32 / 100: avg data time: 6.42e-02, avg batch time: 0.4760, average train loss: 33.5165
[09/26 08:28:18 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1581, average loss: 18.9000
[09/26 08:28:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 42.00	
[09/26 08:28:18 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[09/26 08:28:24 visual_prompt]: Epoch 33 / 100: avg data time: 4.78e-02, avg batch time: 0.4606, average train loss: 33.2663
[09/26 08:28:26 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1580, average loss: 42.0379
[09/26 08:28:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 53.50	
[09/26 08:28:26 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[09/26 08:28:32 visual_prompt]: Epoch 34 / 100: avg data time: 4.42e-02, avg batch time: 0.4579, average train loss: 36.6340
[09/26 08:28:33 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1582, average loss: 42.8088
[09/26 08:28:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.50	
[09/26 08:28:33 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[09/26 08:28:40 visual_prompt]: Epoch 35 / 100: avg data time: 6.55e-02, avg batch time: 0.4777, average train loss: 38.7798
[09/26 08:28:41 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1582, average loss: 48.0291
[09/26 08:28:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 64.00	
[09/26 08:28:41 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[09/26 08:28:48 visual_prompt]: Epoch 36 / 100: avg data time: 5.40e-02, avg batch time: 0.4666, average train loss: 29.0674
[09/26 08:28:49 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1581, average loss: 35.6003
[09/26 08:28:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 08:28:49 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[09/26 08:28:56 visual_prompt]: Epoch 37 / 100: avg data time: 6.24e-02, avg batch time: 0.4746, average train loss: 33.1014
[09/26 08:28:57 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1584, average loss: 31.8102
[09/26 08:28:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.50	top5: 57.50	
[09/26 08:28:57 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[09/26 08:29:04 visual_prompt]: Epoch 38 / 100: avg data time: 6.19e-02, avg batch time: 0.4733, average train loss: 26.3230
[09/26 08:29:06 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1586, average loss: 35.2185
[09/26 08:29:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 37.00	
[09/26 08:29:06 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[09/26 08:29:12 visual_prompt]: Epoch 39 / 100: avg data time: 6.22e-02, avg batch time: 0.4752, average train loss: 42.1202
[09/26 08:29:14 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1583, average loss: 29.0904
[09/26 08:29:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 43.50	
[09/26 08:29:14 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[09/26 08:29:20 visual_prompt]: Epoch 40 / 100: avg data time: 4.39e-02, avg batch time: 0.4578, average train loss: 37.3922
[09/26 08:29:21 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1579, average loss: 37.6321
[09/26 08:29:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 44.00	
[09/26 08:29:21 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[09/26 08:29:28 visual_prompt]: Epoch 41 / 100: avg data time: 5.89e-02, avg batch time: 0.4712, average train loss: 40.4062
[09/26 08:29:29 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1583, average loss: 35.2915
[09/26 08:29:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 40.00	
[09/26 08:29:29 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[09/26 08:29:36 visual_prompt]: Epoch 42 / 100: avg data time: 6.00e-02, avg batch time: 0.4727, average train loss: 36.4469
[09/26 08:29:37 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1582, average loss: 34.8782
[09/26 08:29:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.50	
[09/26 08:29:37 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[09/26 08:29:44 visual_prompt]: Epoch 43 / 100: avg data time: 6.21e-02, avg batch time: 0.4738, average train loss: 33.1023
[09/26 08:29:45 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1580, average loss: 15.7595
[09/26 08:29:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 18.50	top5: 59.00	
[09/26 08:29:45 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[09/26 08:29:52 visual_prompt]: Epoch 44 / 100: avg data time: 6.20e-02, avg batch time: 0.4735, average train loss: 29.3428
[09/26 08:29:53 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1577, average loss: 34.4092
[09/26 08:29:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 52.50	
[09/26 08:29:53 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[09/26 08:30:00 visual_prompt]: Epoch 45 / 100: avg data time: 6.06e-02, avg batch time: 0.4723, average train loss: 31.0947
[09/26 08:30:01 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1581, average loss: 13.5218
[09/26 08:30:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 08:30:01 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[09/26 08:30:08 visual_prompt]: Epoch 46 / 100: avg data time: 4.95e-02, avg batch time: 0.4632, average train loss: 27.3456
[09/26 08:30:09 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1582, average loss: 24.3011
[09/26 08:30:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 58.50	
[09/26 08:30:09 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[09/26 08:30:16 visual_prompt]: Epoch 47 / 100: avg data time: 6.17e-02, avg batch time: 0.4738, average train loss: 23.4360
[09/26 08:30:17 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1585, average loss: 13.8535
[09/26 08:30:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.50	
[09/26 08:30:17 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[09/26 08:30:24 visual_prompt]: Epoch 48 / 100: avg data time: 6.29e-02, avg batch time: 0.4768, average train loss: 32.6776
[09/26 08:30:25 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1584, average loss: 37.9218
[09/26 08:30:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 58.50	
[09/26 08:30:25 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[09/26 08:30:32 visual_prompt]: Epoch 49 / 100: avg data time: 5.77e-02, avg batch time: 0.4708, average train loss: 26.9388
[09/26 08:30:33 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1587, average loss: 31.2698
[09/26 08:30:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 49.00	
[09/26 08:30:33 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[09/26 08:30:39 visual_prompt]: Epoch 50 / 100: avg data time: 4.54e-02, avg batch time: 0.4584, average train loss: 28.1931
[09/26 08:30:41 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1585, average loss: 22.1015
[09/26 08:30:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 41.00	
[09/26 08:30:41 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[09/26 08:30:47 visual_prompt]: Epoch 51 / 100: avg data time: 5.11e-02, avg batch time: 0.4640, average train loss: 19.0464
[09/26 08:30:49 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1580, average loss: 21.2949
[09/26 08:30:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 45.50	
[09/26 08:30:49 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[09/26 08:30:55 visual_prompt]: Epoch 52 / 100: avg data time: 5.87e-02, avg batch time: 0.4704, average train loss: 17.4765
[09/26 08:30:57 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1586, average loss: 23.5609
[09/26 08:30:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 50.50	
[09/26 08:30:57 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[09/26 08:31:03 visual_prompt]: Epoch 53 / 100: avg data time: 5.87e-02, avg batch time: 0.4705, average train loss: 28.5250
[09/26 08:31:05 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1583, average loss: 17.8478
[09/26 08:31:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 41.50	
[09/26 08:31:05 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[09/26 08:31:11 visual_prompt]: Epoch 54 / 100: avg data time: 4.59e-02, avg batch time: 0.4596, average train loss: 19.9979
[09/26 08:31:13 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1579, average loss: 22.9007
[09/26 08:31:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 53.50	
[09/26 08:31:13 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[09/26 08:31:19 visual_prompt]: Epoch 55 / 100: avg data time: 5.74e-02, avg batch time: 0.4699, average train loss: 18.3344
[09/26 08:31:21 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1580, average loss: 22.5276
[09/26 08:31:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.50	
[09/26 08:31:21 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[09/26 08:31:27 visual_prompt]: Epoch 56 / 100: avg data time: 5.84e-02, avg batch time: 0.4713, average train loss: 18.8368
[09/26 08:31:29 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1585, average loss: 9.6744
[09/26 08:31:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 58.00	
[09/26 08:31:29 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[09/26 08:31:35 visual_prompt]: Epoch 57 / 100: avg data time: 6.51e-02, avg batch time: 0.4775, average train loss: 17.9098
[09/26 08:31:37 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.1581, average loss: 25.6937
[09/26 08:31:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.00	
[09/26 08:31:37 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[09/26 08:31:43 visual_prompt]: Epoch 58 / 100: avg data time: 6.01e-02, avg batch time: 0.4730, average train loss: 16.6340
[09/26 08:31:45 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1576, average loss: 34.8842
[09/26 08:31:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 08:31:45 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[09/26 08:31:51 visual_prompt]: Epoch 59 / 100: avg data time: 5.43e-02, avg batch time: 0.4684, average train loss: 21.5069
[09/26 08:31:53 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1583, average loss: 18.5941
[09/26 08:31:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 08:31:53 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[09/26 08:31:59 visual_prompt]: Epoch 60 / 100: avg data time: 5.92e-02, avg batch time: 0.4712, average train loss: 16.3167
[09/26 08:32:01 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1565, average loss: 14.0232
[09/26 08:32:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 57.00	
[09/26 08:32:01 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[09/26 08:32:07 visual_prompt]: Epoch 61 / 100: avg data time: 6.09e-02, avg batch time: 0.4734, average train loss: 23.0230
[09/26 08:32:09 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1585, average loss: 19.8007
[09/26 08:32:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 41.50	
[09/26 08:32:09 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[09/26 08:32:15 visual_prompt]: Epoch 62 / 100: avg data time: 5.57e-02, avg batch time: 0.4697, average train loss: 17.2478
[09/26 08:32:17 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1582, average loss: 12.3323
[09/26 08:32:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 41.50	
[09/26 08:32:17 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[09/26 08:32:23 visual_prompt]: Epoch 63 / 100: avg data time: 5.44e-02, avg batch time: 0.4686, average train loss: 13.5373
[09/26 08:32:25 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1579, average loss: 13.1741
[09/26 08:32:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 36.00	
[09/26 08:32:25 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[09/26 08:32:31 visual_prompt]: Epoch 64 / 100: avg data time: 6.41e-02, avg batch time: 0.4787, average train loss: 14.7417
[09/26 08:32:33 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1584, average loss: 11.2071
[09/26 08:32:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.50	top5: 41.50	
[09/26 08:32:33 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[09/26 08:32:39 visual_prompt]: Epoch 65 / 100: avg data time: 5.07e-02, avg batch time: 0.4636, average train loss: 10.7805
[09/26 08:32:41 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1583, average loss: 11.3994
[09/26 08:32:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 59.00	
[09/26 08:32:41 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[09/26 08:32:47 visual_prompt]: Epoch 66 / 100: avg data time: 4.61e-02, avg batch time: 0.4586, average train loss: 9.7022
[09/26 08:32:48 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1582, average loss: 10.4259
[09/26 08:32:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/26 08:32:48 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[09/26 08:32:55 visual_prompt]: Epoch 67 / 100: avg data time: 5.66e-02, avg batch time: 0.4685, average train loss: 14.3330
[09/26 08:32:56 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1580, average loss: 17.9667
[09/26 08:32:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/26 08:32:56 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[09/26 08:33:03 visual_prompt]: Epoch 68 / 100: avg data time: 6.17e-02, avg batch time: 0.4734, average train loss: 12.6260
[09/26 08:33:04 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.1582, average loss: 15.8384
[09/26 08:33:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 5.50	top5: 58.00	
[09/26 08:33:04 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[09/26 08:33:11 visual_prompt]: Epoch 69 / 100: avg data time: 4.36e-02, avg batch time: 0.4582, average train loss: 10.4359
[09/26 08:33:12 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1582, average loss: 7.2956
[09/26 08:33:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.50	
[09/26 08:33:12 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[09/26 08:33:19 visual_prompt]: Epoch 70 / 100: avg data time: 5.57e-02, avg batch time: 0.4682, average train loss: 7.7058
[09/26 08:33:20 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1583, average loss: 5.5116
[09/26 08:33:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.50	
[09/26 08:33:20 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[09/26 08:33:27 visual_prompt]: Epoch 71 / 100: avg data time: 5.79e-02, avg batch time: 0.4693, average train loss: 5.2554
[09/26 08:33:28 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1580, average loss: 3.7911
[09/26 08:33:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 49.00	
[09/26 08:33:28 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[09/26 08:33:35 visual_prompt]: Epoch 72 / 100: avg data time: 5.91e-02, avg batch time: 0.4732, average train loss: 3.0712
[09/26 08:33:36 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1579, average loss: 4.7901
[09/26 08:33:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.00	
[09/26 08:33:36 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[09/26 08:33:42 visual_prompt]: Epoch 73 / 100: avg data time: 5.00e-02, avg batch time: 0.4630, average train loss: 4.9998
[09/26 08:33:44 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1580, average loss: 3.4535
[09/26 08:33:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.50	
[09/26 08:33:44 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[09/26 08:33:51 visual_prompt]: Epoch 74 / 100: avg data time: 6.34e-02, avg batch time: 0.4772, average train loss: 3.8491
[09/26 08:33:52 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1582, average loss: 2.8351
[09/26 08:33:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 08:33:52 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[09/26 08:33:58 visual_prompt]: Epoch 75 / 100: avg data time: 4.65e-02, avg batch time: 0.4608, average train loss: 2.5673
[09/26 08:34:00 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1583, average loss: 2.3554
[09/26 08:34:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 58.50	
[09/26 08:34:00 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[09/26 08:34:07 visual_prompt]: Epoch 76 / 100: avg data time: 7.01e-02, avg batch time: 0.4817, average train loss: 2.8044
[09/26 08:34:08 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1584, average loss: 3.7912
[09/26 08:34:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 46.50	
[09/26 08:34:08 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[09/26 08:34:15 visual_prompt]: Epoch 77 / 100: avg data time: 6.22e-02, avg batch time: 0.4752, average train loss: 3.5983
[09/26 08:34:16 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1579, average loss: 2.5113
[09/26 08:34:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 08:34:16 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[09/26 08:34:23 visual_prompt]: Epoch 78 / 100: avg data time: 5.83e-02, avg batch time: 0.4704, average train loss: 2.9346
[09/26 08:34:24 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1583, average loss: 2.4706
[09/26 08:34:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 08:34:24 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[09/26 08:34:31 visual_prompt]: Epoch 79 / 100: avg data time: 5.72e-02, avg batch time: 0.4683, average train loss: 2.5982
[09/26 08:34:32 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1581, average loss: 2.6741
[09/26 08:34:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.50	
[09/26 08:34:32 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[09/26 08:34:38 visual_prompt]: Epoch 80 / 100: avg data time: 4.70e-02, avg batch time: 0.4613, average train loss: 2.9630
[09/26 08:34:40 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1582, average loss: 2.6468
[09/26 08:34:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 58.00	
[09/26 08:34:40 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[09/26 08:34:46 visual_prompt]: Epoch 81 / 100: avg data time: 5.92e-02, avg batch time: 0.4722, average train loss: 2.6160
[09/26 08:34:48 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1586, average loss: 2.2993
[09/26 08:34:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.00	
[09/26 08:34:48 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[09/26 08:34:55 visual_prompt]: Epoch 82 / 100: avg data time: 6.22e-02, avg batch time: 0.4739, average train loss: 2.4009
[09/26 08:34:56 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1578, average loss: 2.3570
[09/26 08:34:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 08:34:56 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[09/26 08:35:02 visual_prompt]: Epoch 83 / 100: avg data time: 5.38e-02, avg batch time: 0.4690, average train loss: 2.3119
[09/26 08:35:04 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1579, average loss: 2.3659
[09/26 08:35:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 08:35:04 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[09/26 08:35:10 visual_prompt]: Epoch 84 / 100: avg data time: 5.42e-02, avg batch time: 0.4677, average train loss: 2.3338
[09/26 08:35:12 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1579, average loss: 2.2377
[09/26 08:35:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 08:35:12 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[09/26 08:35:19 visual_prompt]: Epoch 85 / 100: avg data time: 6.50e-02, avg batch time: 0.4766, average train loss: 2.2924
[09/26 08:35:20 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1588, average loss: 2.2741
[09/26 08:35:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 08:35:20 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[09/26 08:35:27 visual_prompt]: Epoch 86 / 100: avg data time: 6.03e-02, avg batch time: 0.4719, average train loss: 2.2847
[09/26 08:35:28 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1584, average loss: 2.2521
[09/26 08:35:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.50	
[09/26 08:35:28 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[09/26 08:35:35 visual_prompt]: Epoch 87 / 100: avg data time: 5.85e-02, avg batch time: 0.4709, average train loss: 2.2901
[09/26 08:35:36 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1584, average loss: 2.2317
[09/26 08:35:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 08:35:36 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[09/26 08:35:43 visual_prompt]: Epoch 88 / 100: avg data time: 6.13e-02, avg batch time: 0.4726, average train loss: 2.2730
[09/26 08:35:44 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1583, average loss: 2.2522
[09/26 08:35:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 08:35:44 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[09/26 08:35:51 visual_prompt]: Epoch 89 / 100: avg data time: 6.50e-02, avg batch time: 0.4780, average train loss: 2.3132
[09/26 08:35:52 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 2.2423
[09/26 08:35:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 08:35:52 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[09/26 08:35:59 visual_prompt]: Epoch 90 / 100: avg data time: 6.46e-02, avg batch time: 0.4770, average train loss: 2.2652
[09/26 08:36:00 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1578, average loss: 2.2303
[09/26 08:36:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 08:36:00 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[09/26 08:36:07 visual_prompt]: Epoch 91 / 100: avg data time: 5.02e-02, avg batch time: 0.4634, average train loss: 2.2570
[09/26 08:36:08 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1581, average loss: 2.2321
[09/26 08:36:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 08:36:08 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[09/26 08:36:15 visual_prompt]: Epoch 92 / 100: avg data time: 6.04e-02, avg batch time: 0.4727, average train loss: 2.2533
[09/26 08:36:16 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1583, average loss: 2.2293
[09/26 08:36:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 08:36:16 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[09/26 08:36:23 visual_prompt]: Epoch 93 / 100: avg data time: 6.08e-02, avg batch time: 0.4739, average train loss: 2.2463
[09/26 08:36:24 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1587, average loss: 2.2287
[09/26 08:36:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 08:36:24 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[09/26 08:36:31 visual_prompt]: Epoch 94 / 100: avg data time: 6.22e-02, avg batch time: 0.4740, average train loss: 2.2424
[09/26 08:36:32 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1585, average loss: 2.2146
[09/26 08:36:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 08:36:32 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[09/26 08:36:39 visual_prompt]: Epoch 95 / 100: avg data time: 5.70e-02, avg batch time: 0.4698, average train loss: 2.2412
[09/26 08:36:40 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.1583, average loss: 2.2174
[09/26 08:36:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 08:36:40 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[09/26 08:36:47 visual_prompt]: Epoch 96 / 100: avg data time: 6.04e-02, avg batch time: 0.4722, average train loss: 2.2389
[09/26 08:36:48 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1578, average loss: 2.2207
[09/26 08:36:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 08:36:48 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[09/26 08:36:55 visual_prompt]: Epoch 97 / 100: avg data time: 5.39e-02, avg batch time: 0.4654, average train loss: 2.2349
[09/26 08:36:56 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1583, average loss: 2.2142
[09/26 08:36:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 08:36:56 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[09/26 08:37:03 visual_prompt]: Epoch 98 / 100: avg data time: 6.17e-02, avg batch time: 0.4734, average train loss: 2.2332
[09/26 08:37:04 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1584, average loss: 2.2145
[09/26 08:37:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 08:37:04 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[09/26 08:37:11 visual_prompt]: Epoch 99 / 100: avg data time: 5.97e-02, avg batch time: 0.4709, average train loss: 2.2321
[09/26 08:37:12 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1581, average loss: 2.2143
[09/26 08:37:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 08:37:12 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[09/26 08:37:19 visual_prompt]: Epoch 100 / 100: avg data time: 4.49e-02, avg batch time: 0.4594, average train loss: 2.2310
[09/26 08:37:20 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1583, average loss: 2.2146
[09/26 08:37:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 08:37:20 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 08:37:20 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 08:37:20 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 08:37:20 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 08:37:20 visual_prompt]: Training with config:
[09/26 08:37:20 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr10.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 08:37:20 visual_prompt]: Loading training data...
[09/26 08:37:20 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 08:37:21 visual_prompt]: Number of images: 800
[09/26 08:37:21 visual_prompt]: Number of classes: 10 / 10
[09/26 08:37:21 visual_prompt]: Loading validation data...
[09/26 08:37:21 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 08:37:21 visual_prompt]: Number of images: 200
[09/26 08:37:21 visual_prompt]: Number of classes: 10 / 10
[09/26 08:37:21 visual_prompt]: Constructing models...
[09/26 08:37:24 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 08:37:24 visual_prompt]: tuned percent:0.543
[09/26 08:37:24 visual_prompt]: Device used for model: 0
[09/26 08:37:24 visual_prompt]: Setting up Evaluator...
[09/26 08:37:24 visual_prompt]: Setting up Trainer...
[09/26 08:37:24 visual_prompt]: 	Setting up the optimizer...
[09/26 08:37:24 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 08:37:30 visual_prompt]: Epoch 1 / 100: avg data time: 4.98e-02, avg batch time: 0.4663, average train loss: 2.6735
[09/26 08:37:32 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1575, average loss: 2.6214
[09/26 08:37:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 08:37:32 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 08:37:32 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[09/26 08:37:38 visual_prompt]: Epoch 2 / 100: avg data time: 5.76e-02, avg batch time: 0.4699, average train loss: 7.4337
[09/26 08:37:40 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1579, average loss: 6.6195
[09/26 08:37:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 08:37:40 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[09/26 08:37:46 visual_prompt]: Epoch 3 / 100: avg data time: 6.19e-02, avg batch time: 0.4740, average train loss: 4.2720
[09/26 08:37:48 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1581, average loss: 2.7979
[09/26 08:37:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 63.50	
[09/26 08:37:48 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[09/26 08:37:54 visual_prompt]: Epoch 4 / 100: avg data time: 5.68e-02, avg batch time: 0.4690, average train loss: 5.0839
[09/26 08:37:56 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.1582, average loss: 5.9593
[09/26 08:37:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.00	
[09/26 08:37:56 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[09/26 08:38:02 visual_prompt]: Epoch 5 / 100: avg data time: 5.31e-02, avg batch time: 0.4648, average train loss: 6.1492
[09/26 08:38:04 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1573, average loss: 4.2987
[09/26 08:38:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 53.00	
[09/26 08:38:04 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[09/26 08:38:10 visual_prompt]: Epoch 6 / 100: avg data time: 5.80e-02, avg batch time: 0.4699, average train loss: 7.6260
[09/26 08:38:12 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1584, average loss: 10.3882
[09/26 08:38:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 44.00	
[09/26 08:38:12 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[09/26 08:38:18 visual_prompt]: Epoch 7 / 100: avg data time: 6.44e-02, avg batch time: 0.4788, average train loss: 19.9983
[09/26 08:38:20 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1581, average loss: 42.5558
[09/26 08:38:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 54.50	
[09/26 08:38:20 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[09/26 08:38:26 visual_prompt]: Epoch 8 / 100: avg data time: 6.10e-02, avg batch time: 0.4732, average train loss: 34.9477
[09/26 08:38:28 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1585, average loss: 35.6975
[09/26 08:38:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 55.00	
[09/26 08:38:28 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[09/26 08:38:34 visual_prompt]: Epoch 9 / 100: avg data time: 5.34e-02, avg batch time: 0.4652, average train loss: 28.1254
[09/26 08:38:36 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1580, average loss: 32.1478
[09/26 08:38:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 58.50	
[09/26 08:38:36 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[09/26 08:38:42 visual_prompt]: Epoch 10 / 100: avg data time: 4.54e-02, avg batch time: 0.4602, average train loss: 42.0895
[09/26 08:38:44 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1589, average loss: 54.8827
[09/26 08:38:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 44.50	
[09/26 08:38:44 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[09/26 08:38:50 visual_prompt]: Epoch 11 / 100: avg data time: 4.64e-02, avg batch time: 0.4613, average train loss: 54.5091
[09/26 08:38:52 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1584, average loss: 26.7147
[09/26 08:38:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 55.50	
[09/26 08:38:52 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[09/26 08:38:58 visual_prompt]: Epoch 12 / 100: avg data time: 5.61e-02, avg batch time: 0.4700, average train loss: 41.3588
[09/26 08:39:00 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1582, average loss: 30.6036
[09/26 08:39:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 64.00	
[09/26 08:39:00 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[09/26 08:39:06 visual_prompt]: Epoch 13 / 100: avg data time: 7.17e-02, avg batch time: 0.4838, average train loss: 47.5907
[09/26 08:39:08 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1584, average loss: 24.6809
[09/26 08:39:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 55.50	
[09/26 08:39:08 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[09/26 08:39:14 visual_prompt]: Epoch 14 / 100: avg data time: 5.55e-02, avg batch time: 0.4713, average train loss: 37.5866
[09/26 08:39:16 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1581, average loss: 30.6818
[09/26 08:39:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.00	
[09/26 08:39:16 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[09/26 08:39:22 visual_prompt]: Epoch 15 / 100: avg data time: 5.97e-02, avg batch time: 0.4719, average train loss: 35.1711
[09/26 08:39:24 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1582, average loss: 19.3096
[09/26 08:39:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 46.00	
[09/26 08:39:24 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[09/26 08:39:30 visual_prompt]: Epoch 16 / 100: avg data time: 5.92e-02, avg batch time: 0.4717, average train loss: 33.8679
[09/26 08:39:32 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1581, average loss: 53.8386
[09/26 08:39:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 36.00	
[09/26 08:39:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[09/26 08:39:38 visual_prompt]: Epoch 17 / 100: avg data time: 5.70e-02, avg batch time: 0.4710, average train loss: 40.6461
[09/26 08:39:40 visual_prompt]: Inference (val):avg data time: 4.32e-05, avg batch time: 0.1580, average loss: 50.4665
[09/26 08:39:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 56.00	
[09/26 08:39:40 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[09/26 08:39:46 visual_prompt]: Epoch 18 / 100: avg data time: 4.86e-02, avg batch time: 0.4635, average train loss: 36.9293
[09/26 08:39:48 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1579, average loss: 38.9414
[09/26 08:39:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 44.00	
[09/26 08:39:48 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[09/26 08:39:54 visual_prompt]: Epoch 19 / 100: avg data time: 4.31e-02, avg batch time: 0.4595, average train loss: 34.2187
[09/26 08:39:56 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1581, average loss: 31.4309
[09/26 08:39:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.00	
[09/26 08:39:56 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[09/26 08:40:02 visual_prompt]: Epoch 20 / 100: avg data time: 5.70e-02, avg batch time: 0.4718, average train loss: 41.5046
[09/26 08:40:04 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1580, average loss: 34.4961
[09/26 08:40:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 36.00	
[09/26 08:40:04 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[09/26 08:40:10 visual_prompt]: Epoch 21 / 100: avg data time: 4.75e-02, avg batch time: 0.4615, average train loss: 33.1548
[09/26 08:40:11 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1583, average loss: 45.5032
[09/26 08:40:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 57.50	
[09/26 08:40:11 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[09/26 08:40:18 visual_prompt]: Epoch 22 / 100: avg data time: 4.59e-02, avg batch time: 0.4605, average train loss: 60.4560
[09/26 08:40:19 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1575, average loss: 41.6736
[09/26 08:40:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 61.00	
[09/26 08:40:19 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[09/26 08:40:26 visual_prompt]: Epoch 23 / 100: avg data time: 4.62e-02, avg batch time: 0.4609, average train loss: 38.9803
[09/26 08:40:27 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1583, average loss: 48.7956
[09/26 08:40:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 51.50	
[09/26 08:40:27 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[09/26 08:40:34 visual_prompt]: Epoch 24 / 100: avg data time: 6.26e-02, avg batch time: 0.4750, average train loss: 48.0269
[09/26 08:40:35 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1586, average loss: 59.3146
[09/26 08:40:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.00	
[09/26 08:40:35 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[09/26 08:40:42 visual_prompt]: Epoch 25 / 100: avg data time: 6.01e-02, avg batch time: 0.4735, average train loss: 50.3917
[09/26 08:40:43 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1586, average loss: 51.0810
[09/26 08:40:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 56.00	
[09/26 08:40:43 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[09/26 08:40:49 visual_prompt]: Epoch 26 / 100: avg data time: 5.62e-02, avg batch time: 0.4714, average train loss: 38.9296
[09/26 08:40:51 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1579, average loss: 51.2797
[09/26 08:40:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 63.00	
[09/26 08:40:51 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[09/26 08:40:57 visual_prompt]: Epoch 27 / 100: avg data time: 5.84e-02, avg batch time: 0.4717, average train loss: 35.8582
[09/26 08:40:59 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1578, average loss: 38.9366
[09/26 08:40:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 38.00	
[09/26 08:40:59 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[09/26 08:41:05 visual_prompt]: Epoch 28 / 100: avg data time: 5.83e-02, avg batch time: 0.4709, average train loss: 38.6324
[09/26 08:41:07 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1581, average loss: 22.0961
[09/26 08:41:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 53.00	
[09/26 08:41:07 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[09/26 08:41:13 visual_prompt]: Epoch 29 / 100: avg data time: 5.25e-02, avg batch time: 0.4655, average train loss: 39.4896
[09/26 08:41:15 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1582, average loss: 38.1835
[09/26 08:41:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 08:41:15 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[09/26 08:41:21 visual_prompt]: Epoch 30 / 100: avg data time: 5.13e-02, avg batch time: 0.4661, average train loss: 37.2208
[09/26 08:41:23 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1580, average loss: 22.2457
[09/26 08:41:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/26 08:41:23 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[09/26 08:41:29 visual_prompt]: Epoch 31 / 100: avg data time: 4.86e-02, avg batch time: 0.4622, average train loss: 32.8281
[09/26 08:41:31 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1581, average loss: 39.0775
[09/26 08:41:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 08:41:31 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[09/26 08:41:37 visual_prompt]: Epoch 32 / 100: avg data time: 6.07e-02, avg batch time: 0.4735, average train loss: 35.1645
[09/26 08:41:39 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1580, average loss: 27.3176
[09/26 08:41:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.50	
[09/26 08:41:39 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[09/26 08:41:45 visual_prompt]: Epoch 33 / 100: avg data time: 5.78e-02, avg batch time: 0.4697, average train loss: 30.2553
[09/26 08:41:47 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1580, average loss: 47.5775
[09/26 08:41:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.50	top5: 42.00	
[09/26 08:41:47 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[09/26 08:41:53 visual_prompt]: Epoch 34 / 100: avg data time: 5.37e-02, avg batch time: 0.4676, average train loss: 65.7514
[09/26 08:41:55 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1582, average loss: 79.2719
[09/26 08:41:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 08:41:55 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[09/26 08:42:01 visual_prompt]: Epoch 35 / 100: avg data time: 5.60e-02, avg batch time: 0.4677, average train loss: 60.7204
[09/26 08:42:03 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1579, average loss: 45.5013
[09/26 08:42:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.00	
[09/26 08:42:03 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[09/26 08:42:09 visual_prompt]: Epoch 36 / 100: avg data time: 5.57e-02, avg batch time: 0.4691, average train loss: 41.7633
[09/26 08:42:11 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1573, average loss: 37.6566
[09/26 08:42:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 18.00	top5: 62.00	
[09/26 08:42:11 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[09/26 08:42:17 visual_prompt]: Epoch 37 / 100: avg data time: 6.15e-02, avg batch time: 0.4739, average train loss: 40.0270
[09/26 08:42:19 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1579, average loss: 51.0134
[09/26 08:42:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.50	
[09/26 08:42:19 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[09/26 08:42:25 visual_prompt]: Epoch 38 / 100: avg data time: 4.66e-02, avg batch time: 0.4601, average train loss: 47.0926
[09/26 08:42:27 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1576, average loss: 39.7628
[09/26 08:42:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 42.50	
[09/26 08:42:27 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[09/26 08:42:33 visual_prompt]: Epoch 39 / 100: avg data time: 5.85e-02, avg batch time: 0.4704, average train loss: 33.3699
[09/26 08:42:35 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1582, average loss: 43.8929
[09/26 08:42:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 61.00	
[09/26 08:42:35 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[09/26 08:42:41 visual_prompt]: Epoch 40 / 100: avg data time: 5.84e-02, avg batch time: 0.4712, average train loss: 31.0381
[09/26 08:42:43 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1580, average loss: 25.0507
[09/26 08:42:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 42.50	
[09/26 08:42:43 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[09/26 08:42:49 visual_prompt]: Epoch 41 / 100: avg data time: 6.37e-02, avg batch time: 0.4755, average train loss: 26.0010
[09/26 08:42:50 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1585, average loss: 24.6433
[09/26 08:42:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.00	
[09/26 08:42:51 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[09/26 08:42:57 visual_prompt]: Epoch 42 / 100: avg data time: 6.65e-02, avg batch time: 0.4786, average train loss: 24.7733
[09/26 08:42:59 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1582, average loss: 20.7203
[09/26 08:42:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 55.50	
[09/26 08:42:59 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[09/26 08:43:05 visual_prompt]: Epoch 43 / 100: avg data time: 5.23e-02, avg batch time: 0.4635, average train loss: 25.7025
[09/26 08:43:06 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1578, average loss: 20.0747
[09/26 08:43:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 58.50	
[09/26 08:43:06 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[09/26 08:43:13 visual_prompt]: Epoch 44 / 100: avg data time: 5.63e-02, avg batch time: 0.4675, average train loss: 23.7642
[09/26 08:43:14 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1582, average loss: 29.0460
[09/26 08:43:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 55.00	
[09/26 08:43:14 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[09/26 08:43:21 visual_prompt]: Epoch 45 / 100: avg data time: 6.18e-02, avg batch time: 0.4730, average train loss: 27.0208
[09/26 08:43:22 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1580, average loss: 22.3636
[09/26 08:43:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 43.50	
[09/26 08:43:22 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[09/26 08:43:29 visual_prompt]: Epoch 46 / 100: avg data time: 6.58e-02, avg batch time: 0.4773, average train loss: 20.5718
[09/26 08:43:31 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1584, average loss: 26.7891
[09/26 08:43:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 59.00	
[09/26 08:43:31 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[09/26 08:43:37 visual_prompt]: Epoch 47 / 100: avg data time: 5.22e-02, avg batch time: 0.4660, average train loss: 31.1909
[09/26 08:43:38 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1581, average loss: 40.6996
[09/26 08:43:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 56.00	
[09/26 08:43:38 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[09/26 08:43:45 visual_prompt]: Epoch 48 / 100: avg data time: 5.79e-02, avg batch time: 0.4697, average train loss: 38.1198
[09/26 08:43:46 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1583, average loss: 31.2900
[09/26 08:43:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 59.50	
[09/26 08:43:46 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[09/26 08:43:53 visual_prompt]: Epoch 49 / 100: avg data time: 5.80e-02, avg batch time: 0.4697, average train loss: 27.0252
[09/26 08:43:54 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1578, average loss: 30.9904
[09/26 08:43:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 08:43:54 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[09/26 08:44:01 visual_prompt]: Epoch 50 / 100: avg data time: 5.36e-02, avg batch time: 0.4665, average train loss: 28.4698
[09/26 08:44:02 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1579, average loss: 31.3418
[09/26 08:44:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/26 08:44:02 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[09/26 08:44:09 visual_prompt]: Epoch 51 / 100: avg data time: 5.96e-02, avg batch time: 0.4732, average train loss: 21.4889
[09/26 08:44:10 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1580, average loss: 26.5630
[09/26 08:44:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 39.00	
[09/26 08:44:10 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[09/26 08:44:17 visual_prompt]: Epoch 52 / 100: avg data time: 5.98e-02, avg batch time: 0.4714, average train loss: 30.4738
[09/26 08:44:18 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1581, average loss: 24.3511
[09/26 08:44:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 60.50	
[09/26 08:44:18 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[09/26 08:44:25 visual_prompt]: Epoch 53 / 100: avg data time: 5.62e-02, avg batch time: 0.4698, average train loss: 26.4908
[09/26 08:44:26 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1576, average loss: 20.5759
[09/26 08:44:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 41.50	
[09/26 08:44:26 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[09/26 08:44:32 visual_prompt]: Epoch 54 / 100: avg data time: 5.56e-02, avg batch time: 0.4673, average train loss: 15.6965
[09/26 08:44:34 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1584, average loss: 17.4498
[09/26 08:44:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 56.00	
[09/26 08:44:34 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[09/26 08:44:40 visual_prompt]: Epoch 55 / 100: avg data time: 4.84e-02, avg batch time: 0.4619, average train loss: 19.4098
[09/26 08:44:42 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1581, average loss: 15.3300
[09/26 08:44:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 46.00	
[09/26 08:44:42 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[09/26 08:44:48 visual_prompt]: Epoch 56 / 100: avg data time: 6.07e-02, avg batch time: 0.4739, average train loss: 18.2164
[09/26 08:44:50 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1581, average loss: 19.4472
[09/26 08:44:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 38.50	
[09/26 08:44:50 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[09/26 08:44:56 visual_prompt]: Epoch 57 / 100: avg data time: 5.68e-02, avg batch time: 0.4690, average train loss: 33.0762
[09/26 08:44:58 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1582, average loss: 24.6360
[09/26 08:44:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 46.00	
[09/26 08:44:58 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[09/26 08:45:04 visual_prompt]: Epoch 58 / 100: avg data time: 4.58e-02, avg batch time: 0.4575, average train loss: 19.7571
[09/26 08:45:06 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1586, average loss: 20.2318
[09/26 08:45:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 64.50	
[09/26 08:45:06 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[09/26 08:45:12 visual_prompt]: Epoch 59 / 100: avg data time: 6.28e-02, avg batch time: 0.4744, average train loss: 19.4741
[09/26 08:45:14 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1580, average loss: 34.1693
[09/26 08:45:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 43.50	
[09/26 08:45:14 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[09/26 08:45:20 visual_prompt]: Epoch 60 / 100: avg data time: 6.82e-02, avg batch time: 0.4800, average train loss: 25.4894
[09/26 08:45:22 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1578, average loss: 22.2671
[09/26 08:45:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 08:45:22 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[09/26 08:45:28 visual_prompt]: Epoch 61 / 100: avg data time: 4.73e-02, avg batch time: 0.4607, average train loss: 22.2490
[09/26 08:45:30 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1585, average loss: 13.1108
[09/26 08:45:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 53.50	
[09/26 08:45:30 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[09/26 08:45:36 visual_prompt]: Epoch 62 / 100: avg data time: 5.91e-02, avg batch time: 0.4705, average train loss: 17.6576
[09/26 08:45:38 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1577, average loss: 18.1674
[09/26 08:45:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 40.50	
[09/26 08:45:38 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[09/26 08:45:44 visual_prompt]: Epoch 63 / 100: avg data time: 5.97e-02, avg batch time: 0.4720, average train loss: 18.8847
[09/26 08:45:46 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1583, average loss: 17.9237
[09/26 08:45:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 08:45:46 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[09/26 08:45:52 visual_prompt]: Epoch 64 / 100: avg data time: 5.57e-02, avg batch time: 0.4685, average train loss: 18.5858
[09/26 08:45:54 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1580, average loss: 22.2589
[09/26 08:45:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 54.00	
[09/26 08:45:54 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[09/26 08:46:00 visual_prompt]: Epoch 65 / 100: avg data time: 6.42e-02, avg batch time: 0.4753, average train loss: 17.8788
[09/26 08:46:02 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1580, average loss: 22.6791
[09/26 08:46:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 08:46:02 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[09/26 08:46:08 visual_prompt]: Epoch 66 / 100: avg data time: 5.76e-02, avg batch time: 0.4689, average train loss: 18.7483
[09/26 08:46:10 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1582, average loss: 10.6580
[09/26 08:46:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 08:46:10 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[09/26 08:46:16 visual_prompt]: Epoch 67 / 100: avg data time: 6.27e-02, avg batch time: 0.4746, average train loss: 13.2286
[09/26 08:46:18 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1577, average loss: 18.0416
[09/26 08:46:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 46.50	
[09/26 08:46:18 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[09/26 08:46:24 visual_prompt]: Epoch 68 / 100: avg data time: 5.93e-02, avg batch time: 0.4721, average train loss: 13.9599
[09/26 08:46:26 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1586, average loss: 14.5823
[09/26 08:46:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 55.00	
[09/26 08:46:26 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[09/26 08:46:32 visual_prompt]: Epoch 69 / 100: avg data time: 5.89e-02, avg batch time: 0.4700, average train loss: 11.9123
[09/26 08:46:34 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1579, average loss: 11.0919
[09/26 08:46:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.50	
[09/26 08:46:34 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[09/26 08:46:40 visual_prompt]: Epoch 70 / 100: avg data time: 6.03e-02, avg batch time: 0.4716, average train loss: 7.4999
[09/26 08:46:42 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1581, average loss: 5.1961
[09/26 08:46:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.50	top5: 53.00	
[09/26 08:46:42 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[09/26 08:46:48 visual_prompt]: Epoch 71 / 100: avg data time: 5.17e-02, avg batch time: 0.4659, average train loss: 4.4871
[09/26 08:46:50 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1581, average loss: 5.6080
[09/26 08:46:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 54.00	
[09/26 08:46:50 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[09/26 08:46:56 visual_prompt]: Epoch 72 / 100: avg data time: 6.08e-02, avg batch time: 0.4720, average train loss: 3.0565
[09/26 08:46:58 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.1580, average loss: 2.5421
[09/26 08:46:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/26 08:46:58 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[09/26 08:47:04 visual_prompt]: Epoch 73 / 100: avg data time: 6.15e-02, avg batch time: 0.4737, average train loss: 2.6387
[09/26 08:47:06 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1577, average loss: 2.6479
[09/26 08:47:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.50	top5: 58.00	
[09/26 08:47:06 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[09/26 08:47:12 visual_prompt]: Epoch 74 / 100: avg data time: 5.97e-02, avg batch time: 0.4716, average train loss: 2.5159
[09/26 08:47:13 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1583, average loss: 2.3642
[09/26 08:47:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.00	
[09/26 08:47:13 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[09/26 08:47:20 visual_prompt]: Epoch 75 / 100: avg data time: 5.57e-02, avg batch time: 0.4672, average train loss: 2.3473
[09/26 08:47:21 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1581, average loss: 2.3369
[09/26 08:47:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 08:47:21 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[09/26 08:47:28 visual_prompt]: Epoch 76 / 100: avg data time: 6.06e-02, avg batch time: 0.4732, average train loss: 2.3698
[09/26 08:47:29 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1586, average loss: 2.4924
[09/26 08:47:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.00	
[09/26 08:47:29 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[09/26 08:47:36 visual_prompt]: Epoch 77 / 100: avg data time: 5.75e-02, avg batch time: 0.4688, average train loss: 2.3598
[09/26 08:47:37 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1582, average loss: 2.3771
[09/26 08:47:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 17.00	top5: 56.50	
[09/26 08:47:37 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[09/26 08:47:44 visual_prompt]: Epoch 78 / 100: avg data time: 5.97e-02, avg batch time: 0.4711, average train loss: 2.3492
[09/26 08:47:45 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1580, average loss: 2.4238
[09/26 08:47:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 08:47:45 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[09/26 08:47:52 visual_prompt]: Epoch 79 / 100: avg data time: 5.65e-02, avg batch time: 0.4702, average train loss: 2.3962
[09/26 08:47:53 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1583, average loss: 2.5967
[09/26 08:47:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 50.50	
[09/26 08:47:53 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[09/26 08:48:00 visual_prompt]: Epoch 80 / 100: avg data time: 5.54e-02, avg batch time: 0.4686, average train loss: 2.3583
[09/26 08:48:01 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1581, average loss: 2.3285
[09/26 08:48:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 08:48:01 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[09/26 08:48:08 visual_prompt]: Epoch 81 / 100: avg data time: 4.70e-02, avg batch time: 0.4606, average train loss: 2.3338
[09/26 08:48:09 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.1581, average loss: 2.2485
[09/26 08:48:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 18.00	top5: 65.00	
[09/26 08:48:09 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[09/26 08:48:16 visual_prompt]: Epoch 82 / 100: avg data time: 6.26e-02, avg batch time: 0.4747, average train loss: 2.2701
[09/26 08:48:17 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.1584, average loss: 2.4405
[09/26 08:48:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.00	
[09/26 08:48:17 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[09/26 08:48:24 visual_prompt]: Epoch 83 / 100: avg data time: 5.51e-02, avg batch time: 0.4684, average train loss: 2.3996
[09/26 08:48:25 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1586, average loss: 2.2549
[09/26 08:48:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 17.50	top5: 63.00	
[09/26 08:48:25 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[09/26 08:48:32 visual_prompt]: Epoch 84 / 100: avg data time: 5.77e-02, avg batch time: 0.4702, average train loss: 2.2820
[09/26 08:48:33 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1584, average loss: 2.2786
[09/26 08:48:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.00	
[09/26 08:48:33 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[09/26 08:48:40 visual_prompt]: Epoch 85 / 100: avg data time: 6.22e-02, avg batch time: 0.4747, average train loss: 2.3064
[09/26 08:48:41 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1578, average loss: 2.2989
[09/26 08:48:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 15.50	top5: 64.50	
[09/26 08:48:41 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[09/26 08:48:48 visual_prompt]: Epoch 86 / 100: avg data time: 4.67e-02, avg batch time: 0.4601, average train loss: 2.3317
[09/26 08:48:49 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1582, average loss: 2.2997
[09/26 08:48:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 08:48:49 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[09/26 08:48:56 visual_prompt]: Epoch 87 / 100: avg data time: 5.88e-02, avg batch time: 0.4716, average train loss: 2.2589
[09/26 08:48:57 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1582, average loss: 2.2682
[09/26 08:48:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 64.00	
[09/26 08:48:57 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[09/26 08:49:04 visual_prompt]: Epoch 88 / 100: avg data time: 6.52e-02, avg batch time: 0.4772, average train loss: 2.2692
[09/26 08:49:05 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1578, average loss: 2.2638
[09/26 08:49:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.00	
[09/26 08:49:05 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[09/26 08:49:12 visual_prompt]: Epoch 89 / 100: avg data time: 5.91e-02, avg batch time: 0.4722, average train loss: 2.2712
[09/26 08:49:13 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1581, average loss: 2.2224
[09/26 08:49:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 08:49:13 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[09/26 08:49:19 visual_prompt]: Epoch 90 / 100: avg data time: 4.93e-02, avg batch time: 0.4631, average train loss: 2.2531
[09/26 08:49:21 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1582, average loss: 2.2176
[09/26 08:49:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 08:49:21 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[09/26 08:49:27 visual_prompt]: Epoch 91 / 100: avg data time: 5.82e-02, avg batch time: 0.4725, average train loss: 2.2582
[09/26 08:49:29 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1580, average loss: 2.2192
[09/26 08:49:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 08:49:29 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[09/26 08:49:35 visual_prompt]: Epoch 92 / 100: avg data time: 5.51e-02, avg batch time: 0.4674, average train loss: 2.2411
[09/26 08:49:37 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1580, average loss: 2.2154
[09/26 08:49:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.00	
[09/26 08:49:37 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[09/26 08:49:44 visual_prompt]: Epoch 93 / 100: avg data time: 6.42e-02, avg batch time: 0.4762, average train loss: 2.2308
[09/26 08:49:45 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1579, average loss: 2.2323
[09/26 08:49:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.00	
[09/26 08:49:45 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[09/26 08:49:52 visual_prompt]: Epoch 94 / 100: avg data time: 6.27e-02, avg batch time: 0.4757, average train loss: 2.2171
[09/26 08:49:53 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1580, average loss: 2.2333
[09/26 08:49:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 08:49:53 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[09/26 08:49:59 visual_prompt]: Epoch 95 / 100: avg data time: 5.61e-02, avg batch time: 0.4679, average train loss: 2.2138
[09/26 08:50:01 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1581, average loss: 2.1990
[09/26 08:50:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 66.50	
[09/26 08:50:01 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[09/26 08:50:07 visual_prompt]: Epoch 96 / 100: avg data time: 5.02e-02, avg batch time: 0.4626, average train loss: 2.1937
[09/26 08:50:09 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1582, average loss: 2.1861
[09/26 08:50:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 68.00	
[09/26 08:50:09 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[09/26 08:50:15 visual_prompt]: Epoch 97 / 100: avg data time: 4.90e-02, avg batch time: 0.4639, average train loss: 2.1631
[09/26 08:50:17 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1580, average loss: 2.2187
[09/26 08:50:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 21.50	top5: 66.00	
[09/26 08:50:17 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[09/26 08:50:23 visual_prompt]: Epoch 98 / 100: avg data time: 5.92e-02, avg batch time: 0.4717, average train loss: 2.1485
[09/26 08:50:25 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1579, average loss: 2.1612
[09/26 08:50:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.00	top5: 69.00	
[09/26 08:50:25 visual_prompt]: Best epoch 98: best metric: 0.250
[09/26 08:50:25 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[09/26 08:50:31 visual_prompt]: Epoch 99 / 100: avg data time: 5.85e-02, avg batch time: 0.4714, average train loss: 2.1332
[09/26 08:50:33 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1583, average loss: 2.1797
[09/26 08:50:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 67.50	
[09/26 08:50:33 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[09/26 08:50:39 visual_prompt]: Epoch 100 / 100: avg data time: 5.59e-02, avg batch time: 0.4692, average train loss: 2.1245
[09/26 08:50:41 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1580, average loss: 2.1678
[09/26 08:50:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.00	top5: 68.50	
[09/26 08:50:41 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 08:50:41 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 08:50:41 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 08:50:41 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 08:50:41 visual_prompt]: Training with config:
[09/26 08:50:41 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr10.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 08:50:41 visual_prompt]: Loading training data...
[09/26 08:50:41 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 08:50:42 visual_prompt]: Number of images: 800
[09/26 08:50:42 visual_prompt]: Number of classes: 10 / 10
[09/26 08:50:42 visual_prompt]: Loading validation data...
[09/26 08:50:42 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 08:50:42 visual_prompt]: Number of images: 200
[09/26 08:50:42 visual_prompt]: Number of classes: 10 / 10
[09/26 08:50:42 visual_prompt]: Constructing models...
[09/26 08:50:45 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 08:50:45 visual_prompt]: tuned percent:0.543
[09/26 08:50:45 visual_prompt]: Device used for model: 0
[09/26 08:50:45 visual_prompt]: Setting up Evaluator...
[09/26 08:50:45 visual_prompt]: Setting up Trainer...
[09/26 08:50:45 visual_prompt]: 	Setting up the optimizer...
[09/26 08:50:45 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 08:50:51 visual_prompt]: Epoch 1 / 100: avg data time: 5.89e-02, avg batch time: 0.4775, average train loss: 2.6726
[09/26 08:50:53 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.1577, average loss: 2.6214
[09/26 08:50:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 08:50:53 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 08:50:53 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[09/26 08:50:59 visual_prompt]: Epoch 2 / 100: avg data time: 5.61e-02, avg batch time: 0.4689, average train loss: 6.9676
[09/26 08:51:01 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1575, average loss: 2.6334
[09/26 08:51:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 56.50	
[09/26 08:51:01 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[09/26 08:51:07 visual_prompt]: Epoch 3 / 100: avg data time: 5.52e-02, avg batch time: 0.4683, average train loss: 5.1624
[09/26 08:51:09 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1584, average loss: 3.2806
[09/26 08:51:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 19.00	top5: 53.50	
[09/26 08:51:09 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[09/26 08:51:15 visual_prompt]: Epoch 4 / 100: avg data time: 5.91e-02, avg batch time: 0.4716, average train loss: 4.6785
[09/26 08:51:17 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1581, average loss: 5.3393
[09/26 08:51:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.50	
[09/26 08:51:17 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[09/26 08:51:23 visual_prompt]: Epoch 5 / 100: avg data time: 5.62e-02, avg batch time: 0.4680, average train loss: 7.7137
[09/26 08:51:25 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1579, average loss: 8.7773
[09/26 08:51:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.50	
[09/26 08:51:25 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[09/26 08:51:31 visual_prompt]: Epoch 6 / 100: avg data time: 5.70e-02, avg batch time: 0.4693, average train loss: 15.3131
[09/26 08:51:33 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1580, average loss: 14.3980
[09/26 08:51:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.00	
[09/26 08:51:33 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[09/26 08:51:39 visual_prompt]: Epoch 7 / 100: avg data time: 6.29e-02, avg batch time: 0.4742, average train loss: 16.1807
[09/26 08:51:41 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1579, average loss: 6.6305
[09/26 08:51:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 57.50	
[09/26 08:51:41 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[09/26 08:51:47 visual_prompt]: Epoch 8 / 100: avg data time: 4.76e-02, avg batch time: 0.4598, average train loss: 13.7614
[09/26 08:51:48 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1578, average loss: 30.0878
[09/26 08:51:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 41.50	
[09/26 08:51:48 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[09/26 08:51:55 visual_prompt]: Epoch 9 / 100: avg data time: 4.95e-02, avg batch time: 0.4620, average train loss: 30.6827
[09/26 08:51:56 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1577, average loss: 20.1660
[09/26 08:51:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 08:51:56 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[09/26 08:52:03 visual_prompt]: Epoch 10 / 100: avg data time: 5.63e-02, avg batch time: 0.4689, average train loss: 26.3470
[09/26 08:52:04 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1581, average loss: 36.0252
[09/26 08:52:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 08:52:04 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[09/26 08:52:11 visual_prompt]: Epoch 11 / 100: avg data time: 5.85e-02, avg batch time: 0.4711, average train loss: 25.0265
[09/26 08:52:12 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1575, average loss: 22.2305
[09/26 08:52:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.50	
[09/26 08:52:12 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[09/26 08:52:19 visual_prompt]: Epoch 12 / 100: avg data time: 4.99e-02, avg batch time: 0.4639, average train loss: 22.2336
[09/26 08:52:20 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1580, average loss: 25.6575
[09/26 08:52:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 08:52:20 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[09/26 08:52:27 visual_prompt]: Epoch 13 / 100: avg data time: 5.13e-02, avg batch time: 0.4634, average train loss: 25.8506
[09/26 08:52:28 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1582, average loss: 19.8329
[09/26 08:52:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 08:52:28 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[09/26 08:52:34 visual_prompt]: Epoch 14 / 100: avg data time: 4.67e-02, avg batch time: 0.4601, average train loss: 25.2864
[09/26 08:52:36 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1581, average loss: 19.9040
[09/26 08:52:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 58.50	
[09/26 08:52:36 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[09/26 08:52:42 visual_prompt]: Epoch 15 / 100: avg data time: 4.92e-02, avg batch time: 0.4646, average train loss: 23.8587
[09/26 08:52:44 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1579, average loss: 18.7749
[09/26 08:52:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 58.00	
[09/26 08:52:44 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[09/26 08:52:50 visual_prompt]: Epoch 16 / 100: avg data time: 6.16e-02, avg batch time: 0.4733, average train loss: 23.9235
[09/26 08:52:52 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1583, average loss: 17.5806
[09/26 08:52:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/26 08:52:52 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[09/26 08:52:58 visual_prompt]: Epoch 17 / 100: avg data time: 6.15e-02, avg batch time: 0.4734, average train loss: 17.3770
[09/26 08:53:00 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1583, average loss: 18.0718
[09/26 08:53:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 08:53:00 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[09/26 08:53:06 visual_prompt]: Epoch 18 / 100: avg data time: 4.95e-02, avg batch time: 0.4614, average train loss: 15.0310
[09/26 08:53:08 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1580, average loss: 16.0688
[09/26 08:53:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 08:53:08 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[09/26 08:53:14 visual_prompt]: Epoch 19 / 100: avg data time: 6.73e-02, avg batch time: 0.4794, average train loss: 15.2362
[09/26 08:53:16 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1578, average loss: 13.5496
[09/26 08:53:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 60.50	
[09/26 08:53:16 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[09/26 08:53:22 visual_prompt]: Epoch 20 / 100: avg data time: 5.86e-02, avg batch time: 0.4705, average train loss: 13.7496
[09/26 08:53:24 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1584, average loss: 14.0205
[09/26 08:53:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 59.00	
[09/26 08:53:24 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[09/26 08:53:30 visual_prompt]: Epoch 21 / 100: avg data time: 4.88e-02, avg batch time: 0.4614, average train loss: 14.6440
[09/26 08:53:32 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1587, average loss: 13.8321
[09/26 08:53:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 48.00	
[09/26 08:53:32 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[09/26 08:53:38 visual_prompt]: Epoch 22 / 100: avg data time: 6.33e-02, avg batch time: 0.4750, average train loss: 15.8903
[09/26 08:53:40 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1581, average loss: 17.1581
[09/26 08:53:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 08:53:40 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[09/26 08:53:46 visual_prompt]: Epoch 23 / 100: avg data time: 4.71e-02, avg batch time: 0.4604, average train loss: 15.3575
[09/26 08:53:48 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1585, average loss: 19.8076
[09/26 08:53:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 56.50	
[09/26 08:53:48 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[09/26 08:53:54 visual_prompt]: Epoch 24 / 100: avg data time: 5.90e-02, avg batch time: 0.4710, average train loss: 22.6450
[09/26 08:53:56 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1585, average loss: 37.5006
[09/26 08:53:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 5.50	top5: 51.00	
[09/26 08:53:56 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[09/26 08:54:02 visual_prompt]: Epoch 25 / 100: avg data time: 5.55e-02, avg batch time: 0.4687, average train loss: 29.4794
[09/26 08:54:03 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1581, average loss: 27.9078
[09/26 08:54:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 08:54:03 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[09/26 08:54:10 visual_prompt]: Epoch 26 / 100: avg data time: 5.98e-02, avg batch time: 0.4729, average train loss: 28.7446
[09/26 08:54:11 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1581, average loss: 20.1314
[09/26 08:54:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 08:54:11 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[09/26 08:54:18 visual_prompt]: Epoch 27 / 100: avg data time: 6.11e-02, avg batch time: 0.4751, average train loss: 27.3828
[09/26 08:54:20 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1580, average loss: 25.2249
[09/26 08:54:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 08:54:20 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[09/26 08:54:26 visual_prompt]: Epoch 28 / 100: avg data time: 6.27e-02, avg batch time: 0.4757, average train loss: 22.3859
[09/26 08:54:28 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1579, average loss: 11.6112
[09/26 08:54:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 43.50	
[09/26 08:54:28 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[09/26 08:54:34 visual_prompt]: Epoch 29 / 100: avg data time: 6.06e-02, avg batch time: 0.4727, average train loss: 18.3942
[09/26 08:54:36 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1584, average loss: 24.4888
[09/26 08:54:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.50	
[09/26 08:54:36 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[09/26 08:54:42 visual_prompt]: Epoch 30 / 100: avg data time: 5.46e-02, avg batch time: 0.4677, average train loss: 18.8205
[09/26 08:54:44 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1575, average loss: 23.1055
[09/26 08:54:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 50.00	
[09/26 08:54:44 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[09/26 08:54:50 visual_prompt]: Epoch 31 / 100: avg data time: 5.87e-02, avg batch time: 0.4717, average train loss: 19.6853
[09/26 08:54:52 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1582, average loss: 17.1188
[09/26 08:54:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 64.00	
[09/26 08:54:52 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[09/26 08:54:58 visual_prompt]: Epoch 32 / 100: avg data time: 6.09e-02, avg batch time: 0.4742, average train loss: 18.6896
[09/26 08:55:00 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1577, average loss: 15.4593
[09/26 08:55:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 50.00	
[09/26 08:55:00 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[09/26 08:55:06 visual_prompt]: Epoch 33 / 100: avg data time: 5.99e-02, avg batch time: 0.4736, average train loss: 19.5034
[09/26 08:55:08 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1580, average loss: 15.2761
[09/26 08:55:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 61.50	
[09/26 08:55:08 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[09/26 08:55:14 visual_prompt]: Epoch 34 / 100: avg data time: 5.93e-02, avg batch time: 0.4725, average train loss: 16.6757
[09/26 08:55:16 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1583, average loss: 17.9679
[09/26 08:55:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.00	
[09/26 08:55:16 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[09/26 08:55:22 visual_prompt]: Epoch 35 / 100: avg data time: 5.97e-02, avg batch time: 0.4718, average train loss: 14.4937
[09/26 08:55:24 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1583, average loss: 14.9360
[09/26 08:55:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 56.50	
[09/26 08:55:24 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[09/26 08:55:30 visual_prompt]: Epoch 36 / 100: avg data time: 4.53e-02, avg batch time: 0.4602, average train loss: 15.7617
[09/26 08:55:32 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1581, average loss: 7.7225
[09/26 08:55:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.00	
[09/26 08:55:32 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[09/26 08:55:38 visual_prompt]: Epoch 37 / 100: avg data time: 5.68e-02, avg batch time: 0.4694, average train loss: 12.0344
[09/26 08:55:39 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1584, average loss: 9.5255
[09/26 08:55:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.00	
[09/26 08:55:39 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[09/26 08:55:46 visual_prompt]: Epoch 38 / 100: avg data time: 5.60e-02, avg batch time: 0.4697, average train loss: 8.8714
[09/26 08:55:47 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1584, average loss: 5.4026
[09/26 08:55:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 08:55:47 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[09/26 08:55:54 visual_prompt]: Epoch 39 / 100: avg data time: 5.30e-02, avg batch time: 0.4668, average train loss: 7.6520
[09/26 08:55:55 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1579, average loss: 11.8701
[09/26 08:55:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 51.50	
[09/26 08:55:55 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[09/26 08:56:02 visual_prompt]: Epoch 40 / 100: avg data time: 5.81e-02, avg batch time: 0.4713, average train loss: 7.8212
[09/26 08:56:03 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1577, average loss: 8.0342
[09/26 08:56:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 55.50	
[09/26 08:56:03 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[09/26 08:56:10 visual_prompt]: Epoch 41 / 100: avg data time: 5.17e-02, avg batch time: 0.4642, average train loss: 5.8767
[09/26 08:56:11 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1578, average loss: 4.5645
[09/26 08:56:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 43.50	
[09/26 08:56:11 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[09/26 08:56:18 visual_prompt]: Epoch 42 / 100: avg data time: 5.68e-02, avg batch time: 0.4702, average train loss: 4.2081
[09/26 08:56:19 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1585, average loss: 3.7739
[09/26 08:56:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 58.00	
[09/26 08:56:19 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[09/26 08:56:26 visual_prompt]: Epoch 43 / 100: avg data time: 5.27e-02, avg batch time: 0.4654, average train loss: 5.0066
[09/26 08:56:27 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1584, average loss: 5.3005
[09/26 08:56:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 08:56:27 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[09/26 08:56:34 visual_prompt]: Epoch 44 / 100: avg data time: 6.58e-02, avg batch time: 0.4779, average train loss: 4.6059
[09/26 08:56:35 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1584, average loss: 4.7201
[09/26 08:56:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/26 08:56:35 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[09/26 08:56:42 visual_prompt]: Epoch 45 / 100: avg data time: 6.22e-02, avg batch time: 0.4755, average train loss: 3.8496
[09/26 08:56:43 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1584, average loss: 4.4082
[09/26 08:56:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 56.50	
[09/26 08:56:43 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[09/26 08:56:50 visual_prompt]: Epoch 46 / 100: avg data time: 5.65e-02, avg batch time: 0.4713, average train loss: 3.6286
[09/26 08:56:51 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1582, average loss: 4.0798
[09/26 08:56:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 08:56:51 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[09/26 08:56:58 visual_prompt]: Epoch 47 / 100: avg data time: 5.22e-02, avg batch time: 0.4672, average train loss: 3.2746
[09/26 08:56:59 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1582, average loss: 2.8447
[09/26 08:56:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 56.50	
[09/26 08:56:59 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[09/26 08:57:06 visual_prompt]: Epoch 48 / 100: avg data time: 6.08e-02, avg batch time: 0.4731, average train loss: 2.9165
[09/26 08:57:07 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1580, average loss: 2.4594
[09/26 08:57:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 08:57:07 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[09/26 08:57:14 visual_prompt]: Epoch 49 / 100: avg data time: 5.74e-02, avg batch time: 0.4696, average train loss: 2.9099
[09/26 08:57:15 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1581, average loss: 2.3427
[09/26 08:57:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 08:57:15 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[09/26 08:57:22 visual_prompt]: Epoch 50 / 100: avg data time: 6.25e-02, avg batch time: 0.4751, average train loss: 2.5145
[09/26 08:57:23 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1583, average loss: 2.5851
[09/26 08:57:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 63.00	
[09/26 08:57:23 visual_prompt]: Best epoch 50: best metric: 0.235
[09/26 08:57:23 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[09/26 08:57:30 visual_prompt]: Epoch 51 / 100: avg data time: 6.22e-02, avg batch time: 0.4757, average train loss: 2.6303
[09/26 08:57:31 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1583, average loss: 2.6550
[09/26 08:57:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 63.50	
[09/26 08:57:31 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[09/26 08:57:38 visual_prompt]: Epoch 52 / 100: avg data time: 6.05e-02, avg batch time: 0.4731, average train loss: 2.4904
[09/26 08:57:39 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1586, average loss: 2.4275
[09/26 08:57:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 13.50	top5: 55.50	
[09/26 08:57:39 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[09/26 08:57:46 visual_prompt]: Epoch 53 / 100: avg data time: 5.95e-02, avg batch time: 0.4718, average train loss: 2.6064
[09/26 08:57:47 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1583, average loss: 2.4020
[09/26 08:57:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 08:57:47 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[09/26 08:57:54 visual_prompt]: Epoch 54 / 100: avg data time: 5.63e-02, avg batch time: 0.4702, average train loss: 2.6664
[09/26 08:57:55 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1584, average loss: 2.8658
[09/26 08:57:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.00	
[09/26 08:57:55 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[09/26 08:58:02 visual_prompt]: Epoch 55 / 100: avg data time: 5.70e-02, avg batch time: 0.4704, average train loss: 2.7625
[09/26 08:58:03 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 2.7216
[09/26 08:58:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 52.50	
[09/26 08:58:03 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[09/26 08:58:10 visual_prompt]: Epoch 56 / 100: avg data time: 5.60e-02, avg batch time: 0.4699, average train loss: 2.6407
[09/26 08:58:11 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1582, average loss: 2.8845
[09/26 08:58:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 30.00	top5: 61.00	
[09/26 08:58:11 visual_prompt]: Best epoch 56: best metric: 0.300
[09/26 08:58:11 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[09/26 08:58:18 visual_prompt]: Epoch 57 / 100: avg data time: 6.67e-02, avg batch time: 0.4795, average train loss: 2.8022
[09/26 08:58:19 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1574, average loss: 2.7473
[09/26 08:58:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 50.00	
[09/26 08:58:19 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[09/26 08:58:26 visual_prompt]: Epoch 58 / 100: avg data time: 5.65e-02, avg batch time: 0.4694, average train loss: 2.7072
[09/26 08:58:27 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1585, average loss: 2.2812
[09/26 08:58:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 31.50	top5: 67.00	
[09/26 08:58:27 visual_prompt]: Best epoch 58: best metric: 0.315
[09/26 08:58:27 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[09/26 08:58:34 visual_prompt]: Epoch 59 / 100: avg data time: 5.67e-02, avg batch time: 0.4700, average train loss: 2.0103
[09/26 08:58:35 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1581, average loss: 2.2938
[09/26 08:58:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 33.00	top5: 71.50	
[09/26 08:58:35 visual_prompt]: Best epoch 59: best metric: 0.330
[09/26 08:58:35 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[09/26 08:58:42 visual_prompt]: Epoch 60 / 100: avg data time: 5.76e-02, avg batch time: 0.4720, average train loss: 1.9360
[09/26 08:58:43 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1578, average loss: 1.4955
[09/26 08:58:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 55.00	top5: 88.00	
[09/26 08:58:43 visual_prompt]: Best epoch 60: best metric: 0.550
[09/26 08:58:43 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[09/26 08:58:50 visual_prompt]: Epoch 61 / 100: avg data time: 5.28e-02, avg batch time: 0.4670, average train loss: 2.3931
[09/26 08:58:51 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1584, average loss: 3.5692
[09/26 08:58:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 68.00	
[09/26 08:58:51 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[09/26 08:58:57 visual_prompt]: Epoch 62 / 100: avg data time: 5.01e-02, avg batch time: 0.4667, average train loss: 3.0448
[09/26 08:58:59 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1586, average loss: 1.8458
[09/26 08:58:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 41.00	top5: 79.50	
[09/26 08:58:59 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[09/26 08:59:05 visual_prompt]: Epoch 63 / 100: avg data time: 5.62e-02, avg batch time: 0.4703, average train loss: 1.5519
[09/26 08:59:07 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1583, average loss: 1.3838
[09/26 08:59:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 53.00	top5: 92.50	
[09/26 08:59:07 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[09/26 08:59:13 visual_prompt]: Epoch 64 / 100: avg data time: 5.66e-02, avg batch time: 0.4698, average train loss: 1.1667
[09/26 08:59:15 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1588, average loss: 1.7195
[09/26 08:59:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 43.00	top5: 91.00	
[09/26 08:59:15 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[09/26 08:59:22 visual_prompt]: Epoch 65 / 100: avg data time: 6.58e-02, avg batch time: 0.4786, average train loss: 1.3796
[09/26 08:59:23 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1586, average loss: 1.3933
[09/26 08:59:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 57.00	top5: 90.00	
[09/26 08:59:23 visual_prompt]: Best epoch 65: best metric: 0.570
[09/26 08:59:23 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[09/26 08:59:30 visual_prompt]: Epoch 66 / 100: avg data time: 6.24e-02, avg batch time: 0.4766, average train loss: 1.3472
[09/26 08:59:31 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1583, average loss: 1.2057
[09/26 08:59:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 59.00	top5: 95.00	
[09/26 08:59:31 visual_prompt]: Best epoch 66: best metric: 0.590
[09/26 08:59:31 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[09/26 08:59:38 visual_prompt]: Epoch 67 / 100: avg data time: 5.92e-02, avg batch time: 0.4731, average train loss: 1.0441
[09/26 08:59:39 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1581, average loss: 0.9607
[09/26 08:59:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 63.50	top5: 96.00	
[09/26 08:59:39 visual_prompt]: Best epoch 67: best metric: 0.635
[09/26 08:59:39 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[09/26 08:59:46 visual_prompt]: Epoch 68 / 100: avg data time: 4.82e-02, avg batch time: 0.4620, average train loss: 0.8463
[09/26 08:59:47 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1584, average loss: 0.9550
[09/26 08:59:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 64.50	top5: 97.50	
[09/26 08:59:47 visual_prompt]: Best epoch 68: best metric: 0.645
[09/26 08:59:47 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[09/26 08:59:53 visual_prompt]: Epoch 69 / 100: avg data time: 5.90e-02, avg batch time: 0.4728, average train loss: 0.7004
[09/26 08:59:55 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1583, average loss: 0.7511
[09/26 08:59:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.50	top5: 98.50	
[09/26 08:59:55 visual_prompt]: Best epoch 69: best metric: 0.775
[09/26 08:59:55 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[09/26 09:00:02 visual_prompt]: Epoch 70 / 100: avg data time: 6.46e-02, avg batch time: 0.4771, average train loss: 0.5338
[09/26 09:00:03 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1588, average loss: 0.8464
[09/26 09:00:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 96.50	
[09/26 09:00:03 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[09/26 09:00:10 visual_prompt]: Epoch 71 / 100: avg data time: 5.82e-02, avg batch time: 0.4711, average train loss: 0.5682
[09/26 09:00:11 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1581, average loss: 0.8608
[09/26 09:00:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 97.00	
[09/26 09:00:11 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[09/26 09:00:18 visual_prompt]: Epoch 72 / 100: avg data time: 6.82e-02, avg batch time: 0.4810, average train loss: 0.5181
[09/26 09:00:19 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1587, average loss: 0.9239
[09/26 09:00:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 95.50	
[09/26 09:00:19 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[09/26 09:00:26 visual_prompt]: Epoch 73 / 100: avg data time: 5.55e-02, avg batch time: 0.4694, average train loss: 0.5747
[09/26 09:00:27 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1585, average loss: 1.1084
[09/26 09:00:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.00	top5: 97.50	
[09/26 09:00:27 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[09/26 09:00:34 visual_prompt]: Epoch 74 / 100: avg data time: 5.42e-02, avg batch time: 0.4666, average train loss: 0.4349
[09/26 09:00:35 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1583, average loss: 1.0027
[09/26 09:00:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 97.00	
[09/26 09:00:35 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[09/26 09:00:42 visual_prompt]: Epoch 75 / 100: avg data time: 5.72e-02, avg batch time: 0.4700, average train loss: 0.3428
[09/26 09:00:43 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1585, average loss: 1.0533
[09/26 09:00:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.00	top5: 96.50	
[09/26 09:00:43 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[09/26 09:00:50 visual_prompt]: Epoch 76 / 100: avg data time: 5.73e-02, avg batch time: 0.4709, average train loss: 0.3473
[09/26 09:00:51 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1580, average loss: 0.8804
[09/26 09:00:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 09:00:51 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[09/26 09:00:58 visual_prompt]: Epoch 77 / 100: avg data time: 5.40e-02, avg batch time: 0.4663, average train loss: 0.1894
[09/26 09:00:59 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1579, average loss: 0.8608
[09/26 09:00:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 99.00	
[09/26 09:00:59 visual_prompt]: Best epoch 77: best metric: 0.795
[09/26 09:00:59 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[09/26 09:01:06 visual_prompt]: Epoch 78 / 100: avg data time: 6.48e-02, avg batch time: 0.4770, average train loss: 0.1682
[09/26 09:01:07 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1587, average loss: 1.2723
[09/26 09:01:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 97.50	
[09/26 09:01:07 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[09/26 09:01:14 visual_prompt]: Epoch 79 / 100: avg data time: 5.24e-02, avg batch time: 0.4667, average train loss: 0.1896
[09/26 09:01:15 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1582, average loss: 0.7328
[09/26 09:01:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.50	
[09/26 09:01:15 visual_prompt]: Best epoch 79: best metric: 0.800
[09/26 09:01:15 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[09/26 09:01:22 visual_prompt]: Epoch 80 / 100: avg data time: 5.53e-02, avg batch time: 0.4684, average train loss: 0.1052
[09/26 09:01:23 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1584, average loss: 0.9846
[09/26 09:01:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 96.50	
[09/26 09:01:23 visual_prompt]: Best epoch 80: best metric: 0.815
[09/26 09:01:23 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[09/26 09:01:30 visual_prompt]: Epoch 81 / 100: avg data time: 5.82e-02, avg batch time: 0.4713, average train loss: 0.0879
[09/26 09:01:31 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1584, average loss: 0.9389
[09/26 09:01:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.50	top5: 98.00	
[09/26 09:01:31 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[09/26 09:01:38 visual_prompt]: Epoch 82 / 100: avg data time: 5.36e-02, avg batch time: 0.4663, average train loss: 0.0560
[09/26 09:01:39 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1585, average loss: 1.1955
[09/26 09:01:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.50	top5: 97.50	
[09/26 09:01:39 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[09/26 09:01:45 visual_prompt]: Epoch 83 / 100: avg data time: 5.59e-02, avg batch time: 0.4677, average train loss: 0.0564
[09/26 09:01:47 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1580, average loss: 1.1980
[09/26 09:01:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 97.00	
[09/26 09:01:47 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[09/26 09:01:53 visual_prompt]: Epoch 84 / 100: avg data time: 6.06e-02, avg batch time: 0.4726, average train loss: 0.0300
[09/26 09:01:55 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1582, average loss: 1.3086
[09/26 09:01:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 98.00	
[09/26 09:01:55 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[09/26 09:02:01 visual_prompt]: Epoch 85 / 100: avg data time: 4.61e-02, avg batch time: 0.4599, average train loss: 0.0104
[09/26 09:02:03 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1582, average loss: 1.4632
[09/26 09:02:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 97.50	
[09/26 09:02:03 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[09/26 09:02:09 visual_prompt]: Epoch 86 / 100: avg data time: 5.67e-02, avg batch time: 0.4690, average train loss: 0.0074
[09/26 09:02:11 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.1583, average loss: 1.3633
[09/26 09:02:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.50	
[09/26 09:02:11 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[09/26 09:02:18 visual_prompt]: Epoch 87 / 100: avg data time: 6.78e-02, avg batch time: 0.4800, average train loss: 0.0128
[09/26 09:02:19 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1583, average loss: 1.4003
[09/26 09:02:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.50	top5: 97.00	
[09/26 09:02:19 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[09/26 09:02:25 visual_prompt]: Epoch 88 / 100: avg data time: 5.21e-02, avg batch time: 0.4659, average train loss: 0.0117
[09/26 09:02:27 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1584, average loss: 1.2551
[09/26 09:02:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 09:02:27 visual_prompt]: Best epoch 88: best metric: 0.835
[09/26 09:02:27 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[09/26 09:02:33 visual_prompt]: Epoch 89 / 100: avg data time: 6.11e-02, avg batch time: 0.4728, average train loss: 0.0064
[09/26 09:02:35 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1587, average loss: 1.1742
[09/26 09:02:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 97.00	
[09/26 09:02:35 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[09/26 09:02:41 visual_prompt]: Epoch 90 / 100: avg data time: 4.74e-02, avg batch time: 0.4626, average train loss: 0.0024
[09/26 09:02:43 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1580, average loss: 1.2157
[09/26 09:02:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 97.50	
[09/26 09:02:43 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[09/26 09:02:49 visual_prompt]: Epoch 91 / 100: avg data time: 5.84e-02, avg batch time: 0.4727, average train loss: 0.0019
[09/26 09:02:51 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1581, average loss: 1.2109
[09/26 09:02:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 98.00	
[09/26 09:02:51 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[09/26 09:02:57 visual_prompt]: Epoch 92 / 100: avg data time: 6.00e-02, avg batch time: 0.4719, average train loss: 0.0013
[09/26 09:02:59 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1581, average loss: 1.2073
[09/26 09:02:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.00	
[09/26 09:02:59 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[09/26 09:03:05 visual_prompt]: Epoch 93 / 100: avg data time: 5.49e-02, avg batch time: 0.4666, average train loss: 0.0008
[09/26 09:03:07 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1583, average loss: 1.2013
[09/26 09:03:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 97.50	
[09/26 09:03:07 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[09/26 09:03:13 visual_prompt]: Epoch 94 / 100: avg data time: 6.03e-02, avg batch time: 0.4728, average train loss: 0.0010
[09/26 09:03:15 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1583, average loss: 1.1974
[09/26 09:03:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 97.50	
[09/26 09:03:15 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[09/26 09:03:21 visual_prompt]: Epoch 95 / 100: avg data time: 4.78e-02, avg batch time: 0.4606, average train loss: 0.0008
[09/26 09:03:23 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1585, average loss: 1.1968
[09/26 09:03:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.00	
[09/26 09:03:23 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[09/26 09:03:29 visual_prompt]: Epoch 96 / 100: avg data time: 6.38e-02, avg batch time: 0.4757, average train loss: 0.0018
[09/26 09:03:31 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1576, average loss: 1.1970
[09/26 09:03:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.00	
[09/26 09:03:31 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[09/26 09:03:37 visual_prompt]: Epoch 97 / 100: avg data time: 5.97e-02, avg batch time: 0.4717, average train loss: 0.0006
[09/26 09:03:39 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1581, average loss: 1.1966
[09/26 09:03:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.00	
[09/26 09:03:39 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[09/26 09:03:45 visual_prompt]: Epoch 98 / 100: avg data time: 5.77e-02, avg batch time: 0.4720, average train loss: 0.0013
[09/26 09:03:47 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1582, average loss: 1.1963
[09/26 09:03:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.00	
[09/26 09:03:47 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[09/26 09:03:53 visual_prompt]: Epoch 99 / 100: avg data time: 5.07e-02, avg batch time: 0.4641, average train loss: 0.0013
[09/26 09:03:55 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1580, average loss: 1.1963
[09/26 09:03:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.00	
[09/26 09:03:55 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[09/26 09:04:01 visual_prompt]: Epoch 100 / 100: avg data time: 5.43e-02, avg batch time: 0.4675, average train loss: 0.0006
[09/26 09:04:03 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1577, average loss: 1.1962
[09/26 09:04:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.00	
[09/26 09:04:03 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 09:04:03 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 09:04:03 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 09:04:03 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 09:04:03 visual_prompt]: Training with config:
[09/26 09:04:03 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr10.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 09:04:03 visual_prompt]: Loading training data...
[09/26 09:04:03 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 09:04:04 visual_prompt]: Number of images: 800
[09/26 09:04:04 visual_prompt]: Number of classes: 10 / 10
[09/26 09:04:04 visual_prompt]: Loading validation data...
[09/26 09:04:04 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 09:04:04 visual_prompt]: Number of images: 200
[09/26 09:04:04 visual_prompt]: Number of classes: 10 / 10
[09/26 09:04:04 visual_prompt]: Constructing models...
[09/26 09:04:06 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 09:04:06 visual_prompt]: tuned percent:0.543
[09/26 09:04:06 visual_prompt]: Device used for model: 0
[09/26 09:04:06 visual_prompt]: Setting up Evaluator...
[09/26 09:04:06 visual_prompt]: Setting up Trainer...
[09/26 09:04:06 visual_prompt]: 	Setting up the optimizer...
[09/26 09:04:06 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 09:04:13 visual_prompt]: Epoch 1 / 100: avg data time: 5.55e-02, avg batch time: 0.4751, average train loss: 2.6837
[09/26 09:04:14 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1577, average loss: 2.6214
[09/26 09:04:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 09:04:14 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 09:04:14 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[09/26 09:04:21 visual_prompt]: Epoch 2 / 100: avg data time: 5.57e-02, avg batch time: 0.4667, average train loss: 6.5865
[09/26 09:04:22 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1580, average loss: 3.8992
[09/26 09:04:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 09:04:22 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[09/26 09:04:29 visual_prompt]: Epoch 3 / 100: avg data time: 5.82e-02, avg batch time: 0.4708, average train loss: 3.3998
[09/26 09:04:30 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1582, average loss: 2.9653
[09/26 09:04:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.00	top5: 59.50	
[09/26 09:04:30 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[09/26 09:04:37 visual_prompt]: Epoch 4 / 100: avg data time: 5.58e-02, avg batch time: 0.4670, average train loss: 4.2701
[09/26 09:04:38 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1578, average loss: 5.1855
[09/26 09:04:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 50.00	
[09/26 09:04:38 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[09/26 09:04:45 visual_prompt]: Epoch 5 / 100: avg data time: 4.72e-02, avg batch time: 0.4587, average train loss: 8.0734
[09/26 09:04:46 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1583, average loss: 9.0916
[09/26 09:04:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.00	
[09/26 09:04:46 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[09/26 09:04:53 visual_prompt]: Epoch 6 / 100: avg data time: 5.14e-02, avg batch time: 0.4643, average train loss: 9.8680
[09/26 09:04:54 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.1579, average loss: 12.1378
[09/26 09:04:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 54.00	
[09/26 09:04:54 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[09/26 09:05:00 visual_prompt]: Epoch 7 / 100: avg data time: 4.72e-02, avg batch time: 0.4584, average train loss: 11.4777
[09/26 09:05:02 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1583, average loss: 13.5118
[09/26 09:05:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 52.50	
[09/26 09:05:02 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[09/26 09:05:08 visual_prompt]: Epoch 8 / 100: avg data time: 6.12e-02, avg batch time: 0.4734, average train loss: 16.2280
[09/26 09:05:10 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1581, average loss: 10.5099
[09/26 09:05:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 09:05:10 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[09/26 09:05:16 visual_prompt]: Epoch 9 / 100: avg data time: 6.09e-02, avg batch time: 0.4729, average train loss: 16.9528
[09/26 09:05:18 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1579, average loss: 22.3181
[09/26 09:05:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 43.00	
[09/26 09:05:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[09/26 09:05:24 visual_prompt]: Epoch 10 / 100: avg data time: 6.44e-02, avg batch time: 0.4770, average train loss: 18.8588
[09/26 09:05:26 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1579, average loss: 22.7647
[09/26 09:05:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 49.00	
[09/26 09:05:26 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[09/26 09:05:32 visual_prompt]: Epoch 11 / 100: avg data time: 5.64e-02, avg batch time: 0.4679, average train loss: 27.1825
[09/26 09:05:34 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1584, average loss: 22.9756
[09/26 09:05:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 52.00	
[09/26 09:05:34 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[09/26 09:05:40 visual_prompt]: Epoch 12 / 100: avg data time: 4.89e-02, avg batch time: 0.4621, average train loss: 27.8746
[09/26 09:05:42 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1582, average loss: 50.5592
[09/26 09:05:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 09:05:42 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[09/26 09:05:48 visual_prompt]: Epoch 13 / 100: avg data time: 6.06e-02, avg batch time: 0.4717, average train loss: 30.8141
[09/26 09:05:50 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1579, average loss: 45.4699
[09/26 09:05:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 09:05:50 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[09/26 09:05:56 visual_prompt]: Epoch 14 / 100: avg data time: 5.93e-02, avg batch time: 0.4704, average train loss: 33.1960
[09/26 09:05:58 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1587, average loss: 28.4859
[09/26 09:05:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.50	
[09/26 09:05:58 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[09/26 09:06:04 visual_prompt]: Epoch 15 / 100: avg data time: 6.01e-02, avg batch time: 0.4711, average train loss: 27.2827
[09/26 09:06:06 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1582, average loss: 18.9902
[09/26 09:06:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 56.50	
[09/26 09:06:06 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[09/26 09:06:12 visual_prompt]: Epoch 16 / 100: avg data time: 5.10e-02, avg batch time: 0.4636, average train loss: 21.4695
[09/26 09:06:14 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1583, average loss: 25.8933
[09/26 09:06:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.00	
[09/26 09:06:14 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[09/26 09:06:20 visual_prompt]: Epoch 17 / 100: avg data time: 6.16e-02, avg batch time: 0.4726, average train loss: 23.1070
[09/26 09:06:22 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1581, average loss: 18.5034
[09/26 09:06:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 59.50	
[09/26 09:06:22 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[09/26 09:06:28 visual_prompt]: Epoch 18 / 100: avg data time: 6.31e-02, avg batch time: 0.4742, average train loss: 20.8733
[09/26 09:06:30 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1582, average loss: 20.7077
[09/26 09:06:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.50	
[09/26 09:06:30 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[09/26 09:06:36 visual_prompt]: Epoch 19 / 100: avg data time: 6.38e-02, avg batch time: 0.4753, average train loss: 21.2405
[09/26 09:06:38 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1584, average loss: 24.4880
[09/26 09:06:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 59.00	
[09/26 09:06:38 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[09/26 09:06:44 visual_prompt]: Epoch 20 / 100: avg data time: 5.77e-02, avg batch time: 0.4690, average train loss: 21.5283
[09/26 09:06:46 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1584, average loss: 17.6570
[09/26 09:06:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 47.50	
[09/26 09:06:46 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[09/26 09:06:52 visual_prompt]: Epoch 21 / 100: avg data time: 4.67e-02, avg batch time: 0.4588, average train loss: 19.5463
[09/26 09:06:54 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1577, average loss: 11.5758
[09/26 09:06:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 56.50	
[09/26 09:06:54 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[09/26 09:07:00 visual_prompt]: Epoch 22 / 100: avg data time: 6.34e-02, avg batch time: 0.4752, average train loss: 12.1889
[09/26 09:07:02 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1580, average loss: 11.6631
[09/26 09:07:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 45.00	
[09/26 09:07:02 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[09/26 09:07:08 visual_prompt]: Epoch 23 / 100: avg data time: 5.74e-02, avg batch time: 0.4697, average train loss: 13.4816
[09/26 09:07:10 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1578, average loss: 16.6060
[09/26 09:07:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 45.50	
[09/26 09:07:10 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[09/26 09:07:16 visual_prompt]: Epoch 24 / 100: avg data time: 5.46e-02, avg batch time: 0.4661, average train loss: 13.2448
[09/26 09:07:18 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1581, average loss: 17.7411
[09/26 09:07:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.00	
[09/26 09:07:18 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[09/26 09:07:24 visual_prompt]: Epoch 25 / 100: avg data time: 6.29e-02, avg batch time: 0.4747, average train loss: 13.6614
[09/26 09:07:26 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1579, average loss: 12.7786
[09/26 09:07:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 58.00	
[09/26 09:07:26 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[09/26 09:07:32 visual_prompt]: Epoch 26 / 100: avg data time: 5.83e-02, avg batch time: 0.4699, average train loss: 12.3985
[09/26 09:07:33 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1584, average loss: 13.1215
[09/26 09:07:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 43.00	
[09/26 09:07:33 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[09/26 09:07:40 visual_prompt]: Epoch 27 / 100: avg data time: 6.17e-02, avg batch time: 0.4731, average train loss: 10.9784
[09/26 09:07:42 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1582, average loss: 14.2301
[09/26 09:07:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 56.00	
[09/26 09:07:42 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[09/26 09:07:48 visual_prompt]: Epoch 28 / 100: avg data time: 5.80e-02, avg batch time: 0.4704, average train loss: 14.9061
[09/26 09:07:50 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1586, average loss: 11.3549
[09/26 09:07:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 49.50	
[09/26 09:07:50 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[09/26 09:07:56 visual_prompt]: Epoch 29 / 100: avg data time: 5.04e-02, avg batch time: 0.4627, average train loss: 13.3366
[09/26 09:07:57 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1583, average loss: 15.8731
[09/26 09:07:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 56.50	
[09/26 09:07:57 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[09/26 09:08:04 visual_prompt]: Epoch 30 / 100: avg data time: 5.80e-02, avg batch time: 0.4721, average train loss: 11.4308
[09/26 09:08:05 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1584, average loss: 12.3702
[09/26 09:08:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.50	
[09/26 09:08:05 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[09/26 09:08:12 visual_prompt]: Epoch 31 / 100: avg data time: 5.64e-02, avg batch time: 0.4715, average train loss: 11.0788
[09/26 09:08:13 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1585, average loss: 14.7379
[09/26 09:08:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.00	
[09/26 09:08:13 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[09/26 09:08:20 visual_prompt]: Epoch 32 / 100: avg data time: 6.79e-02, avg batch time: 0.4810, average train loss: 13.0140
[09/26 09:08:21 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1583, average loss: 6.7169
[09/26 09:08:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.50	top5: 56.00	
[09/26 09:08:21 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[09/26 09:08:28 visual_prompt]: Epoch 33 / 100: avg data time: 5.57e-02, avg batch time: 0.4686, average train loss: 6.8681
[09/26 09:08:29 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1582, average loss: 6.4758
[09/26 09:08:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 56.50	
[09/26 09:08:29 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[09/26 09:08:36 visual_prompt]: Epoch 34 / 100: avg data time: 5.75e-02, avg batch time: 0.4695, average train loss: 6.9997
[09/26 09:08:37 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1581, average loss: 7.7295
[09/26 09:08:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 53.50	
[09/26 09:08:37 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[09/26 09:08:44 visual_prompt]: Epoch 35 / 100: avg data time: 5.08e-02, avg batch time: 0.4656, average train loss: 8.8511
[09/26 09:08:45 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.1583, average loss: 6.3303
[09/26 09:08:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.50	
[09/26 09:08:45 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[09/26 09:08:52 visual_prompt]: Epoch 36 / 100: avg data time: 5.83e-02, avg batch time: 0.4729, average train loss: 8.5787
[09/26 09:08:53 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1584, average loss: 6.8544
[09/26 09:08:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 09:08:53 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[09/26 09:09:00 visual_prompt]: Epoch 37 / 100: avg data time: 5.57e-02, avg batch time: 0.4691, average train loss: 5.7975
[09/26 09:09:01 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1581, average loss: 5.0931
[09/26 09:09:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.00	
[09/26 09:09:01 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[09/26 09:09:08 visual_prompt]: Epoch 38 / 100: avg data time: 5.82e-02, avg batch time: 0.4703, average train loss: 7.3835
[09/26 09:09:09 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.1585, average loss: 6.9994
[09/26 09:09:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 52.50	
[09/26 09:09:09 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[09/26 09:09:16 visual_prompt]: Epoch 39 / 100: avg data time: 6.03e-02, avg batch time: 0.4727, average train loss: 5.7579
[09/26 09:09:17 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1582, average loss: 5.8319
[09/26 09:09:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 54.00	
[09/26 09:09:17 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[09/26 09:09:24 visual_prompt]: Epoch 40 / 100: avg data time: 4.93e-02, avg batch time: 0.4617, average train loss: 5.4830
[09/26 09:09:25 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1580, average loss: 5.9393
[09/26 09:09:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.50	top5: 58.00	
[09/26 09:09:25 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[09/26 09:09:32 visual_prompt]: Epoch 41 / 100: avg data time: 6.08e-02, avg batch time: 0.4739, average train loss: 7.0241
[09/26 09:09:33 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1583, average loss: 8.5617
[09/26 09:09:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 09:09:33 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[09/26 09:09:40 visual_prompt]: Epoch 42 / 100: avg data time: 5.62e-02, avg batch time: 0.4688, average train loss: 6.1057
[09/26 09:09:41 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1580, average loss: 4.8517
[09/26 09:09:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.00	
[09/26 09:09:41 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[09/26 09:09:48 visual_prompt]: Epoch 43 / 100: avg data time: 6.00e-02, avg batch time: 0.4719, average train loss: 4.1798
[09/26 09:09:49 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1582, average loss: 3.1952
[09/26 09:09:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 44.50	
[09/26 09:09:49 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[09/26 09:09:56 visual_prompt]: Epoch 44 / 100: avg data time: 5.41e-02, avg batch time: 0.4670, average train loss: 3.7013
[09/26 09:09:57 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1582, average loss: 3.2908
[09/26 09:09:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 41.50	
[09/26 09:09:57 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[09/26 09:10:04 visual_prompt]: Epoch 45 / 100: avg data time: 6.46e-02, avg batch time: 0.4767, average train loss: 3.1269
[09/26 09:10:05 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1584, average loss: 3.3604
[09/26 09:10:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 36.50	
[09/26 09:10:05 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[09/26 09:10:12 visual_prompt]: Epoch 46 / 100: avg data time: 6.12e-02, avg batch time: 0.4731, average train loss: 3.9234
[09/26 09:10:13 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1583, average loss: 3.7962
[09/26 09:10:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.00	
[09/26 09:10:13 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[09/26 09:10:20 visual_prompt]: Epoch 47 / 100: avg data time: 5.87e-02, avg batch time: 0.4706, average train loss: 3.4939
[09/26 09:10:21 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1584, average loss: 3.2927
[09/26 09:10:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.00	
[09/26 09:10:21 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[09/26 09:10:28 visual_prompt]: Epoch 48 / 100: avg data time: 5.83e-02, avg batch time: 0.4706, average train loss: 2.8698
[09/26 09:10:29 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1584, average loss: 2.4664
[09/26 09:10:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 09:10:29 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[09/26 09:10:36 visual_prompt]: Epoch 49 / 100: avg data time: 5.96e-02, avg batch time: 0.4713, average train loss: 2.9615
[09/26 09:10:37 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1581, average loss: 3.1053
[09/26 09:10:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.00	top5: 58.00	
[09/26 09:10:37 visual_prompt]: Best epoch 49: best metric: 0.240
[09/26 09:10:37 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[09/26 09:10:44 visual_prompt]: Epoch 50 / 100: avg data time: 6.36e-02, avg batch time: 0.4776, average train loss: 2.8281
[09/26 09:10:45 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1576, average loss: 2.8073
[09/26 09:10:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 59.50	
[09/26 09:10:45 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[09/26 09:10:52 visual_prompt]: Epoch 51 / 100: avg data time: 6.64e-02, avg batch time: 0.4797, average train loss: 2.7465
[09/26 09:10:53 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1584, average loss: 2.9361
[09/26 09:10:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 48.00	
[09/26 09:10:53 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[09/26 09:11:00 visual_prompt]: Epoch 52 / 100: avg data time: 5.91e-02, avg batch time: 0.4720, average train loss: 2.6833
[09/26 09:11:01 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1582, average loss: 2.7147
[09/26 09:11:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.00	
[09/26 09:11:01 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[09/26 09:11:08 visual_prompt]: Epoch 53 / 100: avg data time: 5.40e-02, avg batch time: 0.4661, average train loss: 2.7060
[09/26 09:11:09 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1580, average loss: 2.7505
[09/26 09:11:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 56.50	
[09/26 09:11:09 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[09/26 09:11:16 visual_prompt]: Epoch 54 / 100: avg data time: 5.44e-02, avg batch time: 0.4677, average train loss: 2.7942
[09/26 09:11:17 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1583, average loss: 2.4974
[09/26 09:11:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 09:11:17 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[09/26 09:11:24 visual_prompt]: Epoch 55 / 100: avg data time: 5.68e-02, avg batch time: 0.4686, average train loss: 2.8307
[09/26 09:11:25 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1579, average loss: 2.6130
[09/26 09:11:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 53.50	
[09/26 09:11:25 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[09/26 09:11:32 visual_prompt]: Epoch 56 / 100: avg data time: 6.09e-02, avg batch time: 0.4726, average train loss: 2.5754
[09/26 09:11:33 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1584, average loss: 2.4616
[09/26 09:11:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 64.50	
[09/26 09:11:33 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[09/26 09:11:40 visual_prompt]: Epoch 57 / 100: avg data time: 5.96e-02, avg batch time: 0.4721, average train loss: 2.6427
[09/26 09:11:41 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.1581, average loss: 2.5293
[09/26 09:11:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 45.50	
[09/26 09:11:41 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[09/26 09:11:48 visual_prompt]: Epoch 58 / 100: avg data time: 5.90e-02, avg batch time: 0.4714, average train loss: 2.6463
[09/26 09:11:49 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1575, average loss: 2.5334
[09/26 09:11:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 56.50	
[09/26 09:11:49 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[09/26 09:11:56 visual_prompt]: Epoch 59 / 100: avg data time: 5.63e-02, avg batch time: 0.4689, average train loss: 2.5885
[09/26 09:11:57 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1581, average loss: 2.5665
[09/26 09:11:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 59.00	
[09/26 09:11:57 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[09/26 09:12:04 visual_prompt]: Epoch 60 / 100: avg data time: 6.11e-02, avg batch time: 0.4737, average train loss: 2.5800
[09/26 09:12:05 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.1578, average loss: 3.0158
[09/26 09:12:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 13.00	top5: 47.50	
[09/26 09:12:05 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[09/26 09:12:12 visual_prompt]: Epoch 61 / 100: avg data time: 5.39e-02, avg batch time: 0.4657, average train loss: 2.6092
[09/26 09:12:13 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1585, average loss: 2.4958
[09/26 09:12:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 56.00	
[09/26 09:12:13 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[09/26 09:12:19 visual_prompt]: Epoch 62 / 100: avg data time: 5.03e-02, avg batch time: 0.4630, average train loss: 2.5168
[09/26 09:12:21 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1582, average loss: 2.3943
[09/26 09:12:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 09:12:21 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[09/26 09:12:27 visual_prompt]: Epoch 63 / 100: avg data time: 5.86e-02, avg batch time: 0.4709, average train loss: 2.4219
[09/26 09:12:29 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1584, average loss: 2.3470
[09/26 09:12:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 09:12:29 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[09/26 09:12:35 visual_prompt]: Epoch 64 / 100: avg data time: 5.73e-02, avg batch time: 0.4689, average train loss: 2.5015
[09/26 09:12:37 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1582, average loss: 2.7599
[09/26 09:12:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.00	
[09/26 09:12:37 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[09/26 09:12:43 visual_prompt]: Epoch 65 / 100: avg data time: 5.25e-02, avg batch time: 0.4641, average train loss: 2.5693
[09/26 09:12:45 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.1584, average loss: 2.4272
[09/26 09:12:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.50	top5: 57.00	
[09/26 09:12:45 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[09/26 09:12:51 visual_prompt]: Epoch 66 / 100: avg data time: 6.10e-02, avg batch time: 0.4727, average train loss: 2.4715
[09/26 09:12:53 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1575, average loss: 2.5938
[09/26 09:12:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 09:12:53 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[09/26 09:12:59 visual_prompt]: Epoch 67 / 100: avg data time: 6.22e-02, avg batch time: 0.4748, average train loss: 2.4434
[09/26 09:13:01 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1582, average loss: 2.5100
[09/26 09:13:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 58.50	
[09/26 09:13:01 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[09/26 09:13:07 visual_prompt]: Epoch 68 / 100: avg data time: 6.36e-02, avg batch time: 0.4753, average train loss: 2.3741
[09/26 09:13:09 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1581, average loss: 2.4578
[09/26 09:13:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 45.00	
[09/26 09:13:09 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[09/26 09:13:16 visual_prompt]: Epoch 69 / 100: avg data time: 6.48e-02, avg batch time: 0.4764, average train loss: 2.3250
[09/26 09:13:17 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1583, average loss: 2.3980
[09/26 09:13:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 62.50	
[09/26 09:13:17 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[09/26 09:13:23 visual_prompt]: Epoch 70 / 100: avg data time: 4.79e-02, avg batch time: 0.4620, average train loss: 2.3597
[09/26 09:13:25 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1582, average loss: 2.3564
[09/26 09:13:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.50	top5: 64.00	
[09/26 09:13:25 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[09/26 09:13:31 visual_prompt]: Epoch 71 / 100: avg data time: 4.74e-02, avg batch time: 0.4593, average train loss: 2.3954
[09/26 09:13:33 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1582, average loss: 2.3724
[09/26 09:13:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 16.50	top5: 60.00	
[09/26 09:13:33 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[09/26 09:13:39 visual_prompt]: Epoch 72 / 100: avg data time: 5.83e-02, avg batch time: 0.4709, average train loss: 2.3984
[09/26 09:13:41 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1579, average loss: 2.4247
[09/26 09:13:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.00	
[09/26 09:13:41 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[09/26 09:13:47 visual_prompt]: Epoch 73 / 100: avg data time: 5.12e-02, avg batch time: 0.4638, average train loss: 2.3684
[09/26 09:13:49 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1578, average loss: 2.5134
[09/26 09:13:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 52.50	
[09/26 09:13:49 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[09/26 09:13:55 visual_prompt]: Epoch 74 / 100: avg data time: 6.51e-02, avg batch time: 0.4776, average train loss: 2.4125
[09/26 09:13:57 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1579, average loss: 2.3112
[09/26 09:13:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 14.50	top5: 62.00	
[09/26 09:13:57 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[09/26 09:14:03 visual_prompt]: Epoch 75 / 100: avg data time: 6.05e-02, avg batch time: 0.4722, average train loss: 2.3789
[09/26 09:14:05 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1578, average loss: 2.3551
[09/26 09:14:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 62.50	
[09/26 09:14:05 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[09/26 09:14:11 visual_prompt]: Epoch 76 / 100: avg data time: 5.48e-02, avg batch time: 0.4675, average train loss: 2.3489
[09/26 09:14:13 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.1581, average loss: 2.3971
[09/26 09:14:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 62.50	
[09/26 09:14:13 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[09/26 09:14:19 visual_prompt]: Epoch 77 / 100: avg data time: 6.07e-02, avg batch time: 0.4725, average train loss: 2.3036
[09/26 09:14:21 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1584, average loss: 2.2793
[09/26 09:14:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 09:14:21 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[09/26 09:14:27 visual_prompt]: Epoch 78 / 100: avg data time: 5.08e-02, avg batch time: 0.4635, average train loss: 2.2883
[09/26 09:14:29 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1582, average loss: 2.2634
[09/26 09:14:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 09:14:29 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[09/26 09:14:35 visual_prompt]: Epoch 79 / 100: avg data time: 5.38e-02, avg batch time: 0.4681, average train loss: 2.2682
[09/26 09:14:37 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1580, average loss: 2.4254
[09/26 09:14:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 50.50	
[09/26 09:14:37 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[09/26 09:14:43 visual_prompt]: Epoch 80 / 100: avg data time: 6.35e-02, avg batch time: 0.4764, average train loss: 2.2761
[09/26 09:14:45 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1587, average loss: 2.2925
[09/26 09:14:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 61.00	
[09/26 09:14:45 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[09/26 09:14:51 visual_prompt]: Epoch 81 / 100: avg data time: 6.04e-02, avg batch time: 0.4738, average train loss: 2.2888
[09/26 09:14:53 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1583, average loss: 2.2972
[09/26 09:14:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.50	
[09/26 09:14:53 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[09/26 09:14:59 visual_prompt]: Epoch 82 / 100: avg data time: 4.52e-02, avg batch time: 0.4574, average train loss: 2.2535
[09/26 09:15:01 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1579, average loss: 2.2981
[09/26 09:15:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.00	
[09/26 09:15:01 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[09/26 09:15:07 visual_prompt]: Epoch 83 / 100: avg data time: 5.88e-02, avg batch time: 0.4717, average train loss: 2.2828
[09/26 09:15:09 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1583, average loss: 2.3184
[09/26 09:15:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 09:15:09 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[09/26 09:15:15 visual_prompt]: Epoch 84 / 100: avg data time: 5.96e-02, avg batch time: 0.4716, average train loss: 2.2826
[09/26 09:15:17 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.1579, average loss: 2.2345
[09/26 09:15:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.00	top5: 64.50	
[09/26 09:15:17 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[09/26 09:15:23 visual_prompt]: Epoch 85 / 100: avg data time: 5.42e-02, avg batch time: 0.4664, average train loss: 2.2560
[09/26 09:15:25 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1577, average loss: 2.2751
[09/26 09:15:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 13.50	top5: 62.50	
[09/26 09:15:25 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[09/26 09:15:31 visual_prompt]: Epoch 86 / 100: avg data time: 5.61e-02, avg batch time: 0.4687, average train loss: 2.2799
[09/26 09:15:33 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1581, average loss: 2.3830
[09/26 09:15:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.00	top5: 61.00	
[09/26 09:15:33 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[09/26 09:15:39 visual_prompt]: Epoch 87 / 100: avg data time: 5.66e-02, avg batch time: 0.4692, average train loss: 2.2786
[09/26 09:15:41 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1580, average loss: 2.2655
[09/26 09:15:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 14.50	top5: 62.50	
[09/26 09:15:41 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[09/26 09:15:47 visual_prompt]: Epoch 88 / 100: avg data time: 6.30e-02, avg batch time: 0.4758, average train loss: 2.2496
[09/26 09:15:49 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1582, average loss: 2.2343
[09/26 09:15:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 09:15:49 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[09/26 09:15:55 visual_prompt]: Epoch 89 / 100: avg data time: 6.45e-02, avg batch time: 0.4773, average train loss: 2.2404
[09/26 09:15:57 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1584, average loss: 2.2529
[09/26 09:15:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 63.00	
[09/26 09:15:57 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[09/26 09:16:03 visual_prompt]: Epoch 90 / 100: avg data time: 5.58e-02, avg batch time: 0.4678, average train loss: 2.2350
[09/26 09:16:05 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1583, average loss: 2.2348
[09/26 09:16:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 09:16:05 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[09/26 09:16:11 visual_prompt]: Epoch 91 / 100: avg data time: 5.61e-02, avg batch time: 0.4689, average train loss: 2.2449
[09/26 09:16:13 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1581, average loss: 2.2663
[09/26 09:16:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 09:16:13 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[09/26 09:16:19 visual_prompt]: Epoch 92 / 100: avg data time: 5.86e-02, avg batch time: 0.4713, average train loss: 2.2284
[09/26 09:16:21 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1584, average loss: 2.2391
[09/26 09:16:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 09:16:21 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[09/26 09:16:27 visual_prompt]: Epoch 93 / 100: avg data time: 6.31e-02, avg batch time: 0.4774, average train loss: 2.2274
[09/26 09:16:29 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1581, average loss: 2.2491
[09/26 09:16:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 19.00	top5: 64.00	
[09/26 09:16:29 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[09/26 09:16:35 visual_prompt]: Epoch 94 / 100: avg data time: 4.71e-02, avg batch time: 0.4600, average train loss: 2.2332
[09/26 09:16:37 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1579, average loss: 2.2463
[09/26 09:16:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 21.00	top5: 62.50	
[09/26 09:16:37 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[09/26 09:16:43 visual_prompt]: Epoch 95 / 100: avg data time: 5.62e-02, avg batch time: 0.4697, average train loss: 2.2240
[09/26 09:16:45 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1578, average loss: 2.2489
[09/26 09:16:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 09:16:45 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[09/26 09:16:51 visual_prompt]: Epoch 96 / 100: avg data time: 6.51e-02, avg batch time: 0.4773, average train loss: 2.2286
[09/26 09:16:53 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1581, average loss: 2.2407
[09/26 09:16:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 09:16:53 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[09/26 09:16:59 visual_prompt]: Epoch 97 / 100: avg data time: 5.68e-02, avg batch time: 0.4698, average train loss: 2.2257
[09/26 09:17:01 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1582, average loss: 2.2461
[09/26 09:17:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 09:17:01 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[09/26 09:17:07 visual_prompt]: Epoch 98 / 100: avg data time: 5.08e-02, avg batch time: 0.4642, average train loss: 2.2305
[09/26 09:17:09 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1577, average loss: 2.2447
[09/26 09:17:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 09:17:09 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[09/26 09:17:15 visual_prompt]: Epoch 99 / 100: avg data time: 5.33e-02, avg batch time: 0.4659, average train loss: 2.2188
[09/26 09:17:17 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1582, average loss: 2.2453
[09/26 09:17:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 09:17:17 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[09/26 09:17:23 visual_prompt]: Epoch 100 / 100: avg data time: 6.65e-02, avg batch time: 0.4786, average train loss: 2.2196
[09/26 09:17:25 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1578, average loss: 2.2449
[09/26 09:17:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 09:17:25 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 09:17:25 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 09:17:25 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 09:17:25 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 09:17:25 visual_prompt]: Training with config:
[09/26 09:17:25 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr5.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 09:17:25 visual_prompt]: Loading training data...
[09/26 09:17:25 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 09:17:26 visual_prompt]: Number of images: 800
[09/26 09:17:26 visual_prompt]: Number of classes: 10 / 10
[09/26 09:17:26 visual_prompt]: Loading validation data...
[09/26 09:17:26 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 09:17:26 visual_prompt]: Number of images: 200
[09/26 09:17:26 visual_prompt]: Number of classes: 10 / 10
[09/26 09:17:26 visual_prompt]: Constructing models...
[09/26 09:17:28 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 09:17:28 visual_prompt]: tuned percent:0.543
[09/26 09:17:28 visual_prompt]: Device used for model: 0
[09/26 09:17:28 visual_prompt]: Setting up Evaluator...
[09/26 09:17:28 visual_prompt]: Setting up Trainer...
[09/26 09:17:28 visual_prompt]: 	Setting up the optimizer...
[09/26 09:17:28 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 09:17:35 visual_prompt]: Epoch 1 / 100: avg data time: 5.29e-02, avg batch time: 0.4716, average train loss: 2.6840
[09/26 09:17:37 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.1582, average loss: 2.6214
[09/26 09:17:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 09:17:37 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 09:17:37 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/26 09:17:43 visual_prompt]: Epoch 2 / 100: avg data time: 5.40e-02, avg batch time: 0.4670, average train loss: 4.7147
[09/26 09:17:44 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1580, average loss: 2.4127
[09/26 09:17:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 09:17:44 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/26 09:17:51 visual_prompt]: Epoch 3 / 100: avg data time: 6.51e-02, avg batch time: 0.4761, average train loss: 2.7830
[09/26 09:17:53 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1580, average loss: 2.3537
[09/26 09:17:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.00	
[09/26 09:17:53 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/26 09:17:59 visual_prompt]: Epoch 4 / 100: avg data time: 6.50e-02, avg batch time: 0.4773, average train loss: 2.7761
[09/26 09:18:01 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1583, average loss: 2.3494
[09/26 09:18:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 09:18:01 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/26 09:18:07 visual_prompt]: Epoch 5 / 100: avg data time: 4.55e-02, avg batch time: 0.4593, average train loss: 3.7847
[09/26 09:18:08 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1581, average loss: 3.0407
[09/26 09:18:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 09:18:08 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/26 09:18:15 visual_prompt]: Epoch 6 / 100: avg data time: 6.44e-02, avg batch time: 0.4762, average train loss: 3.5402
[09/26 09:18:16 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1584, average loss: 4.3699
[09/26 09:18:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 48.50	
[09/26 09:18:16 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/26 09:18:23 visual_prompt]: Epoch 7 / 100: avg data time: 5.76e-02, avg batch time: 0.4700, average train loss: 5.2965
[09/26 09:18:24 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1586, average loss: 5.8964
[09/26 09:18:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 49.50	
[09/26 09:18:24 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/26 09:18:31 visual_prompt]: Epoch 8 / 100: avg data time: 5.47e-02, avg batch time: 0.4680, average train loss: 5.1834
[09/26 09:18:32 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.1582, average loss: 4.2852
[09/26 09:18:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 48.50	
[09/26 09:18:32 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/26 09:18:39 visual_prompt]: Epoch 9 / 100: avg data time: 5.77e-02, avg batch time: 0.4703, average train loss: 5.3105
[09/26 09:18:40 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1585, average loss: 7.5541
[09/26 09:18:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 44.00	
[09/26 09:18:40 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/26 09:18:47 visual_prompt]: Epoch 10 / 100: avg data time: 5.10e-02, avg batch time: 0.4639, average train loss: 14.3126
[09/26 09:18:48 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1583, average loss: 15.7705
[09/26 09:18:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 09:18:48 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/26 09:18:55 visual_prompt]: Epoch 11 / 100: avg data time: 6.32e-02, avg batch time: 0.4758, average train loss: 19.1392
[09/26 09:18:56 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1585, average loss: 15.8031
[09/26 09:18:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 43.50	
[09/26 09:18:56 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/26 09:19:03 visual_prompt]: Epoch 12 / 100: avg data time: 4.93e-02, avg batch time: 0.4623, average train loss: 26.9061
[09/26 09:19:04 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1585, average loss: 40.7433
[09/26 09:19:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/26 09:19:04 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/26 09:19:11 visual_prompt]: Epoch 13 / 100: avg data time: 5.66e-02, avg batch time: 0.4694, average train loss: 26.6086
[09/26 09:19:12 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.1581, average loss: 17.4688
[09/26 09:19:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/26 09:19:12 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/26 09:19:19 visual_prompt]: Epoch 14 / 100: avg data time: 5.52e-02, avg batch time: 0.4673, average train loss: 22.3931
[09/26 09:19:20 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1580, average loss: 19.7635
[09/26 09:19:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 58.00	
[09/26 09:19:20 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/26 09:19:27 visual_prompt]: Epoch 15 / 100: avg data time: 6.11e-02, avg batch time: 0.4733, average train loss: 16.8435
[09/26 09:19:28 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1582, average loss: 10.3687
[09/26 09:19:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 42.00	
[09/26 09:19:28 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/26 09:19:35 visual_prompt]: Epoch 16 / 100: avg data time: 5.55e-02, avg batch time: 0.4675, average train loss: 18.2038
[09/26 09:19:36 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1580, average loss: 17.7273
[09/26 09:19:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 53.50	
[09/26 09:19:36 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/26 09:19:43 visual_prompt]: Epoch 17 / 100: avg data time: 6.58e-02, avg batch time: 0.4772, average train loss: 15.2286
[09/26 09:19:45 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1582, average loss: 32.8242
[09/26 09:19:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 44.50	
[09/26 09:19:45 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/26 09:19:51 visual_prompt]: Epoch 18 / 100: avg data time: 6.29e-02, avg batch time: 0.4747, average train loss: 23.7251
[09/26 09:19:53 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1583, average loss: 23.4599
[09/26 09:19:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 40.50	
[09/26 09:19:53 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/26 09:19:59 visual_prompt]: Epoch 19 / 100: avg data time: 6.33e-02, avg batch time: 0.4750, average train loss: 33.1769
[09/26 09:20:01 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.1579, average loss: 14.5191
[09/26 09:20:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 41.00	
[09/26 09:20:01 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/26 09:20:07 visual_prompt]: Epoch 20 / 100: avg data time: 6.63e-02, avg batch time: 0.4774, average train loss: 22.4607
[09/26 09:20:09 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1583, average loss: 37.9540
[09/26 09:20:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 42.50	
[09/26 09:20:09 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/26 09:20:15 visual_prompt]: Epoch 21 / 100: avg data time: 5.65e-02, avg batch time: 0.4682, average train loss: 26.0781
[09/26 09:20:17 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.1579, average loss: 14.8598
[09/26 09:20:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.00	
[09/26 09:20:17 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/26 09:20:23 visual_prompt]: Epoch 22 / 100: avg data time: 6.13e-02, avg batch time: 0.4725, average train loss: 19.0174
[09/26 09:20:25 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1583, average loss: 14.4576
[09/26 09:20:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 09:20:25 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/26 09:20:31 visual_prompt]: Epoch 23 / 100: avg data time: 6.34e-02, avg batch time: 0.4764, average train loss: 18.6852
[09/26 09:20:33 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1576, average loss: 17.2978
[09/26 09:20:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 51.50	
[09/26 09:20:33 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/26 09:20:39 visual_prompt]: Epoch 24 / 100: avg data time: 4.88e-02, avg batch time: 0.4612, average train loss: 19.8423
[09/26 09:20:41 visual_prompt]: Inference (val):avg data time: 4.57e-05, avg batch time: 0.1584, average loss: 17.5686
[09/26 09:20:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.00	
[09/26 09:20:41 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/26 09:20:47 visual_prompt]: Epoch 25 / 100: avg data time: 5.99e-02, avg batch time: 0.4711, average train loss: 19.9025
[09/26 09:20:49 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1583, average loss: 10.0922
[09/26 09:20:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 09:20:49 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/26 09:20:55 visual_prompt]: Epoch 26 / 100: avg data time: 6.04e-02, avg batch time: 0.4723, average train loss: 19.6485
[09/26 09:20:56 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1579, average loss: 25.0377
[09/26 09:20:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 59.00	
[09/26 09:20:57 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/26 09:21:03 visual_prompt]: Epoch 27 / 100: avg data time: 5.55e-02, avg batch time: 0.4674, average train loss: 20.5972
[09/26 09:21:04 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1581, average loss: 16.8582
[09/26 09:21:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 09:21:04 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/26 09:21:11 visual_prompt]: Epoch 28 / 100: avg data time: 6.51e-02, avg batch time: 0.4767, average train loss: 19.0986
[09/26 09:21:13 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1582, average loss: 20.6214
[09/26 09:21:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 09:21:13 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/26 09:21:19 visual_prompt]: Epoch 29 / 100: avg data time: 4.67e-02, avg batch time: 0.4599, average train loss: 18.4695
[09/26 09:21:20 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1581, average loss: 28.1687
[09/26 09:21:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.50	
[09/26 09:21:20 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/26 09:21:27 visual_prompt]: Epoch 30 / 100: avg data time: 5.85e-02, avg batch time: 0.4693, average train loss: 20.2947
[09/26 09:21:28 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1584, average loss: 16.5644
[09/26 09:21:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.50	
[09/26 09:21:28 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/26 09:21:35 visual_prompt]: Epoch 31 / 100: avg data time: 6.03e-02, avg batch time: 0.4721, average train loss: 15.6880
[09/26 09:21:36 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1582, average loss: 14.2061
[09/26 09:21:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.00	
[09/26 09:21:36 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/26 09:21:43 visual_prompt]: Epoch 32 / 100: avg data time: 5.86e-02, avg batch time: 0.4705, average train loss: 15.4852
[09/26 09:21:44 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1581, average loss: 20.2858
[09/26 09:21:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.00	
[09/26 09:21:44 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/26 09:21:51 visual_prompt]: Epoch 33 / 100: avg data time: 5.32e-02, avg batch time: 0.4642, average train loss: 17.9155
[09/26 09:21:52 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1582, average loss: 10.7532
[09/26 09:21:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 60.50	
[09/26 09:21:52 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/26 09:21:59 visual_prompt]: Epoch 34 / 100: avg data time: 6.20e-02, avg batch time: 0.4738, average train loss: 16.3171
[09/26 09:22:00 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1583, average loss: 16.6950
[09/26 09:22:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 56.50	
[09/26 09:22:00 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/26 09:22:07 visual_prompt]: Epoch 35 / 100: avg data time: 5.63e-02, avg batch time: 0.4695, average train loss: 15.8007
[09/26 09:22:08 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1585, average loss: 9.7718
[09/26 09:22:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 62.00	
[09/26 09:22:08 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/26 09:22:15 visual_prompt]: Epoch 36 / 100: avg data time: 4.82e-02, avg batch time: 0.4627, average train loss: 15.9375
[09/26 09:22:16 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1584, average loss: 18.7192
[09/26 09:22:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 39.00	
[09/26 09:22:16 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/26 09:22:23 visual_prompt]: Epoch 37 / 100: avg data time: 6.20e-02, avg batch time: 0.4739, average train loss: 19.2327
[09/26 09:22:24 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1583, average loss: 26.3715
[09/26 09:22:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.00	
[09/26 09:22:24 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/26 09:22:31 visual_prompt]: Epoch 38 / 100: avg data time: 6.12e-02, avg batch time: 0.4720, average train loss: 16.2070
[09/26 09:22:32 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1579, average loss: 13.1091
[09/26 09:22:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 09:22:32 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/26 09:22:39 visual_prompt]: Epoch 39 / 100: avg data time: 6.15e-02, avg batch time: 0.4740, average train loss: 13.0089
[09/26 09:22:40 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1580, average loss: 13.3535
[09/26 09:22:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 57.00	
[09/26 09:22:40 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/26 09:22:47 visual_prompt]: Epoch 40 / 100: avg data time: 5.21e-02, avg batch time: 0.4647, average train loss: 11.7675
[09/26 09:22:48 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1580, average loss: 14.8665
[09/26 09:22:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.50	
[09/26 09:22:48 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/26 09:22:55 visual_prompt]: Epoch 41 / 100: avg data time: 6.35e-02, avg batch time: 0.4770, average train loss: 14.6251
[09/26 09:22:56 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1582, average loss: 10.2652
[09/26 09:22:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 09:22:56 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/26 09:23:03 visual_prompt]: Epoch 42 / 100: avg data time: 5.79e-02, avg batch time: 0.4706, average train loss: 10.7860
[09/26 09:23:04 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1583, average loss: 26.4205
[09/26 09:23:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.00	top5: 53.50	
[09/26 09:23:04 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/26 09:23:11 visual_prompt]: Epoch 43 / 100: avg data time: 6.17e-02, avg batch time: 0.4749, average train loss: 14.7833
[09/26 09:23:12 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1578, average loss: 11.4530
[09/26 09:23:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 53.50	
[09/26 09:23:12 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/26 09:23:19 visual_prompt]: Epoch 44 / 100: avg data time: 5.74e-02, avg batch time: 0.4711, average train loss: 14.4019
[09/26 09:23:20 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1579, average loss: 12.6288
[09/26 09:23:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 54.50	
[09/26 09:23:20 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/26 09:23:27 visual_prompt]: Epoch 45 / 100: avg data time: 5.96e-02, avg batch time: 0.4712, average train loss: 12.1706
[09/26 09:23:28 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1579, average loss: 13.6097
[09/26 09:23:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 58.50	
[09/26 09:23:28 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/26 09:23:35 visual_prompt]: Epoch 46 / 100: avg data time: 5.91e-02, avg batch time: 0.4713, average train loss: 11.4252
[09/26 09:23:36 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1585, average loss: 9.4662
[09/26 09:23:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.00	
[09/26 09:23:36 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/26 09:23:43 visual_prompt]: Epoch 47 / 100: avg data time: 5.50e-02, avg batch time: 0.4685, average train loss: 10.7378
[09/26 09:23:44 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1580, average loss: 11.2734
[09/26 09:23:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 56.50	
[09/26 09:23:44 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/26 09:23:51 visual_prompt]: Epoch 48 / 100: avg data time: 5.97e-02, avg batch time: 0.4709, average train loss: 7.8533
[09/26 09:23:52 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1575, average loss: 10.3136
[09/26 09:23:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 46.00	
[09/26 09:23:52 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/26 09:23:59 visual_prompt]: Epoch 49 / 100: avg data time: 6.70e-02, avg batch time: 0.4799, average train loss: 10.4100
[09/26 09:24:00 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1574, average loss: 7.4755
[09/26 09:24:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 44.00	
[09/26 09:24:00 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/26 09:24:07 visual_prompt]: Epoch 50 / 100: avg data time: 6.25e-02, avg batch time: 0.4754, average train loss: 11.4909
[09/26 09:24:08 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1577, average loss: 9.1994
[09/26 09:24:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 36.50	
[09/26 09:24:08 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/26 09:24:15 visual_prompt]: Epoch 51 / 100: avg data time: 5.43e-02, avg batch time: 0.4667, average train loss: 9.0312
[09/26 09:24:16 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1583, average loss: 13.3923
[09/26 09:24:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.00	
[09/26 09:24:16 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/26 09:24:23 visual_prompt]: Epoch 52 / 100: avg data time: 5.80e-02, avg batch time: 0.4698, average train loss: 9.0919
[09/26 09:24:24 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1584, average loss: 5.4322
[09/26 09:24:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 45.50	
[09/26 09:24:24 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/26 09:24:31 visual_prompt]: Epoch 53 / 100: avg data time: 5.67e-02, avg batch time: 0.4692, average train loss: 5.4004
[09/26 09:24:32 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1585, average loss: 9.1964
[09/26 09:24:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 45.50	
[09/26 09:24:32 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/26 09:24:39 visual_prompt]: Epoch 54 / 100: avg data time: 5.53e-02, avg batch time: 0.4691, average train loss: 7.1916
[09/26 09:24:40 visual_prompt]: Inference (val):avg data time: 4.61e-05, avg batch time: 0.1578, average loss: 10.1334
[09/26 09:24:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 51.50	
[09/26 09:24:40 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/26 09:24:47 visual_prompt]: Epoch 55 / 100: avg data time: 6.02e-02, avg batch time: 0.4728, average train loss: 7.4835
[09/26 09:24:48 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1580, average loss: 6.9058
[09/26 09:24:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 57.50	
[09/26 09:24:48 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/26 09:24:55 visual_prompt]: Epoch 56 / 100: avg data time: 6.10e-02, avg batch time: 0.4744, average train loss: 6.5195
[09/26 09:24:56 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1579, average loss: 4.3760
[09/26 09:24:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 43.50	
[09/26 09:24:56 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/26 09:25:03 visual_prompt]: Epoch 57 / 100: avg data time: 6.43e-02, avg batch time: 0.4774, average train loss: 4.7174
[09/26 09:25:04 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1580, average loss: 3.7891
[09/26 09:25:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 45.00	
[09/26 09:25:04 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/26 09:25:11 visual_prompt]: Epoch 58 / 100: avg data time: 5.28e-02, avg batch time: 0.4655, average train loss: 4.8525
[09/26 09:25:12 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1585, average loss: 3.1501
[09/26 09:25:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 09:25:12 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/26 09:25:19 visual_prompt]: Epoch 59 / 100: avg data time: 5.56e-02, avg batch time: 0.4686, average train loss: 3.7768
[09/26 09:25:20 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1583, average loss: 3.3700
[09/26 09:25:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 55.50	
[09/26 09:25:20 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/26 09:25:27 visual_prompt]: Epoch 60 / 100: avg data time: 5.19e-02, avg batch time: 0.4648, average train loss: 5.4780
[09/26 09:25:28 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1583, average loss: 5.5811
[09/26 09:25:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.50	
[09/26 09:25:28 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/26 09:25:35 visual_prompt]: Epoch 61 / 100: avg data time: 5.70e-02, avg batch time: 0.4688, average train loss: 5.5134
[09/26 09:25:36 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1578, average loss: 3.7309
[09/26 09:25:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 48.50	
[09/26 09:25:36 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/26 09:25:43 visual_prompt]: Epoch 62 / 100: avg data time: 6.00e-02, avg batch time: 0.4725, average train loss: 4.7650
[09/26 09:25:44 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1584, average loss: 4.3505
[09/26 09:25:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 42.00	
[09/26 09:25:44 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/26 09:25:51 visual_prompt]: Epoch 63 / 100: avg data time: 5.83e-02, avg batch time: 0.4722, average train loss: 3.5643
[09/26 09:25:52 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1581, average loss: 2.8458
[09/26 09:25:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 41.50	
[09/26 09:25:52 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/26 09:25:59 visual_prompt]: Epoch 64 / 100: avg data time: 5.55e-02, avg batch time: 0.4684, average train loss: 4.0115
[09/26 09:26:00 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1576, average loss: 3.6948
[09/26 09:26:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 43.50	
[09/26 09:26:00 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/26 09:26:07 visual_prompt]: Epoch 65 / 100: avg data time: 5.88e-02, avg batch time: 0.4730, average train loss: 4.6891
[09/26 09:26:08 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1578, average loss: 3.6978
[09/26 09:26:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.50	
[09/26 09:26:08 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/26 09:26:15 visual_prompt]: Epoch 66 / 100: avg data time: 5.86e-02, avg batch time: 0.4713, average train loss: 3.7176
[09/26 09:26:16 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1585, average loss: 5.3372
[09/26 09:26:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 38.50	
[09/26 09:26:16 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/26 09:26:23 visual_prompt]: Epoch 67 / 100: avg data time: 4.41e-02, avg batch time: 0.4583, average train loss: 4.2566
[09/26 09:26:24 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1581, average loss: 3.2274
[09/26 09:26:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 40.50	
[09/26 09:26:24 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/26 09:26:31 visual_prompt]: Epoch 68 / 100: avg data time: 5.91e-02, avg batch time: 0.4723, average train loss: 3.9379
[09/26 09:26:32 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1584, average loss: 6.0434
[09/26 09:26:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.50	
[09/26 09:26:32 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/26 09:26:39 visual_prompt]: Epoch 69 / 100: avg data time: 6.10e-02, avg batch time: 0.4743, average train loss: 6.1607
[09/26 09:26:40 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1580, average loss: 34.7710
[09/26 09:26:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 50.00	
[09/26 09:26:40 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/26 09:26:47 visual_prompt]: Epoch 70 / 100: avg data time: 5.82e-02, avg batch time: 0.4704, average train loss: 8.0443
[09/26 09:26:48 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1585, average loss: 4.1572
[09/26 09:26:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.00	
[09/26 09:26:48 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/26 09:26:55 visual_prompt]: Epoch 71 / 100: avg data time: 5.64e-02, avg batch time: 0.4702, average train loss: 4.9810
[09/26 09:26:56 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1581, average loss: 4.3030
[09/26 09:26:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.00	
[09/26 09:26:56 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/26 09:27:02 visual_prompt]: Epoch 72 / 100: avg data time: 4.79e-02, avg batch time: 0.4632, average train loss: 3.2420
[09/26 09:27:04 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1585, average loss: 2.5613
[09/26 09:27:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.00	
[09/26 09:27:04 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/26 09:27:10 visual_prompt]: Epoch 73 / 100: avg data time: 5.67e-02, avg batch time: 0.4702, average train loss: 2.4695
[09/26 09:27:12 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1584, average loss: 2.4900
[09/26 09:27:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 09:27:12 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/26 09:27:18 visual_prompt]: Epoch 74 / 100: avg data time: 5.73e-02, avg batch time: 0.4703, average train loss: 2.3463
[09/26 09:27:20 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1586, average loss: 2.2569
[09/26 09:27:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 09:27:20 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/26 09:27:26 visual_prompt]: Epoch 75 / 100: avg data time: 5.65e-02, avg batch time: 0.4708, average train loss: 2.3269
[09/26 09:27:28 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1587, average loss: 2.2858
[09/26 09:27:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.50	
[09/26 09:27:28 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/26 09:27:34 visual_prompt]: Epoch 76 / 100: avg data time: 5.33e-02, avg batch time: 0.4679, average train loss: 2.3299
[09/26 09:27:36 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1583, average loss: 2.3054
[09/26 09:27:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 64.00	
[09/26 09:27:36 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/26 09:27:42 visual_prompt]: Epoch 77 / 100: avg data time: 6.02e-02, avg batch time: 0.4736, average train loss: 2.3287
[09/26 09:27:44 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1583, average loss: 2.3384
[09/26 09:27:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 09:27:44 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/26 09:27:50 visual_prompt]: Epoch 78 / 100: avg data time: 5.49e-02, avg batch time: 0.4672, average train loss: 2.3133
[09/26 09:27:52 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1583, average loss: 2.2361
[09/26 09:27:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 09:27:52 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/26 09:27:58 visual_prompt]: Epoch 79 / 100: avg data time: 5.35e-02, avg batch time: 0.4663, average train loss: 2.3206
[09/26 09:28:00 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1581, average loss: 2.2371
[09/26 09:28:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 09:28:00 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/26 09:28:06 visual_prompt]: Epoch 80 / 100: avg data time: 6.69e-02, avg batch time: 0.4792, average train loss: 2.2817
[09/26 09:28:08 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1578, average loss: 2.2387
[09/26 09:28:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 09:28:08 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/26 09:28:14 visual_prompt]: Epoch 81 / 100: avg data time: 6.63e-02, avg batch time: 0.4783, average train loss: 2.3224
[09/26 09:28:16 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1577, average loss: 2.3012
[09/26 09:28:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 09:28:16 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/26 09:28:22 visual_prompt]: Epoch 82 / 100: avg data time: 5.28e-02, avg batch time: 0.4656, average train loss: 2.3459
[09/26 09:28:24 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1585, average loss: 2.3777
[09/26 09:28:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 50.00	
[09/26 09:28:24 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/26 09:28:30 visual_prompt]: Epoch 83 / 100: avg data time: 5.48e-02, avg batch time: 0.4671, average train loss: 2.2975
[09/26 09:28:32 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1582, average loss: 2.2748
[09/26 09:28:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 09:28:32 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/26 09:28:38 visual_prompt]: Epoch 84 / 100: avg data time: 6.15e-02, avg batch time: 0.4741, average train loss: 2.2848
[09/26 09:28:40 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1579, average loss: 2.2793
[09/26 09:28:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.00	
[09/26 09:28:40 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/26 09:28:46 visual_prompt]: Epoch 85 / 100: avg data time: 5.84e-02, avg batch time: 0.4700, average train loss: 2.2887
[09/26 09:28:48 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1583, average loss: 2.2618
[09/26 09:28:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 09:28:48 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/26 09:28:54 visual_prompt]: Epoch 86 / 100: avg data time: 5.64e-02, avg batch time: 0.4696, average train loss: 2.2687
[09/26 09:28:56 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1583, average loss: 2.2314
[09/26 09:28:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 09:28:56 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/26 09:29:02 visual_prompt]: Epoch 87 / 100: avg data time: 6.36e-02, avg batch time: 0.4761, average train loss: 2.2804
[09/26 09:29:04 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1582, average loss: 2.2385
[09/26 09:29:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 64.50	
[09/26 09:29:04 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/26 09:29:10 visual_prompt]: Epoch 88 / 100: avg data time: 5.49e-02, avg batch time: 0.4679, average train loss: 2.2557
[09/26 09:29:12 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1578, average loss: 2.2485
[09/26 09:29:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 09:29:12 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/26 09:29:18 visual_prompt]: Epoch 89 / 100: avg data time: 4.66e-02, avg batch time: 0.4604, average train loss: 2.2620
[09/26 09:29:20 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1584, average loss: 2.2367
[09/26 09:29:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 09:29:20 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/26 09:29:26 visual_prompt]: Epoch 90 / 100: avg data time: 6.27e-02, avg batch time: 0.4750, average train loss: 2.2619
[09/26 09:29:28 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1584, average loss: 2.2245
[09/26 09:29:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 09:29:28 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/26 09:29:34 visual_prompt]: Epoch 91 / 100: avg data time: 5.42e-02, avg batch time: 0.4662, average train loss: 2.2510
[09/26 09:29:36 visual_prompt]: Inference (val):avg data time: 4.84e-05, avg batch time: 0.1583, average loss: 2.2143
[09/26 09:29:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 09:29:36 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/26 09:29:42 visual_prompt]: Epoch 92 / 100: avg data time: 5.20e-02, avg batch time: 0.4645, average train loss: 2.2434
[09/26 09:29:44 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1578, average loss: 2.2209
[09/26 09:29:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 09:29:44 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/26 09:29:50 visual_prompt]: Epoch 93 / 100: avg data time: 5.73e-02, avg batch time: 0.4695, average train loss: 2.2506
[09/26 09:29:52 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1584, average loss: 2.2199
[09/26 09:29:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 09:29:52 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/26 09:29:58 visual_prompt]: Epoch 94 / 100: avg data time: 4.97e-02, avg batch time: 0.4644, average train loss: 2.2387
[09/26 09:30:00 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1583, average loss: 2.2140
[09/26 09:30:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 09:30:00 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/26 09:30:06 visual_prompt]: Epoch 95 / 100: avg data time: 4.40e-02, avg batch time: 0.4585, average train loss: 2.2421
[09/26 09:30:07 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1583, average loss: 2.2169
[09/26 09:30:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 09:30:07 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/26 09:30:14 visual_prompt]: Epoch 96 / 100: avg data time: 5.97e-02, avg batch time: 0.4722, average train loss: 2.2359
[09/26 09:30:15 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1582, average loss: 2.2199
[09/26 09:30:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 09:30:15 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/26 09:30:22 visual_prompt]: Epoch 97 / 100: avg data time: 5.72e-02, avg batch time: 0.4697, average train loss: 2.2368
[09/26 09:30:23 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1580, average loss: 2.2170
[09/26 09:30:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 09:30:23 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/26 09:30:30 visual_prompt]: Epoch 98 / 100: avg data time: 5.85e-02, avg batch time: 0.4722, average train loss: 2.2325
[09/26 09:30:31 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1584, average loss: 2.2151
[09/26 09:30:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 09:30:31 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/26 09:30:38 visual_prompt]: Epoch 99 / 100: avg data time: 5.71e-02, avg batch time: 0.4681, average train loss: 2.2316
[09/26 09:30:39 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1582, average loss: 2.2154
[09/26 09:30:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 09:30:39 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/26 09:30:46 visual_prompt]: Epoch 100 / 100: avg data time: 4.74e-02, avg batch time: 0.4604, average train loss: 2.2312
[09/26 09:30:47 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 2.2155
[09/26 09:30:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 09:30:47 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 09:30:47 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 09:30:47 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 09:30:47 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 09:30:47 visual_prompt]: Training with config:
[09/26 09:30:47 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr5.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 09:30:47 visual_prompt]: Loading training data...
[09/26 09:30:47 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 09:30:48 visual_prompt]: Number of images: 800
[09/26 09:30:48 visual_prompt]: Number of classes: 10 / 10
[09/26 09:30:48 visual_prompt]: Loading validation data...
[09/26 09:30:48 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 09:30:48 visual_prompt]: Number of images: 200
[09/26 09:30:48 visual_prompt]: Number of classes: 10 / 10
[09/26 09:30:48 visual_prompt]: Constructing models...
[09/26 09:30:51 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 09:30:51 visual_prompt]: tuned percent:0.543
[09/26 09:30:51 visual_prompt]: Device used for model: 0
[09/26 09:30:51 visual_prompt]: Setting up Evaluator...
[09/26 09:30:51 visual_prompt]: Setting up Trainer...
[09/26 09:30:51 visual_prompt]: 	Setting up the optimizer...
[09/26 09:30:51 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 09:30:57 visual_prompt]: Epoch 1 / 100: avg data time: 5.18e-02, avg batch time: 0.4693, average train loss: 2.6747
[09/26 09:30:59 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.1579, average loss: 2.6214
[09/26 09:30:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 09:30:59 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 09:30:59 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/26 09:31:05 visual_prompt]: Epoch 2 / 100: avg data time: 4.67e-02, avg batch time: 0.4606, average train loss: 5.0531
[09/26 09:31:07 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1583, average loss: 2.9695
[09/26 09:31:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 48.00	
[09/26 09:31:07 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/26 09:31:13 visual_prompt]: Epoch 3 / 100: avg data time: 4.71e-02, avg batch time: 0.4604, average train loss: 2.8791
[09/26 09:31:15 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1582, average loss: 2.4640
[09/26 09:31:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 48.00	
[09/26 09:31:15 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/26 09:31:21 visual_prompt]: Epoch 4 / 100: avg data time: 6.54e-02, avg batch time: 0.4779, average train loss: 2.4673
[09/26 09:31:23 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1586, average loss: 2.7430
[09/26 09:31:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 09:31:23 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/26 09:31:29 visual_prompt]: Epoch 5 / 100: avg data time: 5.26e-02, avg batch time: 0.4649, average train loss: 2.6428
[09/26 09:31:31 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1581, average loss: 2.4607
[09/26 09:31:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 09:31:31 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/26 09:31:37 visual_prompt]: Epoch 6 / 100: avg data time: 6.12e-02, avg batch time: 0.4739, average train loss: 2.5409
[09/26 09:31:39 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1579, average loss: 2.5435
[09/26 09:31:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 09:31:39 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/26 09:31:45 visual_prompt]: Epoch 7 / 100: avg data time: 5.24e-02, avg batch time: 0.4653, average train loss: 2.7107
[09/26 09:31:47 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1585, average loss: 3.1780
[09/26 09:31:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 43.00	
[09/26 09:31:47 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/26 09:31:53 visual_prompt]: Epoch 8 / 100: avg data time: 5.75e-02, avg batch time: 0.4701, average train loss: 2.7485
[09/26 09:31:55 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1580, average loss: 3.5302
[09/26 09:31:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 40.50	
[09/26 09:31:55 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/26 09:32:01 visual_prompt]: Epoch 9 / 100: avg data time: 5.27e-02, avg batch time: 0.4658, average train loss: 8.0653
[09/26 09:32:03 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1583, average loss: 7.9170
[09/26 09:32:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 43.00	
[09/26 09:32:03 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/26 09:32:09 visual_prompt]: Epoch 10 / 100: avg data time: 5.79e-02, avg batch time: 0.4719, average train loss: 15.5410
[09/26 09:32:10 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1581, average loss: 15.8335
[09/26 09:32:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 40.50	
[09/26 09:32:11 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/26 09:32:17 visual_prompt]: Epoch 11 / 100: avg data time: 6.15e-02, avg batch time: 0.4734, average train loss: 18.7043
[09/26 09:32:19 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1584, average loss: 11.9292
[09/26 09:32:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/26 09:32:19 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/26 09:32:25 visual_prompt]: Epoch 12 / 100: avg data time: 6.04e-02, avg batch time: 0.4727, average train loss: 15.5715
[09/26 09:32:27 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1583, average loss: 12.8559
[09/26 09:32:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/26 09:32:27 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/26 09:32:33 visual_prompt]: Epoch 13 / 100: avg data time: 5.72e-02, avg batch time: 0.4691, average train loss: 10.7670
[09/26 09:32:35 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1583, average loss: 8.6566
[09/26 09:32:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 52.00	
[09/26 09:32:35 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/26 09:32:41 visual_prompt]: Epoch 14 / 100: avg data time: 6.24e-02, avg batch time: 0.4749, average train loss: 10.8311
[09/26 09:32:43 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1582, average loss: 7.1302
[09/26 09:32:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 09:32:43 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/26 09:32:49 visual_prompt]: Epoch 15 / 100: avg data time: 6.57e-02, avg batch time: 0.4804, average train loss: 20.8305
[09/26 09:32:51 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1582, average loss: 16.0669
[09/26 09:32:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 63.50	
[09/26 09:32:51 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/26 09:32:57 visual_prompt]: Epoch 16 / 100: avg data time: 6.12e-02, avg batch time: 0.4739, average train loss: 19.4497
[09/26 09:32:59 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1579, average loss: 22.5926
[09/26 09:32:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 39.00	
[09/26 09:32:59 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/26 09:33:05 visual_prompt]: Epoch 17 / 100: avg data time: 5.65e-02, avg batch time: 0.4707, average train loss: 19.1819
[09/26 09:33:07 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1579, average loss: 14.2604
[09/26 09:33:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 09:33:07 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/26 09:33:13 visual_prompt]: Epoch 18 / 100: avg data time: 4.79e-02, avg batch time: 0.4624, average train loss: 16.8536
[09/26 09:33:15 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1584, average loss: 9.7031
[09/26 09:33:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 09:33:15 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/26 09:33:21 visual_prompt]: Epoch 19 / 100: avg data time: 6.47e-02, avg batch time: 0.4783, average train loss: 16.9514
[09/26 09:33:23 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1580, average loss: 24.7031
[09/26 09:33:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.50	
[09/26 09:33:23 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/26 09:33:29 visual_prompt]: Epoch 20 / 100: avg data time: 4.64e-02, avg batch time: 0.4589, average train loss: 23.6327
[09/26 09:33:31 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1584, average loss: 24.4055
[09/26 09:33:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.00	
[09/26 09:33:31 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/26 09:33:37 visual_prompt]: Epoch 21 / 100: avg data time: 5.88e-02, avg batch time: 0.4712, average train loss: 21.4184
[09/26 09:33:39 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 13.9210
[09/26 09:33:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 53.00	
[09/26 09:33:39 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/26 09:33:45 visual_prompt]: Epoch 22 / 100: avg data time: 6.26e-02, avg batch time: 0.4750, average train loss: 15.6283
[09/26 09:33:47 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.1585, average loss: 20.0052
[09/26 09:33:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 36.00	
[09/26 09:33:47 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/26 09:33:53 visual_prompt]: Epoch 23 / 100: avg data time: 5.77e-02, avg batch time: 0.4710, average train loss: 16.5493
[09/26 09:33:55 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1582, average loss: 12.3059
[09/26 09:33:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 09:33:55 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/26 09:34:01 visual_prompt]: Epoch 24 / 100: avg data time: 6.67e-02, avg batch time: 0.4789, average train loss: 14.7065
[09/26 09:34:03 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1580, average loss: 11.8828
[09/26 09:34:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 09:34:03 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/26 09:34:09 visual_prompt]: Epoch 25 / 100: avg data time: 5.78e-02, avg batch time: 0.4714, average train loss: 17.9634
[09/26 09:34:11 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.1584, average loss: 12.4588
[09/26 09:34:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 21.00	top5: 59.50	
[09/26 09:34:11 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/26 09:34:17 visual_prompt]: Epoch 26 / 100: avg data time: 5.59e-02, avg batch time: 0.4689, average train loss: 12.5310
[09/26 09:34:19 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1581, average loss: 18.6837
[09/26 09:34:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 40.00	
[09/26 09:34:19 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/26 09:34:25 visual_prompt]: Epoch 27 / 100: avg data time: 4.91e-02, avg batch time: 0.4641, average train loss: 16.0890
[09/26 09:34:27 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1580, average loss: 19.5979
[09/26 09:34:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 09:34:27 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/26 09:34:33 visual_prompt]: Epoch 28 / 100: avg data time: 5.73e-02, avg batch time: 0.4713, average train loss: 10.6952
[09/26 09:34:35 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1582, average loss: 8.7729
[09/26 09:34:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 52.00	
[09/26 09:34:35 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/26 09:34:41 visual_prompt]: Epoch 29 / 100: avg data time: 5.48e-02, avg batch time: 0.4681, average train loss: 13.5763
[09/26 09:34:43 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1582, average loss: 17.8295
[09/26 09:34:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 42.50	
[09/26 09:34:43 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/26 09:34:49 visual_prompt]: Epoch 30 / 100: avg data time: 6.35e-02, avg batch time: 0.4770, average train loss: 18.5006
[09/26 09:34:51 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1584, average loss: 27.3665
[09/26 09:34:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 61.50	
[09/26 09:34:51 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/26 09:34:57 visual_prompt]: Epoch 31 / 100: avg data time: 5.90e-02, avg batch time: 0.4730, average train loss: 19.2691
[09/26 09:34:59 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1580, average loss: 20.3533
[09/26 09:34:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 44.00	
[09/26 09:34:59 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/26 09:35:05 visual_prompt]: Epoch 32 / 100: avg data time: 5.42e-02, avg batch time: 0.4661, average train loss: 18.6940
[09/26 09:35:07 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1577, average loss: 12.3971
[09/26 09:35:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 58.50	
[09/26 09:35:07 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/26 09:35:13 visual_prompt]: Epoch 33 / 100: avg data time: 5.37e-02, avg batch time: 0.4672, average train loss: 9.3102
[09/26 09:35:15 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1585, average loss: 11.2451
[09/26 09:35:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.50	
[09/26 09:35:15 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/26 09:35:21 visual_prompt]: Epoch 34 / 100: avg data time: 5.67e-02, avg batch time: 0.4690, average train loss: 10.8699
[09/26 09:35:23 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1580, average loss: 12.1888
[09/26 09:35:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 62.50	
[09/26 09:35:23 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/26 09:35:29 visual_prompt]: Epoch 35 / 100: avg data time: 5.24e-02, avg batch time: 0.4655, average train loss: 10.8344
[09/26 09:35:31 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1580, average loss: 9.4971
[09/26 09:35:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 54.00	
[09/26 09:35:31 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/26 09:35:37 visual_prompt]: Epoch 36 / 100: avg data time: 5.59e-02, avg batch time: 0.4697, average train loss: 8.8369
[09/26 09:35:39 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1579, average loss: 6.6050
[09/26 09:35:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 57.50	
[09/26 09:35:39 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/26 09:35:45 visual_prompt]: Epoch 37 / 100: avg data time: 4.76e-02, avg batch time: 0.4610, average train loss: 6.0650
[09/26 09:35:46 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1581, average loss: 8.5949
[09/26 09:35:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 09:35:46 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/26 09:35:53 visual_prompt]: Epoch 38 / 100: avg data time: 5.54e-02, avg batch time: 0.4681, average train loss: 5.2309
[09/26 09:35:54 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1581, average loss: 3.7752
[09/26 09:35:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 49.50	
[09/26 09:35:54 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/26 09:36:01 visual_prompt]: Epoch 39 / 100: avg data time: 6.36e-02, avg batch time: 0.4766, average train loss: 3.2544
[09/26 09:36:02 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1584, average loss: 2.4722
[09/26 09:36:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 56.50	
[09/26 09:36:02 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/26 09:36:09 visual_prompt]: Epoch 40 / 100: avg data time: 6.03e-02, avg batch time: 0.4723, average train loss: 2.9110
[09/26 09:36:10 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1583, average loss: 2.4838
[09/26 09:36:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.50	
[09/26 09:36:10 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/26 09:36:17 visual_prompt]: Epoch 41 / 100: avg data time: 6.08e-02, avg batch time: 0.4743, average train loss: 2.9314
[09/26 09:36:19 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1582, average loss: 2.9581
[09/26 09:36:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 46.50	
[09/26 09:36:19 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/26 09:36:25 visual_prompt]: Epoch 42 / 100: avg data time: 5.32e-02, avg batch time: 0.4663, average train loss: 6.8791
[09/26 09:36:26 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1588, average loss: 9.1581
[09/26 09:36:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 40.00	
[09/26 09:36:26 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/26 09:36:33 visual_prompt]: Epoch 43 / 100: avg data time: 5.48e-02, avg batch time: 0.4670, average train loss: 13.8267
[09/26 09:36:34 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1583, average loss: 13.7736
[09/26 09:36:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 62.00	
[09/26 09:36:34 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/26 09:36:41 visual_prompt]: Epoch 44 / 100: avg data time: 5.04e-02, avg batch time: 0.4619, average train loss: 14.1018
[09/26 09:36:42 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.1586, average loss: 12.9423
[09/26 09:36:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 46.00	
[09/26 09:36:42 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/26 09:36:49 visual_prompt]: Epoch 45 / 100: avg data time: 6.55e-02, avg batch time: 0.4771, average train loss: 8.1143
[09/26 09:36:50 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1586, average loss: 7.6489
[09/26 09:36:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 57.50	
[09/26 09:36:50 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/26 09:36:57 visual_prompt]: Epoch 46 / 100: avg data time: 6.19e-02, avg batch time: 0.4761, average train loss: 6.5511
[09/26 09:36:58 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1579, average loss: 3.6846
[09/26 09:36:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 09:36:58 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/26 09:37:05 visual_prompt]: Epoch 47 / 100: avg data time: 5.84e-02, avg batch time: 0.4704, average train loss: 3.9364
[09/26 09:37:06 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1578, average loss: 2.8623
[09/26 09:37:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.50	
[09/26 09:37:06 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/26 09:37:13 visual_prompt]: Epoch 48 / 100: avg data time: 5.91e-02, avg batch time: 0.4705, average train loss: 2.6571
[09/26 09:37:14 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1580, average loss: 2.7420
[09/26 09:37:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 42.00	
[09/26 09:37:14 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/26 09:37:21 visual_prompt]: Epoch 49 / 100: avg data time: 5.56e-02, avg batch time: 0.4671, average train loss: 2.5066
[09/26 09:37:22 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1582, average loss: 2.7039
[09/26 09:37:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 39.50	
[09/26 09:37:22 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/26 09:37:29 visual_prompt]: Epoch 50 / 100: avg data time: 5.94e-02, avg batch time: 0.4719, average train loss: 2.5106
[09/26 09:37:30 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1580, average loss: 2.3067
[09/26 09:37:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/26 09:37:30 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/26 09:37:37 visual_prompt]: Epoch 51 / 100: avg data time: 6.00e-02, avg batch time: 0.4730, average train loss: 2.5579
[09/26 09:37:38 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1577, average loss: 2.8011
[09/26 09:37:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 43.50	
[09/26 09:37:38 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/26 09:37:45 visual_prompt]: Epoch 52 / 100: avg data time: 6.21e-02, avg batch time: 0.4744, average train loss: 3.2526
[09/26 09:37:46 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1580, average loss: 2.6946
[09/26 09:37:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.00	
[09/26 09:37:46 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/26 09:37:53 visual_prompt]: Epoch 53 / 100: avg data time: 5.93e-02, avg batch time: 0.4712, average train loss: 3.0967
[09/26 09:37:54 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1580, average loss: 2.4493
[09/26 09:37:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 59.50	
[09/26 09:37:54 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/26 09:38:01 visual_prompt]: Epoch 54 / 100: avg data time: 5.92e-02, avg batch time: 0.4704, average train loss: 2.5146
[09/26 09:38:02 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1577, average loss: 2.6601
[09/26 09:38:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/26 09:38:02 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/26 09:38:09 visual_prompt]: Epoch 55 / 100: avg data time: 4.89e-02, avg batch time: 0.4613, average train loss: 2.5387
[09/26 09:38:10 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1582, average loss: 2.3459
[09/26 09:38:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 09:38:10 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/26 09:38:17 visual_prompt]: Epoch 56 / 100: avg data time: 5.47e-02, avg batch time: 0.4682, average train loss: 2.4725
[09/26 09:38:18 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1583, average loss: 2.2996
[09/26 09:38:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 53.00	
[09/26 09:38:18 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/26 09:38:25 visual_prompt]: Epoch 57 / 100: avg data time: 6.37e-02, avg batch time: 0.4756, average train loss: 2.4620
[09/26 09:38:26 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1580, average loss: 2.3961
[09/26 09:38:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 64.50	
[09/26 09:38:26 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/26 09:38:33 visual_prompt]: Epoch 58 / 100: avg data time: 5.85e-02, avg batch time: 0.4700, average train loss: 2.3565
[09/26 09:38:34 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1583, average loss: 2.4556
[09/26 09:38:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 47.00	
[09/26 09:38:34 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/26 09:38:41 visual_prompt]: Epoch 59 / 100: avg data time: 4.85e-02, avg batch time: 0.4627, average train loss: 2.3322
[09/26 09:38:42 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1586, average loss: 2.3340
[09/26 09:38:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 56.00	
[09/26 09:38:42 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/26 09:38:49 visual_prompt]: Epoch 60 / 100: avg data time: 6.38e-02, avg batch time: 0.4750, average train loss: 2.3662
[09/26 09:38:50 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1581, average loss: 2.2925
[09/26 09:38:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 09:38:50 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/26 09:38:57 visual_prompt]: Epoch 61 / 100: avg data time: 6.20e-02, avg batch time: 0.4732, average train loss: 2.3335
[09/26 09:38:58 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1583, average loss: 2.2800
[09/26 09:38:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 09:38:58 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/26 09:39:05 visual_prompt]: Epoch 62 / 100: avg data time: 5.43e-02, avg batch time: 0.4670, average train loss: 2.3180
[09/26 09:39:06 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1580, average loss: 2.2782
[09/26 09:39:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 09:39:06 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/26 09:39:13 visual_prompt]: Epoch 63 / 100: avg data time: 6.52e-02, avg batch time: 0.4791, average train loss: 2.3524
[09/26 09:39:14 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1581, average loss: 2.7238
[09/26 09:39:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 45.00	
[09/26 09:39:14 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/26 09:39:21 visual_prompt]: Epoch 64 / 100: avg data time: 5.97e-02, avg batch time: 0.4719, average train loss: 2.5650
[09/26 09:39:22 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1582, average loss: 2.4789
[09/26 09:39:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 64.50	
[09/26 09:39:22 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/26 09:39:29 visual_prompt]: Epoch 65 / 100: avg data time: 6.64e-02, avg batch time: 0.4787, average train loss: 2.3787
[09/26 09:39:30 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1581, average loss: 2.4206
[09/26 09:39:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 09:39:30 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/26 09:39:37 visual_prompt]: Epoch 66 / 100: avg data time: 5.35e-02, avg batch time: 0.4668, average train loss: 2.3735
[09/26 09:39:38 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1581, average loss: 2.3785
[09/26 09:39:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.00	
[09/26 09:39:38 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/26 09:39:45 visual_prompt]: Epoch 67 / 100: avg data time: 4.69e-02, avg batch time: 0.4619, average train loss: 2.4193
[09/26 09:39:46 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1579, average loss: 2.3460
[09/26 09:39:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 09:39:46 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/26 09:39:53 visual_prompt]: Epoch 68 / 100: avg data time: 5.81e-02, avg batch time: 0.4712, average train loss: 2.3517
[09/26 09:39:54 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1584, average loss: 2.4053
[09/26 09:39:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 50.00	
[09/26 09:39:54 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/26 09:40:01 visual_prompt]: Epoch 69 / 100: avg data time: 5.28e-02, avg batch time: 0.4656, average train loss: 2.3373
[09/26 09:40:02 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1585, average loss: 2.3004
[09/26 09:40:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 09:40:02 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/26 09:40:09 visual_prompt]: Epoch 70 / 100: avg data time: 5.73e-02, avg batch time: 0.4706, average train loss: 2.3061
[09/26 09:40:10 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1577, average loss: 2.3897
[09/26 09:40:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 09:40:10 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/26 09:40:17 visual_prompt]: Epoch 71 / 100: avg data time: 5.83e-02, avg batch time: 0.4714, average train loss: 2.3364
[09/26 09:40:18 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1581, average loss: 2.2418
[09/26 09:40:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/26 09:40:18 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/26 09:40:25 visual_prompt]: Epoch 72 / 100: avg data time: 4.73e-02, avg batch time: 0.4623, average train loss: 2.2940
[09/26 09:40:26 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1581, average loss: 2.2648
[09/26 09:40:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 09:40:26 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/26 09:40:33 visual_prompt]: Epoch 73 / 100: avg data time: 5.65e-02, avg batch time: 0.4687, average train loss: 2.2845
[09/26 09:40:34 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1581, average loss: 2.2479
[09/26 09:40:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 09:40:34 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/26 09:40:41 visual_prompt]: Epoch 74 / 100: avg data time: 6.43e-02, avg batch time: 0.4771, average train loss: 2.3004
[09/26 09:40:42 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1579, average loss: 2.2935
[09/26 09:40:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 09:40:42 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/26 09:40:49 visual_prompt]: Epoch 75 / 100: avg data time: 5.64e-02, avg batch time: 0.4693, average train loss: 2.3110
[09/26 09:40:50 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1585, average loss: 2.2347
[09/26 09:40:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 09:40:50 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/26 09:40:57 visual_prompt]: Epoch 76 / 100: avg data time: 5.85e-02, avg batch time: 0.4700, average train loss: 2.2946
[09/26 09:40:58 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 2.2631
[09/26 09:40:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 09:40:58 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/26 09:41:05 visual_prompt]: Epoch 77 / 100: avg data time: 5.66e-02, avg batch time: 0.4688, average train loss: 2.3024
[09/26 09:41:06 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1584, average loss: 2.2605
[09/26 09:41:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 09:41:06 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/26 09:41:13 visual_prompt]: Epoch 78 / 100: avg data time: 6.08e-02, avg batch time: 0.4724, average train loss: 2.2714
[09/26 09:41:14 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.1575, average loss: 2.3196
[09/26 09:41:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 09:41:14 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/26 09:41:21 visual_prompt]: Epoch 79 / 100: avg data time: 5.68e-02, avg batch time: 0.4694, average train loss: 2.2902
[09/26 09:41:22 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1581, average loss: 2.3007
[09/26 09:41:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 64.00	
[09/26 09:41:22 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/26 09:41:29 visual_prompt]: Epoch 80 / 100: avg data time: 5.50e-02, avg batch time: 0.4671, average train loss: 2.2694
[09/26 09:41:30 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1579, average loss: 2.2524
[09/26 09:41:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 09:41:30 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/26 09:41:36 visual_prompt]: Epoch 81 / 100: avg data time: 4.77e-02, avg batch time: 0.4613, average train loss: 2.2666
[09/26 09:41:38 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1579, average loss: 2.2432
[09/26 09:41:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 09:41:38 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/26 09:41:45 visual_prompt]: Epoch 82 / 100: avg data time: 6.24e-02, avg batch time: 0.4749, average train loss: 2.2565
[09/26 09:41:46 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1581, average loss: 2.2415
[09/26 09:41:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 09:41:46 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/26 09:41:53 visual_prompt]: Epoch 83 / 100: avg data time: 6.48e-02, avg batch time: 0.4765, average train loss: 2.2571
[09/26 09:41:54 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1583, average loss: 2.2254
[09/26 09:41:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 09:41:54 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/26 09:42:00 visual_prompt]: Epoch 84 / 100: avg data time: 4.89e-02, avg batch time: 0.4622, average train loss: 2.2489
[09/26 09:42:02 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1578, average loss: 2.2355
[09/26 09:42:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 09:42:02 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/26 09:42:08 visual_prompt]: Epoch 85 / 100: avg data time: 4.87e-02, avg batch time: 0.4642, average train loss: 2.2479
[09/26 09:42:10 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1583, average loss: 2.2135
[09/26 09:42:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 09:42:10 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/26 09:42:16 visual_prompt]: Epoch 86 / 100: avg data time: 4.38e-02, avg batch time: 0.4575, average train loss: 2.2353
[09/26 09:42:18 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1576, average loss: 2.2330
[09/26 09:42:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 09:42:18 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/26 09:42:24 visual_prompt]: Epoch 87 / 100: avg data time: 5.57e-02, avg batch time: 0.4682, average train loss: 2.2502
[09/26 09:42:26 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1576, average loss: 2.2248
[09/26 09:42:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 09:42:26 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/26 09:42:32 visual_prompt]: Epoch 88 / 100: avg data time: 5.38e-02, avg batch time: 0.4682, average train loss: 2.2531
[09/26 09:42:34 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1582, average loss: 2.2148
[09/26 09:42:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 09:42:34 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/26 09:42:40 visual_prompt]: Epoch 89 / 100: avg data time: 5.91e-02, avg batch time: 0.4709, average train loss: 2.2401
[09/26 09:42:42 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1580, average loss: 2.2233
[09/26 09:42:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 09:42:42 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/26 09:42:48 visual_prompt]: Epoch 90 / 100: avg data time: 6.16e-02, avg batch time: 0.4733, average train loss: 2.2417
[09/26 09:42:50 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1583, average loss: 2.2170
[09/26 09:42:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 09:42:50 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/26 09:42:56 visual_prompt]: Epoch 91 / 100: avg data time: 5.96e-02, avg batch time: 0.4718, average train loss: 2.2402
[09/26 09:42:58 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1583, average loss: 2.2187
[09/26 09:42:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 09:42:58 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/26 09:43:04 visual_prompt]: Epoch 92 / 100: avg data time: 5.94e-02, avg batch time: 0.4712, average train loss: 2.2318
[09/26 09:43:06 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1598, average loss: 2.2109
[09/26 09:43:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 09:43:06 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/26 09:43:12 visual_prompt]: Epoch 93 / 100: avg data time: 5.72e-02, avg batch time: 0.4713, average train loss: 2.2298
[09/26 09:43:14 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1575, average loss: 2.2144
[09/26 09:43:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 09:43:14 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/26 09:43:20 visual_prompt]: Epoch 94 / 100: avg data time: 5.96e-02, avg batch time: 0.4714, average train loss: 2.2305
[09/26 09:43:22 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1578, average loss: 2.2157
[09/26 09:43:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 09:43:22 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/26 09:43:28 visual_prompt]: Epoch 95 / 100: avg data time: 6.11e-02, avg batch time: 0.4739, average train loss: 2.2280
[09/26 09:43:30 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1582, average loss: 2.2111
[09/26 09:43:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 64.50	
[09/26 09:43:30 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/26 09:43:36 visual_prompt]: Epoch 96 / 100: avg data time: 6.22e-02, avg batch time: 0.4738, average train loss: 2.2258
[09/26 09:43:38 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1583, average loss: 2.2087
[09/26 09:43:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 09:43:38 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/26 09:43:44 visual_prompt]: Epoch 97 / 100: avg data time: 5.73e-02, avg batch time: 0.4695, average train loss: 2.2244
[09/26 09:43:46 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1582, average loss: 2.2079
[09/26 09:43:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 09:43:46 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/26 09:43:52 visual_prompt]: Epoch 98 / 100: avg data time: 5.83e-02, avg batch time: 0.4699, average train loss: 2.2228
[09/26 09:43:54 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1577, average loss: 2.2062
[09/26 09:43:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 09:43:54 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/26 09:44:00 visual_prompt]: Epoch 99 / 100: avg data time: 4.38e-02, avg batch time: 0.4576, average train loss: 2.2221
[09/26 09:44:02 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1579, average loss: 2.2062
[09/26 09:44:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 09:44:02 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/26 09:44:08 visual_prompt]: Epoch 100 / 100: avg data time: 5.75e-02, avg batch time: 0.4692, average train loss: 2.2212
[09/26 09:44:09 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1582, average loss: 2.2062
[09/26 09:44:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 09:44:10 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 09:44:10 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 09:44:10 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 09:44:10 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 09:44:10 visual_prompt]: Training with config:
[09/26 09:44:10 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr5.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 09:44:10 visual_prompt]: Loading training data...
[09/26 09:44:10 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 09:44:10 visual_prompt]: Number of images: 800
[09/26 09:44:10 visual_prompt]: Number of classes: 10 / 10
[09/26 09:44:10 visual_prompt]: Loading validation data...
[09/26 09:44:10 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 09:44:11 visual_prompt]: Number of images: 200
[09/26 09:44:11 visual_prompt]: Number of classes: 10 / 10
[09/26 09:44:11 visual_prompt]: Constructing models...
[09/26 09:44:13 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 09:44:13 visual_prompt]: tuned percent:0.543
[09/26 09:44:13 visual_prompt]: Device used for model: 0
[09/26 09:44:13 visual_prompt]: Setting up Evaluator...
[09/26 09:44:13 visual_prompt]: Setting up Trainer...
[09/26 09:44:13 visual_prompt]: 	Setting up the optimizer...
[09/26 09:44:13 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 09:44:20 visual_prompt]: Epoch 1 / 100: avg data time: 5.21e-02, avg batch time: 0.4729, average train loss: 2.6636
[09/26 09:44:21 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1585, average loss: 2.6214
[09/26 09:44:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 09:44:21 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 09:44:21 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/26 09:44:28 visual_prompt]: Epoch 2 / 100: avg data time: 5.51e-02, avg batch time: 0.4671, average train loss: 4.9460
[09/26 09:44:29 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1575, average loss: 2.5807
[09/26 09:44:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 47.50	
[09/26 09:44:29 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/26 09:44:36 visual_prompt]: Epoch 3 / 100: avg data time: 4.68e-02, avg batch time: 0.4601, average train loss: 3.2615
[09/26 09:44:37 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1582, average loss: 2.8438
[09/26 09:44:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 44.50	
[09/26 09:44:37 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/26 09:44:44 visual_prompt]: Epoch 4 / 100: avg data time: 6.36e-02, avg batch time: 0.4753, average train loss: 2.7582
[09/26 09:44:45 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1581, average loss: 2.6116
[09/26 09:44:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 52.50	
[09/26 09:44:45 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/26 09:44:52 visual_prompt]: Epoch 5 / 100: avg data time: 6.07e-02, avg batch time: 0.4736, average train loss: 2.5708
[09/26 09:44:53 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1582, average loss: 2.5561
[09/26 09:44:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.00	
[09/26 09:44:53 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/26 09:45:00 visual_prompt]: Epoch 6 / 100: avg data time: 6.50e-02, avg batch time: 0.4762, average train loss: 2.6546
[09/26 09:45:01 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1580, average loss: 2.4566
[09/26 09:45:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 52.00	
[09/26 09:45:01 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/26 09:45:08 visual_prompt]: Epoch 7 / 100: avg data time: 6.05e-02, avg batch time: 0.4725, average train loss: 3.2457
[09/26 09:45:09 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1583, average loss: 3.5671
[09/26 09:45:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 44.00	
[09/26 09:45:09 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/26 09:45:16 visual_prompt]: Epoch 8 / 100: avg data time: 7.22e-02, avg batch time: 0.4836, average train loss: 3.1782
[09/26 09:45:17 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1581, average loss: 3.3791
[09/26 09:45:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 52.00	
[09/26 09:45:17 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/26 09:45:24 visual_prompt]: Epoch 9 / 100: avg data time: 6.27e-02, avg batch time: 0.4742, average train loss: 4.4090
[09/26 09:45:26 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1584, average loss: 4.7489
[09/26 09:45:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 41.00	
[09/26 09:45:26 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/26 09:45:32 visual_prompt]: Epoch 10 / 100: avg data time: 5.70e-02, avg batch time: 0.4701, average train loss: 6.1432
[09/26 09:45:34 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1584, average loss: 13.1404
[09/26 09:45:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.00	
[09/26 09:45:34 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/26 09:45:40 visual_prompt]: Epoch 11 / 100: avg data time: 6.66e-02, avg batch time: 0.4797, average train loss: 9.9529
[09/26 09:45:42 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1584, average loss: 11.9450
[09/26 09:45:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 55.50	
[09/26 09:45:42 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/26 09:45:48 visual_prompt]: Epoch 12 / 100: avg data time: 6.14e-02, avg batch time: 0.4741, average train loss: 11.5885
[09/26 09:45:50 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1582, average loss: 7.4546
[09/26 09:45:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 36.00	
[09/26 09:45:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/26 09:45:56 visual_prompt]: Epoch 13 / 100: avg data time: 5.53e-02, avg batch time: 0.4679, average train loss: 15.2302
[09/26 09:45:58 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1583, average loss: 24.5212
[09/26 09:45:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 41.50	
[09/26 09:45:58 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/26 09:46:04 visual_prompt]: Epoch 14 / 100: avg data time: 5.84e-02, avg batch time: 0.4701, average train loss: 21.8674
[09/26 09:46:06 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1580, average loss: 25.8854
[09/26 09:46:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 46.00	
[09/26 09:46:06 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/26 09:46:12 visual_prompt]: Epoch 15 / 100: avg data time: 6.07e-02, avg batch time: 0.4733, average train loss: 23.1145
[09/26 09:46:14 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1578, average loss: 11.3866
[09/26 09:46:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 50.00	
[09/26 09:46:14 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/26 09:46:20 visual_prompt]: Epoch 16 / 100: avg data time: 6.93e-02, avg batch time: 0.4812, average train loss: 16.9130
[09/26 09:46:22 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1585, average loss: 18.3069
[09/26 09:46:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 09:46:22 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/26 09:46:28 visual_prompt]: Epoch 17 / 100: avg data time: 5.63e-02, avg batch time: 0.4686, average train loss: 15.0686
[09/26 09:46:30 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1581, average loss: 10.5958
[09/26 09:46:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 44.00	
[09/26 09:46:30 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/26 09:46:36 visual_prompt]: Epoch 18 / 100: avg data time: 5.67e-02, avg batch time: 0.4684, average train loss: 10.9983
[09/26 09:46:38 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1580, average loss: 10.0423
[09/26 09:46:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.00	top5: 41.50	
[09/26 09:46:38 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/26 09:46:44 visual_prompt]: Epoch 19 / 100: avg data time: 4.90e-02, avg batch time: 0.4624, average train loss: 9.2958
[09/26 09:46:46 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1584, average loss: 6.4562
[09/26 09:46:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 54.00	
[09/26 09:46:46 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/26 09:46:52 visual_prompt]: Epoch 20 / 100: avg data time: 5.65e-02, avg batch time: 0.4701, average train loss: 7.8342
[09/26 09:46:54 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1584, average loss: 7.4217
[09/26 09:46:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 44.50	
[09/26 09:46:54 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/26 09:47:00 visual_prompt]: Epoch 21 / 100: avg data time: 5.58e-02, avg batch time: 0.4687, average train loss: 5.2273
[09/26 09:47:02 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1584, average loss: 4.8928
[09/26 09:47:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 09:47:02 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/26 09:47:08 visual_prompt]: Epoch 22 / 100: avg data time: 6.00e-02, avg batch time: 0.4742, average train loss: 3.7812
[09/26 09:47:10 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.1582, average loss: 3.6414
[09/26 09:47:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 09:47:10 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/26 09:47:16 visual_prompt]: Epoch 23 / 100: avg data time: 5.62e-02, avg batch time: 0.4681, average train loss: 3.2012
[09/26 09:47:18 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1574, average loss: 2.7913
[09/26 09:47:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.00	
[09/26 09:47:18 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/26 09:47:24 visual_prompt]: Epoch 24 / 100: avg data time: 5.63e-02, avg batch time: 0.4698, average train loss: 3.0490
[09/26 09:47:26 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1580, average loss: 3.1316
[09/26 09:47:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.50	
[09/26 09:47:26 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/26 09:47:32 visual_prompt]: Epoch 25 / 100: avg data time: 6.61e-02, avg batch time: 0.4779, average train loss: 2.9483
[09/26 09:47:34 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1580, average loss: 2.6274
[09/26 09:47:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 09:47:34 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/26 09:47:40 visual_prompt]: Epoch 26 / 100: avg data time: 5.79e-02, avg batch time: 0.4702, average train loss: 2.7306
[09/26 09:47:42 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1585, average loss: 2.7480
[09/26 09:47:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/26 09:47:42 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/26 09:47:48 visual_prompt]: Epoch 27 / 100: avg data time: 5.74e-02, avg batch time: 0.4697, average train loss: 2.6165
[09/26 09:47:50 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1580, average loss: 2.5658
[09/26 09:47:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.00	
[09/26 09:47:50 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/26 09:47:56 visual_prompt]: Epoch 28 / 100: avg data time: 5.55e-02, avg batch time: 0.4682, average train loss: 2.5796
[09/26 09:47:58 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1582, average loss: 2.5948
[09/26 09:47:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.50	
[09/26 09:47:58 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/26 09:48:04 visual_prompt]: Epoch 29 / 100: avg data time: 5.90e-02, avg batch time: 0.4727, average train loss: 2.6237
[09/26 09:48:06 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1582, average loss: 2.3095
[09/26 09:48:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 21.00	top5: 59.00	
[09/26 09:48:06 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/26 09:48:13 visual_prompt]: Epoch 30 / 100: avg data time: 6.51e-02, avg batch time: 0.4779, average train loss: 2.6490
[09/26 09:48:14 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1580, average loss: 2.3438
[09/26 09:48:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.50	
[09/26 09:48:14 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/26 09:48:21 visual_prompt]: Epoch 31 / 100: avg data time: 6.71e-02, avg batch time: 0.4787, average train loss: 2.4902
[09/26 09:48:22 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1583, average loss: 2.4130
[09/26 09:48:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/26 09:48:22 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/26 09:48:29 visual_prompt]: Epoch 32 / 100: avg data time: 6.21e-02, avg batch time: 0.4737, average train loss: 2.6063
[09/26 09:48:30 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1582, average loss: 2.4087
[09/26 09:48:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 09:48:30 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/26 09:48:37 visual_prompt]: Epoch 33 / 100: avg data time: 5.42e-02, avg batch time: 0.4658, average train loss: 2.5332
[09/26 09:48:38 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1580, average loss: 2.4774
[09/26 09:48:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.00	top5: 63.00	
[09/26 09:48:38 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/26 09:48:44 visual_prompt]: Epoch 34 / 100: avg data time: 4.58e-02, avg batch time: 0.4591, average train loss: 2.4213
[09/26 09:48:46 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1580, average loss: 2.3968
[09/26 09:48:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.00	top5: 63.00	
[09/26 09:48:46 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/26 09:48:53 visual_prompt]: Epoch 35 / 100: avg data time: 6.12e-02, avg batch time: 0.4728, average train loss: 2.4428
[09/26 09:48:54 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.1581, average loss: 2.5105
[09/26 09:48:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 62.00	
[09/26 09:48:54 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/26 09:49:01 visual_prompt]: Epoch 36 / 100: avg data time: 6.57e-02, avg batch time: 0.4789, average train loss: 2.4771
[09/26 09:49:02 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1584, average loss: 2.4312
[09/26 09:49:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.50	top5: 61.50	
[09/26 09:49:02 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/26 09:49:09 visual_prompt]: Epoch 37 / 100: avg data time: 5.53e-02, avg batch time: 0.4680, average train loss: 2.5019
[09/26 09:49:10 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1582, average loss: 2.3720
[09/26 09:49:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 64.50	
[09/26 09:49:10 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/26 09:49:17 visual_prompt]: Epoch 38 / 100: avg data time: 5.49e-02, avg batch time: 0.4678, average train loss: 2.4292
[09/26 09:49:18 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1581, average loss: 2.5538
[09/26 09:49:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 09:49:18 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/26 09:49:25 visual_prompt]: Epoch 39 / 100: avg data time: 6.61e-02, avg batch time: 0.4785, average train loss: 2.4852
[09/26 09:49:26 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1584, average loss: 2.5352
[09/26 09:49:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 63.50	
[09/26 09:49:26 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/26 09:49:33 visual_prompt]: Epoch 40 / 100: avg data time: 5.78e-02, avg batch time: 0.4708, average train loss: 2.4651
[09/26 09:49:34 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1579, average loss: 2.4875
[09/26 09:49:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 55.00	
[09/26 09:49:34 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/26 09:49:41 visual_prompt]: Epoch 41 / 100: avg data time: 5.53e-02, avg batch time: 0.4671, average train loss: 2.5048
[09/26 09:49:42 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1582, average loss: 2.5461
[09/26 09:49:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 63.00	
[09/26 09:49:42 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/26 09:49:49 visual_prompt]: Epoch 42 / 100: avg data time: 6.60e-02, avg batch time: 0.4776, average train loss: 2.5396
[09/26 09:49:50 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1586, average loss: 2.3864
[09/26 09:49:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 62.50	
[09/26 09:49:50 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/26 09:49:57 visual_prompt]: Epoch 43 / 100: avg data time: 6.26e-02, avg batch time: 0.4765, average train loss: 2.6466
[09/26 09:49:58 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1582, average loss: 2.5198
[09/26 09:49:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 51.50	
[09/26 09:49:58 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/26 09:50:05 visual_prompt]: Epoch 44 / 100: avg data time: 6.21e-02, avg batch time: 0.4739, average train loss: 2.4074
[09/26 09:50:06 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1578, average loss: 2.3166
[09/26 09:50:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 09:50:06 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/26 09:50:13 visual_prompt]: Epoch 45 / 100: avg data time: 5.47e-02, avg batch time: 0.4666, average train loss: 2.4577
[09/26 09:50:14 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1580, average loss: 2.2497
[09/26 09:50:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 66.50	
[09/26 09:50:14 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/26 09:50:21 visual_prompt]: Epoch 46 / 100: avg data time: 5.35e-02, avg batch time: 0.4659, average train loss: 2.3505
[09/26 09:50:22 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1586, average loss: 2.2971
[09/26 09:50:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 09:50:22 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/26 09:50:29 visual_prompt]: Epoch 47 / 100: avg data time: 5.78e-02, avg batch time: 0.4709, average train loss: 2.3410
[09/26 09:50:30 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1580, average loss: 2.4539
[09/26 09:50:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 51.00	
[09/26 09:50:30 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/26 09:50:37 visual_prompt]: Epoch 48 / 100: avg data time: 6.11e-02, avg batch time: 0.4736, average train loss: 2.3925
[09/26 09:50:38 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.1583, average loss: 2.8390
[09/26 09:50:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.50	
[09/26 09:50:38 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/26 09:50:45 visual_prompt]: Epoch 49 / 100: avg data time: 6.49e-02, avg batch time: 0.4770, average train loss: 2.6093
[09/26 09:50:46 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1580, average loss: 2.4883
[09/26 09:50:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 54.50	
[09/26 09:50:46 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/26 09:50:53 visual_prompt]: Epoch 50 / 100: avg data time: 5.65e-02, avg batch time: 0.4700, average train loss: 2.4957
[09/26 09:50:54 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1577, average loss: 2.3542
[09/26 09:50:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.50	top5: 59.50	
[09/26 09:50:54 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/26 09:51:01 visual_prompt]: Epoch 51 / 100: avg data time: 4.70e-02, avg batch time: 0.4603, average train loss: 2.6158
[09/26 09:51:02 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1576, average loss: 2.6039
[09/26 09:51:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 09:51:02 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/26 09:51:09 visual_prompt]: Epoch 52 / 100: avg data time: 5.57e-02, avg batch time: 0.4676, average train loss: 2.4260
[09/26 09:51:10 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1583, average loss: 2.3232
[09/26 09:51:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 09:51:10 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/26 09:51:17 visual_prompt]: Epoch 53 / 100: avg data time: 6.22e-02, avg batch time: 0.4751, average train loss: 2.3733
[09/26 09:51:18 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1577, average loss: 2.3101
[09/26 09:51:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.50	top5: 58.50	
[09/26 09:51:18 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/26 09:51:25 visual_prompt]: Epoch 54 / 100: avg data time: 6.58e-02, avg batch time: 0.4779, average train loss: 2.4288
[09/26 09:51:26 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1581, average loss: 2.4409
[09/26 09:51:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.50	top5: 64.00	
[09/26 09:51:26 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/26 09:51:33 visual_prompt]: Epoch 55 / 100: avg data time: 5.62e-02, avg batch time: 0.4696, average train loss: 2.4498
[09/26 09:51:34 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1579, average loss: 2.5049
[09/26 09:51:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.50	
[09/26 09:51:34 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/26 09:51:41 visual_prompt]: Epoch 56 / 100: avg data time: 6.77e-02, avg batch time: 0.4797, average train loss: 2.4018
[09/26 09:51:42 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.1582, average loss: 2.2664
[09/26 09:51:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.00	top5: 61.00	
[09/26 09:51:42 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/26 09:51:49 visual_prompt]: Epoch 57 / 100: avg data time: 4.52e-02, avg batch time: 0.4592, average train loss: 2.3573
[09/26 09:51:50 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1581, average loss: 2.2888
[09/26 09:51:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 19.50	top5: 59.50	
[09/26 09:51:50 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/26 09:51:57 visual_prompt]: Epoch 58 / 100: avg data time: 6.50e-02, avg batch time: 0.4777, average train loss: 2.3319
[09/26 09:51:58 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1585, average loss: 2.4723
[09/26 09:51:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 09:51:58 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/26 09:52:05 visual_prompt]: Epoch 59 / 100: avg data time: 4.92e-02, avg batch time: 0.4613, average train loss: 2.3578
[09/26 09:52:06 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1581, average loss: 2.4069
[09/26 09:52:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.00	top5: 57.50	
[09/26 09:52:06 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/26 09:52:13 visual_prompt]: Epoch 60 / 100: avg data time: 6.01e-02, avg batch time: 0.4726, average train loss: 2.3069
[09/26 09:52:14 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1581, average loss: 2.2862
[09/26 09:52:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.00	top5: 56.50	
[09/26 09:52:14 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/26 09:52:21 visual_prompt]: Epoch 61 / 100: avg data time: 6.53e-02, avg batch time: 0.4768, average train loss: 2.3438
[09/26 09:52:22 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1582, average loss: 2.4890
[09/26 09:52:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.50	
[09/26 09:52:22 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/26 09:52:29 visual_prompt]: Epoch 62 / 100: avg data time: 6.45e-02, avg batch time: 0.4775, average train loss: 2.3335
[09/26 09:52:30 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1581, average loss: 2.2377
[09/26 09:52:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 09:52:30 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/26 09:52:37 visual_prompt]: Epoch 63 / 100: avg data time: 6.41e-02, avg batch time: 0.4762, average train loss: 2.3115
[09/26 09:52:38 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1584, average loss: 2.4744
[09/26 09:52:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 61.00	
[09/26 09:52:38 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/26 09:52:45 visual_prompt]: Epoch 64 / 100: avg data time: 4.80e-02, avg batch time: 0.4602, average train loss: 2.3666
[09/26 09:52:46 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1584, average loss: 2.4109
[09/26 09:52:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 17.50	top5: 63.00	
[09/26 09:52:46 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/26 09:52:53 visual_prompt]: Epoch 65 / 100: avg data time: 5.63e-02, avg batch time: 0.4685, average train loss: 2.3869
[09/26 09:52:54 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1586, average loss: 2.2481
[09/26 09:52:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 21.00	top5: 65.50	
[09/26 09:52:54 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/26 09:53:01 visual_prompt]: Epoch 66 / 100: avg data time: 5.13e-02, avg batch time: 0.4629, average train loss: 2.3244
[09/26 09:53:02 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1580, average loss: 2.3298
[09/26 09:53:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.50	top5: 64.50	
[09/26 09:53:02 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/26 09:53:09 visual_prompt]: Epoch 67 / 100: avg data time: 6.38e-02, avg batch time: 0.4754, average train loss: 2.3493
[09/26 09:53:10 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1578, average loss: 2.2627
[09/26 09:53:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/26 09:53:10 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/26 09:53:17 visual_prompt]: Epoch 68 / 100: avg data time: 6.66e-02, avg batch time: 0.4784, average train loss: 2.2913
[09/26 09:53:18 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1580, average loss: 2.2710
[09/26 09:53:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 14.00	top5: 63.00	
[09/26 09:53:18 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/26 09:53:24 visual_prompt]: Epoch 69 / 100: avg data time: 4.50e-02, avg batch time: 0.4589, average train loss: 2.2943
[09/26 09:53:26 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1580, average loss: 2.2754
[09/26 09:53:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.00	top5: 64.00	
[09/26 09:53:26 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/26 09:53:33 visual_prompt]: Epoch 70 / 100: avg data time: 6.75e-02, avg batch time: 0.4791, average train loss: 2.2528
[09/26 09:53:34 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.1578, average loss: 2.2649
[09/26 09:53:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 21.50	top5: 64.50	
[09/26 09:53:34 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/26 09:53:41 visual_prompt]: Epoch 71 / 100: avg data time: 6.21e-02, avg batch time: 0.4736, average train loss: 2.2402
[09/26 09:53:42 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1578, average loss: 2.2035
[09/26 09:53:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 21.50	top5: 66.50	
[09/26 09:53:42 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/26 09:53:49 visual_prompt]: Epoch 72 / 100: avg data time: 5.84e-02, avg batch time: 0.4704, average train loss: 2.1963
[09/26 09:53:50 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1575, average loss: 2.2478
[09/26 09:53:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 18.50	top5: 64.00	
[09/26 09:53:50 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/26 09:53:57 visual_prompt]: Epoch 73 / 100: avg data time: 5.96e-02, avg batch time: 0.4719, average train loss: 2.2274
[09/26 09:53:58 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1579, average loss: 2.2323
[09/26 09:53:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 21.00	top5: 65.00	
[09/26 09:53:58 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/26 09:54:05 visual_prompt]: Epoch 74 / 100: avg data time: 6.64e-02, avg batch time: 0.4784, average train loss: 2.1701
[09/26 09:54:06 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1581, average loss: 2.1957
[09/26 09:54:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 65.50	
[09/26 09:54:06 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/26 09:54:13 visual_prompt]: Epoch 75 / 100: avg data time: 5.97e-02, avg batch time: 0.4725, average train loss: 2.1772
[09/26 09:54:14 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1583, average loss: 2.2060
[09/26 09:54:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.00	top5: 63.50	
[09/26 09:54:14 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/26 09:54:21 visual_prompt]: Epoch 76 / 100: avg data time: 5.37e-02, avg batch time: 0.4660, average train loss: 2.1976
[09/26 09:54:22 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1583, average loss: 2.2102
[09/26 09:54:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.00	top5: 68.00	
[09/26 09:54:22 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/26 09:54:29 visual_prompt]: Epoch 77 / 100: avg data time: 6.28e-02, avg batch time: 0.4766, average train loss: 2.1719
[09/26 09:54:30 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1581, average loss: 2.2673
[09/26 09:54:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.00	top5: 66.50	
[09/26 09:54:30 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/26 09:54:37 visual_prompt]: Epoch 78 / 100: avg data time: 5.59e-02, avg batch time: 0.4673, average train loss: 2.1673
[09/26 09:54:38 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1579, average loss: 2.2363
[09/26 09:54:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.00	top5: 66.50	
[09/26 09:54:38 visual_prompt]: Best epoch 78: best metric: 0.240
[09/26 09:54:38 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/26 09:54:45 visual_prompt]: Epoch 79 / 100: avg data time: 6.19e-02, avg batch time: 0.4742, average train loss: 2.2108
[09/26 09:54:46 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1577, average loss: 2.2251
[09/26 09:54:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 65.50	
[09/26 09:54:46 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/26 09:54:53 visual_prompt]: Epoch 80 / 100: avg data time: 5.73e-02, avg batch time: 0.4704, average train loss: 2.1331
[09/26 09:54:54 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1580, average loss: 2.1861
[09/26 09:54:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.00	top5: 71.00	
[09/26 09:54:54 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/26 09:55:01 visual_prompt]: Epoch 81 / 100: avg data time: 6.36e-02, avg batch time: 0.4757, average train loss: 2.1082
[09/26 09:55:03 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1583, average loss: 2.1800
[09/26 09:55:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 66.50	
[09/26 09:55:03 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/26 09:55:09 visual_prompt]: Epoch 82 / 100: avg data time: 5.51e-02, avg batch time: 0.4669, average train loss: 2.0971
[09/26 09:55:11 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1581, average loss: 2.2297
[09/26 09:55:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.00	top5: 72.50	
[09/26 09:55:11 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/26 09:55:17 visual_prompt]: Epoch 83 / 100: avg data time: 5.62e-02, avg batch time: 0.4679, average train loss: 2.1285
[09/26 09:55:19 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1578, average loss: 2.2063
[09/26 09:55:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 17.00	top5: 66.00	
[09/26 09:55:19 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/26 09:55:25 visual_prompt]: Epoch 84 / 100: avg data time: 6.43e-02, avg batch time: 0.4757, average train loss: 2.1051
[09/26 09:55:27 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1583, average loss: 2.2046
[09/26 09:55:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 68.50	
[09/26 09:55:27 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/26 09:55:33 visual_prompt]: Epoch 85 / 100: avg data time: 4.59e-02, avg batch time: 0.4600, average train loss: 2.1020
[09/26 09:55:35 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1579, average loss: 2.2015
[09/26 09:55:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.00	top5: 67.00	
[09/26 09:55:35 visual_prompt]: Best epoch 85: best metric: 0.250
[09/26 09:55:35 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/26 09:55:41 visual_prompt]: Epoch 86 / 100: avg data time: 5.02e-02, avg batch time: 0.4630, average train loss: 2.1025
[09/26 09:55:42 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.1578, average loss: 2.2261
[09/26 09:55:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 71.00	
[09/26 09:55:42 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/26 09:55:49 visual_prompt]: Epoch 87 / 100: avg data time: 5.58e-02, avg batch time: 0.4676, average train loss: 2.0773
[09/26 09:55:50 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1577, average loss: 2.2014
[09/26 09:55:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.00	top5: 71.00	
[09/26 09:55:50 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/26 09:55:57 visual_prompt]: Epoch 88 / 100: avg data time: 5.57e-02, avg batch time: 0.4693, average train loss: 2.0525
[09/26 09:55:58 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1579, average loss: 2.1975
[09/26 09:55:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 69.00	
[09/26 09:55:58 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/26 09:56:05 visual_prompt]: Epoch 89 / 100: avg data time: 5.97e-02, avg batch time: 0.4714, average train loss: 2.0724
[09/26 09:56:06 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1576, average loss: 2.1786
[09/26 09:56:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 74.00	
[09/26 09:56:06 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/26 09:56:13 visual_prompt]: Epoch 90 / 100: avg data time: 6.16e-02, avg batch time: 0.4738, average train loss: 2.0422
[09/26 09:56:14 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1581, average loss: 2.1586
[09/26 09:56:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 70.50	
[09/26 09:56:14 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/26 09:56:21 visual_prompt]: Epoch 91 / 100: avg data time: 5.07e-02, avg batch time: 0.4640, average train loss: 2.0380
[09/26 09:56:22 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1577, average loss: 2.1679
[09/26 09:56:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.50	top5: 71.50	
[09/26 09:56:22 visual_prompt]: Best epoch 91: best metric: 0.255
[09/26 09:56:22 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/26 09:56:29 visual_prompt]: Epoch 92 / 100: avg data time: 6.15e-02, avg batch time: 0.4733, average train loss: 2.0124
[09/26 09:56:30 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1583, average loss: 2.2039
[09/26 09:56:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.00	top5: 70.50	
[09/26 09:56:30 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/26 09:56:37 visual_prompt]: Epoch 93 / 100: avg data time: 6.18e-02, avg batch time: 0.4743, average train loss: 2.0175
[09/26 09:56:38 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1583, average loss: 2.1681
[09/26 09:56:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 72.00	
[09/26 09:56:38 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/26 09:56:45 visual_prompt]: Epoch 94 / 100: avg data time: 6.78e-02, avg batch time: 0.4793, average train loss: 1.9992
[09/26 09:56:46 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1584, average loss: 2.1979
[09/26 09:56:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 73.50	
[09/26 09:56:46 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/26 09:56:53 visual_prompt]: Epoch 95 / 100: avg data time: 5.49e-02, avg batch time: 0.4678, average train loss: 1.9988
[09/26 09:56:54 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1580, average loss: 2.1822
[09/26 09:56:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.00	top5: 72.50	
[09/26 09:56:54 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/26 09:57:01 visual_prompt]: Epoch 96 / 100: avg data time: 5.80e-02, avg batch time: 0.4710, average train loss: 1.9962
[09/26 09:57:02 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1581, average loss: 2.1994
[09/26 09:57:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 70.50	
[09/26 09:57:02 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/26 09:57:09 visual_prompt]: Epoch 97 / 100: avg data time: 5.27e-02, avg batch time: 0.4660, average train loss: 1.9901
[09/26 09:57:10 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.1581, average loss: 2.1872
[09/26 09:57:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 72.00	
[09/26 09:57:10 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/26 09:57:17 visual_prompt]: Epoch 98 / 100: avg data time: 4.92e-02, avg batch time: 0.4626, average train loss: 1.9878
[09/26 09:57:18 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1581, average loss: 2.1859
[09/26 09:57:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 73.00	
[09/26 09:57:18 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/26 09:57:25 visual_prompt]: Epoch 99 / 100: avg data time: 6.17e-02, avg batch time: 0.4762, average train loss: 1.9793
[09/26 09:57:26 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1583, average loss: 2.1891
[09/26 09:57:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 71.50	
[09/26 09:57:26 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/26 09:57:33 visual_prompt]: Epoch 100 / 100: avg data time: 5.46e-02, avg batch time: 0.4686, average train loss: 1.9814
[09/26 09:57:34 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1581, average loss: 2.1894
[09/26 09:57:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 71.50	
[09/26 09:57:34 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 09:57:34 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 09:57:34 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 09:57:34 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 09:57:34 visual_prompt]: Training with config:
[09/26 09:57:34 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr5.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 09:57:34 visual_prompt]: Loading training data...
[09/26 09:57:34 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 09:57:35 visual_prompt]: Number of images: 800
[09/26 09:57:35 visual_prompt]: Number of classes: 10 / 10
[09/26 09:57:35 visual_prompt]: Loading validation data...
[09/26 09:57:35 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 09:57:36 visual_prompt]: Number of images: 200
[09/26 09:57:36 visual_prompt]: Number of classes: 10 / 10
[09/26 09:57:36 visual_prompt]: Constructing models...
[09/26 09:57:38 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 09:57:38 visual_prompt]: tuned percent:0.543
[09/26 09:57:38 visual_prompt]: Device used for model: 0
[09/26 09:57:38 visual_prompt]: Setting up Evaluator...
[09/26 09:57:38 visual_prompt]: Setting up Trainer...
[09/26 09:57:38 visual_prompt]: 	Setting up the optimizer...
[09/26 09:57:38 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 09:57:45 visual_prompt]: Epoch 1 / 100: avg data time: 6.63e-02, avg batch time: 0.4843, average train loss: 2.6736
[09/26 09:57:46 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1582, average loss: 2.6214
[09/26 09:57:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 09:57:46 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 09:57:46 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/26 09:57:53 visual_prompt]: Epoch 2 / 100: avg data time: 6.99e-02, avg batch time: 0.4809, average train loss: 4.8165
[09/26 09:57:55 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1583, average loss: 2.3169
[09/26 09:57:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.00	top5: 61.50	
[09/26 09:57:55 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/26 09:58:01 visual_prompt]: Epoch 3 / 100: avg data time: 5.78e-02, avg batch time: 0.4690, average train loss: 3.1626
[09/26 09:58:03 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1573, average loss: 2.6399
[09/26 09:58:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.50	top5: 47.00	
[09/26 09:58:03 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/26 09:58:09 visual_prompt]: Epoch 4 / 100: avg data time: 6.53e-02, avg batch time: 0.4772, average train loss: 2.5971
[09/26 09:58:11 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1578, average loss: 2.4770
[09/26 09:58:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 62.00	
[09/26 09:58:11 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/26 09:58:17 visual_prompt]: Epoch 5 / 100: avg data time: 6.38e-02, avg batch time: 0.4748, average train loss: 2.6708
[09/26 09:58:19 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1582, average loss: 2.8755
[09/26 09:58:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 51.50	
[09/26 09:58:19 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/26 09:58:25 visual_prompt]: Epoch 6 / 100: avg data time: 6.77e-02, avg batch time: 0.4795, average train loss: 3.0335
[09/26 09:58:27 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1578, average loss: 2.7722
[09/26 09:58:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.00	
[09/26 09:58:27 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/26 09:58:33 visual_prompt]: Epoch 7 / 100: avg data time: 6.02e-02, avg batch time: 0.4717, average train loss: 3.4337
[09/26 09:58:35 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1583, average loss: 3.3050
[09/26 09:58:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 55.00	
[09/26 09:58:35 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/26 09:58:41 visual_prompt]: Epoch 8 / 100: avg data time: 6.45e-02, avg batch time: 0.4762, average train loss: 4.0578
[09/26 09:58:43 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1582, average loss: 5.8011
[09/26 09:58:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 55.50	
[09/26 09:58:43 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/26 09:58:50 visual_prompt]: Epoch 9 / 100: avg data time: 6.77e-02, avg batch time: 0.4793, average train loss: 8.8564
[09/26 09:58:51 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1580, average loss: 9.5130
[09/26 09:58:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 41.50	
[09/26 09:58:51 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/26 09:58:58 visual_prompt]: Epoch 10 / 100: avg data time: 6.39e-02, avg batch time: 0.4760, average train loss: 9.8153
[09/26 09:58:59 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1579, average loss: 9.0010
[09/26 09:58:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 52.50	
[09/26 09:58:59 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/26 09:59:06 visual_prompt]: Epoch 11 / 100: avg data time: 6.29e-02, avg batch time: 0.4743, average train loss: 11.2935
[09/26 09:59:07 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1580, average loss: 12.8137
[09/26 09:59:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 60.50	
[09/26 09:59:07 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/26 09:59:14 visual_prompt]: Epoch 12 / 100: avg data time: 6.67e-02, avg batch time: 0.4783, average train loss: 8.3839
[09/26 09:59:15 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1579, average loss: 10.6581
[09/26 09:59:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 41.50	
[09/26 09:59:15 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/26 09:59:22 visual_prompt]: Epoch 13 / 100: avg data time: 6.25e-02, avg batch time: 0.4757, average train loss: 10.0091
[09/26 09:59:24 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1578, average loss: 10.4209
[09/26 09:59:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 44.00	
[09/26 09:59:24 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/26 09:59:30 visual_prompt]: Epoch 14 / 100: avg data time: 6.62e-02, avg batch time: 0.4775, average train loss: 10.5799
[09/26 09:59:32 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1575, average loss: 12.7242
[09/26 09:59:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 45.50	
[09/26 09:59:32 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/26 09:59:38 visual_prompt]: Epoch 15 / 100: avg data time: 6.51e-02, avg batch time: 0.4761, average train loss: 11.5248
[09/26 09:59:40 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1580, average loss: 10.0121
[09/26 09:59:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.50	
[09/26 09:59:40 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/26 09:59:46 visual_prompt]: Epoch 16 / 100: avg data time: 6.01e-02, avg batch time: 0.4724, average train loss: 10.7646
[09/26 09:59:48 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1584, average loss: 7.7389
[09/26 09:59:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 64.00	
[09/26 09:59:48 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/26 09:59:54 visual_prompt]: Epoch 17 / 100: avg data time: 6.39e-02, avg batch time: 0.4757, average train loss: 7.1776
[09/26 09:59:56 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1580, average loss: 8.3299
[09/26 09:59:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 09:59:56 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/26 10:00:03 visual_prompt]: Epoch 18 / 100: avg data time: 6.87e-02, avg batch time: 0.4804, average train loss: 7.0681
[09/26 10:00:04 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1582, average loss: 4.9163
[09/26 10:00:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.00	top5: 64.00	
[09/26 10:00:04 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/26 10:00:11 visual_prompt]: Epoch 19 / 100: avg data time: 6.21e-02, avg batch time: 0.4743, average train loss: 5.5369
[09/26 10:00:12 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1579, average loss: 5.0481
[09/26 10:00:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 10:00:12 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/26 10:00:19 visual_prompt]: Epoch 20 / 100: avg data time: 6.29e-02, avg batch time: 0.4733, average train loss: 4.2122
[09/26 10:00:20 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1579, average loss: 4.2317
[09/26 10:00:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 41.00	
[09/26 10:00:20 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/26 10:00:27 visual_prompt]: Epoch 21 / 100: avg data time: 6.10e-02, avg batch time: 0.4720, average train loss: 3.6395
[09/26 10:00:28 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1574, average loss: 3.3559
[09/26 10:00:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 55.50	
[09/26 10:00:28 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/26 10:00:35 visual_prompt]: Epoch 22 / 100: avg data time: 6.52e-02, avg batch time: 0.4771, average train loss: 2.8941
[09/26 10:00:37 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1579, average loss: 3.0301
[09/26 10:00:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 10:00:37 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/26 10:00:43 visual_prompt]: Epoch 23 / 100: avg data time: 6.16e-02, avg batch time: 0.4736, average train loss: 3.2933
[09/26 10:00:45 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1580, average loss: 2.5835
[09/26 10:00:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 56.50	
[09/26 10:00:45 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/26 10:00:51 visual_prompt]: Epoch 24 / 100: avg data time: 6.32e-02, avg batch time: 0.4745, average train loss: 2.8839
[09/26 10:00:53 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1579, average loss: 2.7656
[09/26 10:00:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.00	
[09/26 10:00:53 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/26 10:00:59 visual_prompt]: Epoch 25 / 100: avg data time: 5.87e-02, avg batch time: 0.4723, average train loss: 2.6336
[09/26 10:01:01 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1582, average loss: 2.4695
[09/26 10:01:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/26 10:01:01 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/26 10:01:07 visual_prompt]: Epoch 26 / 100: avg data time: 5.41e-02, avg batch time: 0.4670, average train loss: 2.5487
[09/26 10:01:09 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1583, average loss: 2.8488
[09/26 10:01:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 49.00	
[09/26 10:01:09 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/26 10:01:15 visual_prompt]: Epoch 27 / 100: avg data time: 6.29e-02, avg batch time: 0.4743, average train loss: 2.6188
[09/26 10:01:17 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1580, average loss: 2.5596
[09/26 10:01:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 10:01:17 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/26 10:01:23 visual_prompt]: Epoch 28 / 100: avg data time: 5.64e-02, avg batch time: 0.4673, average train loss: 2.6271
[09/26 10:01:25 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1584, average loss: 2.3720
[09/26 10:01:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 13.50	top5: 62.00	
[09/26 10:01:25 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/26 10:01:31 visual_prompt]: Epoch 29 / 100: avg data time: 6.09e-02, avg batch time: 0.4727, average train loss: 2.5361
[09/26 10:01:33 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1582, average loss: 2.5075
[09/26 10:01:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 63.50	
[09/26 10:01:33 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/26 10:01:39 visual_prompt]: Epoch 30 / 100: avg data time: 5.59e-02, avg batch time: 0.4678, average train loss: 2.5824
[09/26 10:01:41 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1580, average loss: 2.3666
[09/26 10:01:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 13.50	top5: 57.00	
[09/26 10:01:41 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/26 10:01:47 visual_prompt]: Epoch 31 / 100: avg data time: 6.29e-02, avg batch time: 0.4742, average train loss: 2.4935
[09/26 10:01:49 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1581, average loss: 2.3575
[09/26 10:01:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 63.00	
[09/26 10:01:49 visual_prompt]: Best epoch 31: best metric: 0.235
[09/26 10:01:49 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/26 10:01:56 visual_prompt]: Epoch 32 / 100: avg data time: 6.73e-02, avg batch time: 0.4785, average train loss: 2.4603
[09/26 10:01:57 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1583, average loss: 2.7403
[09/26 10:01:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 64.00	
[09/26 10:01:57 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/26 10:02:04 visual_prompt]: Epoch 33 / 100: avg data time: 6.57e-02, avg batch time: 0.4791, average train loss: 2.5926
[09/26 10:02:05 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1585, average loss: 2.6313
[09/26 10:02:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 10:02:05 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/26 10:02:12 visual_prompt]: Epoch 34 / 100: avg data time: 6.54e-02, avg batch time: 0.4778, average train loss: 2.4223
[09/26 10:02:14 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1581, average loss: 2.3914
[09/26 10:02:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 65.00	
[09/26 10:02:14 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/26 10:02:20 visual_prompt]: Epoch 35 / 100: avg data time: 6.79e-02, avg batch time: 0.4806, average train loss: 2.2746
[09/26 10:02:22 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1579, average loss: 2.2077
[09/26 10:02:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 30.50	top5: 65.50	
[09/26 10:02:22 visual_prompt]: Best epoch 35: best metric: 0.305
[09/26 10:02:22 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/26 10:02:28 visual_prompt]: Epoch 36 / 100: avg data time: 5.07e-02, avg batch time: 0.4640, average train loss: 2.2782
[09/26 10:02:30 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1582, average loss: 2.5975
[09/26 10:02:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 16.00	top5: 64.50	
[09/26 10:02:30 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/26 10:02:36 visual_prompt]: Epoch 37 / 100: avg data time: 6.16e-02, avg batch time: 0.4738, average train loss: 2.0364
[09/26 10:02:38 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1582, average loss: 1.9035
[09/26 10:02:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 41.50	top5: 75.00	
[09/26 10:02:38 visual_prompt]: Best epoch 37: best metric: 0.415
[09/26 10:02:38 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/26 10:02:44 visual_prompt]: Epoch 38 / 100: avg data time: 6.26e-02, avg batch time: 0.4763, average train loss: 1.8077
[09/26 10:02:46 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1584, average loss: 1.6672
[09/26 10:02:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 46.50	top5: 85.50	
[09/26 10:02:46 visual_prompt]: Best epoch 38: best metric: 0.465
[09/26 10:02:46 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/26 10:02:52 visual_prompt]: Epoch 39 / 100: avg data time: 6.53e-02, avg batch time: 0.4780, average train loss: 1.8423
[09/26 10:02:54 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1584, average loss: 2.1320
[09/26 10:02:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 37.00	top5: 79.00	
[09/26 10:02:54 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/26 10:03:01 visual_prompt]: Epoch 40 / 100: avg data time: 6.10e-02, avg batch time: 0.4737, average train loss: 1.7125
[09/26 10:03:02 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1586, average loss: 1.3495
[09/26 10:03:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 53.00	top5: 91.00	
[09/26 10:03:02 visual_prompt]: Best epoch 40: best metric: 0.530
[09/26 10:03:02 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/26 10:03:09 visual_prompt]: Epoch 41 / 100: avg data time: 6.58e-02, avg batch time: 0.4783, average train loss: 1.2659
[09/26 10:03:10 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1584, average loss: 1.0247
[09/26 10:03:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.00	top5: 95.00	
[09/26 10:03:10 visual_prompt]: Best epoch 41: best metric: 0.650
[09/26 10:03:10 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/26 10:03:17 visual_prompt]: Epoch 42 / 100: avg data time: 6.39e-02, avg batch time: 0.4777, average train loss: 0.9875
[09/26 10:03:18 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1583, average loss: 0.9196
[09/26 10:03:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 96.00	
[09/26 10:03:18 visual_prompt]: Best epoch 42: best metric: 0.730
[09/26 10:03:18 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/26 10:03:25 visual_prompt]: Epoch 43 / 100: avg data time: 6.27e-02, avg batch time: 0.4762, average train loss: 0.8601
[09/26 10:03:26 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1579, average loss: 1.2476
[09/26 10:03:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 56.00	top5: 96.00	
[09/26 10:03:26 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/26 10:03:33 visual_prompt]: Epoch 44 / 100: avg data time: 6.10e-02, avg batch time: 0.4759, average train loss: 0.8971
[09/26 10:03:35 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1580, average loss: 0.9221
[09/26 10:03:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 95.00	
[09/26 10:03:35 visual_prompt]: Best epoch 44: best metric: 0.735
[09/26 10:03:35 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/26 10:03:41 visual_prompt]: Epoch 45 / 100: avg data time: 6.00e-02, avg batch time: 0.4737, average train loss: 0.8540
[09/26 10:03:43 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1586, average loss: 0.6394
[09/26 10:03:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.00	top5: 98.50	
[09/26 10:03:43 visual_prompt]: Best epoch 45: best metric: 0.770
[09/26 10:03:43 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/26 10:03:49 visual_prompt]: Epoch 46 / 100: avg data time: 6.17e-02, avg batch time: 0.4755, average train loss: 0.6002
[09/26 10:03:51 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1582, average loss: 0.7270
[09/26 10:03:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 95.50	
[09/26 10:03:51 visual_prompt]: Best epoch 46: best metric: 0.790
[09/26 10:03:51 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/26 10:03:57 visual_prompt]: Epoch 47 / 100: avg data time: 6.09e-02, avg batch time: 0.4740, average train loss: 0.5484
[09/26 10:03:59 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1582, average loss: 0.8735
[09/26 10:03:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 95.50	
[09/26 10:03:59 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/26 10:04:05 visual_prompt]: Epoch 48 / 100: avg data time: 6.50e-02, avg batch time: 0.4778, average train loss: 0.5298
[09/26 10:04:07 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1581, average loss: 0.6138
[09/26 10:04:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.00	
[09/26 10:04:07 visual_prompt]: Best epoch 48: best metric: 0.795
[09/26 10:04:07 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/26 10:04:13 visual_prompt]: Epoch 49 / 100: avg data time: 5.82e-02, avg batch time: 0.4721, average train loss: 0.5027
[09/26 10:04:15 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1586, average loss: 0.6793
[09/26 10:04:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.00	
[09/26 10:04:15 visual_prompt]: Best epoch 49: best metric: 0.825
[09/26 10:04:15 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/26 10:04:21 visual_prompt]: Epoch 50 / 100: avg data time: 6.01e-02, avg batch time: 0.4738, average train loss: 0.4126
[09/26 10:04:23 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1581, average loss: 0.6395
[09/26 10:04:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 97.00	
[09/26 10:04:23 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/26 10:04:30 visual_prompt]: Epoch 51 / 100: avg data time: 6.31e-02, avg batch time: 0.4769, average train loss: 0.3106
[09/26 10:04:31 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1580, average loss: 0.5904
[09/26 10:04:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 97.50	
[09/26 10:04:31 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/26 10:04:38 visual_prompt]: Epoch 52 / 100: avg data time: 6.58e-02, avg batch time: 0.4785, average train loss: 0.3251
[09/26 10:04:39 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1584, average loss: 0.6002
[09/26 10:04:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 10:04:39 visual_prompt]: Best epoch 52: best metric: 0.835
[09/26 10:04:39 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/26 10:04:46 visual_prompt]: Epoch 53 / 100: avg data time: 6.58e-02, avg batch time: 0.4790, average train loss: 0.2761
[09/26 10:04:47 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1586, average loss: 0.6540
[09/26 10:04:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 98.50	
[09/26 10:04:47 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/26 10:04:54 visual_prompt]: Epoch 54 / 100: avg data time: 6.59e-02, avg batch time: 0.4802, average train loss: 0.3091
[09/26 10:04:56 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1587, average loss: 0.6566
[09/26 10:04:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 96.50	
[09/26 10:04:56 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/26 10:05:02 visual_prompt]: Epoch 55 / 100: avg data time: 6.69e-02, avg batch time: 0.4808, average train loss: 0.3201
[09/26 10:05:04 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1585, average loss: 0.9123
[09/26 10:05:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.50	top5: 98.00	
[09/26 10:05:04 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/26 10:05:10 visual_prompt]: Epoch 56 / 100: avg data time: 6.04e-02, avg batch time: 0.4743, average train loss: 0.3209
[09/26 10:05:12 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1584, average loss: 0.8043
[09/26 10:05:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 97.50	
[09/26 10:05:12 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/26 10:05:19 visual_prompt]: Epoch 57 / 100: avg data time: 6.74e-02, avg batch time: 0.4805, average train loss: 0.2259
[09/26 10:05:20 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1582, average loss: 0.6694
[09/26 10:05:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.50	
[09/26 10:05:20 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/26 10:05:27 visual_prompt]: Epoch 58 / 100: avg data time: 6.21e-02, avg batch time: 0.4765, average train loss: 0.1714
[09/26 10:05:28 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1582, average loss: 0.8110
[09/26 10:05:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 97.00	
[09/26 10:05:28 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/26 10:05:35 visual_prompt]: Epoch 59 / 100: avg data time: 5.90e-02, avg batch time: 0.4735, average train loss: 0.1536
[09/26 10:05:36 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1586, average loss: 0.6694
[09/26 10:05:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 10:05:36 visual_prompt]: Best epoch 59: best metric: 0.840
[09/26 10:05:36 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/26 10:05:43 visual_prompt]: Epoch 60 / 100: avg data time: 6.58e-02, avg batch time: 0.4792, average train loss: 0.1213
[09/26 10:05:45 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1577, average loss: 0.7748
[09/26 10:05:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 97.00	
[09/26 10:05:45 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/26 10:05:51 visual_prompt]: Epoch 61 / 100: avg data time: 6.10e-02, avg batch time: 0.4750, average train loss: 0.0958
[09/26 10:05:53 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1585, average loss: 0.7425
[09/26 10:05:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 10:05:53 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/26 10:05:59 visual_prompt]: Epoch 62 / 100: avg data time: 5.43e-02, avg batch time: 0.4681, average train loss: 0.0894
[09/26 10:06:01 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1580, average loss: 0.7397
[09/26 10:06:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 96.50	
[09/26 10:06:01 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/26 10:06:07 visual_prompt]: Epoch 63 / 100: avg data time: 6.42e-02, avg batch time: 0.4775, average train loss: 0.0782
[09/26 10:06:09 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1579, average loss: 0.8235
[09/26 10:06:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.50	
[09/26 10:06:09 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/26 10:06:15 visual_prompt]: Epoch 64 / 100: avg data time: 6.61e-02, avg batch time: 0.4785, average train loss: 0.0491
[09/26 10:06:17 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1579, average loss: 0.8170
[09/26 10:06:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 10:06:17 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/26 10:06:23 visual_prompt]: Epoch 65 / 100: avg data time: 5.89e-02, avg batch time: 0.4723, average train loss: 0.0277
[09/26 10:06:25 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1580, average loss: 0.9564
[09/26 10:06:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.50	
[09/26 10:06:25 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/26 10:06:31 visual_prompt]: Epoch 66 / 100: avg data time: 5.41e-02, avg batch time: 0.4694, average train loss: 0.0221
[09/26 10:06:33 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1582, average loss: 1.0355
[09/26 10:06:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.50	
[09/26 10:06:33 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/26 10:06:39 visual_prompt]: Epoch 67 / 100: avg data time: 5.34e-02, avg batch time: 0.4665, average train loss: 0.0266
[09/26 10:06:41 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1584, average loss: 1.0784
[09/26 10:06:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 97.50	
[09/26 10:06:41 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/26 10:06:48 visual_prompt]: Epoch 68 / 100: avg data time: 6.81e-02, avg batch time: 0.4805, average train loss: 0.0175
[09/26 10:06:49 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.1585, average loss: 1.1516
[09/26 10:06:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 10:06:49 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/26 10:06:56 visual_prompt]: Epoch 69 / 100: avg data time: 6.36e-02, avg batch time: 0.4780, average train loss: 0.0264
[09/26 10:06:57 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1581, average loss: 1.1000
[09/26 10:06:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.00	
[09/26 10:06:57 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/26 10:07:04 visual_prompt]: Epoch 70 / 100: avg data time: 6.48e-02, avg batch time: 0.4781, average train loss: 0.0189
[09/26 10:07:05 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1586, average loss: 1.2622
[09/26 10:07:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 10:07:05 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/26 10:07:12 visual_prompt]: Epoch 71 / 100: avg data time: 6.34e-02, avg batch time: 0.4754, average train loss: 0.0149
[09/26 10:07:13 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1580, average loss: 1.2110
[09/26 10:07:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 10:07:14 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/26 10:07:20 visual_prompt]: Epoch 72 / 100: avg data time: 6.37e-02, avg batch time: 0.4758, average train loss: 0.0104
[09/26 10:07:22 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1579, average loss: 1.2560
[09/26 10:07:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 98.00	
[09/26 10:07:22 visual_prompt]: Best epoch 72: best metric: 0.850
[09/26 10:07:22 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/26 10:07:28 visual_prompt]: Epoch 73 / 100: avg data time: 6.31e-02, avg batch time: 0.4765, average train loss: 0.0034
[09/26 10:07:30 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1574, average loss: 1.2663
[09/26 10:07:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 98.00	
[09/26 10:07:30 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/26 10:07:36 visual_prompt]: Epoch 74 / 100: avg data time: 4.73e-02, avg batch time: 0.4596, average train loss: 0.0106
[09/26 10:07:38 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1585, average loss: 1.3533
[09/26 10:07:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 96.50	
[09/26 10:07:38 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/26 10:07:44 visual_prompt]: Epoch 75 / 100: avg data time: 6.13e-02, avg batch time: 0.4734, average train loss: 0.0163
[09/26 10:07:46 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1583, average loss: 1.4159
[09/26 10:07:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.00	
[09/26 10:07:46 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/26 10:07:52 visual_prompt]: Epoch 76 / 100: avg data time: 6.19e-02, avg batch time: 0.4741, average train loss: 0.0107
[09/26 10:07:54 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1586, average loss: 1.5248
[09/26 10:07:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.00	
[09/26 10:07:54 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/26 10:08:00 visual_prompt]: Epoch 77 / 100: avg data time: 6.42e-02, avg batch time: 0.4771, average train loss: 0.0188
[09/26 10:08:02 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1580, average loss: 1.5402
[09/26 10:08:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 10:08:02 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/26 10:08:08 visual_prompt]: Epoch 78 / 100: avg data time: 5.97e-02, avg batch time: 0.4717, average train loss: 0.0058
[09/26 10:08:10 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1582, average loss: 1.3700
[09/26 10:08:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 10:08:10 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/26 10:08:16 visual_prompt]: Epoch 79 / 100: avg data time: 6.07e-02, avg batch time: 0.4734, average train loss: 0.0023
[09/26 10:08:18 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1580, average loss: 1.3579
[09/26 10:08:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 10:08:18 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/26 10:08:24 visual_prompt]: Epoch 80 / 100: avg data time: 6.81e-02, avg batch time: 0.4802, average train loss: 0.0054
[09/26 10:08:26 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1586, average loss: 1.3274
[09/26 10:08:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 98.00	
[09/26 10:08:26 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/26 10:08:33 visual_prompt]: Epoch 81 / 100: avg data time: 5.28e-02, avg batch time: 0.4679, average train loss: 0.0019
[09/26 10:08:34 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1581, average loss: 1.3534
[09/26 10:08:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 10:08:34 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/26 10:08:41 visual_prompt]: Epoch 82 / 100: avg data time: 6.55e-02, avg batch time: 0.4781, average train loss: 0.0017
[09/26 10:08:42 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1582, average loss: 1.3686
[09/26 10:08:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 10:08:42 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/26 10:08:49 visual_prompt]: Epoch 83 / 100: avg data time: 7.38e-02, avg batch time: 0.4859, average train loss: 0.0048
[09/26 10:08:51 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1583, average loss: 1.3817
[09/26 10:08:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 98.00	
[09/26 10:08:51 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/26 10:08:57 visual_prompt]: Epoch 84 / 100: avg data time: 6.52e-02, avg batch time: 0.4777, average train loss: 0.0010
[09/26 10:08:59 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1584, average loss: 1.4120
[09/26 10:08:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.00	
[09/26 10:08:59 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/26 10:09:05 visual_prompt]: Epoch 85 / 100: avg data time: 6.28e-02, avg batch time: 0.4760, average train loss: 0.0017
[09/26 10:09:07 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.1583, average loss: 1.4266
[09/26 10:09:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.00	
[09/26 10:09:07 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/26 10:09:13 visual_prompt]: Epoch 86 / 100: avg data time: 5.80e-02, avg batch time: 0.4728, average train loss: 0.0032
[09/26 10:09:15 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1585, average loss: 1.4070
[09/26 10:09:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.00	
[09/26 10:09:15 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/26 10:09:21 visual_prompt]: Epoch 87 / 100: avg data time: 6.24e-02, avg batch time: 0.4757, average train loss: 0.0039
[09/26 10:09:23 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1583, average loss: 1.3952
[09/26 10:09:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.00	
[09/26 10:09:23 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/26 10:09:29 visual_prompt]: Epoch 88 / 100: avg data time: 6.24e-02, avg batch time: 0.4754, average train loss: 0.0013
[09/26 10:09:31 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1575, average loss: 1.3946
[09/26 10:09:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 10:09:31 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/26 10:09:37 visual_prompt]: Epoch 89 / 100: avg data time: 6.01e-02, avg batch time: 0.4732, average train loss: 0.0015
[09/26 10:09:39 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1585, average loss: 1.4005
[09/26 10:09:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 10:09:39 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/26 10:09:46 visual_prompt]: Epoch 90 / 100: avg data time: 5.92e-02, avg batch time: 0.4717, average train loss: 0.0043
[09/26 10:09:47 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1581, average loss: 1.3856
[09/26 10:09:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 10:09:47 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/26 10:09:54 visual_prompt]: Epoch 91 / 100: avg data time: 6.68e-02, avg batch time: 0.4813, average train loss: 0.0008
[09/26 10:09:55 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1582, average loss: 1.3793
[09/26 10:09:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 10:09:55 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/26 10:10:02 visual_prompt]: Epoch 92 / 100: avg data time: 6.06e-02, avg batch time: 0.4741, average train loss: 0.0021
[09/26 10:10:03 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1580, average loss: 1.3792
[09/26 10:10:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 10:10:03 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/26 10:10:10 visual_prompt]: Epoch 93 / 100: avg data time: 6.36e-02, avg batch time: 0.4762, average train loss: 0.0017
[09/26 10:10:12 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1582, average loss: 1.3845
[09/26 10:10:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 10:10:12 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/26 10:10:18 visual_prompt]: Epoch 94 / 100: avg data time: 7.23e-02, avg batch time: 0.4846, average train loss: 0.0027
[09/26 10:10:20 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1586, average loss: 1.3892
[09/26 10:10:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 10:10:20 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/26 10:10:26 visual_prompt]: Epoch 95 / 100: avg data time: 5.15e-02, avg batch time: 0.4643, average train loss: 0.0011
[09/26 10:10:28 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1580, average loss: 1.3904
[09/26 10:10:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 10:10:28 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/26 10:10:34 visual_prompt]: Epoch 96 / 100: avg data time: 6.59e-02, avg batch time: 0.4779, average train loss: 0.0015
[09/26 10:10:36 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1583, average loss: 1.3909
[09/26 10:10:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 10:10:36 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/26 10:10:42 visual_prompt]: Epoch 97 / 100: avg data time: 5.56e-02, avg batch time: 0.4692, average train loss: 0.0007
[09/26 10:10:44 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1580, average loss: 1.3916
[09/26 10:10:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 10:10:44 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/26 10:10:50 visual_prompt]: Epoch 98 / 100: avg data time: 5.46e-02, avg batch time: 0.4668, average train loss: 0.0056
[09/26 10:10:52 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1585, average loss: 1.3914
[09/26 10:10:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 10:10:52 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/26 10:10:59 visual_prompt]: Epoch 99 / 100: avg data time: 7.06e-02, avg batch time: 0.4836, average train loss: 0.0019
[09/26 10:11:00 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1584, average loss: 1.3919
[09/26 10:11:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 10:11:00 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/26 10:11:07 visual_prompt]: Epoch 100 / 100: avg data time: 6.27e-02, avg batch time: 0.4747, average train loss: 0.0020
[09/26 10:11:08 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1582, average loss: 1.3921
[09/26 10:11:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 10:11:08 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 10:11:08 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 10:11:08 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 10:11:08 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 10:11:08 visual_prompt]: Training with config:
[09/26 10:11:08 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr2.5_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 10:11:08 visual_prompt]: Loading training data...
[09/26 10:11:08 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 10:11:09 visual_prompt]: Number of images: 800
[09/26 10:11:09 visual_prompt]: Number of classes: 10 / 10
[09/26 10:11:09 visual_prompt]: Loading validation data...
[09/26 10:11:09 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 10:11:10 visual_prompt]: Number of images: 200
[09/26 10:11:10 visual_prompt]: Number of classes: 10 / 10
[09/26 10:11:10 visual_prompt]: Constructing models...
[09/26 10:11:12 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 10:11:12 visual_prompt]: tuned percent:0.543
[09/26 10:11:12 visual_prompt]: Device used for model: 0
[09/26 10:11:12 visual_prompt]: Setting up Evaluator...
[09/26 10:11:12 visual_prompt]: Setting up Trainer...
[09/26 10:11:12 visual_prompt]: 	Setting up the optimizer...
[09/26 10:11:12 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 10:11:19 visual_prompt]: Epoch 1 / 100: avg data time: 6.73e-02, avg batch time: 0.4858, average train loss: 2.6804
[09/26 10:11:20 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1586, average loss: 2.6214
[09/26 10:11:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 10:11:20 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 10:11:20 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[09/26 10:11:27 visual_prompt]: Epoch 2 / 100: avg data time: 5.90e-02, avg batch time: 0.4717, average train loss: 3.5187
[09/26 10:11:28 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1580, average loss: 2.3332
[09/26 10:11:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.50	
[09/26 10:11:28 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[09/26 10:11:35 visual_prompt]: Epoch 3 / 100: avg data time: 5.78e-02, avg batch time: 0.4705, average train loss: 2.5666
[09/26 10:11:36 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1587, average loss: 2.3380
[09/26 10:11:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 10:11:36 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[09/26 10:11:43 visual_prompt]: Epoch 4 / 100: avg data time: 6.73e-02, avg batch time: 0.4789, average train loss: 2.3285
[09/26 10:11:44 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1581, average loss: 2.2586
[09/26 10:11:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 10:11:44 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[09/26 10:11:51 visual_prompt]: Epoch 5 / 100: avg data time: 6.58e-02, avg batch time: 0.4773, average train loss: 2.2959
[09/26 10:11:53 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1585, average loss: 2.3177
[09/26 10:11:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 10:11:53 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[09/26 10:11:59 visual_prompt]: Epoch 6 / 100: avg data time: 6.39e-02, avg batch time: 0.4755, average train loss: 2.5434
[09/26 10:12:01 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1583, average loss: 2.4452
[09/26 10:12:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 10:12:01 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[09/26 10:12:07 visual_prompt]: Epoch 7 / 100: avg data time: 5.78e-02, avg batch time: 0.4715, average train loss: 2.6080
[09/26 10:12:09 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.1583, average loss: 2.4884
[09/26 10:12:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 10:12:09 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[09/26 10:12:15 visual_prompt]: Epoch 8 / 100: avg data time: 6.33e-02, avg batch time: 0.4756, average train loss: 2.9766
[09/26 10:12:17 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.1585, average loss: 2.8796
[09/26 10:12:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.00	
[09/26 10:12:17 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[09/26 10:12:23 visual_prompt]: Epoch 9 / 100: avg data time: 5.13e-02, avg batch time: 0.4656, average train loss: 3.2846
[09/26 10:12:25 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1579, average loss: 2.6811
[09/26 10:12:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 41.50	
[09/26 10:12:25 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[09/26 10:12:31 visual_prompt]: Epoch 10 / 100: avg data time: 6.50e-02, avg batch time: 0.4773, average train loss: 3.0610
[09/26 10:12:33 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1585, average loss: 2.3482
[09/26 10:12:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 10:12:33 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[09/26 10:12:40 visual_prompt]: Epoch 11 / 100: avg data time: 6.54e-02, avg batch time: 0.4771, average train loss: 2.4817
[09/26 10:12:41 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1581, average loss: 2.7766
[09/26 10:12:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 43.50	
[09/26 10:12:41 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[09/26 10:12:48 visual_prompt]: Epoch 12 / 100: avg data time: 6.79e-02, avg batch time: 0.4796, average train loss: 3.6336
[09/26 10:12:49 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1581, average loss: 3.1025
[09/26 10:12:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 45.00	
[09/26 10:12:49 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[09/26 10:12:56 visual_prompt]: Epoch 13 / 100: avg data time: 6.53e-02, avg batch time: 0.4768, average train loss: 2.8773
[09/26 10:12:57 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1583, average loss: 3.3964
[09/26 10:12:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 41.00	
[09/26 10:12:57 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[09/26 10:13:04 visual_prompt]: Epoch 14 / 100: avg data time: 6.09e-02, avg batch time: 0.4740, average train loss: 4.9553
[09/26 10:13:05 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1582, average loss: 3.8569
[09/26 10:13:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 40.50	
[09/26 10:13:05 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[09/26 10:13:12 visual_prompt]: Epoch 15 / 100: avg data time: 6.24e-02, avg batch time: 0.4742, average train loss: 6.8824
[09/26 10:13:14 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1576, average loss: 8.7602
[09/26 10:13:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 44.00	
[09/26 10:13:14 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[09/26 10:13:20 visual_prompt]: Epoch 16 / 100: avg data time: 6.99e-02, avg batch time: 0.4824, average train loss: 5.1349
[09/26 10:13:22 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1583, average loss: 4.4786
[09/26 10:13:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 10:13:22 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[09/26 10:13:28 visual_prompt]: Epoch 17 / 100: avg data time: 6.63e-02, avg batch time: 0.4789, average train loss: 4.3317
[09/26 10:13:30 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1582, average loss: 5.3291
[09/26 10:13:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 41.50	
[09/26 10:13:30 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[09/26 10:13:36 visual_prompt]: Epoch 18 / 100: avg data time: 5.91e-02, avg batch time: 0.4726, average train loss: 5.6927
[09/26 10:13:38 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1584, average loss: 18.1786
[09/26 10:13:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 10:13:38 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[09/26 10:13:44 visual_prompt]: Epoch 19 / 100: avg data time: 6.44e-02, avg batch time: 0.4773, average train loss: 8.4673
[09/26 10:13:46 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1582, average loss: 6.8393
[09/26 10:13:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 45.50	
[09/26 10:13:46 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[09/26 10:13:53 visual_prompt]: Epoch 20 / 100: avg data time: 6.45e-02, avg batch time: 0.4778, average train loss: 10.9773
[09/26 10:13:54 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1590, average loss: 13.0262
[09/26 10:13:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 38.50	
[09/26 10:13:54 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[09/26 10:14:01 visual_prompt]: Epoch 21 / 100: avg data time: 6.04e-02, avg batch time: 0.4727, average train loss: 6.6614
[09/26 10:14:02 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1584, average loss: 4.4270
[09/26 10:14:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 10:14:02 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[09/26 10:14:09 visual_prompt]: Epoch 22 / 100: avg data time: 6.36e-02, avg batch time: 0.4764, average train loss: 5.0211
[09/26 10:14:10 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1587, average loss: 6.0370
[09/26 10:14:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 42.50	
[09/26 10:14:10 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[09/26 10:14:17 visual_prompt]: Epoch 23 / 100: avg data time: 6.40e-02, avg batch time: 0.4782, average train loss: 7.3535
[09/26 10:14:19 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1580, average loss: 7.8180
[09/26 10:14:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 57.50	
[09/26 10:14:19 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[09/26 10:14:25 visual_prompt]: Epoch 24 / 100: avg data time: 6.44e-02, avg batch time: 0.4777, average train loss: 12.3324
[09/26 10:14:27 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1581, average loss: 9.8427
[09/26 10:14:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 60.00	
[09/26 10:14:27 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[09/26 10:14:33 visual_prompt]: Epoch 25 / 100: avg data time: 6.26e-02, avg batch time: 0.4755, average train loss: 8.5475
[09/26 10:14:35 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1583, average loss: 4.6223
[09/26 10:14:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 39.00	
[09/26 10:14:35 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[09/26 10:14:41 visual_prompt]: Epoch 26 / 100: avg data time: 6.81e-02, avg batch time: 0.4811, average train loss: 4.6852
[09/26 10:14:43 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1590, average loss: 5.2422
[09/26 10:14:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 57.50	
[09/26 10:14:43 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[09/26 10:14:50 visual_prompt]: Epoch 27 / 100: avg data time: 6.27e-02, avg batch time: 0.4755, average train loss: 4.1956
[09/26 10:14:51 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1587, average loss: 3.0345
[09/26 10:14:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 10:14:51 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[09/26 10:14:58 visual_prompt]: Epoch 28 / 100: avg data time: 6.67e-02, avg batch time: 0.4796, average train loss: 3.8755
[09/26 10:14:59 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1589, average loss: 3.2719
[09/26 10:14:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 42.00	
[09/26 10:14:59 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[09/26 10:15:06 visual_prompt]: Epoch 29 / 100: avg data time: 5.87e-02, avg batch time: 0.4731, average train loss: 6.8334
[09/26 10:15:08 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1583, average loss: 5.1823
[09/26 10:15:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 43.50	
[09/26 10:15:08 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[09/26 10:15:14 visual_prompt]: Epoch 30 / 100: avg data time: 6.41e-02, avg batch time: 0.4775, average train loss: 8.8478
[09/26 10:15:16 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1588, average loss: 10.9276
[09/26 10:15:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 52.50	
[09/26 10:15:16 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[09/26 10:15:22 visual_prompt]: Epoch 31 / 100: avg data time: 6.05e-02, avg batch time: 0.4740, average train loss: 8.9626
[09/26 10:15:24 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1590, average loss: 8.9927
[09/26 10:15:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 35.50	
[09/26 10:15:24 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[09/26 10:15:30 visual_prompt]: Epoch 32 / 100: avg data time: 6.31e-02, avg batch time: 0.4786, average train loss: 6.8859
[09/26 10:15:32 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1585, average loss: 4.1466
[09/26 10:15:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 63.00	
[09/26 10:15:32 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[09/26 10:15:38 visual_prompt]: Epoch 33 / 100: avg data time: 6.30e-02, avg batch time: 0.4759, average train loss: 5.2885
[09/26 10:15:40 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1582, average loss: 7.8961
[09/26 10:15:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 55.50	
[09/26 10:15:40 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[09/26 10:15:47 visual_prompt]: Epoch 34 / 100: avg data time: 6.63e-02, avg batch time: 0.4786, average train loss: 5.2239
[09/26 10:15:48 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1583, average loss: 3.9830
[09/26 10:15:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 52.50	
[09/26 10:15:48 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[09/26 10:15:55 visual_prompt]: Epoch 35 / 100: avg data time: 6.28e-02, avg batch time: 0.4761, average train loss: 4.4249
[09/26 10:15:56 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1586, average loss: 3.7279
[09/26 10:15:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.50	
[09/26 10:15:56 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[09/26 10:16:03 visual_prompt]: Epoch 36 / 100: avg data time: 6.23e-02, avg batch time: 0.4762, average train loss: 3.4117
[09/26 10:16:04 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1584, average loss: 3.5893
[09/26 10:16:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.50	top5: 45.50	
[09/26 10:16:05 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[09/26 10:16:11 visual_prompt]: Epoch 37 / 100: avg data time: 6.73e-02, avg batch time: 0.4820, average train loss: 2.9767
[09/26 10:16:13 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1583, average loss: 2.8306
[09/26 10:16:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 53.50	
[09/26 10:16:13 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[09/26 10:16:19 visual_prompt]: Epoch 38 / 100: avg data time: 6.25e-02, avg batch time: 0.4777, average train loss: 2.6242
[09/26 10:16:21 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1588, average loss: 5.9296
[09/26 10:16:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 10:16:21 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[09/26 10:16:27 visual_prompt]: Epoch 39 / 100: avg data time: 6.51e-02, avg batch time: 0.4768, average train loss: 5.0475
[09/26 10:16:29 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1584, average loss: 4.1004
[09/26 10:16:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.50	
[09/26 10:16:29 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[09/26 10:16:36 visual_prompt]: Epoch 40 / 100: avg data time: 7.24e-02, avg batch time: 0.4844, average train loss: 7.1533
[09/26 10:16:37 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1585, average loss: 7.4877
[09/26 10:16:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.00	
[09/26 10:16:37 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[09/26 10:16:44 visual_prompt]: Epoch 41 / 100: avg data time: 6.48e-02, avg batch time: 0.4777, average train loss: 5.4083
[09/26 10:16:45 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1586, average loss: 2.8957
[09/26 10:16:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 41.50	
[09/26 10:16:45 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[09/26 10:16:52 visual_prompt]: Epoch 42 / 100: avg data time: 5.90e-02, avg batch time: 0.4709, average train loss: 3.4734
[09/26 10:16:53 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1584, average loss: 2.7379
[09/26 10:16:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 46.00	
[09/26 10:16:53 visual_prompt]: Training 43 / 100 epoch, with learning rate 1.7979639334863466
[09/26 10:17:00 visual_prompt]: Epoch 43 / 100: avg data time: 6.00e-02, avg batch time: 0.4729, average train loss: 2.7848
[09/26 10:17:02 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1583, average loss: 2.5349
[09/26 10:17:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 43.50	
[09/26 10:17:02 visual_prompt]: Training 44 / 100 epoch, with learning rate 1.7584208038447504
[09/26 10:17:08 visual_prompt]: Epoch 44 / 100: avg data time: 6.73e-02, avg batch time: 0.4806, average train loss: 2.6739
[09/26 10:17:10 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1585, average loss: 2.4683
[09/26 10:17:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 59.00	
[09/26 10:17:10 visual_prompt]: Training 45 / 100 epoch, with learning rate 1.7182582417698902
[09/26 10:17:16 visual_prompt]: Epoch 45 / 100: avg data time: 5.20e-02, avg batch time: 0.4663, average train loss: 2.5370
[09/26 10:17:18 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1585, average loss: 2.5033
[09/26 10:17:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 10:17:18 visual_prompt]: Training 46 / 100 epoch, with learning rate 1.6775251791570862
[09/26 10:17:24 visual_prompt]: Epoch 46 / 100: avg data time: 4.80e-02, avg batch time: 0.4621, average train loss: 2.4697
[09/26 10:17:26 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1585, average loss: 2.5501
[09/26 10:17:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 44.50	
[09/26 10:17:26 visual_prompt]: Training 47 / 100 epoch, with learning rate 1.6362712429686843
[09/26 10:17:32 visual_prompt]: Epoch 47 / 100: avg data time: 6.60e-02, avg batch time: 0.4791, average train loss: 2.6155
[09/26 10:17:34 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1581, average loss: 2.5774
[09/26 10:17:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 59.50	
[09/26 10:17:34 visual_prompt]: Training 48 / 100 epoch, with learning rate 1.5945466947712488
[09/26 10:17:40 visual_prompt]: Epoch 48 / 100: avg data time: 6.72e-02, avg batch time: 0.4801, average train loss: 3.1356
[09/26 10:17:42 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1583, average loss: 4.7383
[09/26 10:17:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 10:17:42 visual_prompt]: Training 49 / 100 epoch, with learning rate 1.5524023694995845
[09/26 10:17:48 visual_prompt]: Epoch 49 / 100: avg data time: 6.32e-02, avg batch time: 0.4750, average train loss: 4.4309
[09/26 10:17:50 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1584, average loss: 2.7209
[09/26 10:17:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 14.50	top5: 55.00	
[09/26 10:17:50 visual_prompt]: Training 50 / 100 epoch, with learning rate 1.509889613522199
[09/26 10:17:57 visual_prompt]: Epoch 50 / 100: avg data time: 6.93e-02, avg batch time: 0.4811, average train loss: 2.8484
[09/26 10:17:58 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1585, average loss: 2.4432
[09/26 10:17:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 59.50	
[09/26 10:17:58 visual_prompt]: Training 51 / 100 epoch, with learning rate 1.467060222083663
[09/26 10:18:05 visual_prompt]: Epoch 51 / 100: avg data time: 5.22e-02, avg batch time: 0.4662, average train loss: 3.3099
[09/26 10:18:06 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1588, average loss: 3.2455
[09/26 10:18:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/26 10:18:06 visual_prompt]: Training 52 / 100 epoch, with learning rate 1.4239663762000818
[09/26 10:18:13 visual_prompt]: Epoch 52 / 100: avg data time: 6.15e-02, avg batch time: 0.4739, average train loss: 4.7037
[09/26 10:18:14 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1581, average loss: 3.7259
[09/26 10:18:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 40.50	
[09/26 10:18:14 visual_prompt]: Training 53 / 100 epoch, with learning rate 1.380660579084567
[09/26 10:18:21 visual_prompt]: Epoch 53 / 100: avg data time: 6.29e-02, avg batch time: 0.4752, average train loss: 3.0845
[09/26 10:18:22 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1581, average loss: 3.2133
[09/26 10:18:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.00	
[09/26 10:18:22 visual_prompt]: Training 54 / 100 epoch, with learning rate 1.3371955921801564
[09/26 10:18:29 visual_prompt]: Epoch 54 / 100: avg data time: 6.05e-02, avg batch time: 0.4735, average train loss: 2.7148
[09/26 10:18:30 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1583, average loss: 2.4702
[09/26 10:18:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 10:18:30 visual_prompt]: Training 55 / 100 epoch, with learning rate 1.2936243708781263
[09/26 10:18:37 visual_prompt]: Epoch 55 / 100: avg data time: 5.26e-02, avg batch time: 0.4645, average train loss: 2.3769
[09/26 10:18:38 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1583, average loss: 2.2952
[09/26 10:18:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 10:18:38 visual_prompt]: Training 56 / 100 epoch, with learning rate 1.25
[09/26 10:18:45 visual_prompt]: Epoch 56 / 100: avg data time: 6.29e-02, avg batch time: 0.4744, average train loss: 2.5039
[09/26 10:18:46 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1587, average loss: 3.2502
[09/26 10:18:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 10:18:46 visual_prompt]: Training 57 / 100 epoch, with learning rate 1.2063756291218741
[09/26 10:18:53 visual_prompt]: Epoch 57 / 100: avg data time: 6.22e-02, avg batch time: 0.4737, average train loss: 3.1290
[09/26 10:18:55 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1581, average loss: 2.5721
[09/26 10:18:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 52.00	
[09/26 10:18:55 visual_prompt]: Training 58 / 100 epoch, with learning rate 1.1628044078198434
[09/26 10:19:01 visual_prompt]: Epoch 58 / 100: avg data time: 5.23e-02, avg batch time: 0.4648, average train loss: 2.8521
[09/26 10:19:03 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1583, average loss: 3.7195
[09/26 10:19:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 43.00	
[09/26 10:19:03 visual_prompt]: Training 59 / 100 epoch, with learning rate 1.1193394209154333
[09/26 10:19:09 visual_prompt]: Epoch 59 / 100: avg data time: 7.08e-02, avg batch time: 0.4824, average train loss: 2.7811
[09/26 10:19:11 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1584, average loss: 2.3017
[09/26 10:19:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/26 10:19:11 visual_prompt]: Training 60 / 100 epoch, with learning rate 1.0760336237999186
[09/26 10:19:17 visual_prompt]: Epoch 60 / 100: avg data time: 6.52e-02, avg batch time: 0.4788, average train loss: 2.6229
[09/26 10:19:19 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1582, average loss: 2.6190
[09/26 10:19:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 41.50	
[09/26 10:19:19 visual_prompt]: Training 61 / 100 epoch, with learning rate 1.0329397779163372
[09/26 10:19:25 visual_prompt]: Epoch 61 / 100: avg data time: 5.82e-02, avg batch time: 0.4715, average train loss: 2.4935
[09/26 10:19:27 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1580, average loss: 2.4544
[09/26 10:19:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 45.00	
[09/26 10:19:27 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.990110386477801
[09/26 10:19:34 visual_prompt]: Epoch 62 / 100: avg data time: 6.41e-02, avg batch time: 0.4758, average train loss: 2.4961
[09/26 10:19:35 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1583, average loss: 2.4746
[09/26 10:19:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.00	
[09/26 10:19:35 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.9475976305004155
[09/26 10:19:42 visual_prompt]: Epoch 63 / 100: avg data time: 6.37e-02, avg batch time: 0.4755, average train loss: 2.4276
[09/26 10:19:43 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1588, average loss: 2.2977
[09/26 10:19:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.50	
[09/26 10:19:43 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.9054533052287511
[09/26 10:19:50 visual_prompt]: Epoch 64 / 100: avg data time: 6.90e-02, avg batch time: 0.4814, average train loss: 2.3150
[09/26 10:19:51 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1582, average loss: 2.2651
[09/26 10:19:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 10:19:51 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.8637287570313158
[09/26 10:19:58 visual_prompt]: Epoch 65 / 100: avg data time: 6.47e-02, avg batch time: 0.4772, average train loss: 2.3009
[09/26 10:20:00 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1583, average loss: 2.3754
[09/26 10:20:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 10:20:00 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.8224748208429142
[09/26 10:20:06 visual_prompt]: Epoch 66 / 100: avg data time: 5.79e-02, avg batch time: 0.4703, average train loss: 2.2982
[09/26 10:20:08 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1580, average loss: 2.2672
[09/26 10:20:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 10:20:08 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.7817417582301099
[09/26 10:20:14 visual_prompt]: Epoch 67 / 100: avg data time: 6.59e-02, avg batch time: 0.4781, average train loss: 2.2820
[09/26 10:20:16 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1582, average loss: 2.2251
[09/26 10:20:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 10:20:16 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.7415791961552499
[09/26 10:20:22 visual_prompt]: Epoch 68 / 100: avg data time: 6.64e-02, avg batch time: 0.4798, average train loss: 2.2687
[09/26 10:20:24 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1584, average loss: 2.3551
[09/26 10:20:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 10:20:24 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.7020360665136531
[09/26 10:20:30 visual_prompt]: Epoch 69 / 100: avg data time: 6.50e-02, avg batch time: 0.4772, average train loss: 2.2945
[09/26 10:20:32 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1584, average loss: 2.2308
[09/26 10:20:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:20:32 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.6631605465176369
[09/26 10:20:38 visual_prompt]: Epoch 70 / 100: avg data time: 5.52e-02, avg batch time: 0.4689, average train loss: 2.2895
[09/26 10:20:40 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1578, average loss: 2.2475
[09/26 10:20:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 10:20:40 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.6250000000000002
[09/26 10:20:46 visual_prompt]: Epoch 71 / 100: avg data time: 6.31e-02, avg batch time: 0.4747, average train loss: 2.2734
[09/26 10:20:48 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1582, average loss: 2.2299
[09/26 10:20:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 10:20:48 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.587600919708494
[09/26 10:20:55 visual_prompt]: Epoch 72 / 100: avg data time: 5.99e-02, avg batch time: 0.4725, average train loss: 2.2733
[09/26 10:20:56 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1583, average loss: 2.2671
[09/26 10:20:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.00	top5: 64.50	
[09/26 10:20:56 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.5510088706615667
[09/26 10:21:03 visual_prompt]: Epoch 73 / 100: avg data time: 6.46e-02, avg batch time: 0.4763, average train loss: 2.2803
[09/26 10:21:04 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1583, average loss: 2.2299
[09/26 10:21:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 10:21:04 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.5152684346344087
[09/26 10:21:11 visual_prompt]: Epoch 74 / 100: avg data time: 6.09e-02, avg batch time: 0.4750, average train loss: 2.2979
[09/26 10:21:12 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1587, average loss: 2.3625
[09/26 10:21:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 59.00	
[09/26 10:21:12 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.48042315584292716
[09/26 10:21:19 visual_prompt]: Epoch 75 / 100: avg data time: 6.74e-02, avg batch time: 0.4790, average train loss: 2.2789
[09/26 10:21:20 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1602, average loss: 2.2616
[09/26 10:21:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 10:21:20 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.4465154878918258
[09/26 10:21:27 visual_prompt]: Epoch 76 / 100: avg data time: 6.38e-02, avg batch time: 0.4762, average train loss: 2.2858
[09/26 10:21:29 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1583, average loss: 2.2411
[09/26 10:21:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 10:21:29 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.41358674205142765
[09/26 10:21:35 visual_prompt]: Epoch 77 / 100: avg data time: 5.81e-02, avg batch time: 0.4702, average train loss: 2.2684
[09/26 10:21:37 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1581, average loss: 2.2588
[09/26 10:21:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 10:21:37 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.3816770369262533
[09/26 10:21:43 visual_prompt]: Epoch 78 / 100: avg data time: 6.11e-02, avg batch time: 0.4727, average train loss: 2.2734
[09/26 10:21:45 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1582, average loss: 2.2800
[09/26 10:21:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 10:21:45 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.35082524957668626
[09/26 10:21:51 visual_prompt]: Epoch 79 / 100: avg data time: 6.65e-02, avg batch time: 0.4795, average train loss: 2.2595
[09/26 10:21:53 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1580, average loss: 2.2259
[09/26 10:21:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:21:53 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.32106896815325703
[09/26 10:21:59 visual_prompt]: Epoch 80 / 100: avg data time: 6.10e-02, avg batch time: 0.4735, average train loss: 2.2768
[09/26 10:22:01 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1581, average loss: 2.2287
[09/26 10:22:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 10:22:01 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.2924444461012776
[09/26 10:22:07 visual_prompt]: Epoch 81 / 100: avg data time: 5.93e-02, avg batch time: 0.4708, average train loss: 2.2597
[09/26 10:22:09 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 2.2535
[09/26 10:22:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 10:22:09 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.26498655799159765
[09/26 10:22:16 visual_prompt]: Epoch 82 / 100: avg data time: 7.13e-02, avg batch time: 0.4830, average train loss: 2.2654
[09/26 10:22:17 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1583, average loss: 2.2436
[09/26 10:22:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:22:17 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.23872875703131582
[09/26 10:22:24 visual_prompt]: Epoch 83 / 100: avg data time: 5.78e-02, avg batch time: 0.4706, average train loss: 2.2538
[09/26 10:22:25 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1585, average loss: 2.2326
[09/26 10:22:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:22:25 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.21370303430619797
[09/26 10:22:32 visual_prompt]: Epoch 84 / 100: avg data time: 6.11e-02, avg batch time: 0.4727, average train loss: 2.2569
[09/26 10:22:33 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1584, average loss: 2.2207
[09/26 10:22:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 10:22:33 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.18993987980446755
[09/26 10:22:40 visual_prompt]: Epoch 85 / 100: avg data time: 6.50e-02, avg batch time: 0.4768, average train loss: 2.2422
[09/26 10:22:41 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1579, average loss: 2.2362
[09/26 10:22:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 10:22:41 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.16746824526945162
[09/26 10:22:48 visual_prompt]: Epoch 86 / 100: avg data time: 6.62e-02, avg batch time: 0.4781, average train loss: 2.2531
[09/26 10:22:50 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1583, average loss: 2.2309
[09/26 10:22:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 10:22:50 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.14631550892634126
[09/26 10:22:56 visual_prompt]: Epoch 87 / 100: avg data time: 6.50e-02, avg batch time: 0.4767, average train loss: 2.2516
[09/26 10:22:58 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1577, average loss: 2.2173
[09/26 10:22:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 10:22:58 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.12650744212604148
[09/26 10:23:04 visual_prompt]: Epoch 88 / 100: avg data time: 5.91e-02, avg batch time: 0.4701, average train loss: 2.2462
[09/26 10:23:06 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1579, average loss: 2.2179
[09/26 10:23:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:23:06 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.10806817794674878
[09/26 10:23:12 visual_prompt]: Epoch 89 / 100: avg data time: 6.63e-02, avg batch time: 0.4768, average train loss: 2.2471
[09/26 10:23:14 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1581, average loss: 2.2217
[09/26 10:23:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:23:14 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.09102018179151586
[09/26 10:23:20 visual_prompt]: Epoch 90 / 100: avg data time: 5.56e-02, avg batch time: 0.4668, average train loss: 2.2404
[09/26 10:23:22 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1582, average loss: 2.2126
[09/26 10:23:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 10:23:22 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0753842240176146
[09/26 10:23:28 visual_prompt]: Epoch 91 / 100: avg data time: 6.15e-02, avg batch time: 0.4728, average train loss: 2.2365
[09/26 10:23:30 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1585, average loss: 2.2212
[09/26 10:23:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:23:30 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.061179354631058086
[09/26 10:23:36 visual_prompt]: Epoch 92 / 100: avg data time: 5.17e-02, avg batch time: 0.4657, average train loss: 2.2376
[09/26 10:23:38 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1584, average loss: 2.2171
[09/26 10:23:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:23:38 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.04842288007710138
[09/26 10:23:44 visual_prompt]: Epoch 93 / 100: avg data time: 5.60e-02, avg batch time: 0.4667, average train loss: 2.2339
[09/26 10:23:46 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1587, average loss: 2.2141
[09/26 10:23:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 10:23:46 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.03713034215500441
[09/26 10:23:52 visual_prompt]: Epoch 94 / 100: avg data time: 5.34e-02, avg batch time: 0.4649, average train loss: 2.2332
[09/26 10:23:54 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1579, average loss: 2.2185
[09/26 10:23:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 10:23:54 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.02731549908274289
[09/26 10:24:00 visual_prompt]: Epoch 95 / 100: avg data time: 5.97e-02, avg batch time: 0.4718, average train loss: 2.2327
[09/26 10:24:02 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1582, average loss: 2.2176
[09/26 10:24:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:24:02 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.018990308734739975
[09/26 10:24:08 visual_prompt]: Epoch 96 / 100: avg data time: 6.72e-02, avg batch time: 0.4783, average train loss: 2.2315
[09/26 10:24:10 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1581, average loss: 2.2170
[09/26 10:24:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:24:10 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.012164914073037048
[09/26 10:24:16 visual_prompt]: Epoch 97 / 100: avg data time: 5.70e-02, avg batch time: 0.4693, average train loss: 2.2311
[09/26 10:24:18 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1584, average loss: 2.2160
[09/26 10:24:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:24:18 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.006847630789658388
[09/26 10:24:24 visual_prompt]: Epoch 98 / 100: avg data time: 6.12e-02, avg batch time: 0.4731, average train loss: 2.2306
[09/26 10:24:26 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1576, average loss: 2.2163
[09/26 10:24:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:24:26 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.003044937175219753
[09/26 10:24:33 visual_prompt]: Epoch 99 / 100: avg data time: 6.50e-02, avg batch time: 0.4757, average train loss: 2.2298
[09/26 10:24:34 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1583, average loss: 2.2172
[09/26 10:24:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:24:34 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0007614662261302974
[09/26 10:24:41 visual_prompt]: Epoch 100 / 100: avg data time: 5.47e-02, avg batch time: 0.4660, average train loss: 2.2291
[09/26 10:24:42 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1584, average loss: 2.2167
[09/26 10:24:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:24:42 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 10:24:42 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 10:24:42 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 10:24:42 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 10:24:42 visual_prompt]: Training with config:
[09/26 10:24:42 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr2.5_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 10:24:42 visual_prompt]: Loading training data...
[09/26 10:24:42 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 10:24:43 visual_prompt]: Number of images: 800
[09/26 10:24:43 visual_prompt]: Number of classes: 10 / 10
[09/26 10:24:43 visual_prompt]: Loading validation data...
[09/26 10:24:43 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 10:24:43 visual_prompt]: Number of images: 200
[09/26 10:24:43 visual_prompt]: Number of classes: 10 / 10
[09/26 10:24:43 visual_prompt]: Constructing models...
[09/26 10:24:46 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 10:24:46 visual_prompt]: tuned percent:0.543
[09/26 10:24:46 visual_prompt]: Device used for model: 0
[09/26 10:24:46 visual_prompt]: Setting up Evaluator...
[09/26 10:24:46 visual_prompt]: Setting up Trainer...
[09/26 10:24:46 visual_prompt]: 	Setting up the optimizer...
[09/26 10:24:46 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 10:24:52 visual_prompt]: Epoch 1 / 100: avg data time: 5.74e-02, avg batch time: 0.4770, average train loss: 2.6667
[09/26 10:24:54 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1580, average loss: 2.6214
[09/26 10:24:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 10:24:54 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 10:24:54 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[09/26 10:25:00 visual_prompt]: Epoch 2 / 100: avg data time: 5.80e-02, avg batch time: 0.4692, average train loss: 3.5001
[09/26 10:25:02 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1578, average loss: 2.2795
[09/26 10:25:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 10:25:02 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[09/26 10:25:08 visual_prompt]: Epoch 3 / 100: avg data time: 5.05e-02, avg batch time: 0.4615, average train loss: 2.6454
[09/26 10:25:10 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1580, average loss: 2.3395
[09/26 10:25:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 57.50	
[09/26 10:25:10 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[09/26 10:25:16 visual_prompt]: Epoch 4 / 100: avg data time: 6.46e-02, avg batch time: 0.4765, average train loss: 2.4803
[09/26 10:25:18 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1582, average loss: 2.4528
[09/26 10:25:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.00	
[09/26 10:25:18 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[09/26 10:25:24 visual_prompt]: Epoch 5 / 100: avg data time: 4.93e-02, avg batch time: 0.4627, average train loss: 2.4308
[09/26 10:25:26 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1581, average loss: 2.3124
[09/26 10:25:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 59.50	
[09/26 10:25:26 visual_prompt]: Best epoch 5: best metric: 0.235
[09/26 10:25:26 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[09/26 10:25:32 visual_prompt]: Epoch 6 / 100: avg data time: 6.33e-02, avg batch time: 0.4764, average train loss: 2.3896
[09/26 10:25:34 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1584, average loss: 2.5033
[09/26 10:25:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 46.00	
[09/26 10:25:34 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[09/26 10:25:41 visual_prompt]: Epoch 7 / 100: avg data time: 6.21e-02, avg batch time: 0.4737, average train loss: 2.4701
[09/26 10:25:42 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1583, average loss: 2.4262
[09/26 10:25:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 57.50	
[09/26 10:25:42 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[09/26 10:25:49 visual_prompt]: Epoch 8 / 100: avg data time: 6.32e-02, avg batch time: 0.4756, average train loss: 2.4064
[09/26 10:25:50 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1588, average loss: 2.4890
[09/26 10:25:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 59.00	
[09/26 10:25:50 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[09/26 10:25:57 visual_prompt]: Epoch 9 / 100: avg data time: 5.99e-02, avg batch time: 0.4729, average train loss: 2.4232
[09/26 10:25:58 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1582, average loss: 2.3099
[09/26 10:25:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 10:25:58 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[09/26 10:26:05 visual_prompt]: Epoch 10 / 100: avg data time: 6.28e-02, avg batch time: 0.4753, average train loss: 2.7134
[09/26 10:26:07 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1582, average loss: 3.7079
[09/26 10:26:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 43.50	
[09/26 10:26:07 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[09/26 10:26:13 visual_prompt]: Epoch 11 / 100: avg data time: 6.13e-02, avg batch time: 0.4742, average train loss: 6.9321
[09/26 10:26:15 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1577, average loss: 10.0411
[09/26 10:26:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 38.50	
[09/26 10:26:15 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[09/26 10:26:21 visual_prompt]: Epoch 12 / 100: avg data time: 6.42e-02, avg batch time: 0.4761, average train loss: 5.3128
[09/26 10:26:23 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1580, average loss: 4.0210
[09/26 10:26:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 10:26:23 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[09/26 10:26:29 visual_prompt]: Epoch 13 / 100: avg data time: 6.45e-02, avg batch time: 0.4762, average train loss: 3.6329
[09/26 10:26:31 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1579, average loss: 3.0225
[09/26 10:26:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 43.50	
[09/26 10:26:31 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[09/26 10:26:37 visual_prompt]: Epoch 14 / 100: avg data time: 6.56e-02, avg batch time: 0.4787, average train loss: 2.8151
[09/26 10:26:39 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1582, average loss: 2.9922
[09/26 10:26:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 10:26:39 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[09/26 10:26:46 visual_prompt]: Epoch 15 / 100: avg data time: 6.34e-02, avg batch time: 0.4753, average train loss: 2.8250
[09/26 10:26:47 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1579, average loss: 2.4621
[09/26 10:26:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 10:26:47 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[09/26 10:26:54 visual_prompt]: Epoch 16 / 100: avg data time: 5.48e-02, avg batch time: 0.4670, average train loss: 2.6423
[09/26 10:26:55 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1582, average loss: 2.6961
[09/26 10:26:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 47.50	
[09/26 10:26:55 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[09/26 10:27:01 visual_prompt]: Epoch 17 / 100: avg data time: 4.52e-02, avg batch time: 0.4575, average train loss: 2.5202
[09/26 10:27:03 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1578, average loss: 2.3779
[09/26 10:27:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 58.50	
[09/26 10:27:03 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[09/26 10:27:10 visual_prompt]: Epoch 18 / 100: avg data time: 6.36e-02, avg batch time: 0.4750, average train loss: 2.3871
[09/26 10:27:11 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1581, average loss: 2.3170
[09/26 10:27:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 10:27:11 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[09/26 10:27:18 visual_prompt]: Epoch 19 / 100: avg data time: 6.32e-02, avg batch time: 0.4764, average train loss: 2.4621
[09/26 10:27:19 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1583, average loss: 2.4297
[09/26 10:27:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/26 10:27:19 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[09/26 10:27:26 visual_prompt]: Epoch 20 / 100: avg data time: 6.33e-02, avg batch time: 0.4751, average train loss: 2.4216
[09/26 10:27:27 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1582, average loss: 2.3591
[09/26 10:27:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 10:27:27 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[09/26 10:27:34 visual_prompt]: Epoch 21 / 100: avg data time: 6.31e-02, avg batch time: 0.4753, average train loss: 2.3547
[09/26 10:27:36 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1581, average loss: 2.4453
[09/26 10:27:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 10:27:36 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[09/26 10:27:42 visual_prompt]: Epoch 22 / 100: avg data time: 6.30e-02, avg batch time: 0.4750, average train loss: 2.5842
[09/26 10:27:44 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1581, average loss: 2.6256
[09/26 10:27:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 10:27:44 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[09/26 10:27:50 visual_prompt]: Epoch 23 / 100: avg data time: 6.55e-02, avg batch time: 0.4769, average train loss: 3.3010
[09/26 10:27:52 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1577, average loss: 5.8474
[09/26 10:27:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 42.00	
[09/26 10:27:52 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[09/26 10:27:58 visual_prompt]: Epoch 24 / 100: avg data time: 4.98e-02, avg batch time: 0.4626, average train loss: 8.8183
[09/26 10:28:00 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1581, average loss: 9.8229
[09/26 10:28:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 10:28:00 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[09/26 10:28:06 visual_prompt]: Epoch 25 / 100: avg data time: 6.46e-02, avg batch time: 0.4763, average train loss: 10.1216
[09/26 10:28:08 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1585, average loss: 8.5152
[09/26 10:28:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 55.00	
[09/26 10:28:08 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[09/26 10:28:14 visual_prompt]: Epoch 26 / 100: avg data time: 5.20e-02, avg batch time: 0.4651, average train loss: 6.9504
[09/26 10:28:16 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1584, average loss: 5.6977
[09/26 10:28:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.50	
[09/26 10:28:16 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[09/26 10:28:22 visual_prompt]: Epoch 27 / 100: avg data time: 5.50e-02, avg batch time: 0.4676, average train loss: 4.9261
[09/26 10:28:24 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1577, average loss: 3.2496
[09/26 10:28:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 10:28:24 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[09/26 10:28:30 visual_prompt]: Epoch 28 / 100: avg data time: 5.95e-02, avg batch time: 0.4727, average train loss: 3.6153
[09/26 10:28:32 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1580, average loss: 3.2346
[09/26 10:28:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 38.00	
[09/26 10:28:32 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[09/26 10:28:38 visual_prompt]: Epoch 29 / 100: avg data time: 6.89e-02, avg batch time: 0.4812, average train loss: 3.0956
[09/26 10:28:40 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1582, average loss: 3.4032
[09/26 10:28:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 46.50	
[09/26 10:28:40 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[09/26 10:28:46 visual_prompt]: Epoch 30 / 100: avg data time: 6.40e-02, avg batch time: 0.4765, average train loss: 2.9299
[09/26 10:28:48 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1581, average loss: 2.5923
[09/26 10:28:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.00	
[09/26 10:28:48 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[09/26 10:28:55 visual_prompt]: Epoch 31 / 100: avg data time: 6.31e-02, avg batch time: 0.4749, average train loss: 2.5019
[09/26 10:28:56 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1587, average loss: 2.6635
[09/26 10:28:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 42.00	
[09/26 10:28:56 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[09/26 10:29:03 visual_prompt]: Epoch 32 / 100: avg data time: 5.92e-02, avg batch time: 0.4709, average train loss: 2.5712
[09/26 10:29:04 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.1581, average loss: 2.3475
[09/26 10:29:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.00	
[09/26 10:29:04 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[09/26 10:29:11 visual_prompt]: Epoch 33 / 100: avg data time: 5.47e-02, avg batch time: 0.4687, average train loss: 2.7219
[09/26 10:29:12 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1581, average loss: 2.5933
[09/26 10:29:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 43.50	
[09/26 10:29:12 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[09/26 10:29:19 visual_prompt]: Epoch 34 / 100: avg data time: 5.85e-02, avg batch time: 0.4703, average train loss: 2.4674
[09/26 10:29:20 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1579, average loss: 2.3456
[09/26 10:29:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.50	
[09/26 10:29:20 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[09/26 10:29:27 visual_prompt]: Epoch 35 / 100: avg data time: 6.38e-02, avg batch time: 0.4753, average train loss: 2.5646
[09/26 10:29:28 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1583, average loss: 2.5373
[09/26 10:29:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 10:29:28 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[09/26 10:29:35 visual_prompt]: Epoch 36 / 100: avg data time: 6.74e-02, avg batch time: 0.4810, average train loss: 2.4564
[09/26 10:29:37 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1576, average loss: 2.5411
[09/26 10:29:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 43.50	
[09/26 10:29:37 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[09/26 10:29:43 visual_prompt]: Epoch 37 / 100: avg data time: 5.60e-02, avg batch time: 0.4688, average train loss: 2.4391
[09/26 10:29:45 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1578, average loss: 2.3312
[09/26 10:29:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/26 10:29:45 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[09/26 10:29:51 visual_prompt]: Epoch 38 / 100: avg data time: 5.19e-02, avg batch time: 0.4651, average train loss: 2.3698
[09/26 10:29:52 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1581, average loss: 2.3555
[09/26 10:29:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 10:29:52 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[09/26 10:29:59 visual_prompt]: Epoch 39 / 100: avg data time: 5.86e-02, avg batch time: 0.4710, average train loss: 2.3839
[09/26 10:30:00 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1579, average loss: 2.2996
[09/26 10:30:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 10:30:01 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[09/26 10:30:07 visual_prompt]: Epoch 40 / 100: avg data time: 5.88e-02, avg batch time: 0.4707, average train loss: 2.3614
[09/26 10:30:08 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1577, average loss: 2.2657
[09/26 10:30:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 10:30:08 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[09/26 10:30:15 visual_prompt]: Epoch 41 / 100: avg data time: 6.29e-02, avg batch time: 0.4743, average train loss: 2.3389
[09/26 10:30:17 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1584, average loss: 2.3719
[09/26 10:30:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 56.50	
[09/26 10:30:17 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[09/26 10:30:23 visual_prompt]: Epoch 42 / 100: avg data time: 6.92e-02, avg batch time: 0.4825, average train loss: 2.3530
[09/26 10:30:25 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1578, average loss: 2.3157
[09/26 10:30:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.00	
[09/26 10:30:25 visual_prompt]: Training 43 / 100 epoch, with learning rate 1.7979639334863466
[09/26 10:30:31 visual_prompt]: Epoch 43 / 100: avg data time: 6.47e-02, avg batch time: 0.4776, average train loss: 2.3408
[09/26 10:30:33 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1581, average loss: 2.3040
[09/26 10:30:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.00	
[09/26 10:30:33 visual_prompt]: Training 44 / 100 epoch, with learning rate 1.7584208038447504
[09/26 10:30:40 visual_prompt]: Epoch 44 / 100: avg data time: 6.74e-02, avg batch time: 0.4790, average train loss: 2.3370
[09/26 10:30:41 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1579, average loss: 2.3587
[09/26 10:30:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.50	
[09/26 10:30:41 visual_prompt]: Training 45 / 100 epoch, with learning rate 1.7182582417698902
[09/26 10:30:48 visual_prompt]: Epoch 45 / 100: avg data time: 6.08e-02, avg batch time: 0.4741, average train loss: 2.3723
[09/26 10:30:49 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1584, average loss: 2.3454
[09/26 10:30:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 10:30:49 visual_prompt]: Training 46 / 100 epoch, with learning rate 1.6775251791570862
[09/26 10:30:56 visual_prompt]: Epoch 46 / 100: avg data time: 6.46e-02, avg batch time: 0.4782, average train loss: 2.3498
[09/26 10:30:57 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1581, average loss: 2.4738
[09/26 10:30:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 46.50	
[09/26 10:30:57 visual_prompt]: Training 47 / 100 epoch, with learning rate 1.6362712429686843
[09/26 10:31:04 visual_prompt]: Epoch 47 / 100: avg data time: 5.18e-02, avg batch time: 0.4645, average train loss: 2.3746
[09/26 10:31:05 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1585, average loss: 2.3772
[09/26 10:31:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.00	
[09/26 10:31:05 visual_prompt]: Training 48 / 100 epoch, with learning rate 1.5945466947712488
[09/26 10:31:12 visual_prompt]: Epoch 48 / 100: avg data time: 5.97e-02, avg batch time: 0.4722, average train loss: 2.3495
[09/26 10:31:13 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1579, average loss: 2.3985
[09/26 10:31:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.50	
[09/26 10:31:13 visual_prompt]: Training 49 / 100 epoch, with learning rate 1.5524023694995845
[09/26 10:31:20 visual_prompt]: Epoch 49 / 100: avg data time: 5.74e-02, avg batch time: 0.4700, average train loss: 2.3394
[09/26 10:31:21 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.1585, average loss: 2.2973
[09/26 10:31:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 10:31:21 visual_prompt]: Training 50 / 100 epoch, with learning rate 1.509889613522199
[09/26 10:31:28 visual_prompt]: Epoch 50 / 100: avg data time: 4.96e-02, avg batch time: 0.4623, average train loss: 2.3373
[09/26 10:31:29 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1581, average loss: 2.2808
[09/26 10:31:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 10:31:29 visual_prompt]: Training 51 / 100 epoch, with learning rate 1.467060222083663
[09/26 10:31:36 visual_prompt]: Epoch 51 / 100: avg data time: 6.15e-02, avg batch time: 0.4742, average train loss: 2.3164
[09/26 10:31:37 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1580, average loss: 2.3063
[09/26 10:31:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 64.50	
[09/26 10:31:37 visual_prompt]: Training 52 / 100 epoch, with learning rate 1.4239663762000818
[09/26 10:31:44 visual_prompt]: Epoch 52 / 100: avg data time: 6.12e-02, avg batch time: 0.4737, average train loss: 2.2928
[09/26 10:31:45 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1579, average loss: 2.2335
[09/26 10:31:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 10:31:45 visual_prompt]: Training 53 / 100 epoch, with learning rate 1.380660579084567
[09/26 10:31:52 visual_prompt]: Epoch 53 / 100: avg data time: 6.61e-02, avg batch time: 0.4774, average train loss: 2.2912
[09/26 10:31:53 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1582, average loss: 2.2661
[09/26 10:31:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 10:31:53 visual_prompt]: Training 54 / 100 epoch, with learning rate 1.3371955921801564
[09/26 10:32:00 visual_prompt]: Epoch 54 / 100: avg data time: 6.69e-02, avg batch time: 0.4785, average train loss: 2.2804
[09/26 10:32:02 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1581, average loss: 2.2736
[09/26 10:32:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 10:32:02 visual_prompt]: Training 55 / 100 epoch, with learning rate 1.2936243708781263
[09/26 10:32:08 visual_prompt]: Epoch 55 / 100: avg data time: 6.70e-02, avg batch time: 0.4793, average train loss: 2.2887
[09/26 10:32:10 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1580, average loss: 2.2680
[09/26 10:32:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:32:10 visual_prompt]: Training 56 / 100 epoch, with learning rate 1.25
[09/26 10:32:16 visual_prompt]: Epoch 56 / 100: avg data time: 5.79e-02, avg batch time: 0.4716, average train loss: 2.2854
[09/26 10:32:18 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1587, average loss: 2.2262
[09/26 10:32:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:32:18 visual_prompt]: Training 57 / 100 epoch, with learning rate 1.2063756291218741
[09/26 10:32:24 visual_prompt]: Epoch 57 / 100: avg data time: 5.06e-02, avg batch time: 0.4627, average train loss: 2.2714
[09/26 10:32:26 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1581, average loss: 2.3916
[09/26 10:32:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.50	
[09/26 10:32:26 visual_prompt]: Training 58 / 100 epoch, with learning rate 1.1628044078198434
[09/26 10:32:32 visual_prompt]: Epoch 58 / 100: avg data time: 6.54e-02, avg batch time: 0.4777, average train loss: 2.3028
[09/26 10:32:34 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1582, average loss: 2.3299
[09/26 10:32:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.00	
[09/26 10:32:34 visual_prompt]: Training 59 / 100 epoch, with learning rate 1.1193394209154333
[09/26 10:32:41 visual_prompt]: Epoch 59 / 100: avg data time: 6.61e-02, avg batch time: 0.4784, average train loss: 2.3070
[09/26 10:32:42 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1577, average loss: 2.3315
[09/26 10:32:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 10:32:42 visual_prompt]: Training 60 / 100 epoch, with learning rate 1.0760336237999186
[09/26 10:32:49 visual_prompt]: Epoch 60 / 100: avg data time: 6.32e-02, avg batch time: 0.4746, average train loss: 2.3117
[09/26 10:32:50 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1579, average loss: 2.3101
[09/26 10:32:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 10:32:50 visual_prompt]: Training 61 / 100 epoch, with learning rate 1.0329397779163372
[09/26 10:32:57 visual_prompt]: Epoch 61 / 100: avg data time: 5.98e-02, avg batch time: 0.4719, average train loss: 2.2945
[09/26 10:32:58 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1577, average loss: 2.2791
[09/26 10:32:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 10:32:58 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.990110386477801
[09/26 10:33:05 visual_prompt]: Epoch 62 / 100: avg data time: 5.70e-02, avg batch time: 0.4698, average train loss: 2.2734
[09/26 10:33:06 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1579, average loss: 2.2661
[09/26 10:33:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 10:33:06 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.9475976305004155
[09/26 10:33:13 visual_prompt]: Epoch 63 / 100: avg data time: 5.51e-02, avg batch time: 0.4684, average train loss: 2.2782
[09/26 10:33:15 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1584, average loss: 2.2469
[09/26 10:33:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.00	top5: 64.50	
[09/26 10:33:15 visual_prompt]: Best epoch 63: best metric: 0.240
[09/26 10:33:15 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.9054533052287511
[09/26 10:33:21 visual_prompt]: Epoch 64 / 100: avg data time: 6.19e-02, avg batch time: 0.4733, average train loss: 2.2711
[09/26 10:33:23 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.1584, average loss: 2.2483
[09/26 10:33:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 10:33:23 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.8637287570313158
[09/26 10:33:29 visual_prompt]: Epoch 65 / 100: avg data time: 6.49e-02, avg batch time: 0.4766, average train loss: 2.3007
[09/26 10:33:31 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1582, average loss: 2.3168
[09/26 10:33:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 10:33:31 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.8224748208429142
[09/26 10:33:37 visual_prompt]: Epoch 66 / 100: avg data time: 6.35e-02, avg batch time: 0.4758, average train loss: 2.3053
[09/26 10:33:39 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1580, average loss: 2.2257
[09/26 10:33:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 10:33:39 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.7817417582301099
[09/26 10:33:45 visual_prompt]: Epoch 67 / 100: avg data time: 6.67e-02, avg batch time: 0.4791, average train loss: 2.2631
[09/26 10:33:47 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1577, average loss: 2.2434
[09/26 10:33:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 10:33:47 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.7415791961552499
[09/26 10:33:53 visual_prompt]: Epoch 68 / 100: avg data time: 5.83e-02, avg batch time: 0.4696, average train loss: 2.2794
[09/26 10:33:55 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1579, average loss: 2.2290
[09/26 10:33:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 10:33:55 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.7020360665136531
[09/26 10:34:01 visual_prompt]: Epoch 69 / 100: avg data time: 6.42e-02, avg batch time: 0.4768, average train loss: 2.2703
[09/26 10:34:03 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1583, average loss: 2.2517
[09/26 10:34:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 10:34:03 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.6631605465176369
[09/26 10:34:10 visual_prompt]: Epoch 70 / 100: avg data time: 5.96e-02, avg batch time: 0.4723, average train loss: 2.2740
[09/26 10:34:11 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1583, average loss: 2.2493
[09/26 10:34:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 10:34:11 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.6250000000000002
[09/26 10:34:18 visual_prompt]: Epoch 71 / 100: avg data time: 6.81e-02, avg batch time: 0.4804, average train loss: 2.2782
[09/26 10:34:19 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1581, average loss: 2.2346
[09/26 10:34:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 10:34:19 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.587600919708494
[09/26 10:34:26 visual_prompt]: Epoch 72 / 100: avg data time: 5.50e-02, avg batch time: 0.4675, average train loss: 2.2684
[09/26 10:34:27 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1580, average loss: 2.2382
[09/26 10:34:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 10:34:27 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.5510088706615667
[09/26 10:34:34 visual_prompt]: Epoch 73 / 100: avg data time: 6.12e-02, avg batch time: 0.4760, average train loss: 2.2672
[09/26 10:34:35 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1580, average loss: 2.2294
[09/26 10:34:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 10:34:35 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.5152684346344087
[09/26 10:34:42 visual_prompt]: Epoch 74 / 100: avg data time: 6.59e-02, avg batch time: 0.4781, average train loss: 2.2632
[09/26 10:34:43 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1583, average loss: 2.2502
[09/26 10:34:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.00	
[09/26 10:34:43 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.48042315584292716
[09/26 10:34:50 visual_prompt]: Epoch 75 / 100: avg data time: 6.34e-02, avg batch time: 0.4769, average train loss: 2.2724
[09/26 10:34:52 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1576, average loss: 2.2215
[09/26 10:34:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 10:34:52 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.4465154878918258
[09/26 10:34:58 visual_prompt]: Epoch 76 / 100: avg data time: 6.20e-02, avg batch time: 0.4747, average train loss: 2.2457
[09/26 10:35:00 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1579, average loss: 2.2489
[09/26 10:35:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:35:00 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.41358674205142765
[09/26 10:35:06 visual_prompt]: Epoch 77 / 100: avg data time: 5.07e-02, avg batch time: 0.4624, average train loss: 2.2587
[09/26 10:35:08 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1583, average loss: 2.2549
[09/26 10:35:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 10:35:08 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.3816770369262533
[09/26 10:35:14 visual_prompt]: Epoch 78 / 100: avg data time: 6.13e-02, avg batch time: 0.4727, average train loss: 2.2534
[09/26 10:35:16 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.1582, average loss: 2.2152
[09/26 10:35:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:35:16 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.35082524957668626
[09/26 10:35:22 visual_prompt]: Epoch 79 / 100: avg data time: 6.26e-02, avg batch time: 0.4744, average train loss: 2.2470
[09/26 10:35:24 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1585, average loss: 2.2211
[09/26 10:35:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 10:35:24 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.32106896815325703
[09/26 10:35:30 visual_prompt]: Epoch 80 / 100: avg data time: 6.27e-02, avg batch time: 0.4749, average train loss: 2.2418
[09/26 10:35:32 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1578, average loss: 2.2185
[09/26 10:35:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:35:32 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.2924444461012776
[09/26 10:35:38 visual_prompt]: Epoch 81 / 100: avg data time: 6.48e-02, avg batch time: 0.4759, average train loss: 2.2435
[09/26 10:35:40 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1580, average loss: 2.2135
[09/26 10:35:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:35:40 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.26498655799159765
[09/26 10:35:46 visual_prompt]: Epoch 82 / 100: avg data time: 6.39e-02, avg batch time: 0.4756, average train loss: 2.2453
[09/26 10:35:48 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1583, average loss: 2.2186
[09/26 10:35:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:35:48 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.23872875703131582
[09/26 10:35:55 visual_prompt]: Epoch 83 / 100: avg data time: 6.29e-02, avg batch time: 0.4741, average train loss: 2.2368
[09/26 10:35:56 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1581, average loss: 2.2162
[09/26 10:35:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:35:56 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.21370303430619797
[09/26 10:36:03 visual_prompt]: Epoch 84 / 100: avg data time: 5.98e-02, avg batch time: 0.4721, average train loss: 2.2383
[09/26 10:36:04 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1580, average loss: 2.2141
[09/26 10:36:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:36:04 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.18993987980446755
[09/26 10:36:11 visual_prompt]: Epoch 85 / 100: avg data time: 5.03e-02, avg batch time: 0.4624, average train loss: 2.2361
[09/26 10:36:12 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1582, average loss: 2.2145
[09/26 10:36:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:36:12 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.16746824526945162
[09/26 10:36:19 visual_prompt]: Epoch 86 / 100: avg data time: 4.80e-02, avg batch time: 0.4641, average train loss: 2.2382
[09/26 10:36:20 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1579, average loss: 2.2149
[09/26 10:36:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 10:36:20 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.14631550892634126
[09/26 10:36:27 visual_prompt]: Epoch 87 / 100: avg data time: 6.62e-02, avg batch time: 0.4786, average train loss: 2.2364
[09/26 10:36:28 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1583, average loss: 2.2175
[09/26 10:36:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 10:36:28 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.12650744212604148
[09/26 10:36:35 visual_prompt]: Epoch 88 / 100: avg data time: 5.64e-02, avg batch time: 0.4690, average train loss: 2.2388
[09/26 10:36:36 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1583, average loss: 2.2131
[09/26 10:36:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:36:36 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.10806817794674878
[09/26 10:36:43 visual_prompt]: Epoch 89 / 100: avg data time: 6.46e-02, avg batch time: 0.4772, average train loss: 2.2355
[09/26 10:36:44 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1582, average loss: 2.2152
[09/26 10:36:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:36:44 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.09102018179151586
[09/26 10:36:51 visual_prompt]: Epoch 90 / 100: avg data time: 6.06e-02, avg batch time: 0.4717, average train loss: 2.2339
[09/26 10:36:52 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1583, average loss: 2.2126
[09/26 10:36:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:36:52 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0753842240176146
[09/26 10:36:59 visual_prompt]: Epoch 91 / 100: avg data time: 6.21e-02, avg batch time: 0.4740, average train loss: 2.2329
[09/26 10:37:01 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1585, average loss: 2.2141
[09/26 10:37:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:37:01 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.061179354631058086
[09/26 10:37:07 visual_prompt]: Epoch 92 / 100: avg data time: 6.19e-02, avg batch time: 0.4759, average train loss: 2.2306
[09/26 10:37:09 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1582, average loss: 2.2112
[09/26 10:37:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:37:09 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.04842288007710138
[09/26 10:37:15 visual_prompt]: Epoch 93 / 100: avg data time: 6.71e-02, avg batch time: 0.4790, average train loss: 2.2321
[09/26 10:37:17 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1577, average loss: 2.2142
[09/26 10:37:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:37:17 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.03713034215500441
[09/26 10:37:23 visual_prompt]: Epoch 94 / 100: avg data time: 6.49e-02, avg batch time: 0.4770, average train loss: 2.2322
[09/26 10:37:25 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1583, average loss: 2.2171
[09/26 10:37:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:37:25 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.02731549908274289
[09/26 10:37:31 visual_prompt]: Epoch 95 / 100: avg data time: 4.94e-02, avg batch time: 0.4623, average train loss: 2.2318
[09/26 10:37:33 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1584, average loss: 2.2152
[09/26 10:37:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:37:33 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.018990308734739975
[09/26 10:37:40 visual_prompt]: Epoch 96 / 100: avg data time: 6.26e-02, avg batch time: 0.4743, average train loss: 2.2311
[09/26 10:37:41 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1582, average loss: 2.2145
[09/26 10:37:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:37:41 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.012164914073037048
[09/26 10:37:48 visual_prompt]: Epoch 97 / 100: avg data time: 6.53e-02, avg batch time: 0.4769, average train loss: 2.2306
[09/26 10:37:49 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1583, average loss: 2.2145
[09/26 10:37:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:37:49 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.006847630789658388
[09/26 10:37:56 visual_prompt]: Epoch 98 / 100: avg data time: 5.93e-02, avg batch time: 0.4711, average train loss: 2.2288
[09/26 10:37:57 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1576, average loss: 2.2134
[09/26 10:37:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:37:57 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.003044937175219753
[09/26 10:38:04 visual_prompt]: Epoch 99 / 100: avg data time: 6.41e-02, avg batch time: 0.4790, average train loss: 2.2290
[09/26 10:38:05 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1583, average loss: 2.2132
[09/26 10:38:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:38:05 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0007614662261302974
[09/26 10:38:12 visual_prompt]: Epoch 100 / 100: avg data time: 6.60e-02, avg batch time: 0.4778, average train loss: 2.2289
[09/26 10:38:13 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1581, average loss: 2.2132
[09/26 10:38:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 10:38:13 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 10:38:13 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 10:38:13 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 10:38:13 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 10:38:13 visual_prompt]: Training with config:
[09/26 10:38:13 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr2.5_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 10:38:13 visual_prompt]: Loading training data...
[09/26 10:38:13 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 10:38:14 visual_prompt]: Number of images: 800
[09/26 10:38:14 visual_prompt]: Number of classes: 10 / 10
[09/26 10:38:14 visual_prompt]: Loading validation data...
[09/26 10:38:14 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 10:38:15 visual_prompt]: Number of images: 200
[09/26 10:38:15 visual_prompt]: Number of classes: 10 / 10
[09/26 10:38:15 visual_prompt]: Constructing models...
[09/26 10:38:17 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 10:38:17 visual_prompt]: tuned percent:0.543
[09/26 10:38:17 visual_prompt]: Device used for model: 0
[09/26 10:38:17 visual_prompt]: Setting up Evaluator...
[09/26 10:38:17 visual_prompt]: Setting up Trainer...
[09/26 10:38:17 visual_prompt]: 	Setting up the optimizer...
[09/26 10:38:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 10:38:24 visual_prompt]: Epoch 1 / 100: avg data time: 6.08e-02, avg batch time: 0.4802, average train loss: 2.6648
[09/26 10:38:25 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1582, average loss: 2.6214
[09/26 10:38:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 10:38:25 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 10:38:25 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[09/26 10:38:32 visual_prompt]: Epoch 2 / 100: avg data time: 6.20e-02, avg batch time: 0.4734, average train loss: 3.5666
[09/26 10:38:33 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1578, average loss: 2.3389
[09/26 10:38:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.00	top5: 50.50	
[09/26 10:38:33 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[09/26 10:38:40 visual_prompt]: Epoch 3 / 100: avg data time: 4.93e-02, avg batch time: 0.4613, average train loss: 2.5363
[09/26 10:38:41 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1583, average loss: 2.3437
[09/26 10:38:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 59.50	
[09/26 10:38:41 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[09/26 10:38:48 visual_prompt]: Epoch 4 / 100: avg data time: 5.93e-02, avg batch time: 0.4718, average train loss: 2.3434
[09/26 10:38:49 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1579, average loss: 2.3506
[09/26 10:38:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 10:38:49 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[09/26 10:38:56 visual_prompt]: Epoch 5 / 100: avg data time: 6.60e-02, avg batch time: 0.4777, average train loss: 2.4579
[09/26 10:38:57 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1581, average loss: 2.3564
[09/26 10:38:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.50	
[09/26 10:38:57 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[09/26 10:39:04 visual_prompt]: Epoch 6 / 100: avg data time: 5.40e-02, avg batch time: 0.4656, average train loss: 2.4945
[09/26 10:39:05 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1582, average loss: 2.4941
[09/26 10:39:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.00	
[09/26 10:39:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[09/26 10:39:12 visual_prompt]: Epoch 7 / 100: avg data time: 5.82e-02, avg batch time: 0.4715, average train loss: 2.4743
[09/26 10:39:13 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1582, average loss: 2.4712
[09/26 10:39:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 53.50	
[09/26 10:39:13 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[09/26 10:39:20 visual_prompt]: Epoch 8 / 100: avg data time: 6.15e-02, avg batch time: 0.4729, average train loss: 2.5911
[09/26 10:39:22 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1581, average loss: 2.3483
[09/26 10:39:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.50	top5: 58.50	
[09/26 10:39:22 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[09/26 10:39:28 visual_prompt]: Epoch 9 / 100: avg data time: 6.01e-02, avg batch time: 0.4715, average train loss: 2.4944
[09/26 10:39:30 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1581, average loss: 2.4131
[09/26 10:39:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 26.50	top5: 61.00	
[09/26 10:39:30 visual_prompt]: Best epoch 9: best metric: 0.265
[09/26 10:39:30 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[09/26 10:39:36 visual_prompt]: Epoch 10 / 100: avg data time: 6.26e-02, avg batch time: 0.4743, average train loss: 2.4838
[09/26 10:39:38 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1582, average loss: 2.2786
[09/26 10:39:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.50	top5: 63.50	
[09/26 10:39:38 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[09/26 10:39:44 visual_prompt]: Epoch 11 / 100: avg data time: 5.29e-02, avg batch time: 0.4676, average train loss: 2.3948
[09/26 10:39:46 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1581, average loss: 2.1710
[09/26 10:39:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 39.50	top5: 69.00	
[09/26 10:39:46 visual_prompt]: Best epoch 11: best metric: 0.395
[09/26 10:39:46 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[09/26 10:39:52 visual_prompt]: Epoch 12 / 100: avg data time: 5.88e-02, avg batch time: 0.4722, average train loss: 2.7586
[09/26 10:39:54 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1579, average loss: 2.9325
[09/26 10:39:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.50	top5: 42.00	
[09/26 10:39:54 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[09/26 10:40:00 visual_prompt]: Epoch 13 / 100: avg data time: 6.06e-02, avg batch time: 0.4729, average train loss: 2.8830
[09/26 10:40:02 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1581, average loss: 2.9662
[09/26 10:40:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 48.00	
[09/26 10:40:02 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[09/26 10:40:08 visual_prompt]: Epoch 14 / 100: avg data time: 6.53e-02, avg batch time: 0.4769, average train loss: 2.7089
[09/26 10:40:10 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1583, average loss: 2.6886
[09/26 10:40:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 63.00	
[09/26 10:40:10 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[09/26 10:40:16 visual_prompt]: Epoch 15 / 100: avg data time: 6.71e-02, avg batch time: 0.4788, average train loss: 2.3969
[09/26 10:40:18 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1583, average loss: 2.4415
[09/26 10:40:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.00	top5: 69.00	
[09/26 10:40:18 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[09/26 10:40:24 visual_prompt]: Epoch 16 / 100: avg data time: 6.08e-02, avg batch time: 0.4728, average train loss: 2.0784
[09/26 10:40:26 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1584, average loss: 1.6719
[09/26 10:40:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 34.00	top5: 85.00	
[09/26 10:40:26 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[09/26 10:40:32 visual_prompt]: Epoch 17 / 100: avg data time: 5.26e-02, avg batch time: 0.4669, average train loss: 1.7344
[09/26 10:40:34 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1585, average loss: 1.9931
[09/26 10:40:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 32.00	top5: 83.00	
[09/26 10:40:34 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[09/26 10:40:41 visual_prompt]: Epoch 18 / 100: avg data time: 6.35e-02, avg batch time: 0.4759, average train loss: 2.8782
[09/26 10:40:42 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1582, average loss: 2.2327
[09/26 10:40:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 82.50	
[09/26 10:40:42 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[09/26 10:40:49 visual_prompt]: Epoch 19 / 100: avg data time: 6.57e-02, avg batch time: 0.4781, average train loss: 1.8107
[09/26 10:40:50 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1581, average loss: 1.4341
[09/26 10:40:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 43.50	top5: 93.50	
[09/26 10:40:50 visual_prompt]: Best epoch 19: best metric: 0.435
[09/26 10:40:50 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[09/26 10:40:57 visual_prompt]: Epoch 20 / 100: avg data time: 6.56e-02, avg batch time: 0.4776, average train loss: 1.4490
[09/26 10:40:59 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1582, average loss: 1.9109
[09/26 10:40:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 37.50	top5: 86.50	
[09/26 10:40:59 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[09/26 10:41:05 visual_prompt]: Epoch 21 / 100: avg data time: 6.57e-02, avg batch time: 0.4778, average train loss: 1.3759
[09/26 10:41:07 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1582, average loss: 1.3737
[09/26 10:41:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 53.00	top5: 95.00	
[09/26 10:41:07 visual_prompt]: Best epoch 21: best metric: 0.530
[09/26 10:41:07 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[09/26 10:41:13 visual_prompt]: Epoch 22 / 100: avg data time: 5.48e-02, avg batch time: 0.4674, average train loss: 1.1166
[09/26 10:41:15 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1584, average loss: 1.4192
[09/26 10:41:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 58.50	top5: 86.00	
[09/26 10:41:15 visual_prompt]: Best epoch 22: best metric: 0.585
[09/26 10:41:15 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[09/26 10:41:21 visual_prompt]: Epoch 23 / 100: avg data time: 6.49e-02, avg batch time: 0.4773, average train loss: 1.0792
[09/26 10:41:23 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1585, average loss: 1.0021
[09/26 10:41:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.00	top5: 97.00	
[09/26 10:41:23 visual_prompt]: Best epoch 23: best metric: 0.650
[09/26 10:41:23 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[09/26 10:41:29 visual_prompt]: Epoch 24 / 100: avg data time: 6.32e-02, avg batch time: 0.4756, average train loss: 0.8112
[09/26 10:41:31 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1583, average loss: 0.9039
[09/26 10:41:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 64.00	top5: 97.50	
[09/26 10:41:31 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[09/26 10:41:37 visual_prompt]: Epoch 25 / 100: avg data time: 5.76e-02, avg batch time: 0.4699, average train loss: 0.7222
[09/26 10:41:39 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1577, average loss: 0.8514
[09/26 10:41:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.00	top5: 98.00	
[09/26 10:41:39 visual_prompt]: Best epoch 25: best metric: 0.700
[09/26 10:41:39 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[09/26 10:41:45 visual_prompt]: Epoch 26 / 100: avg data time: 5.86e-02, avg batch time: 0.4722, average train loss: 0.6388
[09/26 10:41:47 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1582, average loss: 0.8014
[09/26 10:41:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.50	top5: 96.00	
[09/26 10:41:47 visual_prompt]: Best epoch 26: best metric: 0.755
[09/26 10:41:47 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[09/26 10:41:53 visual_prompt]: Epoch 27 / 100: avg data time: 6.58e-02, avg batch time: 0.4797, average train loss: 0.5025
[09/26 10:41:55 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1577, average loss: 0.9626
[09/26 10:41:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.50	top5: 96.50	
[09/26 10:41:55 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[09/26 10:42:02 visual_prompt]: Epoch 28 / 100: avg data time: 6.03e-02, avg batch time: 0.4734, average train loss: 0.4626
[09/26 10:42:03 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1583, average loss: 0.9130
[09/26 10:42:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 97.00	
[09/26 10:42:03 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[09/26 10:42:10 visual_prompt]: Epoch 29 / 100: avg data time: 6.18e-02, avg batch time: 0.4751, average train loss: 0.5677
[09/26 10:42:11 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1577, average loss: 0.8529
[09/26 10:42:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.50	top5: 98.00	
[09/26 10:42:11 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[09/26 10:42:17 visual_prompt]: Epoch 30 / 100: avg data time: 5.13e-02, avg batch time: 0.4638, average train loss: 0.3958
[09/26 10:42:19 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1582, average loss: 0.7493
[09/26 10:42:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.00	
[09/26 10:42:19 visual_prompt]: Best epoch 30: best metric: 0.830
[09/26 10:42:19 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[09/26 10:42:26 visual_prompt]: Epoch 31 / 100: avg data time: 6.15e-02, avg batch time: 0.4764, average train loss: 0.2245
[09/26 10:42:27 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1583, average loss: 1.5168
[09/26 10:42:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.00	top5: 96.50	
[09/26 10:42:27 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[09/26 10:42:34 visual_prompt]: Epoch 32 / 100: avg data time: 6.42e-02, avg batch time: 0.4776, average train loss: 0.3558
[09/26 10:42:35 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1579, average loss: 0.8425
[09/26 10:42:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 97.50	
[09/26 10:42:35 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[09/26 10:42:42 visual_prompt]: Epoch 33 / 100: avg data time: 5.49e-02, avg batch time: 0.4700, average train loss: 0.2492
[09/26 10:42:43 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1579, average loss: 0.6949
[09/26 10:42:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 10:42:43 visual_prompt]: Best epoch 33: best metric: 0.835
[09/26 10:42:43 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[09/26 10:42:50 visual_prompt]: Epoch 34 / 100: avg data time: 6.08e-02, avg batch time: 0.4739, average train loss: 0.2440
[09/26 10:42:51 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1579, average loss: 0.8627
[09/26 10:42:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.50	
[09/26 10:42:51 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[09/26 10:42:58 visual_prompt]: Epoch 35 / 100: avg data time: 5.66e-02, avg batch time: 0.4705, average train loss: 0.2234
[09/26 10:42:59 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1584, average loss: 0.7829
[09/26 10:42:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 95.50	
[09/26 10:42:59 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[09/26 10:43:06 visual_prompt]: Epoch 36 / 100: avg data time: 6.62e-02, avg batch time: 0.4789, average train loss: 0.2055
[09/26 10:43:08 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1582, average loss: 0.8877
[09/26 10:43:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 97.00	
[09/26 10:43:08 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[09/26 10:43:14 visual_prompt]: Epoch 37 / 100: avg data time: 4.84e-02, avg batch time: 0.4622, average train loss: 0.2624
[09/26 10:43:15 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1582, average loss: 0.6247
[09/26 10:43:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 98.50	
[09/26 10:43:15 visual_prompt]: Best epoch 37: best metric: 0.840
[09/26 10:43:15 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[09/26 10:43:22 visual_prompt]: Epoch 38 / 100: avg data time: 5.43e-02, avg batch time: 0.4679, average train loss: 0.2035
[09/26 10:43:23 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1584, average loss: 0.8567
[09/26 10:43:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 96.50	
[09/26 10:43:23 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[09/26 10:43:30 visual_prompt]: Epoch 39 / 100: avg data time: 5.39e-02, avg batch time: 0.4680, average train loss: 0.1146
[09/26 10:43:31 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1584, average loss: 0.7659
[09/26 10:43:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 97.00	
[09/26 10:43:31 visual_prompt]: Best epoch 39: best metric: 0.870
[09/26 10:43:31 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[09/26 10:43:38 visual_prompt]: Epoch 40 / 100: avg data time: 6.39e-02, avg batch time: 0.4776, average train loss: 0.1173
[09/26 10:43:40 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1576, average loss: 0.9268
[09/26 10:43:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 10:43:40 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[09/26 10:43:46 visual_prompt]: Epoch 41 / 100: avg data time: 6.11e-02, avg batch time: 0.4738, average train loss: 0.1785
[09/26 10:43:48 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1583, average loss: 0.8737
[09/26 10:43:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 96.00	
[09/26 10:43:48 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[09/26 10:43:54 visual_prompt]: Epoch 42 / 100: avg data time: 5.38e-02, avg batch time: 0.4665, average train loss: 0.1793
[09/26 10:43:56 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1581, average loss: 1.0060
[09/26 10:43:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 97.00	
[09/26 10:43:56 visual_prompt]: Training 43 / 100 epoch, with learning rate 1.7979639334863466
[09/26 10:44:02 visual_prompt]: Epoch 43 / 100: avg data time: 6.32e-02, avg batch time: 0.4756, average train loss: 0.0887
[09/26 10:44:04 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1584, average loss: 0.9715
[09/26 10:44:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 10:44:04 visual_prompt]: Training 44 / 100 epoch, with learning rate 1.7584208038447504
[09/26 10:44:10 visual_prompt]: Epoch 44 / 100: avg data time: 6.06e-02, avg batch time: 0.4736, average train loss: 0.0661
[09/26 10:44:12 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1585, average loss: 0.9149
[09/26 10:44:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 98.50	
[09/26 10:44:12 visual_prompt]: Training 45 / 100 epoch, with learning rate 1.7182582417698902
[09/26 10:44:18 visual_prompt]: Epoch 45 / 100: avg data time: 5.94e-02, avg batch time: 0.4730, average train loss: 0.1040
[09/26 10:44:20 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1582, average loss: 1.4917
[09/26 10:44:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 96.00	
[09/26 10:44:20 visual_prompt]: Training 46 / 100 epoch, with learning rate 1.6775251791570862
[09/26 10:44:26 visual_prompt]: Epoch 46 / 100: avg data time: 6.54e-02, avg batch time: 0.4779, average train loss: 0.1901
[09/26 10:44:28 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1586, average loss: 0.7145
[09/26 10:44:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 95.50	
[09/26 10:44:28 visual_prompt]: Training 47 / 100 epoch, with learning rate 1.6362712429686843
[09/26 10:44:35 visual_prompt]: Epoch 47 / 100: avg data time: 6.18e-02, avg batch time: 0.4753, average train loss: 0.2026
[09/26 10:44:36 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1585, average loss: 0.7446
[09/26 10:44:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 98.50	
[09/26 10:44:36 visual_prompt]: Training 48 / 100 epoch, with learning rate 1.5945466947712488
[09/26 10:44:43 visual_prompt]: Epoch 48 / 100: avg data time: 6.03e-02, avg batch time: 0.4728, average train loss: 0.1534
[09/26 10:44:44 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1584, average loss: 0.9592
[09/26 10:44:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 97.00	
[09/26 10:44:44 visual_prompt]: Training 49 / 100 epoch, with learning rate 1.5524023694995845
[09/26 10:44:51 visual_prompt]: Epoch 49 / 100: avg data time: 6.15e-02, avg batch time: 0.4742, average train loss: 0.0724
[09/26 10:44:52 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1580, average loss: 0.8673
[09/26 10:44:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.00	
[09/26 10:44:52 visual_prompt]: Training 50 / 100 epoch, with learning rate 1.509889613522199
[09/26 10:44:59 visual_prompt]: Epoch 50 / 100: avg data time: 6.40e-02, avg batch time: 0.4763, average train loss: 0.0563
[09/26 10:45:00 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1579, average loss: 0.8419
[09/26 10:45:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 98.50	
[09/26 10:45:00 visual_prompt]: Training 51 / 100 epoch, with learning rate 1.467060222083663
[09/26 10:45:07 visual_prompt]: Epoch 51 / 100: avg data time: 6.30e-02, avg batch time: 0.4768, average train loss: 0.0193
[09/26 10:45:08 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1580, average loss: 0.7871
[09/26 10:45:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 98.00	
[09/26 10:45:08 visual_prompt]: Best epoch 51: best metric: 0.880
[09/26 10:45:08 visual_prompt]: Training 52 / 100 epoch, with learning rate 1.4239663762000818
[09/26 10:45:15 visual_prompt]: Epoch 52 / 100: avg data time: 6.41e-02, avg batch time: 0.4762, average train loss: 0.0072
[09/26 10:45:17 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1578, average loss: 0.9374
[09/26 10:45:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.50	top5: 98.50	
[09/26 10:45:17 visual_prompt]: Training 53 / 100 epoch, with learning rate 1.380660579084567
[09/26 10:45:23 visual_prompt]: Epoch 53 / 100: avg data time: 6.40e-02, avg batch time: 0.4771, average train loss: 0.0190
[09/26 10:45:25 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1585, average loss: 1.0673
[09/26 10:45:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 99.00	
[09/26 10:45:25 visual_prompt]: Training 54 / 100 epoch, with learning rate 1.3371955921801564
[09/26 10:45:31 visual_prompt]: Epoch 54 / 100: avg data time: 6.15e-02, avg batch time: 0.4747, average train loss: 0.0612
[09/26 10:45:33 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1583, average loss: 1.1675
[09/26 10:45:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.50	
[09/26 10:45:33 visual_prompt]: Training 55 / 100 epoch, with learning rate 1.2936243708781263
[09/26 10:45:39 visual_prompt]: Epoch 55 / 100: avg data time: 6.55e-02, avg batch time: 0.4780, average train loss: 0.1387
[09/26 10:45:41 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1581, average loss: 0.9538
[09/26 10:45:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.00	
[09/26 10:45:41 visual_prompt]: Training 56 / 100 epoch, with learning rate 1.25
[09/26 10:45:48 visual_prompt]: Epoch 56 / 100: avg data time: 6.20e-02, avg batch time: 0.4749, average train loss: 0.1299
[09/26 10:45:49 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1585, average loss: 0.6748
[09/26 10:45:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.00	
[09/26 10:45:49 visual_prompt]: Best epoch 56: best metric: 0.885
[09/26 10:45:49 visual_prompt]: Training 57 / 100 epoch, with learning rate 1.2063756291218741
[09/26 10:45:56 visual_prompt]: Epoch 57 / 100: avg data time: 5.49e-02, avg batch time: 0.4693, average train loss: 0.0372
[09/26 10:45:57 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1581, average loss: 0.7906
[09/26 10:45:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 97.00	
[09/26 10:45:57 visual_prompt]: Training 58 / 100 epoch, with learning rate 1.1628044078198434
[09/26 10:46:04 visual_prompt]: Epoch 58 / 100: avg data time: 5.47e-02, avg batch time: 0.4681, average train loss: 0.0225
[09/26 10:46:05 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1579, average loss: 1.3325
[09/26 10:46:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 98.00	
[09/26 10:46:05 visual_prompt]: Training 59 / 100 epoch, with learning rate 1.1193394209154333
[09/26 10:46:12 visual_prompt]: Epoch 59 / 100: avg data time: 5.49e-02, avg batch time: 0.4690, average train loss: 0.0414
[09/26 10:46:13 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1583, average loss: 0.9535
[09/26 10:46:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 99.50	
[09/26 10:46:13 visual_prompt]: Training 60 / 100 epoch, with learning rate 1.0760336237999186
[09/26 10:46:20 visual_prompt]: Epoch 60 / 100: avg data time: 6.35e-02, avg batch time: 0.4768, average train loss: 0.0260
[09/26 10:46:21 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1577, average loss: 0.8963
[09/26 10:46:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 96.50	
[09/26 10:46:21 visual_prompt]: Training 61 / 100 epoch, with learning rate 1.0329397779163372
[09/26 10:46:28 visual_prompt]: Epoch 61 / 100: avg data time: 5.02e-02, avg batch time: 0.4643, average train loss: 0.0131
[09/26 10:46:29 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1583, average loss: 0.8276
[09/26 10:46:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 98.00	
[09/26 10:46:29 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.990110386477801
[09/26 10:46:36 visual_prompt]: Epoch 62 / 100: avg data time: 5.91e-02, avg batch time: 0.4719, average train loss: 0.0274
[09/26 10:46:37 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1584, average loss: 0.9508
[09/26 10:46:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 97.50	
[09/26 10:46:37 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.9475976305004155
[09/26 10:46:44 visual_prompt]: Epoch 63 / 100: avg data time: 5.74e-02, avg batch time: 0.4703, average train loss: 0.0092
[09/26 10:46:45 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.1582, average loss: 0.7881
[09/26 10:46:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:46:45 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.9054533052287511
[09/26 10:46:52 visual_prompt]: Epoch 64 / 100: avg data time: 5.92e-02, avg batch time: 0.4724, average train loss: 0.0089
[09/26 10:46:53 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1585, average loss: 0.8563
[09/26 10:46:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 89.00	top5: 98.00	
[09/26 10:46:53 visual_prompt]: Best epoch 64: best metric: 0.890
[09/26 10:46:53 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.8637287570313158
[09/26 10:47:00 visual_prompt]: Epoch 65 / 100: avg data time: 6.44e-02, avg batch time: 0.4780, average train loss: 0.0214
[09/26 10:47:01 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.1575, average loss: 0.9314
[09/26 10:47:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 98.00	
[09/26 10:47:01 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.8224748208429142
[09/26 10:47:08 visual_prompt]: Epoch 66 / 100: avg data time: 6.18e-02, avg batch time: 0.4743, average train loss: 0.0209
[09/26 10:47:09 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1586, average loss: 0.8446
[09/26 10:47:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 10:47:09 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.7817417582301099
[09/26 10:47:16 visual_prompt]: Epoch 67 / 100: avg data time: 6.57e-02, avg batch time: 0.4791, average train loss: 0.0063
[09/26 10:47:18 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1580, average loss: 0.7487
[09/26 10:47:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 98.00	
[09/26 10:47:18 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.7415791961552499
[09/26 10:47:24 visual_prompt]: Epoch 68 / 100: avg data time: 6.07e-02, avg batch time: 0.4733, average train loss: 0.0045
[09/26 10:47:26 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1584, average loss: 0.8340
[09/26 10:47:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 97.50	
[09/26 10:47:26 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.7020360665136531
[09/26 10:47:32 visual_prompt]: Epoch 69 / 100: avg data time: 5.79e-02, avg batch time: 0.4711, average train loss: 0.0008
[09/26 10:47:34 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1584, average loss: 0.8210
[09/26 10:47:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 89.00	top5: 97.50	
[09/26 10:47:34 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.6631605465176369
[09/26 10:47:40 visual_prompt]: Epoch 70 / 100: avg data time: 6.26e-02, avg batch time: 0.4748, average train loss: 0.0009
[09/26 10:47:42 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1583, average loss: 0.8100
[09/26 10:47:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:47:42 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.6250000000000002
[09/26 10:47:48 visual_prompt]: Epoch 71 / 100: avg data time: 6.72e-02, avg batch time: 0.4792, average train loss: 0.0018
[09/26 10:47:50 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1584, average loss: 0.8160
[09/26 10:47:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:47:50 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.587600919708494
[09/26 10:47:56 visual_prompt]: Epoch 72 / 100: avg data time: 5.79e-02, avg batch time: 0.4714, average train loss: 0.0006
[09/26 10:47:58 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.1584, average loss: 0.8190
[09/26 10:47:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 97.50	
[09/26 10:47:58 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.5510088706615667
[09/26 10:48:04 visual_prompt]: Epoch 73 / 100: avg data time: 5.99e-02, avg batch time: 0.4726, average train loss: 0.0008
[09/26 10:48:06 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1585, average loss: 0.8090
[09/26 10:48:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 97.50	
[09/26 10:48:06 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.5152684346344087
[09/26 10:48:13 visual_prompt]: Epoch 74 / 100: avg data time: 6.41e-02, avg batch time: 0.4772, average train loss: 0.0007
[09/26 10:48:14 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1581, average loss: 0.8070
[09/26 10:48:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 97.50	
[09/26 10:48:14 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.48042315584292716
[09/26 10:48:21 visual_prompt]: Epoch 75 / 100: avg data time: 6.55e-02, avg batch time: 0.4774, average train loss: 0.0005
[09/26 10:48:22 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1576, average loss: 0.8061
[09/26 10:48:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 97.50	
[09/26 10:48:22 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.4465154878918258
[09/26 10:48:29 visual_prompt]: Epoch 76 / 100: avg data time: 6.40e-02, avg batch time: 0.4776, average train loss: 0.0007
[09/26 10:48:30 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1582, average loss: 0.7968
[09/26 10:48:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 97.50	
[09/26 10:48:30 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.41358674205142765
[09/26 10:48:37 visual_prompt]: Epoch 77 / 100: avg data time: 4.92e-02, avg batch time: 0.4628, average train loss: 0.0004
[09/26 10:48:38 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1581, average loss: 0.7866
[09/26 10:48:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 97.50	
[09/26 10:48:38 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.3816770369262533
[09/26 10:48:45 visual_prompt]: Epoch 78 / 100: avg data time: 6.01e-02, avg batch time: 0.4721, average train loss: 0.0005
[09/26 10:48:46 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1586, average loss: 0.7844
[09/26 10:48:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 97.50	
[09/26 10:48:46 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.35082524957668626
[09/26 10:48:53 visual_prompt]: Epoch 79 / 100: avg data time: 4.95e-02, avg batch time: 0.4638, average train loss: 0.0003
[09/26 10:48:54 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1586, average loss: 0.7831
[09/26 10:48:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 97.50	
[09/26 10:48:54 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.32106896815325703
[09/26 10:49:01 visual_prompt]: Epoch 80 / 100: avg data time: 6.02e-02, avg batch time: 0.4723, average train loss: 0.0004
[09/26 10:49:02 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1580, average loss: 0.7803
[09/26 10:49:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:49:02 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.2924444461012776
[09/26 10:49:09 visual_prompt]: Epoch 81 / 100: avg data time: 6.29e-02, avg batch time: 0.4747, average train loss: 0.0003
[09/26 10:49:11 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1580, average loss: 0.7790
[09/26 10:49:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:49:11 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.26498655799159765
[09/26 10:49:17 visual_prompt]: Epoch 82 / 100: avg data time: 6.02e-02, avg batch time: 0.4732, average train loss: 0.0003
[09/26 10:49:19 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1583, average loss: 0.7751
[09/26 10:49:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:49:19 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.23872875703131582
[09/26 10:49:25 visual_prompt]: Epoch 83 / 100: avg data time: 5.71e-02, avg batch time: 0.4706, average train loss: 0.0004
[09/26 10:49:27 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1578, average loss: 0.7711
[09/26 10:49:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:49:27 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.21370303430619797
[09/26 10:49:33 visual_prompt]: Epoch 84 / 100: avg data time: 6.28e-02, avg batch time: 0.4766, average train loss: 0.0004
[09/26 10:49:35 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1581, average loss: 0.7672
[09/26 10:49:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:49:35 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.18993987980446755
[09/26 10:49:41 visual_prompt]: Epoch 85 / 100: avg data time: 5.57e-02, avg batch time: 0.4679, average train loss: 0.0003
[09/26 10:49:43 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1583, average loss: 0.7635
[09/26 10:49:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:49:43 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.16746824526945162
[09/26 10:49:49 visual_prompt]: Epoch 86 / 100: avg data time: 6.37e-02, avg batch time: 0.4766, average train loss: 0.0003
[09/26 10:49:51 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1580, average loss: 0.7608
[09/26 10:49:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:49:51 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.14631550892634126
[09/26 10:49:58 visual_prompt]: Epoch 87 / 100: avg data time: 6.85e-02, avg batch time: 0.4811, average train loss: 0.0003
[09/26 10:49:59 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1583, average loss: 0.7588
[09/26 10:49:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:49:59 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.12650744212604148
[09/26 10:50:05 visual_prompt]: Epoch 88 / 100: avg data time: 4.89e-02, avg batch time: 0.4633, average train loss: 0.0003
[09/26 10:50:07 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1587, average loss: 0.7565
[09/26 10:50:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:50:07 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.10806817794674878
[09/26 10:50:13 visual_prompt]: Epoch 89 / 100: avg data time: 5.62e-02, avg batch time: 0.4702, average train loss: 0.0003
[09/26 10:50:15 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1582, average loss: 0.7554
[09/26 10:50:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:50:15 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.09102018179151586
[09/26 10:50:22 visual_prompt]: Epoch 90 / 100: avg data time: 6.00e-02, avg batch time: 0.4731, average train loss: 0.0004
[09/26 10:50:23 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1578, average loss: 0.7536
[09/26 10:50:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:50:23 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0753842240176146
[09/26 10:50:29 visual_prompt]: Epoch 91 / 100: avg data time: 5.40e-02, avg batch time: 0.4683, average train loss: 0.0003
[09/26 10:50:31 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.1584, average loss: 0.7529
[09/26 10:50:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:50:31 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.061179354631058086
[09/26 10:50:38 visual_prompt]: Epoch 92 / 100: avg data time: 7.12e-02, avg batch time: 0.4836, average train loss: 0.0003
[09/26 10:50:39 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1584, average loss: 0.7526
[09/26 10:50:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:50:39 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.04842288007710138
[09/26 10:50:46 visual_prompt]: Epoch 93 / 100: avg data time: 6.79e-02, avg batch time: 0.4805, average train loss: 0.0004
[09/26 10:50:47 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1578, average loss: 0.7529
[09/26 10:50:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:50:47 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.03713034215500441
[09/26 10:50:54 visual_prompt]: Epoch 94 / 100: avg data time: 6.63e-02, avg batch time: 0.4788, average train loss: 0.0003
[09/26 10:50:55 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1581, average loss: 0.7533
[09/26 10:50:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:50:55 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.02731549908274289
[09/26 10:51:02 visual_prompt]: Epoch 95 / 100: avg data time: 6.61e-02, avg batch time: 0.4795, average train loss: 0.0003
[09/26 10:51:03 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1583, average loss: 0.7532
[09/26 10:51:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:51:03 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.018990308734739975
[09/26 10:51:10 visual_prompt]: Epoch 96 / 100: avg data time: 6.37e-02, avg batch time: 0.4778, average train loss: 0.0003
[09/26 10:51:12 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1576, average loss: 0.7529
[09/26 10:51:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:51:12 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.012164914073037048
[09/26 10:51:18 visual_prompt]: Epoch 97 / 100: avg data time: 6.48e-02, avg batch time: 0.4771, average train loss: 0.0003
[09/26 10:51:20 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1586, average loss: 0.7527
[09/26 10:51:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:51:20 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.006847630789658388
[09/26 10:51:26 visual_prompt]: Epoch 98 / 100: avg data time: 5.77e-02, avg batch time: 0.4723, average train loss: 0.0002
[09/26 10:51:28 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1584, average loss: 0.7525
[09/26 10:51:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:51:28 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.003044937175219753
[09/26 10:51:34 visual_prompt]: Epoch 99 / 100: avg data time: 6.47e-02, avg batch time: 0.4779, average train loss: 0.0004
[09/26 10:51:36 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1584, average loss: 0.7525
[09/26 10:51:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:51:36 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0007614662261302974
[09/26 10:51:42 visual_prompt]: Epoch 100 / 100: avg data time: 6.37e-02, avg batch time: 0.4768, average train loss: 0.0004
[09/26 10:51:44 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1584, average loss: 0.7525
[09/26 10:51:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 97.50	
[09/26 10:51:44 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 10:51:44 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 10:51:44 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 10:51:44 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 10:51:44 visual_prompt]: Training with config:
[09/26 10:51:44 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr2.5_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 10:51:44 visual_prompt]: Loading training data...
[09/26 10:51:44 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 10:51:45 visual_prompt]: Number of images: 800
[09/26 10:51:45 visual_prompt]: Number of classes: 10 / 10
[09/26 10:51:45 visual_prompt]: Loading validation data...
[09/26 10:51:45 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 10:51:45 visual_prompt]: Number of images: 200
[09/26 10:51:45 visual_prompt]: Number of classes: 10 / 10
[09/26 10:51:45 visual_prompt]: Constructing models...
[09/26 10:51:48 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 10:51:48 visual_prompt]: tuned percent:0.543
[09/26 10:51:48 visual_prompt]: Device used for model: 0
[09/26 10:51:48 visual_prompt]: Setting up Evaluator...
[09/26 10:51:48 visual_prompt]: Setting up Trainer...
[09/26 10:51:48 visual_prompt]: 	Setting up the optimizer...
[09/26 10:51:48 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 10:51:54 visual_prompt]: Epoch 1 / 100: avg data time: 6.56e-02, avg batch time: 0.4842, average train loss: 2.6649
[09/26 10:51:56 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1578, average loss: 2.6214
[09/26 10:51:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 10:51:56 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 10:51:56 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[09/26 10:52:02 visual_prompt]: Epoch 2 / 100: avg data time: 5.02e-02, avg batch time: 0.4645, average train loss: 3.3129
[09/26 10:52:04 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1578, average loss: 2.3005
[09/26 10:52:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.00	
[09/26 10:52:04 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[09/26 10:52:10 visual_prompt]: Epoch 3 / 100: avg data time: 6.61e-02, avg batch time: 0.4784, average train loss: 2.4539
[09/26 10:52:12 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1582, average loss: 2.3494
[09/26 10:52:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.00	
[09/26 10:52:12 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[09/26 10:52:18 visual_prompt]: Epoch 4 / 100: avg data time: 5.29e-02, avg batch time: 0.4657, average train loss: 2.4198
[09/26 10:52:20 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1582, average loss: 2.3832
[09/26 10:52:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 10:52:20 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[09/26 10:52:26 visual_prompt]: Epoch 5 / 100: avg data time: 4.71e-02, avg batch time: 0.4617, average train loss: 2.4209
[09/26 10:52:28 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1585, average loss: 2.2687
[09/26 10:52:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 10:52:28 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[09/26 10:52:35 visual_prompt]: Epoch 6 / 100: avg data time: 7.07e-02, avg batch time: 0.4824, average train loss: 2.3899
[09/26 10:52:36 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1586, average loss: 2.4252
[09/26 10:52:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 10:52:36 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[09/26 10:52:43 visual_prompt]: Epoch 7 / 100: avg data time: 5.93e-02, avg batch time: 0.4716, average train loss: 2.4167
[09/26 10:52:44 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1583, average loss: 2.4112
[09/26 10:52:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 10:52:44 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[09/26 10:52:51 visual_prompt]: Epoch 8 / 100: avg data time: 5.73e-02, avg batch time: 0.4698, average train loss: 2.4424
[09/26 10:52:52 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1584, average loss: 2.5073
[09/26 10:52:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 10:52:52 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[09/26 10:52:58 visual_prompt]: Epoch 9 / 100: avg data time: 4.61e-02, avg batch time: 0.4615, average train loss: 2.4581
[09/26 10:53:00 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1582, average loss: 2.4609
[09/26 10:53:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.00	
[09/26 10:53:00 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[09/26 10:53:06 visual_prompt]: Epoch 10 / 100: avg data time: 5.82e-02, avg batch time: 0.4715, average train loss: 2.6335
[09/26 10:53:08 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1581, average loss: 2.3955
[09/26 10:53:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.00	
[09/26 10:53:08 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[09/26 10:53:14 visual_prompt]: Epoch 11 / 100: avg data time: 5.80e-02, avg batch time: 0.4713, average train loss: 2.5188
[09/26 10:53:16 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1581, average loss: 2.7041
[09/26 10:53:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 61.00	
[09/26 10:53:16 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[09/26 10:53:22 visual_prompt]: Epoch 12 / 100: avg data time: 5.77e-02, avg batch time: 0.4704, average train loss: 2.6843
[09/26 10:53:24 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1584, average loss: 2.4206
[09/26 10:53:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 76.00	
[09/26 10:53:24 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[09/26 10:53:30 visual_prompt]: Epoch 13 / 100: avg data time: 5.66e-02, avg batch time: 0.4710, average train loss: 2.1956
[09/26 10:53:32 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1582, average loss: 2.7885
[09/26 10:53:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 61.50	
[09/26 10:53:32 visual_prompt]: Best epoch 13: best metric: 0.235
[09/26 10:53:32 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[09/26 10:53:38 visual_prompt]: Epoch 14 / 100: avg data time: 5.57e-02, avg batch time: 0.4694, average train loss: 2.8945
[09/26 10:53:40 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1582, average loss: 2.3102
[09/26 10:53:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 26.00	top5: 74.00	
[09/26 10:53:40 visual_prompt]: Best epoch 14: best metric: 0.260
[09/26 10:53:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[09/26 10:53:46 visual_prompt]: Epoch 15 / 100: avg data time: 5.56e-02, avg batch time: 0.4692, average train loss: 2.0808
[09/26 10:53:48 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1585, average loss: 1.9009
[09/26 10:53:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 42.00	top5: 86.50	
[09/26 10:53:48 visual_prompt]: Best epoch 15: best metric: 0.420
[09/26 10:53:48 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[09/26 10:53:54 visual_prompt]: Epoch 16 / 100: avg data time: 5.24e-02, avg batch time: 0.4669, average train loss: 2.0437
[09/26 10:53:56 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1582, average loss: 1.5853
[09/26 10:53:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 41.50	top5: 82.50	
[09/26 10:53:56 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[09/26 10:54:02 visual_prompt]: Epoch 17 / 100: avg data time: 4.87e-02, avg batch time: 0.4636, average train loss: 1.5910
[09/26 10:54:04 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1584, average loss: 1.4227
[09/26 10:54:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 52.00	top5: 87.00	
[09/26 10:54:04 visual_prompt]: Best epoch 17: best metric: 0.520
[09/26 10:54:04 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[09/26 10:54:10 visual_prompt]: Epoch 18 / 100: avg data time: 6.09e-02, avg batch time: 0.4743, average train loss: 1.2294
[09/26 10:54:12 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1583, average loss: 1.4501
[09/26 10:54:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 53.50	top5: 93.50	
[09/26 10:54:12 visual_prompt]: Best epoch 18: best metric: 0.535
[09/26 10:54:12 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[09/26 10:54:18 visual_prompt]: Epoch 19 / 100: avg data time: 4.95e-02, avg batch time: 0.4636, average train loss: 1.0763
[09/26 10:54:20 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1586, average loss: 1.3514
[09/26 10:54:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 56.00	top5: 93.00	
[09/26 10:54:20 visual_prompt]: Best epoch 19: best metric: 0.560
[09/26 10:54:20 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[09/26 10:54:26 visual_prompt]: Epoch 20 / 100: avg data time: 5.98e-02, avg batch time: 0.4749, average train loss: 0.9395
[09/26 10:54:28 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1586, average loss: 1.1327
[09/26 10:54:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 55.50	top5: 97.50	
[09/26 10:54:28 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[09/26 10:54:34 visual_prompt]: Epoch 21 / 100: avg data time: 6.04e-02, avg batch time: 0.4751, average train loss: 1.6327
[09/26 10:54:36 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.1584, average loss: 2.6040
[09/26 10:54:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 37.00	top5: 70.00	
[09/26 10:54:36 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[09/26 10:54:42 visual_prompt]: Epoch 22 / 100: avg data time: 4.80e-02, avg batch time: 0.4623, average train loss: 1.8889
[09/26 10:54:44 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1583, average loss: 1.2531
[09/26 10:54:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 53.50	top5: 92.00	
[09/26 10:54:44 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[09/26 10:54:50 visual_prompt]: Epoch 23 / 100: avg data time: 6.62e-02, avg batch time: 0.4794, average train loss: 1.3044
[09/26 10:54:52 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1584, average loss: 1.4245
[09/26 10:54:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 56.00	top5: 89.00	
[09/26 10:54:52 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[09/26 10:54:58 visual_prompt]: Epoch 24 / 100: avg data time: 5.76e-02, avg batch time: 0.4708, average train loss: 1.0771
[09/26 10:55:00 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1582, average loss: 1.3752
[09/26 10:55:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 57.00	top5: 94.50	
[09/26 10:55:00 visual_prompt]: Best epoch 24: best metric: 0.570
[09/26 10:55:00 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[09/26 10:55:06 visual_prompt]: Epoch 25 / 100: avg data time: 5.28e-02, avg batch time: 0.4677, average train loss: 0.8167
[09/26 10:55:08 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1585, average loss: 1.0410
[09/26 10:55:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 62.50	top5: 97.00	
[09/26 10:55:08 visual_prompt]: Best epoch 25: best metric: 0.625
[09/26 10:55:08 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[09/26 10:55:14 visual_prompt]: Epoch 26 / 100: avg data time: 4.78e-02, avg batch time: 0.4637, average train loss: 0.6940
[09/26 10:55:16 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1581, average loss: 0.8262
[09/26 10:55:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 96.50	
[09/26 10:55:16 visual_prompt]: Best epoch 26: best metric: 0.730
[09/26 10:55:16 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[09/26 10:55:22 visual_prompt]: Epoch 27 / 100: avg data time: 5.97e-02, avg batch time: 0.4725, average train loss: 0.6226
[09/26 10:55:24 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1585, average loss: 0.8447
[09/26 10:55:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 97.50	
[09/26 10:55:24 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[09/26 10:55:30 visual_prompt]: Epoch 28 / 100: avg data time: 5.97e-02, avg batch time: 0.4728, average train loss: 0.4945
[09/26 10:55:32 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1585, average loss: 0.8696
[09/26 10:55:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 97.50	
[09/26 10:55:32 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[09/26 10:55:38 visual_prompt]: Epoch 29 / 100: avg data time: 5.12e-02, avg batch time: 0.4665, average train loss: 0.4151
[09/26 10:55:40 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1585, average loss: 0.8357
[09/26 10:55:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 98.00	
[09/26 10:55:40 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[09/26 10:55:46 visual_prompt]: Epoch 30 / 100: avg data time: 6.04e-02, avg batch time: 0.4744, average train loss: 0.3982
[09/26 10:55:48 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1581, average loss: 0.8418
[09/26 10:55:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 99.50	
[09/26 10:55:48 visual_prompt]: Best epoch 30: best metric: 0.735
[09/26 10:55:48 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[09/26 10:55:54 visual_prompt]: Epoch 31 / 100: avg data time: 5.33e-02, avg batch time: 0.4697, average train loss: 0.3636
[09/26 10:55:56 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1585, average loss: 0.6979
[09/26 10:55:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 97.50	
[09/26 10:55:56 visual_prompt]: Best epoch 31: best metric: 0.790
[09/26 10:55:56 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[09/26 10:56:02 visual_prompt]: Epoch 32 / 100: avg data time: 5.98e-02, avg batch time: 0.4730, average train loss: 0.2743
[09/26 10:56:04 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1583, average loss: 0.8015
[09/26 10:56:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 98.00	
[09/26 10:56:04 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[09/26 10:56:10 visual_prompt]: Epoch 33 / 100: avg data time: 5.56e-02, avg batch time: 0.4696, average train loss: 0.2769
[09/26 10:56:12 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1585, average loss: 0.6911
[09/26 10:56:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 99.00	
[09/26 10:56:12 visual_prompt]: Best epoch 33: best metric: 0.795
[09/26 10:56:12 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[09/26 10:56:18 visual_prompt]: Epoch 34 / 100: avg data time: 4.64e-02, avg batch time: 0.4612, average train loss: 0.2117
[09/26 10:56:20 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1585, average loss: 0.8481
[09/26 10:56:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 97.50	
[09/26 10:56:20 visual_prompt]: Best epoch 34: best metric: 0.820
[09/26 10:56:20 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[09/26 10:56:26 visual_prompt]: Epoch 35 / 100: avg data time: 6.54e-02, avg batch time: 0.4797, average train loss: 0.1462
[09/26 10:56:28 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1587, average loss: 0.8908
[09/26 10:56:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 97.50	
[09/26 10:56:28 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[09/26 10:56:34 visual_prompt]: Epoch 36 / 100: avg data time: 5.75e-02, avg batch time: 0.4722, average train loss: 0.2285
[09/26 10:56:36 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1583, average loss: 0.7359
[09/26 10:56:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 98.00	
[09/26 10:56:36 visual_prompt]: Best epoch 36: best metric: 0.830
[09/26 10:56:36 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[09/26 10:56:42 visual_prompt]: Epoch 37 / 100: avg data time: 5.86e-02, avg batch time: 0.4719, average train loss: 0.0947
[09/26 10:56:44 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1582, average loss: 0.8167
[09/26 10:56:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.00	
[09/26 10:56:44 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[09/26 10:56:50 visual_prompt]: Epoch 38 / 100: avg data time: 5.73e-02, avg batch time: 0.4705, average train loss: 0.0957
[09/26 10:56:52 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1584, average loss: 0.9800
[09/26 10:56:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 97.50	
[09/26 10:56:52 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[09/26 10:56:58 visual_prompt]: Epoch 39 / 100: avg data time: 5.95e-02, avg batch time: 0.4736, average train loss: 0.1019
[09/26 10:57:00 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1585, average loss: 0.8809
[09/26 10:57:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.50	
[09/26 10:57:00 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[09/26 10:57:06 visual_prompt]: Epoch 40 / 100: avg data time: 6.26e-02, avg batch time: 0.4769, average train loss: 0.0619
[09/26 10:57:08 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1586, average loss: 0.8585
[09/26 10:57:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 10:57:08 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[09/26 10:57:14 visual_prompt]: Epoch 41 / 100: avg data time: 5.11e-02, avg batch time: 0.4646, average train loss: 0.0597
[09/26 10:57:16 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1582, average loss: 0.8621
[09/26 10:57:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 98.00	
[09/26 10:57:16 visual_prompt]: Best epoch 41: best metric: 0.845
[09/26 10:57:16 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[09/26 10:57:22 visual_prompt]: Epoch 42 / 100: avg data time: 6.37e-02, avg batch time: 0.4766, average train loss: 0.0461
[09/26 10:57:24 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.1582, average loss: 0.9503
[09/26 10:57:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.50	
[09/26 10:57:24 visual_prompt]: Training 43 / 100 epoch, with learning rate 1.7979639334863466
[09/26 10:57:30 visual_prompt]: Epoch 43 / 100: avg data time: 5.39e-02, avg batch time: 0.4686, average train loss: 0.0171
[09/26 10:57:32 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1582, average loss: 0.9684
[09/26 10:57:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 10:57:32 visual_prompt]: Training 44 / 100 epoch, with learning rate 1.7584208038447504
[09/26 10:57:38 visual_prompt]: Epoch 44 / 100: avg data time: 5.97e-02, avg batch time: 0.4735, average train loss: 0.0097
[09/26 10:57:40 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1583, average loss: 0.9789
[09/26 10:57:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.50	
[09/26 10:57:40 visual_prompt]: Training 45 / 100 epoch, with learning rate 1.7182582417698902
[09/26 10:57:46 visual_prompt]: Epoch 45 / 100: avg data time: 6.42e-02, avg batch time: 0.4773, average train loss: 0.0094
[09/26 10:57:48 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1585, average loss: 1.0543
[09/26 10:57:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 98.00	
[09/26 10:57:48 visual_prompt]: Training 46 / 100 epoch, with learning rate 1.6775251791570862
[09/26 10:57:54 visual_prompt]: Epoch 46 / 100: avg data time: 5.05e-02, avg batch time: 0.4649, average train loss: 0.0118
[09/26 10:57:56 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1587, average loss: 1.0174
[09/26 10:57:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.50	
[09/26 10:57:56 visual_prompt]: Training 47 / 100 epoch, with learning rate 1.6362712429686843
[09/26 10:58:03 visual_prompt]: Epoch 47 / 100: avg data time: 6.38e-02, avg batch time: 0.4778, average train loss: 0.0090
[09/26 10:58:04 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1579, average loss: 1.1054
[09/26 10:58:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 98.00	
[09/26 10:58:04 visual_prompt]: Training 48 / 100 epoch, with learning rate 1.5945466947712488
[09/26 10:58:11 visual_prompt]: Epoch 48 / 100: avg data time: 6.76e-02, avg batch time: 0.4815, average train loss: 0.0038
[09/26 10:58:12 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1583, average loss: 1.2183
[09/26 10:58:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 10:58:12 visual_prompt]: Training 49 / 100 epoch, with learning rate 1.5524023694995845
[09/26 10:58:19 visual_prompt]: Epoch 49 / 100: avg data time: 5.94e-02, avg batch time: 0.4725, average train loss: 0.0025
[09/26 10:58:20 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1586, average loss: 1.2133
[09/26 10:58:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.50	
[09/26 10:58:20 visual_prompt]: Training 50 / 100 epoch, with learning rate 1.509889613522199
[09/26 10:58:27 visual_prompt]: Epoch 50 / 100: avg data time: 5.67e-02, avg batch time: 0.4699, average train loss: 0.0022
[09/26 10:58:28 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1584, average loss: 1.1815
[09/26 10:58:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 10:58:28 visual_prompt]: Training 51 / 100 epoch, with learning rate 1.467060222083663
[09/26 10:58:35 visual_prompt]: Epoch 51 / 100: avg data time: 6.50e-02, avg batch time: 0.4793, average train loss: 0.0027
[09/26 10:58:36 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1586, average loss: 1.1357
[09/26 10:58:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 98.00	
[09/26 10:58:36 visual_prompt]: Training 52 / 100 epoch, with learning rate 1.4239663762000818
[09/26 10:58:43 visual_prompt]: Epoch 52 / 100: avg data time: 6.68e-02, avg batch time: 0.4807, average train loss: 0.0052
[09/26 10:58:44 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1578, average loss: 1.0634
[09/26 10:58:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 10:58:44 visual_prompt]: Training 53 / 100 epoch, with learning rate 1.380660579084567
[09/26 10:58:51 visual_prompt]: Epoch 53 / 100: avg data time: 6.28e-02, avg batch time: 0.4759, average train loss: 0.0023
[09/26 10:58:53 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1584, average loss: 1.0330
[09/26 10:58:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 97.50	
[09/26 10:58:53 visual_prompt]: Training 54 / 100 epoch, with learning rate 1.3371955921801564
[09/26 10:58:59 visual_prompt]: Epoch 54 / 100: avg data time: 5.76e-02, avg batch time: 0.4707, average train loss: 0.0015
[09/26 10:59:01 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1579, average loss: 1.0041
[09/26 10:59:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.50	
[09/26 10:59:01 visual_prompt]: Training 55 / 100 epoch, with learning rate 1.2936243708781263
[09/26 10:59:07 visual_prompt]: Epoch 55 / 100: avg data time: 5.84e-02, avg batch time: 0.4733, average train loss: 0.0014
[09/26 10:59:09 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1585, average loss: 1.0355
[09/26 10:59:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.50	
[09/26 10:59:09 visual_prompt]: Training 56 / 100 epoch, with learning rate 1.25
[09/26 10:59:15 visual_prompt]: Epoch 56 / 100: avg data time: 4.93e-02, avg batch time: 0.4655, average train loss: 0.0010
[09/26 10:59:17 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1585, average loss: 1.0695
[09/26 10:59:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.50	
[09/26 10:59:17 visual_prompt]: Training 57 / 100 epoch, with learning rate 1.2063756291218741
[09/26 10:59:23 visual_prompt]: Epoch 57 / 100: avg data time: 5.76e-02, avg batch time: 0.4724, average train loss: 0.0025
[09/26 10:59:25 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1582, average loss: 1.0158
[09/26 10:59:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 98.00	
[09/26 10:59:25 visual_prompt]: Training 58 / 100 epoch, with learning rate 1.1628044078198434
[09/26 10:59:31 visual_prompt]: Epoch 58 / 100: avg data time: 5.62e-02, avg batch time: 0.4695, average train loss: 0.0015
[09/26 10:59:33 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1575, average loss: 0.9749
[09/26 10:59:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 10:59:33 visual_prompt]: Training 59 / 100 epoch, with learning rate 1.1193394209154333
[09/26 10:59:39 visual_prompt]: Epoch 59 / 100: avg data time: 6.08e-02, avg batch time: 0.4747, average train loss: 0.0013
[09/26 10:59:41 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1586, average loss: 1.0155
[09/26 10:59:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 98.00	
[09/26 10:59:41 visual_prompt]: Training 60 / 100 epoch, with learning rate 1.0760336237999186
[09/26 10:59:47 visual_prompt]: Epoch 60 / 100: avg data time: 5.36e-02, avg batch time: 0.4668, average train loss: 0.0007
[09/26 10:59:49 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1581, average loss: 1.0486
[09/26 10:59:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 98.00	
[09/26 10:59:49 visual_prompt]: Training 61 / 100 epoch, with learning rate 1.0329397779163372
[09/26 10:59:55 visual_prompt]: Epoch 61 / 100: avg data time: 5.36e-02, avg batch time: 0.4679, average train loss: 0.0009
[09/26 10:59:57 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1583, average loss: 1.0652
[09/26 10:59:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 10:59:57 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.990110386477801
[09/26 11:00:03 visual_prompt]: Epoch 62 / 100: avg data time: 6.26e-02, avg batch time: 0.4758, average train loss: 0.0007
[09/26 11:00:05 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1581, average loss: 1.0698
[09/26 11:00:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 98.00	
[09/26 11:00:05 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.9475976305004155
[09/26 11:00:11 visual_prompt]: Epoch 63 / 100: avg data time: 5.95e-02, avg batch time: 0.4754, average train loss: 0.0007
[09/26 11:00:13 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1580, average loss: 1.0733
[09/26 11:00:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 98.00	
[09/26 11:00:13 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.9054533052287511
[09/26 11:00:19 visual_prompt]: Epoch 64 / 100: avg data time: 5.34e-02, avg batch time: 0.4666, average train loss: 0.0006
[09/26 11:00:21 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1583, average loss: 1.0769
[09/26 11:00:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 11:00:21 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.8637287570313158
[09/26 11:00:27 visual_prompt]: Epoch 65 / 100: avg data time: 6.09e-02, avg batch time: 0.4746, average train loss: 0.0006
[09/26 11:00:29 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1575, average loss: 1.0819
[09/26 11:00:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 11:00:29 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.8224748208429142
[09/26 11:00:35 visual_prompt]: Epoch 66 / 100: avg data time: 4.63e-02, avg batch time: 0.4603, average train loss: 0.0006
[09/26 11:00:37 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1581, average loss: 1.0850
[09/26 11:00:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 11:00:37 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.7817417582301099
[09/26 11:00:43 visual_prompt]: Epoch 67 / 100: avg data time: 5.06e-02, avg batch time: 0.4633, average train loss: 0.0006
[09/26 11:00:45 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1587, average loss: 1.0953
[09/26 11:00:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 11:00:45 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.7415791961552499
[09/26 11:00:51 visual_prompt]: Epoch 68 / 100: avg data time: 6.15e-02, avg batch time: 0.4737, average train loss: 0.0006
[09/26 11:00:53 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1580, average loss: 1.1082
[09/26 11:00:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 11:00:53 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.7020360665136531
[09/26 11:00:59 visual_prompt]: Epoch 69 / 100: avg data time: 4.74e-02, avg batch time: 0.4613, average train loss: 0.0006
[09/26 11:01:01 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1589, average loss: 1.1181
[09/26 11:01:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 98.00	
[09/26 11:01:01 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.6631605465176369
[09/26 11:01:07 visual_prompt]: Epoch 70 / 100: avg data time: 5.02e-02, avg batch time: 0.4646, average train loss: 0.0006
[09/26 11:01:09 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1581, average loss: 1.1264
[09/26 11:01:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 98.00	
[09/26 11:01:09 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.6250000000000002
[09/26 11:01:15 visual_prompt]: Epoch 71 / 100: avg data time: 6.15e-02, avg batch time: 0.4736, average train loss: 0.0004
[09/26 11:01:17 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1589, average loss: 1.1281
[09/26 11:01:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:01:17 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.587600919708494
[09/26 11:01:23 visual_prompt]: Epoch 72 / 100: avg data time: 6.25e-02, avg batch time: 0.4751, average train loss: 0.0005
[09/26 11:01:25 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1582, average loss: 1.1278
[09/26 11:01:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:01:25 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.5510088706615667
[09/26 11:01:31 visual_prompt]: Epoch 73 / 100: avg data time: 6.01e-02, avg batch time: 0.4720, average train loss: 0.0004
[09/26 11:01:33 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1586, average loss: 1.1272
[09/26 11:01:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:01:33 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.5152684346344087
[09/26 11:01:39 visual_prompt]: Epoch 74 / 100: avg data time: 6.49e-02, avg batch time: 0.4780, average train loss: 0.0004
[09/26 11:01:41 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1584, average loss: 1.1286
[09/26 11:01:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:01:41 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.48042315584292716
[09/26 11:01:47 visual_prompt]: Epoch 75 / 100: avg data time: 5.87e-02, avg batch time: 0.4712, average train loss: 0.0004
[09/26 11:01:49 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1579, average loss: 1.1316
[09/26 11:01:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:01:49 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.4465154878918258
[09/26 11:01:55 visual_prompt]: Epoch 76 / 100: avg data time: 5.78e-02, avg batch time: 0.4702, average train loss: 0.0003
[09/26 11:01:57 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1586, average loss: 1.1302
[09/26 11:01:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 11:01:57 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.41358674205142765
[09/26 11:02:03 visual_prompt]: Epoch 77 / 100: avg data time: 6.17e-02, avg batch time: 0.4759, average train loss: 0.0003
[09/26 11:02:05 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1580, average loss: 1.1295
[09/26 11:02:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 11:02:05 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.3816770369262533
[09/26 11:02:11 visual_prompt]: Epoch 78 / 100: avg data time: 6.12e-02, avg batch time: 0.4745, average train loss: 0.0005
[09/26 11:02:13 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1583, average loss: 1.1298
[09/26 11:02:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 11:02:13 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.35082524957668626
[09/26 11:02:19 visual_prompt]: Epoch 79 / 100: avg data time: 6.19e-02, avg batch time: 0.4748, average train loss: 0.0005
[09/26 11:02:21 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.1582, average loss: 1.1306
[09/26 11:02:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 98.00	
[09/26 11:02:21 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.32106896815325703
[09/26 11:02:28 visual_prompt]: Epoch 80 / 100: avg data time: 6.09e-02, avg batch time: 0.4746, average train loss: 0.0004
[09/26 11:02:29 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.1586, average loss: 1.1339
[09/26 11:02:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:02:29 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.2924444461012776
[09/26 11:02:36 visual_prompt]: Epoch 81 / 100: avg data time: 6.15e-02, avg batch time: 0.4739, average train loss: 0.0004
[09/26 11:02:37 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.1581, average loss: 1.1363
[09/26 11:02:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:02:37 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.26498655799159765
[09/26 11:02:44 visual_prompt]: Epoch 82 / 100: avg data time: 6.54e-02, avg batch time: 0.4778, average train loss: 0.0004
[09/26 11:02:45 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1582, average loss: 1.1394
[09/26 11:02:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:02:45 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.23872875703131582
[09/26 11:02:52 visual_prompt]: Epoch 83 / 100: avg data time: 6.35e-02, avg batch time: 0.4761, average train loss: 0.0005
[09/26 11:02:53 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1585, average loss: 1.1412
[09/26 11:02:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:02:53 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.21370303430619797
[09/26 11:03:00 visual_prompt]: Epoch 84 / 100: avg data time: 5.97e-02, avg batch time: 0.4721, average train loss: 0.0006
[09/26 11:03:01 visual_prompt]: Inference (val):avg data time: 4.80e-05, avg batch time: 0.1585, average loss: 1.1427
[09/26 11:03:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:03:01 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.18993987980446755
[09/26 11:03:08 visual_prompt]: Epoch 85 / 100: avg data time: 4.54e-02, avg batch time: 0.4603, average train loss: 0.0004
[09/26 11:03:09 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1583, average loss: 1.1444
[09/26 11:03:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:03:09 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.16746824526945162
[09/26 11:03:16 visual_prompt]: Epoch 86 / 100: avg data time: 5.59e-02, avg batch time: 0.4694, average train loss: 0.0004
[09/26 11:03:17 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1580, average loss: 1.1459
[09/26 11:03:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:03:17 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.14631550892634126
[09/26 11:03:24 visual_prompt]: Epoch 87 / 100: avg data time: 6.10e-02, avg batch time: 0.4750, average train loss: 0.0004
[09/26 11:03:25 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1582, average loss: 1.1469
[09/26 11:03:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:03:25 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.12650744212604148
[09/26 11:03:32 visual_prompt]: Epoch 88 / 100: avg data time: 6.14e-02, avg batch time: 0.4745, average train loss: 0.0003
[09/26 11:03:33 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1581, average loss: 1.1467
[09/26 11:03:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:03:33 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.10806817794674878
[09/26 11:03:40 visual_prompt]: Epoch 89 / 100: avg data time: 6.03e-02, avg batch time: 0.4737, average train loss: 0.0003
[09/26 11:03:41 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1585, average loss: 1.1467
[09/26 11:03:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:03:41 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.09102018179151586
[09/26 11:03:48 visual_prompt]: Epoch 90 / 100: avg data time: 5.73e-02, avg batch time: 0.4695, average train loss: 0.0003
[09/26 11:03:49 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1585, average loss: 1.1471
[09/26 11:03:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:03:49 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0753842240176146
[09/26 11:03:56 visual_prompt]: Epoch 91 / 100: avg data time: 6.04e-02, avg batch time: 0.4725, average train loss: 0.0004
[09/26 11:03:57 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1581, average loss: 1.1473
[09/26 11:03:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:03:57 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.061179354631058086
[09/26 11:04:04 visual_prompt]: Epoch 92 / 100: avg data time: 6.00e-02, avg batch time: 0.4731, average train loss: 0.0011
[09/26 11:04:05 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1582, average loss: 1.1425
[09/26 11:04:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 11:04:05 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.04842288007710138
[09/26 11:04:12 visual_prompt]: Epoch 93 / 100: avg data time: 6.07e-02, avg batch time: 0.4736, average train loss: 0.0003
[09/26 11:04:13 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1582, average loss: 1.1409
[09/26 11:04:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 11:04:13 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.03713034215500441
[09/26 11:04:20 visual_prompt]: Epoch 94 / 100: avg data time: 5.99e-02, avg batch time: 0.4730, average train loss: 0.0003
[09/26 11:04:21 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1580, average loss: 1.1408
[09/26 11:04:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 11:04:21 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.02731549908274289
[09/26 11:04:28 visual_prompt]: Epoch 95 / 100: avg data time: 6.72e-02, avg batch time: 0.4792, average train loss: 0.0004
[09/26 11:04:30 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1585, average loss: 1.1409
[09/26 11:04:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 11:04:30 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.018990308734739975
[09/26 11:04:36 visual_prompt]: Epoch 96 / 100: avg data time: 5.75e-02, avg batch time: 0.4704, average train loss: 0.0003
[09/26 11:04:38 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1585, average loss: 1.1410
[09/26 11:04:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 11:04:38 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.012164914073037048
[09/26 11:04:44 visual_prompt]: Epoch 97 / 100: avg data time: 5.82e-02, avg batch time: 0.4718, average train loss: 0.0005
[09/26 11:04:46 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1585, average loss: 1.1411
[09/26 11:04:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 11:04:46 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.006847630789658388
[09/26 11:04:52 visual_prompt]: Epoch 98 / 100: avg data time: 7.03e-02, avg batch time: 0.4829, average train loss: 0.0003
[09/26 11:04:54 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1581, average loss: 1.1411
[09/26 11:04:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 11:04:54 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.003044937175219753
[09/26 11:05:00 visual_prompt]: Epoch 99 / 100: avg data time: 4.62e-02, avg batch time: 0.4592, average train loss: 0.0004
[09/26 11:05:02 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1582, average loss: 1.1411
[09/26 11:05:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 11:05:02 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0007614662261302974
[09/26 11:05:08 visual_prompt]: Epoch 100 / 100: avg data time: 4.69e-02, avg batch time: 0.4600, average train loss: 0.0002
[09/26 11:05:10 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1588, average loss: 1.1411
[09/26 11:05:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 11:05:10 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 11:05:10 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 11:05:10 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 11:05:10 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 11:05:10 visual_prompt]: Training with config:
[09/26 11:05:10 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr1.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 11:05:10 visual_prompt]: Loading training data...
[09/26 11:05:10 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 11:05:11 visual_prompt]: Number of images: 800
[09/26 11:05:11 visual_prompt]: Number of classes: 10 / 10
[09/26 11:05:11 visual_prompt]: Loading validation data...
[09/26 11:05:11 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 11:05:11 visual_prompt]: Number of images: 200
[09/26 11:05:11 visual_prompt]: Number of classes: 10 / 10
[09/26 11:05:11 visual_prompt]: Constructing models...
[09/26 11:05:13 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 11:05:13 visual_prompt]: tuned percent:0.543
[09/26 11:05:13 visual_prompt]: Device used for model: 0
[09/26 11:05:13 visual_prompt]: Setting up Evaluator...
[09/26 11:05:13 visual_prompt]: Setting up Trainer...
[09/26 11:05:13 visual_prompt]: 	Setting up the optimizer...
[09/26 11:05:13 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 11:05:20 visual_prompt]: Epoch 1 / 100: avg data time: 5.45e-02, avg batch time: 0.4722, average train loss: 2.6844
[09/26 11:05:21 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1582, average loss: 2.6214
[09/26 11:05:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 11:05:21 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 11:05:21 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/26 11:05:28 visual_prompt]: Epoch 2 / 100: avg data time: 5.65e-02, avg batch time: 0.4677, average train loss: 2.7918
[09/26 11:05:29 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1582, average loss: 2.2429
[09/26 11:05:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 11:05:29 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/26 11:05:36 visual_prompt]: Epoch 3 / 100: avg data time: 5.12e-02, avg batch time: 0.4659, average train loss: 2.2944
[09/26 11:05:37 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 2.2607
[09/26 11:05:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 11:05:37 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/26 11:05:44 visual_prompt]: Epoch 4 / 100: avg data time: 5.18e-02, avg batch time: 0.4639, average train loss: 2.2754
[09/26 11:05:45 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1584, average loss: 2.2471
[09/26 11:05:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:05:45 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/26 11:05:52 visual_prompt]: Epoch 5 / 100: avg data time: 6.23e-02, avg batch time: 0.4752, average train loss: 2.3086
[09/26 11:05:53 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1582, average loss: 2.2829
[09/26 11:05:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:05:53 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/26 11:06:00 visual_prompt]: Epoch 6 / 100: avg data time: 6.61e-02, avg batch time: 0.4794, average train loss: 2.2844
[09/26 11:06:01 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1581, average loss: 2.3173
[09/26 11:06:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 47.00	
[09/26 11:06:01 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/26 11:06:08 visual_prompt]: Epoch 7 / 100: avg data time: 5.74e-02, avg batch time: 0.4693, average train loss: 2.2904
[09/26 11:06:10 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.1583, average loss: 2.2631
[09/26 11:06:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 11:06:10 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/26 11:06:16 visual_prompt]: Epoch 8 / 100: avg data time: 6.33e-02, avg batch time: 0.4747, average train loss: 2.2772
[09/26 11:06:18 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1582, average loss: 2.2710
[09/26 11:06:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.50	
[09/26 11:06:18 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/26 11:06:24 visual_prompt]: Epoch 9 / 100: avg data time: 6.21e-02, avg batch time: 0.4751, average train loss: 2.2881
[09/26 11:06:26 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1581, average loss: 2.2441
[09/26 11:06:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 11:06:26 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/26 11:06:32 visual_prompt]: Epoch 10 / 100: avg data time: 5.75e-02, avg batch time: 0.4688, average train loss: 2.3143
[09/26 11:06:34 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1575, average loss: 2.2570
[09/26 11:06:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 11:06:34 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/26 11:06:40 visual_prompt]: Epoch 11 / 100: avg data time: 6.01e-02, avg batch time: 0.4723, average train loss: 2.3201
[09/26 11:06:42 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1584, average loss: 2.2339
[09/26 11:06:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 11:06:42 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/26 11:06:48 visual_prompt]: Epoch 12 / 100: avg data time: 6.01e-02, avg batch time: 0.4714, average train loss: 2.3284
[09/26 11:06:50 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1583, average loss: 2.2989
[09/26 11:06:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 11:06:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/26 11:06:56 visual_prompt]: Epoch 13 / 100: avg data time: 6.24e-02, avg batch time: 0.4736, average train loss: 2.4137
[09/26 11:06:58 visual_prompt]: Inference (val):avg data time: 5.23e-05, avg batch time: 0.1585, average loss: 2.4926
[09/26 11:06:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.50	
[09/26 11:06:58 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/26 11:07:04 visual_prompt]: Epoch 14 / 100: avg data time: 5.83e-02, avg batch time: 0.4703, average train loss: 2.4063
[09/26 11:07:06 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1582, average loss: 2.3032
[09/26 11:07:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 60.50	
[09/26 11:07:06 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/26 11:07:12 visual_prompt]: Epoch 15 / 100: avg data time: 6.28e-02, avg batch time: 0.4744, average train loss: 2.6184
[09/26 11:07:14 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1580, average loss: 3.6983
[09/26 11:07:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 49.50	
[09/26 11:07:14 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/26 11:07:20 visual_prompt]: Epoch 16 / 100: avg data time: 4.92e-02, avg batch time: 0.4621, average train loss: 2.6466
[09/26 11:07:22 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1578, average loss: 2.5123
[09/26 11:07:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 6.00	top5: 58.50	
[09/26 11:07:22 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/26 11:07:28 visual_prompt]: Epoch 17 / 100: avg data time: 6.12e-02, avg batch time: 0.4722, average train loss: 2.4749
[09/26 11:07:30 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1584, average loss: 2.2844
[09/26 11:07:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 11:07:30 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/26 11:07:36 visual_prompt]: Epoch 18 / 100: avg data time: 5.66e-02, avg batch time: 0.4681, average train loss: 2.3267
[09/26 11:07:38 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1580, average loss: 2.2716
[09/26 11:07:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 11:07:38 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/26 11:07:44 visual_prompt]: Epoch 19 / 100: avg data time: 6.12e-02, avg batch time: 0.4735, average train loss: 2.3328
[09/26 11:07:46 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1588, average loss: 2.3034
[09/26 11:07:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 57.50	
[09/26 11:07:46 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/26 11:07:52 visual_prompt]: Epoch 20 / 100: avg data time: 6.28e-02, avg batch time: 0.4757, average train loss: 2.3463
[09/26 11:07:54 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1587, average loss: 2.3545
[09/26 11:07:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 55.50	
[09/26 11:07:54 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/26 11:08:00 visual_prompt]: Epoch 21 / 100: avg data time: 5.67e-02, avg batch time: 0.4697, average train loss: 2.3465
[09/26 11:08:02 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1579, average loss: 2.4092
[09/26 11:08:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 46.50	
[09/26 11:08:02 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/26 11:08:08 visual_prompt]: Epoch 22 / 100: avg data time: 4.76e-02, avg batch time: 0.4617, average train loss: 2.3688
[09/26 11:08:10 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1584, average loss: 2.3162
[09/26 11:08:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 59.50	
[09/26 11:08:10 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/26 11:08:16 visual_prompt]: Epoch 23 / 100: avg data time: 5.37e-02, avg batch time: 0.4655, average train loss: 2.3322
[09/26 11:08:18 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1577, average loss: 2.3295
[09/26 11:08:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.50	
[09/26 11:08:18 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/26 11:08:24 visual_prompt]: Epoch 24 / 100: avg data time: 6.53e-02, avg batch time: 0.4774, average train loss: 2.3560
[09/26 11:08:26 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1581, average loss: 2.4329
[09/26 11:08:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.00	
[09/26 11:08:26 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/26 11:08:32 visual_prompt]: Epoch 25 / 100: avg data time: 5.92e-02, avg batch time: 0.4722, average train loss: 2.3901
[09/26 11:08:34 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1583, average loss: 2.3435
[09/26 11:08:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 60.50	
[09/26 11:08:34 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/26 11:08:40 visual_prompt]: Epoch 26 / 100: avg data time: 5.84e-02, avg batch time: 0.4706, average train loss: 2.3709
[09/26 11:08:42 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1585, average loss: 2.2947
[09/26 11:08:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 11:08:42 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/26 11:08:48 visual_prompt]: Epoch 27 / 100: avg data time: 6.28e-02, avg batch time: 0.4761, average train loss: 2.2869
[09/26 11:08:50 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1579, average loss: 2.4688
[09/26 11:08:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 44.50	
[09/26 11:08:50 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[09/26 11:08:56 visual_prompt]: Epoch 28 / 100: avg data time: 5.25e-02, avg batch time: 0.4658, average train loss: 2.3559
[09/26 11:08:58 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1579, average loss: 2.2598
[09/26 11:08:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 11:08:58 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[09/26 11:09:04 visual_prompt]: Epoch 29 / 100: avg data time: 4.36e-02, avg batch time: 0.4560, average train loss: 2.2994
[09/26 11:09:06 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1581, average loss: 2.2871
[09/26 11:09:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.00	
[09/26 11:09:06 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[09/26 11:09:12 visual_prompt]: Epoch 30 / 100: avg data time: 4.69e-02, avg batch time: 0.4623, average train loss: 2.3211
[09/26 11:09:13 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1585, average loss: 2.3076
[09/26 11:09:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.00	
[09/26 11:09:13 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[09/26 11:09:20 visual_prompt]: Epoch 31 / 100: avg data time: 6.09e-02, avg batch time: 0.4730, average train loss: 2.3699
[09/26 11:09:22 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1578, average loss: 2.2362
[09/26 11:09:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 11:09:22 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[09/26 11:09:28 visual_prompt]: Epoch 32 / 100: avg data time: 6.57e-02, avg batch time: 0.4785, average train loss: 2.3138
[09/26 11:09:30 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1584, average loss: 2.2441
[09/26 11:09:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:09:30 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[09/26 11:09:36 visual_prompt]: Epoch 33 / 100: avg data time: 5.07e-02, avg batch time: 0.4632, average train loss: 2.3148
[09/26 11:09:38 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1585, average loss: 2.2623
[09/26 11:09:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 11:09:38 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[09/26 11:09:44 visual_prompt]: Epoch 34 / 100: avg data time: 5.84e-02, avg batch time: 0.4715, average train loss: 2.2896
[09/26 11:09:46 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.1580, average loss: 2.2968
[09/26 11:09:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:09:46 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[09/26 11:09:52 visual_prompt]: Epoch 35 / 100: avg data time: 5.53e-02, avg batch time: 0.4683, average train loss: 2.3692
[09/26 11:09:54 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1582, average loss: 2.4853
[09/26 11:09:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 47.50	
[09/26 11:09:54 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[09/26 11:10:00 visual_prompt]: Epoch 36 / 100: avg data time: 6.47e-02, avg batch time: 0.4772, average train loss: 2.3443
[09/26 11:10:02 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1585, average loss: 2.2409
[09/26 11:10:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 11:10:02 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[09/26 11:10:08 visual_prompt]: Epoch 37 / 100: avg data time: 5.73e-02, avg batch time: 0.4702, average train loss: 2.2843
[09/26 11:10:10 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1576, average loss: 2.2236
[09/26 11:10:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 11:10:10 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[09/26 11:10:16 visual_prompt]: Epoch 38 / 100: avg data time: 5.10e-02, avg batch time: 0.4661, average train loss: 2.2963
[09/26 11:10:18 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1583, average loss: 2.2548
[09/26 11:10:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 11:10:18 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[09/26 11:10:24 visual_prompt]: Epoch 39 / 100: avg data time: 6.83e-02, avg batch time: 0.4807, average train loss: 2.3215
[09/26 11:10:26 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1580, average loss: 2.2943
[09/26 11:10:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.50	
[09/26 11:10:26 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[09/26 11:10:32 visual_prompt]: Epoch 40 / 100: avg data time: 5.78e-02, avg batch time: 0.4705, average train loss: 2.3708
[09/26 11:10:34 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1581, average loss: 2.2682
[09/26 11:10:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:10:34 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[09/26 11:10:40 visual_prompt]: Epoch 41 / 100: avg data time: 5.45e-02, avg batch time: 0.4677, average train loss: 2.3087
[09/26 11:10:42 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1583, average loss: 2.3987
[09/26 11:10:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.50	
[09/26 11:10:42 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[09/26 11:10:48 visual_prompt]: Epoch 42 / 100: avg data time: 5.69e-02, avg batch time: 0.4688, average train loss: 2.4973
[09/26 11:10:50 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1583, average loss: 2.5631
[09/26 11:10:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 11:10:50 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[09/26 11:10:56 visual_prompt]: Epoch 43 / 100: avg data time: 6.07e-02, avg batch time: 0.4739, average train loss: 2.3873
[09/26 11:10:58 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1571, average loss: 2.3966
[09/26 11:10:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 64.00	
[09/26 11:10:58 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[09/26 11:11:04 visual_prompt]: Epoch 44 / 100: avg data time: 6.09e-02, avg batch time: 0.4740, average train loss: 2.3091
[09/26 11:11:06 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1580, average loss: 2.3448
[09/26 11:11:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 63.50	
[09/26 11:11:06 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[09/26 11:11:12 visual_prompt]: Epoch 45 / 100: avg data time: 5.57e-02, avg batch time: 0.4692, average train loss: 2.3356
[09/26 11:11:14 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1581, average loss: 2.2670
[09/26 11:11:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 11:11:14 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[09/26 11:11:20 visual_prompt]: Epoch 46 / 100: avg data time: 5.42e-02, avg batch time: 0.4675, average train loss: 2.3331
[09/26 11:11:22 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1582, average loss: 2.2546
[09/26 11:11:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 11:11:22 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[09/26 11:11:28 visual_prompt]: Epoch 47 / 100: avg data time: 5.14e-02, avg batch time: 0.4645, average train loss: 2.3448
[09/26 11:11:30 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1580, average loss: 2.3136
[09/26 11:11:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 54.50	
[09/26 11:11:30 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[09/26 11:11:36 visual_prompt]: Epoch 48 / 100: avg data time: 5.77e-02, avg batch time: 0.4689, average train loss: 2.2920
[09/26 11:11:38 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1584, average loss: 2.2802
[09/26 11:11:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 11:11:38 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[09/26 11:11:44 visual_prompt]: Epoch 49 / 100: avg data time: 6.49e-02, avg batch time: 0.4762, average train loss: 2.3346
[09/26 11:11:46 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1582, average loss: 2.3395
[09/26 11:11:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 11:11:46 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[09/26 11:11:52 visual_prompt]: Epoch 50 / 100: avg data time: 6.39e-02, avg batch time: 0.4759, average train loss: 2.3007
[09/26 11:11:54 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1582, average loss: 2.2712
[09/26 11:11:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 63.00	
[09/26 11:11:54 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[09/26 11:12:00 visual_prompt]: Epoch 51 / 100: avg data time: 5.51e-02, avg batch time: 0.4684, average train loss: 2.2986
[09/26 11:12:02 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1583, average loss: 2.3310
[09/26 11:12:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 62.50	
[09/26 11:12:02 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[09/26 11:12:08 visual_prompt]: Epoch 52 / 100: avg data time: 5.55e-02, avg batch time: 0.4673, average train loss: 2.3474
[09/26 11:12:10 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1580, average loss: 2.3246
[09/26 11:12:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 11:12:10 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[09/26 11:12:16 visual_prompt]: Epoch 53 / 100: avg data time: 5.36e-02, avg batch time: 0.4692, average train loss: 2.3146
[09/26 11:12:18 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.1583, average loss: 2.2204
[09/26 11:12:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 11:12:18 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[09/26 11:12:24 visual_prompt]: Epoch 54 / 100: avg data time: 5.39e-02, avg batch time: 0.4678, average train loss: 2.2854
[09/26 11:12:26 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1576, average loss: 2.2694
[09/26 11:12:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 11:12:26 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[09/26 11:12:32 visual_prompt]: Epoch 55 / 100: avg data time: 5.85e-02, avg batch time: 0.4707, average train loss: 2.2926
[09/26 11:12:34 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1581, average loss: 2.2814
[09/26 11:12:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.00	
[09/26 11:12:34 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[09/26 11:12:40 visual_prompt]: Epoch 56 / 100: avg data time: 6.64e-02, avg batch time: 0.4786, average train loss: 2.2665
[09/26 11:12:42 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1584, average loss: 2.2426
[09/26 11:12:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 11:12:42 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[09/26 11:12:48 visual_prompt]: Epoch 57 / 100: avg data time: 5.62e-02, avg batch time: 0.4705, average train loss: 2.2893
[09/26 11:12:50 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1583, average loss: 2.2749
[09/26 11:12:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 11:12:50 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[09/26 11:12:57 visual_prompt]: Epoch 58 / 100: avg data time: 5.91e-02, avg batch time: 0.4722, average train loss: 2.2846
[09/26 11:12:58 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1580, average loss: 2.2907
[09/26 11:12:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 56.50	
[09/26 11:12:58 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[09/26 11:13:05 visual_prompt]: Epoch 59 / 100: avg data time: 5.85e-02, avg batch time: 0.4709, average train loss: 2.2940
[09/26 11:13:06 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1580, average loss: 2.2717
[09/26 11:13:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 11:13:06 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[09/26 11:13:13 visual_prompt]: Epoch 60 / 100: avg data time: 6.36e-02, avg batch time: 0.4750, average train loss: 2.2938
[09/26 11:13:14 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1583, average loss: 2.3511
[09/26 11:13:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 60.50	
[09/26 11:13:14 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[09/26 11:13:21 visual_prompt]: Epoch 61 / 100: avg data time: 5.49e-02, avg batch time: 0.4673, average train loss: 2.3118
[09/26 11:13:22 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1583, average loss: 2.3008
[09/26 11:13:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.50	
[09/26 11:13:22 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[09/26 11:13:29 visual_prompt]: Epoch 62 / 100: avg data time: 6.09e-02, avg batch time: 0.4740, average train loss: 2.2952
[09/26 11:13:30 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1582, average loss: 2.2344
[09/26 11:13:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:13:30 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[09/26 11:13:37 visual_prompt]: Epoch 63 / 100: avg data time: 6.35e-02, avg batch time: 0.4763, average train loss: 2.2784
[09/26 11:13:38 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1583, average loss: 2.3854
[09/26 11:13:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 58.50	
[09/26 11:13:38 visual_prompt]: Best epoch 63: best metric: 0.235
[09/26 11:13:38 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[09/26 11:13:45 visual_prompt]: Epoch 64 / 100: avg data time: 4.52e-02, avg batch time: 0.4584, average train loss: 2.3173
[09/26 11:13:46 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.1584, average loss: 2.2264
[09/26 11:13:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 11:13:46 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[09/26 11:13:53 visual_prompt]: Epoch 65 / 100: avg data time: 6.02e-02, avg batch time: 0.4731, average train loss: 2.2959
[09/26 11:13:54 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1579, average loss: 2.2618
[09/26 11:13:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 11:13:54 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[09/26 11:14:01 visual_prompt]: Epoch 66 / 100: avg data time: 5.82e-02, avg batch time: 0.4712, average train loss: 2.3017
[09/26 11:14:02 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1584, average loss: 2.2395
[09/26 11:14:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 11:14:02 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[09/26 11:14:09 visual_prompt]: Epoch 67 / 100: avg data time: 4.74e-02, avg batch time: 0.4599, average train loss: 2.2610
[09/26 11:14:10 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1582, average loss: 2.2510
[09/26 11:14:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.00	
[09/26 11:14:10 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[09/26 11:14:17 visual_prompt]: Epoch 68 / 100: avg data time: 6.80e-02, avg batch time: 0.4806, average train loss: 2.2472
[09/26 11:14:18 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1583, average loss: 2.2244
[09/26 11:14:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:14:18 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[09/26 11:14:25 visual_prompt]: Epoch 69 / 100: avg data time: 5.33e-02, avg batch time: 0.4656, average train loss: 2.2505
[09/26 11:14:26 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1582, average loss: 2.2138
[09/26 11:14:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:14:26 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[09/26 11:14:33 visual_prompt]: Epoch 70 / 100: avg data time: 5.58e-02, avg batch time: 0.4677, average train loss: 2.2443
[09/26 11:14:34 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1586, average loss: 2.2216
[09/26 11:14:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:14:34 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[09/26 11:14:41 visual_prompt]: Epoch 71 / 100: avg data time: 5.64e-02, avg batch time: 0.4698, average train loss: 2.2516
[09/26 11:14:42 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1582, average loss: 2.2160
[09/26 11:14:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:14:42 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[09/26 11:14:49 visual_prompt]: Epoch 72 / 100: avg data time: 4.66e-02, avg batch time: 0.4613, average train loss: 2.2460
[09/26 11:14:50 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1584, average loss: 2.2167
[09/26 11:14:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:14:50 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[09/26 11:14:57 visual_prompt]: Epoch 73 / 100: avg data time: 6.20e-02, avg batch time: 0.4753, average train loss: 2.2460
[09/26 11:14:58 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1581, average loss: 2.2262
[09/26 11:14:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:14:58 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[09/26 11:15:05 visual_prompt]: Epoch 74 / 100: avg data time: 5.11e-02, avg batch time: 0.4658, average train loss: 2.2472
[09/26 11:15:06 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1582, average loss: 2.2182
[09/26 11:15:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:15:06 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[09/26 11:15:13 visual_prompt]: Epoch 75 / 100: avg data time: 6.53e-02, avg batch time: 0.4777, average train loss: 2.2426
[09/26 11:15:14 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1584, average loss: 2.2205
[09/26 11:15:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 11:15:14 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[09/26 11:15:21 visual_prompt]: Epoch 76 / 100: avg data time: 6.03e-02, avg batch time: 0.4738, average train loss: 2.2490
[09/26 11:15:22 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1584, average loss: 2.2210
[09/26 11:15:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 11:15:22 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[09/26 11:15:29 visual_prompt]: Epoch 77 / 100: avg data time: 4.80e-02, avg batch time: 0.4608, average train loss: 2.2591
[09/26 11:15:30 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1579, average loss: 2.2156
[09/26 11:15:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:15:30 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[09/26 11:15:37 visual_prompt]: Epoch 78 / 100: avg data time: 6.50e-02, avg batch time: 0.4782, average train loss: 2.2448
[09/26 11:15:38 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1584, average loss: 2.2173
[09/26 11:15:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 11:15:38 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[09/26 11:15:45 visual_prompt]: Epoch 79 / 100: avg data time: 6.42e-02, avg batch time: 0.4770, average train loss: 2.2402
[09/26 11:15:46 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1583, average loss: 2.2118
[09/26 11:15:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:15:46 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[09/26 11:15:53 visual_prompt]: Epoch 80 / 100: avg data time: 6.09e-02, avg batch time: 0.4737, average train loss: 2.2451
[09/26 11:15:55 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1582, average loss: 2.2238
[09/26 11:15:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:15:55 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[09/26 11:16:01 visual_prompt]: Epoch 81 / 100: avg data time: 6.64e-02, avg batch time: 0.4791, average train loss: 2.2486
[09/26 11:16:03 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1577, average loss: 2.2160
[09/26 11:16:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:16:03 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[09/26 11:16:09 visual_prompt]: Epoch 82 / 100: avg data time: 5.97e-02, avg batch time: 0.4720, average train loss: 2.2375
[09/26 11:16:11 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1581, average loss: 2.2155
[09/26 11:16:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:16:11 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[09/26 11:16:17 visual_prompt]: Epoch 83 / 100: avg data time: 6.37e-02, avg batch time: 0.4756, average train loss: 2.2423
[09/26 11:16:19 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1579, average loss: 2.2191
[09/26 11:16:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:16:19 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[09/26 11:16:25 visual_prompt]: Epoch 84 / 100: avg data time: 6.52e-02, avg batch time: 0.4785, average train loss: 2.2367
[09/26 11:16:27 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1583, average loss: 2.2148
[09/26 11:16:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:16:27 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[09/26 11:16:33 visual_prompt]: Epoch 85 / 100: avg data time: 6.03e-02, avg batch time: 0.4726, average train loss: 2.2342
[09/26 11:16:35 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1586, average loss: 2.2147
[09/26 11:16:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:16:35 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[09/26 11:16:41 visual_prompt]: Epoch 86 / 100: avg data time: 5.67e-02, avg batch time: 0.4707, average train loss: 2.2344
[09/26 11:16:43 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1581, average loss: 2.2151
[09/26 11:16:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:16:43 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[09/26 11:16:50 visual_prompt]: Epoch 87 / 100: avg data time: 6.45e-02, avg batch time: 0.4781, average train loss: 2.2338
[09/26 11:16:51 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1583, average loss: 2.2169
[09/26 11:16:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:16:51 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[09/26 11:16:58 visual_prompt]: Epoch 88 / 100: avg data time: 6.04e-02, avg batch time: 0.4740, average train loss: 2.2382
[09/26 11:16:59 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1586, average loss: 2.2169
[09/26 11:16:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:16:59 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[09/26 11:17:06 visual_prompt]: Epoch 89 / 100: avg data time: 6.18e-02, avg batch time: 0.4746, average train loss: 2.2337
[09/26 11:17:07 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1583, average loss: 2.2169
[09/26 11:17:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:17:07 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[09/26 11:17:14 visual_prompt]: Epoch 90 / 100: avg data time: 6.34e-02, avg batch time: 0.4764, average train loss: 2.2326
[09/26 11:17:15 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1581, average loss: 2.2134
[09/26 11:17:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:17:15 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[09/26 11:17:22 visual_prompt]: Epoch 91 / 100: avg data time: 6.43e-02, avg batch time: 0.4775, average train loss: 2.2307
[09/26 11:17:23 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1585, average loss: 2.2144
[09/26 11:17:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:17:23 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[09/26 11:17:30 visual_prompt]: Epoch 92 / 100: avg data time: 5.88e-02, avg batch time: 0.4718, average train loss: 2.2294
[09/26 11:17:31 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1582, average loss: 2.2172
[09/26 11:17:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:17:31 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[09/26 11:17:38 visual_prompt]: Epoch 93 / 100: avg data time: 5.67e-02, avg batch time: 0.4710, average train loss: 2.2291
[09/26 11:17:39 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1586, average loss: 2.2185
[09/26 11:17:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:17:39 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[09/26 11:17:46 visual_prompt]: Epoch 94 / 100: avg data time: 6.50e-02, avg batch time: 0.4776, average train loss: 2.2263
[09/26 11:17:48 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1582, average loss: 2.2178
[09/26 11:17:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:17:48 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[09/26 11:17:54 visual_prompt]: Epoch 95 / 100: avg data time: 6.20e-02, avg batch time: 0.4753, average train loss: 2.2302
[09/26 11:17:56 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1579, average loss: 2.2154
[09/26 11:17:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:17:56 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[09/26 11:18:02 visual_prompt]: Epoch 96 / 100: avg data time: 6.20e-02, avg batch time: 0.4748, average train loss: 2.2249
[09/26 11:18:04 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1584, average loss: 2.2139
[09/26 11:18:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.50	
[09/26 11:18:04 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[09/26 11:18:10 visual_prompt]: Epoch 97 / 100: avg data time: 5.95e-02, avg batch time: 0.4726, average train loss: 2.2323
[09/26 11:18:12 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1582, average loss: 2.2123
[09/26 11:18:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:18:12 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[09/26 11:18:18 visual_prompt]: Epoch 98 / 100: avg data time: 6.43e-02, avg batch time: 0.4776, average train loss: 2.2273
[09/26 11:18:20 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1579, average loss: 2.2095
[09/26 11:18:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:18:20 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[09/26 11:18:26 visual_prompt]: Epoch 99 / 100: avg data time: 6.43e-02, avg batch time: 0.4771, average train loss: 2.2199
[09/26 11:18:28 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1584, average loss: 2.1986
[09/26 11:18:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:18:28 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[09/26 11:18:34 visual_prompt]: Epoch 100 / 100: avg data time: 5.96e-02, avg batch time: 0.4733, average train loss: 2.2083
[09/26 11:18:36 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1584, average loss: 2.1972
[09/26 11:18:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:18:36 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 11:18:36 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 11:18:36 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 11:18:36 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 11:18:36 visual_prompt]: Training with config:
[09/26 11:18:36 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr1.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 11:18:36 visual_prompt]: Loading training data...
[09/26 11:18:36 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 11:18:37 visual_prompt]: Number of images: 800
[09/26 11:18:37 visual_prompt]: Number of classes: 10 / 10
[09/26 11:18:37 visual_prompt]: Loading validation data...
[09/26 11:18:37 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 11:18:37 visual_prompt]: Number of images: 200
[09/26 11:18:37 visual_prompt]: Number of classes: 10 / 10
[09/26 11:18:37 visual_prompt]: Constructing models...
[09/26 11:18:40 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 11:18:40 visual_prompt]: tuned percent:0.543
[09/26 11:18:40 visual_prompt]: Device used for model: 0
[09/26 11:18:40 visual_prompt]: Setting up Evaluator...
[09/26 11:18:40 visual_prompt]: Setting up Trainer...
[09/26 11:18:40 visual_prompt]: 	Setting up the optimizer...
[09/26 11:18:40 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 11:18:46 visual_prompt]: Epoch 1 / 100: avg data time: 6.43e-02, avg batch time: 0.4826, average train loss: 2.6743
[09/26 11:18:48 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1583, average loss: 2.6214
[09/26 11:18:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 11:18:48 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 11:18:48 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/26 11:18:55 visual_prompt]: Epoch 2 / 100: avg data time: 6.28e-02, avg batch time: 0.4745, average train loss: 2.9005
[09/26 11:18:56 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.1582, average loss: 2.2335
[09/26 11:18:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 11:18:56 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/26 11:19:03 visual_prompt]: Epoch 3 / 100: avg data time: 5.43e-02, avg batch time: 0.4676, average train loss: 2.3318
[09/26 11:19:04 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1582, average loss: 2.2250
[09/26 11:19:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 11:19:04 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/26 11:19:11 visual_prompt]: Epoch 4 / 100: avg data time: 5.99e-02, avg batch time: 0.4739, average train loss: 2.2740
[09/26 11:19:12 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1579, average loss: 2.2668
[09/26 11:19:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.50	top5: 59.00	
[09/26 11:19:12 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/26 11:19:19 visual_prompt]: Epoch 5 / 100: avg data time: 6.30e-02, avg batch time: 0.4755, average train loss: 2.2872
[09/26 11:19:20 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1586, average loss: 2.2881
[09/26 11:19:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 61.00	
[09/26 11:19:20 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/26 11:19:27 visual_prompt]: Epoch 6 / 100: avg data time: 6.34e-02, avg batch time: 0.4779, average train loss: 2.3222
[09/26 11:19:28 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1582, average loss: 2.2595
[09/26 11:19:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 67.00	
[09/26 11:19:28 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/26 11:19:35 visual_prompt]: Epoch 7 / 100: avg data time: 6.10e-02, avg batch time: 0.4733, average train loss: 2.3296
[09/26 11:19:36 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1586, average loss: 2.3202
[09/26 11:19:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 11:19:36 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/26 11:19:43 visual_prompt]: Epoch 8 / 100: avg data time: 5.40e-02, avg batch time: 0.4671, average train loss: 2.3680
[09/26 11:19:44 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1584, average loss: 2.2157
[09/26 11:19:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:19:44 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/26 11:19:51 visual_prompt]: Epoch 9 / 100: avg data time: 6.64e-02, avg batch time: 0.4792, average train loss: 2.3629
[09/26 11:19:52 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1577, average loss: 2.2731
[09/26 11:19:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 68.00	
[09/26 11:19:52 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/26 11:19:59 visual_prompt]: Epoch 10 / 100: avg data time: 5.83e-02, avg batch time: 0.4722, average train loss: 2.2828
[09/26 11:20:00 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1581, average loss: 2.3150
[09/26 11:20:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 58.50	
[09/26 11:20:00 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/26 11:20:07 visual_prompt]: Epoch 11 / 100: avg data time: 5.39e-02, avg batch time: 0.4671, average train loss: 2.2445
[09/26 11:20:08 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1585, average loss: 2.2042
[09/26 11:20:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.50	top5: 75.50	
[09/26 11:20:08 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/26 11:20:15 visual_prompt]: Epoch 12 / 100: avg data time: 5.13e-02, avg batch time: 0.4659, average train loss: 2.2738
[09/26 11:20:16 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1582, average loss: 2.0496
[09/26 11:20:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.00	top5: 65.00	
[09/26 11:20:16 visual_prompt]: Best epoch 12: best metric: 0.250
[09/26 11:20:16 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/26 11:20:23 visual_prompt]: Epoch 13 / 100: avg data time: 5.18e-02, avg batch time: 0.4656, average train loss: 2.1144
[09/26 11:20:24 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1599, average loss: 2.0225
[09/26 11:20:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 34.50	top5: 68.50	
[09/26 11:20:24 visual_prompt]: Best epoch 13: best metric: 0.345
[09/26 11:20:24 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/26 11:20:31 visual_prompt]: Epoch 14 / 100: avg data time: 5.99e-02, avg batch time: 0.4732, average train loss: 2.0222
[09/26 11:20:32 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1585, average loss: 2.3743
[09/26 11:20:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 11:20:32 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/26 11:20:39 visual_prompt]: Epoch 15 / 100: avg data time: 5.34e-02, avg batch time: 0.4658, average train loss: 2.4781
[09/26 11:20:40 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1585, average loss: 2.6157
[09/26 11:20:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 41.50	
[09/26 11:20:40 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/26 11:20:47 visual_prompt]: Epoch 16 / 100: avg data time: 5.73e-02, avg batch time: 0.4699, average train loss: 2.4568
[09/26 11:20:48 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1586, average loss: 2.3894
[09/26 11:20:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 55.00	
[09/26 11:20:48 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/26 11:20:55 visual_prompt]: Epoch 17 / 100: avg data time: 5.34e-02, avg batch time: 0.4676, average train loss: 2.5226
[09/26 11:20:56 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1578, average loss: 2.4064
[09/26 11:20:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 49.00	
[09/26 11:20:56 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/26 11:21:03 visual_prompt]: Epoch 18 / 100: avg data time: 6.00e-02, avg batch time: 0.4743, average train loss: 2.4025
[09/26 11:21:04 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1585, average loss: 2.3098
[09/26 11:21:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 11:21:04 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/26 11:21:11 visual_prompt]: Epoch 19 / 100: avg data time: 6.02e-02, avg batch time: 0.4735, average train loss: 2.4008
[09/26 11:21:12 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1579, average loss: 2.2990
[09/26 11:21:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 11:21:12 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/26 11:21:19 visual_prompt]: Epoch 20 / 100: avg data time: 5.96e-02, avg batch time: 0.4716, average train loss: 2.3801
[09/26 11:21:20 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1578, average loss: 2.3527
[09/26 11:21:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 11:21:20 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/26 11:21:27 visual_prompt]: Epoch 21 / 100: avg data time: 6.39e-02, avg batch time: 0.4773, average train loss: 2.3319
[09/26 11:21:28 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1580, average loss: 2.2538
[09/26 11:21:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 11:21:29 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/26 11:21:35 visual_prompt]: Epoch 22 / 100: avg data time: 6.11e-02, avg batch time: 0.4742, average train loss: 2.2803
[09/26 11:21:36 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1579, average loss: 2.2483
[09/26 11:21:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 11:21:36 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/26 11:21:43 visual_prompt]: Epoch 23 / 100: avg data time: 6.15e-02, avg batch time: 0.4748, average train loss: 2.3056
[09/26 11:21:45 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1581, average loss: 2.2346
[09/26 11:21:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:21:45 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/26 11:21:51 visual_prompt]: Epoch 24 / 100: avg data time: 6.59e-02, avg batch time: 0.4782, average train loss: 2.2830
[09/26 11:21:53 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1582, average loss: 2.2822
[09/26 11:21:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:21:53 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/26 11:21:59 visual_prompt]: Epoch 25 / 100: avg data time: 6.26e-02, avg batch time: 0.4757, average train loss: 2.3278
[09/26 11:22:01 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1582, average loss: 2.2433
[09/26 11:22:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:22:01 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/26 11:22:07 visual_prompt]: Epoch 26 / 100: avg data time: 6.42e-02, avg batch time: 0.4769, average train loss: 2.3015
[09/26 11:22:09 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1578, average loss: 2.3012
[09/26 11:22:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 13.50	top5: 59.00	
[09/26 11:22:09 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/26 11:22:15 visual_prompt]: Epoch 27 / 100: avg data time: 6.80e-02, avg batch time: 0.4804, average train loss: 2.2818
[09/26 11:22:17 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1586, average loss: 2.2703
[09/26 11:22:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 11:22:17 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[09/26 11:22:24 visual_prompt]: Epoch 28 / 100: avg data time: 6.44e-02, avg batch time: 0.4773, average train loss: 2.3290
[09/26 11:22:25 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1579, average loss: 2.3078
[09/26 11:22:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 11:22:25 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[09/26 11:22:32 visual_prompt]: Epoch 29 / 100: avg data time: 5.54e-02, avg batch time: 0.4696, average train loss: 2.2989
[09/26 11:22:33 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1582, average loss: 2.3460
[09/26 11:22:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 58.50	
[09/26 11:22:33 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[09/26 11:22:39 visual_prompt]: Epoch 30 / 100: avg data time: 5.22e-02, avg batch time: 0.4651, average train loss: 2.2980
[09/26 11:22:41 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.1584, average loss: 2.2626
[09/26 11:22:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 11:22:41 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[09/26 11:22:47 visual_prompt]: Epoch 31 / 100: avg data time: 5.70e-02, avg batch time: 0.4701, average train loss: 2.2822
[09/26 11:22:49 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1585, average loss: 2.2315
[09/26 11:22:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 11:22:49 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[09/26 11:22:56 visual_prompt]: Epoch 32 / 100: avg data time: 6.70e-02, avg batch time: 0.4779, average train loss: 2.2832
[09/26 11:22:57 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1581, average loss: 2.2790
[09/26 11:22:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 11:22:57 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[09/26 11:23:04 visual_prompt]: Epoch 33 / 100: avg data time: 5.66e-02, avg batch time: 0.4694, average train loss: 2.2833
[09/26 11:23:05 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1582, average loss: 2.3130
[09/26 11:23:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 11:23:05 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[09/26 11:23:12 visual_prompt]: Epoch 34 / 100: avg data time: 6.07e-02, avg batch time: 0.4724, average train loss: 2.2776
[09/26 11:23:13 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1584, average loss: 2.2595
[09/26 11:23:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:23:13 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[09/26 11:23:20 visual_prompt]: Epoch 35 / 100: avg data time: 5.38e-02, avg batch time: 0.4661, average train loss: 2.2920
[09/26 11:23:21 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1584, average loss: 2.3251
[09/26 11:23:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 11:23:21 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[09/26 11:23:28 visual_prompt]: Epoch 36 / 100: avg data time: 5.28e-02, avg batch time: 0.4651, average train loss: 2.3224
[09/26 11:23:29 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1582, average loss: 2.2612
[09/26 11:23:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 11:23:29 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[09/26 11:23:36 visual_prompt]: Epoch 37 / 100: avg data time: 6.23e-02, avg batch time: 0.4738, average train loss: 2.2725
[09/26 11:23:37 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.1579, average loss: 2.2304
[09/26 11:23:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 11:23:37 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[09/26 11:23:44 visual_prompt]: Epoch 38 / 100: avg data time: 5.40e-02, avg batch time: 0.4671, average train loss: 2.2809
[09/26 11:23:45 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 2.2274
[09/26 11:23:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 11:23:45 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[09/26 11:23:52 visual_prompt]: Epoch 39 / 100: avg data time: 5.80e-02, avg batch time: 0.4695, average train loss: 2.2709
[09/26 11:23:53 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1582, average loss: 2.2376
[09/26 11:23:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 11:23:53 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[09/26 11:24:00 visual_prompt]: Epoch 40 / 100: avg data time: 6.26e-02, avg batch time: 0.4737, average train loss: 2.2659
[09/26 11:24:01 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1582, average loss: 2.2629
[09/26 11:24:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 11:24:01 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[09/26 11:24:08 visual_prompt]: Epoch 41 / 100: avg data time: 5.20e-02, avg batch time: 0.4638, average train loss: 2.2959
[09/26 11:24:09 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1580, average loss: 2.2496
[09/26 11:24:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 11:24:09 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[09/26 11:24:16 visual_prompt]: Epoch 42 / 100: avg data time: 5.92e-02, avg batch time: 0.4722, average train loss: 2.2694
[09/26 11:24:17 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1579, average loss: 2.2805
[09/26 11:24:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 11:24:17 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[09/26 11:24:24 visual_prompt]: Epoch 43 / 100: avg data time: 6.32e-02, avg batch time: 0.4742, average train loss: 2.3047
[09/26 11:24:25 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1580, average loss: 2.2648
[09/26 11:24:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 11:24:25 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[09/26 11:24:32 visual_prompt]: Epoch 44 / 100: avg data time: 6.16e-02, avg batch time: 0.4734, average train loss: 2.3042
[09/26 11:24:33 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1582, average loss: 2.2626
[09/26 11:24:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 64.50	
[09/26 11:24:33 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[09/26 11:24:40 visual_prompt]: Epoch 45 / 100: avg data time: 4.68e-02, avg batch time: 0.4599, average train loss: 2.2997
[09/26 11:24:41 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1583, average loss: 2.2558
[09/26 11:24:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 11:24:41 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[09/26 11:24:47 visual_prompt]: Epoch 46 / 100: avg data time: 4.26e-02, avg batch time: 0.4558, average train loss: 2.2906
[09/26 11:24:49 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1580, average loss: 2.2406
[09/26 11:24:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 11:24:49 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[09/26 11:24:55 visual_prompt]: Epoch 47 / 100: avg data time: 5.92e-02, avg batch time: 0.4704, average train loss: 2.2691
[09/26 11:24:57 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1579, average loss: 2.2690
[09/26 11:24:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.00	
[09/26 11:24:57 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[09/26 11:25:03 visual_prompt]: Epoch 48 / 100: avg data time: 6.47e-02, avg batch time: 0.4752, average train loss: 2.2698
[09/26 11:25:05 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1574, average loss: 2.2845
[09/26 11:25:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 63.50	
[09/26 11:25:05 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[09/26 11:25:12 visual_prompt]: Epoch 49 / 100: avg data time: 6.61e-02, avg batch time: 0.4776, average train loss: 2.2691
[09/26 11:25:13 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1573, average loss: 2.2458
[09/26 11:25:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 11:25:13 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[09/26 11:25:20 visual_prompt]: Epoch 50 / 100: avg data time: 6.12e-02, avg batch time: 0.4732, average train loss: 2.2573
[09/26 11:25:21 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1581, average loss: 2.2361
[09/26 11:25:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:25:21 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[09/26 11:25:28 visual_prompt]: Epoch 51 / 100: avg data time: 6.11e-02, avg batch time: 0.4718, average train loss: 2.2644
[09/26 11:25:29 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1580, average loss: 2.2328
[09/26 11:25:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 11:25:29 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[09/26 11:25:36 visual_prompt]: Epoch 52 / 100: avg data time: 6.71e-02, avg batch time: 0.4779, average train loss: 2.2577
[09/26 11:25:37 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1581, average loss: 2.2369
[09/26 11:25:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 11:25:37 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[09/26 11:25:44 visual_prompt]: Epoch 53 / 100: avg data time: 7.31e-02, avg batch time: 0.4838, average train loss: 2.2500
[09/26 11:25:45 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.1573, average loss: 2.2371
[09/26 11:25:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 11:25:45 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[09/26 11:25:52 visual_prompt]: Epoch 54 / 100: avg data time: 5.67e-02, avg batch time: 0.4670, average train loss: 2.2671
[09/26 11:25:53 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1580, average loss: 2.2317
[09/26 11:25:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:25:53 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[09/26 11:26:00 visual_prompt]: Epoch 55 / 100: avg data time: 5.22e-02, avg batch time: 0.4637, average train loss: 2.2557
[09/26 11:26:01 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1581, average loss: 2.2349
[09/26 11:26:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 11:26:01 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[09/26 11:26:08 visual_prompt]: Epoch 56 / 100: avg data time: 5.01e-02, avg batch time: 0.4634, average train loss: 2.2614
[09/26 11:26:09 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1581, average loss: 2.2193
[09/26 11:26:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:26:09 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[09/26 11:26:16 visual_prompt]: Epoch 57 / 100: avg data time: 5.41e-02, avg batch time: 0.4675, average train loss: 2.2553
[09/26 11:26:17 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1584, average loss: 2.2457
[09/26 11:26:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 11:26:17 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[09/26 11:26:24 visual_prompt]: Epoch 58 / 100: avg data time: 5.33e-02, avg batch time: 0.4676, average train loss: 2.2598
[09/26 11:26:25 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1585, average loss: 2.2201
[09/26 11:26:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:26:25 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[09/26 11:26:32 visual_prompt]: Epoch 59 / 100: avg data time: 5.85e-02, avg batch time: 0.4696, average train loss: 2.2551
[09/26 11:26:33 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1580, average loss: 2.2295
[09/26 11:26:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:26:33 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[09/26 11:26:40 visual_prompt]: Epoch 60 / 100: avg data time: 6.74e-02, avg batch time: 0.4785, average train loss: 2.2522
[09/26 11:26:41 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1579, average loss: 2.2161
[09/26 11:26:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:26:41 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[09/26 11:26:48 visual_prompt]: Epoch 61 / 100: avg data time: 6.13e-02, avg batch time: 0.4736, average train loss: 2.2494
[09/26 11:26:49 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.1576, average loss: 2.2272
[09/26 11:26:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:26:49 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[09/26 11:26:56 visual_prompt]: Epoch 62 / 100: avg data time: 5.80e-02, avg batch time: 0.4694, average train loss: 2.2527
[09/26 11:26:57 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1582, average loss: 2.2220
[09/26 11:26:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 11:26:57 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[09/26 11:27:04 visual_prompt]: Epoch 63 / 100: avg data time: 6.06e-02, avg batch time: 0.4713, average train loss: 2.2525
[09/26 11:27:06 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1582, average loss: 2.2312
[09/26 11:27:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:27:06 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[09/26 11:27:12 visual_prompt]: Epoch 64 / 100: avg data time: 5.19e-02, avg batch time: 0.4643, average train loss: 2.2506
[09/26 11:27:13 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1582, average loss: 2.2170
[09/26 11:27:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:27:13 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[09/26 11:27:20 visual_prompt]: Epoch 65 / 100: avg data time: 6.24e-02, avg batch time: 0.4733, average train loss: 2.2508
[09/26 11:27:22 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1584, average loss: 2.2144
[09/26 11:27:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:27:22 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[09/26 11:27:28 visual_prompt]: Epoch 66 / 100: avg data time: 5.87e-02, avg batch time: 0.4700, average train loss: 2.2453
[09/26 11:27:29 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1581, average loss: 2.2470
[09/26 11:27:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 11:27:29 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[09/26 11:27:36 visual_prompt]: Epoch 67 / 100: avg data time: 5.95e-02, avg batch time: 0.4704, average train loss: 2.2566
[09/26 11:27:38 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1579, average loss: 2.2192
[09/26 11:27:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 11:27:38 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[09/26 11:27:44 visual_prompt]: Epoch 68 / 100: avg data time: 6.30e-02, avg batch time: 0.4752, average train loss: 2.2545
[09/26 11:27:45 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1583, average loss: 2.2144
[09/26 11:27:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:27:45 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[09/26 11:27:52 visual_prompt]: Epoch 69 / 100: avg data time: 5.95e-02, avg batch time: 0.4705, average train loss: 2.2507
[09/26 11:27:54 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1582, average loss: 2.2186
[09/26 11:27:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:27:54 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[09/26 11:28:00 visual_prompt]: Epoch 70 / 100: avg data time: 6.29e-02, avg batch time: 0.4746, average train loss: 2.2432
[09/26 11:28:02 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1579, average loss: 2.2174
[09/26 11:28:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:28:02 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[09/26 11:28:08 visual_prompt]: Epoch 71 / 100: avg data time: 5.88e-02, avg batch time: 0.4701, average train loss: 2.2474
[09/26 11:28:10 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.1581, average loss: 2.2235
[09/26 11:28:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:28:10 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[09/26 11:28:16 visual_prompt]: Epoch 72 / 100: avg data time: 6.07e-02, avg batch time: 0.4727, average train loss: 2.2429
[09/26 11:28:18 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1582, average loss: 2.2178
[09/26 11:28:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 11:28:18 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[09/26 11:28:24 visual_prompt]: Epoch 73 / 100: avg data time: 5.52e-02, avg batch time: 0.4666, average train loss: 2.2450
[09/26 11:28:26 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1586, average loss: 2.2190
[09/26 11:28:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:28:26 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[09/26 11:28:32 visual_prompt]: Epoch 74 / 100: avg data time: 6.17e-02, avg batch time: 0.4732, average train loss: 2.2410
[09/26 11:28:34 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.1577, average loss: 2.2175
[09/26 11:28:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:28:34 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[09/26 11:28:40 visual_prompt]: Epoch 75 / 100: avg data time: 6.03e-02, avg batch time: 0.4716, average train loss: 2.2442
[09/26 11:28:42 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1583, average loss: 2.2198
[09/26 11:28:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:28:42 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[09/26 11:28:48 visual_prompt]: Epoch 76 / 100: avg data time: 6.38e-02, avg batch time: 0.4761, average train loss: 2.2369
[09/26 11:28:50 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1581, average loss: 2.2146
[09/26 11:28:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 11:28:50 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[09/26 11:28:56 visual_prompt]: Epoch 77 / 100: avg data time: 6.02e-02, avg batch time: 0.4713, average train loss: 2.2356
[09/26 11:28:58 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1581, average loss: 2.2187
[09/26 11:28:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:28:58 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[09/26 11:29:04 visual_prompt]: Epoch 78 / 100: avg data time: 5.20e-02, avg batch time: 0.4644, average train loss: 2.2376
[09/26 11:29:06 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.1582, average loss: 2.2148
[09/26 11:29:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:29:06 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[09/26 11:29:12 visual_prompt]: Epoch 79 / 100: avg data time: 5.72e-02, avg batch time: 0.4688, average train loss: 2.2381
[09/26 11:29:14 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1583, average loss: 2.2164
[09/26 11:29:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:29:14 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[09/26 11:29:20 visual_prompt]: Epoch 80 / 100: avg data time: 6.36e-02, avg batch time: 0.4756, average train loss: 2.2340
[09/26 11:29:22 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1587, average loss: 2.2153
[09/26 11:29:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:29:22 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[09/26 11:29:28 visual_prompt]: Epoch 81 / 100: avg data time: 5.44e-02, avg batch time: 0.4675, average train loss: 2.2340
[09/26 11:29:30 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1585, average loss: 2.2156
[09/26 11:29:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:29:30 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[09/26 11:29:36 visual_prompt]: Epoch 82 / 100: avg data time: 6.62e-02, avg batch time: 0.4784, average train loss: 2.2360
[09/26 11:29:38 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1582, average loss: 2.2147
[09/26 11:29:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:29:38 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[09/26 11:29:44 visual_prompt]: Epoch 83 / 100: avg data time: 5.94e-02, avg batch time: 0.4713, average train loss: 2.2333
[09/26 11:29:46 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1583, average loss: 2.2177
[09/26 11:29:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:29:46 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[09/26 11:29:52 visual_prompt]: Epoch 84 / 100: avg data time: 4.86e-02, avg batch time: 0.4605, average train loss: 2.2317
[09/26 11:29:54 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1578, average loss: 2.2145
[09/26 11:29:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.50	
[09/26 11:29:54 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[09/26 11:30:00 visual_prompt]: Epoch 85 / 100: avg data time: 6.49e-02, avg batch time: 0.4780, average train loss: 2.2311
[09/26 11:30:02 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1581, average loss: 2.2145
[09/26 11:30:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:30:02 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[09/26 11:30:08 visual_prompt]: Epoch 86 / 100: avg data time: 5.67e-02, avg batch time: 0.4690, average train loss: 2.2318
[09/26 11:30:10 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1582, average loss: 2.2143
[09/26 11:30:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.00	
[09/26 11:30:10 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[09/26 11:30:16 visual_prompt]: Epoch 87 / 100: avg data time: 6.25e-02, avg batch time: 0.4744, average train loss: 2.2297
[09/26 11:30:18 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1580, average loss: 2.2136
[09/26 11:30:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 66.00	
[09/26 11:30:18 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[09/26 11:30:24 visual_prompt]: Epoch 88 / 100: avg data time: 5.45e-02, avg batch time: 0.4675, average train loss: 2.2278
[09/26 11:30:26 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1582, average loss: 2.2133
[09/26 11:30:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:30:26 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[09/26 11:30:32 visual_prompt]: Epoch 89 / 100: avg data time: 6.39e-02, avg batch time: 0.4766, average train loss: 2.2277
[09/26 11:30:34 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1582, average loss: 2.2114
[09/26 11:30:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:30:34 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[09/26 11:30:40 visual_prompt]: Epoch 90 / 100: avg data time: 5.85e-02, avg batch time: 0.4709, average train loss: 2.2292
[09/26 11:30:42 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 2.2126
[09/26 11:30:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:30:42 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[09/26 11:30:48 visual_prompt]: Epoch 91 / 100: avg data time: 4.23e-02, avg batch time: 0.4555, average train loss: 2.2324
[09/26 11:30:50 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.1584, average loss: 2.2184
[09/26 11:30:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:30:50 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[09/26 11:30:56 visual_prompt]: Epoch 92 / 100: avg data time: 5.34e-02, avg batch time: 0.4662, average train loss: 2.2308
[09/26 11:30:58 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1586, average loss: 2.2141
[09/26 11:30:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:30:58 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[09/26 11:31:04 visual_prompt]: Epoch 93 / 100: avg data time: 4.82e-02, avg batch time: 0.4614, average train loss: 2.2301
[09/26 11:31:06 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1580, average loss: 2.2122
[09/26 11:31:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:31:06 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[09/26 11:31:12 visual_prompt]: Epoch 94 / 100: avg data time: 4.93e-02, avg batch time: 0.4614, average train loss: 2.2288
[09/26 11:31:14 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1582, average loss: 2.2127
[09/26 11:31:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:31:14 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[09/26 11:31:20 visual_prompt]: Epoch 95 / 100: avg data time: 4.76e-02, avg batch time: 0.4595, average train loss: 2.2275
[09/26 11:31:21 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1580, average loss: 2.2123
[09/26 11:31:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:31:21 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[09/26 11:31:28 visual_prompt]: Epoch 96 / 100: avg data time: 6.26e-02, avg batch time: 0.4747, average train loss: 2.2265
[09/26 11:31:29 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1584, average loss: 2.2115
[09/26 11:31:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 11:31:29 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[09/26 11:31:36 visual_prompt]: Epoch 97 / 100: avg data time: 5.85e-02, avg batch time: 0.4700, average train loss: 2.2267
[09/26 11:31:37 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1580, average loss: 2.2114
[09/26 11:31:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:31:37 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[09/26 11:31:44 visual_prompt]: Epoch 98 / 100: avg data time: 5.68e-02, avg batch time: 0.4702, average train loss: 2.2255
[09/26 11:31:45 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1583, average loss: 2.2115
[09/26 11:31:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 11:31:45 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[09/26 11:31:52 visual_prompt]: Epoch 99 / 100: avg data time: 6.39e-02, avg batch time: 0.4753, average train loss: 2.2252
[09/26 11:31:54 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1580, average loss: 2.2113
[09/26 11:31:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:31:54 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[09/26 11:32:00 visual_prompt]: Epoch 100 / 100: avg data time: 5.07e-02, avg batch time: 0.4637, average train loss: 2.2248
[09/26 11:32:02 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1579, average loss: 2.2113
[09/26 11:32:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.00	
[09/26 11:32:02 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 11:32:02 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 11:32:02 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 11:32:02 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 11:32:02 visual_prompt]: Training with config:
[09/26 11:32:02 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr1.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 11:32:02 visual_prompt]: Loading training data...
[09/26 11:32:02 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 11:32:02 visual_prompt]: Number of images: 800
[09/26 11:32:02 visual_prompt]: Number of classes: 10 / 10
[09/26 11:32:02 visual_prompt]: Loading validation data...
[09/26 11:32:02 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 11:32:03 visual_prompt]: Number of images: 200
[09/26 11:32:03 visual_prompt]: Number of classes: 10 / 10
[09/26 11:32:03 visual_prompt]: Constructing models...
[09/26 11:32:05 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 11:32:05 visual_prompt]: tuned percent:0.543
[09/26 11:32:05 visual_prompt]: Device used for model: 0
[09/26 11:32:05 visual_prompt]: Setting up Evaluator...
[09/26 11:32:05 visual_prompt]: Setting up Trainer...
[09/26 11:32:05 visual_prompt]: 	Setting up the optimizer...
[09/26 11:32:05 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 11:32:12 visual_prompt]: Epoch 1 / 100: avg data time: 5.74e-02, avg batch time: 0.4750, average train loss: 2.6751
[09/26 11:32:13 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1585, average loss: 2.6214
[09/26 11:32:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 11:32:13 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 11:32:13 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/26 11:32:20 visual_prompt]: Epoch 2 / 100: avg data time: 5.83e-02, avg batch time: 0.4714, average train loss: 2.9606
[09/26 11:32:22 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1583, average loss: 2.2261
[09/26 11:32:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:32:22 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/26 11:32:28 visual_prompt]: Epoch 3 / 100: avg data time: 5.54e-02, avg batch time: 0.4681, average train loss: 2.3626
[09/26 11:32:29 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1582, average loss: 2.2878
[09/26 11:32:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 11:32:29 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/26 11:32:36 visual_prompt]: Epoch 4 / 100: avg data time: 5.76e-02, avg batch time: 0.4693, average train loss: 2.2794
[09/26 11:32:37 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1576, average loss: 2.2719
[09/26 11:32:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 63.50	
[09/26 11:32:37 visual_prompt]: Best epoch 4: best metric: 0.235
[09/26 11:32:37 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/26 11:32:44 visual_prompt]: Epoch 5 / 100: avg data time: 6.46e-02, avg batch time: 0.4760, average train loss: 2.2949
[09/26 11:32:46 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.1579, average loss: 2.2361
[09/26 11:32:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 63.50	
[09/26 11:32:46 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/26 11:32:52 visual_prompt]: Epoch 6 / 100: avg data time: 4.65e-02, avg batch time: 0.4584, average train loss: 2.2765
[09/26 11:32:54 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1584, average loss: 2.2690
[09/26 11:32:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 55.50	
[09/26 11:32:54 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/26 11:33:00 visual_prompt]: Epoch 7 / 100: avg data time: 5.89e-02, avg batch time: 0.4719, average train loss: 2.2636
[09/26 11:33:02 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1575, average loss: 2.1836
[09/26 11:33:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 26.50	top5: 68.50	
[09/26 11:33:02 visual_prompt]: Best epoch 7: best metric: 0.265
[09/26 11:33:02 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/26 11:33:08 visual_prompt]: Epoch 8 / 100: avg data time: 4.79e-02, avg batch time: 0.4639, average train loss: 2.3069
[09/26 11:33:10 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1582, average loss: 2.3091
[09/26 11:33:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 13.50	top5: 60.00	
[09/26 11:33:10 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/26 11:33:16 visual_prompt]: Epoch 9 / 100: avg data time: 6.29e-02, avg batch time: 0.4756, average train loss: 2.3693
[09/26 11:33:18 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1587, average loss: 2.1948
[09/26 11:33:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.00	top5: 65.00	
[09/26 11:33:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/26 11:33:24 visual_prompt]: Epoch 10 / 100: avg data time: 5.92e-02, avg batch time: 0.4719, average train loss: 2.1146
[09/26 11:33:26 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1584, average loss: 1.7803
[09/26 11:33:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 36.50	top5: 79.50	
[09/26 11:33:26 visual_prompt]: Best epoch 10: best metric: 0.365
[09/26 11:33:26 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/26 11:33:32 visual_prompt]: Epoch 11 / 100: avg data time: 5.91e-02, avg batch time: 0.4722, average train loss: 2.3010
[09/26 11:33:34 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1584, average loss: 2.2078
[09/26 11:33:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 11:33:34 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/26 11:33:40 visual_prompt]: Epoch 12 / 100: avg data time: 5.28e-02, avg batch time: 0.4671, average train loss: 2.1176
[09/26 11:33:42 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1579, average loss: 2.1166
[09/26 11:33:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 32.50	top5: 78.50	
[09/26 11:33:42 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/26 11:33:48 visual_prompt]: Epoch 13 / 100: avg data time: 6.20e-02, avg batch time: 0.4745, average train loss: 2.3074
[09/26 11:33:50 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1585, average loss: 2.1040
[09/26 11:33:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 74.50	
[09/26 11:33:50 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/26 11:33:56 visual_prompt]: Epoch 14 / 100: avg data time: 5.94e-02, avg batch time: 0.4724, average train loss: 2.0148
[09/26 11:33:58 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1580, average loss: 1.6562
[09/26 11:33:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 34.50	top5: 87.00	
[09/26 11:33:58 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/26 11:34:04 visual_prompt]: Epoch 15 / 100: avg data time: 6.74e-02, avg batch time: 0.4796, average train loss: 1.7970
[09/26 11:34:06 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1586, average loss: 1.6003
[09/26 11:34:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 41.00	top5: 93.00	
[09/26 11:34:06 visual_prompt]: Best epoch 15: best metric: 0.410
[09/26 11:34:06 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/26 11:34:12 visual_prompt]: Epoch 16 / 100: avg data time: 5.16e-02, avg batch time: 0.4670, average train loss: 1.7839
[09/26 11:34:14 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1580, average loss: 1.8313
[09/26 11:34:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 44.50	top5: 78.50	
[09/26 11:34:14 visual_prompt]: Best epoch 16: best metric: 0.445
[09/26 11:34:14 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/26 11:34:20 visual_prompt]: Epoch 17 / 100: avg data time: 6.36e-02, avg batch time: 0.4759, average train loss: 1.5325
[09/26 11:34:22 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1580, average loss: 1.5753
[09/26 11:34:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 43.00	top5: 89.50	
[09/26 11:34:22 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/26 11:34:29 visual_prompt]: Epoch 18 / 100: avg data time: 6.28e-02, avg batch time: 0.4759, average train loss: 1.3992
[09/26 11:34:30 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1584, average loss: 1.4121
[09/26 11:34:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 50.50	top5: 95.50	
[09/26 11:34:30 visual_prompt]: Best epoch 18: best metric: 0.505
[09/26 11:34:30 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/26 11:34:36 visual_prompt]: Epoch 19 / 100: avg data time: 5.20e-02, avg batch time: 0.4646, average train loss: 1.2011
[09/26 11:34:38 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1585, average loss: 0.9808
[09/26 11:34:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.50	top5: 97.00	
[09/26 11:34:38 visual_prompt]: Best epoch 19: best metric: 0.655
[09/26 11:34:38 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/26 11:34:45 visual_prompt]: Epoch 20 / 100: avg data time: 6.30e-02, avg batch time: 0.4760, average train loss: 1.1264
[09/26 11:34:46 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1583, average loss: 1.4915
[09/26 11:34:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 52.00	top5: 89.50	
[09/26 11:34:46 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/26 11:34:53 visual_prompt]: Epoch 21 / 100: avg data time: 5.32e-02, avg batch time: 0.4677, average train loss: 0.9886
[09/26 11:34:54 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1584, average loss: 0.8822
[09/26 11:34:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 64.50	top5: 97.00	
[09/26 11:34:54 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/26 11:35:01 visual_prompt]: Epoch 22 / 100: avg data time: 6.52e-02, avg batch time: 0.4773, average train loss: 0.8369
[09/26 11:35:02 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1583, average loss: 0.8665
[09/26 11:35:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.50	top5: 97.00	
[09/26 11:35:02 visual_prompt]: Best epoch 22: best metric: 0.675
[09/26 11:35:02 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/26 11:35:09 visual_prompt]: Epoch 23 / 100: avg data time: 6.23e-02, avg batch time: 0.4758, average train loss: 0.7556
[09/26 11:35:10 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.1584, average loss: 0.8449
[09/26 11:35:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 96.50	
[09/26 11:35:10 visual_prompt]: Best epoch 23: best metric: 0.710
[09/26 11:35:10 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/26 11:35:17 visual_prompt]: Epoch 24 / 100: avg data time: 5.10e-02, avg batch time: 0.4658, average train loss: 0.7118
[09/26 11:35:18 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.1582, average loss: 0.8173
[09/26 11:35:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 96.00	
[09/26 11:35:18 visual_prompt]: Best epoch 24: best metric: 0.730
[09/26 11:35:18 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/26 11:35:25 visual_prompt]: Epoch 25 / 100: avg data time: 5.38e-02, avg batch time: 0.4671, average train loss: 1.0473
[09/26 11:35:26 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1585, average loss: 1.2085
[09/26 11:35:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 61.00	top5: 94.00	
[09/26 11:35:26 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/26 11:35:33 visual_prompt]: Epoch 26 / 100: avg data time: 5.49e-02, avg batch time: 0.4684, average train loss: 1.0015
[09/26 11:35:34 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1582, average loss: 1.1927
[09/26 11:35:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 63.00	top5: 95.50	
[09/26 11:35:34 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/26 11:35:41 visual_prompt]: Epoch 27 / 100: avg data time: 5.89e-02, avg batch time: 0.4719, average train loss: 0.6736
[09/26 11:35:42 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1580, average loss: 0.5469
[09/26 11:35:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 97.50	
[09/26 11:35:42 visual_prompt]: Best epoch 27: best metric: 0.850
[09/26 11:35:42 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[09/26 11:35:49 visual_prompt]: Epoch 28 / 100: avg data time: 5.07e-02, avg batch time: 0.4649, average train loss: 0.5249
[09/26 11:35:50 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1581, average loss: 0.6226
[09/26 11:35:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.50	top5: 97.50	
[09/26 11:35:50 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[09/26 11:35:57 visual_prompt]: Epoch 29 / 100: avg data time: 6.41e-02, avg batch time: 0.4773, average train loss: 0.4839
[09/26 11:35:58 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1585, average loss: 0.6105
[09/26 11:35:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 98.00	
[09/26 11:35:58 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[09/26 11:36:05 visual_prompt]: Epoch 30 / 100: avg data time: 6.05e-02, avg batch time: 0.4743, average train loss: 0.3755
[09/26 11:36:06 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1580, average loss: 0.7224
[09/26 11:36:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 97.50	
[09/26 11:36:06 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[09/26 11:36:13 visual_prompt]: Epoch 31 / 100: avg data time: 6.29e-02, avg batch time: 0.4757, average train loss: 0.4278
[09/26 11:36:15 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.1578, average loss: 0.8437
[09/26 11:36:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.50	top5: 97.00	
[09/26 11:36:15 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[09/26 11:36:21 visual_prompt]: Epoch 32 / 100: avg data time: 5.74e-02, avg batch time: 0.4705, average train loss: 0.4287
[09/26 11:36:23 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1585, average loss: 0.5469
[09/26 11:36:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 11:36:23 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[09/26 11:36:29 visual_prompt]: Epoch 33 / 100: avg data time: 6.32e-02, avg batch time: 0.4765, average train loss: 0.2817
[09/26 11:36:31 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1582, average loss: 0.6383
[09/26 11:36:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 98.50	
[09/26 11:36:31 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[09/26 11:36:37 visual_prompt]: Epoch 34 / 100: avg data time: 5.51e-02, avg batch time: 0.4694, average train loss: 0.2953
[09/26 11:36:39 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.1584, average loss: 0.7264
[09/26 11:36:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.50	
[09/26 11:36:39 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[09/26 11:36:45 visual_prompt]: Epoch 35 / 100: avg data time: 5.82e-02, avg batch time: 0.4705, average train loss: 0.2401
[09/26 11:36:47 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1585, average loss: 0.5378
[09/26 11:36:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 96.50	
[09/26 11:36:47 visual_prompt]: Best epoch 35: best metric: 0.860
[09/26 11:36:47 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[09/26 11:36:53 visual_prompt]: Epoch 36 / 100: avg data time: 5.38e-02, avg batch time: 0.4686, average train loss: 0.2498
[09/26 11:36:55 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.1582, average loss: 0.5745
[09/26 11:36:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.50	
[09/26 11:36:55 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[09/26 11:37:01 visual_prompt]: Epoch 37 / 100: avg data time: 6.12e-02, avg batch time: 0.4741, average train loss: 0.1274
[09/26 11:37:03 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1582, average loss: 0.6044
[09/26 11:37:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 98.50	
[09/26 11:37:03 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[09/26 11:37:09 visual_prompt]: Epoch 38 / 100: avg data time: 6.04e-02, avg batch time: 0.4737, average train loss: 0.2563
[09/26 11:37:11 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1582, average loss: 0.7755
[09/26 11:37:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 97.00	
[09/26 11:37:11 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[09/26 11:37:17 visual_prompt]: Epoch 39 / 100: avg data time: 6.02e-02, avg batch time: 0.4735, average train loss: 0.2845
[09/26 11:37:19 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1582, average loss: 0.9231
[09/26 11:37:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 11:37:19 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[09/26 11:37:26 visual_prompt]: Epoch 40 / 100: avg data time: 6.53e-02, avg batch time: 0.4774, average train loss: 0.2938
[09/26 11:37:27 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1581, average loss: 0.5652
[09/26 11:37:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 11:37:27 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[09/26 11:37:34 visual_prompt]: Epoch 41 / 100: avg data time: 5.56e-02, avg batch time: 0.4687, average train loss: 0.1268
[09/26 11:37:35 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1581, average loss: 0.5130
[09/26 11:37:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 97.50	
[09/26 11:37:35 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[09/26 11:37:42 visual_prompt]: Epoch 42 / 100: avg data time: 5.92e-02, avg batch time: 0.4729, average train loss: 0.0595
[09/26 11:37:43 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1580, average loss: 0.6012
[09/26 11:37:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 97.50	
[09/26 11:37:43 visual_prompt]: Best epoch 42: best metric: 0.875
[09/26 11:37:43 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[09/26 11:37:49 visual_prompt]: Epoch 43 / 100: avg data time: 4.74e-02, avg batch time: 0.4631, average train loss: 0.0432
[09/26 11:37:51 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1580, average loss: 0.7188
[09/26 11:37:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.50	top5: 97.00	
[09/26 11:37:51 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[09/26 11:37:58 visual_prompt]: Epoch 44 / 100: avg data time: 6.12e-02, avg batch time: 0.4745, average train loss: 0.0164
[09/26 11:37:59 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1581, average loss: 0.7039
[09/26 11:37:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 97.00	
[09/26 11:37:59 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[09/26 11:38:06 visual_prompt]: Epoch 45 / 100: avg data time: 6.77e-02, avg batch time: 0.4800, average train loss: 0.0198
[09/26 11:38:07 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1579, average loss: 1.0614
[09/26 11:38:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 97.00	
[09/26 11:38:07 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[09/26 11:38:14 visual_prompt]: Epoch 46 / 100: avg data time: 6.06e-02, avg batch time: 0.4731, average train loss: 0.0262
[09/26 11:38:15 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1580, average loss: 1.0149
[09/26 11:38:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 98.00	
[09/26 11:38:15 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[09/26 11:38:22 visual_prompt]: Epoch 47 / 100: avg data time: 6.15e-02, avg batch time: 0.4739, average train loss: 0.0274
[09/26 11:38:23 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1584, average loss: 1.0113
[09/26 11:38:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 98.00	
[09/26 11:38:23 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[09/26 11:38:30 visual_prompt]: Epoch 48 / 100: avg data time: 6.15e-02, avg batch time: 0.4754, average train loss: 0.0249
[09/26 11:38:31 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1582, average loss: 1.0232
[09/26 11:38:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 11:38:31 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[09/26 11:38:38 visual_prompt]: Epoch 49 / 100: avg data time: 5.56e-02, avg batch time: 0.4677, average train loss: 0.0309
[09/26 11:38:39 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1584, average loss: 0.8476
[09/26 11:38:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 97.50	
[09/26 11:38:39 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[09/26 11:38:46 visual_prompt]: Epoch 50 / 100: avg data time: 5.26e-02, avg batch time: 0.4658, average train loss: 0.0128
[09/26 11:38:47 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1580, average loss: 0.8519
[09/26 11:38:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 98.00	
[09/26 11:38:47 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[09/26 11:38:54 visual_prompt]: Epoch 51 / 100: avg data time: 6.43e-02, avg batch time: 0.4765, average train loss: 0.0210
[09/26 11:38:55 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1581, average loss: 0.8690
[09/26 11:38:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 97.00	
[09/26 11:38:55 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[09/26 11:39:02 visual_prompt]: Epoch 52 / 100: avg data time: 4.71e-02, avg batch time: 0.4606, average train loss: 0.0206
[09/26 11:39:03 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1584, average loss: 1.0414
[09/26 11:39:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 97.50	
[09/26 11:39:03 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[09/26 11:39:10 visual_prompt]: Epoch 53 / 100: avg data time: 5.56e-02, avg batch time: 0.4682, average train loss: 0.0169
[09/26 11:39:11 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1585, average loss: 1.0089
[09/26 11:39:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 97.00	
[09/26 11:39:11 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[09/26 11:39:18 visual_prompt]: Epoch 54 / 100: avg data time: 6.22e-02, avg batch time: 0.4749, average train loss: 0.0167
[09/26 11:39:19 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1586, average loss: 1.0565
[09/26 11:39:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 97.50	
[09/26 11:39:19 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[09/26 11:39:26 visual_prompt]: Epoch 55 / 100: avg data time: 6.36e-02, avg batch time: 0.4760, average train loss: 0.0167
[09/26 11:39:27 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1577, average loss: 0.9671
[09/26 11:39:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 97.00	
[09/26 11:39:27 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[09/26 11:39:34 visual_prompt]: Epoch 56 / 100: avg data time: 5.32e-02, avg batch time: 0.4674, average train loss: 0.0290
[09/26 11:39:35 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1580, average loss: 1.1768
[09/26 11:39:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 97.50	
[09/26 11:39:35 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[09/26 11:39:42 visual_prompt]: Epoch 57 / 100: avg data time: 5.52e-02, avg batch time: 0.4698, average train loss: 0.0263
[09/26 11:39:44 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 1.0474
[09/26 11:39:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 11:39:44 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[09/26 11:39:50 visual_prompt]: Epoch 58 / 100: avg data time: 5.67e-02, avg batch time: 0.4706, average train loss: 0.0184
[09/26 11:39:52 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1582, average loss: 0.9729
[09/26 11:39:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 98.50	
[09/26 11:39:52 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[09/26 11:39:58 visual_prompt]: Epoch 59 / 100: avg data time: 6.38e-02, avg batch time: 0.4765, average train loss: 0.0394
[09/26 11:40:00 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1585, average loss: 1.3593
[09/26 11:40:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.00	
[09/26 11:40:00 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[09/26 11:40:06 visual_prompt]: Epoch 60 / 100: avg data time: 5.57e-02, avg batch time: 0.4699, average train loss: 0.0383
[09/26 11:40:08 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1584, average loss: 0.8232
[09/26 11:40:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 99.00	
[09/26 11:40:08 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[09/26 11:40:14 visual_prompt]: Epoch 61 / 100: avg data time: 5.29e-02, avg batch time: 0.4667, average train loss: 0.0175
[09/26 11:40:16 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1579, average loss: 1.0011
[09/26 11:40:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 98.50	
[09/26 11:40:16 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[09/26 11:40:22 visual_prompt]: Epoch 62 / 100: avg data time: 5.97e-02, avg batch time: 0.4728, average train loss: 0.0113
[09/26 11:40:24 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1587, average loss: 0.8719
[09/26 11:40:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 98.00	
[09/26 11:40:24 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[09/26 11:40:30 visual_prompt]: Epoch 63 / 100: avg data time: 6.21e-02, avg batch time: 0.4750, average train loss: 0.0055
[09/26 11:40:32 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1583, average loss: 0.8358
[09/26 11:40:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 97.50	
[09/26 11:40:32 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[09/26 11:40:38 visual_prompt]: Epoch 64 / 100: avg data time: 5.69e-02, avg batch time: 0.4705, average train loss: 0.0064
[09/26 11:40:40 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1588, average loss: 1.0385
[09/26 11:40:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 98.00	
[09/26 11:40:40 visual_prompt]: Best epoch 64: best metric: 0.880
[09/26 11:40:40 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[09/26 11:40:46 visual_prompt]: Epoch 65 / 100: avg data time: 6.21e-02, avg batch time: 0.4753, average train loss: 0.0049
[09/26 11:40:48 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1579, average loss: 1.0606
[09/26 11:40:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 98.00	
[09/26 11:40:48 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[09/26 11:40:54 visual_prompt]: Epoch 66 / 100: avg data time: 6.19e-02, avg batch time: 0.4756, average train loss: 0.0030
[09/26 11:40:56 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1579, average loss: 0.9502
[09/26 11:40:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.50	top5: 98.00	
[09/26 11:40:56 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[09/26 11:41:03 visual_prompt]: Epoch 67 / 100: avg data time: 6.36e-02, avg batch time: 0.4765, average train loss: 0.0022
[09/26 11:41:04 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1580, average loss: 0.9629
[09/26 11:41:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:41:04 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[09/26 11:41:11 visual_prompt]: Epoch 68 / 100: avg data time: 5.13e-02, avg batch time: 0.4642, average train loss: 0.0006
[09/26 11:41:12 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1586, average loss: 1.1191
[09/26 11:41:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.50	top5: 97.50	
[09/26 11:41:12 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[09/26 11:41:19 visual_prompt]: Epoch 69 / 100: avg data time: 6.39e-02, avg batch time: 0.4787, average train loss: 0.0027
[09/26 11:41:20 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1575, average loss: 1.0273
[09/26 11:41:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.50	top5: 97.50	
[09/26 11:41:20 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[09/26 11:41:27 visual_prompt]: Epoch 70 / 100: avg data time: 5.11e-02, avg batch time: 0.4660, average train loss: 0.0010
[09/26 11:41:28 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1580, average loss: 0.9166
[09/26 11:41:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 98.00	
[09/26 11:41:28 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[09/26 11:41:35 visual_prompt]: Epoch 71 / 100: avg data time: 5.96e-02, avg batch time: 0.4732, average train loss: 0.0009
[09/26 11:41:36 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.1586, average loss: 0.9260
[09/26 11:41:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.50	
[09/26 11:41:36 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[09/26 11:41:43 visual_prompt]: Epoch 72 / 100: avg data time: 5.65e-02, avg batch time: 0.4697, average train loss: 0.0004
[09/26 11:41:44 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1588, average loss: 0.9337
[09/26 11:41:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:41:44 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[09/26 11:41:51 visual_prompt]: Epoch 73 / 100: avg data time: 4.78e-02, avg batch time: 0.4611, average train loss: 0.0006
[09/26 11:41:52 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1582, average loss: 0.9383
[09/26 11:41:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:41:52 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[09/26 11:41:59 visual_prompt]: Epoch 74 / 100: avg data time: 6.21e-02, avg batch time: 0.4772, average train loss: 0.0004
[09/26 11:42:00 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1584, average loss: 0.9378
[09/26 11:42:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:42:00 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[09/26 11:42:07 visual_prompt]: Epoch 75 / 100: avg data time: 5.24e-02, avg batch time: 0.4666, average train loss: 0.0004
[09/26 11:42:08 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1584, average loss: 0.9436
[09/26 11:42:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:42:08 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[09/26 11:42:15 visual_prompt]: Epoch 76 / 100: avg data time: 5.98e-02, avg batch time: 0.4736, average train loss: 0.0005
[09/26 11:42:16 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1582, average loss: 0.9466
[09/26 11:42:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:42:16 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[09/26 11:42:23 visual_prompt]: Epoch 77 / 100: avg data time: 6.12e-02, avg batch time: 0.4740, average train loss: 0.0003
[09/26 11:42:24 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1581, average loss: 0.9506
[09/26 11:42:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:42:24 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[09/26 11:42:31 visual_prompt]: Epoch 78 / 100: avg data time: 5.94e-02, avg batch time: 0.4734, average train loss: 0.0004
[09/26 11:42:32 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1585, average loss: 0.9480
[09/26 11:42:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:42:32 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[09/26 11:42:39 visual_prompt]: Epoch 79 / 100: avg data time: 6.58e-02, avg batch time: 0.4785, average train loss: 0.0003
[09/26 11:42:40 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.1584, average loss: 0.9484
[09/26 11:42:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:42:40 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[09/26 11:42:47 visual_prompt]: Epoch 80 / 100: avg data time: 6.00e-02, avg batch time: 0.4735, average train loss: 0.0006
[09/26 11:42:49 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1582, average loss: 0.9452
[09/26 11:42:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:42:49 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[09/26 11:42:55 visual_prompt]: Epoch 81 / 100: avg data time: 5.76e-02, avg batch time: 0.4707, average train loss: 0.0003
[09/26 11:42:57 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1579, average loss: 0.9164
[09/26 11:42:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 98.00	
[09/26 11:42:57 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[09/26 11:43:03 visual_prompt]: Epoch 82 / 100: avg data time: 5.22e-02, avg batch time: 0.4684, average train loss: 0.0007
[09/26 11:43:05 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1587, average loss: 0.9175
[09/26 11:43:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 98.00	
[09/26 11:43:05 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[09/26 11:43:11 visual_prompt]: Epoch 83 / 100: avg data time: 6.07e-02, avg batch time: 0.4732, average train loss: 0.0002
[09/26 11:43:13 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1588, average loss: 0.9245
[09/26 11:43:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 98.00	
[09/26 11:43:13 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[09/26 11:43:19 visual_prompt]: Epoch 84 / 100: avg data time: 6.20e-02, avg batch time: 0.4756, average train loss: 0.0004
[09/26 11:43:21 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1587, average loss: 0.9262
[09/26 11:43:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:43:21 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[09/26 11:43:27 visual_prompt]: Epoch 85 / 100: avg data time: 6.10e-02, avg batch time: 0.4739, average train loss: 0.0003
[09/26 11:43:29 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 0.9246
[09/26 11:43:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 98.00	
[09/26 11:43:29 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[09/26 11:43:35 visual_prompt]: Epoch 86 / 100: avg data time: 5.44e-02, avg batch time: 0.4691, average train loss: 0.0004
[09/26 11:43:37 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1585, average loss: 0.9265
[09/26 11:43:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:43:37 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[09/26 11:43:43 visual_prompt]: Epoch 87 / 100: avg data time: 5.40e-02, avg batch time: 0.4668, average train loss: 0.0004
[09/26 11:43:45 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1581, average loss: 0.9284
[09/26 11:43:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:43:45 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[09/26 11:43:51 visual_prompt]: Epoch 88 / 100: avg data time: 6.33e-02, avg batch time: 0.4761, average train loss: 0.0002
[09/26 11:43:53 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1581, average loss: 0.9291
[09/26 11:43:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:43:53 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[09/26 11:43:59 visual_prompt]: Epoch 89 / 100: avg data time: 5.50e-02, avg batch time: 0.4683, average train loss: 0.0005
[09/26 11:44:01 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.1584, average loss: 0.9295
[09/26 11:44:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:44:01 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[09/26 11:44:07 visual_prompt]: Epoch 90 / 100: avg data time: 5.85e-02, avg batch time: 0.4719, average train loss: 0.0004
[09/26 11:44:09 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1582, average loss: 0.9304
[09/26 11:44:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:44:09 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[09/26 11:44:15 visual_prompt]: Epoch 91 / 100: avg data time: 4.88e-02, avg batch time: 0.4625, average train loss: 0.0003
[09/26 11:44:16 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1583, average loss: 0.9308
[09/26 11:44:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:44:16 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[09/26 11:44:23 visual_prompt]: Epoch 92 / 100: avg data time: 5.93e-02, avg batch time: 0.4736, average train loss: 0.0002
[09/26 11:44:25 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1581, average loss: 0.9309
[09/26 11:44:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:44:25 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[09/26 11:44:31 visual_prompt]: Epoch 93 / 100: avg data time: 6.26e-02, avg batch time: 0.4748, average train loss: 0.0003
[09/26 11:44:33 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1581, average loss: 0.9311
[09/26 11:44:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:44:33 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[09/26 11:44:39 visual_prompt]: Epoch 94 / 100: avg data time: 4.90e-02, avg batch time: 0.4617, average train loss: 0.0002
[09/26 11:44:40 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1588, average loss: 0.9312
[09/26 11:44:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:44:40 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[09/26 11:44:47 visual_prompt]: Epoch 95 / 100: avg data time: 6.63e-02, avg batch time: 0.4788, average train loss: 0.0003
[09/26 11:44:49 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1581, average loss: 0.9313
[09/26 11:44:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:44:49 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[09/26 11:44:55 visual_prompt]: Epoch 96 / 100: avg data time: 6.32e-02, avg batch time: 0.4751, average train loss: 0.0004
[09/26 11:44:57 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1585, average loss: 0.9316
[09/26 11:44:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:44:57 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[09/26 11:45:03 visual_prompt]: Epoch 97 / 100: avg data time: 5.90e-02, avg batch time: 0.4719, average train loss: 0.0003
[09/26 11:45:05 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1583, average loss: 0.9317
[09/26 11:45:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:45:05 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[09/26 11:45:11 visual_prompt]: Epoch 98 / 100: avg data time: 6.17e-02, avg batch time: 0.4740, average train loss: 0.0005
[09/26 11:45:13 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1585, average loss: 0.9318
[09/26 11:45:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:45:13 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[09/26 11:45:19 visual_prompt]: Epoch 99 / 100: avg data time: 4.97e-02, avg batch time: 0.4621, average train loss: 0.0003
[09/26 11:45:21 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1585, average loss: 0.9319
[09/26 11:45:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:45:21 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[09/26 11:45:27 visual_prompt]: Epoch 100 / 100: avg data time: 6.44e-02, avg batch time: 0.4766, average train loss: 0.0002
[09/26 11:45:29 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1584, average loss: 0.9319
[09/26 11:45:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 11:45:29 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 11:45:29 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 11:45:29 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 11:45:29 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 11:45:29 visual_prompt]: Training with config:
[09/26 11:45:29 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr1.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 11:45:29 visual_prompt]: Loading training data...
[09/26 11:45:29 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 11:45:30 visual_prompt]: Number of images: 800
[09/26 11:45:30 visual_prompt]: Number of classes: 10 / 10
[09/26 11:45:30 visual_prompt]: Loading validation data...
[09/26 11:45:30 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 11:45:30 visual_prompt]: Number of images: 200
[09/26 11:45:30 visual_prompt]: Number of classes: 10 / 10
[09/26 11:45:30 visual_prompt]: Constructing models...
[09/26 11:45:32 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 11:45:32 visual_prompt]: tuned percent:0.543
[09/26 11:45:33 visual_prompt]: Device used for model: 0
[09/26 11:45:33 visual_prompt]: Setting up Evaluator...
[09/26 11:45:33 visual_prompt]: Setting up Trainer...
[09/26 11:45:33 visual_prompt]: 	Setting up the optimizer...
[09/26 11:45:33 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 11:45:39 visual_prompt]: Epoch 1 / 100: avg data time: 5.85e-02, avg batch time: 0.4757, average train loss: 2.6927
[09/26 11:45:41 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1577, average loss: 2.6214
[09/26 11:45:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 11:45:41 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 11:45:41 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/26 11:45:47 visual_prompt]: Epoch 2 / 100: avg data time: 4.66e-02, avg batch time: 0.4596, average train loss: 2.8701
[09/26 11:45:48 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1582, average loss: 2.2176
[09/26 11:45:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 11:45:48 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/26 11:45:55 visual_prompt]: Epoch 3 / 100: avg data time: 6.55e-02, avg batch time: 0.4774, average train loss: 2.3065
[09/26 11:45:56 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1585, average loss: 2.2445
[09/26 11:45:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 11:45:56 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/26 11:46:03 visual_prompt]: Epoch 4 / 100: avg data time: 5.19e-02, avg batch time: 0.4661, average train loss: 2.2994
[09/26 11:46:04 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1582, average loss: 2.2724
[09/26 11:46:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 11:46:04 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/26 11:46:11 visual_prompt]: Epoch 5 / 100: avg data time: 4.97e-02, avg batch time: 0.4630, average train loss: 2.3134
[09/26 11:46:12 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1578, average loss: 2.2461
[09/26 11:46:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 11:46:12 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/26 11:46:19 visual_prompt]: Epoch 6 / 100: avg data time: 6.17e-02, avg batch time: 0.4730, average train loss: 2.3037
[09/26 11:46:20 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1579, average loss: 2.3354
[09/26 11:46:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.50	top5: 52.00	
[09/26 11:46:20 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/26 11:46:27 visual_prompt]: Epoch 7 / 100: avg data time: 6.17e-02, avg batch time: 0.4739, average train loss: 2.2876
[09/26 11:46:29 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1580, average loss: 2.3082
[09/26 11:46:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 18.00	top5: 58.50	
[09/26 11:46:29 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/26 11:46:35 visual_prompt]: Epoch 8 / 100: avg data time: 6.10e-02, avg batch time: 0.4741, average train loss: 2.3014
[09/26 11:46:37 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 2.2742
[09/26 11:46:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 63.50	
[09/26 11:46:37 visual_prompt]: Best epoch 8: best metric: 0.245
[09/26 11:46:37 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/26 11:46:43 visual_prompt]: Epoch 9 / 100: avg data time: 5.70e-02, avg batch time: 0.4688, average train loss: 2.3478
[09/26 11:46:45 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1584, average loss: 2.3578
[09/26 11:46:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 11:46:45 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/26 11:46:51 visual_prompt]: Epoch 10 / 100: avg data time: 4.92e-02, avg batch time: 0.4620, average train loss: 2.3469
[09/26 11:46:52 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1584, average loss: 2.0773
[09/26 11:46:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 31.00	top5: 66.50	
[09/26 11:46:52 visual_prompt]: Best epoch 10: best metric: 0.310
[09/26 11:46:52 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/26 11:46:59 visual_prompt]: Epoch 11 / 100: avg data time: 6.48e-02, avg batch time: 0.4780, average train loss: 2.2311
[09/26 11:47:01 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1582, average loss: 2.0386
[09/26 11:47:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 31.50	top5: 83.50	
[09/26 11:47:01 visual_prompt]: Best epoch 11: best metric: 0.315
[09/26 11:47:01 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/26 11:47:07 visual_prompt]: Epoch 12 / 100: avg data time: 6.08e-02, avg batch time: 0.4732, average train loss: 2.0520
[09/26 11:47:09 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1583, average loss: 1.8835
[09/26 11:47:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 37.00	top5: 77.50	
[09/26 11:47:09 visual_prompt]: Best epoch 12: best metric: 0.370
[09/26 11:47:09 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/26 11:47:15 visual_prompt]: Epoch 13 / 100: avg data time: 5.77e-02, avg batch time: 0.4704, average train loss: 1.7921
[09/26 11:47:17 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1577, average loss: 1.7268
[09/26 11:47:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 43.50	top5: 84.50	
[09/26 11:47:17 visual_prompt]: Best epoch 13: best metric: 0.435
[09/26 11:47:17 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/26 11:47:23 visual_prompt]: Epoch 14 / 100: avg data time: 6.05e-02, avg batch time: 0.4732, average train loss: 1.7216
[09/26 11:47:25 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1580, average loss: 1.6119
[09/26 11:47:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 42.50	top5: 92.50	
[09/26 11:47:25 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/26 11:47:31 visual_prompt]: Epoch 15 / 100: avg data time: 6.54e-02, avg batch time: 0.4780, average train loss: 1.5213
[09/26 11:47:33 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1583, average loss: 1.3136
[09/26 11:47:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 55.00	top5: 92.00	
[09/26 11:47:33 visual_prompt]: Best epoch 15: best metric: 0.550
[09/26 11:47:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/26 11:47:39 visual_prompt]: Epoch 16 / 100: avg data time: 6.03e-02, avg batch time: 0.4720, average train loss: 1.3276
[09/26 11:47:41 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1584, average loss: 1.2478
[09/26 11:47:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 55.00	top5: 94.50	
[09/26 11:47:41 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/26 11:47:47 visual_prompt]: Epoch 17 / 100: avg data time: 6.06e-02, avg batch time: 0.4740, average train loss: 1.2016
[09/26 11:47:49 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1574, average loss: 1.1237
[09/26 11:47:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 59.00	top5: 92.00	
[09/26 11:47:49 visual_prompt]: Best epoch 17: best metric: 0.590
[09/26 11:47:49 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/26 11:47:55 visual_prompt]: Epoch 18 / 100: avg data time: 5.86e-02, avg batch time: 0.4729, average train loss: 1.0960
[09/26 11:47:57 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1580, average loss: 1.4226
[09/26 11:47:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 48.00	top5: 94.00	
[09/26 11:47:57 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/26 11:48:03 visual_prompt]: Epoch 19 / 100: avg data time: 4.61e-02, avg batch time: 0.4611, average train loss: 1.0738
[09/26 11:48:05 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1582, average loss: 0.9659
[09/26 11:48:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 66.50	top5: 96.50	
[09/26 11:48:05 visual_prompt]: Best epoch 19: best metric: 0.665
[09/26 11:48:05 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/26 11:48:11 visual_prompt]: Epoch 20 / 100: avg data time: 5.77e-02, avg batch time: 0.4710, average train loss: 0.8521
[09/26 11:48:13 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1572, average loss: 1.0290
[09/26 11:48:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 60.00	top5: 95.00	
[09/26 11:48:13 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/26 11:48:19 visual_prompt]: Epoch 21 / 100: avg data time: 4.83e-02, avg batch time: 0.4617, average train loss: 0.8633
[09/26 11:48:21 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1578, average loss: 1.4930
[09/26 11:48:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 53.00	top5: 93.50	
[09/26 11:48:21 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/26 11:48:27 visual_prompt]: Epoch 22 / 100: avg data time: 5.80e-02, avg batch time: 0.4704, average train loss: 0.7782
[09/26 11:48:29 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1582, average loss: 0.8977
[09/26 11:48:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.00	top5: 96.00	
[09/26 11:48:29 visual_prompt]: Best epoch 22: best metric: 0.680
[09/26 11:48:29 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/26 11:48:35 visual_prompt]: Epoch 23 / 100: avg data time: 6.03e-02, avg batch time: 0.4736, average train loss: 0.6004
[09/26 11:48:37 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1581, average loss: 0.8417
[09/26 11:48:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.00	top5: 98.00	
[09/26 11:48:37 visual_prompt]: Best epoch 23: best metric: 0.740
[09/26 11:48:37 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/26 11:48:43 visual_prompt]: Epoch 24 / 100: avg data time: 5.01e-02, avg batch time: 0.4645, average train loss: 0.4998
[09/26 11:48:45 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1582, average loss: 1.1674
[09/26 11:48:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.00	top5: 96.50	
[09/26 11:48:45 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/26 11:48:51 visual_prompt]: Epoch 25 / 100: avg data time: 5.95e-02, avg batch time: 0.4718, average train loss: 0.5238
[09/26 11:48:53 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1584, average loss: 1.3728
[09/26 11:48:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.50	top5: 94.00	
[09/26 11:48:53 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/26 11:48:59 visual_prompt]: Epoch 26 / 100: avg data time: 5.92e-02, avg batch time: 0.4720, average train loss: 0.6503
[09/26 11:49:01 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1584, average loss: 0.7667
[09/26 11:49:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 97.00	
[09/26 11:49:01 visual_prompt]: Best epoch 26: best metric: 0.750
[09/26 11:49:01 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/26 11:49:07 visual_prompt]: Epoch 27 / 100: avg data time: 6.31e-02, avg batch time: 0.4760, average train loss: 0.4585
[09/26 11:49:09 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1585, average loss: 0.7090
[09/26 11:49:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 97.00	
[09/26 11:49:09 visual_prompt]: Best epoch 27: best metric: 0.810
[09/26 11:49:09 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[09/26 11:49:15 visual_prompt]: Epoch 28 / 100: avg data time: 6.35e-02, avg batch time: 0.4764, average train loss: 0.4095
[09/26 11:49:17 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1586, average loss: 0.9280
[09/26 11:49:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 96.00	
[09/26 11:49:17 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[09/26 11:49:23 visual_prompt]: Epoch 29 / 100: avg data time: 5.74e-02, avg batch time: 0.4715, average train loss: 0.5076
[09/26 11:49:25 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1581, average loss: 0.8866
[09/26 11:49:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 97.50	
[09/26 11:49:25 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[09/26 11:49:31 visual_prompt]: Epoch 30 / 100: avg data time: 4.97e-02, avg batch time: 0.4649, average train loss: 0.3753
[09/26 11:49:33 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1586, average loss: 0.9867
[09/26 11:49:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 96.00	
[09/26 11:49:33 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[09/26 11:49:39 visual_prompt]: Epoch 31 / 100: avg data time: 5.82e-02, avg batch time: 0.4714, average train loss: 0.3678
[09/26 11:49:41 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1585, average loss: 0.9261
[09/26 11:49:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 97.50	
[09/26 11:49:41 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[09/26 11:49:48 visual_prompt]: Epoch 32 / 100: avg data time: 6.20e-02, avg batch time: 0.4753, average train loss: 0.2378
[09/26 11:49:49 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1585, average loss: 0.7179
[09/26 11:49:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 97.50	
[09/26 11:49:49 visual_prompt]: Best epoch 32: best metric: 0.820
[09/26 11:49:49 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[09/26 11:49:56 visual_prompt]: Epoch 33 / 100: avg data time: 6.19e-02, avg batch time: 0.4753, average train loss: 0.2185
[09/26 11:49:57 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1582, average loss: 0.7197
[09/26 11:49:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.00	
[09/26 11:49:57 visual_prompt]: Best epoch 33: best metric: 0.835
[09/26 11:49:57 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[09/26 11:50:04 visual_prompt]: Epoch 34 / 100: avg data time: 6.30e-02, avg batch time: 0.4758, average train loss: 0.1868
[09/26 11:50:05 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1581, average loss: 0.8446
[09/26 11:50:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 97.00	
[09/26 11:50:05 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[09/26 11:50:12 visual_prompt]: Epoch 35 / 100: avg data time: 5.93e-02, avg batch time: 0.4730, average train loss: 0.1137
[09/26 11:50:13 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1583, average loss: 0.9033
[09/26 11:50:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 97.50	
[09/26 11:50:13 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[09/26 11:50:20 visual_prompt]: Epoch 36 / 100: avg data time: 5.88e-02, avg batch time: 0.4730, average train loss: 0.1052
[09/26 11:50:21 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1585, average loss: 0.7910
[09/26 11:50:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 96.50	
[09/26 11:50:21 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[09/26 11:50:28 visual_prompt]: Epoch 37 / 100: avg data time: 4.80e-02, avg batch time: 0.4615, average train loss: 0.0750
[09/26 11:50:29 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.1581, average loss: 0.8625
[09/26 11:50:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 97.00	
[09/26 11:50:29 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[09/26 11:50:36 visual_prompt]: Epoch 38 / 100: avg data time: 5.90e-02, avg batch time: 0.4724, average train loss: 0.0909
[09/26 11:50:37 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1585, average loss: 0.8859
[09/26 11:50:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 97.00	
[09/26 11:50:37 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[09/26 11:50:44 visual_prompt]: Epoch 39 / 100: avg data time: 6.67e-02, avg batch time: 0.4800, average train loss: 0.1096
[09/26 11:50:45 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1580, average loss: 0.9847
[09/26 11:50:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 95.50	
[09/26 11:50:45 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[09/26 11:50:52 visual_prompt]: Epoch 40 / 100: avg data time: 4.63e-02, avg batch time: 0.4611, average train loss: 0.1112
[09/26 11:50:53 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1580, average loss: 0.8775
[09/26 11:50:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 99.00	
[09/26 11:50:53 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[09/26 11:51:00 visual_prompt]: Epoch 41 / 100: avg data time: 5.93e-02, avg batch time: 0.4725, average train loss: 0.0844
[09/26 11:51:01 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1583, average loss: 0.9196
[09/26 11:51:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.00	
[09/26 11:51:01 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[09/26 11:51:08 visual_prompt]: Epoch 42 / 100: avg data time: 5.00e-02, avg batch time: 0.4642, average train loss: 0.0890
[09/26 11:51:09 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1586, average loss: 0.9056
[09/26 11:51:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 97.00	
[09/26 11:51:09 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[09/26 11:51:16 visual_prompt]: Epoch 43 / 100: avg data time: 6.08e-02, avg batch time: 0.4743, average train loss: 0.0689
[09/26 11:51:17 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1585, average loss: 0.8609
[09/26 11:51:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 97.50	
[09/26 11:51:17 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[09/26 11:51:24 visual_prompt]: Epoch 44 / 100: avg data time: 5.98e-02, avg batch time: 0.4734, average train loss: 0.0405
[09/26 11:51:25 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1584, average loss: 0.8554
[09/26 11:51:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.50	
[09/26 11:51:25 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[09/26 11:51:32 visual_prompt]: Epoch 45 / 100: avg data time: 6.50e-02, avg batch time: 0.4787, average train loss: 0.0300
[09/26 11:51:34 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1587, average loss: 0.9784
[09/26 11:51:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 11:51:34 visual_prompt]: Best epoch 45: best metric: 0.840
[09/26 11:51:34 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[09/26 11:51:40 visual_prompt]: Epoch 46 / 100: avg data time: 5.45e-02, avg batch time: 0.4677, average train loss: 0.0336
[09/26 11:51:42 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.1589, average loss: 1.1185
[09/26 11:51:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 96.50	
[09/26 11:51:42 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[09/26 11:51:48 visual_prompt]: Epoch 47 / 100: avg data time: 5.31e-02, avg batch time: 0.4666, average train loss: 0.0220
[09/26 11:51:50 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1586, average loss: 0.8784
[09/26 11:51:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.00	
[09/26 11:51:50 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[09/26 11:51:56 visual_prompt]: Epoch 48 / 100: avg data time: 5.29e-02, avg batch time: 0.4675, average train loss: 0.0148
[09/26 11:51:58 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1577, average loss: 0.9300
[09/26 11:51:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.50	
[09/26 11:51:58 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[09/26 11:52:04 visual_prompt]: Epoch 49 / 100: avg data time: 5.45e-02, avg batch time: 0.4680, average train loss: 0.0359
[09/26 11:52:06 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1586, average loss: 0.9033
[09/26 11:52:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 11:52:06 visual_prompt]: Best epoch 49: best metric: 0.845
[09/26 11:52:06 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[09/26 11:52:12 visual_prompt]: Epoch 50 / 100: avg data time: 6.49e-02, avg batch time: 0.4780, average train loss: 0.0589
[09/26 11:52:14 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.1586, average loss: 1.0300
[09/26 11:52:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.00	
[09/26 11:52:14 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[09/26 11:52:20 visual_prompt]: Epoch 51 / 100: avg data time: 6.64e-02, avg batch time: 0.4803, average train loss: 0.0736
[09/26 11:52:22 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1577, average loss: 0.9856
[09/26 11:52:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 96.50	
[09/26 11:52:22 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[09/26 11:52:28 visual_prompt]: Epoch 52 / 100: avg data time: 5.81e-02, avg batch time: 0.4714, average train loss: 0.0282
[09/26 11:52:30 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1586, average loss: 1.0275
[09/26 11:52:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.50	top5: 97.50	
[09/26 11:52:30 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[09/26 11:52:37 visual_prompt]: Epoch 53 / 100: avg data time: 6.25e-02, avg batch time: 0.4757, average train loss: 0.0198
[09/26 11:52:38 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1583, average loss: 0.9843
[09/26 11:52:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.00	
[09/26 11:52:38 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[09/26 11:52:44 visual_prompt]: Epoch 54 / 100: avg data time: 4.96e-02, avg batch time: 0.4639, average train loss: 0.0058
[09/26 11:52:46 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1586, average loss: 1.0659
[09/26 11:52:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 97.00	
[09/26 11:52:46 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[09/26 11:52:53 visual_prompt]: Epoch 55 / 100: avg data time: 6.13e-02, avg batch time: 0.4747, average train loss: 0.0038
[09/26 11:52:54 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1586, average loss: 1.0835
[09/26 11:52:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.00	
[09/26 11:52:54 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[09/26 11:53:01 visual_prompt]: Epoch 56 / 100: avg data time: 5.96e-02, avg batch time: 0.4735, average train loss: 0.0031
[09/26 11:53:02 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1585, average loss: 0.9992
[09/26 11:53:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 96.50	
[09/26 11:53:02 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[09/26 11:53:08 visual_prompt]: Epoch 57 / 100: avg data time: 5.57e-02, avg batch time: 0.4688, average train loss: 0.0021
[09/26 11:53:10 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1582, average loss: 0.9821
[09/26 11:53:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 11:53:10 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[09/26 11:53:17 visual_prompt]: Epoch 58 / 100: avg data time: 5.57e-02, avg batch time: 0.4706, average train loss: 0.0024
[09/26 11:53:18 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1584, average loss: 1.0110
[09/26 11:53:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 11:53:18 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[09/26 11:53:25 visual_prompt]: Epoch 59 / 100: avg data time: 5.95e-02, avg batch time: 0.4728, average train loss: 0.0016
[09/26 11:53:26 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1584, average loss: 1.0278
[09/26 11:53:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 11:53:26 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[09/26 11:53:33 visual_prompt]: Epoch 60 / 100: avg data time: 5.88e-02, avg batch time: 0.4722, average train loss: 0.0015
[09/26 11:53:34 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1582, average loss: 1.0246
[09/26 11:53:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 98.00	
[09/26 11:53:34 visual_prompt]: Best epoch 60: best metric: 0.850
[09/26 11:53:34 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[09/26 11:53:41 visual_prompt]: Epoch 61 / 100: avg data time: 6.06e-02, avg batch time: 0.4758, average train loss: 0.0016
[09/26 11:53:42 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1584, average loss: 0.9760
[09/26 11:53:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 11:53:42 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[09/26 11:53:49 visual_prompt]: Epoch 62 / 100: avg data time: 6.00e-02, avg batch time: 0.4746, average train loss: 0.0012
[09/26 11:53:51 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.1583, average loss: 0.9607
[09/26 11:53:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 11:53:51 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[09/26 11:53:57 visual_prompt]: Epoch 63 / 100: avg data time: 5.98e-02, avg batch time: 0.4730, average train loss: 0.0015
[09/26 11:53:58 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1585, average loss: 0.9701
[09/26 11:53:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 97.50	
[09/26 11:53:58 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[09/26 11:54:05 visual_prompt]: Epoch 64 / 100: avg data time: 6.19e-02, avg batch time: 0.4743, average train loss: 0.0011
[09/26 11:54:07 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1581, average loss: 0.9993
[09/26 11:54:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 98.00	
[09/26 11:54:07 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[09/26 11:54:13 visual_prompt]: Epoch 65 / 100: avg data time: 5.63e-02, avg batch time: 0.4699, average train loss: 0.0009
[09/26 11:54:15 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1583, average loss: 1.0175
[09/26 11:54:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 11:54:15 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[09/26 11:54:21 visual_prompt]: Epoch 66 / 100: avg data time: 5.40e-02, avg batch time: 0.4667, average train loss: 0.0013
[09/26 11:54:23 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1581, average loss: 1.0115
[09/26 11:54:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 98.00	
[09/26 11:54:23 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[09/26 11:54:29 visual_prompt]: Epoch 67 / 100: avg data time: 6.27e-02, avg batch time: 0.4748, average train loss: 0.0009
[09/26 11:54:31 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1580, average loss: 1.0090
[09/26 11:54:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 98.00	
[09/26 11:54:31 visual_prompt]: Best epoch 67: best metric: 0.855
[09/26 11:54:31 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[09/26 11:54:37 visual_prompt]: Epoch 68 / 100: avg data time: 5.22e-02, avg batch time: 0.4653, average train loss: 0.0014
[09/26 11:54:39 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1584, average loss: 1.0157
[09/26 11:54:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 11:54:39 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[09/26 11:54:45 visual_prompt]: Epoch 69 / 100: avg data time: 5.67e-02, avg batch time: 0.4706, average train loss: 0.0009
[09/26 11:54:47 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.1581, average loss: 1.0304
[09/26 11:54:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.00	
[09/26 11:54:47 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[09/26 11:54:53 visual_prompt]: Epoch 70 / 100: avg data time: 4.59e-02, avg batch time: 0.4604, average train loss: 0.0009
[09/26 11:54:55 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1583, average loss: 1.0358
[09/26 11:54:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.00	
[09/26 11:54:55 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[09/26 11:55:01 visual_prompt]: Epoch 71 / 100: avg data time: 5.86e-02, avg batch time: 0.4714, average train loss: 0.0009
[09/26 11:55:03 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1586, average loss: 1.0350
[09/26 11:55:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.00	
[09/26 11:55:03 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[09/26 11:55:09 visual_prompt]: Epoch 72 / 100: avg data time: 5.99e-02, avg batch time: 0.4729, average train loss: 0.0009
[09/26 11:55:11 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1582, average loss: 1.0378
[09/26 11:55:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 11:55:11 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[09/26 11:55:17 visual_prompt]: Epoch 73 / 100: avg data time: 5.42e-02, avg batch time: 0.4690, average train loss: 0.0010
[09/26 11:55:19 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 1.0365
[09/26 11:55:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 11:55:19 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[09/26 11:55:25 visual_prompt]: Epoch 74 / 100: avg data time: 6.45e-02, avg batch time: 0.4768, average train loss: 0.0016
[09/26 11:55:27 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1584, average loss: 1.0797
[09/26 11:55:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 11:55:27 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[09/26 11:55:33 visual_prompt]: Epoch 75 / 100: avg data time: 5.39e-02, avg batch time: 0.4681, average train loss: 0.0009
[09/26 11:55:35 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1583, average loss: 1.0808
[09/26 11:55:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 11:55:35 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[09/26 11:55:41 visual_prompt]: Epoch 76 / 100: avg data time: 6.35e-02, avg batch time: 0.4764, average train loss: 0.0008
[09/26 11:55:43 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1585, average loss: 1.0784
[09/26 11:55:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 11:55:43 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[09/26 11:55:49 visual_prompt]: Epoch 77 / 100: avg data time: 4.99e-02, avg batch time: 0.4650, average train loss: 0.0007
[09/26 11:55:51 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1581, average loss: 1.0794
[09/26 11:55:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 11:55:51 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[09/26 11:55:57 visual_prompt]: Epoch 78 / 100: avg data time: 5.18e-02, avg batch time: 0.4662, average train loss: 0.0007
[09/26 11:55:59 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1584, average loss: 1.0798
[09/26 11:55:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 11:55:59 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[09/26 11:56:05 visual_prompt]: Epoch 79 / 100: avg data time: 5.83e-02, avg batch time: 0.4722, average train loss: 0.0008
[09/26 11:56:07 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1588, average loss: 1.0810
[09/26 11:56:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 11:56:07 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[09/26 11:56:13 visual_prompt]: Epoch 80 / 100: avg data time: 6.39e-02, avg batch time: 0.4771, average train loss: 0.0007
[09/26 11:56:15 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1586, average loss: 1.0817
[09/26 11:56:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 11:56:15 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[09/26 11:56:21 visual_prompt]: Epoch 81 / 100: avg data time: 5.96e-02, avg batch time: 0.4723, average train loss: 0.0006
[09/26 11:56:23 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1582, average loss: 1.0816
[09/26 11:56:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 11:56:23 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[09/26 11:56:29 visual_prompt]: Epoch 82 / 100: avg data time: 6.49e-02, avg batch time: 0.4782, average train loss: 0.0010
[09/26 11:56:31 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1582, average loss: 1.0810
[09/26 11:56:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 11:56:31 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[09/26 11:56:37 visual_prompt]: Epoch 83 / 100: avg data time: 5.85e-02, avg batch time: 0.4713, average train loss: 0.0008
[09/26 11:56:39 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.1579, average loss: 1.0800
[09/26 11:56:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 11:56:39 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[09/26 11:56:45 visual_prompt]: Epoch 84 / 100: avg data time: 5.74e-02, avg batch time: 0.4703, average train loss: 0.0006
[09/26 11:56:47 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1581, average loss: 1.0804
[09/26 11:56:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 11:56:47 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[09/26 11:56:54 visual_prompt]: Epoch 85 / 100: avg data time: 5.66e-02, avg batch time: 0.4705, average train loss: 0.0006
[09/26 11:56:55 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1584, average loss: 1.0833
[09/26 11:56:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 11:56:55 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[09/26 11:57:02 visual_prompt]: Epoch 86 / 100: avg data time: 5.80e-02, avg batch time: 0.4715, average train loss: 0.0008
[09/26 11:57:03 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1580, average loss: 1.0830
[09/26 11:57:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 11:57:03 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[09/26 11:57:09 visual_prompt]: Epoch 87 / 100: avg data time: 5.04e-02, avg batch time: 0.4640, average train loss: 0.0006
[09/26 11:57:11 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1583, average loss: 1.0806
[09/26 11:57:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 11:57:11 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[09/26 11:57:17 visual_prompt]: Epoch 88 / 100: avg data time: 5.69e-02, avg batch time: 0.4700, average train loss: 0.0006
[09/26 11:57:19 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1580, average loss: 1.0800
[09/26 11:57:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 11:57:19 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[09/26 11:57:25 visual_prompt]: Epoch 89 / 100: avg data time: 5.52e-02, avg batch time: 0.4690, average train loss: 0.0006
[09/26 11:57:27 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.1586, average loss: 1.0801
[09/26 11:57:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 11:57:27 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[09/26 11:57:34 visual_prompt]: Epoch 90 / 100: avg data time: 6.21e-02, avg batch time: 0.4756, average train loss: 0.0007
[09/26 11:57:35 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1584, average loss: 1.0790
[09/26 11:57:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 11:57:35 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[09/26 11:57:42 visual_prompt]: Epoch 91 / 100: avg data time: 6.05e-02, avg batch time: 0.4746, average train loss: 0.0006
[09/26 11:57:43 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1587, average loss: 1.0776
[09/26 11:57:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 11:57:43 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[09/26 11:57:50 visual_prompt]: Epoch 92 / 100: avg data time: 5.41e-02, avg batch time: 0.4692, average train loss: 0.0007
[09/26 11:57:51 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1577, average loss: 1.0779
[09/26 11:57:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 11:57:51 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[09/26 11:57:58 visual_prompt]: Epoch 93 / 100: avg data time: 5.79e-02, avg batch time: 0.4708, average train loss: 0.0006
[09/26 11:57:59 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1586, average loss: 1.0784
[09/26 11:57:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 11:57:59 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[09/26 11:58:06 visual_prompt]: Epoch 94 / 100: avg data time: 5.94e-02, avg batch time: 0.4731, average train loss: 0.0008
[09/26 11:58:07 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1586, average loss: 1.0784
[09/26 11:58:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 11:58:07 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[09/26 11:58:14 visual_prompt]: Epoch 95 / 100: avg data time: 5.87e-02, avg batch time: 0.4715, average train loss: 0.0006
[09/26 11:58:15 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1585, average loss: 1.0782
[09/26 11:58:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 11:58:15 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[09/26 11:58:22 visual_prompt]: Epoch 96 / 100: avg data time: 6.29e-02, avg batch time: 0.4762, average train loss: 0.0008
[09/26 11:58:23 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1580, average loss: 1.0778
[09/26 11:58:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 11:58:23 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[09/26 11:58:30 visual_prompt]: Epoch 97 / 100: avg data time: 5.54e-02, avg batch time: 0.4703, average train loss: 0.0006
[09/26 11:58:31 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.1583, average loss: 1.0776
[09/26 11:58:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 11:58:31 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[09/26 11:58:38 visual_prompt]: Epoch 98 / 100: avg data time: 5.72e-02, avg batch time: 0.4710, average train loss: 0.0006
[09/26 11:58:39 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1583, average loss: 1.0776
[09/26 11:58:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 11:58:39 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[09/26 11:58:46 visual_prompt]: Epoch 99 / 100: avg data time: 5.27e-02, avg batch time: 0.4661, average train loss: 0.0007
[09/26 11:58:47 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.1579, average loss: 1.0776
[09/26 11:58:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 11:58:47 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[09/26 11:58:54 visual_prompt]: Epoch 100 / 100: avg data time: 5.94e-02, avg batch time: 0.4724, average train loss: 0.0007
[09/26 11:58:56 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1584, average loss: 1.0776
[09/26 11:58:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 11:58:56 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 11:58:56 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 11:58:56 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 11:58:56 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 11:58:56 visual_prompt]: Training with config:
[09/26 11:58:56 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.5_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 11:58:56 visual_prompt]: Loading training data...
[09/26 11:58:56 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 11:58:56 visual_prompt]: Number of images: 800
[09/26 11:58:56 visual_prompt]: Number of classes: 10 / 10
[09/26 11:58:56 visual_prompt]: Loading validation data...
[09/26 11:58:56 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 11:58:57 visual_prompt]: Number of images: 200
[09/26 11:58:57 visual_prompt]: Number of classes: 10 / 10
[09/26 11:58:57 visual_prompt]: Constructing models...
[09/26 11:58:59 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 11:58:59 visual_prompt]: tuned percent:0.543
[09/26 11:58:59 visual_prompt]: Device used for model: 0
[09/26 11:58:59 visual_prompt]: Setting up Evaluator...
[09/26 11:58:59 visual_prompt]: Setting up Trainer...
[09/26 11:58:59 visual_prompt]: 	Setting up the optimizer...
[09/26 11:58:59 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 11:59:06 visual_prompt]: Epoch 1 / 100: avg data time: 6.82e-02, avg batch time: 0.4879, average train loss: 2.6690
[09/26 11:59:08 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1582, average loss: 2.6214
[09/26 11:59:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 11:59:08 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 11:59:08 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[09/26 11:59:14 visual_prompt]: Epoch 2 / 100: avg data time: 6.15e-02, avg batch time: 0.4738, average train loss: 2.8577
[09/26 11:59:16 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1580, average loss: 2.2612
[09/26 11:59:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 11:59:16 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[09/26 11:59:22 visual_prompt]: Epoch 3 / 100: avg data time: 4.74e-02, avg batch time: 0.4615, average train loss: 2.3525
[09/26 11:59:24 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1582, average loss: 2.2559
[09/26 11:59:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.00	
[09/26 11:59:24 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[09/26 11:59:30 visual_prompt]: Epoch 4 / 100: avg data time: 6.03e-02, avg batch time: 0.4722, average train loss: 2.3108
[09/26 11:59:32 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1582, average loss: 2.2525
[09/26 11:59:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 11.50	top5: 64.50	
[09/26 11:59:32 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[09/26 11:59:38 visual_prompt]: Epoch 5 / 100: avg data time: 5.98e-02, avg batch time: 0.4717, average train loss: 2.2684
[09/26 11:59:40 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1581, average loss: 2.2460
[09/26 11:59:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 11:59:40 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[09/26 11:59:46 visual_prompt]: Epoch 6 / 100: avg data time: 4.90e-02, avg batch time: 0.4618, average train loss: 2.2669
[09/26 11:59:47 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1585, average loss: 2.2789
[09/26 11:59:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.00	
[09/26 11:59:47 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[09/26 11:59:54 visual_prompt]: Epoch 7 / 100: avg data time: 5.84e-02, avg batch time: 0.4720, average train loss: 2.3018
[09/26 11:59:55 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1582, average loss: 2.2452
[09/26 11:59:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 11:59:55 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[09/26 12:00:02 visual_prompt]: Epoch 8 / 100: avg data time: 6.23e-02, avg batch time: 0.4759, average train loss: 2.2822
[09/26 12:00:03 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1585, average loss: 2.2648
[09/26 12:00:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 64.00	
[09/26 12:00:03 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[09/26 12:00:10 visual_prompt]: Epoch 9 / 100: avg data time: 5.87e-02, avg batch time: 0.4712, average train loss: 2.2547
[09/26 12:00:12 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1575, average loss: 2.2400
[09/26 12:00:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 12:00:12 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[09/26 12:00:18 visual_prompt]: Epoch 10 / 100: avg data time: 4.60e-02, avg batch time: 0.4588, average train loss: 2.2593
[09/26 12:00:19 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1578, average loss: 2.2492
[09/26 12:00:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 12:00:19 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[09/26 12:00:26 visual_prompt]: Epoch 11 / 100: avg data time: 6.16e-02, avg batch time: 0.4739, average train loss: 2.2547
[09/26 12:00:27 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1581, average loss: 2.2228
[09/26 12:00:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 12:00:27 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[09/26 12:00:34 visual_prompt]: Epoch 12 / 100: avg data time: 5.94e-02, avg batch time: 0.4716, average train loss: 2.2750
[09/26 12:00:36 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1583, average loss: 2.2476
[09/26 12:00:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 12:00:36 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[09/26 12:00:42 visual_prompt]: Epoch 13 / 100: avg data time: 5.84e-02, avg batch time: 0.4701, average train loss: 2.2810
[09/26 12:00:43 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1581, average loss: 2.2398
[09/26 12:00:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 12:00:43 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[09/26 12:00:50 visual_prompt]: Epoch 14 / 100: avg data time: 5.88e-02, avg batch time: 0.4704, average train loss: 2.2776
[09/26 12:00:51 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1584, average loss: 2.2362
[09/26 12:00:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 12:00:51 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[09/26 12:00:58 visual_prompt]: Epoch 15 / 100: avg data time: 4.96e-02, avg batch time: 0.4626, average train loss: 2.2654
[09/26 12:00:59 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.1581, average loss: 2.2467
[09/26 12:00:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 12:00:59 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[09/26 12:01:06 visual_prompt]: Epoch 16 / 100: avg data time: 5.38e-02, avg batch time: 0.4662, average train loss: 2.2614
[09/26 12:01:07 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1584, average loss: 2.2416
[09/26 12:01:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 12:01:07 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[09/26 12:01:14 visual_prompt]: Epoch 17 / 100: avg data time: 5.34e-02, avg batch time: 0.4679, average train loss: 2.2510
[09/26 12:01:15 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1580, average loss: 2.2390
[09/26 12:01:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 12:01:15 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[09/26 12:01:22 visual_prompt]: Epoch 18 / 100: avg data time: 6.19e-02, avg batch time: 0.4752, average train loss: 2.2703
[09/26 12:01:23 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1581, average loss: 2.3162
[09/26 12:01:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 12:01:23 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[09/26 12:01:30 visual_prompt]: Epoch 19 / 100: avg data time: 6.09e-02, avg batch time: 0.4726, average train loss: 2.2772
[09/26 12:01:31 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1581, average loss: 2.2593
[09/26 12:01:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 12:01:31 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[09/26 12:01:38 visual_prompt]: Epoch 20 / 100: avg data time: 5.57e-02, avg batch time: 0.4683, average train loss: 2.2798
[09/26 12:01:39 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1585, average loss: 2.2509
[09/26 12:01:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.00	
[09/26 12:01:39 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[09/26 12:01:46 visual_prompt]: Epoch 21 / 100: avg data time: 5.92e-02, avg batch time: 0.4704, average train loss: 2.2817
[09/26 12:01:47 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1575, average loss: 2.3172
[09/26 12:01:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 62.50	
[09/26 12:01:47 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[09/26 12:01:54 visual_prompt]: Epoch 22 / 100: avg data time: 5.28e-02, avg batch time: 0.4661, average train loss: 2.3035
[09/26 12:01:55 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1582, average loss: 2.2969
[09/26 12:01:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 58.00	
[09/26 12:01:55 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[09/26 12:02:02 visual_prompt]: Epoch 23 / 100: avg data time: 6.39e-02, avg batch time: 0.4760, average train loss: 2.2902
[09/26 12:02:04 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1575, average loss: 2.2834
[09/26 12:02:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 57.00	
[09/26 12:02:04 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[09/26 12:02:10 visual_prompt]: Epoch 24 / 100: avg data time: 5.87e-02, avg batch time: 0.4718, average train loss: 2.2787
[09/26 12:02:12 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1578, average loss: 2.2309
[09/26 12:02:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 12:02:12 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[09/26 12:02:18 visual_prompt]: Epoch 25 / 100: avg data time: 5.91e-02, avg batch time: 0.4715, average train loss: 2.2671
[09/26 12:02:20 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1582, average loss: 2.2270
[09/26 12:02:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:02:20 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[09/26 12:02:26 visual_prompt]: Epoch 26 / 100: avg data time: 6.01e-02, avg batch time: 0.4731, average train loss: 2.2707
[09/26 12:02:28 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1583, average loss: 2.2945
[09/26 12:02:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 61.50	
[09/26 12:02:28 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[09/26 12:02:34 visual_prompt]: Epoch 27 / 100: avg data time: 5.61e-02, avg batch time: 0.4674, average train loss: 2.2766
[09/26 12:02:36 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1581, average loss: 2.2465
[09/26 12:02:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:02:36 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[09/26 12:02:42 visual_prompt]: Epoch 28 / 100: avg data time: 6.30e-02, avg batch time: 0.4745, average train loss: 2.2748
[09/26 12:02:44 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1584, average loss: 2.2472
[09/26 12:02:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 12:02:44 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[09/26 12:02:50 visual_prompt]: Epoch 29 / 100: avg data time: 6.27e-02, avg batch time: 0.4745, average train loss: 2.2708
[09/26 12:02:52 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1581, average loss: 2.2341
[09/26 12:02:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:02:52 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[09/26 12:02:58 visual_prompt]: Epoch 30 / 100: avg data time: 5.96e-02, avg batch time: 0.4720, average train loss: 2.2735
[09/26 12:03:00 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1583, average loss: 2.2316
[09/26 12:03:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:03:00 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[09/26 12:03:06 visual_prompt]: Epoch 31 / 100: avg data time: 5.67e-02, avg batch time: 0.4697, average train loss: 2.2539
[09/26 12:03:08 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1584, average loss: 2.2202
[09/26 12:03:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:03:08 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[09/26 12:03:14 visual_prompt]: Epoch 32 / 100: avg data time: 6.01e-02, avg batch time: 0.4719, average train loss: 2.2847
[09/26 12:03:16 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1586, average loss: 2.2365
[09/26 12:03:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:03:16 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[09/26 12:03:22 visual_prompt]: Epoch 33 / 100: avg data time: 6.11e-02, avg batch time: 0.4736, average train loss: 2.2992
[09/26 12:03:24 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1584, average loss: 2.2382
[09/26 12:03:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:03:24 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[09/26 12:03:30 visual_prompt]: Epoch 34 / 100: avg data time: 5.49e-02, avg batch time: 0.4676, average train loss: 2.2635
[09/26 12:03:32 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1586, average loss: 2.2597
[09/26 12:03:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 12:03:32 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[09/26 12:03:38 visual_prompt]: Epoch 35 / 100: avg data time: 6.12e-02, avg batch time: 0.4727, average train loss: 2.2707
[09/26 12:03:40 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1584, average loss: 2.2719
[09/26 12:03:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 12:03:40 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[09/26 12:03:46 visual_prompt]: Epoch 36 / 100: avg data time: 5.52e-02, avg batch time: 0.4702, average train loss: 2.2666
[09/26 12:03:48 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 2.2300
[09/26 12:03:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:03:48 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[09/26 12:03:54 visual_prompt]: Epoch 37 / 100: avg data time: 6.39e-02, avg batch time: 0.4759, average train loss: 2.2619
[09/26 12:03:56 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1580, average loss: 2.2222
[09/26 12:03:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:03:56 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[09/26 12:04:02 visual_prompt]: Epoch 38 / 100: avg data time: 5.35e-02, avg batch time: 0.4666, average train loss: 2.2922
[09/26 12:04:04 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1580, average loss: 2.2393
[09/26 12:04:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 12:04:04 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[09/26 12:04:10 visual_prompt]: Epoch 39 / 100: avg data time: 4.78e-02, avg batch time: 0.4621, average train loss: 2.2601
[09/26 12:04:12 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1582, average loss: 2.2450
[09/26 12:04:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 12:04:12 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[09/26 12:04:18 visual_prompt]: Epoch 40 / 100: avg data time: 6.36e-02, avg batch time: 0.4756, average train loss: 2.2758
[09/26 12:04:20 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1583, average loss: 2.6192
[09/26 12:04:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.00	top5: 42.50	
[09/26 12:04:20 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[09/26 12:04:26 visual_prompt]: Epoch 41 / 100: avg data time: 5.86e-02, avg batch time: 0.4714, average train loss: 2.2753
[09/26 12:04:28 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1582, average loss: 2.2627
[09/26 12:04:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 64.00	
[09/26 12:04:28 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[09/26 12:04:34 visual_prompt]: Epoch 42 / 100: avg data time: 4.82e-02, avg batch time: 0.4623, average train loss: 2.2548
[09/26 12:04:36 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1587, average loss: 2.2429
[09/26 12:04:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 12:04:36 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[09/26 12:04:42 visual_prompt]: Epoch 43 / 100: avg data time: 6.01e-02, avg batch time: 0.4725, average train loss: 2.2622
[09/26 12:04:44 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1581, average loss: 2.2361
[09/26 12:04:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.50	top5: 63.50	
[09/26 12:04:44 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[09/26 12:04:50 visual_prompt]: Epoch 44 / 100: avg data time: 5.95e-02, avg batch time: 0.4740, average train loss: 2.2653
[09/26 12:04:52 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1582, average loss: 2.2441
[09/26 12:04:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 12:04:52 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[09/26 12:04:58 visual_prompt]: Epoch 45 / 100: avg data time: 6.30e-02, avg batch time: 0.4746, average train loss: 2.2642
[09/26 12:05:00 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.1584, average loss: 2.2326
[09/26 12:05:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 12:05:00 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[09/26 12:05:06 visual_prompt]: Epoch 46 / 100: avg data time: 5.42e-02, avg batch time: 0.4673, average train loss: 2.2660
[09/26 12:05:08 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1581, average loss: 2.2555
[09/26 12:05:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:05:08 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[09/26 12:05:14 visual_prompt]: Epoch 47 / 100: avg data time: 5.52e-02, avg batch time: 0.4678, average train loss: 2.2733
[09/26 12:05:16 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1583, average loss: 2.2475
[09/26 12:05:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 12:05:16 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.3189093389542498
[09/26 12:05:23 visual_prompt]: Epoch 48 / 100: avg data time: 6.26e-02, avg batch time: 0.4749, average train loss: 2.2675
[09/26 12:05:24 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1582, average loss: 2.2468
[09/26 12:05:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:05:24 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.3104804738999169
[09/26 12:05:31 visual_prompt]: Epoch 49 / 100: avg data time: 6.54e-02, avg batch time: 0.4770, average train loss: 2.2510
[09/26 12:05:32 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 2.2326
[09/26 12:05:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 12:05:32 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.3019779227044398
[09/26 12:05:39 visual_prompt]: Epoch 50 / 100: avg data time: 6.01e-02, avg batch time: 0.4717, average train loss: 2.2523
[09/26 12:05:40 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1584, average loss: 2.2316
[09/26 12:05:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:05:40 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.29341204441673263
[09/26 12:05:47 visual_prompt]: Epoch 51 / 100: avg data time: 6.29e-02, avg batch time: 0.4739, average train loss: 2.2518
[09/26 12:05:48 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1581, average loss: 2.2303
[09/26 12:05:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 12:05:48 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.28479327524001635
[09/26 12:05:55 visual_prompt]: Epoch 52 / 100: avg data time: 6.15e-02, avg batch time: 0.4727, average train loss: 2.2530
[09/26 12:05:56 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1578, average loss: 2.2241
[09/26 12:05:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 12:05:56 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.2761321158169134
[09/26 12:06:03 visual_prompt]: Epoch 53 / 100: avg data time: 6.54e-02, avg batch time: 0.4761, average train loss: 2.2467
[09/26 12:06:04 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1579, average loss: 2.2650
[09/26 12:06:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 12:06:04 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.2674391184360313
[09/26 12:06:11 visual_prompt]: Epoch 54 / 100: avg data time: 6.08e-02, avg batch time: 0.4725, average train loss: 2.2538
[09/26 12:06:12 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1580, average loss: 2.2155
[09/26 12:06:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:06:12 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.2587248741756253
[09/26 12:06:19 visual_prompt]: Epoch 55 / 100: avg data time: 6.57e-02, avg batch time: 0.4767, average train loss: 2.2524
[09/26 12:06:20 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1580, average loss: 2.2264
[09/26 12:06:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 12:06:20 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.25
[09/26 12:06:27 visual_prompt]: Epoch 56 / 100: avg data time: 6.00e-02, avg batch time: 0.4712, average train loss: 2.2491
[09/26 12:06:28 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1581, average loss: 2.2377
[09/26 12:06:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 12:06:28 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.24127512582437483
[09/26 12:06:35 visual_prompt]: Epoch 57 / 100: avg data time: 5.75e-02, avg batch time: 0.4702, average train loss: 2.2588
[09/26 12:06:36 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1584, average loss: 2.2249
[09/26 12:06:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 12:06:36 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.23256088156396867
[09/26 12:06:43 visual_prompt]: Epoch 58 / 100: avg data time: 5.02e-02, avg batch time: 0.4644, average train loss: 2.2567
[09/26 12:06:44 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1580, average loss: 2.2140
[09/26 12:06:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 12:06:44 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.22386788418308667
[09/26 12:06:51 visual_prompt]: Epoch 59 / 100: avg data time: 6.07e-02, avg batch time: 0.4730, average train loss: 2.2494
[09/26 12:06:52 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1580, average loss: 2.2205
[09/26 12:06:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:06:52 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.2152067247599837
[09/26 12:06:59 visual_prompt]: Epoch 60 / 100: avg data time: 5.57e-02, avg batch time: 0.4667, average train loss: 2.2449
[09/26 12:07:00 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1580, average loss: 2.2266
[09/26 12:07:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:07:00 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.20658795558326742
[09/26 12:07:07 visual_prompt]: Epoch 61 / 100: avg data time: 6.42e-02, avg batch time: 0.4756, average train loss: 2.2492
[09/26 12:07:08 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1578, average loss: 2.2139
[09/26 12:07:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:07:08 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.1980220772955602
[09/26 12:07:15 visual_prompt]: Epoch 62 / 100: avg data time: 5.23e-02, avg batch time: 0.4657, average train loss: 2.2419
[09/26 12:07:16 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1581, average loss: 2.2172
[09/26 12:07:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:07:16 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.1895195261000831
[09/26 12:07:23 visual_prompt]: Epoch 63 / 100: avg data time: 5.54e-02, avg batch time: 0.4669, average train loss: 2.2399
[09/26 12:07:24 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1585, average loss: 2.2235
[09/26 12:07:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:07:24 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.18109066104575022
[09/26 12:07:31 visual_prompt]: Epoch 64 / 100: avg data time: 6.56e-02, avg batch time: 0.4776, average train loss: 2.2448
[09/26 12:07:32 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1581, average loss: 2.2238
[09/26 12:07:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:07:32 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.17274575140626316
[09/26 12:07:39 visual_prompt]: Epoch 65 / 100: avg data time: 5.54e-02, avg batch time: 0.4681, average train loss: 2.2493
[09/26 12:07:40 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1583, average loss: 2.2155
[09/26 12:07:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 12:07:40 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.16449496416858284
[09/26 12:07:47 visual_prompt]: Epoch 66 / 100: avg data time: 5.57e-02, avg batch time: 0.4681, average train loss: 2.2446
[09/26 12:07:48 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.1582, average loss: 2.2230
[09/26 12:07:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 12:07:48 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.15634835164602198
[09/26 12:07:55 visual_prompt]: Epoch 67 / 100: avg data time: 6.29e-02, avg batch time: 0.4751, average train loss: 2.2486
[09/26 12:07:56 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1572, average loss: 2.2188
[09/26 12:07:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:07:56 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.14831583923105
[09/26 12:08:03 visual_prompt]: Epoch 68 / 100: avg data time: 5.36e-02, avg batch time: 0.4687, average train loss: 2.2490
[09/26 12:08:04 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1583, average loss: 2.2237
[09/26 12:08:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:08:04 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.14040721330273062
[09/26 12:08:11 visual_prompt]: Epoch 69 / 100: avg data time: 5.96e-02, avg batch time: 0.4720, average train loss: 2.2428
[09/26 12:08:13 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1583, average loss: 2.2118
[09/26 12:08:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 12:08:13 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.13263210930352737
[09/26 12:08:19 visual_prompt]: Epoch 70 / 100: avg data time: 6.01e-02, avg batch time: 0.4724, average train loss: 2.2392
[09/26 12:08:21 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1578, average loss: 2.2316
[09/26 12:08:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:08:21 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.12500000000000006
[09/26 12:08:27 visual_prompt]: Epoch 71 / 100: avg data time: 5.82e-02, avg batch time: 0.4703, average train loss: 2.2428
[09/26 12:08:29 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1584, average loss: 2.2133
[09/26 12:08:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:08:29 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.1175201839416988
[09/26 12:08:35 visual_prompt]: Epoch 72 / 100: avg data time: 5.37e-02, avg batch time: 0.4648, average train loss: 2.2364
[09/26 12:08:37 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1575, average loss: 2.2238
[09/26 12:08:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 12:08:37 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.11020177413231333
[09/26 12:08:43 visual_prompt]: Epoch 73 / 100: avg data time: 6.40e-02, avg batch time: 0.4758, average train loss: 2.2370
[09/26 12:08:45 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1583, average loss: 2.2159
[09/26 12:08:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:08:45 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.10305368692688174
[09/26 12:08:51 visual_prompt]: Epoch 74 / 100: avg data time: 6.07e-02, avg batch time: 0.4721, average train loss: 2.2410
[09/26 12:08:53 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1582, average loss: 2.2230
[09/26 12:08:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:08:53 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.09608463116858543
[09/26 12:08:59 visual_prompt]: Epoch 75 / 100: avg data time: 5.97e-02, avg batch time: 0.4717, average train loss: 2.2381
[09/26 12:09:01 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1582, average loss: 2.2215
[09/26 12:09:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:09:01 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.08930309757836516
[09/26 12:09:07 visual_prompt]: Epoch 76 / 100: avg data time: 6.10e-02, avg batch time: 0.4746, average train loss: 2.2409
[09/26 12:09:09 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1584, average loss: 2.2187
[09/26 12:09:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 12:09:09 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.08271734841028552
[09/26 12:09:15 visual_prompt]: Epoch 77 / 100: avg data time: 6.53e-02, avg batch time: 0.4778, average train loss: 2.2372
[09/26 12:09:17 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1585, average loss: 2.2187
[09/26 12:09:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:09:17 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.07633540738525066
[09/26 12:09:23 visual_prompt]: Epoch 78 / 100: avg data time: 5.71e-02, avg batch time: 0.4695, average train loss: 2.2364
[09/26 12:09:25 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1579, average loss: 2.2138
[09/26 12:09:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:09:25 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.07016504991533726
[09/26 12:09:31 visual_prompt]: Epoch 79 / 100: avg data time: 6.09e-02, avg batch time: 0.4741, average train loss: 2.2358
[09/26 12:09:33 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1585, average loss: 2.2130
[09/26 12:09:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:09:33 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.06421379363065141
[09/26 12:09:39 visual_prompt]: Epoch 80 / 100: avg data time: 5.56e-02, avg batch time: 0.4697, average train loss: 2.2354
[09/26 12:09:41 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1585, average loss: 2.2096
[09/26 12:09:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:09:41 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.058488889220255524
[09/26 12:09:48 visual_prompt]: Epoch 81 / 100: avg data time: 5.79e-02, avg batch time: 0.4708, average train loss: 2.2250
[09/26 12:09:49 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1580, average loss: 2.2311
[09/26 12:09:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:09:49 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.052997311598319524
[09/26 12:09:55 visual_prompt]: Epoch 82 / 100: avg data time: 5.25e-02, avg batch time: 0.4650, average train loss: 2.2298
[09/26 12:09:57 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.1577, average loss: 2.2166
[09/26 12:09:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.50	
[09/26 12:09:57 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.047745751406263165
[09/26 12:10:03 visual_prompt]: Epoch 83 / 100: avg data time: 5.82e-02, avg batch time: 0.4702, average train loss: 2.2179
[09/26 12:10:05 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1585, average loss: 2.1909
[09/26 12:10:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 66.50	
[09/26 12:10:05 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.042740606861239594
[09/26 12:10:12 visual_prompt]: Epoch 84 / 100: avg data time: 5.84e-02, avg batch time: 0.4721, average train loss: 2.1952
[09/26 12:10:13 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1585, average loss: 2.1612
[09/26 12:10:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 32.50	top5: 64.50	
[09/26 12:10:13 visual_prompt]: Best epoch 84: best metric: 0.325
[09/26 12:10:13 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.03798797596089351
[09/26 12:10:20 visual_prompt]: Epoch 85 / 100: avg data time: 6.35e-02, avg batch time: 0.4776, average train loss: 2.2350
[09/26 12:10:21 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1582, average loss: 2.2403
[09/26 12:10:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 64.50	
[09/26 12:10:21 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.03349364905389032
[09/26 12:10:28 visual_prompt]: Epoch 86 / 100: avg data time: 5.60e-02, avg batch time: 0.4693, average train loss: 2.2354
[09/26 12:10:29 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1578, average loss: 2.2037
[09/26 12:10:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:10:29 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.029263101785268253
[09/26 12:10:35 visual_prompt]: Epoch 87 / 100: avg data time: 5.64e-02, avg batch time: 0.4700, average train loss: 2.1833
[09/26 12:10:37 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1586, average loss: 2.6433
[09/26 12:10:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 49.50	
[09/26 12:10:37 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.025301488425208296
[09/26 12:10:44 visual_prompt]: Epoch 88 / 100: avg data time: 5.91e-02, avg batch time: 0.4712, average train loss: 2.2865
[09/26 12:10:45 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1585, average loss: 2.1980
[09/26 12:10:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 69.00	
[09/26 12:10:45 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.021613635589349756
[09/26 12:10:52 visual_prompt]: Epoch 89 / 100: avg data time: 5.76e-02, avg batch time: 0.4714, average train loss: 2.2405
[09/26 12:10:53 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1587, average loss: 2.2195
[09/26 12:10:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 66.00	
[09/26 12:10:53 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.01820403635830317
[09/26 12:11:00 visual_prompt]: Epoch 90 / 100: avg data time: 5.30e-02, avg batch time: 0.4667, average train loss: 2.2398
[09/26 12:11:01 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1582, average loss: 2.2232
[09/26 12:11:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 12:11:01 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.01507684480352292
[09/26 12:11:08 visual_prompt]: Epoch 91 / 100: avg data time: 6.02e-02, avg batch time: 0.4719, average train loss: 2.2313
[09/26 12:11:09 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1582, average loss: 2.2113
[09/26 12:11:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:11:09 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.012235870926211617
[09/26 12:11:16 visual_prompt]: Epoch 92 / 100: avg data time: 5.74e-02, avg batch time: 0.4702, average train loss: 2.2274
[09/26 12:11:17 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1585, average loss: 2.2038
[09/26 12:11:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:11:17 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.009684576015420276
[09/26 12:11:23 visual_prompt]: Epoch 93 / 100: avg data time: 5.33e-02, avg batch time: 0.4661, average train loss: 2.2108
[09/26 12:11:25 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1586, average loss: 2.2058
[09/26 12:11:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.00	
[09/26 12:11:25 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.007426068431000882
[09/26 12:11:31 visual_prompt]: Epoch 94 / 100: avg data time: 5.05e-02, avg batch time: 0.4633, average train loss: 2.1841
[09/26 12:11:33 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1583, average loss: 2.2082
[09/26 12:11:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:11:33 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.005463099816548578
[09/26 12:11:40 visual_prompt]: Epoch 95 / 100: avg data time: 6.31e-02, avg batch time: 0.4765, average train loss: 2.2263
[09/26 12:11:41 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1581, average loss: 2.2163
[09/26 12:11:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:11:41 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.003798061746947995
[09/26 12:11:48 visual_prompt]: Epoch 96 / 100: avg data time: 6.03e-02, avg batch time: 0.4721, average train loss: 2.2352
[09/26 12:11:49 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1583, average loss: 2.2141
[09/26 12:11:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:11:49 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0024329828146074095
[09/26 12:11:56 visual_prompt]: Epoch 97 / 100: avg data time: 6.07e-02, avg batch time: 0.4724, average train loss: 2.2308
[09/26 12:11:57 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1584, average loss: 2.2138
[09/26 12:11:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:11:57 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0013695261579316775
[09/26 12:12:04 visual_prompt]: Epoch 98 / 100: avg data time: 5.91e-02, avg batch time: 0.4711, average train loss: 2.2295
[09/26 12:12:05 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1584, average loss: 2.2136
[09/26 12:12:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:12:05 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0006089874350439506
[09/26 12:12:12 visual_prompt]: Epoch 99 / 100: avg data time: 6.06e-02, avg batch time: 0.4724, average train loss: 2.2280
[09/26 12:12:13 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1580, average loss: 2.2101
[09/26 12:12:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:12:13 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00015229324522605947
[09/26 12:12:20 visual_prompt]: Epoch 100 / 100: avg data time: 5.79e-02, avg batch time: 0.4698, average train loss: 2.2256
[09/26 12:12:21 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1582, average loss: 2.2140
[09/26 12:12:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:12:21 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 12:12:21 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 12:12:21 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 12:12:21 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 12:12:21 visual_prompt]: Training with config:
[09/26 12:12:21 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.5_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 12:12:21 visual_prompt]: Loading training data...
[09/26 12:12:21 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 12:12:22 visual_prompt]: Number of images: 800
[09/26 12:12:22 visual_prompt]: Number of classes: 10 / 10
[09/26 12:12:22 visual_prompt]: Loading validation data...
[09/26 12:12:22 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 12:12:23 visual_prompt]: Number of images: 200
[09/26 12:12:23 visual_prompt]: Number of classes: 10 / 10
[09/26 12:12:23 visual_prompt]: Constructing models...
[09/26 12:12:25 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 12:12:25 visual_prompt]: tuned percent:0.543
[09/26 12:12:25 visual_prompt]: Device used for model: 0
[09/26 12:12:25 visual_prompt]: Setting up Evaluator...
[09/26 12:12:25 visual_prompt]: Setting up Trainer...
[09/26 12:12:25 visual_prompt]: 	Setting up the optimizer...
[09/26 12:12:25 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 12:12:32 visual_prompt]: Epoch 1 / 100: avg data time: 5.90e-02, avg batch time: 0.4786, average train loss: 2.6849
[09/26 12:12:33 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1574, average loss: 2.6214
[09/26 12:12:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 12:12:33 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 12:12:33 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[09/26 12:12:40 visual_prompt]: Epoch 2 / 100: avg data time: 5.84e-02, avg batch time: 0.4706, average train loss: 2.7623
[09/26 12:12:42 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1575, average loss: 2.2496
[09/26 12:12:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 12:12:42 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[09/26 12:12:48 visual_prompt]: Epoch 3 / 100: avg data time: 6.12e-02, avg batch time: 0.4733, average train loss: 2.3186
[09/26 12:12:50 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1580, average loss: 2.2274
[09/26 12:12:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 12:12:50 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[09/26 12:12:56 visual_prompt]: Epoch 4 / 100: avg data time: 5.28e-02, avg batch time: 0.4662, average train loss: 2.2597
[09/26 12:12:57 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1583, average loss: 2.2187
[09/26 12:12:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 67.00	
[09/26 12:12:57 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[09/26 12:13:04 visual_prompt]: Epoch 5 / 100: avg data time: 5.67e-02, avg batch time: 0.4691, average train loss: 2.2889
[09/26 12:13:05 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1580, average loss: 2.2841
[09/26 12:13:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.00	
[09/26 12:13:05 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[09/26 12:13:12 visual_prompt]: Epoch 6 / 100: avg data time: 6.13e-02, avg batch time: 0.4738, average train loss: 2.2957
[09/26 12:13:14 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1583, average loss: 2.2663
[09/26 12:13:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 19.50	top5: 62.50	
[09/26 12:13:14 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[09/26 12:13:20 visual_prompt]: Epoch 7 / 100: avg data time: 6.12e-02, avg batch time: 0.4744, average train loss: 2.2674
[09/26 12:13:22 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.1583, average loss: 2.2744
[09/26 12:13:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 8.50	top5: 60.00	
[09/26 12:13:22 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[09/26 12:13:28 visual_prompt]: Epoch 8 / 100: avg data time: 6.29e-02, avg batch time: 0.4774, average train loss: 2.2871
[09/26 12:13:30 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1583, average loss: 2.2345
[09/26 12:13:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:13:30 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[09/26 12:13:36 visual_prompt]: Epoch 9 / 100: avg data time: 5.19e-02, avg batch time: 0.4682, average train loss: 2.3118
[09/26 12:13:38 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1580, average loss: 2.3107
[09/26 12:13:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 56.50	
[09/26 12:13:38 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[09/26 12:13:44 visual_prompt]: Epoch 10 / 100: avg data time: 5.53e-02, avg batch time: 0.4684, average train loss: 2.2983
[09/26 12:13:46 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1585, average loss: 2.2996
[09/26 12:13:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 12:13:46 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[09/26 12:13:52 visual_prompt]: Epoch 11 / 100: avg data time: 5.87e-02, avg batch time: 0.4725, average train loss: 2.3113
[09/26 12:13:54 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1581, average loss: 2.2103
[09/26 12:13:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 12:13:54 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[09/26 12:14:00 visual_prompt]: Epoch 12 / 100: avg data time: 4.95e-02, avg batch time: 0.4635, average train loss: 2.2400
[09/26 12:14:02 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1582, average loss: 2.1720
[09/26 12:14:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 30.00	top5: 64.00	
[09/26 12:14:02 visual_prompt]: Best epoch 12: best metric: 0.300
[09/26 12:14:02 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[09/26 12:14:08 visual_prompt]: Epoch 13 / 100: avg data time: 6.03e-02, avg batch time: 0.4730, average train loss: 2.3133
[09/26 12:14:10 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1584, average loss: 2.2855
[09/26 12:14:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 63.50	
[09/26 12:14:10 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[09/26 12:14:16 visual_prompt]: Epoch 14 / 100: avg data time: 5.29e-02, avg batch time: 0.4688, average train loss: 2.2563
[09/26 12:14:18 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1588, average loss: 2.2194
[09/26 12:14:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 12:14:18 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[09/26 12:14:24 visual_prompt]: Epoch 15 / 100: avg data time: 5.53e-02, avg batch time: 0.4686, average train loss: 2.2328
[09/26 12:14:26 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1581, average loss: 2.1429
[09/26 12:14:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 26.50	top5: 61.00	
[09/26 12:14:26 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[09/26 12:14:32 visual_prompt]: Epoch 16 / 100: avg data time: 6.72e-02, avg batch time: 0.4793, average train loss: 2.0430
[09/26 12:14:34 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1586, average loss: 1.7940
[09/26 12:14:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 34.00	top5: 76.00	
[09/26 12:14:34 visual_prompt]: Best epoch 16: best metric: 0.340
[09/26 12:14:34 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[09/26 12:14:40 visual_prompt]: Epoch 17 / 100: avg data time: 6.06e-02, avg batch time: 0.4740, average train loss: 1.8762
[09/26 12:14:42 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1585, average loss: 1.8118
[09/26 12:14:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 44.00	top5: 83.00	
[09/26 12:14:42 visual_prompt]: Best epoch 17: best metric: 0.440
[09/26 12:14:42 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[09/26 12:14:48 visual_prompt]: Epoch 18 / 100: avg data time: 6.31e-02, avg batch time: 0.4762, average train loss: 1.7573
[09/26 12:14:50 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1579, average loss: 1.6593
[09/26 12:14:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 44.00	top5: 86.50	
[09/26 12:14:50 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[09/26 12:14:57 visual_prompt]: Epoch 19 / 100: avg data time: 6.74e-02, avg batch time: 0.4813, average train loss: 1.5170
[09/26 12:14:58 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1581, average loss: 1.3935
[09/26 12:14:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 51.00	top5: 89.50	
[09/26 12:14:58 visual_prompt]: Best epoch 19: best metric: 0.510
[09/26 12:14:58 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[09/26 12:15:05 visual_prompt]: Epoch 20 / 100: avg data time: 5.99e-02, avg batch time: 0.4732, average train loss: 1.5168
[09/26 12:15:06 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1579, average loss: 1.3822
[09/26 12:15:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 52.00	top5: 92.00	
[09/26 12:15:06 visual_prompt]: Best epoch 20: best metric: 0.520
[09/26 12:15:06 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[09/26 12:15:13 visual_prompt]: Epoch 21 / 100: avg data time: 6.65e-02, avg batch time: 0.4791, average train loss: 1.4560
[09/26 12:15:14 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1581, average loss: 1.3178
[09/26 12:15:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 52.50	top5: 93.50	
[09/26 12:15:14 visual_prompt]: Best epoch 21: best metric: 0.525
[09/26 12:15:14 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[09/26 12:15:21 visual_prompt]: Epoch 22 / 100: avg data time: 5.46e-02, avg batch time: 0.4691, average train loss: 1.5337
[09/26 12:15:22 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1583, average loss: 1.4152
[09/26 12:15:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 50.50	top5: 94.50	
[09/26 12:15:22 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[09/26 12:15:29 visual_prompt]: Epoch 23 / 100: avg data time: 6.19e-02, avg batch time: 0.4745, average train loss: 1.2969
[09/26 12:15:30 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1581, average loss: 1.0973
[09/26 12:15:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 58.00	top5: 95.50	
[09/26 12:15:30 visual_prompt]: Best epoch 23: best metric: 0.580
[09/26 12:15:30 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[09/26 12:15:37 visual_prompt]: Epoch 24 / 100: avg data time: 5.21e-02, avg batch time: 0.4660, average train loss: 1.0765
[09/26 12:15:38 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1584, average loss: 1.6398
[09/26 12:15:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 48.00	top5: 89.50	
[09/26 12:15:38 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[09/26 12:15:45 visual_prompt]: Epoch 25 / 100: avg data time: 6.28e-02, avg batch time: 0.4765, average train loss: 1.1413
[09/26 12:15:46 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1582, average loss: 1.0264
[09/26 12:15:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.00	top5: 93.50	
[09/26 12:15:46 visual_prompt]: Best epoch 25: best metric: 0.670
[09/26 12:15:46 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[09/26 12:15:53 visual_prompt]: Epoch 26 / 100: avg data time: 5.93e-02, avg batch time: 0.4739, average train loss: 0.9353
[09/26 12:15:54 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1583, average loss: 1.1940
[09/26 12:15:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 61.50	top5: 96.00	
[09/26 12:15:54 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[09/26 12:16:01 visual_prompt]: Epoch 27 / 100: avg data time: 6.30e-02, avg batch time: 0.4766, average train loss: 0.8373
[09/26 12:16:02 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1578, average loss: 0.9116
[09/26 12:16:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.00	top5: 96.00	
[09/26 12:16:02 visual_prompt]: Best epoch 27: best metric: 0.700
[09/26 12:16:02 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[09/26 12:16:09 visual_prompt]: Epoch 28 / 100: avg data time: 5.47e-02, avg batch time: 0.4693, average train loss: 0.8097
[09/26 12:16:10 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1586, average loss: 0.9061
[09/26 12:16:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.00	top5: 95.50	
[09/26 12:16:10 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[09/26 12:16:17 visual_prompt]: Epoch 29 / 100: avg data time: 5.82e-02, avg batch time: 0.4734, average train loss: 0.7185
[09/26 12:16:19 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1582, average loss: 0.7352
[09/26 12:16:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.50	top5: 99.50	
[09/26 12:16:19 visual_prompt]: Best epoch 29: best metric: 0.745
[09/26 12:16:19 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[09/26 12:16:25 visual_prompt]: Epoch 30 / 100: avg data time: 4.75e-02, avg batch time: 0.4630, average train loss: 0.5134
[09/26 12:16:26 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1585, average loss: 1.2307
[09/26 12:16:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 61.00	top5: 94.50	
[09/26 12:16:26 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[09/26 12:16:33 visual_prompt]: Epoch 31 / 100: avg data time: 6.49e-02, avg batch time: 0.4785, average train loss: 0.4857
[09/26 12:16:35 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1583, average loss: 1.0871
[09/26 12:16:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.50	top5: 97.00	
[09/26 12:16:35 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[09/26 12:16:41 visual_prompt]: Epoch 32 / 100: avg data time: 5.64e-02, avg batch time: 0.4693, average train loss: 0.5866
[09/26 12:16:43 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1581, average loss: 0.6464
[09/26 12:16:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 99.00	
[09/26 12:16:43 visual_prompt]: Best epoch 32: best metric: 0.810
[09/26 12:16:43 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[09/26 12:16:49 visual_prompt]: Epoch 33 / 100: avg data time: 6.48e-02, avg batch time: 0.4784, average train loss: 0.3941
[09/26 12:16:51 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1577, average loss: 0.7545
[09/26 12:16:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.00	top5: 98.50	
[09/26 12:16:51 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[09/26 12:16:57 visual_prompt]: Epoch 34 / 100: avg data time: 6.09e-02, avg batch time: 0.4737, average train loss: 0.4120
[09/26 12:16:59 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1583, average loss: 0.8013
[09/26 12:16:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.50	top5: 97.00	
[09/26 12:16:59 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[09/26 12:17:05 visual_prompt]: Epoch 35 / 100: avg data time: 5.97e-02, avg batch time: 0.4729, average train loss: 0.3856
[09/26 12:17:07 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.1583, average loss: 0.5976
[09/26 12:17:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.50	top5: 98.00	
[09/26 12:17:07 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[09/26 12:17:13 visual_prompt]: Epoch 36 / 100: avg data time: 6.31e-02, avg batch time: 0.4767, average train loss: 0.2579
[09/26 12:17:15 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1586, average loss: 0.7324
[09/26 12:17:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.50	top5: 98.50	
[09/26 12:17:15 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[09/26 12:17:21 visual_prompt]: Epoch 37 / 100: avg data time: 5.79e-02, avg batch time: 0.4724, average train loss: 0.3272
[09/26 12:17:23 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1581, average loss: 0.7310
[09/26 12:17:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 98.50	
[09/26 12:17:23 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[09/26 12:17:29 visual_prompt]: Epoch 38 / 100: avg data time: 5.92e-02, avg batch time: 0.4719, average train loss: 0.3299
[09/26 12:17:31 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1581, average loss: 0.8718
[09/26 12:17:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.50	top5: 97.50	
[09/26 12:17:31 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[09/26 12:17:38 visual_prompt]: Epoch 39 / 100: avg data time: 6.11e-02, avg batch time: 0.4735, average train loss: 0.2389
[09/26 12:17:39 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1586, average loss: 0.5881
[09/26 12:17:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 98.50	
[09/26 12:17:39 visual_prompt]: Best epoch 39: best metric: 0.830
[09/26 12:17:39 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[09/26 12:17:46 visual_prompt]: Epoch 40 / 100: avg data time: 5.47e-02, avg batch time: 0.4699, average train loss: 0.1693
[09/26 12:17:47 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1580, average loss: 0.7275
[09/26 12:17:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 97.50	
[09/26 12:17:47 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[09/26 12:17:54 visual_prompt]: Epoch 41 / 100: avg data time: 6.31e-02, avg batch time: 0.4769, average train loss: 0.2753
[09/26 12:17:55 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1587, average loss: 0.5017
[09/26 12:17:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 12:17:55 visual_prompt]: Best epoch 41: best metric: 0.850
[09/26 12:17:55 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[09/26 12:18:02 visual_prompt]: Epoch 42 / 100: avg data time: 6.65e-02, avg batch time: 0.4789, average train loss: 0.1464
[09/26 12:18:03 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1582, average loss: 0.5363
[09/26 12:18:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 98.50	
[09/26 12:18:03 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[09/26 12:18:10 visual_prompt]: Epoch 43 / 100: avg data time: 4.71e-02, avg batch time: 0.4606, average train loss: 0.2204
[09/26 12:18:11 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1581, average loss: 0.5740
[09/26 12:18:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.50	
[09/26 12:18:11 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[09/26 12:18:18 visual_prompt]: Epoch 44 / 100: avg data time: 6.52e-02, avg batch time: 0.4779, average train loss: 0.1904
[09/26 12:18:19 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1585, average loss: 0.5491
[09/26 12:18:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.50	
[09/26 12:18:19 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[09/26 12:18:26 visual_prompt]: Epoch 45 / 100: avg data time: 5.10e-02, avg batch time: 0.4655, average train loss: 0.1606
[09/26 12:18:27 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1582, average loss: 0.7645
[09/26 12:18:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 98.00	
[09/26 12:18:27 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[09/26 12:18:34 visual_prompt]: Epoch 46 / 100: avg data time: 6.41e-02, avg batch time: 0.4765, average train loss: 0.2002
[09/26 12:18:35 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1578, average loss: 0.7238
[09/26 12:18:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.50	top5: 98.00	
[09/26 12:18:35 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[09/26 12:18:42 visual_prompt]: Epoch 47 / 100: avg data time: 5.27e-02, avg batch time: 0.4659, average train loss: 0.1170
[09/26 12:18:43 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 0.4740
[09/26 12:18:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 99.00	
[09/26 12:18:43 visual_prompt]: Best epoch 47: best metric: 0.855
[09/26 12:18:43 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.3189093389542498
[09/26 12:18:50 visual_prompt]: Epoch 48 / 100: avg data time: 6.42e-02, avg batch time: 0.4768, average train loss: 0.1018
[09/26 12:18:51 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1584, average loss: 0.5926
[09/26 12:18:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 12:18:51 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.3104804738999169
[09/26 12:18:58 visual_prompt]: Epoch 49 / 100: avg data time: 6.15e-02, avg batch time: 0.4738, average train loss: 0.1148
[09/26 12:19:00 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1579, average loss: 0.4535
[09/26 12:19:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 98.50	
[09/26 12:19:00 visual_prompt]: Best epoch 49: best metric: 0.860
[09/26 12:19:00 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.3019779227044398
[09/26 12:19:06 visual_prompt]: Epoch 50 / 100: avg data time: 6.55e-02, avg batch time: 0.4777, average train loss: 0.0649
[09/26 12:19:08 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1583, average loss: 0.4906
[09/26 12:19:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 99.00	
[09/26 12:19:08 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.29341204441673263
[09/26 12:19:14 visual_prompt]: Epoch 51 / 100: avg data time: 6.62e-02, avg batch time: 0.4793, average train loss: 0.0594
[09/26 12:19:16 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1581, average loss: 0.5365
[09/26 12:19:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 98.00	
[09/26 12:19:16 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.28479327524001635
[09/26 12:19:22 visual_prompt]: Epoch 52 / 100: avg data time: 5.09e-02, avg batch time: 0.4634, average train loss: 0.0475
[09/26 12:19:24 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1591, average loss: 0.4735
[09/26 12:19:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 99.50	
[09/26 12:19:24 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.2761321158169134
[09/26 12:19:30 visual_prompt]: Epoch 53 / 100: avg data time: 6.41e-02, avg batch time: 0.4765, average train loss: 0.1213
[09/26 12:19:32 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1586, average loss: 0.7992
[09/26 12:19:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 97.00	
[09/26 12:19:32 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.2674391184360313
[09/26 12:19:38 visual_prompt]: Epoch 54 / 100: avg data time: 6.13e-02, avg batch time: 0.4745, average train loss: 0.2818
[09/26 12:19:40 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1581, average loss: 0.6240
[09/26 12:19:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 97.50	
[09/26 12:19:40 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.2587248741756253
[09/26 12:19:46 visual_prompt]: Epoch 55 / 100: avg data time: 5.78e-02, avg batch time: 0.4721, average train loss: 0.1920
[09/26 12:19:48 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1582, average loss: 0.5403
[09/26 12:19:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.50	
[09/26 12:19:48 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.25
[09/26 12:19:54 visual_prompt]: Epoch 56 / 100: avg data time: 5.92e-02, avg batch time: 0.4728, average train loss: 0.1291
[09/26 12:19:56 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1583, average loss: 0.5465
[09/26 12:19:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 98.00	
[09/26 12:19:56 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.24127512582437483
[09/26 12:20:02 visual_prompt]: Epoch 57 / 100: avg data time: 5.27e-02, avg batch time: 0.4665, average train loss: 0.0539
[09/26 12:20:04 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1578, average loss: 0.5479
[09/26 12:20:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 98.50	
[09/26 12:20:04 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.23256088156396867
[09/26 12:20:10 visual_prompt]: Epoch 58 / 100: avg data time: 6.00e-02, avg batch time: 0.4732, average train loss: 0.0405
[09/26 12:20:12 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1580, average loss: 0.5755
[09/26 12:20:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 98.50	
[09/26 12:20:12 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.22386788418308667
[09/26 12:20:18 visual_prompt]: Epoch 59 / 100: avg data time: 5.83e-02, avg batch time: 0.4716, average train loss: 0.0272
[09/26 12:20:20 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1583, average loss: 0.5910
[09/26 12:20:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 98.00	
[09/26 12:20:20 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.2152067247599837
[09/26 12:20:26 visual_prompt]: Epoch 60 / 100: avg data time: 4.77e-02, avg batch time: 0.4631, average train loss: 0.0345
[09/26 12:20:28 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1580, average loss: 0.4877
[09/26 12:20:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 98.00	
[09/26 12:20:28 visual_prompt]: Best epoch 60: best metric: 0.880
[09/26 12:20:28 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.20658795558326742
[09/26 12:20:34 visual_prompt]: Epoch 61 / 100: avg data time: 4.81e-02, avg batch time: 0.4632, average train loss: 0.0207
[09/26 12:20:36 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1583, average loss: 0.4692
[09/26 12:20:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 99.00	
[09/26 12:20:36 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.1980220772955602
[09/26 12:20:42 visual_prompt]: Epoch 62 / 100: avg data time: 6.77e-02, avg batch time: 0.4803, average train loss: 0.0112
[09/26 12:20:44 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1581, average loss: 0.5801
[09/26 12:20:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 98.50	
[09/26 12:20:44 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.1895195261000831
[09/26 12:20:50 visual_prompt]: Epoch 63 / 100: avg data time: 4.99e-02, avg batch time: 0.4646, average train loss: 0.0091
[09/26 12:20:52 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1578, average loss: 0.5138
[09/26 12:20:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 99.00	
[09/26 12:20:52 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.18109066104575022
[09/26 12:20:58 visual_prompt]: Epoch 64 / 100: avg data time: 5.13e-02, avg batch time: 0.4649, average train loss: 0.0066
[09/26 12:21:00 visual_prompt]: Inference (val):avg data time: 4.73e-05, avg batch time: 0.1582, average loss: 0.4972
[09/26 12:21:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 12:21:00 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.17274575140626316
[09/26 12:21:06 visual_prompt]: Epoch 65 / 100: avg data time: 6.03e-02, avg batch time: 0.4725, average train loss: 0.0038
[09/26 12:21:08 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1584, average loss: 0.5217
[09/26 12:21:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.50	top5: 98.00	
[09/26 12:21:08 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.16449496416858284
[09/26 12:21:14 visual_prompt]: Epoch 66 / 100: avg data time: 5.54e-02, avg batch time: 0.4687, average train loss: 0.0029
[09/26 12:21:16 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1581, average loss: 0.5081
[09/26 12:21:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 98.50	
[09/26 12:21:16 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.15634835164602198
[09/26 12:21:22 visual_prompt]: Epoch 67 / 100: avg data time: 5.90e-02, avg batch time: 0.4721, average train loss: 0.0029
[09/26 12:21:24 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1581, average loss: 0.4732
[09/26 12:21:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 98.00	
[09/26 12:21:24 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.14831583923105
[09/26 12:21:30 visual_prompt]: Epoch 68 / 100: avg data time: 6.20e-02, avg batch time: 0.4754, average train loss: 0.0026
[09/26 12:21:32 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1584, average loss: 0.4882
[09/26 12:21:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 12:21:32 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.14040721330273062
[09/26 12:21:39 visual_prompt]: Epoch 69 / 100: avg data time: 5.61e-02, avg batch time: 0.4705, average train loss: 0.0027
[09/26 12:21:40 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1582, average loss: 0.4935
[09/26 12:21:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.50	top5: 98.50	
[09/26 12:21:40 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.13263210930352737
[09/26 12:21:47 visual_prompt]: Epoch 70 / 100: avg data time: 6.16e-02, avg batch time: 0.4741, average train loss: 0.0022
[09/26 12:21:48 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1574, average loss: 0.4892
[09/26 12:21:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 98.50	
[09/26 12:21:48 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.12500000000000006
[09/26 12:21:55 visual_prompt]: Epoch 71 / 100: avg data time: 6.69e-02, avg batch time: 0.4793, average train loss: 0.0022
[09/26 12:21:56 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1584, average loss: 0.4916
[09/26 12:21:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 98.50	
[09/26 12:21:56 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.1175201839416988
[09/26 12:22:03 visual_prompt]: Epoch 72 / 100: avg data time: 5.34e-02, avg batch time: 0.4671, average train loss: 0.0022
[09/26 12:22:04 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1579, average loss: 0.4906
[09/26 12:22:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 98.50	
[09/26 12:22:04 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.11020177413231333
[09/26 12:22:11 visual_prompt]: Epoch 73 / 100: avg data time: 5.84e-02, avg batch time: 0.4726, average train loss: 0.0024
[09/26 12:22:12 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1580, average loss: 0.4899
[09/26 12:22:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 98.50	
[09/26 12:22:12 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.10305368692688174
[09/26 12:22:19 visual_prompt]: Epoch 74 / 100: avg data time: 5.70e-02, avg batch time: 0.4708, average train loss: 0.0022
[09/26 12:22:21 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1585, average loss: 0.4864
[09/26 12:22:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 98.50	
[09/26 12:22:21 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.09608463116858543
[09/26 12:22:27 visual_prompt]: Epoch 75 / 100: avg data time: 5.92e-02, avg batch time: 0.4724, average train loss: 0.0022
[09/26 12:22:29 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1584, average loss: 0.4974
[09/26 12:22:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 98.50	
[09/26 12:22:29 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.08930309757836516
[09/26 12:22:35 visual_prompt]: Epoch 76 / 100: avg data time: 6.35e-02, avg batch time: 0.4760, average train loss: 0.0022
[09/26 12:22:37 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1580, average loss: 0.4987
[09/26 12:22:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 98.50	
[09/26 12:22:37 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.08271734841028552
[09/26 12:22:43 visual_prompt]: Epoch 77 / 100: avg data time: 4.83e-02, avg batch time: 0.4617, average train loss: 0.0022
[09/26 12:22:45 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1579, average loss: 0.4961
[09/26 12:22:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 98.50	
[09/26 12:22:45 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.07633540738525066
[09/26 12:22:51 visual_prompt]: Epoch 78 / 100: avg data time: 6.35e-02, avg batch time: 0.4759, average train loss: 0.0024
[09/26 12:22:53 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1581, average loss: 0.4953
[09/26 12:22:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.50	
[09/26 12:22:53 visual_prompt]: Best epoch 78: best metric: 0.885
[09/26 12:22:53 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.07016504991533726
[09/26 12:22:59 visual_prompt]: Epoch 79 / 100: avg data time: 5.40e-02, avg batch time: 0.4698, average train loss: 0.0020
[09/26 12:23:01 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1579, average loss: 0.5000
[09/26 12:23:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.50	
[09/26 12:23:01 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.06421379363065141
[09/26 12:23:07 visual_prompt]: Epoch 80 / 100: avg data time: 6.07e-02, avg batch time: 0.4740, average train loss: 0.0023
[09/26 12:23:09 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1584, average loss: 0.5058
[09/26 12:23:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 98.50	
[09/26 12:23:09 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.058488889220255524
[09/26 12:23:15 visual_prompt]: Epoch 81 / 100: avg data time: 5.36e-02, avg batch time: 0.4685, average train loss: 0.0022
[09/26 12:23:17 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.1584, average loss: 0.5101
[09/26 12:23:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 99.00	
[09/26 12:23:17 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.052997311598319524
[09/26 12:23:23 visual_prompt]: Epoch 82 / 100: avg data time: 6.31e-02, avg batch time: 0.4772, average train loss: 0.0021
[09/26 12:23:25 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1583, average loss: 0.5085
[09/26 12:23:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 99.00	
[09/26 12:23:25 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.047745751406263165
[09/26 12:23:31 visual_prompt]: Epoch 83 / 100: avg data time: 6.34e-02, avg batch time: 0.4766, average train loss: 0.0021
[09/26 12:23:33 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1580, average loss: 0.5118
[09/26 12:23:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 99.00	
[09/26 12:23:33 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.042740606861239594
[09/26 12:23:39 visual_prompt]: Epoch 84 / 100: avg data time: 6.49e-02, avg batch time: 0.4773, average train loss: 0.0021
[09/26 12:23:41 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1580, average loss: 0.5051
[09/26 12:23:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 98.50	
[09/26 12:23:41 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.03798797596089351
[09/26 12:23:47 visual_prompt]: Epoch 85 / 100: avg data time: 5.31e-02, avg batch time: 0.4663, average train loss: 0.0020
[09/26 12:23:49 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1585, average loss: 0.5046
[09/26 12:23:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.50	
[09/26 12:23:49 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.03349364905389032
[09/26 12:23:55 visual_prompt]: Epoch 86 / 100: avg data time: 6.08e-02, avg batch time: 0.4733, average train loss: 0.0022
[09/26 12:23:57 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1581, average loss: 0.5050
[09/26 12:23:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.50	
[09/26 12:23:57 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.029263101785268253
[09/26 12:24:03 visual_prompt]: Epoch 87 / 100: avg data time: 5.31e-02, avg batch time: 0.4684, average train loss: 0.0021
[09/26 12:24:05 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1585, average loss: 0.5058
[09/26 12:24:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.50	
[09/26 12:24:05 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.025301488425208296
[09/26 12:24:11 visual_prompt]: Epoch 88 / 100: avg data time: 4.71e-02, avg batch time: 0.4640, average train loss: 0.0021
[09/26 12:24:13 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1584, average loss: 0.5080
[09/26 12:24:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.50	
[09/26 12:24:13 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.021613635589349756
[09/26 12:24:19 visual_prompt]: Epoch 89 / 100: avg data time: 5.89e-02, avg batch time: 0.4719, average train loss: 0.0020
[09/26 12:24:21 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1583, average loss: 0.5087
[09/26 12:24:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.50	
[09/26 12:24:21 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.01820403635830317
[09/26 12:24:27 visual_prompt]: Epoch 90 / 100: avg data time: 6.22e-02, avg batch time: 0.4751, average train loss: 0.0021
[09/26 12:24:29 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1585, average loss: 0.5109
[09/26 12:24:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 99.00	
[09/26 12:24:29 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.01507684480352292
[09/26 12:24:35 visual_prompt]: Epoch 91 / 100: avg data time: 6.27e-02, avg batch time: 0.4765, average train loss: 0.0022
[09/26 12:24:37 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1580, average loss: 0.5091
[09/26 12:24:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.50	
[09/26 12:24:37 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.012235870926211617
[09/26 12:24:43 visual_prompt]: Epoch 92 / 100: avg data time: 5.48e-02, avg batch time: 0.4684, average train loss: 0.0020
[09/26 12:24:45 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1581, average loss: 0.5083
[09/26 12:24:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.50	
[09/26 12:24:45 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.009684576015420276
[09/26 12:24:52 visual_prompt]: Epoch 93 / 100: avg data time: 5.87e-02, avg batch time: 0.4733, average train loss: 0.0020
[09/26 12:24:53 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1581, average loss: 0.5091
[09/26 12:24:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.50	
[09/26 12:24:53 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.007426068431000882
[09/26 12:25:00 visual_prompt]: Epoch 94 / 100: avg data time: 5.78e-02, avg batch time: 0.4726, average train loss: 0.0020
[09/26 12:25:01 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 0.5100
[09/26 12:25:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.50	
[09/26 12:25:01 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.005463099816548578
[09/26 12:25:08 visual_prompt]: Epoch 95 / 100: avg data time: 5.59e-02, avg batch time: 0.4714, average train loss: 0.0021
[09/26 12:25:09 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1581, average loss: 0.5094
[09/26 12:25:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.50	
[09/26 12:25:09 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.003798061746947995
[09/26 12:25:16 visual_prompt]: Epoch 96 / 100: avg data time: 6.47e-02, avg batch time: 0.4777, average train loss: 0.0021
[09/26 12:25:17 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1582, average loss: 0.5093
[09/26 12:25:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.50	
[09/26 12:25:17 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0024329828146074095
[09/26 12:25:24 visual_prompt]: Epoch 97 / 100: avg data time: 6.25e-02, avg batch time: 0.4768, average train loss: 0.0021
[09/26 12:25:25 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1583, average loss: 0.5097
[09/26 12:25:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.50	
[09/26 12:25:25 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0013695261579316775
[09/26 12:25:32 visual_prompt]: Epoch 98 / 100: avg data time: 6.77e-02, avg batch time: 0.4808, average train loss: 0.0020
[09/26 12:25:34 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1579, average loss: 0.5097
[09/26 12:25:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.50	
[09/26 12:25:34 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0006089874350439506
[09/26 12:25:40 visual_prompt]: Epoch 99 / 100: avg data time: 6.48e-02, avg batch time: 0.4788, average train loss: 0.0020
[09/26 12:25:42 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1581, average loss: 0.5096
[09/26 12:25:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.50	
[09/26 12:25:42 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00015229324522605947
[09/26 12:25:48 visual_prompt]: Epoch 100 / 100: avg data time: 5.02e-02, avg batch time: 0.4651, average train loss: 0.0020
[09/26 12:25:50 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1586, average loss: 0.5096
[09/26 12:25:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 98.50	
[09/26 12:25:50 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 12:25:50 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 12:25:50 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 12:25:50 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 12:25:50 visual_prompt]: Training with config:
[09/26 12:25:50 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.5_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 12:25:50 visual_prompt]: Loading training data...
[09/26 12:25:50 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 12:25:51 visual_prompt]: Number of images: 800
[09/26 12:25:51 visual_prompt]: Number of classes: 10 / 10
[09/26 12:25:51 visual_prompt]: Loading validation data...
[09/26 12:25:51 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 12:25:51 visual_prompt]: Number of images: 200
[09/26 12:25:51 visual_prompt]: Number of classes: 10 / 10
[09/26 12:25:51 visual_prompt]: Constructing models...
[09/26 12:25:53 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 12:25:53 visual_prompt]: tuned percent:0.543
[09/26 12:25:53 visual_prompt]: Device used for model: 0
[09/26 12:25:53 visual_prompt]: Setting up Evaluator...
[09/26 12:25:53 visual_prompt]: Setting up Trainer...
[09/26 12:25:53 visual_prompt]: 	Setting up the optimizer...
[09/26 12:25:53 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 12:26:00 visual_prompt]: Epoch 1 / 100: avg data time: 6.28e-02, avg batch time: 0.4813, average train loss: 2.6798
[09/26 12:26:02 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1578, average loss: 2.6214
[09/26 12:26:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 12:26:02 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 12:26:02 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[09/26 12:26:08 visual_prompt]: Epoch 2 / 100: avg data time: 5.95e-02, avg batch time: 0.4719, average train loss: 2.8140
[09/26 12:26:10 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1583, average loss: 2.2961
[09/26 12:26:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 7.00	top5: 52.50	
[09/26 12:26:10 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[09/26 12:26:16 visual_prompt]: Epoch 3 / 100: avg data time: 6.04e-02, avg batch time: 0.4720, average train loss: 2.3231
[09/26 12:26:18 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1582, average loss: 2.2331
[09/26 12:26:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.50	top5: 64.50	
[09/26 12:26:18 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[09/26 12:26:24 visual_prompt]: Epoch 4 / 100: avg data time: 5.92e-02, avg batch time: 0.4709, average train loss: 2.2453
[09/26 12:26:26 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1584, average loss: 2.2358
[09/26 12:26:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 12:26:26 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[09/26 12:26:32 visual_prompt]: Epoch 5 / 100: avg data time: 5.65e-02, avg batch time: 0.4693, average train loss: 2.2723
[09/26 12:26:34 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1582, average loss: 2.2597
[09/26 12:26:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 12:26:34 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[09/26 12:26:40 visual_prompt]: Epoch 6 / 100: avg data time: 5.61e-02, avg batch time: 0.4696, average train loss: 2.2766
[09/26 12:26:42 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1579, average loss: 2.2263
[09/26 12:26:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 12:26:42 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[09/26 12:26:48 visual_prompt]: Epoch 7 / 100: avg data time: 5.96e-02, avg batch time: 0.4735, average train loss: 2.2419
[09/26 12:26:50 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1580, average loss: 2.2448
[09/26 12:26:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 12:26:50 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[09/26 12:26:56 visual_prompt]: Epoch 8 / 100: avg data time: 5.95e-02, avg batch time: 0.4733, average train loss: 2.2644
[09/26 12:26:58 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 2.2357
[09/26 12:26:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 64.00	
[09/26 12:26:58 visual_prompt]: Best epoch 8: best metric: 0.245
[09/26 12:26:58 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[09/26 12:27:04 visual_prompt]: Epoch 9 / 100: avg data time: 5.70e-02, avg batch time: 0.4720, average train loss: 2.2211
[09/26 12:27:06 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1582, average loss: 2.3502
[09/26 12:27:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.50	
[09/26 12:27:06 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[09/26 12:27:12 visual_prompt]: Epoch 10 / 100: avg data time: 5.62e-02, avg batch time: 0.4695, average train loss: 2.2329
[09/26 12:27:14 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1574, average loss: 2.1017
[09/26 12:27:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 28.50	top5: 70.00	
[09/26 12:27:14 visual_prompt]: Best epoch 10: best metric: 0.285
[09/26 12:27:14 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[09/26 12:27:20 visual_prompt]: Epoch 11 / 100: avg data time: 5.71e-02, avg batch time: 0.4707, average train loss: 2.2279
[09/26 12:27:22 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1585, average loss: 2.5761
[09/26 12:27:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 43.50	
[09/26 12:27:22 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[09/26 12:27:28 visual_prompt]: Epoch 12 / 100: avg data time: 5.94e-02, avg batch time: 0.4716, average train loss: 2.4203
[09/26 12:27:30 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1580, average loss: 2.3656
[09/26 12:27:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 62.00	
[09/26 12:27:30 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[09/26 12:27:36 visual_prompt]: Epoch 13 / 100: avg data time: 5.18e-02, avg batch time: 0.4659, average train loss: 2.2995
[09/26 12:27:38 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1584, average loss: 2.1919
[09/26 12:27:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 59.00	
[09/26 12:27:38 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[09/26 12:27:44 visual_prompt]: Epoch 14 / 100: avg data time: 6.56e-02, avg batch time: 0.4792, average train loss: 2.1352
[09/26 12:27:46 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1586, average loss: 1.9734
[09/26 12:27:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 33.00	top5: 80.00	
[09/26 12:27:46 visual_prompt]: Best epoch 14: best metric: 0.330
[09/26 12:27:46 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[09/26 12:27:53 visual_prompt]: Epoch 15 / 100: avg data time: 5.72e-02, avg batch time: 0.4709, average train loss: 1.9480
[09/26 12:27:54 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1586, average loss: 1.7269
[09/26 12:27:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 38.50	top5: 90.00	
[09/26 12:27:54 visual_prompt]: Best epoch 15: best metric: 0.385
[09/26 12:27:54 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[09/26 12:28:01 visual_prompt]: Epoch 16 / 100: avg data time: 5.94e-02, avg batch time: 0.4733, average train loss: 1.7453
[09/26 12:28:02 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1584, average loss: 1.6901
[09/26 12:28:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 48.00	top5: 83.50	
[09/26 12:28:02 visual_prompt]: Best epoch 16: best metric: 0.480
[09/26 12:28:02 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[09/26 12:28:09 visual_prompt]: Epoch 17 / 100: avg data time: 5.40e-02, avg batch time: 0.4674, average train loss: 1.6271
[09/26 12:28:10 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1583, average loss: 1.4537
[09/26 12:28:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 39.00	top5: 90.00	
[09/26 12:28:10 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[09/26 12:28:17 visual_prompt]: Epoch 18 / 100: avg data time: 5.45e-02, avg batch time: 0.4698, average train loss: 1.4467
[09/26 12:28:18 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1584, average loss: 1.2408
[09/26 12:28:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 55.50	top5: 93.50	
[09/26 12:28:18 visual_prompt]: Best epoch 18: best metric: 0.555
[09/26 12:28:18 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[09/26 12:28:25 visual_prompt]: Epoch 19 / 100: avg data time: 5.71e-02, avg batch time: 0.4710, average train loss: 1.4622
[09/26 12:28:26 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1584, average loss: 1.4263
[09/26 12:28:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 56.50	top5: 89.50	
[09/26 12:28:26 visual_prompt]: Best epoch 19: best metric: 0.565
[09/26 12:28:26 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[09/26 12:28:33 visual_prompt]: Epoch 20 / 100: avg data time: 6.03e-02, avg batch time: 0.4751, average train loss: 1.3071
[09/26 12:28:34 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1588, average loss: 1.2917
[09/26 12:28:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 52.00	top5: 94.00	
[09/26 12:28:34 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[09/26 12:28:41 visual_prompt]: Epoch 21 / 100: avg data time: 6.54e-02, avg batch time: 0.4786, average train loss: 1.1935
[09/26 12:28:42 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.1585, average loss: 1.3138
[09/26 12:28:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 51.00	top5: 87.50	
[09/26 12:28:42 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[09/26 12:28:49 visual_prompt]: Epoch 22 / 100: avg data time: 5.54e-02, avg batch time: 0.4705, average train loss: 1.1060
[09/26 12:28:50 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1581, average loss: 0.9399
[09/26 12:28:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.50	top5: 96.50	
[09/26 12:28:50 visual_prompt]: Best epoch 22: best metric: 0.655
[09/26 12:28:50 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[09/26 12:28:57 visual_prompt]: Epoch 23 / 100: avg data time: 6.55e-02, avg batch time: 0.4793, average train loss: 0.8974
[09/26 12:28:59 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1583, average loss: 0.9656
[09/26 12:28:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 64.50	top5: 97.00	
[09/26 12:28:59 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[09/26 12:29:05 visual_prompt]: Epoch 24 / 100: avg data time: 6.62e-02, avg batch time: 0.4794, average train loss: 0.9512
[09/26 12:29:07 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1588, average loss: 1.0398
[09/26 12:29:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 63.50	top5: 95.50	
[09/26 12:29:07 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[09/26 12:29:13 visual_prompt]: Epoch 25 / 100: avg data time: 5.44e-02, avg batch time: 0.4694, average train loss: 0.8808
[09/26 12:29:15 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1584, average loss: 1.0392
[09/26 12:29:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 64.00	top5: 96.00	
[09/26 12:29:15 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[09/26 12:29:21 visual_prompt]: Epoch 26 / 100: avg data time: 5.40e-02, avg batch time: 0.4686, average train loss: 0.6341
[09/26 12:29:23 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1581, average loss: 0.8193
[09/26 12:29:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.00	top5: 97.00	
[09/26 12:29:23 visual_prompt]: Best epoch 26: best metric: 0.680
[09/26 12:29:23 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[09/26 12:29:29 visual_prompt]: Epoch 27 / 100: avg data time: 6.22e-02, avg batch time: 0.4758, average train loss: 0.5501
[09/26 12:29:31 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1585, average loss: 0.8144
[09/26 12:29:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.50	top5: 97.00	
[09/26 12:29:31 visual_prompt]: Best epoch 27: best metric: 0.685
[09/26 12:29:31 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[09/26 12:29:37 visual_prompt]: Epoch 28 / 100: avg data time: 5.78e-02, avg batch time: 0.4710, average train loss: 0.6422
[09/26 12:29:39 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1584, average loss: 1.0428
[09/26 12:29:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.00	top5: 96.00	
[09/26 12:29:39 visual_prompt]: Best epoch 28: best metric: 0.690
[09/26 12:29:39 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[09/26 12:29:45 visual_prompt]: Epoch 29 / 100: avg data time: 6.53e-02, avg batch time: 0.4784, average train loss: 0.6691
[09/26 12:29:47 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1584, average loss: 0.7401
[09/26 12:29:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 97.50	
[09/26 12:29:47 visual_prompt]: Best epoch 29: best metric: 0.805
[09/26 12:29:47 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[09/26 12:29:53 visual_prompt]: Epoch 30 / 100: avg data time: 4.69e-02, avg batch time: 0.4610, average train loss: 0.4895
[09/26 12:29:55 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1583, average loss: 0.8449
[09/26 12:29:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 97.00	
[09/26 12:29:55 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[09/26 12:30:01 visual_prompt]: Epoch 31 / 100: avg data time: 5.72e-02, avg batch time: 0.4700, average train loss: 0.4087
[09/26 12:30:03 visual_prompt]: Inference (val):avg data time: 5.03e-05, avg batch time: 0.1583, average loss: 0.9222
[09/26 12:30:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 97.00	
[09/26 12:30:03 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[09/26 12:30:09 visual_prompt]: Epoch 32 / 100: avg data time: 6.12e-02, avg batch time: 0.4742, average train loss: 0.3586
[09/26 12:30:11 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1582, average loss: 0.9089
[09/26 12:30:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 97.50	
[09/26 12:30:11 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[09/26 12:30:17 visual_prompt]: Epoch 33 / 100: avg data time: 6.02e-02, avg batch time: 0.4732, average train loss: 0.3605
[09/26 12:30:19 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1584, average loss: 0.9945
[09/26 12:30:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.00	top5: 97.50	
[09/26 12:30:19 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[09/26 12:30:25 visual_prompt]: Epoch 34 / 100: avg data time: 5.37e-02, avg batch time: 0.4697, average train loss: 0.3476
[09/26 12:30:27 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1587, average loss: 0.7115
[09/26 12:30:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.50	top5: 97.50	
[09/26 12:30:27 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[09/26 12:30:34 visual_prompt]: Epoch 35 / 100: avg data time: 6.33e-02, avg batch time: 0.4772, average train loss: 0.2390
[09/26 12:30:35 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 0.6121
[09/26 12:30:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.50	
[09/26 12:30:35 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[09/26 12:30:42 visual_prompt]: Epoch 36 / 100: avg data time: 6.13e-02, avg batch time: 0.4740, average train loss: 0.2096
[09/26 12:30:43 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1583, average loss: 0.8565
[09/26 12:30:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 97.50	
[09/26 12:30:43 visual_prompt]: Best epoch 36: best metric: 0.820
[09/26 12:30:43 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[09/26 12:30:50 visual_prompt]: Epoch 37 / 100: avg data time: 6.23e-02, avg batch time: 0.4759, average train loss: 0.1732
[09/26 12:30:51 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1582, average loss: 1.0259
[09/26 12:30:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.00	top5: 96.50	
[09/26 12:30:51 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[09/26 12:30:58 visual_prompt]: Epoch 38 / 100: avg data time: 5.69e-02, avg batch time: 0.4711, average train loss: 0.1104
[09/26 12:30:59 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1585, average loss: 0.9425
[09/26 12:30:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.00	top5: 97.50	
[09/26 12:30:59 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[09/26 12:31:06 visual_prompt]: Epoch 39 / 100: avg data time: 5.43e-02, avg batch time: 0.4674, average train loss: 0.1606
[09/26 12:31:07 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1585, average loss: 0.7666
[09/26 12:31:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 12:31:07 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[09/26 12:31:14 visual_prompt]: Epoch 40 / 100: avg data time: 6.41e-02, avg batch time: 0.4785, average train loss: 0.1389
[09/26 12:31:15 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1582, average loss: 0.9682
[09/26 12:31:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.00	
[09/26 12:31:15 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[09/26 12:31:22 visual_prompt]: Epoch 41 / 100: avg data time: 6.12e-02, avg batch time: 0.4745, average train loss: 0.0896
[09/26 12:31:23 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1581, average loss: 0.9564
[09/26 12:31:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 98.50	
[09/26 12:31:23 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[09/26 12:31:30 visual_prompt]: Epoch 42 / 100: avg data time: 6.36e-02, avg batch time: 0.4763, average train loss: 0.0760
[09/26 12:31:32 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1581, average loss: 0.9290
[09/26 12:31:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.00	
[09/26 12:31:32 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[09/26 12:31:38 visual_prompt]: Epoch 43 / 100: avg data time: 6.11e-02, avg batch time: 0.4741, average train loss: 0.0625
[09/26 12:31:40 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1586, average loss: 0.7759
[09/26 12:31:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 98.50	
[09/26 12:31:40 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[09/26 12:31:46 visual_prompt]: Epoch 44 / 100: avg data time: 6.49e-02, avg batch time: 0.4787, average train loss: 0.0411
[09/26 12:31:48 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.1585, average loss: 0.9741
[09/26 12:31:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 97.50	
[09/26 12:31:48 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[09/26 12:31:54 visual_prompt]: Epoch 45 / 100: avg data time: 6.65e-02, avg batch time: 0.4791, average train loss: 0.0706
[09/26 12:31:56 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1612, average loss: 1.3158
[09/26 12:31:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.50	top5: 97.50	
[09/26 12:31:56 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[09/26 12:32:02 visual_prompt]: Epoch 46 / 100: avg data time: 6.02e-02, avg batch time: 0.4766, average train loss: 0.1330
[09/26 12:32:04 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1583, average loss: 1.1597
[09/26 12:32:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.00	top5: 98.50	
[09/26 12:32:04 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[09/26 12:32:10 visual_prompt]: Epoch 47 / 100: avg data time: 4.78e-02, avg batch time: 0.4621, average train loss: 0.1200
[09/26 12:32:12 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1583, average loss: 0.9072
[09/26 12:32:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 97.50	
[09/26 12:32:12 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.3189093389542498
[09/26 12:32:19 visual_prompt]: Epoch 48 / 100: avg data time: 6.54e-02, avg batch time: 0.4784, average train loss: 0.1188
[09/26 12:32:20 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1584, average loss: 0.8266
[09/26 12:32:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 97.50	
[09/26 12:32:20 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.3104804738999169
[09/26 12:32:27 visual_prompt]: Epoch 49 / 100: avg data time: 6.50e-02, avg batch time: 0.4780, average train loss: 0.1097
[09/26 12:32:28 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1582, average loss: 0.8725
[09/26 12:32:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.00	top5: 98.00	
[09/26 12:32:28 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.3019779227044398
[09/26 12:32:35 visual_prompt]: Epoch 50 / 100: avg data time: 5.29e-02, avg batch time: 0.4660, average train loss: 0.0594
[09/26 12:32:36 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1582, average loss: 0.7259
[09/26 12:32:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.50	
[09/26 12:32:36 visual_prompt]: Best epoch 50: best metric: 0.830
[09/26 12:32:36 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.29341204441673263
[09/26 12:32:43 visual_prompt]: Epoch 51 / 100: avg data time: 5.80e-02, avg batch time: 0.4709, average train loss: 0.0633
[09/26 12:32:44 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1584, average loss: 0.7498
[09/26 12:32:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.50	
[09/26 12:32:44 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.28479327524001635
[09/26 12:32:51 visual_prompt]: Epoch 52 / 100: avg data time: 6.15e-02, avg batch time: 0.4751, average train loss: 0.0423
[09/26 12:32:52 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1585, average loss: 0.8755
[09/26 12:32:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.50	
[09/26 12:32:52 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.2761321158169134
[09/26 12:32:59 visual_prompt]: Epoch 53 / 100: avg data time: 6.10e-02, avg batch time: 0.4753, average train loss: 0.0241
[09/26 12:33:00 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1587, average loss: 0.9951
[09/26 12:33:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 97.00	
[09/26 12:33:00 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.2674391184360313
[09/26 12:33:07 visual_prompt]: Epoch 54 / 100: avg data time: 5.87e-02, avg batch time: 0.4718, average train loss: 0.0179
[09/26 12:33:08 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1585, average loss: 1.0149
[09/26 12:33:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 12:33:08 visual_prompt]: Best epoch 54: best metric: 0.835
[09/26 12:33:08 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.2587248741756253
[09/26 12:33:15 visual_prompt]: Epoch 55 / 100: avg data time: 6.17e-02, avg batch time: 0.4748, average train loss: 0.0068
[09/26 12:33:16 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1579, average loss: 1.0731
[09/26 12:33:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 12:33:16 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.25
[09/26 12:33:23 visual_prompt]: Epoch 56 / 100: avg data time: 6.02e-02, avg batch time: 0.4725, average train loss: 0.0029
[09/26 12:33:24 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1582, average loss: 1.1681
[09/26 12:33:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 97.00	
[09/26 12:33:24 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.24127512582437483
[09/26 12:33:31 visual_prompt]: Epoch 57 / 100: avg data time: 5.01e-02, avg batch time: 0.4652, average train loss: 0.0040
[09/26 12:33:32 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1582, average loss: 1.0669
[09/26 12:33:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 97.50	
[09/26 12:33:32 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.23256088156396867
[09/26 12:33:39 visual_prompt]: Epoch 58 / 100: avg data time: 6.02e-02, avg batch time: 0.4732, average train loss: 0.0022
[09/26 12:33:40 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1583, average loss: 1.0876
[09/26 12:33:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.50	
[09/26 12:33:40 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.22386788418308667
[09/26 12:33:47 visual_prompt]: Epoch 59 / 100: avg data time: 6.19e-02, avg batch time: 0.4746, average train loss: 0.0010
[09/26 12:33:48 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1582, average loss: 1.0982
[09/26 12:33:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.00	
[09/26 12:33:48 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.2152067247599837
[09/26 12:33:55 visual_prompt]: Epoch 60 / 100: avg data time: 5.83e-02, avg batch time: 0.4712, average train loss: 0.0007
[09/26 12:33:57 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1582, average loss: 1.1262
[09/26 12:33:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.00	
[09/26 12:33:57 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.20658795558326742
[09/26 12:34:03 visual_prompt]: Epoch 61 / 100: avg data time: 5.49e-02, avg batch time: 0.4684, average train loss: 0.0013
[09/26 12:34:05 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1579, average loss: 1.1365
[09/26 12:34:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.00	
[09/26 12:34:05 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.1980220772955602
[09/26 12:34:11 visual_prompt]: Epoch 62 / 100: avg data time: 6.46e-02, avg batch time: 0.4772, average train loss: 0.0010
[09/26 12:34:13 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1582, average loss: 1.2204
[09/26 12:34:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 97.50	
[09/26 12:34:13 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.1895195261000831
[09/26 12:34:19 visual_prompt]: Epoch 63 / 100: avg data time: 5.78e-02, avg batch time: 0.4712, average train loss: 0.0007
[09/26 12:34:21 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1583, average loss: 1.2143
[09/26 12:34:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 97.50	
[09/26 12:34:21 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.18109066104575022
[09/26 12:34:27 visual_prompt]: Epoch 64 / 100: avg data time: 4.59e-02, avg batch time: 0.4592, average train loss: 0.0007
[09/26 12:34:29 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1583, average loss: 1.1873
[09/26 12:34:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 97.50	
[09/26 12:34:29 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.17274575140626316
[09/26 12:34:35 visual_prompt]: Epoch 65 / 100: avg data time: 5.62e-02, avg batch time: 0.4704, average train loss: 0.0006
[09/26 12:34:37 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1583, average loss: 1.1631
[09/26 12:34:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 97.50	
[09/26 12:34:37 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.16449496416858284
[09/26 12:34:43 visual_prompt]: Epoch 66 / 100: avg data time: 5.45e-02, avg batch time: 0.4674, average train loss: 0.0006
[09/26 12:34:45 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1583, average loss: 1.1515
[09/26 12:34:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 12:34:45 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.15634835164602198
[09/26 12:34:51 visual_prompt]: Epoch 67 / 100: avg data time: 6.06e-02, avg batch time: 0.4738, average train loss: 0.0005
[09/26 12:34:53 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1577, average loss: 1.1472
[09/26 12:34:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 12:34:53 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.14831583923105
[09/26 12:34:59 visual_prompt]: Epoch 68 / 100: avg data time: 5.82e-02, avg batch time: 0.4715, average train loss: 0.0007
[09/26 12:35:01 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1584, average loss: 1.1588
[09/26 12:35:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 12:35:01 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.14040721330273062
[09/26 12:35:07 visual_prompt]: Epoch 69 / 100: avg data time: 5.26e-02, avg batch time: 0.4663, average train loss: 0.0005
[09/26 12:35:09 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 1.1713
[09/26 12:35:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.50	
[09/26 12:35:09 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.13263210930352737
[09/26 12:35:15 visual_prompt]: Epoch 70 / 100: avg data time: 6.31e-02, avg batch time: 0.4775, average train loss: 0.0005
[09/26 12:35:17 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1581, average loss: 1.1685
[09/26 12:35:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 12:35:17 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.12500000000000006
[09/26 12:35:23 visual_prompt]: Epoch 71 / 100: avg data time: 5.81e-02, avg batch time: 0.4726, average train loss: 0.0005
[09/26 12:35:25 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1582, average loss: 1.1653
[09/26 12:35:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 12:35:25 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.1175201839416988
[09/26 12:35:31 visual_prompt]: Epoch 72 / 100: avg data time: 6.57e-02, avg batch time: 0.4804, average train loss: 0.0004
[09/26 12:35:33 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1587, average loss: 1.1641
[09/26 12:35:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 12:35:33 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.11020177413231333
[09/26 12:35:39 visual_prompt]: Epoch 73 / 100: avg data time: 5.67e-02, avg batch time: 0.4702, average train loss: 0.0004
[09/26 12:35:41 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1579, average loss: 1.1630
[09/26 12:35:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.50	
[09/26 12:35:41 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.10305368692688174
[09/26 12:35:47 visual_prompt]: Epoch 74 / 100: avg data time: 5.47e-02, avg batch time: 0.4688, average train loss: 0.0005
[09/26 12:35:49 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1579, average loss: 1.1615
[09/26 12:35:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 12:35:49 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.09608463116858543
[09/26 12:35:55 visual_prompt]: Epoch 75 / 100: avg data time: 6.74e-02, avg batch time: 0.4815, average train loss: 0.0004
[09/26 12:35:57 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1581, average loss: 1.1656
[09/26 12:35:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.00	
[09/26 12:35:57 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.08930309757836516
[09/26 12:36:04 visual_prompt]: Epoch 76 / 100: avg data time: 6.48e-02, avg batch time: 0.4797, average train loss: 0.0004
[09/26 12:36:05 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1575, average loss: 1.1632
[09/26 12:36:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.00	
[09/26 12:36:05 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.08271734841028552
[09/26 12:36:12 visual_prompt]: Epoch 77 / 100: avg data time: 6.41e-02, avg batch time: 0.4781, average train loss: 0.0005
[09/26 12:36:13 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1585, average loss: 1.1579
[09/26 12:36:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.00	
[09/26 12:36:13 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.07633540738525066
[09/26 12:36:20 visual_prompt]: Epoch 78 / 100: avg data time: 5.77e-02, avg batch time: 0.4718, average train loss: 0.0004
[09/26 12:36:21 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1582, average loss: 1.1543
[09/26 12:36:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.00	
[09/26 12:36:21 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.07016504991533726
[09/26 12:36:28 visual_prompt]: Epoch 79 / 100: avg data time: 6.23e-02, avg batch time: 0.4770, average train loss: 0.0004
[09/26 12:36:29 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1584, average loss: 1.1530
[09/26 12:36:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 12:36:29 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.06421379363065141
[09/26 12:36:36 visual_prompt]: Epoch 80 / 100: avg data time: 4.76e-02, avg batch time: 0.4626, average train loss: 0.0004
[09/26 12:36:37 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1581, average loss: 1.1508
[09/26 12:36:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.00	
[09/26 12:36:37 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.058488889220255524
[09/26 12:36:44 visual_prompt]: Epoch 81 / 100: avg data time: 4.61e-02, avg batch time: 0.4614, average train loss: 0.0006
[09/26 12:36:45 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1586, average loss: 1.1516
[09/26 12:36:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.50	
[09/26 12:36:45 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.052997311598319524
[09/26 12:36:52 visual_prompt]: Epoch 82 / 100: avg data time: 5.95e-02, avg batch time: 0.4749, average train loss: 0.0004
[09/26 12:36:54 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1587, average loss: 1.1553
[09/26 12:36:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.50	
[09/26 12:36:54 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.047745751406263165
[09/26 12:37:00 visual_prompt]: Epoch 83 / 100: avg data time: 6.11e-02, avg batch time: 0.4756, average train loss: 0.0004
[09/26 12:37:02 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1583, average loss: 1.1549
[09/26 12:37:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.50	
[09/26 12:37:02 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.042740606861239594
[09/26 12:37:08 visual_prompt]: Epoch 84 / 100: avg data time: 5.85e-02, avg batch time: 0.4738, average train loss: 0.0004
[09/26 12:37:10 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1582, average loss: 1.1537
[09/26 12:37:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.50	
[09/26 12:37:10 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.03798797596089351
[09/26 12:37:16 visual_prompt]: Epoch 85 / 100: avg data time: 4.73e-02, avg batch time: 0.4635, average train loss: 0.0005
[09/26 12:37:18 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1588, average loss: 1.1490
[09/26 12:37:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.50	
[09/26 12:37:18 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.03349364905389032
[09/26 12:37:24 visual_prompt]: Epoch 86 / 100: avg data time: 5.51e-02, avg batch time: 0.4701, average train loss: 0.0004
[09/26 12:37:26 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1583, average loss: 1.1470
[09/26 12:37:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.50	
[09/26 12:37:26 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.029263101785268253
[09/26 12:37:32 visual_prompt]: Epoch 87 / 100: avg data time: 5.77e-02, avg batch time: 0.4727, average train loss: 0.0003
[09/26 12:37:34 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1582, average loss: 1.1467
[09/26 12:37:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.50	
[09/26 12:37:34 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.025301488425208296
[09/26 12:37:40 visual_prompt]: Epoch 88 / 100: avg data time: 5.39e-02, avg batch time: 0.4692, average train loss: 0.0004
[09/26 12:37:42 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1585, average loss: 1.1481
[09/26 12:37:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.50	
[09/26 12:37:42 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.021613635589349756
[09/26 12:37:48 visual_prompt]: Epoch 89 / 100: avg data time: 5.88e-02, avg batch time: 0.4721, average train loss: 0.0005
[09/26 12:37:50 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1584, average loss: 1.1489
[09/26 12:37:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.50	
[09/26 12:37:50 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.01820403635830317
[09/26 12:37:56 visual_prompt]: Epoch 90 / 100: avg data time: 6.72e-02, avg batch time: 0.4813, average train loss: 0.0003
[09/26 12:37:58 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1585, average loss: 1.1494
[09/26 12:37:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.50	
[09/26 12:37:58 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.01507684480352292
[09/26 12:38:04 visual_prompt]: Epoch 91 / 100: avg data time: 5.80e-02, avg batch time: 0.4726, average train loss: 0.0006
[09/26 12:38:06 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1588, average loss: 1.1473
[09/26 12:38:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.50	
[09/26 12:38:06 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.012235870926211617
[09/26 12:38:13 visual_prompt]: Epoch 92 / 100: avg data time: 6.18e-02, avg batch time: 0.4770, average train loss: 0.0003
[09/26 12:38:14 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1587, average loss: 1.1451
[09/26 12:38:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.50	
[09/26 12:38:14 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.009684576015420276
[09/26 12:38:21 visual_prompt]: Epoch 93 / 100: avg data time: 6.23e-02, avg batch time: 0.4773, average train loss: 0.0004
[09/26 12:38:22 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1582, average loss: 1.1445
[09/26 12:38:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.50	
[09/26 12:38:22 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.007426068431000882
[09/26 12:38:29 visual_prompt]: Epoch 94 / 100: avg data time: 5.89e-02, avg batch time: 0.4734, average train loss: 0.0004
[09/26 12:38:30 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1586, average loss: 1.1443
[09/26 12:38:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.50	
[09/26 12:38:30 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.005463099816548578
[09/26 12:38:37 visual_prompt]: Epoch 95 / 100: avg data time: 6.27e-02, avg batch time: 0.4761, average train loss: 0.0003
[09/26 12:38:38 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1582, average loss: 1.1445
[09/26 12:38:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.50	
[09/26 12:38:38 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.003798061746947995
[09/26 12:38:45 visual_prompt]: Epoch 96 / 100: avg data time: 5.70e-02, avg batch time: 0.4707, average train loss: 0.0007
[09/26 12:38:46 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1584, average loss: 1.1443
[09/26 12:38:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.50	
[09/26 12:38:46 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0024329828146074095
[09/26 12:38:53 visual_prompt]: Epoch 97 / 100: avg data time: 5.37e-02, avg batch time: 0.4683, average train loss: 0.0003
[09/26 12:38:54 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1581, average loss: 1.1440
[09/26 12:38:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.50	
[09/26 12:38:54 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0013695261579316775
[09/26 12:39:01 visual_prompt]: Epoch 98 / 100: avg data time: 6.41e-02, avg batch time: 0.4777, average train loss: 0.0004
[09/26 12:39:02 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1582, average loss: 1.1439
[09/26 12:39:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.50	
[09/26 12:39:02 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0006089874350439506
[09/26 12:39:09 visual_prompt]: Epoch 99 / 100: avg data time: 6.20e-02, avg batch time: 0.4761, average train loss: 0.0004
[09/26 12:39:11 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1580, average loss: 1.1438
[09/26 12:39:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.50	
[09/26 12:39:11 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00015229324522605947
[09/26 12:39:17 visual_prompt]: Epoch 100 / 100: avg data time: 6.38e-02, avg batch time: 0.4777, average train loss: 0.0004
[09/26 12:39:19 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1587, average loss: 1.1438
[09/26 12:39:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.50	
[09/26 12:39:19 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 12:39:19 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 12:39:19 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 12:39:19 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 12:39:19 visual_prompt]: Training with config:
[09/26 12:39:19 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.5_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 12:39:19 visual_prompt]: Loading training data...
[09/26 12:39:19 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 12:39:20 visual_prompt]: Number of images: 800
[09/26 12:39:20 visual_prompt]: Number of classes: 10 / 10
[09/26 12:39:20 visual_prompt]: Loading validation data...
[09/26 12:39:20 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 12:39:20 visual_prompt]: Number of images: 200
[09/26 12:39:20 visual_prompt]: Number of classes: 10 / 10
[09/26 12:39:20 visual_prompt]: Constructing models...
[09/26 12:39:22 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 12:39:22 visual_prompt]: tuned percent:0.543
[09/26 12:39:22 visual_prompt]: Device used for model: 0
[09/26 12:39:22 visual_prompt]: Setting up Evaluator...
[09/26 12:39:22 visual_prompt]: Setting up Trainer...
[09/26 12:39:22 visual_prompt]: 	Setting up the optimizer...
[09/26 12:39:22 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 12:39:29 visual_prompt]: Epoch 1 / 100: avg data time: 5.63e-02, avg batch time: 0.4752, average train loss: 2.6629
[09/26 12:39:31 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1582, average loss: 2.6214
[09/26 12:39:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 12:39:31 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 12:39:31 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[09/26 12:39:37 visual_prompt]: Epoch 2 / 100: avg data time: 6.16e-02, avg batch time: 0.4743, average train loss: 2.6543
[09/26 12:39:39 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1583, average loss: 2.2817
[09/26 12:39:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 12:39:39 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[09/26 12:39:45 visual_prompt]: Epoch 3 / 100: avg data time: 5.58e-02, avg batch time: 0.4701, average train loss: 2.2955
[09/26 12:39:47 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1585, average loss: 2.2232
[09/26 12:39:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:39:47 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[09/26 12:39:53 visual_prompt]: Epoch 4 / 100: avg data time: 5.73e-02, avg batch time: 0.4713, average train loss: 2.2743
[09/26 12:39:55 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1581, average loss: 2.2425
[09/26 12:39:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 12:39:55 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[09/26 12:40:01 visual_prompt]: Epoch 5 / 100: avg data time: 6.67e-02, avg batch time: 0.4790, average train loss: 2.2708
[09/26 12:40:03 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1582, average loss: 2.2327
[09/26 12:40:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:40:03 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[09/26 12:40:09 visual_prompt]: Epoch 6 / 100: avg data time: 5.04e-02, avg batch time: 0.4646, average train loss: 2.2551
[09/26 12:40:11 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1587, average loss: 2.2318
[09/26 12:40:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:40:11 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[09/26 12:40:17 visual_prompt]: Epoch 7 / 100: avg data time: 6.21e-02, avg batch time: 0.4745, average train loss: 2.2867
[09/26 12:40:19 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1580, average loss: 2.2770
[09/26 12:40:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 67.00	
[09/26 12:40:19 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[09/26 12:40:25 visual_prompt]: Epoch 8 / 100: avg data time: 5.54e-02, avg batch time: 0.4700, average train loss: 2.3162
[09/26 12:40:27 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1586, average loss: 2.2560
[09/26 12:40:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 62.00	
[09/26 12:40:27 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[09/26 12:40:33 visual_prompt]: Epoch 9 / 100: avg data time: 6.58e-02, avg batch time: 0.4793, average train loss: 2.2820
[09/26 12:40:35 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1585, average loss: 2.2383
[09/26 12:40:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 12:40:35 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[09/26 12:40:41 visual_prompt]: Epoch 10 / 100: avg data time: 6.39e-02, avg batch time: 0.4768, average train loss: 2.2230
[09/26 12:40:43 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1583, average loss: 2.0809
[09/26 12:40:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.00	top5: 65.50	
[09/26 12:40:43 visual_prompt]: Best epoch 10: best metric: 0.250
[09/26 12:40:43 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[09/26 12:40:49 visual_prompt]: Epoch 11 / 100: avg data time: 5.34e-02, avg batch time: 0.4668, average train loss: 2.2852
[09/26 12:40:51 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1584, average loss: 2.2398
[09/26 12:40:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 12:40:51 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[09/26 12:40:58 visual_prompt]: Epoch 12 / 100: avg data time: 6.14e-02, avg batch time: 0.4745, average train loss: 2.1352
[09/26 12:40:59 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1585, average loss: 2.4789
[09/26 12:40:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.50	top5: 54.50	
[09/26 12:40:59 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[09/26 12:41:06 visual_prompt]: Epoch 13 / 100: avg data time: 6.08e-02, avg batch time: 0.4735, average train loss: 2.0416
[09/26 12:41:07 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1582, average loss: 1.7969
[09/26 12:41:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 35.50	top5: 77.00	
[09/26 12:41:07 visual_prompt]: Best epoch 13: best metric: 0.355
[09/26 12:41:07 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[09/26 12:41:14 visual_prompt]: Epoch 14 / 100: avg data time: 4.74e-02, avg batch time: 0.4619, average train loss: 1.8388
[09/26 12:41:15 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1583, average loss: 1.6148
[09/26 12:41:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 42.50	top5: 86.50	
[09/26 12:41:15 visual_prompt]: Best epoch 14: best metric: 0.425
[09/26 12:41:15 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[09/26 12:41:22 visual_prompt]: Epoch 15 / 100: avg data time: 5.33e-02, avg batch time: 0.4679, average train loss: 1.7416
[09/26 12:41:23 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1586, average loss: 1.7255
[09/26 12:41:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 39.00	top5: 82.50	
[09/26 12:41:23 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[09/26 12:41:30 visual_prompt]: Epoch 16 / 100: avg data time: 6.52e-02, avg batch time: 0.4794, average train loss: 1.6743
[09/26 12:41:31 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1580, average loss: 1.4981
[09/26 12:41:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 46.50	top5: 90.00	
[09/26 12:41:31 visual_prompt]: Best epoch 16: best metric: 0.465
[09/26 12:41:31 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[09/26 12:41:38 visual_prompt]: Epoch 17 / 100: avg data time: 6.58e-02, avg batch time: 0.4793, average train loss: 1.4653
[09/26 12:41:39 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1583, average loss: 1.3965
[09/26 12:41:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 51.00	top5: 91.50	
[09/26 12:41:39 visual_prompt]: Best epoch 17: best metric: 0.510
[09/26 12:41:39 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[09/26 12:41:46 visual_prompt]: Epoch 18 / 100: avg data time: 4.87e-02, avg batch time: 0.4631, average train loss: 1.2657
[09/26 12:41:47 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.1579, average loss: 1.3333
[09/26 12:41:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 57.50	top5: 92.00	
[09/26 12:41:47 visual_prompt]: Best epoch 18: best metric: 0.575
[09/26 12:41:47 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[09/26 12:41:54 visual_prompt]: Epoch 19 / 100: avg data time: 5.59e-02, avg batch time: 0.4695, average train loss: 1.0832
[09/26 12:41:55 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1583, average loss: 1.1085
[09/26 12:41:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 63.00	top5: 90.50	
[09/26 12:41:55 visual_prompt]: Best epoch 19: best metric: 0.630
[09/26 12:41:55 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[09/26 12:42:02 visual_prompt]: Epoch 20 / 100: avg data time: 6.91e-02, avg batch time: 0.4835, average train loss: 0.9480
[09/26 12:42:03 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1581, average loss: 0.9145
[09/26 12:42:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.50	top5: 97.00	
[09/26 12:42:03 visual_prompt]: Best epoch 20: best metric: 0.695
[09/26 12:42:03 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[09/26 12:42:10 visual_prompt]: Epoch 21 / 100: avg data time: 5.96e-02, avg batch time: 0.4734, average train loss: 0.8388
[09/26 12:42:12 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1585, average loss: 1.3882
[09/26 12:42:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 55.00	top5: 93.00	
[09/26 12:42:12 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[09/26 12:42:18 visual_prompt]: Epoch 22 / 100: avg data time: 6.39e-02, avg batch time: 0.4783, average train loss: 0.9833
[09/26 12:42:20 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1587, average loss: 1.1585
[09/26 12:42:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 60.00	top5: 93.50	
[09/26 12:42:20 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[09/26 12:42:26 visual_prompt]: Epoch 23 / 100: avg data time: 6.46e-02, avg batch time: 0.4796, average train loss: 0.8542
[09/26 12:42:28 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1586, average loss: 1.0891
[09/26 12:42:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 63.50	top5: 92.50	
[09/26 12:42:28 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[09/26 12:42:34 visual_prompt]: Epoch 24 / 100: avg data time: 4.70e-02, avg batch time: 0.4614, average train loss: 0.7811
[09/26 12:42:36 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.1585, average loss: 0.7970
[09/26 12:42:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 98.00	
[09/26 12:42:36 visual_prompt]: Best epoch 24: best metric: 0.705
[09/26 12:42:36 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[09/26 12:42:42 visual_prompt]: Epoch 25 / 100: avg data time: 4.96e-02, avg batch time: 0.4664, average train loss: 0.5956
[09/26 12:42:44 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.1585, average loss: 0.7048
[09/26 12:42:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.50	top5: 96.00	
[09/26 12:42:44 visual_prompt]: Best epoch 25: best metric: 0.775
[09/26 12:42:44 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[09/26 12:42:50 visual_prompt]: Epoch 26 / 100: avg data time: 6.16e-02, avg batch time: 0.4747, average train loss: 0.7308
[09/26 12:42:52 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1582, average loss: 0.9044
[09/26 12:42:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.50	top5: 97.50	
[09/26 12:42:52 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[09/26 12:42:58 visual_prompt]: Epoch 27 / 100: avg data time: 6.54e-02, avg batch time: 0.4793, average train loss: 0.5759
[09/26 12:43:00 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1586, average loss: 0.6480
[09/26 12:43:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 97.00	
[09/26 12:43:00 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[09/26 12:43:06 visual_prompt]: Epoch 28 / 100: avg data time: 5.82e-02, avg batch time: 0.4713, average train loss: 0.4309
[09/26 12:43:08 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1575, average loss: 0.7684
[09/26 12:43:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.50	top5: 98.00	
[09/26 12:43:08 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[09/26 12:43:14 visual_prompt]: Epoch 29 / 100: avg data time: 6.49e-02, avg batch time: 0.4791, average train loss: 0.4164
[09/26 12:43:16 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1578, average loss: 0.5472
[09/26 12:43:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 98.00	
[09/26 12:43:16 visual_prompt]: Best epoch 29: best metric: 0.840
[09/26 12:43:16 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[09/26 12:43:22 visual_prompt]: Epoch 30 / 100: avg data time: 5.65e-02, avg batch time: 0.4697, average train loss: 0.2750
[09/26 12:43:24 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1582, average loss: 0.9393
[09/26 12:43:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 96.50	
[09/26 12:43:24 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[09/26 12:43:30 visual_prompt]: Epoch 31 / 100: avg data time: 6.17e-02, avg batch time: 0.4748, average train loss: 0.3130
[09/26 12:43:32 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1589, average loss: 0.6692
[09/26 12:43:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.00	
[09/26 12:43:32 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[09/26 12:43:39 visual_prompt]: Epoch 32 / 100: avg data time: 6.67e-02, avg batch time: 0.4799, average train loss: 0.1836
[09/26 12:43:40 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1583, average loss: 0.8131
[09/26 12:43:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.50	top5: 97.00	
[09/26 12:43:40 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[09/26 12:43:47 visual_prompt]: Epoch 33 / 100: avg data time: 6.14e-02, avg batch time: 0.4758, average train loss: 0.1997
[09/26 12:43:48 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1582, average loss: 1.0117
[09/26 12:43:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.50	top5: 94.50	
[09/26 12:43:48 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[09/26 12:43:55 visual_prompt]: Epoch 34 / 100: avg data time: 5.84e-02, avg batch time: 0.4731, average train loss: 0.2791
[09/26 12:43:56 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1581, average loss: 0.6121
[09/26 12:43:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 12:43:56 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[09/26 12:44:03 visual_prompt]: Epoch 35 / 100: avg data time: 4.77e-02, avg batch time: 0.4618, average train loss: 0.2052
[09/26 12:44:04 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1576, average loss: 0.7806
[09/26 12:44:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 97.50	
[09/26 12:44:04 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[09/26 12:44:11 visual_prompt]: Epoch 36 / 100: avg data time: 4.82e-02, avg batch time: 0.4635, average train loss: 0.1415
[09/26 12:44:12 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1583, average loss: 0.7791
[09/26 12:44:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 96.50	
[09/26 12:44:12 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[09/26 12:44:19 visual_prompt]: Epoch 37 / 100: avg data time: 5.52e-02, avg batch time: 0.4691, average train loss: 0.1439
[09/26 12:44:20 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1581, average loss: 1.0124
[09/26 12:44:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 97.50	
[09/26 12:44:20 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[09/26 12:44:27 visual_prompt]: Epoch 38 / 100: avg data time: 5.59e-02, avg batch time: 0.4698, average train loss: 0.2524
[09/26 12:44:28 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1581, average loss: 0.6146
[09/26 12:44:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.50	
[09/26 12:44:28 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[09/26 12:44:35 visual_prompt]: Epoch 39 / 100: avg data time: 6.01e-02, avg batch time: 0.4744, average train loss: 0.1435
[09/26 12:44:36 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.1582, average loss: 0.8735
[09/26 12:44:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 97.50	
[09/26 12:44:36 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[09/26 12:44:43 visual_prompt]: Epoch 40 / 100: avg data time: 5.96e-02, avg batch time: 0.4733, average train loss: 0.1155
[09/26 12:44:44 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1582, average loss: 0.7181
[09/26 12:44:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 12:44:44 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[09/26 12:44:51 visual_prompt]: Epoch 41 / 100: avg data time: 5.32e-02, avg batch time: 0.4680, average train loss: 0.0994
[09/26 12:44:52 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1588, average loss: 0.6933
[09/26 12:44:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 12:44:52 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[09/26 12:44:59 visual_prompt]: Epoch 42 / 100: avg data time: 5.28e-02, avg batch time: 0.4669, average train loss: 0.0631
[09/26 12:45:00 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1585, average loss: 0.8529
[09/26 12:45:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 97.50	
[09/26 12:45:00 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[09/26 12:45:07 visual_prompt]: Epoch 43 / 100: avg data time: 5.82e-02, avg batch time: 0.4712, average train loss: 0.0722
[09/26 12:45:08 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1584, average loss: 0.6774
[09/26 12:45:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 12:45:08 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[09/26 12:45:15 visual_prompt]: Epoch 44 / 100: avg data time: 6.78e-02, avg batch time: 0.4818, average train loss: 0.0518
[09/26 12:45:17 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1583, average loss: 0.7514
[09/26 12:45:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.50	
[09/26 12:45:17 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[09/26 12:45:23 visual_prompt]: Epoch 45 / 100: avg data time: 5.88e-02, avg batch time: 0.4730, average train loss: 0.0318
[09/26 12:45:24 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1580, average loss: 0.6770
[09/26 12:45:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 98.00	
[09/26 12:45:24 visual_prompt]: Best epoch 45: best metric: 0.845
[09/26 12:45:24 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[09/26 12:45:31 visual_prompt]: Epoch 46 / 100: avg data time: 6.02e-02, avg batch time: 0.4732, average train loss: 0.0263
[09/26 12:45:33 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1582, average loss: 0.6038
[09/26 12:45:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 98.00	
[09/26 12:45:33 visual_prompt]: Best epoch 46: best metric: 0.860
[09/26 12:45:33 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[09/26 12:45:39 visual_prompt]: Epoch 47 / 100: avg data time: 6.10e-02, avg batch time: 0.4743, average train loss: 0.0152
[09/26 12:45:41 visual_prompt]: Inference (val):avg data time: 5.49e-05, avg batch time: 0.1587, average loss: 0.6390
[09/26 12:45:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:45:41 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.3189093389542498
[09/26 12:45:47 visual_prompt]: Epoch 48 / 100: avg data time: 5.75e-02, avg batch time: 0.4716, average train loss: 0.0091
[09/26 12:45:49 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1580, average loss: 0.6552
[09/26 12:45:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 97.50	
[09/26 12:45:49 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.3104804738999169
[09/26 12:45:55 visual_prompt]: Epoch 49 / 100: avg data time: 6.16e-02, avg batch time: 0.4753, average train loss: 0.0053
[09/26 12:45:57 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1584, average loss: 0.6690
[09/26 12:45:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 98.00	
[09/26 12:45:57 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.3019779227044398
[09/26 12:46:03 visual_prompt]: Epoch 50 / 100: avg data time: 6.64e-02, avg batch time: 0.4806, average train loss: 0.0035
[09/26 12:46:05 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1583, average loss: 0.6783
[09/26 12:46:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 98.00	
[09/26 12:46:05 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.29341204441673263
[09/26 12:46:11 visual_prompt]: Epoch 51 / 100: avg data time: 5.60e-02, avg batch time: 0.4686, average train loss: 0.0053
[09/26 12:46:13 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1582, average loss: 0.6694
[09/26 12:46:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 97.50	
[09/26 12:46:13 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.28479327524001635
[09/26 12:46:19 visual_prompt]: Epoch 52 / 100: avg data time: 6.08e-02, avg batch time: 0.4748, average train loss: 0.0036
[09/26 12:46:21 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1585, average loss: 0.7110
[09/26 12:46:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:46:21 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.2761321158169134
[09/26 12:46:27 visual_prompt]: Epoch 53 / 100: avg data time: 6.20e-02, avg batch time: 0.4750, average train loss: 0.0021
[09/26 12:46:29 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1583, average loss: 0.7670
[09/26 12:46:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:46:29 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.2674391184360313
[09/26 12:46:35 visual_prompt]: Epoch 54 / 100: avg data time: 5.51e-02, avg batch time: 0.4692, average train loss: 0.0022
[09/26 12:46:37 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1580, average loss: 0.7501
[09/26 12:46:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:46:37 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.2587248741756253
[09/26 12:46:43 visual_prompt]: Epoch 55 / 100: avg data time: 5.89e-02, avg batch time: 0.4733, average train loss: 0.0017
[09/26 12:46:45 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1585, average loss: 0.7363
[09/26 12:46:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:46:45 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.25
[09/26 12:46:51 visual_prompt]: Epoch 56 / 100: avg data time: 5.09e-02, avg batch time: 0.4654, average train loss: 0.0013
[09/26 12:46:53 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1580, average loss: 0.7629
[09/26 12:46:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:46:53 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.24127512582437483
[09/26 12:46:59 visual_prompt]: Epoch 57 / 100: avg data time: 5.82e-02, avg batch time: 0.4711, average train loss: 0.0011
[09/26 12:47:01 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.1580, average loss: 0.7779
[09/26 12:47:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:47:01 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.23256088156396867
[09/26 12:47:07 visual_prompt]: Epoch 58 / 100: avg data time: 5.89e-02, avg batch time: 0.4724, average train loss: 0.0012
[09/26 12:47:09 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1579, average loss: 0.7836
[09/26 12:47:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:47:09 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.22386788418308667
[09/26 12:47:16 visual_prompt]: Epoch 59 / 100: avg data time: 6.91e-02, avg batch time: 0.4827, average train loss: 0.0011
[09/26 12:47:17 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1585, average loss: 0.7684
[09/26 12:47:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:47:17 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.2152067247599837
[09/26 12:47:24 visual_prompt]: Epoch 60 / 100: avg data time: 5.77e-02, avg batch time: 0.4709, average train loss: 0.0011
[09/26 12:47:25 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1583, average loss: 0.7794
[09/26 12:47:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:47:25 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.20658795558326742
[09/26 12:47:32 visual_prompt]: Epoch 61 / 100: avg data time: 6.31e-02, avg batch time: 0.4768, average train loss: 0.0011
[09/26 12:47:34 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1575, average loss: 0.7945
[09/26 12:47:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:47:34 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.1980220772955602
[09/26 12:47:40 visual_prompt]: Epoch 62 / 100: avg data time: 5.81e-02, avg batch time: 0.4710, average train loss: 0.0009
[09/26 12:47:42 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1586, average loss: 0.7994
[09/26 12:47:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:47:42 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.1895195261000831
[09/26 12:47:48 visual_prompt]: Epoch 63 / 100: avg data time: 5.96e-02, avg batch time: 0.4731, average train loss: 0.0009
[09/26 12:47:50 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1585, average loss: 0.7953
[09/26 12:47:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:47:50 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.18109066104575022
[09/26 12:47:56 visual_prompt]: Epoch 64 / 100: avg data time: 6.28e-02, avg batch time: 0.4764, average train loss: 0.0009
[09/26 12:47:58 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1581, average loss: 0.7930
[09/26 12:47:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:47:58 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.17274575140626316
[09/26 12:48:04 visual_prompt]: Epoch 65 / 100: avg data time: 5.22e-02, avg batch time: 0.4678, average train loss: 0.0009
[09/26 12:48:06 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1589, average loss: 0.7917
[09/26 12:48:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:48:06 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.16449496416858284
[09/26 12:48:12 visual_prompt]: Epoch 66 / 100: avg data time: 6.37e-02, avg batch time: 0.4761, average train loss: 0.0007
[09/26 12:48:14 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1578, average loss: 0.7949
[09/26 12:48:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:48:14 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.15634835164602198
[09/26 12:48:20 visual_prompt]: Epoch 67 / 100: avg data time: 5.93e-02, avg batch time: 0.4724, average train loss: 0.0008
[09/26 12:48:22 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1585, average loss: 0.8038
[09/26 12:48:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:48:22 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.14831583923105
[09/26 12:48:28 visual_prompt]: Epoch 68 / 100: avg data time: 5.40e-02, avg batch time: 0.4682, average train loss: 0.0006
[09/26 12:48:30 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.1581, average loss: 0.8117
[09/26 12:48:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:48:30 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.14040721330273062
[09/26 12:48:36 visual_prompt]: Epoch 69 / 100: avg data time: 5.47e-02, avg batch time: 0.4714, average train loss: 0.0007
[09/26 12:48:38 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1583, average loss: 0.8105
[09/26 12:48:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:48:38 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.13263210930352737
[09/26 12:48:44 visual_prompt]: Epoch 70 / 100: avg data time: 4.80e-02, avg batch time: 0.4640, average train loss: 0.0006
[09/26 12:48:46 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1583, average loss: 0.8046
[09/26 12:48:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:48:46 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.12500000000000006
[09/26 12:48:52 visual_prompt]: Epoch 71 / 100: avg data time: 5.77e-02, avg batch time: 0.4722, average train loss: 0.0008
[09/26 12:48:54 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1588, average loss: 0.8055
[09/26 12:48:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:48:54 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.1175201839416988
[09/26 12:49:01 visual_prompt]: Epoch 72 / 100: avg data time: 6.79e-02, avg batch time: 0.4827, average train loss: 0.0007
[09/26 12:49:02 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1585, average loss: 0.8142
[09/26 12:49:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:49:02 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.11020177413231333
[09/26 12:49:09 visual_prompt]: Epoch 73 / 100: avg data time: 6.78e-02, avg batch time: 0.4814, average train loss: 0.0007
[09/26 12:49:10 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1584, average loss: 0.8124
[09/26 12:49:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:49:10 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.10305368692688174
[09/26 12:49:17 visual_prompt]: Epoch 74 / 100: avg data time: 5.12e-02, avg batch time: 0.4643, average train loss: 0.0006
[09/26 12:49:18 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1585, average loss: 0.8177
[09/26 12:49:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:49:18 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.09608463116858543
[09/26 12:49:25 visual_prompt]: Epoch 75 / 100: avg data time: 5.42e-02, avg batch time: 0.4673, average train loss: 0.0009
[09/26 12:49:26 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.1581, average loss: 0.8090
[09/26 12:49:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:49:26 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.08930309757836516
[09/26 12:49:33 visual_prompt]: Epoch 76 / 100: avg data time: 6.32e-02, avg batch time: 0.4766, average train loss: 0.0006
[09/26 12:49:34 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1585, average loss: 0.7768
[09/26 12:49:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:49:34 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.08271734841028552
[09/26 12:49:41 visual_prompt]: Epoch 77 / 100: avg data time: 6.51e-02, avg batch time: 0.4780, average train loss: 0.0007
[09/26 12:49:42 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1580, average loss: 0.7729
[09/26 12:49:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:49:42 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.07633540738525066
[09/26 12:49:49 visual_prompt]: Epoch 78 / 100: avg data time: 6.36e-02, avg batch time: 0.4783, average train loss: 0.0005
[09/26 12:49:50 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1584, average loss: 0.7754
[09/26 12:49:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:49:50 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.07016504991533726
[09/26 12:49:57 visual_prompt]: Epoch 79 / 100: avg data time: 6.10e-02, avg batch time: 0.4740, average train loss: 0.0006
[09/26 12:49:59 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1579, average loss: 0.7793
[09/26 12:49:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:49:59 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.06421379363065141
[09/26 12:50:05 visual_prompt]: Epoch 80 / 100: avg data time: 5.99e-02, avg batch time: 0.4741, average train loss: 0.0006
[09/26 12:50:07 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1585, average loss: 0.7789
[09/26 12:50:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:50:07 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.058488889220255524
[09/26 12:50:13 visual_prompt]: Epoch 81 / 100: avg data time: 5.65e-02, avg batch time: 0.4704, average train loss: 0.0006
[09/26 12:50:15 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1579, average loss: 0.7809
[09/26 12:50:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:50:15 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.052997311598319524
[09/26 12:50:21 visual_prompt]: Epoch 82 / 100: avg data time: 6.48e-02, avg batch time: 0.4776, average train loss: 0.0005
[09/26 12:50:23 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1585, average loss: 0.7835
[09/26 12:50:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:50:23 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.047745751406263165
[09/26 12:50:29 visual_prompt]: Epoch 83 / 100: avg data time: 5.71e-02, avg batch time: 0.4699, average train loss: 0.0006
[09/26 12:50:31 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1584, average loss: 0.7871
[09/26 12:50:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:50:31 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.042740606861239594
[09/26 12:50:37 visual_prompt]: Epoch 84 / 100: avg data time: 5.82e-02, avg batch time: 0.4718, average train loss: 0.0007
[09/26 12:50:39 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1583, average loss: 0.7910
[09/26 12:50:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:50:39 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.03798797596089351
[09/26 12:50:45 visual_prompt]: Epoch 85 / 100: avg data time: 6.46e-02, avg batch time: 0.4774, average train loss: 0.0005
[09/26 12:50:47 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1581, average loss: 0.7937
[09/26 12:50:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:50:47 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.03349364905389032
[09/26 12:50:54 visual_prompt]: Epoch 86 / 100: avg data time: 6.54e-02, avg batch time: 0.4789, average train loss: 0.0005
[09/26 12:50:55 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1579, average loss: 0.7952
[09/26 12:50:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:50:55 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.029263101785268253
[09/26 12:51:02 visual_prompt]: Epoch 87 / 100: avg data time: 6.14e-02, avg batch time: 0.4741, average train loss: 0.0006
[09/26 12:51:03 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1577, average loss: 0.7965
[09/26 12:51:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:51:03 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.025301488425208296
[09/26 12:51:10 visual_prompt]: Epoch 88 / 100: avg data time: 6.32e-02, avg batch time: 0.4762, average train loss: 0.0006
[09/26 12:51:11 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1584, average loss: 0.7982
[09/26 12:51:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:51:11 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.021613635589349756
[09/26 12:51:18 visual_prompt]: Epoch 89 / 100: avg data time: 6.24e-02, avg batch time: 0.4760, average train loss: 0.0006
[09/26 12:51:19 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1585, average loss: 0.7999
[09/26 12:51:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:51:19 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.01820403635830317
[09/26 12:51:26 visual_prompt]: Epoch 90 / 100: avg data time: 5.87e-02, avg batch time: 0.4725, average train loss: 0.0005
[09/26 12:51:27 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1582, average loss: 0.8033
[09/26 12:51:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:51:27 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.01507684480352292
[09/26 12:51:34 visual_prompt]: Epoch 91 / 100: avg data time: 4.72e-02, avg batch time: 0.4602, average train loss: 0.0005
[09/26 12:51:35 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1582, average loss: 0.8046
[09/26 12:51:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:51:35 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.012235870926211617
[09/26 12:51:42 visual_prompt]: Epoch 92 / 100: avg data time: 6.27e-02, avg batch time: 0.4769, average train loss: 0.0005
[09/26 12:51:43 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1586, average loss: 0.8053
[09/26 12:51:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:51:43 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.009684576015420276
[09/26 12:51:50 visual_prompt]: Epoch 93 / 100: avg data time: 6.06e-02, avg batch time: 0.4747, average train loss: 0.0005
[09/26 12:51:52 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1585, average loss: 0.8056
[09/26 12:51:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:51:52 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.007426068431000882
[09/26 12:51:58 visual_prompt]: Epoch 94 / 100: avg data time: 6.18e-02, avg batch time: 0.4769, average train loss: 0.0006
[09/26 12:52:00 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1588, average loss: 0.8058
[09/26 12:52:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:52:00 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.005463099816548578
[09/26 12:52:06 visual_prompt]: Epoch 95 / 100: avg data time: 5.42e-02, avg batch time: 0.4696, average train loss: 0.0006
[09/26 12:52:08 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.1580, average loss: 0.8057
[09/26 12:52:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:52:08 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.003798061746947995
[09/26 12:52:14 visual_prompt]: Epoch 96 / 100: avg data time: 6.57e-02, avg batch time: 0.4806, average train loss: 0.0006
[09/26 12:52:16 visual_prompt]: Inference (val):avg data time: 4.34e-05, avg batch time: 0.1584, average loss: 0.8058
[09/26 12:52:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:52:16 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0024329828146074095
[09/26 12:52:22 visual_prompt]: Epoch 97 / 100: avg data time: 5.78e-02, avg batch time: 0.4713, average train loss: 0.0006
[09/26 12:52:24 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1589, average loss: 0.8058
[09/26 12:52:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:52:24 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0013695261579316775
[09/26 12:52:30 visual_prompt]: Epoch 98 / 100: avg data time: 4.93e-02, avg batch time: 0.4634, average train loss: 0.0005
[09/26 12:52:32 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1583, average loss: 0.8059
[09/26 12:52:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 12:52:32 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0006089874350439506
[09/26 12:52:38 visual_prompt]: Epoch 99 / 100: avg data time: 5.98e-02, avg batch time: 0.4749, average train loss: 0.0006
[09/26 12:52:40 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1580, average loss: 0.8059
[09/26 12:52:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:52:40 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00015229324522605947
[09/26 12:52:47 visual_prompt]: Epoch 100 / 100: avg data time: 6.44e-02, avg batch time: 0.4774, average train loss: 0.0005
[09/26 12:52:48 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1585, average loss: 0.8059
[09/26 12:52:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 12:52:48 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 12:52:48 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 12:52:48 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 12:52:48 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 12:52:48 visual_prompt]: Training with config:
[09/26 12:52:48 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.25_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 12:52:48 visual_prompt]: Loading training data...
[09/26 12:52:48 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 12:52:49 visual_prompt]: Number of images: 800
[09/26 12:52:49 visual_prompt]: Number of classes: 10 / 10
[09/26 12:52:49 visual_prompt]: Loading validation data...
[09/26 12:52:49 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 12:52:49 visual_prompt]: Number of images: 200
[09/26 12:52:49 visual_prompt]: Number of classes: 10 / 10
[09/26 12:52:49 visual_prompt]: Constructing models...
[09/26 12:52:52 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 12:52:52 visual_prompt]: tuned percent:0.543
[09/26 12:52:52 visual_prompt]: Device used for model: 0
[09/26 12:52:52 visual_prompt]: Setting up Evaluator...
[09/26 12:52:52 visual_prompt]: Setting up Trainer...
[09/26 12:52:52 visual_prompt]: 	Setting up the optimizer...
[09/26 12:52:52 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 12:52:59 visual_prompt]: Epoch 1 / 100: avg data time: 6.04e-02, avg batch time: 0.4805, average train loss: 2.6823
[09/26 12:53:00 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1580, average loss: 2.6214
[09/26 12:53:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 12:53:00 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 12:53:00 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/26 12:53:07 visual_prompt]: Epoch 2 / 100: avg data time: 6.19e-02, avg batch time: 0.4734, average train loss: 2.5205
[09/26 12:53:08 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1577, average loss: 2.2258
[09/26 12:53:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 12:53:08 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/26 12:53:15 visual_prompt]: Epoch 3 / 100: avg data time: 6.30e-02, avg batch time: 0.4758, average train loss: 2.2660
[09/26 12:53:16 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.1577, average loss: 2.2474
[09/26 12:53:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 17.50	top5: 62.50	
[09/26 12:53:16 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/26 12:53:23 visual_prompt]: Epoch 4 / 100: avg data time: 5.83e-02, avg batch time: 0.4706, average train loss: 2.2529
[09/26 12:53:24 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1582, average loss: 2.2159
[09/26 12:53:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 66.00	
[09/26 12:53:24 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/26 12:53:31 visual_prompt]: Epoch 5 / 100: avg data time: 6.79e-02, avg batch time: 0.4811, average train loss: 2.2491
[09/26 12:53:32 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1584, average loss: 2.2164
[09/26 12:53:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 12:53:32 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/26 12:53:39 visual_prompt]: Epoch 6 / 100: avg data time: 6.04e-02, avg batch time: 0.4723, average train loss: 2.2508
[09/26 12:53:40 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1587, average loss: 2.2195
[09/26 12:53:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 12:53:40 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/26 12:53:47 visual_prompt]: Epoch 7 / 100: avg data time: 6.13e-02, avg batch time: 0.4738, average train loss: 2.2475
[09/26 12:53:48 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1580, average loss: 2.2329
[09/26 12:53:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 65.00	
[09/26 12:53:48 visual_prompt]: Best epoch 7: best metric: 0.235
[09/26 12:53:48 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/26 12:53:55 visual_prompt]: Epoch 8 / 100: avg data time: 6.14e-02, avg batch time: 0.4742, average train loss: 2.2519
[09/26 12:53:57 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1589, average loss: 2.2352
[09/26 12:53:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.00	top5: 62.50	
[09/26 12:53:57 visual_prompt]: Best epoch 8: best metric: 0.290
[09/26 12:53:57 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/26 12:54:03 visual_prompt]: Epoch 9 / 100: avg data time: 5.68e-02, avg batch time: 0.4691, average train loss: 2.2414
[09/26 12:54:05 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1583, average loss: 2.1965
[09/26 12:54:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:54:05 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/26 12:54:11 visual_prompt]: Epoch 10 / 100: avg data time: 4.97e-02, avg batch time: 0.4634, average train loss: 2.2301
[09/26 12:54:12 visual_prompt]: Inference (val):avg data time: 4.99e-05, avg batch time: 0.1582, average loss: 2.4412
[09/26 12:54:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 53.00	
[09/26 12:54:12 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/26 12:54:19 visual_prompt]: Epoch 11 / 100: avg data time: 5.07e-02, avg batch time: 0.4651, average train loss: 2.2972
[09/26 12:54:20 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1582, average loss: 2.2472
[09/26 12:54:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 12:54:20 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/26 12:54:27 visual_prompt]: Epoch 12 / 100: avg data time: 5.83e-02, avg batch time: 0.4713, average train loss: 2.2517
[09/26 12:54:28 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1578, average loss: 2.2116
[09/26 12:54:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 12:54:28 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/26 12:54:35 visual_prompt]: Epoch 13 / 100: avg data time: 6.83e-02, avg batch time: 0.4813, average train loss: 2.2723
[09/26 12:54:37 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1583, average loss: 2.3612
[09/26 12:54:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 47.50	
[09/26 12:54:37 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/26 12:54:43 visual_prompt]: Epoch 14 / 100: avg data time: 5.26e-02, avg batch time: 0.4672, average train loss: 2.2894
[09/26 12:54:44 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1582, average loss: 2.3237
[09/26 12:54:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.50	
[09/26 12:54:44 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/26 12:54:51 visual_prompt]: Epoch 15 / 100: avg data time: 6.29e-02, avg batch time: 0.4760, average train loss: 2.2724
[09/26 12:54:52 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1582, average loss: 2.2461
[09/26 12:54:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 65.50	
[09/26 12:54:52 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/26 12:54:59 visual_prompt]: Epoch 16 / 100: avg data time: 6.07e-02, avg batch time: 0.4734, average train loss: 2.2538
[09/26 12:55:01 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1583, average loss: 2.2376
[09/26 12:55:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:55:01 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/26 12:55:07 visual_prompt]: Epoch 17 / 100: avg data time: 5.38e-02, avg batch time: 0.4673, average train loss: 2.2640
[09/26 12:55:09 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1583, average loss: 2.2175
[09/26 12:55:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 12:55:09 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/26 12:55:15 visual_prompt]: Epoch 18 / 100: avg data time: 5.33e-02, avg batch time: 0.4669, average train loss: 2.2717
[09/26 12:55:17 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1582, average loss: 2.2296
[09/26 12:55:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 12:55:17 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/26 12:55:23 visual_prompt]: Epoch 19 / 100: avg data time: 5.12e-02, avg batch time: 0.4651, average train loss: 2.2693
[09/26 12:55:24 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1583, average loss: 2.2340
[09/26 12:55:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:55:24 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/26 12:55:31 visual_prompt]: Epoch 20 / 100: avg data time: 5.41e-02, avg batch time: 0.4685, average train loss: 2.2592
[09/26 12:55:32 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1582, average loss: 2.2374
[09/26 12:55:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.50	
[09/26 12:55:32 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/26 12:55:39 visual_prompt]: Epoch 21 / 100: avg data time: 5.44e-02, avg batch time: 0.4670, average train loss: 2.2662
[09/26 12:55:40 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1579, average loss: 2.2654
[09/26 12:55:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 12:55:40 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/26 12:55:47 visual_prompt]: Epoch 22 / 100: avg data time: 6.29e-02, avg batch time: 0.4773, average train loss: 2.2649
[09/26 12:55:49 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1582, average loss: 2.2241
[09/26 12:55:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:55:49 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/26 12:55:55 visual_prompt]: Epoch 23 / 100: avg data time: 5.61e-02, avg batch time: 0.4688, average train loss: 2.2570
[09/26 12:55:57 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1578, average loss: 2.2284
[09/26 12:55:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 58.00	
[09/26 12:55:57 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/26 12:56:03 visual_prompt]: Epoch 24 / 100: avg data time: 5.95e-02, avg batch time: 0.4721, average train loss: 2.2595
[09/26 12:56:05 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1579, average loss: 2.2752
[09/26 12:56:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 12:56:05 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/26 12:56:11 visual_prompt]: Epoch 25 / 100: avg data time: 4.66e-02, avg batch time: 0.4597, average train loss: 2.2655
[09/26 12:56:13 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1584, average loss: 2.2345
[09/26 12:56:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.00	
[09/26 12:56:13 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/26 12:56:19 visual_prompt]: Epoch 26 / 100: avg data time: 6.28e-02, avg batch time: 0.4757, average train loss: 2.2601
[09/26 12:56:21 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1585, average loss: 2.2757
[09/26 12:56:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.50	top5: 63.50	
[09/26 12:56:21 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/26 12:56:27 visual_prompt]: Epoch 27 / 100: avg data time: 6.10e-02, avg batch time: 0.4746, average train loss: 2.2675
[09/26 12:56:29 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1582, average loss: 2.2549
[09/26 12:56:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:56:29 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/26 12:56:35 visual_prompt]: Epoch 28 / 100: avg data time: 6.27e-02, avg batch time: 0.4749, average train loss: 2.2662
[09/26 12:56:37 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1586, average loss: 2.2371
[09/26 12:56:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 12:56:37 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/26 12:56:43 visual_prompt]: Epoch 29 / 100: avg data time: 5.52e-02, avg batch time: 0.4682, average train loss: 2.2625
[09/26 12:56:45 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1582, average loss: 2.2207
[09/26 12:56:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:56:45 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/26 12:56:51 visual_prompt]: Epoch 30 / 100: avg data time: 6.06e-02, avg batch time: 0.4729, average train loss: 2.2599
[09/26 12:56:53 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1582, average loss: 2.2372
[09/26 12:56:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:56:53 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/26 12:57:00 visual_prompt]: Epoch 31 / 100: avg data time: 6.55e-02, avg batch time: 0.4783, average train loss: 2.2655
[09/26 12:57:01 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1583, average loss: 2.2362
[09/26 12:57:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 12:57:01 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/26 12:57:08 visual_prompt]: Epoch 32 / 100: avg data time: 5.93e-02, avg batch time: 0.4736, average train loss: 2.2585
[09/26 12:57:09 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1583, average loss: 2.2490
[09/26 12:57:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 12:57:09 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/26 12:57:16 visual_prompt]: Epoch 33 / 100: avg data time: 6.27e-02, avg batch time: 0.4743, average train loss: 2.2644
[09/26 12:57:17 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1582, average loss: 2.2671
[09/26 12:57:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 59.50	
[09/26 12:57:17 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/26 12:57:24 visual_prompt]: Epoch 34 / 100: avg data time: 5.92e-02, avg batch time: 0.4729, average train loss: 2.2619
[09/26 12:57:25 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1579, average loss: 2.2257
[09/26 12:57:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 12:57:25 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/26 12:57:32 visual_prompt]: Epoch 35 / 100: avg data time: 5.98e-02, avg batch time: 0.4727, average train loss: 2.2544
[09/26 12:57:33 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1580, average loss: 2.2151
[09/26 12:57:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 66.00	
[09/26 12:57:33 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/26 12:57:40 visual_prompt]: Epoch 36 / 100: avg data time: 5.74e-02, avg batch time: 0.4701, average train loss: 2.2551
[09/26 12:57:41 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1584, average loss: 2.2240
[09/26 12:57:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 12:57:41 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/26 12:57:48 visual_prompt]: Epoch 37 / 100: avg data time: 5.88e-02, avg batch time: 0.4709, average train loss: 2.2573
[09/26 12:57:49 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1582, average loss: 2.2249
[09/26 12:57:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 12:57:49 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/26 12:57:56 visual_prompt]: Epoch 38 / 100: avg data time: 5.12e-02, avg batch time: 0.4634, average train loss: 2.2533
[09/26 12:57:57 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1581, average loss: 2.2294
[09/26 12:57:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:57:57 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/26 12:58:04 visual_prompt]: Epoch 39 / 100: avg data time: 5.07e-02, avg batch time: 0.4639, average train loss: 2.2563
[09/26 12:58:05 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1586, average loss: 2.2238
[09/26 12:58:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:58:05 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/26 12:58:12 visual_prompt]: Epoch 40 / 100: avg data time: 6.23e-02, avg batch time: 0.4750, average train loss: 2.2573
[09/26 12:58:13 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1577, average loss: 2.2191
[09/26 12:58:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.00	
[09/26 12:58:13 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/26 12:58:20 visual_prompt]: Epoch 41 / 100: avg data time: 4.94e-02, avg batch time: 0.4633, average train loss: 2.2510
[09/26 12:58:21 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1584, average loss: 2.2243
[09/26 12:58:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:58:21 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/26 12:58:28 visual_prompt]: Epoch 42 / 100: avg data time: 4.64e-02, avg batch time: 0.4612, average train loss: 2.2580
[09/26 12:58:29 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1580, average loss: 2.2215
[09/26 12:58:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:58:29 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/26 12:58:36 visual_prompt]: Epoch 43 / 100: avg data time: 6.10e-02, avg batch time: 0.4741, average train loss: 2.2487
[09/26 12:58:37 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1584, average loss: 2.2134
[09/26 12:58:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 12:58:37 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/26 12:58:44 visual_prompt]: Epoch 44 / 100: avg data time: 5.35e-02, avg batch time: 0.4683, average train loss: 2.2517
[09/26 12:58:45 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1584, average loss: 2.2267
[09/26 12:58:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:58:45 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/26 12:58:52 visual_prompt]: Epoch 45 / 100: avg data time: 5.69e-02, avg batch time: 0.4695, average train loss: 2.2487
[09/26 12:58:53 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1582, average loss: 2.2155
[09/26 12:58:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 12:58:53 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/26 12:59:00 visual_prompt]: Epoch 46 / 100: avg data time: 4.54e-02, avg batch time: 0.4575, average train loss: 2.2463
[09/26 12:59:01 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1578, average loss: 2.2174
[09/26 12:59:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:59:01 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/26 12:59:08 visual_prompt]: Epoch 47 / 100: avg data time: 5.73e-02, avg batch time: 0.4705, average train loss: 2.2391
[09/26 12:59:09 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1579, average loss: 2.2179
[09/26 12:59:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:59:09 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/26 12:59:16 visual_prompt]: Epoch 48 / 100: avg data time: 6.30e-02, avg batch time: 0.4758, average train loss: 2.2383
[09/26 12:59:17 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1577, average loss: 2.2334
[09/26 12:59:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:59:17 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/26 12:59:24 visual_prompt]: Epoch 49 / 100: avg data time: 5.76e-02, avg batch time: 0.4704, average train loss: 2.2559
[09/26 12:59:25 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1582, average loss: 2.2139
[09/26 12:59:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 12:59:25 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/26 12:59:32 visual_prompt]: Epoch 50 / 100: avg data time: 5.97e-02, avg batch time: 0.4722, average train loss: 2.2489
[09/26 12:59:33 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 2.2216
[09/26 12:59:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 12:59:33 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/26 12:59:40 visual_prompt]: Epoch 51 / 100: avg data time: 5.42e-02, avg batch time: 0.4697, average train loss: 2.2411
[09/26 12:59:41 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1575, average loss: 2.2133
[09/26 12:59:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:59:41 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/26 12:59:48 visual_prompt]: Epoch 52 / 100: avg data time: 6.03e-02, avg batch time: 0.4732, average train loss: 2.2478
[09/26 12:59:50 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.1582, average loss: 2.2224
[09/26 12:59:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 12:59:50 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/26 12:59:56 visual_prompt]: Epoch 53 / 100: avg data time: 5.93e-02, avg batch time: 0.4730, average train loss: 2.2396
[09/26 12:59:58 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1581, average loss: 2.2186
[09/26 12:59:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 12:59:58 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/26 13:00:04 visual_prompt]: Epoch 54 / 100: avg data time: 6.72e-02, avg batch time: 0.4794, average train loss: 2.2417
[09/26 13:00:06 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.1580, average loss: 2.2231
[09/26 13:00:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:00:06 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/26 13:00:12 visual_prompt]: Epoch 55 / 100: avg data time: 5.15e-02, avg batch time: 0.4651, average train loss: 2.2399
[09/26 13:00:14 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1582, average loss: 2.2129
[09/26 13:00:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:00:14 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/26 13:00:20 visual_prompt]: Epoch 56 / 100: avg data time: 5.99e-02, avg batch time: 0.4730, average train loss: 2.2395
[09/26 13:00:22 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.1584, average loss: 2.2149
[09/26 13:00:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:00:22 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/26 13:00:28 visual_prompt]: Epoch 57 / 100: avg data time: 6.03e-02, avg batch time: 0.4737, average train loss: 2.2395
[09/26 13:00:30 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1583, average loss: 2.2187
[09/26 13:00:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:00:30 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/26 13:00:36 visual_prompt]: Epoch 58 / 100: avg data time: 6.05e-02, avg batch time: 0.4737, average train loss: 2.2367
[09/26 13:00:38 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.1580, average loss: 2.2131
[09/26 13:00:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:00:38 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/26 13:00:45 visual_prompt]: Epoch 59 / 100: avg data time: 6.40e-02, avg batch time: 0.4756, average train loss: 2.2376
[09/26 13:00:46 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1583, average loss: 2.2170
[09/26 13:00:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:00:46 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/26 13:00:53 visual_prompt]: Epoch 60 / 100: avg data time: 5.64e-02, avg batch time: 0.4703, average train loss: 2.2382
[09/26 13:00:54 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1577, average loss: 2.2147
[09/26 13:00:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:00:54 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/26 13:01:01 visual_prompt]: Epoch 61 / 100: avg data time: 6.28e-02, avg batch time: 0.4763, average train loss: 2.2399
[09/26 13:01:02 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1583, average loss: 2.2163
[09/26 13:01:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:01:02 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/26 13:01:09 visual_prompt]: Epoch 62 / 100: avg data time: 5.93e-02, avg batch time: 0.4727, average train loss: 2.2410
[09/26 13:01:10 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1578, average loss: 2.2166
[09/26 13:01:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:01:10 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/26 13:01:17 visual_prompt]: Epoch 63 / 100: avg data time: 6.20e-02, avg batch time: 0.4766, average train loss: 2.2374
[09/26 13:01:18 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1579, average loss: 2.2167
[09/26 13:01:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:01:18 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/26 13:01:25 visual_prompt]: Epoch 64 / 100: avg data time: 6.02e-02, avg batch time: 0.4728, average train loss: 2.2352
[09/26 13:01:26 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1580, average loss: 2.2158
[09/26 13:01:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:01:26 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/26 13:01:33 visual_prompt]: Epoch 65 / 100: avg data time: 6.07e-02, avg batch time: 0.4739, average train loss: 2.2373
[09/26 13:01:35 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1585, average loss: 2.2169
[09/26 13:01:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:01:35 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/26 13:01:41 visual_prompt]: Epoch 66 / 100: avg data time: 5.20e-02, avg batch time: 0.4646, average train loss: 2.2327
[09/26 13:01:42 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1579, average loss: 2.2131
[09/26 13:01:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:01:42 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/26 13:01:49 visual_prompt]: Epoch 67 / 100: avg data time: 5.02e-02, avg batch time: 0.4636, average train loss: 2.2269
[09/26 13:01:50 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1582, average loss: 2.2161
[09/26 13:01:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:01:50 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/26 13:01:57 visual_prompt]: Epoch 68 / 100: avg data time: 5.86e-02, avg batch time: 0.4716, average train loss: 2.2420
[09/26 13:01:58 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1583, average loss: 2.2202
[09/26 13:01:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.50	
[09/26 13:01:58 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/26 13:02:05 visual_prompt]: Epoch 69 / 100: avg data time: 6.40e-02, avg batch time: 0.4758, average train loss: 2.2397
[09/26 13:02:06 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1583, average loss: 2.2277
[09/26 13:02:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:02:06 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/26 13:02:13 visual_prompt]: Epoch 70 / 100: avg data time: 6.05e-02, avg batch time: 0.4729, average train loss: 2.2379
[09/26 13:02:15 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1584, average loss: 2.2152
[09/26 13:02:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:02:15 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/26 13:02:21 visual_prompt]: Epoch 71 / 100: avg data time: 6.60e-02, avg batch time: 0.4784, average train loss: 2.2420
[09/26 13:02:23 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1581, average loss: 2.2153
[09/26 13:02:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:02:23 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/26 13:02:29 visual_prompt]: Epoch 72 / 100: avg data time: 5.28e-02, avg batch time: 0.4647, average train loss: 2.2360
[09/26 13:02:31 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1584, average loss: 2.2168
[09/26 13:02:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:02:31 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/26 13:02:37 visual_prompt]: Epoch 73 / 100: avg data time: 6.55e-02, avg batch time: 0.4782, average train loss: 2.2343
[09/26 13:02:39 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1585, average loss: 2.2190
[09/26 13:02:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:02:39 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/26 13:02:45 visual_prompt]: Epoch 74 / 100: avg data time: 6.45e-02, avg batch time: 0.4771, average train loss: 2.2389
[09/26 13:02:47 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1584, average loss: 2.2186
[09/26 13:02:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 13:02:47 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/26 13:02:53 visual_prompt]: Epoch 75 / 100: avg data time: 6.05e-02, avg batch time: 0.4742, average train loss: 2.2385
[09/26 13:02:55 visual_prompt]: Inference (val):avg data time: 4.40e-05, avg batch time: 0.1582, average loss: 2.2180
[09/26 13:02:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 13:02:55 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/26 13:03:01 visual_prompt]: Epoch 76 / 100: avg data time: 6.24e-02, avg batch time: 0.4741, average train loss: 2.2375
[09/26 13:03:03 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 2.2133
[09/26 13:03:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:03:03 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/26 13:03:10 visual_prompt]: Epoch 77 / 100: avg data time: 6.26e-02, avg batch time: 0.4750, average train loss: 2.2376
[09/26 13:03:11 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1580, average loss: 2.2161
[09/26 13:03:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 13:03:11 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/26 13:03:18 visual_prompt]: Epoch 78 / 100: avg data time: 5.27e-02, avg batch time: 0.4671, average train loss: 2.2342
[09/26 13:03:19 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1585, average loss: 2.2203
[09/26 13:03:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:03:19 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/26 13:03:26 visual_prompt]: Epoch 79 / 100: avg data time: 5.87e-02, avg batch time: 0.4705, average train loss: 2.2337
[09/26 13:03:27 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.1586, average loss: 2.2128
[09/26 13:03:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:03:27 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/26 13:03:34 visual_prompt]: Epoch 80 / 100: avg data time: 6.56e-02, avg batch time: 0.4780, average train loss: 2.2332
[09/26 13:03:35 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1583, average loss: 2.2184
[09/26 13:03:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:03:35 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/26 13:03:42 visual_prompt]: Epoch 81 / 100: avg data time: 6.23e-02, avg batch time: 0.4748, average train loss: 2.2349
[09/26 13:03:43 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.1581, average loss: 2.2154
[09/26 13:03:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 13:03:43 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/26 13:03:50 visual_prompt]: Epoch 82 / 100: avg data time: 5.64e-02, avg batch time: 0.4689, average train loss: 2.2310
[09/26 13:03:52 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1585, average loss: 2.2143
[09/26 13:03:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 13:03:52 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/26 13:03:58 visual_prompt]: Epoch 83 / 100: avg data time: 6.20e-02, avg batch time: 0.4749, average train loss: 2.2307
[09/26 13:04:00 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1581, average loss: 2.2115
[09/26 13:04:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:04:00 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/26 13:04:06 visual_prompt]: Epoch 84 / 100: avg data time: 5.88e-02, avg batch time: 0.4716, average train loss: 2.2298
[09/26 13:04:08 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1583, average loss: 2.2103
[09/26 13:04:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:04:08 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/26 13:04:14 visual_prompt]: Epoch 85 / 100: avg data time: 6.43e-02, avg batch time: 0.4760, average train loss: 2.2245
[09/26 13:04:16 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1582, average loss: 2.2075
[09/26 13:04:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:04:16 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/26 13:04:22 visual_prompt]: Epoch 86 / 100: avg data time: 5.19e-02, avg batch time: 0.4670, average train loss: 2.2156
[09/26 13:04:24 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1584, average loss: 2.1903
[09/26 13:04:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 66.50	
[09/26 13:04:24 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/26 13:04:30 visual_prompt]: Epoch 87 / 100: avg data time: 5.49e-02, avg batch time: 0.4677, average train loss: 2.2143
[09/26 13:04:32 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1587, average loss: 2.2158
[09/26 13:04:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:04:32 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/26 13:04:38 visual_prompt]: Epoch 88 / 100: avg data time: 6.28e-02, avg batch time: 0.4745, average train loss: 2.2275
[09/26 13:04:40 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1582, average loss: 2.2032
[09/26 13:04:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:04:40 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/26 13:04:46 visual_prompt]: Epoch 89 / 100: avg data time: 5.89e-02, avg batch time: 0.4715, average train loss: 2.2044
[09/26 13:04:48 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1573, average loss: 2.1602
[09/26 13:04:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 67.00	
[09/26 13:04:48 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/26 13:04:54 visual_prompt]: Epoch 90 / 100: avg data time: 6.39e-02, avg batch time: 0.4767, average train loss: 2.1515
[09/26 13:04:56 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1587, average loss: 2.2077
[09/26 13:04:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.00	top5: 64.50	
[09/26 13:04:56 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/26 13:05:03 visual_prompt]: Epoch 91 / 100: avg data time: 6.71e-02, avg batch time: 0.4794, average train loss: 2.1924
[09/26 13:05:04 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1585, average loss: 2.2405
[09/26 13:05:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 19.00	top5: 64.50	
[09/26 13:05:04 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/26 13:05:11 visual_prompt]: Epoch 92 / 100: avg data time: 6.19e-02, avg batch time: 0.4740, average train loss: 2.2167
[09/26 13:05:12 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1583, average loss: 2.1477
[09/26 13:05:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 66.50	
[09/26 13:05:12 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/26 13:05:19 visual_prompt]: Epoch 93 / 100: avg data time: 6.46e-02, avg batch time: 0.4766, average train loss: 2.1073
[09/26 13:05:20 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1581, average loss: 2.0731
[09/26 13:05:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 27.00	top5: 69.50	
[09/26 13:05:20 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/26 13:05:27 visual_prompt]: Epoch 94 / 100: avg data time: 6.09e-02, avg batch time: 0.4731, average train loss: 2.0214
[09/26 13:05:28 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1585, average loss: 1.9077
[09/26 13:05:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 33.50	top5: 78.50	
[09/26 13:05:28 visual_prompt]: Best epoch 94: best metric: 0.335
[09/26 13:05:28 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/26 13:05:35 visual_prompt]: Epoch 95 / 100: avg data time: 4.96e-02, avg batch time: 0.4642, average train loss: 1.9509
[09/26 13:05:36 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1575, average loss: 1.8652
[09/26 13:05:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 35.50	top5: 81.00	
[09/26 13:05:36 visual_prompt]: Best epoch 95: best metric: 0.355
[09/26 13:05:36 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/26 13:05:43 visual_prompt]: Epoch 96 / 100: avg data time: 6.07e-02, avg batch time: 0.4725, average train loss: 1.8718
[09/26 13:05:44 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1576, average loss: 1.8018
[09/26 13:05:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 37.00	top5: 81.50	
[09/26 13:05:44 visual_prompt]: Best epoch 96: best metric: 0.370
[09/26 13:05:44 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/26 13:05:51 visual_prompt]: Epoch 97 / 100: avg data time: 5.58e-02, avg batch time: 0.4685, average train loss: 1.8492
[09/26 13:05:52 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1584, average loss: 1.7850
[09/26 13:05:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 36.00	top5: 82.50	
[09/26 13:05:52 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/26 13:05:59 visual_prompt]: Epoch 98 / 100: avg data time: 5.48e-02, avg batch time: 0.4671, average train loss: 1.8175
[09/26 13:06:00 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1588, average loss: 1.7883
[09/26 13:06:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 35.00	top5: 81.50	
[09/26 13:06:00 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/26 13:06:07 visual_prompt]: Epoch 99 / 100: avg data time: 6.58e-02, avg batch time: 0.4781, average train loss: 1.8018
[09/26 13:06:09 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1579, average loss: 1.7675
[09/26 13:06:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 34.50	top5: 80.50	
[09/26 13:06:09 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/26 13:06:15 visual_prompt]: Epoch 100 / 100: avg data time: 5.67e-02, avg batch time: 0.4698, average train loss: 1.7926
[09/26 13:06:17 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1581, average loss: 1.7632
[09/26 13:06:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 34.00	top5: 81.50	
[09/26 13:06:17 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 13:06:17 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 13:06:17 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 13:06:17 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 13:06:17 visual_prompt]: Training with config:
[09/26 13:06:17 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.25_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 13:06:17 visual_prompt]: Loading training data...
[09/26 13:06:17 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 13:06:18 visual_prompt]: Number of images: 800
[09/26 13:06:18 visual_prompt]: Number of classes: 10 / 10
[09/26 13:06:18 visual_prompt]: Loading validation data...
[09/26 13:06:18 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 13:06:18 visual_prompt]: Number of images: 200
[09/26 13:06:18 visual_prompt]: Number of classes: 10 / 10
[09/26 13:06:18 visual_prompt]: Constructing models...
[09/26 13:06:20 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 13:06:20 visual_prompt]: tuned percent:0.543
[09/26 13:06:20 visual_prompt]: Device used for model: 0
[09/26 13:06:20 visual_prompt]: Setting up Evaluator...
[09/26 13:06:20 visual_prompt]: Setting up Trainer...
[09/26 13:06:20 visual_prompt]: 	Setting up the optimizer...
[09/26 13:06:20 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 13:06:27 visual_prompt]: Epoch 1 / 100: avg data time: 6.08e-02, avg batch time: 0.4773, average train loss: 2.6773
[09/26 13:06:28 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1577, average loss: 2.6214
[09/26 13:06:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 13:06:28 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 13:06:28 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/26 13:06:35 visual_prompt]: Epoch 2 / 100: avg data time: 5.37e-02, avg batch time: 0.4652, average train loss: 2.5208
[09/26 13:06:36 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1586, average loss: 2.2233
[09/26 13:06:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:06:36 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/26 13:06:43 visual_prompt]: Epoch 3 / 100: avg data time: 5.95e-02, avg batch time: 0.4720, average train loss: 2.2787
[09/26 13:06:45 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1583, average loss: 2.2322
[09/26 13:06:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 13:06:45 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/26 13:06:51 visual_prompt]: Epoch 4 / 100: avg data time: 6.43e-02, avg batch time: 0.4760, average train loss: 2.2676
[09/26 13:06:53 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1584, average loss: 2.2079
[09/26 13:06:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.00	
[09/26 13:06:53 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/26 13:06:59 visual_prompt]: Epoch 5 / 100: avg data time: 5.10e-02, avg batch time: 0.4635, average train loss: 2.2570
[09/26 13:07:01 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1576, average loss: 2.2284
[09/26 13:07:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.50	
[09/26 13:07:01 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/26 13:07:07 visual_prompt]: Epoch 6 / 100: avg data time: 6.34e-02, avg batch time: 0.4758, average train loss: 2.2387
[09/26 13:07:09 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1585, average loss: 2.2019
[09/26 13:07:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 13:07:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/26 13:07:15 visual_prompt]: Epoch 7 / 100: avg data time: 6.23e-02, avg batch time: 0.4751, average train loss: 2.2407
[09/26 13:07:17 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1581, average loss: 2.2239
[09/26 13:07:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:07:17 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/26 13:07:23 visual_prompt]: Epoch 8 / 100: avg data time: 6.49e-02, avg batch time: 0.4772, average train loss: 2.2410
[09/26 13:07:25 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1581, average loss: 2.2644
[09/26 13:07:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 63.50	
[09/26 13:07:25 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/26 13:07:32 visual_prompt]: Epoch 9 / 100: avg data time: 6.00e-02, avg batch time: 0.4727, average train loss: 2.2414
[09/26 13:07:33 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1577, average loss: 2.2042
[09/26 13:07:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 63.00	
[09/26 13:07:33 visual_prompt]: Best epoch 9: best metric: 0.245
[09/26 13:07:33 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/26 13:07:39 visual_prompt]: Epoch 10 / 100: avg data time: 5.24e-02, avg batch time: 0.4657, average train loss: 2.2473
[09/26 13:07:41 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1581, average loss: 2.2717
[09/26 13:07:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:07:41 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/26 13:07:47 visual_prompt]: Epoch 11 / 100: avg data time: 5.91e-02, avg batch time: 0.4718, average train loss: 2.2342
[09/26 13:07:49 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1585, average loss: 2.2124
[09/26 13:07:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 13:07:49 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/26 13:07:55 visual_prompt]: Epoch 12 / 100: avg data time: 4.82e-02, avg batch time: 0.4620, average train loss: 2.1743
[09/26 13:07:57 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.1579, average loss: 2.1342
[09/26 13:07:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.50	top5: 71.00	
[09/26 13:07:57 visual_prompt]: Best epoch 12: best metric: 0.255
[09/26 13:07:57 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/26 13:08:04 visual_prompt]: Epoch 13 / 100: avg data time: 6.29e-02, avg batch time: 0.4746, average train loss: 2.1646
[09/26 13:08:05 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.1580, average loss: 2.0326
[09/26 13:08:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 26.00	top5: 72.50	
[09/26 13:08:05 visual_prompt]: Best epoch 13: best metric: 0.260
[09/26 13:08:05 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/26 13:08:12 visual_prompt]: Epoch 14 / 100: avg data time: 6.29e-02, avg batch time: 0.4764, average train loss: 2.0006
[09/26 13:08:13 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1577, average loss: 1.9597
[09/26 13:08:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.50	top5: 75.00	
[09/26 13:08:13 visual_prompt]: Best epoch 14: best metric: 0.295
[09/26 13:08:13 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/26 13:08:20 visual_prompt]: Epoch 15 / 100: avg data time: 5.88e-02, avg batch time: 0.4709, average train loss: 1.8281
[09/26 13:08:21 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1581, average loss: 1.6717
[09/26 13:08:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 40.50	top5: 87.00	
[09/26 13:08:21 visual_prompt]: Best epoch 15: best metric: 0.405
[09/26 13:08:21 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/26 13:08:28 visual_prompt]: Epoch 16 / 100: avg data time: 5.78e-02, avg batch time: 0.4697, average train loss: 1.5967
[09/26 13:08:29 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1576, average loss: 2.2435
[09/26 13:08:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.00	top5: 69.00	
[09/26 13:08:29 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/26 13:08:36 visual_prompt]: Epoch 17 / 100: avg data time: 6.07e-02, avg batch time: 0.4732, average train loss: 1.7401
[09/26 13:08:37 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1583, average loss: 1.4454
[09/26 13:08:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 46.50	top5: 88.50	
[09/26 13:08:37 visual_prompt]: Best epoch 17: best metric: 0.465
[09/26 13:08:37 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/26 13:08:44 visual_prompt]: Epoch 18 / 100: avg data time: 5.79e-02, avg batch time: 0.4699, average train loss: 2.0234
[09/26 13:08:45 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1581, average loss: 2.0134
[09/26 13:08:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.00	top5: 73.50	
[09/26 13:08:45 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/26 13:08:52 visual_prompt]: Epoch 19 / 100: avg data time: 5.86e-02, avg batch time: 0.4715, average train loss: 1.8393
[09/26 13:08:53 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1584, average loss: 1.4905
[09/26 13:08:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 46.50	top5: 85.00	
[09/26 13:08:53 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/26 13:09:00 visual_prompt]: Epoch 20 / 100: avg data time: 5.99e-02, avg batch time: 0.4717, average train loss: 1.4953
[09/26 13:09:01 visual_prompt]: Inference (val):avg data time: 4.97e-05, avg batch time: 0.1580, average loss: 1.4564
[09/26 13:09:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 50.50	top5: 87.50	
[09/26 13:09:01 visual_prompt]: Best epoch 20: best metric: 0.505
[09/26 13:09:01 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/26 13:09:08 visual_prompt]: Epoch 21 / 100: avg data time: 6.17e-02, avg batch time: 0.4736, average train loss: 1.3685
[09/26 13:09:09 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1581, average loss: 1.2977
[09/26 13:09:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 56.50	top5: 95.50	
[09/26 13:09:09 visual_prompt]: Best epoch 21: best metric: 0.565
[09/26 13:09:09 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/26 13:09:16 visual_prompt]: Epoch 22 / 100: avg data time: 5.51e-02, avg batch time: 0.4673, average train loss: 1.3005
[09/26 13:09:17 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1579, average loss: 1.3816
[09/26 13:09:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 52.50	top5: 92.00	
[09/26 13:09:17 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/26 13:09:24 visual_prompt]: Epoch 23 / 100: avg data time: 5.91e-02, avg batch time: 0.4726, average train loss: 1.1291
[09/26 13:09:25 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1585, average loss: 0.9689
[09/26 13:09:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.00	top5: 95.50	
[09/26 13:09:25 visual_prompt]: Best epoch 23: best metric: 0.670
[09/26 13:09:25 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/26 13:09:32 visual_prompt]: Epoch 24 / 100: avg data time: 5.21e-02, avg batch time: 0.4643, average train loss: 1.0662
[09/26 13:09:33 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1580, average loss: 1.2889
[09/26 13:09:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 52.00	top5: 93.00	
[09/26 13:09:33 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/26 13:09:40 visual_prompt]: Epoch 25 / 100: avg data time: 6.00e-02, avg batch time: 0.4718, average train loss: 1.0781
[09/26 13:09:41 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1582, average loss: 0.9634
[09/26 13:09:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.00	top5: 96.00	
[09/26 13:09:41 visual_prompt]: Best epoch 25: best metric: 0.700
[09/26 13:09:41 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/26 13:09:48 visual_prompt]: Epoch 26 / 100: avg data time: 6.47e-02, avg batch time: 0.4765, average train loss: 0.9480
[09/26 13:09:49 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.1577, average loss: 0.9302
[09/26 13:09:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 63.50	top5: 96.00	
[09/26 13:09:49 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/26 13:09:56 visual_prompt]: Epoch 27 / 100: avg data time: 6.20e-02, avg batch time: 0.4743, average train loss: 0.8949
[09/26 13:09:57 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1582, average loss: 1.0850
[09/26 13:09:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 57.00	top5: 95.50	
[09/26 13:09:57 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/26 13:10:04 visual_prompt]: Epoch 28 / 100: avg data time: 5.48e-02, avg batch time: 0.4680, average train loss: 0.8271
[09/26 13:10:05 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1581, average loss: 0.9051
[09/26 13:10:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 97.00	
[09/26 13:10:05 visual_prompt]: Best epoch 28: best metric: 0.715
[09/26 13:10:05 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/26 13:10:12 visual_prompt]: Epoch 29 / 100: avg data time: 5.53e-02, avg batch time: 0.4694, average train loss: 0.6919
[09/26 13:10:13 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1578, average loss: 0.8233
[09/26 13:10:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 97.00	
[09/26 13:10:13 visual_prompt]: Best epoch 29: best metric: 0.720
[09/26 13:10:13 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/26 13:10:20 visual_prompt]: Epoch 30 / 100: avg data time: 5.74e-02, avg batch time: 0.4697, average train loss: 0.6162
[09/26 13:10:21 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1582, average loss: 1.1864
[09/26 13:10:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.50	top5: 96.00	
[09/26 13:10:21 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/26 13:10:28 visual_prompt]: Epoch 31 / 100: avg data time: 6.37e-02, avg batch time: 0.4758, average train loss: 0.6706
[09/26 13:10:30 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1580, average loss: 1.0393
[09/26 13:10:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.50	top5: 94.00	
[09/26 13:10:30 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/26 13:10:36 visual_prompt]: Epoch 32 / 100: avg data time: 5.26e-02, avg batch time: 0.4658, average train loss: 0.5633
[09/26 13:10:37 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1580, average loss: 0.8919
[09/26 13:10:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 97.50	
[09/26 13:10:37 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/26 13:10:44 visual_prompt]: Epoch 33 / 100: avg data time: 6.37e-02, avg batch time: 0.4758, average train loss: 0.5695
[09/26 13:10:46 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1577, average loss: 0.7382
[09/26 13:10:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 97.00	
[09/26 13:10:46 visual_prompt]: Best epoch 33: best metric: 0.750
[09/26 13:10:46 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/26 13:10:52 visual_prompt]: Epoch 34 / 100: avg data time: 6.24e-02, avg batch time: 0.4745, average train loss: 0.4209
[09/26 13:10:54 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1585, average loss: 0.5072
[09/26 13:10:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 98.00	
[09/26 13:10:54 visual_prompt]: Best epoch 34: best metric: 0.840
[09/26 13:10:54 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/26 13:11:00 visual_prompt]: Epoch 35 / 100: avg data time: 5.37e-02, avg batch time: 0.4673, average train loss: 0.3937
[09/26 13:11:02 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1582, average loss: 0.7825
[09/26 13:11:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 97.50	
[09/26 13:11:02 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/26 13:11:08 visual_prompt]: Epoch 36 / 100: avg data time: 5.82e-02, avg batch time: 0.4706, average train loss: 0.4381
[09/26 13:11:10 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1582, average loss: 0.8227
[09/26 13:11:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.50	top5: 97.00	
[09/26 13:11:10 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/26 13:11:16 visual_prompt]: Epoch 37 / 100: avg data time: 6.71e-02, avg batch time: 0.4790, average train loss: 0.4179
[09/26 13:11:18 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1579, average loss: 0.9029
[09/26 13:11:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.50	
[09/26 13:11:18 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/26 13:11:24 visual_prompt]: Epoch 38 / 100: avg data time: 5.40e-02, avg batch time: 0.4673, average train loss: 0.3267
[09/26 13:11:26 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1582, average loss: 0.7276
[09/26 13:11:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 96.00	
[09/26 13:11:26 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/26 13:11:32 visual_prompt]: Epoch 39 / 100: avg data time: 5.89e-02, avg batch time: 0.4711, average train loss: 0.2610
[09/26 13:11:34 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1581, average loss: 0.5761
[09/26 13:11:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 99.00	
[09/26 13:11:34 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/26 13:11:40 visual_prompt]: Epoch 40 / 100: avg data time: 4.92e-02, avg batch time: 0.4623, average train loss: 0.3346
[09/26 13:11:42 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1582, average loss: 1.2880
[09/26 13:11:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.00	
[09/26 13:11:42 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/26 13:11:48 visual_prompt]: Epoch 41 / 100: avg data time: 4.73e-02, avg batch time: 0.4615, average train loss: 0.5074
[09/26 13:11:50 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1575, average loss: 0.8147
[09/26 13:11:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 97.00	
[09/26 13:11:50 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/26 13:11:56 visual_prompt]: Epoch 42 / 100: avg data time: 6.61e-02, avg batch time: 0.4784, average train loss: 0.3441
[09/26 13:11:58 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1587, average loss: 0.7802
[09/26 13:11:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 98.00	
[09/26 13:11:58 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/26 13:12:04 visual_prompt]: Epoch 43 / 100: avg data time: 5.53e-02, avg batch time: 0.4683, average train loss: 0.2378
[09/26 13:12:06 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1580, average loss: 0.8054
[09/26 13:12:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 97.00	
[09/26 13:12:06 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/26 13:12:12 visual_prompt]: Epoch 44 / 100: avg data time: 5.94e-02, avg batch time: 0.4730, average train loss: 0.1854
[09/26 13:12:14 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1583, average loss: 0.6950
[09/26 13:12:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.50	
[09/26 13:12:14 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/26 13:12:20 visual_prompt]: Epoch 45 / 100: avg data time: 6.32e-02, avg batch time: 0.4754, average train loss: 0.1373
[09/26 13:12:22 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1583, average loss: 0.5760
[09/26 13:12:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 98.00	
[09/26 13:12:22 visual_prompt]: Best epoch 45: best metric: 0.850
[09/26 13:12:22 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/26 13:12:29 visual_prompt]: Epoch 46 / 100: avg data time: 6.54e-02, avg batch time: 0.4791, average train loss: 0.0990
[09/26 13:12:30 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1576, average loss: 0.6928
[09/26 13:12:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 98.50	
[09/26 13:12:30 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/26 13:12:37 visual_prompt]: Epoch 47 / 100: avg data time: 6.27e-02, avg batch time: 0.4752, average train loss: 0.0912
[09/26 13:12:38 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1583, average loss: 0.6562
[09/26 13:12:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 13:12:38 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/26 13:12:45 visual_prompt]: Epoch 48 / 100: avg data time: 6.03e-02, avg batch time: 0.4725, average train loss: 0.0846
[09/26 13:12:46 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1581, average loss: 0.6110
[09/26 13:12:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 98.50	
[09/26 13:12:46 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/26 13:12:53 visual_prompt]: Epoch 49 / 100: avg data time: 5.56e-02, avg batch time: 0.4696, average train loss: 0.0850
[09/26 13:12:54 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1583, average loss: 0.7097
[09/26 13:12:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 97.50	
[09/26 13:12:54 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/26 13:13:01 visual_prompt]: Epoch 50 / 100: avg data time: 5.48e-02, avg batch time: 0.4685, average train loss: 0.0355
[09/26 13:13:03 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.1581, average loss: 0.7760
[09/26 13:13:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 97.50	
[09/26 13:13:03 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/26 13:13:09 visual_prompt]: Epoch 51 / 100: avg data time: 5.70e-02, avg batch time: 0.4711, average train loss: 0.0409
[09/26 13:13:11 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1582, average loss: 0.6713
[09/26 13:13:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.50	
[09/26 13:13:11 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/26 13:13:17 visual_prompt]: Epoch 52 / 100: avg data time: 6.38e-02, avg batch time: 0.4763, average train loss: 0.0493
[09/26 13:13:19 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1579, average loss: 0.6989
[09/26 13:13:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 98.50	
[09/26 13:13:19 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/26 13:13:25 visual_prompt]: Epoch 53 / 100: avg data time: 5.52e-02, avg batch time: 0.4693, average train loss: 0.0518
[09/26 13:13:27 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1579, average loss: 0.7852
[09/26 13:13:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 13:13:27 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/26 13:13:33 visual_prompt]: Epoch 54 / 100: avg data time: 6.46e-02, avg batch time: 0.4767, average train loss: 0.0820
[09/26 13:13:35 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1582, average loss: 0.5767
[09/26 13:13:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 99.00	
[09/26 13:13:35 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/26 13:13:41 visual_prompt]: Epoch 55 / 100: avg data time: 6.47e-02, avg batch time: 0.4768, average train loss: 0.0378
[09/26 13:13:43 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1583, average loss: 0.8366
[09/26 13:13:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.00	
[09/26 13:13:43 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/26 13:13:49 visual_prompt]: Epoch 56 / 100: avg data time: 5.40e-02, avg batch time: 0.4680, average train loss: 0.0450
[09/26 13:13:51 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1582, average loss: 0.5799
[09/26 13:13:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.50	top5: 99.50	
[09/26 13:13:51 visual_prompt]: Best epoch 56: best metric: 0.865
[09/26 13:13:51 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/26 13:13:57 visual_prompt]: Epoch 57 / 100: avg data time: 5.62e-02, avg batch time: 0.4701, average train loss: 0.0201
[09/26 13:13:59 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1587, average loss: 0.6351
[09/26 13:13:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 99.50	
[09/26 13:13:59 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/26 13:14:06 visual_prompt]: Epoch 58 / 100: avg data time: 6.97e-02, avg batch time: 0.4821, average train loss: 0.0131
[09/26 13:14:07 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1578, average loss: 0.6034
[09/26 13:14:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:14:07 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/26 13:14:14 visual_prompt]: Epoch 59 / 100: avg data time: 6.03e-02, avg batch time: 0.4733, average train loss: 0.0116
[09/26 13:14:15 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1581, average loss: 0.6467
[09/26 13:14:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 99.00	
[09/26 13:14:15 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/26 13:14:22 visual_prompt]: Epoch 60 / 100: avg data time: 6.29e-02, avg batch time: 0.4750, average train loss: 0.0125
[09/26 13:14:23 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1583, average loss: 0.5990
[09/26 13:14:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 99.50	
[09/26 13:14:23 visual_prompt]: Best epoch 60: best metric: 0.870
[09/26 13:14:23 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/26 13:14:30 visual_prompt]: Epoch 61 / 100: avg data time: 5.67e-02, avg batch time: 0.4701, average train loss: 0.0090
[09/26 13:14:31 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1587, average loss: 0.5110
[09/26 13:14:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 89.00	top5: 99.50	
[09/26 13:14:31 visual_prompt]: Best epoch 61: best metric: 0.890
[09/26 13:14:31 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/26 13:14:38 visual_prompt]: Epoch 62 / 100: avg data time: 6.61e-02, avg batch time: 0.4791, average train loss: 0.0056
[09/26 13:14:39 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1586, average loss: 0.5220
[09/26 13:14:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.00	top5: 99.50	
[09/26 13:14:39 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/26 13:14:46 visual_prompt]: Epoch 63 / 100: avg data time: 6.01e-02, avg batch time: 0.4725, average train loss: 0.0060
[09/26 13:14:47 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1586, average loss: 0.5438
[09/26 13:14:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 88.50	top5: 99.00	
[09/26 13:14:47 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/26 13:14:54 visual_prompt]: Epoch 64 / 100: avg data time: 4.66e-02, avg batch time: 0.4621, average train loss: 0.0042
[09/26 13:14:55 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1581, average loss: 0.5725
[09/26 13:14:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 98.50	
[09/26 13:14:55 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/26 13:15:02 visual_prompt]: Epoch 65 / 100: avg data time: 6.12e-02, avg batch time: 0.4748, average train loss: 0.0031
[09/26 13:15:03 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1585, average loss: 0.6047
[09/26 13:15:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.50	
[09/26 13:15:03 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/26 13:15:10 visual_prompt]: Epoch 66 / 100: avg data time: 6.33e-02, avg batch time: 0.4762, average train loss: 0.0038
[09/26 13:15:12 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1581, average loss: 0.6335
[09/26 13:15:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 98.00	
[09/26 13:15:12 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/26 13:15:18 visual_prompt]: Epoch 67 / 100: avg data time: 6.11e-02, avg batch time: 0.4744, average train loss: 0.0023
[09/26 13:15:20 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1585, average loss: 0.6161
[09/26 13:15:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 99.00	
[09/26 13:15:20 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/26 13:15:26 visual_prompt]: Epoch 68 / 100: avg data time: 4.68e-02, avg batch time: 0.4617, average train loss: 0.0023
[09/26 13:15:28 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1579, average loss: 0.6030
[09/26 13:15:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 99.00	
[09/26 13:15:28 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/26 13:15:34 visual_prompt]: Epoch 69 / 100: avg data time: 5.11e-02, avg batch time: 0.4654, average train loss: 0.0017
[09/26 13:15:36 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1586, average loss: 0.5958
[09/26 13:15:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:15:36 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/26 13:15:42 visual_prompt]: Epoch 70 / 100: avg data time: 5.84e-02, avg batch time: 0.4710, average train loss: 0.0018
[09/26 13:15:43 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1586, average loss: 0.5906
[09/26 13:15:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 99.00	
[09/26 13:15:44 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/26 13:15:50 visual_prompt]: Epoch 71 / 100: avg data time: 6.30e-02, avg batch time: 0.4757, average train loss: 0.0018
[09/26 13:15:52 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1586, average loss: 0.6018
[09/26 13:15:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:15:52 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/26 13:15:58 visual_prompt]: Epoch 72 / 100: avg data time: 5.56e-02, avg batch time: 0.4704, average train loss: 0.0018
[09/26 13:16:00 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1584, average loss: 0.6069
[09/26 13:16:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:16:00 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/26 13:16:06 visual_prompt]: Epoch 73 / 100: avg data time: 5.51e-02, avg batch time: 0.4695, average train loss: 0.0017
[09/26 13:16:08 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.1585, average loss: 0.6008
[09/26 13:16:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 99.00	
[09/26 13:16:08 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/26 13:16:14 visual_prompt]: Epoch 74 / 100: avg data time: 6.14e-02, avg batch time: 0.4754, average train loss: 0.0018
[09/26 13:16:16 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1583, average loss: 0.5920
[09/26 13:16:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 99.00	
[09/26 13:16:16 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/26 13:16:22 visual_prompt]: Epoch 75 / 100: avg data time: 6.34e-02, avg batch time: 0.4765, average train loss: 0.0017
[09/26 13:16:24 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1589, average loss: 0.5883
[09/26 13:16:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:16:24 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/26 13:16:30 visual_prompt]: Epoch 76 / 100: avg data time: 5.64e-02, avg batch time: 0.4720, average train loss: 0.0016
[09/26 13:16:32 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1581, average loss: 0.5883
[09/26 13:16:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:16:32 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/26 13:16:38 visual_prompt]: Epoch 77 / 100: avg data time: 6.36e-02, avg batch time: 0.4764, average train loss: 0.0016
[09/26 13:16:40 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1584, average loss: 0.5886
[09/26 13:16:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:16:40 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/26 13:16:47 visual_prompt]: Epoch 78 / 100: avg data time: 6.06e-02, avg batch time: 0.4733, average train loss: 0.0016
[09/26 13:16:48 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1584, average loss: 0.5877
[09/26 13:16:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 99.00	
[09/26 13:16:48 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/26 13:16:55 visual_prompt]: Epoch 79 / 100: avg data time: 6.53e-02, avg batch time: 0.4778, average train loss: 0.0016
[09/26 13:16:56 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1589, average loss: 0.5876
[09/26 13:16:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 99.00	
[09/26 13:16:56 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/26 13:17:03 visual_prompt]: Epoch 80 / 100: avg data time: 6.03e-02, avg batch time: 0.4740, average train loss: 0.0015
[09/26 13:17:04 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1584, average loss: 0.5878
[09/26 13:17:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 99.00	
[09/26 13:17:04 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/26 13:17:11 visual_prompt]: Epoch 81 / 100: avg data time: 6.04e-02, avg batch time: 0.4736, average train loss: 0.0017
[09/26 13:17:12 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1579, average loss: 0.5866
[09/26 13:17:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 99.00	
[09/26 13:17:12 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/26 13:17:19 visual_prompt]: Epoch 82 / 100: avg data time: 6.41e-02, avg batch time: 0.4776, average train loss: 0.0016
[09/26 13:17:20 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1582, average loss: 0.5859
[09/26 13:17:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 99.00	
[09/26 13:17:20 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/26 13:17:27 visual_prompt]: Epoch 83 / 100: avg data time: 5.01e-02, avg batch time: 0.4628, average train loss: 0.0017
[09/26 13:17:28 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1580, average loss: 0.5857
[09/26 13:17:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 99.00	
[09/26 13:17:28 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/26 13:17:35 visual_prompt]: Epoch 84 / 100: avg data time: 6.03e-02, avg batch time: 0.4744, average train loss: 0.0016
[09/26 13:17:36 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1582, average loss: 0.5865
[09/26 13:17:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 99.00	
[09/26 13:17:36 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/26 13:17:43 visual_prompt]: Epoch 85 / 100: avg data time: 5.26e-02, avg batch time: 0.4658, average train loss: 0.0016
[09/26 13:17:44 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1585, average loss: 0.5861
[09/26 13:17:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 99.00	
[09/26 13:17:44 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/26 13:17:51 visual_prompt]: Epoch 86 / 100: avg data time: 6.25e-02, avg batch time: 0.4758, average train loss: 0.0015
[09/26 13:17:52 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1583, average loss: 0.5856
[09/26 13:17:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 99.00	
[09/26 13:17:53 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/26 13:17:59 visual_prompt]: Epoch 87 / 100: avg data time: 5.70e-02, avg batch time: 0.4717, average train loss: 0.0016
[09/26 13:18:01 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1587, average loss: 0.5844
[09/26 13:18:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:18:01 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/26 13:18:07 visual_prompt]: Epoch 88 / 100: avg data time: 6.53e-02, avg batch time: 0.4780, average train loss: 0.0015
[09/26 13:18:09 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1585, average loss: 0.5838
[09/26 13:18:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:18:09 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/26 13:18:15 visual_prompt]: Epoch 89 / 100: avg data time: 5.49e-02, avg batch time: 0.4682, average train loss: 0.0017
[09/26 13:18:17 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1585, average loss: 0.5839
[09/26 13:18:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:18:17 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/26 13:18:23 visual_prompt]: Epoch 90 / 100: avg data time: 6.20e-02, avg batch time: 0.4761, average train loss: 0.0021
[09/26 13:18:25 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1586, average loss: 0.5834
[09/26 13:18:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:18:25 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/26 13:18:31 visual_prompt]: Epoch 91 / 100: avg data time: 5.35e-02, avg batch time: 0.4680, average train loss: 0.0016
[09/26 13:18:33 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1585, average loss: 0.5831
[09/26 13:18:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:18:33 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/26 13:18:39 visual_prompt]: Epoch 92 / 100: avg data time: 4.79e-02, avg batch time: 0.4620, average train loss: 0.0017
[09/26 13:18:41 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1585, average loss: 0.5828
[09/26 13:18:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:18:41 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/26 13:18:47 visual_prompt]: Epoch 93 / 100: avg data time: 5.87e-02, avg batch time: 0.4718, average train loss: 0.0016
[09/26 13:18:49 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1583, average loss: 0.5827
[09/26 13:18:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:18:49 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/26 13:18:55 visual_prompt]: Epoch 94 / 100: avg data time: 6.48e-02, avg batch time: 0.4775, average train loss: 0.0017
[09/26 13:18:57 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1586, average loss: 0.5827
[09/26 13:18:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:18:57 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/26 13:19:03 visual_prompt]: Epoch 95 / 100: avg data time: 6.31e-02, avg batch time: 0.4753, average train loss: 0.0017
[09/26 13:19:05 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1585, average loss: 0.5826
[09/26 13:19:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:19:05 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/26 13:19:11 visual_prompt]: Epoch 96 / 100: avg data time: 6.48e-02, avg batch time: 0.4780, average train loss: 0.0016
[09/26 13:19:13 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1581, average loss: 0.5825
[09/26 13:19:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:19:13 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/26 13:19:19 visual_prompt]: Epoch 97 / 100: avg data time: 5.48e-02, avg batch time: 0.4696, average train loss: 0.0018
[09/26 13:19:21 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1585, average loss: 0.5825
[09/26 13:19:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:19:21 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/26 13:19:27 visual_prompt]: Epoch 98 / 100: avg data time: 4.91e-02, avg batch time: 0.4651, average train loss: 0.0016
[09/26 13:19:29 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1581, average loss: 0.5825
[09/26 13:19:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:19:29 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/26 13:19:35 visual_prompt]: Epoch 99 / 100: avg data time: 5.28e-02, avg batch time: 0.4673, average train loss: 0.0015
[09/26 13:19:37 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1581, average loss: 0.5825
[09/26 13:19:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:19:37 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/26 13:19:43 visual_prompt]: Epoch 100 / 100: avg data time: 6.25e-02, avg batch time: 0.4762, average train loss: 0.0015
[09/26 13:19:45 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1581, average loss: 0.5825
[09/26 13:19:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 99.00	
[09/26 13:19:45 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 13:19:45 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 13:19:45 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 13:19:45 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 13:19:45 visual_prompt]: Training with config:
[09/26 13:19:45 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.25_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 13:19:45 visual_prompt]: Loading training data...
[09/26 13:19:45 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 13:19:46 visual_prompt]: Number of images: 800
[09/26 13:19:46 visual_prompt]: Number of classes: 10 / 10
[09/26 13:19:46 visual_prompt]: Loading validation data...
[09/26 13:19:46 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 13:19:46 visual_prompt]: Number of images: 200
[09/26 13:19:46 visual_prompt]: Number of classes: 10 / 10
[09/26 13:19:46 visual_prompt]: Constructing models...
[09/26 13:19:49 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 13:19:49 visual_prompt]: tuned percent:0.543
[09/26 13:19:49 visual_prompt]: Device used for model: 0
[09/26 13:19:49 visual_prompt]: Setting up Evaluator...
[09/26 13:19:49 visual_prompt]: Setting up Trainer...
[09/26 13:19:49 visual_prompt]: 	Setting up the optimizer...
[09/26 13:19:49 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 13:19:55 visual_prompt]: Epoch 1 / 100: avg data time: 6.30e-02, avg batch time: 0.4823, average train loss: 2.6899
[09/26 13:19:57 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1583, average loss: 2.6214
[09/26 13:19:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 13:19:57 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 13:19:57 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/26 13:20:03 visual_prompt]: Epoch 2 / 100: avg data time: 5.42e-02, avg batch time: 0.4657, average train loss: 2.6209
[09/26 13:20:05 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1585, average loss: 2.2459
[09/26 13:20:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 19.00	top5: 63.50	
[09/26 13:20:05 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/26 13:20:11 visual_prompt]: Epoch 3 / 100: avg data time: 5.97e-02, avg batch time: 0.4719, average train loss: 2.2859
[09/26 13:20:13 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1584, average loss: 2.2528
[09/26 13:20:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 17.00	top5: 63.00	
[09/26 13:20:13 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/26 13:20:20 visual_prompt]: Epoch 4 / 100: avg data time: 6.58e-02, avg batch time: 0.4783, average train loss: 2.2709
[09/26 13:20:21 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1582, average loss: 2.2091
[09/26 13:20:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:20:21 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/26 13:20:28 visual_prompt]: Epoch 5 / 100: avg data time: 6.16e-02, avg batch time: 0.4751, average train loss: 2.2435
[09/26 13:20:29 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1577, average loss: 2.2438
[09/26 13:20:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 17.00	top5: 63.50	
[09/26 13:20:29 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/26 13:20:36 visual_prompt]: Epoch 6 / 100: avg data time: 6.42e-02, avg batch time: 0.4769, average train loss: 2.2461
[09/26 13:20:37 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1577, average loss: 2.2330
[09/26 13:20:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 64.50	
[09/26 13:20:37 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/26 13:20:44 visual_prompt]: Epoch 7 / 100: avg data time: 5.59e-02, avg batch time: 0.4692, average train loss: 2.2527
[09/26 13:20:45 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1585, average loss: 2.2272
[09/26 13:20:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 64.50	
[09/26 13:20:45 visual_prompt]: Best epoch 7: best metric: 0.245
[09/26 13:20:45 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/26 13:20:52 visual_prompt]: Epoch 8 / 100: avg data time: 5.06e-02, avg batch time: 0.4645, average train loss: 2.2506
[09/26 13:20:53 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1586, average loss: 2.2276
[09/26 13:20:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.00	top5: 64.50	
[09/26 13:20:53 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/26 13:21:00 visual_prompt]: Epoch 9 / 100: avg data time: 6.01e-02, avg batch time: 0.4735, average train loss: 2.2480
[09/26 13:21:01 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1582, average loss: 2.2382
[09/26 13:21:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 58.00	
[09/26 13:21:01 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/26 13:21:08 visual_prompt]: Epoch 10 / 100: avg data time: 5.82e-02, avg batch time: 0.4723, average train loss: 2.2482
[09/26 13:21:09 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1580, average loss: 2.1911
[09/26 13:21:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 65.50	
[09/26 13:21:09 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/26 13:21:16 visual_prompt]: Epoch 11 / 100: avg data time: 5.56e-02, avg batch time: 0.4678, average train loss: 2.2016
[09/26 13:21:17 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1582, average loss: 2.2059
[09/26 13:21:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 13:21:17 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/26 13:21:24 visual_prompt]: Epoch 12 / 100: avg data time: 5.66e-02, avg batch time: 0.4694, average train loss: 2.2164
[09/26 13:21:25 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1585, average loss: 2.1036
[09/26 13:21:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 28.00	top5: 71.50	
[09/26 13:21:25 visual_prompt]: Best epoch 12: best metric: 0.280
[09/26 13:21:25 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/26 13:21:32 visual_prompt]: Epoch 13 / 100: avg data time: 5.77e-02, avg batch time: 0.4701, average train loss: 2.1309
[09/26 13:21:33 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1583, average loss: 2.0527
[09/26 13:21:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 27.00	top5: 71.50	
[09/26 13:21:33 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/26 13:21:40 visual_prompt]: Epoch 14 / 100: avg data time: 6.33e-02, avg batch time: 0.4770, average train loss: 2.0619
[09/26 13:21:42 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1585, average loss: 1.8756
[09/26 13:21:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 31.50	top5: 78.00	
[09/26 13:21:42 visual_prompt]: Best epoch 14: best metric: 0.315
[09/26 13:21:42 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/26 13:21:48 visual_prompt]: Epoch 15 / 100: avg data time: 6.67e-02, avg batch time: 0.4801, average train loss: 1.8938
[09/26 13:21:50 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1586, average loss: 1.7697
[09/26 13:21:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 34.50	top5: 76.00	
[09/26 13:21:50 visual_prompt]: Best epoch 15: best metric: 0.345
[09/26 13:21:50 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/26 13:21:56 visual_prompt]: Epoch 16 / 100: avg data time: 6.31e-02, avg batch time: 0.4763, average train loss: 1.7714
[09/26 13:21:58 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1583, average loss: 1.4962
[09/26 13:21:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 54.50	top5: 89.50	
[09/26 13:21:58 visual_prompt]: Best epoch 16: best metric: 0.545
[09/26 13:21:58 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/26 13:22:04 visual_prompt]: Epoch 17 / 100: avg data time: 6.24e-02, avg batch time: 0.4764, average train loss: 1.5543
[09/26 13:22:06 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1587, average loss: 1.3543
[09/26 13:22:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 50.00	top5: 88.50	
[09/26 13:22:06 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/26 13:22:13 visual_prompt]: Epoch 18 / 100: avg data time: 6.74e-02, avg batch time: 0.4807, average train loss: 1.3707
[09/26 13:22:14 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1582, average loss: 1.3770
[09/26 13:22:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 51.00	top5: 90.00	
[09/26 13:22:14 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/26 13:22:21 visual_prompt]: Epoch 19 / 100: avg data time: 6.67e-02, avg batch time: 0.4796, average train loss: 1.2751
[09/26 13:22:22 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1585, average loss: 1.1014
[09/26 13:22:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 59.00	top5: 97.50	
[09/26 13:22:22 visual_prompt]: Best epoch 19: best metric: 0.590
[09/26 13:22:22 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/26 13:22:29 visual_prompt]: Epoch 20 / 100: avg data time: 6.41e-02, avg batch time: 0.4770, average train loss: 1.1343
[09/26 13:22:30 visual_prompt]: Inference (val):avg data time: 4.45e-05, avg batch time: 0.1586, average loss: 1.0366
[09/26 13:22:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 63.50	top5: 94.00	
[09/26 13:22:30 visual_prompt]: Best epoch 20: best metric: 0.635
[09/26 13:22:30 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/26 13:22:37 visual_prompt]: Epoch 21 / 100: avg data time: 5.86e-02, avg batch time: 0.4719, average train loss: 1.0553
[09/26 13:22:39 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1585, average loss: 0.9482
[09/26 13:22:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 66.50	top5: 95.00	
[09/26 13:22:39 visual_prompt]: Best epoch 21: best metric: 0.665
[09/26 13:22:39 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/26 13:22:45 visual_prompt]: Epoch 22 / 100: avg data time: 6.72e-02, avg batch time: 0.4805, average train loss: 0.9427
[09/26 13:22:47 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1588, average loss: 1.0659
[09/26 13:22:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 61.50	top5: 96.00	
[09/26 13:22:47 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/26 13:22:53 visual_prompt]: Epoch 23 / 100: avg data time: 6.08e-02, avg batch time: 0.4746, average train loss: 0.9117
[09/26 13:22:55 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1586, average loss: 0.7579
[09/26 13:22:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 97.50	
[09/26 13:22:55 visual_prompt]: Best epoch 23: best metric: 0.725
[09/26 13:22:55 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/26 13:23:01 visual_prompt]: Epoch 24 / 100: avg data time: 5.71e-02, avg batch time: 0.4708, average train loss: 0.9377
[09/26 13:23:03 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1580, average loss: 1.4938
[09/26 13:23:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 47.50	top5: 86.50	
[09/26 13:23:03 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/26 13:23:09 visual_prompt]: Epoch 25 / 100: avg data time: 6.24e-02, avg batch time: 0.4765, average train loss: 0.8878
[09/26 13:23:11 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1582, average loss: 0.7870
[09/26 13:23:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.00	top5: 98.00	
[09/26 13:23:11 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/26 13:23:17 visual_prompt]: Epoch 26 / 100: avg data time: 4.57e-02, avg batch time: 0.4605, average train loss: 0.6998
[09/26 13:23:19 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1585, average loss: 0.7295
[09/26 13:23:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 97.00	
[09/26 13:23:19 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/26 13:23:25 visual_prompt]: Epoch 27 / 100: avg data time: 5.05e-02, avg batch time: 0.4649, average train loss: 0.6095
[09/26 13:23:27 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1588, average loss: 0.8691
[09/26 13:23:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.50	top5: 98.00	
[09/26 13:23:27 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/26 13:23:33 visual_prompt]: Epoch 28 / 100: avg data time: 5.23e-02, avg batch time: 0.4666, average train loss: 0.6565
[09/26 13:23:35 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1582, average loss: 0.7142
[09/26 13:23:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.00	top5: 96.50	
[09/26 13:23:35 visual_prompt]: Best epoch 28: best metric: 0.740
[09/26 13:23:35 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/26 13:23:41 visual_prompt]: Epoch 29 / 100: avg data time: 5.67e-02, avg batch time: 0.4712, average train loss: 0.4879
[09/26 13:23:43 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1581, average loss: 0.8956
[09/26 13:23:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 96.50	
[09/26 13:23:43 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/26 13:23:49 visual_prompt]: Epoch 30 / 100: avg data time: 5.89e-02, avg batch time: 0.4724, average train loss: 0.4235
[09/26 13:23:51 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1580, average loss: 0.9859
[09/26 13:23:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.50	top5: 97.50	
[09/26 13:23:51 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/26 13:23:57 visual_prompt]: Epoch 31 / 100: avg data time: 5.31e-02, avg batch time: 0.4672, average train loss: 0.4822
[09/26 13:23:59 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1586, average loss: 0.6759
[09/26 13:23:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 97.50	
[09/26 13:23:59 visual_prompt]: Best epoch 31: best metric: 0.760
[09/26 13:23:59 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/26 13:24:05 visual_prompt]: Epoch 32 / 100: avg data time: 5.91e-02, avg batch time: 0.4740, average train loss: 0.3957
[09/26 13:24:07 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1585, average loss: 0.6881
[09/26 13:24:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 98.50	
[09/26 13:24:07 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/26 13:24:13 visual_prompt]: Epoch 33 / 100: avg data time: 5.91e-02, avg batch time: 0.4717, average train loss: 0.3395
[09/26 13:24:15 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1585, average loss: 0.6493
[09/26 13:24:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 97.50	
[09/26 13:24:15 visual_prompt]: Best epoch 33: best metric: 0.790
[09/26 13:24:15 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/26 13:24:21 visual_prompt]: Epoch 34 / 100: avg data time: 6.56e-02, avg batch time: 0.4793, average train loss: 0.2557
[09/26 13:24:23 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1583, average loss: 0.6250
[09/26 13:24:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 97.00	
[09/26 13:24:23 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/26 13:24:29 visual_prompt]: Epoch 35 / 100: avg data time: 5.72e-02, avg batch time: 0.4715, average train loss: 0.1940
[09/26 13:24:31 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1586, average loss: 0.6981
[09/26 13:24:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 99.00	
[09/26 13:24:31 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/26 13:24:37 visual_prompt]: Epoch 36 / 100: avg data time: 5.68e-02, avg batch time: 0.4704, average train loss: 0.1417
[09/26 13:24:39 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1585, average loss: 0.4703
[09/26 13:24:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 98.50	
[09/26 13:24:39 visual_prompt]: Best epoch 36: best metric: 0.855
[09/26 13:24:39 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/26 13:24:45 visual_prompt]: Epoch 37 / 100: avg data time: 5.34e-02, avg batch time: 0.4662, average train loss: 0.1777
[09/26 13:24:47 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1589, average loss: 0.7850
[09/26 13:24:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 96.50	
[09/26 13:24:47 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/26 13:24:53 visual_prompt]: Epoch 38 / 100: avg data time: 4.67e-02, avg batch time: 0.4620, average train loss: 0.1349
[09/26 13:24:55 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1587, average loss: 0.8206
[09/26 13:24:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.00	top5: 97.50	
[09/26 13:24:55 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/26 13:25:01 visual_prompt]: Epoch 39 / 100: avg data time: 6.95e-02, avg batch time: 0.4823, average train loss: 0.1301
[09/26 13:25:03 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1582, average loss: 0.6344
[09/26 13:25:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 99.00	
[09/26 13:25:03 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/26 13:25:09 visual_prompt]: Epoch 40 / 100: avg data time: 6.06e-02, avg batch time: 0.4732, average train loss: 0.0644
[09/26 13:25:11 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1586, average loss: 0.7982
[09/26 13:25:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:25:11 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/26 13:25:17 visual_prompt]: Epoch 41 / 100: avg data time: 6.33e-02, avg batch time: 0.4764, average train loss: 0.0806
[09/26 13:25:19 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1585, average loss: 0.8029
[09/26 13:25:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 98.00	
[09/26 13:25:19 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/26 13:25:26 visual_prompt]: Epoch 42 / 100: avg data time: 6.18e-02, avg batch time: 0.4753, average train loss: 0.1417
[09/26 13:25:27 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1587, average loss: 0.8783
[09/26 13:25:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.00	top5: 98.50	
[09/26 13:25:27 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/26 13:25:34 visual_prompt]: Epoch 43 / 100: avg data time: 5.83e-02, avg batch time: 0.4714, average train loss: 0.1427
[09/26 13:25:35 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1586, average loss: 0.6668
[09/26 13:25:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.00	
[09/26 13:25:35 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/26 13:25:42 visual_prompt]: Epoch 44 / 100: avg data time: 5.70e-02, avg batch time: 0.4697, average train loss: 0.0567
[09/26 13:25:43 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1587, average loss: 0.7968
[09/26 13:25:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 97.50	
[09/26 13:25:43 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/26 13:25:50 visual_prompt]: Epoch 45 / 100: avg data time: 4.94e-02, avg batch time: 0.4648, average train loss: 0.0808
[09/26 13:25:51 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1585, average loss: 0.6998
[09/26 13:25:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.50	
[09/26 13:25:51 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/26 13:25:58 visual_prompt]: Epoch 46 / 100: avg data time: 5.99e-02, avg batch time: 0.4735, average train loss: 0.0520
[09/26 13:25:59 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1583, average loss: 0.5862
[09/26 13:25:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 99.00	
[09/26 13:25:59 visual_prompt]: Best epoch 46: best metric: 0.860
[09/26 13:25:59 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/26 13:26:06 visual_prompt]: Epoch 47 / 100: avg data time: 5.72e-02, avg batch time: 0.4713, average train loss: 0.0264
[09/26 13:26:07 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1583, average loss: 0.7504
[09/26 13:26:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 13:26:07 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/26 13:26:14 visual_prompt]: Epoch 48 / 100: avg data time: 6.12e-02, avg batch time: 0.4741, average train loss: 0.0218
[09/26 13:26:15 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1582, average loss: 0.8932
[09/26 13:26:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 96.50	
[09/26 13:26:15 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/26 13:26:22 visual_prompt]: Epoch 49 / 100: avg data time: 5.77e-02, avg batch time: 0.4712, average train loss: 0.0137
[09/26 13:26:23 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1584, average loss: 0.8265
[09/26 13:26:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 13:26:23 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/26 13:26:30 visual_prompt]: Epoch 50 / 100: avg data time: 6.16e-02, avg batch time: 0.4753, average train loss: 0.0313
[09/26 13:26:32 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1584, average loss: 0.7111
[09/26 13:26:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 98.50	
[09/26 13:26:32 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/26 13:26:38 visual_prompt]: Epoch 51 / 100: avg data time: 5.98e-02, avg batch time: 0.4733, average train loss: 0.0105
[09/26 13:26:40 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1588, average loss: 0.7824
[09/26 13:26:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 97.50	
[09/26 13:26:40 visual_prompt]: Best epoch 51: best metric: 0.870
[09/26 13:26:40 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/26 13:26:46 visual_prompt]: Epoch 52 / 100: avg data time: 5.84e-02, avg batch time: 0.4720, average train loss: 0.0080
[09/26 13:26:48 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1581, average loss: 0.9648
[09/26 13:26:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 13:26:48 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/26 13:26:54 visual_prompt]: Epoch 53 / 100: avg data time: 6.26e-02, avg batch time: 0.4752, average train loss: 0.0170
[09/26 13:26:56 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1580, average loss: 0.8317
[09/26 13:26:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.00	
[09/26 13:26:56 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/26 13:27:02 visual_prompt]: Epoch 54 / 100: avg data time: 6.27e-02, avg batch time: 0.4753, average train loss: 0.0115
[09/26 13:27:04 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1582, average loss: 0.9877
[09/26 13:27:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 96.50	
[09/26 13:27:04 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/26 13:27:10 visual_prompt]: Epoch 55 / 100: avg data time: 5.80e-02, avg batch time: 0.4713, average train loss: 0.0045
[09/26 13:27:12 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1584, average loss: 0.9721
[09/26 13:27:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.00	
[09/26 13:27:12 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/26 13:27:18 visual_prompt]: Epoch 56 / 100: avg data time: 6.48e-02, avg batch time: 0.4780, average train loss: 0.0043
[09/26 13:27:20 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1585, average loss: 0.8539
[09/26 13:27:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 13:27:20 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/26 13:27:26 visual_prompt]: Epoch 57 / 100: avg data time: 5.94e-02, avg batch time: 0.4720, average train loss: 0.0048
[09/26 13:27:28 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1582, average loss: 0.8954
[09/26 13:27:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 13:27:28 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/26 13:27:34 visual_prompt]: Epoch 58 / 100: avg data time: 4.91e-02, avg batch time: 0.4631, average train loss: 0.0062
[09/26 13:27:36 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1582, average loss: 0.9576
[09/26 13:27:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 97.50	
[09/26 13:27:36 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/26 13:27:43 visual_prompt]: Epoch 59 / 100: avg data time: 6.79e-02, avg batch time: 0.4804, average train loss: 0.0021
[09/26 13:27:44 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1584, average loss: 0.8122
[09/26 13:27:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 98.50	
[09/26 13:27:44 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/26 13:27:51 visual_prompt]: Epoch 60 / 100: avg data time: 6.09e-02, avg batch time: 0.4731, average train loss: 0.0015
[09/26 13:27:52 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1584, average loss: 0.7884
[09/26 13:27:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 13:27:52 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/26 13:27:59 visual_prompt]: Epoch 61 / 100: avg data time: 6.17e-02, avg batch time: 0.4752, average train loss: 0.0021
[09/26 13:28:00 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1585, average loss: 0.8931
[09/26 13:28:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 98.00	
[09/26 13:28:00 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/26 13:28:07 visual_prompt]: Epoch 62 / 100: avg data time: 6.43e-02, avg batch time: 0.4783, average train loss: 0.0011
[09/26 13:28:08 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1579, average loss: 0.9731
[09/26 13:28:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 98.00	
[09/26 13:28:08 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/26 13:28:15 visual_prompt]: Epoch 63 / 100: avg data time: 5.75e-02, avg batch time: 0.4720, average train loss: 0.0010
[09/26 13:28:16 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1579, average loss: 0.9694
[09/26 13:28:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 13:28:16 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/26 13:28:23 visual_prompt]: Epoch 64 / 100: avg data time: 6.16e-02, avg batch time: 0.4758, average train loss: 0.0029
[09/26 13:28:25 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1583, average loss: 0.9343
[09/26 13:28:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 13:28:25 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/26 13:28:31 visual_prompt]: Epoch 65 / 100: avg data time: 6.89e-02, avg batch time: 0.4824, average train loss: 0.0024
[09/26 13:28:33 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1582, average loss: 0.9801
[09/26 13:28:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:28:33 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/26 13:28:39 visual_prompt]: Epoch 66 / 100: avg data time: 5.11e-02, avg batch time: 0.4666, average train loss: 0.0025
[09/26 13:28:41 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1581, average loss: 0.9526
[09/26 13:28:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 97.50	
[09/26 13:28:41 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/26 13:28:47 visual_prompt]: Epoch 67 / 100: avg data time: 6.12e-02, avg batch time: 0.4737, average train loss: 0.0010
[09/26 13:28:49 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1579, average loss: 0.9575
[09/26 13:28:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 97.50	
[09/26 13:28:49 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/26 13:28:55 visual_prompt]: Epoch 68 / 100: avg data time: 5.93e-02, avg batch time: 0.4721, average train loss: 0.0013
[09/26 13:28:57 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1588, average loss: 0.9369
[09/26 13:28:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 97.50	
[09/26 13:28:57 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/26 13:29:03 visual_prompt]: Epoch 69 / 100: avg data time: 6.14e-02, avg batch time: 0.4752, average train loss: 0.0008
[09/26 13:29:05 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1580, average loss: 0.9258
[09/26 13:29:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.50	
[09/26 13:29:05 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/26 13:29:11 visual_prompt]: Epoch 70 / 100: avg data time: 5.69e-02, avg batch time: 0.4712, average train loss: 0.0006
[09/26 13:29:13 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1585, average loss: 0.9265
[09/26 13:29:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 13:29:13 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/26 13:29:19 visual_prompt]: Epoch 71 / 100: avg data time: 6.15e-02, avg batch time: 0.4752, average train loss: 0.0006
[09/26 13:29:21 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1581, average loss: 0.9266
[09/26 13:29:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 13:29:21 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/26 13:29:27 visual_prompt]: Epoch 72 / 100: avg data time: 5.78e-02, avg batch time: 0.4724, average train loss: 0.0005
[09/26 13:29:29 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1587, average loss: 0.9246
[09/26 13:29:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 13:29:29 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/26 13:29:35 visual_prompt]: Epoch 73 / 100: avg data time: 5.85e-02, avg batch time: 0.4719, average train loss: 0.0004
[09/26 13:29:37 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.1582, average loss: 0.9249
[09/26 13:29:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 13:29:37 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/26 13:29:43 visual_prompt]: Epoch 74 / 100: avg data time: 5.57e-02, avg batch time: 0.4705, average train loss: 0.0049
[09/26 13:29:45 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1582, average loss: 0.9693
[09/26 13:29:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:29:45 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/26 13:29:51 visual_prompt]: Epoch 75 / 100: avg data time: 5.75e-02, avg batch time: 0.4714, average train loss: 0.0029
[09/26 13:29:53 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1583, average loss: 0.9694
[09/26 13:29:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:29:53 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/26 13:30:00 visual_prompt]: Epoch 76 / 100: avg data time: 6.43e-02, avg batch time: 0.4787, average train loss: 0.0005
[09/26 13:30:01 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1587, average loss: 0.9570
[09/26 13:30:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.50	
[09/26 13:30:01 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/26 13:30:08 visual_prompt]: Epoch 77 / 100: avg data time: 6.50e-02, avg batch time: 0.4786, average train loss: 0.0005
[09/26 13:30:09 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1586, average loss: 0.9513
[09/26 13:30:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:30:09 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/26 13:30:16 visual_prompt]: Epoch 78 / 100: avg data time: 6.02e-02, avg batch time: 0.4741, average train loss: 0.0005
[09/26 13:30:17 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1584, average loss: 0.9538
[09/26 13:30:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:30:17 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/26 13:30:24 visual_prompt]: Epoch 79 / 100: avg data time: 5.65e-02, avg batch time: 0.4699, average train loss: 0.0004
[09/26 13:30:25 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1582, average loss: 0.9537
[09/26 13:30:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:30:25 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/26 13:30:32 visual_prompt]: Epoch 80 / 100: avg data time: 4.81e-02, avg batch time: 0.4637, average train loss: 0.0005
[09/26 13:30:33 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1583, average loss: 0.9531
[09/26 13:30:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:30:33 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/26 13:30:40 visual_prompt]: Epoch 81 / 100: avg data time: 6.13e-02, avg batch time: 0.4758, average train loss: 0.0005
[09/26 13:30:41 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1586, average loss: 0.9509
[09/26 13:30:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:30:41 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/26 13:30:48 visual_prompt]: Epoch 82 / 100: avg data time: 5.98e-02, avg batch time: 0.4740, average train loss: 0.0008
[09/26 13:30:49 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1586, average loss: 0.9396
[09/26 13:30:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:30:49 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/26 13:30:56 visual_prompt]: Epoch 83 / 100: avg data time: 6.05e-02, avg batch time: 0.4734, average train loss: 0.0005
[09/26 13:30:58 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1585, average loss: 0.9295
[09/26 13:30:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 13:30:58 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/26 13:31:04 visual_prompt]: Epoch 84 / 100: avg data time: 5.64e-02, avg batch time: 0.4697, average train loss: 0.0007
[09/26 13:31:06 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1583, average loss: 0.9155
[09/26 13:31:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 13:31:06 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/26 13:31:12 visual_prompt]: Epoch 85 / 100: avg data time: 6.31e-02, avg batch time: 0.4776, average train loss: 0.0004
[09/26 13:31:14 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1580, average loss: 0.9135
[09/26 13:31:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 13:31:14 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/26 13:31:20 visual_prompt]: Epoch 86 / 100: avg data time: 5.76e-02, avg batch time: 0.4719, average train loss: 0.0005
[09/26 13:31:22 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.1588, average loss: 0.9148
[09/26 13:31:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 13:31:22 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/26 13:31:28 visual_prompt]: Epoch 87 / 100: avg data time: 6.15e-02, avg batch time: 0.4752, average train loss: 0.0005
[09/26 13:31:30 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1587, average loss: 0.9161
[09/26 13:31:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 13:31:30 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/26 13:31:36 visual_prompt]: Epoch 88 / 100: avg data time: 5.84e-02, avg batch time: 0.4736, average train loss: 0.0004
[09/26 13:31:38 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1582, average loss: 0.9189
[09/26 13:31:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 13:31:38 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/26 13:31:44 visual_prompt]: Epoch 89 / 100: avg data time: 5.87e-02, avg batch time: 0.4727, average train loss: 0.0005
[09/26 13:31:46 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1582, average loss: 0.9188
[09/26 13:31:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 13:31:46 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/26 13:31:52 visual_prompt]: Epoch 90 / 100: avg data time: 4.82e-02, avg batch time: 0.4617, average train loss: 0.0004
[09/26 13:31:54 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1584, average loss: 0.9183
[09/26 13:31:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 13:31:54 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/26 13:32:00 visual_prompt]: Epoch 91 / 100: avg data time: 4.98e-02, avg batch time: 0.4672, average train loss: 0.0005
[09/26 13:32:02 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.1580, average loss: 0.9193
[09/26 13:32:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.50	
[09/26 13:32:02 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/26 13:32:08 visual_prompt]: Epoch 92 / 100: avg data time: 5.63e-02, avg batch time: 0.4704, average train loss: 0.0023
[09/26 13:32:10 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1582, average loss: 0.9244
[09/26 13:32:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 13:32:10 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/26 13:32:16 visual_prompt]: Epoch 93 / 100: avg data time: 6.28e-02, avg batch time: 0.4757, average train loss: 0.0006
[09/26 13:32:18 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1588, average loss: 0.9326
[09/26 13:32:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:32:18 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/26 13:32:25 visual_prompt]: Epoch 94 / 100: avg data time: 6.44e-02, avg batch time: 0.4794, average train loss: 0.0003
[09/26 13:32:26 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1582, average loss: 0.9342
[09/26 13:32:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:32:26 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/26 13:32:33 visual_prompt]: Epoch 95 / 100: avg data time: 5.91e-02, avg batch time: 0.4725, average train loss: 0.0004
[09/26 13:32:34 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1578, average loss: 0.9346
[09/26 13:32:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:32:34 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/26 13:32:41 visual_prompt]: Epoch 96 / 100: avg data time: 6.32e-02, avg batch time: 0.4764, average train loss: 0.0008
[09/26 13:32:42 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1584, average loss: 0.9346
[09/26 13:32:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:32:42 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/26 13:32:49 visual_prompt]: Epoch 97 / 100: avg data time: 5.35e-02, avg batch time: 0.4686, average train loss: 0.0004
[09/26 13:32:50 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1581, average loss: 0.9345
[09/26 13:32:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:32:50 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/26 13:32:57 visual_prompt]: Epoch 98 / 100: avg data time: 6.17e-02, avg batch time: 0.4754, average train loss: 0.0005
[09/26 13:32:59 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1587, average loss: 0.9345
[09/26 13:32:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:32:59 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/26 13:33:05 visual_prompt]: Epoch 99 / 100: avg data time: 4.48e-02, avg batch time: 0.4601, average train loss: 0.0006
[09/26 13:33:06 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1583, average loss: 0.9345
[09/26 13:33:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:33:06 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/26 13:33:13 visual_prompt]: Epoch 100 / 100: avg data time: 5.96e-02, avg batch time: 0.4725, average train loss: 0.0005
[09/26 13:33:15 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1582, average loss: 0.9345
[09/26 13:33:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:33:15 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 13:33:15 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 13:33:15 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 13:33:15 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 13:33:15 visual_prompt]: Training with config:
[09/26 13:33:15 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.25_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 13:33:15 visual_prompt]: Loading training data...
[09/26 13:33:15 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 13:33:15 visual_prompt]: Number of images: 800
[09/26 13:33:15 visual_prompt]: Number of classes: 10 / 10
[09/26 13:33:16 visual_prompt]: Loading validation data...
[09/26 13:33:16 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 13:33:16 visual_prompt]: Number of images: 200
[09/26 13:33:16 visual_prompt]: Number of classes: 10 / 10
[09/26 13:33:16 visual_prompt]: Constructing models...
[09/26 13:33:18 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 13:33:18 visual_prompt]: tuned percent:0.543
[09/26 13:33:18 visual_prompt]: Device used for model: 0
[09/26 13:33:18 visual_prompt]: Setting up Evaluator...
[09/26 13:33:18 visual_prompt]: Setting up Trainer...
[09/26 13:33:18 visual_prompt]: 	Setting up the optimizer...
[09/26 13:33:18 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 13:33:25 visual_prompt]: Epoch 1 / 100: avg data time: 5.80e-02, avg batch time: 0.4765, average train loss: 2.6804
[09/26 13:33:26 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.1590, average loss: 2.6214
[09/26 13:33:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 13:33:26 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 13:33:26 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/26 13:33:33 visual_prompt]: Epoch 2 / 100: avg data time: 5.41e-02, avg batch time: 0.4667, average train loss: 2.6003
[09/26 13:33:34 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1579, average loss: 2.2318
[09/26 13:33:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 13:33:34 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/26 13:33:41 visual_prompt]: Epoch 3 / 100: avg data time: 5.80e-02, avg batch time: 0.4716, average train loss: 2.3023
[09/26 13:33:43 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1584, average loss: 2.2813
[09/26 13:33:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 62.50	
[09/26 13:33:43 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/26 13:33:49 visual_prompt]: Epoch 4 / 100: avg data time: 5.05e-02, avg batch time: 0.4647, average train loss: 2.2708
[09/26 13:33:51 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1576, average loss: 2.2103
[09/26 13:33:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:33:51 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/26 13:33:57 visual_prompt]: Epoch 5 / 100: avg data time: 6.06e-02, avg batch time: 0.4728, average train loss: 2.2450
[09/26 13:33:59 visual_prompt]: Inference (val):avg data time: 5.01e-05, avg batch time: 0.1578, average loss: 2.2182
[09/26 13:33:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 13:33:59 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/26 13:34:05 visual_prompt]: Epoch 6 / 100: avg data time: 6.53e-02, avg batch time: 0.4782, average train loss: 2.2481
[09/26 13:34:07 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1588, average loss: 2.2209
[09/26 13:34:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.00	
[09/26 13:34:07 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/26 13:34:13 visual_prompt]: Epoch 7 / 100: avg data time: 6.08e-02, avg batch time: 0.4730, average train loss: 2.2458
[09/26 13:34:15 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1584, average loss: 2.2304
[09/26 13:34:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 14.50	top5: 64.50	
[09/26 13:34:15 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/26 13:34:21 visual_prompt]: Epoch 8 / 100: avg data time: 6.25e-02, avg batch time: 0.4754, average train loss: 2.2508
[09/26 13:34:23 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1577, average loss: 2.2907
[09/26 13:34:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 65.00	
[09/26 13:34:23 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/26 13:34:29 visual_prompt]: Epoch 9 / 100: avg data time: 5.69e-02, avg batch time: 0.4695, average train loss: 2.2530
[09/26 13:34:31 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1582, average loss: 2.2046
[09/26 13:34:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 66.00	
[09/26 13:34:31 visual_prompt]: Best epoch 9: best metric: 0.245
[09/26 13:34:31 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/26 13:34:37 visual_prompt]: Epoch 10 / 100: avg data time: 5.18e-02, avg batch time: 0.4667, average train loss: 2.2538
[09/26 13:34:39 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1582, average loss: 2.2518
[09/26 13:34:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 13:34:39 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/26 13:34:46 visual_prompt]: Epoch 11 / 100: avg data time: 6.34e-02, avg batch time: 0.4768, average train loss: 2.2264
[09/26 13:34:47 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1584, average loss: 2.2357
[09/26 13:34:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 16.50	top5: 62.50	
[09/26 13:34:47 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/26 13:34:54 visual_prompt]: Epoch 12 / 100: avg data time: 5.39e-02, avg batch time: 0.4701, average train loss: 2.2341
[09/26 13:34:55 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1584, average loss: 2.2966
[09/26 13:34:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 13:34:55 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/26 13:35:02 visual_prompt]: Epoch 13 / 100: avg data time: 5.76e-02, avg batch time: 0.4709, average train loss: 2.2302
[09/26 13:35:03 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.1580, average loss: 2.1196
[09/26 13:35:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 27.00	top5: 72.00	
[09/26 13:35:03 visual_prompt]: Best epoch 13: best metric: 0.270
[09/26 13:35:03 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/26 13:35:10 visual_prompt]: Epoch 14 / 100: avg data time: 5.28e-02, avg batch time: 0.4673, average train loss: 2.0588
[09/26 13:35:11 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1583, average loss: 1.9059
[09/26 13:35:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 35.50	top5: 77.50	
[09/26 13:35:11 visual_prompt]: Best epoch 14: best metric: 0.355
[09/26 13:35:11 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/26 13:35:18 visual_prompt]: Epoch 15 / 100: avg data time: 6.04e-02, avg batch time: 0.4742, average train loss: 1.9722
[09/26 13:35:19 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1588, average loss: 1.7908
[09/26 13:35:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 37.00	top5: 83.50	
[09/26 13:35:19 visual_prompt]: Best epoch 15: best metric: 0.370
[09/26 13:35:19 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/26 13:35:26 visual_prompt]: Epoch 16 / 100: avg data time: 5.37e-02, avg batch time: 0.4685, average train loss: 1.8858
[09/26 13:35:27 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1577, average loss: 1.6966
[09/26 13:35:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 35.00	top5: 88.00	
[09/26 13:35:27 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/26 13:35:34 visual_prompt]: Epoch 17 / 100: avg data time: 6.01e-02, avg batch time: 0.4744, average train loss: 1.6311
[09/26 13:35:35 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1584, average loss: 1.4716
[09/26 13:35:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 45.50	top5: 94.00	
[09/26 13:35:35 visual_prompt]: Best epoch 17: best metric: 0.455
[09/26 13:35:35 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/26 13:35:42 visual_prompt]: Epoch 18 / 100: avg data time: 5.93e-02, avg batch time: 0.4748, average train loss: 1.4376
[09/26 13:35:44 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1579, average loss: 1.5541
[09/26 13:35:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 43.50	top5: 90.00	
[09/26 13:35:44 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/26 13:35:50 visual_prompt]: Epoch 19 / 100: avg data time: 6.50e-02, avg batch time: 0.4789, average train loss: 1.4627
[09/26 13:35:52 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1583, average loss: 1.3032
[09/26 13:35:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 52.00	top5: 93.50	
[09/26 13:35:52 visual_prompt]: Best epoch 19: best metric: 0.520
[09/26 13:35:52 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/26 13:35:58 visual_prompt]: Epoch 20 / 100: avg data time: 5.96e-02, avg batch time: 0.4746, average train loss: 1.2338
[09/26 13:36:00 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1582, average loss: 1.2068
[09/26 13:36:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 55.50	top5: 94.00	
[09/26 13:36:00 visual_prompt]: Best epoch 20: best metric: 0.555
[09/26 13:36:00 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/26 13:36:06 visual_prompt]: Epoch 21 / 100: avg data time: 6.23e-02, avg batch time: 0.4764, average train loss: 1.1713
[09/26 13:36:08 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1583, average loss: 1.1758
[09/26 13:36:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 60.00	top5: 97.00	
[09/26 13:36:08 visual_prompt]: Best epoch 21: best metric: 0.600
[09/26 13:36:08 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/26 13:36:14 visual_prompt]: Epoch 22 / 100: avg data time: 6.31e-02, avg batch time: 0.4762, average train loss: 0.9763
[09/26 13:36:16 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1584, average loss: 0.9518
[09/26 13:36:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.00	top5: 96.00	
[09/26 13:36:16 visual_prompt]: Best epoch 22: best metric: 0.650
[09/26 13:36:16 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/26 13:36:22 visual_prompt]: Epoch 23 / 100: avg data time: 6.13e-02, avg batch time: 0.4752, average train loss: 0.9178
[09/26 13:36:24 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1581, average loss: 1.0019
[09/26 13:36:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.00	top5: 95.50	
[09/26 13:36:24 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/26 13:36:30 visual_prompt]: Epoch 24 / 100: avg data time: 5.13e-02, avg batch time: 0.4648, average train loss: 0.8913
[09/26 13:36:32 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1586, average loss: 1.0557
[09/26 13:36:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 62.00	top5: 97.00	
[09/26 13:36:32 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/26 13:36:38 visual_prompt]: Epoch 25 / 100: avg data time: 6.00e-02, avg batch time: 0.4733, average train loss: 0.7747
[09/26 13:36:40 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1586, average loss: 0.8002
[09/26 13:36:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 98.00	
[09/26 13:36:40 visual_prompt]: Best epoch 25: best metric: 0.715
[09/26 13:36:40 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/26 13:36:46 visual_prompt]: Epoch 26 / 100: avg data time: 6.00e-02, avg batch time: 0.4735, average train loss: 0.6942
[09/26 13:36:48 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1582, average loss: 0.7987
[09/26 13:36:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 98.00	
[09/26 13:36:48 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/26 13:36:55 visual_prompt]: Epoch 27 / 100: avg data time: 6.10e-02, avg batch time: 0.4747, average train loss: 0.6605
[09/26 13:36:56 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1581, average loss: 0.7513
[09/26 13:36:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 98.00	
[09/26 13:36:56 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/26 13:37:03 visual_prompt]: Epoch 28 / 100: avg data time: 5.75e-02, avg batch time: 0.4708, average train loss: 0.5564
[09/26 13:37:04 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1588, average loss: 1.0278
[09/26 13:37:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.00	
[09/26 13:37:04 visual_prompt]: Best epoch 28: best metric: 0.725
[09/26 13:37:04 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/26 13:37:11 visual_prompt]: Epoch 29 / 100: avg data time: 6.12e-02, avg batch time: 0.4743, average train loss: 0.4987
[09/26 13:37:12 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1583, average loss: 0.7947
[09/26 13:37:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 98.50	
[09/26 13:37:12 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/26 13:37:19 visual_prompt]: Epoch 30 / 100: avg data time: 5.98e-02, avg batch time: 0.4735, average train loss: 0.5051
[09/26 13:37:20 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1581, average loss: 1.0139
[09/26 13:37:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.00	top5: 96.50	
[09/26 13:37:20 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/26 13:37:27 visual_prompt]: Epoch 31 / 100: avg data time: 6.07e-02, avg batch time: 0.4736, average train loss: 0.4452
[09/26 13:37:28 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1583, average loss: 1.0458
[09/26 13:37:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 62.50	top5: 96.00	
[09/26 13:37:28 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/26 13:37:35 visual_prompt]: Epoch 32 / 100: avg data time: 6.55e-02, avg batch time: 0.4805, average train loss: 0.4797
[09/26 13:37:36 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1583, average loss: 0.8632
[09/26 13:37:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.50	
[09/26 13:37:36 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/26 13:37:43 visual_prompt]: Epoch 33 / 100: avg data time: 5.40e-02, avg batch time: 0.4688, average train loss: 0.4861
[09/26 13:37:45 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1589, average loss: 0.8814
[09/26 13:37:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.50	top5: 97.00	
[09/26 13:37:45 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/26 13:37:51 visual_prompt]: Epoch 34 / 100: avg data time: 5.20e-02, avg batch time: 0.4678, average train loss: 0.3556
[09/26 13:37:52 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1581, average loss: 0.7651
[09/26 13:37:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.50	top5: 96.50	
[09/26 13:37:52 visual_prompt]: Best epoch 34: best metric: 0.775
[09/26 13:37:52 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/26 13:37:59 visual_prompt]: Epoch 35 / 100: avg data time: 6.23e-02, avg batch time: 0.4749, average train loss: 0.3018
[09/26 13:38:01 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1584, average loss: 0.6122
[09/26 13:38:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 97.00	
[09/26 13:38:01 visual_prompt]: Best epoch 35: best metric: 0.820
[09/26 13:38:01 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/26 13:38:07 visual_prompt]: Epoch 36 / 100: avg data time: 6.83e-02, avg batch time: 0.4807, average train loss: 0.2329
[09/26 13:38:09 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1584, average loss: 0.9803
[09/26 13:38:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 94.00	
[09/26 13:38:09 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/26 13:38:15 visual_prompt]: Epoch 37 / 100: avg data time: 5.92e-02, avg batch time: 0.4732, average train loss: 0.1568
[09/26 13:38:17 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1584, average loss: 1.0186
[09/26 13:38:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.50	top5: 98.00	
[09/26 13:38:17 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/26 13:38:23 visual_prompt]: Epoch 38 / 100: avg data time: 5.37e-02, avg batch time: 0.4683, average train loss: 0.1685
[09/26 13:38:25 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1581, average loss: 0.8926
[09/26 13:38:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 98.50	
[09/26 13:38:25 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/26 13:38:31 visual_prompt]: Epoch 39 / 100: avg data time: 6.11e-02, avg batch time: 0.4750, average train loss: 0.1714
[09/26 13:38:33 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1583, average loss: 0.8625
[09/26 13:38:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 95.00	
[09/26 13:38:33 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/26 13:38:39 visual_prompt]: Epoch 40 / 100: avg data time: 6.12e-02, avg batch time: 0.4746, average train loss: 0.1491
[09/26 13:38:41 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1584, average loss: 0.8375
[09/26 13:38:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 96.50	
[09/26 13:38:41 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/26 13:38:47 visual_prompt]: Epoch 41 / 100: avg data time: 5.34e-02, avg batch time: 0.4673, average train loss: 0.1223
[09/26 13:38:49 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1578, average loss: 1.0386
[09/26 13:38:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 13:38:49 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/26 13:38:55 visual_prompt]: Epoch 42 / 100: avg data time: 5.02e-02, avg batch time: 0.4628, average train loss: 0.0762
[09/26 13:38:57 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1581, average loss: 0.8216
[09/26 13:38:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 97.00	
[09/26 13:38:57 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/26 13:39:03 visual_prompt]: Epoch 43 / 100: avg data time: 5.48e-02, avg batch time: 0.4685, average train loss: 0.1047
[09/26 13:39:05 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1589, average loss: 0.9612
[09/26 13:39:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.00	
[09/26 13:39:05 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/26 13:39:11 visual_prompt]: Epoch 44 / 100: avg data time: 5.89e-02, avg batch time: 0.4717, average train loss: 0.1187
[09/26 13:39:13 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1584, average loss: 0.8087
[09/26 13:39:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 96.50	
[09/26 13:39:13 visual_prompt]: Best epoch 44: best metric: 0.830
[09/26 13:39:13 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/26 13:39:19 visual_prompt]: Epoch 45 / 100: avg data time: 5.35e-02, avg batch time: 0.4675, average train loss: 0.0726
[09/26 13:39:21 visual_prompt]: Inference (val):avg data time: 4.25e-05, avg batch time: 0.1583, average loss: 0.7251
[09/26 13:39:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 98.00	
[09/26 13:39:21 visual_prompt]: Best epoch 45: best metric: 0.855
[09/26 13:39:21 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/26 13:39:27 visual_prompt]: Epoch 46 / 100: avg data time: 6.08e-02, avg batch time: 0.4739, average train loss: 0.0501
[09/26 13:39:29 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1586, average loss: 0.8194
[09/26 13:39:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.00	
[09/26 13:39:29 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/26 13:39:36 visual_prompt]: Epoch 47 / 100: avg data time: 6.11e-02, avg batch time: 0.4751, average train loss: 0.0418
[09/26 13:39:37 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1581, average loss: 0.8262
[09/26 13:39:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 97.50	
[09/26 13:39:37 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/26 13:39:43 visual_prompt]: Epoch 48 / 100: avg data time: 4.79e-02, avg batch time: 0.4606, average train loss: 0.0430
[09/26 13:39:45 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1589, average loss: 0.8761
[09/26 13:39:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 96.50	
[09/26 13:39:45 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/26 13:39:51 visual_prompt]: Epoch 49 / 100: avg data time: 5.60e-02, avg batch time: 0.4685, average train loss: 0.0261
[09/26 13:39:53 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1578, average loss: 0.8790
[09/26 13:39:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.50	
[09/26 13:39:53 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/26 13:39:59 visual_prompt]: Epoch 50 / 100: avg data time: 5.89e-02, avg batch time: 0.4730, average train loss: 0.0366
[09/26 13:40:01 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1582, average loss: 0.8902
[09/26 13:40:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 13:40:01 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/26 13:40:07 visual_prompt]: Epoch 51 / 100: avg data time: 5.11e-02, avg batch time: 0.4648, average train loss: 0.0222
[09/26 13:40:09 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1582, average loss: 0.8575
[09/26 13:40:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 98.00	
[09/26 13:40:09 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/26 13:40:15 visual_prompt]: Epoch 52 / 100: avg data time: 5.78e-02, avg batch time: 0.4711, average train loss: 0.0169
[09/26 13:40:17 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1586, average loss: 0.8815
[09/26 13:40:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 97.50	
[09/26 13:40:17 visual_prompt]: Best epoch 52: best metric: 0.860
[09/26 13:40:17 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/26 13:40:24 visual_prompt]: Epoch 53 / 100: avg data time: 6.61e-02, avg batch time: 0.4785, average train loss: 0.0053
[09/26 13:40:25 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1581, average loss: 0.9476
[09/26 13:40:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 96.50	
[09/26 13:40:25 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/26 13:40:32 visual_prompt]: Epoch 54 / 100: avg data time: 5.78e-02, avg batch time: 0.4721, average train loss: 0.0070
[09/26 13:40:33 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1583, average loss: 0.8510
[09/26 13:40:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 96.00	
[09/26 13:40:33 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/26 13:40:40 visual_prompt]: Epoch 55 / 100: avg data time: 5.47e-02, avg batch time: 0.4688, average train loss: 0.0044
[09/26 13:40:41 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.1578, average loss: 0.9752
[09/26 13:40:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 97.50	
[09/26 13:40:41 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/26 13:40:48 visual_prompt]: Epoch 56 / 100: avg data time: 5.72e-02, avg batch time: 0.4705, average train loss: 0.0096
[09/26 13:40:49 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1585, average loss: 1.0971
[09/26 13:40:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 95.50	
[09/26 13:40:49 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/26 13:40:56 visual_prompt]: Epoch 57 / 100: avg data time: 5.33e-02, avg batch time: 0.4674, average train loss: 0.0050
[09/26 13:40:57 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1579, average loss: 0.9699
[09/26 13:40:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 97.50	
[09/26 13:40:57 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/26 13:41:04 visual_prompt]: Epoch 58 / 100: avg data time: 6.02e-02, avg batch time: 0.4744, average train loss: 0.0092
[09/26 13:41:05 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1582, average loss: 0.9246
[09/26 13:41:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.00	
[09/26 13:41:05 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/26 13:41:12 visual_prompt]: Epoch 59 / 100: avg data time: 6.10e-02, avg batch time: 0.4745, average train loss: 0.0044
[09/26 13:41:13 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1581, average loss: 1.0613
[09/26 13:41:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 96.50	
[09/26 13:41:13 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/26 13:41:20 visual_prompt]: Epoch 60 / 100: avg data time: 6.42e-02, avg batch time: 0.4778, average train loss: 0.0055
[09/26 13:41:22 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1585, average loss: 0.9747
[09/26 13:41:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 96.50	
[09/26 13:41:22 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/26 13:41:28 visual_prompt]: Epoch 61 / 100: avg data time: 6.34e-02, avg batch time: 0.4761, average train loss: 0.0077
[09/26 13:41:30 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1584, average loss: 0.9245
[09/26 13:41:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.00	
[09/26 13:41:30 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/26 13:41:36 visual_prompt]: Epoch 62 / 100: avg data time: 5.74e-02, avg batch time: 0.4703, average train loss: 0.0023
[09/26 13:41:38 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1582, average loss: 1.0320
[09/26 13:41:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 97.50	
[09/26 13:41:38 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/26 13:41:44 visual_prompt]: Epoch 63 / 100: avg data time: 6.08e-02, avg batch time: 0.4749, average train loss: 0.0019
[09/26 13:41:46 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 1.0572
[09/26 13:41:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 97.50	
[09/26 13:41:46 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/26 13:41:52 visual_prompt]: Epoch 64 / 100: avg data time: 6.13e-02, avg batch time: 0.4751, average train loss: 0.0013
[09/26 13:41:54 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1587, average loss: 1.0393
[09/26 13:41:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.00	
[09/26 13:41:54 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/26 13:42:00 visual_prompt]: Epoch 65 / 100: avg data time: 4.60e-02, avg batch time: 0.4613, average train loss: 0.0013
[09/26 13:42:02 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1585, average loss: 1.0295
[09/26 13:42:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 13:42:02 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/26 13:42:08 visual_prompt]: Epoch 66 / 100: avg data time: 6.02e-02, avg batch time: 0.4728, average train loss: 0.0012
[09/26 13:42:10 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1587, average loss: 1.0165
[09/26 13:42:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 96.50	
[09/26 13:42:10 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/26 13:42:16 visual_prompt]: Epoch 67 / 100: avg data time: 5.94e-02, avg batch time: 0.4725, average train loss: 0.0005
[09/26 13:42:18 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1578, average loss: 1.0103
[09/26 13:42:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:42:18 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/26 13:42:24 visual_prompt]: Epoch 68 / 100: avg data time: 5.45e-02, avg batch time: 0.4704, average train loss: 0.0007
[09/26 13:42:26 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1582, average loss: 1.0131
[09/26 13:42:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:42:26 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/26 13:42:32 visual_prompt]: Epoch 69 / 100: avg data time: 5.98e-02, avg batch time: 0.4744, average train loss: 0.0007
[09/26 13:42:34 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1581, average loss: 1.0183
[09/26 13:42:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 97.00	
[09/26 13:42:34 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/26 13:42:40 visual_prompt]: Epoch 70 / 100: avg data time: 5.28e-02, avg batch time: 0.4666, average train loss: 0.0006
[09/26 13:42:42 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1586, average loss: 1.0201
[09/26 13:42:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:42:42 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/26 13:42:48 visual_prompt]: Epoch 71 / 100: avg data time: 5.71e-02, avg batch time: 0.4710, average train loss: 0.0006
[09/26 13:42:50 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1581, average loss: 1.0224
[09/26 13:42:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:42:50 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/26 13:42:56 visual_prompt]: Epoch 72 / 100: avg data time: 5.24e-02, avg batch time: 0.4665, average train loss: 0.0005
[09/26 13:42:58 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1582, average loss: 1.0233
[09/26 13:42:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:42:58 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/26 13:43:04 visual_prompt]: Epoch 73 / 100: avg data time: 5.38e-02, avg batch time: 0.4674, average train loss: 0.0005
[09/26 13:43:06 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1584, average loss: 1.0241
[09/26 13:43:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:43:06 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/26 13:43:12 visual_prompt]: Epoch 74 / 100: avg data time: 5.65e-02, avg batch time: 0.4712, average train loss: 0.0006
[09/26 13:43:14 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1580, average loss: 1.0275
[09/26 13:43:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 13:43:14 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/26 13:43:20 visual_prompt]: Epoch 75 / 100: avg data time: 5.27e-02, avg batch time: 0.4663, average train loss: 0.0006
[09/26 13:43:22 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1582, average loss: 1.0278
[09/26 13:43:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 13:43:22 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/26 13:43:28 visual_prompt]: Epoch 76 / 100: avg data time: 6.58e-02, avg batch time: 0.4791, average train loss: 0.0006
[09/26 13:43:30 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1583, average loss: 1.0284
[09/26 13:43:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 13:43:30 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/26 13:43:36 visual_prompt]: Epoch 77 / 100: avg data time: 5.24e-02, avg batch time: 0.4664, average train loss: 0.0006
[09/26 13:43:38 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1585, average loss: 1.0320
[09/26 13:43:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 96.50	
[09/26 13:43:38 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/26 13:43:44 visual_prompt]: Epoch 78 / 100: avg data time: 6.71e-02, avg batch time: 0.4812, average train loss: 0.0005
[09/26 13:43:46 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1586, average loss: 1.0315
[09/26 13:43:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 13:43:46 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/26 13:43:52 visual_prompt]: Epoch 79 / 100: avg data time: 5.65e-02, avg batch time: 0.4728, average train loss: 0.0006
[09/26 13:43:54 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1585, average loss: 1.0316
[09/26 13:43:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 13:43:54 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/26 13:44:01 visual_prompt]: Epoch 80 / 100: avg data time: 5.66e-02, avg batch time: 0.4710, average train loss: 0.0005
[09/26 13:44:02 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1582, average loss: 1.0336
[09/26 13:44:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 13:44:02 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/26 13:44:09 visual_prompt]: Epoch 81 / 100: avg data time: 5.69e-02, avg batch time: 0.4721, average train loss: 0.0004
[09/26 13:44:10 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1584, average loss: 1.0348
[09/26 13:44:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 13:44:10 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/26 13:44:17 visual_prompt]: Epoch 82 / 100: avg data time: 6.16e-02, avg batch time: 0.4753, average train loss: 0.0004
[09/26 13:44:18 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1585, average loss: 1.0346
[09/26 13:44:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 13:44:18 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/26 13:44:25 visual_prompt]: Epoch 83 / 100: avg data time: 6.02e-02, avg batch time: 0.4744, average train loss: 0.0005
[09/26 13:44:26 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1578, average loss: 1.0330
[09/26 13:44:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:44:26 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/26 13:44:33 visual_prompt]: Epoch 84 / 100: avg data time: 6.08e-02, avg batch time: 0.4755, average train loss: 0.0007
[09/26 13:44:34 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1582, average loss: 1.0297
[09/26 13:44:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:44:34 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/26 13:44:41 visual_prompt]: Epoch 85 / 100: avg data time: 6.18e-02, avg batch time: 0.4746, average train loss: 0.0005
[09/26 13:44:42 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1583, average loss: 1.0274
[09/26 13:44:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 13:44:42 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/26 13:44:49 visual_prompt]: Epoch 86 / 100: avg data time: 5.30e-02, avg batch time: 0.4667, average train loss: 0.0005
[09/26 13:44:50 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1583, average loss: 1.0274
[09/26 13:44:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:44:50 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/26 13:44:57 visual_prompt]: Epoch 87 / 100: avg data time: 6.49e-02, avg batch time: 0.4776, average train loss: 0.0004
[09/26 13:44:59 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1581, average loss: 1.0283
[09/26 13:44:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:44:59 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/26 13:45:05 visual_prompt]: Epoch 88 / 100: avg data time: 6.22e-02, avg batch time: 0.4765, average train loss: 0.0006
[09/26 13:45:07 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1584, average loss: 1.0293
[09/26 13:45:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:45:07 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/26 13:45:13 visual_prompt]: Epoch 89 / 100: avg data time: 4.72e-02, avg batch time: 0.4611, average train loss: 0.0005
[09/26 13:45:14 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1584, average loss: 1.0299
[09/26 13:45:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 13:45:14 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/26 13:45:21 visual_prompt]: Epoch 90 / 100: avg data time: 5.57e-02, avg batch time: 0.4698, average train loss: 0.0004
[09/26 13:45:22 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1586, average loss: 1.0304
[09/26 13:45:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 13:45:22 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/26 13:45:29 visual_prompt]: Epoch 91 / 100: avg data time: 5.97e-02, avg batch time: 0.4733, average train loss: 0.0005
[09/26 13:45:31 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1580, average loss: 1.0309
[09/26 13:45:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 97.00	
[09/26 13:45:31 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/26 13:45:37 visual_prompt]: Epoch 92 / 100: avg data time: 5.91e-02, avg batch time: 0.4736, average train loss: 0.0004
[09/26 13:45:39 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.1585, average loss: 1.0314
[09/26 13:45:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:45:39 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/26 13:45:45 visual_prompt]: Epoch 93 / 100: avg data time: 6.35e-02, avg batch time: 0.4763, average train loss: 0.0005
[09/26 13:45:47 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1582, average loss: 1.0313
[09/26 13:45:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:45:47 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/26 13:45:53 visual_prompt]: Epoch 94 / 100: avg data time: 5.69e-02, avg batch time: 0.4709, average train loss: 0.0003
[09/26 13:45:55 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1586, average loss: 1.0310
[09/26 13:45:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:45:55 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/26 13:46:01 visual_prompt]: Epoch 95 / 100: avg data time: 6.09e-02, avg batch time: 0.4741, average train loss: 0.0006
[09/26 13:46:03 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1583, average loss: 1.0312
[09/26 13:46:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:46:03 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/26 13:46:09 visual_prompt]: Epoch 96 / 100: avg data time: 6.19e-02, avg batch time: 0.4748, average train loss: 0.0004
[09/26 13:46:11 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1583, average loss: 1.0313
[09/26 13:46:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:46:11 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/26 13:46:18 visual_prompt]: Epoch 97 / 100: avg data time: 6.78e-02, avg batch time: 0.4809, average train loss: 0.0005
[09/26 13:46:19 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.1582, average loss: 1.0314
[09/26 13:46:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:46:19 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/26 13:46:26 visual_prompt]: Epoch 98 / 100: avg data time: 6.49e-02, avg batch time: 0.4782, average train loss: 0.0005
[09/26 13:46:27 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 1.0315
[09/26 13:46:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:46:27 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/26 13:46:34 visual_prompt]: Epoch 99 / 100: avg data time: 6.21e-02, avg batch time: 0.4758, average train loss: 0.0004
[09/26 13:46:35 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1584, average loss: 1.0315
[09/26 13:46:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:46:35 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/26 13:46:42 visual_prompt]: Epoch 100 / 100: avg data time: 5.88e-02, avg batch time: 0.4715, average train loss: 0.0005
[09/26 13:46:43 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1586, average loss: 1.0315
[09/26 13:46:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.00	
[09/26 13:46:43 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 13:46:43 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 13:46:43 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 13:46:43 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 13:46:43 visual_prompt]: Training with config:
[09/26 13:46:43 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.1_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 13:46:43 visual_prompt]: Loading training data...
[09/26 13:46:43 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 13:46:44 visual_prompt]: Number of images: 800
[09/26 13:46:44 visual_prompt]: Number of classes: 10 / 10
[09/26 13:46:44 visual_prompt]: Loading validation data...
[09/26 13:46:44 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 13:46:45 visual_prompt]: Number of images: 200
[09/26 13:46:45 visual_prompt]: Number of classes: 10 / 10
[09/26 13:46:45 visual_prompt]: Constructing models...
[09/26 13:46:47 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 13:46:47 visual_prompt]: tuned percent:0.543
[09/26 13:46:47 visual_prompt]: Device used for model: 0
[09/26 13:46:47 visual_prompt]: Setting up Evaluator...
[09/26 13:46:47 visual_prompt]: Setting up Trainer...
[09/26 13:46:47 visual_prompt]: 	Setting up the optimizer...
[09/26 13:46:47 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 13:46:54 visual_prompt]: Epoch 1 / 100: avg data time: 6.14e-02, avg batch time: 0.4793, average train loss: 2.6890
[09/26 13:46:55 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1577, average loss: 2.6214
[09/26 13:46:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 13:46:55 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 13:46:55 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[09/26 13:47:02 visual_prompt]: Epoch 2 / 100: avg data time: 6.56e-02, avg batch time: 0.4786, average train loss: 2.3473
[09/26 13:47:03 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1583, average loss: 2.2263
[09/26 13:47:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 13:47:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[09/26 13:47:10 visual_prompt]: Epoch 3 / 100: avg data time: 6.10e-02, avg batch time: 0.4736, average train loss: 2.2696
[09/26 13:47:12 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1580, average loss: 2.2409
[09/26 13:47:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 13:47:12 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[09/26 13:47:18 visual_prompt]: Epoch 4 / 100: avg data time: 5.48e-02, avg batch time: 0.4688, average train loss: 2.2516
[09/26 13:47:20 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1579, average loss: 2.2108
[09/26 13:47:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 13:47:20 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[09/26 13:47:26 visual_prompt]: Epoch 5 / 100: avg data time: 5.86e-02, avg batch time: 0.4725, average train loss: 2.2538
[09/26 13:47:28 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1586, average loss: 2.2142
[09/26 13:47:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 13:47:28 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[09/26 13:47:34 visual_prompt]: Epoch 6 / 100: avg data time: 6.53e-02, avg batch time: 0.4776, average train loss: 2.2504
[09/26 13:47:36 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1584, average loss: 2.2240
[09/26 13:47:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:47:36 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[09/26 13:47:42 visual_prompt]: Epoch 7 / 100: avg data time: 5.98e-02, avg batch time: 0.4721, average train loss: 2.2533
[09/26 13:47:44 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1583, average loss: 2.2195
[09/26 13:47:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 13:47:44 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[09/26 13:47:50 visual_prompt]: Epoch 8 / 100: avg data time: 6.15e-02, avg batch time: 0.4749, average train loss: 2.2586
[09/26 13:47:52 visual_prompt]: Inference (val):avg data time: 4.81e-05, avg batch time: 0.1584, average loss: 2.2364
[09/26 13:47:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 13.00	top5: 64.00	
[09/26 13:47:52 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[09/26 13:47:58 visual_prompt]: Epoch 9 / 100: avg data time: 6.05e-02, avg batch time: 0.4751, average train loss: 2.2612
[09/26 13:48:00 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1577, average loss: 2.2313
[09/26 13:48:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.00	top5: 65.00	
[09/26 13:48:00 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[09/26 13:48:06 visual_prompt]: Epoch 10 / 100: avg data time: 6.56e-02, avg batch time: 0.4790, average train loss: 2.2426
[09/26 13:48:08 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1582, average loss: 2.2060
[09/26 13:48:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 13:48:08 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[09/26 13:48:15 visual_prompt]: Epoch 11 / 100: avg data time: 6.34e-02, avg batch time: 0.4780, average train loss: 2.2375
[09/26 13:48:16 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1587, average loss: 2.2386
[09/26 13:48:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 60.00	
[09/26 13:48:16 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[09/26 13:48:23 visual_prompt]: Epoch 12 / 100: avg data time: 6.44e-02, avg batch time: 0.4778, average train loss: 2.2410
[09/26 13:48:24 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1585, average loss: 2.1859
[09/26 13:48:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:48:24 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[09/26 13:48:31 visual_prompt]: Epoch 13 / 100: avg data time: 6.05e-02, avg batch time: 0.4732, average train loss: 2.2174
[09/26 13:48:32 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1585, average loss: 2.1680
[09/26 13:48:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.50	top5: 64.50	
[09/26 13:48:32 visual_prompt]: Best epoch 13: best metric: 0.255
[09/26 13:48:32 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[09/26 13:48:39 visual_prompt]: Epoch 14 / 100: avg data time: 6.32e-02, avg batch time: 0.4775, average train loss: 2.1799
[09/26 13:48:40 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1586, average loss: 2.1012
[09/26 13:48:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 27.50	top5: 65.50	
[09/26 13:48:40 visual_prompt]: Best epoch 14: best metric: 0.275
[09/26 13:48:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[09/26 13:48:47 visual_prompt]: Epoch 15 / 100: avg data time: 5.70e-02, avg batch time: 0.4709, average train loss: 2.1219
[09/26 13:48:48 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1581, average loss: 2.0528
[09/26 13:48:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.00	top5: 69.00	
[09/26 13:48:48 visual_prompt]: Best epoch 15: best metric: 0.290
[09/26 13:48:48 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[09/26 13:48:55 visual_prompt]: Epoch 16 / 100: avg data time: 5.87e-02, avg batch time: 0.4718, average train loss: 2.1164
[09/26 13:48:56 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1587, average loss: 1.9991
[09/26 13:48:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.50	top5: 77.50	
[09/26 13:48:56 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[09/26 13:49:03 visual_prompt]: Epoch 17 / 100: avg data time: 5.21e-02, avg batch time: 0.4672, average train loss: 2.0040
[09/26 13:49:04 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1585, average loss: 2.5824
[09/26 13:49:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 48.50	
[09/26 13:49:04 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[09/26 13:49:11 visual_prompt]: Epoch 18 / 100: avg data time: 6.49e-02, avg batch time: 0.4780, average train loss: 2.1128
[09/26 13:49:12 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1580, average loss: 2.0358
[09/26 13:49:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 27.50	top5: 76.00	
[09/26 13:49:12 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[09/26 13:49:19 visual_prompt]: Epoch 19 / 100: avg data time: 5.25e-02, avg batch time: 0.4663, average train loss: 1.9942
[09/26 13:49:20 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1586, average loss: 2.1027
[09/26 13:49:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.50	top5: 67.00	
[09/26 13:49:20 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[09/26 13:49:27 visual_prompt]: Epoch 20 / 100: avg data time: 5.34e-02, avg batch time: 0.4672, average train loss: 2.2666
[09/26 13:49:28 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1577, average loss: 2.0176
[09/26 13:49:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 30.50	top5: 73.00	
[09/26 13:49:28 visual_prompt]: Best epoch 20: best metric: 0.305
[09/26 13:49:28 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[09/26 13:49:35 visual_prompt]: Epoch 21 / 100: avg data time: 4.89e-02, avg batch time: 0.4632, average train loss: 2.0421
[09/26 13:49:36 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1580, average loss: 2.0644
[09/26 13:49:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.50	top5: 68.50	
[09/26 13:49:36 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[09/26 13:49:43 visual_prompt]: Epoch 22 / 100: avg data time: 5.89e-02, avg batch time: 0.4719, average train loss: 2.1233
[09/26 13:49:44 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1582, average loss: 1.9488
[09/26 13:49:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 37.00	top5: 82.00	
[09/26 13:49:44 visual_prompt]: Best epoch 22: best metric: 0.370
[09/26 13:49:44 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[09/26 13:49:51 visual_prompt]: Epoch 23 / 100: avg data time: 6.36e-02, avg batch time: 0.4789, average train loss: 1.8386
[09/26 13:49:52 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1584, average loss: 2.1902
[09/26 13:49:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 67.50	
[09/26 13:49:52 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[09/26 13:49:59 visual_prompt]: Epoch 24 / 100: avg data time: 5.88e-02, avg batch time: 0.4734, average train loss: 2.1563
[09/26 13:50:00 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1585, average loss: 1.8354
[09/26 13:50:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 33.50	top5: 77.50	
[09/26 13:50:00 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[09/26 13:50:07 visual_prompt]: Epoch 25 / 100: avg data time: 5.55e-02, avg batch time: 0.4690, average train loss: 1.9162
[09/26 13:50:08 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1580, average loss: 2.4541
[09/26 13:50:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 54.50	
[09/26 13:50:08 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[09/26 13:50:15 visual_prompt]: Epoch 26 / 100: avg data time: 6.76e-02, avg batch time: 0.4814, average train loss: 2.1557
[09/26 13:50:17 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1585, average loss: 2.2497
[09/26 13:50:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.00	
[09/26 13:50:17 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[09/26 13:50:23 visual_prompt]: Epoch 27 / 100: avg data time: 5.79e-02, avg batch time: 0.4711, average train loss: 2.2730
[09/26 13:50:25 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1583, average loss: 2.2148
[09/26 13:50:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 13:50:25 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[09/26 13:50:31 visual_prompt]: Epoch 28 / 100: avg data time: 5.70e-02, avg batch time: 0.4703, average train loss: 2.2527
[09/26 13:50:33 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1583, average loss: 2.2236
[09/26 13:50:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 15.50	top5: 72.50	
[09/26 13:50:33 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[09/26 13:50:39 visual_prompt]: Epoch 29 / 100: avg data time: 6.43e-02, avg batch time: 0.4774, average train loss: 2.1983
[09/26 13:50:41 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1585, average loss: 2.2142
[09/26 13:50:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 13:50:41 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[09/26 13:50:47 visual_prompt]: Epoch 30 / 100: avg data time: 5.77e-02, avg batch time: 0.4733, average train loss: 2.2465
[09/26 13:50:49 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1582, average loss: 2.2206
[09/26 13:50:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 13:50:49 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[09/26 13:50:55 visual_prompt]: Epoch 31 / 100: avg data time: 5.99e-02, avg batch time: 0.4739, average train loss: 2.2353
[09/26 13:50:57 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1585, average loss: 2.2085
[09/26 13:50:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 64.50	
[09/26 13:50:57 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[09/26 13:51:03 visual_prompt]: Epoch 32 / 100: avg data time: 5.64e-02, avg batch time: 0.4723, average train loss: 2.1932
[09/26 13:51:05 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1581, average loss: 2.1790
[09/26 13:51:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 71.50	
[09/26 13:51:05 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[09/26 13:51:12 visual_prompt]: Epoch 33 / 100: avg data time: 5.86e-02, avg batch time: 0.4717, average train loss: 2.1750
[09/26 13:51:13 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1582, average loss: 2.1941
[09/26 13:51:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 13:51:13 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[09/26 13:51:20 visual_prompt]: Epoch 34 / 100: avg data time: 5.65e-02, avg batch time: 0.4697, average train loss: 2.1803
[09/26 13:51:21 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1580, average loss: 2.2260
[09/26 13:51:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 18.00	top5: 63.50	
[09/26 13:51:21 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[09/26 13:51:28 visual_prompt]: Epoch 35 / 100: avg data time: 5.56e-02, avg batch time: 0.4701, average train loss: 2.1145
[09/26 13:51:29 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1584, average loss: 2.2757
[09/26 13:51:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 59.50	
[09/26 13:51:29 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[09/26 13:51:36 visual_prompt]: Epoch 36 / 100: avg data time: 5.16e-02, avg batch time: 0.4655, average train loss: 2.2304
[09/26 13:51:37 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1583, average loss: 2.2279
[09/26 13:51:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 28.50	top5: 65.00	
[09/26 13:51:37 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[09/26 13:51:44 visual_prompt]: Epoch 37 / 100: avg data time: 5.45e-02, avg batch time: 0.4712, average train loss: 2.1821
[09/26 13:51:45 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1584, average loss: 1.9943
[09/26 13:51:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 33.00	top5: 72.50	
[09/26 13:51:45 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[09/26 13:51:52 visual_prompt]: Epoch 38 / 100: avg data time: 6.25e-02, avg batch time: 0.4755, average train loss: 2.0056
[09/26 13:51:53 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1582, average loss: 1.8414
[09/26 13:51:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 32.00	top5: 79.50	
[09/26 13:51:53 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.07795964517353735
[09/26 13:52:00 visual_prompt]: Epoch 39 / 100: avg data time: 6.00e-02, avg batch time: 0.4742, average train loss: 1.8886
[09/26 13:52:02 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1586, average loss: 1.9442
[09/26 13:52:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 26.00	top5: 78.00	
[09/26 13:52:02 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.07649596321166025
[09/26 13:52:08 visual_prompt]: Epoch 40 / 100: avg data time: 6.57e-02, avg batch time: 0.4794, average train loss: 2.1440
[09/26 13:52:10 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1583, average loss: 1.8680
[09/26 13:52:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 33.50	top5: 83.50	
[09/26 13:52:10 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.07500000000000001
[09/26 13:52:16 visual_prompt]: Epoch 41 / 100: avg data time: 5.22e-02, avg batch time: 0.4672, average train loss: 1.9514
[09/26 13:52:18 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1584, average loss: 1.7233
[09/26 13:52:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 32.50	top5: 88.50	
[09/26 13:52:18 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.07347357813929455
[09/26 13:52:24 visual_prompt]: Epoch 42 / 100: avg data time: 4.42e-02, avg batch time: 0.4606, average train loss: 1.7586
[09/26 13:52:26 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1583, average loss: 1.6057
[09/26 13:52:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 44.00	top5: 89.00	
[09/26 13:52:26 visual_prompt]: Best epoch 42: best metric: 0.440
[09/26 13:52:26 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.07191855733945388
[09/26 13:52:32 visual_prompt]: Epoch 43 / 100: avg data time: 6.52e-02, avg batch time: 0.4781, average train loss: 1.7132
[09/26 13:52:34 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1582, average loss: 1.4553
[09/26 13:52:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 47.50	top5: 90.00	
[09/26 13:52:34 visual_prompt]: Best epoch 43: best metric: 0.475
[09/26 13:52:34 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.07033683215379002
[09/26 13:52:40 visual_prompt]: Epoch 44 / 100: avg data time: 5.51e-02, avg batch time: 0.4702, average train loss: 1.5745
[09/26 13:52:42 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1584, average loss: 1.5090
[09/26 13:52:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 41.00	top5: 92.00	
[09/26 13:52:42 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0687303296707956
[09/26 13:52:48 visual_prompt]: Epoch 45 / 100: avg data time: 6.16e-02, avg batch time: 0.4745, average train loss: 1.5481
[09/26 13:52:50 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1583, average loss: 1.3208
[09/26 13:52:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 55.00	top5: 93.00	
[09/26 13:52:50 visual_prompt]: Best epoch 45: best metric: 0.550
[09/26 13:52:50 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.06710100716628345
[09/26 13:52:56 visual_prompt]: Epoch 46 / 100: avg data time: 6.57e-02, avg batch time: 0.4793, average train loss: 1.4097
[09/26 13:52:58 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1580, average loss: 1.1770
[09/26 13:52:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 58.00	top5: 93.00	
[09/26 13:52:58 visual_prompt]: Best epoch 46: best metric: 0.580
[09/26 13:52:58 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.06545084971874737
[09/26 13:53:05 visual_prompt]: Epoch 47 / 100: avg data time: 6.32e-02, avg batch time: 0.4766, average train loss: 1.1988
[09/26 13:53:06 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1588, average loss: 1.2168
[09/26 13:53:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 52.50	top5: 95.00	
[09/26 13:53:06 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.06378186779084996
[09/26 13:53:13 visual_prompt]: Epoch 48 / 100: avg data time: 5.81e-02, avg batch time: 0.4738, average train loss: 1.2103
[09/26 13:53:14 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1585, average loss: 1.0440
[09/26 13:53:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 61.00	top5: 96.00	
[09/26 13:53:14 visual_prompt]: Best epoch 48: best metric: 0.610
[09/26 13:53:14 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.06209609477998338
[09/26 13:53:21 visual_prompt]: Epoch 49 / 100: avg data time: 5.84e-02, avg batch time: 0.4715, average train loss: 1.0438
[09/26 13:53:22 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1581, average loss: 1.3437
[09/26 13:53:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 47.00	top5: 95.50	
[09/26 13:53:22 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.06039558454088796
[09/26 13:53:29 visual_prompt]: Epoch 50 / 100: avg data time: 4.69e-02, avg batch time: 0.4609, average train loss: 1.0881
[09/26 13:53:30 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1587, average loss: 1.3867
[09/26 13:53:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 53.00	top5: 92.50	
[09/26 13:53:30 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.058682408883346526
[09/26 13:53:37 visual_prompt]: Epoch 51 / 100: avg data time: 4.84e-02, avg batch time: 0.4638, average train loss: 1.0023
[09/26 13:53:38 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1578, average loss: 0.9034
[09/26 13:53:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.50	top5: 96.50	
[09/26 13:53:38 visual_prompt]: Best epoch 51: best metric: 0.685
[09/26 13:53:38 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.05695865504800327
[09/26 13:53:45 visual_prompt]: Epoch 52 / 100: avg data time: 5.48e-02, avg batch time: 0.4683, average train loss: 0.8597
[09/26 13:53:46 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.1582, average loss: 0.9668
[09/26 13:53:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.00	top5: 95.00	
[09/26 13:53:46 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.05522642316338268
[09/26 13:53:52 visual_prompt]: Epoch 53 / 100: avg data time: 4.86e-02, avg batch time: 0.4649, average train loss: 0.7453
[09/26 13:53:54 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1583, average loss: 0.9645
[09/26 13:53:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.50	top5: 97.00	
[09/26 13:53:54 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.05348782368720626
[09/26 13:54:01 visual_prompt]: Epoch 54 / 100: avg data time: 5.52e-02, avg batch time: 0.4696, average train loss: 0.6040
[09/26 13:54:02 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1584, average loss: 0.8640
[09/26 13:54:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.00	top5: 97.50	
[09/26 13:54:02 visual_prompt]: Best epoch 54: best metric: 0.690
[09/26 13:54:02 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.05174497483512506
[09/26 13:54:09 visual_prompt]: Epoch 55 / 100: avg data time: 5.70e-02, avg batch time: 0.4716, average train loss: 0.6191
[09/26 13:54:10 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1583, average loss: 0.9982
[09/26 13:54:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.50	top5: 96.00	
[09/26 13:54:10 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.05
[09/26 13:54:17 visual_prompt]: Epoch 56 / 100: avg data time: 5.99e-02, avg batch time: 0.4729, average train loss: 0.5965
[09/26 13:54:18 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1582, average loss: 1.1043
[09/26 13:54:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.00	top5: 96.00	
[09/26 13:54:18 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.04825502516487497
[09/26 13:54:25 visual_prompt]: Epoch 57 / 100: avg data time: 5.43e-02, avg batch time: 0.4692, average train loss: 0.5400
[09/26 13:54:26 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1587, average loss: 0.8806
[09/26 13:54:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 95.50	
[09/26 13:54:26 visual_prompt]: Best epoch 57: best metric: 0.705
[09/26 13:54:26 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.04651217631279374
[09/26 13:54:33 visual_prompt]: Epoch 58 / 100: avg data time: 6.32e-02, avg batch time: 0.4785, average train loss: 0.5187
[09/26 13:54:34 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1581, average loss: 0.7212
[09/26 13:54:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.00	top5: 96.50	
[09/26 13:54:34 visual_prompt]: Best epoch 58: best metric: 0.770
[09/26 13:54:34 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.04477357683661734
[09/26 13:54:41 visual_prompt]: Epoch 59 / 100: avg data time: 5.36e-02, avg batch time: 0.4675, average train loss: 0.4151
[09/26 13:54:42 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1586, average loss: 0.6937
[09/26 13:54:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 98.00	
[09/26 13:54:42 visual_prompt]: Best epoch 59: best metric: 0.780
[09/26 13:54:42 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.04304134495199674
[09/26 13:54:49 visual_prompt]: Epoch 60 / 100: avg data time: 6.16e-02, avg batch time: 0.4747, average train loss: 0.4015
[09/26 13:54:50 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1582, average loss: 0.7970
[09/26 13:54:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 97.50	
[09/26 13:54:50 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.041317591116653486
[09/26 13:54:57 visual_prompt]: Epoch 61 / 100: avg data time: 6.91e-02, avg batch time: 0.4829, average train loss: 0.3346
[09/26 13:54:59 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1583, average loss: 0.8849
[09/26 13:54:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.50	
[09/26 13:54:59 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.039604415459112044
[09/26 13:55:05 visual_prompt]: Epoch 62 / 100: avg data time: 7.21e-02, avg batch time: 0.4850, average train loss: 0.3730
[09/26 13:55:07 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1582, average loss: 0.8126
[09/26 13:55:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 97.00	
[09/26 13:55:07 visual_prompt]: Best epoch 62: best metric: 0.790
[09/26 13:55:07 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.037903905220016625
[09/26 13:55:13 visual_prompt]: Epoch 63 / 100: avg data time: 5.89e-02, avg batch time: 0.4737, average train loss: 0.3358
[09/26 13:55:15 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1583, average loss: 0.7809
[09/26 13:55:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 13:55:15 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.036218132209150045
[09/26 13:55:21 visual_prompt]: Epoch 64 / 100: avg data time: 5.41e-02, avg batch time: 0.4688, average train loss: 0.3156
[09/26 13:55:23 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1583, average loss: 0.6633
[09/26 13:55:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 97.50	
[09/26 13:55:23 visual_prompt]: Best epoch 64: best metric: 0.795
[09/26 13:55:23 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.03454915028125263
[09/26 13:55:29 visual_prompt]: Epoch 65 / 100: avg data time: 5.68e-02, avg batch time: 0.4717, average train loss: 0.3107
[09/26 13:55:31 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1574, average loss: 0.6904
[09/26 13:55:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.00	top5: 99.00	
[09/26 13:55:31 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.03289899283371657
[09/26 13:55:38 visual_prompt]: Epoch 66 / 100: avg data time: 6.54e-02, avg batch time: 0.4785, average train loss: 0.3298
[09/26 13:55:39 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1584, average loss: 0.7615
[09/26 13:55:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 97.50	
[09/26 13:55:39 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0312696703292044
[09/26 13:55:46 visual_prompt]: Epoch 67 / 100: avg data time: 6.16e-02, avg batch time: 0.4751, average train loss: 0.2572
[09/26 13:55:47 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1581, average loss: 0.8149
[09/26 13:55:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 98.00	
[09/26 13:55:47 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.02966316784621
[09/26 13:55:54 visual_prompt]: Epoch 68 / 100: avg data time: 6.01e-02, avg batch time: 0.4741, average train loss: 0.3152
[09/26 13:55:55 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1585, average loss: 0.6733
[09/26 13:55:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 97.50	
[09/26 13:55:55 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.028081442660546126
[09/26 13:56:02 visual_prompt]: Epoch 69 / 100: avg data time: 6.62e-02, avg batch time: 0.4787, average train loss: 0.2159
[09/26 13:56:03 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1581, average loss: 0.6116
[09/26 13:56:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 98.00	
[09/26 13:56:03 visual_prompt]: Best epoch 69: best metric: 0.820
[09/26 13:56:03 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.026526421860705474
[09/26 13:56:10 visual_prompt]: Epoch 70 / 100: avg data time: 6.62e-02, avg batch time: 0.4787, average train loss: 0.1449
[09/26 13:56:11 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1580, average loss: 0.7199
[09/26 13:56:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.50	top5: 96.50	
[09/26 13:56:11 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.025000000000000012
[09/26 13:56:18 visual_prompt]: Epoch 71 / 100: avg data time: 5.40e-02, avg batch time: 0.4677, average train loss: 0.1796
[09/26 13:56:19 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1582, average loss: 0.6418
[09/26 13:56:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 97.00	
[09/26 13:56:19 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.02350403678833976
[09/26 13:56:26 visual_prompt]: Epoch 72 / 100: avg data time: 5.82e-02, avg batch time: 0.4723, average train loss: 0.1400
[09/26 13:56:27 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1585, average loss: 0.7738
[09/26 13:56:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 96.50	
[09/26 13:56:27 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.022040354826462667
[09/26 13:56:34 visual_prompt]: Epoch 73 / 100: avg data time: 5.62e-02, avg batch time: 0.4694, average train loss: 0.1562
[09/26 13:56:35 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1581, average loss: 0.7386
[09/26 13:56:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 96.50	
[09/26 13:56:35 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.02061073738537635
[09/26 13:56:42 visual_prompt]: Epoch 74 / 100: avg data time: 5.26e-02, avg batch time: 0.4671, average train loss: 0.1735
[09/26 13:56:43 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1583, average loss: 0.7742
[09/26 13:56:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 95.50	
[09/26 13:56:43 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.019216926233717086
[09/26 13:56:50 visual_prompt]: Epoch 75 / 100: avg data time: 4.99e-02, avg batch time: 0.4631, average train loss: 0.1339
[09/26 13:56:51 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1587, average loss: 0.7685
[09/26 13:56:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 96.50	
[09/26 13:56:51 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.017860619515673033
[09/26 13:56:58 visual_prompt]: Epoch 76 / 100: avg data time: 4.61e-02, avg batch time: 0.4611, average train loss: 0.0797
[09/26 13:56:59 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1579, average loss: 0.7300
[09/26 13:56:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 96.50	
[09/26 13:56:59 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.016543469682057107
[09/26 13:57:06 visual_prompt]: Epoch 77 / 100: avg data time: 5.45e-02, avg batch time: 0.4682, average train loss: 0.0643
[09/26 13:57:07 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1580, average loss: 0.7700
[09/26 13:57:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 95.00	
[09/26 13:57:07 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.015267081477050132
[09/26 13:57:14 visual_prompt]: Epoch 78 / 100: avg data time: 4.91e-02, avg batch time: 0.4633, average train loss: 0.0788
[09/26 13:57:15 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1585, average loss: 0.7132
[09/26 13:57:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 96.00	
[09/26 13:57:15 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.014033009983067453
[09/26 13:57:22 visual_prompt]: Epoch 79 / 100: avg data time: 5.94e-02, avg batch time: 0.4722, average train loss: 0.0428
[09/26 13:57:23 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1584, average loss: 0.7509
[09/26 13:57:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.00	
[09/26 13:57:23 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.012842758726130284
[09/26 13:57:30 visual_prompt]: Epoch 80 / 100: avg data time: 6.09e-02, avg batch time: 0.4748, average train loss: 0.0431
[09/26 13:57:31 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1578, average loss: 0.7378
[09/26 13:57:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 96.50	
[09/26 13:57:31 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.011697777844051106
[09/26 13:57:38 visual_prompt]: Epoch 81 / 100: avg data time: 5.27e-02, avg batch time: 0.4695, average train loss: 0.0287
[09/26 13:57:39 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1585, average loss: 0.8728
[09/26 13:57:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 94.50	
[09/26 13:57:39 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.010599462319663905
[09/26 13:57:46 visual_prompt]: Epoch 82 / 100: avg data time: 6.64e-02, avg batch time: 0.4796, average train loss: 0.0270
[09/26 13:57:48 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1581, average loss: 0.7398
[09/26 13:57:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 96.00	
[09/26 13:57:48 visual_prompt]: Best epoch 82: best metric: 0.835
[09/26 13:57:48 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.009549150281252633
[09/26 13:57:54 visual_prompt]: Epoch 83 / 100: avg data time: 5.35e-02, avg batch time: 0.4672, average train loss: 0.0250
[09/26 13:57:56 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1583, average loss: 0.8407
[09/26 13:57:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 96.00	
[09/26 13:57:56 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.008548121372247919
[09/26 13:58:02 visual_prompt]: Epoch 84 / 100: avg data time: 5.74e-02, avg batch time: 0.4709, average train loss: 0.0203
[09/26 13:58:04 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1586, average loss: 0.8088
[09/26 13:58:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 96.00	
[09/26 13:58:04 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.007597595192178702
[09/26 13:58:10 visual_prompt]: Epoch 85 / 100: avg data time: 5.29e-02, avg batch time: 0.4675, average train loss: 0.0184
[09/26 13:58:12 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.1580, average loss: 0.8047
[09/26 13:58:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 96.00	
[09/26 13:58:12 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.006698729810778065
[09/26 13:58:18 visual_prompt]: Epoch 86 / 100: avg data time: 5.66e-02, avg batch time: 0.4701, average train loss: 0.0178
[09/26 13:58:20 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1583, average loss: 0.8110
[09/26 13:58:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 96.00	
[09/26 13:58:20 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.005852620357053651
[09/26 13:58:26 visual_prompt]: Epoch 87 / 100: avg data time: 6.12e-02, avg batch time: 0.4748, average train loss: 0.0171
[09/26 13:58:28 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1582, average loss: 0.8030
[09/26 13:58:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 96.00	
[09/26 13:58:28 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00506029768504166
[09/26 13:58:34 visual_prompt]: Epoch 88 / 100: avg data time: 6.04e-02, avg batch time: 0.4725, average train loss: 0.0170
[09/26 13:58:36 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1582, average loss: 0.8030
[09/26 13:58:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 96.00	
[09/26 13:58:36 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.004322727117869951
[09/26 13:58:42 visual_prompt]: Epoch 89 / 100: avg data time: 6.38e-02, avg batch time: 0.4770, average train loss: 0.0167
[09/26 13:58:44 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1586, average loss: 0.7936
[09/26 13:58:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 96.00	
[09/26 13:58:44 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0036408072716606343
[09/26 13:58:50 visual_prompt]: Epoch 90 / 100: avg data time: 6.35e-02, avg batch time: 0.4767, average train loss: 0.0168
[09/26 13:58:52 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1584, average loss: 0.7903
[09/26 13:58:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 96.00	
[09/26 13:58:52 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0030153689607045845
[09/26 13:58:59 visual_prompt]: Epoch 91 / 100: avg data time: 6.71e-02, avg batch time: 0.4810, average train loss: 0.0163
[09/26 13:59:00 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1584, average loss: 0.7926
[09/26 13:59:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 96.00	
[09/26 13:59:00 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0024471741852423235
[09/26 13:59:07 visual_prompt]: Epoch 92 / 100: avg data time: 6.27e-02, avg batch time: 0.4752, average train loss: 0.0164
[09/26 13:59:08 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1585, average loss: 0.7921
[09/26 13:59:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 96.00	
[09/26 13:59:08 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0019369152030840554
[09/26 13:59:15 visual_prompt]: Epoch 93 / 100: avg data time: 6.42e-02, avg batch time: 0.4785, average train loss: 0.0165
[09/26 13:59:16 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1582, average loss: 0.7881
[09/26 13:59:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.00	top5: 96.00	
[09/26 13:59:16 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0014852136862001765
[09/26 13:59:23 visual_prompt]: Epoch 94 / 100: avg data time: 5.03e-02, avg batch time: 0.4632, average train loss: 0.0161
[09/26 13:59:24 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1582, average loss: 0.7905
[09/26 13:59:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 96.00	
[09/26 13:59:24 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0010926199633097156
[09/26 13:59:31 visual_prompt]: Epoch 95 / 100: avg data time: 6.32e-02, avg batch time: 0.4762, average train loss: 0.0163
[09/26 13:59:32 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1586, average loss: 0.7924
[09/26 13:59:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 96.00	
[09/26 13:59:32 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0007596123493895991
[09/26 13:59:39 visual_prompt]: Epoch 96 / 100: avg data time: 6.56e-02, avg batch time: 0.4783, average train loss: 0.0163
[09/26 13:59:40 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1583, average loss: 0.7930
[09/26 13:59:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 96.00	
[09/26 13:59:40 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00048659656292148194
[09/26 13:59:47 visual_prompt]: Epoch 97 / 100: avg data time: 6.45e-02, avg batch time: 0.4770, average train loss: 0.0163
[09/26 13:59:49 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1581, average loss: 0.7932
[09/26 13:59:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 96.00	
[09/26 13:59:49 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0002739052315863355
[09/26 13:59:55 visual_prompt]: Epoch 98 / 100: avg data time: 6.68e-02, avg batch time: 0.4794, average train loss: 0.0163
[09/26 13:59:57 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1583, average loss: 0.7932
[09/26 13:59:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 96.00	
[09/26 13:59:57 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.00012179748700879012
[09/26 14:00:03 visual_prompt]: Epoch 99 / 100: avg data time: 5.41e-02, avg batch time: 0.4672, average train loss: 0.0162
[09/26 14:00:05 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1583, average loss: 0.7932
[09/26 14:00:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 96.00	
[09/26 14:00:05 visual_prompt]: Training 100 / 100 epoch, with learning rate 3.0458649045211895e-05
[09/26 14:00:11 visual_prompt]: Epoch 100 / 100: avg data time: 6.44e-02, avg batch time: 0.4777, average train loss: 0.0162
[09/26 14:00:13 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1581, average loss: 0.7932
[09/26 14:00:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 82.50	top5: 96.00	
[09/26 14:00:13 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 14:00:13 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 14:00:13 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 14:00:13 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 14:00:13 visual_prompt]: Training with config:
[09/26 14:00:13 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.1_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 14:00:13 visual_prompt]: Loading training data...
[09/26 14:00:13 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 14:00:14 visual_prompt]: Number of images: 800
[09/26 14:00:14 visual_prompt]: Number of classes: 10 / 10
[09/26 14:00:14 visual_prompt]: Loading validation data...
[09/26 14:00:14 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 14:00:14 visual_prompt]: Number of images: 200
[09/26 14:00:14 visual_prompt]: Number of classes: 10 / 10
[09/26 14:00:14 visual_prompt]: Constructing models...
[09/26 14:00:16 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 14:00:16 visual_prompt]: tuned percent:0.543
[09/26 14:00:16 visual_prompt]: Device used for model: 0
[09/26 14:00:16 visual_prompt]: Setting up Evaluator...
[09/26 14:00:16 visual_prompt]: Setting up Trainer...
[09/26 14:00:16 visual_prompt]: 	Setting up the optimizer...
[09/26 14:00:16 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 14:00:23 visual_prompt]: Epoch 1 / 100: avg data time: 7.03e-02, avg batch time: 0.4886, average train loss: 2.6842
[09/26 14:00:25 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1581, average loss: 2.6214
[09/26 14:00:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 14:00:25 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 14:00:25 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[09/26 14:00:31 visual_prompt]: Epoch 2 / 100: avg data time: 6.08e-02, avg batch time: 0.4740, average train loss: 2.4056
[09/26 14:00:33 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1580, average loss: 2.2251
[09/26 14:00:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 14:00:33 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[09/26 14:00:39 visual_prompt]: Epoch 3 / 100: avg data time: 6.02e-02, avg batch time: 0.4719, average train loss: 2.2605
[09/26 14:00:41 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1582, average loss: 2.2282
[09/26 14:00:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 14:00:41 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[09/26 14:00:47 visual_prompt]: Epoch 4 / 100: avg data time: 5.19e-02, avg batch time: 0.4669, average train loss: 2.2456
[09/26 14:00:49 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1586, average loss: 2.2097
[09/26 14:00:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 14:00:49 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[09/26 14:00:55 visual_prompt]: Epoch 5 / 100: avg data time: 5.75e-02, avg batch time: 0.4722, average train loss: 2.2540
[09/26 14:00:57 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1577, average loss: 2.2082
[09/26 14:00:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 14:00:57 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[09/26 14:01:03 visual_prompt]: Epoch 6 / 100: avg data time: 6.27e-02, avg batch time: 0.4756, average train loss: 2.2499
[09/26 14:01:05 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1582, average loss: 2.2099
[09/26 14:01:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.50	
[09/26 14:01:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[09/26 14:01:11 visual_prompt]: Epoch 7 / 100: avg data time: 6.52e-02, avg batch time: 0.4771, average train loss: 2.2519
[09/26 14:01:13 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1581, average loss: 2.2721
[09/26 14:01:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 9.00	top5: 63.50	
[09/26 14:01:13 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[09/26 14:01:19 visual_prompt]: Epoch 8 / 100: avg data time: 5.95e-02, avg batch time: 0.4720, average train loss: 2.2641
[09/26 14:01:21 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1586, average loss: 2.2383
[09/26 14:01:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 14:01:21 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[09/26 14:01:27 visual_prompt]: Epoch 9 / 100: avg data time: 5.85e-02, avg batch time: 0.4711, average train loss: 2.2494
[09/26 14:01:29 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1578, average loss: 2.2208
[09/26 14:01:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 14:01:29 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[09/26 14:01:35 visual_prompt]: Epoch 10 / 100: avg data time: 6.69e-02, avg batch time: 0.4789, average train loss: 2.2526
[09/26 14:01:37 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1585, average loss: 2.2109
[09/26 14:01:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 14:01:37 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[09/26 14:01:44 visual_prompt]: Epoch 11 / 100: avg data time: 5.41e-02, avg batch time: 0.4681, average train loss: 2.2439
[09/26 14:01:45 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1584, average loss: 2.2302
[09/26 14:01:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 17.00	top5: 65.50	
[09/26 14:01:45 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[09/26 14:01:52 visual_prompt]: Epoch 12 / 100: avg data time: 6.26e-02, avg batch time: 0.4754, average train loss: 2.2343
[09/26 14:01:53 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1587, average loss: 2.2482
[09/26 14:01:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 19.50	top5: 59.00	
[09/26 14:01:53 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[09/26 14:02:00 visual_prompt]: Epoch 13 / 100: avg data time: 5.19e-02, avg batch time: 0.4665, average train loss: 2.2338
[09/26 14:02:01 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1580, average loss: 2.1926
[09/26 14:02:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 60.50	
[09/26 14:02:01 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[09/26 14:02:08 visual_prompt]: Epoch 14 / 100: avg data time: 5.83e-02, avg batch time: 0.4720, average train loss: 2.1879
[09/26 14:02:09 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1585, average loss: 2.1482
[09/26 14:02:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.00	top5: 67.50	
[09/26 14:02:09 visual_prompt]: Best epoch 14: best metric: 0.250
[09/26 14:02:09 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[09/26 14:02:16 visual_prompt]: Epoch 15 / 100: avg data time: 6.33e-02, avg batch time: 0.4759, average train loss: 2.1674
[09/26 14:02:17 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1588, average loss: 2.1382
[09/26 14:02:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 27.50	top5: 64.50	
[09/26 14:02:17 visual_prompt]: Best epoch 15: best metric: 0.275
[09/26 14:02:17 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[09/26 14:02:24 visual_prompt]: Epoch 16 / 100: avg data time: 5.70e-02, avg batch time: 0.4712, average train loss: 2.1591
[09/26 14:02:25 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1584, average loss: 2.2053
[09/26 14:02:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 19.00	top5: 64.50	
[09/26 14:02:25 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[09/26 14:02:32 visual_prompt]: Epoch 17 / 100: avg data time: 6.32e-02, avg batch time: 0.4765, average train loss: 2.1431
[09/26 14:02:33 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1583, average loss: 2.0990
[09/26 14:02:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 30.00	top5: 69.00	
[09/26 14:02:33 visual_prompt]: Best epoch 17: best metric: 0.300
[09/26 14:02:33 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[09/26 14:02:40 visual_prompt]: Epoch 18 / 100: avg data time: 6.57e-02, avg batch time: 0.4779, average train loss: 2.0705
[09/26 14:02:42 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1582, average loss: 2.0576
[09/26 14:02:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 26.50	top5: 69.50	
[09/26 14:02:42 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[09/26 14:02:48 visual_prompt]: Epoch 19 / 100: avg data time: 5.89e-02, avg batch time: 0.4714, average train loss: 1.9729
[09/26 14:02:50 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.1583, average loss: 2.1266
[09/26 14:02:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 64.50	
[09/26 14:02:50 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[09/26 14:02:56 visual_prompt]: Epoch 20 / 100: avg data time: 5.72e-02, avg batch time: 0.4706, average train loss: 1.9346
[09/26 14:02:58 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1580, average loss: 1.8742
[09/26 14:02:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 33.50	top5: 82.00	
[09/26 14:02:58 visual_prompt]: Best epoch 20: best metric: 0.335
[09/26 14:02:58 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[09/26 14:03:04 visual_prompt]: Epoch 21 / 100: avg data time: 6.14e-02, avg batch time: 0.4738, average train loss: 1.7693
[09/26 14:03:06 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1585, average loss: 1.6294
[09/26 14:03:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 45.50	top5: 89.00	
[09/26 14:03:06 visual_prompt]: Best epoch 21: best metric: 0.455
[09/26 14:03:06 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[09/26 14:03:12 visual_prompt]: Epoch 22 / 100: avg data time: 6.34e-02, avg batch time: 0.4764, average train loss: 1.6062
[09/26 14:03:14 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1586, average loss: 1.5417
[09/26 14:03:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 42.50	top5: 91.00	
[09/26 14:03:14 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[09/26 14:03:20 visual_prompt]: Epoch 23 / 100: avg data time: 6.32e-02, avg batch time: 0.4770, average train loss: 1.4335
[09/26 14:03:22 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1583, average loss: 1.3447
[09/26 14:03:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 50.50	top5: 96.00	
[09/26 14:03:22 visual_prompt]: Best epoch 23: best metric: 0.505
[09/26 14:03:22 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[09/26 14:03:29 visual_prompt]: Epoch 24 / 100: avg data time: 6.34e-02, avg batch time: 0.4769, average train loss: 1.3767
[09/26 14:03:30 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1581, average loss: 1.2690
[09/26 14:03:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 52.00	top5: 94.50	
[09/26 14:03:30 visual_prompt]: Best epoch 24: best metric: 0.520
[09/26 14:03:30 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[09/26 14:03:37 visual_prompt]: Epoch 25 / 100: avg data time: 6.50e-02, avg batch time: 0.4783, average train loss: 1.2156
[09/26 14:03:38 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1584, average loss: 1.5671
[09/26 14:03:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 44.00	top5: 92.00	
[09/26 14:03:38 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[09/26 14:03:45 visual_prompt]: Epoch 26 / 100: avg data time: 5.21e-02, avg batch time: 0.4658, average train loss: 1.1148
[09/26 14:03:46 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1585, average loss: 1.1509
[09/26 14:03:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 59.00	top5: 96.00	
[09/26 14:03:46 visual_prompt]: Best epoch 26: best metric: 0.590
[09/26 14:03:46 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[09/26 14:03:53 visual_prompt]: Epoch 27 / 100: avg data time: 6.13e-02, avg batch time: 0.4743, average train loss: 1.1121
[09/26 14:03:54 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1578, average loss: 0.9536
[09/26 14:03:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.50	top5: 96.50	
[09/26 14:03:54 visual_prompt]: Best epoch 27: best metric: 0.655
[09/26 14:03:54 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[09/26 14:04:01 visual_prompt]: Epoch 28 / 100: avg data time: 6.08e-02, avg batch time: 0.4737, average train loss: 0.9356
[09/26 14:04:02 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1578, average loss: 1.0114
[09/26 14:04:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 63.00	top5: 98.00	
[09/26 14:04:02 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[09/26 14:04:09 visual_prompt]: Epoch 29 / 100: avg data time: 5.25e-02, avg batch time: 0.4665, average train loss: 1.0691
[09/26 14:04:10 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1585, average loss: 1.0209
[09/26 14:04:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 64.00	top5: 97.00	
[09/26 14:04:10 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[09/26 14:04:17 visual_prompt]: Epoch 30 / 100: avg data time: 5.72e-02, avg batch time: 0.4705, average train loss: 0.8265
[09/26 14:04:18 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1584, average loss: 0.9234
[09/26 14:04:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 64.00	top5: 97.00	
[09/26 14:04:18 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[09/26 14:04:25 visual_prompt]: Epoch 31 / 100: avg data time: 5.92e-02, avg batch time: 0.4728, average train loss: 0.7177
[09/26 14:04:26 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1585, average loss: 0.9665
[09/26 14:04:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.00	top5: 97.00	
[09/26 14:04:26 visual_prompt]: Best epoch 31: best metric: 0.670
[09/26 14:04:26 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[09/26 14:04:33 visual_prompt]: Epoch 32 / 100: avg data time: 6.49e-02, avg batch time: 0.4784, average train loss: 0.6779
[09/26 14:04:34 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1582, average loss: 0.8372
[09/26 14:04:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 97.50	
[09/26 14:04:34 visual_prompt]: Best epoch 32: best metric: 0.705
[09/26 14:04:34 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[09/26 14:04:41 visual_prompt]: Epoch 33 / 100: avg data time: 6.52e-02, avg batch time: 0.4782, average train loss: 0.5389
[09/26 14:04:43 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.1581, average loss: 0.9180
[09/26 14:04:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.00	top5: 97.50	
[09/26 14:04:43 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[09/26 14:04:49 visual_prompt]: Epoch 34 / 100: avg data time: 5.92e-02, avg batch time: 0.4729, average train loss: 0.5912
[09/26 14:04:51 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1582, average loss: 0.9404
[09/26 14:04:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.00	top5: 96.50	
[09/26 14:04:51 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[09/26 14:04:57 visual_prompt]: Epoch 35 / 100: avg data time: 5.99e-02, avg batch time: 0.4738, average train loss: 0.5143
[09/26 14:04:59 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1582, average loss: 1.0356
[09/26 14:04:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.50	top5: 97.00	
[09/26 14:04:59 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[09/26 14:05:05 visual_prompt]: Epoch 36 / 100: avg data time: 6.11e-02, avg batch time: 0.4739, average train loss: 0.4471
[09/26 14:05:07 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1587, average loss: 0.6976
[09/26 14:05:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 96.50	
[09/26 14:05:07 visual_prompt]: Best epoch 36: best metric: 0.780
[09/26 14:05:07 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[09/26 14:05:13 visual_prompt]: Epoch 37 / 100: avg data time: 5.86e-02, avg batch time: 0.4721, average train loss: 0.3752
[09/26 14:05:15 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1578, average loss: 0.9355
[09/26 14:05:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.50	top5: 98.00	
[09/26 14:05:15 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[09/26 14:05:21 visual_prompt]: Epoch 38 / 100: avg data time: 6.19e-02, avg batch time: 0.4755, average train loss: 0.3542
[09/26 14:05:23 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1585, average loss: 0.9373
[09/26 14:05:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 97.00	
[09/26 14:05:23 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.07795964517353735
[09/26 14:05:29 visual_prompt]: Epoch 39 / 100: avg data time: 5.79e-02, avg batch time: 0.4720, average train loss: 0.3607
[09/26 14:05:31 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1581, average loss: 0.7977
[09/26 14:05:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.50	top5: 97.50	
[09/26 14:05:31 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.07649596321166025
[09/26 14:05:37 visual_prompt]: Epoch 40 / 100: avg data time: 5.35e-02, avg batch time: 0.4683, average train loss: 0.3050
[09/26 14:05:39 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1584, average loss: 0.7673
[09/26 14:05:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.00	top5: 97.50	
[09/26 14:05:39 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.07500000000000001
[09/26 14:05:45 visual_prompt]: Epoch 41 / 100: avg data time: 6.05e-02, avg batch time: 0.4750, average train loss: 0.2312
[09/26 14:05:47 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1580, average loss: 0.7293
[09/26 14:05:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 97.50	
[09/26 14:05:47 visual_prompt]: Best epoch 41: best metric: 0.810
[09/26 14:05:47 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.07347357813929455
[09/26 14:05:53 visual_prompt]: Epoch 42 / 100: avg data time: 6.13e-02, avg batch time: 0.4750, average train loss: 0.2279
[09/26 14:05:55 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1582, average loss: 1.0054
[09/26 14:05:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 97.50	
[09/26 14:05:55 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.07191855733945388
[09/26 14:06:02 visual_prompt]: Epoch 43 / 100: avg data time: 6.49e-02, avg batch time: 0.4778, average train loss: 0.2249
[09/26 14:06:03 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1584, average loss: 0.9965
[09/26 14:06:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 96.00	
[09/26 14:06:03 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.07033683215379002
[09/26 14:06:10 visual_prompt]: Epoch 44 / 100: avg data time: 6.41e-02, avg batch time: 0.4767, average train loss: 0.2333
[09/26 14:06:11 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1583, average loss: 0.9778
[09/26 14:06:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.00	top5: 98.00	
[09/26 14:06:11 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0687303296707956
[09/26 14:06:18 visual_prompt]: Epoch 45 / 100: avg data time: 6.41e-02, avg batch time: 0.4795, average train loss: 0.1903
[09/26 14:06:19 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.1581, average loss: 0.7917
[09/26 14:06:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 98.50	
[09/26 14:06:19 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.06710100716628345
[09/26 14:06:26 visual_prompt]: Epoch 46 / 100: avg data time: 4.79e-02, avg batch time: 0.4624, average train loss: 0.1628
[09/26 14:06:27 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1584, average loss: 0.9333
[09/26 14:06:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 97.00	
[09/26 14:06:27 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.06545084971874737
[09/26 14:06:34 visual_prompt]: Epoch 47 / 100: avg data time: 5.81e-02, avg batch time: 0.4732, average train loss: 0.1235
[09/26 14:06:35 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1584, average loss: 0.9307
[09/26 14:06:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 97.00	
[09/26 14:06:35 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.06378186779084996
[09/26 14:06:42 visual_prompt]: Epoch 48 / 100: avg data time: 6.11e-02, avg batch time: 0.4742, average train loss: 0.0935
[09/26 14:06:43 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1580, average loss: 1.0496
[09/26 14:06:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.50	top5: 96.50	
[09/26 14:06:43 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.06209609477998338
[09/26 14:06:50 visual_prompt]: Epoch 49 / 100: avg data time: 6.79e-02, avg batch time: 0.4809, average train loss: 0.0974
[09/26 14:06:52 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1586, average loss: 1.1475
[09/26 14:06:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 96.50	
[09/26 14:06:52 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.06039558454088796
[09/26 14:06:58 visual_prompt]: Epoch 50 / 100: avg data time: 5.96e-02, avg batch time: 0.4726, average train loss: 0.0659
[09/26 14:07:00 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1585, average loss: 1.1418
[09/26 14:07:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.50	top5: 96.00	
[09/26 14:07:00 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.058682408883346526
[09/26 14:07:06 visual_prompt]: Epoch 51 / 100: avg data time: 5.49e-02, avg batch time: 0.4679, average train loss: 0.0566
[09/26 14:07:08 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1584, average loss: 1.3160
[09/26 14:07:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.50	top5: 97.00	
[09/26 14:07:08 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.05695865504800327
[09/26 14:07:14 visual_prompt]: Epoch 52 / 100: avg data time: 5.91e-02, avg batch time: 0.4725, average train loss: 0.1130
[09/26 14:07:16 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1585, average loss: 1.0302
[09/26 14:07:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 96.50	
[09/26 14:07:16 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.05522642316338268
[09/26 14:07:22 visual_prompt]: Epoch 53 / 100: avg data time: 6.17e-02, avg batch time: 0.4747, average train loss: 0.0704
[09/26 14:07:24 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1585, average loss: 1.1116
[09/26 14:07:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.50	top5: 96.00	
[09/26 14:07:24 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.05348782368720626
[09/26 14:07:30 visual_prompt]: Epoch 54 / 100: avg data time: 5.44e-02, avg batch time: 0.4675, average train loss: 0.0855
[09/26 14:07:32 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1581, average loss: 1.0037
[09/26 14:07:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 98.00	
[09/26 14:07:32 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.05174497483512506
[09/26 14:07:38 visual_prompt]: Epoch 55 / 100: avg data time: 5.85e-02, avg batch time: 0.4726, average train loss: 0.0727
[09/26 14:07:40 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1585, average loss: 1.2027
[09/26 14:07:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 98.50	
[09/26 14:07:40 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.05
[09/26 14:07:46 visual_prompt]: Epoch 56 / 100: avg data time: 6.31e-02, avg batch time: 0.4757, average train loss: 0.1399
[09/26 14:07:48 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1585, average loss: 0.9867
[09/26 14:07:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 97.50	
[09/26 14:07:48 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.04825502516487497
[09/26 14:07:54 visual_prompt]: Epoch 57 / 100: avg data time: 5.35e-02, avg batch time: 0.4688, average train loss: 0.0589
[09/26 14:07:56 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1581, average loss: 1.0114
[09/26 14:07:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 97.50	
[09/26 14:07:56 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.04651217631279374
[09/26 14:08:03 visual_prompt]: Epoch 58 / 100: avg data time: 5.92e-02, avg batch time: 0.4731, average train loss: 0.0306
[09/26 14:08:04 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.1584, average loss: 1.0175
[09/26 14:08:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.50	top5: 96.50	
[09/26 14:08:04 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.04477357683661734
[09/26 14:08:11 visual_prompt]: Epoch 59 / 100: avg data time: 6.41e-02, avg batch time: 0.4776, average train loss: 0.0127
[09/26 14:08:12 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1589, average loss: 1.0663
[09/26 14:08:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 97.50	
[09/26 14:08:12 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.04304134495199674
[09/26 14:08:19 visual_prompt]: Epoch 60 / 100: avg data time: 7.26e-02, avg batch time: 0.4873, average train loss: 0.0090
[09/26 14:08:21 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1583, average loss: 1.0442
[09/26 14:08:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 97.50	
[09/26 14:08:21 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.041317591116653486
[09/26 14:08:27 visual_prompt]: Epoch 61 / 100: avg data time: 6.14e-02, avg batch time: 0.4768, average train loss: 0.0053
[09/26 14:08:29 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1581, average loss: 1.0791
[09/26 14:08:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.50	top5: 97.50	
[09/26 14:08:29 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.039604415459112044
[09/26 14:08:35 visual_prompt]: Epoch 62 / 100: avg data time: 6.21e-02, avg batch time: 0.4750, average train loss: 0.0033
[09/26 14:08:37 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1585, average loss: 1.1000
[09/26 14:08:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 98.00	
[09/26 14:08:37 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.037903905220016625
[09/26 14:08:43 visual_prompt]: Epoch 63 / 100: avg data time: 5.10e-02, avg batch time: 0.4645, average train loss: 0.0025
[09/26 14:08:45 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1586, average loss: 1.1072
[09/26 14:08:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.50	top5: 98.00	
[09/26 14:08:45 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.036218132209150045
[09/26 14:08:51 visual_prompt]: Epoch 64 / 100: avg data time: 6.14e-02, avg batch time: 0.4744, average train loss: 0.0026
[09/26 14:08:53 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1585, average loss: 1.1253
[09/26 14:08:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.50	top5: 97.50	
[09/26 14:08:53 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.03454915028125263
[09/26 14:08:59 visual_prompt]: Epoch 65 / 100: avg data time: 5.85e-02, avg batch time: 0.4715, average train loss: 0.0020
[09/26 14:09:01 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1588, average loss: 1.1260
[09/26 14:09:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 97.50	
[09/26 14:09:01 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.03289899283371657
[09/26 14:09:08 visual_prompt]: Epoch 66 / 100: avg data time: 5.80e-02, avg batch time: 0.4734, average train loss: 0.0015
[09/26 14:09:09 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1586, average loss: 1.1138
[09/26 14:09:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.00	
[09/26 14:09:09 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0312696703292044
[09/26 14:09:16 visual_prompt]: Epoch 67 / 100: avg data time: 6.04e-02, avg batch time: 0.4740, average train loss: 0.0014
[09/26 14:09:17 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1583, average loss: 1.0967
[09/26 14:09:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.00	
[09/26 14:09:17 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.02966316784621
[09/26 14:09:24 visual_prompt]: Epoch 68 / 100: avg data time: 6.20e-02, avg batch time: 0.4756, average train loss: 0.0018
[09/26 14:09:25 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1583, average loss: 1.0992
[09/26 14:09:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.00	
[09/26 14:09:25 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.028081442660546126
[09/26 14:09:32 visual_prompt]: Epoch 69 / 100: avg data time: 5.91e-02, avg batch time: 0.4726, average train loss: 0.0018
[09/26 14:09:33 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1584, average loss: 1.1097
[09/26 14:09:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.50	
[09/26 14:09:33 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.026526421860705474
[09/26 14:09:40 visual_prompt]: Epoch 70 / 100: avg data time: 5.92e-02, avg batch time: 0.4739, average train loss: 0.0015
[09/26 14:09:41 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1584, average loss: 1.1187
[09/26 14:09:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.50	
[09/26 14:09:41 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.025000000000000012
[09/26 14:09:48 visual_prompt]: Epoch 71 / 100: avg data time: 5.96e-02, avg batch time: 0.4731, average train loss: 0.0013
[09/26 14:09:49 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1589, average loss: 1.1316
[09/26 14:09:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 97.50	
[09/26 14:09:49 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.02350403678833976
[09/26 14:09:56 visual_prompt]: Epoch 72 / 100: avg data time: 6.14e-02, avg batch time: 0.4746, average train loss: 0.0012
[09/26 14:09:57 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1579, average loss: 1.1315
[09/26 14:09:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 97.50	
[09/26 14:09:57 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.022040354826462667
[09/26 14:10:04 visual_prompt]: Epoch 73 / 100: avg data time: 5.71e-02, avg batch time: 0.4713, average train loss: 0.0012
[09/26 14:10:06 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1581, average loss: 1.1248
[09/26 14:10:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 97.50	
[09/26 14:10:06 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.02061073738537635
[09/26 14:10:12 visual_prompt]: Epoch 74 / 100: avg data time: 5.99e-02, avg batch time: 0.4746, average train loss: 0.0013
[09/26 14:10:14 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1584, average loss: 1.1210
[09/26 14:10:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.00	
[09/26 14:10:14 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.019216926233717086
[09/26 14:10:20 visual_prompt]: Epoch 75 / 100: avg data time: 6.23e-02, avg batch time: 0.4756, average train loss: 0.0013
[09/26 14:10:22 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1587, average loss: 1.1190
[09/26 14:10:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:10:22 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.017860619515673033
[09/26 14:10:28 visual_prompt]: Epoch 76 / 100: avg data time: 6.87e-02, avg batch time: 0.4841, average train loss: 0.0010
[09/26 14:10:30 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1584, average loss: 1.1177
[09/26 14:10:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 97.50	
[09/26 14:10:30 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.016543469682057107
[09/26 14:10:37 visual_prompt]: Epoch 77 / 100: avg data time: 6.49e-02, avg batch time: 0.4780, average train loss: 0.0010
[09/26 14:10:38 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1588, average loss: 1.1156
[09/26 14:10:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:10:38 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.015267081477050132
[09/26 14:10:45 visual_prompt]: Epoch 78 / 100: avg data time: 6.43e-02, avg batch time: 0.4776, average train loss: 0.0010
[09/26 14:10:46 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1586, average loss: 1.1140
[09/26 14:10:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:10:46 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.014033009983067453
[09/26 14:10:53 visual_prompt]: Epoch 79 / 100: avg data time: 5.98e-02, avg batch time: 0.4736, average train loss: 0.0011
[09/26 14:10:54 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1586, average loss: 1.1118
[09/26 14:10:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:10:54 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.012842758726130284
[09/26 14:11:01 visual_prompt]: Epoch 80 / 100: avg data time: 6.33e-02, avg batch time: 0.4764, average train loss: 0.0010
[09/26 14:11:02 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1586, average loss: 1.1121
[09/26 14:11:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:11:02 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.011697777844051106
[09/26 14:11:09 visual_prompt]: Epoch 81 / 100: avg data time: 6.00e-02, avg batch time: 0.4732, average train loss: 0.0010
[09/26 14:11:10 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1588, average loss: 1.1118
[09/26 14:11:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:11:10 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.010599462319663905
[09/26 14:11:17 visual_prompt]: Epoch 82 / 100: avg data time: 5.16e-02, avg batch time: 0.4679, average train loss: 0.0010
[09/26 14:11:18 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1585, average loss: 1.1133
[09/26 14:11:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 97.50	
[09/26 14:11:18 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.009549150281252633
[09/26 14:11:25 visual_prompt]: Epoch 83 / 100: avg data time: 4.87e-02, avg batch time: 0.4626, average train loss: 0.0011
[09/26 14:11:26 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1584, average loss: 1.1126
[09/26 14:11:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 97.50	
[09/26 14:11:26 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.008548121372247919
[09/26 14:11:33 visual_prompt]: Epoch 84 / 100: avg data time: 4.81e-02, avg batch time: 0.4621, average train loss: 0.0010
[09/26 14:11:34 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1583, average loss: 1.1137
[09/26 14:11:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 97.50	
[09/26 14:11:34 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.007597595192178702
[09/26 14:11:41 visual_prompt]: Epoch 85 / 100: avg data time: 5.87e-02, avg batch time: 0.4718, average train loss: 0.0013
[09/26 14:11:42 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1584, average loss: 1.1098
[09/26 14:11:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:11:42 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.006698729810778065
[09/26 14:11:49 visual_prompt]: Epoch 86 / 100: avg data time: 5.98e-02, avg batch time: 0.4729, average train loss: 0.0009
[09/26 14:11:50 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1584, average loss: 1.1069
[09/26 14:11:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:11:50 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.005852620357053651
[09/26 14:11:57 visual_prompt]: Epoch 87 / 100: avg data time: 6.42e-02, avg batch time: 0.4784, average train loss: 0.0009
[09/26 14:11:59 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1587, average loss: 1.1066
[09/26 14:11:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:11:59 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00506029768504166
[09/26 14:12:05 visual_prompt]: Epoch 88 / 100: avg data time: 6.73e-02, avg batch time: 0.4815, average train loss: 0.0010
[09/26 14:12:07 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1586, average loss: 1.1076
[09/26 14:12:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:12:07 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.004322727117869951
[09/26 14:12:13 visual_prompt]: Epoch 89 / 100: avg data time: 5.87e-02, avg batch time: 0.4716, average train loss: 0.0009
[09/26 14:12:15 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1582, average loss: 1.1076
[09/26 14:12:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:12:15 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0036408072716606343
[09/26 14:12:21 visual_prompt]: Epoch 90 / 100: avg data time: 5.13e-02, avg batch time: 0.4656, average train loss: 0.0012
[09/26 14:12:23 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1585, average loss: 1.1083
[09/26 14:12:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:12:23 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0030153689607045845
[09/26 14:12:29 visual_prompt]: Epoch 91 / 100: avg data time: 5.39e-02, avg batch time: 0.4679, average train loss: 0.0010
[09/26 14:12:31 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1579, average loss: 1.1085
[09/26 14:12:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:12:31 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0024471741852423235
[09/26 14:12:37 visual_prompt]: Epoch 92 / 100: avg data time: 6.26e-02, avg batch time: 0.4764, average train loss: 0.0010
[09/26 14:12:39 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1586, average loss: 1.1084
[09/26 14:12:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:12:39 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0019369152030840554
[09/26 14:12:45 visual_prompt]: Epoch 93 / 100: avg data time: 5.99e-02, avg batch time: 0.4738, average train loss: 0.0009
[09/26 14:12:47 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1586, average loss: 1.1082
[09/26 14:12:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:12:47 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0014852136862001765
[09/26 14:12:53 visual_prompt]: Epoch 94 / 100: avg data time: 6.24e-02, avg batch time: 0.4752, average train loss: 0.0010
[09/26 14:12:55 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1584, average loss: 1.1080
[09/26 14:12:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:12:55 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0010926199633097156
[09/26 14:13:02 visual_prompt]: Epoch 95 / 100: avg data time: 6.57e-02, avg batch time: 0.4797, average train loss: 0.0010
[09/26 14:13:03 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1583, average loss: 1.1079
[09/26 14:13:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:13:03 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0007596123493895991
[09/26 14:13:10 visual_prompt]: Epoch 96 / 100: avg data time: 6.31e-02, avg batch time: 0.4769, average train loss: 0.0009
[09/26 14:13:11 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1581, average loss: 1.1079
[09/26 14:13:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:13:11 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00048659656292148194
[09/26 14:13:18 visual_prompt]: Epoch 97 / 100: avg data time: 5.95e-02, avg batch time: 0.4744, average train loss: 0.0009
[09/26 14:13:19 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1582, average loss: 1.1080
[09/26 14:13:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:13:19 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0002739052315863355
[09/26 14:13:26 visual_prompt]: Epoch 98 / 100: avg data time: 6.32e-02, avg batch time: 0.4778, average train loss: 0.0010
[09/26 14:13:27 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1592, average loss: 1.1080
[09/26 14:13:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:13:27 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.00012179748700879012
[09/26 14:13:34 visual_prompt]: Epoch 99 / 100: avg data time: 5.93e-02, avg batch time: 0.4729, average train loss: 0.0009
[09/26 14:13:35 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1586, average loss: 1.1080
[09/26 14:13:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:13:35 visual_prompt]: Training 100 / 100 epoch, with learning rate 3.0458649045211895e-05
[09/26 14:13:42 visual_prompt]: Epoch 100 / 100: avg data time: 6.01e-02, avg batch time: 0.4742, average train loss: 0.0011
[09/26 14:13:44 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1584, average loss: 1.1080
[09/26 14:13:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.00	top5: 98.00	
[09/26 14:13:44 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 14:13:44 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 14:13:44 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 14:13:44 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 14:13:44 visual_prompt]: Training with config:
[09/26 14:13:44 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.1_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 14:13:44 visual_prompt]: Loading training data...
[09/26 14:13:44 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 14:13:45 visual_prompt]: Number of images: 800
[09/26 14:13:45 visual_prompt]: Number of classes: 10 / 10
[09/26 14:13:45 visual_prompt]: Loading validation data...
[09/26 14:13:45 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 14:13:45 visual_prompt]: Number of images: 200
[09/26 14:13:45 visual_prompt]: Number of classes: 10 / 10
[09/26 14:13:45 visual_prompt]: Constructing models...
[09/26 14:13:47 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 14:13:47 visual_prompt]: tuned percent:0.543
[09/26 14:13:47 visual_prompt]: Device used for model: 0
[09/26 14:13:47 visual_prompt]: Setting up Evaluator...
[09/26 14:13:47 visual_prompt]: Setting up Trainer...
[09/26 14:13:47 visual_prompt]: 	Setting up the optimizer...
[09/26 14:13:47 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 14:13:54 visual_prompt]: Epoch 1 / 100: avg data time: 5.74e-02, avg batch time: 0.4775, average train loss: 2.6758
[09/26 14:13:55 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.1582, average loss: 2.6214
[09/26 14:13:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 14:13:56 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 14:13:56 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[09/26 14:14:02 visual_prompt]: Epoch 2 / 100: avg data time: 5.20e-02, avg batch time: 0.4652, average train loss: 2.3742
[09/26 14:14:03 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1583, average loss: 2.2226
[09/26 14:14:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 14:14:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[09/26 14:14:10 visual_prompt]: Epoch 3 / 100: avg data time: 6.60e-02, avg batch time: 0.4785, average train loss: 2.2671
[09/26 14:14:12 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1582, average loss: 2.2253
[09/26 14:14:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 14:14:12 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[09/26 14:14:18 visual_prompt]: Epoch 4 / 100: avg data time: 6.05e-02, avg batch time: 0.4735, average train loss: 2.2435
[09/26 14:14:20 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1586, average loss: 2.2101
[09/26 14:14:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 14:14:20 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[09/26 14:14:26 visual_prompt]: Epoch 5 / 100: avg data time: 5.26e-02, avg batch time: 0.4663, average train loss: 2.2423
[09/26 14:14:28 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1582, average loss: 2.2321
[09/26 14:14:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 63.00	
[09/26 14:14:28 visual_prompt]: Best epoch 5: best metric: 0.235
[09/26 14:14:28 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[09/26 14:14:34 visual_prompt]: Epoch 6 / 100: avg data time: 5.28e-02, avg batch time: 0.4681, average train loss: 2.2499
[09/26 14:14:36 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1586, average loss: 2.2104
[09/26 14:14:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 14:14:36 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[09/26 14:14:42 visual_prompt]: Epoch 7 / 100: avg data time: 5.63e-02, avg batch time: 0.4694, average train loss: 2.2393
[09/26 14:14:44 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1583, average loss: 2.2077
[09/26 14:14:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 67.50	
[09/26 14:14:44 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[09/26 14:14:50 visual_prompt]: Epoch 8 / 100: avg data time: 6.26e-02, avg batch time: 0.4767, average train loss: 2.2393
[09/26 14:14:52 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1586, average loss: 2.2097
[09/26 14:14:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 14:14:52 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[09/26 14:14:58 visual_prompt]: Epoch 9 / 100: avg data time: 5.33e-02, avg batch time: 0.4673, average train loss: 2.2420
[09/26 14:15:00 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1580, average loss: 2.2066
[09/26 14:15:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.50	
[09/26 14:15:00 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[09/26 14:15:06 visual_prompt]: Epoch 10 / 100: avg data time: 5.98e-02, avg batch time: 0.4735, average train loss: 2.2469
[09/26 14:15:08 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1586, average loss: 2.2304
[09/26 14:15:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 16.00	top5: 65.50	
[09/26 14:15:08 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[09/26 14:15:14 visual_prompt]: Epoch 11 / 100: avg data time: 5.61e-02, avg batch time: 0.4696, average train loss: 2.2272
[09/26 14:15:16 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1585, average loss: 2.2280
[09/26 14:15:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 14:15:16 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[09/26 14:15:22 visual_prompt]: Epoch 12 / 100: avg data time: 5.96e-02, avg batch time: 0.4726, average train loss: 2.2468
[09/26 14:15:24 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1586, average loss: 2.2016
[09/26 14:15:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.50	top5: 65.00	
[09/26 14:15:24 visual_prompt]: Best epoch 12: best metric: 0.255
[09/26 14:15:24 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[09/26 14:15:31 visual_prompt]: Epoch 13 / 100: avg data time: 6.37e-02, avg batch time: 0.4772, average train loss: 2.2134
[09/26 14:15:32 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.1584, average loss: 2.1853
[09/26 14:15:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.00	
[09/26 14:15:32 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[09/26 14:15:39 visual_prompt]: Epoch 14 / 100: avg data time: 5.38e-02, avg batch time: 0.4689, average train loss: 2.2048
[09/26 14:15:40 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1582, average loss: 2.1599
[09/26 14:15:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 64.00	
[09/26 14:15:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[09/26 14:15:47 visual_prompt]: Epoch 15 / 100: avg data time: 6.02e-02, avg batch time: 0.4733, average train loss: 2.1615
[09/26 14:15:48 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.1581, average loss: 2.1666
[09/26 14:15:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 20.50	top5: 66.00	
[09/26 14:15:48 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[09/26 14:15:55 visual_prompt]: Epoch 16 / 100: avg data time: 4.86e-02, avg batch time: 0.4629, average train loss: 2.1217
[09/26 14:15:56 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1583, average loss: 2.1395
[09/26 14:15:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 26.50	top5: 69.00	
[09/26 14:15:56 visual_prompt]: Best epoch 16: best metric: 0.265
[09/26 14:15:56 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[09/26 14:16:03 visual_prompt]: Epoch 17 / 100: avg data time: 4.92e-02, avg batch time: 0.4642, average train loss: 2.0925
[09/26 14:16:04 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1585, average loss: 2.1053
[09/26 14:16:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.00	top5: 68.50	
[09/26 14:16:04 visual_prompt]: Best epoch 17: best metric: 0.290
[09/26 14:16:04 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[09/26 14:16:11 visual_prompt]: Epoch 18 / 100: avg data time: 6.30e-02, avg batch time: 0.4764, average train loss: 2.0581
[09/26 14:16:12 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1584, average loss: 2.0239
[09/26 14:16:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 30.00	top5: 70.50	
[09/26 14:16:12 visual_prompt]: Best epoch 18: best metric: 0.300
[09/26 14:16:12 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[09/26 14:16:19 visual_prompt]: Epoch 19 / 100: avg data time: 6.26e-02, avg batch time: 0.4769, average train loss: 2.0631
[09/26 14:16:20 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1583, average loss: 2.1078
[09/26 14:16:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.00	top5: 69.00	
[09/26 14:16:20 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[09/26 14:16:27 visual_prompt]: Epoch 20 / 100: avg data time: 6.24e-02, avg batch time: 0.4758, average train loss: 1.9414
[09/26 14:16:29 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1588, average loss: 1.7724
[09/26 14:16:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 34.00	top5: 82.00	
[09/26 14:16:29 visual_prompt]: Best epoch 20: best metric: 0.340
[09/26 14:16:29 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[09/26 14:16:35 visual_prompt]: Epoch 21 / 100: avg data time: 5.71e-02, avg batch time: 0.4713, average train loss: 1.6901
[09/26 14:16:37 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1589, average loss: 1.6472
[09/26 14:16:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 40.00	top5: 84.50	
[09/26 14:16:37 visual_prompt]: Best epoch 21: best metric: 0.400
[09/26 14:16:37 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[09/26 14:16:43 visual_prompt]: Epoch 22 / 100: avg data time: 6.48e-02, avg batch time: 0.4783, average train loss: 1.5463
[09/26 14:16:45 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1590, average loss: 1.4856
[09/26 14:16:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 47.50	top5: 84.50	
[09/26 14:16:45 visual_prompt]: Best epoch 22: best metric: 0.475
[09/26 14:16:45 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[09/26 14:16:51 visual_prompt]: Epoch 23 / 100: avg data time: 5.24e-02, avg batch time: 0.4663, average train loss: 1.3589
[09/26 14:16:53 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1585, average loss: 1.5804
[09/26 14:16:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 45.50	top5: 92.00	
[09/26 14:16:53 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[09/26 14:16:59 visual_prompt]: Epoch 24 / 100: avg data time: 5.21e-02, avg batch time: 0.4672, average train loss: 1.3507
[09/26 14:17:01 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1589, average loss: 1.5090
[09/26 14:17:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 49.00	top5: 93.00	
[09/26 14:17:01 visual_prompt]: Best epoch 24: best metric: 0.490
[09/26 14:17:01 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[09/26 14:17:07 visual_prompt]: Epoch 25 / 100: avg data time: 4.96e-02, avg batch time: 0.4647, average train loss: 1.3192
[09/26 14:17:09 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1586, average loss: 1.4013
[09/26 14:17:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 47.50	top5: 92.50	
[09/26 14:17:09 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[09/26 14:17:15 visual_prompt]: Epoch 26 / 100: avg data time: 5.61e-02, avg batch time: 0.4704, average train loss: 1.2445
[09/26 14:17:17 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1587, average loss: 1.1548
[09/26 14:17:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 57.50	top5: 96.00	
[09/26 14:17:17 visual_prompt]: Best epoch 26: best metric: 0.575
[09/26 14:17:17 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[09/26 14:17:23 visual_prompt]: Epoch 27 / 100: avg data time: 6.06e-02, avg batch time: 0.4755, average train loss: 1.0170
[09/26 14:17:25 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1589, average loss: 1.0958
[09/26 14:17:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 60.50	top5: 96.50	
[09/26 14:17:25 visual_prompt]: Best epoch 27: best metric: 0.605
[09/26 14:17:25 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[09/26 14:17:31 visual_prompt]: Epoch 28 / 100: avg data time: 5.87e-02, avg batch time: 0.4736, average train loss: 0.9040
[09/26 14:17:33 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1584, average loss: 1.2088
[09/26 14:17:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 56.50	top5: 95.00	
[09/26 14:17:33 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[09/26 14:17:39 visual_prompt]: Epoch 29 / 100: avg data time: 6.05e-02, avg batch time: 0.4750, average train loss: 0.8113
[09/26 14:17:41 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.1587, average loss: 1.0975
[09/26 14:17:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.50	top5: 96.00	
[09/26 14:17:41 visual_prompt]: Best epoch 29: best metric: 0.655
[09/26 14:17:41 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[09/26 14:17:47 visual_prompt]: Epoch 30 / 100: avg data time: 6.19e-02, avg batch time: 0.4753, average train loss: 0.8352
[09/26 14:17:49 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1581, average loss: 1.1987
[09/26 14:17:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 63.50	top5: 95.50	
[09/26 14:17:49 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[09/26 14:17:55 visual_prompt]: Epoch 31 / 100: avg data time: 6.07e-02, avg batch time: 0.4751, average train loss: 0.7032
[09/26 14:17:57 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1579, average loss: 1.1434
[09/26 14:17:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 63.50	top5: 95.00	
[09/26 14:17:57 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[09/26 14:18:03 visual_prompt]: Epoch 32 / 100: avg data time: 6.24e-02, avg batch time: 0.4769, average train loss: 0.6623
[09/26 14:18:05 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1579, average loss: 0.9349
[09/26 14:18:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.50	top5: 95.50	
[09/26 14:18:05 visual_prompt]: Best epoch 32: best metric: 0.675
[09/26 14:18:05 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[09/26 14:18:11 visual_prompt]: Epoch 33 / 100: avg data time: 5.73e-02, avg batch time: 0.4723, average train loss: 0.5678
[09/26 14:18:13 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1580, average loss: 1.1967
[09/26 14:18:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 64.00	top5: 94.00	
[09/26 14:18:13 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[09/26 14:18:20 visual_prompt]: Epoch 34 / 100: avg data time: 6.61e-02, avg batch time: 0.4809, average train loss: 0.6258
[09/26 14:18:21 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1581, average loss: 1.0131
[09/26 14:18:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.00	top5: 97.00	
[09/26 14:18:21 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[09/26 14:18:28 visual_prompt]: Epoch 35 / 100: avg data time: 6.21e-02, avg batch time: 0.4767, average train loss: 0.4704
[09/26 14:18:29 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1583, average loss: 1.1067
[09/26 14:18:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.00	top5: 96.50	
[09/26 14:18:29 visual_prompt]: Best epoch 35: best metric: 0.700
[09/26 14:18:29 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[09/26 14:18:36 visual_prompt]: Epoch 36 / 100: avg data time: 4.50e-02, avg batch time: 0.4611, average train loss: 0.4859
[09/26 14:18:37 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1588, average loss: 0.9944
[09/26 14:18:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.50	top5: 98.00	
[09/26 14:18:37 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[09/26 14:18:44 visual_prompt]: Epoch 37 / 100: avg data time: 5.47e-02, avg batch time: 0.4680, average train loss: 0.3574
[09/26 14:18:45 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1582, average loss: 0.8425
[09/26 14:18:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.50	top5: 98.00	
[09/26 14:18:45 visual_prompt]: Best epoch 37: best metric: 0.745
[09/26 14:18:45 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[09/26 14:18:52 visual_prompt]: Epoch 38 / 100: avg data time: 6.51e-02, avg batch time: 0.4782, average train loss: 0.3421
[09/26 14:18:53 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1586, average loss: 1.2362
[09/26 14:18:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.00	top5: 97.50	
[09/26 14:18:53 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.07795964517353735
[09/26 14:19:00 visual_prompt]: Epoch 39 / 100: avg data time: 6.41e-02, avg batch time: 0.4772, average train loss: 0.2945
[09/26 14:19:02 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1578, average loss: 1.1477
[09/26 14:19:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 96.50	
[09/26 14:19:02 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.07649596321166025
[09/26 14:19:08 visual_prompt]: Epoch 40 / 100: avg data time: 5.77e-02, avg batch time: 0.4720, average train loss: 0.2447
[09/26 14:19:10 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1584, average loss: 1.1577
[09/26 14:19:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 97.50	
[09/26 14:19:10 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.07500000000000001
[09/26 14:19:16 visual_prompt]: Epoch 41 / 100: avg data time: 6.51e-02, avg batch time: 0.4777, average train loss: 0.3305
[09/26 14:19:18 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1579, average loss: 1.3897
[09/26 14:19:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.50	top5: 95.00	
[09/26 14:19:18 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.07347357813929455
[09/26 14:19:24 visual_prompt]: Epoch 42 / 100: avg data time: 6.24e-02, avg batch time: 0.4761, average train loss: 0.3072
[09/26 14:19:26 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1586, average loss: 1.0673
[09/26 14:19:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 14:19:26 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.07191855733945388
[09/26 14:19:32 visual_prompt]: Epoch 43 / 100: avg data time: 5.82e-02, avg batch time: 0.4725, average train loss: 0.2327
[09/26 14:19:34 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1575, average loss: 1.1968
[09/26 14:19:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 97.00	
[09/26 14:19:34 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.07033683215379002
[09/26 14:19:40 visual_prompt]: Epoch 44 / 100: avg data time: 6.09e-02, avg batch time: 0.4764, average train loss: 0.1452
[09/26 14:19:42 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1584, average loss: 1.2720
[09/26 14:19:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 97.50	
[09/26 14:19:42 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0687303296707956
[09/26 14:19:48 visual_prompt]: Epoch 45 / 100: avg data time: 5.16e-02, avg batch time: 0.4654, average train loss: 0.2308
[09/26 14:19:50 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1588, average loss: 1.0796
[09/26 14:19:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 96.50	
[09/26 14:19:50 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.06710100716628345
[09/26 14:19:57 visual_prompt]: Epoch 46 / 100: avg data time: 6.12e-02, avg batch time: 0.4748, average train loss: 0.2358
[09/26 14:19:58 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1585, average loss: 1.3288
[09/26 14:19:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 97.50	
[09/26 14:19:58 visual_prompt]: Best epoch 46: best metric: 0.760
[09/26 14:19:58 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.06545084971874737
[09/26 14:20:05 visual_prompt]: Epoch 47 / 100: avg data time: 6.43e-02, avg batch time: 0.4770, average train loss: 0.1560
[09/26 14:20:06 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1581, average loss: 1.2895
[09/26 14:20:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.00	top5: 97.00	
[09/26 14:20:06 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.06378186779084996
[09/26 14:20:13 visual_prompt]: Epoch 48 / 100: avg data time: 5.02e-02, avg batch time: 0.4645, average train loss: 0.0886
[09/26 14:20:14 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 1.2291
[09/26 14:20:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 96.00	
[09/26 14:20:14 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.06209609477998338
[09/26 14:20:21 visual_prompt]: Epoch 49 / 100: avg data time: 5.45e-02, avg batch time: 0.4690, average train loss: 0.0395
[09/26 14:20:22 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1581, average loss: 1.4924
[09/26 14:20:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.50	top5: 97.50	
[09/26 14:20:22 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.06039558454088796
[09/26 14:20:29 visual_prompt]: Epoch 50 / 100: avg data time: 4.87e-02, avg batch time: 0.4636, average train loss: 0.0517
[09/26 14:20:30 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1616, average loss: 1.3609
[09/26 14:20:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 97.50	
[09/26 14:20:30 visual_prompt]: Best epoch 50: best metric: 0.780
[09/26 14:20:30 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.058682408883346526
[09/26 14:20:37 visual_prompt]: Epoch 51 / 100: avg data time: 6.59e-02, avg batch time: 0.4791, average train loss: 0.0385
[09/26 14:20:38 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1581, average loss: 1.4415
[09/26 14:20:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.50	top5: 97.00	
[09/26 14:20:38 visual_prompt]: Best epoch 51: best metric: 0.785
[09/26 14:20:38 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.05695865504800327
[09/26 14:20:45 visual_prompt]: Epoch 52 / 100: avg data time: 5.64e-02, avg batch time: 0.4725, average train loss: 0.0774
[09/26 14:20:46 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1583, average loss: 1.4653
[09/26 14:20:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.50	top5: 96.00	
[09/26 14:20:46 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.05522642316338268
[09/26 14:20:53 visual_prompt]: Epoch 53 / 100: avg data time: 6.03e-02, avg batch time: 0.4749, average train loss: 0.0535
[09/26 14:20:54 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1579, average loss: 1.5026
[09/26 14:20:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 97.00	
[09/26 14:20:54 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.05348782368720626
[09/26 14:21:01 visual_prompt]: Epoch 54 / 100: avg data time: 5.42e-02, avg batch time: 0.4691, average train loss: 0.0407
[09/26 14:21:02 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1582, average loss: 1.2972
[09/26 14:21:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 98.00	
[09/26 14:21:02 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.05174497483512506
[09/26 14:21:09 visual_prompt]: Epoch 55 / 100: avg data time: 6.61e-02, avg batch time: 0.4792, average train loss: 0.0453
[09/26 14:21:10 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1582, average loss: 1.3137
[09/26 14:21:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.00	top5: 97.50	
[09/26 14:21:10 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.05
[09/26 14:21:17 visual_prompt]: Epoch 56 / 100: avg data time: 6.15e-02, avg batch time: 0.4755, average train loss: 0.0548
[09/26 14:21:19 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1586, average loss: 1.7261
[09/26 14:21:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.00	top5: 96.00	
[09/26 14:21:19 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.04825502516487497
[09/26 14:21:25 visual_prompt]: Epoch 57 / 100: avg data time: 5.81e-02, avg batch time: 0.4711, average train loss: 0.0575
[09/26 14:21:27 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.1585, average loss: 1.3082
[09/26 14:21:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.50	top5: 97.50	
[09/26 14:21:27 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.04651217631279374
[09/26 14:21:33 visual_prompt]: Epoch 58 / 100: avg data time: 6.11e-02, avg batch time: 0.4749, average train loss: 0.0334
[09/26 14:21:35 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1582, average loss: 1.3879
[09/26 14:21:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 97.50	
[09/26 14:21:35 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.04477357683661734
[09/26 14:21:41 visual_prompt]: Epoch 59 / 100: avg data time: 5.77e-02, avg batch time: 0.4707, average train loss: 0.1129
[09/26 14:21:43 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1585, average loss: 1.5877
[09/26 14:21:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 99.00	
[09/26 14:21:43 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.04304134495199674
[09/26 14:21:49 visual_prompt]: Epoch 60 / 100: avg data time: 6.05e-02, avg batch time: 0.4731, average train loss: 0.1738
[09/26 14:21:51 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.1581, average loss: 1.2781
[09/26 14:21:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 97.00	
[09/26 14:21:51 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.041317591116653486
[09/26 14:21:57 visual_prompt]: Epoch 61 / 100: avg data time: 5.10e-02, avg batch time: 0.4665, average train loss: 0.1450
[09/26 14:21:59 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1581, average loss: 1.0146
[09/26 14:21:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 97.00	
[09/26 14:21:59 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.039604415459112044
[09/26 14:22:05 visual_prompt]: Epoch 62 / 100: avg data time: 5.53e-02, avg batch time: 0.4694, average train loss: 0.0610
[09/26 14:22:07 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.1580, average loss: 1.4420
[09/26 14:22:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 97.00	
[09/26 14:22:07 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.037903905220016625
[09/26 14:22:13 visual_prompt]: Epoch 63 / 100: avg data time: 4.99e-02, avg batch time: 0.4644, average train loss: 0.0409
[09/26 14:22:15 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1582, average loss: 1.2243
[09/26 14:22:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 97.00	
[09/26 14:22:15 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.036218132209150045
[09/26 14:22:21 visual_prompt]: Epoch 64 / 100: avg data time: 5.31e-02, avg batch time: 0.4668, average train loss: 0.0188
[09/26 14:22:23 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1586, average loss: 1.2941
[09/26 14:22:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 98.50	
[09/26 14:22:23 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.03454915028125263
[09/26 14:22:29 visual_prompt]: Epoch 65 / 100: avg data time: 6.32e-02, avg batch time: 0.4757, average train loss: 0.0231
[09/26 14:22:31 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1576, average loss: 1.2165
[09/26 14:22:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 97.50	
[09/26 14:22:31 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.03289899283371657
[09/26 14:22:37 visual_prompt]: Epoch 66 / 100: avg data time: 6.01e-02, avg batch time: 0.4726, average train loss: 0.0124
[09/26 14:22:39 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1585, average loss: 1.2429
[09/26 14:22:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.50	top5: 98.00	
[09/26 14:22:39 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0312696703292044
[09/26 14:22:45 visual_prompt]: Epoch 67 / 100: avg data time: 6.18e-02, avg batch time: 0.4759, average train loss: 0.0093
[09/26 14:22:47 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1585, average loss: 1.1903
[09/26 14:22:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 98.00	
[09/26 14:22:47 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.02966316784621
[09/26 14:22:54 visual_prompt]: Epoch 68 / 100: avg data time: 6.10e-02, avg batch time: 0.4751, average train loss: 0.0029
[09/26 14:22:55 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1586, average loss: 1.2725
[09/26 14:22:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.50	top5: 98.00	
[09/26 14:22:55 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.028081442660546126
[09/26 14:23:02 visual_prompt]: Epoch 69 / 100: avg data time: 5.26e-02, avg batch time: 0.4683, average train loss: 0.0027
[09/26 14:23:03 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1580, average loss: 1.2936
[09/26 14:23:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.50	
[09/26 14:23:03 visual_prompt]: Best epoch 69: best metric: 0.790
[09/26 14:23:03 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.026526421860705474
[09/26 14:23:10 visual_prompt]: Epoch 70 / 100: avg data time: 6.01e-02, avg batch time: 0.4742, average train loss: 0.0038
[09/26 14:23:11 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1583, average loss: 1.2928
[09/26 14:23:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.50	top5: 98.00	
[09/26 14:23:11 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.025000000000000012
[09/26 14:23:18 visual_prompt]: Epoch 71 / 100: avg data time: 5.96e-02, avg batch time: 0.4739, average train loss: 0.0034
[09/26 14:23:19 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1583, average loss: 1.3575
[09/26 14:23:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 98.00	
[09/26 14:23:19 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.02350403678833976
[09/26 14:23:26 visual_prompt]: Epoch 72 / 100: avg data time: 5.69e-02, avg batch time: 0.4699, average train loss: 0.0025
[09/26 14:23:27 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1586, average loss: 1.3613
[09/26 14:23:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 98.00	
[09/26 14:23:27 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.022040354826462667
[09/26 14:23:34 visual_prompt]: Epoch 73 / 100: avg data time: 4.60e-02, avg batch time: 0.4595, average train loss: 0.0016
[09/26 14:23:35 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1582, average loss: 1.3588
[09/26 14:23:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.50	top5: 98.00	
[09/26 14:23:35 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.02061073738537635
[09/26 14:23:42 visual_prompt]: Epoch 74 / 100: avg data time: 5.88e-02, avg batch time: 0.4726, average train loss: 0.0018
[09/26 14:23:43 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1582, average loss: 1.3620
[09/26 14:23:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.00	top5: 98.00	
[09/26 14:23:43 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.019216926233717086
[09/26 14:23:50 visual_prompt]: Epoch 75 / 100: avg data time: 6.07e-02, avg batch time: 0.4745, average train loss: 0.0017
[09/26 14:23:51 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1580, average loss: 1.3805
[09/26 14:23:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 98.00	
[09/26 14:23:51 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.017860619515673033
[09/26 14:23:58 visual_prompt]: Epoch 76 / 100: avg data time: 5.42e-02, avg batch time: 0.4676, average train loss: 0.0021
[09/26 14:23:59 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1583, average loss: 1.3860
[09/26 14:23:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 98.00	
[09/26 14:23:59 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.016543469682057107
[09/26 14:24:06 visual_prompt]: Epoch 77 / 100: avg data time: 5.84e-02, avg batch time: 0.4720, average train loss: 0.0018
[09/26 14:24:07 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1585, average loss: 1.3822
[09/26 14:24:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 98.00	
[09/26 14:24:07 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.015267081477050132
[09/26 14:24:14 visual_prompt]: Epoch 78 / 100: avg data time: 5.37e-02, avg batch time: 0.4675, average train loss: 0.0020
[09/26 14:24:15 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1582, average loss: 1.3400
[09/26 14:24:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.50	top5: 98.00	
[09/26 14:24:15 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.014033009983067453
[09/26 14:24:22 visual_prompt]: Epoch 79 / 100: avg data time: 6.28e-02, avg batch time: 0.4757, average train loss: 0.0018
[09/26 14:24:23 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1585, average loss: 1.3307
[09/26 14:24:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.50	
[09/26 14:24:23 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.012842758726130284
[09/26 14:24:30 visual_prompt]: Epoch 80 / 100: avg data time: 4.89e-02, avg batch time: 0.4638, average train loss: 0.0015
[09/26 14:24:31 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.1578, average loss: 1.3332
[09/26 14:24:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.50	
[09/26 14:24:31 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.011697777844051106
[09/26 14:24:38 visual_prompt]: Epoch 81 / 100: avg data time: 6.24e-02, avg batch time: 0.4765, average train loss: 0.0012
[09/26 14:24:39 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1581, average loss: 1.3340
[09/26 14:24:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.50	
[09/26 14:24:39 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.010599462319663905
[09/26 14:24:46 visual_prompt]: Epoch 82 / 100: avg data time: 5.21e-02, avg batch time: 0.4656, average train loss: 0.0013
[09/26 14:24:47 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1586, average loss: 1.3342
[09/26 14:24:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.50	
[09/26 14:24:47 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.009549150281252633
[09/26 14:24:54 visual_prompt]: Epoch 83 / 100: avg data time: 5.81e-02, avg batch time: 0.4722, average train loss: 0.0017
[09/26 14:24:55 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1584, average loss: 1.3375
[09/26 14:24:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.50	
[09/26 14:24:55 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.008548121372247919
[09/26 14:25:02 visual_prompt]: Epoch 84 / 100: avg data time: 6.50e-02, avg batch time: 0.4792, average train loss: 0.0016
[09/26 14:25:04 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1584, average loss: 1.3392
[09/26 14:25:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.50	
[09/26 14:25:04 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.007597595192178702
[09/26 14:25:10 visual_prompt]: Epoch 85 / 100: avg data time: 6.05e-02, avg batch time: 0.4747, average train loss: 0.0018
[09/26 14:25:12 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1583, average loss: 1.3512
[09/26 14:25:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.50	
[09/26 14:25:12 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.006698729810778065
[09/26 14:25:18 visual_prompt]: Epoch 86 / 100: avg data time: 6.00e-02, avg batch time: 0.4735, average train loss: 0.0009
[09/26 14:25:20 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1582, average loss: 1.3577
[09/26 14:25:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.50	
[09/26 14:25:20 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.005852620357053651
[09/26 14:25:26 visual_prompt]: Epoch 87 / 100: avg data time: 6.39e-02, avg batch time: 0.4793, average train loss: 0.0019
[09/26 14:25:28 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1583, average loss: 1.3546
[09/26 14:25:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.50	
[09/26 14:25:28 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00506029768504166
[09/26 14:25:34 visual_prompt]: Epoch 88 / 100: avg data time: 5.55e-02, avg batch time: 0.4695, average train loss: 0.0011
[09/26 14:25:36 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1588, average loss: 1.3568
[09/26 14:25:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.50	
[09/26 14:25:36 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.004322727117869951
[09/26 14:25:43 visual_prompt]: Epoch 89 / 100: avg data time: 5.87e-02, avg batch time: 0.4739, average train loss: 0.0011
[09/26 14:25:44 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1587, average loss: 1.3572
[09/26 14:25:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.50	
[09/26 14:25:44 visual_prompt]: Best epoch 89: best metric: 0.795
[09/26 14:25:44 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0036408072716606343
[09/26 14:25:51 visual_prompt]: Epoch 90 / 100: avg data time: 6.27e-02, avg batch time: 0.4758, average train loss: 0.0014
[09/26 14:25:52 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1582, average loss: 1.3573
[09/26 14:25:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.50	
[09/26 14:25:52 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0030153689607045845
[09/26 14:25:59 visual_prompt]: Epoch 91 / 100: avg data time: 7.47e-02, avg batch time: 0.4886, average train loss: 0.0012
[09/26 14:26:01 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1586, average loss: 1.3569
[09/26 14:26:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.50	
[09/26 14:26:01 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0024471741852423235
[09/26 14:26:07 visual_prompt]: Epoch 92 / 100: avg data time: 6.61e-02, avg batch time: 0.4787, average train loss: 0.0016
[09/26 14:26:09 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1581, average loss: 1.3573
[09/26 14:26:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.50	
[09/26 14:26:09 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0019369152030840554
[09/26 14:26:15 visual_prompt]: Epoch 93 / 100: avg data time: 6.18e-02, avg batch time: 0.4748, average train loss: 0.0011
[09/26 14:26:17 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1583, average loss: 1.3591
[09/26 14:26:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.00	top5: 98.50	
[09/26 14:26:17 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0014852136862001765
[09/26 14:26:23 visual_prompt]: Epoch 94 / 100: avg data time: 5.87e-02, avg batch time: 0.4713, average train loss: 0.0013
[09/26 14:26:25 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1583, average loss: 1.3584
[09/26 14:26:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.50	
[09/26 14:26:25 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0010926199633097156
[09/26 14:26:31 visual_prompt]: Epoch 95 / 100: avg data time: 5.67e-02, avg batch time: 0.4704, average train loss: 0.0011
[09/26 14:26:33 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1581, average loss: 1.3580
[09/26 14:26:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.50	
[09/26 14:26:33 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0007596123493895991
[09/26 14:26:39 visual_prompt]: Epoch 96 / 100: avg data time: 6.57e-02, avg batch time: 0.4781, average train loss: 0.0009
[09/26 14:26:41 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1582, average loss: 1.3580
[09/26 14:26:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.50	
[09/26 14:26:41 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00048659656292148194
[09/26 14:26:48 visual_prompt]: Epoch 97 / 100: avg data time: 7.04e-02, avg batch time: 0.4829, average train loss: 0.0012
[09/26 14:26:49 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1584, average loss: 1.3581
[09/26 14:26:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.50	
[09/26 14:26:49 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0002739052315863355
[09/26 14:26:56 visual_prompt]: Epoch 98 / 100: avg data time: 6.02e-02, avg batch time: 0.4739, average train loss: 0.0015
[09/26 14:26:57 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1584, average loss: 1.3583
[09/26 14:26:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.50	
[09/26 14:26:57 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.00012179748700879012
[09/26 14:27:04 visual_prompt]: Epoch 99 / 100: avg data time: 5.05e-02, avg batch time: 0.4639, average train loss: 0.0011
[09/26 14:27:05 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1582, average loss: 1.3584
[09/26 14:27:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.50	
[09/26 14:27:05 visual_prompt]: Training 100 / 100 epoch, with learning rate 3.0458649045211895e-05
[09/26 14:27:12 visual_prompt]: Epoch 100 / 100: avg data time: 6.29e-02, avg batch time: 0.4775, average train loss: 0.0011
[09/26 14:27:13 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1580, average loss: 1.3584
[09/26 14:27:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 98.50	
[09/26 14:27:13 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 14:27:13 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 14:27:13 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 14:27:13 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 14:27:13 visual_prompt]: Training with config:
[09/26 14:27:13 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.1_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 14:27:13 visual_prompt]: Loading training data...
[09/26 14:27:13 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 14:27:14 visual_prompt]: Number of images: 800
[09/26 14:27:14 visual_prompt]: Number of classes: 10 / 10
[09/26 14:27:14 visual_prompt]: Loading validation data...
[09/26 14:27:14 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 14:27:15 visual_prompt]: Number of images: 200
[09/26 14:27:15 visual_prompt]: Number of classes: 10 / 10
[09/26 14:27:15 visual_prompt]: Constructing models...
[09/26 14:27:17 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 14:27:17 visual_prompt]: tuned percent:0.543
[09/26 14:27:17 visual_prompt]: Device used for model: 0
[09/26 14:27:17 visual_prompt]: Setting up Evaluator...
[09/26 14:27:17 visual_prompt]: Setting up Trainer...
[09/26 14:27:17 visual_prompt]: 	Setting up the optimizer...
[09/26 14:27:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 14:27:24 visual_prompt]: Epoch 1 / 100: avg data time: 6.28e-02, avg batch time: 0.4807, average train loss: 2.6864
[09/26 14:27:25 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1583, average loss: 2.6214
[09/26 14:27:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 14:27:25 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 14:27:25 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[09/26 14:27:32 visual_prompt]: Epoch 2 / 100: avg data time: 5.89e-02, avg batch time: 0.4722, average train loss: 2.3825
[09/26 14:27:33 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1585, average loss: 2.2294
[09/26 14:27:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 14:27:33 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[09/26 14:27:40 visual_prompt]: Epoch 3 / 100: avg data time: 5.30e-02, avg batch time: 0.4662, average train loss: 2.2558
[09/26 14:27:41 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1576, average loss: 2.2271
[09/26 14:27:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 14:27:41 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[09/26 14:27:48 visual_prompt]: Epoch 4 / 100: avg data time: 6.24e-02, avg batch time: 0.4743, average train loss: 2.2560
[09/26 14:27:49 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1576, average loss: 2.2223
[09/26 14:27:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 14:27:49 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[09/26 14:27:56 visual_prompt]: Epoch 5 / 100: avg data time: 5.99e-02, avg batch time: 0.4717, average train loss: 2.2421
[09/26 14:27:58 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1580, average loss: 2.2203
[09/26 14:27:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 14:27:58 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[09/26 14:28:04 visual_prompt]: Epoch 6 / 100: avg data time: 5.94e-02, avg batch time: 0.4712, average train loss: 2.2403
[09/26 14:28:06 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1587, average loss: 2.2171
[09/26 14:28:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.50	
[09/26 14:28:06 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[09/26 14:28:12 visual_prompt]: Epoch 7 / 100: avg data time: 6.14e-02, avg batch time: 0.4731, average train loss: 2.2446
[09/26 14:28:14 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1585, average loss: 2.2036
[09/26 14:28:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 14:28:14 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[09/26 14:28:20 visual_prompt]: Epoch 8 / 100: avg data time: 5.52e-02, avg batch time: 0.4680, average train loss: 2.2390
[09/26 14:28:22 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1579, average loss: 2.2298
[09/26 14:28:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 64.00	
[09/26 14:28:22 visual_prompt]: Best epoch 8: best metric: 0.235
[09/26 14:28:22 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[09/26 14:28:28 visual_prompt]: Epoch 9 / 100: avg data time: 6.99e-02, avg batch time: 0.4816, average train loss: 2.2532
[09/26 14:28:30 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1580, average loss: 2.2541
[09/26 14:28:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.50	top5: 63.50	
[09/26 14:28:30 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[09/26 14:28:36 visual_prompt]: Epoch 10 / 100: avg data time: 6.62e-02, avg batch time: 0.4774, average train loss: 2.2351
[09/26 14:28:38 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1580, average loss: 2.2267
[09/26 14:28:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 18.00	top5: 64.00	
[09/26 14:28:38 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[09/26 14:28:44 visual_prompt]: Epoch 11 / 100: avg data time: 6.08e-02, avg batch time: 0.4732, average train loss: 2.2371
[09/26 14:28:46 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1580, average loss: 2.2572
[09/26 14:28:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 64.50	
[09/26 14:28:46 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[09/26 14:28:52 visual_prompt]: Epoch 12 / 100: avg data time: 4.93e-02, avg batch time: 0.4636, average train loss: 2.2143
[09/26 14:28:54 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1578, average loss: 2.1946
[09/26 14:28:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.00	top5: 68.50	
[09/26 14:28:54 visual_prompt]: Best epoch 12: best metric: 0.250
[09/26 14:28:54 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[09/26 14:29:01 visual_prompt]: Epoch 13 / 100: avg data time: 6.64e-02, avg batch time: 0.4787, average train loss: 2.2180
[09/26 14:29:02 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1582, average loss: 2.2005
[09/26 14:29:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 66.50	
[09/26 14:29:02 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[09/26 14:29:09 visual_prompt]: Epoch 14 / 100: avg data time: 6.33e-02, avg batch time: 0.4748, average train loss: 2.1801
[09/26 14:29:10 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1581, average loss: 2.1637
[09/26 14:29:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 27.00	top5: 66.00	
[09/26 14:29:10 visual_prompt]: Best epoch 14: best metric: 0.270
[09/26 14:29:10 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[09/26 14:29:17 visual_prompt]: Epoch 15 / 100: avg data time: 5.89e-02, avg batch time: 0.4715, average train loss: 2.1602
[09/26 14:29:18 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1580, average loss: 2.1387
[09/26 14:29:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 27.50	top5: 66.00	
[09/26 14:29:18 visual_prompt]: Best epoch 15: best metric: 0.275
[09/26 14:29:18 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[09/26 14:29:25 visual_prompt]: Epoch 16 / 100: avg data time: 5.67e-02, avg batch time: 0.4691, average train loss: 2.1109
[09/26 14:29:26 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.1584, average loss: 2.1091
[09/26 14:29:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.50	top5: 74.50	
[09/26 14:29:26 visual_prompt]: Best epoch 16: best metric: 0.295
[09/26 14:29:26 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[09/26 14:29:33 visual_prompt]: Epoch 17 / 100: avg data time: 4.92e-02, avg batch time: 0.4620, average train loss: 2.1070
[09/26 14:29:34 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.1582, average loss: 2.0449
[09/26 14:29:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.00	top5: 71.00	
[09/26 14:29:34 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[09/26 14:29:41 visual_prompt]: Epoch 18 / 100: avg data time: 5.75e-02, avg batch time: 0.4699, average train loss: 2.0476
[09/26 14:29:42 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.1582, average loss: 1.9614
[09/26 14:29:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 28.00	top5: 77.50	
[09/26 14:29:42 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[09/26 14:29:49 visual_prompt]: Epoch 19 / 100: avg data time: 6.38e-02, avg batch time: 0.4768, average train loss: 1.9330
[09/26 14:29:51 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1580, average loss: 1.9752
[09/26 14:29:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 31.50	top5: 77.50	
[09/26 14:29:51 visual_prompt]: Best epoch 19: best metric: 0.315
[09/26 14:29:51 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[09/26 14:29:57 visual_prompt]: Epoch 20 / 100: avg data time: 5.93e-02, avg batch time: 0.4713, average train loss: 1.8500
[09/26 14:29:59 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.1581, average loss: 1.6269
[09/26 14:29:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 40.50	top5: 87.50	
[09/26 14:29:59 visual_prompt]: Best epoch 20: best metric: 0.405
[09/26 14:29:59 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[09/26 14:30:05 visual_prompt]: Epoch 21 / 100: avg data time: 6.05e-02, avg batch time: 0.4726, average train loss: 1.6414
[09/26 14:30:07 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1583, average loss: 1.5251
[09/26 14:30:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 46.50	top5: 90.00	
[09/26 14:30:07 visual_prompt]: Best epoch 21: best metric: 0.465
[09/26 14:30:07 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[09/26 14:30:13 visual_prompt]: Epoch 22 / 100: avg data time: 6.09e-02, avg batch time: 0.4735, average train loss: 1.5529
[09/26 14:30:15 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1574, average loss: 1.8149
[09/26 14:30:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.00	top5: 93.00	
[09/26 14:30:15 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[09/26 14:30:21 visual_prompt]: Epoch 23 / 100: avg data time: 4.98e-02, avg batch time: 0.4633, average train loss: 1.5482
[09/26 14:30:23 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1583, average loss: 1.4331
[09/26 14:30:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 45.50	top5: 91.50	
[09/26 14:30:23 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[09/26 14:30:29 visual_prompt]: Epoch 24 / 100: avg data time: 5.14e-02, avg batch time: 0.4652, average train loss: 1.3586
[09/26 14:30:31 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1588, average loss: 1.2532
[09/26 14:30:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 56.00	top5: 94.50	
[09/26 14:30:31 visual_prompt]: Best epoch 24: best metric: 0.560
[09/26 14:30:31 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[09/26 14:30:37 visual_prompt]: Epoch 25 / 100: avg data time: 5.59e-02, avg batch time: 0.4685, average train loss: 1.2280
[09/26 14:30:39 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1585, average loss: 1.4169
[09/26 14:30:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 55.00	top5: 92.00	
[09/26 14:30:39 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[09/26 14:30:45 visual_prompt]: Epoch 26 / 100: avg data time: 5.57e-02, avg batch time: 0.4697, average train loss: 1.1470
[09/26 14:30:47 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1586, average loss: 1.1839
[09/26 14:30:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 58.00	top5: 96.00	
[09/26 14:30:47 visual_prompt]: Best epoch 26: best metric: 0.580
[09/26 14:30:47 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[09/26 14:30:53 visual_prompt]: Epoch 27 / 100: avg data time: 5.12e-02, avg batch time: 0.4653, average train loss: 1.0232
[09/26 14:30:55 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.1584, average loss: 1.0629
[09/26 14:30:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 61.00	top5: 93.50	
[09/26 14:30:55 visual_prompt]: Best epoch 27: best metric: 0.610
[09/26 14:30:55 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[09/26 14:31:01 visual_prompt]: Epoch 28 / 100: avg data time: 5.89e-02, avg batch time: 0.4711, average train loss: 0.8376
[09/26 14:31:03 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1587, average loss: 1.1972
[09/26 14:31:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 60.00	top5: 96.50	
[09/26 14:31:03 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[09/26 14:31:09 visual_prompt]: Epoch 29 / 100: avg data time: 5.36e-02, avg batch time: 0.4673, average train loss: 0.7949
[09/26 14:31:11 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1582, average loss: 1.0544
[09/26 14:31:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 66.00	top5: 96.50	
[09/26 14:31:11 visual_prompt]: Best epoch 29: best metric: 0.660
[09/26 14:31:11 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[09/26 14:31:17 visual_prompt]: Epoch 30 / 100: avg data time: 5.70e-02, avg batch time: 0.4709, average train loss: 0.7741
[09/26 14:31:19 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1581, average loss: 1.0517
[09/26 14:31:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 66.00	top5: 97.00	
[09/26 14:31:19 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[09/26 14:31:25 visual_prompt]: Epoch 31 / 100: avg data time: 5.44e-02, avg batch time: 0.4679, average train loss: 0.6815
[09/26 14:31:27 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1585, average loss: 1.2661
[09/26 14:31:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 66.50	top5: 96.50	
[09/26 14:31:27 visual_prompt]: Best epoch 31: best metric: 0.665
[09/26 14:31:27 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[09/26 14:31:33 visual_prompt]: Epoch 32 / 100: avg data time: 5.86e-02, avg batch time: 0.4726, average train loss: 0.6600
[09/26 14:31:35 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1583, average loss: 1.1947
[09/26 14:31:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.00	top5: 96.50	
[09/26 14:31:35 visual_prompt]: Best epoch 32: best metric: 0.670
[09/26 14:31:35 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[09/26 14:31:41 visual_prompt]: Epoch 33 / 100: avg data time: 6.16e-02, avg batch time: 0.4751, average train loss: 0.5780
[09/26 14:31:43 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1585, average loss: 1.1105
[09/26 14:31:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.00	top5: 95.00	
[09/26 14:31:43 visual_prompt]: Best epoch 33: best metric: 0.690
[09/26 14:31:43 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[09/26 14:31:49 visual_prompt]: Epoch 34 / 100: avg data time: 5.40e-02, avg batch time: 0.4669, average train loss: 0.5709
[09/26 14:31:51 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1580, average loss: 1.4064
[09/26 14:31:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 61.00	top5: 96.50	
[09/26 14:31:51 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[09/26 14:31:57 visual_prompt]: Epoch 35 / 100: avg data time: 5.54e-02, avg batch time: 0.4683, average train loss: 0.5375
[09/26 14:31:59 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1584, average loss: 1.1211
[09/26 14:31:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 66.50	top5: 95.00	
[09/26 14:31:59 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[09/26 14:32:05 visual_prompt]: Epoch 36 / 100: avg data time: 6.51e-02, avg batch time: 0.4777, average train loss: 0.4558
[09/26 14:32:07 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1588, average loss: 1.0675
[09/26 14:32:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.00	top5: 96.50	
[09/26 14:32:07 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[09/26 14:32:14 visual_prompt]: Epoch 37 / 100: avg data time: 5.88e-02, avg batch time: 0.4717, average train loss: 0.3588
[09/26 14:32:15 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1581, average loss: 1.2155
[09/26 14:32:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.00	top5: 97.00	
[09/26 14:32:15 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[09/26 14:32:22 visual_prompt]: Epoch 38 / 100: avg data time: 6.67e-02, avg batch time: 0.4797, average train loss: 0.2886
[09/26 14:32:23 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1583, average loss: 1.1579
[09/26 14:32:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 96.50	
[09/26 14:32:23 visual_prompt]: Best epoch 38: best metric: 0.705
[09/26 14:32:23 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.07795964517353735
[09/26 14:32:30 visual_prompt]: Epoch 39 / 100: avg data time: 5.04e-02, avg batch time: 0.4647, average train loss: 0.2883
[09/26 14:32:31 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1580, average loss: 1.0456
[09/26 14:32:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 96.00	
[09/26 14:32:31 visual_prompt]: Best epoch 39: best metric: 0.760
[09/26 14:32:31 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.07649596321166025
[09/26 14:32:38 visual_prompt]: Epoch 40 / 100: avg data time: 6.28e-02, avg batch time: 0.4758, average train loss: 0.3226
[09/26 14:32:39 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1583, average loss: 1.1147
[09/26 14:32:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 95.00	
[09/26 14:32:39 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.07500000000000001
[09/26 14:32:46 visual_prompt]: Epoch 41 / 100: avg data time: 5.88e-02, avg batch time: 0.4727, average train loss: 0.2690
[09/26 14:32:48 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1580, average loss: 1.4358
[09/26 14:32:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.00	top5: 98.00	
[09/26 14:32:48 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.07347357813929455
[09/26 14:32:54 visual_prompt]: Epoch 42 / 100: avg data time: 5.53e-02, avg batch time: 0.4697, average train loss: 0.2334
[09/26 14:32:56 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1585, average loss: 1.0855
[09/26 14:32:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.50	
[09/26 14:32:56 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.07191855733945388
[09/26 14:33:02 visual_prompt]: Epoch 43 / 100: avg data time: 4.68e-02, avg batch time: 0.4613, average train loss: 0.2624
[09/26 14:33:04 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1587, average loss: 1.2126
[09/26 14:33:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.50	top5: 96.00	
[09/26 14:33:04 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.07033683215379002
[09/26 14:33:10 visual_prompt]: Epoch 44 / 100: avg data time: 5.65e-02, avg batch time: 0.4712, average train loss: 0.1826
[09/26 14:33:12 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1586, average loss: 1.3588
[09/26 14:33:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 95.50	
[09/26 14:33:12 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0687303296707956
[09/26 14:33:18 visual_prompt]: Epoch 45 / 100: avg data time: 5.80e-02, avg batch time: 0.4719, average train loss: 0.1640
[09/26 14:33:20 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1584, average loss: 1.0868
[09/26 14:33:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.50	top5: 96.50	
[09/26 14:33:20 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.06710100716628345
[09/26 14:33:26 visual_prompt]: Epoch 46 / 100: avg data time: 6.02e-02, avg batch time: 0.4748, average train loss: 0.1297
[09/26 14:33:28 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1585, average loss: 1.4761
[09/26 14:33:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.00	top5: 96.50	
[09/26 14:33:28 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.06545084971874737
[09/26 14:33:34 visual_prompt]: Epoch 47 / 100: avg data time: 6.29e-02, avg batch time: 0.4764, average train loss: 0.1415
[09/26 14:33:36 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1584, average loss: 1.1930
[09/26 14:33:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.00	top5: 97.50	
[09/26 14:33:36 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.06378186779084996
[09/26 14:33:43 visual_prompt]: Epoch 48 / 100: avg data time: 6.07e-02, avg batch time: 0.4752, average train loss: 0.1312
[09/26 14:33:44 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.1587, average loss: 1.3781
[09/26 14:33:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 97.50	
[09/26 14:33:44 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.06209609477998338
[09/26 14:33:51 visual_prompt]: Epoch 49 / 100: avg data time: 6.28e-02, avg batch time: 0.4765, average train loss: 0.1459
[09/26 14:33:52 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1588, average loss: 1.1577
[09/26 14:33:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.50	top5: 97.50	
[09/26 14:33:52 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.06039558454088796
[09/26 14:33:59 visual_prompt]: Epoch 50 / 100: avg data time: 6.54e-02, avg batch time: 0.4790, average train loss: 0.0807
[09/26 14:34:00 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1584, average loss: 1.4163
[09/26 14:34:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.50	top5: 97.00	
[09/26 14:34:00 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.058682408883346526
[09/26 14:34:07 visual_prompt]: Epoch 51 / 100: avg data time: 4.76e-02, avg batch time: 0.4635, average train loss: 0.0685
[09/26 14:34:08 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1586, average loss: 1.2663
[09/26 14:34:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 96.00	
[09/26 14:34:08 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.05695865504800327
[09/26 14:34:15 visual_prompt]: Epoch 52 / 100: avg data time: 6.33e-02, avg batch time: 0.4770, average train loss: 0.0460
[09/26 14:34:17 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1584, average loss: 1.4134
[09/26 14:34:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 96.00	
[09/26 14:34:17 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.05522642316338268
[09/26 14:34:23 visual_prompt]: Epoch 53 / 100: avg data time: 5.77e-02, avg batch time: 0.4722, average train loss: 0.0344
[09/26 14:34:25 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1584, average loss: 1.4866
[09/26 14:34:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.00	top5: 97.00	
[09/26 14:34:25 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.05348782368720626
[09/26 14:34:31 visual_prompt]: Epoch 54 / 100: avg data time: 5.81e-02, avg batch time: 0.4733, average train loss: 0.0467
[09/26 14:34:33 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1585, average loss: 1.4363
[09/26 14:34:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 97.00	
[09/26 14:34:33 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.05174497483512506
[09/26 14:34:39 visual_prompt]: Epoch 55 / 100: avg data time: 5.16e-02, avg batch time: 0.4659, average train loss: 0.0345
[09/26 14:34:41 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1585, average loss: 1.3022
[09/26 14:34:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 97.00	
[09/26 14:34:41 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.05
[09/26 14:34:47 visual_prompt]: Epoch 56 / 100: avg data time: 6.14e-02, avg batch time: 0.4753, average train loss: 0.0375
[09/26 14:34:49 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1587, average loss: 1.4458
[09/26 14:34:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 97.00	
[09/26 14:34:49 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.04825502516487497
[09/26 14:34:55 visual_prompt]: Epoch 57 / 100: avg data time: 6.15e-02, avg batch time: 0.4749, average train loss: 0.0355
[09/26 14:34:57 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1586, average loss: 1.5627
[09/26 14:34:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 96.50	
[09/26 14:34:57 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.04651217631279374
[09/26 14:35:03 visual_prompt]: Epoch 58 / 100: avg data time: 5.81e-02, avg batch time: 0.4730, average train loss: 0.0211
[09/26 14:35:05 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1581, average loss: 1.3893
[09/26 14:35:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 97.50	
[09/26 14:35:05 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.04477357683661734
[09/26 14:35:11 visual_prompt]: Epoch 59 / 100: avg data time: 6.10e-02, avg batch time: 0.4751, average train loss: 0.0124
[09/26 14:35:13 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1584, average loss: 1.4884
[09/26 14:35:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.50	top5: 97.00	
[09/26 14:35:13 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.04304134495199674
[09/26 14:35:19 visual_prompt]: Epoch 60 / 100: avg data time: 6.30e-02, avg batch time: 0.4771, average train loss: 0.0050
[09/26 14:35:21 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1584, average loss: 1.5158
[09/26 14:35:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.50	top5: 97.00	
[09/26 14:35:21 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.041317591116653486
[09/26 14:35:28 visual_prompt]: Epoch 61 / 100: avg data time: 5.54e-02, avg batch time: 0.4702, average train loss: 0.0337
[09/26 14:35:29 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1584, average loss: 1.5282
[09/26 14:35:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 97.00	
[09/26 14:35:29 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.039604415459112044
[09/26 14:35:36 visual_prompt]: Epoch 62 / 100: avg data time: 5.81e-02, avg batch time: 0.4719, average train loss: 0.0231
[09/26 14:35:37 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1580, average loss: 1.5542
[09/26 14:35:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 97.50	
[09/26 14:35:37 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.037903905220016625
[09/26 14:35:44 visual_prompt]: Epoch 63 / 100: avg data time: 5.46e-02, avg batch time: 0.4700, average train loss: 0.0120
[09/26 14:35:45 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1588, average loss: 1.4692
[09/26 14:35:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 97.00	
[09/26 14:35:45 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.036218132209150045
[09/26 14:35:52 visual_prompt]: Epoch 64 / 100: avg data time: 6.28e-02, avg batch time: 0.4777, average train loss: 0.0093
[09/26 14:35:53 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1583, average loss: 1.3789
[09/26 14:35:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.50	top5: 97.50	
[09/26 14:35:53 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.03454915028125263
[09/26 14:36:00 visual_prompt]: Epoch 65 / 100: avg data time: 6.18e-02, avg batch time: 0.4742, average train loss: 0.0095
[09/26 14:36:01 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1584, average loss: 1.5219
[09/26 14:36:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.50	top5: 97.00	
[09/26 14:36:01 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.03289899283371657
[09/26 14:36:08 visual_prompt]: Epoch 66 / 100: avg data time: 5.73e-02, avg batch time: 0.4712, average train loss: 0.0065
[09/26 14:36:09 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1586, average loss: 1.4641
[09/26 14:36:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 96.50	
[09/26 14:36:09 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0312696703292044
[09/26 14:36:16 visual_prompt]: Epoch 67 / 100: avg data time: 6.08e-02, avg batch time: 0.4738, average train loss: 0.0043
[09/26 14:36:17 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1584, average loss: 1.4401
[09/26 14:36:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.00	top5: 96.50	
[09/26 14:36:17 visual_prompt]: Best epoch 67: best metric: 0.770
[09/26 14:36:17 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.02966316784621
[09/26 14:36:24 visual_prompt]: Epoch 68 / 100: avg data time: 6.65e-02, avg batch time: 0.4798, average train loss: 0.0039
[09/26 14:36:26 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1585, average loss: 1.4460
[09/26 14:36:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:36:26 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.028081442660546126
[09/26 14:36:32 visual_prompt]: Epoch 69 / 100: avg data time: 5.06e-02, avg batch time: 0.4660, average train loss: 0.0027
[09/26 14:36:34 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1580, average loss: 1.4460
[09/26 14:36:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:36:34 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.026526421860705474
[09/26 14:36:40 visual_prompt]: Epoch 70 / 100: avg data time: 5.59e-02, avg batch time: 0.4689, average train loss: 0.0020
[09/26 14:36:42 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1583, average loss: 1.4492
[09/26 14:36:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.50	top5: 96.50	
[09/26 14:36:42 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.025000000000000012
[09/26 14:36:48 visual_prompt]: Epoch 71 / 100: avg data time: 5.91e-02, avg batch time: 0.4723, average train loss: 0.0023
[09/26 14:36:50 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1579, average loss: 1.4465
[09/26 14:36:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:36:50 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.02350403678833976
[09/26 14:36:56 visual_prompt]: Epoch 72 / 100: avg data time: 5.89e-02, avg batch time: 0.4725, average train loss: 0.0029
[09/26 14:36:58 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1581, average loss: 1.4517
[09/26 14:36:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.50	top5: 96.50	
[09/26 14:36:58 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.022040354826462667
[09/26 14:37:04 visual_prompt]: Epoch 73 / 100: avg data time: 4.81e-02, avg batch time: 0.4612, average train loss: 0.0019
[09/26 14:37:06 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1582, average loss: 1.4599
[09/26 14:37:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 96.50	
[09/26 14:37:06 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.02061073738537635
[09/26 14:37:12 visual_prompt]: Epoch 74 / 100: avg data time: 6.59e-02, avg batch time: 0.4786, average train loss: 0.0016
[09/26 14:37:14 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1580, average loss: 1.4475
[09/26 14:37:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 96.50	
[09/26 14:37:14 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.019216926233717086
[09/26 14:37:20 visual_prompt]: Epoch 75 / 100: avg data time: 6.09e-02, avg batch time: 0.4733, average train loss: 0.0013
[09/26 14:37:22 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1582, average loss: 1.4512
[09/26 14:37:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 96.50	
[09/26 14:37:22 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.017860619515673033
[09/26 14:37:28 visual_prompt]: Epoch 76 / 100: avg data time: 5.84e-02, avg batch time: 0.4719, average train loss: 0.0011
[09/26 14:37:30 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1580, average loss: 1.4591
[09/26 14:37:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 96.50	
[09/26 14:37:30 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.016543469682057107
[09/26 14:37:37 visual_prompt]: Epoch 77 / 100: avg data time: 6.79e-02, avg batch time: 0.4805, average train loss: 0.0016
[09/26 14:37:38 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1581, average loss: 1.4666
[09/26 14:37:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:37:38 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.015267081477050132
[09/26 14:37:45 visual_prompt]: Epoch 78 / 100: avg data time: 5.89e-02, avg batch time: 0.4737, average train loss: 0.0016
[09/26 14:37:46 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1580, average loss: 1.4723
[09/26 14:37:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 96.50	
[09/26 14:37:46 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.014033009983067453
[09/26 14:37:53 visual_prompt]: Epoch 79 / 100: avg data time: 6.41e-02, avg batch time: 0.4764, average train loss: 0.0018
[09/26 14:37:54 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1582, average loss: 1.4663
[09/26 14:37:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 96.50	
[09/26 14:37:54 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.012842758726130284
[09/26 14:38:01 visual_prompt]: Epoch 80 / 100: avg data time: 5.75e-02, avg batch time: 0.4723, average train loss: 0.0013
[09/26 14:38:02 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1583, average loss: 1.4630
[09/26 14:38:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 96.50	
[09/26 14:38:02 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.011697777844051106
[09/26 14:38:09 visual_prompt]: Epoch 81 / 100: avg data time: 6.97e-02, avg batch time: 0.4830, average train loss: 0.0012
[09/26 14:38:11 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1583, average loss: 1.4603
[09/26 14:38:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 97.00	
[09/26 14:38:11 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.010599462319663905
[09/26 14:38:17 visual_prompt]: Epoch 82 / 100: avg data time: 6.42e-02, avg batch time: 0.4764, average train loss: 0.0015
[09/26 14:38:19 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1577, average loss: 1.4644
[09/26 14:38:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:38:19 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.009549150281252633
[09/26 14:38:25 visual_prompt]: Epoch 83 / 100: avg data time: 5.38e-02, avg batch time: 0.4682, average train loss: 0.0012
[09/26 14:38:27 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1590, average loss: 1.4666
[09/26 14:38:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:38:27 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.008548121372247919
[09/26 14:38:33 visual_prompt]: Epoch 84 / 100: avg data time: 6.13e-02, avg batch time: 0.4767, average train loss: 0.0013
[09/26 14:38:35 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1583, average loss: 1.4707
[09/26 14:38:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:38:35 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.007597595192178702
[09/26 14:38:41 visual_prompt]: Epoch 85 / 100: avg data time: 6.63e-02, avg batch time: 0.4796, average train loss: 0.0015
[09/26 14:38:43 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1586, average loss: 1.4728
[09/26 14:38:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:38:43 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.006698729810778065
[09/26 14:38:50 visual_prompt]: Epoch 86 / 100: avg data time: 5.99e-02, avg batch time: 0.4736, average train loss: 0.0013
[09/26 14:38:51 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1582, average loss: 1.4745
[09/26 14:38:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:38:51 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.005852620357053651
[09/26 14:38:58 visual_prompt]: Epoch 87 / 100: avg data time: 6.28e-02, avg batch time: 0.4751, average train loss: 0.0014
[09/26 14:38:59 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1581, average loss: 1.4759
[09/26 14:38:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:38:59 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00506029768504166
[09/26 14:39:06 visual_prompt]: Epoch 88 / 100: avg data time: 6.10e-02, avg batch time: 0.4730, average train loss: 0.0015
[09/26 14:39:07 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1584, average loss: 1.4795
[09/26 14:39:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:39:07 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.004322727117869951
[09/26 14:39:14 visual_prompt]: Epoch 89 / 100: avg data time: 5.61e-02, avg batch time: 0.4697, average train loss: 0.0015
[09/26 14:39:15 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1577, average loss: 1.4846
[09/26 14:39:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:39:15 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0036408072716606343
[09/26 14:39:22 visual_prompt]: Epoch 90 / 100: avg data time: 5.54e-02, avg batch time: 0.4688, average train loss: 0.0010
[09/26 14:39:23 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1580, average loss: 1.4866
[09/26 14:39:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:39:23 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0030153689607045845
[09/26 14:39:30 visual_prompt]: Epoch 91 / 100: avg data time: 4.71e-02, avg batch time: 0.4599, average train loss: 0.0011
[09/26 14:39:31 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1582, average loss: 1.4886
[09/26 14:39:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:39:31 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0024471741852423235
[09/26 14:39:38 visual_prompt]: Epoch 92 / 100: avg data time: 6.04e-02, avg batch time: 0.4741, average train loss: 0.0010
[09/26 14:39:39 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1582, average loss: 1.4895
[09/26 14:39:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:39:39 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0019369152030840554
[09/26 14:39:46 visual_prompt]: Epoch 93 / 100: avg data time: 6.85e-02, avg batch time: 0.4807, average train loss: 0.0011
[09/26 14:39:48 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1585, average loss: 1.4901
[09/26 14:39:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:39:48 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0014852136862001765
[09/26 14:39:54 visual_prompt]: Epoch 94 / 100: avg data time: 6.17e-02, avg batch time: 0.4734, average train loss: 0.0013
[09/26 14:39:56 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1583, average loss: 1.4907
[09/26 14:39:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:39:56 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0010926199633097156
[09/26 14:40:02 visual_prompt]: Epoch 95 / 100: avg data time: 6.52e-02, avg batch time: 0.4768, average train loss: 0.0008
[09/26 14:40:04 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1580, average loss: 1.4910
[09/26 14:40:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:40:04 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0007596123493895991
[09/26 14:40:10 visual_prompt]: Epoch 96 / 100: avg data time: 6.31e-02, avg batch time: 0.4752, average train loss: 0.0013
[09/26 14:40:12 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1581, average loss: 1.4910
[09/26 14:40:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:40:12 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00048659656292148194
[09/26 14:40:18 visual_prompt]: Epoch 97 / 100: avg data time: 6.18e-02, avg batch time: 0.4741, average train loss: 0.0015
[09/26 14:40:20 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1584, average loss: 1.4911
[09/26 14:40:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:40:20 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0002739052315863355
[09/26 14:40:26 visual_prompt]: Epoch 98 / 100: avg data time: 5.64e-02, avg batch time: 0.4701, average train loss: 0.0011
[09/26 14:40:28 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1579, average loss: 1.4912
[09/26 14:40:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:40:28 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.00012179748700879012
[09/26 14:40:34 visual_prompt]: Epoch 99 / 100: avg data time: 5.06e-02, avg batch time: 0.4638, average train loss: 0.0010
[09/26 14:40:36 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1582, average loss: 1.4913
[09/26 14:40:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:40:36 visual_prompt]: Training 100 / 100 epoch, with learning rate 3.0458649045211895e-05
[09/26 14:40:42 visual_prompt]: Epoch 100 / 100: avg data time: 5.32e-02, avg batch time: 0.4662, average train loss: 0.0012
[09/26 14:40:44 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1583, average loss: 1.4913
[09/26 14:40:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 96.50	
[09/26 14:40:44 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 14:40:44 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 14:40:44 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 14:40:44 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 14:40:44 visual_prompt]: Training with config:
[09/26 14:40:44 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.05_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 14:40:44 visual_prompt]: Loading training data...
[09/26 14:40:44 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 14:40:45 visual_prompt]: Number of images: 800
[09/26 14:40:45 visual_prompt]: Number of classes: 10 / 10
[09/26 14:40:45 visual_prompt]: Loading validation data...
[09/26 14:40:45 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 14:40:45 visual_prompt]: Number of images: 200
[09/26 14:40:45 visual_prompt]: Number of classes: 10 / 10
[09/26 14:40:45 visual_prompt]: Constructing models...
[09/26 14:40:48 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 14:40:48 visual_prompt]: tuned percent:0.543
[09/26 14:40:48 visual_prompt]: Device used for model: 0
[09/26 14:40:48 visual_prompt]: Setting up Evaluator...
[09/26 14:40:48 visual_prompt]: Setting up Trainer...
[09/26 14:40:48 visual_prompt]: 	Setting up the optimizer...
[09/26 14:40:48 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 14:40:54 visual_prompt]: Epoch 1 / 100: avg data time: 6.11e-02, avg batch time: 0.4789, average train loss: 2.6705
[09/26 14:40:56 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1579, average loss: 2.6214
[09/26 14:40:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 14:40:56 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 14:40:56 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[09/26 14:41:02 visual_prompt]: Epoch 2 / 100: avg data time: 5.75e-02, avg batch time: 0.4696, average train loss: 2.3492
[09/26 14:41:04 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1585, average loss: 2.2302
[09/26 14:41:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 14:41:04 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[09/26 14:41:10 visual_prompt]: Epoch 3 / 100: avg data time: 4.62e-02, avg batch time: 0.4590, average train loss: 2.2584
[09/26 14:41:12 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1581, average loss: 2.2164
[09/26 14:41:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.50	
[09/26 14:41:12 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[09/26 14:41:18 visual_prompt]: Epoch 4 / 100: avg data time: 5.43e-02, avg batch time: 0.4657, average train loss: 2.2562
[09/26 14:41:20 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1583, average loss: 2.2220
[09/26 14:41:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 14:41:20 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[09/26 14:41:26 visual_prompt]: Epoch 5 / 100: avg data time: 6.71e-02, avg batch time: 0.4793, average train loss: 2.2593
[09/26 14:41:28 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1581, average loss: 2.2331
[09/26 14:41:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 14:41:28 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[09/26 14:41:34 visual_prompt]: Epoch 6 / 100: avg data time: 6.32e-02, avg batch time: 0.4747, average train loss: 2.2534
[09/26 14:41:36 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.1581, average loss: 2.2149
[09/26 14:41:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 14:41:36 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[09/26 14:41:42 visual_prompt]: Epoch 7 / 100: avg data time: 5.97e-02, avg batch time: 0.4727, average train loss: 2.2544
[09/26 14:41:44 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1581, average loss: 2.2335
[09/26 14:41:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.00	
[09/26 14:41:44 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[09/26 14:41:50 visual_prompt]: Epoch 8 / 100: avg data time: 5.00e-02, avg batch time: 0.4646, average train loss: 2.2588
[09/26 14:41:52 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1580, average loss: 2.2090
[09/26 14:41:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 14:41:52 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[09/26 14:41:58 visual_prompt]: Epoch 9 / 100: avg data time: 6.03e-02, avg batch time: 0.4722, average train loss: 2.2379
[09/26 14:42:00 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1581, average loss: 2.2185
[09/26 14:42:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 14:42:00 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[09/26 14:42:06 visual_prompt]: Epoch 10 / 100: avg data time: 6.21e-02, avg batch time: 0.4747, average train loss: 2.2428
[09/26 14:42:08 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1579, average loss: 2.2489
[09/26 14:42:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 12.00	top5: 65.00	
[09/26 14:42:08 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[09/26 14:42:15 visual_prompt]: Epoch 11 / 100: avg data time: 6.11e-02, avg batch time: 0.4740, average train loss: 2.2607
[09/26 14:42:16 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1581, average loss: 2.2123
[09/26 14:42:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 21.00	top5: 64.50	
[09/26 14:42:16 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[09/26 14:42:23 visual_prompt]: Epoch 12 / 100: avg data time: 6.03e-02, avg batch time: 0.4725, average train loss: 2.2393
[09/26 14:42:24 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1581, average loss: 2.2027
[09/26 14:42:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 14:42:24 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[09/26 14:42:31 visual_prompt]: Epoch 13 / 100: avg data time: 6.08e-02, avg batch time: 0.4739, average train loss: 2.2316
[09/26 14:42:32 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 2.1909
[09/26 14:42:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.00	
[09/26 14:42:32 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[09/26 14:42:39 visual_prompt]: Epoch 14 / 100: avg data time: 6.15e-02, avg batch time: 0.4738, average train loss: 2.2034
[09/26 14:42:40 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1582, average loss: 2.1809
[09/26 14:42:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 68.00	
[09/26 14:42:40 visual_prompt]: Best epoch 14: best metric: 0.245
[09/26 14:42:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[09/26 14:42:47 visual_prompt]: Epoch 15 / 100: avg data time: 5.04e-02, avg batch time: 0.4656, average train loss: 2.2010
[09/26 14:42:48 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1580, average loss: 2.1608
[09/26 14:42:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 26.00	top5: 67.00	
[09/26 14:42:48 visual_prompt]: Best epoch 15: best metric: 0.260
[09/26 14:42:48 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[09/26 14:42:55 visual_prompt]: Epoch 16 / 100: avg data time: 6.49e-02, avg batch time: 0.4769, average train loss: 2.1758
[09/26 14:42:56 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1582, average loss: 2.1316
[09/26 14:42:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.50	top5: 68.50	
[09/26 14:42:56 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[09/26 14:43:03 visual_prompt]: Epoch 17 / 100: avg data time: 5.51e-02, avg batch time: 0.4678, average train loss: 2.1139
[09/26 14:43:04 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1583, average loss: 2.3856
[09/26 14:43:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 66.50	
[09/26 14:43:04 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[09/26 14:43:11 visual_prompt]: Epoch 18 / 100: avg data time: 5.71e-02, avg batch time: 0.4713, average train loss: 2.1647
[09/26 14:43:12 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1587, average loss: 2.0753
[09/26 14:43:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.50	top5: 72.50	
[09/26 14:43:12 visual_prompt]: Best epoch 18: best metric: 0.295
[09/26 14:43:12 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[09/26 14:43:19 visual_prompt]: Epoch 19 / 100: avg data time: 5.26e-02, avg batch time: 0.4666, average train loss: 2.0444
[09/26 14:43:21 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.1586, average loss: 1.7850
[09/26 14:43:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 37.50	top5: 87.50	
[09/26 14:43:21 visual_prompt]: Best epoch 19: best metric: 0.375
[09/26 14:43:21 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[09/26 14:43:27 visual_prompt]: Epoch 20 / 100: avg data time: 5.05e-02, avg batch time: 0.4645, average train loss: 1.9854
[09/26 14:43:29 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1579, average loss: 1.7823
[09/26 14:43:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 39.50	top5: 82.50	
[09/26 14:43:29 visual_prompt]: Best epoch 20: best metric: 0.395
[09/26 14:43:29 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[09/26 14:43:35 visual_prompt]: Epoch 21 / 100: avg data time: 6.43e-02, avg batch time: 0.4768, average train loss: 1.8153
[09/26 14:43:37 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1588, average loss: 1.7674
[09/26 14:43:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 35.00	top5: 83.50	
[09/26 14:43:37 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[09/26 14:43:43 visual_prompt]: Epoch 22 / 100: avg data time: 6.13e-02, avg batch time: 0.4740, average train loss: 1.7092
[09/26 14:43:45 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1588, average loss: 1.4418
[09/26 14:43:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 45.00	top5: 92.50	
[09/26 14:43:45 visual_prompt]: Best epoch 22: best metric: 0.450
[09/26 14:43:45 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[09/26 14:43:51 visual_prompt]: Epoch 23 / 100: avg data time: 5.66e-02, avg batch time: 0.4719, average train loss: 1.7224
[09/26 14:43:53 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1579, average loss: 1.8156
[09/26 14:43:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 39.00	top5: 79.50	
[09/26 14:43:53 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[09/26 14:43:59 visual_prompt]: Epoch 24 / 100: avg data time: 6.28e-02, avg batch time: 0.4761, average train loss: 1.7037
[09/26 14:44:01 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1582, average loss: 1.5566
[09/26 14:44:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 46.50	top5: 88.50	
[09/26 14:44:01 visual_prompt]: Best epoch 24: best metric: 0.465
[09/26 14:44:01 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[09/26 14:44:07 visual_prompt]: Epoch 25 / 100: avg data time: 6.27e-02, avg batch time: 0.4766, average train loss: 1.5345
[09/26 14:44:09 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.1583, average loss: 1.3439
[09/26 14:44:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 47.50	top5: 92.50	
[09/26 14:44:09 visual_prompt]: Best epoch 25: best metric: 0.475
[09/26 14:44:09 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[09/26 14:44:15 visual_prompt]: Epoch 26 / 100: avg data time: 5.79e-02, avg batch time: 0.4715, average train loss: 1.3848
[09/26 14:44:17 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.1584, average loss: 1.3333
[09/26 14:44:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 53.50	top5: 94.00	
[09/26 14:44:17 visual_prompt]: Best epoch 26: best metric: 0.535
[09/26 14:44:17 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.04620120240391065
[09/26 14:44:24 visual_prompt]: Epoch 27 / 100: avg data time: 6.23e-02, avg batch time: 0.4763, average train loss: 1.3213
[09/26 14:44:25 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1585, average loss: 1.4153
[09/26 14:44:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 53.00	top5: 92.00	
[09/26 14:44:25 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.04572593931387604
[09/26 14:44:32 visual_prompt]: Epoch 28 / 100: avg data time: 6.32e-02, avg batch time: 0.4767, average train loss: 1.2042
[09/26 14:44:33 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1584, average loss: 1.0573
[09/26 14:44:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 63.00	top5: 95.00	
[09/26 14:44:33 visual_prompt]: Best epoch 28: best metric: 0.630
[09/26 14:44:33 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.04522542485937369
[09/26 14:44:40 visual_prompt]: Epoch 29 / 100: avg data time: 5.56e-02, avg batch time: 0.4694, average train loss: 1.1733
[09/26 14:44:41 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.1588, average loss: 1.1553
[09/26 14:44:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 56.50	top5: 96.50	
[09/26 14:44:41 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.04470026884016805
[09/26 14:44:48 visual_prompt]: Epoch 30 / 100: avg data time: 5.73e-02, avg batch time: 0.4709, average train loss: 1.0304
[09/26 14:44:49 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1585, average loss: 1.5065
[09/26 14:44:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 49.00	top5: 91.00	
[09/26 14:44:49 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.04415111107797445
[09/26 14:44:56 visual_prompt]: Epoch 31 / 100: avg data time: 6.54e-02, avg batch time: 0.4789, average train loss: 1.1390
[09/26 14:44:58 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1582, average loss: 1.1686
[09/26 14:44:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 59.50	top5: 93.50	
[09/26 14:44:58 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.04357862063693486
[09/26 14:45:04 visual_prompt]: Epoch 32 / 100: avg data time: 6.07e-02, avg batch time: 0.4739, average train loss: 1.0273
[09/26 14:45:06 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1585, average loss: 0.9606
[09/26 14:45:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.00	top5: 96.50	
[09/26 14:45:06 visual_prompt]: Best epoch 32: best metric: 0.680
[09/26 14:45:06 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.04298349500846628
[09/26 14:45:12 visual_prompt]: Epoch 33 / 100: avg data time: 5.82e-02, avg batch time: 0.4721, average train loss: 0.9401
[09/26 14:45:14 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1580, average loss: 0.9530
[09/26 14:45:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.00	top5: 95.00	
[09/26 14:45:14 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.042366459261474934
[09/26 14:45:20 visual_prompt]: Epoch 34 / 100: avg data time: 6.27e-02, avg batch time: 0.4758, average train loss: 0.8712
[09/26 14:45:22 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1586, average loss: 0.8909
[09/26 14:45:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.50	top5: 95.50	
[09/26 14:45:22 visual_prompt]: Best epoch 34: best metric: 0.745
[09/26 14:45:22 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.04172826515897146
[09/26 14:45:28 visual_prompt]: Epoch 35 / 100: avg data time: 5.88e-02, avg batch time: 0.4738, average train loss: 0.7308
[09/26 14:45:30 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1584, average loss: 0.7946
[09/26 14:45:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.00	top5: 97.00	
[09/26 14:45:30 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.04106969024216348
[09/26 14:45:36 visual_prompt]: Epoch 36 / 100: avg data time: 5.23e-02, avg batch time: 0.4667, average train loss: 0.7318
[09/26 14:45:38 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1583, average loss: 0.9890
[09/26 14:45:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 61.00	top5: 95.50	
[09/26 14:45:38 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.040391536883141455
[09/26 14:45:44 visual_prompt]: Epoch 37 / 100: avg data time: 6.23e-02, avg batch time: 0.4751, average train loss: 0.6616
[09/26 14:45:46 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1579, average loss: 0.7314
[09/26 14:45:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 97.50	
[09/26 14:45:46 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.03969463130731183
[09/26 14:45:52 visual_prompt]: Epoch 38 / 100: avg data time: 5.28e-02, avg batch time: 0.4678, average train loss: 0.5721
[09/26 14:45:54 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1581, average loss: 0.7817
[09/26 14:45:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.00	top5: 98.00	
[09/26 14:45:54 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.03897982258676867
[09/26 14:46:00 visual_prompt]: Epoch 39 / 100: avg data time: 5.98e-02, avg batch time: 0.4737, average train loss: 0.4902
[09/26 14:46:02 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1583, average loss: 0.7778
[09/26 14:46:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 96.50	
[09/26 14:46:02 visual_prompt]: Best epoch 39: best metric: 0.750
[09/26 14:46:02 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.03824798160583012
[09/26 14:46:08 visual_prompt]: Epoch 40 / 100: avg data time: 6.02e-02, avg batch time: 0.4747, average train loss: 0.6479
[09/26 14:46:10 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1581, average loss: 0.7417
[09/26 14:46:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 96.50	
[09/26 14:46:10 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.037500000000000006
[09/26 14:46:17 visual_prompt]: Epoch 41 / 100: avg data time: 6.08e-02, avg batch time: 0.4739, average train loss: 0.7619
[09/26 14:46:18 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1584, average loss: 0.8137
[09/26 14:46:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.50	
[09/26 14:46:18 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.03673678906964727
[09/26 14:46:25 visual_prompt]: Epoch 42 / 100: avg data time: 5.93e-02, avg batch time: 0.4722, average train loss: 0.6302
[09/26 14:46:26 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1581, average loss: 1.0337
[09/26 14:46:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 66.50	top5: 95.00	
[09/26 14:46:26 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.03595927866972694
[09/26 14:46:33 visual_prompt]: Epoch 43 / 100: avg data time: 6.42e-02, avg batch time: 0.4783, average train loss: 0.6277
[09/26 14:46:34 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1583, average loss: 0.5987
[09/26 14:46:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 98.00	
[09/26 14:46:34 visual_prompt]: Best epoch 43: best metric: 0.805
[09/26 14:46:34 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.03516841607689501
[09/26 14:46:41 visual_prompt]: Epoch 44 / 100: avg data time: 6.84e-02, avg batch time: 0.4812, average train loss: 0.4959
[09/26 14:46:42 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1582, average loss: 0.7252
[09/26 14:46:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.50	top5: 97.50	
[09/26 14:46:42 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0343651648353978
[09/26 14:46:49 visual_prompt]: Epoch 45 / 100: avg data time: 6.30e-02, avg batch time: 0.4754, average train loss: 0.5932
[09/26 14:46:51 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1579, average loss: 0.7801
[09/26 14:46:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.00	top5: 94.50	
[09/26 14:46:51 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.033550503583141725
[09/26 14:46:57 visual_prompt]: Epoch 46 / 100: avg data time: 6.13e-02, avg batch time: 0.4738, average train loss: 1.5790
[09/26 14:46:59 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1582, average loss: 1.8640
[09/26 14:46:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 38.50	top5: 86.50	
[09/26 14:46:59 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.032725424859373686
[09/26 14:47:05 visual_prompt]: Epoch 47 / 100: avg data time: 5.28e-02, avg batch time: 0.4658, average train loss: 1.3032
[09/26 14:47:07 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1581, average loss: 1.0669
[09/26 14:47:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 62.00	top5: 94.50	
[09/26 14:47:07 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.03189093389542498
[09/26 14:47:13 visual_prompt]: Epoch 48 / 100: avg data time: 5.91e-02, avg batch time: 0.4731, average train loss: 0.6672
[09/26 14:47:15 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1586, average loss: 0.6624
[09/26 14:47:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 97.50	
[09/26 14:47:15 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.03104804738999169
[09/26 14:47:21 visual_prompt]: Epoch 49 / 100: avg data time: 5.77e-02, avg batch time: 0.4704, average train loss: 0.4595
[09/26 14:47:23 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1582, average loss: 0.6555
[09/26 14:47:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 97.00	
[09/26 14:47:23 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.03019779227044398
[09/26 14:47:29 visual_prompt]: Epoch 50 / 100: avg data time: 6.79e-02, avg batch time: 0.4815, average train loss: 0.5203
[09/26 14:47:31 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1584, average loss: 0.9313
[09/26 14:47:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 93.00	
[09/26 14:47:31 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.029341204441673263
[09/26 14:47:37 visual_prompt]: Epoch 51 / 100: avg data time: 6.11e-02, avg batch time: 0.4751, average train loss: 0.5039
[09/26 14:47:39 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1580, average loss: 0.6285
[09/26 14:47:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 79.50	top5: 97.50	
[09/26 14:47:39 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.028479327524001636
[09/26 14:47:45 visual_prompt]: Epoch 52 / 100: avg data time: 5.53e-02, avg batch time: 0.4680, average train loss: 0.3770
[09/26 14:47:47 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1582, average loss: 0.7111
[09/26 14:47:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 97.50	
[09/26 14:47:47 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.02761321158169134
[09/26 14:47:54 visual_prompt]: Epoch 53 / 100: avg data time: 5.37e-02, avg batch time: 0.4677, average train loss: 0.2837
[09/26 14:47:55 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1584, average loss: 0.6471
[09/26 14:47:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.00	top5: 98.00	
[09/26 14:47:55 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.02674391184360313
[09/26 14:48:02 visual_prompt]: Epoch 54 / 100: avg data time: 5.42e-02, avg batch time: 0.4680, average train loss: 0.2726
[09/26 14:48:03 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1585, average loss: 0.5886
[09/26 14:48:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 97.00	
[09/26 14:48:03 visual_prompt]: Best epoch 54: best metric: 0.815
[09/26 14:48:03 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.02587248741756253
[09/26 14:48:10 visual_prompt]: Epoch 55 / 100: avg data time: 6.45e-02, avg batch time: 0.4781, average train loss: 0.2787
[09/26 14:48:11 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1582, average loss: 0.5739
[09/26 14:48:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 98.00	
[09/26 14:48:11 visual_prompt]: Best epoch 55: best metric: 0.840
[09/26 14:48:11 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.025
[09/26 14:48:18 visual_prompt]: Epoch 56 / 100: avg data time: 4.98e-02, avg batch time: 0.4630, average train loss: 0.2934
[09/26 14:48:19 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1583, average loss: 0.6953
[09/26 14:48:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.50	top5: 98.00	
[09/26 14:48:19 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.024127512582437484
[09/26 14:48:26 visual_prompt]: Epoch 57 / 100: avg data time: 6.05e-02, avg batch time: 0.4742, average train loss: 0.2031
[09/26 14:48:27 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1583, average loss: 0.6295
[09/26 14:48:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 99.50	
[09/26 14:48:27 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.02325608815639687
[09/26 14:48:34 visual_prompt]: Epoch 58 / 100: avg data time: 5.87e-02, avg batch time: 0.4725, average train loss: 0.1548
[09/26 14:48:35 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1583, average loss: 0.6971
[09/26 14:48:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 78.00	top5: 98.50	
[09/26 14:48:35 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.02238678841830867
[09/26 14:48:42 visual_prompt]: Epoch 59 / 100: avg data time: 6.04e-02, avg batch time: 0.4733, average train loss: 0.1752
[09/26 14:48:44 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1583, average loss: 0.5979
[09/26 14:48:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 96.50	
[09/26 14:48:44 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.02152067247599837
[09/26 14:48:50 visual_prompt]: Epoch 60 / 100: avg data time: 6.52e-02, avg batch time: 0.4815, average train loss: 0.1294
[09/26 14:48:52 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1582, average loss: 0.6489
[09/26 14:48:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.50	
[09/26 14:48:52 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.020658795558326743
[09/26 14:48:58 visual_prompt]: Epoch 61 / 100: avg data time: 6.34e-02, avg batch time: 0.4768, average train loss: 0.1484
[09/26 14:49:00 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1583, average loss: 0.5590
[09/26 14:49:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.00	
[09/26 14:49:00 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.019802207729556022
[09/26 14:49:06 visual_prompt]: Epoch 62 / 100: avg data time: 6.53e-02, avg batch time: 0.4782, average train loss: 0.1397
[09/26 14:49:08 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1579, average loss: 0.6112
[09/26 14:49:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 99.00	
[09/26 14:49:08 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.018951952610008312
[09/26 14:49:14 visual_prompt]: Epoch 63 / 100: avg data time: 5.01e-02, avg batch time: 0.4641, average train loss: 0.1521
[09/26 14:49:16 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1582, average loss: 0.4891
[09/26 14:49:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 98.00	
[09/26 14:49:16 visual_prompt]: Best epoch 63: best metric: 0.855
[09/26 14:49:16 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.018109066104575022
[09/26 14:49:23 visual_prompt]: Epoch 64 / 100: avg data time: 6.35e-02, avg batch time: 0.4758, average train loss: 0.1001
[09/26 14:49:24 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1578, average loss: 0.7870
[09/26 14:49:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 77.00	top5: 97.00	
[09/26 14:49:24 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.017274575140626316
[09/26 14:49:31 visual_prompt]: Epoch 65 / 100: avg data time: 5.56e-02, avg batch time: 0.4690, average train loss: 0.1363
[09/26 14:49:32 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1581, average loss: 0.6009
[09/26 14:49:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.50	top5: 98.50	
[09/26 14:49:32 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.016449496416858285
[09/26 14:49:39 visual_prompt]: Epoch 66 / 100: avg data time: 6.35e-02, avg batch time: 0.4759, average train loss: 0.1113
[09/26 14:49:40 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1585, average loss: 0.6856
[09/26 14:49:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 80.50	top5: 97.00	
[09/26 14:49:40 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0156348351646022
[09/26 14:49:47 visual_prompt]: Epoch 67 / 100: avg data time: 5.65e-02, avg batch time: 0.4704, average train loss: 0.0870
[09/26 14:49:48 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1581, average loss: 0.6004
[09/26 14:49:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 97.00	
[09/26 14:49:48 visual_prompt]: Best epoch 67: best metric: 0.860
[09/26 14:49:48 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.014831583923105
[09/26 14:49:55 visual_prompt]: Epoch 68 / 100: avg data time: 5.76e-02, avg batch time: 0.4724, average train loss: 0.0508
[09/26 14:49:56 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1585, average loss: 0.6991
[09/26 14:49:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 81.00	top5: 98.00	
[09/26 14:49:56 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.014040721330273063
[09/26 14:50:03 visual_prompt]: Epoch 69 / 100: avg data time: 5.73e-02, avg batch time: 0.4709, average train loss: 0.0394
[09/26 14:50:04 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1584, average loss: 0.6163
[09/26 14:50:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.50	top5: 98.00	
[09/26 14:50:04 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.013263210930352737
[09/26 14:50:11 visual_prompt]: Epoch 70 / 100: avg data time: 5.85e-02, avg batch time: 0.4718, average train loss: 0.0287
[09/26 14:50:12 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1580, average loss: 0.7194
[09/26 14:50:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 83.00	top5: 97.50	
[09/26 14:50:12 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.012500000000000006
[09/26 14:50:19 visual_prompt]: Epoch 71 / 100: avg data time: 6.11e-02, avg batch time: 0.4735, average train loss: 0.0241
[09/26 14:50:20 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1581, average loss: 0.6262
[09/26 14:50:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.00	top5: 98.00	
[09/26 14:50:20 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.01175201839416988
[09/26 14:50:27 visual_prompt]: Epoch 72 / 100: avg data time: 5.90e-02, avg batch time: 0.4721, average train loss: 0.0206
[09/26 14:50:29 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1588, average loss: 0.6183
[09/26 14:50:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 97.50	
[09/26 14:50:29 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.011020177413231334
[09/26 14:50:35 visual_prompt]: Epoch 73 / 100: avg data time: 6.40e-02, avg batch time: 0.4763, average train loss: 0.0181
[09/26 14:50:37 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1586, average loss: 0.6220
[09/26 14:50:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 98.00	
[09/26 14:50:37 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.010305368692688175
[09/26 14:50:43 visual_prompt]: Epoch 74 / 100: avg data time: 6.48e-02, avg batch time: 0.4768, average train loss: 0.0176
[09/26 14:50:45 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1582, average loss: 0.6215
[09/26 14:50:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 87.00	top5: 98.00	
[09/26 14:50:45 visual_prompt]: Best epoch 74: best metric: 0.870
[09/26 14:50:45 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.009608463116858543
[09/26 14:50:51 visual_prompt]: Epoch 75 / 100: avg data time: 5.20e-02, avg batch time: 0.4656, average train loss: 0.0172
[09/26 14:50:53 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1585, average loss: 0.6172
[09/26 14:50:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 98.00	
[09/26 14:50:53 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.008930309757836517
[09/26 14:50:59 visual_prompt]: Epoch 76 / 100: avg data time: 5.70e-02, avg batch time: 0.4710, average train loss: 0.0171
[09/26 14:51:01 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1582, average loss: 0.6184
[09/26 14:51:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 98.00	
[09/26 14:51:01 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.008271734841028553
[09/26 14:51:07 visual_prompt]: Epoch 77 / 100: avg data time: 6.37e-02, avg batch time: 0.4757, average train loss: 0.0164
[09/26 14:51:09 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1579, average loss: 0.6179
[09/26 14:51:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 97.50	
[09/26 14:51:09 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.007633540738525066
[09/26 14:51:15 visual_prompt]: Epoch 78 / 100: avg data time: 4.99e-02, avg batch time: 0.4639, average train loss: 0.0169
[09/26 14:51:17 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1582, average loss: 0.6187
[09/26 14:51:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 97.50	
[09/26 14:51:17 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.0070165049915337264
[09/26 14:51:24 visual_prompt]: Epoch 79 / 100: avg data time: 6.97e-02, avg batch time: 0.4833, average train loss: 0.0160
[09/26 14:51:25 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1577, average loss: 0.6189
[09/26 14:51:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 97.50	
[09/26 14:51:25 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.006421379363065142
[09/26 14:51:32 visual_prompt]: Epoch 80 / 100: avg data time: 5.58e-02, avg batch time: 0.4706, average train loss: 0.0156
[09/26 14:51:33 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1586, average loss: 0.6244
[09/26 14:51:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 97.50	
[09/26 14:51:33 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.005848888922025553
[09/26 14:51:40 visual_prompt]: Epoch 81 / 100: avg data time: 5.91e-02, avg batch time: 0.4715, average train loss: 0.0158
[09/26 14:51:41 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1581, average loss: 0.6319
[09/26 14:51:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 97.50	
[09/26 14:51:41 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.005299731159831953
[09/26 14:51:48 visual_prompt]: Epoch 82 / 100: avg data time: 5.01e-02, avg batch time: 0.4631, average train loss: 0.0155
[09/26 14:51:49 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1574, average loss: 0.6353
[09/26 14:51:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 97.50	
[09/26 14:51:49 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.004774575140626317
[09/26 14:51:56 visual_prompt]: Epoch 83 / 100: avg data time: 5.73e-02, avg batch time: 0.4699, average train loss: 0.0156
[09/26 14:51:57 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1580, average loss: 0.6361
[09/26 14:51:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 97.50	
[09/26 14:51:57 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.004274060686123959
[09/26 14:52:04 visual_prompt]: Epoch 84 / 100: avg data time: 5.47e-02, avg batch time: 0.4670, average train loss: 0.0154
[09/26 14:52:05 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1582, average loss: 0.6397
[09/26 14:52:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 84.50	top5: 97.50	
[09/26 14:52:05 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.003798797596089351
[09/26 14:52:12 visual_prompt]: Epoch 85 / 100: avg data time: 5.32e-02, avg batch time: 0.4669, average train loss: 0.0155
[09/26 14:52:13 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1580, average loss: 0.6407
[09/26 14:52:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 97.50	
[09/26 14:52:13 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.0033493649053890325
[09/26 14:52:20 visual_prompt]: Epoch 86 / 100: avg data time: 6.50e-02, avg batch time: 0.4772, average train loss: 0.0155
[09/26 14:52:21 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1584, average loss: 0.6386
[09/26 14:52:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.50	top5: 97.50	
[09/26 14:52:21 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.0029263101785268254
[09/26 14:52:28 visual_prompt]: Epoch 87 / 100: avg data time: 5.65e-02, avg batch time: 0.4698, average train loss: 0.0154
[09/26 14:52:30 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 0.6392
[09/26 14:52:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.00	top5: 98.00	
[09/26 14:52:30 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00253014884252083
[09/26 14:52:36 visual_prompt]: Epoch 88 / 100: avg data time: 5.96e-02, avg batch time: 0.4730, average train loss: 0.0154
[09/26 14:52:38 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1580, average loss: 0.6431
[09/26 14:52:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 98.00	
[09/26 14:52:38 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.0021613635589349755
[09/26 14:52:44 visual_prompt]: Epoch 89 / 100: avg data time: 5.60e-02, avg batch time: 0.4702, average train loss: 0.0152
[09/26 14:52:46 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.1581, average loss: 0.6436
[09/26 14:52:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 86.50	top5: 98.00	
[09/26 14:52:46 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0018204036358303172
[09/26 14:52:52 visual_prompt]: Epoch 90 / 100: avg data time: 6.10e-02, avg batch time: 0.4740, average train loss: 0.0154
[09/26 14:52:54 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1581, average loss: 0.6397
[09/26 14:52:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 97.50	
[09/26 14:52:54 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0015076844803522923
[09/26 14:53:00 visual_prompt]: Epoch 91 / 100: avg data time: 5.38e-02, avg batch time: 0.4664, average train loss: 0.0159
[09/26 14:53:02 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1587, average loss: 0.6378
[09/26 14:53:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 97.50	
[09/26 14:53:02 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0012235870926211618
[09/26 14:53:08 visual_prompt]: Epoch 92 / 100: avg data time: 6.31e-02, avg batch time: 0.4775, average train loss: 0.0154
[09/26 14:53:10 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1583, average loss: 0.6424
[09/26 14:53:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 98.00	
[09/26 14:53:10 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0009684576015420277
[09/26 14:53:16 visual_prompt]: Epoch 93 / 100: avg data time: 4.90e-02, avg batch time: 0.4617, average train loss: 0.0156
[09/26 14:53:18 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1583, average loss: 0.6442
[09/26 14:53:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.50	top5: 97.50	
[09/26 14:53:18 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0007426068431000883
[09/26 14:53:24 visual_prompt]: Epoch 94 / 100: avg data time: 5.97e-02, avg batch time: 0.4724, average train loss: 0.0152
[09/26 14:53:26 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1576, average loss: 0.6451
[09/26 14:53:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 97.50	
[09/26 14:53:26 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0005463099816548578
[09/26 14:53:32 visual_prompt]: Epoch 95 / 100: avg data time: 6.38e-02, avg batch time: 0.4764, average train loss: 0.0153
[09/26 14:53:34 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1575, average loss: 0.6450
[09/26 14:53:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 97.50	
[09/26 14:53:34 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00037980617469479954
[09/26 14:53:40 visual_prompt]: Epoch 96 / 100: avg data time: 5.33e-02, avg batch time: 0.4687, average train loss: 0.0154
[09/26 14:53:42 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.1581, average loss: 0.6446
[09/26 14:53:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 97.50	
[09/26 14:53:42 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00024329828146074097
[09/26 14:53:49 visual_prompt]: Epoch 97 / 100: avg data time: 6.41e-02, avg batch time: 0.4774, average train loss: 0.0154
[09/26 14:53:50 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1585, average loss: 0.6445
[09/26 14:53:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 97.50	
[09/26 14:53:50 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.00013695261579316776
[09/26 14:53:57 visual_prompt]: Epoch 98 / 100: avg data time: 6.06e-02, avg batch time: 0.4742, average train loss: 0.0154
[09/26 14:53:58 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1583, average loss: 0.6443
[09/26 14:53:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 97.50	
[09/26 14:53:58 visual_prompt]: Training 99 / 100 epoch, with learning rate 6.089874350439506e-05
[09/26 14:54:05 visual_prompt]: Epoch 99 / 100: avg data time: 6.32e-02, avg batch time: 0.4759, average train loss: 0.0153
[09/26 14:54:06 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1585, average loss: 0.6443
[09/26 14:54:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 97.50	
[09/26 14:54:06 visual_prompt]: Training 100 / 100 epoch, with learning rate 1.5229324522605948e-05
[09/26 14:54:13 visual_prompt]: Epoch 100 / 100: avg data time: 5.07e-02, avg batch time: 0.4643, average train loss: 0.0156
[09/26 14:54:14 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1586, average loss: 0.6443
[09/26 14:54:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 85.00	top5: 97.50	
[09/26 14:54:14 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 14:54:14 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 14:54:14 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 14:54:14 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 14:54:14 visual_prompt]: Training with config:
[09/26 14:54:14 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.05_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 14:54:14 visual_prompt]: Loading training data...
[09/26 14:54:14 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 14:54:15 visual_prompt]: Number of images: 800
[09/26 14:54:15 visual_prompt]: Number of classes: 10 / 10
[09/26 14:54:15 visual_prompt]: Loading validation data...
[09/26 14:54:15 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 14:54:16 visual_prompt]: Number of images: 200
[09/26 14:54:16 visual_prompt]: Number of classes: 10 / 10
[09/26 14:54:16 visual_prompt]: Constructing models...
[09/26 14:54:18 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 14:54:18 visual_prompt]: tuned percent:0.543
[09/26 14:54:18 visual_prompt]: Device used for model: 0
[09/26 14:54:18 visual_prompt]: Setting up Evaluator...
[09/26 14:54:18 visual_prompt]: Setting up Trainer...
[09/26 14:54:18 visual_prompt]: 	Setting up the optimizer...
[09/26 14:54:18 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 14:54:25 visual_prompt]: Epoch 1 / 100: avg data time: 6.43e-02, avg batch time: 0.4844, average train loss: 2.6777
[09/26 14:54:26 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1583, average loss: 2.6214
[09/26 14:54:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 14:54:26 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 14:54:26 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[09/26 14:54:33 visual_prompt]: Epoch 2 / 100: avg data time: 6.07e-02, avg batch time: 0.4738, average train loss: 2.3955
[09/26 14:54:34 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1576, average loss: 2.2408
[09/26 14:54:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 14:54:34 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[09/26 14:54:41 visual_prompt]: Epoch 3 / 100: avg data time: 6.02e-02, avg batch time: 0.4740, average train loss: 2.2567
[09/26 14:54:43 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.1583, average loss: 2.2333
[09/26 14:54:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.00	
[09/26 14:54:43 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[09/26 14:54:49 visual_prompt]: Epoch 4 / 100: avg data time: 6.01e-02, avg batch time: 0.4734, average train loss: 2.2548
[09/26 14:54:51 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1583, average loss: 2.2212
[09/26 14:54:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 14:54:51 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[09/26 14:54:57 visual_prompt]: Epoch 5 / 100: avg data time: 7.01e-02, avg batch time: 0.4819, average train loss: 2.2581
[09/26 14:54:59 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1583, average loss: 2.2126
[09/26 14:54:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 14:54:59 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[09/26 14:55:05 visual_prompt]: Epoch 6 / 100: avg data time: 6.44e-02, avg batch time: 0.4786, average train loss: 2.2556
[09/26 14:55:07 visual_prompt]: Inference (val):avg data time: 4.63e-05, avg batch time: 0.1579, average loss: 2.2250
[09/26 14:55:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 14:55:07 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[09/26 14:55:13 visual_prompt]: Epoch 7 / 100: avg data time: 5.75e-02, avg batch time: 0.4707, average train loss: 2.2431
[09/26 14:55:15 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1583, average loss: 2.2284
[09/26 14:55:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 14:55:15 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[09/26 14:55:22 visual_prompt]: Epoch 8 / 100: avg data time: 6.51e-02, avg batch time: 0.4772, average train loss: 2.2791
[09/26 14:55:23 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1583, average loss: 2.2246
[09/26 14:55:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 14:55:23 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[09/26 14:55:30 visual_prompt]: Epoch 9 / 100: avg data time: 5.27e-02, avg batch time: 0.4676, average train loss: 2.2803
[09/26 14:55:31 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1579, average loss: 2.2348
[09/26 14:55:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 14:55:31 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[09/26 14:55:38 visual_prompt]: Epoch 10 / 100: avg data time: 5.68e-02, avg batch time: 0.4693, average train loss: 2.2565
[09/26 14:55:39 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1584, average loss: 2.2089
[09/26 14:55:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 14:55:39 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[09/26 14:55:46 visual_prompt]: Epoch 11 / 100: avg data time: 5.28e-02, avg batch time: 0.4674, average train loss: 2.2518
[09/26 14:55:47 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1582, average loss: 2.2311
[09/26 14:55:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.50	
[09/26 14:55:47 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[09/26 14:55:54 visual_prompt]: Epoch 12 / 100: avg data time: 5.80e-02, avg batch time: 0.4707, average train loss: 2.2426
[09/26 14:55:55 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1583, average loss: 2.2523
[09/26 14:55:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 14.50	top5: 64.00	
[09/26 14:55:55 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[09/26 14:56:02 visual_prompt]: Epoch 13 / 100: avg data time: 5.76e-02, avg batch time: 0.4705, average train loss: 2.2403
[09/26 14:56:03 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1578, average loss: 2.1981
[09/26 14:56:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 64.50	
[09/26 14:56:03 visual_prompt]: Best epoch 13: best metric: 0.245
[09/26 14:56:03 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[09/26 14:56:10 visual_prompt]: Epoch 14 / 100: avg data time: 6.29e-02, avg batch time: 0.4751, average train loss: 2.2075
[09/26 14:56:12 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1581, average loss: 2.1815
[09/26 14:56:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.00	
[09/26 14:56:12 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[09/26 14:56:18 visual_prompt]: Epoch 15 / 100: avg data time: 5.63e-02, avg batch time: 0.4700, average train loss: 2.1980
[09/26 14:56:20 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1583, average loss: 2.2003
[09/26 14:56:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 64.00	
[09/26 14:56:20 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[09/26 14:56:26 visual_prompt]: Epoch 16 / 100: avg data time: 5.88e-02, avg batch time: 0.4726, average train loss: 2.2081
[09/26 14:56:28 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1585, average loss: 2.1647
[09/26 14:56:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 66.50	
[09/26 14:56:28 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[09/26 14:56:34 visual_prompt]: Epoch 17 / 100: avg data time: 6.01e-02, avg batch time: 0.4730, average train loss: 2.1720
[09/26 14:56:36 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1582, average loss: 2.0936
[09/26 14:56:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.50	top5: 71.50	
[09/26 14:56:36 visual_prompt]: Best epoch 17: best metric: 0.295
[09/26 14:56:36 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[09/26 14:56:42 visual_prompt]: Epoch 18 / 100: avg data time: 6.12e-02, avg batch time: 0.4741, average train loss: 2.1044
[09/26 14:56:44 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1587, average loss: 2.0634
[09/26 14:56:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.00	top5: 74.00	
[09/26 14:56:44 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[09/26 14:56:50 visual_prompt]: Epoch 19 / 100: avg data time: 6.24e-02, avg batch time: 0.4748, average train loss: 2.0729
[09/26 14:56:52 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1583, average loss: 2.1365
[09/26 14:56:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 74.00	
[09/26 14:56:52 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[09/26 14:56:58 visual_prompt]: Epoch 20 / 100: avg data time: 6.22e-02, avg batch time: 0.4749, average train loss: 2.1352
[09/26 14:57:00 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1581, average loss: 2.0588
[09/26 14:57:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 31.00	top5: 71.00	
[09/26 14:57:00 visual_prompt]: Best epoch 20: best metric: 0.310
[09/26 14:57:00 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[09/26 14:57:06 visual_prompt]: Epoch 21 / 100: avg data time: 6.36e-02, avg batch time: 0.4760, average train loss: 2.0383
[09/26 14:57:08 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.1580, average loss: 1.9787
[09/26 14:57:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 31.00	top5: 80.50	
[09/26 14:57:08 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[09/26 14:57:15 visual_prompt]: Epoch 22 / 100: avg data time: 5.28e-02, avg batch time: 0.4670, average train loss: 1.9845
[09/26 14:57:16 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1583, average loss: 2.0705
[09/26 14:57:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.00	top5: 74.50	
[09/26 14:57:16 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[09/26 14:57:23 visual_prompt]: Epoch 23 / 100: avg data time: 6.11e-02, avg batch time: 0.4741, average train loss: 2.0294
[09/26 14:57:24 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1583, average loss: 1.9292
[09/26 14:57:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 32.50	top5: 81.00	
[09/26 14:57:24 visual_prompt]: Best epoch 23: best metric: 0.325
[09/26 14:57:24 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[09/26 14:57:31 visual_prompt]: Epoch 24 / 100: avg data time: 6.36e-02, avg batch time: 0.4760, average train loss: 1.9046
[09/26 14:57:32 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1579, average loss: 1.8134
[09/26 14:57:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 32.50	top5: 82.50	
[09/26 14:57:32 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[09/26 14:57:39 visual_prompt]: Epoch 25 / 100: avg data time: 6.07e-02, avg batch time: 0.4740, average train loss: 1.7499
[09/26 14:57:40 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1584, average loss: 1.6987
[09/26 14:57:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 38.00	top5: 82.50	
[09/26 14:57:40 visual_prompt]: Best epoch 25: best metric: 0.380
[09/26 14:57:40 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[09/26 14:57:47 visual_prompt]: Epoch 26 / 100: avg data time: 6.20e-02, avg batch time: 0.4743, average train loss: 1.5589
[09/26 14:57:48 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1587, average loss: 1.8500
[09/26 14:57:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 34.50	top5: 79.00	
[09/26 14:57:48 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.04620120240391065
[09/26 14:57:55 visual_prompt]: Epoch 27 / 100: avg data time: 6.18e-02, avg batch time: 0.4753, average train loss: 1.5253
[09/26 14:57:57 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1581, average loss: 1.5329
[09/26 14:57:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 43.50	top5: 87.00	
[09/26 14:57:57 visual_prompt]: Best epoch 27: best metric: 0.435
[09/26 14:57:57 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.04572593931387604
[09/26 14:58:03 visual_prompt]: Epoch 28 / 100: avg data time: 5.40e-02, avg batch time: 0.4682, average train loss: 1.3695
[09/26 14:58:05 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1587, average loss: 1.6185
[09/26 14:58:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 49.00	top5: 93.50	
[09/26 14:58:05 visual_prompt]: Best epoch 28: best metric: 0.490
[09/26 14:58:05 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.04522542485937369
[09/26 14:58:11 visual_prompt]: Epoch 29 / 100: avg data time: 6.89e-02, avg batch time: 0.4823, average train loss: 1.3324
[09/26 14:58:13 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1583, average loss: 1.6277
[09/26 14:58:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 48.50	top5: 88.50	
[09/26 14:58:13 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.04470026884016805
[09/26 14:58:19 visual_prompt]: Epoch 30 / 100: avg data time: 4.75e-02, avg batch time: 0.4607, average train loss: 1.2720
[09/26 14:58:21 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1582, average loss: 1.4507
[09/26 14:58:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 50.50	top5: 92.00	
[09/26 14:58:21 visual_prompt]: Best epoch 30: best metric: 0.505
[09/26 14:58:21 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.04415111107797445
[09/26 14:58:27 visual_prompt]: Epoch 31 / 100: avg data time: 5.54e-02, avg batch time: 0.4684, average train loss: 1.0759
[09/26 14:58:29 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1584, average loss: 1.1537
[09/26 14:58:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 56.50	top5: 93.00	
[09/26 14:58:29 visual_prompt]: Best epoch 31: best metric: 0.565
[09/26 14:58:29 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.04357862063693486
[09/26 14:58:35 visual_prompt]: Epoch 32 / 100: avg data time: 5.72e-02, avg batch time: 0.4707, average train loss: 0.9343
[09/26 14:58:37 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1577, average loss: 1.0039
[09/26 14:58:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.00	top5: 95.50	
[09/26 14:58:37 visual_prompt]: Best epoch 32: best metric: 0.650
[09/26 14:58:37 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.04298349500846628
[09/26 14:58:43 visual_prompt]: Epoch 33 / 100: avg data time: 5.36e-02, avg batch time: 0.4671, average train loss: 0.9030
[09/26 14:58:45 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1587, average loss: 1.1968
[09/26 14:58:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 55.50	top5: 96.00	
[09/26 14:58:45 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.042366459261474934
[09/26 14:58:51 visual_prompt]: Epoch 34 / 100: avg data time: 6.30e-02, avg batch time: 0.4768, average train loss: 0.7523
[09/26 14:58:53 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1587, average loss: 1.2361
[09/26 14:58:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 60.50	top5: 96.00	
[09/26 14:58:53 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.04172826515897146
[09/26 14:58:59 visual_prompt]: Epoch 35 / 100: avg data time: 4.90e-02, avg batch time: 0.4636, average train loss: 0.6710
[09/26 14:59:01 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1586, average loss: 1.1546
[09/26 14:59:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 63.00	top5: 97.00	
[09/26 14:59:01 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.04106969024216348
[09/26 14:59:08 visual_prompt]: Epoch 36 / 100: avg data time: 6.21e-02, avg batch time: 0.4749, average train loss: 0.7047
[09/26 14:59:09 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1583, average loss: 1.0544
[09/26 14:59:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 64.50	top5: 97.00	
[09/26 14:59:09 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.040391536883141455
[09/26 14:59:16 visual_prompt]: Epoch 37 / 100: avg data time: 5.69e-02, avg batch time: 0.4713, average train loss: 0.6003
[09/26 14:59:17 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.1584, average loss: 1.1589
[09/26 14:59:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.00	top5: 94.00	
[09/26 14:59:17 visual_prompt]: Best epoch 37: best metric: 0.680
[09/26 14:59:17 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.03969463130731183
[09/26 14:59:24 visual_prompt]: Epoch 38 / 100: avg data time: 5.24e-02, avg batch time: 0.4664, average train loss: 0.6769
[09/26 14:59:25 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1584, average loss: 1.1772
[09/26 14:59:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 66.50	top5: 97.00	
[09/26 14:59:25 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.03897982258676867
[09/26 14:59:32 visual_prompt]: Epoch 39 / 100: avg data time: 6.17e-02, avg batch time: 0.4752, average train loss: 0.5499
[09/26 14:59:33 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1582, average loss: 1.0114
[09/26 14:59:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 97.00	
[09/26 14:59:33 visual_prompt]: Best epoch 39: best metric: 0.720
[09/26 14:59:33 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.03824798160583012
[09/26 14:59:40 visual_prompt]: Epoch 40 / 100: avg data time: 5.46e-02, avg batch time: 0.4676, average train loss: 0.4850
[09/26 14:59:41 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1587, average loss: 1.2369
[09/26 14:59:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.50	top5: 96.00	
[09/26 14:59:41 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.037500000000000006
[09/26 14:59:48 visual_prompt]: Epoch 41 / 100: avg data time: 5.38e-02, avg batch time: 0.4679, average train loss: 0.4430
[09/26 14:59:49 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1580, average loss: 1.2162
[09/26 14:59:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.50	top5: 96.50	
[09/26 14:59:49 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.03673678906964727
[09/26 14:59:56 visual_prompt]: Epoch 42 / 100: avg data time: 6.10e-02, avg batch time: 0.4741, average train loss: 0.5114
[09/26 14:59:57 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1583, average loss: 1.7290
[09/26 14:59:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 57.00	top5: 95.50	
[09/26 14:59:57 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.03595927866972694
[09/26 15:00:04 visual_prompt]: Epoch 43 / 100: avg data time: 5.25e-02, avg batch time: 0.4658, average train loss: 0.5049
[09/26 15:00:05 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1586, average loss: 1.3041
[09/26 15:00:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 63.00	top5: 97.50	
[09/26 15:00:05 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.03516841607689501
[09/26 15:00:12 visual_prompt]: Epoch 44 / 100: avg data time: 5.97e-02, avg batch time: 0.4739, average train loss: 0.3591
[09/26 15:00:14 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1583, average loss: 1.1735
[09/26 15:00:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 66.00	top5: 98.00	
[09/26 15:00:14 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0343651648353978
[09/26 15:00:20 visual_prompt]: Epoch 45 / 100: avg data time: 6.14e-02, avg batch time: 0.4735, average train loss: 0.2928
[09/26 15:00:22 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1576, average loss: 1.3395
[09/26 15:00:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 64.50	top5: 97.50	
[09/26 15:00:22 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.033550503583141725
[09/26 15:00:28 visual_prompt]: Epoch 46 / 100: avg data time: 6.15e-02, avg batch time: 0.4764, average train loss: 0.2186
[09/26 15:00:30 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1582, average loss: 1.2659
[09/26 15:00:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.00	top5: 96.50	
[09/26 15:00:30 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.032725424859373686
[09/26 15:00:36 visual_prompt]: Epoch 47 / 100: avg data time: 5.89e-02, avg batch time: 0.4719, average train loss: 0.1950
[09/26 15:00:38 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1584, average loss: 1.3195
[09/26 15:00:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 98.00	
[09/26 15:00:38 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.03189093389542498
[09/26 15:00:44 visual_prompt]: Epoch 48 / 100: avg data time: 6.10e-02, avg batch time: 0.4742, average train loss: 0.1517
[09/26 15:00:46 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1578, average loss: 1.4889
[09/26 15:00:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.50	top5: 96.50	
[09/26 15:00:46 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.03104804738999169
[09/26 15:00:52 visual_prompt]: Epoch 49 / 100: avg data time: 5.84e-02, avg batch time: 0.4714, average train loss: 0.1402
[09/26 15:00:54 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.1587, average loss: 1.5119
[09/26 15:00:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.50	top5: 97.50	
[09/26 15:00:54 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.03019779227044398
[09/26 15:01:00 visual_prompt]: Epoch 50 / 100: avg data time: 5.97e-02, avg batch time: 0.4722, average train loss: 0.1107
[09/26 15:01:02 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.1582, average loss: 1.4488
[09/26 15:01:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.00	top5: 97.00	
[09/26 15:01:02 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.029341204441673263
[09/26 15:01:08 visual_prompt]: Epoch 51 / 100: avg data time: 5.12e-02, avg batch time: 0.4642, average train loss: 0.1168
[09/26 15:01:10 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1582, average loss: 1.4518
[09/26 15:01:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 96.50	
[09/26 15:01:10 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.028479327524001636
[09/26 15:01:16 visual_prompt]: Epoch 52 / 100: avg data time: 5.96e-02, avg batch time: 0.4747, average train loss: 0.0814
[09/26 15:01:18 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1582, average loss: 1.5778
[09/26 15:01:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.00	top5: 97.50	
[09/26 15:01:18 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.02761321158169134
[09/26 15:01:25 visual_prompt]: Epoch 53 / 100: avg data time: 5.99e-02, avg batch time: 0.4737, average train loss: 0.0794
[09/26 15:01:26 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1582, average loss: 1.4495
[09/26 15:01:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 97.50	
[09/26 15:01:26 visual_prompt]: Best epoch 53: best metric: 0.730
[09/26 15:01:26 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.02674391184360313
[09/26 15:01:33 visual_prompt]: Epoch 54 / 100: avg data time: 6.56e-02, avg batch time: 0.4782, average train loss: 0.0559
[09/26 15:01:34 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1584, average loss: 1.8567
[09/26 15:01:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.50	top5: 94.00	
[09/26 15:01:34 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.02587248741756253
[09/26 15:01:41 visual_prompt]: Epoch 55 / 100: avg data time: 5.75e-02, avg batch time: 0.4700, average train loss: 0.1861
[09/26 15:01:42 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1584, average loss: 1.7702
[09/26 15:01:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 66.00	top5: 95.00	
[09/26 15:01:42 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.025
[09/26 15:01:49 visual_prompt]: Epoch 56 / 100: avg data time: 5.94e-02, avg batch time: 0.4738, average train loss: 0.1617
[09/26 15:01:50 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1584, average loss: 1.3193
[09/26 15:01:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 97.50	
[09/26 15:01:50 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.024127512582437484
[09/26 15:01:57 visual_prompt]: Epoch 57 / 100: avg data time: 5.86e-02, avg batch time: 0.4717, average train loss: 0.1504
[09/26 15:01:58 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1582, average loss: 1.8201
[09/26 15:01:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 64.00	top5: 95.50	
[09/26 15:01:58 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.02325608815639687
[09/26 15:02:05 visual_prompt]: Epoch 58 / 100: avg data time: 6.41e-02, avg batch time: 0.4763, average train loss: 0.0981
[09/26 15:02:07 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1583, average loss: 1.6807
[09/26 15:02:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.50	top5: 95.50	
[09/26 15:02:07 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.02238678841830867
[09/26 15:02:13 visual_prompt]: Epoch 59 / 100: avg data time: 5.27e-02, avg batch time: 0.4666, average train loss: 0.0703
[09/26 15:02:15 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1589, average loss: 1.3608
[09/26 15:02:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 97.00	
[09/26 15:02:15 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.02152067247599837
[09/26 15:02:21 visual_prompt]: Epoch 60 / 100: avg data time: 6.39e-02, avg batch time: 0.4770, average train loss: 0.0720
[09/26 15:02:23 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1579, average loss: 1.7735
[09/26 15:02:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.00	top5: 97.00	
[09/26 15:02:23 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.020658795558326743
[09/26 15:02:29 visual_prompt]: Epoch 61 / 100: avg data time: 6.00e-02, avg batch time: 0.4741, average train loss: 0.0860
[09/26 15:02:31 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1581, average loss: 1.4853
[09/26 15:02:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 97.00	
[09/26 15:02:31 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.019802207729556022
[09/26 15:02:37 visual_prompt]: Epoch 62 / 100: avg data time: 6.47e-02, avg batch time: 0.4786, average train loss: 0.0513
[09/26 15:02:39 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1583, average loss: 1.4948
[09/26 15:02:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.50	
[09/26 15:02:39 visual_prompt]: Best epoch 62: best metric: 0.735
[09/26 15:02:39 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.018951952610008312
[09/26 15:02:45 visual_prompt]: Epoch 63 / 100: avg data time: 5.97e-02, avg batch time: 0.4726, average train loss: 0.0505
[09/26 15:02:47 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1585, average loss: 1.3903
[09/26 15:02:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 97.50	
[09/26 15:02:47 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.018109066104575022
[09/26 15:02:54 visual_prompt]: Epoch 64 / 100: avg data time: 6.99e-02, avg batch time: 0.4823, average train loss: 0.0342
[09/26 15:02:55 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1579, average loss: 1.4418
[09/26 15:02:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 96.50	
[09/26 15:02:55 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.017274575140626316
[09/26 15:03:02 visual_prompt]: Epoch 65 / 100: avg data time: 5.83e-02, avg batch time: 0.4720, average train loss: 0.0243
[09/26 15:03:03 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1581, average loss: 1.4132
[09/26 15:03:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.00	top5: 96.50	
[09/26 15:03:03 visual_prompt]: Best epoch 65: best metric: 0.740
[09/26 15:03:03 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.016449496416858285
[09/26 15:03:10 visual_prompt]: Epoch 66 / 100: avg data time: 5.80e-02, avg batch time: 0.4721, average train loss: 0.0116
[09/26 15:03:11 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1586, average loss: 1.6318
[09/26 15:03:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.50	
[09/26 15:03:11 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0156348351646022
[09/26 15:03:18 visual_prompt]: Epoch 67 / 100: avg data time: 5.72e-02, avg batch time: 0.4716, average train loss: 0.0114
[09/26 15:03:19 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1584, average loss: 1.5579
[09/26 15:03:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 96.50	
[09/26 15:03:19 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.014831583923105
[09/26 15:03:26 visual_prompt]: Epoch 68 / 100: avg data time: 5.37e-02, avg batch time: 0.4695, average train loss: 0.0068
[09/26 15:03:27 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1580, average loss: 1.5345
[09/26 15:03:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:03:27 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.014040721330273063
[09/26 15:03:34 visual_prompt]: Epoch 69 / 100: avg data time: 5.66e-02, avg batch time: 0.4707, average train loss: 0.0093
[09/26 15:03:35 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.1583, average loss: 1.6136
[09/26 15:03:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.50	
[09/26 15:03:35 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.013263210930352737
[09/26 15:03:42 visual_prompt]: Epoch 70 / 100: avg data time: 5.88e-02, avg batch time: 0.4716, average train loss: 0.0091
[09/26 15:03:43 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1587, average loss: 1.5674
[09/26 15:03:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 96.00	
[09/26 15:03:44 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.012500000000000006
[09/26 15:03:50 visual_prompt]: Epoch 71 / 100: avg data time: 7.15e-02, avg batch time: 0.4851, average train loss: 0.0047
[09/26 15:03:52 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 1.6465
[09/26 15:03:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:03:52 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.01175201839416988
[09/26 15:03:58 visual_prompt]: Epoch 72 / 100: avg data time: 5.97e-02, avg batch time: 0.4725, average train loss: 0.0049
[09/26 15:04:00 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1589, average loss: 1.6556
[09/26 15:04:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 97.00	
[09/26 15:04:00 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.011020177413231334
[09/26 15:04:06 visual_prompt]: Epoch 73 / 100: avg data time: 6.51e-02, avg batch time: 0.4780, average train loss: 0.0039
[09/26 15:04:08 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1585, average loss: 1.6965
[09/26 15:04:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 96.50	
[09/26 15:04:08 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.010305368692688175
[09/26 15:04:15 visual_prompt]: Epoch 74 / 100: avg data time: 6.35e-02, avg batch time: 0.4771, average train loss: 0.0033
[09/26 15:04:16 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1586, average loss: 1.6408
[09/26 15:04:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 96.50	
[09/26 15:04:16 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.009608463116858543
[09/26 15:04:23 visual_prompt]: Epoch 75 / 100: avg data time: 6.27e-02, avg batch time: 0.4768, average train loss: 0.0028
[09/26 15:04:24 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1582, average loss: 1.6095
[09/26 15:04:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.00	
[09/26 15:04:24 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.008930309757836517
[09/26 15:04:31 visual_prompt]: Epoch 76 / 100: avg data time: 6.64e-02, avg batch time: 0.4793, average train loss: 0.0035
[09/26 15:04:32 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1580, average loss: 1.6287
[09/26 15:04:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 96.00	
[09/26 15:04:32 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.008271734841028553
[09/26 15:04:39 visual_prompt]: Epoch 77 / 100: avg data time: 6.04e-02, avg batch time: 0.4740, average train loss: 0.0021
[09/26 15:04:41 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1577, average loss: 1.6350
[09/26 15:04:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 96.00	
[09/26 15:04:41 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.007633540738525066
[09/26 15:04:47 visual_prompt]: Epoch 78 / 100: avg data time: 6.42e-02, avg batch time: 0.4779, average train loss: 0.0024
[09/26 15:04:49 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1585, average loss: 1.6545
[09/26 15:04:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 96.00	
[09/26 15:04:49 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.0070165049915337264
[09/26 15:04:55 visual_prompt]: Epoch 79 / 100: avg data time: 5.81e-02, avg batch time: 0.4714, average train loss: 0.0040
[09/26 15:04:57 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.1586, average loss: 1.6400
[09/26 15:04:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 96.00	
[09/26 15:04:57 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.006421379363065142
[09/26 15:05:03 visual_prompt]: Epoch 80 / 100: avg data time: 6.34e-02, avg batch time: 0.4773, average train loss: 0.0022
[09/26 15:05:05 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1588, average loss: 1.6340
[09/26 15:05:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 96.00	
[09/26 15:05:05 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.005848888922025553
[09/26 15:05:11 visual_prompt]: Epoch 81 / 100: avg data time: 5.96e-02, avg batch time: 0.4737, average train loss: 0.0021
[09/26 15:05:13 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1581, average loss: 1.6459
[09/26 15:05:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 96.00	
[09/26 15:05:13 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.005299731159831953
[09/26 15:05:19 visual_prompt]: Epoch 82 / 100: avg data time: 5.27e-02, avg batch time: 0.4659, average train loss: 0.0027
[09/26 15:05:21 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1585, average loss: 1.6549
[09/26 15:05:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.00	
[09/26 15:05:21 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.004774575140626317
[09/26 15:05:27 visual_prompt]: Epoch 83 / 100: avg data time: 6.12e-02, avg batch time: 0.4752, average train loss: 0.0027
[09/26 15:05:29 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1582, average loss: 1.6618
[09/26 15:05:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.00	
[09/26 15:05:29 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.004274060686123959
[09/26 15:05:36 visual_prompt]: Epoch 84 / 100: avg data time: 5.99e-02, avg batch time: 0.4737, average train loss: 0.0022
[09/26 15:05:37 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1582, average loss: 1.6539
[09/26 15:05:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.00	
[09/26 15:05:37 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.003798797596089351
[09/26 15:05:44 visual_prompt]: Epoch 85 / 100: avg data time: 6.11e-02, avg batch time: 0.4742, average train loss: 0.0028
[09/26 15:05:45 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1581, average loss: 1.6687
[09/26 15:05:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
[09/26 15:05:45 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.0033493649053890325
[09/26 15:05:52 visual_prompt]: Epoch 86 / 100: avg data time: 5.91e-02, avg batch time: 0.4728, average train loss: 0.0026
[09/26 15:05:53 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1584, average loss: 1.6779
[09/26 15:05:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
[09/26 15:05:53 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.0029263101785268254
[09/26 15:06:00 visual_prompt]: Epoch 87 / 100: avg data time: 6.54e-02, avg batch time: 0.4787, average train loss: 0.0025
[09/26 15:06:01 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1578, average loss: 1.6766
[09/26 15:06:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
[09/26 15:06:01 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00253014884252083
[09/26 15:06:08 visual_prompt]: Epoch 88 / 100: avg data time: 4.76e-02, avg batch time: 0.4641, average train loss: 0.0019
[09/26 15:06:09 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1581, average loss: 1.6759
[09/26 15:06:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
[09/26 15:06:09 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.0021613635589349755
[09/26 15:06:16 visual_prompt]: Epoch 89 / 100: avg data time: 5.40e-02, avg batch time: 0.4681, average train loss: 0.0020
[09/26 15:06:17 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1585, average loss: 1.6809
[09/26 15:06:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.00	
[09/26 15:06:17 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0018204036358303172
[09/26 15:06:24 visual_prompt]: Epoch 90 / 100: avg data time: 5.97e-02, avg batch time: 0.4737, average train loss: 0.0018
[09/26 15:06:25 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1582, average loss: 1.6835
[09/26 15:06:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.00	
[09/26 15:06:25 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0015076844803522923
[09/26 15:06:32 visual_prompt]: Epoch 91 / 100: avg data time: 6.13e-02, avg batch time: 0.4745, average train loss: 0.0018
[09/26 15:06:33 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1586, average loss: 1.6855
[09/26 15:06:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.50	
[09/26 15:06:33 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0012235870926211618
[09/26 15:06:40 visual_prompt]: Epoch 92 / 100: avg data time: 5.57e-02, avg batch time: 0.4705, average train loss: 0.0020
[09/26 15:06:41 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1578, average loss: 1.6859
[09/26 15:06:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.50	
[09/26 15:06:41 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0009684576015420277
[09/26 15:06:48 visual_prompt]: Epoch 93 / 100: avg data time: 5.43e-02, avg batch time: 0.4687, average train loss: 0.0021
[09/26 15:06:49 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1579, average loss: 1.6863
[09/26 15:06:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.50	
[09/26 15:06:49 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0007426068431000883
[09/26 15:06:56 visual_prompt]: Epoch 94 / 100: avg data time: 6.23e-02, avg batch time: 0.4766, average train loss: 0.0017
[09/26 15:06:57 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1582, average loss: 1.6854
[09/26 15:06:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.00	
[09/26 15:06:57 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0005463099816548578
[09/26 15:07:04 visual_prompt]: Epoch 95 / 100: avg data time: 5.84e-02, avg batch time: 0.4723, average train loss: 0.0025
[09/26 15:07:06 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.1583, average loss: 1.6847
[09/26 15:07:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.00	
[09/26 15:07:06 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00037980617469479954
[09/26 15:07:12 visual_prompt]: Epoch 96 / 100: avg data time: 5.93e-02, avg batch time: 0.4738, average train loss: 0.0019
[09/26 15:07:14 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1586, average loss: 1.6848
[09/26 15:07:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.00	
[09/26 15:07:14 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00024329828146074097
[09/26 15:07:20 visual_prompt]: Epoch 97 / 100: avg data time: 5.89e-02, avg batch time: 0.4720, average train loss: 0.0019
[09/26 15:07:22 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1584, average loss: 1.6847
[09/26 15:07:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.00	
[09/26 15:07:22 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.00013695261579316776
[09/26 15:07:28 visual_prompt]: Epoch 98 / 100: avg data time: 6.24e-02, avg batch time: 0.4757, average train loss: 0.0021
[09/26 15:07:30 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1587, average loss: 1.6845
[09/26 15:07:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.00	
[09/26 15:07:30 visual_prompt]: Training 99 / 100 epoch, with learning rate 6.089874350439506e-05
[09/26 15:07:36 visual_prompt]: Epoch 99 / 100: avg data time: 6.20e-02, avg batch time: 0.4759, average train loss: 0.0025
[09/26 15:07:38 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1582, average loss: 1.6843
[09/26 15:07:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.00	
[09/26 15:07:38 visual_prompt]: Training 100 / 100 epoch, with learning rate 1.5229324522605948e-05
[09/26 15:07:44 visual_prompt]: Epoch 100 / 100: avg data time: 6.09e-02, avg batch time: 0.4741, average train loss: 0.0018
[09/26 15:07:46 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1585, average loss: 1.6842
[09/26 15:07:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.00	
[09/26 15:07:46 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 15:07:46 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 15:07:46 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 15:07:46 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 15:07:46 visual_prompt]: Training with config:
[09/26 15:07:46 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.05_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 15:07:46 visual_prompt]: Loading training data...
[09/26 15:07:46 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 15:07:48 visual_prompt]: Number of images: 800
[09/26 15:07:48 visual_prompt]: Number of classes: 10 / 10
[09/26 15:07:48 visual_prompt]: Loading validation data...
[09/26 15:07:48 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 15:07:48 visual_prompt]: Number of images: 200
[09/26 15:07:48 visual_prompt]: Number of classes: 10 / 10
[09/26 15:07:48 visual_prompt]: Constructing models...
[09/26 15:07:50 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 15:07:50 visual_prompt]: tuned percent:0.543
[09/26 15:07:50 visual_prompt]: Device used for model: 0
[09/26 15:07:50 visual_prompt]: Setting up Evaluator...
[09/26 15:07:50 visual_prompt]: Setting up Trainer...
[09/26 15:07:50 visual_prompt]: 	Setting up the optimizer...
[09/26 15:07:50 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 15:07:57 visual_prompt]: Epoch 1 / 100: avg data time: 5.51e-02, avg batch time: 0.4752, average train loss: 2.6783
[09/26 15:07:59 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1585, average loss: 2.6214
[09/26 15:07:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 15:07:59 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 15:07:59 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[09/26 15:08:05 visual_prompt]: Epoch 2 / 100: avg data time: 6.49e-02, avg batch time: 0.4768, average train loss: 2.3679
[09/26 15:08:07 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1581, average loss: 2.2320
[09/26 15:08:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 15:08:07 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[09/26 15:08:13 visual_prompt]: Epoch 3 / 100: avg data time: 6.07e-02, avg batch time: 0.4740, average train loss: 2.2657
[09/26 15:08:15 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1586, average loss: 2.2289
[09/26 15:08:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 15:08:15 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[09/26 15:08:21 visual_prompt]: Epoch 4 / 100: avg data time: 6.32e-02, avg batch time: 0.4754, average train loss: 2.2607
[09/26 15:08:23 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1580, average loss: 2.2198
[09/26 15:08:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 15:08:23 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[09/26 15:08:29 visual_prompt]: Epoch 5 / 100: avg data time: 6.23e-02, avg batch time: 0.4752, average train loss: 2.2537
[09/26 15:08:31 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1582, average loss: 2.2142
[09/26 15:08:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 15:08:31 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[09/26 15:08:38 visual_prompt]: Epoch 6 / 100: avg data time: 5.94e-02, avg batch time: 0.4731, average train loss: 2.2531
[09/26 15:08:39 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1585, average loss: 2.2271
[09/26 15:08:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 15:08:39 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[09/26 15:08:45 visual_prompt]: Epoch 7 / 100: avg data time: 4.58e-02, avg batch time: 0.4599, average train loss: 2.2513
[09/26 15:08:47 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1587, average loss: 2.2441
[09/26 15:08:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 13.00	top5: 63.00	
[09/26 15:08:47 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[09/26 15:08:54 visual_prompt]: Epoch 8 / 100: avg data time: 6.53e-02, avg batch time: 0.4782, average train loss: 2.2465
[09/26 15:08:55 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1583, average loss: 2.2114
[09/26 15:08:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.00	
[09/26 15:08:55 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[09/26 15:09:02 visual_prompt]: Epoch 9 / 100: avg data time: 6.69e-02, avg batch time: 0.4796, average train loss: 2.2420
[09/26 15:09:03 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1586, average loss: 2.2231
[09/26 15:09:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.00	
[09/26 15:09:03 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[09/26 15:09:10 visual_prompt]: Epoch 10 / 100: avg data time: 4.35e-02, avg batch time: 0.4571, average train loss: 2.2572
[09/26 15:09:11 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1582, average loss: 2.2157
[09/26 15:09:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 62.50	
[09/26 15:09:11 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[09/26 15:09:18 visual_prompt]: Epoch 11 / 100: avg data time: 6.25e-02, avg batch time: 0.4746, average train loss: 2.2542
[09/26 15:09:19 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.1575, average loss: 2.2072
[09/26 15:09:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 66.00	
[09/26 15:09:19 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[09/26 15:09:26 visual_prompt]: Epoch 12 / 100: avg data time: 6.50e-02, avg batch time: 0.4784, average train loss: 2.2506
[09/26 15:09:27 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1584, average loss: 2.2168
[09/26 15:09:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 15:09:27 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[09/26 15:09:34 visual_prompt]: Epoch 13 / 100: avg data time: 5.99e-02, avg batch time: 0.4740, average train loss: 2.2173
[09/26 15:09:36 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1584, average loss: 2.2644
[09/26 15:09:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 10.00	top5: 63.50	
[09/26 15:09:36 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[09/26 15:09:42 visual_prompt]: Epoch 14 / 100: avg data time: 5.92e-02, avg batch time: 0.4727, average train loss: 2.2303
[09/26 15:09:44 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1584, average loss: 2.2007
[09/26 15:09:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 66.00	
[09/26 15:09:44 visual_prompt]: Best epoch 14: best metric: 0.245
[09/26 15:09:44 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[09/26 15:09:50 visual_prompt]: Epoch 15 / 100: avg data time: 4.78e-02, avg batch time: 0.4620, average train loss: 2.2175
[09/26 15:09:51 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1583, average loss: 2.2046
[09/26 15:09:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.00	top5: 67.50	
[09/26 15:09:51 visual_prompt]: Best epoch 15: best metric: 0.250
[09/26 15:09:51 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[09/26 15:09:58 visual_prompt]: Epoch 16 / 100: avg data time: 5.99e-02, avg batch time: 0.4724, average train loss: 2.2066
[09/26 15:10:00 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.1583, average loss: 2.1855
[09/26 15:10:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 22.50	top5: 63.50	
[09/26 15:10:00 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[09/26 15:10:06 visual_prompt]: Epoch 17 / 100: avg data time: 6.05e-02, avg batch time: 0.4740, average train loss: 2.1710
[09/26 15:10:08 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1583, average loss: 2.1759
[09/26 15:10:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.00	top5: 67.00	
[09/26 15:10:08 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[09/26 15:10:14 visual_prompt]: Epoch 18 / 100: avg data time: 4.61e-02, avg batch time: 0.4593, average train loss: 2.1446
[09/26 15:10:16 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1586, average loss: 2.1319
[09/26 15:10:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 27.50	top5: 68.50	
[09/26 15:10:16 visual_prompt]: Best epoch 18: best metric: 0.275
[09/26 15:10:16 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[09/26 15:10:22 visual_prompt]: Epoch 19 / 100: avg data time: 6.30e-02, avg batch time: 0.4780, average train loss: 2.1362
[09/26 15:10:24 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1584, average loss: 2.2747
[09/26 15:10:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 18.00	top5: 62.50	
[09/26 15:10:24 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[09/26 15:10:30 visual_prompt]: Epoch 20 / 100: avg data time: 6.64e-02, avg batch time: 0.4800, average train loss: 2.1265
[09/26 15:10:32 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1583, average loss: 2.1504
[09/26 15:10:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 26.00	top5: 67.50	
[09/26 15:10:32 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[09/26 15:10:38 visual_prompt]: Epoch 21 / 100: avg data time: 5.63e-02, avg batch time: 0.4700, average train loss: 2.0887
[09/26 15:10:40 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1585, average loss: 2.0743
[09/26 15:10:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.50	top5: 74.50	
[09/26 15:10:40 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[09/26 15:10:46 visual_prompt]: Epoch 22 / 100: avg data time: 5.61e-02, avg batch time: 0.4702, average train loss: 2.0213
[09/26 15:10:48 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1586, average loss: 2.0833
[09/26 15:10:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 21.50	top5: 76.00	
[09/26 15:10:48 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[09/26 15:10:54 visual_prompt]: Epoch 23 / 100: avg data time: 5.74e-02, avg batch time: 0.4699, average train loss: 1.9727
[09/26 15:10:56 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1587, average loss: 1.9653
[09/26 15:10:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 29.00	top5: 75.50	
[09/26 15:10:56 visual_prompt]: Best epoch 23: best metric: 0.290
[09/26 15:10:56 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[09/26 15:11:02 visual_prompt]: Epoch 24 / 100: avg data time: 5.40e-02, avg batch time: 0.4685, average train loss: 1.9355
[09/26 15:11:04 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.1582, average loss: 1.9241
[09/26 15:11:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 30.50	top5: 80.00	
[09/26 15:11:04 visual_prompt]: Best epoch 24: best metric: 0.305
[09/26 15:11:04 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[09/26 15:11:10 visual_prompt]: Epoch 25 / 100: avg data time: 4.92e-02, avg batch time: 0.4614, average train loss: 1.8406
[09/26 15:11:12 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1583, average loss: 1.9033
[09/26 15:11:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 34.50	top5: 78.50	
[09/26 15:11:12 visual_prompt]: Best epoch 25: best metric: 0.345
[09/26 15:11:12 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[09/26 15:11:19 visual_prompt]: Epoch 26 / 100: avg data time: 6.03e-02, avg batch time: 0.4729, average train loss: 1.7346
[09/26 15:11:20 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1585, average loss: 1.7512
[09/26 15:11:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 39.50	top5: 84.50	
[09/26 15:11:20 visual_prompt]: Best epoch 26: best metric: 0.395
[09/26 15:11:20 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.04620120240391065
[09/26 15:11:27 visual_prompt]: Epoch 27 / 100: avg data time: 5.68e-02, avg batch time: 0.4701, average train loss: 1.6167
[09/26 15:11:28 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1583, average loss: 1.6908
[09/26 15:11:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 39.50	top5: 83.00	
[09/26 15:11:28 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.04572593931387604
[09/26 15:11:35 visual_prompt]: Epoch 28 / 100: avg data time: 5.84e-02, avg batch time: 0.4710, average train loss: 1.5162
[09/26 15:11:36 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.1583, average loss: 1.6457
[09/26 15:11:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 37.50	top5: 87.50	
[09/26 15:11:36 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.04522542485937369
[09/26 15:11:43 visual_prompt]: Epoch 29 / 100: avg data time: 5.89e-02, avg batch time: 0.4723, average train loss: 1.4729
[09/26 15:11:44 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1585, average loss: 1.6233
[09/26 15:11:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 39.50	top5: 90.50	
[09/26 15:11:44 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.04470026884016805
[09/26 15:11:51 visual_prompt]: Epoch 30 / 100: avg data time: 6.33e-02, avg batch time: 0.4754, average train loss: 1.3115
[09/26 15:11:52 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1581, average loss: 1.4396
[09/26 15:11:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 43.50	top5: 93.00	
[09/26 15:11:52 visual_prompt]: Best epoch 30: best metric: 0.435
[09/26 15:11:52 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.04415111107797445
[09/26 15:11:59 visual_prompt]: Epoch 31 / 100: avg data time: 5.20e-02, avg batch time: 0.4642, average train loss: 1.2924
[09/26 15:12:00 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1582, average loss: 1.4065
[09/26 15:12:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 54.00	top5: 92.50	
[09/26 15:12:00 visual_prompt]: Best epoch 31: best metric: 0.540
[09/26 15:12:00 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.04357862063693486
[09/26 15:12:07 visual_prompt]: Epoch 32 / 100: avg data time: 6.62e-02, avg batch time: 0.4789, average train loss: 1.2017
[09/26 15:12:08 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1587, average loss: 1.2272
[09/26 15:12:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 58.00	top5: 93.00	
[09/26 15:12:08 visual_prompt]: Best epoch 32: best metric: 0.580
[09/26 15:12:08 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.04298349500846628
[09/26 15:12:15 visual_prompt]: Epoch 33 / 100: avg data time: 6.32e-02, avg batch time: 0.4764, average train loss: 1.0983
[09/26 15:12:16 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1583, average loss: 1.3604
[09/26 15:12:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 56.00	top5: 94.50	
[09/26 15:12:16 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.042366459261474934
[09/26 15:12:23 visual_prompt]: Epoch 34 / 100: avg data time: 6.71e-02, avg batch time: 0.4795, average train loss: 1.0038
[09/26 15:12:25 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1579, average loss: 1.1610
[09/26 15:12:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 62.50	top5: 95.00	
[09/26 15:12:25 visual_prompt]: Best epoch 34: best metric: 0.625
[09/26 15:12:25 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.04172826515897146
[09/26 15:12:31 visual_prompt]: Epoch 35 / 100: avg data time: 5.50e-02, avg batch time: 0.4689, average train loss: 0.8704
[09/26 15:12:33 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1585, average loss: 1.0878
[09/26 15:12:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 64.00	top5: 95.00	
[09/26 15:12:33 visual_prompt]: Best epoch 35: best metric: 0.640
[09/26 15:12:33 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.04106969024216348
[09/26 15:12:39 visual_prompt]: Epoch 36 / 100: avg data time: 6.44e-02, avg batch time: 0.4777, average train loss: 0.7744
[09/26 15:12:41 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1579, average loss: 1.2252
[09/26 15:12:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 61.00	top5: 94.50	
[09/26 15:12:41 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.040391536883141455
[09/26 15:12:47 visual_prompt]: Epoch 37 / 100: avg data time: 6.85e-02, avg batch time: 0.4811, average train loss: 0.7353
[09/26 15:12:49 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1584, average loss: 1.0213
[09/26 15:12:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.00	top5: 95.50	
[09/26 15:12:49 visual_prompt]: Best epoch 37: best metric: 0.690
[09/26 15:12:49 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.03969463130731183
[09/26 15:12:55 visual_prompt]: Epoch 38 / 100: avg data time: 5.72e-02, avg batch time: 0.4699, average train loss: 0.6509
[09/26 15:12:57 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1587, average loss: 1.0123
[09/26 15:12:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.50	top5: 96.50	
[09/26 15:12:57 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.03897982258676867
[09/26 15:13:03 visual_prompt]: Epoch 39 / 100: avg data time: 4.90e-02, avg batch time: 0.4651, average train loss: 0.5798
[09/26 15:13:05 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1579, average loss: 1.0495
[09/26 15:13:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.00	top5: 96.50	
[09/26 15:13:05 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.03824798160583012
[09/26 15:13:11 visual_prompt]: Epoch 40 / 100: avg data time: 6.36e-02, avg batch time: 0.4782, average train loss: 0.5353
[09/26 15:13:13 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1588, average loss: 1.3632
[09/26 15:13:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 63.00	top5: 96.50	
[09/26 15:13:13 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.037500000000000006
[09/26 15:13:19 visual_prompt]: Epoch 41 / 100: avg data time: 5.32e-02, avg batch time: 0.4679, average train loss: 0.5339
[09/26 15:13:21 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1585, average loss: 1.0596
[09/26 15:13:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.00	top5: 96.00	
[09/26 15:13:21 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.03673678906964727
[09/26 15:13:27 visual_prompt]: Epoch 42 / 100: avg data time: 5.72e-02, avg batch time: 0.4715, average train loss: 0.4214
[09/26 15:13:29 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1586, average loss: 1.1439
[09/26 15:13:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.00	top5: 96.00	
[09/26 15:13:29 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.03595927866972694
[09/26 15:13:35 visual_prompt]: Epoch 43 / 100: avg data time: 5.50e-02, avg batch time: 0.4700, average train loss: 0.4463
[09/26 15:13:37 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.1585, average loss: 1.0904
[09/26 15:13:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 66.50	top5: 96.50	
[09/26 15:13:37 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.03516841607689501
[09/26 15:13:44 visual_prompt]: Epoch 44 / 100: avg data time: 6.30e-02, avg batch time: 0.4762, average train loss: 0.3749
[09/26 15:13:45 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1586, average loss: 1.1730
[09/26 15:13:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.50	top5: 96.50	
[09/26 15:13:45 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0343651648353978
[09/26 15:13:52 visual_prompt]: Epoch 45 / 100: avg data time: 6.42e-02, avg batch time: 0.4772, average train loss: 0.2574
[09/26 15:13:53 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1586, average loss: 1.2844
[09/26 15:13:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.50	top5: 97.00	
[09/26 15:13:53 visual_prompt]: Best epoch 45: best metric: 0.695
[09/26 15:13:53 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.033550503583141725
[09/26 15:14:00 visual_prompt]: Epoch 46 / 100: avg data time: 6.44e-02, avg batch time: 0.4781, average train loss: 0.3240
[09/26 15:14:02 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1585, average loss: 1.4778
[09/26 15:14:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.00	top5: 93.50	
[09/26 15:14:02 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.032725424859373686
[09/26 15:14:08 visual_prompt]: Epoch 47 / 100: avg data time: 6.32e-02, avg batch time: 0.4772, average train loss: 0.3340
[09/26 15:14:10 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1589, average loss: 1.3080
[09/26 15:14:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.50	top5: 94.00	
[09/26 15:14:10 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.03189093389542498
[09/26 15:14:16 visual_prompt]: Epoch 48 / 100: avg data time: 6.38e-02, avg batch time: 0.4773, average train loss: 0.3582
[09/26 15:14:18 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1587, average loss: 1.1643
[09/26 15:14:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.00	top5: 94.50	
[09/26 15:14:18 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.03104804738999169
[09/26 15:14:24 visual_prompt]: Epoch 49 / 100: avg data time: 5.60e-02, avg batch time: 0.4690, average train loss: 0.2541
[09/26 15:14:26 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1584, average loss: 1.1872
[09/26 15:14:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.50	top5: 97.00	
[09/26 15:14:26 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.03019779227044398
[09/26 15:14:32 visual_prompt]: Epoch 50 / 100: avg data time: 5.87e-02, avg batch time: 0.4722, average train loss: 0.2104
[09/26 15:14:34 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1584, average loss: 1.6452
[09/26 15:14:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.00	top5: 95.00	
[09/26 15:14:34 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.029341204441673263
[09/26 15:14:41 visual_prompt]: Epoch 51 / 100: avg data time: 6.18e-02, avg batch time: 0.4755, average train loss: 0.1852
[09/26 15:14:42 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1583, average loss: 1.6458
[09/26 15:14:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.50	top5: 96.50	
[09/26 15:14:42 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.028479327524001636
[09/26 15:14:49 visual_prompt]: Epoch 52 / 100: avg data time: 5.51e-02, avg batch time: 0.4713, average train loss: 0.1449
[09/26 15:14:50 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1584, average loss: 1.4827
[09/26 15:14:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 97.00	
[09/26 15:14:50 visual_prompt]: Best epoch 52: best metric: 0.720
[09/26 15:14:50 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.02761321158169134
[09/26 15:14:57 visual_prompt]: Epoch 53 / 100: avg data time: 5.64e-02, avg batch time: 0.4695, average train loss: 0.0859
[09/26 15:14:58 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1586, average loss: 1.5896
[09/26 15:14:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 94.50	
[09/26 15:14:58 visual_prompt]: Best epoch 53: best metric: 0.735
[09/26 15:14:58 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.02674391184360313
[09/26 15:15:05 visual_prompt]: Epoch 54 / 100: avg data time: 5.28e-02, avg batch time: 0.4666, average train loss: 0.0777
[09/26 15:15:06 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.1585, average loss: 1.4645
[09/26 15:15:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 95.00	
[09/26 15:15:06 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.02587248741756253
[09/26 15:15:13 visual_prompt]: Epoch 55 / 100: avg data time: 5.44e-02, avg batch time: 0.4704, average train loss: 0.0783
[09/26 15:15:14 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1587, average loss: 1.4731
[09/26 15:15:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.50	
[09/26 15:15:14 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.025
[09/26 15:15:21 visual_prompt]: Epoch 56 / 100: avg data time: 6.41e-02, avg batch time: 0.4776, average train loss: 0.0680
[09/26 15:15:22 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1584, average loss: 1.4291
[09/26 15:15:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.50	top5: 96.50	
[09/26 15:15:22 visual_prompt]: Best epoch 56: best metric: 0.745
[09/26 15:15:22 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.024127512582437484
[09/26 15:15:29 visual_prompt]: Epoch 57 / 100: avg data time: 5.26e-02, avg batch time: 0.4657, average train loss: 0.0463
[09/26 15:15:30 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1588, average loss: 1.5026
[09/26 15:15:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 96.50	
[09/26 15:15:30 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.02325608815639687
[09/26 15:15:37 visual_prompt]: Epoch 58 / 100: avg data time: 5.35e-02, avg batch time: 0.4691, average train loss: 0.0375
[09/26 15:15:38 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1585, average loss: 1.5136
[09/26 15:15:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.50	top5: 96.50	
[09/26 15:15:38 visual_prompt]: Best epoch 58: best metric: 0.755
[09/26 15:15:38 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.02238678841830867
[09/26 15:15:45 visual_prompt]: Epoch 59 / 100: avg data time: 6.11e-02, avg batch time: 0.4735, average train loss: 0.0267
[09/26 15:15:47 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1580, average loss: 1.6413
[09/26 15:15:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.00	top5: 96.00	
[09/26 15:15:47 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.02152067247599837
[09/26 15:15:53 visual_prompt]: Epoch 60 / 100: avg data time: 4.92e-02, avg batch time: 0.4628, average train loss: 0.0189
[09/26 15:15:54 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1585, average loss: 1.6660
[09/26 15:15:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 96.00	
[09/26 15:15:54 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.020658795558326743
[09/26 15:16:01 visual_prompt]: Epoch 61 / 100: avg data time: 6.88e-02, avg batch time: 0.4821, average train loss: 0.0137
[09/26 15:16:03 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1581, average loss: 1.5685
[09/26 15:16:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.50	
[09/26 15:16:03 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.019802207729556022
[09/26 15:16:09 visual_prompt]: Epoch 62 / 100: avg data time: 5.76e-02, avg batch time: 0.4707, average train loss: 0.0199
[09/26 15:16:11 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1584, average loss: 1.6708
[09/26 15:16:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 98.00	
[09/26 15:16:11 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.018951952610008312
[09/26 15:16:17 visual_prompt]: Epoch 63 / 100: avg data time: 6.52e-02, avg batch time: 0.4791, average train loss: 0.0388
[09/26 15:16:19 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1584, average loss: 1.7079
[09/26 15:16:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 76.00	top5: 96.50	
[09/26 15:16:19 visual_prompt]: Best epoch 63: best metric: 0.760
[09/26 15:16:19 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.018109066104575022
[09/26 15:16:25 visual_prompt]: Epoch 64 / 100: avg data time: 4.98e-02, avg batch time: 0.4632, average train loss: 0.0321
[09/26 15:16:27 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1585, average loss: 1.6187
[09/26 15:16:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 75.00	top5: 96.00	
[09/26 15:16:27 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.017274575140626316
[09/26 15:16:33 visual_prompt]: Epoch 65 / 100: avg data time: 5.29e-02, avg batch time: 0.4658, average train loss: 0.0272
[09/26 15:16:35 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1587, average loss: 1.9198
[09/26 15:16:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 96.50	
[09/26 15:16:35 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.016449496416858285
[09/26 15:16:41 visual_prompt]: Epoch 66 / 100: avg data time: 6.37e-02, avg batch time: 0.4757, average train loss: 0.0157
[09/26 15:16:43 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1584, average loss: 1.5782
[09/26 15:16:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 96.50	
[09/26 15:16:43 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0156348351646022
[09/26 15:16:49 visual_prompt]: Epoch 67 / 100: avg data time: 6.14e-02, avg batch time: 0.4741, average train loss: 0.0161
[09/26 15:16:51 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1584, average loss: 1.7824
[09/26 15:16:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 97.00	
[09/26 15:16:51 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.014831583923105
[09/26 15:16:57 visual_prompt]: Epoch 68 / 100: avg data time: 5.98e-02, avg batch time: 0.4727, average train loss: 0.0119
[09/26 15:16:59 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1582, average loss: 1.8078
[09/26 15:16:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 96.00	
[09/26 15:16:59 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.014040721330273063
[09/26 15:17:05 visual_prompt]: Epoch 69 / 100: avg data time: 5.99e-02, avg batch time: 0.4724, average train loss: 0.0062
[09/26 15:17:07 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 1.8440
[09/26 15:17:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 96.50	
[09/26 15:17:07 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.013263210930352737
[09/26 15:17:14 visual_prompt]: Epoch 70 / 100: avg data time: 6.42e-02, avg batch time: 0.4758, average train loss: 0.0058
[09/26 15:17:15 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1582, average loss: 1.8146
[09/26 15:17:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.50	
[09/26 15:17:15 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.012500000000000006
[09/26 15:17:22 visual_prompt]: Epoch 71 / 100: avg data time: 6.08e-02, avg batch time: 0.4736, average train loss: 0.0041
[09/26 15:17:23 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1581, average loss: 1.8079
[09/26 15:17:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:17:23 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.01175201839416988
[09/26 15:17:30 visual_prompt]: Epoch 72 / 100: avg data time: 6.05e-02, avg batch time: 0.4725, average train loss: 0.0032
[09/26 15:17:31 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1586, average loss: 1.8039
[09/26 15:17:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 96.00	
[09/26 15:17:31 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.011020177413231334
[09/26 15:17:38 visual_prompt]: Epoch 73 / 100: avg data time: 5.03e-02, avg batch time: 0.4659, average train loss: 0.0034
[09/26 15:17:39 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1579, average loss: 1.8134
[09/26 15:17:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 96.00	
[09/26 15:17:39 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.010305368692688175
[09/26 15:17:46 visual_prompt]: Epoch 74 / 100: avg data time: 6.10e-02, avg batch time: 0.4736, average train loss: 0.0036
[09/26 15:17:47 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1582, average loss: 1.8121
[09/26 15:17:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 96.50	
[09/26 15:17:47 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.009608463116858543
[09/26 15:17:54 visual_prompt]: Epoch 75 / 100: avg data time: 6.55e-02, avg batch time: 0.4768, average train loss: 0.0044
[09/26 15:17:55 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1581, average loss: 1.8135
[09/26 15:17:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.00	top5: 96.50	
[09/26 15:17:55 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.008930309757836517
[09/26 15:18:02 visual_prompt]: Epoch 76 / 100: avg data time: 5.83e-02, avg batch time: 0.4702, average train loss: 0.0032
[09/26 15:18:04 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1580, average loss: 1.8514
[09/26 15:18:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.50	
[09/26 15:18:04 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.008271734841028553
[09/26 15:18:10 visual_prompt]: Epoch 77 / 100: avg data time: 6.12e-02, avg batch time: 0.4729, average train loss: 0.0026
[09/26 15:18:12 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.1578, average loss: 1.8588
[09/26 15:18:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.50	
[09/26 15:18:12 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.007633540738525066
[09/26 15:18:18 visual_prompt]: Epoch 78 / 100: avg data time: 6.00e-02, avg batch time: 0.4730, average train loss: 0.0025
[09/26 15:18:20 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1582, average loss: 1.8577
[09/26 15:18:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 96.50	
[09/26 15:18:20 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.0070165049915337264
[09/26 15:18:26 visual_prompt]: Epoch 79 / 100: avg data time: 5.63e-02, avg batch time: 0.4688, average train loss: 0.0037
[09/26 15:18:28 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1586, average loss: 1.8497
[09/26 15:18:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:18:28 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.006421379363065142
[09/26 15:18:34 visual_prompt]: Epoch 80 / 100: avg data time: 5.28e-02, avg batch time: 0.4680, average train loss: 0.0027
[09/26 15:18:36 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1577, average loss: 1.8352
[09/26 15:18:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:18:36 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.005848888922025553
[09/26 15:18:42 visual_prompt]: Epoch 81 / 100: avg data time: 6.08e-02, avg batch time: 0.4727, average train loss: 0.0024
[09/26 15:18:44 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1584, average loss: 1.8470
[09/26 15:18:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 97.00	
[09/26 15:18:44 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.005299731159831953
[09/26 15:18:50 visual_prompt]: Epoch 82 / 100: avg data time: 6.40e-02, avg batch time: 0.4758, average train loss: 0.0022
[09/26 15:18:52 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1583, average loss: 1.8392
[09/26 15:18:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:18:52 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.004774575140626317
[09/26 15:18:58 visual_prompt]: Epoch 83 / 100: avg data time: 6.01e-02, avg batch time: 0.4746, average train loss: 0.0020
[09/26 15:19:00 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1581, average loss: 1.8336
[09/26 15:19:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.00	top5: 97.00	
[09/26 15:19:00 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.004274060686123959
[09/26 15:19:07 visual_prompt]: Epoch 84 / 100: avg data time: 6.61e-02, avg batch time: 0.4782, average train loss: 0.0034
[09/26 15:19:08 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1587, average loss: 1.8502
[09/26 15:19:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 97.00	
[09/26 15:19:08 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.003798797596089351
[09/26 15:19:15 visual_prompt]: Epoch 85 / 100: avg data time: 5.99e-02, avg batch time: 0.4728, average train loss: 0.0019
[09/26 15:19:16 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1584, average loss: 1.8673
[09/26 15:19:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 97.00	
[09/26 15:19:16 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.0033493649053890325
[09/26 15:19:23 visual_prompt]: Epoch 86 / 100: avg data time: 4.70e-02, avg batch time: 0.4598, average train loss: 0.0017
[09/26 15:19:24 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1585, average loss: 1.8657
[09/26 15:19:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 97.00	
[09/26 15:19:24 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.0029263101785268254
[09/26 15:19:31 visual_prompt]: Epoch 87 / 100: avg data time: 6.66e-02, avg batch time: 0.4795, average train loss: 0.0024
[09/26 15:19:32 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1584, average loss: 1.8629
[09/26 15:19:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 97.00	
[09/26 15:19:32 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00253014884252083
[09/26 15:19:39 visual_prompt]: Epoch 88 / 100: avg data time: 4.68e-02, avg batch time: 0.4602, average train loss: 0.0033
[09/26 15:19:40 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1581, average loss: 1.8569
[09/26 15:19:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:19:40 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.0021613635589349755
[09/26 15:19:47 visual_prompt]: Epoch 89 / 100: avg data time: 5.01e-02, avg batch time: 0.4635, average train loss: 0.0026
[09/26 15:19:48 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1581, average loss: 1.8524
[09/26 15:19:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:19:48 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0018204036358303172
[09/26 15:19:55 visual_prompt]: Epoch 90 / 100: avg data time: 6.31e-02, avg batch time: 0.4763, average train loss: 0.0018
[09/26 15:19:56 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1586, average loss: 1.8522
[09/26 15:19:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:19:56 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0015076844803522923
[09/26 15:20:03 visual_prompt]: Epoch 91 / 100: avg data time: 6.28e-02, avg batch time: 0.4760, average train loss: 0.0020
[09/26 15:20:04 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1581, average loss: 1.8531
[09/26 15:20:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:20:04 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0012235870926211618
[09/26 15:20:11 visual_prompt]: Epoch 92 / 100: avg data time: 6.24e-02, avg batch time: 0.4754, average train loss: 0.0018
[09/26 15:20:13 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1584, average loss: 1.8526
[09/26 15:20:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:20:13 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0009684576015420277
[09/26 15:20:19 visual_prompt]: Epoch 93 / 100: avg data time: 6.10e-02, avg batch time: 0.4739, average train loss: 0.0018
[09/26 15:20:21 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1586, average loss: 1.8530
[09/26 15:20:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:20:21 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0007426068431000883
[09/26 15:20:27 visual_prompt]: Epoch 94 / 100: avg data time: 5.19e-02, avg batch time: 0.4646, average train loss: 0.0023
[09/26 15:20:29 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.1587, average loss: 1.8526
[09/26 15:20:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:20:29 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0005463099816548578
[09/26 15:20:35 visual_prompt]: Epoch 95 / 100: avg data time: 6.32e-02, avg batch time: 0.4756, average train loss: 0.0023
[09/26 15:20:37 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1587, average loss: 1.8519
[09/26 15:20:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 74.00	top5: 97.00	
[09/26 15:20:37 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00037980617469479954
[09/26 15:20:43 visual_prompt]: Epoch 96 / 100: avg data time: 5.03e-02, avg batch time: 0.4644, average train loss: 0.0021
[09/26 15:20:45 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1580, average loss: 1.8512
[09/26 15:20:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:20:45 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00024329828146074097
[09/26 15:20:51 visual_prompt]: Epoch 97 / 100: avg data time: 5.75e-02, avg batch time: 0.4703, average train loss: 0.0024
[09/26 15:20:53 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1586, average loss: 1.8506
[09/26 15:20:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:20:53 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.00013695261579316776
[09/26 15:20:59 visual_prompt]: Epoch 98 / 100: avg data time: 4.37e-02, avg batch time: 0.4571, average train loss: 0.0019
[09/26 15:21:01 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1583, average loss: 1.8503
[09/26 15:21:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:21:01 visual_prompt]: Training 99 / 100 epoch, with learning rate 6.089874350439506e-05
[09/26 15:21:07 visual_prompt]: Epoch 99 / 100: avg data time: 6.27e-02, avg batch time: 0.4751, average train loss: 0.0021
[09/26 15:21:09 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1584, average loss: 1.8501
[09/26 15:21:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:21:09 visual_prompt]: Training 100 / 100 epoch, with learning rate 1.5229324522605948e-05
[09/26 15:21:15 visual_prompt]: Epoch 100 / 100: avg data time: 5.40e-02, avg batch time: 0.4684, average train loss: 0.0019
[09/26 15:21:17 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1582, average loss: 1.8501
[09/26 15:21:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.50	top5: 97.00	
[09/26 15:21:17 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 15:21:17 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A100-PCIE-40GB
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 15:21:17 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-svhn', 'DATA.NUMBER_CLASSES', '10', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 15:21:17 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 15:21:17 visual_prompt]: Training with config:
[09/26 15:21:17 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-svhn/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.05_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-svhn', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 10, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 15:21:17 visual_prompt]: Loading training data...
[09/26 15:21:17 visual_prompt]: Constructing vtab-svhn dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[:800], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 15:21:18 visual_prompt]: Number of images: 800
[09/26 15:21:18 visual_prompt]: Number of classes: 10 / 10
[09/26 15:21:18 visual_prompt]: Loading validation data...
[09/26 15:21:18 visual_prompt]: Constructing vtab-svhn dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset svhn_cropped (visual_prompt_tuning/data_path/svhn_cropped/3.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset svhn_cropped for split train[65931:66131], from visual_prompt_tuning/data_path/svhn_cropped/3.0.0
[09/26 15:21:18 visual_prompt]: Number of images: 200
[09/26 15:21:18 visual_prompt]: Number of classes: 10 / 10
[09/26 15:21:18 visual_prompt]: Constructing models...
[09/26 15:21:20 visual_prompt]: Total Parameters: 86267146	 Gradient Parameters: 468490
[09/26 15:21:20 visual_prompt]: tuned percent:0.543
[09/26 15:21:21 visual_prompt]: Device used for model: 0
[09/26 15:21:21 visual_prompt]: Setting up Evaluator...
[09/26 15:21:21 visual_prompt]: Setting up Trainer...
[09/26 15:21:21 visual_prompt]: 	Setting up the optimizer...
[09/26 15:21:21 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 15:21:27 visual_prompt]: Epoch 1 / 100: avg data time: 5.78e-02, avg batch time: 0.4745, average train loss: 2.6804
[09/26 15:21:29 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1586, average loss: 2.6214
[09/26 15:21:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 54.00	
[09/26 15:21:29 visual_prompt]: Best epoch 1: best metric: 0.230
[09/26 15:21:29 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[09/26 15:21:35 visual_prompt]: Epoch 2 / 100: avg data time: 5.63e-02, avg batch time: 0.4697, average train loss: 2.3684
[09/26 15:21:37 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1581, average loss: 2.2447
[09/26 15:21:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 15:21:37 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[09/26 15:21:43 visual_prompt]: Epoch 3 / 100: avg data time: 6.09e-02, avg batch time: 0.4727, average train loss: 2.2593
[09/26 15:21:45 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1578, average loss: 2.2319
[09/26 15:21:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.00	
[09/26 15:21:45 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[09/26 15:21:51 visual_prompt]: Epoch 4 / 100: avg data time: 5.42e-02, avg batch time: 0.4687, average train loss: 2.2465
[09/26 15:21:53 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1583, average loss: 2.2149
[09/26 15:21:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 15:21:53 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[09/26 15:21:59 visual_prompt]: Epoch 5 / 100: avg data time: 5.53e-02, avg batch time: 0.4677, average train loss: 2.2550
[09/26 15:22:01 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1586, average loss: 2.2216
[09/26 15:22:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.50	
[09/26 15:22:01 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[09/26 15:22:07 visual_prompt]: Epoch 6 / 100: avg data time: 6.17e-02, avg batch time: 0.4753, average train loss: 2.2420
[09/26 15:22:09 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1588, average loss: 2.2127
[09/26 15:22:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 65.00	
[09/26 15:22:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[09/26 15:22:15 visual_prompt]: Epoch 7 / 100: avg data time: 6.76e-02, avg batch time: 0.4799, average train loss: 2.2464
[09/26 15:22:17 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1582, average loss: 2.2154
[09/26 15:22:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 63.50	
[09/26 15:22:17 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[09/26 15:22:23 visual_prompt]: Epoch 8 / 100: avg data time: 5.91e-02, avg batch time: 0.4729, average train loss: 2.2400
[09/26 15:22:25 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1583, average loss: 2.2146
[09/26 15:22:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 64.00	
[09/26 15:22:25 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[09/26 15:22:32 visual_prompt]: Epoch 9 / 100: avg data time: 6.35e-02, avg batch time: 0.4757, average train loss: 2.2376
[09/26 15:22:33 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1584, average loss: 2.2125
[09/26 15:22:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 61.50	
[09/26 15:22:33 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[09/26 15:22:40 visual_prompt]: Epoch 10 / 100: avg data time: 6.08e-02, avg batch time: 0.4740, average train loss: 2.2375
[09/26 15:22:41 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1577, average loss: 2.2052
[09/26 15:22:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.00	top5: 67.00	
[09/26 15:22:41 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[09/26 15:22:48 visual_prompt]: Epoch 11 / 100: avg data time: 6.33e-02, avg batch time: 0.4759, average train loss: 2.2240
[09/26 15:22:49 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1584, average loss: 2.2456
[09/26 15:22:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 15.50	top5: 67.50	
[09/26 15:22:49 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[09/26 15:22:56 visual_prompt]: Epoch 12 / 100: avg data time: 5.57e-02, avg batch time: 0.4681, average train loss: 2.2300
[09/26 15:22:57 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1586, average loss: 2.2128
[09/26 15:22:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 21.50	top5: 61.50	
[09/26 15:22:57 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[09/26 15:23:04 visual_prompt]: Epoch 13 / 100: avg data time: 5.75e-02, avg batch time: 0.4707, average train loss: 2.2021
[09/26 15:23:06 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1588, average loss: 2.1784
[09/26 15:23:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.00	top5: 66.50	
[09/26 15:23:06 visual_prompt]: Best epoch 13: best metric: 0.250
[09/26 15:23:06 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[09/26 15:23:12 visual_prompt]: Epoch 14 / 100: avg data time: 6.31e-02, avg batch time: 0.4764, average train loss: 2.1798
[09/26 15:23:14 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1588, average loss: 2.1392
[09/26 15:23:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 25.00	top5: 69.50	
[09/26 15:23:14 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[09/26 15:23:20 visual_prompt]: Epoch 15 / 100: avg data time: 5.80e-02, avg batch time: 0.4704, average train loss: 2.2127
[09/26 15:23:22 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1588, average loss: 2.1656
[09/26 15:23:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.00	top5: 70.00	
[09/26 15:23:22 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[09/26 15:23:28 visual_prompt]: Epoch 16 / 100: avg data time: 6.08e-02, avg batch time: 0.4733, average train loss: 2.1697
[09/26 15:23:30 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1589, average loss: 2.1011
[09/26 15:23:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 28.00	top5: 70.50	
[09/26 15:23:30 visual_prompt]: Best epoch 16: best metric: 0.280
[09/26 15:23:30 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[09/26 15:23:36 visual_prompt]: Epoch 17 / 100: avg data time: 5.02e-02, avg batch time: 0.4643, average train loss: 2.0979
[09/26 15:23:38 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1581, average loss: 2.1438
[09/26 15:23:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 24.50	top5: 69.50	
[09/26 15:23:38 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[09/26 15:23:44 visual_prompt]: Epoch 18 / 100: avg data time: 5.87e-02, avg batch time: 0.4711, average train loss: 2.0623
[09/26 15:23:46 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1589, average loss: 2.1570
[09/26 15:23:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 21.00	top5: 71.00	
[09/26 15:23:46 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[09/26 15:23:52 visual_prompt]: Epoch 19 / 100: avg data time: 6.41e-02, avg batch time: 0.4769, average train loss: 2.0803
[09/26 15:23:54 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1586, average loss: 2.1725
[09/26 15:23:54 visual_prompt]: Classification results with val_vtab-svhn: top1: 23.50	top5: 63.00	
[09/26 15:23:54 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[09/26 15:24:00 visual_prompt]: Epoch 20 / 100: avg data time: 6.26e-02, avg batch time: 0.4760, average train loss: 2.0315
[09/26 15:24:02 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1586, average loss: 2.0043
[09/26 15:24:02 visual_prompt]: Classification results with val_vtab-svhn: top1: 30.50	top5: 73.50	
[09/26 15:24:02 visual_prompt]: Best epoch 20: best metric: 0.305
[09/26 15:24:02 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[09/26 15:24:09 visual_prompt]: Epoch 21 / 100: avg data time: 6.22e-02, avg batch time: 0.4759, average train loss: 1.9467
[09/26 15:24:10 visual_prompt]: Inference (val):avg data time: 4.63e-05, avg batch time: 0.1588, average loss: 2.0343
[09/26 15:24:10 visual_prompt]: Classification results with val_vtab-svhn: top1: 30.50	top5: 78.50	
[09/26 15:24:10 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[09/26 15:24:17 visual_prompt]: Epoch 22 / 100: avg data time: 5.44e-02, avg batch time: 0.4682, average train loss: 1.8907
[09/26 15:24:18 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1591, average loss: 1.8958
[09/26 15:24:18 visual_prompt]: Classification results with val_vtab-svhn: top1: 35.00	top5: 82.00	
[09/26 15:24:18 visual_prompt]: Best epoch 22: best metric: 0.350
[09/26 15:24:18 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[09/26 15:24:25 visual_prompt]: Epoch 23 / 100: avg data time: 5.25e-02, avg batch time: 0.4674, average train loss: 1.7702
[09/26 15:24:26 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1586, average loss: 2.0847
[09/26 15:24:26 visual_prompt]: Classification results with val_vtab-svhn: top1: 37.00	top5: 83.00	
[09/26 15:24:26 visual_prompt]: Best epoch 23: best metric: 0.370
[09/26 15:24:26 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[09/26 15:24:33 visual_prompt]: Epoch 24 / 100: avg data time: 4.84e-02, avg batch time: 0.4637, average train loss: 1.6331
[09/26 15:24:34 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1586, average loss: 1.6203
[09/26 15:24:34 visual_prompt]: Classification results with val_vtab-svhn: top1: 38.50	top5: 88.50	
[09/26 15:24:34 visual_prompt]: Best epoch 24: best metric: 0.385
[09/26 15:24:34 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[09/26 15:24:41 visual_prompt]: Epoch 25 / 100: avg data time: 6.48e-02, avg batch time: 0.4771, average train loss: 1.4556
[09/26 15:24:42 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1586, average loss: 1.6972
[09/26 15:24:42 visual_prompt]: Classification results with val_vtab-svhn: top1: 41.50	top5: 86.50	
[09/26 15:24:42 visual_prompt]: Best epoch 25: best metric: 0.415
[09/26 15:24:42 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[09/26 15:24:49 visual_prompt]: Epoch 26 / 100: avg data time: 4.98e-02, avg batch time: 0.4638, average train loss: 1.4673
[09/26 15:24:50 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1583, average loss: 1.5435
[09/26 15:24:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 45.00	top5: 90.50	
[09/26 15:24:50 visual_prompt]: Best epoch 26: best metric: 0.450
[09/26 15:24:50 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.04620120240391065
[09/26 15:24:57 visual_prompt]: Epoch 27 / 100: avg data time: 6.32e-02, avg batch time: 0.4763, average train loss: 1.3628
[09/26 15:24:58 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1587, average loss: 1.5217
[09/26 15:24:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 47.50	top5: 91.00	
[09/26 15:24:58 visual_prompt]: Best epoch 27: best metric: 0.475
[09/26 15:24:58 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.04572593931387604
[09/26 15:25:05 visual_prompt]: Epoch 28 / 100: avg data time: 6.37e-02, avg batch time: 0.4771, average train loss: 1.2568
[09/26 15:25:06 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1586, average loss: 1.4656
[09/26 15:25:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 45.00	top5: 92.50	
[09/26 15:25:06 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.04522542485937369
[09/26 15:25:13 visual_prompt]: Epoch 29 / 100: avg data time: 5.97e-02, avg batch time: 0.4720, average train loss: 1.0829
[09/26 15:25:15 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1584, average loss: 1.0758
[09/26 15:25:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 60.00	top5: 95.00	
[09/26 15:25:15 visual_prompt]: Best epoch 29: best metric: 0.600
[09/26 15:25:15 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.04470026884016805
[09/26 15:25:21 visual_prompt]: Epoch 30 / 100: avg data time: 5.51e-02, avg batch time: 0.4686, average train loss: 0.8953
[09/26 15:25:23 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1582, average loss: 1.1489
[09/26 15:25:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.00	top5: 95.00	
[09/26 15:25:23 visual_prompt]: Best epoch 30: best metric: 0.650
[09/26 15:25:23 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.04415111107797445
[09/26 15:25:29 visual_prompt]: Epoch 31 / 100: avg data time: 6.06e-02, avg batch time: 0.4731, average train loss: 0.8228
[09/26 15:25:31 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1586, average loss: 1.1852
[09/26 15:25:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 57.00	top5: 96.00	
[09/26 15:25:31 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.04357862063693486
[09/26 15:25:37 visual_prompt]: Epoch 32 / 100: avg data time: 5.54e-02, avg batch time: 0.4687, average train loss: 0.7347
[09/26 15:25:39 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1577, average loss: 1.4464
[09/26 15:25:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 56.50	top5: 92.00	
[09/26 15:25:39 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.04298349500846628
[09/26 15:25:45 visual_prompt]: Epoch 33 / 100: avg data time: 5.52e-02, avg batch time: 0.4677, average train loss: 0.7902
[09/26 15:25:47 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1584, average loss: 1.1801
[09/26 15:25:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.00	top5: 95.50	
[09/26 15:25:47 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.042366459261474934
[09/26 15:25:53 visual_prompt]: Epoch 34 / 100: avg data time: 5.21e-02, avg batch time: 0.4681, average train loss: 0.6046
[09/26 15:25:55 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1587, average loss: 1.2633
[09/26 15:25:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 57.50	top5: 96.00	
[09/26 15:25:55 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.04172826515897146
[09/26 15:26:01 visual_prompt]: Epoch 35 / 100: avg data time: 4.63e-02, avg batch time: 0.4606, average train loss: 0.5926
[09/26 15:26:03 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1584, average loss: 1.1330
[09/26 15:26:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 61.00	top5: 96.50	
[09/26 15:26:03 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.04106969024216348
[09/26 15:26:09 visual_prompt]: Epoch 36 / 100: avg data time: 5.16e-02, avg batch time: 0.4653, average train loss: 0.5422
[09/26 15:26:11 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1584, average loss: 1.2983
[09/26 15:26:11 visual_prompt]: Classification results with val_vtab-svhn: top1: 61.50	top5: 95.00	
[09/26 15:26:11 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.040391536883141455
[09/26 15:26:17 visual_prompt]: Epoch 37 / 100: avg data time: 5.81e-02, avg batch time: 0.4701, average train loss: 0.4820
[09/26 15:26:19 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1589, average loss: 1.2554
[09/26 15:26:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.50	top5: 94.50	
[09/26 15:26:19 visual_prompt]: Best epoch 37: best metric: 0.655
[09/26 15:26:19 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.03969463130731183
[09/26 15:26:25 visual_prompt]: Epoch 38 / 100: avg data time: 5.40e-02, avg batch time: 0.4681, average train loss: 0.3622
[09/26 15:26:27 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1590, average loss: 1.1866
[09/26 15:26:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 66.50	top5: 94.00	
[09/26 15:26:27 visual_prompt]: Best epoch 38: best metric: 0.665
[09/26 15:26:27 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.03897982258676867
[09/26 15:26:33 visual_prompt]: Epoch 39 / 100: avg data time: 5.77e-02, avg batch time: 0.4707, average train loss: 0.2816
[09/26 15:26:35 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1589, average loss: 1.7123
[09/26 15:26:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 64.00	top5: 94.00	
[09/26 15:26:35 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.03824798160583012
[09/26 15:26:41 visual_prompt]: Epoch 40 / 100: avg data time: 5.74e-02, avg batch time: 0.4698, average train loss: 0.3498
[09/26 15:26:43 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1589, average loss: 1.5155
[09/26 15:26:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 63.50	top5: 95.00	
[09/26 15:26:43 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.037500000000000006
[09/26 15:26:49 visual_prompt]: Epoch 41 / 100: avg data time: 5.92e-02, avg batch time: 0.4714, average train loss: 0.2960
[09/26 15:26:51 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1585, average loss: 1.2880
[09/26 15:26:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.00	top5: 96.00	
[09/26 15:26:51 visual_prompt]: Best epoch 41: best metric: 0.690
[09/26 15:26:51 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.03673678906964727
[09/26 15:26:57 visual_prompt]: Epoch 42 / 100: avg data time: 5.67e-02, avg batch time: 0.4725, average train loss: 0.2295
[09/26 15:26:59 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1584, average loss: 1.4642
[09/26 15:26:59 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.00	top5: 95.50	
[09/26 15:26:59 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.03595927866972694
[09/26 15:27:05 visual_prompt]: Epoch 43 / 100: avg data time: 5.44e-02, avg batch time: 0.4665, average train loss: 0.2115
[09/26 15:27:07 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1586, average loss: 1.6805
[09/26 15:27:07 visual_prompt]: Classification results with val_vtab-svhn: top1: 60.00	top5: 94.00	
[09/26 15:27:07 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.03516841607689501
[09/26 15:27:13 visual_prompt]: Epoch 44 / 100: avg data time: 4.98e-02, avg batch time: 0.4646, average train loss: 0.2615
[09/26 15:27:15 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1584, average loss: 1.5287
[09/26 15:27:15 visual_prompt]: Classification results with val_vtab-svhn: top1: 66.50	top5: 95.00	
[09/26 15:27:15 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0343651648353978
[09/26 15:27:21 visual_prompt]: Epoch 45 / 100: avg data time: 6.18e-02, avg batch time: 0.4762, average train loss: 0.1806
[09/26 15:27:23 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1583, average loss: 1.7292
[09/26 15:27:23 visual_prompt]: Classification results with val_vtab-svhn: top1: 65.50	top5: 94.00	
[09/26 15:27:23 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.033550503583141725
[09/26 15:27:29 visual_prompt]: Epoch 46 / 100: avg data time: 6.36e-02, avg batch time: 0.4767, average train loss: 0.1455
[09/26 15:27:31 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1585, average loss: 1.4785
[09/26 15:27:31 visual_prompt]: Classification results with val_vtab-svhn: top1: 64.00	top5: 94.50	
[09/26 15:27:31 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.032725424859373686
[09/26 15:27:38 visual_prompt]: Epoch 47 / 100: avg data time: 6.11e-02, avg batch time: 0.4756, average train loss: 0.1237
[09/26 15:27:39 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1582, average loss: 1.4249
[09/26 15:27:39 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.00	top5: 94.00	
[09/26 15:27:39 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.03189093389542498
[09/26 15:27:46 visual_prompt]: Epoch 48 / 100: avg data time: 6.29e-02, avg batch time: 0.4790, average train loss: 0.1014
[09/26 15:27:47 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1584, average loss: 1.4276
[09/26 15:27:47 visual_prompt]: Classification results with val_vtab-svhn: top1: 67.50	top5: 95.00	
[09/26 15:27:47 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.03104804738999169
[09/26 15:27:54 visual_prompt]: Epoch 49 / 100: avg data time: 6.04e-02, avg batch time: 0.4734, average train loss: 0.0959
[09/26 15:27:55 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1576, average loss: 1.5857
[09/26 15:27:55 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.00	top5: 92.00	
[09/26 15:27:55 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.03019779227044398
[09/26 15:28:02 visual_prompt]: Epoch 50 / 100: avg data time: 6.24e-02, avg batch time: 0.4747, average train loss: 0.1208
[09/26 15:28:03 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1585, average loss: 1.5412
[09/26 15:28:03 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.00	top5: 95.00	
[09/26 15:28:03 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.029341204441673263
[09/26 15:28:10 visual_prompt]: Epoch 51 / 100: avg data time: 5.98e-02, avg batch time: 0.4724, average train loss: 0.0758
[09/26 15:28:12 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1583, average loss: 1.5570
[09/26 15:28:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 68.00	top5: 97.00	
[09/26 15:28:12 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.028479327524001636
[09/26 15:28:18 visual_prompt]: Epoch 52 / 100: avg data time: 4.65e-02, avg batch time: 0.4625, average train loss: 0.0333
[09/26 15:28:19 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1584, average loss: 1.5469
[09/26 15:28:19 visual_prompt]: Classification results with val_vtab-svhn: top1: 66.00	top5: 96.50	
[09/26 15:28:19 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.02761321158169134
[09/26 15:28:26 visual_prompt]: Epoch 53 / 100: avg data time: 4.78e-02, avg batch time: 0.4619, average train loss: 0.0278
[09/26 15:28:27 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.1582, average loss: 1.7521
[09/26 15:28:27 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.50	top5: 96.00	
[09/26 15:28:27 visual_prompt]: Best epoch 53: best metric: 0.695
[09/26 15:28:27 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.02674391184360313
[09/26 15:28:34 visual_prompt]: Epoch 54 / 100: avg data time: 4.65e-02, avg batch time: 0.4602, average train loss: 0.0258
[09/26 15:28:35 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1581, average loss: 1.5593
[09/26 15:28:35 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
[09/26 15:28:35 visual_prompt]: Best epoch 54: best metric: 0.720
[09/26 15:28:35 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.02587248741756253
[09/26 15:28:42 visual_prompt]: Epoch 55 / 100: avg data time: 6.18e-02, avg batch time: 0.4755, average train loss: 0.0159
[09/26 15:28:43 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1587, average loss: 1.5007
[09/26 15:28:43 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.50	
[09/26 15:28:43 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.025
[09/26 15:28:50 visual_prompt]: Epoch 56 / 100: avg data time: 5.82e-02, avg batch time: 0.4716, average train loss: 0.0122
[09/26 15:28:51 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.1587, average loss: 1.6175
[09/26 15:28:51 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.50	top5: 96.00	
[09/26 15:28:51 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.024127512582437484
[09/26 15:28:58 visual_prompt]: Epoch 57 / 100: avg data time: 6.04e-02, avg batch time: 0.4736, average train loss: 0.0102
[09/26 15:29:00 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1581, average loss: 1.5515
[09/26 15:29:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 96.00	
[09/26 15:29:00 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.02325608815639687
[09/26 15:29:06 visual_prompt]: Epoch 58 / 100: avg data time: 5.82e-02, avg batch time: 0.4717, average train loss: 0.0069
[09/26 15:29:08 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1582, average loss: 1.6122
[09/26 15:29:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 97.00	
[09/26 15:29:08 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.02238678841830867
[09/26 15:29:14 visual_prompt]: Epoch 59 / 100: avg data time: 5.98e-02, avg batch time: 0.4743, average train loss: 0.0099
[09/26 15:29:16 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1586, average loss: 1.6447
[09/26 15:29:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 96.50	
[09/26 15:29:16 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.02152067247599837
[09/26 15:29:22 visual_prompt]: Epoch 60 / 100: avg data time: 6.01e-02, avg batch time: 0.4743, average train loss: 0.0078
[09/26 15:29:24 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.1583, average loss: 1.5416
[09/26 15:29:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 97.00	
[09/26 15:29:24 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.020658795558326743
[09/26 15:29:30 visual_prompt]: Epoch 61 / 100: avg data time: 6.18e-02, avg batch time: 0.4744, average train loss: 0.0042
[09/26 15:29:32 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1585, average loss: 1.6220
[09/26 15:29:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.50	top5: 96.00	
[09/26 15:29:32 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.019802207729556022
[09/26 15:29:38 visual_prompt]: Epoch 62 / 100: avg data time: 6.06e-02, avg batch time: 0.4728, average train loss: 0.0041
[09/26 15:29:40 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1583, average loss: 1.6530
[09/26 15:29:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 69.50	top5: 95.50	
[09/26 15:29:40 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.018951952610008312
[09/26 15:29:47 visual_prompt]: Epoch 63 / 100: avg data time: 5.73e-02, avg batch time: 0.4708, average train loss: 0.0029
[09/26 15:29:48 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1587, average loss: 1.6689
[09/26 15:29:48 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 96.00	
[09/26 15:29:48 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.018109066104575022
[09/26 15:29:54 visual_prompt]: Epoch 64 / 100: avg data time: 5.12e-02, avg batch time: 0.4644, average train loss: 0.0036
[09/26 15:29:56 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.1587, average loss: 1.6961
[09/26 15:29:56 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 95.50	
[09/26 15:29:56 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.017274575140626316
[09/26 15:30:03 visual_prompt]: Epoch 65 / 100: avg data time: 6.02e-02, avg batch time: 0.4728, average train loss: 0.0036
[09/26 15:30:04 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1587, average loss: 1.6868
[09/26 15:30:04 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 96.00	
[09/26 15:30:04 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.016449496416858285
[09/26 15:30:11 visual_prompt]: Epoch 66 / 100: avg data time: 6.03e-02, avg batch time: 0.4735, average train loss: 0.0026
[09/26 15:30:12 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1585, average loss: 1.6796
[09/26 15:30:12 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 96.50	
[09/26 15:30:12 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0156348351646022
[09/26 15:30:19 visual_prompt]: Epoch 67 / 100: avg data time: 4.66e-02, avg batch time: 0.4624, average train loss: 0.0038
[09/26 15:30:20 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1586, average loss: 1.6186
[09/26 15:30:20 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 95.50	
[09/26 15:30:20 visual_prompt]: Best epoch 67: best metric: 0.730
[09/26 15:30:20 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.014831583923105
[09/26 15:30:27 visual_prompt]: Epoch 68 / 100: avg data time: 5.78e-02, avg batch time: 0.4710, average train loss: 0.0028
[09/26 15:30:28 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1584, average loss: 1.6499
[09/26 15:30:28 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 95.50	
[09/26 15:30:28 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.014040721330273063
[09/26 15:30:35 visual_prompt]: Epoch 69 / 100: avg data time: 5.96e-02, avg batch time: 0.4722, average train loss: 0.0025
[09/26 15:30:36 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1587, average loss: 1.6818
[09/26 15:30:36 visual_prompt]: Classification results with val_vtab-svhn: top1: 73.00	top5: 96.00	
[09/26 15:30:36 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.013263210930352737
[09/26 15:30:43 visual_prompt]: Epoch 70 / 100: avg data time: 5.47e-02, avg batch time: 0.4676, average train loss: 0.0019
[09/26 15:30:44 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.1582, average loss: 1.6673
[09/26 15:30:44 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.50	top5: 96.00	
[09/26 15:30:44 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.012500000000000006
[09/26 15:30:51 visual_prompt]: Epoch 71 / 100: avg data time: 4.74e-02, avg batch time: 0.4600, average train loss: 0.0018
[09/26 15:30:52 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1584, average loss: 1.6773
[09/26 15:30:52 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
[09/26 15:30:52 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.01175201839416988
[09/26 15:30:59 visual_prompt]: Epoch 72 / 100: avg data time: 6.19e-02, avg batch time: 0.4747, average train loss: 0.0014
[09/26 15:31:00 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1584, average loss: 1.6944
[09/26 15:31:00 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 96.00	
[09/26 15:31:00 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.011020177413231334
[09/26 15:31:07 visual_prompt]: Epoch 73 / 100: avg data time: 4.64e-02, avg batch time: 0.4608, average train loss: 0.0022
[09/26 15:31:08 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1588, average loss: 1.7038
[09/26 15:31:08 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 96.00	
[09/26 15:31:08 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.010305368692688175
[09/26 15:31:15 visual_prompt]: Epoch 74 / 100: avg data time: 5.23e-02, avg batch time: 0.4665, average train loss: 0.0018
[09/26 15:31:16 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.1583, average loss: 1.7082
[09/26 15:31:16 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
[09/26 15:31:16 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.009608463116858543
[09/26 15:31:23 visual_prompt]: Epoch 75 / 100: avg data time: 6.18e-02, avg batch time: 0.4763, average train loss: 0.0022
[09/26 15:31:24 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1587, average loss: 1.7071
[09/26 15:31:24 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
[09/26 15:31:24 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.008930309757836517
[09/26 15:31:31 visual_prompt]: Epoch 76 / 100: avg data time: 6.17e-02, avg batch time: 0.4739, average train loss: 0.0018
[09/26 15:31:32 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1585, average loss: 1.7078
[09/26 15:31:32 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
[09/26 15:31:32 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.008271734841028553
[09/26 15:31:39 visual_prompt]: Epoch 77 / 100: avg data time: 5.74e-02, avg batch time: 0.4719, average train loss: 0.0017
[09/26 15:31:40 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1582, average loss: 1.7156
[09/26 15:31:40 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 96.00	
[09/26 15:31:40 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.007633540738525066
[09/26 15:31:47 visual_prompt]: Epoch 78 / 100: avg data time: 6.50e-02, avg batch time: 0.4783, average train loss: 0.0018
[09/26 15:31:49 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1584, average loss: 1.7159
[09/26 15:31:49 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 96.00	
[09/26 15:31:49 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.0070165049915337264
[09/26 15:31:55 visual_prompt]: Epoch 79 / 100: avg data time: 5.12e-02, avg batch time: 0.4634, average train loss: 0.0017
[09/26 15:31:57 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1586, average loss: 1.7265
[09/26 15:31:57 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 96.00	
[09/26 15:31:57 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.006421379363065142
[09/26 15:32:03 visual_prompt]: Epoch 80 / 100: avg data time: 4.89e-02, avg batch time: 0.4634, average train loss: 0.0013
[09/26 15:32:05 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1587, average loss: 1.7345
[09/26 15:32:05 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 96.00	
[09/26 15:32:05 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.005848888922025553
[09/26 15:32:11 visual_prompt]: Epoch 81 / 100: avg data time: 5.75e-02, avg batch time: 0.4710, average train loss: 0.0014
[09/26 15:32:13 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1586, average loss: 1.7468
[09/26 15:32:13 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.00	top5: 96.00	
[09/26 15:32:13 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.005299731159831953
[09/26 15:32:19 visual_prompt]: Epoch 82 / 100: avg data time: 6.14e-02, avg batch time: 0.4741, average train loss: 0.0012
[09/26 15:32:21 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1583, average loss: 1.7570
[09/26 15:32:21 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 96.00	
[09/26 15:32:21 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.004774575140626317
[09/26 15:32:27 visual_prompt]: Epoch 83 / 100: avg data time: 5.82e-02, avg batch time: 0.4713, average train loss: 0.0014
[09/26 15:32:29 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1581, average loss: 1.7571
[09/26 15:32:29 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 96.00	
[09/26 15:32:29 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.004274060686123959
[09/26 15:32:35 visual_prompt]: Epoch 84 / 100: avg data time: 6.11e-02, avg batch time: 0.4748, average train loss: 0.0014
[09/26 15:32:37 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1581, average loss: 1.7567
[09/26 15:32:37 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 96.00	
[09/26 15:32:37 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.003798797596089351
[09/26 15:32:43 visual_prompt]: Epoch 85 / 100: avg data time: 5.92e-02, avg batch time: 0.4732, average train loss: 0.0013
[09/26 15:32:45 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1581, average loss: 1.7553
[09/26 15:32:45 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 96.00	
[09/26 15:32:45 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.0033493649053890325
[09/26 15:32:51 visual_prompt]: Epoch 86 / 100: avg data time: 6.22e-02, avg batch time: 0.4753, average train loss: 0.0016
[09/26 15:32:53 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1584, average loss: 1.7553
[09/26 15:32:53 visual_prompt]: Classification results with val_vtab-svhn: top1: 70.50	top5: 96.00	
[09/26 15:32:53 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.0029263101785268254
[09/26 15:33:00 visual_prompt]: Epoch 87 / 100: avg data time: 6.61e-02, avg batch time: 0.4783, average train loss: 0.0011
[09/26 15:33:01 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1582, average loss: 1.7566
[09/26 15:33:01 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 96.00	
[09/26 15:33:01 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00253014884252083
[09/26 15:33:08 visual_prompt]: Epoch 88 / 100: avg data time: 5.31e-02, avg batch time: 0.4668, average train loss: 0.0016
[09/26 15:33:09 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1586, average loss: 1.7575
[09/26 15:33:09 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 96.00	
[09/26 15:33:09 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.0021613635589349755
[09/26 15:33:16 visual_prompt]: Epoch 89 / 100: avg data time: 6.09e-02, avg batch time: 0.4738, average train loss: 0.0012
[09/26 15:33:17 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1583, average loss: 1.7589
[09/26 15:33:17 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.00	top5: 96.00	
[09/26 15:33:17 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0018204036358303172
[09/26 15:33:24 visual_prompt]: Epoch 90 / 100: avg data time: 5.39e-02, avg batch time: 0.4668, average train loss: 0.0020
[09/26 15:33:25 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.1584, average loss: 1.7592
[09/26 15:33:25 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 96.00	
[09/26 15:33:25 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0015076844803522923
[09/26 15:33:32 visual_prompt]: Epoch 91 / 100: avg data time: 5.97e-02, avg batch time: 0.4721, average train loss: 0.0016
[09/26 15:33:33 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1582, average loss: 1.7600
[09/26 15:33:33 visual_prompt]: Classification results with val_vtab-svhn: top1: 71.50	top5: 96.00	
[09/26 15:33:33 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0012235870926211618
[09/26 15:33:40 visual_prompt]: Epoch 92 / 100: avg data time: 6.88e-02, avg batch time: 0.4828, average train loss: 0.0013
[09/26 15:33:41 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1584, average loss: 1.7600
[09/26 15:33:41 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
[09/26 15:33:41 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0009684576015420277
[09/26 15:33:48 visual_prompt]: Epoch 93 / 100: avg data time: 6.11e-02, avg batch time: 0.4743, average train loss: 0.0016
[09/26 15:33:50 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1589, average loss: 1.7591
[09/26 15:33:50 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
[09/26 15:33:50 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0007426068431000883
[09/26 15:33:56 visual_prompt]: Epoch 94 / 100: avg data time: 5.70e-02, avg batch time: 0.4702, average train loss: 0.0013
[09/26 15:33:58 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1584, average loss: 1.7576
[09/26 15:33:58 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
[09/26 15:33:58 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0005463099816548578
[09/26 15:34:04 visual_prompt]: Epoch 95 / 100: avg data time: 5.43e-02, avg batch time: 0.4684, average train loss: 0.0013
[09/26 15:34:06 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1581, average loss: 1.7567
[09/26 15:34:06 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
[09/26 15:34:06 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00037980617469479954
[09/26 15:34:12 visual_prompt]: Epoch 96 / 100: avg data time: 5.85e-02, avg batch time: 0.4716, average train loss: 0.0013
[09/26 15:34:14 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1587, average loss: 1.7564
[09/26 15:34:14 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
[09/26 15:34:14 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00024329828146074097
[09/26 15:34:20 visual_prompt]: Epoch 97 / 100: avg data time: 5.91e-02, avg batch time: 0.4735, average train loss: 0.0011
[09/26 15:34:22 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1587, average loss: 1.7562
[09/26 15:34:22 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
[09/26 15:34:22 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.00013695261579316776
[09/26 15:34:28 visual_prompt]: Epoch 98 / 100: avg data time: 6.08e-02, avg batch time: 0.4738, average train loss: 0.0012
[09/26 15:34:30 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1587, average loss: 1.7560
[09/26 15:34:30 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
[09/26 15:34:30 visual_prompt]: Training 99 / 100 epoch, with learning rate 6.089874350439506e-05
[09/26 15:34:37 visual_prompt]: Epoch 99 / 100: avg data time: 6.65e-02, avg batch time: 0.4786, average train loss: 0.0015
[09/26 15:34:38 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.1580, average loss: 1.7558
[09/26 15:34:38 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
[09/26 15:34:38 visual_prompt]: Training 100 / 100 epoch, with learning rate 1.5229324522605948e-05
[09/26 15:34:45 visual_prompt]: Epoch 100 / 100: avg data time: 6.18e-02, avg batch time: 0.4746, average train loss: 0.0013
[09/26 15:34:46 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1585, average loss: 1.7557
[09/26 15:34:46 visual_prompt]: Classification results with val_vtab-svhn: top1: 72.00	top5: 96.00	
Traceback (most recent call last):
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_vtab.py", line 289, in <module>
    main(args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_vtab.py", line 284, in main
    train(cfg, args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_vtab.py", line 113, in train
    seed(cfg)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_vtab.py", line 136, in seed
    torch.manual_seed(SEED)
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/torch/random.py", line 36, in manual_seed
    seed = int(seed)
           ^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'tuple'
